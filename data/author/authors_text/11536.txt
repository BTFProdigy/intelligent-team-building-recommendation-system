Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 450?458,
Singapore, 6-7 August 2009.
c?2009 ACL and AFNLP
Joint Learning of Preposition Senses and
Semantic Roles of Prepositional Phrases
Daniel Dahlmeier
1
, Hwee Tou Ng
1,2
, Tanja Schultz
3
1
NUS Graduate School for Integrative Sciences and Engineering
2
Department of Computer Science, National University of Singapore
3
Cognitive Systems Lab, University of Karlsruhe
{danielhe,nght}@comp.nus.edu.sg
tanja@ira.uka.de
Abstract
The sense of a preposition is related to the
semantics of its dominating prepositional
phrase. Knowing the sense of a prepo-
sition could help to correctly classify the
semantic role of the dominating preposi-
tional phrase and vice versa. In this pa-
per, we propose a joint probabilistic model
for word sense disambiguation of preposi-
tions and semantic role labeling of prepo-
sitional phrases. Our experiments on the
PropBank corpus show that jointly learn-
ing the word sense and the semantic role
leads to an improvement over state-of-the-
art individual classifier models on the two
tasks.
1 Introduction
Word sense disambiguation (WSD) and seman-
tic role labeling (SRL) are two key components
in natural language processing to find a semantic
representation for a sentence. Semantic role la-
beling is the task of determining the constituents
of a sentence that represent semantic arguments
with respect to a predicate and labeling each with
a semantic role. Word sense disambiguation tries
to determine the correct meaning of a word in a
given context. Ambiguous words occur frequently
in normal English text.
One word class which is both frequent and
highly ambiguous is preposition. The different
senses of a preposition express different relations
between the preposition complement and the rest
of the sentence. Semantic roles and word senses
offer two different inventories of ?meaning? for
prepositional phrases (PP): semantic roles distin-
guish between different verb complements while
word senses intend to fully capture the preposition
semantics at a more fine-grained level. In this pa-
per, we use the semantic roles from the PropBank
corpus and the preposition senses from the Prepo-
sition Project (TPP). Both corpora are explained
in more detail in the following section. The re-
lationship between the two inventories (PropBank
semantic roles and TPP preposition senses) is not
a simple one-to-one mapping, as we can see from
the following examples:
? She now lives with relatives [in
sense1
Alabama.]
ARGM-LOC
? The envelope arrives [in
sense1
the mail.]
ARG4
? [In
sense5
separate statements]
ARGM-LOC
the two
sides said they want to have ?further discus-
sions.?
In the first two examples, the sense of the preposi-
tion in is annotated as sense 1 (?surrounded by or
enclosed in?), following the definitions of the TPP,
but the semantic roles are different. In the first
example the semantic role is a locative adjunctive
argument (ARGM-LOC), while in the second ex-
ample it is ARG4 which denotes the ?end point or
destination? of the arriving action
1
. In the first and
third example, the semantic roles are the same, but
the preposition senses are different, i.e., sense 1
and sense 5 (?inclusion or involvement?).
Preposition senses and semantic roles provide
two different views on the semantics of PPs.
Knowing the semantic role of the PP could be
helpful to successfully disambiguate the sense of
the preposition. Likewise, the preposition sense
could provide valuable information to classify the
semantic role of the PP. This is especially so for
the semantic roles ARGM-LOC and ARGM-TMP,
where we expect a strong correlation with spatial
and temporal preposition senses respectively.
In this paper, we propose a probabilistic model
for joint inference on preposition senses and se-
mantic roles. For each prepositional phrase that
1
http://verbs.colorado.edu/framesets/arrive-v.html
450
has been identified as an argument of the pred-
icate, we jointly infer its semantic role and the
sense of the preposition that is the lexical head of
the prepositional phrase. That is, our model maxi-
mizes the joint probability of the semantic role and
the preposition sense.
Previous research has shown the benefit of
jointly learning semantic roles of multiple con-
stituents (Toutanova et al, 2008; Koomen et al,
2005). In contrast, our joint model makes pre-
dictions for a single constituent, but multiple tasks
(WSD and SRL) .
Our experiments show that adding the SRL
information leads to statistically significant im-
provements over an independent, state-of-the-art
WSD classifier. For the SRL task, we show statis-
tically significant improvements of our joint model
over an independent, state-of-the-art SRL clas-
sifier for locative and temporal adjunctive argu-
ments, even though the overall improvement over
all semantic roles is small. To the best of our
knowledge, no previous research has attempted to
perform preposition WSD and SRL of preposi-
tional phrases in a joint learning approach.
The remainder of this paper is structured as fol-
lows: First, we give an introduction to the WSD
and SRL task. Then, in Section 3, we describe the
individual and joint classifier models. The details
of the data set used in our experiments are given
in Section 4. In Section 5, we present experiments
and results. Section 6 summarizes related work,
before we conclude in the final section.
2 Task Description
This section gives an introduction to preposition
sense disambiguation and semantic role labeling
of prepositional phrases.
2.1 Preposition Sense Disambiguation
The task of word sense disambiguation is to find
the correct meaning of a word, given its context.
Most prior research on word sense disambigua-
tion has focused on disambiguating the senses of
nouns, verbs, and adjectives, but not on preposi-
tions. Word sense disambiguation can be framed
as a classification task. For each preposition, a
classifier is trained on a corpus of training exam-
ples annotated with preposition senses, and tested
on a set of unseen test examples.
To perform WSD for prepositions, it is neces-
sary to first find a set of suitable sense classes.
We adopt the sense inventory from the Preposition
Project (TPP) (Litkowski and Hargraves, 2005)
that was also used in the SemEval 2007 preposi-
tion WSD task (Litkowski and Hargraves, 2007).
TPP is an attempt to create a comprehensive lex-
ical database of English prepositions that is suit-
able for use in computational linguistics research.
For each of the over 300 prepositions and phrasal
prepositions, the database contains a set of sense
definitions, which are based on the Oxford Dic-
tionary of English. Every preposition has a set
of fine-grained senses, which are grouped together
into a smaller number of coarse-grained senses. In
our experiments, we only focus on coarse-grained
senses since better inter-annotator agreement can
be achieved on coarse-grained senses, which also
results in higher accuracy of the trainedWSD clas-
sifier.
2.2 Semantic Role Labeling
The task of semantic role labeling in the context
of PropBank (Palmer et al, 2005) is to label tree
nodes with semantic roles in a syntactic parse tree.
The PropBank corpus adds a semantic layer to
parse trees from the Wall Street Journal section of
the Penn Treebank II corpus (Marcus et al, 1993).
There are two classes of semantic roles: core argu-
ments and adjunctive arguments. Core arguments
are verb sense specific, i.e., their meaning is de-
fined relative to a specific verb sense. They are
labeled with consecutive numbers ARG0, ARG1,
etc. ARG0 usually denotes the AGENT and ARG1
the THEME of the event. Besides the core ar-
guments, a verb can have a number of adjunc-
tive arguments that express more general proper-
ties like time, location, or manner. They are la-
beled as ARGM plus a functional tag, e.g., LOC for
locative or TMP for temporal modifiers. Preposi-
tional phrases can appear as adjunctive arguments
or core arguments.
The standard approach to semantic role labeling
is to divide the task into two sequential sub-tasks:
identification and classification. During the identi-
fication phase, the system separates the nodes that
fill some semantic roles from the rest. During the
classification phase, the system assigns the exact
semantic roles for all nodes that are identified as
arguments. In this paper, we focus on the classi-
fication phase. That is, we assume that preposi-
tional phrases that are semantic arguments have
been identified correctly and concentrate on the
451
task of determining the semantic role of preposi-
tional phrases. The reason is that argument identi-
fication mostly relies on syntactic features, like the
path from the constituent to the predicate (Pradhan
et al, 2005). Consider, for example, the phrase in
the dark in the sentence: ?We are in the dark?, he
said. The phrase is clearly not an argument to the
verb say. But if we alter the syntactic structure
of the sentence appropriately (while the sense of
the preposition in remains unchanged), the same
phrase suddenly becomes an adjunctive argument:
In the dark, he said ?We are?. On the other hand,
we can easily find examples, where in has a differ-
ent sense, but the phrase always fills some seman-
tic role:
? In a separate manner, he said . . .
? In 1998, he said . . .
? In Washington, he said . . .
This illustrates that the preposition sense is inde-
pendent of whether the PP is an argument or not.
Thus, a joint learning model for argument identifi-
cation and preposition sense is unlikely to perform
better than the independent models.
3 Models
This section describes the models for preposition
sense disambiguation and semantic role labeling.
We compare three different models for each
task: First, we implement an independent model
that only uses task specific features from the liter-
ature. This serves as the baseline model. Second,
we extend the baseline model by adding the most
likely prediction of the other task as an additional
feature. This is equivalent to a pipeline model of
classifiers that feeds the prediction of one classifi-
cation step into the next stage. Finally, we present
a joint model to determine the preposition sense
and semantic role that maximize the joint proba-
bility.
3.1 WSD model
Our approach to building a preposition WSD clas-
sifier follows that of Lee and Ng (2002), who eval-
uated a set of different knowledge sources and
learning algorithms for WSD. However, in this pa-
per we use maximum entropy models
2
(instead of
support vector machines (SVM) reported in (Lee
2
Zhang Le?s Maximum Entropy Modeling Toolkit,
http://homepages.inf.ed.ac.uk/s0450736/maxent toolkit.html
and Ng, 2002)), because maximum entropy mod-
els output probability distributions, unlike SVM.
This property is useful in the joint model, as we
will see later. Maxent models have been success-
fully applied to various NLP tasks and achieve
state-of-the-art performance. There are two train-
ing parameters that have to be adjusted for maxent
models: the number of training iterations and the
Gaussian smoothing parameter. We find optimal
values for both parameters through 10-fold cross-
validation on the training set.
For every preposition, a baseline maxent model
is trained using a set of features reported in
the state-of-the-art WSD system of Lee and
Ng (2002). These features encode three knowl-
edge sources:
? Part-of-speech (POS) of surrounding words
? Single words in the surrounding context
? Local collocations
For part-of-speech features, we include the POS
tags of surrounding tokens from the same sentence
within a window of seven tokens around the target
prepositions. All tokens (i.e., all words and punc-
tuation symbols) are considered. We use the Penn
Treebank II POS tag set.
For the knowledge source single words in the
surrounding context, we consider all words from
the same sentence. The input sentence is tokenized
and all tokens that do not contain at least one al-
phabetical character (such as punctuation symbols
and numbers) and all words that appear on a stop-
word list are removed. The remaining words are
converted to lower case and replaced by their mor-
phological root form. Every unique morphologi-
cal root word contributes one binary feature, in-
dicating whether or not the word is present in the
context. The position of a word in the sentence is
ignored in this knowledge source.
The third knowledge source, local collocations,
encodes position-specific information of words
within a small window around the target prepo-
sition. For this knowledge source, we consider
unigrams, bigrams, and trigrams from a window
of seven tokens. The position of the target prepo-
sition inside the n-gram is marked with a special
character ? ?. Words are converted to lower case,
but no stemming or removal of stopwords is per-
formed. If a token falls outside the sentence, it is
replaced by the empty token symbol nil.
During testing, the maxent model computes the
452
conditional probability of the sense, given the fea-
ture representation of the surrounding context c.
The classifier outputs the sense that receives the
highest probability:
s? = argmax
s
P (s|?(c)) (1)
where ?(?) is a feature map from the surrounding
context to the feature representation.
To ensure that our model is competitive, we
tested our system on the data set from the SemEval
2007 preposition WSD task (Litkowski and Har-
graves, 2007). Our baseline classifier achieved a
coarse-grained accuracy of 70.7% (micro-average)
on the official test set. This would have made our
system the second best system in the competition,
behind the MELB-YB system (Ye and Baldwin,
2007).
We also investigate the effect of the semantic
role label by adding it as a feature to the base-
line model. This pipeline model is inspired by the
work of Dang and Palmer (2005) who investigated
the role of SRL features in verb WSD. We add
the semantic role of the prepositional phrase dom-
inating the preposition as a feature to the WSD
model. During training, the PropBank gold SRL
label is used. During testing, we rely on the base-
line SRL model (to be introduced in the next sub-
section) to predict the semantic role of the prepo-
sitional phrase. This is equivalent to first per-
forming semantic role labeling and adding the out-
put as a feature to the WSD classifier. In ear-
lier experiments, we found that training on gold
SRL labels gave better results than training on
automatically predicted SRL labels (using cross-
validation). Note that our approach uses automati-
cally assigned SRL labels during testing, while the
system of Dang and Palmer (2005) only uses gold
SRL labels.
3.2 SRL model
Our semantic role labeling classifier is also based
on maxent models. It has been shown that max-
imum entropy models achieve state-of-the-art re-
sults on SRL (Xue and Palmer, 2004; Toutanova
et al, 2008). Again, we find optimal values
for the training parameters through 10-fold cross-
validation on the training set.
By treating SRL as a classification problem, the
choice of appropriate features becomes a key is-
sue. Features are encoded as binary-valued func-
tions. During testing, the maxent model computes
Baseline Features (Gildea and Jurafsky, 2002)
pred predicate lemma
path path from constituent to predicate
ptype syntactic category (NP, PP, etc.)
pos relative position to the predicate
voice active or passive voice
hw syntactic head word of the phrase
sub-cat rule expanding the predicate?s parent
Advanced Features (Pradhan et al, 2005)
hw POS POS of the syntactic head word
PP hw/POS head word and POS of the rightmost
NP child if the phrase is a PP
first/last word first/last word and POS in the con-
stituent
parent ptype syntactic category of the parent node
parent hw/POS head word and POS of the parent
sister ptype phrase type of left and right sister
sister hw/POS head word and POS of left and right
sister
temporal temporal key words present
partPath partial path predicate
proPath projected path without directions
Feature Combinations (Xue and Palmer, 2004)
pred & ptype predicate and phrase type
pred & hw predicate and head word
pred & path predicate and path
pred & pos predicate and relative position
Table 1: SRL features for the baseline model
the conditional probability P (a|t, p, v) of the ar-
gument label a, given the parse tree t, predicate p,
and constituent node v. The classifier outputs the
semantic role with the highest probability:
a? = argmax
a
P (a|t, p, v) (2)
= argmax
a
P (a|?(t, p, v)) (3)
where ?(?, ?, ?) is a feature map to an appropriate
feature representation.
For our baseline SRL model, we adopt the fea-
tures used in other state-of-the-art SRL systems,
which include the seven baseline features from the
original work of Gildea and Jurafsky (2002), addi-
tional features taken from Pradhan et al (2005),
and feature combinations which are inspired by
the system in Xue and Palmer (2004). Table 1 lists
the features we use for easy reference.
In the pipeline model, we investigate the use-
fulness of the preposition sense as a feature for
SRL by adding the preposition lemma concate-
nated with the sense number (e.g., on 1) as a fea-
ture. During training, the gold annotated prepo-
sition sense is used. During testing, the sense is
automatically tagged by the baseline WSD model.
This is equivalent to first running the WSD clas-
sifier for all prepositions, and adding the output
preposition sense as a feature to our baseline SRL
453
system.
3.3 Joint Inference Model
The two previous models seek to maximize the
probability of the semantic role and the preposi-
tion sense individually, thus ignoring possible de-
pendencies between the two. Instead of maximiz-
ing the individual probabilities, we would like to
maximize the joint probability of the semantic role
and the preposition sense, given the parse tree,
predicate, constituent node, and surrounding con-
text.
?
(a, s) = argmax
(a,s)
P (a, s|t, p, v, c) (4)
We assume that the probability of the semantic
role is already determined by the syntactic parse
tree t, the predicate p, and the constituent node v,
and is conditionally independent of the remaining
surrounding context c given t, p, and v. Likewise,
we assume that the probability of the preposition
sense is conditionally independent of the parse tree
t, predicate p, and constituent v, given the sur-
rounding context c and the semantic role a. This
assumption allows us to factor the joint probability
into an SRL and a WSD component:
?
(a, s) = argmax
(a,s)
P (a|t, p, v)?P (s|c, a) (5)
= argmax
(a,s)
P (a|?(t, p, v))?P (s|?(c, a))(6)
We observe that the first component in our joint
model corresponds to the baseline SRL model
and the second component corresponds to the
WSD pipeline model. Because our maxent mod-
els output a complete probability distribution, we
can combine both components by multiplying the
probabilities. Theoretically, the joint probability
could be factored in the other way, by first com-
puting the probability of the preposition sense and
then conditioning the SRL model on the predicted
preposition sense. However, in our early exper-
iments, we found that this approach gave lower
classification accuracy.
During testing, the classifier seeks to find the
tuple of semantic role and preposition sense that
maximizes the joint probability. For every se-
mantic role, the classifier computes its probability
given the SRL features, and multiplies it by the
probability of the most likely preposition sense,
given the context and the semantic role. The tu-
ple that receives the highest joint probability is the
final output of the joint classifier.
Semantic Role Total Training Test
ARG0 28 15 13
ARG1 374 208 166
ARG2 649 352 297
ARG3 111 67 44
ARG4 177 91 86
ARGM-ADV 141 101 40
ARGM-CAU 31 23 8
ARGM-DIR 28 19 9
ARGM-DIS 29 9 20
ARGM-EXT 61 42 19
ARGM-LOC 954 668 286
ARGM-MNR 316 225 91
ARGM-PNC 115 78 37
ARGM-PRD 1 1 0
ARGM-REC 1 0 1
ARGM-TMP 838 563 275
Total 3854 2462 1392
Table 2: Number of annotated prepositional
phrases for each semantic role
4 Data Set
The joint model uses the probability of a prepo-
sition sense, given the semantic role of the dom-
inating prepositional phrase. To estimate this
probability, we need a corpus which is annotated
with both preposition senses and semantic roles.
Unfortunately, PropBank is not annotated with
preposition senses. Instead, we manually anno-
tated the seven most frequent prepositions in four
sections of the PropBank corpus with their senses
from the TPP dictionary. According to Juraf-
sky and Martin (2008), the most frequent English
prepositions are: of, in, for, to, with, on and at (in
order of frequency). Our counts on Sections 2 to
21 of PropBank revealed that these top 7 prepo-
sitions account for about 65% of all prepositional
phrases that are labeled with semantic roles.
The annotation proceeds in the following way.
First, we automatically extract all sentences which
have one of the prepositions as the lexical head of
a prepositional phrase. The position of the prepo-
sition is marked in the sentence. By only consid-
ering prepositional phrases, we automatically ex-
clude occurrences of the word to before infinitives
and instances of particle usage of prepositions,
such as phrasal verbs. The extracted prepositions
are manually tagged with their senses from the
TPP dictionary. Idiomatic usage of prepositions
like for example or in fact, and complex preposi-
tion constructions that involve more than one word
(e.g., because of, instead of, etc.) are excluded by
the annotators and compiled into a stoplist.
We annotated 3854 instances of the top 7 prepo-
454
Preposition Total Training Test
at 404 260 144
for 478 307 171
in 1590 1083 507
of 97 51 46
on 408 246 162
to 532 304 228
with 345 211 134
Total 3854 2462 1392
Table 3: Number of annotated prepositional
phrases for each preposition
sitions in Sections 2 to 4 and 23 of the PropBank
corpus. The data shows a strong correlation be-
tween semantic roles and preposition senses that
express a spatial or temporal meaning. For the
preposition in, 90.8% of the instances that ap-
pear inside an ARGM-LOC are tagged with sense 1
(?surrounded by or enclosed in?) or sense 5 (?in-
clusion or involvement?). 94.6% of the instances
that appear inside an ARGM-TMP role are tagged
with sense 2 (?period of time?). Our counts fur-
thermore show that about one third of the anno-
tated prepositional phrases fill core roles and that
ARGM-LOC and ARGM-TMP are the most fre-
quent roles. The detailed breakdown of semantic
roles is shown in Table 2.
To see how consistent humans can perform the
annotation task, we computed the inter-annotator
agreement between two annotators on Section 4 of
the PropBank corpus. We found that the two anno-
tators assigned the same sense in 86% of the cases.
Although not directly comparable, it is interesting
to note that this figure is similar to inter-annotator
agreement for open-class words reported in previ-
ous work (Palmer et al, 2000). In our final data
set, all labels were tagged by the same annotator,
which we believe makes our annotation reason-
ably consistent across different instances. Because
we annotate running text, not all prepositions have
the same number of annotated instances. The
numbers for all seven prepositions are shown in
Table 3. In our experiments, we use Sections 2 to 4
to train the models, and Section 23 is kept for test-
ing. Although our experiments are limited to three
sections of training data, it still allows us to train
competitive SRL models. Pradhan et al (2005)
have shown that the benefit of using more training
data diminishes after a few thousand training in-
stances. We found that the accuracy of our SRL
baseline model, which is trained on the 5275 sen-
tences of these three sections, is only an absolute
Baseline
Pipeline
Joint
  30%
  40%
  50%
  60%
  70%
  80%
  90%
at for in of on to with total
Ac
cur
acy
Figure 1: Classification accuracy of the WSD
models for the seven most frequent prepositions
in test section 23
3.89% lower than the accuracy of the same model
when it is trained on twenty sections (71.71% ac-
curacy compared to 75.60% accuracy).
5 Experiments and Results
We evaluate the performance of the joint model on
the annotated prepositional phrases in test section
23 and compare the results with the performance
of the baseline models and the pipeline models.
Figure 1 shows the classification accuracy of the
WSD models for each of the seven prepositions in
the test section. The results show that the pipeline
model and the joint model perform almost equally,
with the joint model performing marginally better
in the overall score. The detailed scores are given
in Table 4. Both models outperform the baseline
classifier for three of the seven prepositions: at,
for, and to. For the prepositions in, of, and on, the
SRL feature did not affect the WSD classification
accuracy significantly. For the preposition with,
the classification accuracy even dropped by about
6%.
Performing the student?s t-test, we found that
the improvement for the prepositions at, for, and
to is statistical significant (p < 0.05), as is the
overall improvement. This confirms our hypoth-
esis that the semantic role of the prepositional
phrase is a strong hint for the preposition sense.
However, our results also show that it is the
SRL feature that brings the improvement, not the
joint model, because the pipeline and joint model
achieve about the same performance.
For the SRL task, we report the classification
accuracy over all annotated prepositional phrases
in the test section and the F
1
measure for the se-
mantic roles ARGM-LOC and ARGM-TMP. Fig-
455
Preposition Baseline Pipeline Joint
at 70.83 78.47
?
78.47
?
for 41.52 49.12
?
49.12
?
in 62.33 61.74 61.93
of 43.48 43.48 43.48
on 51.85 51.85 52.47
to 58.77 67.11
?
66.67
?
with 44.78 38.06 38.06
Total 56.54 58.76
?
58.84
?
Table 4: Classification accuracy of the baseline,
pipeline, and joint model on the WSD task in test
section 23, statistically significant improvements
over the baseline are marked with an (*)
Baseline
Pipeline
Joint
  65%
  70%
  75%
  80%
  85%
  90%
Argm?LOC Argm?TMP Overall
 
f1?
me
asu
re
Figure 2: F
1
measure of the SRL models for
ARGM-LOC and ARGM-TMP, and overall accu-
racy on prepositional phrases in test section 23
ure 2 shows the results. The joint model shows
a small performance increase of 0.43% over the
baseline in the overall accuracy. Adding the
preposition sense as a feature, on the other hand,
significantly lowers the accuracy by over 2%. For
ARGM-LOC and ARGM-TMP, the joint model im-
proves the F
1
measure by about 1.3% each. The
improvement of the joint model for these roles
is statistically significant (p ? 0.05, student?s t-
test). Simply adding the preposition sense in the
pipeline model again lowers the F
1
measure. The
detailed results are listed in Table 5.
Semantic Role Baseline Pipeline Joint
ARGM-LOC(F
1
) 72.88 71.54 74.27*
ARGM-TMP(F
1
) 81.87 79.43 83.24*
Overall(A) 71.71 69.47 72.14
Table 5: F
1
measure and accuracy of the baseline,
pipeline, and joint model on the SRL task in test
section 23, statistically significant improvements
over the baseline are marked with an (*)
Our SRL experiments show that a pipeline
model degrades the performance. The reason is
the relatively high degree of noise in the WSD
classification and that the pipeline model does not
discriminate whether the previous classifier pre-
dicts the extra feature with high or low confi-
dence. Instead, the model only passes on the 1-
best WSD prediction, which can cause the next
classifier to make a wrong classification based on
the erroneous prediction of the previous step. In
principle, this problem can be mitigated by train-
ing the pipeline model on automatically predicted
labels using cross-validation, but in our case we
found that automatically predicted WSD labels
decreased the performance of the pipeline model
even more. In contrast, the joint model computes
the full probability distribution over the semantic
roles and preposition senses. If the noise level in
the first classification step is low, the joint model
and the pipeline model perform almost identically,
as we have seen in the previousWSD experiments.
But if the noise level is high, the joint model can
still improve while the pipeline model drops in
performance. Our experiments show that the joint
model is more robust in the presence of noisy fea-
tures than the pipeline model.
6 Related Work
There is relatively less prior research on preposi-
tions and prepositional phrases in the NLP com-
munity. O?Hara and Wiebe (2003) proposed a
WSD system to disambiguate function tags of
prepositional phrases. An extended version of
their work was recently presented in (O?Hara and
Wiebe, 2009). Ye and Baldwin (2006) extended
their work to a semantic role tagger specifically
for prepositional phrases. Their system first classi-
fies the semantic roles of all prepositional phrases
and later merges the output with a general SRL
system. Ye and Baldwin (2007) used semantic
role tags from surrounding tokens as part of the
MELB-YB preposition WSD system. They found
that the SRL features did not significantly help
their classifier, which is different from our find-
ings. Dang and Palmer (2005) showed that se-
mantic role features are helpful to disambiguate
verb senses. Their approach is similar to our
pipeline WSD model, but they do not present re-
sults with automatically predicted semantic roles.
Toutanova et al (2008) presented a re-ranking
model to jointly learn the semantic roles of mul-
tiple constituents in the SRL task. Their work
dealt with joint learning in SRL, but it is not di-
rectly comparable to ours. The difference is that
456
Toutanova et al attempt to jointly learn semantic
role assignment of different constituents for one
task (SRL), while we attempt to jointly learn two
tasks (WSD and SRL) for one constituent. Be-
cause we only look at one constituent at a time,
we do not have to restrict ourselves to a re-ranking
approach like Toutanova et al, but can calculate
the full joint probability distribution of both tasks.
Andrew et al (2004) propose a method to learn a
joint generative inference model from partially la-
beled data and apply their method to the problems
of word sense disambiguation for verbs and deter-
mination of verb subcategorization frames. Their
motivation is similar to ours, but they focus on
learning from partially labeled data and they in-
vestigate different tasks.
None of these systems attempted to jointly learn
the semantics of the prepositional phrase and the
preposition in a single model, which is the main
contribution of our work reported in this paper.
7 Conclusion
We propose a probabilistic model to jointly clas-
sify the semantic role of a prepositional phrase
and the sense of the associated preposition. We
show that learning both tasks together leads to an
improvement over competitive, individual models
for both subtasks. For the WSD task, we show
that the SRL information improves the classifi-
cation accuracy, although joint learning does not
significantly outperform a simpler pipeline model
here. For the SRL task, we show that the joint
model improves over both the baseline model and
the pipeline model, especially for temporal and lo-
cation arguments. As we only disambiguate the
seven most frequent prepositions, potentially more
improvement could be gained by including more
prepositions into our data set.
Acknowledgements
This research was supported by a research grant
R-252-000-225-112 from National University of
Singapore Academic Research Fund.
References
Galen Andrew, Trond Grenager, and Christopher D.
Manning. 2004. Verb Sense and Subcategorization:
Using Joint Inference to Improve Performance on
Complementary Tasks. In Proceedings of the 2004
Conference on Empirical Methods in Natural Lan-
guage Processing (EMNLP 2004), pages 150?157.
Hoa Trang Dang and Martha Palmer. 2005. The
Role of Semantic Roles in Disambiguating Verb
Senses. In Proceedings of the 43rd Annual Meet-
ing of the Association for Computational Linguistics
(ACL-05), pages 42?49.
Daniel Gildea and Daniel Jurafsky. 2002. Automatic
Labeling of Semantic Roles. Computational Lin-
guistics, 28(3):245?288.
Daniel Jurafsky and James H. Martin. 2008. Speech
and Language Processing. Prentice-Hall, Inc. Up-
per Saddle River, NJ, USA.
Peter Koomen, Vasin Punyakanok, Dan Roth, and
Wen-tau Yih. 2005. Generalized Inference with
Multiple Semantic Role Labeling Systems. In Pro-
ceedings of the 9th Conference on Computational
Natural Language Learning (CoNLL 2005), pages
181?184.
Yoong Keok Lee and Hwee Tou Ng. 2002. An Empir-
ical Evaluation of Knowledge Sources and Learn-
ing Algorithms for Word Sense Disambiguation. In
Proceedings of the 2002 Conference on Empirical
Methods in Natural Language Processing (EMNLP
2002), pages 41?48.
Kenneth C. Litkowski and Orin Hargraves. 2005. The
Preposition Project. In Proceedings of the 2nd ACL-
SIGSEM Workshop on The Linguistic Dimensions of
Prepositions and Their Use in Computational Lin-
guistic Formalisms and Applications, pages 171?
179.
Kenneth C. Litkowski and Orin Hargraves. 2007.
SemEval-2007 Task 06: Word-Sense Disambigua-
tion of Prepositions. In Proceedings of the 4th In-
ternational Workshop on Semantic Evaluations (Se-
mEval 2007), pages 24?29.
Mitchell P. Marcus, Mary A. Marcinkiewicz, and Beat-
rice Santorini. 1993. Building a Large Annotated
Corpus of English: The Penn Treebank. Computa-
tional Linguistics, 19(2):313?330.
Tom O?Hara and Janyce Wiebe. 2003. Preposi-
tion Semantic Classification via Penn Treebank and
FrameNet. In Proceedings of the 7th Conference on
Computational Natural Language Learning (CoNLL
2003), pages 79?86.
Tom O?Hara and Janyce Wiebe. 2009. Exploiting Se-
mantic Role Resources for Preposition Disambigua-
tion. Computational Linguistics, 35(2):151?184.
Martha Palmer, Hoa Trang Dang, and Joseph Rosen-
zweig. 2000. Sense Tagging the Penn Treebank. In
Proceedings of the 2nd International Conference on
Language Resources and Evaluation (LREC 2000).
Martha Palmer, Daniel Gildea, and Paul Kingsbury.
2005. The Proposition Bank: An Annotated Cor-
pus of Semantic Roles. Computational Linguistics,
31(1):71?105.
457
Sameer Pradhan, Kadri Hacioglu, Valerie Krugler,
Wayne Ward, James H. Martin, and Daniel Juraf-
sky. 2005. Support Vector Learning for Semantic
Argument Classification. Machine Learning, 60(1?
3):11?39.
Kristina Toutanova, Aria Haghighi, and Christopher D.
Manning. 2008. A Global Joint Model for Se-
mantic Role Labeling. Computational Linguistics,
34(2):161?191.
Nianwen Xue and Martha Palmer. 2004. Calibrating
Features for Semantic Role Labeling. In Proceed-
ings of the 2004 Conference on Empirical Methods
in Natural Language Processing (EMNLP 2004),
pages 88?94.
Patrick Ye and Timothy Baldwin. 2006. Seman-
tic Role Labeling of Prepositional Phrases. ACM
Transactions on Asian Language Information Pro-
cessing (TALIP), 5(3):228?244.
Patrick Ye and Timothy Baldwin. 2007. MELB-YB:
Preposition Sense Disambiguation Using Rich Se-
mantic Features. In Proceedings of the 4th Interna-
tional Workshop on Semantic Evaluations (SemEval
2007), pages 241?244.
458
Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 923?932,
MIT, Massachusetts, USA, 9-11 October 2010. c?2010 Association for Computational Linguistics
PEM: A Paraphrase Evaluation Metric Exploiting Parallel Texts
Chang Liu1 and Daniel Dahlmeier2 and Hwee Tou Ng1,2
1Department of Computer Science, National University of Singapore
2NUS Graduate School for Integrative Sciences and Engineering
{liuchan1,danielhe,nght}@comp.nus.edu.sg
Abstract
We present PEM, the first fully automatic met-
ric to evaluate the quality of paraphrases, and
consequently, that of paraphrase generation
systems. Our metric is based on three crite-
ria: adequacy, fluency, and lexical dissimilar-
ity. The key component in our metric is a ro-
bust and shallow semantic similarity measure
based on pivot language N-grams that allows
us to approximate adequacy independently of
lexical similarity. Human evaluation shows
that PEM achieves high correlation with hu-
man judgments.
1 Introduction
In recent years, there has been an increasing inter-
est in the task of paraphrase generation (PG) (Barzi-
lay and Lee, 2003; Pang et al, 2003; Quirk et al,
2004; Bannard and Callison-Burch, 2005; Kauchak
and Barzilay, 2006; Zhao et al, 2008; Zhao et al,
2009). At the same time, the task has seen appli-
cations such as machine translation (MT) (Callison-
Burch et al, 2006; Madnani et al, 2007; Madnani
et al, 2008), MT evaluation (Kauchak and Barzilay,
2006; Zhou et al, 2006a; Owczarzak et al, 2006),
summary evaluation (Zhou et al, 2006b), and ques-
tion answering (Duboue and Chu-Carroll, 2006).
Despite the research activities, we see two major
problems in the field. First, there is currently no con-
sensus on what attributes characterize a good para-
phrase. As a result, works on the application of para-
phrases tend to build their own PG system in view
of the immediate needs instead of using an existing
system.
Second, and as a consequence, no automatic eval-
uation metric exists for paraphrases. Most works in
this area resort to ad hoc manual evaluations, such as
the percentage of ?yes? judgments to the question of
?is the meaning preserved?. This type of evaluation
is incomprehensive, expensive, and non-comparable
between different studies, making progress hard to
judge.
In this work we address both problems. We pro-
pose a set of three criteria for good paraphrases: ad-
equacy, fluency, and lexical dissimilarity. Consid-
ering that paraphrase evaluation is a very subjec-
tive task with no rigid definition, we conduct ex-
periments with human judges to show that humans
generally have a consistent intuition for good para-
phrases, and that the three criteria are good indica-
tors.
Based on these criteria, we construct PEM (Para-
phrase Evaluation Metric), a fully automatic evalua-
tion metric for PG systems. PEM takes as input the
original sentence R and its paraphrase candidate P ,
and outputs a single numeric score b estimating the
quality of P as a paraphrase of R. PG systems can
be compared based on the average scores of their
output paraphrases. To the best of our knowledge,
this is the first automatic metric that gives an objec-
tive and unambiguous ranking of different PG sys-
tems, which serves as a benchmark of progress in
the field of PG.
The main difficulty of deriving PEM is to measure
semantic closeness without relying on lexical level
similarity. To this end, we propose bag of pivot lan-
guage N-grams (BPNG) as a robust, broad-coverage,
and knowledge-lean semantic representation for nat-
ural language sentences. Most importantly, BPNG
does not depend on lexical or syntactic similarity,
allowing us to address the conflicting requirements
of paraphrase evaluation. The only linguistic re-
923
source required to evaluate BPNG is a parallel text
of the target language and an arbitrary other lan-
guage, known as the pivot language.
We highlight that paraphrase evaluation and para-
phrase recognition (Heilman and Smith, 2010; Das
and Smith, 2009; Wan et al, 2006; Qiu et al, 2006)
are related yet distinct tasks. Consider two sentences
S1 and S2 that are the same except for the substitu-
tion of a single synonym. A paraphrase recognition
system should assign them a very high score, but a
paraphrase evaluation system would assign a rela-
tively low one. Indeed, the latter is often a better
indicator of how useful a PG system potentially is
for the applications of PG described earlier.
The rest of the paper is organized as follows. We
survey other automatic evaluation metrics in natural
language processing (NLP) in Section 2. We define
the task of paraphrase evaluation in Section 3 and
develop our metric in Section 4. We conduct a hu-
man evaluation and analyze the results in Section 5.
The correlation of PEM with human judgments is
studied in Section 6. Finally, we discuss our find-
ings and future work in Section 7 and conclude in
Section 8.
2 Related work
Themost well-known automatic evaluation metric in
NLP is BLEU (Papineni et al, 2002) for MT, based
on N-gram matching precisions. The simplicity of
BLEU lends well to MT techniques that directly op-
timize the evaluation metric.
The weakness of BLEU is that it operates purely
at the lexical surface level. Later works attempt to
take more syntactic and semantic features into con-
sideration (see (Callison-Burch et al, 2009) for an
overview). The whole spectrum of NLP resources
has found application in machine translation eval-
uation, including POS tags, constituent and depen-
dency parses, WordNet (Fellbaum, 1998), semantic
roles, textual entailment features, and more. Many
of these metrics have been shown to correlate bet-
ter with human judges than BLEU (Chan and Ng,
2008; Liu et al, 2010). Interestingly, few MT eval-
uation metrics exploit parallel texts as a source of
information, when statistical MT is centered almost
entirely around mining parallel texts.
Compared to these MT evaluation metrics, our
method focuses on addressing the unique require-
ment of paraphrase evaluation: that lexical closeness
does not necessarily entail goodness, contrary to the
basis of MT evaluation.
Inspired by the success of automatic MT evalua-
tion, Lin (2004) and Hovy et al (2006) propose au-
tomatic metrics for summary evaluation. The for-
mer is entirely lexical based, whereas the latter also
exploits constituent and dependency parses, and se-
mantic features derived from WordNet.
The only prior attempt to devise an automatic
evaluation metric for paraphrases that we are aware
of is ParaMetric (Callison-Burch et al, 2008), which
compares the collection of paraphrases discovered
by automatic paraphrasing algorithms against a
manual gold standard collected over the same sen-
tences. The recall and precision of several current
paraphrase generation systems are evaluated. Para-
Metric does not attempt to propose a single metric
to correlate well with human judgments. Rather, it
consists of a few indirect and partial measures of the
quality of PG systems.
3 Task definition
The first step in defining a paraphrase evaluation
metric is to define a good paraphrase. Merriam-
Webster dictionary gives the following definition:
a restatement of a text, passage, or work giving
the meaning in another form. We identify two key
points in this definition: (1) that the meaning is pre-
served, and (2) that the lexical form is different. To
which we add a third, that the paraphrase must be
fluent.
The first and last point are similar to MT evalua-
tion, where adequacy and fluency have been estab-
lished as the standard criteria. In paraphrase evalu-
ation, we have one more: lexical dissimilarity. Al-
though lexical dissimilarity is seemingly the easiest
to judge automatically among the three, it poses an
interesting challenge to automatic evaluation met-
rics, as overlap with the reference has been the basis
of almost all evaluation metrics. That is, while MT
evaluation and paraphrase evaluation are conceptu-
ally closely related, the latter actually highlights the
deficiencies of the former, namely that in most au-
tomatic evaluations, semantic equivalence is under-
represented and substituted by lexical and syntactic
924
equivalence.
The task of paraphrase evaluation is then defined
as follows: Given an original sentence R and a para-
phrase candidate P , output a numeric score b esti-
mating the quality of P as a paraphrase ofR by con-
sidering adequacy, fluency, and lexical dissimilarity.
In this study, we use a scale of 1 to 5 (inclusive) for
b, although that can be transformed linearly into any
range desired.
We observe here that the overall assessment b is
not a linear combination of the three measures. In
particular, a high dissimilarity score is meaningless
by itself. It could simply be that the paraphrase is
unrelated to the source sentence, or is incoherent.
However, when accompanied by high adequacy and
fluency scores, it differentiates the mediocre para-
phrases from the good ones.
4 Paraphrase Evaluation Metric (PEM)
In this section we devise our metric according to the
three proposed evaluation criteria, namely adequacy,
fluency, and dissimilarity. The main challenge is to
measure the adequacy, or semantic similarity, com-
pletely independent of any lexical similarity. We ad-
dress this problem in Sections 4.1 to 4.3. The re-
maining two criteria are addressed in Section 4.4,
and we describe the final combined metric PEM in
Section 4.5.
4.1 Phrase-level semantic representation
Without loss of generality, suppose we are to eval-
uate English paraphrases, and have been supplied
many sentence-aligned parallel texts of French and
English as an additional resource. We can then align
the parallel texts at word level automatically using
well-known algorithms such as GIZA++ (Och and
Ney, 2003) or the Berkeley aligner (Liang et al,
2006; Haghighi et al, 2009).
To measure adequacy without relying on lexical
similarity, we make the key observation that the
aligned French texts can act as a proxy of the se-
mantics to a fragment of an English text. If two En-
glish phrases are often mapped to the same French
phrase, they can be considered similar in mean-
ing. Similar observations have been made by previ-
ous researchers (Wu and Zhou, 2003; Bannard and
Callison-Burch, 2005; Callison-Burch et al, 2006;
Snover et al, 2009). We can treat the distribution
of aligned French phrases as a semantic representa-
tion of the English phrase. The semantic distance
between two English phrases can then be measured
by their degree of overlap in this representation.
In this work, we use the widely-used phrase ex-
traction heuristic in (Koehn et al, 2003) to extract
phrase pairs from parallel texts into a phrase table1.
The phrases extracted do not necessarily correspond
to the speakers? intuition. Rather, they are units
whose boundaries are preserved during translation.
However, the distinction does not affect our work.
4.2 Segmenting a sentence into phrases
Having established a way to measure the similarity
of two English phrases, we now extend the concept
to sentences. Here we discuss how to segment an
English sentence (the original or the paraphrase) into
phrases.
From the phrase table, we know the frequencies of
all the phrases and we approximate the probability
of a phrase p by:
Pr(p) =
N(p)
?
p? N(p?)
(1)
N(?) is the count of a phrase in the phrase table, and
the denominator is a constant for all p. We define
the likelihood of segmenting a sentence S into a se-
quence of phrases (p1, p2, . . . , pn) by:
Pr(p1, p2, . . . , pn|S) =
1
Z(S)
n?
i=1
Pr(pi) (2)
where Z(S) is a normalizing constant. The best seg-
mentation of S according to Equation 2 can be cal-
culated efficiently using a dynamic programming al-
gorithm. Note that Z(S) does not need to be calcu-
lated, as it is the same for all different segmentations
of S. The formula has a strong preference for longer
phrases, since every Pr(pi) has a large denominator.
Many sentences are impossible to segment into
known phrases, including all those containing out-
of-vocabulary words. We therefore allow any sin-
gle word w to be considered as a phrase, and if
N(w) = 0, we use N(w) = 0.5 instead.
1The same heuristic is used in the popular MT package
Moses.
925
Bonjour , / 0.9
Salut , / 0.1
Querrien / 1.0 . / 1.0
Figure 1: A confusion network in the pivot language
Bonjour , /on0.9ur S a lSt
Figure 2: A degenerated confusion network in the pivot
language
4.3 Sentence-level semantic representation
Simply merging the phrase-level semantic represen-
tations is insufficient to produce a sensible sentence-
level semantic representation. For example, assume
the English sentence Morning , sir . is segmented as
a single phrase, because the following phrase pair is
found in the phrase table:
En: Morning , sir .
Fr: Bonjour , monsieur .
However, another English sentence Hello , Quer-
rien . has an out-of-vocabulary word Querrien
and consequently the most probable segmentation is
found to be ?Hello , ||| Querrien ||| .?:
En: Hello ,
Fr: Bonjour , (Pr(Bonjour ,|Hello ,) = 0.9)
Fr: Salut , (Pr(Salut ,|Hello ,) = 0.1)
En: Querrien
Fr: Querrien
En: .
Fr: .
A naive comparison of the bags of French phrases
aligned to Morning , sir . and Hello , Querrien . de-
picted above would conclude that the two sentences
are completely unrelated, as their bags of aligned
French phrases are completely disjoint. We tackle
this problem by constructing a confusion network
representation of the French phrases, as shown in
Figures 1 and 2. The confusion network is formed
by first joining the different French translations of
every English phrase in parallel, and then joining
these segments in series.
The confusion network is a compact representa-
tion of an exponentially large number of (likely mal-
formed) weighted French sentences. We can easily
enumerate the N-grams from the confusion network
representation and collect the statistics for this en-
semble of French sentences efficiently. In this work,
we consider N up to 4. The N-grams for Hello ,
Querrien . are:
1-grams: Bonjour (0.9), Salut (0.1), comma
(1.0), Querrien (1.0), period (1.0).
2-grams: Bonjour comma (0.9), Salut comma
(0.1), comma Querrien (1.0), Querrien period (1.0).
3-grams: Bonjour comma Querrien (0.9), Salut
comma Querrien (0.1), comma Querrien period
(1.0).
4-grams: Bonjour comma Querrien period (0.9),
Salut comma Querrien period (0.1).
We call this representation of an English sentence
a bag of pivot language N-grams (BPNG), where
French is the pivot language in our illustrating ex-
ample. We can extract the BPNG of Morning , sir .
analogously:
1-grams: Bonjour (1.0), comma (1.0), monsieur
(1.0), period (1.0).
2-grams: Bonjour comma (1.0), comma mon-
sieur (1.0), monsieur period (1.0).
3-grams: Bonjour comma monsieur (1.0),
comma monsieur period (1.0).
4-grams: Bonjour comma monsieur period (1.0).
The BPNG of Hello , Querrien. can now be com-
pared sensibly with that of the sentence Morning ,
sir . We use the F1 agreement between the two BP-
NGs as a measure of the semantic similarity. The F1
agreement is defined as
F1 =
2 ? Precision ? Recall
Precision + Recall
The precision and the recall for an original sen-
tence R and a paraphrase P is defined as follows.
Let French N-gram g ? BPNG(R)?BPNG(P ), and
WR(g) and WP (g) be the weights of g in the BPNG
of R and P respectively, then
Precision =
?
g min(WR(g),WP (g))
?
g WP (g)
Recall =
?
g min(WR(g),WP (g))
?
g WR(g)
In our example, the numerators for both the preci-
sion and the recall are 0.9 + 1 + 1 + 0.9, for the N-
grams Bonjour, comma, period, and Bonjour comma
926
respectively. The denominators for both terms are
10.0. Consequently, F1 = Precision = Recall =
0.38, and we conclude that the two sentences are
38% similar. We call the resulting metric the pivot
language F1. Note that since F1 is symmetric with
respect to the precision and the recall, our metric is
unaffected whether we consider Morning, sir. as the
paraphrase of Hello, Querrien . or the other way
round.
An actual example from our corpus is:
Reference sihanouk ||| put forth ||| this proposal |||
in ||| a statement ||| made ||| yesterday ||| .
Paraphrase shihanuk ||| put forward ||| this pro-
posal ||| in his ||| yesterday ||| ?s statement ||| .
The ||| sign denotes phrase segmentation as de-
scribed earlier. Our semantic representation suc-
cessfully recognizes that put forth and put forward
are paraphrases of each other, based on their similar
Chinese translation statistics (ti2 chu1 in Chinese).
4.4 Fluency and dissimilarity
We measure the fluency of a paraphrase by a nor-
malized language model score Pn, defined by
Pn =
logPr(S)
length(S)
where Pr(S) is the sentence probability predicted
by a standard 4-gram language model.
We measure dissimilarity between two English
sentences using the target language F1, where we
collect the bag of all N-grams up to 4-grams from
each English (referred to as the target language) sen-
tence. The target language F1 is then defined as the
F1 agreement of the two bags of N-grams, analogous
to the definition of the pivot language F1. The target
language F1 correlates positively with the similar-
ity of the two sentences, or equivalently, negatively
with the dissimilarity of the two sentences.
4.5 The metric
To produce the final PEM metric, we combine the
three component automatic metrics, pivot language
F1, normalized language model, and target language
F1, which measure adequacy, fluency, and dissimi-
larity respectively.
As discussed previously, a linear combination of
the three component metrics is insufficient. We turn
to support vector machine (SVM) regression with
the radial basis function (RBF) kernel. The RBF is
a simple and expressive function, commonly used to
introduce non-linearity into large margin classifica-
tions and regressions.
RBF(xi, xj) = e
???xi?xj?2
We use the implementation in SVM light
(Joachims, 1999). The SVM is to be trained on a set
of human-judged paraphrase pairs, where the three
component automatic metrics are fit to the human
overall assessment. After training, the model can
then be used to evaluate new paraphrase pairs in a
fully automatic fashion.
5 Human evaluation
To validate our definition of paraphrase evaluation
and the PEM method, we conduct an experiment
to evaluate paraphrase qualities manually, which al-
lows us to judge whether paraphrase evaluation ac-
cording to our definition is an inherently coherent
and well-defined problem. The evaluation also al-
lows us to establish an upper bound for the para-
phrase evaluation task, and to validate the contribu-
tion of the three proposed criteria to the overall para-
phrase score.
5.1 Evaluation setup
We use the Multiple-Translation Chinese Corpus
(MTC)2 as a source of paraphrases. The MTC
corpus consists of Chinese news articles (993 sen-
tences in total) and multiple sentence-aligned En-
glish translations. We select one human transla-
tion as the original text. Two other human transla-
tions and two automatic machine translations serve
as paraphrases of the original sentences. We refer to
the two human translations and the two MT systems
as paraphrase systems human1, human2, machine1,
and machine2.
We employ three human judges to manually as-
sess the quality of 300 original sentences paired
with each of the four paraphrases. Therefore, each
judge assesses 1,200 paraphrase pairs in total. The
2LDC Catalog No.: LDC2002T01
927
judgment for each paraphrase pair consists of four
scores, each given on a five-point scale:
? Adequacy (Is the meaning preserved ade-
quately?)
? Fluency (Is the paraphrase fluent English?)
? Lexical Dissimilarity (How much has the para-
phrase changed the original sentence?)
? Overall score
The instructions given to the judges for the overall
score were as follows.
A good paraphrase should convey the
same meaning as the original sentence,
while being as different as possible on the
surface form and being fluent and gram-
matical English. With respect to this defi-
nition, give an overall score from 5 (per-
fect) to 1 (unacceptable) for this para-
phrase.
The paraphrases are presented to the judges in a ran-
dom order and without any information as to which
paraphrase system produced the paraphrase.
In addition to the four paraphrase systems men-
tioned above, for each original English sentence, we
add three more artificially constructed paraphrases
with pre-determined ?human? judgment scores: (1)
the original sentence itself, with adequacy 5, fluency
5, dissimilarity 1, and overall score 2; (2) a random
sentence drawn from the same domain, with ade-
quacy 1, fluency 5, dissimilarity 5, and overall score
1; and (3) a random sentence generated by a uni-
gram language model, with adequacy 1, fluency 1,
dissimilarity 5, and overall score 1. These artificial
paraphrases serve as controls in our evaluation. Our
final data set therefore consists of 2,100 paraphrase
pairs with judgments on 4 different criteria.
5.2 Inter-judge correlation
The first step in our evaluation is to investigate the
correlation between the human judges. We use Pear-
son?s correlation coefficient, a common measure of
the linear dependence between two random vari-
ables.
We investigate inter-judge correlation at the sen-
tence and at the system level. At the sentence
level, we construct three vectors, each containing
the 1,200 sentence level judgments from one judge
Sentence Level System Level
Judge A Judge B Judge A Judge B
Judge B 0.6406 - 0.9962 -
Judge C 0.6717 0.5993 0.9995 0.9943
Table 1: Inter-judge correlation for overall paraphrase
score
Sentence Level System Level
Adequacy 0.7635 0.7616
Fluency 0.3736 0.3351
Dissimilarity -0.3737 -0.3937
Dissimilarity (A,F?4) 0.8881 0.9956
Table 2: Correlation of paraphrase criteria with overall
score
for the overall score. The pair-wise correlations be-
tween these three vectors are then taken. Note that
we exclude the three artificial control paraphrase
systems from consideration, as that would inflate the
correlation. At the system level, we construct three
vectors each of size four, containing the average
scores given by one judge to each of the four para-
phrase systems human1, human2, machine1, and
machine2. The correlations are then taken in the
same fashion.
The results are listed in Table 1. The inter-judge
correlation is between 0.60 and 0.67 at the sentence
level and above 0.99 at the system level. These cor-
relation scores can be considered very high when
compared to similar results reported in MT evalu-
ations, e.g., Blatz et al (2003). The high correlation
confirms that our evaluation task is well defined.
Having confirmed that human judgments corre-
late strongly, we combine the scores of the three
judges by taking their arithmetic mean. Together
with the three artificial control paraphrase systems,
they form the human reference evaluation which we
use for the remainder of the experiments.
5.3 Adequacy, fluency, and dissimilarity
In this section, we empirically validate the impor-
tance of our three proposed criteria: adequacy, flu-
ency, and lexical dissimilarity. This can be done by
measuring the correlation of each criterion with the
overall score. The system and sentence level corre-
lations are shown in Table 2.
We can see a positive correlation of adequacy and
928
Figure 3: Scatter plot of dissimilarity vs. overall score
for paraphrases with high adequacy and fluency.
fluency with the overall score, and the correlation
with adequacy is particularly strong. Thus, higher
adequacy and to a lesser degree higher fluency indi-
cate higher paraphrase quality to the human judges.
On the other hand, dissimilarity is found to have a
negative correlation with the overall score. This can
be explained by the fact that the two human trans-
lations usually have much higher similarity with the
reference translation, and at the same time are scored
as better paraphrases. This effect dominates a sim-
ple linear fitting of the paraphrase score vs. the dis-
similarity, resulting in the counter intuitive negative
correlation. We note that a high dissimilarity alone
tells us little about the quality of the paraphrase.
Rather, we expect dissimilarity to be a differentia-
tor between the mediocre and good paraphrases.
To test this hypothesis, we select the subset of
paraphrase pairs that receive adequacy and fluency
scores of at least four and again measure the cor-
relation of the dissimilarity and the overall score.
The result is tabulated in the last row of Table 2 and
shows a strong correlation. Figure 3 shows a scatter
plot of the same result3.
The empirical results presented so far confirm that
paraphrase evaluation is a well-defined task permit-
ting consistent subjective judgments, and that ade-
quacy, fluency, and dissimilarity are suitable criteria
for paraphrase quality.
3We automatically add jitter (small amounts of noise) for
ease of presentation.
6 PEM vs. human evaluation
In the last section, we have shown that the three
proposed criteria are good indicators of paraphrase
quality. In this section, we investigate how well
PEM can predict the overall paraphrase quality from
the three automatic metrics (pivot language F1, nor-
malized language model, and target language F1),
designed to match the three evaluation criteria. We
describe the experimental setup in Section 6.1, be-
fore we show the results in Section 6.2.
6.1 Experimental setup
We build the phrase table used to evaluate the pivot
language F1 from the FBIS Chinese-English corpus,
consisting of about 250,000 Chinese sentences, each
with a single English translation. The paraphrases
are taken from the MTC corpus in the same way
as the human experiment described in Section 5.1.
Both FBIS and MTC are in the Chinese newswire
domain.
We stem all English words in both data sets with
the Porter stemmer (Porter, 1980). We use the maxi-
mum entropy segmenter of (Low et al, 2005) to seg-
ment the Chinese part of the FBIS corpus. Subse-
quently, word level Chinese-English alignments are
generated using the Berkeley aligner (Liang et al,
2006; Haghighi et al, 2009) with five iterations of
training. Phrases are then extracted with the widely-
used heuristic in Koehn et al (2003). We extract
phrases of up to four words in length.
Bags of Chinese pivot language N-grams are ex-
tracted for all paraphrase pairs as described in Sec-
tion 4.3. For computational efficiency, we consider
only edges of the confusion network with probabil-
ities higher than 0.1, and only N-grams with proba-
bilities higher than 0.01 in the bag of N-grams. We
collect N-grams up to length four.
The language model used to judge fluency is
trained on the English side of the FBIS parallel text.
We use SRILM (Stolcke, 2002) to build a 4-gram
model with the default parameters.
The PEM SVM regression is trained on the para-
phrase pairs for the first 200 original English sen-
tences and tested on the paraphrase pairs of the re-
maining 100 original English sentences. Thus, there
are 1,400 instances for training and 700 instances for
testing. For each instance, we calculate the values
929
Figure 4: Scatter plot of PEM vs. human judgment (over-
all score) at the sentence level
Figure 5: Scatter plot of PEM vs. human judgment (over-
all score) at the system level
of pivot language F1, normalized language model
score, and target language F1. These values serve
as the input features to the SVM regression and the
target value is the human assessment of the overall
score, on a scale of 1 to 5.
6.2 Results
As in the human evaluation, we investigate the cor-
relation of the PEM scores with the human judg-
ments at the sentence and at the system level. Fig-
ure 4 shows the sentence level PEM scores plotted
against the human overall scores, where each human
overall score is the arithmetic mean of the scores
given by the three judges. The Pearson correlation
between the automatic PEM scores and the human
judgments is 0.8073. This is substantially higher
than the sentence level correlation of MT metrics
Sentence Level System Level
PEM vs. Human Avg. 0.8073 0.9867
PEM vs. Judge A 0.5777 0.9757
PEM vs. Judge B 0.5281 0.9892
PEM vs. Judge C 0.5231 0.9718
Table 3: Correlation of PEMwith human judgment (over-
all score)
like BLEU. For example, the highest sentence level
Pearson correlation by any metric in the Metrics-
MATR 2008 competition (Przybocki et al, 2009)
was 0.6855 by METEOR-v0.6; BLEU achieved a
correlation of 0.4513.
Figure 5 shows the system level PEM scores plot-
ted against the human scores. The Pearson correla-
tion between PEM scores and the human scores at
the system level is 0.9867.
We also calculate the Pearson correlation between
PEM and each individual human judge. Here, we
exclude the three artificial control paraphrase sys-
tems from the data, to make the results compara-
ble to the inter-judge correlation presented in Sec-
tion 5.2. The correlation is between 0.52 and 0.57
at the sentence level and between 0.97 and 0.98 at
the system level. As we would expect, the correla-
tion between PEM and a human judge is not as high
as the correlation between two human judges, but
PEM still shows a strong and consistent correlation
with all three judges. The results are summarized in
Table 3.
7 Discussion and future work
The paraphrases that we use in this study are not
actual machine generated paraphrases. Instead, the
English paraphrases are multiple translations of the
same Chinese source sentence. Our seven ?para-
phrase systems? are two human translators, two ma-
chine translation systems, and three artificially cre-
ated extreme scenarios. The reason for using multi-
ple translations is that we could not find any PG sys-
tem that can paraphrase a whole input sentence and
is publicly available. We intend to obtain and evalu-
ate paraphrases generated from real PG systems and
compare their performances in a follow-up study.
Our method models paraphrasing up to the phrase
level. Unfortunately, it makes no provisions for syn-
930
tactic paraphrasing at the sentence level, which is
probably a much greater challenge, and the literature
offers few successes to draw inspirations from. We
hope to be able to partially address this deficiency in
future work.
The only external linguistic resource required by
PEM is a parallel text of the target language and
another arbitrary language. While we only use
Chinese-English parallel text in this study, other lan-
guage pairs need to be explored too. Another alter-
native is to collect parallel texts against multiple for-
eign languages, e.g., using Europarl (Koehn, 2005).
We leave this for future work.
Our evaluation method does not require human-
generated references like in MT evaluation. There-
fore, we can easily formulate a paraphrase genera-
tor by directly optimizing the PEM metric, although
solving it is not trivial:
paraphrase(R) = argmax
P
PEM(P,R)
where R is the original sentence and P is the para-
phrase.
Finally, the PEM metric, in particular the seman-
tic representation BPNG, can be useful in many
other contexts, such as MT evaluation, summary
evaluation, and paraphrase recognition. To facil-
itate future research, we will package and release
PEM under an open source license at http://
nlp.comp.nus.edu.sg/software.
8 Conclusion
We proposed PEM, a novel automatic metric for
paraphrase evaluation based on adequacy, fluency,
and lexical dissimilarity. The key component in our
metric is a novel technique to measure the seman-
tic similarity of two sentences through their N-gram
overlap in an aligned foreign language text. We
conducted an extensive human evaluation of para-
phrase quality which shows that our proposed met-
ric achieves high correlation with human judgments.
To the best of our knowledge, PEM is the first auto-
matic metric for paraphrase evaluation.
Acknowledgments
This research was done for CSIDM Project No.
CSIDM-200804 partially funded by a grant from
the National Research Foundation (NRF) adminis-
tered by the Media Development Authority (MDA)
of Singapore.
References
C. Bannard and C. Callison-Burch. 2005. Paraphrasing
with bilingual parallel corpora. In Proc. of ACL.
R. Barzilay and L. Lee. 2003. Learning to paraphrase:
An unsupervised approach using multiple-sequence
alignment. In Proc. of HLT-NAACL.
J. Blatz, E. Fitzgerald, G. Foster, S. Gandrabur, C. Goutte,
A. Kulesza, A. Sanchis, and N. Ueffing. 2003. Con-
fidence estimation for machine translation. Technical
report, CLSP Workshop Johns Hopkins University.
C. Callison-Burch, P. Koehn, and M. Osborne. 2006.
Improved statistical machine translation using para-
phrases. In Proc. of HLT-NAACL.
C. Callison-Burch, T. Cohn, and M. Lapata. 2008. Para-
Metric: An automatic evaluation metric for paraphras-
ing. In Proc. of COLING.
C. Callison-Burch, P. Koehn, C. Monz, and J. Schroeder.
2009. Findings of the 2009 Workshop on Statistical
Machine Translation. In Proceedings of WMT.
Y.S. Chan and H.T. Ng. 2008. MAXSIM: A maximum
similarity metric for machine translation evaluation.
In Proc. of ACL-08: HLT.
D. Das and N.A. Smith. 2009. Paraphrase identifica-
tion as probabilistic quasi-synchronous recognition. In
Proc. of ACL-IJCNLP.
P. Duboue and J. Chu-Carroll. 2006. Answering the
question you wish they had asked: The impact of para-
phrasing for question answering. In Proc. of HLT-
NAACL Companion Volume: Short Papers.
C. Fellbaum, editor. 1998. WordNet: An electronic lexi-
cal database. MIT Press, Cambridge, MA.
A. Haghighi, J. Blitzer, J. DeNero, and D. Klein. 2009.
Better word alignments with supervised ITG models.
In Proc. of ACL-IJCNLP.
M. Heilman and N.A. Smith. 2010. Tree edit models
for recognizing textual entailments, paraphrases, and
answers to questions. In Proc. of NAACL.
E. Hovy, C.Y. Lin, L. Zhou, and J. Fukumoto. 2006.
Automated summarization evaluation with basic ele-
ments. In Proc. of LREC.
T. Joachims. 1999. Making large-scale SVM learning
practical. In B. Sch?lkopf, C. Burges, and A. Smola,
editors, Advances in Kernel Methods - Support Vector
Learning. MIT Press.
D. Kauchak and R. Barzilay. 2006. Paraphrasing for
automatic evaluation. In Proc. of HLT-NAACL.
P. Koehn, F.J. Och, and D. Marcu. 2003. Statistical
phrase-based translation. In Proc. of HLT-NAACL.
931
P. Koehn. 2005. Europarl: A parallel corpus for statisti-
cal machine translation. In MT Summit, volume 5.
P. Liang, B. Taskar, and D. Klein. 2006. Alignment by
agreement. In Proc. of HLT-NAACL.
C.Y. Lin. 2004. ROUGE: A package for automatic eval-
uation of summaries. In Proc. of the ACL-04 Work-
shop on Text Summarization Branches Out.
C. Liu, D. Dahlmeier, and H.T. Ng. 2010. TESLA:
translation evaluation of sentences with linear-
programming-based analysis. In Proc. of WMT.
J.K. Low, H.T. Ng, and W. Guo. 2005. A maximum
entropy approach to Chinese word segmentation. In
Proc. of the 4th SIGHAN Workshop.
N. Madnani, N.F. Ayan, P. Resnik, and B.J. Dorr. 2007.
Using paraphrases for parameter tuning in statistical
machine translation. In Proc. of WMT.
N. Madnani, P. Resnik, B.J. Dorr, and R. Schwartz. 2008.
Are multiple reference translations necessary? Investi-
gating the value of paraphrased reference translations
in parameter optimization. In Proc. of AMTA.
F.J. Och and H. Ney. 2003. A systematic comparison of
various statistical alignment models. Computational
Linguistics, 29(1).
K. Owczarzak, D. Groves, J. Van Genabith, and A. Way.
2006. Contextual bitext-derived paraphrases in auto-
matic MT evaluation. In Proc. of WMT.
B. Pang, K. Knight, and D. Marcu. 2003. Syntax-based
alignment of multiple translations: Extracting para-
phrases and generating new sentences. In Proc. of
HLT-NAACL.
K. Papineni, S. Roukos, T. Ward, and W.J. Zhu. 2002.
BLEU: a method for automatic evaluation of machine
translation. In Proc. of ACL.
M. Porter. 1980. An algorithm for suffix stripping. Pro-
gram, 40(3).
M. Przybocki, K. Peterson, S. Bronsart, and G Sanders.
2009. Evaluating machine translation with LFG de-
pendencies. Machine Translation, 23(2).
L. Qiu, M.Y. Kan, and T.S. Chua. 2006. Paraphrase
recognition via dissimilarity significance classifica-
tion. In Proc. of EMNLP.
C. Quirk, C. Brockett, and W. Dolan. 2004. Monolin-
gual machine translation for paraphrase generation. In
Proc. of EMNLP.
M. Snover, N. Madnani, B. Dorr, and R. Schwartz. 2009.
Fluency, adequacy, or HTER? Exploring different hu-
man judgments with a tunable MT metric. In Proc. of
WMT.
A. Stolcke. 2002. SRILM - an extensible language mod-
eling toolkit. In Proc. of ICSLP.
S. Wan, M. Dras, R. Dale, and C Paris. 2006. Using
dependency-based features to take the ?para-farce? out
of paraphrase. In Proc. of ALTW 2006.
H. Wu and M. Zhou. 2003. Synonymous collocation
extraction using translation information. In Proc. of
ACL.
S.Q. Zhao, C. Niu, M. Zhou, T. Liu, and S. Li. 2008.
Combining multiple resources to improve SMT-based
paraphrasing model. In Proc. of ACL-08: HLT.
S.Q. Zhao, X. Lan, T. Liu, and S. Li. 2009. Application-
driven statistical paraphrase generation. In Proc. of
ACL-IJCNLP.
L. Zhou, C.Y. Lin, and E. Hovy. 2006a. Re-evaluating
machine translation results with paraphrase support.
In Proc. of EMNLP.
L. Zhou, C.Y. Lin, D.S. Munteanu, and E. Hovy. 2006b.
ParaEval: Using paraphrases to evaluate summaries
automatically. In Proc. of HLT-NAACL.
932
Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 107?117,
Edinburgh, Scotland, UK, July 27?31, 2011. c?2011 Association for Computational Linguistics
Correcting Semantic Collocation Errors with L1-induced Paraphrases
Daniel Dahlmeier1 and Hwee Tou Ng1,2
1NUS Graduate School for Integrative Sciences and Engineering
2Department of Computer Science, National University of Singapore
{danielhe,nght}@comp.nus.edu.sg
Abstract
We present a novel approach for automatic
collocation error correction in learner English
which is based on paraphrases extracted from
parallel corpora. Our key assumption is that
collocation errors are often caused by se-
mantic similarity in the first language (L1-
language) of the writer. An analysis of a
large corpus of annotated learner English con-
firms this assumption. We evaluate our ap-
proach on real-world learner data and show
that L1-induced paraphrases outperform tradi-
tional approaches based on edit distance, ho-
mophones, and WordNet synonyms.
1 Introduction
Grammatical error correction (GEC) is emerging as
a commercially attractive application of natural lan-
guage processing (NLP) for the booming market of
English as foreign or second language (EFL/ESL1).
The de facto standard approach to GEC is to build
a statistical model that can choose the most likely
correction from a confusion set of possible correc-
tion choices. The way the confusion set is defined
depends on the type of error. Work in context-
sensitive spelling error correction (Golding and
Roth, 1999) has traditionally focused on confusion
sets with similar spelling (e.g., {dessert, desert}) or
similar pronunciation (e.g., {there, their}). In other
words, the words in a confusion set are deemed con-
fusable because of orthographic or phonetic simi-
larity. Other work in GEC has defined the confu-
1For simplicity, we will collectively refer to both terms as
English as a foreign language (EFL)
sion sets based on syntactic similarity, for exam-
ple all English articles or the most frequent English
prepositions form a confusion set (see for example
(Tetreault et al, 2010; Rozovskaya and Roth, 2010;
Gamon, 2010; Dahlmeier and Ng, 2011) among oth-
ers).
In contrast, we investigate in this paper a class of
grammatical errors where the source of confusion is
the similar semantics of the words, rather than or-
thography, phonetics, or syntax. In particular, we
focus on collocation errors in EFL writing. The
term collocation (Firth, 1957) describes a sequence
of words that is conventionally used together in a
particular way by native speakers and appears more
often together than one would expect by chance. The
correct use of collocations is a major difficulty for
EFL students (Farghal and Obiedat, 1995).
In this work, we present a novel approach for au-
tomatic correction of collocation errors in EFL writ-
ing. Our key observation is that words are poten-
tially confusable for an EFL student if they have
similar translations in the writer?s first language (L1-
language), or in other words if they have the same
semantics in the L1-language of the writer. The
Chinese word ? (k?n), for example, has over a
dozen translations in English, including the words
see, look, read, and watch. A Chinese speaker who
still ?thinks? in Chinese has to choose from all these
possible translations when he wants to express a sen-
tence like I like to watch movies and might instead
produce a sentence like *I like to look movies. Al-
though the meanings of watch and look are simi-
lar, the former is clearly the more fluent choice in
this context. While these types of L1-transfer er-
107
rors have been known in the EFL teaching litera-
ture (Swan and Smith, 2001; Meng, 2008), research
in GEC has mostly ignored this fact.
We first analyze collocation errors in the NUS
Corpus of Learner English (NUCLE), a fully an-
notated one-million-word corpus of learner English
which we will make available to the community for
research purposes (see Section 3 for details about
the corpus). Our analysis confirms that many col-
location errors can be traced to similar translations
in the writer?s L1-language. Based on this result,
we propose a novel approach for automatic collo-
cation error correction. The key component in our
approach generates L1-induced paraphrases which
we automatically extract from an L1-English par-
allel corpus. Our proposed approach outperforms
traditional approaches based on edit distance, ho-
mophones, and WordNet synonyms on a test set of
real-world learner data in an automatic and a human
evaluation. Finally, we present a detailed analysis of
unsolved instances in our data set to highlight direc-
tions for future work.
Our work adds to a growing body of research that
leverages parallel corpora for semantic NLP tasks,
for example in word sense disambiguation (Ng et
al., 2003; Chan and Ng, 2005; Ng and Chan, 2007;
Zhong and Ng, 2009), paraphrasing (Bannard and
Callison-Burch, 2005; Liu et al, 2010a), and ma-
chine translation evaluation (Snover et al, 2009; Liu
et al, 2010b).
The remainder of this paper is organized as fol-
lows. The next section reviews related work. Sec-
tion 3 presents our analysis of collocation errors.
Section 4 describes our approach for automatic col-
location error correction. The experimental setup
and the results are described in Sections 5 and 6, re-
spectively. Section 7 provides further analysis. Sec-
tion 8 concludes the paper.
2 Related Work
In this section, we give an overview of related work
on collocation error correction. We also highlight
differences between collocation error correction and
related NLP tasks like context-sensitive spelling er-
ror correction, synonym extraction, lexical substitu-
tion, and paraphrasing.
Most work in collocation error correction has re-
lied on dictionaries or manually created databases
to generate collocation candidates (Shei and Pain,
2000; Wible et al, 2003; Futagi et al, 2008). Other
work has focused on finding candidates that collo-
cate with similar words, e.g., verbs that appear with
the same noun objects form a confusion set (Liu et
al., 2009; Wu et al, 2010). The work most similar
to ours is probably the one presented by Chang et
al. (2008), as they also use translation information to
generate collocation candidates. However, they do
not use automatically derived paraphrases from par-
allel corpora but bilingual dictionaries. Dictionaries
usually have lower coverage, do not contain longer
phrases or inflected forms, and do not provide any
translation probability estimates. Also, their work
focuses solely on verb-noun collocations, while we
target collocations of arbitrary syntactic type.
Context-sensitive spelling error correction is the
task of correcting spelling mistakes that result in
another valid word, see for example (Golding and
Roth, 1999). It has traditionally focused on a small
number of pre-defined confusion sets, like homo-
phones or frequent spelling errors. Even when the
confusion sets are formed automatically, the simi-
larity of words in a confusion set has been based
on edit distance or phonetic similarity (Carlson et
al., 2001). In contrast, we focus on words that are
confusable due to their similar semantics instead of
similar spelling or pronunciation. Also, we do not
assume that the set of confusion sets is already given
to us. Instead, we automatically extract confusable
candidates from a parallel corpus.
Synonym extraction (Wu and Zhou, 2003), lexi-
cal substitution (McCarthy and Navigli, 2007) and
paraphrasing (Madnani and Dorr, 2010) are related
to collocation correction in the sense that they try to
find semantically equivalent words or phrases. How-
ever, there is a subtle but important difference be-
tween these tasks and collocation correction. In the
former, the main criterion is whether the original
phrase and the synonym/paraphrase candidate are
substitutable, i.e., both form a grammatical sentence
when substituted for each other in a particular con-
text. In contrast, in collocation correction, we are
primarily interested in finding candidates which are
not substitutable in their English context but appear
to be substitutable in the L1-language of the writer,
i.e., one forms a grammatical English sentence but
the other does not.
108
Sentences 52,149
Words 1,149,100
Distinct words 27,593
Avg. sentence length (words) 22.04
Collocation errors 2,747
Avg. collocation error length (words) 1.17
Avg. correction length (words) 1.13
Table 1: Statistics of the NUS Corpus of Learner En-
glish (NUCLE)
3 Analysis of EFL collocation errors
While the fact that collocation errors can be caused
by L1-transfer has been ascertained by EFL re-
searchers (Meng, 2008), we need to quantify how
frequent collocation errors can be traced to these
types of transfer errors in order to estimate how
many errors in EFL writing we can potentially hope
to correct with information about the writer?s L1-
language.
We base our analysis on the NUS Corpus of
Learner English (NUCLE). The corpus consists of
about 1,400 essays written by EFL university stu-
dents on a wide range of topics, like environmen-
tal pollution or healthcare. Most of the students are
native Chinese speakers. The corpus contains over
one million words which are completely annotated
with error tags and corrections. All annotations have
been performed by professional English instructors.
The statistics of the corpus are summarized in Ta-
ble 1. The annotation is stored in a stand-off fashion.
Each error tag consists of the start and end offset of
the annotation, the type of the error, and the appro-
priate gold correction as deemed by the annotator.
The annotators were asked to provide a correction
that would result in a grammatical sentence if the
selected word or phrase would be replaced by the
correction.
In this work, we focus on errors which have
been marked with the error tag wrong colloca-
tion/idiom/preposition. As preposition errors are not
the focus of this work, we automatically filter out
all instances which represent simple substitutions of
prepositions, using a fixed list of frequent English
prepositions. In a similar way, we filter out a small
number of article errors which were marked as collo-
cation errors. Finally, we filter out instances where
the annotated phrase or the suggested correction is
longer than 3 words, as we observe that they contain
highly context-specific corrections and are unlikely
to generalize well (e.g., ?for the simple reasons that
these can help them?? ?simply to?).
After filtering, we end up with 2,747 collocation
errors and their respective corrections, which ac-
count for about 6% of all errors in NUCLE. This
makes collocation errors the 7th largest class of er-
rors in the corpus after article errors, redundancies,
prepositions, noun number, verb tense, and mechan-
ics. Not counting duplicates, there are 2,412 distinct
collocation errors and corrections. Although there
are other error types which are more frequent, collo-
cation errors represent a particular challenge as the
possible corrections are not restricted to a closed set
of choices and they are directly related to seman-
tics rather than syntax. We analyzed the collocation
errors and found that they can be attributed to the
following sources of confusion:
Spelling: We suspect that an error is caused by simi-
lar orthography if the edit distance between the erro-
neous phrase and its correction is less than a certain
threshold.
Homophones: We suspect that an error is caused by
similar pronunciation if the erroneous word and its
correction have the same pronunciation. We use the
CuVPlus English dictionary (Mitton, 1992) to map
words to their phonetic representations.
Synonyms: We suspect that an error is caused by
synonymy if the erroneous word and its correction
are synonyms in WordNet (Fellbaum, 1998). We use
WordNet 3.0.
L1-transfer: We suspect that an error is caused by
L1-transfer if the erroneous phrase and its correction
share a common translation in a Chinese-English
phrase table. The details of the phrase table con-
struction are described in Section 4. We note that
although we focus on Chinese-English translation,
our method is applicable to any language pair where
parallel corpora are available.
As CuVPlus and WordNet are defined for indi-
vidual words, we extend the matching process to
phrases in the following way: two phrases A and B
are deemed homophones/synonyms if they have the
same length and the i-th word in phrase A is a ho-
mophone/synonym of the corresponding i-th word
in phrase B.
109
Spelling . . . it received critics (criticism) as much as complaints . . .
. . . budget for the aged to improvise (improve) other areas.
Homophones . . . diverse spending can aide (aid) our country.
. . . insure (ensure) the safety of civilians . . .
Synonyms . . . rapid increment (increase) of the seniors . . .
. . . energy that we can apply (use) in the future . . .
L1-transfer . . . and give (provide, ?? ) reasonable fares to the public . . .
. . . and concerns (attention, ?? ) that the nation put on technology and engineering . . .
Table 3: Examples of collocation errors with different sources of confusion. The correction is shown in parenthesis.
For L1-transfer, we also show the shared Chinese translation. The L1-transfer examples shown here do not belong to
any of the other categories.
Suspected Error Source Tokens Types
Spelling 154 131
Homophones 2 2
Synonyms 74 60
L1-transfer 1016 782
L1-transfer w/o spelling 954 727
L1-transfer w/o homophones 1015 781
L1-transfer w/o synonyms 958 737
L1-transfer w/o spelling,
homophones, 906 692
synonyms
Table 2: Analysis of collocation errors. The threshold for
spelling errors is one for phrases of up to six characters
and two for the remaining phrases.
The results of the analysis are shown in Table 2.
Tokens refer to running erroneous phrase-correction
pairs including duplicates, and types refer to distinct
erroneous phrase-correction pairs. As a collocation
error can be part of more than one category, the rows
in the table do not sum up to the total number of
errors. The number of errors that can be traced to
L1-transfer greatly outnumbers all other categories.
The table also shows the number of collocation er-
rors that can be traced to L1-transfer but not the
other sources. 906 collocation errors with 692 dis-
tinct collocation error types can be attributed only to
L1-transfer but not to spelling, homophones, or syn-
onyms. Table 3 shows some examples of collocation
errors for each category from our corpus. We note
that there are also collocation error types that cannot
be traced to any of the above sources. We will return
to these errors in Section 7.
4 Correcting Collocation Errors
In this section, we propose a novel approach for cor-
recting collocation errors in EFL writing.
4.1 L1-induced Paraphrases
We use the popular technique of paraphrasing
with parallel corpora (Bannard and Callison-Burch,
2005) to automatically find collocation candidates
from a sentence-aligned L1-English parallel corpus.
As most of the essays in our corpus are written by
native Chinese speakers, we use the FBIS Chinese-
English corpus, which consists of about 230,000
Chinese sentences (8.5 million words) from news
articles, each with a single English translation. We
tokenize and lowercase the English half of the cor-
pus in the standard way. We segment the Chinese
half of the corpus using the maximum entropy seg-
menter from (Ng and Low, 2004; Low et al, 2005).
Subsequently, we automatically align the texts at the
word level using the Berkeley aligner (Liang et al,
2006; Haghighi et al, 2009). We extract English-L1
and L1-English phrases of up to three words from
the aligned texts using the widely used phrase ex-
traction heuristic in (Koehn et al, 2003). The para-
phrase probability of an English phrase e1 given an
English phrase e2 is defined as
p(e1|e2) =
?
f
p(e1|f)p(f |e2) (1)
where f denotes a foreign phrase in the L1 language.
The phrase translation probabilities p(e1|f) and
p(f |e2) are estimated by maximum likelihood es-
timation and smoothed using Good-Turing smooth-
ing (Foster et al, 2006). Finally, we only keep para-
110
phrases with a probability above a certain threshold
(set to 0.001 in our work).
4.2 Collocation Correction with Phrase-based
SMT
We implement our approach in the framework
of phrase-based statistical machine transla-
tion (SMT) (Koehn et al, 2003). Phrase-based
SMT tries to find the highest scoring translation e
given an input sentence f . The decoding process of
finding the highest scoring translation is guided by a
log-linear model which scores translation candidates
using a set of feature functions hi, i = 1, . . . , n
score(e|f) = exp
( n?
i=1
?ihi(e, f)
)
. (2)
Typical features include a phrase translation proba-
bility p(e|f), an inverse phrase translation probabil-
ity p(f |e), a language model score p(e), and a con-
stant phrase penalty. The optimization of the feature
weights ?i, i = 1, . . . , n can be done using mini-
mum error rate training (MERT) (Och, 2003) on a
development set of input sentences and their refer-
ence translations.
Because of the great flexibility of the log-linear
model, researchers have used the framework for
other tasks outside SMT, including grammatical er-
ror correction (Brockett et al, 2006). We adopt a
similar approach in this work. We modify the phrase
table of the popular phrase-based SMT decoder
MOSES (Koehn et al, 2007) to include collocation
corrections with features derived from spelling, ho-
mophones, synonyms, and L1-induced paraphrases.
? Spelling: For each English word, the phrase ta-
ble contains entries consisting of the word itself
and each word that is within a certain edit dis-
tance from the original word. Each entry has a
constant feature of 1.0.
? Homophones: For each English word, the
phrase table contains entries consisting of the
word itself and each of the word?s homophones.
We determine homophones using the CuVPlus
dictionary. Each entry has a constant feature of
1.0.
? Synonyms: For each English word, the phrase
table contains entries consisting of the word it-
self and each of its synonyms in WordNet. If a
word has more than one sense, we consider all
its senses. Each entry has a constant feature of
1.0.
? L1-paraphrases: For each English phrase, the
phrase table contains entries consisting of the
phrase and each of its L1-derived paraphrases
as described in Section 4.1. Each entry has two
real-valued features: a paraphrase probability
according to Equation 1 and an inverse para-
phrase probability.
? Baseline We combine the phrase tables built
for spelling, homophones, and synonyms. The
combined phrase table contains three binary
features for spelling, homophones, and syn-
onyms, respectively.
? All We combine the phrase tables from
spelling, homophones, synonyms, and L1-
paraphrases. The combined phrase table con-
tains five features: three binary features for
spelling, homophones, and synonyms, and
two real-valued features for the L1-paraphrase
probability and inverse L1-paraphrase proba-
bility.
Additionally, each phrase table contains the standard
constant phrase penalty feature. The first four ta-
bles only contain collocation candidates for individ-
ual words. We leave it to the decoder to construct
corrections for longer phrases during the decoding
process if necessary.
5 Experiments
In this section, we empirically evaluate our approach
on real collocation errors in learner English.
5.1 Data Set
We randomly sample a development set of 770 sen-
tences and a test set of 856 sentences from our cor-
pus. Each sentence contains exactly one collocation
error. The sampling is performed in a way that sen-
tences from the same document cannot end up in
both the development and the test set. In order to
111
keep conditions as realistic as possible, we make no
attempt to filter the test set in any way.
We build phrase tables as described in Section 4.2.
For the purpose of the experiments reported in this
paper, we only need to generate phrase table entries
for words and phrases which actually appear in the
development or test set.
5.2 Evaluation Metrics
We conduct an automatic and a human evalua-
tion. Our main evaluation metric is mean recipro-
cal rank (MRR) which is the arithmetic mean of the
inverse ranks of the first correct answer returned by
the system
MRR = 1N
N?
i=1
1
rank(i) (3)
where N is the size of the test set. If the system did
not return a correct answer for a test instance, we set
1
rank(i) to zero.In the human evaluation, we additionally report
precision at rank k, k = 1, 2, 3, which we calculate
as follows:
P@k =
?
a?A score(a)
|A| (4)
where A is the set of returned answers of rank k or
less and score(?) is a real-valued scoring function
between zero and one.
5.3 Collocation Error Experiments
Automatic correction of collocation errors can con-
ceptually be divided into two steps: i) identification
of wrong collocations in the input, and ii) correc-
tion of the identified collocations. In this work, we
focus on the second step and assume that the erro-
neous collocation has already been identified. While
this might seem like a simplification, it has been the
common evaluation setup in collocation error cor-
rection (see for example (Wu et al, 2010)). It also
has a practical application where the user first selects
a word or phrase and the system displays possible
corrections.
In our experiments, we use the start and end offset
of the collocation error provided by the human anno-
tator to identify the location of the collocation error.
We fix the translation of the rest of the sentence to
its identity. We remove phrase table entries where
the phrase and the candidate correction are identi-
cal, thus practically forcing the system to change
the identified phrase. We set the distortion limit of
the decoder to zero to achieve monotone decoding.
We previously observed that word order errors are
virtually absent in our collocation errors. For the
language model, we use a 5-gram language model
trained on the English Gigaword corpus with modi-
fied Kneser-Ney smoothing. All experiments use the
same language model to allow a fair comparison.
We perform MERT training with the popular
BLEU metric (Papineni et al, 2002) on the devel-
opment set of erroneous sentences and their correc-
tions. As the search space is restricted to changing
a single phrase per sentence, training converges rel-
atively quickly after two or three iterations. After
convergence, the model can be used to automatically
correct new collocation errors.
6 Results
We evaluate the performance of the proposed
method on our test set of 856 sentences, each with
one collocation error. We conduct both an automatic
and a human evaluation. In the automatic evalua-
tion, the system?s performance is measured by com-
puting the rank of the gold answer provided by the
human annotator in the n-best list of the system. We
limit the size of the n-best list to the top 100 out-
puts. If the gold answer is not found in the top 100
outputs, the rank is considered to be infinity, or in
other words, the inverse of the rank is zero. We also
report the number of test instances for which the
gold answer was ranked among the top k answers,
k = 1, 2, 3, 10, 100. The results of the automatic
evaluation are shown in Table 4
For collocation errors, there are usually more than
one possible correct answer. Therefore, automatic
evaluation underestimates the actual performance of
the system by only considering the single gold an-
swer as correct and all other answers as wrong. As
such, we carried out a human evaluation for the sys-
tems BASELINE and ALL. We recruited two English
speakers to judge a subset of 500 test sentences. For
each sentence, a judge was shown the original sen-
tence and the 3-best candidates of each of the two
systems. We restricted human evaluation to the 3-
best candidates, as we believe that answers at a rank
112
Model Rank = 1 Rank ? 2 Rank ? 3 Rank ? 10 Rank ? 100 MRR
Spelling 35 41 42 44 44 4.51
Homophones 1 1 1 1 1 0.11
Synonyms 32 47 52 60 61 4.98
Baseline 49 68 80 93 96 7.61
L1-paraphrases 93 133 154 216 243 15.43
All 112 150 166 216 241 17.21
Table 4: Results of automatic evaluation. Columns two to six show the number of gold answers that are ranked within
the top k answers. The last column shows the mean reciprocal rank in percentage. Bigger values are better.
P(A) 0.8076
Kappa 0.6152
Table 5: Inter-annotator agreement. P (E) = 0.5.
larger than three will not be very useful in a prac-
tical application. The candidates are displayed to-
gether in alphabetical order without any information
about their rank or which system produced them or
the gold answer by the annotator. The difference
between the candidates and the original sentence is
highlighted. The judges were asked to make a bi-
nary judgment for each of the candidates on whether
the proposed candidate is a valid correction of the
original or not. We represent valid corrections with
a score of 1.0 and invalid corrections with a score
of 0.0. Inter-annotator agreement is reported in Ta-
ble 5. The chance of agreement P (A) is the percent-
age of times that the annotators agree, and P (E) is
the expected agreement by chance, which is 0.5 in
our case. The Kappa coefficient is defined as
Kappa = P(A)? P(E)1? P(E)
We obtain a Kappa coefficient of 0.6152. A Kappa
coefficient between 0.6 and 0.8 is considered as
showing substantial agreement according to Landis
and Koch (1977). To compute precision at rank k,
we average the judgments. Thus, a system can re-
ceive a score of 0.0 (both judgments negative), 0.5
(judges disagree), or 1.0 (both judgments positive)
for each returned answer. To compute MRR, we
cannot simply average the judgments as MRR re-
quires binary judgments on whether an item is cor-
rect or not. Instead, we report MRR on the union and
the intersection of the judgments. In the first case,
the rank of the first correct item is the minimum
rank of any item judged correct by either judge. In
the second case, the rank of the first correct item
is the minimum rank of any item judged correct by
both judges. The results for the human evaluation
are shown in Table 6. Our best system ALL outper-
forms the BASELINE approach on all measures. It
receives a precision at rank 1 of 38.20% and a MRR
of 33.16% (intersection) and 57.26% (union). Ta-
ble 7 shows some examples from our test set.
Unfortunately, comparison of our results with pre-
vious work is complicated by the fact that there cur-
rently exists no standard data set for collocation er-
ror correction. We will make our corpus available
for research purposes in the hope that it will allow
researchers to more directly compare their results in
future.
7 Analysis
In this section, we analyze and categorize those test
instances for which the ALL system could not pro-
duce an acceptable correction in the top 3 candi-
dates. We manually analyze 100 test sentences for
which neither judge had deemed any candidate an-
swer to be a valid correction. Based on our findings,
we categorize the 100 sentences into eight categories
which are shown below. Table 8 shows examples
from each category.
Out-of-vocabulary (21/100) The most frequent rea-
son why the system does not produce a good correc-
tion is that the erroneous collocation is out of vocab-
ulary. These collocations often involve compound
words, like man-hours or carefully-nurturing, or in-
frequent expressions, like copy phenomena, which
do not appear in the FBIS parallel corpus. We ex-
pect that this problem can be reduced by using larger
parallel corpora for paraphrase extraction.
Near miss (18/100) The second largest category
113
Model Rank = 1 Rank ? 2 Rank ? 3 P@1 P@2 P@3 MRR
Baseline 43 | 141 69 | 201 83 | 237 18.40 16.68 15.36 12.13 | 36.60
All 137 | 245 176 | 303 204 | 340 38.20 32.87 29.30 33.16 | 57.26
Table 6: Results of human evaluation. Rank and MRR results are shown for the intersection (first value) and union
(second value) of human judgments.
Original it must be clear, concise and unambiguous to prevent any off-track
Gold it must be clear, concise and unambiguous to avoid any off-track
All it must be clear, concise and unambiguous to avoid any off-track
it must be clear, concise and unambiguous to stop any off-track
it must be clear, concise and unambiguous to block any off-track
Baseline *it must be clear, concise and unambiguous to present any off-track
it must be clear, concise and unambiguous to forestall any off-track
*it must be clear, concise and unambiguous to lock any off-track
Original although many may agree that public spending on the elderly should be limited . . .
Gold although many may argue that public spending on the elderly should be limited . . .
All although many may believe that public spending on the elderly should be limited . . .
although many may think that public spending on the elderly should be limited . . .
although many may accept that public spending on the elderly should be limited . . .
Baseline *although many may agreed that public spending on the elderly should be limited . . .
*although many may hold that public spending on the elderly should be limited . . .
*although many may agrees that public spending on the elderly should be limited . . .
Table 7: Examples of test sentences with the top 3 answers of the ALL and BASELINE system. An answer judged
incorrect by at least one judge is marked with an asterisk (*).
Out of vocabulary . . . many illegal copy phenomena (copy phenomena, copies) in china.
. . . lead to reduced man-hours (man-hours, productivity) as people fall sick . . .
Near miss . . . smaller groups of people, sometimes even (more, only) individual .
. . . take pre-emptive actions (activities, measures) . . .
Function/auxiliary words . . . entertainment an elderly person can have (be, enjoy) .
. . . and the security issue is solved also (and, too)
Discourse specific . . . make other countries respect and fear you (<question mark>, a country)
. . . will contribute nothing to the accident (explosion, problem) .
Spelling errors this incidence (rate, incident) had also resulted in 4 fatalities . . .
refrigerator did not compromise (yield, comprise) of any moving parts . . .
Word sense . . . refers to the desire or shortage of a good (better, commodity) and . . .
. . . members are always from different majors (major league, specialties)
Preposition constructions . . . can be an area worth investing (investing, investing in)
. . . in spending their resources (resources, resources on)
Others this might redirect (make sound, reduce) foreign investments . . .
. . . a trading hub since british ?s (british ?s, british) rule.
Table 8: Examples of sentences without valid corrections by the ALL model. The top-1 suggestion of the system and
the gold answer (in bold) are shown in parenthesis.
114
consists of instances where the system barely misses
the gold standard answer. This includes cases where
the extracted L1-paraphrases do not contain the ex-
act phrase required, e.g., the paraphrase table con-
tains even|||only get when the gold correction was
even ? only, or the phrase table actually contains
the gold answer but fails to rank it among the top 3
answers. The first problem could be addressed by
modifying the phrase extraction heuristic to produce
more fine-grained phrase pairs. The second prob-
lem requires a better language model. Although our
language model is trained on the large English Giga-
word corpus, it is not always successful in promot-
ing the correct candidate to the top. The domain mis-
match between the newswire domain of Gigaword
and student essays could be one reason for this.
Function/auxiliary words (14/100) We observe
that collocation errors that involve function words
or auxiliary words are not handled very well by our
model. Function words and auxiliary words in En-
glish lack direct counterparts in Chinese, which is
why the word alignments and therefore the extracted
phrases for these words contain a high amount of
noise. As function words and auxiliaries are essen-
tially a closed set, it might be more promising to
build separate models with fixed confusion sets for
them.
Discourse specific (14/100) Some of the gold an-
swers are highly specific to the particular discourse
that they appear in. As our model corrects colloca-
tion errors at the sentence level, such gold answers
will be very difficult or impossible to determine cor-
rectly. Including more context beyond the sentence
level might help to overcome this problem, although
it is not easy to integrate this larger context informa-
tion.
Spelling errors (9/100) Some of the collocation er-
rors are caused by spelling mistakes, e.g., incidence
instead of incident. Although the ALL model in-
cludes candidates which are created through edit dis-
tance, paraphrase candidates created from the mis-
spelled word can dominate the top 3 ranks, e.g., rate
and frequently are paraphrases of incidence. A pos-
sible solution would be to perform spell-checking as
a separate pre-processing step prior to collocation
correction.
Word sense (7/100) Some of the failures of the
model can be attributed to ambiguous senses of the
collocation phrase. As we do not perform word
sense disambiguation in our current work, candi-
dates from other word senses can end up as the top
candidates. Including word sense disambiguation
into the model might help, although accurate word
sense disambiguation on noisy learner text may not
be easy.
Preposition constructions (6/100) Some of the col-
location errors involve preposition constructions,
e.g., the student wrote attend instead of attend
to. Because prepositions do not have a direct
counterpart in Chinese, the L1-paraphrases do not
model their semantics very well. This category is
closely related to the function/auxiliary word cate-
gory. Again, since prepositions are a closed set, it
might be more promising to build a separate model
for them.
Others (11/100) Other mistakes include collocation
errors where the gold answer slightly changed the
semantics of the target word, e.g., redirect potential
foreign investments ? reduce potential foreign in-
vestments, active-passive alternation (enhanced eco-
nomics? was economical), and noun possessive er-
rors (british ?s rule? british rule).
8 Conclusion and Future Work
We have presented a novel approach for correcting
collocation errors in written learner text. Our ap-
proach exploits the semantic similarity of words in
the writer?s L1-language based on paraphrases ex-
tracted from an L1-English parallel corpus. Our ex-
periments on real-world learner data show that our
approach outperforms traditional approaches based
on edit distance, homophones, and synonyms by a
large margin.
In future work, we plan to extend our system to
fully automatic collocation correction that involves
both identification and correction of collocation er-
rors.
Acknowledgments
This research was done for CSIDM Project No.
CSIDM-200804 partially funded by a grant from
the National Research Foundation (NRF) adminis-
tered by the Media Development Authority (MDA)
of Singapore.
115
References
C. Bannard and C. Callison-Burch. 2005. Paraphras-
ing with bilingual parallel corpora. In Proceedings of
ACL.
C. Brockett, W. B. Dolan, and M. Gamon. 2006. Cor-
recting ESL errors using phrasal SMT techniques. In
Proceedings of ACL.
A. J. Carlson, J. Rosen, and D. Roth. 2001. Scaling
up context-sensitive text correction. In Proceedings of
IAAI.
Y. S. Chan and H. T. Ng. 2005. Scaling up word sense
disambiguation via parallel texts. In Proceedings of
AAAI.
Y. C. Chang, J. S. Chang, H. J. Chen, and H. C. Liou.
2008. An automatic collocation writing assistant for
Taiwanese EFL learners: A case of corpus-based NLP
technology. Computer Assisted Language Learning,
21(3):283?299.
D. Dahlmeier and H. T. Ng. 2011. Grammatical error
correction with alternating structure optimization. In
Proceedings of ACL.
M. Farghal and H. Obiedat. 1995. Collocations: A ne-
glected variable in EFL. International Review of App-
plied Linguistics, 33.
C. Fellbaum, editor. 1998. WordNet: An electronic lexi-
cal database. MIT Press, Cambridge,MA.
J. R. Firth. 1957. Papers in Linguistics 1934-1951. Ox-
ford University Press, London.
G. Foster, R. Kuhn, and H. Johnson. 2006. Phrasetable
smoothing for statistical machine translation. In Pro-
ceedings of EMNLP.
Y. Futagi, P. Deane, M. Chodorow, and J. Tetreault. 2008.
A computational approach to detecting collocation er-
rors in the writing of non-native speakers of English.
Journal of Computer-Assisted Learning, 21.
M. Gamon. 2010. Using mostly native data to correct
errors in learners? writing: A meta-classifier approach.
In Proceedings of HLT-NAACL.
A. R. Golding and D. Roth. 1999. A winnow-based ap-
proach to context-sensitive spelling correction. Ma-
chine Learning, 34:107?130.
A. Haghighi, J. Blitzer, J. DeNero, and D. Klein. 2009.
Better word alignments with supervised ITG models.
In Proceedings of ACL-IJCNLP.
P. Koehn, F. J. Och, and D. Marcu. 2003. Statisti-
cal phrase-based translation. In Proceedings of HLT-
NAACL.
P. Koehn, H. Hoang, A. Birch, C. Callison-Burch,
M. Federico, N. Bertoldi, B. Cowan, W. Shen,
C. Moran, R. Zens, C. Dyer, O. Bojar, A. Constantin,
and E. Herbst. 2007. Moses: Open source toolkit for
statistical machine translation. In Proceedings of ACL
demonstration session.
J. R. Landis and G. G. Koch. 1977. The measurement of
observer agreement for categorical data. Biometrics,
33(1).
P. Liang, B. Taskar, and D. Klein. 2006. Alignment by
agreement. In Proceedings of HLT-NAACL.
A. L. Liu, D. Wible, and N. L. Tsao. 2009. Automated
suggestions for miscollocations. In Proceedings of the
NAACL HLT Workshop on Innovative Use of NLP for
Building Educational Applications.
C. Liu, D. Dahlmeier, and H. T. Ng. 2010a. PEM: a
paraphrase evaluation metric exploiting parallel texts.
In Proceedings of EMNLP.
C. Liu, D. Dahlmeier, and H. T. Ng. 2010b.
TESLA: Translation evaluation of sentences with
linear-programming-based analysis. In Proceedings of
WMT and MetricsMATR.
J. K. Low, H. T. Ng, and W. Guo. 2005. A maximum
entropy approach to Chinese word segmentation. In
Proceedings of the 4th SIGHAN Workshop on Chinese
Language Processing.
N. Madnani and B. J. Dorr. 2010. Generating phrasal and
sentential paraphrases: A survey of data-driven meth-
ods. Computational Linguistics, 36(3):341?387.
D. McCarthy and R. Navigli. 2007. Semeval-2007 task
10: English lexical substitution task. In Proceedings
of the 4th International Workshop on Semantic Evalu-
ations (SemEval-2007).
J. Meng. 2008. Erroneous collocations caused by lan-
guage transfer in Chinese EFL writing. US-China For-
eign Language, 6:57?61.
R. Mitton. 1992. A description of a computer-usable dic-
tionary file based on the Oxford Advanced Learner?s
Dictionary of Current English.
H. T. Ng and Y. S. Chan. 2007. Semeval-2007 task 11:
English lexical sample task via english-chinese paral-
lel text. In Proceedings of the 4th International Work-
shop on Semantic Evaluations (SemEval 2007).
H. T. Ng and J. K. Low. 2004. Chinese part-of-speech
tagging: One-at-a-time or all-at-once? word-based or
character-based? In Proceedings of EMNLP.
H. T. Ng, B. Wang, and Y. S. Chan. 2003. Exploiting
parallel texts for word sense disambiguation: An em-
pirical study. In Proceedings of ACL.
F. Och. 2003. Minimum error rate training in statistical
machine translation. In Proceedings of ACL.
K. Papineni, S. Roukos, T. Ward, and W.-J. Zhu. 2002.
BLEU: A method for automatic evaluation of machine
translation. In Proceedings of ACL.
A. Rozovskaya and D. Roth. 2010. Generating confu-
sion sets for context-sensitive error correction. In Pro-
ceedings of EMNLP.
C. C. Shei and H. Pain. 2000. An ESL writer?s collo-
cational aid. Computer Assisted Language Learning,
13.
116
M. Snover, N. Madnani, B. Dorr, and R. Schwartz. 2009.
Fluency, adequacy, or HTER? Exploring different hu-
man judgments with a tunable MT metric. In Proceed-
ings of WMT.
M. Swan and B. Smith. 2001. Learner English: A
Teacher?s Guide to Interference and Other Problems.
Cambridge University Press, Cambridge, UK.
J. Tetreault, J. Foster, and M. Chodorow. 2010. Using
parse features for preposition selection and error de-
tection. In Proceedings of ACL.
D. Wible, C. H. Kuo, N. L. Tsao, A. Liu, and H. L. Lin.
2003. Bootstrapping in a language learning environ-
ment. Journal of Computer-Assisted Learning, 19.
H. Wu and M. Zhou. 2003. Synonymous collocation ex-
traction using translation information. In Proceedings
of ACL.
J. C. Wu, Y. C. Chang, T. Mitamura, and J. S. Chang.
2010. Automatic collocation suggestion in academic
writing. In Proceedings of the ACL 2010 Conference
Short Papers.
Z. Zhong and H. T. Ng. 2009. Word sense disambigua-
tion for all words without hard labor. In Proceedings
of IJCAI.
117
Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 375?384,
Edinburgh, Scotland, UK, July 27?31, 2011. c?2011 Association for Computational Linguistics
Better Evaluation Metrics Lead to Better Machine Translation
Chang Liu1 and Daniel Dahlmeier2 and Hwee Tou Ng1,2
1Department of Computer Science, National University of Singapore
2NUS Graduate School for Integrative Sciences and Engineering
{liuchan1,danielhe,nght}@comp.nus.edu.sg
Abstract
Many machine translation evaluation met-
rics have been proposed after the seminal
BLEU metric, and many among them have
been found to consistently outperform BLEU,
demonstrated by their better correlations with
human judgment. It has long been the hope
that by tuning machine translation systems
against these new generation metrics, ad-
vances in automatic machine translation eval-
uation can lead directly to advances in auto-
matic machine translation. However, to date
there has been no unambiguous report that
these new metrics can improve a state-of-the-
art machine translation system over its BLEU-
tuned baseline.
In this paper, we demonstrate that tuning
Joshua, a hierarchical phrase-based statistical
machine translation system, with the TESLA
metrics results in significantly better human-
judged translation quality than the BLEU-
tuned baseline. TESLA-M in particular is
simple and performs well in practice on large
datasets. We release all our implementation
under an open source license. It is our hope
that this work will encourage the machine
translation community to finally move away
from BLEU as the unquestioned default and
to consider the new generation metrics when
tuning their systems.
1 Introduction
The dominant framework of machine translation
(MT) today is statistical machine translation (SMT)
(Hutchins, 2007). At the core of the system is the
decoder, which performs the actual translation. The
decoder is parameterized, and estimating the optimal
set of parameter values is of paramount importance
in getting good translations. In SMT, the parame-
ter space is explored by a tuning algorithm, typically
MERT (Minimum Error Rate Training) (Och, 2003),
though the exact method is not important for our
purpose. The tuning algorithm carries out repeated
experiments with different decoder parameter val-
ues over a development data set, for which reference
translations are given. An automatic MT evaluation
metric compares the output of the decoder against
the reference(s), and guides the tuning algorithm to-
wards iteratively better decoder parameters and out-
put translations. The quality of the automatic MT
evaluation metric therefore has an immediate effect
on the whole system.
The first automatic MT evaluation metric to show
a high correlation with human judgment is BLEU
(Papineni et al, 2002). Together with its close vari-
ant the NIST metric, they have quickly become the
standard way of tuning statistical machine transla-
tion systems. While BLEU is an impressively sim-
ple and effective metric, recent evaluations have
shown that many new generation metrics can out-
perform BLEU in terms of correlation with human
judgment (Callison-Burch et al, 2009; Callison-
Burch et al, 2010). Some of these new metrics in-
clude METEOR (Banerjee and Lavie, 2005; Lavie
and Agarwal, 2007), TER (Snover et al, 2006),
MAXSIM (Chan and Ng, 2008; Chan and Ng,
2009), and TESLA (Liu et al, 2010).
Given the close relationship between automatic
MT and automatic MT evaluation, the logical expec-
tation is that a better MT evaluation metric would
375
lead to better MT systems. However, this linkage
has not yet been realized. In the SMT community,
MT tuning still uses BLEU almost exclusively.
Some researchers have investigated the use of bet-
ter metrics for MT tuning, with mixed results. Most
notably, Pado? et al (2009) reported improved human
judgment using their entailment-based metric. How-
ever, the metric is heavy weight and slow in practice,
with an estimated runtime of 40 days on the NIST
MT 2002/2006/2008 dataset, and the authors had to
resort to a two-phase MERT process with a reduced
n-best list. As we shall see, our experiments use the
similarly sized WMT 2010 dataset, and most of our
runs take less than one day.
Cer et al (2010) compared tuning a phrase-based
SMT system with BLEU, NIST, METEOR, and
TER, and concluded that BLEU and NIST are still
the best choices for MT tuning, despite the proven
higher correlation of METEOR and TER with hu-
man judgment.
In this work, we investigate the effect of MERT
using BLEU, TER, and two variants of TESLA,
TESLA-M and TESLA-F, on Joshua (Li et al,
2009), a state-of-the-art hierarchical phrase-based
SMT system (Chiang, 2005; Chiang, 2007). Our
empirical study is carried out in the context of WMT
2010, for the French-English, Spanish-English, and
German-English machine translation tasks. We
show that Joshua responds well to the change of
evaluation metric, in that a system trained on met-
ric M typically does well when judged by the same
metric M. We further evaluate the different systems
with manual judgments and show that the TESLA
family of metrics (both TESLA-M and TESLA-F)
significantly outperforms BLEU when used to guide
the MERT search.
The rest of this paper is organized as follows. In
Section 2, we describe the four evaluation metrics
used. Section 3 outlines our experimental set up us-
ing the WMT 2010 machine translation tasks. Sec-
tion 4 presents the evaluation results, both automatic
and manual. Finally, we discuss our findings in Sec-
tion 5, future work in Section 6, and conclude in
Section 7.
2 Evaluation metrics
This section describes the metrics used in our exper-
iments. We do not seek to explain all their variants
and intricate details, but rather to outline their core
characteristics and to highlight their similarities and
differences. In particular, since all our experiments
are based on single references, we omit the com-
plications due to multiple references and refer our
readers instead to the respective original papers for
the details.
2.1 BLEU
BLEU is fundamentally based on n-gram match pre-
cisions. Given a reference text R and a translation
candidate T , we generate the bag of all n-grams con-
tained inR and T for n = 1, 2, 3, 4, and denote them
as BNGnR and BNGnT respectively. The n-gram pre-
cision is thus defined as
Pn =
|BNGnR ? BNGnT|
|BNGnT|
To compensate for the lack of the recall measure,
and hence the tendency to produce short translations,
BLEU introduces a brevity penalty, defined as
BP =
{
1 if|T| > |R|
e1?|R|/|T | if|T| ? |R|
where the | ? | operator denotes the size of a bag or
the number of words in a text. The metric is finally
defined as
BLEU(R,T) = BP? 4
?
P1P2P3P4
BLEU is a very simple metric requiring neither
training nor language-specific resources. Its use of
the brevity penalty is however questionable, as sub-
sequent research on n-gram-based metrics has con-
sistently found that recall is in fact a more potent
indicator than precision (Banerjee and Lavie, 2005;
Zhou et al, 2006; Chan and Ng, 2009). As we
shall see, despite the BP term, BLEU still exhibits
a strong tendency to produce short translations.
2.2 TER
TER is based on counting transformations rather
than n-gram matches. The metric is defined as the
376
minimum number of edits needed to change a can-
didate translation T to the reference R, normalized
by the length of the reference, i.e.,
TER(R,T) = number of edits|R|
One edit is defined as one insertion, deletion, or
substitution of a single word, or the shift of a con-
tiguous sequence of words, regardless of size and
distance. Minimizing the edit distance so defined
has been shown to be NP-complete, so the evalua-
tion is carried out in practice by a heuristic greedy
search algorithm.
TER is a strong contender as the leading new gen-
eration automatic metric and has been used in major
evaluation campaigns such as GALE. Like BLEU,
it is simple and requires no language specific re-
sources. TER also corresponds well to the human
intuition of an evaluation metric.
2.3 TESLA-M
TESLA1 is a family of linear programming-based
metrics proposed by Liu et al (2010) that incor-
porates many newer ideas. The simplest varia-
tion is TESLA-M2, based on matching bags of n-
grams (BNG) like BLEU. However, unlike BLEU,
TESLA-M formulates the matching process as a
real-valued linear programming problem, thereby
allowing the use of weights. An example weighted
BNG matching problem is shown in Figure 1.
Two kinds of weights are used in TESLA-M.
First, the metric emphasizes the content words by
discounting the weight of an n-gram by 0.1 for ev-
ery function word it contains. Second, the similarity
between two n-grams is a function dependent on the
lemmas, the WordNet synsets (Fellbaum, 1998), and
the POS tag of every word in the n-grams.
Each node in Figure 1 represents one weighted n-
gram. The four in the top row represent one BNG,
and the three at the bottom represent the other BNG.
The goal of the linear programming problem is to
assign weights to the links between the two BNGs,
so as to maximize the sum of the products of the link
weights and their corresponding similarity scores.
1The source code of TESLA is available at
nlp.comp.nus.edu.sg/software/
2M stands for minimal.
w=1.0 w=0.1 w=0.1 w=0.1
w=0.01 w=0.1 w=0.1
s=0.1 s=0.8s=0.5 s=0.8
Good morning morning , , sir sir .
Hello , , Querrien Querrien .
s=0.4
(a) The matching problem
w=1.0 w=0.1 w=0.1 w=0.1
w=0.01 w=0.1 w=0.1
w=0.1w=0.01 w=0.1
s885Go8d m r o8d m r Gn nGimd imdG.
g,HH8Gn nGel,ddm, el,ddm, G.
(b) The solution
Figure 1: Matching two weighted bags of n-grams.
w denotes the weight and s denotes the similarity.
The constraints of the linear programming prob-
lem are: (1) all assigned weights must be non-
negative, and (2) the sum of weights assigned to all
links connecting a node cannot exceed the node?s
weight. Intuitively, we allow splitting n-grams into
fractional counts, and match them giving priority to
the pairs with the highest similarities.
The linear programming formulation ensures that
the matching can be solved uniquely and efficiently.
Once the solution is found and let the maximized
objective function value be S, the precision is com-
puted as S over the sum of weights of the translation
candidate n-grams. Similarly, the recall is S over the
sum of weights of the reference n-grams. The pre-
cision and the recall are then combined to form the
F-0.8 measure:
Fn =
Precision? Recall
0.8? Precision + 0.2? Recall
This F-measure gives more importance to the re-
call, reflecting its closer correlation with human
judgment. Fn for n = 1, 2, 3 are calculated and av-
eraged to produce the final score.
TESLA-M gains an edge over the previous two
metrics by the use of lightweight linguistic features
such as lemmas, synonym dictionaries, and POS
377
Metric Spearman?s rho
TESLA-F .94
TESLA-M .93
meteor-next-* .92
1-TERp .90
BLEU-4-v13a-c .89
Table 1: Selected system-level Spearman?s rho cor-
relation with the human judgment for the into-
English task, as reported in WMT 2010.
Metric Spearman?s rho
TESLA-M .93
meteor-next-rank .82
1-TERp .81
BLEU-4-v13a-c .80
TESLA-F .76
Table 2: Selected system-level Spearman?s rho cor-
relation with the human judgment for the out-of-
English task, as reported in WMT 2010.
tags. While such tools are usually available even for
languages other than English, it does make TESLA-
M more troublesome to port to non-English lan-
guages.
TESLA-M did well in the WMT 2010 evaluation
campaign. According to the system-level correla-
tion with human judgments (Tables 1 and 2), it ranks
top for the out-of-English task and very close to the
top for the into-English task (Callison-Burch et al,
2010).
2.4 TESLA-F3
TESLA-F builds on top of TESLA-M. While word-
level synonyms are handled in TESLA-M by exam-
ining WordNet synsets, no modeling of phrase-level
synonyms is possible. TESLA-F attempts to rem-
edy this shortcoming by exploiting a phrase table
between the target language and another language,
known as the pivot language.
Assume the target language is English and the
pivot language is French, i.e., we are provided with
an English-French phrase table. Let R and T be the
3TESLA-F refers to the metric called TESLA in (Liu et al,
2010). To minimize confusion, in this work we call the metric
TESLA-F and refer to the whole family of metrics as TESLA.
F stands for full.
w=1.=0s858G=1od 0s8m8r8nmi
Figure 2: A degenerate confusion network in
French. The phrase table maps Good morning , sir .
to Bonjour , monsieur .
w=1.=0s858G8od 
mrn0i858G8odg
,0HsseH18G8gdo d8G8gdo
Figure 3: A confusion network in French. The
phrase table maps Hello , to Bonjour , with P = 0.9
and to Salut , with P = 0.1.
reference and the translation candidate respectively,
both in English. As an example,
R: Good morning , sir .
T: Hello , Querrien .
TESLA-F first segments both R and T into
phrases to maximize the probability of the sen-
tences. For example, suppose both Good morning ,
sir . and Hello , can be found in the English-French
phrase table, and proper name Querrien is out-of-
vocabulary, then a likely segmentation is:
R: ||| Good morning , sir . |||
T: ||| Hello , ||| Querrien ||| . |||
Each English phrase is then mapped to a bag
of weighted French phrases using the phrase table,
transforming the English sentences into confusion
networks resembling Figures 2 and 3. French n-
grams are extracted from these confusion network
representations, known as pivot language n-grams.
The bag of pivot language n-grams generated by R
is then matched against that generated by T with
the same linear programming formulation used in
TESLA-M.
TESLA-F incorporates all the F-measures used in
TESLA-M, with the addition of (1) the F-measures
generated over the pivot language n-grams described
above, and (2) the normalized language model score,
defined as 1n logP , where n is the length of thetranslation, and P the language model probability.
Unlike BLEU and TESLA-M which rely on simple
averages (geometric and arithmetic average respec-
tively) to combine the component scores, TESLA-
378
F trains the weights over a set of human judg-
ments using a linear ranking support vector machine
(RSVM). This allows TESLA-F to exploit its com-
ponents more effectively, but also makes it more te-
dious to work with and introduces potential domain
mismatch problems.
TESLA-F makes use of even more linguistic in-
formation than TESLA-M, and has the capability
of recognizing some forms of phrase synonyms.
TESLA-F ranked top for the into-English evalua-
tion task in WMT 2010 (Table 1). However, the
added complexity, in particular the use of the lan-
guage model score and the tuning of the component
weights appear to make it less stable than TESLA-M
in practice. For example, it did not perform as well
in the out-of-English task.
3 Experimental setup
We run our experiments in the setting of the WMT
2010 news commentary machine translation cam-
paign, for three language pairs:
1. French-English (fr-en): the training text con-
sists of 84624 sentences of French-English bi-
text. The average French sentence length is 25
words.
2. Spanish-English (es-en): the training text con-
sists of 98598 sentences of Spanish-English bi-
text. The average Spanish sentence length is 25
words.
3. German-English (de-en): the training text con-
sists of 100269 sentences of German-English
bitext. The average German sentence length is
22 words.
The average English sentence length is 21 words
for all three language pairs. The text domain is
newswire report, and the English sides of the train-
ing texts for the three language pairs overlap sub-
stantially. The development data are 2525 four-way
translated sentences, in English, French, Spanish,
and German respectively. Similarly, the test data
are 2489 four-way translated sentences. As a conse-
quence, all MT evaluations involve only single ref-
erences.
We follow the standard approach for training hi-
erarchical phrase-based SMT systems. First, we to-
kenize and lowercase the training texts and create
fr-en es-en de-en
BLEU 3:49 (4) 5:09 (6) 2:41 (4)
TER 4:03 (4) 3:59 (4) 3:59 (5)
TESLA-M 13:00 (3) 17:34 (5) 13:40 (4)
TESLA-F 35:07 (4) 40:54 (4) 40:28 (5)
Table 3: Z-MERT training times in hours:minutes
and number of iterations in parenthesis
word alignments using the Berkeley aligner (Liang
et al, 2006; Haghighi et al, 2009) with five iter-
ations of training. Then, we create suffix arrays
and extract translation grammars for the develop-
ment and test set with Joshua in its default setting.
The maximum phrase length is 10. For the language
model, we use SRILM (Stolcke, 2002) to build a
trigram model with modified Kneser-Ney smooth-
ing from the monolingual training data supplied in
WMT 2010.
Parameter tuning is carried out using Z-
MERT (Zaidan, 2009). TER and BLEU are al-
ready implemented in the publicly released version
of Z-MERT, and Z-MERT?s modular design makes
it easy to integrate TESLA-M and TESLA-F into the
package. The maximum number of MERT iterations
is set to 100, although we observe that in practice,
the algorithm converges after 3 to 6 iterations. The
number of intermediate initial points per iteration is
set to 20 and the n-best list is capped to 300 trans-
lations. Table 3 shows the training times and the
number of MERT iterations for each of the language
pairs and evaluation metrics.
We use the publicly available version of TESLA-
F, which comes with phrase tables and a ranking
SVM model trained on the WMT 2010 development
data.
4 Automatic and manual evaluations
The results of the automatic evaluations are pre-
sented in Table 4. The best score according to each
metric is shown in bold. Note that smaller TER
scores are better, as are larger BLEU, TESLA-M,
and TESLA-F scores.4
We note that Joshua generally responds well to
the change of tuning metric. A system tuned on met-
4The TESLA-F scores shown here have been monotonically
scaled.
379
tune\test BLEU TER TESLA-M TESLA-F
BLEU 0.5237 0.6029 0.3922 0.4114
TER 0.5239 0.6028 0.3880 0.4095
TESLA-M 0.5005 0.6359 0.4170 0.4223
TESLA-F 0.4992 0.6377 0.4164 0.4224
(a) The French-English task
tune\test BLEU TER TESLA-M TESLA-F
BLEU 0.5641 0.5764 0.4315 0.4328
TER 0.5667 0.5725 0.4204 0.4282
TESLA-M 0.5253 0.6246 0.4511 0.4398
TESLA-F 0.5331 0.6111 0.4498 0.4409
(b) The Spanish-English task
tune\test BLEU TER TESLA-M TESLA-F
BLEU 0.4963 0.6329 0.3369 0.3927
TER 0.4963 0.6355 0.3191 0.3851
TESLA-M 0.4557 0.7055 0.3784 0.4070
TESLA-F 0.4642 0.6888 0.3753 0.4068
(c) The German-English task
Table 4: Automatic evaluation scores
P(A) Kappa
French-English 0.6846 0.5269
Spanish-English 0.6124 0.4185
German-English 0.3973 0.0960
Table 5: Inter-annotator agreement
ric M usually does the best or very close to the best
when evaluated by M. On the other hand, the dif-
ferences between different systems can be substan-
tial, especially between BLEU/TER and TESLA-
M/TESLA-F.
In addition to the automatic evaluation, we en-
listed twelve judges to manually evaluate the first
200 test sentences. Four judges are assigned to
each of the three language pairs. For each test sen-
tence, the judges are presented with the source sen-
tence, the reference English translation, and the out-
put from the four competing Joshua systems. The
order of the translation candidates is randomized so
that the judges will not see any patterns. The judges
are instructed to rank the four candidates, and ties
are allowed.
The inter-annotator agreement is reported in Ta-
ble 5. We consider the judgment for a pair of system
outputs as one data point. Let P (A) be the propor-
tion of times that the annotators agree, and P (E)
fr-en es-en de-en
BLEU 44.1% 33.8% 49.6%
TER 41.4% 34.4% 47.8%
TESLA-M 65.8% 49.5% 57.8%
TESLA-F 66.4% 53.8% 55.1%
Table 6: Percentage of times each system produces
the best translation
be the proportion of times that they would agree by
chance. The Kappa coefficient is defined as
Kappa = P(A)? P(E)1? P(E)
In our experiments, each data point has three pos-
sible values: A is preferred, B is preferred, and no
preference, hence P (E) = 1/3. Our Kappa is cal-
culated in the same way as the WMT workshops
(Callison-Burch et al, 2009; Callison-Burch et al,
2010).
Kappa coefficients between 0.4 and 0.6 are con-
sidered moderate, and our values are in line with
those reported in the WMT 2010 translation cam-
paign. The exception is the German-English pair,
where the annotators only reach slight agreement.
This might be caused by the lower quality of Ger-
man to English translations compared to the other
two language pairs.
Table 6 shows the proportion of times each sys-
tem produces the best translation among the four.
We observe that the rankings are largely consis-
tent across different language pairs: Both TESLA-
F and TESLA-M strongly outperform BLEU and
TER. Note that the values in each column do not
add up to 100%, since the candidate translations are
often identical, and even a different translation can
receive the same human judgment.
Table 7 shows our main result, the pairwise com-
parison between the four systems for each of the lan-
guage pairs. Again the rankings consistently show
that both TESLA-F and TESLA-M strongly out-
perform BLEU and TER. All differences are sta-
tistically significant under the Sign Test at p =
0.01, with the exception of TESLA-M vs TESLA-
F in the French-English task, BLEU vs TER in the
Spanish-English task, and TESLA-M vs TESLA-F
and BLEU vs TER in the German-English task. The
results provide strong evidence that tuning machine
380
A\B BLEU TER TESLA-M TESLA-F
BLEU - 11.4% / 6.5% 29.1% / 52.1% 28.0% / 52.3%
TER 6.5% / 11.4% - 28.6% / 54.5% 27.5% / 55.0%
TESLA-M 52.1% / 29.1% 54.5% / 28.6% - 7.6% / 8.8%
TESLA-F 52.3% / 28.0% 55.0% / 27.5% 8.8% / 7.6% -
(a) The French-English task. All differences are significant under the Sign Test at p = 0.01, except the
strikeout TESLA-M vs TESLA-F.
A\B BLEU TER TESLA-M TESLA-F
BLEU - 25.8% / 22.3% 31.0% / 50.6% 24.4% / 50.8%
TER 22.3% / 25.8% - 31.9% / 51.0% 26.4% / 52.4%
TESLA-M 50.6% / 31.0% 51.0% / 31.9% - 25.9% / 33.4%
TESLA-F 50.8% / 24.4% 52.4% / 26.4% 33.4% / 25.9% -
(b) The Spanish-English task. All differences are significant under the Sign Test at p = 0.01, except
the strikeout BLEU vs TER.
A\B BLEU TER TESLA-M TESLA-F
BLEU - 21.8% / 18.4% 28.1% / 36.9% 27.3% / 35.3%
TER 18.4% / 21.8% - 26.9% / 39.5% 27.3% / 37.5%
TESLA-M 36.9% / 28.1% 39.5% / 26.9% - 24.3% / 21.3%
TESLA-F 35.3% / 27.3% 37.5% / 27.3% 21.3% / 24.3% -
(c) The German-English task. All differences are significant under the Sign Test at p = 0.01, except
the strikeout BLEU vs TER, and TESLA-M vs TESLA-F.
Table 7: Pairwise system comparisons. Each cell shows the proportion of time the system tuned on A is
preferred over the system tuned on B, and the proportion of time the opposite happens. Notice that the upper
right half of each table is the mirror image of the lower left half.
381
translation systems using the TESLA metrics leads
to significantly better translation output.
5 Discussion
We examined the results manually, and found that
the relationship between the types of mistakes each
system makes and the characteristics of the corre-
sponding metric to be intricate. We discuss our find-
ings in this section.
First we observe that BLEU and TER tend to pro-
duce very similar translations, and so do TESLA-
F and TESLA-M. Of the 2489 test sentences in the
French-English task, BLEU and TER produced dif-
ferent translations for only 760 sentences, or 31%.
Similarly, TESLA-F and TESLA-M gave different
outputs for only 857 sentences, or 34%. In contrast,
BLEU and TESLA-M gave different translations for
2248 sentences, or 90%. It is interesting to find that
BLEU and TER should be so similar, considering
that they are based on very different principles. As a
metric, TESLA-M is certainly much more similar to
BLEU than TER is, yet they behave very differently
when used as a tuning metric.
We also observe that TESLA-F and TESLA-M
tend to produce much longer sentences than do
BLEU and TER. The average sentence lengths of the
TESLA-F- and TESLA-M-tuned systems across all
three language pairs are 26.5 and 26.6 words respec-
tively, whereas those for BLEU and TER are only
22.4 and 21.7 words. Comparing the translations
from the two groups, the tendency of BLEU and
TER to pick shorter paraphrases and to drop func-
tion words is unmistakable, often to the detriment of
the translation quality. Some typical examples from
the French-English task are shown in Figure 4.
Interestingly, the human translations average only
22 words, so BLEU and TER translations are in fact
much closer on average to the reference lengths, yet
their translations often feel too short. In contrast,
manual inspections reveal no tendency for TESLA-F
and TESLA-M to produce overly long translations.
These observations suggest that the brevity
penalty of BLEU is not aggressive enough. Nei-
ther is TER, which penalizes insertions and dele-
tions equally. Interestingly, by placing much more
emphasis on the recall, TESLA-M and TESLA-F
produce translations that are statistically too long,
but feel much more ?correct? lengthwise.
Another major difference between TESLA-
M/TESLA-F and BLEU/TER is that the TESLAs
heavily discount n-grams with function words. One
might thus expect the TESLA-tuned systems to be
less adept at function words; yet they translate them
surprisingly well, as shown in Figure 4. One ex-
planation is of course the sentence length effect we
have discussed. Another reason may be that since
the metric does not care much about function words,
the language model is given more freedom to pick
function words as it sees fit, without the fear of large
penalties. Paradoxically, by reducing the weights
of function words, we end up making better trans-
lations for them.
TER is the only metric that allows cheap block
movements, regardless of size or distance. One
might reasonably speculate that a TER-tuned system
should be more prone to reordering phrases. How-
ever, we find no evidence that this is so.
The relative performance of TESLA-M vs
TESLA-F is unsurprising. TESLA-F, being heav-
ier and slower, produces somewhat better results
than its minimalist counterpart, though the margin
is far less pronounced than that between TESLA-
M and the conventional BLEU and TER. Since ex-
tra resources including bitexts are needed in using
TESLA-F, TESLA-M emerges as the MT evaluation
metric of choice for tuning SMT systems.
6 Future work
We have presented empirical evidence that the
TESLA metrics outperform BLEU for MT tuning
in a hierarchical phrase-based SMT system. At
the same time, some open questions remain unan-
swered. We intend to investigate them in our future
work.
The work of (Cer et al, 2010) investigated the ef-
fect of tuning a phrase-based SMT system and found
that of the MT evaluation metrics that they tried,
none of them could outperform BLEU. We would
like to verify whether TESLA tuning is still pre-
ferred over BLEU tuning in a phrase-based SMT
system.
Based on our observations, it may be possible to
improve the performance of BLEU-based tuning by
(1) increasing the brevity penalty; (2) introducing
382
BLEU in the future , americans want a phone that allow the user to . . .
TER in the future , americans want a phone that allow the user to . . .
TESLA-M in the future , the americans want a cell phone , which allow the user to . . .
TESLA-F in the future , the americans want a phone that allow the user to . . .
BLEU . . . also for interest on debt of the state . . .
TER . . . also for interest on debt of the state . . .
TESLA-M . . . also for the interest on debt of the state . . .
TESLA-F . . . also for the interest on debt of the state . . .
BLEU and it is hardly the end of carnival-like transfers .
TER and it is hardly the end of carnival-like transfers .
TESLA-M and it is far from being the end of the carnival-like transfers .
TESLA-F and it is far from being the end of the carnival-like transfers .
BLEU it is not certain that the state can act without money .
TER it is not certain that the state can act without money .
TESLA-M it is not certain that the state can act without this money .
TESLA-F it is not certain that the state can act without this money .
BLEU but the expense of a debt of the state . . .
TER but the expense of a debt of the state . . .
TESLA-M but at the expense of a greater debt of the state . . .
TESLA-F but at the expense of a great debt of the state . . .
Figure 4: Comparison of selected translations from the French-English task
a recall measure and emphasizing it over precision;
and/or (3) introducing function word discounting. In
the ideal case, such a modified BLEU metric would
deliver results similar to that of TESLA-M, yet with
a runtime cost closer to BLEU. It would also make
porting existing tuning code easier.
7 Conclusion
We demonstrate for the first time that a practical
new generation MT evaluation metric can signifi-
cantly improve the quality of automatic MT com-
pared to BLEU, as measured by human judgment.
We hope this work will encourage the MT research
community to finally move away from BLEU and to
consider tuning their systems with a new generation
metric.
All the data, source code, and results reported in
this work can be downloaded from our website at
http://nlp.comp.nus.edu.sg/software.
Acknowledgments
This research was done for CSIDM Project No.
CSIDM-200804 partially funded by a grant from
the National Research Foundation (NRF) adminis-
tered by the Media Development Authority (MDA)
of Singapore.
References
Satanjeev Banerjee and Alon Lavie. 2005. METEOR:
An automatic metric for MT evaluation with improved
correlation with human judgments. Proceedings of the
ACL 2005 Workshop on Intrinsic and Extrinsic Evalu-
ation Measures for Machine Translation and/or Sum-
marization.
Chris Callison-Burch, Philipp Koehn, Christof Monz,
and Josh Schroeder. 2009. Findings of the 2009 work-
shop on statistical machine translation. In Proceedings
of the Fourth Workshop on Statistical Machine Trans-
lation.
Chris Callison-Burch, Philipp Koehn, Christof Monz,
Kay Peterson, Mark Przybocki, and Omar F. Zaidan.
2010. Findings of the 2010 joint workshop on statisti-
cal machine translation and metrics for machine trans-
lation. In Proceedings of the Joint Fifth Workshop on
Statistical Machine Translation and MetricsMATR.
Daniel Cer, Christopher D. Manning, and Daniel Juraf-
sky. 2010. The best lexical metric for phrase-based
statistical MT system optimization. In Human Lan-
guage Technologies: The 2010 Annual Conference of
383
the North American Chapter of the Association for
Computational Linguistics.
Yee Seng Chan and Hwee Tou Ng. 2008. MaxSim:
A maximum similarity metric for machine translation
evaluation. In Proceedings of the 46th Annual Meeting
of the Association for Computational Linguistics.
Yee Seng Chan and Hwee Tou Ng. 2009. MaxSim: per-
formance and effects of translation fluency. Machine
Translation, 23(2):157?168, September.
David Chiang. 2005. A hierarchical phrase-based model
for statistical machine translation. In Proceedings of
the 43rd Annual Meeting of the Association for Com-
putational Linguistics.
David Chiang. 2007. Hierarchical phrase-based transla-
tion. Computational Linguistics, 33(2):201?228.
Christiane Fellbaum. 1998. WordNet: An Electronic
Lexical Database. The MIT press.
Aria Haghighi, John Blitzer, John DeNero, and Dan
Klein. 2009. Better word alignments with supervised
ITG models. In Proceedings of 47th Annual Meeting
of the Association for Computational Linguistics and
the 4th IJCNLP of the AFNLP.
John W. Hutchins. 2007. Machine translation: A con-
cise history. Computer Aided Translation: Theory and
Practice.
Alon Lavie and Abhaya Agarwal. 2007. METEOR: An
automatic metric for MT evaluation with high levels of
correlation with human judgments. In Proceedings of
the Second Workshop on Statistical Machine Transla-
tion.
Zhifei Li, Chris Callison-Burch, Chris Dyer, Juri
Ganitkevitch, Sanjeev Khudanpur, Lane Schwartz,
Wren N.G. Thornton, Jonathan Weese, and Omar F.
Zaidan. 2009. Joshua: An open source toolkit for
parsing-based machine translation. In Proceedings of
the Fourth Workshop on Statistical Machine Transla-
tion.
Percy Liang, Ben Taskar, and Dan Klein. 2006. Align-
ment by agreement. In Proceedings of the Human
Language Technology Conference of the North Ameri-
can Chapter of the Association for Computational Lin-
guistics.
Chang Liu, Daniel Dahlmeier, and Hwee Tou Ng. 2010.
Tesla: Translation evaluation of sentences with linear-
programming-based analysis. In Proceedings of the
Joint Fifth Workshop on Statistical Machine Transla-
tion and MetricsMATR.
Franz Josef Och. 2003. Minimum error rate training in
statistical machine translation. In Proceedings of the
41st Annual Meeting of the Association for Computa-
tional Linguistics.
Sebastian Pado?, Daniel Cer, Michel Galley, Dan Jurafsky,
and Christopher D. Manning. 2009. Measuring ma-
chine translation quality as semantic equivalence: A
metric based on entailment features. Machine Trans-
lation, 23(2):181?193, August.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: a method for automatic eval-
uation of machine translation. In Proceedings of the
40th Annual Meeting of the Association for Computa-
tional Linguistics.
Matthew Snover, Bonnie Dorr, Richard Schwartz, Lin-
nea Micciulla, and John Makhoul. 2006. A study of
translation edit rate with targeted human annotation.
In Proceedings of the Seventh Conference of the Asso-
ciation for Machine Translation in the Americas.
Andreas Stolcke. 2002. SRILM - an extensible language
modeling toolkit. In Proceedings of the International
Conference on Spoken Language Processing.
Omar Zaidan. 2009. Z-MERT: A fully configurable open
source tool for minimum error rate training of machine
translation systems. The Prague Bulletin of Mathe-
matical Linguistics, 91:79?88.
Liang Zhou, Chin-Yew Lin, and Eduard Hovy. 2006. Re-
evaluating machine translation results with paraphrase
support. In Proceedings of the 2006 Conference on
Empirical Methods in Natural Language Processing.
384
Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural
Language Learning, pages 568?578, Jeju Island, Korea, 12?14 July 2012. c?2012 Association for Computational Linguistics
A Beam-Search Decoder for Grammatical Error Correction
Daniel Dahlmeier1 and Hwee Tou Ng1,2
1NUS Graduate School for Integrative Sciences and Engineering
2Department of Computer Science, National University of Singapore
{danielhe,nght}@comp.nus.edu.sg
Abstract
We present a novel beam-search decoder for
grammatical error correction. The decoder
iteratively generates new hypothesis correc-
tions from current hypotheses and scores them
based on features of grammatical correctness
and fluency. These features include scores
from discriminative classifiers for specific er-
ror categories, such as articles and preposi-
tions. Unlike all previous approaches, our
method is able to perform correction of whole
sentences with multiple and interacting er-
rors while still taking advantage of powerful
existing classifier approaches. Our decoder
achieves an F1 correction score significantly
higher than all previous published scores on
the Helping Our Own (HOO) shared task data
set.
1 Introduction
Grammatical error correction is an important prob-
lem in natural language processing (NLP) that has
attracted an increasing amount of interest over
the last few years. Grammatical error correction
promises to provide instantaneous accurate feedback
to language learners, e.g., learners of English as a
Second Language (ESL).
The dominant paradigm that underlies most er-
ror correction systems to date is multi-class clas-
sification. A classifier is trained to predict a word
from a confusion set of possible correction choices,
given some feature representation of the surround-
ing sentence context. During testing, the classifier
predicts the most likely correction for each test in-
stance. If the prediction differs from the observed
word used by the writer and the classifier is suffi-
ciently confident in its prediction, the observed word
is replaced by the prediction. Although considerable
progress has been made, the classification approach
suffers from some serious shortcomings. Each clas-
sifier corrects a single word for a specific error cat-
egory individually. This ignores dependencies be-
tween the words in a sentence. Also, by conditioning
on the surrounding context, the classifier implicitly
assumes that the surrounding context is free of gram-
matical errors, which is often not the case. Finally,
the classifier typically has to commit to a single one-
best prediction and is not able to change its deci-
sion later or explore multiple corrections. Instead of
correcting each word individually, we would like to
perform global inference over corrections of whole
sentences which can contain multiple and interact-
ing errors.
An alternative paradigm is to view error correc-
tion as a statistical machine translation (SMT) prob-
lem from ?bad? to ?good? English. While this ap-
proach can naturally correct whole sentences, a stan-
dard SMT system cannot easily incorporate mod-
els for specific grammatical errors. It also suffers
from the paucity of error-annotated training data for
grammar correction. As a result, applying a stan-
dard SMT system to error correction does not pro-
duce good results, as we show in this work.
In this work, we present a novel beam-search de-
coder for grammatical error correction that com-
bines the advantages of the classification approach
and the SMT approach. Starting from the origi-
nal input sentence, the decoder performs an itera-
tive search over possible sentence-level hypotheses
568
to find the best sentence-level correction. In each
iteration, a set of proposers generates new hypothe-
ses by making incremental changes to the hypothe-
ses found so far. A set of experts scores the new
hypotheses on criteria of grammatical correctness.
These experts include discriminative classifiers for
specific error categories, such as articles and prepo-
sitions. The decoder model calculates the overall hy-
pothesis score for each hypothesis as a linear com-
bination of the expert scores. The weights of the de-
coder model are discriminatively trained on a devel-
opment set of error-annotated sentences. The high-
est scoring hypotheses are kept in the search beam
for the next iteration. This search procedure contin-
ues until the beam is empty or the maximum number
of iterations has been reached. The highest scoring
hypothesis is returned as the sentence-level correc-
tion. We evaluate our proposed decoder in the con-
text of the Helping Our Own (HOO) shared task on
grammatical error correction (Dale and Kilgarriff,
2011). Our decoder achieves an F1 score of 25.48%
which improves upon the current state of the art.
The remainder of this paper is organized as fol-
lows. The next section gives an overview of related
work. Section 3 describes the proposed beam-search
decoder. Sections 4 and 5 describe the experimental
setup and results, respectively. Section 6 provides
further discussion. Section 7 concludes the paper.
2 Related Work
In this section, we summarize related work in gram-
matical error correction. For a more detailed review,
the readers can refer to (Leacock et al2010).
The classification approach to error correction has
mainly focused on correcting article and preposition
errors (Knight and Chander, 1994; Han et al2006;
Chodorow et al2007; Tetreault and Chodorow,
2008; Gamon, 2010; Dahlmeier and Ng, 2011b; Ro-
zovskaya and Roth, 2011). The advantage of the
classification approach is that it can make use of
powerful machine learning algorithms in connection
with arbitrary features from the sentence context.
Typical features include surrounding N-grams, part-
of-speech (POS) tags, chunks, etc. In fact, a consid-
erable amount of research effort has been invested in
finding better features.
The SMT approach to error corrections has re-
ceived comparatively less attention. Brockett et
al. (2006) use an SMT system to correct errors in-
volving mass noun errors. Because no large anno-
tated learner corpus was available, the training data
was created artificially from non-learner text. Lee
and Seneff (2006) describe a lattice-based correc-
tion systemwith a domain-specific grammar for spo-
ken utterances from the flight domain. The work in
(De?silets and Hermet, 2009) uses simple round-trip
translation with a standard SMT system to correct
grammatical errors. Dahlmeier and Ng (2011a) cor-
rect collocation errors using phrase-based SMT and
paraphrases induced from the writer?s native lan-
guage. Park and Levy (2011) propose a noisy chan-
nel model for error correction. While their motiva-
tion to correct whole sentences is similar to ours,
their proposed generative method differs substan-
tially from our discriminative decoder. Park and
Levy?s model does not allow the use of discrim-
inative expert classifiers as our decoder does, but
instead relies on a bigram language model to find
grammatical corrections. Indeed, they point out that
the language model often fails to distinguish gram-
matical and ungrammatical sentences.
To the best of our knowledge, our work is the first
discriminatively trained decoder for whole-sentence
grammatical error correction.
3 Decoder
In this section, we describe the proposed beam-
search decoder and its components.
The task of the decoder is to find the best hypoth-
esis (i.e., the best corrected sentence) for a given in-
put sentence. To accomplish this, the decoder needs
to be able to perform two tasks: generating new
hypotheses from current ones, and discriminating
good hypotheses from bad ones. This is achieved
by two groups of modules which we call proposers
and experts, respectively. Proposers take a hypothe-
sis and generate a set of new hypotheses, where each
new hypothesis is the result of making an incremen-
tal change to the current hypothesis. Experts score
hypotheses on particular aspects of grammaticality.
This can be a general language model score, or the
output of classifiers for particular error categories,
for example for article and preposition usage. The
overall score for a hypothesis is a linear combina-
569
tion of the expert scores. Note that in our decoder,
each hypothesis corresponds to a complete sentence.
This makes it easy to apply syntactic processing,
like POS tagging, chunking, and dependency pars-
ing, which provides necessary features for the expert
models. The highest scoring hypotheses are kept in
the search beam for the next iteration. The search
ends when the beam is empty or the maximum num-
ber of iterations has been reached. The highest scor-
ing hypothesis found during the search is returned as
the sentence-level correction. The modular design
of the decoder makes it easy to extend the model
to new error categories by adding specific proposers
and experts without having to change the decoding
algorithm.
3.1 Proposers
The proposers generate new hypotheses, given a hy-
pothesis. Because the number of possible hypothe-
ses grows exponentially with the sentence length,
enumerating all possible hypotheses is infeasible.
Instead, each proposer only makes a small incre-
mental change to the hypothesis in each iteration. A
change corresponds to a correction of a single word
or phrase. We experiment with the following pro-
posers in this work. Additional proposers for other
error categories can easily be added to the decoder.
? Spelling Generate a set of new hypotheses, by
replacing a misspelled word with each correc-
tion proposed by a spellchecker.
? Articles For each noun phrase (NP), generate
two new hypotheses by changing the observed
article. Possible article choices are a/an, the,
and the empty article .
? Prepositions For each prepositional
phrase (PP), generate a set of new hy-
potheses by changing the observed preposition.
For each preposition, we define a confusion set
of possible corrections.
? Punctuation insertion Insert commas, peri-
ods, and hyphens based on a set of simple rules.
? Noun number For each noun, change its num-
ber from singular to plural or vice versa.
3.2 Experts
The experts score hypotheses on particular aspects
of grammaticality to help the decoder to discrim-
inate grammatical hypotheses from ungrammatical
ones. We employ two types of expert models. The
first type of expert model is a standard N-gram lan-
guage model. The language model expert is not spe-
cialized for any particular type of error. The second
type of experts is based on linear classifiers and is
specialized for particular error categories. We use
the following classifier experts in our work. The fea-
tures for the classifier expert models include features
from N-grams, part-of-speech (POS) tags, chunks,
web-scale N-gram counts, and dependency parse
trees. Additional experts can easily be added to the
decoder.
? Article expert Predict the correct article for a
noun phrase.
? Preposition expert Predict the correct preposi-
tion for a prepositional phrase.
? Noun number expert Predict whether a noun
should be in the singular or plural form.
The outputs of the experts are used as hypothesis
features in the decoder, as described in the next sec-
tion.
3.3 Hypothesis Features
Each hypothesis is associated with a vector of real-
valued features which are indicators of grammatical-
ity and are computed from the output of the expert
models. We call these features hypothesis features
to distinguish them from the features of the expert
classifiers. The simplest hypothesis feature is the
log probability of the hypothesis under the N-gram
language model expert. To avoid a bias towards
shorter hypotheses, we normalize the probability by
the length of the hypothesis:
scorelm =
1
|h|
logPr(h), (1)
where h is a hypothesis sentence and |h| is the hy-
pothesis length in tokens.
For the classifier-based experts, we define two
types of features. The first is the average score of
570
the hypothesis under the expert model:
scoreavg =
1
n
n?
i=1
(
uT f(xhi , y
h
i )
)
, (2)
where u is the expert classifier weight vector, xhi and
yhi are the feature vector and the observed class, re-
spectively, for the i-th instance extracted from the
hypothesis h (e.g., the i-th NP in the hypothesis
for the article expert), and f is a feature map that
computes the expert classifier features. The average
score reflects how much the expert model ?likes? the
hypothesis. The second expert score, which we call
delta score, is the maximum difference between the
highest scoring class and the observed class in any
instance from the hypothesis:
scoredelta = max
i,y
(
uT f(xhi , y) ? u
T f(xhi , y
h
i )
)
.
(3)
Generally speaking, the delta score measures how
much the model ?disagrees? with the hypothesis.
Finally, each hypothesis has a number of correc-
tion count features that keep track of how many cor-
rections have been made to the hypothesis so far. For
example, there is a feature that counts how often the
article correction  ? the has been applied. We also
add aggregated correction count features for each
error category, e.g., how many article corrections
have been applied in total. The correction count fea-
tures allow the decoder to learn a bias against over-
correcting sentences and to learn which types of cor-
rections are more likely and which are less likely.
3.4 Decoder Model
The hypothesis features described in the previous
subsection are combined to compute the score of a
hypothesis according to the following linear model:
s = wT fE(h), (4)
where w is the decoder model weight vector and
fE is a feature map that computes the hypothesis
features described above, given a set of experts E.
The weight vector w is tuned on a development set
of error-annotated sentences using the PRO ranking
optimization algorithm (Hopkins and May, 2011).1
1We also experimented with the MERT algorithm (Och,
2003) but found that PRO achieved better results.
PRO performs decoder parameter tuning through a
pair-wise ranking approach. The algorithm starts by
sampling hypothesis pairs from the N-best list of the
decoder. The metric score for each hypothesis in-
duces a ranking of the two hypotheses in each pair.
The task of finding a weight vector that correctly
ranks hypotheses can then be reduced to a simple bi-
nary classification task. In this work, we use PRO to
optimize the F1 correction score, which is defined in
Section 4.2. PRO requires a sentence-level score for
each hypothesis. As F1 score is not decomposable,
we optimize sentence-level F1 score which serves
as an approximation of the corpus-level F1 score.
Similarly, Hopkins and May optimize a sentence-
level BLEU approximation (Lin and Och, 2004) in-
stead of the corpus-level BLEU score (Papineni et
al., 2002). We observed that optimizing sentence-
level F1 score worked well in practice in our experi-
ments.
3.5 Decoder Search
Given a set of proposers, experts, and a tuned de-
coder model, the decoder can be used to correct
new unseen sentences. This is done by performing
a search over possible hypothesis candidates. The
decoder starts with the input sentence as the initial
hypothesis, i.e., assuming that all words are correct.
It then performs a beam search over the space of
possible hypotheses to find the best hypothesis cor-
rection h? for an input sentence e. The search pro-
ceeds in iterations until the beam is empty or the
maximum number of iterations has been reached. In
each iteration, the decoder takes each hypothesis in
the beam and generates new hypothesis candidates
using all the available proposers. The hypotheses
are evaluated by the expert models that compute the
hypothesis features and finally scored using the de-
coder model. As the search space grows exponen-
tially, it is infeasible to perform exhaustive search.
Therefore, we prune the search space by only ac-
cepting the most promising hypotheses to the pool
of hypotheses for future consideration. If a hypothe-
sis has a higher score than the best hypothesis found
in previous iterations, it is definitely added to the
pool. Otherwise, we use a simulated annealing strat-
egy where hypotheses with a lower score can still be
accepted with a certain probability which depends
on the difference between the hypothesis score and
571
hand? hands
In? For
On? About
hands? hand
In? At
In? Into
On? By
In? Of
In? At
In? For
the? an In? On
the? an
In? For
? an
In? At
On? To
In?With
the? 
In?With
? the
In?With
To the otherhand ..score = 6.32
About the otherhand ..score = 9.71
For other hands ..score = 7.00
On an other hand..score = 2.48
On other hand ..score = 4.94
...At other hands ..score = 5.34
At the otherhands ..score = 6.05
For the otherhands ..score = 9.05
With the otherhands ..score = 5.25
In the other hand , they might be rightscore = 11.69
...
...
In other hands , they might be right .score = 9.10
Into the otherhand ..score = 5.47
For the otherhand ..score = 10.75
At the otherhand ..score = 9.40
In an other hand..score = 3.96
In the other hands , they might be right .score = 9.63In an other ..score = -1.58
On the other hand , they might be rightscore = 15.36
Of the otherhand ..score = 8.94
In other hand ..score = 8.29
...
With other hands ..score = 6.31
By the otherhand ..score = 5.80
With the otherhand ..score = 8.69
Figure 1: Example of a search tree produced by the beam-search decoder for the input In other hands, they might be
right. The highest scoring hypothesis found is On the other hand, they might be right. Some hypotheses are omitted
due to space constraints.
the score of the best hypothesis and the ?tempera-
ture? of the system. We lower the temperature after
each iteration according to an exponential cooling
schedule. Hypotheses that have been explored be-
fore are not considered again to avoid cycles in the
search. From all hypotheses in the pool, we select
the top k hypotheses and add them to the beam for
the next search iteration. The decoding algorithm
is shown in Algorithm 1. The decoder can be con-
sidered an anytime algorithm (Russell and Norvig,
2010), as it has a current best hypothesis correction
available at any point of the search, while gradually
improving the result by searching for better hypothe-
ses. An example of a search tree produced by our
decoder is shown in Figure 1.
The decoding algorithm shares some similarities
with the beam-search algorithm frequently used in
SMT. There is however a difference between SMT
decoding and grammar correction decoding that is
worth pointing out. In SMT decoding, every input
word needs to be translated exactly once. In con-
trast, in grammar correction decoding, the majority
of the words typically do not need any correction
(in the HOO data, for example, there are on aver-
age 6 errors per 100 words). On the other hand,
some words might require multiple corrections, for
example spelling correction followed by noun num-
ber correction. Errors can also be inter-dependent,
where correcting one word makes it necessary to
change another word, for example to preserve agree-
ment. Our decoding algorithm has the option to cor-
rect some words multiple times, while leaving other
words unchanged.
4 Experiments
We evaluate our decoder in the context of the HOO
shared task on grammatical error correction. The
goal of the task is to automatically correct errors in
academic papers from NLP. The readers can refer to
the overview paper (Dale and Kilgarriff, 2011) for
details. We compare our proposed method with two
baselines: a phrase-based SMT system (described in
Section 4.3) and a pipeline of classifiers (described
in Section 4.4).
4.1 Data
We split the HOO development data into an equal
sized training (HOO-TRAIN) and tuning (HOO-
TUNE) set. The official HOO test data (HOO-TEST)
is used for evaluation. In the HOO shared task, par-
ticipants were allowed to raise objections regarding
the gold-standard annotations (corrections) of the
test data after the test data was released. As a result,
the gold-standard annotations could be biased in fa-
572
Algorithm 1 The beam-search decoding algorithm. e:
original sentence, w: decoder weight vector, P : set of
proposers, E: set of experts, k: beam width, M : maxi-
mum number of iterations, T, c: initial temperature and
cooling schedule for simulated annealing (0 < c < 1).
procedure decode(e, w, P , E, k, M )
1: beam? {e}
2: previous? {e}
3: hbest ? e
4: sbest ? wT fE(hbest)
5: i? 0
6: while beam 6= ? ? i < M do
7: pool? {}
8: for all h ? beam do
9: for all p ? P do
10: for all h? ? p.propose(h) do
11: if h? ? previous then
12: continue
13: previous? previous ? {h?}
14: sh? ? w
T fE(h?)
15: if accept(sh? , sbest, T ) then
16: pool? pool ? {(h?, sh?)}
17: beam? ?
18: for all (h, sh) ? nbest(pool, k) do
19: beam? beam ? {h}
20: if sh > sbest then
21: hbest ? h
22: sbest ? sh
23: T ? T ? c
24: i? i + 1
25: return hbest
procedure accept(sh, sbest, T )
1: ? ? sh ? sbest
2: if ? > 0 then
3: return true
4: if exp( ?T ) > random() then
5: return true else return false
vor of specific systems participating in the shared
task. We obtain both the original and the final offi-
cial gold-standard annotations and report evaluation
results on both annotations.
We use the ACL Anthology2 as training data for
the expert models. We crawl all non-OCR docu-
ments from the anthology, except those documents
that overlap with the HOO data. Section headers,
references, etc. are automatically removed. The
Web 1T 5-gram corpus (Brants and Franz, 2006) is
used for language modeling and collecting web N-
gram counts. Table 1 gives an overview of the data
sets.
2http://www.aclweb.org/anthology-new/
Data Set Sentences Tokens
HOO-TRAIN 467 11,373
HOO-TUNE 472 11,435
HOO-TEST 722 18,790
ACL-ANTHOLOGY 943,965 22,465,690
Table 1: Overview of the data sets.
4.2 Evaluation
We evaluate performance by computing precision,
recall, and F1 correction score without bonus as de-
fined in the official HOO report (Dale and Kilgar-
riff, 2011)3. F1 correction score is simply the F1
measure (van Rijsbergen, 1979) between the correc-
tions (called edits in HOO) proposed by a system
and the gold-standard corrections. Let {e1, . . . , en}
be a set of test sentences and let {g1, . . . ,gn} be
the set of gold-standard edits for the sentences. Let
{h1, . . . ,hn} be the set of corrected sentences out-
put by a system. One difficulty in the evaluation is
that the set of system edits {d1, . . . ,dn} between
the test sentences and the system outputs is ambigu-
ous. For example, assume that the original test sen-
tence is The data is similar with test set., the system
output is The data is similar to the test set., and the
gold-standard edits are two corrections with ? to,
 ? the that change with to to and insert the be-
fore test set. The official HOO scorer however ex-
tracts a single system edit with ? to the for this
instance. As the extracted system edit is different
from the gold-standard edits, the system would be
considered wrong, although it proposes the exact
same corrected sentence as the gold standard ed-
its. This problem has also been recognized by the
HOO shared task organizers (see (Dale and Kilgar-
riff, 2011), Section 5).
Our MaxMatch (M2) scorer (Dahlmeier and Ng,
2012) overcomes this problem through an efficient
algorithm that computes the set of system edits
which has the maximum overlap with the gold-
standard edits. We use the M2 scorer as the main
evaluation metric in our experiments. Additionally,
we also report results with the official HOO scorer.
Once the set of system edits is extracted, precision,
recall, and F1 measure are computed as follows.
3?Without bonus? means that a system does not receive extra
credit for not making corrections that are considered optional in
the gold standard.
573
P =
?n
i=1 |di ? gi|?n
i=1 |di|
(5)
R =
?n
i=1 |di ? gi|?n
i=1 |gi|
(6)
F1 = 2 ?
P ?R
P + R
(7)
We note that the M2 scorer and the HOO scorer ad-
here to the same score definition and only differ in
the way the system edits are computed. For statisti-
cal significance testing, we use sign-test with boot-
strap re-sampling (Koehn, 2004) with 1,000 sam-
ples.
4.3 SMT Baseline
We build a baseline error correction system, using
the MOSES SMT system (Koehn et al2007). Word
alignments are created automatically on ?good-bad?
parallel text from HOO-TRAIN using GIZA++ (Och
and Ney, 2003), followed by phrase extraction us-
ing the standard heuristic (Koehn et al2003). The
maximum phrase length is 5. Parameter tuning is
done on the HOO-TUNE data with the PRO al-
gorithm (Hopkins and May, 2011) implemented in
MOSES. The optimization objective is sentence-
level BLEU (Lin and Och, 2004). We note that the
objective function is not the same as the final evalu-
ation F1 score. Also, the training and tuning data are
small by SMT standards. The aim for the SMT base-
line is not to achieve a state-of-the-art system, but to
serve as the simplest possible baseline that uses only
off-the-shelf software.
4.4 Pipeline Baseline
The second baseline system is a pipeline of
classifier-based and rule-based correction steps.
Each step takes sentence segmented plain text as in-
put, corrects one particular error category, and feeds
the corrected text into the next step. No search or
global inference is applied. The correction steps are:
1. Spelling errors
2. Article errors
3. Preposition errors
4. Punctuation errors
5. Noun number errors
We use the following tools for syntactic process-
ing: OpenNLP4 for POS tagging, YamCha (Kudo
andMatsumoto, 2003) for constituent chunking, and
the MALT parser (Nivre et al2007) for depen-
dency parsing. For language modeling, we use Ran-
dLM (Talbot and Osborne, 2007).
For spelling correction, we use GNU Aspell5.
Words that contain upper-case characters inside the
word or are shorter than four characters are excluded
from spell checking. The spelling dictionary is aug-
mented with all words that appear at least 10 times
in the ACL-ANTHOLOGY data set.
Article correction is cast as a multi-class clas-
sification problem. As the learning algorithm,
we choose multi-class confidence-weighted (CW)
learning (Crammer et al2009) which has been
shown to perform well for NLP problems with high
dimensional and sparse feature spaces. The possi-
ble classes are the articles a, the, and the empty ar-
ticle . The article an is normalized as a and re-
stored later using a rule-based heuristic. We con-
sider all NPs that are not pronouns and do not have a
non-article determiner, e.g., this, that. The classifier
is trained on over 5 million instances from ACL-
ANTHOLOGY. We use a combination of features
proposed by (Rozovskaya et al2011) (which in-
clude lexical and POS N-grams, lexical head words,
etc.), web-scale N-gram count features from the
Web 1T 5-gram corpus following (Bergsma et al
2009), and dependency head and child features.
During testing, a correction is proposed if the pre-
dicted article is different from the observed article
used by the writer, and the difference between the
confidence score for the predicted article and the
confidence score for the observed article is larger
than a threshold. Threshold parameters are tuned
via a grid-search on the HOO-TUNE data. We tune
a separate threshold value for each class.
Preposition correction and noun number correc-
tion are analogous to article correction. They differ
only in terms of the classes and the features. For
preposition correction, the classes are 36 frequent
English prepositions6. The features are surrounding
4http://opennlp.sourceforge.net
5http://aspell.net
6about, along, among, around, as, at, beside, besides, be-
tween, by, down, during, except, for, from, in, inside, into, of,
off, on, onto, outside, over, through, to, toward, towards, under,
574
lexical N-grams, web-scale N-gram counts, and de-
pendency features following (Tetreault et al2010).
The preposition classifier is trained on 1 million
training examples from the ACL-ANTHOLOGY. For
noun number correction, the classes are singular
and plural. The features are lexical N-grams, web-
scale N-gram counts, dependency features, the noun
lemma, and a binary countability feature. The noun
number classifier is trained on over 5 million exam-
ples from ACL-ANTHOLOGY. During testing, the
singular or plural word surface form is generated us-
ing WordNet (Fellbaum, 1998) and simple heuris-
tics. Punctuation correction is done using a set of
simple rules developed on the HOO development
data.
At the end of every correction step, all proposed
corrections are filtered using a 5-gram language
model from the Web 1T 5-gram corpus and only
corrections that strictly increase the normalized lan-
guage model score of the sentence are applied.
4.5 Decoder
We experiment with different decoder configura-
tions with different proposers and expert models.
In the simplest configuration, the decoder only has
the spelling proposer and the language model ex-
pert. We then add the article proposer and expert,
the preposition proposer and expert, the punctua-
tion proposer, and finally the noun number proposer
and expert. We refer to the final configuration with
all proposers and experts as the full decoder model.
Note that error categories are corrected jointly and
not in sequential steps as in the pipeline.
To make the results directly comparable to the
pipeline, the decoder uses the same resources as the
pipeline. As the expert models, we use a 5-gram
language model from the Web 1T 5-gram corpus
with the Berkeley LM (Pauls and Klein, 2011)7 in
the decoder and the CW-classifiers described in the
last section. The spelling proposer uses the same
spellchecker as the pipeline, and the punctuation
proposer uses the same rules as the pipeline. The
beam width and the maximum number of iterations
are set to 10. In earlier experiments, we found that
larger values had no effect on the result. The simu-
underneath, until, up, upon, with, within, without
7Berkeley LM is written in Java and was easier to integrate
into our Java-based decoder than RandLM.
lated annealing temperature T is initialized to 10 and
the exponential cooling schedule c is set to 0.9. The
decoder weight vector is initialized as follows. The
weight for the language model score and the weights
for the classifier expert average scores are initialized
to 1.0, and the weights for the classifier expert delta
scores are initialized to ?1.0. The weights for the
correction count features are initialized to zero. For
PRO optimization, we use the HOO-TUNE data and
the default PRO parameters from (Hopkins andMay,
2011): we sample 5,000 hypothesis pairs from the
N-best list (N = 100) for every input sentence and
keep the top 50 sample pairs with the highest dif-
ference in F1 measure. The weights are optimized
using MegaM (Daume? III, 2004) and interpolated
with the previous weight vector with an interpola-
tion parameter of 0.1. We normalize feature val-
ues to avoid having features on a larger scale dom-
inate features on a smaller scale. We linearly scale
all hypothesis features to a unit interval [0, 1]. The
minimum and maximum values for each feature are
estimated from the development data. We use an
early stopping criterion that terminates PRO if the
objective function on the tuning data drops. To bal-
ance the skewed data where samples without errors
greatly outnumber samples with errors, we give a
higher weight to sample pairs where the decoder
proposed a valid correction. We found a weight of
20 to work well, based on initial experiments on
the HOO-TUNE data. We keep all these parameters
fixed for all experiments.
5 Results
The complete results of our experiments are shown
in Table 2. Each row contains the results for one
error correction system. Each system is scored on
the original and official gold-standard annotations,
both with the M2 scorer and the official HOO scorer.
This results in four sets of precision, recall, and F1
scores for each system. The best published result
to date on this data set is the UI Run1 system from
the HOO shared task. We include their system as a
reference point.
We make the following observations. First, the
scores on the official gold-standard annotations are
higher compared to the original gold-standard an-
notations. We note that the gap between the two
575
System Original gold-standard Official gold-standard
M2 scorer HOO scorer M2 scorer HOO scorer
P R F1 P R F1 P R F1 P R F1
UI Run1 40.86 11.21 17.59 38.13 10.42 16.37 54.61 14.57 23.00 50.72 13.34 21.12
P R F1 P R F1 P R F1 P R F1
SMT 9.84 7.77 8.68 15.25 5.31 7.87 23.35 7.38 11.21 15.82 5.30 7.93
Pipeline P R F1 P R F1 P R F1 P R F1
Spelling 50.00 0.79 1.55 40.00 0.64 1.25 50.00 0.76 1.49 40.00 0.61 1.20
+ Articles 30.86 10.23 15.36 28.04 9.55 14.25 34.42 10.97 16.64 31.78 10.41 15.68
+ Prepositions 27.44 11.90 16.60 24.82 11.15 15.38 30.54 12.77 18.01 27.90 12.04 16.82
+ Punctuation 28.91 14.55 19.36 ? 26.57 13.91 18.25 ? 32.88 15.99 21.51 30.63 15.41 20.50
+ Noun number 28.77 16.13 20.67 ? 24.68 14.22 18.04 ? 32.34 17.50 22.71 28.36 15.71 20.22
Decoder P R F1 P R F1 P R F1 P R F1
Spelling 36.84 0.69 1.35 22.22 0.41 0.80 36.84 0.66 1.30 22.22 0.42 0.83
+ Articles 19.84 12.59 15.40 17.99 12.00 14.39 22.45 13.72 17.03 ? 20.70 13.27 16.16
+ Prepositions 22.62 14.26 17.49 ? 19.30 12.95 15.50 24.84 15.14 18.81 ? 21.36 13.78 16.74
+ Punctuation 24.27 18.09 20.73 ?? 20.40 16.24 18.08 27.13 19.58 22.75 ? 23.07 17.65 19.99
+ Noun number 30.28 19.17 23.48 ?? 24.29 16.24 19.46 ?? 33.59 20.53 25.48 ?? 27.30 17.55 21.36 ?
Table 2: Experimental results on HOO-TEST. Precision, recall, and F1 score are shown in percent. The best F1 score
for each system is highlighted in bold. Statistically significant improvements (p < 0.01) over the pipeline baseline are
marked with an asterisk (?). Statistically significant improvements over the UI Run1 system are marked with a dagger
(?). All improvements of the pipeline and the decoder over the SMT baseline are statistically significant.
annotations is the largest for the UI Run1 system
which confirms the suspected bias of the official
gold-standard annotations in favor of participating
systems. Second, the scores computed with the M2
scorer are higher than the scores computed with the
official HOO scorer. With more error categories and
more ambiguity in the edits segmentation, the gap
between the scorers widens. In the case of the full
pipeline and decoder model, the HOO scorer even
shows a decrease in F1 score when the score actu-
ally goes up as shown by the M2 scorer. We there-
fore focus on the scores of the M2 scorer from now
on. The SMT baseline achieves 8.68% and 11.21%
F1 on the original and official gold standard, respec-
tively. Although the worst system in our experi-
ments, it would still have claimed the third place
in the HOO shared task. One problem is certainly
the small amount of training data. Another reason is
that the phrase-based model is unaware of syntactic
structure and cannot express correction rules of the
form NP ? the NP . Instead, it has to have seen
the exact correction rule, e.g., house ? the house,
in the training data. As a result, the model does not
generalize well. The pipeline achieves state-of-the-
art results. Each additional correction step improves
the score. Our proposed decoder achieves the best
result. When only a few error categories are cor-
rected, the pipeline and the decoder are close to each
other. When more error categories are added, the
gap between the pipeline and the decoder becomes
larger. The full decoder model achieves an F1 score
of 23.48% and 25.48% on the original and official
gold standard, respectively, which is statistically sig-
nificantly better than both the pipeline system and
the UI Run1 system.
6 Discussion
As pointed out in Section 3.5, the majority of sen-
tences require zero or few corrections. Therefore,
the depth of the search tree is typically small. In our
experiments, the average depth of the search tree is
only 1.9 (i.e., 0.9 corrections per sentence) on the
test set. Usually, the search depth will be one larger
than the number of corrections made, since the de-
coder will explore the next level of the search tree
before deciding that none of the new hypotheses are
better than the current best one. On the other hand,
there are many possible hypotheses that can be pro-
posed for any sentence. The breadth of the search
tree is therefore quite large. In our experiments, the
decoder explored on average 99 hypotheses per sen-
tence on the test set.
576
PRO iteration P R F1
1 14.13 20.17 16.62
2 19.71 20.85 20.27
3 23.12 21.03 22.02
4 24.35 20.85 22.47
5 25.53 20.51 22.75
6 26.27 20.34 22.93
7 27.25 20.68 23.52
8 26.73 19.83 22.77
Table 3: PRO tuning of the full decoder model on HOO-
TUNE
Feature Weight
a? the -1.3660
a?  0.5253
the? a -0.9997
the?  0.0532
? a 0.0694
? the -0.0529
Table 4: Example of PRO-tuned weights for article cor-
rection count features for the full decoder model.
We found that PRO tuning is very important to
achieve good performance for our decoder. Most
importantly, PRO tunes the correction count features
that bias the decoder against over-correcting sen-
tences thus improving precision. But PRO is also
able to improve recall during tuning. Table 3 shows
the trajectory of the performance for the full decoder
model during PRO tuning on HOO-TUNE. After
PRO tuning has converged, we inspect the learned
weight vector and observe some interpretable pat-
terns learned by PRO. First, the language model
score and all classifier expert average scores receive
positive weights, while all classifier expert delta
scores receive negative weights, in line with our ini-
tial intuition described in Section 3.3. Second, most
correction count features receive negative weights,
thus acting as a bias against correction if it is not
necessary. Finally, the correction count features re-
veal which corrections are more likely and which are
less likely. For example, article replacement errors
are less common in the HOO-TUNE data than arti-
cle insertions or deletions. The weights learned for
the article correction count features shown in Table 4
reflect this.
Although our decoder achieves state-of-the-art re-
sults, there remain many error categories which the
decoder currently cannot correct. This includes, for
example, verb form errors (Much research (have ?
has) been put into . . . ) and lexical choice errors (The
(concerned ? relevant) relation . . . ). We believe
that our decoder provides a promising framework to
build grammatical error correction systems that in-
clude these types of errors in the future.
7 Conclusion
We have presented a novel beam-search decoder for
grammatical error correction. The model performs
end-to-end correction of whole sentences with mul-
tiple, interacting errors, is discriminatively trained,
and incorporates existing classifier-based models for
error correction. Our decoder achieves an F1 correc-
tion score of 25.48% on the HOO shared task which
outperforms the current state of the art on this data
set.
Acknowledgments
This research is supported by the Singapore Na-
tional Research Foundation under its International
Research Centre @ Singapore Funding Initiative
and administered by the IDM Programme Office.
References
S. Bergsma, D. Lin, and R. Goebel. 2009. Web-scale N-
gram models for lexical disambiguation. In Proceed-
ings of IJCAI.
T. Brants and A. Franz. 2006. Web 1T 5-gram corpus
version 1.1. Technical report, Google Research.
C. Brockett, W. B. Dolan, and M. Gamon. 2006. Cor-
recting ESL errors using phrasal SMT techniques. In
Proceedings of COLING-ACL.
M. Chodorow, J. Tetreault, and N.R. Han. 2007. De-
tection of grammatical errors involving prepositions.
In Proceedings of the 4th ACL-SIGSEM Workshop on
Prepositions.
K. Crammer, M. Dredze, and A. Kulesza. 2009. Multi-
class confidence weighted algorithms. In Proceedings
of EMNLP.
D. Dahlmeier and H.T. Ng. 2011a. Correcting seman-
tic collocation errors with L1-induced paraphrases. In
Proceedings of EMNLP.
D. Dahlmeier and H.T. Ng. 2011b. Grammatical error
correction with alternating structure optimization. In
Proceedings of ACL.
D. Dahlmeier and H.T. Ng. 2012. Better evaluation for
grammatical error correction. In Proceedings of HLT-
NAACL.
577
R. Dale and A. Kilgarriff. 2011. Helping Our Own: The
HOO 2011 pilot shared task. In Proceedings of the
2011 European Workshop on Natural Language Gen-
eration.
H. Daume? III. 2004. Notes on CG and LM-BFGS
optimization of logistic regression. Paper available at
http://pub.hal3.name#daume04cg-bfgs,
implementation available at http://hal3.name/
megam/.
A. De?silets and M. Hermet. 2009. Using automatic
roundtrip translation to repair general errors in second
language writing. In Proceedings of MT-Summit XII.
C. Fellbaum, editor. 1998. WordNet: An electronic lexi-
cal database. MIT Press, Cambridge,MA.
M. Gamon. 2010. Using mostly native data to correct
errors in learners? writing: A meta-classifier approach.
In Proceedings of HLT-NAACL.
N.R. Han, M. Chodorow, and C. Leacock. 2006. De-
tecting errors in English article usage by non-native
speakers. Natural Language Engineering, 12(02).
M. Hopkins and J. May. 2011. Tuning as ranking. In
Proceedings of EMNLP.
K. Knight and I. Chander. 1994. Automated postediting
of documents. In Proceedings of AAAI.
P. Koehn, F.J. Och, and D. Marcu. 2003. Statisti-
cal phrase-based translation. In Proceedings of HLT-
NAACL.
P. Koehn, H. Hoang, A. Birch, C. Callison-Burch,
M. Federico, N. Bertoldi, B. Cowan, W. Shen,
C. Moran, R. Zens, C. Dyer, O. Bojar, A. Constantin,
and E. Herbst. 2007. Moses: Open source toolkit for
statistical machine translation. In Proceedings of ACL
Demonstration Session.
P. Koehn. 2004. Statistical significance tests for machine
translation evaluation. In Proceedings of EMNLP.
T Kudo and Y. Matsumoto. 2003. Fast methods for
kernel-based text analysis. In Proceedings of ACL.
C. Leacock, M. Chodorow, M. Gamon, and J. Tetreault.
2010. Automated Grammatical Error Detection for
Language Learners. Morgan & Claypool Publishers.
J. Lee and S. Seneff. 2006. Automatic grammar correc-
tion for second-language learners. In Proceedings of
Interspeech.
C.-Y. Lin and F.J. Och. 2004. ORANGE: a method for
evaluating automatic evaluation metrics for machine
translation. In Proceedings of COLING.
J. Nivre, J. Hall, J. Nilsson, A. Chanev, G. Eryigit,
S. Ku?bler, S. Marinov, and M. Marsi. 2007. Malt-
Parser: A language-independent system for data-
driven dependency parsing. Natural Language Engi-
neering, 13.
F.J. Och and H. Ney. 2003. A systematic comparison of
various statistical alignment models. Computational
Linguistics, 29(1).
F.J. Och. 2003. Minimum error rate training in statistical
machine translation. In Proceedings of ACL.
K. Papineni, S. Roukos, T. Ward, and W.-J. Zhu. 2002.
BLEU: A method for automatic evaluation of machine
translation. In Proceedings of ACL.
Y. A. Park and R. Levy. 2011. Automated whole
sentence grammar correction using a noisy channel
model. In Proceedings of ACL.
A. Pauls and D. Klein. 2011. Faster and smaller N-gram
language models. In Proceedings of ACL-HLT.
A. Rozovskaya and D. Roth. 2011. Algorithm selec-
tion and model adaptation for ESL correction tasks. In
Proceedings of ACL-HLT.
A. Rozovskaya, M. Sammons, J. Gioja, and D. Roth.
2011. University of Illinois system in HOO text cor-
rection shared task. In Proceedings of the Generation
Challenges Session at the 13th European Workshop on
Natural Language Generation.
S. Russell and P. Norvig, 2010. Artificial Intelligence: A
Modern Approach, chapter 27. Prentice Hall.
D. Talbot and M. Osborne. 2007. Randomised language
modelling for statistical machine translation. In Pro-
ceedings of ACL.
J. Tetreault and M. Chodorow. 2008. The ups and downs
of preposition error detection in ESL writing. In Pro-
ceedings of COLING.
J. Tetreault, J. Foster, and M. Chodorow. 2010. Using
parse features for preposition selection and error de-
tection. In Proceedings of ACL.
C. J. van Rijsbergen. 1979. Information Retrieval. But-
terworth, 2nd edition.
578
2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 568?572,
Montre?al, Canada, June 3-8, 2012. c?2012 Association for Computational Linguistics
Better Evaluation for Grammatical Error Correction
Daniel Dahlmeier1 and Hwee Tou Ng1,2
1NUS Graduate School for Integrative Sciences and Engineering
2Department of Computer Science, National University of Singapore
{danielhe,nght}@comp.nus.edu.sg
Abstract
We present a novel method for evaluating
grammatical error correction. The core of
our method, which we call MaxMatch (M2),
is an algorithm for efficiently computing the
sequence of phrase-level edits between a
source sentence and a system hypothesis that
achieves the highest overlap with the gold-
standard annotation. This optimal edit se-
quence is subsequently scored using F1 mea-
sure. We test our M2 scorer on the Helping
Our Own (HOO) shared task data and show
that our method results in more accurate eval-
uation for grammatical error correction.
1 Introduction
Progress in natural language processing (NLP) re-
search is driven and measured by automatic eval-
uation methods. Automatic evaluation allows fast
and inexpensive feedback during development, and
objective and reproducible evaluation during testing
time. Grammatical error correction is an important
NLP task with useful applications for second lan-
guage learning. Evaluation for error correction is
typically done by computing F1 measure between
a set of proposed system edits and a set of human-
annotated gold-standard edits (Leacock et al, 2010).
Unfortunately, evaluation is complicated by the
fact that the set of edit operations for a given system
hypothesis is ambiguous. This is due to two reasons.
First, the set of edits that transforms one string into
another is not necessarily unique, even at the token
level. Second, edits can consist of longer phrases
which introduce additional ambiguity. To see how
this can affect evaluation, consider the following
source sentence and system hypothesis from the re-
cent Helping Our Own (HOO) shared task (Dale and
Kilgarriff, 2011) on grammatical error correction:
Source : Our baseline system feeds word
into PB-SMT pipeline.
Hypot. : Our baseline system feeds a word
into PB-SMT pipeline.
The HOO evaluation script extracts the system edit
( ? a), i.e., inserting the article a. Unfortunately,
the gold-standard annotation instead contains the ed-
its (word ? {a word, words}). Although the ex-
tracted system edit results in the same corrected sen-
tence as the first gold-standard edit option, the sys-
tem hypothesis was considered to be invalid.
In this work, we propose a method, called Max-
Match (M2), to overcome this problem. The key idea
is that if there are multiple possible ways to arrive
at the same correction, the system should be eval-
uated according to the set of edits that matches the
gold-standard as often as possible. To this end, we
propose an algorithm for efficiently computing the
set of phrase-level edits with the maximum overlap
with the gold standard. The edits are subsequently
scored using F1 measure. We test our method in the
context of the HOO shared task and show that our
method results in a more accurate evaluation for er-
ror correction.
The remainder of this paper is organized as fol-
lows: Section 2 describes the proposed method; Sec-
tion 3 presents experimental results; Section 4 dis-
cusses some details of grammar correction evalua-
tion; and Section 5 concludes the paper.
568
2 Method
We begin by establishing some notation. We con-
sider a set of source sentences S = {s1, . . . , sn} to-
gether with a set of hypotheses H = {h1, . . . ,hn}
generated by an error correction system. Let G =
{g1, . . . ,gn} be the set of gold standard annota-
tions for the same sentences. Each annotation gi =
{g1i , . . . , g
r
i } is a set of edits. An edit is a triple
(a, b, C), consisting of:
? start and end (token-) offsets a and b with re-
spect to a source sentence,
? a correction C. For gold-standard edits, C is a
set containing one or more possible corrections.
For system edits, C is a single correction.
Evaluation of the system output involves the follow-
ing two steps:
1. Extracting a set of system edits ei for each
source-hypothesis pair (si,hi).
2. Evaluating the system edits for the complete
test set with respect to the gold standard G.
The remainder of this section describes a method for
solving these two steps. We start by describing how
to construct an edit lattice from a source-hypothesis
pair. Then, we show that finding the optimal se-
quence of edits is equivalent to solving a shortest
path search through the lattice. Finally, we describe
how to evaluate the edits using F1 measure.
2.1 Edit lattice
We start from the well-established Levenshtein dis-
tance (Levenshtein, 1966), which is defined as the
minimum number of insertions, deletions, and sub-
stitutions needed to transform one string into an-
other. The Levenshtein distance between a source
sentence si = s1i , . . . , s
k
i and a hypothesis hi =
h1i , . . . , h
l
i can be efficiently computed using a two
dimensional matrix that is filled using a classic dy-
namic programming algorithm. We assume that
both si and hi have been tokenized. The matrix for
the example from Section 1 is shown in Figure 1. By
performing a simple breadth-first search, similar to
the Viterbi algorithm, we can extract the lattice of
all shortest paths that lead from the top-left corner
to the bottom-right corner of the Levenshtein ma-
trix. Each vertex in the lattice corresponds to a cell
Our baseline system feeds a word into PB-SMT pipeline .
0 1 2 3 4 5 6 7 8 9 10Our 1 0 1 2 3 4 5 6 7 8 9baseline 2 1 0 1 2 3 4 5 6 7 8system 3 2 1 0 1 2 3 4 5 6 7feeds 4 3 2 1 0 1 2 3 4 5 6word 5 4 3 2 1 1 1 2 3 4 5into 6 5 4 3 2 2 2 1 2 3 4PB-SMT 7 6 5 4 3 3 3 2 1 2 3pipeline 8 7 6 5 4 4 4 3 2 1 2. 9 8 7 6 5 5 5 4 3 2 1
Figure 1: The Levenshtein matrix and the shortest path
for a source sentence ?Our baseline system feeds word
into PB-SMT pipeline .? and a hypothesis ?Our baseline
system feeds a word into PB-SMT pipeline .?
in the Levenshtein matrix, and each edge in the lat-
tice corresponds to an atomic edit operation: insert-
ing a token, deleting a token, substituting a token,
or leaving a token unchanged. Each path through
the lattice corresponds to a shortest sequence of ed-
its that transform si into hi. We assign a unit cost to
each edge in the lattice.
We have seen that annotators can use longer
phrases and that phrases can include un-
changed words from the context, e.g., the
gold edit from the example in Section 1 is
(4, 5,word, {a word, words}). However, it seems
unrealistic to allow an arbitrary number of un-
changed words in an edit. In particular, we want to
avoid very large edits that cover complete sentences.
Therefore, we limit the number of unchanged words
by a parameter u. To allow for phrase-level edits,
we add transitive edges to the lattice as long as the
number of unchanged words in the newly added edit
is not greater than u and the edit changes at least one
word. Let e1 = (a1, b1, C1) and e2 = (a2, b2, C2)
be two edits corresponding to adjacent edges in the
lattice, with the first end offset b1 being equal to the
second start offset a2. We can combine them into a
new edit e3 = (a1, b2, C1 + C2), where C1 + C2 is
the concatenation of strings C1 and C2. The cost of
a transitive edge is the sum of the costs of its parts.
The lattice extracted from the example sentence is
shown in Figure 2.
2.2 Finding maximally matching edit sequence
Our goal is to find the sequence of edits ei with
the maximum overlap with the gold standard. Let
L = (V,E) be the edit lattice graph from the last
section. We change the cost of each edge whose cor-
569
0,0 1,1Our (1) 2,2baseline (1)
3,3system (1) 4,5feeds/feeds a (2)
4,4feeds (1) 5,6word (1)?/a (1)
word/a word (-45) 6,7into (1) 7,8PB-SMT (1) 8,9pipeline (1) 9,10. (1)
system feeds/system feeds a (3) feeds word/feeds a word (3)
word into/a word into (3)
Figure 2: The edit lattice for ?Our baseline system feeds (? a) word into PB-SMT pipeline .? Edge costs are shown
in parentheses. The edge from (4,4) to (5,6) matches the gold annotation and carries a negative cost.
responding edit has a match in the gold standard to
?(u + 1) ? |E|. An edit e matches a gold edit g
iff they have the same offsets and e?s correction is
included in g:
match(e, g)? e.a = g.a ? e.b = g.b ? e.C ? g.C
(1)
Then, we perform a single-source shortest path
search with negative edge weights from the start to
the end vertex1. This can be done efficiently, for ex-
ample with the Bellman-Ford algorithm (Cormen et
al., 2001). As the lattice is acyclic, the algorithm is
guaranteed to terminate and return a shortest path.
Theorem 1. The set of edits corresponding to the
shortest path has the maximum overlap with the gold
standard annotation.
Proof. Let e = e1, . . . , ek be the edit sequence cor-
responding to the shortest path and let p be the num-
ber of matched edits. Assume that there exists an-
other edit sequence e? with higher total edge weights
but p? > p matching edits. Then we have
p(?(u+ 1)|E|) + q ? p?(?(u+ 1)|E|) + q?(2)
? (q ? q?) ? (p? ? p)(?(u+ 1)|E|),
where q and q? denote the combined cost of all non-
matching edits in the two paths, respectively. Be-
cause p? ? p ? 1, the right hand side is at most
?(u + 1)|E|. Because q and q? are positive and
bounded by (u+ 1)|E|, the left hand side cannot be
smaller than or equal to ?(u+ 1)|E|. This is a con-
tradiction. Therefore there cannot exist such an edit
sequence e?, and e is the sequence with the maxi-
mum overlap with the gold-standard annotation.
1To break ties between non-matching edges, we add a small
cost ?  1 to all non-matching edges, thus favoring paths that
use fewer edges, everything else being equal.
2.3 Evaluating edits
What is left to do is to evaluate the set of edits
with respect to the gold standard. This is done by
computing precision, recall, and F1 measure (van
Rijsbergen, 1979) between the set of system edits
{e1, . . . , en} and the set of gold edits {g1, . . . ,gn}
for all sentences
P =
?n
i=1 |ei ? gi|?n
i=1 |ei|
(3)
R =
?n
i=1 |ei ? gi|?n
i=1 |gi|
(4)
F1 = 2?
P ?R
P +R
, (5)
where we define the intersection between ei and gi
as
ei ? gi = {e ? ei | ? g ? gi(match(e, g))}. (6)
3 Experiments and Results
We experimentally test our M2 method in the con-
text of the HOO shared task. The HOO test data2
consists of text fragments from NLP papers to-
gether with manually-created gold-standard correc-
tions (see (Dale and Kilgarriff, 2011) for details).
We test our method by re-scoring the best runs of
the participating teams3 in the HOO shared task with
our M2 scorer and comparing the scores with the of-
ficial HOO scorer, which simply uses GNU wdiff4
to extract system edits. We obtain each system?s
output and segment it at the sentence level accord-
ing to the gold standard sentence segmentation. The
2Available at http://groups.google.com/group/hoo-nlp/ after
registration.
3Except one team that did not submit any plain text output.
4http://www.gnu.org/s/wdiff/
570
M2 scorer . . . should basic translational unit be (word? a word) . . .
HOO scorer . . . should basic translational unit be *(? a) word . . .
M2 scorer . . . development set similar (with? to) (? the) test set . . .
HOO scorer . . . development set similar *(with? to the) test set . . .
M2 scorer (? The) *(Xinhua portion of? xinhua portion of) the English Gigaword3 . . .
HOO scorer *(Xinhua? The xinhua) portion of the English Gigaword3 . . .
Table 2: Examples of different edits extracted by the M2 scorer and the official HOO scorer. Edits that do not match
the gold-standard annotation are marked with an asterisk (*).
Team HOO scorer M2 scorer
P R F1 P R F1
JU (0) 10.39 3.78 5.54 12.30 4.45 6.53
LI (8) 20.86 3.22 5.57 21.12 3.22 5.58
NU (0) 29.10 7.38 11.77 31.09 7.85 12.54
UI (1) 50.72 13.34 21.12 54.61 14.57 23.00
UT (1) 5.01 4.07 4.49 5.72 4.45 5.01
Table 1: Results for participants in the HOO shared task.
The run of the system is shown in parentheses.
source sentences, system hypotheses, and correc-
tions are tokenized using the Penn Treebank stan-
dard (Marcus et al, 1993). The character edit offsets
are automatically converted to token offsets. We set
the parameter u to 2, allowing up to two unchanged
words per edit. The results are shown in Table 1.
Note that the M2 scorer and the HOO scorer adhere
to the same score definition and only differ in the
way the system edits are computed. We can see that
the M2 scorer results in higher scores than the offi-
cial scorer for all systems, showing that the official
scorer missed some valid edits. For example, the
M2 scorer finds 155 valid edits for the UI system
compared to 141 found by the official scorer, and 83
valid edits for the NU system, compared to 78 by
the official scorer. We manually inspect the output
of the scorers and find that the M2 scorer indeed ex-
tracts the correct edits matching the gold standard
where possible. Examples are shown in Table 2.
4 Discussion
The evaluation framework proposed in this work dif-
fers slightly from the one in the HOO shared task.
Sentence-by-sentence. We compute the edits
between source-hypothesis sentence pairs, while
the HOO scorer computes edits at the document
level. As the HOO data comes in a sentence-
segmented format, both approaches are equivalent,
while sentence-by-sentence is easier to work with.
Token-level offsets. In our work, the start and
end of an edit are given as token offsets, while the
HOO data uses character offsets. Character offsets
make the evaluation procedure very brittle as a small
change, e.g., an additional whitespace character, will
affect all subsequent edits. Character offsets also in-
troduce ambiguities in the annotation, e.g., whether
a comma is part of the preceding token.
Alternative scoring. The HOO shared task de-
fines three different scores: detection, recognition,
and correction. Effectively, all three scores are F1
measures and only differ in the conditions on when
an edit is counted as valid. Additionally, each score
is reported under a ?with bonus? alternative, where
a system receives rewards for missed optional ed-
its. The F1 measure defined in Section 2.3 is equiv-
alent to correction without bonus. Our method can
be used to compute detection and recognition scores
and scores with bonus as well.
5 Conclusion
We have presented a novel method, called Max-
Match (M2), for evaluating grammatical error cor-
rection. Our method computes the sequence of
phrase-level edits that achieves the highest over-
lap with the gold-standard annotation. Experi-
ments on the HOO data show that our method
overcomes deficiencies in the current evaluation
method. The M2 scorer is available for download
at http://nlp.comp.nus.edu.sg/software/.
Acknowledgments
We thank Chang Liu for comments on an earlier
draft. This research is supported by the Singa-
pore National Research Foundation under its Inter-
national Research Centre @ Singapore Funding Ini-
tiative and administered by the IDM Programme Of-
fice.
571
References
T. Cormen, C.E. Leiserson, R.L. Rivest, and C. Stein.
2001. Introduction to Algorithms. MIT Press, Cam-
bridge, MA.
R. Dale and A. Kilgarriff. 2011. Helping Our Own: The
HOO 2011 pilot shared task. In Proceedings of the
2011 European Workshop on Natural Language Gen-
eration.
C. Leacock, M. Chodorow, M. Gamon, and J. Tetreault,
2010. Automated Grammatical Error Detection for
Language Learners, chapter 5. Morgan and Claypool
Publishers.
V. Levenshtein. 1966. Binary codes capable of correct-
ing deletions, insertions, and reversals. Soviet Physics
Doklady, 10(8):707?710.
M.P. Marcus, M.A. Marcinkiewicz, and B. Santorini.
1993. Building a large annotated corpus of English:
The Penn Treebank. Computational Linguistics, 19.
C. J. van Rijsbergen. 1979. Information Retrieval. But-
terworth, 2nd edition.
572
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 915?923,
Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational Linguistics
Grammatical Error Correction with Alternating Structure Optimization
Daniel Dahlmeier1 and Hwee Tou Ng1,2
1NUS Graduate School for Integrative Sciences and Engineering
2Department of Computer Science, National University of Singapore
{danielhe,nght}@comp.nus.edu.sg
Abstract
We present a novel approach to grammatical
error correction based on Alternating Struc-
ture Optimization. As part of our work, we
introduce the NUS Corpus of Learner En-
glish (NUCLE), a fully annotated one mil-
lion words corpus of learner English available
for research purposes. We conduct an exten-
sive evaluation for article and preposition er-
rors using various feature sets. Our exper-
iments show that our approach outperforms
two baselines trained on non-learner text and
learner text, respectively. Our approach also
outperforms two commercial grammar check-
ing software packages.
1 Introduction
Grammatical error correction (GEC) has been rec-
ognized as an interesting as well as commercially
attractive problem in natural language process-
ing (NLP), in particular for learners of English as
a foreign or second language (EFL/ESL).
Despite the growing interest, research has been
hindered by the lack of a large annotated corpus of
learner text that is available for research purposes.
As a result, the standard approach to GEC has been
to train an off-the-shelf classifier to re-predict words
in non-learner text. Learning GEC models directly
from annotated learner corpora is not well explored,
as are methods that combine learner and non-learner
text. Furthermore, the evaluation of GEC has been
problematic. Previous work has either evaluated on
artificial test instances as a substitute for real learner
errors or on proprietary data that is not available to
other researchers. As a consequence, existing meth-
ods have not been compared on the same test set,
leaving it unclear where the current state of the art
really is.
In this work, we aim to overcome both problems.
First, we present a novel approach to GEC based
on Alternating Structure Optimization (ASO) (Ando
and Zhang, 2005). Our approach is able to train
models on annotated learner corpora while still tak-
ing advantage of large non-learner corpora. Sec-
ond, we introduce the NUS Corpus of Learner En-
glish (NUCLE), a fully annotated one million words
corpus of learner English available for research pur-
poses. We conduct an extensive evaluation for ar-
ticle and preposition errors using six different fea-
ture sets proposed in previous work. We com-
pare our proposed ASO method with two baselines
trained on non-learner text and learner text, respec-
tively. To the best of our knowledge, this is the
first extensive comparison of different feature sets
on real learner text which is another contribution
of our work. Our experiments show that our pro-
posed ASO algorithm significantly improves over
both baselines. It also outperforms two commercial
grammar checking software packages in a manual
evaluation.
The remainder of this paper is organized as fol-
lows. The next section reviews related work. Sec-
tion 3 describes the tasks. Section 4 formulates GEC
as a classification problem. Section 5 extends this to
the ASO algorithm. The experiments are presented
in Section 6 and the results in Section 7. Section 8
contains a more detailed analysis of the results. Sec-
tion 9 concludes the paper.
915
2 Related Work
In this section, we give a brief overview on related
work on article and preposition errors. For a more
comprehensive survey, see (Leacock et al, 2010).
The seminal work on grammatical error correc-
tion was done by Knight and Chander (1994) on arti-
cle errors. Subsequent work has focused on design-
ing better features and testing different classifiers,
including memory-based learning (Minnen et al,
2000), decision tree learning (Nagata et al, 2006;
Gamon et al, 2008), and logistic regression (Lee,
2004; Han et al, 2006; De Felice, 2008). Work
on preposition errors has used a similar classifica-
tion approach and mainly differs in terms of the fea-
tures employed (Chodorow et al, 2007; Gamon et
al., 2008; Lee and Knutsson, 2008; Tetreault and
Chodorow, 2008; Tetreault et al, 2010; De Felice,
2008). All of the above works only use non-learner
text for training.
Recent work has shown that training on anno-
tated learner text can give better performance (Han
et al, 2010) and that the observed word used by
the writer is an important feature (Rozovskaya and
Roth, 2010b). However, training data has either
been small (Izumi et al, 2003), only partly anno-
tated (Han et al, 2010), or artificially created (Ro-
zovskaya and Roth, 2010b; Rozovskaya and Roth,
2010a).
Almost no work has investigated ways to combine
learner and non-learner text for training. The only
exception is Gamon (2010), who combined features
from the output of logistic-regression classifiers and
language models trained on non-learner text in a
meta-classifier trained on learner text. In this work,
we show a more direct way to combine learner and
non-learner text in a single model.
Finally, researchers have investigated GEC in
connection with web-based models in NLP (Lapata
and Keller, 2005; Bergsma et al, 2009; Yi et al,
2008). These methods do not use classifiers, but rely
on simple n-gram counts or page hits from the Web.
3 Task Description
In this work, we focus on article and preposition er-
rors, as they are among the most frequent types of
errors made by EFL learners.
3.1 Selection vs. Correction Task
There is an important difference between training on
annotated learner text and training on non-learner
text, namely whether the observed word can be used
as a feature or not. When training on non-learner
text, the observed word cannot be used as a feature.
The word choice of the writer is ?blanked out? from
the text and serves as the correct class. A classifier
is trained to re-predict the word given the surround-
ing context. The confusion set of possible classes
is usually pre-defined. This selection task formula-
tion is convenient as training examples can be cre-
ated ?for free? from any text that is assumed to be
free of grammatical errors. We define the more re-
alistic correction task as follows: given a particular
word and its context, propose an appropriate correc-
tion. The proposed correction can be identical to the
observed word, i.e., no correction is necessary. The
main difference is that the word choice of the writer
can be encoded as part of the features.
3.2 Article Errors
For article errors, the classes are the three articles a,
the, and the zero-article. This covers article inser-
tion, deletion, and substitution errors. During train-
ing, each noun phrase (NP) in the training data is one
training example. When training on learner text, the
correct class is the article provided by the human
annotator. When training on non-learner text, the
correct class is the observed article. The context is
encoded via a set of feature functions. During test-
ing, each NP in the test set is one test example. The
correct class is the article provided by the human an-
notator when testing on learner text or the observed
article when testing on non-learner text.
3.3 Preposition Errors
The approach to preposition errors is similar to ar-
ticles but typically focuses on preposition substitu-
tion errors. In our work, the classes are 36 frequent
English prepositions (about, along, among, around,
as, at, beside, besides, between, by, down, during,
except, for, from, in, inside, into, of, off, on, onto,
outside, over, through, to, toward, towards, under,
underneath, until, up, upon, with, within, without),
which we adopt from previous work. Every prepo-
sitional phrase (PP) that is governed by one of the
916
36 prepositions is one training or test example. We
ignore PPs governed by other prepositions.
4 Linear Classifiers for Grammatical
Error Correction
In this section, we formulate GEC as a classification
problem and describe the feature sets for each task.
4.1 Linear Classifiers
We use classifiers to approximate the unknown rela-
tion between articles or prepositions and their con-
texts in learner text, and their valid corrections. The
articles or prepositions and their contexts are repre-
sented as feature vectors X ? X . The corrections
are the classes Y ? Y .
In this work, we employ binary linear classifiers
of the form uTX where u is a weight vector. The
outcome is considered +1 if the score is positive and
?1 otherwise. A popular method for finding u is
empirical risk minimization with least square regu-
larization. Given a training set {Xi, Yi}i=1,...,n, we
aim to find the weight vector that minimizes the em-
pirical loss on the training data
u? = arg min
u
(
1
n
n?
i=1
L(uTXi, Yi) + ? ||u||
2
)
,
(1)
where L is a loss function. We use a modification of
Huber?s robust loss function. We fix the regulariza-
tion parameter ? to 10?4. A multi-class classifica-
tion problem with m classes can be cast as m binary
classification problems in a one-vs-rest arrangement.
The prediction of the classifier is the class with the
highest score Y? = arg maxY ?Y (u
T
Y X). In earlier
experiments, this linear classifier gave comparable
or superior performance compared to a logistic re-
gression classifier.
4.2 Features
We re-implement six feature extraction methods
from previous work, three for articles and three for
prepositions. The methods require different lin-
guistic pre-processing: chunking, CCG parsing, and
constituency parsing.
4.2.1 Article Errors
? DeFelice The system in (De Felice, 2008) for
article errors uses a CCG parser to extract a
rich set of syntactic and semantic features, in-
cluding part of speech (POS) tags, hypernyms
from WordNet (Fellbaum, 1998), and named
entities.
? Han The system in (Han et al, 2006) relies on
shallow syntactic and lexical features derived
from a chunker, including the words before, in,
and after the NP, the head word, and POS tags.
? Lee The system in (Lee, 2004) uses a con-
stituency parser. The features include POS
tags, surrounding words, the head word, and
hypernyms from WordNet.
4.2.2 Preposition Errors
? DeFelice The system in (De Felice, 2008) for
preposition errors uses a similar rich set of syn-
tactic and semantic features as the system for
article errors. In our re-implementation, we do
not use a subcategorization dictionary, as this
resource was not available to us.
? TetreaultChunk The system in (Tetreault and
Chodorow, 2008) uses a chunker to extract
features from a two-word window around the
preposition, including lexical and POS n-
grams, and the head words from neighboring
constituents.
? TetreaultParse The system in (Tetreault et al,
2010) extends (Tetreault and Chodorow, 2008)
by adding additional features derived from a
constituency and a dependency parse tree.
For each of the above feature sets, we add the ob-
served article or preposition as an additional feature
when training on learner text.
5 Alternating Structure Optimization
This section describes the ASO algorithm and shows
how it can be used for grammatical error correction.
5.1 The ASO algorithm
Alternating Structure Optimization (Ando and
Zhang, 2005) is a multi-task learning algorithm that
takes advantage of the common structure of multiple
related problems. Let us assume that we have m bi-
nary classification problems. Each classifier ui is a
917
weight vector of dimension p. Let ? be an orthonor-
mal h ? p matrix that captures the common struc-
ture of the m weight vectors. We assume that each
weight vector can be decomposed into two parts:
one part that models the particular i-th classification
problem and one part that models the common struc-
ture
ui = wi + ?Tvi. (2)
The parameters [{wi,vi},?] can be learned by joint
empirical risk minimization, i.e., by minimizing the
joint empirical loss of the m problems on the train-
ing data
m?
l=1
(
1
n
n?
i=1
L
((
wl + ?Tvl
)T
Xli, Y
l
i
)
+ ? ||wl||
2
)
.
(3)
The key observation in ASO is that the problems
used to find ? do not have to be same as the target
problems that we ultimately want to solve. Instead,
we can automatically create auxiliary problems for
the sole purpose of learning a better ?.
Let us assume that we have k target problems and
m auxiliary problems. We can obtain an approxi-
mate solution to Equation 3 by performing the fol-
lowing algorithm (Ando and Zhang, 2005):
1. Learn m linear classifiers ui independently.
2. Let U = [u1,u2, . . . ,um] be the p ? m matrix
formed from the m weight vectors.
3. Perform Singular Value Decomposition (SVD) on
U : U = V1DV T2 . The first h column vectors of V1
are stored as rows of ?.
4. Learn wj and vj for each of the target problems by
minimizing the empirical risk:
1
n
n?
i=1
L
((
wj + ?Tvj
)T
Xi, Yi
)
+ ? ||wj ||
2 .
5. The weight vector for the j-th target problem is:
uj = wj + ?Tvj .
5.2 ASO for Grammatical Error Correction
The key observation in our work is that the selection
task on non-learner text is a highly informative aux-
iliary problem for the correction task on learner text.
For example, a classifier that can predict the pres-
ence or absence of the preposition on can be help-
ful for correcting wrong uses of on in learner text,
e.g., if the classifier?s confidence for on is low but
the writer used the preposition on, the writer might
have made a mistake. As the auxiliary problems can
be created automatically, we can leverage the power
of very large corpora of non-learner text.
Let us assume a grammatical error correction task
with m classes. For each class, we define a bi-
nary auxiliary problem. The feature space of the
auxiliary problems is a restriction of the original
feature space X to all features except the observed
word: X\{Xobs}. The weight vectors of the aux-
iliary problems form the matrix U in Step 2 of the
ASO algorithm from which we obtain ? through
SVD. Given ?, we learn the vectors wj and vj ,
j = 1, . . . , k from the annotated learner text using
the complete feature space X .
This can be seen as an instance of transfer learn-
ing (Pan and Yang, 2010), as the auxiliary problems
are trained on data from a different domain (non-
learner text) and have a slightly different feature
space (X\{Xobs}). We note that our method is gen-
eral and can be applied to any classification problem
in GEC.
6 Experiments
6.1 Data Sets
The main corpus in our experiments is the NUS Cor-
pus of Learner English (NUCLE). The corpus con-
sists of about 1,400 essays written by EFL/ESL uni-
versity students on a wide range of topics, like en-
vironmental pollution or healthcare. It contains over
one million words which are completely annotated
with error tags and corrections. All annotations have
been performed by professional English instructors.
We use about 80% of the essays for training, 10% for
development, and 10% for testing. We ensure that
no sentences from the same essay appear in both the
training and the test or development data. NUCLE
is available to the community for research purposes.
On average, only 1.8% of the articles and 1.3%
of the prepositions in NUCLE contain an error.
This figure is considerably lower compared to other
learner corpora (Leacock et al, 2010, Ch. 3) and
shows that our writers have a relatively high profi-
ciency of English. We argue that this makes the task
considerably more difficult. Furthermore, to keep
the task as realistic as possible, we do not filter the
918
test data in any way.
In addition to NUCLE, we use a subset of the
New York Times section of the Gigaword corpus1
and the Wall Street Journal section of the Penn Tree-
bank (Marcus et al, 1993) for some experiments.
We pre-process all corpora using the following tools:
We use NLTK2 for sentence splitting, OpenNLP3
for POS tagging, YamCha (Kudo and Matsumoto,
2003) for chunking, the C&C tools (Clark and Cur-
ran, 2007) for CCG parsing and named entity recog-
nition, and the Stanford parser (Klein and Manning,
2003a; Klein and Manning, 2003b) for constituency
and dependency parsing.
6.2 Evaluation Metrics
For experiments on non-learner text, we report ac-
curacy, which is defined as the number of correct
predictions divided by the total number of test in-
stances. For experiments on learner text, we report
F1-measure
F1 = 2?
Precision? Recall
Precision + Recall
where precision is the number of suggested correc-
tions that agree with the human annotator divided
by the total number of proposed corrections by the
system, and recall is the number of suggested cor-
rections that agree with the human annotator divided
by the total number of errors annotated by the human
annotator.
6.3 Selection Task Experiments on WSJ Test
Data
The first set of experiments investigates predicting
articles and prepositions in non-learner text. This
primarily serves as a reference point for the correc-
tion task described in the next section. We train
classifiers as described in Section 4 on the Giga-
word corpus. We train with up to 10 million train-
ing instances, which corresponds to about 37 million
words of text for articles and 112 million words of
text for prepositions. The test instances are extracted
from section 23 of the WSJ and no text from the
WSJ is included in the training data. The observed
article or preposition choice of the writer is the class
1LDC2009T13
2www.nltk.org
3opennlp.sourceforge.net
we want to predict. Therefore, the article or prepo-
sition cannot be part of the input features. Our pro-
posed ASO method is not included in these experi-
ments, as it uses the observed article or preposition
as a feature which is only applicable when testing on
learner text.
6.4 Correction Task Experiments on NUCLE
Test Data
The second set of experiments investigates the pri-
mary goal of this work: to automatically correct
grammatical errors in learner text. The test instances
are extracted from NUCLE. In contrast to the previ-
ous selection task, the observed word choice of the
writer can be different from the correct class and the
observed word is available during testing. We inves-
tigate two different baselines and our ASO method.
The first baseline is a classifier trained on the Gi-
gaword corpus in the same way as described in the
selection task experiment. We use a simple thresh-
olding strategy to make use of the observed word
during testing. The system only flags an error if the
difference between the classifier?s confidence for its
first choice and the confidence for the observed word
is higher than a threshold t. The threshold parame-
ter t is tuned on the NUCLE development data for
each feature set. In our experiments, the value for t
is between 0.7 and 1.2.
The second baseline is a classifier trained on NU-
CLE. The classifier is trained in the same way as
the Gigaword model, except that the observed word
choice of the writer is included as a feature. The cor-
rect class during training is the correction provided
by the human annotator. As the observed word is
part of the features, this model does not need an ex-
tra thresholding step. Indeed, we found that thresh-
olding is harmful in this case. During training, the
instances that do not contain an error greatly out-
number the instances that do contain an error. To re-
duce this imbalance, we keep all instances that con-
tain an error and retain a random sample of q percent
of the instances that do not contain an error. The
undersample parameter q is tuned on the NUCLE
development data for each data set. In our experi-
ments, the value for q is between 20% and 40%.
Our ASO method is trained in the following way.
We create binary auxiliary problems for articles or
prepositions, i.e., there are 3 auxiliary problems for
919
articles and 36 auxiliary problems for prepositions.
We train the classifiers for the auxiliary problems on
the complete 10 million instances from Gigaword in
the same ways as in the selection task experiment.
The weight vectors of the auxiliary problems form
the matrixU . We perform SVD to get U = V1DV T2 .
We keep all columns of V1 to form ?. The target
problems are again binary classification problems
for each article or preposition, but this time trained
on NUCLE. The observed word choice of the writer
is included as a feature for the target problems. We
again undersample the instances that do not contain
an error and tune the parameter q on the NUCLE de-
velopment data. The value for q is between 20% and
40%. No thresholding is applied.
We also experimented with a classifier that is
trained on the concatenated data from NUCLE and
Gigaword. This model always performed worse than
the better of the individual baselines. The reason is
that the two data sets have different feature spaces
which prevents simple concatenation of the training
data. We therefore omit these results from the paper.
7 Results
The learning curves of the selection task experi-
ments on WSJ test data are shown in Figure 1. The
three curves in each plot correspond to different fea-
ture sets. Accuracy improves quickly in the be-
ginning but improvements get smaller as the size
of the training data increases. The best results are
87.56% for articles (Han) and 68.25% for prepo-
sitions (TetreaultParse). The best accuracy for ar-
ticles is comparable to the best reported results of
87.70% (Lee, 2004) on this data set.
The learning curves of the correction task ex-
periments on NUCLE test data are shown in Fig-
ure 2 and 3. Each sub-plot shows the curves of
three models as described in the last section: ASO
trained on NUCLE and Gigaword, the baseline clas-
sifier trained on NUCLE, and the baseline classifier
trained on Gigaword. For ASO, the x-axis shows
the number of target problem training instances. The
first observation is that high accuracy for the selec-
tion task on non-learner text does not automatically
entail high F1-measure on learner text. We also note
that feature sets with similar performance on non-
learner text can show very different performance on
0.68
0.70
0.72
0.74
0.76
0.78
0.80
0.82
0.84
0.86
0.88
 1000  10000  100000  1e+06  1e+07
AC
CU
RA
CY
Number of training examples
GIGAWORD DEFELICE
GIGAWORD HAN
GIGAWORD LEE
(a) Articles
0.25
0.30
0.35
0.40
0.45
0.50
0.55
0.60
0.65
0.70
 1000  10000  100000  1e+06  1e+07
AC
CU
RA
CY
Number of training examples
GIGAWORD DEFELICE
GIGAWORD TETRAULTCHUNK
GIGAWORD TETRAULTPARSE
(b) Prepositions
Figure 1: Accuracy for the selection task on WSJ
test data.
learner text. The second observation is that train-
ing on annotated learner text can significantly im-
prove performance. In three experiments (articles
DeFelice, Han, prepositions DeFelice), the NUCLE
model outperforms the Gigaword model trained on
10 million instances. Finally, the ASO models show
the best results. In the experiments where the NU-
CLE models already perform better than the Giga-
word baseline, ASO gives comparable or slightly
better results (articles DeFelice, Han, Lee, preposi-
tions DeFelice). In those experiments where neither
baseline shows good performance (TetreaultChunk,
TetreaultParse), ASO results in a large improvement
over either baseline. The best results are 19.29% F1-
measure for articles (Han) and 11.15% F1-measure
for prepositions (TetreaultParse) achieved by the
ASO model.
920
0.02
0.04
0.06
0.08
0.10
0.12
0.14
0.16
0.18
 1000  10000  100000  1e+06  1e+07
F1
Number of training examples
ASONUCLEGIGAWORD
(a) DeFelice
0.02
0.04
0.06
0.08
0.10
0.12
0.14
0.16
0.18
0.20
 1000  10000  100000  1e+06  1e+07
F1
Number of training examples
ASONUCLEGIGAWORD
(b) Han
0.02
0.03
0.04
0.05
0.06
0.07
0.08
0.09
 1000  10000  100000  1e+06  1e+07
F1
Number of training examples
ASONUCLEGIGAWORD
(c) Lee
Figure 2: F1-measure for the article correction task on NUCLE test data. Each plot shows ASO and two
baselines for a particular feature set.
0.00
0.01
0.02
0.03
0.04
0.05
0.06
0.07
0.08
0.09
0.10
 1000  10000  100000  1e+06  1e+07
F1
Number of training examples
ASONUCLEGIGAWORD
(a) DeFelice
0.00
0.01
0.02
0.03
0.04
0.05
0.06
0.07
0.08
0.09
0.10
 1000  10000  100000  1e+06  1e+07
F1
Number of training examples
ASONUCLEGIGAWORD
(b) TetreaultChunk
0.00
0.02
0.04
0.06
0.08
0.10
0.12
 1000  10000  100000  1e+06  1e+07
F1
Number of training examples
ASONUCLEGIGAWORD
(c) TetreaultParse
Figure 3: F1-measure for the preposition correction task on NUCLE test data. Each plot shows ASO and
two baselines for a particular feature set.
8 Analysis
In this section, we analyze the results in more detail
and show examples from our test set for illustration.
Table 1 shows precision, recall, and F1-measure
for the best models in our experiments. ASO
achieves a higher F1-measure than either baseline.
We use the sign-test with bootstrap re-sampling for
statistical significance testing. The sign-test is a non-
parametric test that makes fewer assumptions than
parametric tests like the t-test. The improvements in
F1-measure of ASO over either baseline are statis-
tically significant (p < 0.001) for both articles and
prepositions.
The difficulty in GEC is that in many cases, more
than one word choice can be correct. Even with a
threshold, the Gigaword baseline model suggests too
many corrections, because the model cannot make
use of the observed word as a feature. This results in
low precision. For example, the model replaces as
Articles
Model Prec Rec F1
Gigaword (Han) 10.33 21.81 14.02
NUCLE (Han) 29.48 12.91 17.96
ASO (Han) 26.44 15.18 19.29
Prepositions
Model Prec Rec F1
Gigaword (TetreaultParse ) 4.77 14.81 7.21
NUCLE (DeFelice) 13.84 5.55 7.92
ASO (TetreaultParse) 18.30 8.02 11.15
Table 1: Best results for the correction task on NU-
CLE test data. Improvements for ASO over either
baseline are statistically significant (p < 0.001) for
both tasks.
with by in the sentence ?This group should be cate-
gorized as the vulnerable group?, which is wrong.
In contrast, the NUCLE model learns a bias to-
wards the observed word and therefore achieves
higher precision. However, the training data is
921
smaller and therefore recall is low as the model has
not seen enough examples during training. This is
especially true for prepositions which can occur in a
large variety of contexts. For example, the preposi-
tion in should be on in the sentence ?... psychology
had an impact in the way we process and manage
technology?. The phrase ?impact on the way? does
not appear in the NUCLE training data and the NU-
CLE baseline fails to detect the error.
The ASO model is able to take advantage of both
the annotated learner text and the large non-learner
text, thus achieving overall high F1-measure. The
phrase ?impact on the way?, for example, appears
many times in the Gigaword training data. With the
common structure learned from the auxiliary prob-
lems, the ASO model successfully finds and corrects
this mistake.
8.1 Manual Evaluation
We carried out a manual evaluation of the best ASO
models and compared their output with two com-
mercial grammar checking software packages which
we call System A and System B. We randomly sam-
pled 1000 test instances for articles and 2000 test
instances for prepositions and manually categorized
each test instance into one of the following cate-
gories: (1) Correct means that both human and sys-
tem flag an error and suggest the same correction.
If the system?s correction differs from the human
but is equally acceptable, it is considered (2) Both
Ok. If the system identifies an error but fails to cor-
rect it, we consider it (3) Both Wrong, as both the
writer and the system are wrong. (4) Other Error
means that the system?s correction does not result
in a grammatical sentence because of another gram-
matical error that is outside the scope of article or
preposition errors, e.g., a noun number error as in
?all the dog?. If the system corrupts a previously
correct sentence it is a (5) False Flag. If the hu-
man flags an error but the system does not, it is a
(6) Miss. (7) No Flag means that neither the human
annotator nor the system flags an error. We calculate
precision by dividing the count of category (1) by the
sum of counts of categories (1), (3), and (5), and re-
call by dividing the count of category (1) by the sum
of counts of categories (1), (3), and (6). The results
are shown in Table 2. Our ASO method outperforms
both commercial software packages. Our evalua-
Articles
ASO System A System B
(1) Correct 4 1 1
(2) Both Ok 16 12 18
(3) Both Wrong 0 1 0
(4) Other Error 1 0 0
(5) False Flag 1 0 4
(6) Miss 3 5 6
(7) No Flag 975 981 971
Precision 80.00 50.00 20.00
Recall 57.14 14.28 14.28
F1 66.67 22.21 16.67
Prepositions
ASO System A System B
(1) Correct 3 3 0
(2) Both Ok 35 39 24
(3) Both Wrong 0 2 0
(4) Other Error 0 0 0
(5) False Flag 5 11 1
(6) Miss 12 11 15
(7) No Flag 1945 1934 1960
Precision 37.50 18.75 0.00
Recall 20.00 18.75 0.00
F1 26.09 18.75 0.00
Table 2: Manual evaluation and comparison with
commercial grammar checking software.
tion shows that even commercial software packages
achieve low F1-measure for article and preposition
errors, which confirms the difficulty of these tasks.
9 Conclusion
We have presented a novel approach to grammati-
cal error correction based on Alternating Structure
Optimization. We have introduced the NUS Corpus
of Learner English (NUCLE), a fully annotated cor-
pus of learner text. Our experiments for article and
preposition errors show the advantage of our ASO
approach over two baseline methods. Our ASO ap-
proach also outperforms two commercial grammar
checking software packages in a manual evaluation.
Acknowledgments
This research was done for CSIDM Project No.
CSIDM-200804 partially funded by a grant from
the National Research Foundation (NRF) adminis-
tered by the Media Development Authority (MDA)
of Singapore.
922
References
R.K. Ando and T. Zhang. 2005. A framework for learn-
ing predictive structures from multiple tasks and un-
labeled data. Journal of Machine Learning Research,
6.
S. Bergsma, D. Lin, and R. Goebel. 2009. Web-scale n-
gram models for lexical disambiguation. In Proceed-
ings of IJCAI.
M. Chodorow, J. Tetreault, and N.R. Han. 2007. De-
tection of grammatical errors involving prepositions.
In Proceedings of the 4th ACL-SIGSEM Workshop on
Prepositions.
S. Clark and J.R. Curran. 2007. Wide-coverage effi-
cient statistical parsing with CCG and log-linear mod-
els. Computational Linguistics, 33(4).
R. De Felice. 2008. Automatic Error Detection in Non-
native English. Ph.D. thesis, University of Oxford.
C. Fellbaum, editor. 1998. WordNet: An electronic lexi-
cal database. MIT Press, Cambridge,MA.
M. Gamon, J. Gao, C. Brockett, A. Klementiev, W.B.
Dolan, D. Belenko, and L. Vanderwende. 2008. Using
contextual speller techniques and language modeling
for ESL error correction. In Proceedings of IJCNLP.
M. Gamon. 2010. Using mostly native data to correct
errors in learners? writing: A meta-classifier approach.
In Proceedings of HLT-NAACL.
N.R. Han, M. Chodorow, and C. Leacock. 2006. De-
tecting errors in English article usage by non-native
speakers. Natural Language Engineering, 12(02).
N.R. Han, J. Tetreault, S.H. Lee, and J.Y. Ha. 2010.
Using an error-annotated learner corpus to develop an
ESL/EFL error correction system. In Proceedings of
LREC.
E. Izumi, K. Uchimoto, T. Saiga, T. Supnithi, and H. Isa-
hara. 2003. Automatic error detection in the Japanese
learners? English spoken data. In Companion Volume
to the Proceedings of ACL.
D. Klein and C.D. Manning. 2003a. Accurate unlexical-
ized parsing. In Proceedings of ACL.
D. Klein and C.D. Manning. 2003b. Fast exact inference
with a factored model for natural language processing.
Advances in Neural Information Processing Systems
(NIPS 2002), 15.
K. Knight and I. Chander. 1994. Automated postediting
of documents. In Proceedings of AAAI.
T Kudo and Y. Matsumoto. 2003. Fast methods for
kernel-based text analysis. In Proceedings of ACL.
M. Lapata and F. Keller. 2005. Web-based models for
natural language processing. ACM Transactions on
Speech and Language Processing, 2(1).
C. Leacock, M. Chodorow, M. Gamon, and J. Tetreault.
2010. Automated Grammatical Error Detection for
Language Learners. Morgan & Claypool Publishers,
San Rafael,CA.
J. Lee and O. Knutsson. 2008. The role of PP attachment
in preposition generation. In Proceedings of CICLing.
J. Lee. 2004. Automatic article restoration. In Proceed-
ings of HLT-NAACL.
M.P. Marcus, M.A. Marcinkiewicz, and B. Santorini.
1993. Building a large annotated corpus of English:
The Penn Treebank. Computational Linguistics, 19.
G. Minnen, F. Bond, and A. Copestake. 2000. Memory-
based learning for article generation. In Proceedings
of CoNLL.
R. Nagata, A. Kawai, K. Morihiro, and N. Isu. 2006.
A feedback-augmented method for detecting errors in
the writing of learners of English. In Proceedings of
COLING-ACL.
S.J. Pan and Q. Yang. 2010. A survey on transfer learn-
ing. IEEE Transactions on Knowledge and Data En-
gineering, 22(10).
A. Rozovskaya and D. Roth. 2010a. Generating con-
fusion sets for context-sensitive error correction. In
Proceedings of EMNLP.
A. Rozovskaya and D. Roth. 2010b. Training paradigms
for correcting errors in grammar and usage. In Pro-
ceedings of HLT-NAACL.
J. Tetreault and M. Chodorow. 2008. The ups and downs
of preposition error detection in ESL writing. In Pro-
ceedings of COLING.
J. Tetreault, J. Foster, and M. Chodorow. 2010. Using
parse features for preposition selection and error de-
tection. In Proceedings of ACL.
X. Yi, J. Gao, and W.B. Dolan. 2008. A web-based En-
glish proofing system for English as a second language
users. In Proceedings of IJCNLP.
923
Proceedings of the Third Joint Conference on Lexical and Computational Semantics (*SEM 2014), pages 63?68,
Dublin, Ireland, August 23-24 2014.
Learning the Peculiar Value of Actions
Daniel Dahlmeier
Research & Innovation, SAP Asia, Singapore
d.dahlmeier@sap.com
Abstract
We consider the task of automatically es-
timating the value of human actions. We
cast the problem as a supervised learning-
to-rank problem between pairs of action
descriptions. We present a large, novel
data set for this task which consists of
challenges from the I Will If You Will
Earth Hour challenge. We show that an
SVM ranking model with simple linguistic
features can accurately predict the relative
value of actions.
1 Introduction
The question on how humans conceptualize value
is of great interest to researchers in various fields,
including linguistics (Jackendoff, 2006). The link
between value and language arises from the fact
that we cannot directly observe value due to its ab-
stract nature and instead often study language ex-
pressions that describe actions which have some
value attached to them. This creates an interesting
link between the semantics of the words that de-
scribe the actions and the underlying moral value
of the actions.
Jackendoff (2006) describes value as an ?inter-
nal accounting system? for ethical decision pro-
cesses that exhibits both valence (good or bad)
and magnitude (better or worse). Most interest-
ingly, value is governed by a ?peculiar logic? that
provides constraints on which actions are deemed
morally acceptable and which are not. In par-
ticular, the principal of reciprocity states that the
valence and magnitude of reciprocal actions (ac-
tions that are done ?in return? for something else)
should match, i.e., positive valued actions should
This work is licenced under a Creative Commons Attribution
4.0 International License. Page numbers and proceedings
footer are added by the organizers. License details: http:
//creativecommons.org/licenses/by/4.0/
match with positive valued reciprocal actions (re-
actions) of similar magnitude, and conversely neg-
atively valued actions should match with nega-
tive valued reciprocal actions (reactions) of similar
magnitude.
In this paper, we consider the task of automati-
cally estimating the value of actions. We present a
simple and effective method for learning the value
of actions from ranked pairs of textual action de-
scriptions based on a statistical learning-to-rank
approach. Our experiments are based on a novel
data set that we create from challenges submit-
ted to the I Will if You Will Earth Hour challenge
where participants pledge to do something daring
or challenging if other people commit to sustain-
able actions for the planet. Our method achieves
a surprisingly high accuracy of up to 94.72% in a
10-fold cross-validation experiment. The results
show that the value of actions can accurately be
estimated by machine learning methods based on
lexical descriptions of the actions.
The main contribution of this paper is that we
show how the semantics of value in language can
accurately be learned from empirical data using a
learning-to-rank approach. Our work shows an in-
teresting link between empirical research on se-
mantics in natural language processing and the
concept of value.
2 The Logic of Value
Our approach is based on the concept of value
as presented by Jackendoff (2006) who describes
value as an abstract property that is attributed to
objects, persons, and actions. He further describes
logical inference rules that humans use to deter-
mine which actions are deemed morally accept-
able and which are not. The most important in-
ference rule for our work is the principal of recip-
rocation, things that are done ?in return? for some
other action (Fiengo and Lasnik, 1973). In En-
glish, this relation is often expressed by the prepo-
63
sition for, as shown by the following example sen-
tences (Jackendoff, 2006).
1. Susan praised Sam for behaving nicely.
2. Fred cooked Lois dinner for fixing his com-
puter.
3. Susan insulted Sam for behaving badly.
4. Lois slashed Fred?s tires for insulting her.
The first two examples describe actions with pos-
itive value, while the last two examples describe
actions with negative value. We expect that the
valence values of reciprocal actions match: posi-
tively valued actions demand a positively valued
action in return, while negatively valued actions
trigger negatively valued responses. If we switch
the example sentences and match positive actions
with negative actions, we get sentences that sound
counter-intuitive or perhaps comical (we prefix
counter-intuitive sentences with a hash character
?#?).
1. #Susan insulted Sam for behaving nicely.
2. #Lois slashed Fred?s tires for fixing her com-
puter.
Similarly, we expect that the magnitudes of value
between reciprocal actions match. Sentences
where the magnitude of the value of the response
action does not match the magnitude of the initial
action seem odd or socially inappropriate (over-
acting/underacting).
1. #Fred cooked Lois dinner for saying hello to
him.
2. #Fred cooked Lois dinner for rescuing all his
relatives from certain death.
3. #Fred slashed Lois?s tires for eating too little
at dinner.
4. #Fred slashed Lois?s tires for murdering his
entire family.
We observe that reciprocal actions typically match
each other in valence and magnitude. Coming
back to our initial goal of learning the value of
actions, this gives us a method for comparing the
value of actions that were done in return to the
same initial action.
3 I Will If You Will challenge
The I Will If You Will (IWIYW) challenge
1
is part
of the World Wildlife Fund?s Earth Hour campaign
1
www.earthhour.org/i-will-if-you-will
I will quit smoking if you will start recycling.
(500 people)
I will adopt a panda if you will start recycling.
(1000 people)
I will dance gangnam style if you will plant
a tree. (100 people)
I will dye my hair red if you will upload an
IWIYW challenge. (500 people)
I will learn Java if you will upload an IWIYW
challenge. (10,000 people)
Table 1: Examples of I Will If You Will chal-
lenges.
which has the goal to increase awareness of sus-
tainability issues. In this challenge, participants
make a pledge to do something daring or challeng-
ing if a certain number of people commit to sus-
tainable actions for the planet. The challenges are
created by ordinary people on the Earth Hour cam-
paign website. Each challenge takes the form of a
simple school yard dare: I will do X, if you will do
Y, where X is typically some daring or challenging
task that the challenge creator commits to do if a
sufficient number of people commit to do action
Y which is some sustainable action for the planet.
Together with the textual description, each chal-
lenge includes the number of people that need to
commit to doing Y in order for the challenge cre-
ator to perform X. Examples of the challenges are
shown in Table 1.
It is important to note that during the challenge
creation on the IWIYW website, the X challenge
is a free text input field that allows the author to
come up with creative and interesting challenges.
The sustainable actions Y and the number of peo-
ple that need to commit to it are usually chosen
from a fixed list of choices. As a result, there is
a large number of different X actions and a com-
parably smaller number of Y actions. The col-
lected challenges provide a unique data set that al-
lows us to quantitatively measure the value of each
promised task by the number of people that need
to fulfill the sustainable action.
4 Method
In this section, we present our approach for esti-
mating the value of actions. Our approach casts
the problem as a supervised learning-to-rank prob-
lem between pairs of actions. Given, a textual de-
scription of an action a, we want to estimate its
64
value magnitude v. We represent the action a via a
set of features that are extracted from the descrip-
tion of the action. We use a linear model that com-
bines the features into a single scalar value for the
value v
v = w
T
x
a
, (1)
where x
a
is the feature vector for action descrip-
tion a and w is a learned weight vector. The goal
is to learn a suitable weight vector w that approxi-
mates the true relationship between textual expres-
sions of actions and their magnitude of value.
Instead of estimating the value directly, we take
an alternative approach and consider the task of
learning the relative ranking of pairs of actions.
We follow the pairwise approach to ranking (Her-
brich et al., 1999; Cao et al., 2007) that reduces
ranking to a binary classification problem. Rank-
ing the values v
1
and v
2
of two actions a
1
and a
2
is
equivalent to determining the sign of the dot prod-
uct between the weight vector w and the difference
between the feature vectors x
a
1
and x
a
2
.
v
1
> v
2
? w
T
x
a
1
> w
T
x
a
2
? w
T
x
a
1
? w
T
x
a
2
> 0
? w
T
(x
a
1
? x
a
2
) > 0 (2)
For each ranking pair of actions, we create two
complimentary classification instances: (x
a
1
?
x
a
2
, l
1
) and (x
a
2
? x
a
1
, l
2
), where the labels are
l
1
= +1, l
2
= ?1 if the first challenge has higher
value than the second challenge and l
1
= ?1, l
2
=
+1 otherwise. We can train a standard linear clas-
sifier on the generated training instances to learn
the weight vector w.
In the case of the IWIYW data, there is no ex-
plicit ranking between actions. However, we are
able to create ranking pairs for the IWIYW data
in the following way. As we have seen, there is
only a small set of different You Will challenges
that are reciprocal actions for a diverse set of I
Will challenges. Thus, many I Will challenges will
end up having the same You Will challenge. We
can use the You Will challenges as a pivot to ef-
fectively ?join? the I Will challenges. The number
of required people to perform Y induces a natu-
ral ordering between the values of the I Will ac-
tions where a higher number of required partici-
pants means that the I Will task has higher value.
For example, for the challenges displayed in Ta-
ble 1, we can use the common You Will challenges
to create the following ranked challenge pairs.
I will quit smoking < I will adopt a panda
I will dye my hair red < I will learn Java (3)
According to the examples, adopting a panda has
higher value than quitting smoking and learning
Java has higher value than dying ones hair red.
The third challenge does not share a common You
Will challenge with any other challenge and there-
fore no ranking pairs can be formed with it.
As the IWIYW challenges are created online in
a non-controlled environment, we have to expect
that there is some noise in the automatically cre-
ated ranked challenges. However, a robust learn-
ing algorithm has to be able to handle a certain
amount of noise. We note that our method is not
limited to the IWIYW data set but can be applied
to any data set of actions where relative rankings
are provided or can be induced.
4.1 Features
The choice of appropriate feature representations
is crucial to the success of any machine learning
method. We start by parsing each I Will If You
Will challenge with a constituency parser. Be-
cause each challenge has the same I Will If You
Will structure, it is easy to identify the subtrees that
correspond to the I Will and You Will parts of the
challenge. An example parse tree of a challenge
is shown in Figure 1. The yield of the You Will
subtree serves as a pivot to join different I Will
challenges. To represent the I Will action a as a
feature vector x
a
, we extract the following lexical
and syntax features from the I Will subtree of the
sentence.
? Verb: We extract the verb of the I Will clause
as a feature. To identify the verb, we pick
the left-most verb of the I Will subtree based
on its part-of-speech (POS) tag. We extract
the lowercased word token as a feature. For
example, for the sentence in Figure 1, the
verb feature is verb=quit. If the verb is
negated (the left sibling of the I Will sub-
tree spans exactly the word not), we add the
postfix NOT to the verb feature, for example
verb=quit NOT.
? Object: We take the right sibling of the I
will verb as the object of the action. If the
right sibling is a particle with constituent la-
bel PRT, e.g., travel around the UK on bike,
65
SNP
PRP
I
VP
MD
will
VP
I will
VB
quit
NP
smoking
SBAR
Y ou Will
IN
if
S
you will commit to recycling.
Figure 1: Parse tree of a I Will If You Will challenge. The subtrees governing the I Will and You Will part
of the sentence are marked.
we skip the particle and take the next sib-
ling as the object. If the object is a prepo-
sitional phrase with constituent tag PP, e.g.,
go without electricity for a month, we take
the second child of the prepositional phrase
as the object phrase. We then extract two fea-
tures to represent the object. First, we extract
the lowercased head word of the object as a
feature. Second, we extract the concatena-
tion of all the words in the yield of the object
node as a single feature to capture the com-
plete argument for longer objects. In our ex-
ample sentence, the object head feature and
the complete object feature are identical: ob-
ject head=smoking and object=smoking.
? Unigram: We take all lowercased words that
are not stopwords in the I Will part of the
sentence as binary features. In our example
sentence, the unigram features unigr quit and
unigr smoking would be active.
? Bigram: We take all lowercased bigrams in
the I Will part of the sentence as binary fea-
tures. We do not remove stopwords for bi-
gram features. In our example sentence, the
bigram features bigr quit smoking would be
active.
We note that our method is not restricted to these
feature templates. More sophisticated features,
like tree kernels (Collins and Duffy, 2002) or se-
mantic role labeling (Palmer et al., 2010), can be
imagined.
5 Experiments
We evaluate our approach using standard 10-fold
cross-validation and report macro-average accu-
racy scores for each of the feature sets. The classi-
fier in all our experiments is a linear SVM imple-
mented in SVM-light (Joachims, 2006).
5.1 Data
We obtained a snapshot of 18,290 challenges cre-
ated during the 2013 IWIYW challenge. The snap-
shot was taken in mid May 2013, just 1.5 weeks
before the 2013 Earth Hour event day. We per-
form the following pre-processing. We normal-
ize the text to proper UTF-8 encoding and remove
challenges where the complete sentence contained
less than 7 tokens. These challenges were usually
empty or incomplete. We filter the challenges us-
ing the langid.py tool (Lui and Baldwin, 2012)
and only keep English challenges. We normal-
ized the casing of the sentences by first lower-
casing all texts and then re-casing each sentence
with a simple re-casing model that replaces a word
with its most frequent casing form. The re-casing
model is trained on the Brown corpus (Ku and
Francis, 1967). We tokenize the sentences with
the Penn Treebank tokenizer. We parse the sen-
tences with the Stanford parser (Klein and Man-
ning, 2003a; Klein and Manning, 2003b) to ob-
66
Features Accuracy
random 0.5000
verb 0.6241
unigrams 0.8481
unigrams + verb 0.8573
object 0.8904
verb + object 0.9115
bigrams 0.9251
unigrams + bigrams 0.9343
unigrams + bigrams + verb 0.9361
unigrams + bigrams + verb + object 0.9472
Table 2: Results of 10-fold cross-validation exper-
iments.
tain a constituency parse tree for each challenge.
After pre-processing, we are left with 5,499 chal-
lenges (4,982 unique), with 4,474 unique I Will
challenges and 70 unique You Will challenges.
We create binary classifications examples be-
tween pairs of actions as described in Section 4.
As we create all possible combinations between I
Will challenges with common You Will challenges,
the number of ranking pairs for training is large.
In our case, we ended up with over 840,000 classi-
fication instances. We note that not every I Will ac-
tion is guaranteed to be included in the final set of
ranking pairs as challenges with a unique You Will
part that is not found in any other challenge cannot
be joined and are effectively ignored. However,
this is not a problem for our experiments. The bi-
nary classification instances are used to train and
test a ranking model for estimating the value of
actions as described in the last section.
5.2 Results
The results of our cross-validation experiments are
shown in Table 2.
The random baseline for all experiments is 50%.
Just using the verb of the I Will action as a fea-
ture improves over the random baseline to 62.41%.
Using a unigram bag-of-words representation of
the actions achieves a very respectable score of
84.81%. When we combine unigrams with the
verb feature, we achieve 85.73%. One of the most
surprising results of our experiments is that the
object of the action alone is a very effective fea-
ture, achieving 89.04%. When combined with the
verb feature, the object feature achieves 91.15%
which shows that the verb and object carry most
of the relevant information that the model requires
to gauge the value of actions. Using bigrams as
features, seems to catch this information just as ac-
curately, achieving 92.51% accuracy. The score is
further improved by combining the different fea-
ture sets. The best result of 94.72% is obtained
by combining all the features: unigrams, bigrams,
verb, and object. In summary, these results show
that our method is able to accurately predict the
relative value of actions using simple linguistic
features, which is the main contribution of this
work.
6 Related Work
The concept of value and reciprocity has been
extensively studied in the social sciences (Ger-
gen and Greenberg, 1980), anthropology (Sahlins,
1972), economics (Fehr and G?achter, 2000), and
philosophy (Becker, 1990). In linguistics, value
has been studied by Jackendoff (2006). His work
forms the starting point of our approach.
In natural language processing, there has been
very little work on the concept of value. Paul et al.
(2009) and Girju and Paul (2011) address the prob-
lem of semi-automatically mining patterns that en-
code reciprocal relationships using pronoun tem-
plates. Their work focuses on mining patterns of
reciprocity while our work uses expressions of re-
ciprocal actions to learn the value of actions.
None of the above works tries to estimate the
value of actions, as we do in this work. In fact, we
are not aware of any other work that tries to esti-
mate the value of actions from lexical expressions
of value.
7 Conclusion
We have presented a simple and effective method
for learning the value of actions from reciprocal
sentences. We show that our SVM-based ranking
model with simple linguistic features is able to ac-
curately rank pairs of actions from the I Will If
You Will Earth Hour challenge, achieving an ac-
curacy of up to 94.72%.
Acknowledgement
We thank Sid Das from Earth Hour for shar-
ing the IWIYW data with us. We thank Marek
Kowalkiewicz for helpful discussions. The re-
search is partially funded by the Economic Devel-
opment Board and the National Research Founda-
tion of Singapore.
67
References
Lawrence C Becker, editor. 1990. Reciprocity. Uni-
versity of Chicago Press.
Zhe Cao, Tao Qin, Tie-Yan Liu, Ming-Feng Tsai, and
Hang Li. 2007. Learning to rank: from pairwise
approach to listwise approach. In Proceedings of the
24th International Conference on Machine Learning
(ICML), pages 129?136.
Michael Collins and Nigel Duffy. 2002. Convolution
kernels for natural language. In Advances in Neu-
ral Information Processing Systems 14 (NIPS 2001),
pages 625?632.
Ernst Fehr and Simon G?achter. 2000. Cooperation
and punishment in public goods experiments. pages
980?994.
Robert Fiengo and Howard Lasnik. 1973. The logical
structure of reciprocal sentences in English. Foun-
dations of language, pages 447?468.
Kenneth J. Gergen and Willis Richard H. Greenberg,
Martin S., editors. 1980. Social exchange: Ad-
vances in theory and research. Plenum Press.
Roxana Girju and Michael J Paul. 2011. Modeling
reciprocity in social interactions with probabilistic
latent space models. Natural Language Engineer-
ing, 17(1):1?36.
Ralf Herbrich, Thore Graepel, and Klaus Obermayer.
1999. Support vector learning for ordinal regres-
sion. In In Proceedings of the 1999 International
Conference on Articial Neural Networks, pages 97?
102.
Ray Jackendoff. 2006. The peculiar logic of value.
Journal of Cognition and Culture, 6(3-4):375?407.
Thorsten Joachims. 2006. Training linear SVMs in lin-
ear time. In Proceedings of the 12th ACM SIGKDD
international conference on Knowledge discovery
and data mining, pages 217?226.
Dan Klein and Christopher D. Manning. 2003a. Ac-
curate unlexicalized parsing. In Proceedings of the
41st Annual Meeting of the Association for Compu-
tational Linguistics (ACL 2003), pages 423?430.
Dan Klein and Christopher D. Manning. 2003b. Fast
exact inference with a factored model for natural
language parsing. Advances in Neural Information
Processing Systems 15 (NIPS 2002), pages 423?430.
Henry Ku and W. Nelson Francis. 1967. Computa-
tional Analysis of Present-Day American English.
Brown University Press.
Marco Lui and Timothy Baldwin. 2012. An off-the-
shelf language identification tool. In Proceedings
of the 50th Annual Meeting of the Association for
Computational Linguistics (ACL 2012), pages 25?
30.
Martha Palmer, Daniel Gildea, and Nianwen Xue.
2010. Semantic role labeling. Synthesis Lectures
on Human Language Technologies, 3(1):1?103.
Michael Paul, Roxana Girju, and Chen Li. 2009. Min-
ing the web for reciprocal relationships. In Proceed-
ings of the 13th Conference on Computational Nat-
ural Language Learning (CoNLL), pages 75?83.
Marshall D. Sahlins. 1972. Stone age economics.
Transaction Publishers.
68
Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014), pages 517?521,
Dublin, Ireland, August 23-24, 2014.
SAP-RI: A Constrained and Supervised Approach for Aspect-Based
Sentiment Analysis
Nishtha Malhotra
1,2?
, Akriti Vij
1,2,?
, Naveen Nandan
1
and Daniel Dahlmeier
1
1
Research & Innovation, SAP Asia, Singapore
2
Nanyang Technological University, Singapore
{nishtha.malhotra,akriti.vij,naveen.nandan,d.dahlmeier}@sap.com
Abstract
We describe the submission of the SAP
Research & Innovation team to the Se-
mEval 2014 Task 4: Aspect-Based Senti-
ment Analysis (ABSA). Our system fol-
lows a constrained and supervised ap-
proach for aspect term extraction, catego-
rization and sentiment classification of on-
line reviews and the details are included in
this paper.
1 Introduction
The increasing popularity of the internet as a
source of information, and e-commerce as a way
of life, has led to a major surge in the number of
reviews that can be found online, for a wide range
of products and services. Consequently, more and
more consumers have taken to consulting these on-
line reviews as part of their pre-purchase research
before deciding on availing services from a local
business or investing in a product from a particu-
lar brand. This calls for innovative techniques for
the sentiment analysis of online reviews so as to
generate accurate and relevant recommendations.
Sentiment analysis has been extensively studied
and applied in different domains. Predicting the
sentiment polarity (positive, negative, neutral) of
user opinions by mining user reviews (Hu and Liu,
2004; Liu, 2012; Pang and Lee, 2008; Liu, 2010)
has been of high commercial and research interest.
In these studies, sentiment analysis is often con-
ducted at one of the three levels: document level,
sentence level or attribute level.
Through the SemEval 2014 Task 4 on Aspect
Based Sentiment Analysis (Pontiki et al., 2014),
we explore sentiment analysis at the aspect level.
?
The work was done during an internship at SAP.
This work is licenced under a Creative Commons Attribution
4.0 International License. Page numbers and proceedings
footer are added by the organizers. License details: http:
//creativecommons.org/licenses/by/4.0/
The task consists of four subtasks: in subtask 1 as-
pect term extraction, participants need to identify
the aspect terms present in a sentence and return
a list containing all distinct aspect terms, in sub-
task 2 aspect term polarity, participants were to
determine the polarity of each aspect term in a sen-
tence, in subtask 3 aspect category detection, par-
ticipants had to identify the aspect categories dis-
cussed in a given sentence, and in subtask 4 aspect
category polarity, participants were to determine
the polarity of each aspect category. The polarity
classification subtasks consider sentiment analysis
to be a three-way classification problem between
positive, negative and neutral sentiment. On the
other hand, the aspect category detection subtask
is a multi-label classification problem where one
sentence can be labelled with more than one as-
pect category.
In this paper, we describe the submission of the
SAP-RI team to the SemEval 2014 Task 4. We
make use of supervised techniques to extract the
aspects of interest (Jakob and Gurevych, 2010),
categorize them (Lu et al., 2011) and predict the
sentiment of customer online reviews on Laptops
and Restaurants. We developed a constrained sys-
tem for aspect-based sentiment analysis of these
online reviews. The system is constrained in the
sense that we only use the training data that was
provided by the challenge organizers and no other
external data sources. Our system performed rea-
sonably well, especially with a F
1
score of 75.61%
for the aspect category polarity subtask, 79.04%
F
1
score on the aspect category detection task and
66.61% F
1
score on the aspect term extraction
task.
2 Subtask 1: Aspect Term Extraction
Given a review with annotated entities in the train-
ing set, the task was to extract the aspect terms for
reviews in the test set. For this subtask, training,
development and testing were conducted for both
517
the laptop and the restaurant domain.
2.1 Features
Each review was represented as a feature vector
made up of the following features:
? Word N-grams: all unigrams, bigrams and
trigrams from the review text
? Casing: presence or absence of capital case/
title case words
? POS tags: POS tags of a word and its neigh-
bours
? Parse dependencies and relations: parse
dependency relations of the aspects, i.e.,
presence/absence of adjectives and adverbs in
the dependency parse tree
? Punctuation Marks: presence/absence of
punctuation marks, such as ?, !
2.2 Method
We approach the task by casting it as a sequence
tagging task where each token in a candidate sen-
tence is labelled as either Beginning, Inside or
Outside (BIO). We then employ conditional ran-
dom fields (CRF), which is a discriminative, prob-
abilistic model for sequence data with state-of-the-
art performance (Lafferty et al., 2001). A linear-
chain CRF tries to estimate the conditional prob-
ability of a label sequence y given the observed
features x, where each label y
t
is conditioned on
the previous label y
t?1
. In our case, we use BIO
CoNLL-style tags (Sang and De Meulder, 2003).
During development, we split the training data
in the ratio of 60:20:20 as training, development
(dev) and testing (dev-test). We train the CRF
model on the training set of the data, perform
feature selection based on the dev set, and test
the resulting model on the dev-test. In all ex-
periments, we use the CRF++
1
implementation
of conditional random fields with the parameter
c=4.0. This value was chosen based on manual
observation. We perform a feature ablation study
and the results are reported in Table 1. Features
listed in section 2.1 were those that were retained
for the final run.
1
code.google.com/p/crfpp/
3 Subtask 2: Aspect Term Polarity
Estimation
For this subtask, the training, development and
testing was done using reviews on laptops and
restaurants. Given the aspect terms in a sentence,
the task was to predict their sentiment polarities.
3.1 Features
For each review, we used the following features:
? Word N-grams: all lowercased unigrams,
bigrams and trigrams from the review text
? Polarity of neighbouring adjectives: ex-
tracted word sentiment from SentiWordNet
lexicon (Baccianella et al., 2010)
? Neighbouring POS tags: the POS tags of up
to neighbouring 3 words
? Parse dependencies and relations: parse
dependency relations of the aspects, i.e.,
presence/absence of adjectives and adverbs in
the dependency parse tree
3.2 Method
For each aspect term of a sentence, the afore-
mentioned features were extracted. For exam-
ple, for the term Sushi in the sentence Sushi
was delicious., the following feature vector is
constructed, {aspect: ?sushi?, advmod:?null?,
amod:?delicious?, uni sushi: 1, uni was: 1,
uni delicious, uni the: 0, .. }.
We then treat the aspect sentiment polarity es-
timation as a multi-class classification task where
each instance would be labelled as either positive,
negative or neutral. For the classification task, we
experimented with Naive Bayes and Support Vec-
tor Machines (SVM) ? both linear and RBF ker-
nels ? and it was observed that linear SVM per-
formed best. Hence, we use linear SVM for the
classification task. Table 2 summarizes the results
obtained from our experiments for various feature
combinations. The classifiers used are implemen-
tations from scikit-learn
2
, which is also used for
the remaining tasks.
4 Subtask3: Aspect Category Detection
Given a review with annotated entities or aspect
terms, the task was to predict the aspect categories.
2
scikit-learn.org/stable/
518
Features Precision Recall F1-Score
N-grams, POS tags 0.7655 0.4283 0.5496
N-grams, Parse relations, POS tags 0.8192 0.6641 0.7336
N-Grams, Parse relations, POS tags, casing 0.8101 0.6641 0.7299
N-grams, Parse relations, POS tags, ! 0.8116 0.6641 0.7305
N-grams, Parse relations, POS tags,!, ? 0.8123 0.6672 0.7326
Table 1: Training-phase experimental results for Subtask 1 on Restaurant reviews.
Features Laptops Restaurants
Neighbouring words, 2,3 POS grams, bigrams, trigrams, Sentiment,1,2 ngram lower 0.4196 0.5997
Parse Relations, 2,3 POS grams, bigrams, trigrams, Sentiment, 1,2 ngram lower 0.5869 0.6375
Parse Relations, Neighbouring words, bigram, trigrams, Sentiment, 1,2 ngram lower 0.5848 0.6380
Parse Relations, 2,3 POS grams, Neighbouring words, Sentiment, 1,2 ngram lower 0.5890 0.6240
Parse Relations, 2,3 POS grams , Neighbouring words, bigram, trigrams, 1,2 ngram lower 0.5626 0.6239
Parse Relations, 2,3 POS grams , Neighbouring words, bigram, trigrams, Sentiment 0.5922 0.6409
Table 2: Training-phase experimental results (Accuracy) for Subtask 2.
As one sentence in a review could belong to mul-
tiple aspect categories, we model the task as a
multi-label classification problem, i.e., given an
instance, predict all labels that the instance fits to.
4.1 Features
We experimented with different features, for ex-
ample unigrams, dependency tree relations, bi-
grams, POS tags and sentiment of the words (Sen-
tiWordNet), but using just the unigrams alone hap-
pened to yield the best result. The feature vector
was merely a bag-of-words vector indicating the
presence or absence of a word in an instance.
4.2 Method
The training instances were divided into 5 sets
based on the aspect categories and thereby, we
treated the multi-label classification task as 5 dif-
ferent binary classification tasks. Hence, we used
an ensemble of binary classifiers for the multi-
label classification. An SVM model was trained
using one classifier per class to distinguish it from
all other classes. For the binary classification
tasks, directly estimating a linear separating func-
tion (such as linear SVM) gave better results, as
shown in Table 3. Finally, the results of the 5 bi-
nary classifiers were combined to label the test in-
stance.
The category Miscellaneous was observed to
have the lowest accuracy, probably due to the fact
that miscellaneous captures all those aspects terms
that do not have a clearly defined category.
5 Subtask4 Aspect Category Polarity
Detection
For each review with pre-labelled aspect cate-
gories, the task was to produce a model which
predicts the sentiment polarity of each aspect cat-
egory.
5.1 Features
The training data contains reviews with the po-
larity for the corresponding aspect category. The
models performed best on using just unigram and
bigram features.
5.2 Method
The training instances were split into 5 sets based
on the aspect categories. We make use of the sen-
timent polarity classifier, as described in section
3.2, thereby, training one sentiment polarity classi-
fier for each aspect category. Table 4 indicates the
performance of different classifiers for this task,
using features as discussed in section 5.1.
6 Results
Table 5 gives an overview of the performance of
our system in this year?s task based on the offi-
cial scores from the organizers. We see that our
system performs relatively well for subtasks 1, 3
and 4, while for subtask 2 the F
1
scores are be-
hind the best system by about 12%. As observed,
a sentence could have more than one aspect and
each of these aspects could have different polar-
ities expressed. Including features that preserve
the context of the aspect could probably improve
the performance in the subtask 2. In most cases,
a simple set of features was enough to result in a
519
Restaurants Category Naive Bayes AdaBoost LinearSVC
Food 0.7130 0.8000 0.8470
Service 0.6064 0.9137 0.8997
Miscellaneous 0.6710 0.7490 0.7890
Ambience 0.6770 0.9063 0.8940
Price 0.7608 0.8548 0.9590
Table 3: Training-phase experimental results (F
1
score) for Subtask 3.
Restaurants Category Naive Bayes AdaBoost LinearSVC
Food 0.7136 0.6711 0.7417
Service 0.6733 0.5244 0.6688
Miscellaneous 0.4756 0.3170 0.4756
Ambience 0.6574 0.7232 0.6885
Price 0.7477 0.7752 0.6651
Table 4: Training-phase experimental results (F
1
score) for Subtask 4.
high F
1
score, for example, in subtask 3 a bag-of-
words feature set proved to yield a relatively high
F
1
score. In general, for the classification tasks,
we observe that the linear SVM performs best.
Subtask Dataset Best score Our score Rank
1 Laptops 74.55 66.61 8/27
1 Restaurants 84.01 77.88 12/29
2 Laptops 70.48 58.56 18/32
2 Restaurants 80.95 69.92 22/36
3 Restaurants 88.57 79.04 7/21
4 Restaurants 82.92 75.61 5/25
Table 5: Results (F
1
score and ranking) for the
Semeval-2014 test set.
7 Conclusion
In this paper, we have described the submission of
the SAP-RI team to the SemEval 2014 Task 4. We
model the classification tasks using linear SVM
and the term extraction task using CRF in order
to develop an aspect-based sentiment analysis sys-
tem that performs reasonably well.
Acknowledgement
The research is partially funded by the Economic
Development Board and the National Research
Foundation of Singapore.
References
Stefano Baccianella, Andrea Esuli, and Fabrizio Sebas-
tiani. 2010. SentiWordNet 3.0: An enhanced lexi-
cal resource for sentiment analysis and opinion min-
ing. In Proceedings of the Seventh Conference on
International Language Resources and Evaluation
(LREC?10), volume 10, pages 2200?2204.
Minqing Hu and Bing Liu. 2004. Mining and summa-
rizing customer reviews. In Proceedings of the tenth
ACM SIGKDD International Conference on Knowl-
edge Discovery and Data Mining, pages 168?177.
Niklas Jakob and Iryna Gurevych. 2010. Extracting
opinion targets in a single-and cross-domain setting
with conditional random fields. In Proceedings of
the 2010 Conference on Empirical Methods in Nat-
ural Language Processing, pages 1035?1045.
John Lafferty, Andrew McCallum, and Fernando
Pereira. 2001. Conditional random fields: Prob-
abilistic models for segmenting and labeling se-
quence data. In Proceedings of the Eighteenth In-
ternational Conference on Machine Learning, pages
282?289.
Bing Liu. 2010. Sentiment analysis and subjectiv-
ity. In Handbook of Natural Language Processing,
pages 627?666. Chapman & Hall, 2 edition.
Bing Liu. 2012. Sentiment analysis and opinion min-
ing. Synthesis Lectures on Human Language Tech-
nologies, 5(1):1?167.
Bin Lu, Myle Ott, Claire Cardie, and Benjamin Tsou.
2011. Multi-aspect sentiment analysis with topic
models. In Proceedings of Sentiment Elicitation
from Natural Text for Information Retrieval and Ex-
traction, pages 81?88.
Bo Pang and Lillian Lee. 2008. Opinion mining and
sentiment analysis. Foundations and trends in infor-
mation retrieval, 2(1-2):1?135.
Maria Pontiki, Dimitrios Galanis, John Pavlopou-
los, Haris Papageorgiou, Ion Androutsopoulos, and
Suresh Manandhar. 2014. SemEval-2014 Task 4:
Aspect based sentiment analysis. In Proceedings of
the 8th International Workshop on Semantic Evalu-
ation (SemEval 2014).
Erik Tjong Kim Sang and Fien De Meulder. 2003.
Introduction to the CoNLL-2003 shared task:
Language-independent named entity recognition. In
520
Proceedings of the Seventh Conference on Natural
Language Learning at HLT-NAACL, pages 142?147.
521
Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014), pages 522?526,
Dublin, Ireland, August 23-24, 2014.
SAP-RI: Twitter Sentiment Analysis in Two Days
Akriti Vij
1,2?
, Nishtha Malhotra
1,2?
, Naveen Nandan
1
and Daniel Dahlmeier
1
1
SAP Research & Innovation, Singapore
2
Nanyang Technological University, Singapore
{akriti.vij,nishtha.malhotra,naveen.nandan,d.dahlmeier}@sap.com
Abstract
We describe the submission of the SAP
Research & Innovation team to the Se-
mEval 2014 Task 9: Sentiment Analy-
sis in Twitter. We challenged ourselves
to develop a competitive sentiment anal-
ysis system within a very limited time
frame. Our submission was developed
in less than two days and achieved an
F
1
score of 77.26% for contextual polar-
ity disambiguation and 55.47% for mes-
sage polarity classification, which shows
that rapid prototyping of sentiment anal-
ysis systems with reasonable accuracy is
possible.
1 Introduction
Microblogging platforms and social networks
have become increasingly popular for expressing
opinions on a wide range of topics, hence mak-
ing them valuable and ever-growing logs of pub-
lic sentiment. This has motivated the development
of automatic natural language processing (NLP)
methods to analyse the sentiment expressed in
these short, informal messages (Liu, 2012; Pang
and Lee, 2008).
In particular, the study of sentiment and opin-
ions in messages from the Twitter microblogging
platform has attracted a lot of interest (Jansen et
al., 2009; Pak and Paroubek, 2010; Barbosa and
Feng, 2010; O?Connor et al., 2010; Bifet et al.,
2011). However, comparative evaluations of senti-
ment analysis of Twitter messages have previously
been hindered by the lack of a large benchmark
data set. The goal of the SemEval 2013 task 2:
Sentiment Analysis in Twitter (Nakov et al., 2013)
?
The work was done during an internship at SAP.
This work is licenced under a Creative Commons Attribution
4.0 International License. Page numbers and proceedings
footer are added by the organizers. License details: http:
//creativecommons.org/licenses/by/4.0/
and this year?s continuation in the SemEval 2014
task 9: Sentiment Analysis in Twitter (Rosenthal
et al., 2014) is to close this gap by hosting a shared
task competition which provided a large corpus of
Twitter messages which are annotated with sen-
timent polarity labels. The task consists of two
subtasks: in subtask A contextual polarity disam-
biguation, participants need to predict the polarity
of a given word or phrase in the context of a tweet
message, in subtask B message polarity classifica-
tion, participants need to predict the dominating
sentiment of the complete message. Both tasks
consider sentiment analysis to be a three-way clas-
sification problem between positive, negative, and
neutral sentiment.
In this paper, we describe the submission of the
SAP-RI team to the SemEval 2014 task 9. We
challenged ourselves to develop a competitive sen-
timent analysis system within a very limited time
frame. The complete system was implemented
within only two days. Our system is based on
supervised classification with support vector ma-
chines with lexical and dictionary-based features.
Our system achieved an F
1
score of 77.26% for
contextual polarity disambiguation and 55.47%
for message polarity classification. Although our
scores are about 10-20% behind the top-scoring
systems, we show that it is possible to develop
sentiment analysis systems via rapid prototyping
with reasonable accuracy in a very short amount
of time.
2 Methods
Our system is based on supervised classification
with support vector machines and a variety of lex-
ical and dictionary-based features. From the be-
ginning, we decided to restrict ourselves to super-
vised classification and to focus on the constrained
system setting. Experiments with more data or
semi-supervised learning would have required ad-
ditional time and the results of last year?s task
522
did not show any convincing improvements using
from additional unconstrained data (Nakov et al.,
2013). We cast sentiment analysis as a multi-class
classification problem with three classes: positive,
negative, and neutral. For the features, we tried to
re-implement most of the features from the NRC-
Canada system (Mohammad et al., 2013) which
was the best performing system in last year?s task.
We describe the features in the following sections.
2.1 Task A : Features
For the contextual polarity disambiguation task,
we extract features from the target phrase itself
and from a surrounding word window of four
words before and after the target phrase. To handle
negation, we append the suffix -neg to all words
in a negated context. A negated context includes
any word in the target phrase or context that is fol-
lowing a negation word
1
up to the next following
punctuation symbol.
? Word N-grams: all lowercased unigrams
and bigrams from the target phrase and the
context. We extract the lowercased full string
of the target phrase as an additional feature.
? Character N-grams: lowercased character
bigram and trigram prefixes and suffixes from
all words in the target phrase and the context.
? Elongations: binary feature that indicates the
presence of one or more words in the target
phrase or context that have a letter repeated
for 3 for more times e.g., coool.
? Emoticons: two binary features that indicate
the presence of positive or negative emoti-
cons in the target phrase or the context, re-
spectively. Two additional binary features
indicate the presence of positive or negative
emoticons at the end of the target phrase or
context
2
.
? Punctuation: three count features for the
number of tokens that consist only of excla-
mation marks, only of questions marks, or
a mix of exclamation marks and questions
marks, in the target phrase and context, re-
ceptively.
1
http://sentiment.christopherpotts.
net/lingstruc.html
2
positive emoticons: :-), :), :B, :-B, :3, =), <3, :D, :-D,
=D, :?), :d, ;), :}, :], :P, :-P, :-p, :p. negative emoticons: :-(,
:/, :{, :[, -.-, - -, :O, :o, :
?
(, :x, :X, v.v, ;(
? Casing: two binary features that indicate the
presence of at least one all upper-case word
and at least one title-cased word in the target
phrase or context, respectively.
? Stop words: a binary feature that indicates if
all the words in the target phrase or context
are stop words. If so, an additional feature
indicates the number of stop words: 1, 2, 3,
or more stop words.
? Length: the number of tokens in the target
phrase and the context, plus a binary feature
that indicates the presence of any word with
more than three characters.
? Position: three binary features that indicate
whether a target phrase is at the beginning, in
the middle, or at the end of the tweet.
? Hashtags: all hashtags in the target phrase
or the context. To handle hashtags which are
formed by concatenating words, e.g., #ihate-
mondays, we additionally split hashtags us-
ing a simple dictionary-based approach and
add each token of the segmented hashtag as
an additional features.
? Twitter user: binary feature that indicates
whether the context or the target phrase con-
tain a mention of a Twitter user.
? URL: binary feature that indicates whether
the context or the target phrase contains a
URL.
? Brown cluster: the word cluster index for
each word in the context. Cluster indexes are
obtained from the Brown word clusters of the
ARK Twitter tagger (Owoputi et al., 2013).
? Sentiment lexicons: we add the follow-
ing sentiment dictionary features for the tar-
get phrase and the context for four differ-
ent sentiment lexicons (NRC sentiment lex-
icon, NRC Hashtag lexicon (Mohammad et
al., 2013), MPQA sentiment lexicon (Wilson
et al., 2005), and Bing Liu lexicon (Hu and
Liu, 2004)):
? the count of words with positive senti-
ment score.
? the sum of the sentiment scores for all
words.
523
? the maximum non-negative sentiment
score for any word.
? the sentiment score of the last word with
positive sentiment score.
We extract these features for both the target
phrase and the context. For words that are
marked as negated, the sign of the sentiment
scores flipped. The MPQA lexicons requires
part of speech information. We use the ARK
Twitter part-of-speech tagger (Owoputi et al.,
2013) to tag the input with part of speech
tags.
2.2 Task B : Features
For the message polarity task, we extract features
from the entire tweet message. The features are
similar to the features for phrase polarity disam-
biguation. As before we handle negation by ap-
pending the suffix -neg to all words that appear in
a negated context.
? Word N-grams: all lowercased N-grams for
N=1, . . . , 4 from the message. We also in-
clude ?skipgrams? for each N-gram by re-
placing each token in the N-gram with a as-
terisk place holder, e.g., the cat ? * cat,
the *.
? Character N-grams: lowercased charac-
ter level N-grams for N=3, . . . , 5 for all the
words in the message. Character N-grams do
not cross word boundaries.
? Elongations: count of words in the message
which have a letter repeated for 3 for more
times.
? Emoticons: similar to the contextual polarity
disambiguation task: two binary features for
presence of positive or negative emoticons in
the message and two binary features indicate
the presence of positive or negative emoti-
cons at the end of the message.
? Punctuation: similar to the contextual polar-
ity disambiguation task: three count features
for the number of tokens that consist only of
exclamation marks, only of questions marks,
or a mix of exclamation marks and questions
marks.
? Hashtags: all hashtags in the message. We
do not split concatonated hashtags.
# Tokens # Tweets
Subtask A
Training (SemEval 2014 train) 160,992 7,884
Development (SemEval 2013 test) 76,409 3,710
Subtask B
Training (SemEval 2014 train) 139,128 7,112
Development (SemEval 2013 test) 47,052 2,405
Table 1: Overview of the data sets.
? Casing: the count of all upper-case words in
the message.
? Brown cluster: similar to the contextual po-
larity disambiguation task: the cluster index
for each word in the message.
3 Experiment and Results
In this section, we report experimental result for
our method. We used the scikit-learn Python ma-
chine learning library (Pedregosa et al., 2011) to
implement the feature extraction pipeline and the
support vector machine classifier. We use a linear
kernel for the support vector machine and fixed the
SVM hyper-parameter C to 1.0. We found that
scikit-learn allowed us to implement the system
faster and resulted in much more compact code
than other machine learning tools we have worked
with in the past.
We used the official training set provided for the
SemEval 2014 task to train our system and evalu-
ated on the test set of the SemEval 2013 task which
served as development data for this year?s task
3
.
Tweets in the training data that were not available
any more through the Twitter API were removed
from the training set. An overview of the data sets
is shown in Table 1. For the evaluation, we com-
pute precision, recall and F
1
measure for the pos-
itive, negative, and neutral sentiment tweets. Fol-
lowing the official evaluation metric, the overall
precision, recall, and F
1
measure of the system is
the average of the precision, recall, and F
1
mea-
sures for positive and negative sentiment, respec-
tively.
Here, we report a feature ablation study: we
omitted each individual feature category from the
complete feature set to determine its influence on
the overall performance. Table 2 summarizes the
results for subtask A and B. Surprisingly many of
the features do not result in a reduction of the F
1
score when removed, or even increase the score,
3
We also did some experiments with a 60:40 training/test
split of the SemEval 2014 training data which showed com-
parable results
524
Features Positive Negative Neutral Overall
Subtask A P R F
1
P R F
1
P R F
1
P R F
1
All features 0.86 0.87 0.86 0.78 0.78 0.78 0.23 0.13 0.17 0.82 0.83 0.82
w/o Word N-grams 0.84 0.82 0.83 0.71 0.74 0.72 0.14 0.16 0.15 0.77 0.78 0.78
w/o character N-grams 0.85 0.89 0.87 0.80 0.78 0.79 0.27 0.12 0.17 0.82 0.83 0.83
w/o elongation 0.86 0.87 0.86 0.78 0.78 0.78 0.23 0.13 0.17 0.81 0.82 0.81
w/o emoticons 0.85 0.87 0.86 0.78 0.78 0.78 0.24 0.14 0.18 0.82 0.83 0.82
w/o punctuation 0.86 0.87 0.86 0.78 0.78 0.78 0.23 0.13 0.17 0.81 0.83 0.82
w/o casing 0.86 0.87 0.87 0.78 0.78 0.78 0.23 0.13 0.17 0.82 0.83 0.82
w/o stop words 0.86 0.87 0.86 0.78 0.79 0.78 0.24 0.15 0.18 0.82 0.83 0.82
w/o length 0.86 0.87 0.86 0.78 0.78 0.78 0.23 0.14 0.17 0.82 0.83 0.82
w/o position 0.86 0.87 0.86 0.77 0.78 0.78 0.24 0.13 0.17 0.81 0.83 0.82
w/o hashtags 0.86 0.87 0.87 0.78 0.78 0.78 0.24 0.14 0.18 0.82 0.83 0.82
w/o twitter user 0.86 0.87 0.86 0.78 0.78 0.78 0.23 0.13 0.17 0.82 0.83 0.82
w/o URL 0.86 0.87 0.86 0.78 0.78 0.78 0.23 0.13 0.17 0.81 0.82 0.81
w/o Brown cluster 0.86 0.88 0.87 0.78 0.80 0.79 0.25 0.13 0.17 0.82 0.84 0.83
w/o Sentiment lexicon 0.81 0.84 0.82 0.70 0.68 0.69 0.16 0.09 0.11 0.75 0.76 0.76
Subtask B
All features 0.81 0.54 0.65 0.66 0.34 0.44 0.59 0.89 0.71 0.74 0.44 0.54
w/o word N-grams 0.73 0.59 0.65 0.52 0.46 0.49 0.61 0.75 0.67 0.62 0.52 0.57
w/o character N-grams 0.80 0.49 0.61 0.65 0.23 0.34 0.56 0.90 0.69 0.72 0.36 0.48
w/o elongation 0.81 0.54 0.65 0.66 0.34 0.44 0.59 0.89 0.71 0.74 0.44 0.55
w/o emoticons 0.82 0.54 0.65 0.66 0.33 0.44 0.59 0.89 0.72 0.74 0.44 0.55
w/o punctuation 0.81 0.54 0.65 0.66 0.34 0.45 0.59 0.89 0.71 0.74 0.44 0.55
w/o casing 0.81 0.54 0.65 0.66 0.33 0.44 0.59 0.89 0.71 0.74 0.44 0.55
w/o hashtags 0.82 0.54 0.65 0.65 0.33 0.44 0.59 0.89 0.71 0.74 0.44 0.54
w/o Brown cluster 0.81 0.54 0.65 0.65 0.33 0.44 0.59 0.89 0.71 0.73 0.43 0.54
Table 2: Experimental Results for feature ablation study. Each row shows the precision, recall, and F
1
score for the positive, negative, and neutral class and the overall precision, recall, and F
1
score after
removing the particular feature from the features set.
although not significantly. The most effective fea-
tures are word N-grams and the sentiment lexi-
cons. It is interesting that the performance for the
neutral class is very low for subtask A and high
for subtask B. We can also see that for subtask B,
our system clearly has a problem with recall for
the positive and negative sentiment.
For the performance of our system in the Se-
mEval 2014 shared task, we report the official
overall F
1
scores of our system as released by the
organizers on the official test set in Table 3. The
scores were reported separately for different test
sets: the SemEval 2013 Twitter test set, a new Se-
mEval 2014 Twitter test set, a new test set from
LiveJournal blogs, the SMS test set from the NUS
SMS corpus (Chen and Kan, 2012), and a new
test set of sarcastic tweets. We also include the F
1
score of the best participating system for each test
set and the rank of our system among all partic-
ipating systems. The results of our system were
fairly robust across different domains, with the
exception of messages containing sarcasm which
shows understanding sarcasm requires a deeper
and more subtle understanding of the text that is
not captured well in a simple linear model.
Dataset Best score Our score Rank
Subtask A
LiveJournal 2014 85.61 77.68 18 / 27
SMS 2013 89.31 80.26 13 / 27
Twitter 2013 90.14 80.32 17 / 27
Twitter 2014 86.63 77.26 15 / 27
Twitter 2014 Sarcasm 82.75 70.64 14 / 27
Subtask B
LiveJournal 2014 74.84 57.86 33 / 42
SMS 2013 70.28 49.00 34 / 42
Twitter 2013 72.12 50.18 37 / 42
Twitter 2014 70.96 55.47 32 / 42
Twitter 2014 Sarcasm 58.16 48.64 15 / 42
Table 3: Official results for Semeval 2014 test set.
Reported scores are overall F
1
scores.
4 Conclusion
In this paper, we have described the submission of
the SAP-RI team to the SemEval 2014 task 9. We
showed that is possible to develop sentiment anal-
ysis systems via rapid prototyping with reasonable
accuracy within a couple of days.
Acknowledgement
The research is partially funded by the Economic
Development Board and the National Research
Foundation of Singapore.
525
References
Luciano Barbosa and Junlan Feng. 2010. Robust sen-
timent detection on Twitter from biased and noisy
data. In Proceedings of the 23rd International Con-
ference on Computational Linguistics, pages 36?44.
Albert Bifet, Geoffrey Holmes, Bernhard Pfahringer,
and Ricard Gavalda. 2011. Detecting sentiment
change in Twitter streaming data. Journal of Ma-
chine LearningResearch - Proceedings Track, 17:5?
11.
Tao Chen and Min-Yen Kan. 2012. Creating a live,
public short message service corpus: the NUS SMS
corpus. Language Resources and Evaluation, pages
1?37.
Minqing Hu and Bing Liu. 2004. Mining and summa-
rizing customer reviews. In Proceedings of the 10th
ACM SIGKDD International Conference on Knowl-
edge Discovery and Data Mining, pages 168?177.
Bernhard J. Jansen, Mimi Zhang, Kate Sobel, and Ab-
dur Chowdury. 2009. Twitter power: Tweets as
electronic word of mouth. J. Am. Soc Inf. Sci. Tech-
nol., 60(11):2169?2188.
Bing Liu. 2012. Sentiment analysis and opinion min-
ing. Synthesis Lectures on Human Language Tech-
nologies, 5(1):1?167.
Saif Mohammad, Svetlana Kiritchenko, and Xiaodan
Zhu. 2013. NRC-Canada: Building the state-of-
the-art in sentiment analysis of Tweets. In Proceed-
ings of the 7th International Workshop on Semantic
Evaluation, pages 321?327.
Preslav Nakov, Zornitsa Kozareva, Alan Ritter, Sara
Rosenthal, Veselin Stoyanov, and Theresa Wilson.
2013. Semeval-2013 task 2: Sentiment analysis
in Twitter. In Proceedings of the 7th International
Workshop on Semantic Evaluation, pages 312?320.
Brendan O?Connor, Routledge Bryan R. Balasubra-
manyan, Ramnath, and Noah A. Smith. 2010. From
Tweets to polls: Linking text sentiment to public
opinion time series. In Proceedings of the Fourth In-
ternational Conference on Weblogs and Social Me-
dia (ICWSM ?10), pages 122?129.
Olutobi Owoputi, Brendan O?Connor, Chris Dyer,
Kevin Gimpel, Nathan Schneider, and Noah A.
Smith. 2013. Improved part-of-speech tagging for
online conversational text with word clusters. In
Proceedings of the 2013 Conference of the North
American Chapter of the Association for Computa-
tional Linguistics: Human Language Technologies
(NAACL-HLT), pages 380?390.
Alexander Pak and Patrick Paroubek. 2010. Twitter
based system: Using Twitter to disambiguating sen-
timent ambiguous adjectives. In Proceedings of the
8th International Workshop on Semantic Evaluation,
pages 436?439.
Bo Pang and Lillian Lee. 2008. Opinion mining and
sentiment analysis. Foundations and trends in infor-
mation retrieval, 2(1-2):1?135.
Fabian Pedregosa, Ga?el Varoquaux, Alexandre Gram-
fort, Vincent Michel, Bertrand Thirion, Oliver
Grisel, Mathieu Blondel, Peter. Prettenhofer, Ron
Weiss, Vincent Dubourg, Jake Vanderplas, Alexan-
dre Passos, David Cournapeau, Matthieu Brucher,
Matthieu Perrot, and
?
Edouard Duchesnay. 2011.
Scikit-learn: Machine learning in Python. Journal
of Machine Learning Research, 12:2825?2830.
Sara Rosenthal, Preslav Nakov, Alan Ritter, and
Veselin Stoyanov. 2014. SemEval-2014 Task 9:
Sentiment Analysis in Twitter. In Preslav Nakov and
Torsten Zesch, editors, Proceedings of the 8th In-
ternational Workshop on Semantic Evaluation, Se-
mEval ?14, Dublin, Ireland.
Theresa Wilson, Janyce Wiebe, and Paul Hoffmann.
2005. Recognizing contextual polarity in phrase-
level sentiment analysis. In Proceedings of Human
Language Technology and Empirical Methods in
Natural Language Processing (HLT-EMNLP), pages
307?314.
526
Proceedings of the Joint 5th Workshop on Statistical Machine Translation and MetricsMATR, pages 354?359,
Uppsala, Sweden, 15-16 July 2010. c?2010 Association for Computational Linguistics
TESLA: Translation Evaluation of Sentences with
Linear-programming-based Analysis
Chang Liu1 and Daniel Dahlmeier2 and Hwee Tou Ng1,2
1Department of Computer Science, National University of Singapore
2NUS Graduate School for Integrative Sciences and Engineering
{liuchan1,danielhe,nght}@comp.nus.edu.sg
Abstract
We present TESLA-M and TESLA, two
novel automatic machine translation eval-
uation metrics with state-of-the-art perfor-
mances. TESLA-M builds on the suc-
cess of METEOR and MaxSim, but em-
ploys a more expressive linear program-
ming framework. TESLA further exploits
parallel texts to build a shallow seman-
tic representation. We evaluate both on
the WMT 2009 shared evaluation task and
show that they outperform all participating
systems in most tasks.
1 Introduction
In recent years, many machine translation (MT)
evaluation metrics have been proposed, exploiting
varying amounts of linguistic resources.
Heavyweight linguistic approaches including
RTE (Pado et al, 2009) and ULC (Gim?nez and
M?rquez, 2008) performed the best in the WMT
2009 shared evaluation task. They exploit an ex-
tensive array of linguistic features such as parsing,
semantic role labeling, textual entailment, and dis-
course representation, which may also limit their
practical applications.
Lightweight linguistic approaches such as ME-
TEOR (Banerjee and Lavie, 2005), MaxSim
(Chan and Ng, 2008), wpF and wpBleu (Popovic?
and Ney, 2009) exploit a limited range of linguis-
tic information that is relatively cheap to acquire
and to compute, including lemmatization, part-of-
speech (POS) tagging, and synonym dictionaries.
Non-linguistic approaches include BLEU (Pap-
ineni et al, 2002) and its variants, TER (Snover et
al., 2006), among others. They operate purely at
the surface word level and no linguistic resources
are required. Although still very popular with MT
researchers, they have generally shown inferior
performances than the linguistic approaches.
We believe that the lightweight linguistic ap-
proaches are a good compromise given the current
state of computational linguistics research and re-
sources. In this paper, we devise TESLA-M and
TESLA, two lightweight approaches to MT eval-
uation. Specifically: (1) the core features are F-
measures derived by matching bags of N-grams;
(2) both recall and precision are considered, with
more emphasis on recall; and (3) WordNet syn-
onyms feature prominently.
The main novelty of TESLA-M compared to
METEOR and MaxSim is that we match the N-
grams under a very expressive linear programming
framework, which allows us to assign weights to
the N-grams. This is in contrast to the greedy ap-
proach ofMETEOR, and the more restrictive max-
imum bipartite matching formulation of MaxSim.
In addition, we present a heavier version
TESLA, which combines the features using a lin-
ear model trained on development data, making
it easy to exploit features not on the same scale,
and leaving open the possibility of domain adapta-
tion. It also exploits parallel texts of the target lan-
guage with other languages as a shallow semantic
representation, which allows us to model phrase
synonyms and idioms. In contrast, METEOR and
MaxSim are capable of processing only word syn-
onyms from WordNet.
The rest of this paper is organized as follows.
Section 2 gives a high level overview of the eval-
uation task. Sections 3 and 4 describe TESLA-M
and TESLA, respectively. Section 5 presents ex-
perimental results in the setting of the WMT 2009
shared evaluation task. Finally, Section 6 con-
cludes the paper.
2 Overview
We consider the task of evaluating machine trans-
lation systems in the direction of translating the
source language to the target language. Given a
reference translation and a system translation, the
354
goal of an automatic machine translation evalua-
tion algorithm such as TESLA(-M) is to output a
score predicting the quality of the system transla-
tion. Neither TESLA-M nor TESLA requires the
source text, but as additional linguistic resources,
TESLAmakes use of phrase tables generated from
parallel texts of the target language and other lan-
guages, which we refer to as pivot languages. The
source language may or may not be one of the
pivot languages.
3 TESLA-M
This section describes TESLA-M, the lighter
one among the two metrics. At the highest
level, TESLA-M is the arithmetic average of F-
measures between bags of N-grams (BNGs). A
BNG is a multiset of weighted N-grams. Math-
ematically, a BNG B consists of tuples (bi, bWi ),
where each bi is an N-gram and bWi is a posi-
tive real number representing its weight. In the
simplest case, a BNG contains every N-gram in a
translated sentence, and the weights are just the
counts of the respective N-grams. However, to
emphasize the content words over the function
words, we discount the weight of an N-gram by
a factor of 0.1 for every function word in the N-
gram. We decide whether a word is a function
word based on its POS tag.
In TESLA-M, the BNGs are extracted in the tar-
get language, so we call them bags of target lan-
guage N-grams (BTNGs).
3.1 Similarity functions
To match two BNGs, we first need a similarity
measure between N-grams. In this section, we
define the similarity measures used in our exper-
iments.
We adopt the similarity measure from MaxSim
as sms. For unigrams x and y,
? If lemma(x) = lemma(y), then sms = 1.
? Otherwise, let
a = I(synsets(x) overlap with synsets(y))
b = I(POS(x) = POS(y))
where I(?) is the indicator function, then
sms = (a + b)/2.
The synsets are obtained by querying WordNet
(Fellbaum, 1998). For languages other than En-
glish, a synonym dictionary is used instead.
We define two other similarity functions be-
tween unigrams:
slem(x, y) = I(lemma(x) = lemma(y))
spos(x, y) = I(POS(x) = POS(y))
All the three unigram similarity functions general-
ize to N-grams in the same way. For two N-grams
x = x1,2,...,n and y = y1,2,...,n,
s(x, y) =
{
0 if ?i, s(xi, yi) = 0
1
n
?n
i=1 s(x
i, yi) otherwise
3.2 Matching two BNGs
Now we describe the procedure of matching two
BNGs. We take as input the following:
1. Two BNGs, X and Y . The ith entry in X
is xi and has weight xWi (analogously for yj
and yWj ).
2. A similarity measure, s, that gives a similar-
ity score between any two entries in the range
of 0 to 1.
Intuitively, we wish to align the entries of the two
BNGs in a way that maximizes the overall simi-
larity. As translations often contain one-to-many
or many-to-many alignments, we allow one entry
to split its weight among multiple alignments. An
example matching problem is shown in Figure 1a,
where the weight of each node is shown, along
with the similarity for each edge. Edges with a
similarity of zero are not shown. The solution to
the matching problem is shown in Figure 1b, and
the overall similarity is 0.5 ? 1.0 + 0.5 ? 0.6 +
1.0 ? 0.2 + 1.0 ? 0.1 = 1.1.
Mathematically, we formulate this as a (real-
valued) linear programming problem1. The vari-
ables are the allocated weights for the edges
w(xi, yj) ?i, j
We maximize
?
i,j
s(xi, yj)w(xi, yj)
subject to
w(xi, yj) ? 0 ?i, j
?
j
w(xi, yj) ? x
W
i ?i
?
i
w(xi, yj) ? y
W
j ?j
1While integer linear programming is NP-complete, real-
valued linear programming can be solved efficiently.
355
w=1.0 w=0.8 w=0.2 w=0.1
w=1.0 w=0.8 w=0.1
.2
s=0.5 s=1.0s=0.5 s=1.0
(a) The matching problem
w=1.0 w=0.8 w=0.2 w=0.1
w=1.0 w=0.8 w=0.1
.2
w=1.0 w=0.2w=0.s w=0.1
(b) The solution
Figure 1: A BNG matching problem
The value of the objective function is the overall
similarity S. Assuming X is the reference and Y
is the system translation, we have
Precision =
S
?
j y
W
j
Recall =
S
?
i x
W
i
The F-measure is derived from the precision and
the recall:
F =
Precision ? Recall
?? Precision + (1 ? ?) ? Recall
In this work, we set ? = 0.8, following MaxSim.
The value gives more importance to the recall than
the precision.
3.3 Scoring
The TESLA-M sentence-level score for a refer-
ence and a system translation is the arithmetic av-
erage of the BTNG F-measures for unigrams, bi-
grams, and trigrams based on similarity functions
sms and spos. We thus have 3? 2 = 6 features for
TESLA-M.
We can compute a system-level score for a ma-
chine translation system by averaging its sentence-
level scores over the complete test set.
3.4 Reduction
When every xWi and y
W
j is 1, the linear program-
ming problem proposed above reduces toweighted
bipartite matching. This is a well known result;
see for example, Cormen et al (2001) for details.
This is the formalism of MaxSim, which precludes
the use of fractional weights.
If the similarity function is binary-valued
and transitive, such as slem and spos, then
we can use a much simpler and faster greedy
matching procedure: the best match is simply
?
g min(
?
xi=g
xWi ,
?
yi=g
yWi ).
4 TESLA
Unlike the simple arithmetic average used in
TESLA-M, TESLA uses a general linear com-
bination of three types of features: BTNG F-
measures as in TESLA-M, F-measures between
bags of N-grams in each of the pivot languages,
called bags of pivot language N-grams (BPNGs),
and normalized language model scores of the sys-
tem translation, defined as 1n logP , where n is
the length of the translation, and P the language
model probability. The method of training the lin-
ear model depends on the development data. In
the case of WMT, the development data is in the
form of manual rankings, so we train SVM rank
(Joachims, 2006) on these instances to build the
linear model. In other scenarios, some form of re-
gression can be more appropriate.
The rest of this section focuses on the genera-
tion of the BPNGs. Their matching is done in the
same way as described for BTNGs in the previous
section.
4.1 Phrase level semantic representation
Given a sentence-aligned bitext between the target
language and a pivot language, we can align the
text at the word level using well known tools such
as GIZA++ (Och and Ney, 2003) or the Berkeley
aligner (Liang et al, 2006; Haghighi et al, 2009).
We observe that the distribution of aligned
phrases in a pivot language can serve as a se-
mantic representation of a target language phrase.
That is, if two target language phrases are often
aligned to the same pivot language phrase, then
they can be inferred to be similar in meaning.
Similar observations have been made by previous
researchers (Bannard and Callison-Burch, 2005;
Callison-Burch et al, 2006; Snover et al, 2009).
We note here two differences from WordNet
synonyms: (1) the relationship is not restricted to
the word level only, and (2) the relationship is not
binary. The degree of similarity can be measured
by the percentage of overlap between the seman-
tic representations. For example, at the word level,
356
the phrases good morning and hello are unrelated
even with a synonym dictionary, but they both very
often align to the same French phrase bonjour, and
we conclude they are semantically related to a high
degree.
4.2 Segmenting a sentence into phrases
To extend the concept of this semantic represen-
tation of phrases to sentences, we segment a sen-
tence in the target language into phrases. Given a
phrase table, we can approximate the probability
of a phrase p by:
Pr(p) =
N(p)
?
p? N(p
?)
(1)
where N(?) is the count of a phrase in the phrase
table. We then define the likelihood of seg-
menting a sentence S into a sequence of phrases
(p1, p2, . . . , pn) by:
Pr(p1, p2, . . . , pn|S) =
1
Z(S)
n?
i=1
Pr(pi) (2)
where Z(S) is a normalizing constant. The seg-
mentation of S that maximizes the probability can
be determined efficiently using a dynamic pro-
gramming algorithm. The formula has a strong
preference for longer phrases, as every Pr(p) is
a small fraction. To deal with out-of-vocabulary
(OOV) words, we allow any single word w to be
considered a phrase, and if N(w) = 0, we set
N(w) = 0.5 instead.
4.3 BPNGs as sentence level semantic
representation
Simply merging the phrase-level semantic rep-
resentation is insufficient to produce a sensible
sentence-level semantic representation. As an ex-
ample, we consider two target language (English)
sentences segmented as follows:
1. ||| Hello , ||| Querrien ||| . |||
2. ||| Morning , sir . |||
A naive comparison of the bags of aligned pivot
language (French) phrases would likely conclude
that the two sentences are completely unrelated,
as the bags of aligned phrases are likely to be
completely disjoint. We tackle this problem by
constructing a confusion network representation
of the aligned phrases, as shown in Figures 2 and
w=1.=082s252

02s252

0881252
252

Figure 2: A confusion network as a semantic rep-
resentation
w=1.=082s25=1
08222
Figure 3: A degenerate confusion network as a se-
mantic representation
3. A confusion network is a compact representa-
tion of a potentially exponentially large number of
weighted and likely malformed French sentences.
We can collect the N-gram statistics of this ensem-
ble of French sentences efficiently from the confu-
sion network representation. For example, the tri-
gram Bonjour , Querrien 2 would receive a weight
of 0.9 ? 1.0 = 0.9 in Figure 2. As with BTNGs,
we discount the weight of an N-gram by a factor
of 0.1 for every function word in the N-gram, so
as to place more emphasis on the content words.
The collection of all such N-grams and their
corresponding weights forms the BPNG of a sen-
tence. The reference and system BPNGs are then
matched using the algorithm outlined in Section
3.2.
4.4 Scoring
The TESLA sentence-level score is a linear com-
bination of (1) BTNG F-measures for unigrams,
bigrams, and trigrams based on similarity func-
tions sms and spos, (2) BPNG F-measures for un-
igrams, bigrams, and trigrams based on similar-
ity functions slem and spos for each pivot lan-
guage, and (3) normalized language model scores.
In this work, we use two language models. We
thus have 3 ? 2 features from the BTNGs, 3 ?
2 ? #pivot languages features from the BPNGs, and
2 features from the language models. Again, we
can compute system-level scores by averaging the
sentence-level scores.
5 Experiments
5.1 Setup
We test our metrics in the setting of the WMT
2009 evaluation task (Callison-Burch et al, 2009).
The manual judgments from WMT 2008 are used
2Note that the N-gram can span more than one segment.
357
as the development data and the metric is evalu-
ated on WMT 2009 manual judgments with re-
spect to two criteria: sentence level consistency
and system level correlation.
The sentence level consistency is defined as the
percentage of correctly predicted pairs among all
the manually judged pairs. Pairs judged as ties
by humans are excluded from the evaluation. The
system level correlation is defined as the average
Spearman?s rank correlation coefficient across all
translation tracks.
5.2 Pre-processing
We POS tag and lemmatize the texts using the fol-
lowing tools: for English, OpenNLP POS-tagger3
and WordNet lemmatizer; for French and German,
TreeTagger4; for Spanish, the FreeLing toolkit
(Atserias et al, 2006); and for Czech, the Morce
morphological tagger5.
For German, we additionally perform noun
compound splitting. For each noun, we choose the
split that maximizes the geometric mean of the fre-
quency counts of its parts, following the method in
(Koehn and Knight, 2003):
max
n,p1,p2,...,pn
[
n?
i=1
N(pi)
] 1
n
The resulting compound split sentence is then POS
tagged and lemmatized.
Finally, we remove all non-alphanumeric tokens
from the text in all languages. To generate the lan-
guage model features, we train SRILM (Stolcke,
2002) trigram models with modified Kneser-Ney
discounting on the supplied monolingual Europarl
and news commentary texts.
We build phrase tables from the supplied news
commentary bitexts. Word alignments are pro-
duced by the Berkeley aligner. The widely used
phrase extraction heuristic in (Koehn et al, 2003)
is used to extract phrase pairs and phrases of up to
4 words are collected.
5.3 Into-English task
For each of the BNG features, we generate three
scores, for unigrams, bigrams, and trigrams re-
spectively. For BPNGs, we generate one such
triple for each of the four pivot languages supplied,
namely Czech, French, German, and Spanish.
3opennlp.sourceforge.net
4www.ims.uni-stuttgart.de/projekte/corplex/TreeTagger
5ufal.mff.cuni.cz/morce/index.php
System
correlation
Sentence
consistency
TESLA 0.8993 0.6324
TESLA-M 0.8718 0.6097
ulc 0.83 0.63
maxsim 0.80 0.62
meteor-0.6 0.72 0.50
Table 1: Into-English task on WMT 2009 data
Table 1 compares the scores of TESLA and
TESLA-M against three participants in WMT
2009 under identical settings6: ULC (a heavy-
weight linguistic approach with the best per-
formance in WMT 2009), MaxSim, and ME-
TEOR. The results show that TESLA outperforms
all these systems by a substantial margin, and
TESLA-M is very competitive too.
5.4 Out-of-English task
A synonym dictionary is required for target lan-
guages other than English. We use the freely avail-
able Wiktionary dictionary7 for each language.
For Spanish, we additionally use the Spanish
WordNet, a component of FreeLing.
Only one pivot language (English) is used for
the BPNG. For the English-Czech task, we only
have one language model instead of two, as the
Europarl language model is not available.
Tables 2 and 3 show the sentence-level consis-
tency and system-level correlation respectively of
TESLA and TESLA-M against the best reported
results in WMT 2009 under identical setting. The
results show that both TESLA and TESLA-M
give very competitive performances. Interestingly,
TESLA and TESLA-M obtain similar scores in the
out-of-English task. This could be because we use
only one pivot language (English), compared to
four in the into-English task. We plan to inves-
tigate this phenomenon in our future work.
6 Conclusion
This paper describes TESLA-M and TESLA. Our
main contributions are: (1) we generalize the
bipartite matching formalism of MaxSim into a
more expressive linear programming framework;
6The original WMT09 report contained erroneous results.
The scores here are the corrected results released after publi-
cation.
7www.wiktionary.org
358
en-fr en-de en-es en-cz Overall
TESLA 0.6828 0.5734 0.5940 0.5519 0.5796
TESLA-M 0.6390 0.5890 0.5927 0.5656 0.5847
wcd6p4er 0.67 0.58 0.61 0.59 0.60
wpF 0.66 0.60 0.61 n/a 0.61
terp 0.62 0.50 0.54 0.31 0.43
Table 2: Out-of-English task sentence-level con-
sistency on WMT 2009 data
en-fr en-de en-es en-cz Overall
TESLA 0.8529 0.7857 0.7272 0.3141 0.6700
TESLA-M 0.9294 0.8571 0.7909 0.0857 0.6657
wcd6p4er -0.89 0.54 -0.45 -0.1 -0.22
wpF 0.90 -0.06 0.58 n/a n/a
terp -0.89 0.03 -0.58 -0.40 -0.46
Table 3: Out-of-English task system-level correla-
tion on WMT 2009 data
(2) we exploit parallel texts to create a shallow se-
mantic representation of the sentences; and (3) we
show that they outperform all participants in most
WMT 2009 shared evaluation tasks.
Acknowledgments
This research was done for CSIDM Project No.
CSIDM-200804 partially funded by a grant from
the National Research Foundation (NRF) ad-
ministered by the Media Development Authority
(MDA) of Singapore.
References
J. Atserias, B. Casas, E. Comelles, M. Gonz?lez,
L. Padr?, and M. Padr?. 2006. Freeling 1.3: Syn-
tactic and semantic services in an open-source NLP
library. In Proceedings of LREC.
S. Banerjee and A. Lavie. 2005. METEOR: An auto-
matic metric for MT evaluation with improved cor-
relation with human judgments. In Proceedings of
the ACL Workshop on Intrinsic and Extrinsic Eval-
uation Measures for Machine Translation and/or
Summarization.
C. Bannard and C. Callison-Burch. 2005. Paraphras-
ing with bilingual parallel corpora. In Proceedings
of ACL.
C. Callison-Burch, P. Koehn, and M. Osborne. 2006.
Improved statistical machine translation using para-
phrases. In Proceedings of HLT-NAACL.
C. Callison-Burch, P. Koehn, C. Monz, and
J. Schroeder. 2009. Findings of the 2009
Workshop on Statistical Machine Translation. In
Proceedings of WMT.
Y.S. Chan and H.T. Ng. 2008. MAXSIM: A maximum
similarity metric for machine translation evaluation.
In Proceedings of ACL.
T. Cormen, C.E. Leiserson, R.L. Rivest, and C. Stein,
2001. Introduction to Algorithms. MIT Press, Cam-
bridge, MA.
C. Fellbaum, editor. 1998. WordNet: An electronic
lexical database. MIT Press, Cambridge, MA.
J. Gim?nez and L. M?rquez. 2008. A smorgasbord of
features for automatic MT evaluation. In Proceed-
ings of the Third WMT.
A. Haghighi, J. Blitzer, J. DeNero, and D. Klein. 2009.
Better word alignments with supervised ITG mod-
els. In Proceedings of ACL-IJCNLP.
T. Joachims. 2006. Training linear svms in linear time.
In Proceedings of KDD.
P. Koehn and K. Knight. 2003. Empirical methods for
compound splitting. In Proceedings of EACL.
P. Koehn, F.J. Och, and D. Marcu. 2003. Statistical
phrase-based translation. In Proceedings of HLT-
NAACL.
P. Liang, B. Taskar, and D. Klein. 2006. Alignment by
agreement. In Proceedings of HLT-NAACL.
F.J. Och and N. Ney. 2003. A systematic comparison
of various statistical alignment models. Computa-
tional Linguistics, 29(1).
S. Pado, M. Galley, D. Jurafsky, and C.D. Man-
ning. 2009. Robust machine translation evaluation
with entailment features. In Proceedings of ACL-
IJCNLP.
K. Papineni, S. Roukos, T. Ward, and W.-J. Zhu. 2002.
BLEU: A method for automatic evaluation of ma-
chine translation. In Proceedings of ACL.
M. Popovic? and H. Ney. 2009. Syntax-oriented eval-
uation measures for machine translation output. In
Proceedings of WMT.
M. Snover, B. Dorr, R. Schwartz, L. Micciulla, and
J. Makhoul. 2006. A study of translation edit rate
with targeted human annotation. In Proceedings of
AMTA.
M. Snover, N. Madnani, B. Dorr, and R. Schwartz.
2009. Fluency, adequacy, or HTER? Exploring dif-
ferent human judgments with a tunable MT metric.
In Proceedings of WMT.
A. Stolcke. 2002. SRILM - an extensible language
modeling toolkit. In Proceedings of ICSLP.
359
Proceedings of the 6th Workshop on Statistical Machine Translation, pages 78?84,
Edinburgh, Scotland, UK, July 30?31, 2011. c?2011 Association for Computational Linguistics
TESLA at WMT 2011: Translation Evaluation and Tunable Metric
Daniel Dahlmeier1 and Chang Liu2 and Hwee Tou Ng1,2
1NUS Graduate School for Integrative Sciences and Engineering
2Department of Computer Science, National University of Singapore
{danielhe,liuchan1,nght}@comp.nus.edu.sg
Abstract
This paper describes the submission from the
National University of Singapore to the WMT
2011 Shared Evaluation Task and the Tunable
Metric Task. Our entry is TESLA in three dif-
ferent configurations: TESLA-M, TESLA-F,
and the new TESLA-B.
1 Introduction
TESLA (Translation Evaluation of Sentences with
Linear-programming-based Analysis) was first pro-
posed in Liu et al (2010). The simplest variant,
TESLA-M (M stands for minimal), is based on N-
gram matching, and utilizes light-weight linguis-
tic analysis including lemmatization, part-of-speech
tagging, and WordNet synonym relations. TESLA-
B (B stands for basic) additionally takes advan-
tage of bilingual phrase tables to model phrase syn-
onyms. It is a new configuration proposed in this pa-
per. The most sophisticated configuration TESLA-F
(F stands for full) additionally uses language mod-
els and a ranking support vector machine instead of
simple averaging. TESLA-F was called TESLA in
Liu et al (2010). In this paper, we rationalize the
naming convention by using TESLA to refer to the
whole family of metrics.
The rest of this paper is organized as follows. Sec-
tions 2 to 4 describe the TESLA variants TESLA-M,
TESLA-B, and TESLA-F, respectively. Section 5
describes MT tuning with TESLA. Section 6 shows
experimental results for the evaluation and the tun-
able metric task. The last section concludes the pa-
per.
2 TESLA-M
The version of TESLA-M used in WMT 2011 is ex-
actly the same as in Liu et al (2010). The descrip-
tion is reproduced here for completeness.
We consider the task of evaluating machine trans-
lation systems in the direction of translating a source
language to a target language. We are given a refer-
ence translation produced by a professional human
translator and a machine-produced system transla-
tion. At the highest level, TESLA-M is the arith-
metic average of F-measures between bags of N-
grams (BNGs). A BNG is a multiset of weighted
N-grams. Mathematically, a BNG B consists of tu-
ples (bi, bWi ), where each bi is an N-gram and b
W
i is
a positive real number representing the weight of bi.
In the simplest case, a BNG contains every N-gram
in a translated sentence, and the weights are just the
counts of the respective N-grams. However, to em-
phasize the content words over the function words,
we discount the weight of an N-gram by a factor of
0.1 for every function word in the N-gram. We de-
cide whether a word is a function word based on its
POS tag.
In TESLA-M, the BNGs are extracted in the target
language, so we call them bags of target language
N-grams (BTNGs).
2.1 Similarity functions
To match two BNGs, we first need a similarity mea-
sure between N-grams. In this section, we define
the similarity measures used in our experiments. We
adopt the similarity measure from MaxSim (Chan
and Ng, 2008; Chan and Ng, 2009) as sms. For uni-
grams x and y,
78
? If lemma(x) = lemma(y), then sms = 1.
? Otherwise, let
a = I(synsets(x) overlap with synsets(y))
b = I(POS(x) = POS(y))
where I(?) is the indicator function, then sms =
(a + b)/2.
The synsets are obtained by querying WordNet
(Fellbaum, 1998). For languages other than English,
a synonym dictionary is used instead.
We define two other similarity functions between
unigrams:
slem(x, y) = I(lemma(x) = lemma(y))
spos(x, y) = I(POS(x) = POS(y))
All the three unigram similarity functions generalize
to N-grams in the same way. For two N-grams x =
x1,2,...,n and y = y1,2,...,n,
s(x, y) =
{
0 if ?i, s(xi, yi) = 0
1
n
?n
i=1 s(x
i, yi) otherwise
2.2 Matching two BNGs
Now we describe the procedure of matching two
BNGs. We take as input BNGs X and Y and a sim-
ilarity measure s. The i-th entry in X is xi and has
weight xWi (analogously for yj and y
W
j ).
Intuitively, we wish to align the entries of the two
BNGs in a way that maximizes the overall similar-
ity. An example matching problem for bigrams is
shown in Figure 1a, where the weight of each node
is shown, along with the hypothetical similarity for
each edge. Edges with a similarity of zero are not
shown. Note that for each function word, we dis-
count the weight by a factor of ten. The solution to
the matching problem is shown in Figure 1b, and the
overall similarity is 0.5 ? 0.01 + 0.8 ? 0.1 + 0.8 ?
0.1 = 0.165.
Mathematically, we formulate this as a (real-
valued) linear programming problem1. The vari-
ables are the allocated weights for the edges
w(xi, yj) ?i, j
1While integer linear programming is NP-complete, real-
valued linear programming can be solved efficiently.
w=1.0 w=0.1 w=0.1 w=0.1
w=0.01 w=0.1 w=0.1
s=0.1 s=0.8s=0.5 s=0.8
Good morning morning , , sir sir .
Hello , , Querrien Querrien .
s=0.4
(a) The matching problem
w=1.0 w=0.1 w=0.1 w=0.1
w=0.01 w=0.1 w=0.1
w=0.1w=0.01 w=0.1
s885Go8d m r o8d m r Gn nGimd imdG.
g,HH8Gn nGel,ddm, el,ddm, G.
(b) The solution
Figure 1: A BNG matching problem
We maximize
?
i,j
s(xi, yj)w(xi, yj)
subject to
w(xi, yj) ? 0 ?i, j
?
j
w(xi, yj) ? x
W
i ?i
?
i
w(xi, yj) ? y
W
j ?j
The value of the objective function is the overall
similarity S. Assuming X is the reference and Y
is the system translation, we have
Precision =
S
?
j y
W
j
Recall =
S
?
i x
W
i
The F-measure is derived from the precision and the
recall:
F =
Precision ? Recall
? ? Precision + (1 ? ?) ? Recall
In this work, we set ? = 0.8, following MaxSim.
The value gives more importance to the recall than
the precision.
79
If the similarity function is binary-valued and
transitive, such as slem and spos, then we
can use a much simpler and faster greedy
matching procedure: the best match is simply
?
g min(
?
xi=g
xWi ,
?
yi=g
yWi ).
2.3 Scoring
The TESLA-M sentence-level score for a reference
and a system translation is the arithmetic average of
the BTNG F-measures for unigrams, bigrams, and
trigrams based on similarity functions sms and spos.
We thus have 3 ? 2 = 6 BTNG F-measures for
TESLA-M.
We can compute a system-level score for a ma-
chine translation system by averaging its sentence-
level scores over the complete test set.
3 TESLA-B
TESLA-B uses the average of two types of F-
measures: (1) BTNG F-measures as in TESLA-M
and (2) F-measures between bags of N-grams in one
or more pivot languages, called bags of pivot lan-
guage N-grams (BPNGs), The rest of this section fo-
cuses on the generation of the BPNGs. Their match-
ing is done in the same way as described for BTNGs
in the previous section.
3.1 Phrase level semantic representation
Given a sentence-aligned bitext between the target
language and a pivot language, we can align the
text at the word level using well known tools such
as GIZA++ (Och and Ney, 2003) or the Berkeley
aligner (Liang et al, 2006; Haghighi et al, 2009).
We observe that the distribution of aligned
phrases in a pivot language can serve as a seman-
tic representation of a target language phrase. That
is, if two target language phrases are often aligned
to the same pivot language phrase, then they can be
inferred to be similar in meaning. Similar observa-
tions have been made by previous researchers (Ban-
nard and Callison-Burch, 2005; Callison-Burch et
al., 2006; Snover et al, 2009).
We note here two differences from WordNet syn-
onyms: (1) the relationship is not restricted to the
word level only, and (2) the relationship is not bi-
nary. The degree of similarity can be measured by
the percentage of overlap between the semantic rep-
resentations.
3.2 Segmenting a sentence into phrases
To extend the concept of this semantic representa-
tion of phrases to sentences, we segment a sentence
in the target language into phrases. Given a phrase
table, we can approximate the probability of a phrase
p by:
Pr(p) =
N(p)
?
p? N(p
?)
(1)
where N(?) is the count of a phrase in the phrase
table. We then define the likelihood of seg-
menting a sentence S into a sequence of phrases
(p1, p2, . . . , pn) by:
Pr(p1, p2, . . . , pn|S) =
1
Z(S)
n?
i=1
Pr(pi) (2)
where Z(S) is a normalizing constant. The segmen-
tation of S that maximizes the probability can be de-
termined efficiently using a dynamic programming
algorithm. The formula has a strong preference for
longer phrases, as every Pr(p) is a small fraction.
To deal with out-of-vocabulary (OOV) words, we
allow any single word w to be considered a phrase,
and if N(w) = 0, we set N(w) = 0.5 instead.
3.3 BPNGs as sentence level semantic
representation
Simply merging the phrase-level semantic represen-
tation is insufficient to produce a sensible sentence-
level semantic representation. As an example, we
consider two target language (English) sentences
segmented as follows:
1. ||| Hello , ||| Querrien ||| . |||
2. ||| Good morning , sir . |||
A naive comparison of the bags of aligned pivot lan-
guage (French) phrases would likely conclude that
the two sentences are completely unrelated, as the
bags of aligned phrases are likely to be completely
disjoint. We tackle this problem by constructing
a confusion network representation of the aligned
phrases, as shown in Figures 2 and 3. A confusion
network is a compact representation of a potentially
exponentially large number of weighted and likely
malformed French sentences. We can collect the N-
gram statistics of this ensemble of French sentences
80
w=1.=0s858G8od 
mrn0i858G8odg
,0HsseH18G8gdo d8G8gdo
Figure 2: A confusion network as a semantic repre-
sentation
w=1.=0s858G=1od 0s8m8r8nmi
Figure 3: A degenerate confusion network as a se-
mantic representation
efficiently from the confusion network representa-
tion. For example, the trigram Bonjour , Querrien 2
would receive a weight of 0.9 ? 1.0 = 0.9 in Fig-
ure 2. As with BTNGs, we discount the weight of an
N-gram by a factor of 0.1 for every function word in
the N-gram, so as to place more emphasis on the
content words.
The collection of all such N-grams and their cor-
responding weights forms the BPNG of a sentence.
The reference and system BPNGs are then matched
using the algorithm outlined in Section 2.2.
3.4 Scoring
The TESLA-B sentence-level score is a linear com-
bination of (1) BTNG F-measures for unigrams,
bigrams, and trigrams based on similarity func-
tions sms and spos, and (2) BPNG F-measures for
unigrams, bigrams, and trigrams based on sim-
ilarity functions slem and spos. We thus have
3 ? 2 F-measures from the BTNGs and 3 ? 2 ?
#pivot languages F-measures from the BPNGs. We
average the BTNG and BPNG scores to obtain
sBTNG and sBPNG, respectively. The sentence-
level TESLA-B score is then defined as 12(sBTNG +
sBPNG). The two-step averaging process prevents
the BPNG scores from overwhelming the BTNG
scores, especially when we have many pivot lan-
guages. The system-level TESLA-B score is the
arithmetic average of the sentence-level TESLA-B
scores.
2Note that an N-gram can span more than one segment.
4 TESLA-F
Unlike the simple arithmetic averages used in
TESLA-M and TESLA-B, TESLA-F uses a gen-
eral linear combination of three types of scores: (1)
BTNG F-measures as in TESLA-M and TESLA-B,
(2) BPNG F-measures as in TESLA-B, and (3) nor-
malized language model scores of the system trans-
lation, defined as 1n logP , where n is the length of
the translation, and P the language model probabil-
ity. The method of training the linear model depends
on the development data. In the case of WMT, the
development data is in the form of manual rankings,
so we train SVM rank (Joachims, 2006) on these in-
stances to build the linear model. In other scenarios,
some form of regression can be more appropriate.
The BTNG and BPNG scores are the same as
used in TESLA-B. In the WMT campaigns, we use
two language models, one generated from the Eu-
roparl dataset and one from the news-train dataset.
We thus have 3 ? 2 features from the BTNGs,
3 ? 2 ? #pivot languages features from the BPNGs,
and 2 features from the language models. Again, we
can compute system-level scores by averaging the
sentence-level scores.
4.1 Scaling of TESLA-F Scores
While machine translation evaluation is concerned
only with the relative order of the different trans-
lations but not with the absolute scores, there are
practical advantages in normalizing the evaluation
scores to a range between 0 and 1. For TESLA-M
and TESLA-B, this is already the case, since every
F-measure has a range of [0, 1] and so do their av-
erages. In contrast, the SVM rank -produced model
typically gives scores very close to zero.
To remedy that, we note that we have the free-
dom to scale and shift the linear SVM model with-
out changing the metric. We observe that the F-
measures have a range of [0, 1], and studying the
data reveals that [?15, 0] is a good approximation of
the range for normalized language model scores, for
all languages involved in the WMT campaign. Since
we know the range of values of an F-measure feature
(between 0 and 1) and assuming that the range of
the normalized LM score is between ?15 and 0, we
can find the maximum and minimum possible score
given the weights. Then we linearly scale the range
81
of scores from [min, max] to [0, 1]. We provide an
option of scaling TESLA-F scores in the new release
of TESLA.
5 MT tuning with TESLA
All variants of TESLA can be used for automatic
MT tuning using Z-MERT (Zaidan, 2009). Z-
MERT?s modular design makes it easy to integrate a
new metric. As TESLA already computes scores at
the sentence level, integrating TESLA into Z-MERT
was straightforward. First, we created a ?streaming?
version of each TESLA metric which reads trans-
lation candidates from standard input and prints the
sentence-level scores to standard output. This allows
Z-MERT to easily query the metric for sentence-
level scores during MT tuning. Second, we wrote
a Java wrapper that calls the TESLA code from Z-
MERT. The resulting metric can be used for MERT
tuning in the standard fashion. All that a user has
to do is to change the metric in the Z-MERT config-
uration file to TESLA. All the necessary code for
Z-MERT tuning is included in the new release of
TESLA.
6 Experiments
6.1 Evaluation Task
We evaluate TESLA using the publicly available
data from WMT 2009 for into-English and out-
of-English translation. The pivot language phrase
tables and language models are built using the
WMT 2009 training data. The SVM rank model for
TESLA-F is trained on manual rankings from WMT
2008. The results for TESLA-M and TESLA-F have
previously been reported in Liu et al (2010)3. We
add results for the new variant TESLA-B here.
Tables 1 and 2 show the sentence-level consis-
tency and system-level Spearman?s rank correlation,
respectively for into-English translation. For com-
parison, we include results for some of the best per-
forming metrics in WMT 2009. Tables 3 and 4 show
the same results for out-of-English translation. We
do not include the English-Czech language pair in
our experiments, as we unfortunately do not have
good linguistic resources for the Czech language.
3The English-Spanish system correlation differs from our
previous result after fixing a minor mistake in the language
model.
cz-en fr-en de-en es-en hu-en Overall
TESLA-M 0.60 0.61 0.61 0.59 0.63 0.61
TESLA-B 0.63 0.64 0.63 0.62 0.63 0.63
TESLA-F 0.63 0.65 0.64 0.62 0.66 0.63
ulc 0.63 0.64 0.64 0.61 0.60 0.63
maxsim 0.60 0.63 0.63 0.61 0.62 0.62
meteor-0.6 0.47 0.51 0.52 0.49 0.48 0.50
Table 1: Into-English sentence-level consistency on
WMT 2009 data
cz-en fr-en de-en es-en hu-en Avg
TESLA-M 1.00 0.86 0.85 0.99 0.66 0.87
TESLA-B 1.00 0.92 0.67 0.95 0.83 0.87
TESLA-F 1.00 0.92 0.68 0.94 0.94 0.90
ulc 1.00 0.92 0.78 0.86 0.60 0.83
maxsim 0.70 0.91 0.76 0.98 0.66 0.80
meteor-0.6 0.70 0.93 0.56 0.87 0.54 0.72
Table 2: Into-English system-level Spearman?s rank
correlation on WMT 2009 data
The new TESLA-B metric proves to be competi-
tive to its siblings and is often on par with the more
sophisticated TESLA-F metric. The exception is
the English-German language pair, where TESLA-
B has very low system-level correlation. We have
two possible explanations for this. First, the system-
level correlation is computed on a very small sample
size (the ranked list of MT systems). This makes the
system-level correlation score more volatile com-
pared to the sentence-level consistency score which
is computed on thousands of sentence pairs. Sec-
ond, German has a relatively free word order which
potentially makes word alignment and phrase table
extraction more noisy. Interestingly, all participating
metrics in WMT 2009 had low system-level correla-
tion for the English-German language pair.
en-fr en-de en-es Overall
TESLA-M 0.64 0.59 0.59 0.60
TESLA-B 0.65 0.59 0.60 0.61
TESLA-F 0.68 0.57 0.60 0.61
wpF 0.66 0.60 0.61 0.61
wpbleu 0.60 0.47 0.49 0.51
Table 3: Out-of-English sentence-level consistency
on WMT 2009 data
82
en-fr en-de en-es Avg
TESLA-M 0.93 0.86 0.79 0.86
TESLA-B 0.91 0.05 0.63 0.53
TESLA-F 0.85 0.78 0.67 0.77
wpF 0.90 -0.06 0.58 0.47
wpbleu 0.92 0.07 0.63 0.54
Table 4: Out-of-English system-level Spearman?s
rank correlation on WMT 2009 data
6.2 Tunable Metric Task
The goal of the new tunable metric task is to explore
MT tuning with metrics other than BLEU (Papineni
et al, 2002). To allow for a fair comparison, the
WMT organizers provided participants with a com-
plete Joshua MT system for an Urdu-English trans-
lation task. We tuned models for each variant of
TESLA, using Z-MERT in the default configuration
provided by the organizers. There are four reference
translations for each Urdu source sentence. The size
of the N-best list is set to 300.
For our own experiments, we randomly split the
development set into a development portion (781
sentences) and a held-out test portion (200 sen-
tences). We run the same Z-MERT tuning process
for each TESLA variant on this reduced develop-
ment set and evaluate the resulting models on the
held out test set. We include a model trained with
BLEU as an additional reference point. The results
are shown in Table 5. We observe that the model
trained with TESLA-F achieves the best results
when evaluated with any of the TESLA metrics, al-
though the differences between the scores are small.
We found that TESLA produces slightly longer
translations than BLEU: 22.4 words (TESLA-M),
21.7 words (TESLA-B), and 22.5 words (TESLA-
F), versus 18.7 words (BLEU). The average refer-
ence length is 19.8 words.
The official evaluation for the tunable metric task
is performed using manual rankings. The score of
a system is calculated as the percentage of times
the system is judged to be either better or equal
(score1) or strictly better (score2) compared to each
other system in pairwise comparisons. Although
we submit results for all TESLA variants, only our
primary submission TESLA-F is included in the
manual evaluation. The results for TESLA-F are
mixed. When evaluated with score1, TESLA-F is
Tune\Test BLEU TESLA-M TESLA-B TESLA-F
BLEU 0.2715 0.3756 0.3129 0.3920
TESLA-M 0.2279 0.4056 0.3279 0.3981
TESLA-B 0.2370 0.4001 0.3257 0.3977
TESLA-F 0.2432 0.4076 0.3299 0.4007
Table 5: Automatic evaluation scores on held out
test portion for the tunable metric task. The best re-
sult in each column is printed in bold.
ranked 7th out of 8 participating systems, but when
evaluated with score2, TESLA-F is ranked second
best. These findings differ from previous results
that we reported in Liu et al (2011) where MT
systems tuned with TESLA-M and TESLA-F con-
sistently outperform two other systems tuned with
BLEU and TER for translations from French, Ger-
man, and Spanish into English on the WMT 2010
news data set. A manual inspection of the references
in the tunable metric task shows that the translations
are of lower quality compared to the news data sets
used in WMT. As the SVM model in TESLA-F is
trained with rankings from WMT 2008, it is possible
that the model is less robust when applied to Urdu-
English translations. This could explain the mixed
performance of TESLA-F in the tunable metric task.
7 Conclusion
We introduce TESLA-B, a new variant of the
TESLA machine translation metric and present ex-
perimental results for all TESLA variants in the set-
ting of the WMT evaluation task and tunable met-
ric task. All TESLA variants are integrated into Z-
MERT for automatic machine translation tuning.
Acknowledgments
This research was done for CSIDM Project No.
CSIDM-200804 partially funded by a grant from
the National Research Foundation (NRF) adminis-
tered by the Media Development Authority (MDA)
of Singapore.
References
Colin Bannard and Chris Callison-Burch. 2005. Para-
phrasing with bilingual parallel corpora. In Proceed-
ings of the 43rd Annual Meeting of the Association for
Computational Linguistics.
83
Chris Callison-Burch, Philipp Koehn, and Miles Os-
borne. 2006. Improved statistical machine translation
using paraphrases. In Proceedings of the Human Lan-
guage Technology Conference of the North American
Chapter of the Association for Computational Linguis-
tics.
Yee Seng Chan and Hwee Tou Ng. 2008. MaxSim:
A maximum similarity metric for machine translation
evaluation. In Proceedings of the 46th Annual Meeting
of the Association for Computational Linguistics.
Yee Seng Chan and Hwee Tou Ng. 2009. MaxSim: Per-
formance and effects of translation fluency. Machine
Translation, 23(2?3):157?168.
Christiane Fellbaum. 1998. WordNet: An Electronic
Lexical Database. The MIT Press.
Aria Haghighi, John Blitzer, John DeNero, and Dan
Klein. 2009. Better word alignments with supervised
ITG models. In Proceedings of 47th Annual Meeting
of the Association for Computational Linguistics and
the 4th IJCNLP of the AFNLP.
Thorsten Joachims. 2006. Training linear SVMs in lin-
ear time. In Proceedings of the ACM Conference on
Knowledge Discovery and Data Mining.
Percy Liang, Ben Taskar, and Dan Klein. 2006. Align-
ment by agreement. In Proceedings of the Human
Language Technology Conference of the North Ameri-
can Chapter of the Association for Computational Lin-
guistics.
Chang Liu, Daniel Dahlmeier, and Hwee Tou Ng.
2010. TESLA: Translation evaluation of sentences
with linear-programming-based analysis. In Proceed-
ings of the Joint Fifth Workshop on Statistical Machine
Translation and MetricsMATR.
Chang Liu, Daniel Dahlmeier, and Hwee Tou Ng. 2011.
Better evaluation metrics lead to better machine trans-
lation. In Proceedings of the 2011 Conference on Em-
pirical Methods in Natural Language Processing.
Franz Josef Och and Hermann Ney. 2003. A system-
atic comparison of various statistical alignment mod-
els. Computational Linguistics, 29(1).
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: a method for automatic eval-
uation of machine translation. In Proceedings of the
40th Annual Meeting of the Association for Computa-
tional Linguistics.
Matthew Snover, Nitin Madnani, Bonnie Dorr, and
Richard Schwartz. 2009. Fluency, adequacy, or
HTER? Exploring different human judgments with a
tunable MT metric. In Proceedings of of the Fourth
Workshop on Statistical Machine Translation.
Omar Zaidan. 2009. Z-MERT: A fully configurable open
source tool for minimum error rate training of machine
translation systems. The Prague Bulletin of Mathe-
matical Linguistics, 91:79?88.
84
The 7th Workshop on the Innovative Use of NLP for Building Educational Applications, pages 216?224,
Montre?al, Canada, June 3-8, 2012. c?2012 Association for Computational Linguistics
NUS at the HOO 2012 Shared Task
Daniel Dahlmeier1, Hwee Tou Ng1,2, and Eric Jun Feng Ng2
1NUS Graduate School for Integrative Sciences and Engineering
2Department of Computer Science, National University of Singapore
{danielhe,nght,eng}@comp.nus.edu.sg
Abstract
This paper describes the submission of the Na-
tional University of Singapore (NUS) to the
HOO 2012 shared task. Our system uses a
pipeline of confidence-weighted linear classi-
fiers to correct determiner and preposition er-
rors. Our system achieves the highest correc-
tion F1 score on the official test set among all
14 participating teams, based on gold-standard
edits both before and after revision.
1 Introduction
Grammatical error correction is the task of automati-
cally detecting and correcting erroneous word usage
and ill-formed grammatical constructions in text.
Determiner and preposition errors are the two most
prominent types of errors made by non-native speak-
ers of English. Although there has been much work
on automatic correction of determiner and preposi-
tion errors over the last few years, it has so far been
impossible to directly compare results because dif-
ferent teams have evaluated on different data sets.
The HOO 2012 shared task evaluates grammatical
error correction systems for determiner and prepo-
sition errors. Participants are provided with a set
of documents written by non-native speakers of En-
glish. The task is to automatically detect and cor-
rect determiner and preposition errors and produce a
set of corrections (called edits). Evaluation is done
by computing precision, recall, and F1 score be-
tween the system edits and a manually created set
of gold-standard edits. The details of the HOO 2012
shared task are described in the official overview pa-
per (Dale et al, 2012).
In this paper, we describe the system submission
from the National University of Singapore (NUS).
Our system treats determiner and preposition correc-
tion as classification problems. We use confidence-
weighted linear classifiers to predict the correct
word from a confusion set of possible correction op-
tions. Separate classifiers are built for determiner
errors, preposition replacement errors, and preposi-
tion insertion and deletion errors. The classifiers are
combined into a pipeline of correction steps to form
an end-to-end error correction system. Our system
achieves the highest correction F1 score on the offi-
cial test set among all 14 participating teams, based
on gold-standard edits both before and after revision.
The remainder of this paper is organized as fol-
lows. The next section presents our error correction
system. Section 3 describes the features. Section 4
presents experimental results. Section 5 contains
further discussion. Section 6 concludes the paper.
2 System Architecture
Our system consists of a pipeline of sequential steps
where the output of one step serves as the input to
the next step. The steps in sequence are:
1. Pre-processing
2. Determiner correction (Det)
3. Replacement preposition correction (RT)
4. Missing and unwanted preposition correction
(MT, UT)
The final output after the last step forms our submis-
sion to the shared task. Each correction step (i.e.,
steps 2, 3, 4) involves three internal steps:
1. Feature extraction
216
2. Classification
3. Language model filter
Feature extraction first analyzes the syntactic struc-
ture of the input sentences (part-of-speech (POS)
tagging, chunking, and parsing) and identifies
relevant instances for correction (e.g., all noun
phrases (NP) for determiner correction). Each in-
stance is mapped to a real-valued feature vector.
Next, a classifier predicts the most likely correction
for each feature vector. Finally, the proposed correc-
tions are filtered using a language model and only
corrections that strictly increase the language model
score are kept.
2.1 Confidence-Weighted Learning
As the learning algorithm for all classifiers, we
choose confidence-weighted (CW) learning (Dredze
et al, 2008; Crammer et al, 2009), which has been
shown to perform well for natural language pro-
cessing (NLP) problems with high dimensional and
sparse feature spaces. Instead of keeping a single
weight vector, CW learning maintains a distribu-
tion over weight vectors, parametrized by a multi-
variate normal distribution N (?,?) with mean ?
and covariance matrix ?. In practice, ? is of-
ten approximated by a diagonal matrix (Dredze et
al., 2008). CW is an online learning algorithm
that proceeds in rounds over a labeled training set
((y1,x1), (y2,x2), . . . , (yn,xn)), one example at a
time. After the i-th round, CW learning updates the
distribution over weight vectors such that the i-th ex-
ample is predicted correctly with probability at least
0 < ? < 1 while choosing the update step that min-
imizes the Kullback-Leibler (KL) distance from the
current distribution. The CW update rule is:
(?i+1,?i+1) = (1)
arg min
?,?
DKL (N (?,?)||N (?i,?i))
s.t. Pr[yi|xi,?,?] ? ?.
Dredze et al (2008) show that in the binary case, the
CW update rule has a closed-form solution. In the
multi-class case, there exists no closed-form solu-
tion but the solution can be efficiently approximated.
2.2 Pre-processing
Pre-processing involves sentence splitting, tokeniza-
tion, re-casing, and spelling correction. We noticed
that the HOO 2012 training data contained a large
number of spelling mistakes and that some docu-
ments are written in all upper case. Both have a neg-
ative effect on tagging and classification accuracy.
We automatically identify and re-case upper-case
documents using a standard re-casing model from
statistical machine translation (SMT). Re-casing is
modeled as monotone decoding (without reorder-
ing) involving translation of an un-cased sentence
to a mixed-case sentence. Next, we automatically
correct spelling mistakes using an open-source spell
checker. Words are excluded from spelling correc-
tion if they are shorter than a threshold (set to 4 char-
acters in our work), or if they include hyphens or up-
per case characters inside the word. We apply a lan-
guage model filter (described in the next subsection)
to filter the proposed spelling corrections. Note that
spelling correction is only performed to improve the
accuracy of subsequent correction steps. Spelling
corrections themselves are not part of the edits sub-
mitted for evaluation.
2.3 Determiner Correction
Determiner errors include three error types: replace-
ment determiner (RD), missing determiner (MD),
and unwanted determiner (UD). Although determin-
ers are not limited to articles (a, an, the, empty arti-
cle ), article errors account for the majority of de-
terminer errors. We therefore focus our efforts on
errors involving only articles.
2.3.1 Correction as Classification
We treat determiner error correction as a multi-
class classification problem. A classifier is trained
to predict the correct article from a confusion set of
possible article choices {a, the, }, given the sen-
tence context. The article an is normalized as a and
restored later using a rule-based heuristic. During
training, every NP in the training data generates one
training example. The class y ?{a, the, } is the
correct article as annotated by the gold standard or
the observed article used by the writer if the arti-
cle is not annotated (i.e., the article is correct). The
surrounding context is represented as a real-valued
feature vector x ? X . The features of our classifiers
are described in Section 3.
One challenge in training classifiers for grammat-
ical error correction is that the data is highly skewed.
217
Training examples without any error (i.e., the ob-
served article equals the correct article) greatly out-
number those examples with an error (i.e., the ob-
served article is different from the correct article).
As the observed article is highly correlated with the
correct article, the observed article is a valuable fea-
ture (Rozovskaya and Roth, 2010; Dahlmeier and
Ng, 2011). However, the high correlation can have
the undesirable effect that the classifier always pre-
dicts the observed article and never proposes any
corrections. To mitigate this problem, we re-sample
the training data, either by oversampling examples
with an error or undersampling examples without an
error. The sampling parameter is chosen through a
grid search so as to maximize the F1 score on the de-
velopment data. After training, the classifier can be
used to predict the correct article for NPs from new
unseen sentences.
During testing, every NP in the test data generates
one test example. If the article predicted by the clas-
sifier differs from the observed article and the differ-
ence between the classifier?s confidence score for its
first choice and the classifier?s confidence score for
the observed article is higher than some threshold
parameter t, the observed article is replaced by the
proposed correction. The threshold parameter t is
tuned through a grid search so as to maximize the F1
score on the development data. We found that using
a separate threshold parameter value for each class
worked better than using a single threshold value.
2.3.2 Language Model Filter
All corrections are filtered using a large language
model. Only corrections that strictly increase the
normalized language model score of a sentence are
kept. The normalized language model score is de-
fined as
scorelm =
1
|s|
logPr(s), (2)
where s is the corrected sentence and |s| is the sen-
tence length in tokens. The final set of article correc-
tions is applied to an input sentence (i.e., replacing
the observed article with the predicted article).
2.4 Replacement Preposition Correction
Replacement preposition correction follows the
same strategy as determiner correction, but with a
different confusion set and different features. The
confusion set consists of 36 frequent prepositions
which we adopt from our previous work (Dahlmeier
and Ng, 2011).1 These prepositions account for
the majority of preposition replacement errors in the
HOO 2012 training data. During training, every
prepositional phrase (PP) in the training data which
is headed by a preposition from the confusion set
generates one training example. The class y is the
correct preposition. During testing, every PP in the
test data which is headed by a preposition from the
confusion set generates one test example.
2.5 Missing Preposition Correction
Our system corrects missing and unwanted prepo-
sition errors for the seven most frequently missed
or wrongly inserted prepositions in the HOO 2012
training data. These preposition are about, at, for,
in, of, on, and to. While developing our system, we
found that adding more prepositions did not increase
performance in our experiments.
We treat missing preposition (MT) correction as a
binary classification problem.2 For each preposition
p, we train a binary classifier that predicts the pres-
ence or absence of that preposition. Thus, the con-
fusion set consists only of the preposition p and the
?empty preposition?. During training, we require
examples of contexts where p should be used and
where it should be omitted. As prepositions typi-
cally appear before NPs, we take every NP in the
training data as one training example. If the prepo-
sition p appears right in front of the NP (i.e., the
preposition p and the NP form a PP), the example
is a positive example, otherwise (i.e., another prepo-
sition or no preposition appears before the NP) it
is a negative example. During testing, every NP
which does not directly follow a preposition gener-
ates one test example. If the classifier predicts that
the preposition p should have been used in this con-
text with sufficiently high confidence and inserting
p increases the normalized language model score, p
is inserted before the NP.
1about, along, among, around, as, at, beside, besides, be-
tween, by, down, during, except, for, from, in, inside, into, of,
off, on, onto, outside, over, through, to, toward, towards, under,
underneath, until, up, upon, with, within, without
2Alternatively, missing preposition error correction could be
treated as a multi-class problem, but we found that binary clas-
sifiers gave better performance in initial experiments.
218
2.6 Unwanted Preposition Correction
Unwanted preposition correction is treated as a bi-
nary classification problem similar to missing prepo-
sition correction but with different training and test
examples. When training the classifier for preposi-
tion p, every PP where the writer used the preposi-
tion p is one training example. If the gold-standard
annotation labels p as unwanted, the example is a
positive example for deleting p, otherwise it is a
negative example. During testing, every PP with
the preposition p generates one test example. If the
classifier predicts that p should be deleted with suffi-
ciently high confidence and deleting p increases the
normalized language model score, p is deleted.
We found that separate classifiers for missing and
unwanted preposition correction gave slightly bet-
ter results compared to using a single classifier for
both tasks. As the test examples for missing and
unwanted preposition correction of a preposition p
are disjoint, both steps can be performed in paral-
lel. This also prevents the case of the system ?con-
tradicting? itself by first inserting a preposition and
later deleting it. We perform missing preposition
correction and unwanted preposition correction for
each preposition in turn, before moving to the next
preposition.
3 Features
In this section, we describe the features used in our
system. The choice of features can have an impor-
tant effect on classification performance. The exact
features used for determiner, replacement preposi-
tion, and missing and unwanted preposition correc-
tion are listed in Tables 1, 2, 3, and 4, respectively.
The features were chosen empirically through exper-
iments on the development data.
The most commonly used features for grammat-
ical error correction are lexical and POS N-grams,
and chunk features. We adopt the features from
previous work by Han et al (2006), Tetreault and
Chodorow (2008), and Rozovskaya et al (2011) for
our system. Tetreault et al (2010) show that parse
features can further increase performance, and we
use the dependency parse features based on their
work. For all the above features, the observed ar-
ticle or preposition used by the writer is ?blanked
out? when computing the features. However, we add
the observed article or preposition as an additional
feature for determiner and replacement preposition
correction.
The features described so far are all binary-
valued, i.e., they indicate whether some feature is
present in the input or not. Additionally, we can
construct real-valued features by counting the log
frequency of surface N-grams on the web or in a
web-scale corpus (Bergsma et al, 2009). Web-scale
N-gram count features can harness the power of the
web in connection with supervised classification and
have successfully been used for a number of NLP
generation and disambiguation problems (Bergsma
et al, 2009; Bergsma et al, 2010), although we
are not aware of any previous application in gram-
matical error correction. Web-scale N-gram count
features usually use N-grams of consecutive tokens.
The release of web-scale parsed corpora like the
WaCky project (Baroni et al, 2009) makes it pos-
sible to extend the idea to dependency N-grams of
child-parent tuples over the dependency arcs in the
dependency parse tree, e.g., {(child, node), (node,
parent)} for bigrams, {(child?s child, child, node),
(child, node, parent), (node, parent, parent?s par-
ent)} for trigrams. We collect log frequency counts
for dependency N-grams from a large dependency-
parsed web corpus and use the log frequency count
as a feature. We normalize all real-valued feature
values to a unit interval [0, 1] to avoid features with
larger values dominating features with smaller val-
ues.
4 Experiments
In this section, we report experimental results of our
system on two different data sets: a held-out test
split of the HOO 2012 training data, and the official
HOO 2012 test set.
4.1 Data Sets
The HOO 2012 training data consists of 1,000 doc-
uments together with gold-standard annotation. The
documents are a subset of the 1,244 documents
in the Cambridge Learner Corpus FCE (First Cer-
tificate in English) data set (Yannakoudakis et al,
2011). The HOO 2012 gold-standard annotation
only contains edits for six determiner and prepo-
sition error types and discards all other gold edits
219
Feature Example
Lexical features
Observed article? the
First word in NP? black
Word i before (i = 1, 2, 3)? {on, sat, ..}
Word i before NP (i = 1, 2) {on, sat, ..}
Word + POS i before (i = 1, 2, 3)? {on+IN, sat+VBD, ..}
Word i after (i = 1, 2, 3)? {black, door, ..}
Word after NP period
Word + POS i after (N = 1, 2)? {period+period, .. }
Bag of words in NP? {black, door, mat}
N-grams (N = 2, .., 5)? {on X, X black, .. }
Word before + NP? on+black door mat
NP + N-gram after NP { black door mat+period, ..}
(N = 1, 2, 3)?
Noun compound (NC)? door mat
Adj + NC? black+door mat
Adj POS + NC? JJ+door mat
NP POS + NC? JJ NN NN+door mat
POS features
First POS in NP JJ
POS i before (i = 1, 2, 3) {IN, VBD, ..}
POS i before NP (i = 1, 2) {IN, VBD, ..}
POS i after (i = 1, 2, 3) {JJ, NN, ..}
POS after NP period
Bag of POS in NP {JJ, NN, NN}
POS N-grams (N = 2, .., 4) {IN X, X JJ, .. }
Head word features
Head of NP? mat
Head POS NN
Head word + POS? mat+NN
Head number singular
Head countable yes
NP POS + head? JJ NN NN+mat
Word before + head? on+mat
Head + N-gram after NP ? mat+period, ..
(N = 1, 2, 3)
Adjective + head? black+mat
Adjective POS + head? JJ+mat
Word before + adj + head? on+black+mat
Word before + adj POS + head? on+JJ+mat
Word before + NP POS + head? on+JJ NN NN+mat
Web N-gram count features
Web N-gram log counts {log freq(on a black),
N = 3, .., 5 log freq(on the black),
log freq(on black),..}
Dependency features
Dep NP head-child? {mat-black-amod, ..}
Dep NP head-parent? mat-on-pobj
Dep child-NP head-parent? {black-mat-on-amod-pobj, ..}
Preposition features
Prep before + head on+mat
Prep before + NC on+door mat
Prep before + NP on+black door mat
Prep before + adj + head on+black+mat
Prep before + adj POS + head on+JJ+mat
Prep before + adj + NC on+black+door mat
Prep before + adj POS + NC on+JJ+door mat
Prep before + NP POS + head on+JJ NN NN+mat
Prep before + NP POS + NC on+JJ NN NN+door mat
Table 1: Features for determiner correction. Exam-
ple: ?The cat sat on the black door mat.? ? : lexical
tokens in lower case, ?: lexical tokens in both origi-
nal and lower case
Feature Example
Verb object features
Verb obj? sat on
Verb obj + head? sat on+mat
Verb obj + NC? sat on+door mat
Verb obj + NP? sat on+black door mat
Verb obj + adj + head? sat on+black+mat
Verb obj + adj POS + head? sat on+JJ+mat
Verb obj + adj + NC? sat on+black+door mat
Verb obj + adj POS + NC? sat on+JJ+door mat
Verb obj + NP POS + head? sat on+JJ NN NN+mat
Verb obj + NP POS + NC? sat on+JJ NN NN+door mat
Table 1: (continued)
from the original FCE data set. This can lead to
?wrong? gold edits that produce ungrammatical sen-
tences, like the following sentence
There are a lot of possibilities ( ? of) to
earn some money ...
where the preposition of is inserted before to earn.
The FCE data set contains another edit (to earn ?
earning) but this edit is not included in the HOO
2012 gold annotation. This necessarily introduces
noise into the training data as a classifier trained on
this data will learn that inserting of before to earn
is correct. We sidestep this problem by directly us-
ing the FCE data set for training, and applying all
gold edits except the six determiner and preposition
error types. This gives us training data that only
contains those types of grammatical errors that we
are interested in. Note that this only applies to the
training data. For our development and develop-
ment test data, we use the HOO 2012 released data
where the texts contain all types of errors and do
not make use of the annotations in the FCE data
set. For system development, we randomly select
100 documents from the HOO 2012 training data
as our development set (HOO-DEV) and another
100 disjoint documents as our held-out development
test set (HOO-DEVTEST). We train classifiers on
the remaining 1,044 documents of the FCE data set
(FCE(1044)), tune parameters on HOO-DEV, and
test on HOO-DEVTEST. For our final submission,
we train classifiers on all FCE documents, except
those 100 documents in HOO-DEV which are used
for parameter tuning. Finally, we fix all parameters
and re-train the classifiers on the complete FCE cor-
pus (FCE(1244)). This allows us to make maxi-
mum use of the FCE corpus as training data. The
220
Features Example
Lexical and POS features
Observed preposition? on
Word i before (i = 1, 2, 3)? {sitting, cat, ..}
Word i after (i = 1, 2, 3)? {the, mat, ..}
N-grams (N = 2, .., 5)? {sitting X, X the, .. }
POS N-grams (N = 2, 3) {VBG X, X DT, .. }
Head word features
Head of prev VP? sitting
POS head of prev VP VBG
Head of prev NP? cat
POS head of prev NP NN
Head of next NP? mat
POS head of next NP NN
Head prev NP + head next NP? cat+mat
POS head prev NP NN+NN
+POS head next NP
Head prev VP + head prev NP sitting+cat+mat
+ head next NP?
POS head prev VP VBG+NN+NN
+ POS head prev NP
+ POS head next NP
N-gram before + {sitting+mat}
head of next NP (N = 1, 2)?
Web N-gram count features
Web N-gram log counts {log freq(sitting at),
N = 2, .., 5 log freq(sitting in),
.., log freq(sitting on),
.., log freq(sitting with), ..}
Web dep N-gram log counts {log freq(sitting-at),
N = 2, 3 log freq(sitting-in),
.., log freq(sitting-on),
.., log freq(sitting-with),
.., log freq(at-mat),
.., log freq(on-mat),
.., log freq(with-mat),
.., log freq(sitting-at-mat), ..
.., log freq(sitting-on-mat), ..}
Dependency features
Dep parent? sitting
Dep parent POS VBG
Dep parent relation prep
Dep child? {mat}
Dep child POS {NN}
Dep child relation {pobj}
Dep parent+child? sitting+mat
Dep parent POS+child POS? VBG+NN
Dep parent+child POS? sitting+NN
Dep parent POS+child? VBG+mat
Dep parent+relation? sitting+prep
Dep child+relation? mat+pobj
Dep parent+child+relation? sitting+mat+prep+pobj
Table 2: Features for replacement preposition cor-
rection. Example: ?He saw a cat sitting on the mat.?
?: lexical tokens in lower case, ?: lexical tokens in
both original and lower case
Features Example
Lexical and POS features
Word i before (i = 1, 2, 3)? {sitting, cat, ..}
Word i after (i = 1, 2, 3)? {the, mat, ..}
N-grams (N = 2, .., 5)? {sitting X, X the, .. }
POS N-grams (N = 2, 3) {VBG X, X DT, .. }
Head word features
Head of prev VP? sitting
POS head of prev VP VBG
Head of prev NP? cat
POS head of prev NP NN
Head of next NP? mat
POS head of next NP NN
Head prev NP + head next NP? cat+mat
POS head prev NP NN+NN
+ POS head next NP
Head prev VP + head prev NP sitting+cat+mat
+ head next NP?
POS head prev VP VBG+NN+NN
+ POS head prev NP
+ POS head next NP
N-gram before + {sitting+mat, ..}
head of next NP (N = 1, 2)?
Web N-gram count features
Web N-gram log counts {log freq(sitting on the),
N = 3, .., 5 log freq(sitting the),
.. ,log freq(sitting on the mat),
.., log freq(sitting the mat), ..}
Table 3: Features for missing preposition correction.
Example: ?He saw a cat sitting the mat.?? : lexical
tokens in lower case, ?: lexical tokens in both origi-
nal and lower case
Features Example
Web N-gram count features
Web N-gram log counts {log freq(went to home),
N = 3, .., 5 log freq(went home),
.. ,log freq(cat went to home),
.., log freq(cat went home), ..}
Table 4: Features for unwanted preposition correc-
tion. Example: ?The cat went to home.?
Data set # Documents # Sentences # Tokens
FCE(1044) 1,044 22,434 339,902
FCE(1244) 1,244 28,033 423,850
HOO-DEV 100 2,798 42,347
HOO-DEVTEST 100 2,674 41,518
HOO-TEST 100 1,393 20,563
Table 5: Overview of the data sets.
221
official HOO 2012 test data (HOO-TEST), which
is not part of the FCE corpus, is completely unob-
served during system development. Table 5 gives an
overview of the data. Besides the FCE and HOO
2012 data sets, we use the following corpora. The
Google Web 1T 5-gram corpus (Brants and Franz,
2006) is used for language modeling and collect-
ing N-gram counts, the PukWaC corpus from the
WaCky project (Baroni et al, 2009) is used for col-
lecting web-scale dependency N-gram counts, and
the New York Times section of the Gigaword cor-
pus3 is used for training the re-casing model. All
data sets used in our system are publicly available.
4.2 Resources
We use the following NLP resources in our sys-
tem. Sentence splitting is performed with the NLTK
toolkit.4 For spelling correction, we use the free
software Aspell.5 All words that appear at least ten
times in the HOO 2012 training data are added to the
spelling dictionary. We use the OpenNLP tools (ver-
sion 1.5.2)6 for POS tagging, YamCha (version
0.33) (Kudo and Matsumoto, 2003) for chunk-
ing, and the MaltParser (version 1.6.1) (Nivre et
al., 2007) for dependency parsing. We use Ran-
dLM (Talbot and Osborne, 2007) for language mod-
eling. The re-casing model is built with the Moses
SMT system (Koehn et al, 2007) from the Gigaword
New York Times section and all normal-cased docu-
ments in the HOO 2012 training data. The CuVPlus
English dictionary (Mitton, 1992) is used to deter-
mine the countability of nouns. The CW learning
algorithm is implemented by our group. The source
code is available from our website.7 All resources
used in our system are publicly available.
4.3 Evaluation
Evaluation is performed by computing detection,
recognition, and correction F1 score between the set
of system edits and the set of gold-standard edits
as defined in the HOO 2012 overview paper (Dale
et al, 2012). Detection scores are very similar to
recognition scores (about 1?2% higher). We omit
3LDC2009T13
4http://www.nltk.org
5http://aspell.net
6http://opennlp.apache.org
7http://nlp.comp.nus.edu.sg/software
Step Recognition Correction
P R F1 P R F1
Det 62.26 12.68 21.06 54.09 11.01 18.30
+ RT 64.34 22.41 33.24 57.35 19.97 29.63
+ MT/UT 60.75 28.94 39.20 54.84 26.12 35.39
Table 6: Overall precision, recall, and F1 score on
the HOO-DEVTEST data after determiner correc-
tion (Det), replacement preposition correction (RT),
and missing and unwanted preposition correction
(MT/UT).
detection scores due to space limitations. Evaluation
on the official test set is performed with respect to
two different gold standards: the original gold stan-
dard from Cambridge University Press and a revised
version which was created in the HOO 2012 shared
task in response to change requests from participat-
ing teams. All scores are computed with the official
scorer. The official gold-standard edits are given in
character offsets, while our system internally works
with token offsets. Therefore, all token offsets are
automatically mapped back to character offsets be-
fore we submit our system edits. We only submitted
one run of our system.
Type Recognition Correction
P R F1 P R F1
RD 30.00 5.66 9.52 30.00 5.66 9.52
MD 69.67 41.67 52.15 59.02 35.29 44.17
UD 40.74 11.00 17.32 40.74 11.00 17.32
Det 62.26 27.73 38.37 54.09 24.09 33.33
RT 69.09 33.63 45.24 63.64 30.97 41.67
MT 53.25 35.34 42.49 49.35 32.76 39.38
UT 38.46 12.20 18.52 38.46 12.20 18.52
Prep 59.62 29.95 39.87 55.40 27.83 37.05
Table 7: Individual scores for each error type on the
HOO-DEVTEST data.
4.4 Results
Tables 6 and 8 show the overall precision, recall and
F1 score of our system after each processing step on
the held-out HOO-DEVTEST set and the official test
set, respectively. All numbers are shown in percent-
ages. We note that each processing step improves
the overall performance. The final F1 correction
score on the official test set is 28.70% before revi-
sion and 37.83% after revision, which are the highest
scores achieved by any participating team. Tables 7
and 9 show individual precision, recall, and F1 score
222
Step Recognition Correction
P R F1 P R F1
Det 57.76 14.79 23.55 48.28 12.36 19.68
+ RT 58.93 21.85 31.88 47.02 17.44 25.44
+ MT/UT 55.98 25.83 35.35 45.45 20.97 28.70
(a) Before revisions
Step Recognition Correction
P R F1 P R F1
Det 68.10 16.70 26.83 62.93 15.43 24.79
+ RT 71.43 25.37 37.44 63.10 22.41 33.07
+ MT/UT 69.38 30.66 42.52 61.72 27.27 37.83
(b) After revisions
Table 8: Overall precision, recall, and F1 score on the HOO-TEST data after determiner correction (Det),
replacement preposition correction (RT), and missing and unwanted preposition correction (MT/UT).
Type Recognition Correction
P R F1 P R F1
RD 33.33 2.56 4.76 33.33 2.56 4.76
MD 62.24 48.80 54.71 51.02 40.00 44.84
UD 33.33 9.43 14.71 33.33 9.43 14.71
Det 57.76 30.88 40.24 48.28 25.81 33.63
RT 61.54 23.53 34.04 44.23 16.91 24.47
MT 46.15 21.05 28.92 38.46 17.54 24.10
UT 40.00 13.95 20.69 40.00 13.95 20.69
Prep 53.76 21.19 30.40 41.94 16.53 23.71
(a) Before revisions
Type Recognition Correction
P R F1 P R F1
RD 100.00 8.33 15.38 66.67 5.56 10.26
MD 70.41 52.67 60.26 65.31 48.85 55.90
UD 46.67 11.29 18.18 46.67 11.29 18.18
Det 68.10 34.50 45.80 62.93 31.88 42.32
RT 78.85 27.52 40.80 63.46 22.15 32.84
MT 61.54 28.57 39.02 53.85 25.00 34.15
UT 60.00 23.08 33.33 60.00 23.08 33.33
Prep 70.97 27.05 39.17 60.22 22.95 33.23
(b) After revisions
Table 9: Individual scores for each error type on the HOO-TEST data.
for each of the six error types, and for determiners
(Det: aggregate of RD, MD, UD) and prepositions
(Prep: aggregate of RT, MT, UT) on the held-out
HOO-DEVTEST set and the official test set HOO-
TEST, respectively.
5 Discussion
The main differences between our submission to the
HOO 2011 shared task (Dahlmeier et al, 2011) and
to this year?s shared task are the use of the CW learn-
ing algorithm, the use of web-scale N-gram count
features, and the use of the observed article or prepo-
sition as a feature. The CW learning algorithm per-
formed slightly better than the empirical risk mini-
mization batch learning algorithm that we have used
previously while being significantly faster during
training. Adding the web-scale N-gram count fea-
tures showed significant improvements in initial ex-
periments. Using the observed article or preposition
feature allows the classifier to learn a bias against
unnecessary corrections. We believe that our good
precision scores are a result of using this feature.
In our experiments, we tried adding additional
training data from other text corpora: the NUS Cor-
pus of Learner English (NUCLE) (Dahlmeier and
Ng, 2011) and the Gigaword corpus. Unfortunately,
we did not see any consistent improvements over
simply using the FCE corpus. The general rule of
thumb that ?more data is better data? did not seem to
hold true in this case. After the evaluation had com-
pleted, we also tried training on additional training
data and tested the resulting system on the official
test set but did not see improvements either. We be-
lieve that no improvements were obtained due to the
similarity between the training and test data, since
all of them are student essays written in response to
question prompts from the Cambridge FCE exam.
6 Conclusion
We have presented the system from the National
University of Singapore that participated in the HOO
2012 shared task. Our system achieves the highest
correction F1 score on the official test set among all
14 participating teams, based on gold-standard edits
both before and after revision.
Acknowledgments
This research is supported by the Singapore Na-
tional Research Foundation under its International
Research Centre @ Singapore Funding Initiative
and administered by the IDM Programme Office.
223
References
M. Baroni, S. Bernardini, A. Ferraresi, and E. Zanchetta.
2009. The WaCky wide web: A collection of very
large linguistically processed web-crawled corpora.
Language Resources and Evaluation, 43(3):209?226.
S. Bergsma, D. Lin, and R. Goebel. 2009. Web-scale
N-gram models for lexical disambiguation. In Pro-
ceedings of the Twenty-First International Joint Con-
ference on Artificial Intelligence, pages 1507?1512,
Pasadena, California, USA.
S. Bergsma, E. Pitler, and D. Lin. 2010. Creating robust
supervised classifiers via web-scale N-gram data. In
Proceedings of the 48th Annual Meeting of the Associ-
ation for Computational Linguistics, pages 865?874,
Uppsala, Sweden.
T. Brants and A. Franz. 2006. Web 1T 5-gram corpus
version 1.1. Technical report, Google Research.
K. Crammer, M. Dredze, and A. Kulesza. 2009. Multi-
class confidence weighted algorithms. In Proceedings
of the 2009 Conference on Empirical Methods in Nat-
ural Language Processing, pages 496?504, Singapore.
D. Dahlmeier and H.T. Ng. 2011. Grammatical error cor-
rection with alternating structure optimization. In Pro-
ceedings of the 49th Annual Meeting of the Associa-
tion for Computational Linguistics: Human Language
Technologies, pages 915?923, Portland, Oregon, USA.
D. Dahlmeier, H.T. Ng, and T.P. Tran. 2011. NUS at
the HOO 2011 pilot shared task. In Proceedings of
the Generation Challenges Session at the 13th Eu-
ropean Workshop on Natural Language Generation,
pages 257?259, Nancy, France.
R. Dale, I. Anisimoff, and G. Narroway. 2012. HOO
2012: A report on the preposition and determiner error
correction shared task. In Proceedings of the Seventh
Workshop on Innovative Use of NLP for Building Ed-
ucational Applications, Montre?al, Que?bec, Canada.
M. Dredze, K. Crammer, and F. Pereira. 2008.
Confidence-weighted linear classification. In Pro-
ceedings of the 25th International Conference on Ma-
chine Learning, pages 184?191, Helsinki, Finland.
N.-R. Han, M. Chodorow, and C. Leacock. 2006. De-
tecting errors in English article usage by non-native
speakers. Natural Language Engineering, 12(2):115?
129.
P. Koehn, H. Hoang, A. Birch, C. Callison-Burch,
M. Federico, N. Bertoldi, B. Cowan, W. Shen,
C. Moran, R. Zens, C. Dyer, O. Bojar, A. Constantin,
and E. Herbst. 2007. Moses: Open source toolkit for
statistical machine translation. In Proceedings of the
ACL 2007 Demo and Poster Sessions, pages 177?180,
Prague, Czech Republic.
T. Kudo and Y. Matsumoto. 2003. Fast methods for
kernel-based text analysis. In Proceedings of the 41st
Annual Meeting of the Association for Computational
Linguistics, pages 24?31, Sapporo, Japan.
R. Mitton. 1992. A description of a computer-usable dic-
tionary file based on the Oxford Advanced Learner?s
Dictionary of Current English.
J. Nivre, J. Hall, J. Nilsson, A. Chanev, G. Eryigit,
S. Ku?bler, S. Marinov, and M. Marsi. 2007. Malt-
Parser: A language-independent system for data-
driven dependency parsing. Natural Language Engi-
neering, 13(2):95?135.
A. Rozovskaya and D. Roth. 2010. Training paradigms
for correcting errors in grammar and usage. In Pro-
ceedings of Human Language Technologies: The 2010
Annual Conference of the North American Chapter of
the ACL, pages 154?162, Los Angeles, California.
A. Rozovskaya, M. Sammons, J. Gioja, and D. Roth.
2011. University of Illinois system in HOO text cor-
rection shared task. In Proceedings of the Generation
Challenges Session at the 13th European Workshop on
Natural Language Generation, pages 263?266, Nancy,
France.
D. Talbot and M. Osborne. 2007. Randomised language
modelling for statistical machine translation. In Pro-
ceedings of the 45th Annual Meeting of the Association
of Computational Linguistics, pages 512?519, Prague,
Czech Republic.
J. Tetreault and M. Chodorow. 2008. The ups and downs
of preposition error detection in ESL writing. In
Proceedings of the 22nd International Conference on
Computational Linguistics, pages 865?872, Manch-
ester, UK.
J. Tetreault, J. Foster, and M. Chodorow. 2010. Using
parse features for preposition selection and error de-
tection. In Proceedings of the ACL 2010 Conference
Short Papers, pages 353?358, Uppsala, Sweden.
H. Yannakoudakis, T. Briscoe, and B. Medlock. 2011.
A new dataset and method for automatically grading
ESOL texts. In Proceedings of the 49th Annual Meet-
ing of the Association for Computational Linguistics:
Human Language Technologies, pages 180?189, Port-
land, Oregon, USA.
224
Proceedings of the Eighth Workshop on Innovative Use of NLP for Building Educational Applications, pages 22?31,
Atlanta, Georgia, June 13 2013. c?2013 Association for Computational Linguistics
Building a Large Annotated Corpus of Learner English:
The NUS Corpus of Learner English
Daniel Dahlmeier1,2 and Hwee Tou Ng2,3 and Siew Mei Wu4
1SAP Technology and Innovation Platform, SAP Singapore
d.dahlmeier@sap.com
2NUS Graduate School for Integrative Sciences and Engineering
3Department of Computer Science, National University of Singapore
{danielhe,nght}@comp.nus.edu.sg
4Centre for English Language Communication, National University of Singapore
elcwusm@nus.edu.sg
Abstract
We describe the NUS Corpus of Learner En-
glish (NUCLE), a large, fully annotated cor-
pus of learner English that is freely available
for research purposes. The goal of the cor-
pus is to provide a large data resource for the
development and evaluation of grammatical
error correction systems. Although NUCLE
has been available for almost two years, there
has been no reference paper that describes the
corpus in detail. In this paper, we address
this need. We describe the annotation schema
and the data collection and annotation process
of NUCLE. Most importantly, we report on
an unpublished study of annotator agreement
for grammatical error correction. Finally, we
present statistics on the distribution of gram-
matical errors in the NUCLE corpus.
1 Introduction
Grammatical error correction for language learners
has recently attracted increasing interest in the natu-
ral language processing (NLP) community. Gram-
matical error correction has the potential to cre-
ate commercially viable software tools for the large
number of students around the world who are
studying a foreign language, in particular the large
number of students of English as a Foreign Lan-
guage (EFL).
The success of statistical methods in NLP over
the last two decades can largely be attributed to
advances in machine learning and the availability
of large, annotated corpora that can be used to
train and evaluate statistical models for various NLP
tasks. The biggest obstacle for grammatical error
correction has been that until recently, there was no
large, annotated corpus of learner text that could
have served as a standard resource for empirical ap-
proaches to grammatical error correction (Leacock
et al, 2010). The existing annotated learner corpora
were all either too small or proprietary and not avail-
able to the research community. That is why we
decided to create the NUS Corpus of Learner En-
glish (NUCLE), a large, annotated corpus of learner
texts that is freely available for research purposes.
The corpus was built in collaboration with the Cen-
tre for English Language Communication (CELC)
at NUS. NUCLE consists of about 1,400 student es-
says from undergraduate university students at NUS
with a total of over one million words which are
completely annotated with error tags and correc-
tions. All annotations and corrections have been per-
formed by professional English instructors. To the
best of our knowledge, NUCLE is the first annotated
learner corpus of this size that is freely available for
research purposes. However, although the NUCLE
corpus has been available for almost two years now,
there has been no reference paper that describes the
details of the corpus. That makes it harder for other
researchers to start working with the NUCLE cor-
pus. In this paper, we address this need by giving a
detailed description of the NUCLE corpus, includ-
ing a description of the annotation schema, the data
collection and annotation process, and various statis-
tics on the distribution of grammatical errors in the
corpus. Most importantly, we report on an unpub-
lished study of annotator agreement for grammatical
error correction that was conducted prior to creating
22
Figure 1: The WAMP annotation interface
the NUCLE corpus. The study gives some insights
regarding the difficulty of the annotation task.
The remainder of this paper is organized as fol-
lows. The next section explains the annotation
schema that was used for labeling grammatical er-
rors. Section 3 reports the results of the inter-
annotator agreement study. Section 4 describes the
data collection and annotation process. Section 5
contains the error statistics. Section 6 gives the re-
lated work, and Section 7 concludes the paper.
2 Annotation Schema
Before starting the corpus creation, we had to de-
velop a set of annotation guidelines. This was done
in a pilot study before the actual corpus was cre-
ated. Three instructors from CELC participated in
the pilot study. The instructors annotated a small set
of student essays that had been collected by CELC
previously. The annotation was performed using an
in-house, online annotation tool, calledWriting, An-
notation, and Marking Platform (WAMP), that was
developed by the NUS NLP group specially for cre-
ating the NUCLE corpus. The annotation tool al-
lows the annotators to work over the Internet using
a web browser. Figure 1 shows a screen shot of the
WAMP interface. Annotators can browse through a
batch of essays that has been assigned to them and
perform the following tasks:
? Select arbitrary, contiguous text spans using the
cursor to identify grammatical errors.
? Classify errors by choosing an error tag from a
drop-down menu.
? Correct errors by typing the correction into a
text box.
? Comment to give additional explanations if
necessary.
We wanted to impose as few constraints as possi-
ble on the annotators and to give them an experience
that would closely resemble their usual marking us-
ing pen and paper. Therefore, the WAMP annotation
tool allows annotators to select arbitrary text spans,
including overlapping text spans.
After some annotation trials, we decided to use
a tag set which had been developed by CELC in
a previous study. Some minor modifications were
made to the original tag set based on the feedback
of the annotators. The result of the pilot study was
a tag set of 27 error categories which are grouped
into 13 categories. The tag set is listed in Table 1.
It is important to note that our annotation schema
not only labels each grammatical error with an error
category, but also requires an annotator to provide a
suitable correction for the error as well. The anno-
tators were asked to provide a correction that would
fix the grammatical error if the selected text span
containing the grammatical error is replaced with the
correction. If multiple alternative text spans could be
selected, the annotators were asked to select the min-
imal text span so that minimal changes were made to
arrive at the corrected text.
We chose to use the tag set in Table 1 since this
tag set was developed and used in a previous study
at CELC and was found to be a suitable tag set. Fur-
thermore, the tag set offers a reasonable compro-
mise in terms of its complexity. With 27 error cate-
gories, it is sufficiently fine-grained to enable mean-
ingful statistics for different error categories, yet not
as complex as other tag sets that are much larger in
size.
3 Annotator Agreement
How reliably can human annotators agree on
whether a word or sentence is grammatically cor-
rect? The pilot annotation project gave us the op-
portunity to investigate this question in a quantita-
tive analysis. Annotator agreement is also a mea-
sure for how difficult a task is and serves as a test of
whether humans can reliable perform the annotation
task with the given tag set. During the pilot study,
we randomly sampled 100 essays for measuring an-
notator agreement. These essays are part of the pilot
23
Error Tag Error Category Description / Example
Verbs
Vt Verb Tense A university [had conducted ? conducted] the survey
last year.
Vm Verb modal No one [will ? would] bother to consider a natural bal-
ance.
V0 Missing verb This [may ? may be] due to a traditional notion that
boys would be the main labor force in a farm family.
Vform Verb form Will the child blame the parents after he [growing ?
grows] up?
Subject-verb agreement
SVA Subject-verb-agreement The boy [play ? plays] soccer.
Articles/determiners
ArtOrDet Article or Determiner From the ethical aspect, sex selection technology should
not be used in [non-medical ? a non-medical] situa-
tion.
Nouns
Nn Noun Number Sex selection should therefore be used for medical [rea-
son ? reasons] and nothing else.
Npos Noun possessive The education of [mother?s ? mothers] is a significant
factor in reducing son preference.
Pronouns
Pform Pronoun form 90% of couples seek treatment for family balancing rea-
sons and 80% of [those ? them] want girls.
Pref Pronoun reference Moreover, children may find it hard to communicate with
[his/her ? their] parents.
Word choice
Wcip Wrong colloca-
tion/idiom/preposition
Singapore, for example, has invested heavily [on ? in]
the establishment of Biopolis
Wa Acronyms Using acronyms without explaining what they stand for.
Wform Word form Sex-selection may also result in [addition? additional]
stress for the family.
Wtone Tone [Isn?t it ? Is it not] what you always dreamed for?
Sentence Structure
Srun Runons, comma splice [Do spare some thought and time, we can make a dif-
ference! ? Do spare some thought and time. We can
make a difference!] (Should be split into two sentences)
Smod Dangling modifier [Faced ? When we are faced ] with the unprecedented
energy crisis, finding an alternative energy resource has
naturally become the top priority issue.
Spar Parallelism The use of sex selection would prevent rather than [con-
tributing ? contribute] to a distorted sex ratio.
Sfrag Fragment Although he is a student from the Arts faculty.
Ssub Subordinate clause It is the wrong mindset of people that boys are more su-
perior than girls [should ? that should] be corrected.
Table 1: NUCLE error categories. Grammatical errors in the examples are printed in bold face in the form
[<mistake>? <correction>].
24
Error Tag Error Category Description / Example
Word Order
WOinc Incorrect sentence form Why can [not we ? we not] choose more intelligent and
beautiful babies?
WOadv Adverb/adjective position It is similar to the murder of many valuable lives [only
based ? based only] on the couple?s own wish.
Transitions
Trans Link words/phrases In the process of selecting the gender of the child, ethical
problems arise [where ? because] many innocent lives
of unborn fetuses are taken away.
Mechanics
Mec Punctuation, capitalization,
spelling, typos
The [affect ? effect] of that policy has yet to be felt.
Redundancy
Rloc Local redundancy Currently, abortion is available to end a life only [because
of ? because] the fetus or embryo has the wrong sex.
Citation
Cit Citation Poor citation practice.
Others
Others Other errors Any error that does not fit into any other category, but can
still be corrected.
Um Unclear meaning The quality of the passage is so poor that it cannot be
corrected.
Table 1: NUCLE error categories (continued)
data set and are not included in the official NUCLE
corpus. The essays were then annotated by our three
annotators in a way that each essay was annotated
independently by two annotators. Four essays had to
be discarded as they were of very poor quality and
did not allow for any meaningful correction. This
left us with 96 essays with double annotation.
Comparing two sets of annotation is complicated
by the fact that the set of annotations that corrects
an input text to a corrected output text is ambigu-
ous (Dahlmeier and Ng, 2012). In other words, it is
possible that two different sets of annotations pro-
duce the same correction. For example, one anno-
tator could choose to select a whole phrase as one
error, while the other annotator selects each word
individually. Our annotation guidelines ask annota-
tors to select the minimum span that is necessary to
correct the error, but we do not enforce any hard con-
straints and different annotators can have a different
perception of where an error starts or ends.
An especially difficult case is the annotation of
omission errors, for example missing articles. Se-
lecting a range of whitespace characters is difficult
for annotators, especially if the annotation tool is
web-based (as whitespace is variable in web pages).
We asked annotators to select the previous or next
word and include them into the suggested correc-
tion. To change conduct survey to conduct a sur-
vey, the annotator could change conduct to conduct
a, or change survey to a survey. If we only com-
pare the exact text spans selected by the annotators
when measuring agreement, these different ways to
select the context could easily cause us to conclude
that the annotators disagree when they in fact agree
on the corrected phrase. This would lead to an un-
derestimation of annotator agreement. To address
this problem, we perform a simple text span nor-
malization. First, we ?grow? the selected context
to align with whitespace boundaries. For example,
if an annotator just selected the last character e of
the word use and provided ed as a correction, we
grow this annotation so that the whole word use is
selected and used is the correction. Second, we to-
kenize the text and ?trim? the context by removing
tokens at the start and end that are identical in the
original and the correction. Finally, the annotations
are ?projected? onto the individual tokens they span,
i.e., an annotation that spans a phrase of multiple to-
25
Source : This phenomenon opposes the real .
Annotator A : This phenomenon opposes (the ?  (ArtOrDet)) (real ? reality (Wform)) .
Annotator B : This phenomenon opposes the (real ? reality (Wform)) .
Table 2: Example of a sentence from the annotator agreement study with annotations from two different annotators.
kens is broken up into multiple token-level annota-
tions. We align the tokens in the original text span
and the tokenized correction string using minimum
edit distance. Now, we can compare two annotations
in a more meaningful way at the token level. Table 2
shows a tokenized example sentence from the anno-
tator agreement study with annotations from two dif-
ferent annotators. Annotator A and B agree that the
first three words This, phenomenon, and opposes and
the final period are correct and do not need any cor-
rection. The annotators also agree that the word real
is part of a word form (Wform) error and should be
replaced with reality. However, they disagree with
respect to the article the: annotator A believes there
is an article error (ArtOrDet) and that the article has
to be deleted while annotator B believes that the ar-
ticle is acceptable in this position.
The example shows that annotator agreement can
be measured with respect to three different criteria:
whether there is an error, what type of error it is,
and how the error should be corrected. Accordingly,
we analyze annotator agreement under three differ-
ent conditions:
? Identification Agreement of tagged tokens re-
gardless of error category or correction.
? Classification Agreement of error category,
given identification.
? Exact Agreement of error category and correc-
tion, given identification.
In the identification task, we are interested to see
how well annotators agree on whether something is
a grammatical error or not. In the example above,
annotators A and B agree on 5 out of 6 tokens and
disagree on one token (the). That results in an identi-
fication agreement of 5/6 = 83%. In the classifica-
tion task, we investigate how well annotators agree
on the type of error, given that both have tagged the
token as an error. In the example, the classification
agreement is 100% as both annotator A and B tagged
the word real as a word form (Wform) error. Finally,
for the exact task, annotators are considered to agree
if they agree on the error category and the correction
given that they both have tagged the token as an er-
ror. In the example, the exact agreement is 100% as
both annotators give the same error category Wform
and the same correction reality for the word real. We
use the popular Cohen?s Kappa coefficient (Cohen,
1960) to measure annotator agreement between an-
notators. Cohen?s Kappa is defined as
? =
Pr(a)? Pr(e)
1? Pr(e)
(1)
where Pr(a) is the probability of agreement and
Pr(e) is the probability of chance agreement. We
can estimate Pr(a) and Pr(e) from the double an-
notated essays through maximum likelihood estima-
tion. For two annotators A and B, the probability of
agreement is
Pr(a) =
#agreed tokens
#total tokens
(2)
where the number of agreed tokens is counted as de-
scribed above, and the total number of tokens is the
total token count of the subset of jointly annotated
documents. The probability of chance agreement is
computed as
Pr(e) = Pr(A = 1, B = 1) + Pr(A = 0, B = 0)
= Pr(A = 1)? Pr(B = 1)
+Pr(A = 0)? Pr(B = 0)
where Pr(A = 1) and Pr(A = 0) symbolize the
events of annotator A tagging a token as ?error? or
?no error? respectively. We make use of the fact
that both annotators perform the task independently.
Pr(A = 1) and Pr(A = 0) can be computed
through maximum likelihood estimation.
Pr(A = 1) =
# annotated tokens of annotator A
# total tokens
Pr(A = 0) =
# unannotated tokens of annotator A
# total tokens
26
Annotators Kappa-iden Kappa-class Kappa-exact
A ? B 0.4775 0.6206 0.5313
A ? C 0.3627 0.5352 0.4956
B ? C 0.3230 0.4894 0.4246
Average 0.3877 0.5484 0.4838
Table 3: Cohen?s Kappa for annotator agreement.
The probabilities Pr(B = 1) and Pr(B = 0) are
computed analogously. The chance agreement for
this task is quite high, as the number of un-annotated
tokens is much higher than the number of annotated
tokens. Cohen?s Kappa coefficients for the three an-
notators and the average Kappa coefficient are listed
in Table 3. We observe that the Kappa scores are
relatively low and that there is a substantial amount
of variability in the Kappa coefficients; annotator A
and B show a higher agreement with each other than
they do with annotator C. According to Landis and
Koch (1977), Kappa scores between 0.21 and 0.40
are considered fair, and scores between 0.41 and
0.60 are considered moderate. The average Kappa
score for identification can therefore only be consid-
ered fair and the Kappa scores for classification and
exact agreement are moderate. Thus, an interesting
result of the pilot study was that annotators find it
harder to agree on whether a word is grammatically
correct than agreeing on the type of error or how it
should be corrected. The annotator agreement study
shows that grammatical error correction, especially
grammatical error identification, is a difficult prob-
lem.
Our findings support previous research on an-
notator agreement that has shown that grammati-
cal error correction is a challenging task (Tetreault
and Chodorow, 2008; Lee et al, 2009). Tetreault
and Chodorow (2008) report a Kappa score of 0.63
which in their words ?shows the difficulty of this
task and also show how two highly trained raters
can produce very different judgments.? An interest-
ing related work is (Lee et al, 2009) which investi-
gates the annotation of article and noun number er-
rors. The annotation is performed with either a sin-
gle sentence context only or the five preceding sen-
tences. The agreement between annotators increases
when more context is given, from a Kappa score of
0.55 to a Kappa score of 0.60. Madnani et al (2011)
and Tetreault et al (2010) propose crowdsourcing to
overcome the problem of annotator variability.
4 Data Collection and Annotation
The main data collection for the NUCLE corpus
took place between August and December 2009. We
collected a total of 2,249 student essays from 6 En-
glish courses at CELC. The courses are designed for
students who need language support for their aca-
demic studies. The essays were written as course
assignments on a wide range of topics, like technol-
ogy innovation or health care. Some example ques-
tion prompts are shown in Table 4. All students are
at a similar academic level, as they are all undergrad-
uate students at NUS. Students would typically have
to write two essay assignments during a course. The
length of each essay was supposed to be around 500
words, although most essays were longer than the re-
quired length. From this data set, a team of 10 CELC
English instructors annotated 1,414 essays with over
1.2 million words between October 2009 and April
2010. Due to budget constraints, we were unfortu-
nately not able to perform double annotations for the
main corpus. Annotators were allowed to label an
error multiple times if the error could be assigned
to more than one error tag, although we observed
that annotators did not make much use of this option.
Minimal post-processing was done after the annota-
tion process. Annotators were asked to review some
corrections that appeared to contain annotation mis-
takes, for example redundancy errors that did not re-
move the annotated word. The final results of the
annotation exercise were a total of 46,597 error tags.
The essays and the annotations were released as the
NUCLE corpus through the NUS Enterprise R2M
portal in June 2011. The link to the corpus can be
found on the NUS NLP group?s website1.
5 NUCLE Corpus Statistics
This section provides basic statistics about the NU-
CLE corpus and the collected annotations. These
statistics already reveal some interesting insights
about the nature of grammatical errors in learner
text. In particular, we are interested in the follow-
ing questions: how frequent are errors in the NU-
CLE corpus and what are the most frequent error
1www.comp.nus.edu.sg/?nlp/corpora.html
27
?Public spending on the aged should be limited so that money can be diverted to other areas of the country?s develop-
ment.? Do you agree?
Surveillance technology such as RFID (radio-frequency identification) should not be used to track people (e.g., human
implants and RFID tags on people or products). Do you agree? Support your argument with concrete examples.
Choose a concept or prototype currently in research and development and not widely available in the market. Present
an argument on how the design can be improved to enhance safety. Remember to consider influential factors such as
cost or performance when you summarize and rebut opposing views. You will need to include very recently published
sources in your references.
Table 4: Example question prompts from the NUCLE corpus.
NUS Corpus of Learner English
Documents 1,414
Sentences 59,871
Word tokens 1,220,257
Word types 30,492
Error annotations 46,597
# of sentences per document 42.34
# of word tokens per document 862.98
# of word tokens per sentence 20.38
# of error annotations per document 32.95
# of error annotations per 100 word tokens 3.82
Table 5: Overview of the NUCLE corpus
categories? The basic statistics of the NUCLE cor-
pus are shown in Table 5. In these statistics, we
treat multiple alternative annotations for the same
error as separate errors, although it could be argued
that these should be merged into a single error with
multiple alternative corrections. Fortunately, only
about 1% of the errors are labeled with more than
one annotation. We can see that grammatical errors
are very sparse, even in learner text. In the NU-
CLE corpus, there are 46,597 annotated errors for
1,220,257 word tokens. That makes an error density
of 3.82 errors per hundred words. In other words,
most of the word tokens in the corpus are grammat-
ically correct. This shows that the students whose
essays were used for the corpus already have a rel-
ative high proficiency of English. When we look
at the distribution of errors across documents, we
can make another interesting observation. Figure 2
shows a histogram of the number of error annota-
tions per document. The distribution appears non-
Gaussian and is heavily skewed to the left with most
documents having less than 30 errors while some
documents have significantly more errors than the
average document. That means that although gram-
matical errors are rare in general, there are also doc-
 0
 5
 10
 15
 20
 25
 30
 35
 40
 0  20  40  60  80  100  120  140  160  180  200
Nu
mb
er o
f do
cum
ent
s
Number of error annotations
Histogram: error annotation per document
Figure 2: Histogram of error annotations per document
in NUCLE.
uments with many error annotations. 32 documents
have more than 100 error annotations and the highest
number of error annotations in a document is 194.
The mode, i.e., the most frequent value in the his-
togram, is 15 which is to the left of the average of
32.95. A similar pattern can be observed when we
look at the distribution of errors per sentence. Fig-
ure 3 shows a histogram of the number of error anno-
tations per sentence in the NUCLE corpus. For this
histogram, only the error annotations which start and
end within sentence boundaries are considered (this
accounts for 98.6% of all error annotations). Sen-
tence boundaries are determined automatically using
the NLTK Punkt sentence splitter2. The histogram
shows that 57.64% of all sentences have zero errors,
20.48% have exactly one error, and 10.66% have ex-
actly two errors, and 11.21% of all sentences have
more than two errors. Although the frequency de-
creases quickly for higher error counts, the highest
observed number of error annotations for a sentence
is 28.
2nltk.org
28
 0
 5000
 10000
 15000
 20000
 25000
 30000
 35000
 0  5  10  15  20  25
Nu
mb
er o
f se
nte
nce
s
Number of error annotations
Histogram: error annotation per sentence
Figure 3: Histogram of error annotations per sentence in
NUCLE.
The skewed distribution of errors in the NUCLE
corpus is an interesting observation. A possible ex-
planation for the long tail of the distribution could be
a ?rich-get-richer? type of dynamics: if a learner has
made a lot of mistakes in her essay so far, the chance
of her making more errors in the remainder of the
essay increases, for example because she makes sys-
tematic errors which are likely to be repeated. Ex-
plaining the cognitive processes that produce the ob-
served error distribution is beyond the scope of this
paper, but it would certainly be an interesting ques-
tion to investigate.
So far, we have only been concerned with how
many errors learners make overall. But it is also
important to understand what types of errors lan-
guage learners make. Error categories that appear
more frequently should be addressed with higher
priority when creating an automatic error correction
system. Figure 4 shows a histogram of error cate-
gories. Again, we can observe a skewed distribu-
tion with a few error categories being very frequent
and many error categories being comparatively in-
frequent. The top five error categories are wrong
collocation/idiom/preposition (Wcip) with 7,312 in-
stances or 15.69% of all annotations, local redun-
dancies (Rloc) (6,390 instances, 13.71%), article or
determiner (ArtOrDet) (6,004 instances, 12.88%),
noun number (Nn) (3,955 instances, 8.49%), and
mechanics (Mec) (3,290 instances, 7.06%). These
top five error categories account for 57.83% of all er-
ror annotations. The next 5 categories are verb tense
(Vt) (3,288 instances, 7.06%) word form (Wform)
 0
 1000
 2000
 3000
 4000
 5000
 6000
 7000
 8000
Wcip
Rloc
ArtOrDet
Nn Mec
Vt Wform
SVA
Others
Vform
Trans
Um Pref
Srun
Cit WOinc
Wtone
Spar
Vm V0 Ssub
WOadv
Npos
Sfrag
Pform
Smod
Wa
Nu
mb
er 
of 
an
no
tat
ion
s
Error categories 
Error categories
Figure 4: Error categories histogram for the NUCLE cor-
pus.
(2,241 instances, 4.81%), subject-verb agreement
(SVA) (1,578 instances, 3.38%), other errors that
could not be grouped into any of the error categories
(1,532 instances, 3.29%), and Verb form (Vform)
(1,531, 3.29%). Together, the top 10 error cate-
gories account for 79.66% of all annotated errors.
A manual inspection showed that a large percentage
of the local redundancy errors involve articles that
are deemed redundant by the annotator and should
be deleted. These errors could also be considered
article or determiner errors. For the Wcip errors,
we observed that most Wcip errors are preposition
errors. This confirms that articles and prepositions
are the two most frequent error categories for EFL
learners (Leacock et al, 2010).
6 Related Work
In this section, we compare NUCLE with other
learner corpora. While there were almost no an-
notated learner corpora available for research pur-
poses until recently, non-annotated learner corpora
have been available for a while. Two examples are
the International Corpus of Learner English (ICLE)
(Granger et al, 2002) and the Chinese Learner En-
glish Corpus (Gui and Yang., 2003)3. Rozovskaya
and Roth (2010) annotated a portion of each of these
two learner corpora with error categories and correc-
tions. However, with 63,000 words, the annotated
data is small compared to NUCLE.
3The Chinese Learner English Corpus contains annotations
for error types but does not include corrections for the errors.
29
The Cambridge Learner Corpus (CLC) (Nicholls,
2003) is possibly the largest annotated English
learner corpus. Unfortunately, to our knowledge,
the corpus is not freely available for research pur-
poses. A subset of the CLC was released in 2011
by Yannakoudakis et al (2011). The released data
set contains short essays written by students taking
the First Certificate in English (FCE) examination.
The data set was also used in the recent HOO 2012
shared task on preposition and determiner correction
(Dale et al, 2012). Comparing the essays in the FCE
data set and NUCLE, we observe that the essays in
the FCE data set are shorter than the essays in NU-
CLE and show a higher density of grammatical er-
rors. One reason for the higher number of errors (in
particular spelling errors) is most likely that the FCE
data was not collected from take-home assignments
where students have the chance to spell check their
writing before submission. But it could also mean
that the essays in FCE are from students with a lower
proficiency in English compared to NUCLE. With
regards to the annotation schema, the CLC annota-
tions include both the type of error (missing, unnec-
essary, replacement, form) and the part of speech.
As a result, the CLC tag set is large with 88 differ-
ent error categories, far more than the 27 error cate-
gories in NUCLE.
Finally, the HOO 2011 shared task (Dale and Kil-
garriff, 2011) released an annotated corpus of frag-
ments from academic papers written by non-native
speakers and published in a conference or work-
shop of the Association for Computational Linguis-
tics. The corpus uses the annotation schema from
the CLC. Comparing the data set with NUCLE, the
HOO 2011 data set is much smaller (about 20,000
words for training and testing, respectively) and rep-
resents a specific writing genre (NLP papers). The
NUCLE corpus is much larger and covers a broader
range of topics.
7 Conclusion
We have presented the NUS Corpus of Learner En-
glish (NUCLE), a large, annotated corpus of learner
English. The corpus contains over one million
words which are completely annotated with gram-
matical errors and corrections. The NUCLE corpus
is freely available for research purposes. We have
also reported an inter-annotator agreement study for
grammatical error correction. The study shows that
grammatical error correction is a difficult task, even
for humans. The error statistics from the NUCLE
corpus show that learner errors are generally sparse
and have a long-tail distribution.
Acknowledgments
This research is supported by the Singapore Na-
tional Research Foundation under its International
Research Centre @ Singapore Funding Initiative
and administered by the IDM Programme Office.
References
J. Cohen. 1960. A coefficient of agreement for nominal
scales. Educational and Psychological Measurement,
20(1):37?46.
D. Dahlmeier and H.T. Ng. 2012. Better evaluation for
grammatical error correction. In Proceedings of HLT-
NAACL, pages 568?572.
R. Dale and A. Kilgarriff. 2011. Helping Our Own:
The HOO 2011 pilot shared task. In Proceedings of
the Generation Challenges Session at the 13th Eu-
ropean Workshop on Natural Language Generation,
pages 242?249.
R. Dale, I. Anisimoff, and G. Narroway. 2012. HOO
2012: A report on the preposition and determiner error
correction shared task. In Proceedings of the Seventh
Workshop on Innovative Use of NLP for Building Ed-
ucational Applications, pages 54?62.
S. Granger, F. Dagneaux, E. Meunier, and M. Paquot.
2002. The International Corpus of Learner English.
Presses Universitaires de Louvain, Louvain-la-Neuve,
Belgium.
S. Gui and H. Yang. 2003. Zhongguo Xuexizhe Yingyu
Yuliaohu (Chinese Learner English Corpus). Shang-
hai Waiyu Jiaoyu Chubanshe. In Chinese.
J.R. Landis and G.G Koch. 1977. The measurement of
observer agreement for categorical data. Biometrics,
33(1):159?174.
C. Leacock, M. Chodorow, M. Gamon, and J. Tetreault.
2010. Automated Grammatical Error Detection for
Language Learners. Morgan & Claypool Publishers.
J. Lee, J. Tetreault, and M. Chodorow. 2009. Human
evaluation of article and noun number usage: Influ-
ences of context and construction variability. In Pro-
ceedings of the Linguistic Annotation Workshop III
(LAW3), pages 60?63.
30
N. Madnani, J. Tetreault, M. Chodorow, and R. Ro-
zovskaya. 2011. They can help: using crowdsourc-
ing to improve the evaluation of grammatical error de-
tection systems. In Proceedings of ACL:HLT, pages
508?513.
D. Nicholls. 2003. The Cambridge learner corpus: Error
coding and analysis for lexicography and ELT. In Pro-
ceedings of the Corpus Linguistics 2003 Conference,
pages 572?581.
A. Rozovskaya and D. Roth. 2010. Annotating ESL er-
rors: Challenges and rewards. In Proceedings of the
Fifth Workshop on Innovative Use of NLP for Building
Educational Applications, pages 28?36.
J. Tetreault and M. Chodorow. 2008. Native judgments
of non-native usage: Experiments in preposition error
detection. In Proceedings of the Workshop on Human
Judgements in Computational Linguistics, pages 24?
32.
J. Tetreault, E. Filatova, and M. Chodorow. 2010. Re-
thinking grammatical error annotation and evaluation
with the Amazon Mechanical Turk. In Proceedings
of the Fifth Workshop on Innovative Use of NLP for
Building Educational Applications, pages 45?48.
H. Yannakoudakis, T. Briscoe, and B. Medlock. 2011.
A new dataset and method for automatically grading
ESOL texts. In Proceedings of ACL:HLT, pages 180?
189.
31
