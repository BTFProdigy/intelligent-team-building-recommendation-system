Proceedings of the Third Workshop on Statistical Machine Translation, pages 35?43,
Columbus, Ohio, USA, June 2008. c?2008 Association for Computational Linguistics
Learning Performance of a Machine Translation System: a Statistical and
Computational Analysis
Marco Turchi Tijl De Bie
Dept. of Engineering Mathematics
University of Bristol,
Bristol, BS8 1TR, UK
{Marco.Turchi, Tijl.DeBie}@bristol.ac.uk
nello@support-vector.net
Nello Cristianini
Abstract
We present an extensive experimental study
of a Statistical Machine Translation system,
Moses (Koehn et al, 2007), from the point
of view of its learning capabilities. Very ac-
curate learning curves are obtained, by us-
ing high-performance computing, and extrap-
olations are provided of the projected perfor-
mance of the system under different condi-
tions. We provide a discussion of learning
curves, and we suggest that: 1) the represen-
tation power of the system is not currently a
limitation to its performance, 2) the inference
of its models from finite sets of i.i.d. data
is responsible for current performance limita-
tions, 3) it is unlikely that increasing dataset
sizes will result in significant improvements
(at least in traditional i.i.d. setting), 4) it is un-
likely that novel statistical estimation methods
will result in significant improvements. The
current performance wall is mostly a conse-
quence of Zipf?s law, and this should be taken
into account when designing a statistical ma-
chine translation system. A few possible re-
search directions are discussed as a result of
this investigation, most notably the integra-
tion of linguistic rules into the model inference
phase, and the development of active learning
procedures.
1 Introduction and Background
The performance of every learning system is the re-
sult of (at least) two combined effects: the repre-
sentation power of the hypothesis class, determin-
ing how well the system can approximate the target
behaviour; and statistical effects, determining how
well the system can approximate the best element of
the hypothesis class, based on finite and noisy train-
ing information. The two effects interact, with richer
classes being better approximators of the target be-
haviour but requiring more training data to reliably
identify the best hypothesis. The resulting trade-
off, equally well known in statistics and in machine
learning, can be expressed in terms of bias variance,
capacity-control, or model selection. Various theo-
ries on learning curves have been proposed to deal
with it, where a learning curve is a plot describing
performance as a function of some parameters, typ-
ically training set size.
In the context of Statistical Machine Translation
(SMT), where large bilingual corpora are used to
train adaptive software to translate text, this task is
further complicated by the peculiar distribution un-
derlying the data, where the probability of encoun-
tering new words or expressions never vanishes. If
we want to understand the potential and limitations
of the current technology, we need to understand the
interplay between these two factors affecting perfor-
mance. In an age where the creation of intelligent
behaviour is increasingly data driven, this is a ques-
tion of great importance to all of Artificial Intelli-
gence.
These observations lead us to an analysis of learn-
ing curves in machine translation, and to a number of
related questions, including an analysis of the flexi-
bility of the representation class used, an analysis of
the stability of the models with respect to perturba-
tions of the parameters, and an analysis of the com-
putational resources needed to train these systems.
Using the open source package Moses (Koehn et
35
al., 2007) and the Spanish-English Europarl corpus
(Koehn, 2005) we have performed a complete inves-
tigation of the influence of training set size on the
quality of translations and on the cost of training; the
influence of several design choices; the role of data
sizes in training various components of the system.
We use this data to inform a discussion about learn-
ing curves. An analysis of learning curves has pre-
viously been proposed by (Al-Onaizan et al, 1999).
Recent advances in software, data availability and
computing power have enabled us to undertake the
present study, where very accurate curves are ob-
tained on a large corpus.
Since our goal was to obtain high accuracy learn-
ing curves, that can be trusted both for compar-
ing different system settings, and to extrapolate per-
formance under unseen conditions, we conducted a
large-scale series of tests, to reduce uncertainty in
the estimations and to obtain the strongest possible
signals. This was only possible, to the degree of ac-
curacy needed by our analysis, by the extensive use
of a high performance computer cluster over several
weeks of computation.
One of our key findings is that the current per-
formance is not limited by the representation power
of the hypothesis class, but rather by model estima-
tion from data. And that increasing of the size of
the dataset is not likely to bridge that gap (at least
not for realistic amounts in the i.i.d. setting), nor is
the development of new parameter estimation prin-
ciples. The main limitation seems to be a direct
consequence of Zipf?s law, and the introduction of
constraints from linguistics seems to be an unavoid-
able step, to help the system in the identification of
the optimal models without resorting to massive in-
creases in training data, which would also result in
significantly higher training times, and model sizes.
2 Statistical Machine Translation
What is the best function class to map Spanish doc-
uments into English documents? This is a question
of linguistic nature, and has been the subject of a
long debate. The de-facto answer came during the
1990?s from the research community on Statistical
Machine Translation, who made use of statistical
tools based on a noisy channel model originally de-
veloped for speech recognition (Brown et al, 1994;
Och and Weber, 1998; R.Zens et al, 2002; Och and
Ney, 2001; Koehn et al, 2003). A Markovian lan-
guage model, based on phrases rather than words,
coupled with a phrase-to-phrase translation table are
at the heart of most modern systems. Translating a
text amounts to computing the most likely transla-
tion based on the available model parameters. Infer-
ring the parameters of these models from bilingual
corpora is a matter of statistics. By model inference
we mean the task of extracting all tables, parameters
and functions, from the corpus, that will be used to
translate.
How far can this representation take us towards
the target of achieving human-quality translations?
Are the current limitations due to the approximation
error of this representation, or to lack of sufficient
training data? How much space for improvement
is there, given new data or new statistical estima-
tion methods or given different models with differ-
ent complexities?
We investigate both the approximation and the es-
timation components of the error in machine transla-
tion systems. After analysing the two contributions,
we focus on the role of various design choices in
determining the statistical part of the error. We in-
vestigate learning curves, measuring both the role of
the training set and the optimization set size, as well
as the importance of accuracy in the numeric param-
eters.
We also address the trade-off between accuracy
and computational cost. We perform a complete
analysis of Moses as a learning system, assessing the
various contributions to its performance and where
improvements are more likely, and assessing com-
putational and statistical aspects of the system.
A general discussion of learning curves in Moses-
like systems and an extrapolation of performance
are provided, showing that the estimation gap is un-
likely to be closed by adding more data in realistic
amounts.
3 Experimental Setup
We have performed a large number of detailed ex-
periments. In this paper we report just a few, leaving
the complete account of our benchmarking to a full
journal version (Turchi et al, In preparation). Three
experiments allow us to assess the most promis-
36
ing directions of research, from a machine learning
point of view.
1. Learning curve showing translation perfor-
mance as a function of training set size, where
translation is performed on unseen sentences.
The curves, describing the statistical part of the
performance, are seen to grow very slowly with
training set size.
2. Learning curve showing translation perfor-
mance as a function of training set size, where
translation is performed on known sentences.
This was done to verify that the hypothesis
class is indeed capable of representing high
quality translations in the idealized case when
all the necessary phrases have been observed
in training phase. By limiting phrase length
to 7 words, and using test sentences mostly
longer than 20 words, we have ensured that this
was a genuine task of decoding. We observed
that translation in these idealized conditions is
worse than human translation, but much better
than machine translation of unseen sentences.
3. Plot of performance of a model when the nu-
meric parameters are corrupted by an increas-
ing amount of noise. This was done to simu-
late the effect of inaccurate parameter estima-
tion algorithms (due either to imprecise objec-
tive functions, or to lack of sufficient statistics
from the corpus). We were surprised to observe
that accurate estimation of these parameters ac-
counts for at most 10% of the final score. It is
the actual list of phrases that forms the bulk of
the knowledge in the system.
We conclude that the availability of the right mod-
els in the system would allow the system to have a
much higher performance, but these models will not
come from increased datasets or estimation proce-
dures. Instead, they will come from the results of ei-
ther the introduction of linguistic knowledge, or the
introduction of query algorithms, themselves result-
ing necessarily from confidence estimation meth-
ods. Hence these appear to be the two most pressing
questions in this research area.
3.1 Software
Moses (Koehn et al, 2007) is a complete translation
toolkit for academic purposes. It provides all the
components needed to create a machine translation
system from one language to another. It contains dif-
ferent modules to preprocess data, train the language
models and the translation models. These mod-
els can be tuned using minimum error rate training
(Och, 2003). Moses uses standard external tools for
some of these tasks, such as GIZA++ (Och and Ney,
2003) for word alignments and SRILM (Stolcke,
2002) for language modeling. Notice that Moses is a
very sophisticated system, capable of learning trans-
lation tables, language models and decoding param-
eters from data. We analyse the contribution of each
component to the overall score.
Given a parallel training corpus, Moses prepro-
cesses it removing long sentences, lowercasing and
tokenizing sentences. These sentences are used to
train the language and translation models. This
phase requires several steps as aligning words, com-
puting the lexical translation, extracting phrases,
scoring the phrases and creating the reordering
model. When the models have been created, the de-
velopment set is used to run the minimum error rate
training algorithm to optimize their weights. We re-
fer to that step as the optimization step in the rest of
the paper. Test set is used to evaluate the quality of
models on the data. The translated sentences are em-
bedded in a sgm format, such that the quality of the
translation can be evaluated using the most common
machine translation scores. Moses provides BLEU
(K.Papineni et al, 2001) and NIST (Doddington,
2002), but Meteor (Banerjee and Lavie, 2005; Lavie
and Agarwal, 2007) and TER (Snover et al, 2006)
can easily be used instead. NIST is used in this paper
as evaluation score after we observed its high corre-
lation to the other scores on the corpus (Turchi et al,
In preparation).
All experiments have been run using the default
parameter configuration of Moses. It means that
Giza++ has used IBM model 1, 2, 3, and 4 with
number of iterations for model 1 equal to 5, model
2 equal to 0, model 3 and 4 equal to 3; SRILM
has used n-gram order equal to 3 and the Kneser-
Ney smoothing algorithm; Mert has been run fix-
ing to 100 the number of nbest target sentence for
37
each develop sentence, and it stops when none of
the weights changed more than 1e-05 or the nbest
list does not change.
The training, development and test set sentences
are tokenized and lowercased. The maximum num-
ber of tokens for each sentence in the training pair
has been set to 50, whilst no limit is applied to the
development or test set. TMs were limited to a
phrase-length of 7 words and LMs were limited to
3.
3.2 Data
The Europarl Release v3 Spanish-English corpus
has been used for the experiments. All the pairs of
sentences are extracted from the proceedings of the
European Parliament.
This dataset is made of three sets of pairs of sen-
tences. Each of them has a different role: training,
development and test set. The training set contains
1,259,914 pairs, while there are 2,000 pairs for de-
velopment and test sets.
This work contains several experiments on differ-
ent types and sizes of data set. To be consistent
and to avoid anomalies due to overfitting or par-
ticular data combinations, each set of pairs of sen-
tences have been randomly sampled. The number of
pairs is fixed and a software selects them randomly
from the whole original training, development or test
set using a uniform distribution (bootstrap). Redun-
dancy of pairs is allowed inside each subset.
3.3 Hardware
All the experiments have been run on a cluster ma-
chine, http://www.acrc.bris.ac.uk/acrc/hpc.htm. It
includes 96 nodes each with two dual-core opteron
processors, 8 GB of RAM memory per node (2 GB
per core); 4 thick nodes each with four dual-core
opteron processors, 32 GB of RAM memory per
node (4 GB per core); ClearSpeed accelerator boards
on the thick nodes; SilverStorm Infiniband high-
speed connectivity throughout for parallel code mes-
sage passing; General Parallel File System (GPFS)
providing data access from all the nodes; storage -
11 terabytes. Each experiment has been run using
one core and allocating 4Gb of RAM.
4 Experiments
4.1 Experiment 1: role of training set size on
performance on new sentences
In this section we analyse how performance is af-
fected by training set size, by creating learning
curves (NIST score vs training set size).
We have created subsets of the complete corpus
by sub-sampling sentences from a uniform distribu-
tion, with replacement. We have created 10 random
subsets for each of the 20 chosen sizes, where each
size represents 5%, 10%, etc of the complete cor-
pus. For each subset a new instance of the SMT
system has been created, for a total of 200 mod-
els. These have been optimized using a fixed size
development set (of 2,000 sentences, not included
in any other phase of the experiment). Two hun-
dred experiments have then been run on an indepen-
dent test set (of 2,000 sentences, also not included in
any other phase of the experiment). This allowed us
to calculate the mean and variance of NIST scores.
This has been done for the models with and without
the optimization step, hence producing the learning
curves with error bars plotted in Figure 1, represent-
ing translation performance versus training set size,
in the two cases.
The growth of the learning curve follows a typi-
cal pattern, growing fast at first, then slowing down
(traditional learning curves are power laws, in theo-
retical models). In this case it appears to be grow-
ing even slower than a power law, which would be
a surprise under traditional statistical learning the-
ory models. In any case, the addition of massive
amounts of data from the same distribution will re-
sult into smaller improvements in the performance.
The small error bars that we have obtained also al-
low us to neatly observe the benefits of the optimiza-
tion phase, which are small but clearly significant.
4.2 Experiment 2: role of training set size on
performance on known sentences
The performance of a learning system depends both
on the statistical estimation issues discussed in the
previous subsection, and on functional approxima-
tion issues: how well can the function class repro-
duce the desired behaviour? In order to measure this
quantity, we have performed an experiment much
like the one described above, with one key differ-
38
0 2 4 6 8 10 12 14
x 105
6.8
6.9
7
7.1
7.2
7.3
7.4
7.5
7.6
Nist Score vs Training Size
Training Size
N
is
t S
co
re
 
 
Optimized
Not Optimized
Figure 1: ?Not Optimized? has been obtained using a
fixed test set and no optimization phase. ?Optimized?
using a fixed test set and the optimization phase.
ence: the test set was selected randomly from the
training set (after cleaning phase). In this way we
are guaranteed that the system has seen all the nec-
essary information in training phase, and we can as-
sess its limitations in these very ideal conditions.
We are aware this condition is extremely idealized
and it will never happen in real life, but we wanted
to have an upper bound on the performance achiev-
able by this architecture if access to ideal data was
not an issue. We also made sure that the perfor-
mance on translating training sentences was not due
to simple memorization of the entire sentence, ver-
ifying that the vast majority of the sentences were
not present in the translation table (where the max-
imal phrase size was 7), not even in reduced form.
Under these favourable conditions, the system ob-
tained a NIST score of around 11, against a score
of about 7.5 on unseen sentences. This suggests
that the phrase-based Markov-chain representation
is sufficiently rich to obtain a high score, if the nec-
essary information is contained in the translation and
language models.
For each model to be tested on known sentences,
we have sampled ten subsets of 2,000 sentences each
from the training set.
The ?Optimized, Test on Training Set? learn-
ing curve, see figure 2, represents a possible upper
bound on the best performance of this SMT sys-
tem, since it has been computed in favourable con-
ditions. It does suggest that this hypothesis class
has the power of approximating the target behaviour
more accurately than we could think based on per-
formance on unseen sentences. If the right informa-
tion has been seen, the system can reconstruct the
sentences rather accurately. The NIST score com-
puted using the reference sentences as target sen-
tences is around 15, we identify the relative curve as
?Human Translation?. At this point, it seems likely
that the process with which we learn the necessary
tables representing the knowledge of the system is
responsible for the performance limitations.
The gap between the ?Optimized, Test on Train-
ing Set? and the ?Optimized? curves is even more in-
teresting if related to the slow growth rate in the pre-
vious learning curve: although the system can repre-
sent internally a good model of translation, it seems
unlikely that this will ever be inferred by increasing
the size of training datasets in realistic amounts.
The training step results in various forms of
knowledge: translation table, language model and
parameters from the optimization. The internal
models learnt by the system are essentially lists
of phrases, with probabilities associated to them.
Which of these components is mostly responsible
for performance limitations?
4.3 Experiment 3: effect on performance of
increasing noise levels in parameters
Much research has focused on devising improved
principles for the statistical estimation of the pa-
rameters in language and translation models. The
introduction of discriminative graphical models has
marked a departure from traditional maximum like-
lihood estimation principles, and various approaches
have been proposed.
The question is: how much information is con-
tained in the fine grain structure of the probabilities
estimated by the model? Is the performance improv-
ing with more data because certain parameters are
estimated better, or just because the lists are grow-
ing? In the second case, it is likely that more sophis-
ticated statistical algorithms to improve the estima-
tion of probabilities will have limited impact.
In order to simulate the effect of inaccurate esti-
mation of the numeric parameters, we have added
increasing amount of noise to them. This can either
represent the effect of insufficient statistics in esti-
mating them, or the use of imperfect parameter esti-
39
0 2 4 6 8 10 12 14
x 105
6
7
8
9
10
11
12
13
14
15
16
Nist Score vs Training Size
Training Size
N
is
t S
co
re
 
 
Not Optimized
Optimized
Optimized, Test On Training Set
Human Translation
Figure 2: Four learning curves have been compared. ?Not Optimized? has been obtained using a fixed test set and no
optimization phase. ?Optimized? using a fixed test set and the optimization phase. ?Optimized Test On Training Set?
a test set selected by the training set for each training set size and the optimization phase. ?Human Translation? has
been obtained by computing NIST using the reference English sentence of the test set as target sentences.
mation biases. We have corrupted the parameters in
the language and translation models, by adding in-
creasing levels of noise to them, and measured the
effect of this on performance.
One model trained with 62,995 pairs of sentences
has been chosen from the experiments in Section
4.1. A percentage of noise has been added to each
probability in the language model, including condi-
tional probability and back off, translation model,
bidirectional translation probabilities and lexical-
ized weighting. Given a probability p and a percent-
age of noise, pn, a value has been randomly selected
from the interval [-x,+x], where x = p * pn, and
added to p. If this quantity is bigger than one it has
been approximated to one. Different values of per-
centage have been used. For each value of pn, five
experiment have been run. The optimization step
has not been run.
We see from Figure 3 that the performance does
not seem to depend crucially on the fine structure of
the parameter vectors, and that even a large addition
of noise (100%) produces a 10% decline in NIST
score. This suggests that it is the list itself, rather
0 10 20 30 40 50 60 70 80 90 100 110
6.6
6.65
6.7
6.75
6.8
6.85
"Perturbed" Nist Score vs Percentage of Perturbation
Percentage of Perturbation
"
Pe
rtu
rb
ed
" N
ist
 S
co
re
Figure 3: Each probability of the language and translation
models has been perturbed adding a percentage of noise.
This learning curve reports the not optimized NIST score
versus the percentage of perturbation applied. These re-
sults have been obtained using a fixed training set size
equal to 62,995 pairs of sentences.
40
0 2 4 6 8 10 12 14
x 105
0
500
1000
1500
2000
2500
CPU Computational Time in minutes vs Training Size
Training Size
CP
U 
Co
m
pu
ta
tio
na
l T
im
e 
in
 m
in
ut
es
 
 
Training Time
Tuning Time
Figure 4: Training and tuning user time vs training set
size. Time quantities are expressed in minutes.
than the probabilities in it, that controls the perfor-
mance. Different estimation methods can produce
different parameters, but this does not seem to mat-
ter very much. The creation of a more complete list
of words, however, seems to be the key to improve
the score. Combined with the previous findings, this
would mean that neither more data nor better statis-
tics will bridge the performance gap. The solution
might have to be found elsewhere, and in our Dis-
cussion section we outline a few possible avenues.
5 Computational Cost
The computational cost of models creation and
development-phase has been measured during the
creation of the learning curves. Despite its efficiency
in terms of data usage, the development phase has a
high cost in computational terms, if compared with
the cost of creating the complete language and trans-
lation models.
For each experiment, the user CPU time is com-
puted as the sum of the user time of the main process
and the user time of the children.
These quantities are collected for training, devel-
opment, testing and evaluation phases. In figure 4,
training and tuning user times are plotted as a func-
tion of the training set size. It is evident that increas-
ing the training size causes an increase in training
time in a roughly linear fashion.
It is hard to find a similar relationship for the tun-
ing time of the development phase. In fact, the tun-
ing time is strictly connected with the optimization
algorithm and the sentences in the development set.
We can also see in figure 4 that even a small devel-
opment set size can require a large amount of tun-
ing time. Each point of the tuning time curve has a
big variance. The tuning phase involves translating
the development set many times and hence its cost
depends very weakly on the training set size, since a
large training set leads to larger tables and these lead
to slightly longer test times.
6 Discussion
The impressive capability of current machine trans-
lation systems is not only a testament to an incredi-
bly productive and creative research community, but
can also be seen as a paradigm for other Artificial In-
telligence tasks. Data driven approaches to all main
areas of AI currently deliver the state of the art per-
formance, from summarization to speech recogni-
tion to machine vision to information retrieval. And
statistical learning technology is central to all ap-
proaches to data driven AI.
Understanding how sophisticated behaviour can
be learnt from data is hence not just a concern for
machine learning, or to individual applied commu-
nities, such as Statistical Machine Translation, but
rather a general concern for modern Artificial Intelli-
gence. The analysis of learning curves, and the iden-
tification of the various limitations to performance
is a crucial part of the machine learning method,
and one where statistics and algorithmics interact
closely.
In the case of Statistical Machine Translation, the
analysis of Moses suggests that the current bottle-
neck is the lack of sufficient data, not the function
class used for the representation of translation sys-
tems. The clear gap between performance on train-
ing and testing set, together with the rate of the
learning curves, suggests that improvements may be
possible but not by adding more data in i.i.d. way as
done now. The perturbation analysis suggests that
improved statistical principles are unlikely to make
a big difference either.
Since it is unlikely that sufficient data will be
available by simply sampling a distribution, one
needs to address a few possible ways to transfer
large amounts of knowledge into the system. All of
them lead to open problems either in machine learn-
41
ing or in machine translation, most of them having
been already identified by their respective communi-
ties as important questions. They are actively being
worked on.
The gap between performances on training and
on test sets is typically affected by model selection
choices, ultimately controlling the trade off between
overfitting and underfitting. In these experiments the
system used phrases of length 7 or less. Changing
this parameter might reflect on the gap and this is
the focus of our current work.
A research programme naturally follows from
our analysis. The first obvious approach is an ef-
fort to identify or produce datasets on demand (ac-
tive learning, where the learning system can request
translations of specific sentences, to satisfy its infor-
mation needs). This is a classical machine learning
question, that however comes with the need for fur-
ther theoretical work, since it breaks the traditional
i.i.d. assumptions on the origin of data. Further-
more, it would also require an effective way to do
confidence estimation on translations, as traditional
active learning approaches are effectively based on
the identification (or generation) of instances where
there is low confidence in the output (Blatz et al,
2004; Ueffing and Ney, 2004; Ueffing and Ney,
2005b; Ueffing and Ney, 2005a).
The second natural direction involves the intro-
duction of significant domain knowledge in the form
of linguistic rules, so to dramatically reduce the
amount of data needed to essentially reconstruct
them by using statistics. These rules could take the
form of generation of artificial training data, based
on existing training data, or a posteriori expansion of
translation and language tables. Any way to enforce
linguistic constraints will result in a reduced need
for data, and ultimately in more complete models,
given the same amount of data (Koehn and Hoang,
2007).
Obviously, it is always possible that the identifi-
cation of radically different representations of lan-
guage might introduce totally different constraints
on both approximation and estimation error, and this
might be worth considering.
What is not likely to work. It does not seem that
the introduction of more data will change the situ-
ation significantly, as long as the data is sampled
i.i.d. from the same distribution. It also does not
seem that more flexible versions of Markov mod-
els would be likely to change the situation. Finally,
it does not seem that new and different methods to
estimate probabilities would make much of a differ-
ence. Our perturbation studies show that significant
amounts of noise in the parameters result into very
small variations in the performance. Note also that
the current algorithm is not even working on refin-
ing the probability estimates, as the rate of growth of
the tables suggests that new n-grams are constantly
appearing, reducing the proportion of time spent re-
fining probabilities of old n-grams.
It does seem that the control of the performance
relies on the length of the translation and language
tables. Ways are needed to make these tables grow
much faster as a function of training set size; they
can either involve active selection of documents to
translate, or the incorporation of linguistic rules to
expand the tables without using extra data.
It is important to note that many approaches sug-
gested above are avenues currently being actively
pursued, and this analysis might be useful to decide
which one of them should be given priority.
7 Conclusions
We have started a series of extensive experimental
evaluations of performance of Moses, using high
performance computing, with the goal of under-
standing the system from a machine learning point
of view, and use this information to identify weak-
nesses of the system that can lead to improvements.
We have performed many more experiments that
cannot be reported in this workshop paper, and will
be published in a longer report (Turchi et al, In
preparation). In general, our goal is to extrapolate
the performance of the system under many condi-
tions, to be able to decide which directions of re-
search are most likely to deliver improvements in
performance.
Acknowledgments
Marco Turchi is supported by the EU Project
SMART. The authors thank Callum Wright, Bris-
tol HPC Systems Administrator, and Moses mailing
list.
42
References
Y. Al-Onaizan, J. Curin, M. Jahr, K. Knight, J. Lafferty,
D. Melamed, F. J. Och, D. Purdy, N. A. Smith, and
D. Yarowsky. 1999. Statistical machine translation:
Final report. Technical report, Johns Hopkins Univer-
sity 1999 Summer Workshop on Language Engineer-
ing, Center for Speech and Language Processing.
S. Banerjee and A. Lavie. 2005. Meteor: An auto-
matic metric for mt evaluation with improved correla-
tion with human judgments. In ACL ?05: Proceedings
of the 43rd Annual Meeting on Association for Com-
putational Linguistics, Morristown, NJ, USA. Associ-
ation for Computational Linguistics.
J. Blatz, E. Fitzgerald, G. Foster, S. Gandrabur, C. Goutte,
A. Kulesza, A. Sanchis, and N. Ueffing. 2004. Confi-
dence estimation for machine translation. In COLING
?04: Proceedings of the 20th international conference
on Computational Linguistics, page 315, Morristown,
NJ, USA. Association for Computational Linguistics.
P. F. Brown, S. Della Pietra, V.t J. Della Pietra, and R. L.
Mercer. 1994. The mathematic of statistical machine
translation: Parameter estimation. Computational Lin-
guistics, 19(2):263?311.
G. Doddington. 2002. Automatic evaluation of machine
translation quality using n-gram co-occurrence statis-
tics. In Proceedings of the second international con-
ference on Human Language Technology Research,
pages 138?145, San Francisco, CA, USA. Morgan
Kaufmann Publishers Inc.
P. Koehn and H. Hoang. 2007. Factored translation
models. In Proceedings of the 2007 Joint Conference
on Empirical Methods in Natural Language Process-
ing and Computational Natural Language Learning
(EMNLP-CoNLL), pages 868?876.
P. Koehn, F. J. Och, and D. Marcu. 2003. Statistical
phrase-based translation. In NAACL ?03: Proceedings
of the 2003 Conference of the North American Chap-
ter of the Association for Computational Linguistics
on Human Language Technology, pages 48?54, Mor-
ristown, NJ, USA. Association for Computational Lin-
guistics.
P. Koehn, H. Hoang, A. Birch, C. Callison-Burch,
M. Federico, N. Bertoldi, B. Cowan, W. Shen,
C. Moran, R. Zens, C. Dyer, O. Bojar, A. Constantin,
and E. Herbst. 2007. Moses: Open source toolkit for
statistical machine translation. In the Annual Meet-
ing of the Association for Computational Linguistics,
demonstration session.
P. Koehn. 2005. Europarl: A parallel corpus for sta-
tistical machine translation. In Machine Translation
Summit X, pages 79?86.
K.Papineni, S. Roukos, T. Ward, and W. J. Zhu. 2001.
Bleu: a method for automatic evaluation of machine
translation. In Proceedings of ACL ?02, pages 311?
318, Morristown, NJ, USA. Association for Computa-
tional Linguistics.
A. Lavie and A. Agarwal. 2007. Meteor: An automatic
metric for mt evaluation with high levels of correla-
tion with human judgments. In ACL ?07: Proceedings
of 45th Annual Meeting of the Association for Com-
putational Linguistics. Association for Computational
Linguistics.
F. J. Och and H. Ney. 2001. Discriminative training
and maximum entropy models for statistical machine
translation. In Proceedings of ACL ?02, pages 295?
302, Morristown, NJ, USA. Association for Computa-
tional Linguistics.
F. J. Och and H. Ney. 2003. A systematic comparison of
various statistical alignment models. Computational
Linguistics, 29(1):19?51.
F. J. Och and H. Weber. 1998. Improving statistical nat-
ural language translation with categories and rules. In
COLING-ACL, pages 985?989.
F. J. Och. 2003. Minimum error rate training in statis-
tical machine translation. In Proceedings of ACL ?03,
pages 160?167, Morristown, NJ, USA. Association for
Computational Linguistics.
R.Zens, F. J.Och, and H. Ney. 2002. Phrase-based sta-
tistical machine translation. In KI ?02: Proceedings
of the 25th Annual German Conference on AI, pages
18?32, London, UK. Springer-Verlag.
M. Snover, B. Dorr, R. Schwartz, L. Micciulla, and
J. Makhoul. 2006. A study of translation edit rate
with targeted human annotation. In Proceedings of the
7th Conference of the Association for Machine Trans-
lation in the Americas, pages 223?231. Association for
Machine Translation in the Americas.
A. Stolcke. 2002. Srilm ? an extensible language mod-
eling toolkit. In Intl. Conf. on Spoken Language Pro-
cessing.
M. Turchi, T. De Bie, and N. Cristianini. In preparation.
Learning analysis of a machine translation system.
N. Ueffing and H. Ney. 2004. Bayes decision rules
and confidence measures for statistical machine trans-
lation. In EsTAL-2004, pages 70?81.
N. Ueffing and H. Ney. 2005a. Application of word-level
confidence measures in interactive statistical machine
translation. In EAMT-2005, pages 262?270.
N. Ueffing and H. Ney. 2005b. Word-level confidence
estimation for machine translation using phrase-based
translation models. In Proceedings of HLT ?05, pages
763?770, Morristown, NJ, USA. Association for Com-
putational Linguistics.
43
Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics, pages 82?86,
Avignon, France, April 23 - 27 2012. c?2012 Association for Computational Linguistics
ElectionWatch: Detecting Patterns in News Coverage of US Elections
Saatviga Sudhahar, Thomas Lansdall-Welfare, Ilias Flaounas, Nello Cristianini
Intelligent Systems Laboratory
University of Bristol
(saatviga.sudhahar, Thomas.Lansdall-Welfare,
ilias.flaounas, nello.cristianini)@bristol.ac.uk
Abstract
We present a web tool that allows users to
explore news stories concerning the 2012
US Presidential Elections via an interac-
tive interface. The tool is based on con-
cepts of ?narrative analysis?, where the key
actors of a narration are identified, along
with their relations, in what are sometimes
called ?semantic triplets? (one example of
a triplet of this kind is ?Romney Criticised
Obama?). The network of actors and their
relations can be mined for insights about
the structure of the narration, including the
identification of the key players, of the net-
work of political support of each of them, a
representation of the similarity of their po-
litical positions, and other information con-
cerning their role in the media narration of
events. The interactive interface allows the
users to retrieve news report supporting the
relations of interest.
1 Introduction
U.S presidential elections are major media events,
following a fixed calendar, where two or more
public relation ?machines? compete to send out
their message. From the point of view of the me-
dia, this event is often framed as a race, with con-
tenders, front runners, and complex alliances. By
the end of the campaign, which lasts for about one
year, two line-ups are created in the media, one for
each major party. This event provides researchers
an opportunity to analyse the narrative structures
found in the news coverage, the amounts of media
attention that is devoted to the main contenders
and their allies, and other patterns of interest.
We propose to study the U.S Presidential Elec-
tions with the tools of (quantitative) narrative
analysis, identifying the key actors and their polit-
ical relations, and using this information to infer
the overall structure of the political coalitions. We
are also interested in how the media covers such
event that is which role is attributed to each actor
within this narration.
Quantitative Narrative Analysis (QNA) is an
approach to the analysis of news content that re-
quires the identification of the key actors, and of
the kind of interactions they have with each other
(Franzosi, 2010). It usually requires a signifi-
cant amount of manual labour, for ?coding? the
news articles, and this limits the analysis to small
samples. We claim that the most interesting rela-
tions come from analysing large networks result-
ing from tens of thousands of articles, and there-
fore that QNA needs to be automated.
Our approach is to use a parser to extract simple
SVO triplets, forming a semantic graph to identify
the noun phrases with actors, and to classify the
verbal links between actors in three simple cate-
gories: those expressing political support, those
expressing political opposition, and the rest. By
identifying the most important actors and triplets,
we form a large weighted and directed network
which we analyse for various types of patterns.
In this paper we demonstrate an automated sys-
tem that can identify articles relative to the 2012
US Presidential Election, from 719 online news
outlets, and can extract information about the key
players, their relations, and the role they play in
the electoral narrative. The system refreshes its
information every 24 hours, and has already anal-
ysed tens of thousands of news articles. The tool
allows the user to browse the growing set of news
articles by the relations between actors, for ex-
ample retrieving all articles where Mitt Romney
82
praises Obama1.
A set of interactive plots allows users to ex-
plore the news data by following specific candi-
dates and also specific types of relations, to see
a spectrum of all key actors sorted by their po-
litical affinity, a network representing relations
of political support between actors, and a two-
dimensional space where proximity again repre-
sents political affinity, but also they can access in-
formation about the role mostly played by a given
actor in the media narrative: that of a subject or
that of an object.
The ElectionWatch system is built on top of our
infrastructure for news content analysis, which
has been described elsewhere. It has also access
to named entities information, with which it can
generate timelines and activity-maps. These are
also available through the web interface.
2 Data Collection
Our system collects news articles from 719 En-
glish language news outlets. We monitor both U.S
and International media. A detailed description of
the underlying infrastructure has been presented
in our previous work (Flaounas, 2011).
In this demo we use only articles related to
US Elections. We detect those articles using a
topic detector based on Support Vector Machines
(Chang, 2011). We trained and validated our
classifier using the specialised Election news feed
from Yahoo!. The performance of the classifier
reached 83.46% precision, 73.29% recall, vali-
dated on unseen articles.
While the main focus of the paper is to present
Narrative patterns in elections stories, the system
presents also timelines and activity maps gener-
ated by detected Named Entities associated with
the election process.
3 Methodology
We perform a series of methodologies for narra-
tive analysis. Figure 1 illustrates the main compo-
nents that are used to analyse news and create the
website.
Preprocessing. First, we perform co-reference
and anaphora resolution on each U.S Election
article. This is based on the ANNIE plugin
in GATE (Cunningham, 2002). Next, we ex-
1Barack Obama and Mitt Romney are the two main op-
posing candidates in 2012 U.S Presidential Elections.
tract Subject-Verb-Object (SVO) triplets using the
Minipar parser output (Lin, 1998). An extracted
triplet is denoted for example like ?Obama(S)?
Accuse(V)?Republicans(O)?. We found that news
media contains less than 5% of passive sentences
and therefore it is ignored. We store each triplet in
a database annotated with a reference to the arti-
cle from which it was extracted. This allows us to
track the background information of each triplet
in the database.
Key Actors. From triplets extracted, we make
a list of actors which are defined as subjects and
objects of triplets. We rank actors according to
their frequencies and consider the top 50 subjects
and objects as the key actors.
Polarity of Actions. The verb element in
triplets are defined as actions. We map actions
to two specific action types which are endorse-
ment and opposing. We obtained the endorse-
ment/opposing polarity of verbs using the Verbnet
data (Kipper et al2006)).
Extraction of Relations. We retain all triplets
that have a) the key actors as subjects or ob-
jects; and b) an endorse/oppose verb. To ex-
tract relations we introduced a weighting scheme.
Each endorsement-relation between actors a, b is
weighted by wa,b:
wa,b =
fa,b (+)? fa,b (?)
fa,b (+) + fa,b (?)
(1)
where fa,b(+) denotes the number of triplets be-
tween a, b with positive relation and fa,b(?) with
negative relation. This way, actors who had
equal number of positive and negative relations
are eliminated.
Endorsement Network. We generate a triplet
network with the weighted relations where actors
are the nodes and weights calculated by Eq. 1 are
the links. This network reveals endorse/oppose
relations between key actors. The network in the
main page of ElectionWatch website, illustrated
in Fig. 2, is a typical example of such a network.
Network Partitioning. By using graph parti-
tioning methods we can analyse the allegiance of
actors to a party, and therefore their role in the
political discourse. The Endorsement Network
is a directed graph. To perform its partitioning
we first omit directionality by calculating graph
B = A+AT , where A is the adjacency matrix of
the Endorsement Network. We computed eigen-
vectors of the B and selected the eigenvector that
83
Figure 1: The Pipeline
correspond to the highest eigenvalue. The ele-
ments of the eigenvector represent actors. We sort
them by their magnitude and we obtain a sorted
list of actors. In the website we display only ac-
tors that are very polarised politically in the sides
of the list. These two sets of actors correlate well
with the left-right political ordering in our exper-
iments on past US Elections. Since in the first
phase of the campaign there are more than two
sides, we added a scatter plot using the first two
eigenvectors.
Subject/Object Bias of Actors. The Sub-
ject/Object bias Sa of actor a reveals the role it
plays in the news narrative. It is computed as:
Sa =
fSubj (a)? fObj (a)
fSubj (a) + fObj (a)
(2)
A positive value of S for actor a indicates that the
actor is used more often as a subject and a neg-
ative value indicates that the actor is used more
often as an object.
4 The Website
We analyse news related to U.S Elections 2012
every day, automatically, and the results of our
analysis are presented integrated under a publicly
available website2. Figure 2 illustrates the home-
page of ElectionWatch. Here, we list the key fea-
tures of the site:
Triplet Graph ? The main network in Fig. 2
is created using the weighted relations. A positive
sign for the edge indicates an endorsement rela-
tion and a negative sign indicates an opposition
relation in the network. By clicking on each edge
in the network, we display triplets and articles that
support the relation.
2ElectionWatch: http://electionwatch.enm.bris.ac.uk
Actor Spectrum ? The left side of Fig. 2
shows the Actor Spectrum, coloured from blue
for Democrats to red for Republicans. Actor spec-
trum was obtained by applying spectral graph par-
titioning methods to the triplet network.Note, that
currently there are more than two campaigns that
run in parallel between key actors that dominate
the elections news coverage. Nevertheless, we
still find that the two main opposing candidates
in each party were in either sides of the list.
Relations ? On the right hand side of the
website we show the endorsement/opposition re-
lations between key actors. For example, ?Re-
publicans Oppose Democrats?. When clicking on
a relation the webpage displays the news articles
that support the relation.
Actor Space ? The tab labelled ?Actor Space?
plots the first and second eigenvector values for
all actors in the actor spectrum.
Actor Bias The tab labelled ?Actor Bias? plots
the subject/object bias of actors against the first
eigenvector in a two dimensional space.
Pie Chart ? Pie Chart on the left bottom in
the webpage shows the share of each actor with
regard to the total number of articles mentioning
an endorse/oppose relation.
Map ? The map geo-locates articles related to
US Elections and refer to US locations.
Bar Chart ? The bar chart tab, illustrated in
Fig. 3, plots the number of articles in which ac-
tors were involved in a endorse/oppose relation.
The height of each column reveals the frequency
of it. The default plot focuses on only the first five
actors in the actor spectrum.
Timelines & Activity Map ? We track the ac-
tivity of each named entity in the actor spectrum
within the United States and present it in a time-
line. The activity map monitors the media atten-
84
Figure 2: Screenshot of the home page of ElectionWatch
Figure 3: Barchart showing endorse/oppose article fre-
quencies for actor ?Obama? with other top actors.
tion for Presidential candidates in each state in the
Unites States. At present we monitor this activity
for Mitt Romney, Rick Perry, Michele Bachmann,
Herman Cain and Barack Obama.
5 Discussion
We have demonstrated the system ElectionWatch
that presents key actors in U.S election news ar-
ticles and their role in political discourse. This
builds on various recent contributions from the
field of Pattern Analysis, such as (Trampus,
2011), augmenting them with multiple analysis
tools that respond to the needs of social sciences
investigations.
We agree on the fact that the triplets extracted
by the system are not very clean. This noise can
be ignored since we perform analysis on only fil-
tered triplets containing key actors and specific
type of actions, and also it?s extracted from huge
amount of data.
We have tested this system on data from all pre-
vious six elections, using the New York Times
corpus as well as our own database. We use only
support/criticism relations revealing a strong po-
larisation among actors and this seems to corre-
spond to the left/right political dimension. Evalu-
ation is an issue due to lack of data but results on
the past six election cycles on New York Times
always seperated the two competing candidates
along the eigenvector spectrum. This is not so
easy in the primary part of the elections, when
multiple candidates compete with each other for
the role of contender. To cover this case, we gen-
erate also a two-dimensional plot using the first
two eigenvalues of the adjacency matrix, which
seems to capture the main groupings in the politi-
cal narrative.
Future work will include making better use of
the information coming from the parser, which
85
goes well beyond the simple SVO structure of
sentences, and developing more sophisticated
methods for the analysis of large and complex net-
works that can be inferred with the methodology
we have developed.
Acknowledgments
I. Flaounas and N. Cristianini are supported by
FP7 CompLACS; N. Cristianini is supported by a
Royal Society Wolfson Merit Award; The mem-
bers of the Intelligent Systems Laboratory are
supported by the ?Pascal2? Network of Excel-
lence. Authors would like to thank Omar Ali and
Roberto Franzosi.
References
Chang C.C., and Lin C.J. 2011. LIBSVM: a library
for support vector machines. ACM Transactions on
Intelligent Systems and Technology 2(3):1?27
Cunningham H., Maynard D., Bontcheva K. and
Tablan V. 2002. GATE: A Framework and Graph-
ical Development Environment for Robust NLP
Tools and Applications. Proc. of the 40th Anniver-
sary Meeting of the Association for Computational
Linguistics 168?175.
Earl J., Martin A., McCarthy J.D., Soule S.A. 2004.
The Use of Newspaper Data in the Study of Collec-
tive Action. Annual Review of Sociology, 30:65?
80.
Flaounas I., Ali O., Turchi M., Snowsill T., Nicart F.,
De Bie T., Cristianini N. 2011. NOAM:News Out-
lets Analysis and Monitoring system. Proc. of the
2011 ACM SIGMOD international conference on
Management of data, 1275?1278.
Franzosi R. 2010. Quantitative Narrative Analysis.
Sage Publications Inc, Quantitative Applications in
the Social Sciences, 162?200.
Kipper K., Korhonen A., Ryant N., Palmer M. 2006.
Extensive Classifications of English verbs. 12th
EURALEX International Congress, Turin, Italy.
Lin D. 1998. Dependency-Based Evaluation of
Minipar. Text, Speech and Language Technology
20:317?329.
Sandhaus, E. 2008. The New York Times Annotated
Corpus. Linguistic Data Consortium
Trampus M., Mladenic D. 2011. Learning Event Pat-
terns from Text. Informatica 35
86
Proceedings of the ACL 2010 Conference Short Papers, pages 382?386,
Uppsala, Sweden, 11-16 July 2010. c?2010 Association for Computational Linguistics
Wrapping up a Summary:
from Representation to Generation
Josef Steinberger and Marco Turchi and
Mijail Kabadjov and Ralf Steinberger
EC Joint Research Centre
21027, Ispra (VA), Italy
{Josef.Steinberger, Marco.Turchi,
Mijail.Kabadjov, Ralf.Steinberger}
@jrc.ec.europa.eu
Nello Cristianini
University of Bristol,
Bristol, BS8 1UB, UK
nello@support-vector.net
Abstract
The main focus of this work is to investi-
gate robust ways for generating summaries
from summary representations without re-
curring to simple sentence extraction and
aiming at more human-like summaries.
This is motivated by empirical evidence
from TAC 2009 data showing that human
summaries contain on average more and
shorter sentences than the system sum-
maries. We report encouraging prelimi-
nary results comparable to those attained
by participating systems at TAC 2009.
1 Introduction
In this paper we adopt the general framework
for summarization put forward by Spa?rck-Jones
(1999) ? which views summarization as a three-
fold process: interpretation, transformation and
generation ? and attempt to provide a clean in-
stantiation for each processing phase, with a par-
ticular emphasis on the last, summary-generation
phase often omitted or over-simplified in the main-
stream work on summarization.
The advantages of looking at the summarization
problem in terms of distinct processing phases are
numerous. It not only serves as a common ground
for comparing different systems and understand-
ing better the underlying logic and assumptions,
but it also provides a neat framework for devel-
oping systems based on clean and extendable de-
signs. For instance, Gong and Liu (2002) pro-
posed a method based on Latent Semantic Anal-
ysis (LSA) and later J. Steinberger et al (2007)
showed that solely by enhancing the first source
interpretation phase, one is already able to pro-
duce better summaries.
There has been limited work on the last sum-
mary generation phase due to the fact that it is
unarguably a very challenging problem. The vast
amount of approaches assume simple sentence se-
lection, a type of extractive summarization, where
often the summary representation and the end
summary are, indeed, conflated.
The main focus of this work is, thus, to in-
vestigate robust ways for generating summaries
from summary representations without recurring
to simple sentence extraction and aiming at more
human-like summaries. This decision is also mo-
tivated by empirical evidence from TAC 2009 data
(see table 1) showing that human summaries con-
tain on average more and shorter sentences than
the system summaries. The intuition behind this is
that, by containing more sentences, a summary is
able to capture more of the important content from
the source.
Our initial experimental results show that our
approach is feasible, since it produces summaries,
which when evaluated against the TAC 2009 data1
yield ROUGE scores (Lin and Hovy, 2003) com-
parable to the participating systems in the Sum-
marization task at TAC 2009. Taking into account
that our approach is completely unsupervised and
language-independent, we find our preliminary re-
sults encouraging.
The remainder of the paper is organised as fol-
lows: in the next section we briefly survey the
related work, in ?3 we describe our approach to
summarization, in ?4 we explain how we tackle
the generation step, in ?5 we present and discuss
our experimental results and towards the end we
conclude and give pointers to future work.
2 Related Work
There is a large body of literature on summariza-
tion (Hovy, 2005; Erkan and Radev, 2004; Kupiec
et al, 1995). The most closely related work to the
approach presented hereby is work on summariza-
tion attempting to go beyond simple sentence ex-
1http://www.nist.gov/tac/
382
traction and to a lesser degree work on sentence
compression. We survey below work along these
lines.
Although our approach is related to sentence
compression (Knight and Marcu, 2002; Clarke
and Lapata, 2008), it is subtly different. Firstly, we
reduce the number of terms to be used in the sum-
mary at a global level, not at a local per-sentence
level. Secondly, we directly exploit the resulting
structures from the SVD making the last genera-
tion step fully aware of previous processing stages,
as opposed to tackling the problem of sentence
compression in isolation.
A similar approach to our sentence reconstruc-
tion method has been developed by Quirk et al
(2004) for paraphrase generation. In their work,
training and test sets contain sentence pairs that
are composed of two different proper English sen-
tences and a paraphrase of a source sentence is
generated by finding the optimal path through a
paraphrases lattice.
Finally, it is worth mentioning that we are aware
of the ?capsule overview? summaries proposed by
Boguraev and Kennedy (1997) which is similar to
our TSR (see below), however, as opposed to their
emphasis on a suitable browsing interface rather
than producing a readable summary, we precisely
attempt the latter.
3 Three-fold Summarization:
Interpretation, Transformation and
Generation
We chose the LSA paradigm for summarization,
since it provides a clear and direct instantiation of
Spa?rck-Jones? three-stage framework.
In LSA-based summarization the interpreta-
tion phase takes the form of building a term-by-
sentence matrix A = [A1, A2, . . . , An], where
each column Aj = [a1j , a2j , . . . , anj ]T represents
the weighted term-frequency vector of sentence j
in a given set of documents. We adopt the same
weighting scheme as the one described in (Stein-
berger et al, 2007), as well as their more general
definition of term entailing not only unigrams and
bigrams, but also named entities.
The transformation phase is done by applying
singular value decomposition (SVD) to the initial
term-by-sentence matrix defined as A = U?V T .
The generation phase is where our main contri-
bution comes in. At this point we depart from stan-
dard LSA-based approaches and aim at produc-
ing a succinct summary representation comprised
only of salient terms ? Term Summary Represen-
tation (TSR). Then this TSR is passed on to an-
other module which attempts to produce complete
sentences. The module for sentence reconstruc-
tion is described in detail in section 4, in what fol-
lows we explain the method for producing a TSR.
3.1 Term Summary Representation
To explain how a term summary representation
(TSR) is produced, we first need to define two con-
cepts: salience score of a given term and salience
threshold. Salience score for each term in matrix
A is given by the magnitude of the corresponding
vector in the matrix resulting from the dot product
of the matrix of left singular vectors with the diag-
onal matrix of singular values. More formally, let
T = U ? ? and then for each term i, the salience
score is given by |~Ti|. Salience threshold is equal
to the salience score of the top kth term, when all
terms are sorted in descending order on the basis
of their salience scores and a cutoff is defined as a
percentage (e.g., top 15%). In other words, if the
total number of terms is n, then 100?k/n must be
equal to the percentage cutoff specified.
The generation of a TSR is performed in two
steps. First, an initial pool of sentences is selected
by using the same technique as in (Steinberger and
Jez?ek, 2009) which exploits the dot product of the
diagonal matrix of singular values with the right
singular vectors: ? ? V T .2 This initial pool of sen-
tences is the output of standard LSA approaches.
Second, the terms from the source matrix A are
identified in the initial pool of sentences and those
terms whose salience score is above the salience
threshold are copied across to the TSR. Thus, the
TSR is formed by the most (globally) salient terms
from each one of the sentences. For example:
? Extracted Sentence: ?Irish Prime Minister Bertie
Ahern admitted on Tuesday that he had held a series of
private one-on-one meetings on the Northern Ireland
peace process with Sinn Fein leader Gerry Adams, but
denied they had been secret in any way.?
? TSR Sentence at 10%: ?Irish Prime Minister
Bertie Ahern Tuesday had held one-on-one meetings
Northern Ireland peace process Sinn Fein leader Gerry
Adams?3
2Due to space constraints, full details on that step are
omitted here, see (Steinberger and Jez?ek, 2009).
3The TSR sentence is stemmed just before feeding it to
the reconstruction module discussed in the next section.
383
Average Human System At 100% At 15% At 10% At 5% At 1%
number of: Summaries Summaries
Sentences/summary 6.17 3.82 3.8 3.95 4.39 5.18 12.58
Words/sentence 15.96 25.01 26.24 25.1 22.61 19.08 7.55
Words/summary 98.46 95.59 99.59 99.25 99.18 98.86 94.96
Table 1: Summary statistics on TAC?09 data (initial summaries).
Metric LSAextract At 100% At 15% At 10% At 5% At 1%
ROUGE-1 0.371 0.361 0.362 0.365 0.372 0.298
ROUGE-2 0.096 0.08 0.081 0.083 0.083 0.083
ROUGE-SU4 0.131 0.125 0.126 0.128 0.131 0.104
Table 2: Summarization results on TAC?09 data (initial summaries).
4 Noisy-channel model for sentence
reconstruction
This section describes a probabilistic approach to
the reconstruction problem. We adopt the noisy-
channel framework that has been widely used in a
number of other NLP applications. Our interpre-
tation of the noisy channel consists of looking at a
stemmed string without stopwords and imagining
that it was originally a long string and that some-
one removed or stemmed some text from it. In our
framework, reconstruction consists of identifying
the original long string.
To model our interpretation of the noisy chan-
nel, we make use of one of the most popular
classes of SMT systems: the Phrase Based Model
(PBM) (Zens et al, 2002; Och and Ney, 2001;
Koehn et al, 2003). It is an extension of the noisy-
channel model and was introduced by Brown et al
(1994), using phrases rather than words. In PBM,
a source sentence f is segmented into a sequence
of I phrases f I = [f1, f2, . . . fI ] and the same is
done for the target sentence e, where the notion of
phrase is not related to any grammatical assump-
tion; a phrase is an n-gram. The best translation
ebest of f is obtained by:
ebest = argmaxe p(e|f) = argmaxe
I?
i=1
?(fi|ei)
??
d(ai ? bi?1)
?d
|e|?
i=1
pLM (ei|e1 . . . ei?1)
?LM
where ?(fi|ei) is the probability of translating
a phrase ei into a phrase fi. d(ai ? bi?1) is
the distance-based reordering model that drives
the system to penalize substantial reorderings of
words during translation, while still allowing some
flexibility. In the reordering model, ai denotes the
start position of the source phrase that was trans-
lated into the ith target phrase, and bi?1 denotes
the end position of the source phrase translated
into the (i?1th) target phrase. pLM (ei|e1 . . . ei?1)
is the language model probability that is based on
the Markov chain assumption. It assigns a higher
probability to fluent/grammatical sentences. ??,
?LM and ?d are used to give a different weight to
each element (for more details see (Koehn et al,
2003)).
In our reconstruction problem, the difference
between the source and target sentences is not in
terms of languages, but in terms of forms. In fact,
our source sentence f is a stemmed sentence with-
out stopwords, while the target sentence e is a
complete English sentence. ?Translate? means to
reconstruct the most probable sentence e given f
inserting new words and reproducing the inflected
surface forms of the source words.
4.1 Training of the model
In Statistical Machine Translation, a PBM system
is trained using parallel sentences, where each sen-
tence in a language is paired with another sentence
in a different language and one is the translation of
the other.
In the reconstruction problem, we use a set, S1
of 2,487,414 English sentences extracted from the
news. This set is duplicated, S2, and for each sen-
tence in S2, stopwords are removed and the re-
maining words are stemmed using Porter?s stem-
mer (Porter, 1980). Our stopword list contains 488
words. Verbs are not included in this list, because
they are relevant for the reconstruction task. To
optimize the lambda parameters, we select 2,000
pairs as development set.
384
An example of training sentence pair is:
? Source Sentence: ?royal mail ha doubl profit 321
million huge fall number letter post?
? Target Sentence: ?royal mail has doubled its prof-
its to 321 million despite a huge fall in the number of
letters being posted?
In this work we use Moses (Koehn et al, 2007),
a complete phrase-based translation toolkit for
academic purposes. It provides all the state-of-the-
art components needed to create a phrase-based
machine translation system. It contains different
modules to preprocess data, train the Language
Models and the Translation Models.
5 Experimental Results
For our experiments we made use of the TAC
2009 data which conveniently contains human-
produced summaries against which we could eval-
uate the output of our system (NIST, 2009).
To begin our inquiry we carried out a phase
of exploratory data analysis, in which we mea-
sured the average number of sentences per sum-
mary, words per sentence and words per summary
in human vs. system summaries in the TAC 2009
data. Additionally, we also measured these statis-
tics of summaries produced by our system at five
different percentage cutoffs: 100%, 15%, 10%,
5% and 1%. 4 The results from this exploration
are summarised in table 1. The most notable thing
is that human summaries contain on average more
and shorter sentences than the system summaries
(see 2nd and 3rd column from left to right). Sec-
ondly, we note that as the percentage cutoff de-
creases (from 4th column rightwards) the charac-
teristics of the summaries produced by our system
are increasingly more similar to those of the hu-
man summaries. In other words, within the 100-
word window imposed by the TAC guidelines, our
system is able to fit more (and hence shorter) sen-
tences as we decrease the percentage cutoff.
Summarization performance results are shown
in table 2. We used the standard ROUGE evalu-
ation (Lin and Hovy, 2003) which has been also
used for TAC. We include the usual ROUGE met-
rics: R1 is the maximum number of co-occurring
unigrams, R2 is the maximum number of co-
occurring bigrams and RSU4 is the skip bigram
measure with the addition of unigrams as counting
4Recall from section ?3 that the salience threshold is a
function of the percentage cutoff.
unit. The last five columns of table 2 (from left to
right) correspond to summaries produced by our
system at various percentage cutoffs. The 2nd col-
umn, LSAextract, corresponds to the performance
of our system at producing summaries by sentence
extraction only.5
In the light of the above, the decrease in per-
formance from column LSAextract to column ?At
100%? can be regarded as reconstruction error.6
Then, as we decrease the percentage cutoff (from
4th column rightwards) we are increasingly cover-
ing more of the content comprised by the human
summaries (as far as the ROUGE metrics are able
to gauge this, of course). In other words, the im-
provement of content coverage makes up for the
reconstruction error, and at 5% cutoff we already
obtain ROUGE scores comparable to LSAextract.
This suggests that if we improve the quality of our
sentence reconstruction we would potentially end
up with a better performing system than a typical
LSA system based on sentence selection. Hence,
we find these results very encouraging.
Finally, we admittedly note that by applying a
percentage cutoff on the initial term set and further
performing the sentence reconstruction we gain in
content coverage, to a certain extent, on the ex-
pense of sentence readability.
6 Conclusion
In this paper we proposed a novel approach to
summary generation from summary representa-
tion based on the LSA summarization framework
and on a machine-translation-inspired technique
for sentence reconstruction.
Our preliminary results show that our approach
is feasible, since it produces summaries which re-
semble better human summaries in terms of the av-
erage number of sentences per summary and yield
ROUGE scores comparable to the participating
systems in the Summarization task at TAC 2009.
Bearing in mind that our approach is completely
unsupervised and language-independent, we find
our results promising.
In future work we plan on working towards im-
proving the quality of our sentence reconstruction
step in order to produce better and more readable
sentences.
5These are, effectively, what we called initial pool of sen-
tences in section 3, before the TSR generation.
6The only difference between the two types of summaries
is the reconstruction step, since we are including 100% of the
terms.
385
References
B. Boguraev and C. Kennedy. 1997. Salience-
based content characterisation of text documents. In
I. Mani, editor, Proceedings of the Workshop on In-
telligent and Scalable Text Summarization at the An-
nual Joint Meeting of the ACL/EACL, Madrid.
P. Brown, S. Della Pietra, V. Della Pietra, and R. Mer-
cer. 1994. The mathematic of statistical machine
translation: Parameter estimation. Computational
Linguistics, 19(2):263?311.
J. Clarke and M. Lapata. 2008. Global inference for
sentence compression: An integer linear program-
ming approach. Journal of Artificial Intelligence Re-
search, 31:273?318.
G. Erkan and D. Radev. 2004. LexRank: Graph-based
centrality as salience in text summarization. Journal
of Artificial Intelligence Research (JAIR).
Y. Gong and X. Liu. 2002. Generic text summarization
using relevance measure and latent semantic analy-
sis. In Proceedings of ACM SIGIR, New Orleans,
US.
E. Hovy. 2005. Automated text summarization. In
Ruslan Mitkov, editor, The Oxford Handbook of
Computational Linguistics, pages 583?598. Oxford
University Press, Oxford, UK.
K. Knight and D. Marcu. 2002. Summarization be-
yond sentence extraction: A probabilistic approach
to sentence compression. Artificial Intelligence,
139(1):91?107.
P. Koehn, F. Och, and D. Marcu. 2003. Statistical
phrase-based translation. In Proceedings of NAACL
?03, pages 48?54, Morristown, NJ, USA.
P. Koehn, H. Hoang, A. Birch, C. Callison-Burch,
M. Federico, N. Bertoldi, B. Cowan, W. Shen,
C. Moran, R. Zens, C. Dyer, O. Bojar, A. Constantin,
and E. Herbst. 2007. Moses: Open source toolkit
for statistical machine translation. In Proceedings
of ACL ?07, demonstration session.
J. Kupiec, J. Pedersen, and F. Chen. 1995. A trainable
document summarizer. In Proceedings of the ACM
SIGIR, pages 68?73, Seattle, Washington.
C. Lin and E. Hovy. 2003. Automatic evaluation of
summaries using n-gram co-occurrence statistics. In
Proceedings of HLT-NAACL, Edmonton, Canada.
NIST, editor. 2009. Proceeding of the Text Analysis
Conference, Gaithersburg, MD, November.
F. Och and H. Ney. 2001. Discriminative training
and maximum entropy models for statistical ma-
chine translation. In Proceedings of ACL ?02, pages
295?302, Morristown, NJ, USA.
M. Porter. 1980. An algorithm for suffix stripping.
Program, 14(3):130?137.
C. Quirk, C. Brockett, and W. Dolan. 2004. Monolin-
gual machine translation for paraphrase generation.
In Proceedings of EMNLP, volume 149. Barcelona,
Spain.
K. Spa?rck-Jones. 1999. Automatic summarising: Fac-
tors and directions. In I. Mani and M. Maybury,
editors, Advances in Automatic Text Summarization.
MIT Press.
J. Steinberger and K. Jez?ek. 2009. Update summariza-
tion based on novel topic distribution. In Proceed-
ings of the 9th ACM DocEng, Munich, Germany.
J. Steinberger, M. Poesio, M. Kabadjov, and K. Jez?ek.
2007. Two uses of anaphora resolution in summa-
rization. Information Processing and Management,
43(6):1663?1680. Special Issue on Text Summari-
sation (Donna Harman, ed.).
R. Zens, F. J. Och, and H. Ney. 2002. Phrase-based
statistical machine translation. In Proceedings of KI
?02, pages 18?32, London, UK. Springer-Verlag.
386
