Proceedings of the Thirteenth Conference on Computational Natural Language Learning (CoNLL): Shared Task, pages 19?24,
Boulder, Colorado, June 2009. c?2009 Association for Computational Linguistics
An Iterative Approach for Joint  
Dependency Parsing and Semantic Role Labeling 
 
 
Qifeng Dai 
Department of Computer Sci-
ence, University of Science and 
Technology of China, Hefei, 
China 
daiqifeng001@126.com  
Enhong Chen 
Department of Computer Sci-
ence, University of Science and 
Technology of China, Hefei, 
China 
cheneh@ustc.edu.cn 
Liu Shi 
Department of Computer Sci-
ence, University of Science and 
Technology of China, Hefei, 
China 
shiliu@ustc.edu 
 
 
 
Abstract 
We propose a system to carry out the joint pars-
ing of syntactic and semantic dependencies in 
multiple languages for our participation in the 
shared task of CoNLL-2009. We present an it-
erative approach for dependency parsing and 
semantic role labeling. We have participated in 
the closed challenge, and our system achieves 
73.98% on labeled macro F1 for the complete 
problem, 77.11% on labeled attachment score 
for syntactic dependencies, and 70.78% on la-
beled F1 for semantic dependencies. The cur-
rent experimental results  show that our method 
effectively improves system performance. 
1 Introduction 
In this paper we describe the system submitted to 
the closed challenge of the CoNLL-2009 shared 
task on joint parsing of syntactic and semantic de-
pendencies in multiple languages.  
Give a sentence, the task of dependency parsing 
is to identify the syntactic head of each word in the 
sentence and classify the relation between the de-
pendent and its head. The task of semantic role 
labeling is to label the senses of predicates in the 
sentence and labeling the semantic role of each 
word in the sentence relative to each predicate. 
The difficulty of this shared task is to perform 
joint task on dependency parsing and semantic role 
labeling. We split the shared task into four sub-
problems: syntactic dependency parsing, syntactic 
dependency label classification, word sense disam-
biguation, and semantic role labeling. And we pro-
pose a novel iterative approach to perform the joint 
task. In the first step, the system performs depend-
ency parsing and semantic role labeling in a pipe-
lined manner and the four sub-problems extract 
features based on the known information. In the 
iterative step, the system performs the four tasks in 
a pipelined manner but uses features extracted 
from the previous parsing result. 
The remainder of the paper is structured as fol-
lows. Section 2 presents the technical details of our 
system. Section 3 presents experimental results and 
the performance analysis. Section 4 looks into a 
few issues concerning our forthcoming work for 
this shared task, and concludes the paper. 
2 System description 
This section briefly describes the main components 
of our system: a) system flow; b) syntactic parsing; 
c) semantic role labeling; d) an iterative approach 
to perform joint syntactic-semantic parsing. 
2.1 System flow 
As many systems did in CoNLL Shared Task 2008, 
the most direct way for such task is pipeline ap-
proach. First, Split the system into four subtasks: 
syntactic dependency parsing, syntactic depend-
ency relation labeling, predicate sense labeling and 
semantic role labeling. Then, execute them one by 
one. In our system, we extend this pipeline system 
to an iterative system so that it can do a joint label-
ing to improve the performance. 
Our iterative system is based on the pipeline 
system. For the first iteration (original step), we 
use the pipeline system to parse and label the 
19
whole sentence. For the rest iterations (iterative 
step), we use another pipeline system to parse and 
label it. The structure of this pipeline is the same as 
the original one, but each subtask can have much 
more features than the original subtask. Because 
the whole sentence has been labeled in the original 
step, all information is available for every subtask. 
For example, when doing syntactic dependency 
relation labeling, we can add some features about 
sense and semantic role. It seems like using syntac-
tic results to do semantic labeling, then using se-
mantic results to improve syntactic labeling. This 
is the core idea of our joint system. Figure 1 shows 
the main flow of our system. 
 
 
Figure 1. The main flow of iteration system 
 
 
 
 
2.2 Dependency Parsing 
In the dependency parsing step, we split the task 
into two sub-problems: syntactic dependency pars-
ing and syntactic dependency relation labeling. 
In the syntactic dependency parsing stage, 
MSTParser1, a dependency parser that searches for 
maximum spanning trees over directed graphs, is 
applied. Due to the differences between the seven 
languages, we use different parameters to train a 
parsing model. Specifically, as Czech and German 
languages are none-projective and the others are 
projective, we train Czech and German languages 
with parameter ?none-projective? and the others 
with ?projective?. 
On the syntactic dependency label classification 
step, we used the max-entropy classification algo-
rithm to train the model. This step contains two 
processes. In the first process the sub-problem 
trains the model with the following basic features: 
Start 
End 
Syntactic dependency 
parsing 
Syntactic dependency 
relation labeling 
Set count = iterate times 
Set isIterStep = false 
Predicate sense label-
ing 
Semantic role labeling 
count -- 
isIterStep = true 
count = 0 
Y
 
Get fea-
tures: 
this step 
return the 
feature of 
system 
judge by 
the type of 
sub task  
and the 
parameter 
isIterStep. 
N
? FORM1: FORM of the head. 
? LEMMA1: LEMMA of the head. 
? STEM1 (English only): STEM of the head. 
? POS1: POS of the head. 
? IS_PRED1: the value of FILLPRED of the 
head. 
? FEAT1: FEAT of the head. 
? LM_STEM1 (English only): the left-most 
modifier?s STEM of head. 
? LM_POS1: the left-most modifier?s POS 
of head. 
? L_NUM1: number of the head?s left modi-
fiers. 
? RM_STEM1 (English only): the right-
most modifier?s STEM of head. 
? RM_POS1: the right-most modifier?s POS 
of head. 
? M_NUM1: number of modifiers of the 
head. 
? SUFFIX1 (English only): suffix of the 
head. 
? FORM2: FORM of the dependent. 
? LEMMA2: LEMMA of the dependent. 
? STEM2 (English only): STEM of the de-
pendent. 
? POS2: POS of the dependent. 
? IS_PRED2: the value of FILLPRED of the 
dependent. 
                                                          
1 http://sourceforge.net/projects/mstparser 
20
? FEAT2: FEAT of the dependent. 
? LM_STEM2 (English only): the left-most 
modifier?s STEM of dependent. 
? LM_POS2: the left-most modifier?s POS 
of dependent. 
? L_NUM2:  number of the dependent?s left 
modifiers. 
? RM_STEM2 (English only): the right-
most modifier?s STEM of dependent. 
? RM_POS2: the right-most modifier?s POS 
of dependent. 
? M_NUM2: number of modifiers of the de-
pendent. 
? SUFFIX2 (English only): suffix of the de-
pendent. 
? DEP_PATH_ROOT_POS2: POS list from 
dependent to tree?s root through the syn-
tactic dependency path. 
? DEP_PATH_ROOT_LEN2: length from 
dependent to tree?s root through the syn-
tactic dependency path.  
? POSITION: The position of the word with 
respect to its predicate. It has three values, 
?before?, ?is? and ?after?, for the predicate. 
In the iterative step, in addition to the features 
mentioned above, the sub-task trains the model 
with the following features: 
? DEP_PATH_ROOT_POS1: POS list from 
head to tree?s root through the syntactic 
dependency path. 
? DEP_PATH_ROOT_REL1: length from 
dependent to tree?s root through the syn-
tactic dependency path. 
? PRED_POS: POS list of all predicates in 
the sentence. 
? FORM2 + DEP_PATH_REL: component 
of FORM2 and the POS list from head to 
the dependent through the syntactic de-
pendency path. 
? POSITION + FORM2 
? STEM1 + FORM2 (English only) 
? STEM1 + STEM2 (English only) 
? POSITION + POS2 
? ROLE_LIST2: list of APRED when the 
dependent is a predicate. 
? ROLE: list of APRED and PRED when 
the head is predicate. 
? L_ROLE: the nearest semantic role in its 
left side when head is a predicate. 
? R_ROLE: the nearest semantic role in its 
right side when head is a predicate. 
? IS_ROLE1: whether dependent is a se-
mantic role of head when head is a predi-
cate. 
2.3 Semantic role labeling 
Unlike CoNLL-2008 shared task, this shared task 
does not need to identify predicates. So the main 
task of this step is to label the sense of each predi-
cate and label the semantic role for each predicate. 
When labeling the sense of each predicate, we 
build a classification model for each predicate. As 
the senses of different predicates are usually unre-
lated even if they have the same sense label, this 
makes it difficult for us to use only one classifier to 
label them. But this approach leads to another issue. 
The set of predicates in the training set cannot 
cover all predicates. For new predicates in the test 
set, no classification model can be found for them, 
and we build a most common sense for them. The 
features we used are as follow: 
? DEPREL1: DEPREL of the predicate. 
? STEM1 
? POS1 
? RM_STEM1 (English only) 
? RM_POS1 
? FORM2 
? POS2 
? SUFFIX2 
? VOICE (English only): VOICE of predi-
cate. 
? POSITION + POS2 
? L_POS1 + POS1 + R_POS1: component 
of left word?s POS and predicate POS and 
right word?s POS. 
? FORM2 + DEP_PATH_REL 
? DEP_PATH_ROOT_POS1 
? DEP_PATH_ROOT_REL1 
When labeling the semantic role, we use a simi-
lar approach as we did in CoNLL Shared Task 
2008. However, as the frames information is not 
supplied for all languages, we do not use it in this 
task. The features we use are as follows: 
? DEPREL1 
? STEM1 (English only) 
? POS1 
? RM_STEM1 (English only) 
? RM_POS1 
21
? FORM2 
? POS2 
? SUFFIX2 
? VOICE2 (English only) 
? POSITION 
? DEP_PATH_REL 
? DEP_PATH_POS 
? SENSE2 
? SENSE2 + VOICE2 
? POSITION +  VOICE2 
? DEP_PATH_LEN 
? DEP_PATH_ROOT_REL1 
Moreover, we build an iterative model in this 
shared task. When doing an iterative labeling, the 
previous labeling results are known. So we can 
design some new features for checking the previ-
ous results in a global view. The features we add 
for the iterative model are as follows: 
? SENSE1: SENSE of the predicate. 
? SENSE1 + VOICE1: component of the 
SENSE + VOICE of predicate. 
? VOICE1 + FORM1: component of VOICE 
and FORM. 
? ROLE_LIST1: list of APRED of predicate. 
2.4 Iterative Approach 
As described above, some subtasks have two 
groups of features. One is for the pipeline model, 
and the other is for the iterative model. The usage 
of these two types of model is the same. The only 
difference is that they use different features. The 
iterative model can get more information, so they 
can use more features. These additional features 
can contain some joint and global (like frame and 
global structure) information. The performance 
may be improved because the viewer is extended. 
Some structural error and semantic conflict can be 
fixed. 
Although the usage of the two types of model is 
the same, there are some differences when building 
the models. 
In the iterative step, all information is available 
for doing parsing and labeling. For example, when 
doing syntactic dependency relation labeling in the 
iterative step, the fields ?HEAD?, ?DEPREL?, 
?PRED? and ?APREDs? are filled by the pervious 
iteration. So all these information can be used in 
the iterative step. This will cause one issue: use 
?HEAD1? to label ?HEAD2?. When training the 
model, ?HEAD1? is golden. The classifier will 
build a model directly and let ?HEAD2? equal to 
?HEAD1?. However, in the iterative step, 
?HEAD1? is not golden, but such model makes it 
impossible to change the results.. The iterative step 
will be useless. 
We design a simple method to avoid this issue.  
? Firstly, split the training set into N (N>1) 
subsets.  
? Secondly, for each subset, use the left N-1 
subsets to build an original sub-model (use 
features in the pipeline step). 
? Thirdly, use each sub-model to label the 
corresponding subset. 
? Lastly, use these labeled N subsets to ex-
tract samples (use features in the iterative 
step) for building the iterative model. 
In this way, the ?HEAD1? is not golden any 
more. And for each sub-task, we can use the simi-
lar method to build the original model and the it-
erative model.  
Moreover, in our system, we only build the it-
erative models for syntactic dependency relation 
labeling and semantic role labeling. For syntactic 
dependency parsing, we use an approach with very 
high time and space complexity, so it is not added 
to the iterative step. Thus, its results will not be 
changed in the iterative step. For sense labeling, 
we build classification models for every predicate. 
There are too many models and each model con-
tains only a few classes. We think they are not 
suitable for building the iterative model. But, as its 
previous sub-task (syntactic dependency relation 
labeling) is added to the iterative step, it is useful 
to add it to the iterative step. Though we do not 
build an iterative model for sense labeling, we can 
directly use its pipeline model. This is another ad-
vantage of our iterative model: if one subtask is not 
suitable for doing iterative labeling/parsing, we can 
use its pipeline model instead. 
3 Experiments and Results 
We have tested our system with the test set and 
obtained official results as shown in Table 1. We 
have tried to find how the iterative step influences 
syntactic dependency parsing and semantic role 
labeling. For syntactic dependency parsing and 
semantic role labeling, we do experiments on the 
test set. 
  
22
 Macro F1 Score 
Average 73.98 
Catalan 72.09 
Chinese 72.72 
Czech 67.14 
English 81.89 
German 75.00 
Japanese 80.89 
Spanish 68.14 
Table 1. The Macro F1 Score of every languages and 
the average value. 
3.1 Syntactic Dependency Parsing 
Dependency Parsing can be split into two sub-
problems: syntactic dependency parsing and syn-
tactic dependency label classification. We use the 
iterative method on syntactic dependency label 
classification. We do experiments on the test set.  
On the test set, we do two group experiments. In 
the first group, we build a subtest to test this sub-
task only. All other information is given, and we 
just label the dependency relation. The results are 
shown in Table 2. The row of ?Initial step? shows 
the results of this sub task in the original step. The 
left two rows show the results in the iterative step 
with iterating once and twice. The table shows that 
the iterative approach improves the performance. 
Especially for Catalan, the performance increases 
by 2.89%. 
Certainly, in the whole system, this subtask can-
not get golden information about sense and seman-
tic roles. So we test it in the whole system (joint 
test) on the test set in the second group of experi-
ments. As shown in Table 3, the iterative step is 
not as good as previous test. But it is still useful for 
some languages. The reason that some languages 
have no improvements on the iterative step is that 
the result of the initial step is not so good. 
3.2 Semantic Role Labeling 
Like syntactic dependency parsing, we do two tests 
on Semantic Role Labeling. This result is not con-
sistent with the official data because we have add-
ed some features of the subtask. The results of 
subtest can be found in Table 4. And Table 5 
shows the results of the joint test. These two 
groups of results show that the advantage of the 
iterative step is not as good as that of syntactic de-
pendency labeling in subtest. But it improves the 
performance for most languages. The iterative step 
improves the performance in both two tests.  
3.3 Analysis of Results  
From the experimental results, we can see that the 
effect of each part of the iterative step depends on 
the overall labeling result of the previous step. And 
the labeling effect varies with different languages. 
Iterative approach can improve the performance of 
the system but it strongly depends on the initial 
labeling result.  
4 Conclusion and Future Work  
This paper has presented a simple discriminative 
system submitted to the CoNLL-2009 shared task 
to address the learning task of syntactic and seman-
tic dependencies. The paper first describes how to 
carry out syntactic dependency parsing and seman-
tic role labeling, and then a new iterative approach 
is presented for joint parsing. The experimental 
results show that the iterative process can improve 
the labeling accuracy on syntactic and semantic 
analysis. However, this approach probably depends 
on the accuracy of the initial labeling results. The 
results of the initial labeling results will affect the 
effect of the iterative process.  
Because of time constraints and inadequate ex-
perimental environment, our first results do not 
meet our expectation, and the effect of the iterative 
step is not so clear. Next, we will strive to refine 
our approach to produce good results for the syn-
tactic dependency parsing, since it has a great im-
pact on the final parsing results. 
Acknowledgments 
The authors would like to thank the reviewers for 
their helpful comments. This work was supported 
by National Natural Science Foundation of China 
(No.60573077, No.60775037) and the National 
High Technology Research and Development Pro-
gram of China (863 Program) (grant no. 
2009AA01Z123). We also thank the High-
Performance Center of USTC for providing us 
with the experimental platform. 
 
 
 
 
 
 
 
 
23
 Average Catalan Chinese Czech Czech-ood English English-ood German German-ood Japanese Spanish
Initial step 93.64 95.66 95.01 88.10 88.10 96.79 92.98 96.41 89.71* 98.17 95.48
Iteration 1 94.60 98.56* 96.08* 88.59 88.29 97.31* 94.57* 96.63* 89.31 98.34 98.30
Iteration 2 94.65 98.55 96.08* 88.68* 88.45* 97.29 94.56 96.63* 89.53 98.35* 98.33*
Table 2. The subtest result of Labeled Syntactic Accuracy of each language and the average performance value 
on test set. (* denotes the best score for the system) 
 
 Average Catalan Chinese Czech Czech-ood English English-ood German German-ood Japanese Spanish
Initial step 74.02 77.75 73.81 58.69* 55.50* 84.75 78.85 82.45 66.27* 90.45* 71.64
Iteration 1 73.90 77.82 73.86* 58.17 54.95 84.81 78.95 82.51* 65.78 90.43 71.68
Iteration 2 73.94 77.85* 73.86* 58.31 55.13 84.82* 79.02* 82.46 65.85 90.45* 71.69*
Table 3. The joint test result of Labeled Syntactic Accuracy of each language and the average performance value 
on test set. (* denotes the best score for the system) 
 
 Average Catalan Chinese Czech Czech-ood English English-ood German German-ood Japanese Spanish
Initial step 83.83 88.56 85.86 88.08 86.20* 86.23 82.09 80.98 78.82 74.32* 87.45
Iteration 1 84.34 89.02* 87.14* 87.88 86.09 86.66 82.07 83.66* 79.28* 74.06 87.59
Iteration 2 84.36 89.02* 87.01 88.10* 86.17 86.78* 82.34* 83.15 79.18 74.06 87.81*
Table 4. The sub test result of Semantic Labeled F1 of each language and the average performance value on test 
set. (* denotes the best score for the system) 
 Average Catalan Chinese Czech Czech-ood English English-ood German German-ood Japanese Spanish
Initial step 70.01 66.87 71.63 75.50 75.71 78.97 69.87 67.50 58.47 70.91* 64.64
Iteration 1 70.15 67.12 71.98 75.54 75.68 79.40 70.17* 68.08* 58.55* 70.69 64.32
Iteration 2 70.20 67.33* 71.99* 75.65* 75.90* 79.47* 69.98 67.98 58.33 70.70 64.65*
Table 5. The joint test result of Semantic Labeled F1 of each language and the average performance value on test 
set. (* denotes the best score for the system) 
References  
Jan Haji?, Massimiliano Ciaramita, Richard Johansson, 
Daisuke Kawahara, Maria Antonia Mart?, Llu?s 
M?rquez, Adam Meyers, Joakim Nivre, Sebastian 
Pad?, Jan ?t?p?nek, Pavel Stra??k, Mihai Surdeanu, 
Nianwen Xue and Yi Zhang. 2009. The CoNLL-2009 
Shared Task: Syntactic and Semantic Dependencies 
in Multiple Languages. Proceedings of the 13th Con-
ference on Computational Natural Language Learn-
ing (CoNLL-2009). Boulder, Colorado, USA. June 4-
5. pp. 3-22. 
Mariona Taul?, Maria Ant?nia Mart? and Marta Re-
casens. 2008. AnCora: Multilevel Annotated Corpora 
for Catalan and Spanish. Proceedings of the 6th In-
ternational Conference on Language Resources and 
Evaluation (LREC-2008). Marrakech, Morocco. 
Nianwen Xue and Martha Palmer. 2009. Adding seman-
tic roles to the Chinese Treebank.  Natural Language 
Engineering, 15(1):143-172. 
Jan Haji?, Jarmila Panevov?, Eva Haji?ov?, Petr Sgall, 
Petr Pajas, Jan ?t?p?nek, Ji?? Havelka, Marie Miku-
lov? and Zden?k ?abokrtsk?. 2006. The Prague De-
pendency Treebank 2.0. CD-ROM. Linguistic Data 
Consortium, Philadelphia, Pennsylvania, USA. ISBN 
1-58563-370-4. LDC Cat. No. LDC2006T01. URL: 
http://ldc.upenn.edu.  
Surdeanu, Mihai, Richard Johansson, Adam Meyers, 
Llu?s M?rquez, and Joakim Nivre. 2008. The 
CoNLL-2008 Shared Task on Joint Parsing of Syn-
tactic and Semantic Dependencies. In Proceedings of 
the 12th Conference on Computational Natural Lan-
guage Learning (CoNLL-2008).  
Aljoscha Burchardt, Katrin Erk, Anette Frank, Andrea 
Kowalski, Sebastian Pad? and Manfred Pinkal. 2006. 
The SALSA Corpus: a German Corpus Resource for 
Lexical Semantics. Proceedings of the 5th Interna-
tional Conference on Language Resources and 
Evaluation (LREC-2006). Genoa, Italy. 
Daisuke Kawahara, Sadao Kurohashi and Koiti Hasida. 
2002. Construction of a Japanese Relevance-tagged 
Corpus. Proceedings of the 3rd International Confer-
ence on Language Resources and Evaluation (LREC-
2002). Las Palmas, Spain. pp. 2008-2013. 
McDonald, Ryan. 2006. Discriminative learning and 
Spanning Tree Algorithms for Dependency    parsing. 
Ph.D. thesis, University of Pennyslvania. 
Stanley F. Chen and Ronald Rosenfeld. 1999. A gaus- 
sian prior for smoothing maximum entropy models. 
Technical. Report CMU-CS-99-108. 
24
Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers,
pages 151?160, Dublin, Ireland, August 23-29 2014.
A Probabilistic Model for Learning Multi-Prototype Word Embeddings
Fei Tian
?
, Hanjun Dai
?
, Jiang Bian
?
, Bin Gao
?
,
Rui Zhang
?
, Enhong Chen
?
, Tie-Yan Liu
?
?
University of Science and Technology of China, Hefei, P.R.China
?
Fudan University, Shanghai, P.R.China
?
Microsoft Research, Building 2, No. 5 Danling Street, Beijing, P.R.China
?
Sun Yat-Sen University, Guangzhou, P.R.China
?
tianfei@mail.ustc.edu.cn,
?
cheneh@ustc.edu.cn,
?
daihanjun@gmail.com,
?
{jibian, bingao, tyliu}@microsoft.com,
?
rayz0620@hotmail.com
Abstract
Distributed word representations have been widely used and proven to be useful in quite a few
natural language processing and text mining tasks. Most of existing word embedding models aim
at generating only one embedding vector for each individual word, which, however, limits their
effectiveness because huge amounts of words are polysemous (such as bank and star). To address
this problem, it is necessary to build multi embedding vectors to represent different meanings of
a word respectively. Some recent studies attempted to train multi-prototype word embeddings
through clustering context window features of the word. However, due to a large number of
parameters to train, these methods yield limited scalability and are inefficient to be trained with
big data. In this paper, we introduce a much more efficient method for learning multi embedding
vectors for polysemous words. In particular, we first propose to model word polysemy from a
probabilistic perspective and integrate it with the highly efficient continuous Skip-Gram model.
Under this framework, we design an Expectation-Maximization algorithm to learn the word?s
multi embedding vectors. With much less parameters to train, our model can achieve comparable
or even better results on word-similarity tasks compared with conventional methods.
1 Introduction
Distributed word representations usually refer to low dimensional and dense real value vectors (a.k.a.
word embeddings) to represent words, which are assumed to convey semantic information contained in
words. With the exploding text data on the Web and fast development of deep neural network technolo-
gies, distributed word embeddings have been effectively trained and widely used in a lot of text mining
tasks (Bengio et al., 2003) (Morin and Bengio, 2005) (Mnih and Hinton, 2007) (Collobert et al., 2011)
(Mikolov et al., 2010) (Mikolov et al., 2013b).
While word embedding plays an increasingly important role in many tasks, most of word embedding
models, which assume one embedding vector for each individual word, suffer from a critical limitation
for modeling tremendous polysemous words (e.g. bank, left, doctor). Using the same embedding vec-
tor to represent the different meanings (we will call prototype of a word in the rest of the paper) of a
polysemous word is somehow unreasonable and sometimes it even hurts the model?s expression ability.
To address this problem, some recent efforts, such as (Reisinger and Mooney, 2010) (Huang et al.,
2012), have investigated how to obtain multi embedding vectors for the respective different prototypes
of a polysemous word. Specifically, these works usually take a two-step approach: they first train single
prototype word representations through a multi-layer neural network with the assumption that one word
only yields single word embedding; then, they identify multi word embeddings for each polysemous
word by clustering all its context window features, which are usually computed as the average of single
prototype embeddings of its neighboring words in the context window.
Compared with traditional single prototype model, these models have demonstrated significant im-
provements in many semantic natural language processing (NLP) tasks. However, they suffer from a
This work is licenced under a Creative Commons Attribution 4.0 International License. Page numbers and proceedings footer
are added by the organizers. License details: http://creativecommons.org/licenses/by/4.0/
151
crucial restriction in terms of scalability when facing exploding training text corpus, mainly due to the
deep layers and huge amounts of parameters in the neural networks in these models. Moreover, the
performance of these multi-prototype models is quite sensitive to the clustering algorithm and requires
much effort in clustering implementation and parameter tuning. The lack of probabilistic explanation
also refrains clustering based methods from being applied to many text mining tasks, such as language
modeling.
To address these challenges, in this work, we propose a new probabilistic multi-prototype model and
integrate it into a highly efficient continuous Skip-Gram model, which was recently introduced in the
well-known Word2Vec toolkit (Mikolov et al., 2013b). Compared with conventional neural network
language models which usually set up a multi-layer neural network, Word2Vec merely leverages a three-
layer neural network to learn word embeddings, resulting in greatly decreased number of parameters and
largely increased scalability. However, similar to most of existing word embedding models, Word2Vec
also assumes one embedding for one word. We break this limitation by introducing a new probabilistic
framework which employs hidden variables to indicate which prototype each word belongs to in the con-
text. In this framework, the conditional probability of observing word w
O
conditioned on the presence
of neighboring word w
I
(i.e. P(w
O
|w
I
)) can be formulated as a mixture model, where mixtures corre-
sponds to w
I
?s different prototypes. This is a more natural way to define P(w
O
|w
I
), since it has taken the
polysemy of word w
I
into consideration. After defining the model, we design an efficient Expectation-
Maximization (EM) algorithm to learn various word embedding vectors corresponding to each of w
I
?s
prototypes. Evaluations on widely used word similarity tasks demonstrate that our algorithm produces
comparable or even better word embeddings compared with either clustering-based multi-prototype mod-
els or the original Skip-Gram model. Furthermore, as a unified way to obtain multi word embeddings,
our proposed method can effectively avoid the sensitivity to the clustering algorithm applied by previous
multi-prototype word embedding approach.
The following of the paper is organized as follows: we introduce related work in Section 2. Then,
Section 3 describes our new model and algorithm in details and conducts a comparison in terms of
complexity between our algorithm and the previous method. We present our experimental results in
Section 4. The paper is concluded in Section 5.
2 Related Work
Since the initial work (Bengio et al., 2003), there have been quite a lot of neural network based models
to obtain distributed word representations (Morin and Bengio, 2005) (Mnih and Hinton, 2007) (Mikolov
et al., 2010) (Collobert et al., 2011) (Mikolov et al., 2013b). Most of these models assume that one
word has only one embedding, except the work of Eric Huang (Huang et al., 2012), in which the authors
propose to leverage global context information and multi-prototype embeddings to achieve performance
gains in word similarity task. To obtain multi-prototype word embeddings, this work conducts clustering
on a word?s all context words? features in the corpus. The features are the embedding vectors trained
previously via a three-layer neural network. Each cluster?s centroid is regarded as the embedding vector
for each prototype. Their reported experimental results verify the importance of considering multi-
prototype models.
Note that (Reisinger and Mooney, 2010) also proposes to deal with the word polysemy problem by
assigning to each prototype a real value vector. However their embedding vectors are obtained through
a tf-idf counting model, which is usually called as distributional representations (Turian et al., 2010),
rather than through a neural network. Therefore, we do not regard their paper as very related to our
work. The similar statement holds for other works on vector model for word meaning in context such as
(Erk and Pad?o, 2008) (Thater et al., 2011) (Reddy et al., 2011) (Van de Cruys et al., 2011).
Our model is mainly based on the recent proposed Word2Vec model, more concretely, the continuous
Skip-Gram model (Mikolov et al., 2013a) (Mikolov et al., 2013b). The continuous Skip-Gram model
specifies the probability of observing the context words conditioned on the central word w
I
in the win-
dow via a three-layer neural network. With less parameters to train (thus higher scalability), Word2Vec
discovers interesting analogical semantic relations between words like Japan - Tokyo = France - Paris.
152
3 Model Description
In this section, we introduce our algorithm for learning multi-prototype embeddings in details. In partic-
ular, since our new model is based on the continuous Skip-Gram model, we first make a brief introduction
to the Skip-Gram model. Then, we present our new multi-prototype algorithm and how we integrate it
into the Skip-Gram model. After that, we propose an EM algorithm to conduct the training process. We
also conduct a comparison on the number of parameters between the new EM algorithm and the state-
of-the-art multi-prototype model proposed in (Huang et al., 2012), which can illustrate the efficiency
superior of our algorithm.
3.1 Multi-Prototype Skip-Gram Model
In contrast to the conventional ways of using context words to predict the next word or the central
word, the Skip-Gram model (Mikolov et al., 2013b) aims to leverage the central word to predict its
context words. Specifically, assuming that the central word is w
I
and one of its neighboring word is w
O
,
P(w
O
|w
I
) is modeled in the following way:
P(w
O
|w
I
) =
exp(V
T
w
I
U
w
O
)
?
w?W
exp(V
T
w
I
U
w
)
, (1)
where W denotes the dictionary consisting of all words, U
w
?R
d
and V
w
?R
d
represent the d-dimensional
?output? and ?input? embedding vectors of word w, respectively. Note that all the parameters to be learned
are the input and output embedding vectors of all words, i.e. U = {U
w
|w ?W} and V = {V
w
|w ?W}.
This corresponds to a three-layer neural network, in which U and V denote the two parameter matrices of
the neural network. Compared with the conventional neural networks employed in the literature which
yield at least four layers (including the look-up table layer), the Skip-Gram model greatly reduces the
number of parameters and thus gives rise to a significant improvement in terms of training efficiency.
Our proposed Multi-Prototype Skip-Gram model is similar to the original Skip-Gram model in that it
also aims to model P(w
O
|w
I
) and uses two matrices (the input and output embedding matrices) as the
parameters. The difference lies in that given word w
I
, the occurrence of word w
O
is described as a finite
mixture model, in which each mixture corresponds to a prototype of word w
I
. To be specific, suppose
that word w has N
w
prototypes and it appears in its h
w
-th prototype, i.e., h
w
? {1, ? ? ? ,N
w
} is the index of
w?s prototype. Then P(w
O
|w
I
) is expanded as:
p(w
O
|w
I
) =
N
w
I
?
i=1
P(w
O
|h
w
I
= i,w
I
)P(h
w
I
= i|w
I
) (2)
=
N
w
I
?
i=1
exp(U
T
w
O
V
w
I
,i
)
?
w?W
exp(U
T
w
V
w
I
,i
)
P(h
w
I
= i|w
I
), (3)
where V
w
I
,i
? R
d
refers to the embedding vector of w
I
?s i-th prototype. This equation states that P(w
O
|w
I
)
is a weighted average of the probabilities of observing w
O
conditioned on the appearance of w
I
?s every
prototype. The probability P(w
O
|h
w
I
= i,w
I
) takes the similar softmax form to equation (1) and the
weight is specified as a prior probability of word w
I
falls in its every prototype.
The general idea behind the Multi-Prototype Skip-Gram model is very intuitive: the surrounding words
under different prototypes of the same word are usually different. For example, when the word bank
refers to the side of a river, it is very possible to observe the corresponding context words such as
river, water, and slope; however, when bank falls into the meaning of the financial organization, the
surrounding word set is likely to be comprised of quite different words, such as money, account, and
investment.
The probability formulation in (3) brings much computation cost because of the linear dependency of
|W | in the denominator
?
w?W
exp(U
T
w
V
w
I
,i
). To address this issue, several efficient methods have been
proposed such as Hierarchical Softmax Tree (Morin and Bengio, 2005) (Mnih and Kavukcuoglu, 2013)
and Negative Sampling (Mnih and Kavukcuoglu, 2013) (Mikolov et al., 2013b). Taking Hierarchical
153
Softmax Tree as an example, through a binary tree in which every word is a leaf node, word w
O
is
associated with a binary vector b
(w
O
)
? {?1,+1}
L
w
O
specifying a path from the root of the tree to leaf
w
O
, where L
w
O
is the length of vector b
(w
O
)
. Then the conditional probability is described as
P(w
O
|h
w
I
= i,w
I
) =
L
w
O
?
t=1
P
(
b
(w
O
)
t
|w
I
,h
w
I
= i
)
=
L
w
O
?
t=1
?
(
b
(w
O
)
t
U
T
w
O
,t
V
w
I
,i
)
, (4)
where ?(x) = 1/(1+ exp(?x)) is the sigmoid function, and U
w
O
,t
specifies the d-dimensional parameter
vector associated with the t-th node in the path from the root to the leaf node w
O
. Substituting (4) into
(2) to replace the large softmax operator in (3) leads to a much more efficient probability form.
3.2 EM Algorithm
In this section, we describe the EM algorithm adopted to train the Multi-Prototype Skip-Gram model.
Without loss of generality, we will focus on obtaining multi embeddings for a specified word w ?W
with N
w
prototypes. Word w?s embedding vectors are denoted as V
w
? R
d?N
w
. Suppose there are M
word pairs for training: {(w
1
,w),(w
2
,w), ? ? ? ,(w
M
,w)}, where all the inputs words (i.e., word w) are the
same, and the set of output words to be predicted are denoted as X= {w
1
,w
2
, ? ? ? ,w
M
}. That is, X are M
surrounding words of w in the training corpus.
For ease of reference and without loss of generality, we make some changes to the notations in Section
3.1. We will use h
m
as the index of w?s prototype in the pair (w
m
,w), m ? {1,2, ? ? ? ,M}. Besides,
some new notations are introduced: P(h
w
= i|w
I
) is simplified as pi
i
, and ?
m,k
, where m ? {1,2, ? ? ? ,M},
k ? {1,2, ? ? ?N
w
}, are the hidden binary variables indicating whether the m-th presence of word w is in
its k-th prototype, i.e. ?
m,k
= 1
h
m
=k
, where 1 is the indicator function. Other notations are the same as
before: V
w,i
? R
d
is the embedding vector for word w?s i-th prototype, U
w,t
? R
d
is the embedding vector
for the t-th node on the path from the tree root to the leaf node representing word w, and b
(w)
t
? {?1,1}
is the t-th bit of the binary coding vector of word w along its corresponding path on the Hierarchical
Softmax Tree.
Then the parameter set we aim to learn is ? = {pi
1
, ? ? ? ,pi
N
w
;U ;V
w
}. The hidden variable set is ? =
{?
m,k
|m ? (1,2, ? ? ? ,M),k ? (1,2, ? ? ? ,N
w
)}. Considering equation (2) and (4), we have the log likelihood
of X as below:
logP(X,?|?) =
M
?
m=1
N
w
?
k=1
?
m,k
(
logpi
k
+ logP(w
m
|h
m
= k,w)
)
=
M
?
m=1
N
w
?
k=1
?
m,k
(
logpi
k
+
L
w
m
?
t=1
log?(b
(w
m
)
t
U
T
w
m
,t
V
w,k
)
)
.
(5)
With equation (5), the E-Step and M-Step are:
E-Step:
The conditional expectation of hidden variable ?
m,k
, denoted as
?
?
m,k
, is:
?
?
m,k
= P(?
m,k
= 1|X,?) =
pi
k
P(w
m
|h
m
= k,w)
?
N
w
i=1
pi
i
P(w
m
|h
m
= i,w)
.
(6)
The Q function w.r.t. the parameters at the i-th iteration ?
(i)
is written as:
Q(? ,?
(i)
) =
N
w
?
k=1
M
?
m=1
?
?
m,k
(logpi
k
+ logP(w
m
|h
m
= k,w))
=
M
?
m=1
N
w
?
k=1
?
?
m,k
(
logpi
k
+
L
w
m
?
t=1
log?(b
(w
m
)
t
U
T
w
m
,t
V
w,k
)
)
.
(7)
M-Step:
pi can be updated by
154
pik
=
?
M
m=1
?
?
m,k
M
, k = 1,2 ? ? ? ,N
w
. (8)
We leave the detailed derivations for equation (6), (7), and (8) to the appendix of the paper. Then we
discuss how we obtain the update of the embedding parameters U
w
m
,t
and V
w,k
. Note that the optimization
problem is non-convex, and it is hard to compute the exact solution of
?Q
?U
w
m
,t
= 0 and
?Q
?V
w,k
= 0. Therefore,
we use gradient ascent to optimize in the M-step. The gradients of Q function w.r.t. embedding vectors
are given by:
?Q
?U
w
m
,t
=
N
w
?
k=1
?
?
m,k
b
(w
m
)
t
(
1? ?(b
(w
m
)
t
U
T
w
m
,t
V
w,k
)
)
V
w,k
,
(9)
?Q
?V
w,k
=
M
?
m=1
?
?
m,k
L
w
m
?
t=1
b
(w
m
)
t
(
1? ?(b
(w
m
)
t
U
T
w
m
,t
V
w,k
)
)
U
w
m
,t
.
(10)
Iterating between E-Step and M-Step till the convergence of the value of function Q makes the EM
algorithm complete.
In order to enhance the scalability of our approach, we propose a fast computing method to boost
the implementation of the EM algorithm. Note that the most expensive computing operations in both
the E-Step and M-Step are the inner product of the input and output embedding vectors, as well as the
sigmoid function. However, if we take the Hierarchical Softmax Tree form as shown in Equation (4) to
model P(w
m
|h
m
= i,w), and perform only one step gradient ascent in M-Step, the aforementioned two
expensive operations in M-Step will be avoided by leveraging the pre-computed results in the E-Step.
Specifically, since the gradient of the function f (x) = log?(x) is given by f
?
(x) = 1? ?(x), the sigmoid
values computed in the E-Step to obtain P(w
m
|h
m
= i,w) (i.e. the term ?(b
(w
m
)
t
U
T
w
m
,t
V
w,k
) in equation (5),
(9), and (10)) can be re-used to derive the gradients in the M-Step.
However, such enhanced computation method cannot benefit the second order optimization methods
in the M-Step such as L-BFGS and Conjugate Gradient, since they usually rely on multiple iterations to
converge. In fact, we tried these two optimization methods in our experiments but they have brought no
improvement compared with simple one-step gradient ascent method.
3.3 Model Comparison
To show that our model is more scalable than the former multi-prototype model in (Huang et al., 2012)
(We denote it as EHModel in the rest of the paper), we conduct a comparison on the number of parameters
with respect to each of these two models in this subsection.
We use n
embedding
and n
window
to denote the numbers of all word embedding vectors and context win-
dow words, respectively. It is clear that n
embeddings
=
?
w?W
N
w
. EHModel aims to compute two scores,
i.e., the local score and the global score, both with hidden layer node activations. We denote the hidden
layer node number as h
l
and h
g
for these two scores. The parameter numbers are listed in Table 1.
Model EHModel Our Model
#parameters dn
words
+dn
embeddings
+(dn
window
+1)h
l
+(2d +1)h
g
dn
words
+dn
embeddings
Table 1: Comparison of parameter numbers of two models
Note that d in Table 1 denotes the embedding vector size. It can be observed that EHModel has
(dn
window
+1)h
l
+(2d +1)h
g
more parameters than our model, which is mainly because EHModel has
one more layer in the neural network and it considers global context. In previous study (Huang et al.,
2012), d, n
window
, h
l
, and h
g
are set to be 50, 10, 100, 100, respectively, which greatly increases the gap
of parameter numbers between the two models.
155
4 Experiments
In this section, we will present our experimental settings and results. Particularly, we first describe the
data collection and the training configuration we used in the experiments; then, we conduct a qualitative
case study followed by quantitative evaluation results on a public word similarity task to demonstrate the
performance of our proposed model.
4.1 Experimental Setup
Dataset: To make a fair comparison with the state-of-the-art methods, we employ a publicly available
dataset, which is used in (Huang et al., 2012), to train word embeddings in our experiments. Particularly,
this training corpus is a snapshot of Wikipedia at April, 2010 (Shaoul, 2010), which contains about 990
million tokens. We removed the infrequent words from this corpus and kept a dictionary of about 1
million most frequent words. Similar to Word2Vec, we removed pure digit words such as 2014 as well
as about 100 stop words like how, for, and we.
Training Configuration: In order to boost the training speed, we take advantage of the Hierarchical
Softmax Tree structure. More concretely, we use the Huffman tree structure, as introduced in Word2Vec,
to further increase the training speed. All the embedding size, including both word embedding vectors
and the Huffman tree node embedding vectors, are set to be 50, which is the same as the size used in
(Huang et al., 2012). To train word embedding, we set the context window size as 10, i.e., for a word w,
10 of the closest neighboring words to w are regarded as ws contexts. For the numbers of word prototypes,
i.e., N
w
introduced in Section 3.2, we set the top 7 thousand frequent words as multi-prototype words by
experience, with all of them having 10 prototypes (i.e. N
w
= 10).
During the training process, we used the same strategy to set the learning rate as what Word2Vec did.
Specifically, we set the initial learning rate to 0.025 and diminished the value linearly along with the
increasing number of training words. Our experimental results illustrate that this learning rate strategy
can lead to the best results for our algorithm.
For the hyper parameters of the EM algorithm, we set the batch size to 1, i.e. M = 1 in Section 3.2,
since our experimental results reveal that smaller batch size can result in better experimental results. The
reason is explained as the following. Our optimization problem is highly non-convex. Smaller batch size
yields more frequent updates of parameters, and thus avoids trapping in local optima, while larger batch
size, associated with more infrequent parameter updating, may cause higher probability to encounter
local optima. In our experiments, we observe that only one iteration of E-Step and M-Step can reach the
embedding vectors with good enough performance on the word similarity task, whereas increasing the
iteration number just leads to slight performance improvement with much longer training time. Under
the above configuration, our model runs about three times faster than EHModel.
4.2 Case Study
This section gives some qualitative evaluations of our model by demonstrating how our model can ef-
fectively identify multi-prototype word embeddings on some specific cases. In Table 2, we list several
polysemous words. For each word, we pick some of their prototypes learned by our model, including
the prototype prior probability (i.e. pi
i
introduced in Section 3.2) and three of the most similar words
with each prototype, respectively. The similarity is calculated by the cosine similarity score between the
embedding vectors.
From the table we can observe some interesting results of the multi-prototype embedding vectors
produced by our model:
? For a polysemous word, its different embedding vectors represent its different semantic meanings.
For example, the first embedding vector of the word apple corresponds to its sense as a kind of fruit,
whereas the second one represents its meaning as an IT company.
? The prior probability reflects the likelihood of the occurrence of various prototypes to some extent.
For example, the word cell is more likely to represent the meaning of the smallest part of living
structure (with probability 0.81), than to be used as the meaning of cellphone (with probability
156
Word Prior Probability Most Similar Words
apple 1 0.82 strawberry, cherry, blueberry
apple 2 0.17 iphone, macintosh, microsoft
bank 1 0.15 river, canal, waterway
bank 2 0.6 citibank, jpmorgan, bancorp
bank 3 0.25 stock, exchange, banking
cell 1 0.09 phones, cellphones, mobile
cell 2 0.81 protein, tissues, lysis
cell 3 0.01 locked, escape, handcuffed
Table 2: Most similar words with different prototypes of the same word
0.09) or prisoned (with probability 0.01). Note that the three prior probability scores of cell do not
sum to 1. The reason is that there are some other embeddings not presented in the table which are
found to have high similarities with the three embeddings. We do not present them due to the space
limitation.
? By setting the prototype number to a fairly large value (e.g. N
w
= 10), the model tends to learn
more fine-grained separations of the word?s different meanings. For example, we can observe from
Table 2 that the second and the third prototypes of the word bank seem similar to each other as both
of them denote a financial concept. However, there are subtle differences between them: the second
prototype represents concrete banks, such as citibank and jpmorgan, whereas the third one denotes
what is done in the banks, since it is most similar to the words stock, exchange, and banking. We
believe that such a fine-grained separation will bring more expressiveness to the multi-prototype
word embeddings learned by our model.
4.3 Results on Word Similarity in Context Dataset
In this subsection, we give quantitative comparison of our method with conventional word embedding
models, including Word2Vec and EHModel (Huang et al., 2012).
The task we perform is the word similarity evaluation introduced in (Huang et al., 2012). Word simi-
larity tasks evaluate a model?s performance by calculating the Spearman?s rank correlation between the
ranking of ground truth similarity scores (given by human labeling) and the ranking based on the simi-
larity scores produced by the model. Traditional word similarity tasks such as WordSim353 (Finkelstein
et al., 2001) and RG (Rubenstein and Goodenough, 1965) are not suitable for evaluating multi-prototype
models since there is neither enough number of polysemous words in these datasets nor context infor-
mation to infer the prototype index. To address this issue, a new word similarity benchmark dataset
including context information was released in (Huang et al., 2012). Following (Luong et al., 2013), we
use SCWS to denote this dataset. Similar to WordSim353, SCWS contains some word pairs (concretely,
2003 pairs), together with human labeled similarity scores for these word pairs. What makes SCWS
different from WS353 is that the words in SCWS are contained in sentences, i.e., there are 2003 pairs of
sentences containing these words, while words in WS353 are not associated with sentences. Therefore,
the human labeled scores are based on the meanings of the words in the context. Given the presence
of the context, the word similarity scores, especially those scores depending on polysemous words, are
much more convincing for evaluating different models? performance in our experiments.
Then, we propose a method to compute the similarity score for a pair of words {w
1
,w
2
} in the context
based on our model. Suppose that the context of a word w is defined as all its neighboring words in a
T +1 sized window, where w is the central word in the window. We use Context
1
= {c
1
1
,c
1
2
, ? ? ? ,c
1
T
} and
Context
2
= {c
2
1
,c
2
2
, ? ? ? ,c
2
T
} to separately denote the context of w
1
and w
2
, where c
1
t
and c
2
t
are the t-th
context word of w
1
and w
2
, respectively. According to Bayesian rule, we have that for i? {1,2, ? ? ? ,N
w
1
}:
P(h
w
1
= i|Context
1
,w
1
) ? P(Context
1
|h
w
1
= i,w
1
)P(h
w
1
= i|w
1
)
=
T
?
t=1
P(c
1
t
|h
w
1
= i,w
1
)P(h
w
1
= i|w
1
),
(11)
157
where P(c
1
t
|h
w
1
= i,w
1
) can be calculated by equation (4) and P(h
w
1
= i|w
1
) is the prior probability
we learned in the EM algorithm (equation (8)). The similar equation holds for word w
2
as well. Here
we make an assumption that the context words are independent with each other given the central word.
Furthermore, suppose that the most likely prototype index for w
1
given Context
1
is
?
h
w
1
, i.e., we de-
note
?
h
w
1
= argmax
i?{1,2,??? ,N
w
1
}
P(h
w
1
= i|Context
1
,w
1
). Similarly,
?
h
w
2
is denoted as the corresponding
meaning for w
2
.
We calculate two similarity scores base on equation (11), i.e., MaxSim Score and WeightedSim Score:
MaxSim(w
1
,w
2
) =Cosine(V
w
1
,
?
h
w
1
,V
w
2
,
?
h
w
2
), (12)
WeightedSim(w
1
,w
2
) =
N
w
1
?
i=1
N
w
2
?
j=1
P(h
w
1
= i|Context
1
,w
1
)P(h
w
2
= j|Context
2
,w
2
)Cosine(V
w
1
,i
,V
w
2
, j
).
(13)
In the above similarity scores, Cosine(x,y) denotes the cosine similarity score of vector x and y, and
V
w,i
? R
d
is the embedding vector for the word w?s i-th prototype.
The detailed experimental results are listed in Table 3, where ? refers to the Spearman?s rank cor-
relation. The higher value of ? indicates the better performance. The performance score of EHModel
is borrowed from its original paper (Huang et al., 2012). For Word2Vec model, we use Hierarchical
Huffman Tree rather than Negative Sampling to do the acceleration. Our Model M uses the MaxSim
score in testing and our Model W uses the WeightedSim score. All of these models are run on the same
aforementioned Wikipedia corpus, with the dimension of the embedding space to be 50.
From the table, we can observe that our Model W (65.4%) outperforms the original Word2Vec model
(61.7%), and achieves almost the same performance with the state-of-the-art EHModel (65.7%). Among
the two similarity measures used in testing, the WeightedSim score performs better (65.4%) than the
MaxSim score (63.6%), indicating that the overall consideration of all prototype probabilities are more
effective.
Model ??100
Word2Vec 61.7
EHModel 65.7
Model M 63.6
Model W 65.4
Table 3: Spearman?s rank correlations on SCWS dataset.
5 Conclusion
In this paper, we introduce a fast and probabilistic method to generate multiple embedding vectors for
polysemous words, based on the continuous Skip-Gram model. On one hand, our method addresses
the drawbacks of the original Word2Vec model by leveraging multi-prototype word embeddings; on the
other hand, our model yields much less complexity without performance loss compared with the former
clustering based multi-prototype algorithms. In addition, the probabilistic framework of our method
avoids the extra efforts to perform clustering besides training word embeddings.
For the future work, we plan to apply the proposed probabilistic framework to other neural network
language models. Moreover, we would like to apply the multi-prototype embeddings to more real world
text mining tasks, such as information retrieval and knowledge mining, with the expectation that the
multi-prototype embeddings produced by our model will benefit these tasks.
References
Yoshua Bengio, R?ejean Ducharme, Pascal Vincent, and Christian Janvin. 2003. A neural probabilistic language
model. In Journal of Machine Learning Research, pages 1137?1155.
Ronan Collobert, Jason Weston, L?eon Bottou, Michael Karlen, Koray Kavukcuoglu, and Pavel Kuksa. 2011.
Natural language processing (almost) from scratch. The Journal of Machine Learning Research, 12:2493?2537.
158
Katrin Erk and Sebastian Pad?o. 2008. A structured vector space model for word meaning in context. In Proceed-
ings of the Conference on Empirical Methods in Natural Language Processing, EMNLP ?08, pages 897?906,
Stroudsburg, PA, USA. Association for Computational Linguistics.
Lev Finkelstein, Evgeniy Gabrilovich, Yossi Matias, Ehud Rivlin, Zach Solan, Gadi Wolfman, and Eytan Ruppin.
2001. Placing search in context: The concept revisited. In Proceedings of the 10th international conference on
World Wide Web, pages 406?414. ACM.
Eric H Huang, Richard Socher, Christopher D Manning, and Andrew Y Ng. 2012. Improving word representations
via global context and multiple word prototypes. In Proceedings of the 50th Annual Meeting of the Associa-
tion for Computational Linguistics: Long Papers-Volume 1, pages 873?882. Association for Computational
Linguistics.
Minh-Thang Luong, Richard Socher, and Christopher D Manning. 2013. Better word representations with recur-
sive neural networks for morphology. CoNLL-2013, 104.
Tomas Mikolov, Martin Karafi?at, Lukas Burget, Jan Cernock`y, and Sanjeev Khudanpur. 2010. Recurrent neural
network based language model. In INTERSPEECH, pages 1045?1048.
Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey Dean. 2013a. Efficient estimation of word representations
in vector space. arXiv preprint arXiv:1301.3781.
Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S. Corrado, and Jeff Dean. 2013b. Distributed representations
of words and phrases and their compositionality. In C.J.C. Burges, L. Bottou, M. Welling, Z. Ghahramani, and
K.Q. Weinberger, editors, Advances in Neural Information Processing Systems 26, pages 3111?3119.
Andriy Mnih and Geoffrey Hinton. 2007. Three new graphical models for statistical language modelling. In
Proceedings of the 24th international conference on Machine learning, pages 641?648. ACM.
Andriy Mnih and Koray Kavukcuoglu. 2013. Learning word embeddings efficiently with noise-contrastive esti-
mation. In C.J.C. Burges, L. Bottou, M. Welling, Z. Ghahramani, and K.Q. Weinberger, editors, Advances in
Neural Information Processing Systems 26, pages 2265?2273.
Frederic Morin and Yoshua Bengio. 2005. Hierarchical probabilistic neural network language model. In Proceed-
ings of the international workshop on artificial intelligence and statistics, pages 246?252.
Siva Reddy, Ioannis P Klapaftis, Diana McCarthy, and Suresh Manandhar. 2011. Dynamic and static prototype
vectors for semantic composition. In IJCNLP, pages 705?713.
Joseph Reisinger and Raymond J Mooney. 2010. Multi-prototype vector-space models of word meaning. In
Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association
for Computational Linguistics, pages 109?117. Association for Computational Linguistics.
Herbert Rubenstein and John B Goodenough. 1965. Contextual correlates of synonymy. Communications of the
ACM, 8(10):627?633.
Westbury C Shaoul, C. 2010. The westbury lab wikipedia corpus.
Stefan Thater, Hagen F?urstenau, and Manfred Pinkal. 2011. Word meaning in context: A simple and effective
vector model. In IJCNLP, pages 1134?1143.
Joseph Turian, Lev Ratinov, and Yoshua Bengio. 2010. Word representations: a simple and general method for
semi-supervised learning. In Proceedings of the 48th Annual Meeting of the Association for Computational
Linguistics, pages 384?394. Association for Computational Linguistics.
Tim Van de Cruys, Thierry Poibeau, and Anna Korhonen. 2011. Latent vector weighting for word meaning in
context. In Proceedings of the Conference on Empirical Methods in Natural Language Processing, EMNLP
?11, pages 1012?1022, Stroudsburg, PA, USA. Association for Computational Linguistics.
6 Appendix
6.1 Derivations for the EM Algorithm
We give detailed derivations for the updating rules used in the EM algorithms in Section 3.2., i.e., the
derivations for equation (6), (7), and (8).
159
According to the properties of conditional probability, we have
?
?
m,k
= P(?
m,k
= 1|X,?) =
P(?
m,k
= 1,X|?)
?
N
w
i=1
P(?
m,i
= 1,X|?)
=
P(?
m,k
= 1|?)P(X|?
m,k
= 1,?)
?
N
w
i=1
P(?
m,i
= 1|?)P(X|?
m,i
= 1,?)
=
pi
k
P(w
m
|h
m
= k,w)
?
N
w
i=1
pi
i
P(w
m
|h
m
= i,w)
.
(14)
From equation (7), the Q function is calculated as:
Q(? ,?
(i)
) = E[logP(X,?|?)|?
(i)
]
=
N
w
?
k=1
M
?
m=1
E[?
m,k
|?
(i)
]
(
logpi
k
+
L
w
m
?
t=1
log?(b
(w
m
)
t
U
T
w
m
,t
V
w,k
)
)
=
N
w
?
k=1
M
?
m=1
?
?
m,k
(
logpi
k
+
L
w
m
?
t=1
log?(b
(w
m
)
t
U
T
w
m
,t
V
w,k
)
)
=
M
?
m=1
N
w
?
k=1
?
?
m,k
(
logpi
k
+
L
w
m
?
t=1
log?(b
(w
m
)
t
U
T
w
m
,t
V
w,k
)
)
.
(15)
Then we give the derivations for pi?s updating rule, i.e., equation (8). Note that for parameters pi
k
,
k = {1,2, ? ? ? ,N
w
}, they need to satisfy the condition that
?
N
w
k=1
pi
k
= 1. From equation (7) (or equivalently
equation (15)), the loss with regard to pi is:
L
[pi]
=
M
?
m=1
N
w
?
k=1
?
?
m,k
logpi
k
+? (
N
w
?
k=1
pi
k
?1), (16)
where ? is the Language multiplier. Letting
?L
[pi]
?pi
= 0, we obtain:
pi
k
?
M
?
m=1
?
?
m,k
. (17)
Further considering the fact that
?
N
w
k=1
?
M
m=1
?
?
m,k
= M, we have pi
k
=
?
M
m=1
?
?
m,k
M
.
160
Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 935?945,
Seattle, Washington, USA, 18-21 October 2013. c?2013 Association for Computational Linguistics
A Dataset for Research on Short-Text Conversation ?
Hao Wang? Zhengdong Lu? Hang Li? Enhong Chen?
? xdwangh@mail.ustc.edu.cn ?lu.zhengdong@huawei.com
?hangli.hl@huawei.com ?cheneh@ustc.edu.cn
?Univ. of Sci & Tech of China, China ?Noah?s Ark Lab, Huawei Technologies, Hong Kong
Abstract
Natural language conversation is widely re-
garded as a highly difficult problem, which
is usually attacked with either rule-based or
learning-based models. In this paper we
propose a retrieval-based automatic response
model for short-text conversation, to exploit
the vast amount of short conversation in-
stances available on social media. For this
purpose we introduce a dataset of short-text
conversation based on the real-world instances
from Sina Weibo (a popular Chinese mi-
croblog service), which will be soon released
to public. This dataset provides rich collec-
tion of instances for the research on finding
natural and relevant short responses to a given
short text, and useful for both training and test-
ing of conversation models. This dataset con-
sists of both naturally formed conversation-
s, manually labeled data, and a large repos-
itory of candidate responses. Our prelimi-
nary experiments demonstrate that the simple
retrieval-based conversation model performs
reasonably well when combined with the rich
instances in our dataset.
1 Introduction
Natural language conversation is one of the holy
grail of artificial intelligence, and has been taken as
the original form of the celebrated Turing test. Pre-
vious effort in this direction has largely focused on
analyzing the text and modeling the state of the con-
versation through dialogue models, while in this pa-
?The work is done when the first author worked as intern at
Noah?s Ark Lab, Huawei Techologies.
per we take one step back and focus on a much easi-
er task of finding the response for a given short text.
This task is in clear contrast with previous effort in
dialogue modeling in the following two aspects
? we do not consider the context or history of
conversations, and assume that the given short
text is self-contained;
? we only require the response to be natural, rel-
evant, and human-like, and do not require it to
contain particular opinion, content, or to be of
particular style.
This task is much simpler than modeling a complete
dialogue session (e.g., as proposed in Turing test),
and probably not enough for real conversation sce-
nario which requires often several rounds of interac-
tions (e.g., automatic question answering system as
in (Litman et al, 2000)). However it can shed impor-
tant light on understanding the complicated mecha-
nism of the interaction between an utterance and it-
s response. The research in this direction will not
only instantly help the applications of short session
dialogue such as automatic message replying on mo-
bile phone and the chatbot employed in voice assis-
tant like Siri1, but also it will eventually benefit the
modeling of dialogues in a more general setting.
Previous effort in modeling lengthy dialogues fo-
cused either on rule-based or learning-based models
(Carpenter, 1997; Litman et al, 2000; Williams and
Young, 2007; Schatzmann et al, 2006; Misu et al,
2012). This category of approaches require relative-
ly less data (e.g. reinforcement learning based) for
1http://en.wikipedia.org/wiki/Siri
935
training or no training at all, but much manual ef-
fort in designing the rules or the particular learning
algorithms. In this paper, we propose to attack this
problem using an alternative approach, by leverag-
ing the vast amount of training data available from
the social media. Similar ideas have appeared in (Ja-
farpour and Burges, 2010; Leuski and Traum, 2011)
as an initial step for training a chatbot.
With the emergence of social media, especially
microblogs such as Twitter, in the past decade, they
have become an important form of communication
for many people. As the result, it has collected con-
versation history with volume previously unthink-
able, which brings opportunity for attacking the con-
versation problem from a whole new angle. More
specifically, instead of generating a response to an
utterance, we pick a massive suitable one from the
candidate set. The hope is, with a reasonable re-
trieval model and a large enough candidate set, the
system can produce fairly natural and appropriate re-
sponses.
This retrieval-based model is somewhat like non-
parametric model in machine learning communities,
which performs well only when we have abundan-
t data. In our model, it needs only a relatively s-
mall labeled dataset for training the retrieval model,
but requires a rather large unlabeled set (e.g., one
million instances) for candidate responses. To fur-
ther promote the research in similar direction, we
create a dataset for training and testing the retrieval
model, with a candidate responses set of reason-
able size. Sina Weibo is the most popular Twitter-
like microblog service in China, hosting over 500
million registered users and generating over 100
million messages per day 2. As almost all mi-
croblog services, Sina Weibo allows users to com-
ment on a published post3, which forms a natural
one-round conversation. Due to the great abundance
of those (post, response) pairs, it provides an ideal
data source and test bed for one-round conversation.
We will make this dataset publicly available in the
near future.
2http://en.wikipedia.org/wiki/Sina_Weibo
3Actually it also allows users to comment on other users?
comments, but we will not consider that in the dataset.
2 The Dialogues on Sina Weibo
Sina Weibo is a Twitter-like microblog service, on
which a user can publish short messages (will be re-
ferred to as post in the remainder of the paper) visi-
ble to public or a group specified by the user. Simi-
lar to Twitter, Sina Weibo has the word limit of 140
Chinese characters. Other users can comment on a
published post, with the same length limit, as shown
in the real example given in Figure 6 (in Chinese).
Those comments will be referred to as responses in
the remainder of the paper.
Figure 1: An example of Sina Weibo post and the com-
ments it received.
We argue that the (post, response) pairs on Sina
Weibo provide rather valuable resource for studying
one round dialogue between users. The comments
to a post can be of rather flexible forms and diverse
topics, as illustrated in the example in Table 1. With
a post stating the user?s status (traveling to Hawaii),
the comments can be of quite different styles and
contents, but apparently all appropriate.
In many cases, the (post, response) pair is self-
contained, which means one does not need any back-
ground and contextual information to get the main
point of the conversation (Examples of that include
the responses from B, D, G and H). In some cas-
es, one may need extra knowledge to understand the
conversation. For example, the response from user
E will be fairly elusive if taken out of the context
that A?s Hawaii trip is for an international confer-
ence and he is going to give a talk there. We argue
that the number of self-contained (post, response)
pairs is vast, and therefore the extracted (post, re-
936
Post
User A: The first day at Hawaii. Watching sunset at the balcony with a big glass of wine in hand.
Responses
User B: Enjoy it & don?t forget to share your photos!
User C: Please take me with you next time!
User D: How long are you going to stay there?
User E: When will be your talk?
User F: Haha, I am doing the same thing right now. Which hotel are you staying in?
User G: Stop showing-off, buddy. We are still coding crazily right now in the lab.
User H: Lucky you! Our flight to Honolulu is delayed and I am stuck in the airport. Chewing French
fries in MacDonald?s right now.
Table 1: A typical example of Sina Weibo post and the comments it received. The original text is in Chinese, and we
translated it into English for easy access of readers. We did the same thing for all the examples throughout this paper.
sponse) pairs can serve as a rich resource for ex-
ploring rather sophisticated patterns and structures
in natural language conversation.
3 Content of the Dataset
The dataset consists of three parts, as illustrated in
Figure 2. Part 1 contains the original (post, re-
sponse) pairs, indicated by the dark-grey section in
Figure 2. Part 2, indicated by the light-gray section
in Figure 2, consists labeled (post, response) pairs
for some Weibo posts, including positive and nega-
tive examples. Part 3 collects all the responses, in-
cluding but not limited to the responses in Part 1 and
2. Some of the basic statistics are summarized in
Table 2.
# posts # responses vocab. # labeled pairs
4,6345 1,534,874 105,732 12,427
Table 2: Some statistics of the dataset
Original (Post, Response) Pairs This part of
dataset gives (post, response) pairs naturally pre-
sented in the microblog service. In other words,
we create a (post, response) pair there when the re-
sponse is actually given to the post in Sina Weibo.
The part of data is noisy since the responses given
to a Weibo post could still be inappropriate for d-
ifferent reasons, for example, they could be spams
or targeting some responses given earlier. We have
628, 833 pairs.
Labeled Pairs This part of data contains the (post,
response) pairs that are labeled by human. Note that
1) the labeling is only on a small subset of posts,
and 2) for each selected post, the labeled responses
are not originally given to it. The labeling is done
in an active manner (see Section 4 for more detail-
s), so the obtained labels are much more informative
than the those on randomly selected pairs (over 98%
of which are negative). This part of data can be di-
rectly used for training and testing of retrieval-based
response models. We have labeled 422 posts and for
each of them, about 30 candidate responses.
Responses This part of dataset contains only re-
sponses, but they are not necessarily for a certain
post. These extra responses are mainly filtered out
by our data cleaning strategy (see Section 4.2) for
original (post, response) pairs, including those from
filtered-out Weibo posts and those addressing oth-
er responses. Nevertheless, those responses are still
valid candidate for responses. We have about 1.5
million responses in the dataset.
3.1 Using the Dataset for Retrieval-based
Response Models
Our data can be used for training and testing of
retrieval-based response model, or just as a bank of
responses. More specifically, it can be used in at
least the following three ways.
Training Low-level Matching Features The
rather abundant original (post, response) pairs pro-
vide rather rich supervision signal for learning dif-
ferent matching patterns between a post and a re-
sponse. These matching patterns could be of dif-
937
Figure 2: Content of the dataset.
ferent levels. For example, one may discover from
the data that when the word ?Hawaii? occurs in the
post, the response are more likely to contain word-
s like ?trip?, ?flight?, or ?Honolulu?. On a slight-
ly more abstract level, one may learn that when an
entity name is mentioned in the post, it tends to be
mentioned again in the response. More complicated
matching pattern could also be learned. For exam-
ple, the response to a post asking ?how to? is statisti-
cally longer than average responses. As a particular
case, Ritter et al (2011) applied translation model
(Brown et al, 1993) on similar parallel data extract-
ed from Twitter in order to extract the word-to-word
correlation. Please note that with more sophisticat-
ed natural language processing, we can go beyond
bag-of-words for more complicated correspondence
between post and response.
Training Automatic Response Models Although
the original (post, response) pairs are rather abun-
dant, they are not enough for discriminative training
and testing of retrieval models, for the following rea-
sons. In the labeled pairs, both positive and negative
ones are ranked high by some baseline models, and
hence more difficult to tell apart. This supervision
will naturally tune the model parameters to find the
real good responses from the seemingly good ones.
Please note that without the labeled negative pairs,
we need to generate negative pairs with randomly
chosen responses, which in most of the cases are too
easy to differentiate by the ranking model and can-
not fully tune the model parameters. This intuition
has been empirically verified by our experiments.
Testing Automatic Response Models In testing a
retrieval-based system, although we can simply use
the original responses associated with the query post
as positive and treat all the others as negative, this
strategy suffers from the problem of spurious neg-
ative examples. In other words, with a reasonably
good model, the retrieved responses are often good
even if they are not the original ones, which brings
significant bias to the evaluation. With the labeled
pairs, this problem can be solved if we limit the test-
ing only in the small pool of labeled responses.
3.2 Using the Dataset for Other Purposes
Our dataset can also be used for other researches re-
lated to short-text conversations, namely anaphora
resolution, sentiment analysis, and speech act anal-
ysis, based on the large collection of original (post,
response) pairs. For example, to determine the sen-
timent of a response, one needs to consider both
the original post as well as the observed interaction
between the two. In Figure 3, if we want to un-
derstand user?s sentiment towards the ?invited talk?
mentioned in the post, the two responses should be
taken as positive, although the sentiment in the mere
responses is either negative or neutral.
4 Creation of the Dataset
The (post, comment) pairs are sampled from the
Sina Weibo posts published by users in a loosely
connected community and the comments they re-
ceived (may not be from this community). This
community is mainly posed of professors, re-
searchers, and students of natural language process-
ing (NLP) and related areas in China, and the users
938
Figure 3: An example (original Chinese and the English
translation) on the difficulty of sentiment analysis on re-
sponses.
commonly followed them.
The creation process of the dataset, as illustrated
in Figure 4, consists of three consecutive steps: 1)
crawling the community of users, 2) crawling their
Weibo posts and their responses, 3) cleaning the da-
ta, with more details described in the remainder of
this section.
4.1 Sampling Strategy
We take the following sampling strategy for collect-
ing the (post, response) pairs to make the topic rel-
atively focused. We first locate 3,200 users from a
loosely connected community of Natural Language
Processing (NLP) and Machine Learning (ML) in
China. This is done through crawling followees4 of
ten manually selected seed users who are NLP re-
searchers active on Sina Weibo (with no less than 2
posts per day on average) and popular enough (with
no less than 100 followers).
We crawl the posts and the responses they re-
ceived (not necessarily from the crawled communi-
ty) for two months (from April 5th, 2013, to June
5th, 2013). The topics are relatively limited due to
our choice of the users, with the most saliently ones
being:
? Research: discussion on research ideas, paper-
s, books, tutorials, conferences, and researchers
in NLP and machine learning, etc;
? General Arts and Science: mathematics,
physics, biology, music, painting, etc;
4When user A follows user B, A is called B?s follower, and
B is called A?s followee.
? IT Technology: Mobile phones, IT companies,
jobs opportunities, etc;
? Life: traveling (both touring or conference trip-
s), food, photography, etc.
4.2 Processing, Filtering, and Data Cleaning
On the crawled posts and responses, we first perform
a four-step filtering on the post and responses
? We first remove the Weibo posts and their re-
sponses if the length of post is less than 10 Chi-
nese characters or the length of the response is
less than 5 characters. The reason for that is
two-fold: 1) if the text is too short, it can bare-
ly contain information that can be reliably cap-
tured, e.g. the following example
P: Three down, two to go.
and 2) some of the posts or responses are too
general to be interesting for other cases, e.g. the
response in the example below,
P: Nice restaurant. I?d strong recommend it.
Everything here is good except the long
waiting line
R: wow.
? In the remained posts, we only keep the first
100 responses in the original (post, response)
pairs, since we observe that after the first 100
responses there will be a non-negligible propor-
tion of responses addressing things other than
the original Weibo post (e.g., the responses giv-
en earlier). We however will still keep the re-
sponses in the bank of responses.
? The last step is to filter out the potential adver-
tisements. We will find the long responses that
have been posted more than twice on different
posts and scrub them out of both original (post,
response) pairs and the response repository.
For the remained posts and responses, we remove
the punctuation marks and emoticons, and use ICT-
CLAS (Zhang et al, 2003) for Chinese word seg-
mentation.
939
Figure 4: Diagram of the process for creating the dataset.
4.3 Labeling
We employ a pooling strategy widely used in in-
formation retrieval for getting the instance to label
(Voorhees, 2002). More specifically, for a given
post, we use three baseline retrieval models to each
select 10 responses (see Section 5 for the descrip-
tion of the baselines), and merge them to form a
much reduced candidate set with size ? 30. Then
we label the reduced candidate set into ?suitable?
and ?unsuitable? categories. Basically we consider
a response suitable for a given post if we cannot tell
whether it is an original response. More specifically
the suitability of a response is judged based on the
following three criteria5:
Semantic Relevance: This requires the content of
the response to be semantically relevant to the post.
As shown in the example right below, the post P is
about soccer, and so is response R1 (hence seman-
tically relevant), whereas response R2 is about food
(hence semantically irrelevant).
P: There are always 8 English players in their
own penalty area. Unbelievable!
R1: Haha, it is still 0:0, no goal so far.
R2: The food in England is horrible.
Another important aspect of semantic relevance is
the entity association. This requires the entities in
the response to be correctly aligned with those in
the post. In other words, if the post is about entity
5Note that although our criteria in general favor short and
general answers like ?Well said!? or ?Nice?, most of these gen-
eral answers have already been filtered out due to their length
(see Section 4.2).
A, while the response is about entity B, they are very
likely to be mismatched. As shown in the following
example, where the original post is about Paris, and
the response R2 talks about London:
P: It is my last day in Paris. So hard to say
goodbye.
R1: Enjoy your time in Paris.
R2: Man, I wish I am in London right now.
This is however not absolute, since a response con-
taining a different entity could still be sound, as
demonstrated by the following two responses to the
post above
R1: Enjoy your time in France.
R2: The fall of London is nice too.
Logic Consistency: This requires the content of
the response to be logically consistent with the post.
For example, in the table right below, post P states
that the Huawei mobile phone ?Honor? is already in
the market of mainland China. Response R1 talk-
s about a personal preference over the same phone
model (hence logically consistent), whereas R2 asks
the question the answer to which is already clear
from P (hence logically inconsistent).
P: HUAWEI?s mobile phone, Honor, sells
well in Chinese Mainland.
R1: HUAWEI Honor is my favorite phone
R2: When will HUAWEI Honor get to the
market in mainland China?
Speech Act Alignment: Another important factor
in determining the suitability of a response is the
940
speech act. For example, when a question is posed in
the Weibo post, a certain act (e.g., answering or for-
warding it) is expected. In the example below, post
P asks a special question about location. Response
R1 and R2 either forwards or answers the question,
whereas R3 is a negative sentence and therefore does
not align well in speech act.
P: Any one knows where KDD will be held the
year after next?
R1: co-ask. Hopefully Europe
R2: New York, as I heard
R3: No, it is still in New York City
5 Retrieval-based Response Model
In a retrieval-based response model, for a given post
x we pick from the candidate set the response with
the highest ranking score, where the score is the en-
semble of several individual matching features
score(x, y) =
?
i??
wi?i(x, y). (1)
with y stands for a candidate response.
We perform a two-stage retrieval to handle the s-
calability associated with the massive candidate set,
as illustrated in Figure 5. In Stage I, the system em-
ploys several fast baseline matching models to re-
trieve a number of candidate responses for the giv-
en post x, forming a much reduced candidate set
C(reduced)x . In Stage II, the system uses a ranking
function with more and sophisticated features to fur-
ther evaluate all the responses in C(reduced)x , return-
ing a matching score for each response. Our re-
sponse model then decides whether to respond and
which candidate response to choose.
In Stage II, we use the linear score function de-
fined in Equation 1 with 15 features, trained with
RankSVM (Joachims, 2002). The training and test-
ing are both performed on the 422 labeled posts,
with about 12,000 labeled (post, response) pairs. We
use a 5-fold cross validation with a fixed penalty pa-
rameter for slack variable. 6
5.1 Baseline Matching Models
We use the following matching models as the base-
line model for Stage I fast retrieval. Moreover, the
6The performance is fairly insensitive to the choice of the
penalty, so we only report the result with a typical choice of it.
matching features used in the ranking function in
Stage II are generated, directly or indirectly, from
the those matching models:
POST-RESPONSE SEMANTIC MATCHING:
This particular matching function relies on a learned
mapping from the original sparse representation for
text to a low-dimensional but dense representation
for both Weibo posts and responses. The level of
matching score between a post and a response can
be measured as the inner product between their
images in the low-dimensional space
SemMatch(x, y) = x>LXL>Yy. (2)
where x and y are respectively the 1-in-N represen-
tations of x and y. This is to capture the seman-
tic matching between a Weibo post and a response,
which may not be well captured by a word-by-word
matching. More specifically, we find LX and LY
through a large margin variant of (Wu et al, 2013)
arg minLX ,LY
?
i
max(1?
?
i
x>i LXL
>
Yyi, 0)
s.t. ?Ln,X ?1 ? ?1, n = 1, 2, ? ? ? , Nx
?Lm,Y?1 ? ?1, m = 1, 2, ? ? ? , Ny
?Ln,X ?2 = ?2, n = 1, 2, ? ? ? , Nx
?Lm,Y?2 = ?2m = 1, 2, ? ? ? , Ny.
where i indices the original (post, response) pairs.
Our experiments (Section 6) indicate that this sim-
ple linear model can learn meaningful patterns, due
to the massive training set. For example, the im-
age of the word ?Italy? in the post in the latent s-
pace matches well word ?Sicily?, ?Mediterranean
sea? and ?travel?. Once the mapping LX and LY
are learned, the semantic matching score x>LXL>Yy
will be treated as a feature for modeling the overall
suitability of y as a response to post x.
POST-RESPONSE SIMILARITY: Here we use a
simple vector-space model for measuring the simi-
larity between a post and a response
simPR(x,y) =
x>y
?x??y?
. (3)
Although it is not necessarily true that a good re-
sponse has many common words as the post, but this
measurement is often helpful in finding relevant re-
sponses. For example, when the post and response
941
Figure 5: Diagram of the retrieval-based automatic response system.
both have ?National Palace Museum in Taipei?, it
is a strong signal that they are about similar topic-
s. Unlike the semantic matching feature, this simple
similarity requires no learning and works on infre-
quent words. Our empirical results show that it can
often capture the Post-Response relation failed with
semantic matching feature.
POST-POST SIMILARITY: The basic idea here is
to find posts similar to x and use their responses as
the candidates. Again we use the vector space model
for measuring the post-post similarity
simPP (x, x?) =
x>x?
?x??x??
. (4)
The intuition here is that if a post x? is similar to x its
responses might be appropriate for x. It however of-
ten fails, especially when a response to x? addresses
parts of x not contained by x, which fortunately can
be alleviated when combined with other measures.
5.2 Learning to Rank with Labeled Data
With all the matching features, we can learn a rank-
ing model with the labeled (post, response) pairs,
e.g., through off-the-shelf ranking algorithms. From
the labeled data, we can extract triples (x, y+, y?)
to ensure that score(x, y+) > score(x, y?). Appar-
ently y+ can be selected from labeled positive re-
sponse of x, while y? can be sampled either from
labeled negative negative or randomly selected ones.
Since the manually labeled negative instances are
top-ranked candidates according to some individual
retrieval model (see Section 5.1) and therefore gen-
erally yield slightly better results.
The matching features are mostly constructed by
combining the individual matching models, for ex-
ample the following two
? ?7(x, y): this feature measures the length of
the longest common string in the post and the
response;
? ?12(x, y): this feature considers both seman-
tic matching score between query post x and
candidate response y, as well as the similarity
between x and y?s original post x?:
?12(x, y) = SemMatch(x, y)simPP (x, x?).
In addition to the matching features, we also have
simple features describing responses only, such as
the length of it.
6 Experimental Evaluation
We perform experiments on the proposed dataset to
test our retrieval-based model as an algorithm for au-
tomatically generating response.
6.1 Performance of Models
We evaluate the retrieved models based on the fol-
lowing two metrics:
MAP This one measures the mean average preci-
sion (MAP)(Manning et al, 2008) associated
with the ranked list on C(reduced)x .
P@1 This one simply measures the precision of the
top one response in the ranked list:
P@1 =
#good top-1 responses
#posts
942
We perform a 5-fold cross-validation on the 422 la-
beled posts, with the results reported in Table 1. As
it shows, the semantic matching helps slightly im-
prove the overall performance on P@1.
Model MAP P@1
P2R 0.565 0.489
P2R + P2P 0.621 0.567
P2R + MATCH 0.575 0.513
P2R + P2P + MATCH 0.621 0.574
Table 3: Comparison of different choices of features,
where P2R stands for the features based on post-response
similarity, P2P stands for the features based on post-post
similarity, and MATCH stands for the semantic match fea-
ture.
To mimic a more realistic scenario on automatic
response model on Sina Weibo, we allow the system
to choose which post to respond to. Here we simply
set the response algorithm to respond only when the
highest score of the candidate response passes a cer-
tain threshold. Our experiments show that when we
choose to respond only to 50% of the posts, the P@1
increases to 0.76, while if the system only respond
to 25% of the posts, P@1 keeps increasing to 81%.
6.2 Case Study
Although our preliminary retrieval model does not
consider more complicated syntax, it is still able to
capture some useful coupling structure between the
appropriate (post, response) pairs, as well as the sim-
ilar (post, post) pairs.
Figure 6: An actual instance (the original Chinese text
and its English translation) of response returned by our
retrieval-based system.
Case study shows that our retrieval is fairly ef-
fective at capturing the semantic relevance (Section
6.2.1), but relative weak on modeling the logic con-
sistency (Section 6.2.2). Also it is clear that the se-
mantic matching feature (described in Section 5.1)
helps find matched responses that do not share any
words with the post (Section 6.2.3).
6.2.1 On Semantic Relevance
The features employed in our retrieval model are
mostly vector-space based, which are fairly good at
capturing the semantic relevance, as illustrated by
Example 1 & 2.
EXAMPLE 1:
P: It is a small town on an Spanish with 500
population, and guess what, they even
have a casino!
R: If you travel to Spain, you need to spend
some time there.
EXAMPLE 2:
P: One quote from Benjamin Franklin: ?We
are all born ignorant, but one must
work hard to remain stupid.?
R: Benjamin Franklin is a wise man, and
one of the founding fathers of USA.
However our retrieval model also makes bad
choice, especially when either the query post or the
response is long, as shown in Example 3. Here the
response is picked up because 1) the correspondence
between the word ?IT? in the post and the word
?mobile phone? in the candidate, and 2) the Chinese
word for ?lay off? in the post and the word for ?out-
dated? in the response are the same.
EXAMPLE 3:
P: As to the laying-off, I haven?t heard anything
about it. ?Elimination of the least competent?
is kind-off conventional in IT, but the ratio is
actually quite small.
R: Please don?t speak that way, otherwise you can
get outdated. Mobile phones are very expensive
when they were just out, but now they are fairly
cheap. Look forward, or you will be outdated.
The entity association is only partially addressed
with features like post-response cosine similarity,
treating entity name just as a word, which is appar-
ently not enough for preventing the following type
943
of mistakes (see Example 4 & 5) when the post and
response match well on other parts
EXAMPLE 4:
P: Professor Wang will give a curse on natural
language processing, starting next semester.
R: Jealous.. I wish I can attend Prof. Li?s
course too some time in the future.
EXAMPLE 5:
P: The fine China from Exhibition at the National
Palace Museum in Taipei
R: This drawing looks so nice. National Palace
Museum in Taipei is full of national treasures
6.2.2 On Logic Consistency
Our current model does not explicitly maintain
the logic consistency between the response and the
post, since Logic consistency requires a deeper anal-
ysis of the text, and therefore hard to capture with
just a vector space model. Below are two examples
which are semantically relevant, and correct with re-
spect to speech act, but logically inappropriate.
EXAMPLE 1:
P: I checked. Wang Fengyi is not my great grand-
father, although they?ve done similar deeds
and both were called ?Wang the Well-doer?.
R: wow, Wang Fengyi is your great grand-father
EXAMPLE 2:
P: We are looking for summer interns. We provide
books and lunch. If you are in Wu Han and
interested, drop us an email. Sorry we don?t
take any students outside Wu Han.
R: Are you looking for summer intern?
6.2.3 The Effect of Semantic Matching
The experiments also show that we may find inter-
esting and appropriate responses that have no com-
mon words as the post, as shown in the example be-
low. Our bi-linear semantic matching model how-
ever performs relatively poorly on long posts, where
the topics of the sentence cannot be well captured
by the sum of the latent vectors associated with each
word.
P: Eight England players stand in the penalty
area.
R1: What a classic match
R2: Haha, it is still 0:0, no goal so far
7 Summary
In this paper we propose a retrieval-based response
model for short-text based conversation, to leverage
the massive instances collected from social media.
For research in similar directions, we create a dataset
based on the posts and comments from Sina Weibo.
Our preliminary experiments show that our retrieval-
based response model, when combined with a large
candidate set, can achieve fairly good performance.
This dataset will be valuable for both training and
testing automatic response models for short texts.
References
Peter F. Brown, Vincent J. Della Pietra, Stephen A. Della
Pietra, and Robert L. Mercer. 1993. The mathematic-
s of statistical machine translation: parameter estima-
tion. Comput. Linguist., 19(2).
Rollo Carpenter. 1997. Cleverbot.
Sina Jafarpour and Christopher J. C. Burges. 2010. Fil-
ter, rank, and transfer the knowledge: Learning to chat.
Thorsten Joachims. 2002. Optimizing search engines
using clickthrough data. In Proceedings of the eighth
ACM SIGKDD international conference on Knowl-
edge discovery and data mining, KDD ?02, pages 133?
142, New York, NY, USA. ACM.
Anton Leuski and David R. Traum. 2011. Npceditor:
Creating virtual human dialogue using information re-
trieval techniques. AI Magazine, 32(2):42?56.
Diane Litman, Satinder Singh, Michael Kearns, and Mar-
ilyn Walker. 2000. Njfun: a reinforcement learning
spoken dialogue system. In Proceedings of the 2000
ANLP/NAACL Workshop on Conversational systems -
Volume 3, ANLP/NAACL-ConvSyst ?00, pages 17?
20, Stroudsburg, PA, USA. Association for Computa-
tional Linguistics.
Christopher D. Manning, Prabhakar Raghavan, and Hin-
rich Schu?tze. 2008. Introduction to Information Re-
trieval. Cambridge University Press, New York, NY,
USA.
Teruhisa Misu, Kallirroi Georgila, Anton Leuski, and
David Traum. 2012. Reinforcement learning of
question-answering dialogue policies for virtual muse-
um guides. In Proceedings of the 13th Annual Meeting
944
of the Special Interest Group on Discourse and Dia-
logue, SIGDIAL ?12, pages 84?93.
Alan Ritter, Colin Cherry, and William B. Dolan. 2011.
Data-driven response generation in social media. In
Proceedings of the Conference on Empirical Meth-
ods in Natural Language Processing, EMNLP ?11,
pages 583?593, Stroudsburg, PA, USA. Association
for Computational Linguistics.
Jost Schatzmann, Karl Weilhammer, Matt Stuttle, and
Steve Young. 2006. A survey of statistical user sim-
ulation techniques for reinforcement-learning of dia-
logue management strategies. Knowl. Eng. Rev., pages
97?126.
Ellen M Voorhees. 2002. The philosophy of infor-
mation retrieval evaluation. In Evaluation of cross-
language information retrieval systems, pages 355?
370. Springer.
Jason D. Williams and Steve Young. 2007. Partially ob-
servable markov decision processes for spoken dialog
systems. Comput. Speech Lang., 21(2):393?422.
Wei Wu, Zhengdong Lu, and Hang Li. 2013. Learning
bilinear model for matching queries and documents.
Journal of Machine Learning Research (2013 to ap-
pear).
Hua-Ping Zhang, Hong-Kui Yu, De-Yi Xiong, and Qun
Liu. 2003. Hhmm-based chinese lexical analyzer ict-
clas. SIGHAN ?03.
945
CoNLL 2008: Proceedings of the 12th Conference on Computational Natural Language Learning, pages 263?267
Manchester, August 2008
Probabilistic Model for Syntactic and Semantic Dependency Parsing 
Enhong Chen 
Department of Computer 
Science, University of Sci-
ence and Technology of 
China, Hefei, China 
cheneh@ustc.edu.cn 
Liu Shi 
Department of Computer 
Science, University of Sci-
ence and Technology of 
China, Hefei, China 
shiliu@ustc.edu 
Dawei Hu 
Department of Computer 
Science, University of Sci-
ence and Technology of 
China, Hefei, China 
dwhu@mail.ustc.edu.cn
 
Abstract 
This paper proposes a novel method to 
analyze syntactic dependencies and label 
semantic dependencies around both the 
verbal predicates and the nouns. In this 
method, a probabilistic model is designed 
to obtain a global optimal result. More-
over, a predicate identification model and 
a disambiguation model are proposed to 
label predicates and their senses. The ex-
perimental results obtained on the wsj 
and brown test sets show that our system 
obtains 77% of labeled macro F1 score 
for the whole task, 84.47% of labeled at-
tachment score for syntactic dependency 
task, and 69.45% of labeled F1 score for 
semantic dependency task. 
1 Introduction 
There are two difficulties in the CoNLL 2008 
shared task. One is how to label semantic role on 
a dependency-based representation and how to 
label verbal predicates and nouns. The other one 
is how to combine the syntactic task with the 
semantic task together. 
On the basis of statistical analysis of labeling 
results, we optimize the traditional approaches of 
syntactic dependency parsing and semantic role 
labeling. Moreover, we design a predicate 
identification model and a disambiguation model, 
which will be described in section 2.3, for 
labeling predicates and their senses. In the 
disambiguation model, an exhaustion method is 
used to find the best sense which is 
corresponding to a frame of predicate. In order to 
obtain a global optimization result for every 
                                                 
? 2008. Licensed under the Creative Commons Attri-
bution-Noncommercial-Share Alike 3.0 Unported 
license (http://creativecommons.org/licenses/by-nc-
sa/3.0/). Some rights reserved. 
sentence, a probabilistic model is designed to 
combine all subtasks. 
The rest of this paper is organized as follows:   
our system is described in section 2; and section 
3 reports our results on development and test sets; 
at last we conclude the paper in section 4. 
2 A Probabilistic Model for Syntactic 
and Semantic Dependency Labeling 
Compared with previous tasks, this shared task is 
more complex. It aims to merge both syntactic 
and semantic dependencies under a unified 
representation. Obviously, it can be divided into 
two subtasks: syntactic dependency parsing and 
semantic dependency labeling. For the second 
subtask, predicates and their senses should be 
labeled before semantic arguments for predicates 
are labeled. Since many predicates have only one 
sense, it is inefficient to build a multi-label 
classifier to classify each predicate. When a 
classification approach is used, it is mandatory to 
consider multiple senses for those predicates 
with only one or two senses. To prevent 
assigning irrelevant senses to predicates, we do 
not adopt classification approach. Instead, two 
more subtasks, i.e., predicate identification and 
predicate sense labeling, are introduced in this 
paper. The predicate sense labeling and semantic 
dependency labeling are performed together with 
a disambiguation model. 
To ensure that we can get an optimal overall 
syntactic and semantic dependency results 
through integrating the above steps, a probability 
model is proposed. The probabilistic model is 
described in Equation (1), where the score P  
of a sentence labeling is the combined 
conditional probability of its all subtasks,  is 
the probability of syntactic dependency parsing, 
 is the probability of predicate 
identification,  is the probability of 
sent
synP
predP
)(iPsem
263
semantic dependency labeling for the ith-
predicate, and n is the number of predicates. 
?
=
=
n
1i
sempredsynsent )(** iPPPP            (1) 
For each sentence, its top-N candidates using 
syntactic dependency parsing are obtained. Then 
for each candidate, predicates and semantic ar-
guments are labeled. At last, the best one with 
the highest  is chosen as final labeling result. sentP
2.1 Syntactic Dependency Parsing 
There are several approaches for syntactic de-
pendency parsing, as demonstrated in the CoNLL 
2007 shared task. A commonly used LR algo-
rithm is applied to this task. Unlike the best-first 
probabilistic shift-reduce LR algorithm used by 
(Kenji and Jun, 2007), here a combined probabil-
ity of all parsing steps is used to evaluate parsing 
results, and the best one is obtained as the final 
result. The probability of syntactic dependency 
parsing is defined in Equation (2). 
?= j actsyn iPP )(
=i 1
act
                      (2)  
where  is the probability of every LR ac-
tion act at step i, and j is the number of all steps. 
)(iP
As the search space of LR parser is exponen-
tial growth with the word number, the maximum 
size of candidate states is limited to 50. 
The features that we use are similar to (Kenji 
and Jun, 2007). Hence we do not describe them 
in this paper. 
2.2 Predicate Identification 
In this subtask, a MaxEnt model is adopted for 
classification. The features we used are as follow: 
? Base info: FORM, LEMMA, POS (GPOS 
if available, or is PPOS), SPLIT_FORM, 
SPLIT_LEMMA, PPOSS. 
? Base syntactic dependency info:  
o Number of modifiers; 
o Number of modifiers of the previous word; 
o Number of modifiers of the next word; 
o PPOSS of left-most modifier; 
o Deprel of left-most modifier; 
o PPOSS of right-most modifier; 
o Deprel of right-most modifier. 
? Modifiers info 
o POS list of all modifiers: if GPOS is avail-
able, POS is GPOS. Otherwise it is PPOS. 
o DEPREL list of all modifiers; 
o SPLIT_LEMMA list of all modifiers; 
o PPOSS list of all modifiers. 
? Head?s base info 
? Head?s base syntactic dependency info 
? Head?s modifiers info 
? Deprel:  the syntactic dependency relation 
to head. 
? Word stem 
? Stem of right-most modifier 
? PPOSS of right-most modifier 
? Suffix: The suffix of the word. We use the 
last 3 characters as this feature. 
? Voice: Check if the word is a verb and is 
passive voice. 
? Previous word info: Check if the previous 
word is a predicate. 
? Pos path to ROOT: PPOSS list from 
word to ROOT through the syntactic de-
pendency path. 
? Deprel path to ROOT: DEPREL list from 
word to ROOT through the syntactic de-
pendency path. 
Through statistical analysis, we find that 
PPOSS of nearly all predicates are in a particular 
category which contains NN, NNP, NNS, VB, 
VBD, VBG, VBN, VBP, VBZ, and JJ. Hence we 
ignore the words without these PPOSS to reduce 
the number of samples and speed up the process 
of training and recognition. Meanwhile, we also 
ignore the words having no relational frame in 
PropBank or NomBank. 
2.3 Predicate Sense Labeling 
In this subtask, we label the sense of each predi-
cate. Different predicates are usually unrelated 
even if they have the same sense number, which 
makes us hardly use a classifier to label them. 
Hence, we design a disambiguation model to 
solve this problem.  
Firstly, for each word which has been identi-
fied to be a predicate, we find out all of its prob-
able sense forms (corresponding to the field of 
?PRED?). According to statistical analysis, only 
about 0.05% PREDs are not described in 
PropBank frames or NomBank frames. So it is 
reasonable to assume that all PREDs could be 
found in PropBank or NomBank. Moreover, we 
find that about 96% PREDs are formed as 
?SPLIT_LEMMA + .sense? or ?LEMMA 
+ .sense?. As a result, when a word is identified 
to be a predicate, we use its LEMMA and 
SPLIT_LEMMA to find all possible PREDs 
from PropBank and NomBank. Furthermore, if 
some special words are unsuitable for these two 
forms, we should convert them into their original 
forms first and then find their possible PREDs. 
264
For the rest anomalistic words, we build a map-
ping dictionary from training data. 
Secondly, for each possible sense form, we la-
bel semantic argument for all words. If a word is 
not a semantic argument, it would be labeled as 
?_?. The score of the current possible sense form 
is calculated as the combination of all probability 
of each labeling. More details about semantic 
dependency labeling will be described in section 
2.4. 
Thirdly, we choose the sense form and its se-
mantic arguments with the highest score. The 
above steps will be repeated until all predicates 
have definite senses. 
2.4 Semantic Dependency Labeling 
Unlike CoNLL-2005 shared task, this shared 
task performing Semantic Role Labeling on a 
dependency-based representation (DSRL). It is a 
novel way for SRL and the traditional SRL 
methods can not directly be used here. 
Constituent-based SRL model needs to find out 
all probable constituents, while DSRL only 
considers the semantic dependency between 
word and predicate. Moreover, DSRL uses 
syntactic dependency parsing tree instead of 
traditional full syntactic parsing tree. As a result, 
the traditional features need to be amended 
accordingly. The features we used are as follows: 
? Deprel 
? Word stem 
? POS: if GPOS is available, POS is GPOS. 
Otherwise it is PPOS. 
? Stem of right-most modifier 
? PPOSS of right-most modifier 
? Predicate: the FORM of predicate. 
? PPOSS of predicate 
? Suffix of predicate 
? Voice: voice of predicate 
? Position: The position of the word with re-
spect to its predicate. It has three values, 
?before?, ?is? and ?after?, for the predicate.  
? Deprel path to predicate: DEPREL list 
from word to its predicate through the syn-
tactic dependency path. 
? Length of syntactic dependency path to 
predicate 
? Sense: the sense of predicate 
Moreover, we try to find more features with 
frames. Since the PropBank and NomBank are 
available and all predicates with senses are avail-
able for this subtask. Statistical analysis shows 
that nearly all core semantic arguments (AA, A0, 
A1, A2 ?) of a predicate are described in the 
frame of predicate. But it is incorrect contrarily. 
Based on these observations, we design features 
the following features for five frequently used 
core arguments: 
? A0 is in predicate?s frame: Have two 
values: ?YES? and ?NO?. 
? A1 is in predicate?s frame 
? A2 is in predicate?s frame 
? A3 is in predicate?s frame 
? A4 is in predicate?s frame 
Because the other core semantic arguments are 
rare, we do not need to design features for them. 
With this method, the labeling efficiency is im-
proved while the precision almost keeps un-
changed. 
As the frame information has been used in fea-
tures, we do not add any valency check on the 
labeling result. 
3 Experiments and Analysis 
3.1 Data and Environment 
The data provided for this Closed Challenge of 
shared task is part of TreeBank and Brown cor-
pus. Training set covers sections 02-21 of Tree-
Bank. Development set covers section 24 of 
TreeBank. Wsj test set covers section 23 of 
TreeBank. Brown test set covers sections ck01, 
ck02, and ck03 of the Brown corpus. 
The maximum entropy classier (Berger et al 
1996) used is Le Zhang's Maximum Entropy 
Modeling Toolkit and the L-BFGS parameter 
estimation algorithm with gaussian prior smooth-
ing (Chen and Rosenfeld, 1999). The gaussian 
prior is set to 2 and the iteration count is set to 
500. All results we list here are post-evaluated 
because there are some small modifications. 
The experiments are performed on a PC with 
AMD Athlon? 64 x2 4400+ CPU and 2GB 
main memory running Microsoft Windows XP 
with sp2.  Our system is developed using C++. 
In our experimental analysis, the abbreviations 
used are listed as follows: 
? LAS1: Labeled attachment score 
? UAS: Unlabeled attachment score 
? LAS2: Label accuracy score 
? LP: Labeled precision 
? LR: Labeled recall 
? LF1: Labeled F1 
? UP: Unlabeled precision 
? UR: Unlabeled recall 
? UF1: Unlabeled F1 
265
3.2 Syntactic Dependency Parsing 
We trained two LR models for syntactic depend-
ency parsing. The first LR model uses MaxEnt 
classification to determine possible parser actions 
and their probabilities. The second LR model 
also uses MaxEnt classification, but parsing is 
performed backwards simply by reversing the 
sentence before parsing starts.  
For a sentence, each model can label top-N 
candidates and calculate the probability for every 
result. We join these two models by finding the 
candidate with the highest probability from all 
candidates as the final result for the sentence. 
Table 1 shows the results of each model and joint 
model. We can see that the two LR models ob-
tain similar results. The joint model can obtain 
better result and increase almost one percentage. 
The processing time of joint model is twice more 
than that of the two other models. 
 
 LR Model 
LR-back 
Model 
Joint 
Model
LAS1 83.05 83.38 84.43 
UAS 86.36 86.74 87.74 dev 
LAS2 89.15 89.63 90.08 
LAS1 84.84 84.06 85.48 
UAS 87.60 86.74 88.13 wsj 
LAS2 90.70 90.47 91.21 
LAS1 77.29 76.95 78.91 
UAS 82.75 82.61 84.38 brown 
LAS2 85.00 84.82 85.76 
LAS1 84.00 83.27 84.75 
UAS 87.06 86.28 87.71 
wsj + 
brown LAS2 90.07 89.84 90.6 
Speed (sec/sent) 0.49 0.42 0.92 
Table 1:  Syntactic dependency parsing results 
3.3 Predicate Identification 
Our predicate identification approach is de-
scribed in section 2.2. We use the gold HEAD 
and DEPREL fields to test our approach. The 
results are shown in Table 2. The labeling for 
each sentence spends about 14ms.  
 
 dev wsj brown 
Precision 93.56 93.61 87.51 
Recall 93.24 93.39 89.04 
F1 93.40 93.50 88.27 
Table 2:  Predicate identification results 
3.4 Semantic Dependency Labeling 
Semantic dependency labeling is the last sub-
task. Our DSRL model uses MaxEnt classifica-
tion to determine the semantic dependency be-
tween each word and its corresponding predicate. 
The gold HEAD and DEPREL and PRED fields 
is used to test the model. 
Statistical analysis shows that, for about 99% 
semantic argument labels, the length of syntactic 
dependency path from word to predicate is less 
than 7. So we ignore the words with the length of 
7 or more. 
The final results of semantic dependency la-
beling are shown in Table 3. The labeling for 
each sentence spends about 10ms. 
Brown set is an out-of-domain set and wsj set 
is an in-domain set. Usually, the results on wsj 
are much better than those on brown. But here 
we found that the unlabeled scores are nearly the 
same between wsj and brown. It shows that our 
model performs well at unlabeled labeling on 
out-of-domain set, and should be improved at 
labeled labeling. 
 
 dev wsj brown 
LP 80.50 82.47 77.29 
LR 70.73 73.58 67.16 
LF1 75.30 77.77 71.87 
UP 92.10 92.65 92.87 
UR 80.92 82.65 80.69 
UF1 86.15 87.36 86.35 
Table 3:  Semantic dependency labeling results 
3.5 Overall Result 
As described in section 2, we use a probabilistic 
model to integrate all subtasks. In the probabilis-
tic model, syntactic dependency parsing should 
parse top-N candidate results. We do the rest 
parsing for each candidate result and get N inte-
grated results. Then, for each integrated result, its 
 is calculated and the best one is chose as 
the final result. 
sentP
The DSRL results around verbal predicates 
and nouns on wsj set are shown in Table 4. It 
shows that verbal predicates are labeled much 
better than nouns. 
 
 Unlabeled Predicate
Labeled 
Predicate 
Labeled Semantic 
Arguments 
NN* 87.79 79.52 58.09 
VB* 96.85 80.25 73.77 
Table 4:  The F1 values of DSRL around verbal 
predicates and nouns on wsj  
 
Table 5 shows the overall results with differ-
ent N.  The results are improved when N changes 
from 1 to 2. However, there is nearly no im-
provement by increasing N from 2 to 3. So N is 
set to be 2 in our system. Meanwhile, the effect 
of this approach is not obvious. We find that 
266
there are nearly only one or two different points 
between the top-2 candidate dependency parsing 
results. This leads to that the DSRL results with 
these top-2 candidate results are almost the same. 
This is the probable reason that the approach is 
not much improved with the increase of N. In the 
future it would be necessary for us to consider 
the number of different points when finding the 
top-N dependency results. 
 
 N=1 N=2 N=3 
LP 78.58 78.93 79.01 
LR 75.58 75.52 75.33 
LF1 77.05 77.19 77.13 
UP 86.56 86.95 87.07 
UR 83.04 82.94 82.75 
dev 
UF1 84.76 84.90 84.85 
LP 79.41 79.76 79.96 
LR 76.67 76.59 76.49 
LF1 78.02 78.15 78.19 
UP 86.59 86.92 87.11 
UR 83.40 83.25 83.10 
wsj 
UF1 84.97 85.04 85.06 
LP 70.52 70.95 70.79 
LR 68 67.88 67.54 
LF1 69.24 69.38 69.13 
UP 81.87 82.39 82.28 
UR 78.65 78.47 78.14 
brown 
UF1 80.23 80.39 80.16 
LP 78.45 78.8 78.96 
LR 75.72 75.64 75.5 
LF1 77.06 77.18 77.19 
UP 86.08 86.43 86.59 
UR 82.89 82.73 82.56 
wsj + 
brown 
UF1 84.45 84.54 84.53 
Speed (sec/sent) 0.93 0.94 0.95 
Table 5: Overall macro scores (Wsem = 0.50) 
4 Conclusion 
We divide this shared task into four subtasks: 
syntactic dependency parsing, predicate identifi-
cation, predicate sense labeling and semantic 
dependency labeling. Then, we design a prob-
abilistic model to combine them. The purpose of 
our system is to find a global optimal result for 
every sentence. If a syntactic dependency parsing 
result has the highest probability but it is unrea-
sonable, it would be difficult to get a semantic 
parsing result with high probability again. Hence, 
a more reasonable result may be found with 
lower syntactic dependency parsing probability. 
In our system, we have not distinguished be-
tween nouns and verbal predicates. The experi-
mental results show that the results of verbal 
predicates are much better than those of nouns. 
In the future, it is necessary for us to deal with 
them separately. 
Acknowledgments 
This work was supported by National Natural 
Science Foundation of China (No.60573077, 
No.60775037), Specialized Research Fund for 
the Doctoral Program of Higher Education 
(No.2007105), and Program for New Century 
Excellent Talents in University (No.NCET-05-
0549). 
References 
Berger, S. A. Della Pietra, and V. J. Della Pietra. 1996. 
A maximum entropy approach to naturallanguage 
processing. Computational Linguistics, 22(1):39?
71. 
Che Wanxiang, Ting Liu, Sheng Li, Yuxuan Hu, and 
Huaijun Liu. 2005. Semantic role labeling system 
using maximum entropy classifier. In Proceedings 
of Computational Natural Language Learning 
(CoNLL-2005). 
Gildea Daniel and Daniel Jurafsky. 2002. Automatic 
labeling of semantic roles. Computational Linguis-
tics, 28(3):245?288. 
Duan Xiangyu, Zhao Jun  and Xu Bo. 2007. Probabil-
istic Parsing Action Models for Multi-Lingual De-
pendency Parsing. In Proceedings of the CoNLL 
Shared Task Session of EMNLP-CoNLL 2007. 
Hacioglu K. 2004. Semantic Role Labeling Using 
Dependency Trees. In Proceedings of COLING-
2004. 
Johansson R. and Nugues P. 2007. Extended Con-
stituent-to-dependency Conversion for English. In 
Proceedings of NODALIDA 2007. 
Sagae, Kenji  and  Tsujii, Jun'ichi. 2007. Dependency 
Parsing and Domain Adaptation with LR Models 
and Parser Ensembles. In Proceedings of the 
CoNLL Shared Task Session of EMNLP-CoNLL 
2007. 
Stanley F. Chen and Ronald Rosenfeld. 1999. A gaus-
sian prior for smoothing maximum entropy models. 
Technical Report CMU-CS-99-108. 
Surdeanu, Mihai, Richard Johansson, Adam Meyers, 
Llu?s M?rquez, and Joakim Nivre. 2008. The 
CoNLL-2008 Shared Task on Joint Parsing of Syn-
tactic and Semantic Dependencies. In Proceedings 
of the 12th Conference on Computational Natural 
Language Learning (CoNLL-2008).  
Tsai Tzong-Han, Chia-Wei Wu, Yu-Chun Lin, and 
Wen-Lian Hsu. 2005. Exploiting full parsing in-
formation to label semantic roles using an ensem-
ble of me and svm via integer linear programming. 
In Proceedings of Computational Natural Lan-
guage Learning (CoNLL-2005). 
267
Proceedings of the Fourteenth Conference on Computational Natural Language Learning: Shared Task, pages 151?156,
Uppsala, Sweden, 15-16 July 2010. c?2010 Association for Computational Linguistics
Hedge Classification with Syntactic Dependency Features based on an 
Ensemble Classifier 
Yi Zheng, Qifeng Dai, Qiming Luo, Enhong Chen 
Department of Computer Science,  
University of Science and Technology of China, Hefei, China. 
{xiaoe, dqf2008}@mail.ustc.edu.cn 
{luoq, cheneh}@ustc.edu.cn 
 
Abstract 
We present our CoNLL-2010 Shared Task 
system in the paper. The system operates in 
three steps: sequence labeling, syntactic de-
pendency parsing, and classification. We have 
participated in the Shared Task 1. Our experi-
mental results measured by the in-domain and 
cross-domain F-scores on the biological do-
main are 81.11% and 67.99%, and on the 
Wikipedia domain 55.48% and 55.41%. 
1 Introduction 
The goals of the Shared Task (Farkas et al, 2010) 
are: (1) learning to detect sentences containing 
uncertainty and (2) learning to resolve the in-
sentence scope of hedge cues. We have partici-
pated in the in-domain and cross-domain chal-
lenges of Task 1. Specifically, the aim of Task 1 
is to identify sentences in texts that contain unre-
liable or uncertain information, and it is formu-
lated as a binary classification problem. 
Similar to Morante et al (2009), we use the 
BIO-cue labels for all tokens in a sentence to 
predict whether a token is the first one of a hedge 
cue (B-cue), inside a hedge cue (I-cue), or out-
side of a hedge cue (O-cue). Thus we formulate 
the problem at the token level, and our task is to 
label tokens in every sentence with BIO-cue. Fi-
nally, sentences that contain at least one B-cue or 
I-cue are considered as uncertain.  
Our system operates in three steps: sequence 
labeling, syntactic dependency parsing, and clas-
sification. Sequence labeling is a preprocessing 
step for splitting sentence into tokens and obtain-
ing features of tokens. Then a syntactic depend-
ency parser is applied to obtain the dependency 
information of tokens. Finally, we employ an 
ensemble classifier based on combining CRF 
(conditional random field) and MaxEnt (maxi-
mum entropy) classifiers to label each token with 
the BIO-cue. 
Our experiments are conducted on two train-
ing data sets: one is the abstracts and full articles 
from BioScope (biomedical domain) corpus 
(Vincze et al, 2008)1, the other one is paragraphs 
from Wikipedia possibly containing weasel in-
formation. Both training data sets have been an-
notated manually for hedge/weasel cues. The 
annotation of weasel/hedge cues is carried out at 
the phrase level. Sentences containing at least 
one hedge/weasel cue are considered as uncertain, 
while sentences with no hedge/weasel cues are 
considered as factual. The results show that em-
ploying the ensemble classifier outperforms the 
single classifier system on the Wikipedia data set, 
and using the syntactic dependency information 
in the feature set outperform the system without 
syntactic dependency information on the biologi-
cal data set (in-domain). 
In related work, Szarvas (2008) extended the 
methodology of Medlock and Briscoe (2007), 
and presented a hedge detection method in bio-
medical texts with a weakly supervised selection 
of keywords. Ganter and Strube (2009) proposed 
an approach for automatic detection of sentences 
containing linguistic hedges using Wikipedia 
weasel tags and syntactic patterns. 
The remainder of this paper is organized as 
follows. Section 2 presents the technical details 
of our system. Section 3 presents experimental 
results and performance analysis. Section 4 pre-
sents our discussion of the experiments. Section 
5 concludes the paper and proposes future work. 
2 System Description 
This section describes the implementation of our 
system. 
2.1 Information Flow of Our System 
Common classification systems consist of two 
steps: feature set construction and classification. 
The feature set construction process of our sys-
                                                 
1
 http://www.inf.u-szeged.hu/rgai/bioscope 
151
tem consists of sequence labeling and syntactic 
dependency parsing. Figure 1 shows the main 
information flow of our system. 
 
Figure 1: The main information flow of our sys-
tem 
2.2 Sequence labeling 
The sequence labeling step consists of the fol-
lowing consecutive stages: (1) tokenizing, (2) 
chunking, (3) POS-tagging, (4) lemmatizing. 
Firstly, the PTBTokenizer2 is employed to split 
sentence into tokens. Then, tokens are labeled 
with BIO-tags by the OpenNLP 3  chunker. Fi-
nally, Stanford Parser4 is used to obtain the POS 
and lemma of tokens. 
2.3 Syntactic Dependency Parsing 
In the syntactic dependency parsing stage, we 
use the Stanford Parser again to obtain depend-
ency information of tokens. Based on the Stan-
ford typed dependencies manual (Marneffe and 
Manning 2008), we have decided to obtain the 
tree dependencies structure. During the process 
of parsing, we found that the parser may fail due 
                                                 
2
 a tokenizer from Stanford Parser. 
3
 http://www.opennlp.org/ 
4
 http://nlp.stanford.edu/software/lex-parser.shtml 
to either empty sentences or very long sentences. 
To deal with very long sentences, we decided to 
allocate more memory. To deal with empty sen-
tences, we decided to simply label them as cer-
tain ones because there are only a few empty 
sentences in the training and test data sets and we 
could ignore their influence. 
2.4 Features 
After sequence labeling and syntactic depend-
ency parsing, we obtain candidate features. In 
our system, all the features belong to the follow-
ing five categories: (1) token features, (2) de-
pendency features, (3) neighbor features, (4) data 
features, (5) bigram and trigram features. 
Token features of the current token are listed 
below: 
? token: the current token. 
? index: index of the current token in the sen-
tence 
? pos: POS of the current token. 
? lemma: lemma of the current token. 
? chunk: BIO-chunk tags of the current token. 
Dependency features of the current token are 
listed below: 
? parent_index: the index of the parent token 
of the current token. 
? parent_token: the parent token of the current 
token. 
? parent_lemma: the lemma of the parent token 
of the current token. 
? parent_pos: the POS of the parent token of 
the current token. 
? parent_relation: the dependency relation of 
the current token and its parent token. 
Neighbor features of the current token include 
token, lemma, pos, chunk tag of three tokens to 
the right and three to the left. 
Data features of current token are listed below: 
? type: indicating documentPart 5  type of the 
sentence which contains the current token, 
such as Text, SectionTitle and so on.  
? domain: distinguishing the Wikipedia and 
biological domain. 
? abstract_article: indicating document type of 
the sentence which contains the current token, 
abstract or article. 
                                                 
5
 documentPart, SectionTitle, Text and so on are tags 
in the training and test data sets. 
CRF  MaxEnt 
Start 
Merging 
End 
Tokenizing 
Chunking 
POS-tagging 
Lemmatizing 
Syntactic 
Dependency 
Parsing 
152
We empirically selected some bigram features 
and trigram features as listed below: 
? left_token_2+left_token_1 
? left_token_1+token 
? token+right_token_1 
? right_token_1+right_token_2 
? left_token_2+left_token_1+token 
? left_token_1+token+right_token_1 
? token+right_token_1+right_token_2 
These are the complete set of features for our 
system. If the value of a feature is empty, we set 
it to a default value. In the ensemble classifier, 
we have selected different features for each indi-
vidual classifier. Details of this are described in 
the next subsection. 
2.5 Classification 
In our system, we have combined CRF++6 and 
OpenNLP MaxEnt7 classifiers into an ensemble 
classifier. The set of features for each classifier 
are shown in the column named ?system? of Ta-
ble 6. And the two classifiers are used in training 
and prediction separately, based on their individ-
ual set of features. Then we merge the results in 
this way: for each token, if the two predictions 
for it are both O-cue, then we label the token 
with an O-cue; otherwise, we label the token 
with a B-cue (one of the predictions is B-cue) or 
an I-cue (no B-cue in the predictions). The moti-
vation of the ensemble classifier approach is 
based on the observation of our internal experi-
ments using 10-fold cross validation, which we 
describe in Section 3. In addition, the parameters 
of OpenNLP MaxEnt classifier are all set to de-
fault values (number of iterations is 100, cutoff is 
0 and without smoothing). For CRF++, we only 
set the option ?-f? as 3 and the option ?-c? as 1.5, 
and the others are set to default values. 
3 Experimental Results 
We have participated in four subtasks, biological 
in-domain challenge (Bio-in-domain), biological 
cross-domain challenge (Bio-cross-domain), 
Wikipedia in-domain challenge (Wiki-in-
domain) and Wikipedia cross-domain challenge 
(Wiki-cross-domain). In all the experiments, TP, 
FP, FN and F-Score for the uncertainty class are 
used as the performance measures. We have 
                                                 
6
 http://crfpp.sourceforge.net/ 
7
 http://maxent.sourceforge.net/ 
tested our system with the test data set and ob-
tained official results as shown in Table 1. In 
addition, we have performed several internal ex-
periments on the training data set and several 
experiments on the test data set, which we de-
scribe in the next two subsections. The feature 
sets used for each subtask in our system are 
shown in Table 6, where each column denotes a 
feature set named after the title of the column 
(?System?, ?dep?, ?).  Actually, for different 
subtasks, we make use of the same feature set 
named ?system?. 
 
SubTask TP FP FN F-Score 
Bio-in-domain 717 261 73 81.11 
Bio-cross-domain 566 309 224 67.99 
Wiki-in-domain 974 303 1260 55.48 
Wiki-cross-domain 991 352 1243 55.41 
 
Table 1: Official results of our system. 
3.1 Internal Experiments 
Initially we only used a single classifier instead 
of an ensemble classifier. We performed 10-fold 
cross validation experiments on the training data 
set at the sentence level with different feature 
sets. The results of these experiments are shown 
in Table 2 and Table 3. 
In internal experiments, we mainly focus on 
the results of different models and different fea-
ture sets. In Table 2 and Table 3, CRF and ME 
(MaxEnt) indicate the two classifiers; ENSMB 
stands for the ensemble classifier obtained by 
combining CRF and MaxEnt classifiers; the three 
words ?dep?, ?neighbor? and ?together? indicate 
the feature sets for different experiments shown 
in Table 6, and ?together? is the union set of 
?dep? and ?neighbor?. 
The results of ME and CRF experiments (third 
column of Table 2 and Table 3) show that the 
individual classifier wrongly predicts many un-
certain sentences as certain ones. The number of 
such errors is much greater than the number of 
errors of predicting certain ones as uncertain. In 
other words, FN is greater than FP in our ex-
periments and the recall ratio is very low, espe-
cially for the Wikipedia data set. 
153
 Biological in-domain Biological cross-domain Experiment TP FP FN F-Score TP FP FN F-Score 
ME-dep 244 28 34 88.73 220 24 58 84.29 
CRF-dep 244 20 34 90.04 230 19 48 87.29 
ENSMB-dep 248 32 30 88.89 235 28 43 86.88 
ME-neighbor 229 14 49 87.91 211 12 67 84.23 
CRF-neighbor 244 16 34 90.71 228 21 50 86.53 
ENSMB-neighbor 247 22 31 90.31 241 26 37 88.44 
ME-together 234 11 44 89.48 205 12 73 82.83 
CRF-together 247 13 31 91.82 234 21 44 87.80 
ENSMB-together 253 17 25 92.36 242 26 36 88.64 
 
Table 2: Results of internal experiments on the biological training data set. 
 
Wikipedia in-domain Wikipedia cross-domain Experiment TP FP FN F-Score TP FP FN F-Score 
ME-dep 131 91 117 55.74 145 108 103 57.88 
CRF-dep 108 51 140 53.07 115 60 133 54.37 
ENSMB-dep 148 103 100 59.32 153 119 95 58.85 
ME-neighbor 106 52 142 52.22 130 77 118 57.14 
CRF-neighbor 123 44 125 59.28 123 72 125 55.53 
ENSMB-neighbor 145 71 103 62.50 154 116 94 59.46 
ME-together 100 57 148 49.38 117 69 131 53.92 
CRF-together 125 54 123 58.55 127 67 121 57.47 
ENSMB-together 141 83 107 59.75 146 104 102 58.63 
 
Table 3: Results of internal experiments on the Wikipedia training data set. 
 
Biological in-domain Biological cross-domain Experiment TP FP FN F-Score TP FP FN F-Score 
System-ME 650 159 140 81.30 518 265 272 65.86 
System-CRF 700 197 90 82.99 464 97 326 68.69 
System-ENSMB 717 261 73 81.11 566 309 224 67.99 
 
Table 4: Results of additional experiment of biological test data set. 
 
Wikipedia in-domain Wikipedia cross-domain Experiment TP FP FN F-Score TP FP FN F-Score 
System-ME 794 235 1440 48.67 798 284 1436 48.13 
System-CRF 721 112 1513 47.02 747 153 1487 47.67 
System-ENSMB 974 303 1260 55.48 991 352 1243 55.41 
 
Table 5: Results of additional experiment of Wikipedia test data set. 
 
Based on this analysis, we propose an ensem-
ble classifier approach to decrease FN in order to 
improve the recall ratio. The results of the en-
semble classifier show that: along with the de-
creasing of FN, FP and TP are both increasing. 
Although the recall ratio increases, the precision 
ratio decreases at the same time. Therefore, the 
ensemble classifier approach is a trade-off be-
tween precision and recall. For data sets with low 
recall ratio, such as Wikipedia, the ensemble 
classifier outperforms each single classifier in 
terms of F-score, just as the ME, CRF and 
ENSMB experiments show in Table 2 and Table 
3. 
In addition, we have performed simple feature 
selection in the internal experiments. The com-
parison of ?dep?, ?neighbor? and ?together? ex-
periments shown in Table 2 demonstrates that 
the dependency and neighbor features are both 
beneficial only for the biological in-domain ex-
periment. This may be because that sentences of 
the biological data are more regular than those of 
the Wikipedia data. 
3.2 Additional experiments on test data set 
We have also performed experiments on the test 
data set, and the results are shown in Table 4 and 
Table 5. With the same set of features of our sys-
154
tem as shown in Table 6, we have performed 
three experiments: System-ME (ME denotes 
MaxEnt classifier), System-CRF (CRF denotes 
CRF classifier) and System-ENSMB (ENSMB 
denotes ensemble classifier), where ?System? 
denotes the feature set in Table 6. The meanings 
of these words are similar to internal experiments. 
As Table 4 and Table 5 show, for the Wikipe-
dia test data set, the ensemble classifier outper-
forms each single classifier in terms of F-score 
by improving the recall ratio with a larger extent 
than the extent of the decreasing of the precision 
ratio. For the biological test data set, the ensem-
ble classifier outperforms System-ME but under-
performs System-CRF. This may be due to the 
relatively high values of the precision and recall 
ratios already obtained by each single classifier. 
4 Discussion 
The features in our experiments are selected em-
pirically, and the performance of our system 
could be improved with more elaborate feature 
selection. From the experimental results, we ob-
serve that there are still many uncertain sen-
tences predicted as certain ones. This indicates 
that the ability of learning uncertain information 
with the current classifiers and feature sets needs 
to be improved. We had the plan of exploring the 
ensemble classifier by combining CRF, MaxEnt 
and SVM (Support Vector Machine), but it was 
given up due to limited time. In addition, we 
were not able to complete experiments with 
MaxEnt classifier based on bigram and trigram 
features due to limited time. Actually only two 
labels I and O are needed for Task 1. We have 
not done the experiments with only I and O la-
bels, and we plan to do it in the future. 
According to our observation, the low F-score 
on the Wikipedia data set is due to many uncer-
tain phrases. By contrast, for the biological data 
set, the uncertain information consists of mostly 
single words rather than phrases. It is difficult for 
a classifier to learn uncertain information con-
sisting of 3 words or more. As we have observed, 
these uncertain phrases follow several patterns. 
A hybrid approach based on rule-based and sta-
tistical approaches to recognize them seems to be 
a promising. 
5 Conclusion and Future Work 
Our CoNLL-2010 Shared Task system operates 
in three steps: sequence labeling, syntactic de-
pendency parsing, and classification. The results 
show that employing the ensemble classifier out-
performs each single classifier for the Wikipedia 
data set, and using the syntactic dependency in-
formation in the feature set outperform the sys-
tem without syntactic dependency information 
for the biological data set (in-domain). Our final 
system achieves promising results. Due to lim-
ited time, we have only performed simple feature 
selection empirically. In the future, we plan to 
explore more elaborate feature selection and ex-
plore ensemble classifier by combining more 
classifiers. 
 
Acknowledgments 
The work was supported by the National Natural 
Science Foundation of China (No.60775037), the 
National High Technology Research and Devel-
opment Program of China (863 Program, 
No.2009 AA01Z123), and Specialized Research 
Fund for the Doctoral Program of Higher Educa-
tion (No.20093402110017). 
References 
Rich?rd Farkas, Veronika Vincze, Gy?rgy M?ra, 
J?nos Csirik, and Gy?rgy Szarvas. 2010. The 
CoNLL-2010 Shared Task: Learning to Detect 
Hedges and their Scope in Natural Language Text. 
In Proceedings of CoNLL-2010: Shared Task, 
pages 1?12. 
Viola Ganter, and Michael Strube. 2009. Finding 
hedges by chasing weasels: Hedge detection using 
Wikipedia tags and shallow linguistic features. In 
Proceedings of the ACL-IJCNLP 2009 Con-
ference Short Papers, pages 173?176. 
Marie-Catherine de Marneffe, and Christopher D. 
Manning. 2008. Stanford typed dependencies 
manual, September 2008. 
Ben Medlock, and Ted Briscoe. 2007. Weakly super-
vised learning for hedge classification in scientific 
literature. In Proc. of ACL 2007, pages 992?999. 
Roser Morante, and Walter Daelemans. 2009. Learn-
ing the scope of hedge cues in biomedical texts. In 
Proc. of the BioNLP 2009 Workshop, pages 
28?36. 
Gy?rgy Szarvas. 2008. Hedge classification in bio-
medical texts with a weakly supervised selection of 
keywords. In Proc. of ACL 2008, pages 281?289. 
Veronika Vincze, Gy?rgy Szarvas, Rich?rd Farkas, 
Gy?rgy M?ra, and J?nos Csirik. 2008. The Bio-
Scope corpus: biomedical texts annotated for un-
certainty, negation and their scopes. BMC Bioin-
formatics, 9(Suppl 11):S9. 
 
155
 Feature System Dep Neighbor Together 
token mc mc mc mc 
index m m m m 
pos mc mc mc mc 
lemma mc mc mc mc 
chunk 
 mc mc mc 
parent_index mc mc  mc 
parent_token 
 mc  mc 
parent_lemma mc mc  mc 
parent_relation mc mc  mc 
parent_pos mc mc  mc 
left_token_1 c  c c 
left_lemma_1 mc  mc mc 
left_pos_1 mc  mc mc 
left_chunk_1 
  mc mc 
left_token_2 c  c c 
left_lemma_2 c  mc mc 
left_pos_2 mc  mc mc 
left_chunk_2 
  mc mc 
left_token_3 
    
left_lemma_3 mc  m m 
left_pos_3 mc  m m 
left_chunk_3 
  m m 
right_token_1 c  c c 
right_lemma_1 mc  mc mc 
right _pos_1 mc  mc mc 
right _chunk_1 
  mc mc 
right_token_2 c  c c 
right _lemma_2 mc  mc mc 
right _pos_2 c  mc mc 
right _chunk_2 
  mc mc 
right_token_3 
    
right _lemma_3 c  m m 
right _pos_3 mc  m m 
right _chunk_3 
  m m 
type m mc mc mc 
domain m mc mc mc 
abstract_article m mc mc mc 
left_token_2+left_token_1 c  c c 
left_token_1+token c  c c 
token+right_token_1 c  c c 
right_token_1+right_token_2 c  c c 
left_token_2+left_token_1+token c  c c 
left_token_1+token+right_token_1 c  c c 
token+right_token_1+right_token_2 c  c c 
 
Table 6: Features selected for different experiments. The symbol m indicates MaxEnt classifier and c indicates 
CRF classifier.  
156
