Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, pages 1011?1020,
Honolulu, October 2008. c?2008 Association for Computational Linguistics
Graph-based Analysis of Semantic Drift in Espresso-like
Bootstrapping Algorithms
Mamoru Komachi
NAIST, Japan
mamoru-k@is.naist.jp
Taku Kudo
Google Inc.
taku@google.com
Masashi Shimbo
NAIST, Japan
shimbo@is.naist.jp
Yuji Matsumoto
NAIST, Japan
matsu@is.naist.jp
Abstract
Bootstrapping has a tendency, called seman-
tic drift, to select instances unrelated to the
seed instances as the iteration proceeds. We
demonstrate the semantic drift of bootstrap-
ping has the same root as the topic drift of
Kleinberg?s HITS, using a simplified graph-
based reformulation of bootstrapping. We
confirm that two graph-based algorithms, the
von Neumann kernels and the regularized
Laplacian, can reduce semantic drift in the
task of word sense disambiguation (WSD)
on Senseval-3 English Lexical Sample Task.
Proposed algorithms achieve superior perfor-
mance to Espresso and previous graph-based
WSD methods, even though the proposed al-
gorithms have less parameters and are easy to
calibrate.
1 Introduction
In recent years machine learning techniques be-
come widely used in natural language processing
(NLP). These techniques offer various ways to ex-
ploit large corpora and are known to perform well
in many tasks. However, these techniques often re-
quire tagged corpora, which are not readily available
to many languages. So far, reducing the cost of hu-
man annotation is one of the important problems for
building NLP systems.
To mitigate the problem of hand-tagging re-
sources, semi(or minimally)-supervised and unsu-
pervised techniques have been actively studied.
Hearst (1992) first presented a bootstrapping method
which requires only a small amount of instances
(seed instances) to start with, but can easily mul-
tiply the number of tagged instances with mini-
mal human annotation cost, by iteratively apply-
ing the following phases: pattern induction, pattern
ranking/selection, and instance extraction. Boot-
strapping has been widely adopted in NLP applica-
tions such as word sense disambiguation (Yarowsky,
1995), named entity recognition (Collins and Singer,
1999) and relation extraction (Riloff and Jones,
1999; Pantel and Pennacchiotti, 2006).
However, it is known that bootstrapping often ac-
quires instances not related to seed instances. For
example, consider the task of collecting the names
of common tourist sites from web corpora. Given
words like ?Geneva? and ?Bali? as seed instances,
bootstrapping would eventually learn generic pat-
terns such as ?pictures? and ?photos,? which also
co-occur with many other unrelated instances. The
subsequent iterations would likely acquire frequent
words that co-occur with these generic patterns,
such as ?Britney Spears.? This phenomenon is
called semantic drift (Curran et al, 2007).
A straightforward approach to avoid semantic
drift is to terminate iterations before hitting generic
patterns, but the optimal number of iterations is task
dependent and is hard to come by. The recently pro-
posed Espresso (Pantel and Pennacchiotti, 2006) al-
gorithm incorporates sophisticated scoring functions
to cope with generic patterns, but as Komachi and
Suzuki (2008) pointed out, Espresso still shows se-
mantic drift unless iterations are terminated appro-
priately.
Another deficiency in bootstrapping is its sensi-
tivity to many parameters such as the number of
1011
seed instances, the stopping criterion of iteration, the
number of instances and patterns selected on each it-
eration, and so forth. These parameters also need to
be calibrated for each task.
In this paper, we present a graph-theoretic anal-
ysis of Espresso-like bootstrapping algorithms. We
argue that semantic drift is inherent in these algo-
rithms, and propose to use two graph-based algo-
rithms that are theoretically less prone to semantic
drift, as an alternative to bootstrapping.
After a brief review of related work in Section 2,
we analyze in Section 3 a bootstrapping algorithm
(Simplified Espresso) which can be thought of as a
degenerate version of Espresso. Simplified Espresso
is simple enough to allow an algebraic treatment,
and its equivalence to Kleinberg?s HITS algorithm
(Kleinberg, 1999) is shown. An implication of this
equivalence is that semantic drift in this bootstrap-
ping algorithm is essentially the same phenomenon
as topic drift observed in link analysis. Another im-
plication is that semantic drift is inevitable in Sim-
plified Espresso as it converges to the same score
vector regardless of seed instances.
The original Espresso also suffers from the same
problem as its simplified version does. It incorpo-
rates heuristics not present in Simplified Espresso to
reduce semantic drift, but these heuristics have lim-
ited effect as we demonstrate in Section 3.3.
In Section 4, we propose two graph-based algo-
rithms to reduce semantic drift. These algorithms
are used in link analysis community to reduce the
effect of topic drift. In Section 5 we apply them to
the task of word sense disambiguation on Senseval-3
Lexical Sample Task and verify that they indeed re-
duce semantic drift. Finally, we conclude our work
in Section 6.
2 Related Work
2.1 Overview of Bootstrapping
Bootstrapping (or self-training) is a general frame-
work for reducing the requirement of manual an-
notation. Hearst (1992) described a bootstrapping
procedure for extracting words in hyponym (is-a)
relation, starting with three manually given lexico-
syntactic patterns.
The idea of learning with a bootstrapping method
was adopted for many tasks. Yarowsky (1995) pre-
sented an unsupervised WSD system which rivals
supervised techniques. Abney (2004) presented a
thorough discussion on the Yarowsky algorithm. He
extended the original Yarowsky algorithm to a new
family of bootstrapping algorithms that are mathe-
matically well understood.
Li and Li (2004) proposed a method called Bilin-
gual Bootstrapping. It makes use of a translation
dictionary and a comparable corpus to help disam-
biguate word senses in the source language, by ex-
ploiting the asymmetric many-to-many sense map-
ping relationship between words in two languages.
Curran et al (2007) presented an algorithm called
Mutual Exclusion Bootstrapping, which minimizes
semantic drift using mutual exclusion between se-
mantic classes of learned instances. They prepared
a list of so-called stop classes similar to a stop word
list used in information retrieval to help bound the
semantic classes. Stop classes are sets of terms
known to cause semantic drift in particular seman-
tic classes. However, stop classes vary from task to
task and domain to domain, and human intervention
is essential to create an effective list of stop classes.
A major drawback of bootstrapping is the lack
of principled method for selecting optimal param-
eter values (Ng and Cardie, 2003; Banko and Brill,
2001). Also, there is an issue of generic patterns
which deteriorates the quality of acquired instances.
Previously proposed bootstrapping algorithms differ
in how they deal with the problem of semantic drift.
We will take recently proposed Espresso algorithm
as the example to explain common configuration for
bootstrapping in detail.
2.2 The Espresso Algorithm
Pantel and Pennachiotti (2006) proposed a boot-
strapping algorithm called Espresso to learn binary
semantic relations such as is-a and part-of from
a corpus. What distinguishes Espresso from other
bootstrapping algorithms is that it benefits from
generic patterns by using a principled measure of
instance and pattern reliability. The key idea of
Espresso is recursive definition of pattern-instance
scoring metrics. The reliability scores of pattern p
and instance i, denoted respectively as rpi(p) and
1012
r?(i), are given as follows:
rpi(p) =
?
i?I
pmi(i,p)
max pmir?(i)
|I|
(1)
r?(i) =
?
p?P
pmi(i,p)
max pmirpi(p)
|P |
(2)
where
pmi(i, p) = log2
|i, p|
|i, ?||?, p|
(3)
is pointwise mutual information between i and p, P
and I are sets of patterns and instances, and |P | and
|I| are the numbers of patterns and instances, respec-
tively. |i, ?| and |?, p| are the frequencies of pattern
p and instance i in a given corpus, respectively, and
|i, p| is the frequency of pattern p which co-occurs
with instance i. max pmi is a maximum value of
the pointwise mutual information over all instances
and patterns. The intuition behind these definitions
is that a reliable pattern co-occurs with many reli-
able instances, and a reliable instance co-occurs with
many reliable patterns.
Espresso and other bootstrapping methods iterate
the following three phases: pattern induction, pat-
tern ranking/selection, and instance extraction.
We describe these phases below, along with the
parameters that controls each phase.
Phase 1. Pattern Induction Induce patterns from
a corpus given seed instances. Patterns may be sur-
face text patterns, lexico-syntactic patterns, and/or
just features.
Phase 2. Pattern Ranking/Selection Create a
pattern ranker from a corpus using instances as fea-
tures and select patterns which co-occur with seed
instances for the next instance extraction phase. The
main issue here is to avoid ranking generic patterns
high and to choose patterns with high relatedness to
the seed instances. Parameters and configurations:
(a) a pattern scoring metrics and (b) the number of
patterns to use for extraction of instances.
Phase 3. Instance Extraction Select high-
confidence instances to the seed instance set. It is
desirable to keep only high-confidence instances at
this phase, as they are used as seed instances for the
input:
seed vector i0
pattern-instance co-occurrence matrix M
output:
instance and pattern score vectors i and p
1: i = i0
2: loop
3: p ? M i
4: Normalize p
5: i ? MTp
6: Normalize i
7: if i and p have both converged then
8: return i and p
9: end if
10: end loop
Figure 1: A simple bootstrapping algorithm
next iteration. Optionally, instances can be cumula-
tively obtained on each iteration to retain highly rel-
evant instances learned in early iterations. Parame-
ters and configurations: (c) instance scoring metrics,
(d) whether to retain extracted instances on each it-
eration or not, and (e) the number of instances to
pass to the next iteration.
Bootstrapping iterates the above three phases sev-
eral times until stopping criteria are met. Acquired
instances tend to become noisy as the iteration pro-
ceeds, so it is important to terminate before semantic
drift occurs. Thus, we have another configuration:
(f) stopping criterion.
Espresso uses Equations (1) for (a) and (2) for (c)
respectively, whereas other parameters rely on the
tasks and need calibration. Even though Espresso
greatly improves recall while keeping high precision
by using these pattern and instance scoring metrics,
Komachi and Suzuki (2008) observed that extracted
instances matched against generic patterns may be-
come erroneous after tens of iterations, showing the
difficulty of applying bootstrapping methods to dif-
ferent domains.
3 Analysis of an Espresso-like
Bootstrapping Algorithm
3.1 Simplified Espresso
Let us consider a simple bootstrapping algorithm
illustrated in Figure 1, in order to elucidate the cause
1013
of semantic drift.
As before, let |I| and |P | be the numbers of
instances and patterns, respectively. The algo-
rithm takes a seed vector i0, and a pattern-instance
co-occurrence matrix M as input. i0 is a |I|-
dimensional vector with 1 at the position of seed in-
stances, and 0 elsewhere. M is a |P | ? |I|-matrix
whose (p, i)-element [M ]pi holds the (possibly re-
weighted) number of co-occurrence of pattern p and
instance i in the corpus. If both i and p have con-
verged, the algorithm returns the pair of i and p as
output.
This algorithm, though simple, can encode
Espresso?s update formulae (1) and (2) as Steps 3
through 6 if we pose
[M ]pi =
pmi(i, p)
max pmi
, (4)
and normalize p and i in Steps 4 and 6 by
p ? p/|I| and i ? i/|P |, (5)
respectively.
This specific instance of the algorithm of Fig-
ure 1, obtained by specialization through Equations
(4) and (5), will be henceforth referred to as Simpli-
fied Espresso. Indeed, it is an instance of the origi-
nal Espresso in which the iteration is not terminated
until convergence, all instances are carried over to
the next iteration, and instances are not cumulatively
learned.
3.2 Simplified Espresso as Link Analysis
Let n denote the number of times Steps 2?10 are
iterated. Plugging (4) and (5) into Steps 3?6, we
see that the score vector of instances after the nth
iteration is
in = Ani0 (6)
where
A = 1
|I||P |
MTM. (7)
Suppose matrix A is irreducible; i.e., the graph
induced by taking A as the adjacency matrix is con-
nected. If n is increased and in is normalized on
each iteration, in tends to the principal eigenvec-
tor of A. This implies that no matter what seed in-
stances are input, the algorithm will end up with the
same ranking of instances, if it is run until conver-
gence. Because A = MTM|I||P | , the principal eigen-
vector of A is identical to the authority vector of
HITS (Kleinberg, 1999) algorithm run on the graph
induced by M . 1 This similarity of Equations (1),
(2) and HITS is not discussed in (Pantel and Pen-
nacchiotti, 2006).
As a consequence of the above discussion, se-
mantic drift in simplified Espresso seems to be in-
evitable as the iteration proceeds, since the principal
eigenvector of A need not resemble seed vector i0.
A similar phenomenon is reported for HITS and is
known as topic drift, in which pages of the dominant
topic are ranked high regardless of the given query.
(Bharat and Henzinger, 1998)
Unlike HITS and Simplified Espresso, how-
ever, Espresso and other bootstrapping algo-
rithms (Yarowsky, 1995; Riloff and Jones, 1999),
incorporate heuristics so that only patterns and in-
stances with high confidence score are carried over
to the next iteration.
3.3 Convergence Process of Espresso
To investigate the effect of semantic drift on
Espresso with and without the heuristics of selecting
the most confident instances on each iteration (i.e.,
the original Espresso and Simplified Espresso of
Section 3.2), we apply them to the task of word sense
disambiguation of word ?bank? in the Senseval-3
Lexical Sample (S3LS) Task data.2 There are 394
instances of word ?bank? and their occurring con-
text in this dataset, and each of them is annotated
with its true sense. Of the ten senses of bank, the
most frequent is the bank as in ?bank of the river.?
We use the standard training-test split provided with
the data set.
We henceforth denote Espresso with the follow-
ing filtering strategy as Filtered Espresso to stress
the distinction from Simplified Espresso. For Fil-
tered Espresso, we cleared all but the 100 top-
scoring instances in the instance vector on each iter-
ation, and the number of non-zeroed instance scores
1As long as the relative magnitude of the components of vec-
tor in is preserved, the vector can be normalized in any way on
each iteration. Hence HITS and Simplified Espresso use differ-
ent normalization but both converge to the principal eigenvector
of A.
2http://www.senseval.org/senseval3/data.html
1014
grows by 100 on each iteration. On the other hand,
we cleared all but the 20 top-scoring patterns in the
pattern vector on each iteration, and the number of
non-zeroed pattern scores grows by 1 on each iter-
ation following (Pantel and Pennacchiotti, 2006).3
The values of other parameters (b), (d), (e) and (f)
remains the same as those for simplified Espresso in
Section 3.1.
The task of WSD is to correctly predict the senses
of test instances whose true sense is hidden from the
system, using training data and their true senses. To
predict the sense of a given instance i, we apply k-
nearest neighbor algorithm.
Given a test instance i, its sense is predicted with
the following procedure:
1. Compute the instance-pattern matrix M from
the entire set of instances. We defer the details
of this step to Section 5.2.
2. Run Simplified- and Filtered Espresso using
the given instance i as the only seed instance.
3. After the termination of the algorithm, select k
training instances with the highest scores in the
score vector i output by the algorithm.
4. Since the selected k instances are training
instances, their true senses are accessible.
Choose the majority sense s from these k in-
stances, and output s as the prediction for the
given instance i. When there is a tie, output the
sense of the instance with the highest score in
i. Note that only Step 4 uses sense information.
Figure 2 shows the convergence process of
Simplified- and Filtered Espresso. X-axis indicates
the number of bootstrapping iterations and Y-axis
indicates the recall, which in this case equals pre-
cision, as the coverage is 100% in all cases.
3We conducted preliminary experiment to find these param-
eters to maximize the performance of Filtered Espresso. (These
numbers are different from the original Espresso (Pantel and
Pennacchiotti, 2006).) The number of initial patterns is rel-
atively large because of a data sparseness problem in WSD,
unlike relation extraction and named entity recognition. Also,
WSD basically uses more features than relation extraction and
thus it is hard to determine the stopping criterion based on the
number and scores of patterns, as (Pantel and Pennacchiotti,
2006) does.
 0.4
 0.5
 0.6
 0.7
 0.8
 0.9
 1
 5  10  15  20  25  30
re
ca
ll o
f "b
ank
"
iteration
Simplified EspressoFiltered Espresso
most frequent sense (baseline)
Figure 2: Recall of Simplified- and Filtered Espresso
Simplified Espresso tends to select the most fre-
quent sense as the iteration proceeds, and after nine
iterations it selects the most frequent sense (?the
bank of the river?) regardless of the seed instances.
As expected from the discussion in Section 3.2,
generic patterns gradually got more weight and se-
mantic drift occurred in later iterations. Indeed, the
ranking of the instances after convergence was iden-
tical to the HITS authority ranking computed from
instance-pattern matrix M (i.e., the ranking induced
by the dominant eigenvector of MTM ).
On the other hand, Filtered Espresso suffers less
from semantic drift. The final recall achieved
was 0.773 after convergence on the 20th iteration,
outperforming the most-frequent sense baseline by
0.10. However, a closer look reveals that the filter-
ing heuristics is limited in effectiveness.
Figure 3 plots the learning curve of Filtered
Espresso on the set of test instances. We show re-
call ( |correct instances||total true instances| ) of each sense to see how
Filtered Espresso tends to select the most frequent
sense. If semantic drift takes place, the number
of instances predicted as the most frequent sense
should increase as the iteration proceeds, resulting
in increased recall on the most frequent sense and
decreased recall on other senses. Figure 3 exactly
exhibit this trend, meaning that Filtered Espresso is
not completely free from semantic drift. Figure 2
also shows that the recall of Filtered Espresso starts
to decay after the seventh iteration.
1015
 0.3
 0.4
 0.5
 0.6
 0.7
 0.8
 0.9
 1
 5  10  15  20  25  30
re
ca
ll
iteration
most frequent sense
other senses
Figure 3: Recall of Filtered Espresso on the instances
having ?bank of the river? and other senses
4 Two Graph-based Algorithms for
Exploiting Generic Patterns
We explore two graph-based methods which have
the advantage of Espresso to harness the property of
generic patterns by the mutual recursive definition
of instance and pattern scores. They also have less
parameters than bootstrapping, and are less prone to
semantic drift.
4.1 Von Neumann Kernel
Kandola et al (2002) proposed the von Neumann
kernels for measuring similarity of documents us-
ing words. If we apply the von Neumann kernels to
the pattern-instance co-occurrence matrix instead of
the document-word matrix, the relative importance
of an instance to seed instances can be estimated.
Let A = MTM be the instance similarity matrix
obtained from pattern-instance matrix M , and ? be
the principal eigenvalue of A. The von Neumann
kernel matrix K? with diffusion factor ? (0 ? ? <
??1) is defined as follows:
K? = A
?
?
n=0
?nAn = A(I ? ?A)?1. (8)
The similarity between two instances i, j is given by
the (i, j) element of K? . Hence, the i-th column
vector can be used as the score vector for seed in-
stance i.
Ito et al (2005) showed that the von Neumann
kernels represent a mixture of the co-citation re-
latedness and Kleinberg?s HITS importance. They
compute the weighted sum of all paths between two
nodes in the co-citation graph induced by A =
MTM . The (MTM)n term of smaller n corre-
sponds to the relatedness to the seed instances, and
the (MTM)n term of larger n corresponds to HITS
importance. The von Neumann kernels calculate the
weighted sum of (MTM)n from n = 1 to ?, and
therefore smaller diffusion factor ? results in rank-
ing by relatedness, and larger ? returns ranking by
HITS importance.
In NLP literature, Schu?tze (1998) introduced the
notion of first- and second-order co-occurrence.
First-order co-occurrence is a context which directly
co-occurs with a word, whereas second-order co-
occurrence is a context which occurs with the (con-
textual) words that co-occur with a word. Higher-
order co-occurrence information is less sparse and
more robust than lower-order co-occurrence, and
thus is useful for a proximity measure.
Given these definitions, we see that the (MTM)n
term of smaller n corresponds to lower-order co-
occurrence, which is accurate but sparse, and the
(MTM)n term of larger n corresponds to higher-
order co-occurrence, which is dense but possibly
giving too much weight on unrelated instances ex-
tracted by generic patterns.
As a result, it is expected that setting diffusion
factor ? to a small value prevents semantic drift and
also takes higher order pattern vectors into account.
We verify this claim in Section 5.3.
4.2 Regularized Laplacian Kernel
The von Neumann kernels can be regarded as a mix-
ture of relatedness and importance, and diffusion
factor ? controls the trade-off between relatedness
and importance. In practice, however, setting the
right parameter value becomes an issue. We solve
this problem by the regularized Laplacian (Smola
and Kondor, 2003; Chebotarev and Shamis, 1998),
which are stable across diffusion factors and can
safely benefit from generic patterns.
LetG be a weighted undirected graph whose adja-
cency (weight) matrix is a symmetric matrix A. The
(combinatorial) graph Laplacian L of a graph G is
defined as follows:
L = D ?A (9)
where D is a diagonal matrix, and the ith diagonal
1016
Table 1: Recall of predicted labels of bank
algorithm MFS others
Simplified Espresso 100.0 0.0
Filtered Espresso 100.0 30.2
Filtered Espresso (optimal stopping) 94.4 67.4
von Neumann kernels 92.1 65.1
regularized Laplacian 92.1 62.8
element [D]ii is given by
[D]ii =
?
j
[A]ij . (10)
Here, [A]ij stands for the (i, j) element of A. By re-
placing A with ?L in Equation (8) and deleting the
first A, we obtain a regularized Laplacian kernel 4.
R? =
?
?
n=0
?n(?L)n = (I + ?L)?1 (11)
Again, ?(? 0) is called the diffusion factor.
Both the regularized Laplacian and the von Neu-
mann kernels compute all the possible paths in a
graph, and consequently they can calculate influence
between nodes in a long distance in the graph. Also,
Equations (9) and (10) show that the negative Lapla-
cian ?L can be regarded as a modification to the
graph G with the weight of self-loops re-weighted
to negative values. In this modified graph, if an in-
stance co-occurs with a pattern which also co-occurs
with a large number of other instances, a self-loop
of a node in the instance similarity graph induced
by MTM will receive a higher negative weight.
In other words, instances co-occurring with generic
patterns will get less weight in the regularized Lapla-
cian than in the von Neumann kernels.
5 Experiments and Results
5.1 Experiment 1: Reducing Semantic Drift
We test the von Neumann kernels and the regular-
ized Laplacian on the same task as we used in Sec-
tion 3.3; i.e., word sense disambiguation of word
4It has been reported that normalization of A improves per-
formance in application (Johnson and Zhang, 2007), so we nor-
malize L by L = I ?D?
1
2AD?
1
2 .
?bank.? During the training phase, a pattern-instance
matrix M was constructed using the training and
testing data from Senseval-3 Lexical Sample (S3LS)
Task. The (i, j) element of M of both kernels is set
to pointwise mutual information of a pattern i and
an instance j, just the same as in Espresso. Recall is
used in evaluation.5 The diffusion parameter ? is set
to 10?5 and 10?2 for the von Neumann kernels and
the regularized Laplacian, respectively.
Table 1 illustrates how well the proposed meth-
ods reduce semantic drift, just the same as the ex-
periment of Figure 3 in Section 3.3. We evalu-
ate the recall on predicting the most frequent sense
(MFS) and the recall on predicting other less fre-
quent senses (others). For Filtered Espresso, two
results are shown: the result on the seventh iter-
ation, which maximizes the performance (Filtered
Espresso (optimal stopping)), and the one after con-
vergence. As in Section 3.3, if semantic drift oc-
curs, recall of prediction on the most frequent sense
increases while recall of prediction on other senses
declines. Even Filtered Espresso was affected by se-
mantic drift, which is again a consequence of the
inherent graphical nature of Espresso-like bootstrap-
ping algorithms. On the other hand, both proposed
methods succeeded to balance the most frequent
sense and other senses. Filtered Espresso at the op-
timal number of iterations achieved the best perfor-
mance. Nevertheless, the number of iterations has to
be estimated separately.
5.2 Experiment 2: WSD Benchmark Data
We conducted experiments on the task of word sense
disambiguation of S3LS data, this time not just on
the word ?bank? but on all target nouns in the data,
following (Agirre et al, 2006). We used two types
of patterns.
Unordered single words (bag-of-words) We
used all single words (unigrams) in the provided
context from S3LS data sets. Each word in the con-
text constructs one pattern. The pattern correspond-
ing to a word w is set to 1 if it appears in the con-
text of instance i. Words were lowercased and pre-
processed with the Porter Stemmer6.
5Again, recall equals precision in this case as the coverage
is 100% in all cases.
6http://tartarus.org/?martin/PorterStemmer/def.txt
1017
Table 2: Comparison of WSD algorithms
algorithm Recall
most frequent sense 54.5
HyperLex (Ve?ronis, 2004) 64.6
PageRank (Agirre et al, 2006) 64.5
Simplified Espresso 44.1
Filtered Espresso 46.9
Filtered Espresso (optimal stopping) 66.5
von Neumann kernels (? = 10?5) 67.2
regularized Laplacian (? = 10?2) 67.1
Local collocations A local collocation refers to
the ordered sequence of tokens in the local, narrow
context of the target word. We allowed a pattern to
have wildcard expressions like ?sale of * interest in
* *? for the target word interest. We set the window
size to ?3 by a preliminary experiment.
We report the results of Filtered Espresso both af-
ter convergence, and with its optimal number of iter-
ations to show the upper bound of its performance.
Table 2 compares proposed methods with
Espresso with various configurations. The proposed
methods outperform by a large margin the most fre-
quent sense baseline and both Simplified- and Fil-
tered Espresso. This means that the proposed meth-
ods effectively prevent semantic drift.
Also, Filtered Espresso without early stopping
shows more or less identical performance to Sim-
plified Espresso. It is implied that the heuristics of
filtering and early stopping is a crucial step not to
select generic patterns in Espresso, and the result is
consistent with the experiment of convergence pro-
cess of Espresso in Section 3.3.
Filtered Espresso halted after the seventh itera-
tion (Filtered Espresso (optimal stopping)) is com-
parable to the proposed methods. However, in boot-
strapping, not only the number of iterations but also
a large number of parameters must be adjusted for
each task and domain. This shortcoming makes it
hard to adapt bootstrapping in practical cases. One
of the main advantages of the proposed methods is
that they have only one parameter ? and are much
easier to tune.
It is suggested in Sections 3.3 and 4.1 that
Espresso and the von Neumann kernel with large ?
 40
 45
 50
 55
 60
 65
 70
 75
 1e-07  1e-06  1e-05  0.0001  0.001
re
ca
ll
diffusion factor  
von Neumann kernelSimplified Espresso
most frequent sense
Figure 4: Recall of the von Neumann kernels with a dif-
ferent diffusion factor ? on S3LS WSD task
converge to the principal eigenvector of A, though
the result does not seem to support this claim (both
Simplified- and Filtered Espresso are 10 points
lower than the most frequent sense baseline). The
reason seems to be because Espresso and the von
Neumann kernels use pointwise mutual information
as a weighting factor so that the principal eigenvec-
tor of A may not always represent the most frequent
sense.7
We also show the results of previous graph-based
methods (Agirre et al, 2006), based on Hyper-
Lex (Ve?ronis, 2004) and PageRank (Brin and Page,
1998). The experimental set-up is the same as ours
in that they do not use the sense tags of training cor-
pus to construct a co-occurrence graph, and they use
the sense tags of all the S3LS training corpus for
mapping senses to clusters. However, these meth-
ods have seven parameters to tune in order to achieve
the best performance, and hence are difficult to opti-
mize.
5.3 Experiment 3: Sensitivity to a Different
Diffusion Factor
Figure 4 shows the performance of the von Neu-
mann kernels with a diffusion factor ?. As ex-
pected, smaller ? leads to relatedness to seed in-
stances, and larger ? asymptotically converges to the
HITS authority ranking (or equivalently, Simplified
7A similar but more extreme case is described in (Ito et al,
2005) in which the use of a normalized weight matrixM results
in an unintuitive principal eigenvector.
1018
 40
 45
 50
 55
 60
 65
 70
 75
 0.001  0.01  0.1  1  10  100  1000
re
ca
ll
diffusion factor  
regularized Laplacian
most frequent sense
Figure 5: Recall of the regularized Laplacian with a dif-
ferent diffusion factor ? on S3LS WSD task
Espresso).
One of the disadvantages of the von Neumann
kernels over the regularized Laplacian is their sen-
sitivity to parameter ?. Figure 5 illustrates the per-
formance of the regularized Laplacian with a diffu-
sion factor ?. The regularized Laplacian is stable for
various values of ?, while the von Neumann kernels
change their behavior drastically depending on the
value of ?. However, ? in the von Neumann kernels
is upper-bounded by the reciprocal 1/? of the prin-
cipal eigenvalue of A, and the derivatives of kernel
matrices with respect to ? can be used to guide sys-
tematic calibration of ? (see (Ito et al, 2005) for
detail).
6 Conclusion and Future Work
This paper gives a graph-based analysis of seman-
tic drift in Espresso-like bootstrapping algorithms.
We indicate that semantic drift in bootstrapping is a
parallel to topic drift in HITS. We confirm that the
von Neumann kernels and the regularized Laplacian
reduce semantic drift in the Senseval-3 Lexical Sam-
ple task. Our proposed methods have only one pa-
rameters and are easy to calibrate.
Beside the regularized Laplacian, many other ker-
nels based on the eigenvalue regularization of the
Laplacian matrix have been proposed in machine
learning community (Kondor and Lafferty, 2002;
Nadler et al, 2006; Saerens et al, 2004). One such
kernel is the commute-time kernel (Saerens et al,
2004) defined as the pseudo-inverse of Laplacian.
Despite having no parameters at all, it has been re-
ported to perform well in many collaborative filter-
ing tasks (Fouss et al, 2007). We plan to test these
kernels in our task as well.
Another research topic is to investigate other
semi-supervised learning techniques such as co-
training (Blum and Mitchell, 1998). As we have
described in this paper, self-training can be thought
of a graph-based algorithm. It is also interesting to
analyze how co-training is related to the proposed
algorithm.
Bootstrapping algorithms have been used in many
NLP applications. Two major tasks of bootstrap-
ping are word sense disambiguation and named en-
tity recognition. In named entity recognition task,
instances are usually retained on each iteration and
added to seed instance set. This seems to be be-
cause named entity recognition suffers from seman-
tic drift more severely than word sense disambigua-
tion. Even though this problem setting is different
from ours, it needs to be verified that the graph-
based approaches presented in this paper are also ef-
fective in named entity recognition.
Acknowledgements
We thank anonymous reviewers for helpful com-
ments and for making us aware of Abney?s work.
The first author is partially supported by the Japan
Society for Promotion of Science (JSPS), Grant-in-
Aid for JSPS Fellows.
References
Steven Abney. 2004. Understanding the Yarowsky Al-
gorithm. Computational Linguistics, 30(3):365?395.
Eneko Agirre, David Mart??nez, Oier Lo?pez de Lacalle,
and Aitor Soroa. 2006. Two graph-based algorithms
for state-of-the-art WSD. In Proceedings of the 2006
Conference on Empirical Methods in Natural Lan-
guage Processing, pages 585?593.
Michele Banko and Eric Brill. 2001. Scaling to Very
Very Large Corpora for Natural Language Disam-
biguation. In Proceedings of the 39th Annual Meeting
on Association for Computational Linguistics, pages
26?33.
Krishna Bharat and Monika R. Henzinger. 1998. Im-
proved algorithms for topic distillation in a hyper-
linked environment. In Proceedings of the 21st ACM
SIGIR Conference.
1019
Avrim Blum and Tom Mitchell. 1998. Combining La-
beled and Unlabeled Data with Co-Training. In Pro-
ceedings of the Workshop on Computational Learning
Theory (COLT), pages 92?100. Morgan Kaufmann.
Sergey Brin and Lawrence Page. 1998. The anatomy of
a large-scale hypertextual Web search engine. Com-
puter Networks and ISDN Systems, 30(1?7):107?117.
Pavel Yu Chebotarev and Elena V. Shamis. 1998. On
proximity measures for graph vertices. Automation
and Remote Control, 59(10):1443?1459.
Michael Collins and Yoram Singer. 1999. Unsuper-
vised Models for Named Entity Classification. In Pro-
ceedings of the Joint SIGDAT Conference on Empiri-
cal Methods in Natural Language Processing and Very
Large Corpora, pages 100?110.
James R. Curran, Tara Murphy, and Bernhard Scholz.
2007. Minimising semantic drift with Mutual Exclu-
sion Bootstrapping. In Proceedings of the 10th Con-
ference of the Pacific Association for Computational
Linguistics, pages 172?180.
Franc?ois Fouss, Luh Yen, Pierr Dupont, and Marco
Saerens. 2007. Random-walk computation of simi-
larities between nodes of a graph with application to
collaborative recommendation. IEEE Transactions on
Knowledge and Data Engineering, 19(3):355?369.
Marti Hearst. 1992. Automatic Acquisition of Hy-
ponyms from Large Text Corpora. In Proceedings of
the Fourteenth International Conference on Computa-
tional Linguistics, pages 539?545.
Takahiko Ito, Masashi Shimbo, Taku Kudo, and Yuji
Matsumoto. 2005. Application of Kernels to
Link Analysis. In Proceedings of the Eleventh
ACM SIGKDD International Conference on Knowl-
edge Discovery and Data Mining, pages 586?592.
Rie Johnson and Tong Zhang. 2007. On the Effec-
tiveness of Laplacian Normalization for Graph Semi-
supervised Learning. Journal of Machine Learning
Research, 8:1489?1517.
Jaz Kandola, John Shawe-Taylor, and Nello Cristianini.
2002. Learning Semantic Similarity. In Advances
in Neural Information Processing Systems 15, pages
657?664.
Jon Kleinberg. 1999. Authoritative Sources in a Hyper-
linked Environment. Journal of the ACM, 46(5):604?
632.
Mamoru Komachi and Hisami Suzuki. 2008. Mini-
mally Supervised Learning of Semantic Knowledge
from Query Logs. In Proceedings of the 3rd Inter-
national Joint Conference on Natural Language Pro-
cessing, pages 358?365.
Risi Imre Kondor and John Lafferty. 2002. Diffusion
kernels on graphs and other discrete input spaces. In
Proceedings of the 19th International Conference on
Machine Learning (ICML-2002).
Hang Li and Cong Li. 2004. Word Translation Disam-
biguation Using Bilingual Bootstrapping. Computa-
tional Linguistics, 30(1):1?22.
Boaz Nadler, Stephane Lafon, Ronald Coifman, and
Ioannis Kevrekidis. 2006. Diffusion maps, spectral
clustering and eigenfunctions of fokker-planck opera-
tors. Advances in Neural Information Processing Sys-
tems 18, pages 955?962.
Vincent Ng and Claire Cardie. 2003. Weakly Su-
pervised Natural Language Learning Without Redun-
dant Views. In Proceedings of the HLT-NAACL 2003,
pages 94?101.
Patrick Pantel and Marco Pennacchiotti. 2006. Espresso:
Leveraging Generic Patterns for Automatically Har-
vesting Semantic Relations. In Proceedings of the 21st
International Conference on Computational Linguis-
tics and the 44th annual meeting of the ACL, pages
113?120.
Ellen Riloff and Rosie Jones. 1999. Learning Dic-
tionaries for Information Extraction by Multi-Level
Bootstrapping. In Proceedings of the Sixteenth Na-
tional Conference on Artificial Intellligence (AAAI-
99), pages 474?479.
Marco Saerens, Franc?ois Fouss, Luh Yen, and Pierre
Dupont. 2004. The principal component analysis
of a graph, and its relationship to spectral clustering.
In Proceedings of European Conference on Machine
Learning (ECML 2004), pages 371?383. Springer.
Heinrich Schu?tze. 1998. Automatic Word Sense Dis-
crimination. Computational Linguistics, 24(1):97?
123.
Alex J. Smola and Risi Imre Kondor. 2003. Kernels and
Regularization of Graphs. In Proceedings of the 16th
Annual Conference on Learning Theory, pages 144?
158.
Jean Ve?ronis. 2004. HyperLex: Lexical Cartography for
Information Retrieval. Computer Speech & Language,
18(3):223?252.
David Yarowsky. 1995. Unsupervised Word Sense Dis-
ambiguation Rivaling Supervised Methods. In Pro-
ceedings of the 33rd Annual Meeting of the Associa-
tion for Computational Linguistics, pages 189?196.
1020
Minimally Supervised Learning of Semantic Knowledge                 
from Query Logs 
                    Mamoru Komachi                                        Hisami Suzuki 
         Nara Institute of Science and Technology                      Microsoft Research 
   8916-5 Takayama                                      One Microsoft Way 
Ikoma, Nara 630-0192, Japan                          Redmond, WA 98052 USA   
 mamoru-k@is.naist.jp            hisamis@microsoft.com 
 
 
Abstract 
We propose a method for learning semantic 
categories of words with minimal supervi-
sion from web search query logs. Our me-
thod is based on the Espresso algorithm 
(Pantel and Pennacchiotti, 2006) for ex-
tracting binary lexical relations, but makes 
important modifications to handle query 
log data for the task of acquiring semantic 
categories. We present experimental results 
comparing our method with two state-of-
the-art minimally supervised lexical know-
ledge extraction systems using Japanese 
query log data, and show that our method 
achieves higher precision than the pre-
viously proposed methods. We also show 
that the proposed method offers an addi-
tional advantage for knowledge acquisition 
in an Asian language for which word seg-
mentation is an issue, as the method utiliz-
es no prior knowledge of word segmenta-
tion, and is able to harvest new terms with 
correct word segmentation.  
1 Introduction 
Extraction of lexical knowledge from a large col-
lection of text data with minimal supervision has 
become an active area of research in recent years. 
Automatic extraction of relations by exploiting 
recurring patterns in text was pioneered by Hearst 
(1992), who describes a bootstrapping procedure 
for extracting words in the hyponym (is-a) relation, 
starting with three manually given lexico-syntactic 
patterns. This idea of learning with a minimally 
supervised bootstrapping method using surface text 
patterns was subsequently adopted for many tasks, 
including relation extraction (e.g., Brin, 1998; Ri-
loff and Jones, 1999; Pantel and Pennacchiotti, 
2006) and named entity recognition (e.g., Collins 
and Singer, 1999; Etzioni et al, 2005).  
In this paper, we describe a method of learning 
semantic categories of words using a large collec-
tion of Japanese search query logs. Our method is 
based on the Espresso algorithm (Pantel and Pen-
nacchiotti, 2006) for extracting binary lexical rela-
tions, adapting it to work well on learning unary 
relations from query logs. The use of query data as 
a source of knowledge extraction offers some 
unique advantages over using regular text. 
? Web search queries capture the interest of search 
users directly, while the distribution of the Web 
documents do not necessarily reflect the distri-
bution of  what people search (Silverstein et al,  
1998). The word categories acquired from query 
logs are thus expected to be more useful for the 
tasks related to search.  
? Though user-generated queries are often very 
short, the words that appear in queries are gen-
erally highly relevant for the purpose of word 
classification.  
? Many search queries consist of keywords, which 
means that the queries include word segmenta-
tion specified by users. This is a great source of 
knowledge for learning word boundaries for 
those languages whose regularly written text 
does not indicate word boundaries, such as Chi-
nese and Japanese. 
Although our work naturally fits into the larger 
goal of building knowledge bases automatically 
from text, to our knowledge we are the first to ex-
plore the use of Japanese query logs for the pur-
pose of minimally supervised semantic category 
acquisition. Our work is similar to Sekine and Su-
zuki (2007), whose goal is to augment a manually 
created dictionary of named entities by finding 
358
contextual patterns from English query logs. Our 
work is different in that it does not require a full-
scale list of categorized named entities but a small 
number of seed words, and iterates over the data to 
extract more patterns and instances. Recent work 
by Pa?ca (2007) and Pa?ca and Van Durme (2007) 
also uses English query logs to extract lexical 
knowledge, but their focus is on learning attributes 
for named entities, a different focus from ours.  
2 Related Work 
In this section, we describe three state-of-the-art 
algorithms of relation extraction, which serve as 
the baseline for our work. They are briefly summa-
rized in Table 1. The goal of these algorithms is to 
learn target instances, which are the words belong-
ing to certain categories (e.g., cat for the Animal 
class), or in the case of relation extraction, the 
pairs of words standing in a particular relationship 
(e.g., pasta::food for is-a relationship), given the 
context patterns for the categories or relation types 
found in source data.  
2.1 Pattern Induction 
The first step toward the acquisition of instances is 
to extract context patterns. In previous work, these 
are surface text patterns, e.g., X such as Y, for ex-
tracting words in an is-a relation, with some heu-
ristics for finding the pattern boundaries in text. As 
we use query logs as the source of knowledge, we 
simply used everything but the instance string in a 
query as the pattern for the instance, in a manner 
similar to Pa?ca et al (2006). For example, the 
seed word JAL in the query ?JAL+flight_schedule? 
yields the pattern "#+flight_schedule".1 Note that 
we perform no word segmentation or boundary 
detection heuristics in identifying these patterns, 
which makes our approach fast and robust, as the 
                                                 
1 # indicates where the instance occurs in the query 
string, and + indicates a white space in the original Jap-
anese query. The underscore symbol (_) means there 
was originally no white space; it is used merely to make 
the translation in English more readable.  
2 The manual classification assigns only one category 
segmentation errors introduce noise in extracted 
patterns, especially when the source data contains 
many out of vocabulary items. 
The extracted context patterns must then be as-
signed a score reflecting their usefulness in extract-
ing the instances of a desired type. Frequency is a 
poor metric here, because frequent patterns may be 
extremely generic, appearing across multiple cate-
gories. Previously proposed methods differ in how 
to assign the desirability scores to the patterns they 
find and in using the score to extract instances, as 
well as in the treatment of generic patterns, whose 
precision is low but whose recall is high.   
2.2 Sekine and Suzuki (2007)?s Algorithm 
For the purpose of choosing the set of context pat-
terns that best characterizes the categories, Sekine 
and Suzuki (2007) report that none of the conven-
tional co-occurrence metrics such as tf.idf, mutual 
information and chi-squared tests achieved good 
results on their task, and propose a new measure, 
which is based on the number of different instances 
of the category a context c co-occurs with, 
lized by its token frequency for all categories: 
CcgfcScore type )(log)( ?
 
)1000()1000(
)()()(
ctopFctopfC
cFcfcg
insttype
insttype
?
?  
where ftype is the type frequency of instance terms 
that c co-occurs with in the category, Finst is the 
token frequency of context c in the entire data and 
ctop1000 is the 1000 most frequent contexts. Since 
they start with a large and reliable named entity 
dictionary, and can therefore use several hundred 
seed terms, they simply used the top-k highest-
scoring contexts and extracted new named entities 
once and for all, without iteration. Generic patterns 
receive low scores, and are therefore ignored by 
this algorithm.  
2.3 The Basilisk Algorithm 
Thelen and Riloff (2002) present a framework 
called Basilisk, which extracts semantic lexicons 
 # of seed Target # of iteration Corpus Language 
Sekine & Suzuki ~600 Categorized NEs 1 Query log English 
Basilisk 10 Semantic lexicon ? MUC-4 English 
Espresso ~10 Semantic relations ? TREC English 
Tchai 5 Categorized words ? Query log Japanese 
Table 1: Summary of algorithms 
359
for multiple categories. It starts with a small set of 
seed words and finds all patterns that match these 
seed words in the corpus. The bootstrapping 
process begins by selecting a subset of the patterns 
by the RlogF metric (Riloff, 1996): 
)log()(log i
i
ii FN
FpatternFR ??
 
where Fi is the number of category members ex-
tracted by patterni and Ni is the total number of 
instances extracted by patterni. It then identifies 
instances by these patterns and scores each in-
stance by the following formula: 
i
P
j
j
i P
F
wordAvgLog
i?
?
?
? 1
)1log(
)(  
where Pi is the number of patterns that extract 
wordi. They use the average logarithm to select 
instances to balance the recall and precision of ge-
neric patterns. They add five best instances to the 
lexicon according to this formula, and the boot-
strapping process starts again. Instances are cumu-
latively collected across iterations, while patterns 
are discarded at the end of each iteration.  
2.4 The Espresso Algorithm 
We will discuss the Espresso framework (Pantel 
and Pennacchiotti, 2006) in some detail because 
our method is based on it. It is a general-purpose, 
minimally supervised bootstrapping algorithm that 
takes as input a few seed instances and iteratively 
learns surface patterns to extract more instances. 
The key to Espresso lies in its use of generic pat-
terns: Pantel and Pennacchiotti (2006) assume that 
correct instances captured by a generic pattern will 
also be instantiated by some reliable patterns, 
which denote high precision and low recall pat-
terns.  
Espresso starts from a small set of seed in-
stances of a binary relation, finds a set of surface 
patterns P, selects the top-k patterns, extracts the 
highest scoring m instances, and repeats the 
process. Espresso ranks all patterns in P according 
to reliability r?, and retains the top-k patterns for 
instance extraction. The value of k is incremented 
by one after each iteration. 
 The reliability of a pattern p is based on the in-
tuition that a reliable pattern co-occurs with many 
reliable instances. They use pointwise mutual in-
formation (PMI) and define the reliability of a pat-
tern p as its average strength of association across 
each input instance i in the set of instances I, 
weighted by the reliability of each instance i: 
I
irpipmi
pr Ii pm i
?
? ?
?
?
?
???
?
?
?
)(max
),(
)(
?
?
 
where r?(i) is the reliability of the instance i  and 
maxpmi is the maximum PMI between all patterns 
and all instances. The PMI between instance i = 
{x,y} and pattern p  is estimated by: 
,**,,*,
,,log),( pyx
ypxpipmi ?
 
where ypx ,, is the frequency of pattern p instan-
tiated with terms x and y (recall that Espresso is 
targeted at extracting binary relations) and where 
the asterisk represents a wildcard. They multiplied 
pmi(i,p) with the discounting factor suggested in 
Pantel and Ravichandran (2004) to alleviate a bias 
towards infrequent events. 
The reliability of an instance is defined similar-
ly: a reliable instance is one that associates with as 
many reliable patterns as possible. 
 
P
prpipmi
ir Pp pm i
?
? ?
?
?
?
???
?
?
?
)(max
),(
)(
?
?
 
where r?(p) is the reliability of pattern p, and P is 
the set of surface patterns. Note that r?(i) and r?(p) 
are recursively defined: the computation of the pat-
tern and instance reliability alternates between per-
forming pattern reranking and instance extraction. 
Similarly to Basilisk, instances are cumulatively 
learned, but patterns are discarded at the end of 
each iteration.  
3 The Tchai Algorithm 
In this section, we describe the modifications we 
made to Espresso to derive our algorithm called 
Tchai.  
3.1 Filtering Ambiguous Instances and Pat-
terns 
As mentioned above, the treatment of high-recall, 
low-precision generic patterns (e.g., #+map, 
#+animation) present a challenge to minimally 
supervised learning algorithms due to their am-
guity. In the case of semantic category acquisition, 
the problem of ambiguity is exacerbated, because 
not only the acquired patterns, but also the in-
stances can be highly ambiguous. For example, 
360
once we learn an ambiguous instance such as Po-
kemon, it will start collecting patterns for multiple 
categories (e.g., Game, Animation and Movie), 
which is not desirable.  
In order to control the negative effect of the ge-
neric patterns, Espresso introduces a confidence 
metric, which is similar but separate from the re-
liability measure, and uses it to filter out the gener-
ic patterns falling below a confidence threshold. In 
our experiments, however, this metric did not pro-
duce a score that was substantially different from 
the reliability score. Therefore, we did not use a 
confidence metric, and instead opted for not 
ing ambiguous instances and patterns, where we 
define ambiguous instance as one that induces 
more than 1.5 times the number of patterns of 
viously accepted reliable instances, and ambiguous 
(or generic) pattern as one that extracts more than 
twice the number of instances of previously ac-
cepted reliable patterns. As we will see in Section 
4, this modification improves the precision of the 
extracted instances, especially in the early stages of 
iteration.   
3.2 Scaling Factor in Reliability Scores 
Another modification to the Espresso algorithm to 
reduce the power of generic patterns is to use local 
maxpmi instead of global maxpmi. Since PMI ranges 
[??, +?], the point of dividing pmi(i,p) by maxpmi 
in Espresso is to normalize the reliability to [0, 1]. 
However, using PMI directly to estimate the relia-
bility of a pattern when calculating the reliability 
of an instance may lead to unexpected results be-
cause the absolute value of PMI is highly variable 
across instances and patterns. We define the local 
maxpmi of the reliability of an instance to be the 
absolute value of the maximum PMI for a given 
instance, as opposed to taking the maximum for all 
instances in a given iteration. Local maxpmi of the 
reliability of a pattern is defined in the same way. 
As we show in the next section, this modification 
has a large impact on the effectiveness of our algo-
rithm. 
3.3 Performance Improvements 
Tchai, unlike Espresso, does not perform the 
pattern induction step between iterations; rather, it 
simply recomputes the reliability of the patterns 
induced at the beginning. Our assumption is that 
fairly reliable patterns will occur with at least one 
of the seed instances if they occur frequently 
enough in query logs. Since pattern induction is 
computationally expensive, this modification 
reduces the computation time by a factor of 400. 
4 Experiment 
In this section, we present an empirical comparison 
of Tchai with the systems described in Section 2. 
4.1 Experimental Setup 
Query logs: The data source for instance extrac-
tion is an anonymized collection of query logs 
submitted to Live Search from January to February 
2007, taking the top 1 million unique queries. Que-
ries with garbage characters are removed. Almost 
all queries are in Japanese, and are accompanied 
by their frequency within the logs. 
Target categories: Our task is to learn word cate-
gories that closely reflect the interest of web search 
users. We believe that a useful categorization of 
words is task-specific, therefore we did not start 
with any externally available ontology, but chose 
to start with a small number of seed words. For our 
task, we were given a list of 23 categories relevant 
for web search, with a manual classification of the 
10,000 most frequent search words in the log of 
December 2006 (which we henceforth refer to as 
the 10K list) into one of these categories. 2  For 
evaluation, we chose two of the categories, Travel 
and Financial Services: Travel is the largest cate-
gory containing 712 words of the 10K list (as all 
the location names are classified into this category), 
while Financial Services was the smallest, contain-
ing 240 words.   
Systems: We compared three different systems 
described in Section 2 that implement an iterative 
algorithm for lexical learning:  
                                                 
2 The manual classification assigns only one category 
per word, which is not optimal given how ambiguous 
the category memberships are. However, it is also very 
difficult to reliably perform a multi-class categorization 
by hand.  
Category Seeds (with English translation) 
Travel jal, ana, jr, ????(jalan), his 
Finance ?????(Mizuho Bank), ?????
? (SMBC), jcb, ? ? ? ? (Shinsei 
Bank), ????(Nomura Securities) 
 
Table 2: Seed instances for Travel and Financial Ser-
vices categories 
361
? Basilisk: The algorithm by (Thelen and Riloff, 
2002) described in Section 2.  
? Espresso: The algorithm by (Pantel and Pennac-
chiotti, 2006) described in Sections 2 and 3. 
? Tchai: The Tchai algorithm described in this 
paper. 
For each system, we gave the same seed instances. 
The seed instances are the 5 most frequent words 
belonging to these categories in the 10K list; they 
are given in Table 2. For the Travel category, ?jal? 
and ?ana? are airline companies, ?jr? stand for Ja-
pan Railways, ?jalan? is an online travel informa-
tion site, and ?his? is a travel agency. In the 
Finance category, three of them are banks, and the 
other two are a securities company and a credit 
card firm. Basilisk starts by extracting 20 patterns, 
and adds 100 instances per iteration. Espresso and 
Tchai start by extracting 5 patterns and add 200 
instances per iteration. Basilisk and Tchai iterated 
20 times, while Espresso iterated only 5 times due 
to computation time. 
4.2 Results 
4.2.1 Results of the Tchai algorithm 
Tables 3 and 4 are the results of the Tchai algo-
rithm compared to the manual classification. Table 
3 shows the results for the Travel category. The 
precision of Tchai is very high: out of the 297 
words classified into the Travel domain that were 
also in the 10K list, 280 (92.1%) were learned 
rectly. 3  It turned out that the 17 instances that 
                                                 
3 As the 10K list contained 712 words in the Travel cat-
egory, the recall against that list is fairly low (~40%). 
The primary reason for this is that all location names are 
classified as Travel in the 10K list, and 20 iterations are 
represent the precision error were due to the ambi-
guity of hand labeling, as in?????????? 
?Tokyo Disneyland?, which is a popular travel des-
tination, but is classified as Entertainment in the 
manual annotation. We were also able to correctly 
learn 251 words that were not in the 10K list ac-
cording to manual verification; we also harvested 
125 new words ?incorrectly? into the Travel do-
main, but these words include common nouns re-
lated to Travel, such as ?? ?fishing? and ????
?  ?rental car?. Results for the Finance domain 
show a similar trend, but fewer instances are ex-
tracted.  
Sample instances harvested by our algorithm 
are given in Table 5. It includes subclasses of tra-
vel-related terms, for some of which no seed words 
were given (such as Hotels and Attractions). We 
also note that segmentation errors are entirely ab-
sent from the collected terms, demonstrating that 
query logs are in fact excellently suited for acquir-
ing new words for languages with no explicit word 
segmentation in text.  
4.2.2 Comparison with Basilisk and Espresso 
Figures 1 and 2 show the precision results compar-
ing Tchai with Basilisk and Espresso for the Travel 
and Finance categories. Tchai outperforms Basilisk 
and Espresso for both categories: its precision is 
constantly higher for the Travel category, and it 
achieves excellent precision for the Finance cate-
gory, especially in early iterations. The differences 
in behavior between these two categories are due 
to the inherent size of these domains. For the 
                                                                             
not enough to enumerate all frequent location names. 
Another reason is that the 10K list consists of queries 
but our algorithm extracts instances ? this sometimes 
causes a mismatch, e.g.,Tchai extracts??? ?Ritz? but 
the 10K list contains ??????  ?Ritz Hotel?.  
 
 
 
10K list Not in 
10K list Travel Not Travel 
Travel 280 17 251 
Not Travel 0 7 125 
Table 3: Comparison with manual annotation: 
Travel category 
 10K list  Not in 
10K list Finance Not Finance 
Finance 41 30 30 
Not Finance 0 5 99 
Table 4: Comparison with manual annotation: 
Financial Services category 
 
Type Examples (with translation) 
Place ??? (Turkey), ????? (Las 
Vegas), ??? (Bali Island) 
Travel agency Jtb, ???  (www.tocoo.jp), ya-
hoo (Yahoo ! Travel), net cruiser 
Attraction ????????  (Disneyland), 
usj (Universal Studio Japan) 
Hotel ?????(Imperial Hotel), ??
?(Ritz Hotel) 
Transportation ????(Keihin Express), ???
?(Nara Kotsu Bus Lines) 
 
Table 5: Extracted Instances 
362
smaller Finance category, Basilisk and Espresso 
both suffered from the effect of generic patterns 
such as #?????? ?homepage? and #??? 
?card? in early iterations, whereas Tchai did not 
select these patterns.  
 
Figure 1: Basilisk, Espresso vs. Tchai: Travel 
 
Figure 2: Basilisk, Espresso vs. Tchai: Finance 
Comparing these algorithms in terms of recall 
is more difficult, as the complete set of words for 
each category is not known. However, we can es-
timate the relative recall given the recall of another 
system. Pantel and Ravichandran (2004) defined 
relative recall as: 
||
||
| BP
AP
C
C
CC
CC
R
RR
B
A
B
A
B
A
B
A
BA ?
?????
 
where RA|B is the relative recall of system A given 
system B, CA and CB are the number of correct in-
stances of each system, and C is the number of true 
correct instances. CA and CB can be calculated by 
using the precision, PA and PB, and the number of 
instances from each system. Using this formula, 
we estimated the relative recall of each system rel-
ative to Espresso. Tables 6 and 7 show that Tchai 
achieved the best results in both precision and rela-
tive recall in the Travel domain. In the Finance 
domain, Espresso received the highest relative 
call but the lowest precision. This is because Tchai 
uses a filtering method so as not to select generic 
patterns and instances. 
Table 8 shows the context patterns acquired by 
different systems after 4 iterations for the Travel 
domain.4 The patterns extracted by Basilisk are not 
entirely characteristic of the Travel category. For 
example, ?p#sonic? and ?google+#lytics? only 
match the seed word ?ana?, and are clearly irrele-
vant to the domain. Basilisk uses token count to 
estimate the score of a pattern, which may explain 
the extraction of these patterns. Both Basilisk and 
Espresso identify location names as context pat-
terns (e.g., #?? ?Tokyo?, #?? ?Kyushu?), which 
may be too generic to be characteristic of the do-
main. In contrast, Tchai finds context patterns that 
are highly characteristic, including terms related to 
transportation (#+????? ?discount plane tick-
et?, #?????  ?mileage?) and accommodation 
(#+??? ?hotel?).  
4.2.3 Contributions of Tchai components 
In this subsection, we examine the contribution of 
each modification to the Espresso algorithm we 
made in Tchai.  
Figure 3 illustrates the effect of each 
modification proposed for the Tchai algorithm in 
Section 3 on the Travel category. Each line in the 
graph corresponds to the Tchai algorithm with and 
without the modification described in Sections 3.1 
and 3.2. It shows that the modification to the 
maxpmi function (purple) contributes most signifi-
cantly to the improved accuracy of our system. The 
filtering of generic patterns (green) does not show 
                                                 
4 Note that Basilisk and Espresso use context patterns 
only for the sake of collecting instances, and are not 
interested in the patterns per se. However, they can be 
quite useful in characterizing the semantic categories 
they are acquired for, so we chose to compare them here.  
 # of inst. Precision Rel.recall 
Basilisk 651 63.4 1.26 
Espresso 500 65.6 1.00 
Tchai 680 80.6 1.67 
Table 6: Precision (%) and relative recall: Tra-
vel domain 
 # of inst. Precision Rel.recall 
Basilisk 278 27.3 0.70 
Espresso 704 15.2 1.00 
Tchai 223 35.0 0.73 
Table 7: Precision (%) and relative recall: Finan-
cial Services domain 
 
363
a large effect in the precision of the acquired in-
stances for this category, but produces steadily bet-
ter results than the system without it. 
Figure 4 compares the original Espresso algo-
rithm and the modified Espresso algorithm which 
performs the pattern induction step only at the be-
ginning of the bootstrapping process, as described 
in Section 3.3. Although there is no significant dif-
ference in precision between the two systems, this 
modification greatly improves the computation 
time and enables efficient extraction of instances. 
We believe that our choice of the seed instances to 
be the most frequent words in the category produc-
es sufficient patterns for extracting new instances. 
 
Figure 3: System precision w/o each modification 
 
Figure 4: Modification to the pattern induction step 
 
5 Conclusion 
We proposed a minimally supervised bootstrap-
ping algorithm called Tchai. The main contribution 
of the paper is to adapt the general-purpose Es-
presso algorithm to work well on the task of learn-
ing semantic categories of words from query logs. 
The proposed method not only has a superior per-
formance in the precision of the acquired words 
into semantic categories, but is faster and collects 
more meaningful context patterns for characteriz-
ing the categories than the unmodified Espresso 
algorithm. We have also shown that the proposed 
method requires no pre-segmentation of the source 
text for the purpose of knowledge acquisition.  
Acknowledgements 
This research was conducted during the first au-
thor?s internship at Microsoft Research. We would 
like to thank the colleagues at Microsoft Research, 
especially Dmitriy Belenko and Christian K?nig, 
for their help in conducting this research.  
References 
Sergey Brin. 1998. Extracting Patterns and Relations 
from the World Wide Web. WebDB Workshop at 6th 
International Conference on Extending Database 
Technology, EDBT '98. pp. 172-183. 
Michael Collins and Yoram Singer. 1999. Unsupervised 
Models for Named Entity Classification. Proceedings 
of the Joint SIGDAT Conference on Empirical Me-
thods in Natural Language Processing and Very 
Large Corpora. pp. 100-110. 
Oren Etzioni, Michael Cafarella, Dong Downey, Ana-
Maria Popescu, Tal Shaked, Stephen Soderland, Da-
niel S. Weld, and Alexander Yates. 2005. Unsuper-
vised Named-Entity Extraction from the Web: An 
Experimental Study. Artificial Intelligence. 165(1). 
pp. 91-134. 
Marti Hearst. 1992. Automatic Acquisition of Hypo-
nyms from Large Text Corpora. Proceedings of the 
System Sample Patterns (with English translation) 
Basilisk #???(east_japan), #???(west_japan), p#sonic, #???(timetable), #??(Kyushu),  #+???
??(mileage), #??(bus),  google+#lytics, #+??(fare),  #+??(domestic), #???(hotel) 
Espresso #??(bus), ??#(Japan), #???(hotel), #??(road), #??(inn), ??#(Fuji), #??(Tokyo), #?
?(fare), #??(Kyushu), #???(timetable), #+??(travel), #+???(Nagoya) 
Tchai #+???(hotel), #+???(tour), #+??(travel), #??(reserve), #+???(flight_ticket), #+???
??(discount_flight_titcket), #?????(mileage), ????+#(Haneda Airport) 
 
Table 8: Sample patterns acquired by three algorithms 
364
Fourteenth International Conference on Computa-
tional Linguistics. pp 539-545. 
Patrick Pantel and Marco Pennacchiotti. 2006. Espresso: 
Leveraging Generic Patterns for Automatically Har-
vesting Semantic Relations. Proceedings of the 21st 
International Conference on Computational Linguis-
tics and the 44th annual meeting of the ACL. pp. 113-
120.  
Patrick Pantel and Deepak Ravichandran. 2004. Auto-
matically Labeling Semantic Classes. Proceedings of 
Human Language Technology Conference of the 
North American Chapter of the Association for Com-
putational Linguistics (HLT/NAACL-04). pp. 321-
328. 
Marius Pa?ca. 2004. Acquisition of Categorized Named 
Entities for Web Search. Proceedings of the 13th 
ACM Conference on Information and Knowledge 
Management (CIKM-04). pp. 137-145. 
Marius Pa?ca. 2007. Organizing and Searching the 
World Wide Web of Fact ? Step Two: Harnessing the 
Wisdom of the Crowds. Proceedings of the 16th In-
ternational World Wide Web Conference (WWW-07). 
pp. 101-110. 
Marius Pa?ca and Benjamin Van Durme. 2007. What 
You Seek is What You Get: Extraction of Class 
Attributes from Query Logs. Proceedings of the 20th 
International Joint Conference on Artificial Intelli-
gence (IJCAI-07). pp. 2832-2837. 
Marius Pa?ca, Dekang Lin, Jeffrey Bigham, Andrei Lif-
chits and Alpa Jain. 2006. Organizing and Searching 
the World Wide Web of Facts ? Step One: the One-
Million Fact Extraction Challenge. Proceedings of 
the 21st National Conference on Artificial Intelli-
gence (AAAI-06). pp. 1400-1405. 
Ellen Riloff. 1996. Automatically Generating Extraction 
Patterns from Untagged Text. Proceedings of the 
Thirteenth National Conference on Artificial Intelli-
gence. pp. 1044-1049. 
Ellen Riloff and Rosie Jones. 1999. Learning Dictiona-
ries for Information Extraction by Multi-Level Boot-
strapping. Proceedings of the Sixteenth National 
Conference on Artificial Intellligence (AAAI-99). pp. 
474-479. 
Satoshi Sekine and Hisami Suzuki. 2007. Acquiring 
Ontological Knowledge from Query Logs. Proceed-
ings of the 16th international conference on World 
Wide Web. pp. 1223-1224. 
Craig Silverstein, Monika Henzinger, Hannes Marais, 
and Michael Moricz. 1998. Analysis of a Very Large 
AltaVista Query Log. Digital SRC Technical Note 
#1998-014. 
Michael Thelen and Ellen Riloff. 2002. A Bootstrapping 
Method for Learning Semantic Lexicons using Ex-
traction Pattern Contexts. Proceedings of Conference 
on Empirical Methods in Natural Language 
Processing. pp. 214-221. 
 
365
Proceedings of the ACL-IJCNLP 2009 Conference Short Papers, pages 189?192,
Suntec, Singapore, 4 August 2009. c?2009 ACL and AFNLP
Learning Semantic Categories from Clickthrough Logs
Mamoru Komachi
Nara Institute of Science and Technology (NAIST)
8916-5 Takayama, Ikoma, Nara 630-0192, Japan
mamoru-k@is.naist.jp
Shimpei Makimoto and Kei Uchiumi and Manabu Sassano
Yahoo Japan Corporation
Midtown Tower, 9-7-1 Akasaka, Minato-ku, Tokyo 107-6211, Japan
{smakimot,kuchiumi,msassano}@yahoo-corp.jp
Abstract
As the web grows larger, knowledge ac-
quisition from the web has gained in-
creasing attention. In this paper, we pro-
pose using web search clickthrough logs
to learn semantic categories. Experimen-
tal results show that the proposed method
greatly outperforms previous work using
only web search query logs.
1 Introduction
Compared to other text resources, search queries
more directly reflect search users? interests (Sil-
verstein et al, 1998). Web search logs are get-
ting a lot more attention lately as a source of in-
formation for applications such as targeted adver-
tisement and query suggestion.
However, it may not be appropriate to use
queries themselves because query strings are often
too heterogeneous or inspecific to characterize the
interests of the user population. Although it is not
clear that query logs are the best source of learning
semantic categories, all the previous studies using
web search logs rely on web search query logs.
Therefore, we propose to use web search
clickthrough logs to learn semantic categories.
Joachims (2002) developed a method that utilizes
clickthrough logs for training ranking of search
engines. A search clickthrough is a link which
search users click when they see the result of
their search. The intentions of two distinct search
queries are likely to be similar, if not identical,
when they have the same clickthrough. Search
clickthrough logs are thus potentially useful for
learnin semantic categories. Clickthrough logs
have the additional advantage that they are avail-
able in abundance and can be stored at very low
cost.1 Our proposed method employs search click-
1As for data availability, MSN Search query logs
(RFP 2006 dataset) were provided to WSCD09: Work-
through logs to improve semantic category acqui-
sition in both precision and recall.
We cast semantic category acquisition from
search logs as the task of learning labeled in-
stances from few labeled seeds. To our knowledge
this is the first study that exploits search click-
through logs for semantic category learning.2
2 Related Work
There are many techniques that have been devel-
oped to help elicit knowledge from query logs.
These algorithms use contextual patterns to extract
a category or a relation in order to learn a target in-
stance which belongs to the category (e.g. cat in
animal class) or a pair of words in specific relation
(e.g. headquarter to a company). In this work,
we focus on extracting named entities of the same
class to learn semantic categories.
Pas?ca and Durme (2007) were the first to dis-
cover the importance of search query logs in nat-
ural language processing applications. They fo-
cused on learning attributes of named entities, and
thus their objective is different from ours. An-
other line of new research is to combine various re-
sources such as web documents with search query
logs (Pas?ca and Durme, 2008; Talukdar et al,
2008). We differ from this work in that we use
search clickthrough logs rather than search query
logs.
Komachi and Suzuki (2008) proposed a boot-
strapping algorithm called Tchai, dedicated to the
task of semantic category acquisition from search
query logs. It achieves state-of-the-art perfor-
mance for this task, but it only uses web search
query logs.
shop on Web Search Click Data 2009 participants. http://
research.microsoft.com/en-US/um/people/nickcr/WSCD09/
2After the submission of this paper, we found that (Xu et
al., 2009) also applies search clickthrough logs to this task.
This work independently confirms the effectiveness of click-
through logs to this task using different sources.
189
Figure 1: Labels of seeds are propagated to unla-
beled nodes.
3 Quetchup3 Algorithm
In this section, we describe an algorithm for
learning semantic categories from search logs us-
ing label propagation. We name the algorithm
Quetchup.
3.1 Semi-supervised Learning by Laplacian
Label Propagation
Graph-based semi-supervised methods such as la-
bel propagation are known to achieve high perfor-
mance with only a few seeds and have the advan-
tage of scalability.
Figure 1 illustrates the process of label propa-
gation using a seed term ?singapore? to learn the
Travel domain.
This is a bipartite graph whose left-hand side
nodes are terms and right-hand side nodes are
patterns. The strength of lines indicates related-
ness between each node. The darker a node, the
more likely it belongs to the Travel domain. Start-
ing from ?singapore,? the pattern ?? airlines? 4 is
strongly related to ?singapore,? and thus the label
of ?singapore? will be propagated to the pattern.
On the other hand, the pattern ?? map? is a neu-
tral pattern which co-occurs with terms other than
the Travel domain such as ?google? and ?yahoo.?
Since the term ?china? shares two patterns, ?? air-
lines? and ?? map,? with ?singapore,? the label of
the seed term ?singapore? propagates to ?china.?
?China? will then be classified in the Travel do-
main. In this way, label propagation gradually
propagates the label of seed instances to neigh-
bouring nodes, and optimal labels are given as the
3Query Term Chunk Processor
4
? is the place into which a query fits.
Input:
Seed instance vector F (0)
Instance similarity matrix A
Output:
Instance score vector F (t)
1: Construct the normalized Laplacian matrix L = I ?
D
?1/2
AD
?1/2
2: Iterate F (t + 1) = ?(?L)F (t) + (1 ? ?)F (0) until
convergence
Figure 2: Laplacian label propagation algorithm
labels at which the label propagation process has
converged.
Figure 2 describes label propagation based on
the regularized Laplacian. Let a sample x
i
be x
i
?
X , F (0) be a score vector of x comprised of a
label set y
i
? Y , and F (t) be a score vector of
x after step t. Instance-instance similarity matrix
A is defined as A = W TW where W is a row-
normalized instance-pattern matrix. The (i, j)-th
element of W
ij
contains the normalized frequency
of co-occurrence of instance x
i
and pattern p
j
. D
is a diagonal degree matrix of N where the (i, i)th
element of D is given as D
ii
=
?
j
N
ij
.
This algorithm in Figure 2 is similar to (Zhou
et al, 2004) except for the method of construct-
ing A and the use of graph Laplacian. Zhou et al
proposed a heuristic to set A
ii
= 0 to avoid self-
reinforcement5 because Gaussian kernel was used
to create A. The Laplacian label propagation does
not need such a heuristic because the graph Lapla-
cian automatically reduces self-reinforcement by
assigning negative weights to self-loops.
In the task of learning one category, scores of la-
beled (seed) instances are set to 1 whereas scores
of unlabeled instances are set to 0. The output is
a score vector which holds relatedness to seed in-
stances in descending order. In the task of learning
two categories, scores of seed instances are set to
either 1 or ?1, respectively, and the final label of
instance x
i
will be determined by the sign of out-
put score vector y
i
.
Label propagation has a parameter ? ? (0, 1]
that controls how much the labels of seeds are em-
phasized. As ? approaches 0 it puts more weight
on labeled instances, while as ? increases it em-
ploys both labeled and unlabeled data.
There exists a closed-form solution for Lapla-
cian label propagation:
5Avoiding self-reinforcement is important because it
causes semantic drift, a phenomenon where frequent in-
stances and patterns unrelated to seed instances infect seman-
tic category acquisition as iteration proceeds.
190
Category Seed
Travel jal (Japan Airlines), ana (All Nippon
Airways), jr (Japan Railways),
(jalan: online travel guide site), his
(H.I.S.Co.,Ltd.: travel agency)
Finance (Mizuho Bank),
(Sumitomo Mitsui Banking Corporation),
jcb, (Shinsei Bank),
(Nomura Securities)
Table 1: Seed terms for each category
F
?
=
?
?
t=0
(?(?L))
t
F (0) = (I + ?L)
?1
F (0)
However, the matrix inversion leads to O(n3)
complexity, which is far from realistic in a real-
world configuration. Nonetheless, it can be ap-
proximated by fixing the number of steps for label
propagation.
4 Experiments with Web Search Logs
We will describe experimental result comparing
a previous method Tchai to the proposed method
Quetchup with clickthrough logs (Quetchup
click
)
and with query logs (Quetchup
query
).
4.1 Experimental Settings
Search logs We used Japanese search logs col-
lected in August 2008 from Yahoo! JAPAN Web
Search. We thresholded both search query and
clickthrough logs and retained the top 1 million
distinct queries. Search logs are accompanied by
their frequencies within the logs.
Construction of an instance-pattern matrix
We used clicked links as clickthrough patterns.
Links clicked less than 200 times were removed.
After that, links which had only one co-occurring
query were pruned. 6 On the other hand, we used
two term queries as contextual patterns. For in-
stance, if one has the term ?singapore? and the
query ?singapore airlines,? the contextual pattern
?? airlines? will be created. Query patterns appear-
ing less than 100 times were discarded.
The (i, j)-th element of a row-normalized
instance-pattern matrix W is given by
W
ij
=
|x
i
,p
j
|
?
k
|x
i
,p
k
|
.
Target categories We used two categories,
Travel and Finance, to compare proposed methods
with (Komachi and Suzuki, 2008).
6Pruning facilitates the computation time and reduces the
size of instance-pattern matrix drastically.
When a query was a variant of a term or con-
tains spelling mistakes, we estimated original form
and manually assigned a semantic category. We
allowed a query to have more than two categories.
When a query had more than two terms, we as-
signed a semantic category to the whole query tak-
ing each term into account.7
System We used the same seeds presented in Ta-
ble 1 for both Tchai and Quetchup. We used the
same parameter for Tchai described in (Komachi
and Suzuki, 2008), and collected 100 instances by
iterating 10 times and extracting 10 instances per
iteration. The number of iteration of Quetchup is
set to 10. The parameter ? is set to 0.0001.
Evaluation It is difficult in general to define re-
call for the task of semantic category acquisition
since the true set of instances is not known. Thus,
we evaluated all systems using precision at k and
relative recall (Pantel and Ravichandran, 2004).8
Relative recall is the coverage of a system given
another system as baseline.
4.2 Experimental Result
4.2.1 Effectiveness of Clickthrough Logs
Figures 3 to 6 plot precision and relative recall
for three systems to show effectiveness of search
clickthrough logs in improvement of precision and
relative recall. Relative recall of Quetchup
click
and
Tchai were calculated against Quetchup
query
.
Quetchup
click
gave the best precision among
three systems, and did not degenerate going down
through the list. In addition, it was demonstrated
that Quetchup
click
gives high recall. This result
shows that search clickthrough logs effectively im-
prove both precision and recall for the task of se-
mantic category acquisition.
On the other hand, Quetchup
query
degraded in
precision as its rank increased. Manual check of
the extracted queries revealed that the most promi-
nent queries were Pornographic queries, followed
by Food, Job and Housing, which frequently ap-
pear in web search logs. Other co-occurrence met-
rics such as pointwise mutual information would
be explored in the future to suppress the effect of
frequent queries.
In addition, Quetchup
click
constantly out-
performed Tchai in both the Travel and Fi-
7Since web search query logs contain many spelling mis-
takes, we experimented in a realistic configuration.
8Typically, precision at k is the most important measure
since the top k highest scored terms are evaluated by hand.
191
 0
 0.2
 0.4
 0.6
 0.8
 1
 10  20  30  40  50  60  70  80  90  100
Pr
ec
is
io
n
Rank
Quetchup (click)
Quetchup (query)
Tchai
Figure 3: Precision of Travel domain
 0
 0.2
 0.4
 0.6
 0.8
 1
 10  20  30  40  50  60  70  80  90  100
Pr
ec
is
io
n
Rank
Quetchup (click)
Quetchup (query)
Tchai
Figure 4: Precision of Finance domain
 0
 2
 4
 6
 8
 10
 10  20  30  40  50  60  70  80  90  100
R
el
at
iv
e 
re
ca
ll
Rank
Quetchup (click)
Tchai
Figure 5: Relative recall of Travel domain
 0
 2
 4
 6
 8
 10
 10  20  30  40  50  60  70  80  90  100
R
el
at
iv
e 
re
ca
ll
Rank
Quetchup (click)
Tchai
Figure 6: Relative recall of Finance domain
nance domains in precision and outperfomed
Quetchup
query
in relative recall. The differences
between the two domains of query-based systems
seem to lie in the size of correct instances. The Fi-
nance domain is a closed set which has only a few
effective query patterns, whereas Travel domain is
an open set which has many query patterns that
match correct instances. Quetchup
click
has an ad-
ditional advantage that it is stable across over the
ranked list, because the variance of the number of
clicked links is small thanks to the nature of the
ranking algorithm of search engines.
5 Conclusion
We have proposed a method called Quetchup
to learn semantic categories from search click-
through logs using Laplacian label propagation.
The proposed method greatly outperforms previ-
ous method, taking the advantage of search click-
through logs.
Acknowledgements
The first author is partly supported by the grant-in-
aid JSPS Fellowship for Young Researchers. We
thank the anonymous reviewers for helpful com-
ments and suggestions.
References
T. Joachims. 2002. Optimizing Search Engines Using Click-
through Data. KDD, pages 133?142.
M. Komachi and H. Suzuki. 2008. Minimally Supervised
Learning of Semantic Knowledge from Query Logs. IJC-
NLP, pages 358?365.
M. Pas?ca and B. V. Durme. 2007. What You Seek is What
You Get: Extraction of Class Attributes from Query Logs.
IJCAI-07, pages 2832?2837.
M. Pas?ca and B. V. Durme. 2008. Weakly-Supervised Ac-
quisition of Open-Domain Classes and Class Attributes
from Web Documents and Query Logs. ACL-2008, pages
19?27.
P. Pantel and D. Ravichandran. 2004. Automatically Label-
ing Semantic Classes. HLT/NAACL-04, pages 321?328.
C. Silverstein, M. Henzinger, H. Marais, and M. Moricz.
1998. Analysis of a Very Large AltaVista Query Log. Dig-
ital SRC Technical Note 1998-014.
P. P. Talukdar, J. Reisinger, M. Pas?ca, D. Ravichandran,
R. Bhagat, and F. Pereira. 2008. Weakly-Supervised Ac-
quisition of Labeled Class Instances using Graph Random
Walks. EMNLP-2008, pages 581?589.
G. Xu, S. Yang, and H. Li. 2009. Named Entity Mining
from Click-Through Log Using Weakly Supervised Latent
Dirichlet Allocation. KDD. to appear.
D. Zhou, O. Bousquet, T. N. Lal, J. Weston, and B. Scho?kopf.
2004. Learning with Local and Global Consistency.
NIPS, 16:321?328.
192
Proceedings of the Linguistic Annotation Workshop, pages 132?139,
Prague, June 2007. c?2007 Association for Computational Linguistics
Annotating a Japanese Text Corpus with
Predicate-Argument and Coreference Relations
Ryu Iida, Mamoru Komachi, Kentaro Inui and Yuji Matsumoto
Graduate School of Information Science,
Nara Institute of Science and Technology
8916-5 Takayama, Ikoma, Nara, 630-0192, Japan
{ryu-i,mamoru-k,inui,matsu}@is.naist.jp
Abstract
In this paper, we discuss how to anno-
tate coreference and predicate-argument re-
lations in Japanese written text. There
have been research activities for building
Japanese text corpora annotated with coref-
erence and predicate-argument relations as
are done in the Kyoto Text Corpus version
4.0 (Kawahara et al, 2002) and the GDA-
Tagged Corpus (Hasida, 2005). However,
there is still much room for refining their
specifications. For this reason, we discuss
issues in annotating these two types of re-
lations, and propose a new specification for
each. In accordance with the specification,
we built a large-scaled annotated corpus, and
examined its reliability. As a result of our
current work, we have released an anno-
tated corpus named the NAIST Text Corpus1,
which is used as the evaluation data set in
the coreference and zero-anaphora resolu-
tion tasks in Iida et al (2005) and Iida et al
(2006).
1 Introduction
Coreference resolution and predicate-argument
structure analysis has recently been a growing field
of research due to the demands from NLP appli-
cation such as information extraction and machine
translation. With the research focus placed on these
tasks, the specification of annotating corpora and the
1The NAIST Text Corpus is downloadable from
http://cl.naist.jp/nldata/corpus/, and it has already been
downloaded by 102 unique users.
data sets used in supervised techniques (Soon et al,
2001; Ng and Cardie, 2002, etc.) have also grown in
sophistication.
For English, several annotation schemes have al-
ready been proposed for both coreference relation
and argument structure, and annotated corpora have
been developed accordingly (Hirschman, 1997; Poe-
sio et al, 2004; Doddington et al, 2004). For in-
stance, in the Coreference task on Message Under-
standing Conference (MUC) and the Entity Detec-
tion and Tracking (EDT) task in the Automatic Con-
tent Extraction (ACE) program, which is the suc-
cessor of MUC, the details of specification of anno-
tating coreference relation have been discussed for
several years. On the other hand, the specification
of predicate-argument structure analysis has mainly
been discussed in the context of the CoNLL shared
task2 on the basis of the PropBank (Palmer et al,
2005).
In parallel with these efforts, there have also been
research activities for building Japanese text corpora
annotated with coreference and predicate-argument
relations such as the Kyoto Text Corpus version 4.0
(Kawahara et al, 2002) and the GDA3-Tagged Cor-
pus (Hasida, 2005). However, as we discuss in this
paper, there is still much room for arguing and re-
fining the specification of such sorts of semantic an-
notation. In fact, for neither of the above two cor-
pora, the adequacy and reliability of the annotation
scheme has been deeply examined.
In this paper, we discuss how to annotate coref-
erence and predicate-argument relations in Japanese
2http://www.lsi.upc.edu/?srlconll/
3The Global Document Annotation
132
text. In Section 2 to Section 4, we examine the an-
notation issues of coreference, predicate-argument
relations, and event-nouns and their argument rela-
tions respectively, and define adequate specification
of each annotation task. Then, we report the results
of actual annotation taking the Kyoto Corpus 3.0 as a
starting point. Section 6 discusses the open issues of
each annotation task and we conclude in Section 7.
2 Annotating coreference relations
2.1 Approaches to coreference annotation
Coreference annotation in English has been evolving
mainly in the context of information extraction. For
instance, in the 6th and 7th Message Understand-
ing Conferences (MUC), coreference resolution is
treated as a subtask of information extraction4. The
annotated corpora built in the MUC contain coref-
erence relations between NPs, which are used as a
gold standard data set for machine learning-based
approaches to coreference resolution by researchers
such as Soon et al (2001) and Ng and Cardie (2002).
However, van Deemter and Kibble (1999) claim
that the specification of the MUC coreference task
guides us to annotate expressions that are not nor-
mally considered coreferential, such as appositive
relations (e.g. Julius Caesari, a well-known em-
perori, ...).
In the task of Entity Detection and Tracking
(EDT) in the Automatic Content Extraction (ACE)
program (Doddington et al, 2004), the successor
of MUC, the coreference relations are redefined in
terms of two concepts, mentions and entities, in or-
der to avoid inappropriate co-indexing. In the speci-
fication of EDT, mentions are defined as the expres-
sions appearing in the texts, and entities mean the
collective set of specific entities referred to by the
mentions in the texts. Entities are limited to named
entities such as PERSON and ORGANIZATION for
adequacy and reliability of annotation. Therefore,
the ACE data set has the drawback that not all coref-
erence relations in the text are exhaustively anno-
tated. It is insufficient to resolve only the annotated
coreference relations in order to properly analyze a
text.
4http://www-nlpir.nist.gov/related projects/muc/
proceedings/co task.html
2.2 Coreference annotated corpora of Japanese
In parallel with these efforts, Japanese corpora have
been developed that are annotated with coreference
relations, such as the Kyoto Text Corpus version
4.0 (Kawahara et al, 2002) and GDA-Tagged Cor-
pus (Hasida, 2005). Before reviewing these works,
we explain the relationship between anaphora and
coreference in Japanese, referring to the following
examples. In example (1), the pronoun sorei (it)
points back to iPodi, and these two mentions refer
to the same entity in the world and thus are consid-
ered both anaphoric and coreferential.
(1) Tom-wa iPodi-o ka-tta .
Tom-TOP iPodi-ACC buy-PAST PUNC
Tom bought an iPod.
kare-wa sorei-de ongaku-o ki-ita .
he-TOP iti-INS music-ACC listen to-PAST PUNC
He listened to music on it.
On the other hand, in example (2), we still see an
anaphoric relation between iPodi (iPodi) and sorej
(itj) and sorej points back to iPodi. However, these
two mentions are not coreferential since they refer
to different entities in the world.
(2) Tom-wa iPodi-o ka-tta .
Tom-TOP iPodi-ACC buy-PAST PUNC
Tom bought an iPod.
Mary-mo sorej-o ka-tta .
Mary-TOP onej -ACC buy-PAST PUNC
Mary also bought one.
As in the above examples, an anaphoric relation
can be either coreferential or not. The former case is
called an identity-of-reference anaphora (IRA) and
the latter an identity-of-sense anaphora (ISA) (see
Mitkov (2002)). In English the difference between
IRA and ISA is clearly expressed by the anaphoric
relations formed with ?it? and ?one? respectively.
This makes it possible to treat these classes sepa-
rately. However, in Japanese, no such clear lexical
distinction can be drawn. In both the Kyoto Cor-
pus and GDA-Tagged Corpus, there is no discussion
in regards to distinction between ISA and IRA, thus
it is unclear what types of coreference relations the
annotators annotated. To make matters worse, their
approaches do not consider whether or not a mention
refers to a specific entity like in the EDT task.
2.3 Annotating IRA relations in Japanese
As described in the previous section, conventional
specifications in Japanese are not based on a pre-
133
cise definition of coreference relations, resulting in
inappropriate annotation. On the other hand, in our
specification, we consider two or more mentions as
coreferential in case they satisfy the following two
conditions:
? The mentions refer to not a generic entity but
to a specific entity.
? The relation between the mentions is consid-
ered as an IRA relation.
3 Annotating predicate-argument relations
3.1 Labels of predicate-argument relations
One debatable issue in the annotation of predicate-
argument relations is what level of abstraction we
should label those relations at.
The GDA-Tagged Corpus, for example, adopts a
fixed set of somewhat ?traditional? semantic roles
such as Agent, Theme, and Goal that are defined
across verbs. The PropBank (Palmer et al, 2005),
on the other hand, defines a set of semantic roles (la-
beled ARG0, ARG1, and AM-ADV, etc.) for each
verb and annotates each sentence in the corpus with
those labels as in (3).
(3) [ARGM?TMP A year earlier], [ARG0 the refiner] [rel
earned] [ARG1 $66 million, or $1.19 a share].
In the FrameNet (Fillmore and Baker, 2000), a spe-
cific set of semantic roles is defined for each set of
semantically-related verbs called a FrameNet frame.
However, there is still only limited consensus on
how many kinds of semantic roles should be iden-
tified and which linguistic theory we should adopt
to define them at least for the Japanese language.
An alternative way of labeling predicate-
argument relations is to use syntactic cases as
labels. In Japanese, arguments of a verb are marked
by a postposition, which functions as a case marker.
In sentence (4), for example, the verb tabe has
two arguments, each of which is marked by a
postposition, ga or o.
(4) Tom-ga ringo-o tabe-ru
Tom-NOM apple-ACC eat-PRES
(Tom eats an apple.)
Labeling predicate-argument relations in terms of
syntactic cases has a few more advantages over se-
mantic roles as far as Japanese is concerned:
? Manual annotation of syntactic cases is likely
to be more cost-efficient than semantic roles
because they are often explicitly marked by
case markers. This fact also allows us to avoid
the difficulties in defining a label set.
? In Japanese, the mapping from syntactic cases
to semantic roles tends to be reasonably
straightforward if a semantically rich lexicon of
verbs like the VerbNet (Kipper et al, 2000) is
available.
? Furthermore, we have not yet found many NLP
applications for which the utility of seman-
tic roles is actually demonstrated. One may
think of using semantic roles in textual infer-
ence as exemplified by, for example, Tatu and
Moldovan (2006). However, similar sort of
inference may well be realized with syntactic
cases as demonstrated in the information ex-
traction and question answering literature.
Taking these respects into account, we choose to
label predicate-argument relations in terms of syn-
tactic cases, which follows the annotation scheme
adopted in the Kyoto Corpus.
3.2 Syntactic case alternation
Once the level of syntactic cases is chosen for our
annotation, another issue immediately arises, alter-
ation of syntactic cases by syntactic transformations
such as passivization and causativization. For exam-
ple, sentence (5) is an example of causativization,
where Mary causes Tom?s eating action.
(5) Mary-ga Tom-ni ringo-o tabe-saseru
Mary-NOM Tom-DAT apple-ACC eat-CAUSATIVIZED
(Mary helps Tom eat an apple.)
One way of annotating these arguments is some-
thing like (6), where the relations between the
causativized predicate tabe-saseru (to make some-
one eat) and its arguments are indicated in terms of
surface syntactic cases.
(6) [REL=tabe-saseru (eat-CAUSATIVE),
GA=Mary, NI=Tom, O=ringo (apple)]
In fact, the Kyoto Corpus adopts this way of label-
ing.
An alternative way of treating such case alterna-
tions is to identify logical (or deep) case relations,
i.e. the relations between the base form of each pred-
icate and its arguments. (7) illustrates how the ar-
guments in sentence (5) are annotated with logical
case relations: Tom is labeled as the ga-case (Nom-
inative) filler of the verb tabe (to eat) and Mary is
134
labeled as the Extra-Nominative (EX-GA) which we
newly invent to indicate the Causer of a syntactically
causativized clause.
(7) [REL=tabe-(ru) (eat), GA=Tom, O=ringo (ap-
ple), EX-GA=Mary]
In the NAIST Text Corpus, we choose to this lat-
ter way of annotation motivated by such considera-
tions as follows:
? Knowing that, for example, Tom is the filler of
the ga-case (Nominative) of the verb tabe (to
eat) in (5) is more useful than knowing that Tom
is the ni-case (Dative) of the causativized verb
tabe-saseru (to make someone eat) for such ap-
plications as information extraction.
? The mapping from syntactic cases to semantic
roles should be described in terms of logical
case relations associated with bare verbs.
3.3 Zero-anaphora
In the PropBank the search space for a given pred-
icate?s arguments is limited to the sentence that
predicate appears in, because, syntactically, English
obligatory arguments are overtly expressed except
pro-form (e.g. John hopes [PRO to leave.]).
In contrast, Japanese is characterized by extensive
use of nominal ellipses, called zero-pronouns, which
behave like pronouns in English texts. Thus, if an
argument is omitted, and an expression correspond-
ing to that argument does not appear in the same
sentence, annotators should search for its antecedent
outside of the sentence. Furthermore, if an argument
is not explicitly mentioned in the text, they need to
annotate that relation as ?exophoric.? In the second
sentence of example (8), for instance, the ga (Nomi-
native) argument of the predicate kaeru (go back) is
omitted and refers to Tom in the first sentence. The
kara (Ablative) argument of that predicate is also
omitted, however the corresponding argument does
not explicitly appear in the text. In such cases, omit-
ted arguments should be considered as ?exophoric.?
(8) Tomi-wa kyo gakko-ni it-ta .
Tomi-TOP today school-LOC go-PAST PUNC
Tom went to school today.
(?i-ga) (?exophoric-kara) kae-tte suguni
?i-NOM ?exophoric-ABL go back immediately
(?i-ga) kouen-ni dekake-ta .
?i-NOM park-LOC go out-PAST PUNC
He went to the park as soon as he came back
from school.
Table 1: Comparison of annotating predicate-
argument relations
corpus label search space
PropBank semantic role intra
GDA Corpus semantic role inter, exo
Kyoto Corpus surface case intra, inter,
(voice alternation involved) exo
NAIST Corpus logical (deep) case intra, inter,
(our corpus) (relation with bare verb) exo
intra: intra-sentential relations, inter: inter-sentential relations,
exo: exophoric relations
To the best of our knowledge, the GDA-Tagged Cor-
pus does not contain intra-sentential zero-anaphoric
relations as predicate-argument relations, so it has a
serious drawback when used as training data in ma-
chine learning approaches.
Unlike coreference between two explicit nouns
where only an IRA is possible, the relation between
a zero-pronoun and its antecedent can be either IRA
or ISA. For example, in example (8), ?i is annotated
as having an IRA relation with its antecedent Tomi.
In contrast, example (9) exhibits an ISA relation be-
tween iPodi and ?i.
(9) Tom-wa iPodi-o kaa-tta .
Tom-TOP iPodi-ACC buya-PAST PUNC
Tom bought an iPod.
Mary-mo (?i-o) kab-tta .
Mary-TOP ?i-ACC buyb-PAST PUNC
Mary also bought one.
[REL=ka-(u) (buy), GA=Mary, O=iPodi]
The above examples indicate that predicate-
argument annotation in Japanese can potentially be
annotated as either an IRA or ISA relation. Note that
in Japanese these two relations cannot be explicitly
separated by syntactic clues. Thus, in our corpus
we annotate them without explicit distinction. It is
arguable that separate treatment of IRA and ISA in
predicate-argument annotation could be preferable.
We consider this issue as a task of future work.
A comparison of the specification is summarized
in Table 1.
4 Annotating event-noun-argument
relations
Meyers et al (2004) propose to annotate seman-
tic relations between nouns referring to an event
in the context, which we call event-nouns in this
135
paper. They release the NomBank corpus, in
which PropBank-style semantic relations are anno-
tated for event-nouns. In (10), for example, the
noun ?growth? refers to an event and ?dividends?
and ?next year? are annotated as ARG1 (roughly
corresponding to the theme role) and ARGM-TMP
(temporal adjunct).
(10) 12% growth in dividends next year [REL=growth,
ARG1=in dividends, ARGM-TMP=next year]
Following the PropBank-style annotation, the Nom-
Bank also restricts the search space for the argu-
ments of a given event-noun to the sentence in which
the event-noun appears. In Japanese, on the other
hand, since predicate-argument relations are often
zero-anaphoric, this restriction should be relaxed.
4.1 Labels of event-noun-relations
Regarding the choice between semantic roles and
syntactic cases, we take the same approach as
that for predicate-argument relations, which is also
adopted in the Kyoto Corpus. For example, in (11),
akajii (deficit) is identified as the ga argument of the
event-noun eikyo (influence).
(11) kono boueki akajii-wa waga kuni-no
this trade deficit-TOP our country-OF
kyosoryokuj-ni eikyo-o oyobosu
competitiveness-DAT influence-ACC affect
[REL=eikyo (influence), GA=akajii (deficit),
O=kyosoryokuj (competitiveness)]
The trade deficit affects our competitiveness.
Note that unlike verbal predicates, event-nouns can
never be a subject of voice alternation. An event-
noun-argument relation is, therefore, necessarily an-
notated in terms of the relation between the bare
verb corresponding to the event-noun and its argu-
ment. This is another reason why we consider it
reasonable to annotate the logical case relations be-
tween bare verbs and their arguments for predicate-
argument relations.
4.2 Event-hood
Another issue to be addressed is on the determina-
tion of the ?event-hood? of noun phrases, i.e. the
task of determining whether a given noun refers to
an event or not. In Japanese, since neither singular-
plural nor definite-indefinite distinction is explic-
itly marked, event-hood determination tends to be
highly context-dependent. In sentence (12), for ex-
ample, the first occurrence of denwa (phone-call),
subscripted with i, should be interpreted as Tom?s
calling event, whereas the second occurrence of the
same noun denwa should be interpreted as a physical
telephone (cellphone).
(12) karea-karano denwai-niyoruto watashib-wa
hea-ABL phone-calli according to Ib-NOM
kare-no ie-ni denwaj-o wasure-tarasii
his-OF home-LOC phonej -ACC leave-PAST
According to his phone call, I might have left
my cell phone at his home.
To control the quality of event-hood determina-
tion, we constrain the range of potential event-nouns
from two different points of view, neither of which
is explicitly discussed in designing the specifications
of the Kyoto Corpus.
First, we impose a POS-based constraint. In our
corpus annotation, we consider only verbal nouns
(sahen-verbs; e.g. denwa (phone) ) and deverbal
nouns (the nominalized forms of verbs; e.g. furumai
(behavior)) as potential event-nouns. This means
that event-nouns that are not associated with a verb,
such as jiko (accident), are out of scope of our anno-
tation.
Second, the determination of the event-hood of
a noun tends to be obscure when the noun consti-
tutes a compound. In (13), for example, the ver-
bal noun kensetsu (construction) constituting a com-
pound douro-kensetsu (road construction) can be in-
terpreted as a constructing event. We annotate it as
an event and douro (road) as the o argument.
(13) (?-ga) douro-kensetsu-o tsuzukeru
?-NOM road construction-ACC continue
Someone continues road construction.
In (14), on the other hand, since the compound
furansu kakumei (French Revolution) is a named-
entity and is not semantically decomposable, it is
not reasonable to consider any sort of predicate-
argument-like relations between its constituents fu-
ransu (France) and kakumei (revolution).
(14) furansu-kakumei-ga okoru
French Revolution-NOM take place
The French Revolution took place.
We therefore do not consider constituents of such se-
mantically non-decomposable compounds as a tar-
get of annotation.
5 Statistics of the new corpus
Two annotators annotated predicate-argument and
coreference relations according to the specifications,
136
using all the documents in Kyoto Text Corpus ver-
sion 3.0 (containing 38,384 sentences in 2,929 texts)
as a target corpus. We have so far annotated
predicate-argument relations with only three major
cases: ga (Nominative), o (Accusative) and ni (Da-
tive). We decided not to annotate other case relations
like kara-case (Ablative) because the annotation of
those cases was considered even further unreliable at
the point where we did not have enough experiences
in this annotation task. Annotating other cases is one
of our future directions.
The numbers of the annotated predicate-argument
relations are shown in Table 2. These relations are
categorized into five cases: (a) a predicate and its
argument appear in the same phrase, (b) the argu-
ment syntactically depends on its predicate or vice
versa, (c) the predicate and its argument have an
intra-sentential zero-anaphora relation, (d) the pred-
icate and its argument have an inter-sentential zero-
anaphora relation and (e) the argument does not ex-
plicitly appear in the text (i.e. exophoric). Table 2
shows that in annotation for predicates over 80%
of both o- and ni-arguments were found in depen-
dency relations, while around 60% of ga-arguments
were in zero-anaphoric relations. In comparison, in
the case of event-nouns, o- and ni-arguments are
likely to appear in the same phrase of given event-
nouns, and about 80% of ga-arguments have zero-
anaphoric relations with event-nouns. With respect
to the corpus size, we created a large-scaled anno-
tated corpus with predicate-argument and corefer-
ence relations. The data size of our corpus along
with other corpora is shown in Table 3.
Next, to evaluate the agreement between the two
human annotators, 287 randomly selected articles
were annotated by both of them. The results are
evaluated by calculating recall and precision in
which one annotation result is regarded as correct
and the other?s as the output of system. Note that
only the predicates annotated by both annotators are
used in calculating recall and precision. For eval-
uation of coreference relations, we calculated re-
call and precision based on the MUC score (Vilain
et al, 1995). The results are shown in Table 4,
where we can see that most annotating work was
done with high quality except for the ni-argument of
event-nouns. The most common source of error was
caused by verb alternation, and we will discuss this
Table 3: Data size of each corpus
corpus size
PropBank I 7,891 sentences
NomBank 0.8 24,311 sentences
ACE (2005 English) 269 articles
GDA Corpus 2,177 articles
Kyoto Corpus 555 articles (5,127 sentences)
NAIST Corpus (ours) 2,929 articles (38,384 sentences)
Table 4: Agreement of annotating each relation
recall precision
predicate 0.947 (6512/6880) 0.941 (6512/6920)
ga (NOM) 0.861 (5638/6549) 0.856 (5638/6567)
o (ACC) 0.943 (2447/2595) 0.919 (2447/2664)
ni (DAT) 0.892 (1060/1189) 0.817 (1060/1298)
event-noun 0.905 (1281/1415) 0.810 (1281/1582)
ga (NOM) 0.798 (1038/1300) 0.804 (1038/1291)
o (ACC) 0.893 (469/525) 0.765 (469/613)
ni (DAT) 0.717 (66/92) 0.606 (66/109)
coreference 0.893 (1802/2019) 0.831 (1802/2168)
issue in detail in Section 6. Such investigation of the
reliability of annotation has not been reported for ei-
ther the Kyoto Corpus or the GDA-Tagged Corpus.
However, our results also show that each annotating
task still leaves room for improvement. We summa-
rize open issues and discuss the future directions in
the next section.
6 Discussion
6.1 Identification of predicates and
event-nouns
Identification of predicates is sometimes unreliable
due to the ambiguity between a literal usage and a
compound functional usage. For instance, the ex-
pression ?to-shi-te?, which includes the verb shi (to
do), is ambiguous: either the verb shi functions as a
content word, i.e. an event-denoting word, or it con-
stitutes a multi-word expression together with to and
te. In the latter case, it does not make sense to inter-
pret the verb shi to denote an event. However, this
judgment is highly context-dependent and we have
not been able to devise a reliable criterion for it.
Tsuchiya et al (2006) have built a functional
expression-tagged corpus for automatically classify-
ing these usages. They reported that the agreement
ratio of functional expressions is higher than ours.
We believe their findings to also become helpful in-
formation for annotating predicates in our corpus.
With regards to event-nouns, a similar problem
137
Table 2: Statistics: annotating predicate-arguments relations
ga (Nominative) o (Accusative) ni (Dative)
predicates (a) in same phrase 177 (0.002) 60 (0.001) 591 (0.027)
106,628 (b) dependency relations 44,402 (0.419) 35,882 (0.835) 18,912 (0.879)
(c) zero-anaphoric (intra-sentential) 32,270 (0.305) 5,625 (0.131) 1,417 (0.066)
(d) zero-anaphoric (inter-sentential) 13,181 (0.124) 1,307 (0.030) 542 (0.025)
(e) exophoric 15,885 (0.150) 96 (0.002) 45 (0.002)
total 105,915 (1.000) 42,970 (1.000) 21,507 (1.000)
event-nouns (a) in same phrase 2,195 (0.077) 5,574 (0.506) 846 (0.436)
28,569 (b) dependency relations 4,332 (0.152) 2,890 (0.263) 298 (0.154)
(c) zero-anaphoric (intra-sentential) 9,222 (0.324) 1,645 (0.149) 586 (0.302)
(d) zero-anaphoric (inter-sentential) 5,190 (0.183) 854 (0.078) 201 (0.104)
(e) exophoric 7,525 (0.264) 42 (0.004) 10 (0.005)
total 28,464 (1.000) 11,005 (1.000) 1,941 (1.000)
also arises. If, for example, a compound noun con-
tains a verbal noun, we have to judge whether the
verbal noun can be interpreted as an event-noun or
not. Currently, we ask annotators to check if the
meaning of a given compound noun can be compo-
sitionally decomposed into those of its constituents.
However, the judgement of compositionality tends
to be highly subjective, causing the degradation of
the agreement ratio of event-nouns as shown in
Table 4. We are planning to investigate this problem
more closely and refine the current compositionality
criterion. One option is to build lexical resources of
multi-word expressions and compounds.
6.2 Identification of arguments
As we mentioned in 3.1, we use (deep) cases instead
of semantic roles as labels of predicate-argument re-
lations. While it has several advantages as discussed
in 3.1, this choice has also a drawback that should
be removed. The problem arises from lexical verb
alternation. It can sometimes be hard for annota-
tors to determine a case frame of a given predicate
when verb alternation takes place. For example, sen-
tence (15) can be analyzed simply as in (16a). How-
ever, since the verb shibaru (bind) has also another
alternative case frame as in (16b), the labeling of the
case of the argument kisoku (rule), i.e. either GA
(NOM) or DE (INST) may be undecidable if the argu-
ment is omitted.
(15) kisoku-ga hitobito-o shibaru
rule-NOM people-ACC bind
The rule binds people.
(16) a. [REL = shibaru (bind), GA = kisoku (rule), O = hitobito
(people)]
b. [REL = shibaru (bind), GA = ? (exophoric), O = hito-
bito (people), DE (Instrumental) = kisoku (rule)]
Similar problems occur for event-nouns as well.
For example, the event-noun hassei (realization) has
both transitive and intransitive readings, which may
produce awkward ambiguities.
To avoid this problem, we have two options; one
is to predefine the preference in case frames as a
convention for annotation and the other is to deal
with such alternations based on generic resources of
lexical semantics such as Lexical Conceptual Struc-
ture (LCS) (Jackendoff, 1990). Creating a Japanese
LCS dictionary is another on-going project, so we
can collaborate with them in developing the valuable
resources.
6.3 Event-hood determination
Event-nouns of some semantic types such as keiyaku
(contract), kisei (regulation) and toushi (investment)
are interpreted as either an event or an entity result-
ing from an event depending on are context. How-
ever, it is sometimes difficult to judge whether such
an event-noun should be interpreted as an event or a
resultant entity even by considering the whole con-
text, which degrades the stability of annotation. This
phenomena is also discussed in the NomBank, and
we will share their insights and refine our annotation
manual in the next step.
6.4 Identification of coreference relation
Even though coreference relation is defined as IRA
relations, the lack of agreement on the granularity of
noun classes makes the agreement ratio worse. In
other words, it is crucial to decide how to annotate
abstract nouns in order to improve the annotation.
138
Annotators judge coreference relations as whether
or not abstract nouns refer to the same entity in the
world. However, the equivalence of the referents of
abstract nouns cannot be reconciled based on real-
world existence since by definition abstract nouns
have no physical entities in the real world.
As far as predicate-argument relation is con-
cerned, there might be a need for treating generic
entities in addition to specific entities as coreferen-
tial in some application. For example, one may want
to relate kids to children in sentence (17).
(17) We all want children to be fit and healthy.
However, the current invasion of fast food is
creating overweight and unhealthy kids.
The coreference relation between generic nouns are
missed in the current specification since we annotate
only IRA relations between specific nouns. Even
though there are various discussions in the area of
semantics, the issue of how to deal with generic
nouns as either coreferential or not in real texts is
still left open.
7 Conclusion
In this paper, we reported on the current specifica-
tion of our annotated corpus for coreference reso-
lution and predicate-argument analysis. Taking the
previous work of corpus annotation into account, we
decided to annotate predicate-argument relations by
ISA and IRA relations, and coreference relations ac-
cording to IRA relations. With the Kyoto Text Cor-
pus version 3.0 as a starting point, we built a large
annotated corpus. We also discussed the revelations
made from annotating our corpus, and discussed fu-
ture directions for refining our specifications of the
NAIST Text Corpus.
Acknowledgement
This work is partially supported by the Grant-in-Aid
for Scientific Research in Priority Areas JAPANESE
CORPUS (http://tokuteicorpus.jp).
References
G. Doddington, A. Mitchell, M. Przybocki, L. Ramshaw,
S. Strassel, and R. Weischedel. 2004. Automatic content
extraction (ace) program - task definitions and performance
measures. In Proceedings of the 4rd International Confer-
ence on Language Resources and Evaluation (LREC-2004),
pages 837?840.
Charles J. Fillmore and Collin F. Baker. 2000. Framenet:
Frame semantics meets the corpus. In Proceedings of the
74th Annual Meeting of the Linguistic Society of America.
K. Hasida. 2005. Global document annotation (gda) annotation
manual. http://i-content.org/tagman.html.
L. Hirschman. 1997. MUC-7 coreference task definition. ver-
sion 3.0.
R. Iida, K. Inui, and Y. Matsumoto. 2005. Anaphora reso-
lution by antecedent identification followed by anaphoricity
determination. ACM Transactions on Asian Language Infor-
mation Processing (TALIP), 4:417?434.
R. Iida, K. Inui, and Y. Matsumoto. 2006. Exploiting syntac-
tic patterns as clues in zero-anaphora resolution. In Proced-
dings of the 21st International Conference on Computational
Linguistics and 44th Annual Meeting of the Association for
Computational Linguistics (COLING-ACL), pages 625?632.
R. Jackendoff. 1990. Semantic Structures. Current Studies in
Linguistics 18. The MIT Press.
D. Kawahara, T. Kurohashi, and K. Hasida. 2002. Construc-
tion of a japanese relevance-tagged corpus (in japanese). In
Proceedings of the 8th Annual Meeting of the Association for
Natural Language Processing, pages 495?498.
K. Kipper, H. T. Dang, and M. Palmer. 2000. Class-based con-
struction of a verb lexicon. In Proceedings of the 17th Na-
tional Conference on Artificial Intelligence and 12th Confer-
ence on on Innovative Applications of Artificial Intelligence,
pages 691?696.
A. Meyers, R. Reeves, C. Macleod, R. Szekely, V. Zielinska,
B. Young, and R. Grishman. 2004. The nombank project:
An interimreport. In Proceedings of the HLT-NAACL Work-
shop on Frontiers in Corpus Annotation.
Ruslan Mitkov. 2002. Anaphora Resolution. Studies in Lan-
guage and Linguistics. Pearson Education.
V. Ng and C. Cardie. 2002. Improving machine learning ap-
proaches to coreference resolution. In Proceedings of the
40th ACL, pages 104?111.
M. Palmer, D. Gildea, and P. Kingsbury. 2005. The proposi-
tion bank: An annotated corpus of semantic roles. Compu-
tational Linguistics, 31(1):71?106.
M. Poesio, R. Mehta, A. Maroudas, and J. Hitzeman. 2004.
Learning to resolve bridging references. In Proceddings of
the 42nd Annual Meeting of the Association for Computa-
tional Linguistics (ACL), pages 144?151.
W. M. Soon, H. T. Ng, and D. C. Y. Lim. 2001. A machine
learning approach to coreference resolution of noun phrases.
Computational Linguistics, 27(4):521?544.
M. Tatu and D. Moldovan. 2006. A logic-based semantic ap-
proach to recognizing textual entailment. In Proceddings
of the 21st International Conference on Computational Lin-
guistics and 44th Annual Meeting of the Association for
Computational Linguistics (COLING-ACL), pages 819?826.
M. Tsuchiya, T. Utsuro, S. Matsuyoshi, S. Sato, and S. Nak-
agawa. 2006. Development and analysis of an exam-
ple database of japanese compound functional expressions.
IPSJ Journal, 47(6):1728?1741.
K. van Deemter and R. Kibble. 1999. What is coreference, and
what should coreference annotation be? In Proceedings of
the ACL ?99 Workshop on Coreference and its applications,
pages 90?96.
M. Vilain, J. Burger, J. Aberdeen, D. Connolly, and
L. Hirschman. 1995. A model-theoretic coreference scoring
scheme. In Proceedings of the 6th Message Understanding
Conference (MUC-6), pages 45?52.
139
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:shortpapers, pages 30?36,
Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational Linguistics
HITS-based Seed Selection and Stop List Construction for Bootstrapping
Tetsuo Kiso Masashi Shimbo Mamoru Komachi Yuji Matsumoto
Graduate School of Information Science
Nara Institute of Science and Technology
Ikoma, Nara 630-0192, Japan
{tetsuo-s,shimbo,komachi,matsu}@is.naist.jp
Abstract
In bootstrapping (seed set expansion), select-
ing good seeds and creating stop lists are two
effective ways to reduce semantic drift, but
these methods generally need human super-
vision. In this paper, we propose a graph-
based approach to helping editors choose ef-
fective seeds and stop list instances, appli-
cable to Pantel and Pennacchiotti?s Espresso
bootstrapping algorithm. The idea is to select
seeds and create a stop list using the rankings
of instances and patterns computed by Klein-
berg?s HITS algorithm. Experimental results
on a variation of the lexical sample task show
the effectiveness of our method.
1 Introduction
Bootstrapping (Yarowsky, 1995; Abney, 2004) is a
technique frequently used in natural language pro-
cessing to expand limited resources with minimal
supervision. Given a small amount of sample data
(seeds) representing a particular semantic class of
interest, bootstrapping first trains a classifier (which
often is a weighted list of surface patterns character-
izing the seeds) using the seeds, and then apply it on
the remaining data to select instances most likely to
be of the same class as the seeds. These selected in-
stances are added to the seed set, and the process is
iterated until sufficient labeled data are acquired.
Many bootstrapping algorithms have been pro-
posed for a variety of tasks: word sense disambigua-
tion (Yarowsky, 1995; Abney, 2004), information
extraction (Hearst, 1992; Riloff and Jones, 1999;
Thelen and Riloff, 2002; Pantel and Pennacchiotti,
2006), named entity recognition (Collins and Singer,
1999), part-of-speech tagging (Clark et al, 2003),
and statistical parsing (Steedman et al, 2003; Mc-
Closky et al, 2006).
Bootstrapping algorithms, however, are known to
suffer from the problem called semantic drift: as the
iteration proceeds, the algorithms tend to select in-
stances increasingly irrelevant to the seed instances
(Curran et al, 2007). For example, suppose we want
to collect the names of common tourist sites from a
web corpus. Given seed instances {New York City,
Maldives Islands}, bootstrapping might learn, at one
point of the iteration, patterns like ?pictures of X?
and ?photos of X,? which also co-occur with many
irrelevant instances. In this case, a later iteration
would likely acquire frequent words co-occurring
with these generic patterns, such as Michael Jack-
son.
Previous work has tried to reduce the effect of se-
mantic drift by making the stop list of instances that
must not be extracted (Curran et al, 2007; McIntosh
and Curran, 2009). Drift can also be reduced with
carefully selected seeds. However, both of these ap-
proaches require expert knowledge.
In this paper, we propose a graph-based approach
to seed selection and stop list creation for the state-
of-the-art bootstrapping algorithm Espresso (Pantel
and Pennacchiotti, 2006). An advantage of this ap-
proach is that it requires zero or minimal super-
vision. The idea is to use the hubness score of
instances and patterns computed from the point-
wise mutual information matrix with the HITS al-
gorithm (Kleinberg, 1999). Komachi et al (2008)
pointed out that semantic drift in Espresso has the
same root as topic drift (Bharat and Henzinger,
1998) observed with HITS, noting the algorithmic
similarity between them. While Komachi et al pro-
posed to use algorithms different from Espresso to
30
avoid semantic drift, in this paper we take advantage
of this similarity to make better use of Espresso.
We demonstrate the effectiveness of our approach
on a word sense disambiguation task.
2 Background
In this section, we review related work on seed se-
lection and stop list construction. We also briefly in-
troduce the Espresso bootstrapping algorithm (Pan-
tel and Pennacchiotti, 2006) for which we build our
seed selection and stop list construction methods.
2.1 Seed Selection
The performance of bootstrapping can be greatly in-
fluenced by a number of factors such as the size of
the seed set, the composition of the seed set and the
coherence of the concept being expanded (Vyas et
al., 2009). Vyas et al (2009) studied the impact of
the composition of the seed sets on the expansion
performance, confirming that seed set composition
has a significant impact on the quality of expansions.
They also found that the seeds chosen by non-expert
editors are often worse than randomly chosen ones.
A similar observation was made by McIntosh and
Curran (2009), who reported that randomly chosen
seeds from the gold-standard set often outperformed
seeds chosen by domain experts. These results sug-
gest that even for humans, selecting good seeds is a
non-trivial task.
2.2 Stop Lists
Yangarber et al (2002) proposed to run multiple
bootstrapping sessions in parallel, with each session
trying to extract one of several mutually exclusive
semantic classes. Thus, the instances harvested in
one bootstrapping session can be used as the stop
list of the other sessions. Curran et al (2007) pur-
sued a similar idea in their Mutual Exclusion Boot-
strapping, which uses multiple semantic classes in
addition to hand-crafted stop lists. While multi-class
bootstrapping is a clever way to reduce human su-
pervision in stop list construction, it is not generally
applicable to bootstrapping for a single class. To ap-
ply the idea of multi-class bootstrapping to single-
class bootstrapping, one has to first find appropri-
ate competing semantic classes and good seeds for
them, which is in itself a difficult problem. Along
this line of research, McIntosh (2010) recently used
Algorithm 1 Espresso algorithm
1: Input: Seed vector i0
2: Instance-pattern co-occurrence matrix A
3: Instance cutoff parameter k
4: Pattern cutoff parameter m
5: Number of iterations ?
6: Output: Instance score vector i
7: Pattern score vector p
8: function ESPRESSO(i0,A,k,m,?)
9: i? i0
10: for t = 1,2, ...,? do
11: p? AT i
12: Scale p so that the components sum to one.
13: p? SELECTKBEST(p,k)
14: i? Ap
15: Scale i so that the components sum to one.
16: i? SELECTKBEST(i,m)
17: return i and p
18: function SELECTKBEST(v,k)
19: Retain only the k largest components of v, resetting the
remaining components to 0.
20: return v
clustering to find competing semantic classes (nega-
tive categories).
2.3 Espresso
Espresso (Pantel and Pennacchiotti, 2006) is one of
the state-of-the-art bootstrapping algorithms used in
many natural language tasks (Komachi and Suzuki,
2008; Abe et al, 2008; Ittoo and Bouma, 2010;
Yoshida et al, 2010). Espresso takes advantage of
pointwise mutual information (pmi) (Manning and
Schu?tze, 1999) between instances and patterns to
evaluate their reliability. Let n be the number of all
instances in the corpus, and p the number of all pos-
sible patterns. We denote all pmi values as an n? p
instance-pattern matrix A, with the (i, j) element of
A holding the value of pmi between the ith instance
and the jth pattern. Let AT denote the matrix trans-
pose of A.
Algorithm 1 shows the pseudocode of Espresso.
The input vector i0 (called seed vector) is an n-
dimensional binary vector with 1 at the ith com-
ponent for every seed instance i, and 0 elsewhere.
The algorithm outputs an n-dimensional vector i and
an p-dimensional vector p, respectively representing
the final scores of instances and patterns. Note that
for brevity, the pseudocode assumes fixed numbers
(k and m) of components in i and p are carried over
to the subsequent iteration, but the original Espresso
31
allows them to gradually increase with the number
of iterations.
3 HITS-based Approach to Seed Selection
and Stop List Construction
3.1 Espresso and HITS
Komachi et al (2008) pointed out the similarity
between Espresso and Kleinberg?s HITS web page
ranking algorithm (Kleinberg, 1999). Indeed, if we
remove the pattern/instance selection steps of Algo-
rithm 1 (lines 13 and 16), the algorithm essentially
reduces to HITS. In this case, the outputs i and p
match respectively the hubness and authority score
vectors of HITS, computed on the bipartite graph of
instances and patterns induced by matrix A.
An implication of this algorithmic similarity is
that the outputs of Espresso are inherently biased
towards the HITS vectors, which is likely to be
the cause of semantic drift. Even though the pat-
tern/instance selection steps in Espresso reduce such
a bias to some extent, the bias still persists, as em-
pirically verified by Komachi et al (2008). In other
words, the expansion process does not drift in ran-
dom directions, but tend towards the set of instances
and patterns with the highest HITS scores, regard-
less of the target semantic class. We exploit this ob-
servation in seed selection and stop list construction
for Espresso, in order to reduce semantic drift.
3.2 The Procedure
Our strategy is extremely simple, and can be sum-
marized as follows.
1. First, compute the HITS ranking of instances
in the graph induced by the pmi matrix A. This
can be done by calling Algorithm 1 with k =
m = ? and a sufficiently large ? .
2. Next, check the top instances in the HITS rank-
ing list manually, and see if these belong to the
target class.
3. The third step depends on the outcome of the
second step.
(a) If the top instances are of the target class,
use them as the seeds. We do not use a
stop list in this case.
(b) If not, these instances are likely to make a
vector for which semantic drift is directed;
hence, use them as the stop list. In this
case, the seed set must be prepared manu-
ally, just like the usual bootstrapping pro-
cedure.
4. Run Espresso with the seeds or stop list found
in the last step.
4 Experimental Setup
We evaluate our methods on a variant of the lexi-
cal sample word sense disambiguation task. In the
lexical sample task, a small pre-selected set of a tar-
get word is given, along with an inventory of senses
for each word (Jurafsky and Martin, 2008). Each
word comes with a number of instances (context
sentences) in which the target word occur, and some
of these sentences are manually labeled with the cor-
rect sense of the target word in each context. The
goal of the task is to classify unlabeled context sen-
tences by the sense of the target word in each con-
text, using the set of labeled sentences.
To apply Espresso for this task, we reformulate
the task to be that of seed set expansion, and not
classification. That is, the hand-labeled sentences
having the same sense label are used as the seed set,
and it is expanded over all the remaining (unlabeled)
sentences.
The reason we use the lexical sample task is that
every sentence (instance) belongs to one of the pre-
defined senses (classes), and we can expect the most
frequent sense in the corpus to form the highest
HITS ranking instances. This allows us to com-
pletely automate our experiments, without the need
to manually check the HITS ranking in Step 2 of
Section 3.2. That is, for the most frequent sense
(majority sense), we take Step 3a and use the highest
ranked instances as seeds; for the rest of the senses
(minority senses), we take Step 3b and use them as
the stop list.
4.1 Datasets
We used the seven most frequent polysemous nouns
(arm, bank, degree, difference, paper, party and
shelter) in the SENSEVAL-3 dataset, and line (Lea-
cock et al, 1993) and interest (Bruce and Wiebe,
32
Task Method MAP AUC R-Precision P@30 P@50 P@100
arm Random 84.3 ?4.1 59.6 ?8.1 80.9 ?2.2 89.5 ?10.8 87.7 ?9.6 85.4 ?7.2
HITS 85.9 59.7 79.3 100 98.0 89.0
bank Random 74.8 ?6.5 61.6 ?9.6 72.6 ?4.5 82.9 ?14.8 80.1 ?13.5 76.6 ?10.9
HITS 84.8 77.6 78.0 100 100 94.0
degree Random 69.4 ?3.0 54.3 ?4.2 66.7 ?2.3 76.8 ?9.5 73.8 ?7.5 70.5 ?5.3
HITS 62.4 49.3 63.2 56.7 64.0 66.0
difference Random 48.3 ?3.8 54.5 ?5.0 47.0 ?4.4 53.9 ?10.7 50.7 ?8.8 47.9 ?6.1
HITS 50.2 60.1 51.1 60.0 60.0 48.0
paper Random 75.2 ?4.1 56.4 ?7.1 71.6 ?3.3 82.3 ?9.8 79.6 ?8.8 76.9 ?6.1
HITS 75.2 61.0 75.2 73.3 80.0 78.0
party Random 79.1 ?5.0 57.0 ?9.7 76.6 ?3.1 84.5 ?10.7 82.7 ?9.2 80.2 ?7.5
HITS 85.2 68.2 78.5 100 96.0 87.0
shelter Random 74.9 ?2.3 51.5 ?3.3 73.2 ?1.3 77.3 ?7.8 76.0 ?5.6 74.5 ?3.5
HITS 77.0 54.6 72.0 76.7 84.0 79.0
line Random 44.5 ?15.1 36.3 ?16.9 40.1 ?14.6 75.0 ?21.0 69.8 ?24.1 62.3 ?27.9
HITS 72.2 68.6 68.5 100 100 100
interest Random 64.9 ?8.3 64.9 ?12.0 63.7 ?10.2 87.6 ?13.2 85.3 ?13.7 81.2 ?13.9
HITS 75.3 83.0 80.1 100 94.0 77.0
Avg. Random 68.4 55.1 65.8 78.9 76.2 72.8
HITS 74.2 64.7 71.8 85.2 86.2 79.8
Table 1: Comparison of seed selection for Espresso (? = 5, nseed = 7). For Random, results are reported as (mean ?
standard deviation). All figures are expressed in percentage terms. The row labeled ?Avg.? lists the values macro-
averaged over the nine tasks.
1994) datasets1 for our experiments. We lowercased
words in the sentence and pre-processed them with
the Porter stemmer (Porter, 1980) to get the stems of
words.
Following (Komachi et al, 2008), we used two
types of features extracted from neighboring con-
texts: collocational features and bag-of-words fea-
tures. For collocational features, we set a window of
three words to the right and left of the target word.
4.2 Evaluation methodology
We run Espresso on the above datasets using differ-
ent seed selection methods (for majority sense of tar-
get words), and with or without stop lists created by
our method (for minority senses of target words).
We evaluate the performance of the systems ac-
cording to the following evaluation metrics: mean
average precision (MAP), area under the ROC curve
(AUC), R-precision, and precision@n (P@n) (Man-
ning et al, 2008). The output of Espresso may con-
tain seed instances input to the system, but seeds are
excluded from the evaluation.
1http://www.d.umn.edu/?tpederse/data.html
5 Results and Discussion
5.1 Effect of Seed Selection
We first evaluate the performance of our seed se-
lection method for the majority sense of the nine
polysemous nouns. Table 1 shows the performance
of Espresso with the seeds chosen by the proposed
HITS-based seed selection method (HITS), and with
the seed sets randomly chosen from the gold stan-
dard sets (Random; baseline). The results for Ran-
dom were averaged over 1000 runs. We set the num-
ber of seeds nseed = 7 and number of iterations ? = 5
in this experiment.
As shown in the table, HITS outperforms the
baseline systems except degree. Especially, the
MAP reported in Table 1 shows that our approach
achieved improvements of 10 percentage points on
bank, 6.1 points on party, 27.7 points on line, and
10.4 points on interest over the baseline, respec-
tively. AUC and R-precision mostly exhibit a trend
similar to MAP, except R-precision in arm and shel-
ter, for which the baseline is better. It can be seen
from the P@n (P@30, P@50 and P@100) reported
in Table 1 that our approach performed considerably
better than baseline, e.g., around 17?20 points above
33
Task Method MAP AUC R-Precision P@10 P@20 P@30
arm NoStop 12.7 ?4.3 51.8 ?10.8 13.9 ?9.8 21.4 ?19.1 15.1 ?12.0 14.1 ?10.4
HITS 13.4 ?4.1 53.7 ?10.5 15.0 ?9.5 23.8 ?17.7 17.5 ?12.0 15.5 ?10.2
bank NoStop 32.5 ?5.1 73.0 ?8.5 45.1 ?10.3 80.4 ?21.8 70.3 ?21.2 62.6 ?18.1
HITS 33.7 ?3.7 75.4 ?5.7 47.6 ?8.1 82.6 ?18.1 72.7 ?18.5 65.3 ?15.5
degree NoStop 34.7 ?4.2 69.7 ?5.6 43.0 ?7.1 70.0 ?18.7 62.8 ?15.7 55.8 ?14.3
HITS 35.7 ?4.3 71.7 ?5.6 44.3 ?7.6 72.4 ?16.4 64.4 ?15.9 58.3 ?16.2
difference NoStop 20.2 ?3.9 57.1 ?6.7 22.3 ?8.3 35.8 ?18.7 27.7 ?14.0 25.5 ?11.9
HITS 21.2 ?3.8 59.1 ?6.3 24.2 ?8.4 38.2 ?20.5 30.2 ?14.0 28.0 ?11.9
paper NoStop 25.9 ?6.6 53.1 ?10.0 27.7 ?9.8 55.2 ?34.7 42.4 ?25.4 36.0 ?17.8
HITS 27.2 ?6.3 56.3 ?9.1 29.4 ?9.5 57.4 ?35.3 45.6 ?25.3 38.7 ?17.5
party NoStop 23.0 ?5.3 59.4 ?10.8 30.5 ?9.1 59.6 ?25.8 46.8 ?17.4 38.7 ?12.7
HITS 24.1 ?5.0 62.5 ?9.8 32.1 ?9.4 61.6 ?26.4 47.9 ?16.6 40.8 ?12.7
shelter NoStop 24.3 ?2.4 50.6 ?3.2 25.1 ?4.6 25.4 ?11.7 26.9 ?10.3 25.9 ?8.7
HITS 25.6 ?2.3 53.4 ?3.0 26.5 ?4.8 28.8 ?12.9 29.0 ?10.4 28.1 ?8.2
line NoStop 6.5 ?1.8 38.3 ?5.3 2.1 ?4.1 0.8 ?4.4 1.8 ?8.9 2.3 ?11.0
HITS 6.7 ?1.9 38.8 ?5.8 2.4 ?4.4 1.0 ?4.6 2.0 ?8.9 2.5 ?11.1
interest NoStop 29.4 ?7.6 61.0 ?12.1 33.7 ?13.2 69.6 ?40.3 67.0 ?39.1 65.7 ?37.8
HITS 31.2 ?5.6 63.6 ?9.1 36.1 ?10.5 81.0 ?29.4 78.1 ?27.0 77.4 ?24.3
Avg. NoStop 23.2 57.1 27.0 46.5 40.1 36.3
HITS 24.3 59.4 28.6 49.6 43.0 39.4
Table 2: Effect of stop lists for Espresso (nstop = 10, nseed = 10, ? = 20). Results are reported as (mean ? standard
deviation). All figures are expressed in percentage. The row labeled ?Avg.? shows the values macro-averaged over all
nine tasks.
the baseline on bank and 25?37 points on line.
5.2 Effect of Stop List
Table 2 shows the performance of Espresso using
the stop list built with our proposed method (HITS),
compared with the vanilla Espresso not using any
stop list (NoStop).
In this case, the size of the stop list is set to nstop =
10, and the number of seeds nseed = 10 and iterations
? = 20. For both HITS and NoStop, the seeds are
selected at random from the gold standard data, and
the reported results were averaged over 50 runs of
each system. Due to lack of space, only the results
for the second most frequent sense for each word are
reported; i.e., the results for more minor senses are
not in the table. However, they also showed a similar
trend.
As shown in the table, our method (HITS) outper-
forms the baseline not using a stop list (NoStop), in
all evaluation metrics. In particular, the P@n listed
in Table 2 shows that our method provides about
11 percentage points absolute improvement over the
baseline on interest, for all n = 10, 20, and 30.
6 Conclusions
We have proposed a HITS-based method for allevi-
ating semantic drift in the bootstrapping algorithm
Espresso. Our idea is built around the concept of
hubs in the sense of Kleinberg?s HITS algorithm, as
well as the algorithmic similarity between Espresso
and HITS. Hub instances are influential and hence
make good seeds if they are of the target seman-
tic class, but otherwise, they may trigger semantic
drift. We have demonstrated that our method works
effectively on lexical sample tasks. We are currently
evaluating our method on other bootstrapping tasks,
including named entity extraction.
Acknowledgements
We thank Masayuki Asahara and Kazuo Hara for
helpful discussions and the anonymous reviewers
for valuable comments. MS was partially supported
by Kakenhi Grant-in-Aid for Scientific Research C
21500141.
34
References
Shuya Abe, Kentaro Inui, and Yuji Matsumoto. 2008.
Acquiring event relation knowledge by learning cooc-
currence patterns and fertilizing cooccurrence samples
with verbal nouns. In Proceedings of the 3rd Interna-
tional Joint Conference on Natural Language Process-
ing (IJCNLP ?08), pages 497?504.
Steven Abney. 2004. Understanding the Yarowsky algo-
rithm. Computational Linguistics, 30:365?395.
Krishna Bharat and Monika R. Henzinger. 1998. Im-
proved algorithms for topic distillation environment in
a hyperlinked. In Proceedings of the 21st Annual In-
ternational ACM SIGIR Conference on Research and
Development in Information Retrieval (SIGIR ?98),
pages 104?111.
Rebecca Bruce and Janyce Wiebe. 1994. Word-sense
disambiguation using decomposable models. In Pro-
ceedings of the 32nd Annual Meeting of the Associa-
tion for Computational Linguistics (ACL ?94), pages
139?146.
Stephen Clark, James R. Curran, and Miles Osborne.
2003. Bootstrapping POS taggers using unlabelled
data. In Proceedings of the 7th Conference on Natural
Language Learning (CoNLL ?03), pages 49?55.
Michael Collins and Yoram Singer. 1999. Unsupervised
models for named entity classification. In Proceedings
of the Joint SIGDAT Conference on Empirical Meth-
ods in Natural Language Processing and Very Large
Corpora (EMNLP-VLC ?99), pages 189?196.
James R. Curran, Tara Murphy, and Bernhard Scholz.
2007. Minimising semantic drift with mutual exclu-
sion bootstrapping. In Proceedings of the 10th Con-
ference of the Pacific Association for Computational
Linguistics (PACLING ?07), pages 172?180.
Marti A. Hearst. 1992. Automatic acquisition of hy-
ponyms from large text corpora. In Proceedings of the
14th Conference on Computational Linguistics (COL-
ING ?92), pages 539?545.
Ashwin Ittoo and Gosse Bouma. 2010. On learning
subtypes of the part-whole relation: do not mix your
seeds. In Proceedings of the 48th Annual Meeting of
the Association for Computational Linguistics (ACL
?10), pages 1328?1336.
Daniel Jurafsky and James H. Martin. 2008. Speech and
Language Processing. Prentice Hall, 2nd edition.
Jon M. Kleinberg. 1999. Authoritative sources in
a hyperlinked environment. Journal of the ACM,
46(5):604?632.
Mamoru Komachi and Hisami Suzuki. 2008. Minimally
supervised learning of semantic knowledge from query
logs. In Proceedings of the 3rd International Joint
Conference on Natural Language Processing (IJCNLP
?08), pages 358?365.
Mamoru Komachi, Taku Kudo, Masashi Shimbo, and
Yuji Matsumoto. 2008. Graph-based analysis of se-
mantic drift in Espresso-like bootstrapping algorithms.
In Proceedings of the Conference on Empirical Meth-
ods in Natural Language Processing (EMNLP ?08),
pages 1011?1020.
Claudia Leacock, Geoffrey Towell, and Ellen Voorhees.
1993. Corpus-based statistical sense resolution. In
Proceedings of the ARPA Workshop on Human Lan-
guage Technology (HLT ?93), pages 260?265.
Christopher D. Manning and Hinrich Schu?tze. 1999.
Foundations of Statistical Natural Language Process-
ing. MIT Press.
Christopher D. Manning, Prabhakar Raghavan, and Hin-
rich Schu?tze. 2008. Introduction to Information Re-
trieval. Cambridge University Press.
David McClosky, Eugene Charniak, and Mark Johnson.
2006. Effective self-training for parsing. In Proceed-
ings of the Human Language Technology Conference
of the North American Chapter of the Association of
Computational Linguistics (HLT-NAACL ?06), pages
152?159.
Tara McIntosh and James R. Curran. 2009. Reducing
semantic drift with bagging and distributional similar-
ity. In Proceedings of the Joint Conference of the 47th
Annual Meeting of the ACL and the 4th International
Joint Conference on Natural Language Processing of
the AFNLP (ACL-IJCNLP ?09), volume 1, pages 396?
404.
Tara McIntosh. 2010. Unsupervised discovery of nega-
tive categories in lexicon bootstrapping. In Proceed-
ings of the 2010 Conference on Empirical Methods
in Natural Language Processing (EMNLP ?10), pages
356?365.
Patrick Pantel and Marco Pennacchiotti. 2006. Espresso:
Leveraging generic patterns for automatically harvest-
ing semantic relations. In Proceedings of the 21st In-
ternational Conference on Computational Linguistics
and the 44th Annual Meeting of the Association for
Computational Linguistics (COLING-ACL ?06), pages
113?120.
M. F. Porter. 1980. An algorithm for suffix stripping.
Program, 14(3):130?137.
Ellen Riloff and Rosie Jones. 1999. Learning dictio-
naries for information extraction by multi-level boot-
strapping. In Proceedings of the 16th National Confer-
ence on Artificial Intelligence and the 11th Innovative
Applications of Artificial Intelligence (AAAI/IAAI ?99),
pages 474?479.
Mark Steedman, Rebecca Hwa, Stephen Clark, Miles Os-
borne, Anoop Sarkar, Julia Hockenmaier, Paul Ruhlen,
Steven Baker, and Jeremiah Crim. 2003. Example
35
selection for bootstrapping statistical parsers. In Pro-
ceedings of the 2003 Conference of the North Amer-
ican Chapter of the Association for Computational
Linguistics on Human Language Technology (HLT-
NAACL ?03), volume 1, pages 157?164.
Michael Thelen and Ellen Riloff. 2002. A bootstrapping
method for learning semantic lexicons using extraction
pattern contexts. In Proceedings of the ACL-02 Con-
ference on Empirical Methods in Natural Language
Processing (EMNLP ?02), pages 214?221.
Vishnu Vyas, Patrick Pantel, and Eric Crestan. 2009.
Helping editors choose better seed sets for entity set
expansion. In Proceeding of the 18th ACM Conference
on Information and Knowledge Management (CIKM
?09), pages 225?234.
Roman Yangarber, Winston Lin, and Ralph Grishman.
2002. Unsupervised learning of generalized names.
In Proceedings of the 19th International Conference
on Computational Linguistics (COLING ?02).
David Yarowsky. 1995. Unsupervised word sense dis-
ambiguation rivaling supervised methods. In Proceed-
ings of the 33rd Annual Meeting on Association for
Computational Linguistics (ACL ?95), pages 189?196.
Minoru Yoshida, Masaki Ikeda, Shingo Ono, Issei Sato,
and Hiroshi Nakagawa. 2010. Person name dis-
ambiguation by bootstrapping. In Proceeding of the
33rd International ACM SIGIR Conference on Re-
search and Development in Information Retrieval (SI-
GIR ?10), pages 10?17.
36
Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 198?202,
Jeju, Republic of Korea, 8-14 July 2012. c?2012 Association for Computational Linguistics
Tense and Aspect Error Correction for ESL Learners
Using Global Context
Toshikazu Tajiri Mamoru Komachi Yuji Matsumoto
Graduate School of Information Science
Nara Institute of Science and Technology
8916-5 Takayama, Ikoma, Nara, 630-0192, Japan
{toshikazu-t, komachi, matsu}@is.naist.jp
Abstract
As the number of learners of English is con-
stantly growing, automatic error correction of
ESL learners? writing is an increasingly ac-
tive area of research. However, most research
has mainly focused on errors concerning arti-
cles and prepositions even though tense/aspect
errors are also important. One of the main
reasons why tense/aspect error correction is
difficult is that the choice of tense/aspect is
highly dependent on global context. Previous
research on grammatical error correction typ-
ically uses pointwise prediction that performs
classification on each word independently, and
thus fails to capture the information of neigh-
boring labels. In order to take global infor-
mation into account, we regard the task as se-
quence labeling: each verb phrase in a doc-
ument is labeled with tense/aspect depending
on surrounding labels. Our experiments show
that the global context makes a moderate con-
tribution to tense/aspect error correction.
1 Introduction
Because of the growing number of learners of En-
glish, there is an increasing demand to help learn-
ers of English. It is highly effective for learners to
receive feedback on their essays from a human tu-
tor (Nagata and Nakatani, 2010). However, man-
ual feedback needs a lot of work and time, and it
also requires much grammatical knowledge. Thus,
a variety of automatic methods for helping English
learning and education have been proposed.
The mainstream of English error detection and
correction has focused on article errors (Knight and
Chander, 1994; Brockett et al, 2006) and preposi-
tion errors (Chodorow et al, 2007; Rozovskaya and
Roth, 2011), that commonly occur in essays by ESL
learners. On the other hand, tense and aspect errors
have been little studied, even though they are also
commonly found in learners? essays (Lee and Sen-
eff, 2006; Bitchener et al, 2005). For instance, Lee
(2008) corrects English verb inflection errors, but
they do not deal with tense/aspect errors because the
choice of tense and aspect highly depends on global
context, which makes correction difficult. Consider
the following sentences taken from a corpus of a
Japanese learner of English.
(1) I had a good time this Summer Vacation.
First, I *go to KAIYUKAN 1 with my friends.
In this example, go in the second sentence should
be written as went. It is difficult to correct this type
of error because there are two choices for correc-
tion, namely went and will go. In this case, we
can exploit global context to determine which cor-
rection is appropriate: the first sentence describes a
past event, and the second sentence refers the first
sentence. Thus, the verb should be changed to past
tense. This deduction is easy for humans, but is dif-
ficult for machines.
One way to incorporate such global context into
tense/aspect error correction is to use a machine
learning-based sequence labeling approach. There-
fore, we regard the task as sequence labeling:
each verb phrase in the document is labeled with
tense/aspect depending on surrounding labels. This
model naturally takes global context into account.
Our experiments show that global context makes a
moderate contribution to tense/aspect correction.
1Kaiyukan is an aquarium in Osaka, Japan.
198
2 Tense/Aspect Error Corpus
Developing a high-quality tense and aspect error
correction system requires a large corpus annotated
with tense/aspect errors. However, existing anno-
tated corpora are limited in size,2 which precludes
the possibility of machine learning-based approach.
Therefore, we constructed a large-scale tense/aspect
corpus from Lang-8,3 a social networking service
for learners of foreign languages. ESL learners post
their writing to be collaboratively corrected by na-
tive speakers. We leverage these corrections in creat-
ing our tense/aspect annotation. Lang-8 has 300,000
users from 180 countries worldwide, with more than
580,000 entries, approximately 170,000 of them
in English.4 After cleaning the data, the corpus
consists of approximately 120,000 English entries
containing 2,000,000 verb phrases with 750,000
verb phrases having corrections.5 The annotated
tense/aspect labels include 12 combinations of tense
(past, present, future) and aspect (nothing, perfect,
progressive, perfect progressive).
3 Error Correction Using Global Context
As we described in Section 1, using only local in-
formation about the target verb phrase may lead to
inaccurate correction of tense/aspect errors. Thus,
we take into account global context: the relation be-
tween target and preceding/following verb phrases.
In this paper, we formulate the task as sequence la-
beling, and use Conditional Random Fields (Laf-
ferty, 2001), which provides state-of-the-art perfor-
mance in sequence labeling while allowing flexible
feature design for combining local and global fea-
ture sets.
3.1 Local Features
Table 1 shows the local features used to train the er-
ror correction model.
2Konan-JIEM Learner Corpus Second Edition (http://
gsk.or.jp/catalog/GSK2011-B/catalog.html)
contains 170 essays, and Cambridge English First Certificate in
English (http://www.cambridgeesol.org/exams/
fce/index.html) contains 1244 essays.
3http://lang-8.com/
4As of January, 2012. More details about the Lang-8 corpus
can be found in (Mizumoto et al, 2011).
5Note that not all the 750,000 verb phrases were corrected
due to the misuse of tense/aspect.
Table 1: Local features for a verb phrase
name description
t-learn tense/aspect written by the learner
(surface tense/aspect)
bare the verb lemma
L the word to the left
R the word to the right
nsubj nominal subject
dobj direct object
aux auxiliary verb
pobj object of a preposition
p-tmod temporal adverb
norm-p-tmod normalized temporal adverb
advmod other adverb
conj subordinating conjunction
main-clause true if the target VP is in main clause
sub-clause true if the target VP is in subordinate clause
We use dependency relations such as nsubj, dobj,
aux, pobj, and advmod for syntactic features. If a
sentence including a target verb phrase is a complex
sentence, we use the conj feature and add either the
main-clause or the sub-clause feature depending on
whether the target verb is in the main clause or in a
subordinate clause. For example, the following two
sentences have the same features although they have
different structures.
(2) It pours when it rains.
(3) When it rains it pours.
In both sentences, we use the feature main-clause
for the verb phrase pours, and sub-clause for the
verb phrase rains along with the feature conj:when
for both verb phrases.
Regarding p-tmod, we extract a noun phrase in-
cluding a word labeled tmod (temporal adverb). For
instance, consider the following sentence containing
a temporal adverb:
(4) I had a good time last night.
In (4), the word night is the head of the noun phrase
last night and is a temporal noun,6 so we add the
feature p-tmod:last night for the verb phrase had.
Additionally, norm-p-tmod is a normalized form
of p-tmod. Table 2 shows the value of the fea-
ture norm-p-tmod and the corresponding tempo-
ral keywords. We use norm-p-tmod when p-tmod
6We made our own temporal noun list.
199
Table 2: The value of the feature norm-p-tmod and cor-
responding temporal keywords
temporal keywords value
yesterday or last past
now present
tomorrow or next future
today or this this
Table 3: Feature templates
Local Feature Templates
<head> <head, t-learn> <head, L, R> <L> <L, head>
<L, t-learn> <R> <R, head> <R, t-learn> <nsubj>
<nsubj, t-learn> <aux> <aux, head> <aux, t-learn>
<pobj> <pobj, t-learn> <norm-p-tmod>
<norm-p-tmod, t-learn> <advmod> <advmod, t-learn>
<tmod> <tmod, t-learn> <conj> <conj, t-learn>
<main-clause> <main-clause, t-learn>
<sub-clause> <sub-clause, t-learn>
<conj, main-clause> <conj, sub-clause>
Global Context Feature Templates
<p-tmod?> <p-tmod?, t-learn> <p-tmod?, t-learn?>
<p-tmod?, t-learn?, t-learn> <norm-p-tmod?>
<norm-p-tmod?, t-learn> <norm-p-tmod?, t-learn?>
<norm-p-tmod?, t-learn?, t-learn>
includes any temporal keywords. For instance, in
the sentence (4), we identify last night as temporal
adverb representing past, and thus create a feature
time:past for the verb phrase had.
3.2 Feature Template
Table 3 shows feature templates. <a> represents a
singleton feature and <a, b> represents a combina-
tion of features a and b. Also, a? means the feature
a of the preceding verb phrase. A local feature tem-
plate is a feature function combining features in the
target verb phrase, and a global context feature tem-
plate is a feature function including features from a
non-target verb phrase. Suppose we have following
learner?s sentences:
(5) I went to Kyoto yesterday.
I *eat yatsuhashi7 and drank green tea.
In (5), the verb before eat is went, and p-
tmod:yesterday and norm-p-tmod:past are added
to the feature set of verb went. Accordingly,
7Yatsuhashi is a Japanese snack.
Table 4: Example of global context feature functions gen-
erated by feature templates
<p-tmod?:yesterday>
<p-tmod?:yesterday, t-learn?:simple past>
<p-tmod?:yesterday, t-learn:simple present>
<p-tmod?:yesterday, t-learn?:simple past, t-learn:simple past>
<norm-p-tmod?:past>
<norm-p-tmod?:past, t-learn?:simple past>
<norm-p-tmod?:past, t-learn:simple present>
<norm-p-tmod?:past, t-learn?:simple past, t-learn:simple present>
the global context features p-tmod?:yesterday and
norm-p-tmod?:past are added to the verb eat.
Table 4 lists all the global context features for the
verb eat generated by the feature templates.
3.3 Trade-off between Precision and Recall
Use of surface tense/aspect forms of target verbs im-
proves precision but harms recall. This is because
in most cases the surface tense/aspect and the cor-
rect tense/aspect form of a verb are the same. It is,
of course, desirable to achieve high precision, but
very low recall leads to the system making no cor-
rections. In order to control the trade-off between
precision and recall, we re-estimate the best output
label y? based on the originally estimated label y as
follows:
y? = argmax
y
s(y)
s(y) =
{
?c(y), if y is the same as learner?s tense/aspect
c(y) otherwise.
where c(y) is the confidence value of y estimated
by the originally trained model (explained in 4.3),
and ? (0 ? ? < 1) is the weight of the surface
tense/aspect.
We first calculate c(y) of all the labels, and dis-
count only the label that is the same as learner?s
tense/aspect, and finally we choose the best output
label. This process leads to an increase of recall. We
call this method T-correction.
4 Experiments
4.1 Data and Feature Extraction
We used the Lang-8 tense/aspect corpus described
in Section 2. We randomly selected 100,000 entries
for training and 1,000 entries for testing. The test
200
00.2
0.40.6
0.81
0 0.2 0.4 0.6P
R
(a) tense
00.2
0.40.6
0.81
0 0.2 0.4 0.6P
R
(b) aspect
00.2
0.40.6
0.81
0 0.2 0.4 0.6P
R
(c) tense/aspect
Figure 1: Precision-Recall curve for error detection
00.2
0.40.6
0.81
0 0.2 0.4 0.6 P
R
(a) tense
00.2
0.40.6
0.81
0 0.2 0.4 0.6P
R
(b) aspect
00.2
0.40.6
0.81
0 0.2 0.4 0.6P
R
(c) tense/aspect
Figure 2: Precision-Recall curve for error correction
0 0.2 4 .6 0 8 1 
0 .2 0.4 0.6 0.8 1 SV M  M AXEN T  CRF  
data includes 16,308 verb phrases, of which 1,072
(6.6%) contain tense/aspect errors. We used Stan-
ford Parser 1.6.9 8 for generating syntactic features
and tense/aspect tagging.
4.2 Classifiers
Because we want to know the effect of using global
context information with CRF, we trained a one-
versus-rest multiclass SVM and a maximum entropy
classifier (MAXENT) as baselines.
We built a SVM model with LIBLINEAR 1.89
and a CRF and a MAXENT model with CRF++
0.54.10 We use the default parameters for each
toolkit.
In every method, we use the same features and
feature described in Section 3, and use T-correction
for choosing the final output. The confidence mea-
sure of the SVM is the distance to the separating hy-
perplane, and that of the MAXENT and the CRF is
the marginal probability of the estimated label.
8http://nlp.stanford.edu/software/
lex-parser.shtml
9http://www.csie.ntu.edu.tw/~cjlin/
liblinear/
10http://crfpp.sourceforge.net/
5 Results
Figures 1 and 2 show the Precision-Recall curves
of the error detection and correction performance of
each model. The figures are grouped by error types:
tense, aspect, and both tense and aspect. All figures
indicate that the CRF model achieves better perfor-
mance than SVM and MAXENT.
6 Analysis
We analysed the results of experiments with the ?
parameter of the CRF model set to 0.1. The most
frequent type of error in the corpus is using simple
present tense instread of simple past, with 211 in-
stances. Of these our system detected 61 and suc-
cessfully corrected 52 instances. However, of the
second most frequent error type (using simple past
instead of simple present), with 94 instances in the
corpus, our system only detected 9 instances. One
reason why the proposed method achieves high per-
formance in the first type of errors is that tense errors
with action verbs written as simple present are rela-
tively easy to detect.
201
References
John Bitchener, Stuart Young, and Denise Cameron.
2005. The Effect of Different Types of Corrective
Feedback on ESL Student Writing. Journal of Second
Language Writing, 14(3):191?205.
Chris Brockett, William B. Dolan, and Michael Gamon.
2006. Correcting ESL Errors Using Phrasal SMT
Techniques. In Proceedings of COLING-ACL, pages
249?256.
Martin Chodorow, Joel R. Tetreault, and Na-Rae Han.
2007. Detection of Grammatical Errors Involving
Prepositions. In Proceedings of ACL-SIGSEM, pages
25?30.
Kevin Knight and Ishwar Chander. 1994. Automated
Postediting of Documents. In Proceedings of the
AAAI?94, pages 779?784.
John Lafferty. 2001. Conditional Random Fields: Proba-
bilistic Models for Segmenting and Labeling Sequence
Data. In Proceedings of ICML, pages 282?289.
John Lee and Stephanie Seneff. 2006. Automatic Gram-
mar Correction for Second-Language Learners. In
Proceedings of the 9th ICSLP, pages 1978?1981.
John Lee and Stephanie Seneff. 2008. CorrectingMisuse
of Verb Forms. In Proceedings of the 46th ACL:HLT,
pages 174?182.
Tomoya Mizumoto, Mamoru Komachi, Masaaki Nagata,
and Yuji Matsumoto. 2011. Mining Revision Log of
Language Learning SNS for Automated Japanese Er-
ror Correction of Second Language Learners. In Pro-
ceedings of 5th IJCNLP, pages 147?155.
Ryo Nagata and Kazuhide Nakatani. 2010. Evaluating
Performance of Grammatical Error Detection to Max-
imize Learning Effect. In Proceedings of COLING,
pages 894?900.
Alla Rozovskaya and Dan Roth. 2011. Algorithm Selec-
tion and Model Adaptation for ESL Correction Tasks.
In Proceedings of the 49th ACL:HLT, pages 924?933.
202
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 238?242,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
Discriminative Approach to Fill-in-the-Blank Quiz Generation for
Language Learners
Keisuke Sakaguchi1? Yuki Arase2 Mamoru Komachi1?
1Nara Institute of Science and Technology
8916-5 Takayama, Ikoma, Nara, 630-0192, Japan
2Microsoft Research Asia
Bldg.2, No. 5 Danling St., Haidian Dist., Beijing, P. R. China
{keisuke-sa, komachi}@is.naist.jp, yukiar@microsoft.com
Abstract
We propose discriminative methods to
generate semantic distractors of fill-in-the-
blank quiz for language learners using a
large-scale language learners? corpus. Un-
like previous studies, the proposed meth-
ods aim at satisfying both reliability and
validity of generated distractors; distrac-
tors should be exclusive against answers
to avoid multiple answers in one quiz,
and distractors should discriminate learn-
ers? proficiency. Detailed user evaluation
with 3 native and 23 non-native speakers
of English shows that our methods achieve
better reliability and validity than previous
methods.
1 Introduction
Fill-in-the-blank is a popular style used for eval-
uating proficiency of language learners, from
homework to official tests, such as TOEIC1 and
TOEFL2. As shown in Figure 1, a quiz is com-
posed of 4 parts; (1) sentence, (2) blank to fill in,
(3) correct answer, and (4) distractors (incorrect
options). However, it is not easy to come up with
appropriate distractors without rich experience in
language education. There are two major require-
ments that distractors should satisfy: reliability
and validity (Alderson et al, 1995). First, distrac-
tors should be reliable; they are exclusive against
the answer and none of distractors can replace the
answer to avoid allowing multiple correct answers
in one quiz. Second, distractors should be valid;
they discriminate learners? proficiency adequately.
?This work has been done when the author was visiting
Microsoft Research Asia.
? Now at Tokyo Metropolitan University (Email: ko-
machi@tmu.ac.jp).
1http://www.ets.org/toeic
2http://www.ets.org/toefl
Each side, government and opposition, is _____   
the other for the political crisis, and for the violence.  
 
(a) blaming   (b) accusing   (c) BOTH 	

Figure 1: Example of a fill-in-the-blank quiz,
where (a) blaming is the answer and (b) accusing
is a distractor.
There are previous studies on distractor gener-
ation for automatic fill-in-the-blank quiz genera-
tion (Mitkov et al, 2006). Hoshino and Nakagawa
(2005) randomly selected distractors from words
in the same document. Sumita et al (2005) used
an English thesaurus to generate distractors. Liu et
al. (2005) collected distractor candidates that are
close to the answer in terms of word-frequency,
and ranked them by an association/collocation
measure between the candidate and surrounding
words in a given context. Dahlmeier and Ng
(2011) generated candidates for collocation er-
ror correction for English as a Second Language
(ESL) writing using paraphrasing with native lan-
guage (L1) pivoting technique. This method takes
an sentence containing a collocation error as in-
put and translates it into L1, and then translate it
back to English to generate correction candidates.
Although the purpose is different, the technique is
also applicable for distractor generation. To our
best knowledge, there have not been studies that
fully employed actual errors made by ESL learn-
ers for distractor generation.
In this paper, we propose automated distrac-
tor generation methods using a large-scale ESL
corpus with a discriminative model. We focus
on semantically confusing distractors that measure
learners? competence to distinguish word-sense
and select an appropriate word. We especially tar-
get verbs, because verbs are difficult for language
learners to use correctly (Leacock et al, 2010).
Our proposed methods use discriminative models238
Orig.	 I	 stop	 company	 on	 today	 .	Corr.	 I	 quit	 a	 company	 today	 .	Type	 NA	 #REP#	 #DEL#	 NA	 #INS#	 NA	 NA	
Figure 2: Example of a sentence correction pair
and error tags (Replacement, Deletion and Inser-
tion).
trained on error patterns extracted from an ESL
corpus, and can generate exclusive distractors with
taking context of a given sentence into considera-
tion.
We conduct human evaluation using 3 native
and 23 non-native speakers of English. The result
shows that 98.3% of distractors generated by our
methods are reliable. Furthermore, the non-native
speakers? performance on quiz generated by our
method has about 0.76 of correlation coefficient
with their TOEIC scores, which shows that dis-
tractors generated by our methods satisfy validity.
Contributions of this paper are twofold; (1) we
present methods for generating reliable and valid
distractors, (2) we also demonstrate the effective-
ness of ESL corpus and discriminative models on
distractor generation.
2 Proposed Method
To generate distractors, we first need to decide
which word to be blanked. We then generate can-
didates of distractors and rank them based on a
certain criterion to select distractors to output.
In this section, we propose our methods for ex-
tracting target words from ESL corpus and select-
ing distractors by a discriminative model that con-
siders long-distance context of a given sentence.
2.1 Error-Correction Pair Extraction
We use the Lang-8 Corpus of Learner English3 as
a large-scale ESL corpus, which consists of 1.2M
sentence correction pairs. For generating semantic
distractors, we regard a correction as a target and
the misused word as one of the distractor candi-
dates.
In the Lang-8 corpus, there is no clue to align
the original and corrected words. In addition,
words may be deleted and inserted in the corrected
sentence, which makes the alignment difficult.
Therefore, we detect word deletion, insertion, and
replacement by dynamic programming4. We com-
3http://cl.naist.jp/nldata/lang-8/
4The implementation is available at https:
//github.com/tkyf/epair
Feature Example
Word[i-2] ,
Word[i-1] is
Word[i+1] the
Word[i+2] other
Dep[i] child nsubj side, aux is, dobj other, prep for
Class accuse
Table 1: Example of features and class label ex-
tracted from a sentence: Each side, government
and opposition, is *accusing/blaming the other for
the political crisis, and for the violence.
pare a corrected sentence against its original sen-
tence, and when word insertion and deletion er-
rors are identified, we put a spaceholder (Figure
2). We then extract error-correction (i.e. replace-
ment) pairs by comparing trigrams around the re-
placement in the original and corrected sentences,
for considering surrounding context of the target.
These error-correction pairs are a mixture of gram-
matical mistakes, spelling errors, and semantic
confusions. Therefore, we identify pairs due to se-
mantic confusion; we exclude grammatical error
corrections by eliminating pairs whose error and
correction have different part-of-speech (POS)5,
and exclude spelling error corrections based on
edit-distance. As a result, we extract 689 unique
verbs (lemma) and 3,885 correction pairs in total.
Using the error-correction pairs, we calculate
conditional probabilities P (we|wc), which repre-
sent how probable that ESL learners misuse the
word wc as we. Based on the probabilities, we
compute a confusion matrix. The confusion ma-
trix can generate distractors reflecting error pat-
terns of ESL learners. Given a sentence, we iden-
tify verbs appearing in the confusion matrix and
make them blank, then outputs distractor candi-
dates that have high confusion probability. We
rank the candidates by a generative model to
consider the surrounding context (e.g. N-gram).
We refer to this generative method as Confusion-
matrix Method (CFM).
2.2 Discriminative Model for Distractor
Generation and Selection
To generate distractors that considers long-
distance context and reflects detailed syntactic in-
formation of the sentence, we train multiple clas-
sifiers for each target word using error-correction
pairs extracted from ESL corpus. A classifier for
5Because the Lang-8 corpus does not have POS tags, we
assign POS by the NLTK (http://nltk.org/) toolkit.239
a target word takes a sentence (in which the tar-
get word appears) as an input and outputs a verb
as the best distractor given the context using fol-
lowing features: 5-gram (?1 and ?2 words of the
target) lemmas and dependency type with the tar-
get child (lemma). The dependent is normalized
when it is a pronoun, date, time, or number (e.g. he
? #PRP#) to avoid making feature space sparse.
Table 1 shows an example of features and a class
label for the classifier of a target verb (blame).
These classifiers are based on a discriminative
model: Support Vector Machine (SVM)6 (Vapnik,
1995). We propose two methods for training the
classifiers.
First, we directly use the corrected sentences in
the Lang-8 corpus. As shown in Table 1, we use
the 5-gram and dependency features7, and use the
original word (misused word by ESL learners) as
a class. We refer to this method as DiscESL.
Second, we train classifiers with an ESL-
simulated native corpus, because (1) the number
of sentences containing a certain error-correction
pair is still limited in the ESL corpus and (2)
corrected sentences are still difficult to parse cor-
rectly due to inherent noise in the Lang-8 corpus.
Specifically, we use articles collected from Voice
of America (VOA) Learning English8, which con-
sist of 270k sentences. For each target in a given
sentence, we artificially change the target into an
incorrect word according to the error probabilities
obtained from the learners confusion matrix ex-
plained in Section 2.2. In order to collect a suf-
ficient amount of training data, we generate 100
samples for each training sentence in which the
target word is replaced into an erroneous word.
We refer to this method as DiscSimESL9.
3 Evaluation with Native-Speakers
In this experiment, we evaluate the reliability of
generated distractors. The authors asked the help
of 3 native speakers of English (1 male and 2 fe-
males, majoring computer science) from an au-
thor?s graduate school. We provide each partici-
pant a gift card of $30 as a compensation when
completing the task.
6We use Linear SVM with default settings in the scikit-
learn toolkit 0.13.1. http://scikit-learn.org
7We use the Stanford CoreNLP 1.3.4 http://nlp.
stanford.edu/software/corenlp.shtml
8http://learningenglish.voanews.com/
9The implementation is available at https:
//github.com/keisks/disc-sim-esl
Method Corpus Model
Proposed
CFM ESL Generative
DiscESL ESL Discriminative
DiscSimESL Pseudo-ESL Discriminative
Baseline
THM Native Generative
RTM Native Generative
Table 2: Summary of proposed methods (CFM:
Confusion Matrix Method, DiscESL: Discrimina-
tive model with ESL corpus, DiscSimESL: Dis-
criminative model with simulated ESL corpus)
and baseline (THM: Thesaurus Method, RTM:
Roundtrip Method).
In order to compare distractors generated by dif-
ferent methods, we ask participants to solve the
generated fill-in-the-blank quiz presented in Fig-
ure 1. Each quiz has 3 options: (a) only word A
is correct, (b) only word B is correct, (c) both are
correct. The source sentences to generate a quiz
are collected from VOA, which are not included in
the training dataset of the DiscSimESL. We gen-
erate 50 quizzes using different sentences per each
method to avoid showing the same sentence mul-
tiple times to participants. We randomly ordered
the quizzes generated by different methods for fair
comparison.
We compare the proposed methods to two base-
lines implementing previous studies: Thesaurus-
based Method (THM) and Roundtrip Translation
Method (RTM). Table 2 shows a summary of each
method. The THM is based on (Sumita et al,
2005) and extract distractor candidates from syn-
onyms of the target extracted from WordNet10.
The RTM is based on (Dahlmeier and Ng, 2011)
and extracts distractor candidates from roundtrip
(pivoting) translation lexicon constructed from the
WIT3 corpus (Cettolo et al, 2012)11, which cov-
ers a wide variety of topics. We build English-
Japanese and Japanese-English word-based trans-
lation tables using GIZA++ (IBM Model4). In
this dictionary, the target word is translated into
Japanese words and they are translated back to En-
glish as distractor candidates. To consider (local)
context, the candidates generated by the THM,
RTM, and CFM are re-ranked by 5-gram language
10WordNet 3.0 http://wordnet.princeton.
edu/wordnet/
11Available at http://wit3.fbk.eu240
Method RAD (%) ?
Proposed
CFM 94.5 (93.1 - 96.0) 0.55
DiscESL 95.0 (93.6 - 96.3) 0.73
DiscSimESL 98.3 (97.5 - 99.1) 0.69
Baseline
THM 89.3 (87.4 - 91.3) 0.57
RTM 93.6 (92.1 - 95.1) 0.53
Table 3: Ratio of appropriate distractors (RAD)
with a 95% confidence interval and inter-rater
agreement statistics ?.
model score trained on Google 1T Web Corpus
(Brants and Franz, 2006) with IRSTLM toolkit12.
As an evaluation metric, we compute the ratio
of appropriate distractors (RAD) by the following
equation: RAD = NAD/NALL, where NALL is
the total number of quizzes and NAD is the num-
ber of quizzes on which more than or equal to 2
participants agree by selecting the correct answer.
When at least 2 participants select the option (c)
(both options are correct), we determine the dis-
tractor as inappropriate. We also compute the av-
erage of inter-rater agreement ? among all partici-
pants for each method.
Table 3 shows the results of the first experiment;
RAD with a 95% confidence interval and inter-
rater agreement ?. All of our proposed methods
outperform baselines regarding RAD with high
inter-rater agreement. In particular, DiscSimESL
achieves 9.0% and 4.7% higher RAD than THM
and RTM, respectively. These results show that
the effectiveness of using ESL corpus to gener-
ate reliable distractors. With respect to ?, our
discriminative models achieve from 0.12 to 0.2
higher agreement than baselines, indicating that
the discriminative models can generate sound dis-
tractors more effectively than generative models.
The lower ? on generative models may be because
the distractors are semantically too close to the tar-
get (correct answer) as following examples:
The coalition has *published/issued a
report saying that ... .
As a result, the quiz from generative models is not
reliable since both published and issued are cor-
rect.
4 Evaluation with ESL Learners
In this experiment, we evaluate the validity of gen-
erated distractors regarding ESL learners? profi-
12The irstlm toolkit 5.80 http://sourceforge.
net/projects/irstlm/files/irstlm/
Method r Corr Dist Both Std
Proposed
CFM 0.71 56.7 29.6 13.5 11.5
DiscESL 0.48 62.4 27.9 10.4 12.8
DiscSimESL 0.76 64.0 20.7 15.1 13.4
Baseline
THM 0.68 57.2 28.1 14.6 10.7
RTM 0.67 63.4 26.9 9.5 13.2
Table 4: (1) Correlation coefficient r against par-
ticipants? TOEIC scores, (2) the average percent-
age of correct answer (Corr), incorrect answer of
distractor (Dist), and incorrect answer that both
are correct (Both) chosen by participants, and (3)
standard deviation (Std) of Corr.
300 400 500 600 700 800 900 1000TOEIC Score20
30
40
50
60
70
80
90
100
Accu
racy
(%)
DiscSimESLThesaurus (THM)
Figure 3: Correlation between the participants?
TOEIC scores and accuracy on THM and Disc-
SimESL.
ciency. Twenty-three Japanese native speakers (15
males and 8 females) are participated. All the par-
ticipants, who have taken at least 8 years of En-
glish education, self-report proficiency levels as
the TOEIC scores from 380 to 99013. All the par-
ticipants are graduate students majoring in science
related courses. We call for participants by e-
mailing to a graduate school. We provide each
participant a gift card of $10 as a compensation
when completing the task. We ask participants
to solve 20 quizzes per each method in the same
manner as Section 3. To evaluate validity of dis-
tractors, we use only reliable quizzes accepted in
Section 3. Namely, we exclude quizzes whose op-
tions are both correct. We evaluate correlation be-
tween learners? accuracy for the generated quizzes
and the TOEIC score.
Table 4 represents the results; the highest corre-
13The official score range of the TOEIC is from 10 to 990.241
lation coefficient r and standard deviation on Disc-
SimESL shows that its distractors achieve best va-
lidity. Figure 3 depicts the correlations between
the participants? TOEIC scores and accuracy (i.e.
Corr.) on THM and DiscSimESL. It illustrates that
DiscSimESL achieves higher level of positive cor-
relation than THM. Table 4 also shows high per-
centage of choosing ?(c) both are correct? on Disc-
SimESL, which indicates that distractors gener-
ated from DiscSimESL are difficult to distinguish
for ESL learners but not for native speakers as a
following example:
..., she found herself on stage ...
*playing/performing a number one hit.
A relatively lower correlation coefficient on
DiscESLmay be caused by inherent noise on pars-
ing the Lang-8 corpus and domain difference from
quiz sentences (VOA).
5 Conclusion
We have presented methods that automatically
generate semantic distractors of a fill-in-the-blank
quiz for ESL learners. The proposed methods em-
ploy discriminative models trained using error pat-
terns extracted from ESL corpus and can gener-
ate reliable distractors by taking context of a given
sentence into consideration. The human evalua-
tion shows that 98.3% of distractors are reliable
when generated by our method (DiscSimESL).
The results also demonstrate 0.76 of correlation
coefficient to their TOEIC scores, indicating that
the distractors have better validity than previous
methods. As future work, we plan to extend
our methods for other POS, such as adjective and
noun. Moreover, we will take ESL learners? pro-
ficiency into account for generating distractors of
appropriate levels for different learners.
Acknowledgments
This work was supported by the Microsoft Re-
search Collaborative Research (CORE) Projects.
We are grateful to Yangyang Xi for granting per-
mission to use text from Lang-8 and Takuya Fu-
jino for his error pair extraction algorithm. We
would also thank anonymous reviewers for valu-
able comments and suggestions.
References
Charles Alderson, Caroline Clapham, and DianneWall.
1995. Language Test Construction and Evaluation.
Cambridge University Press.
Thorsten Brants and Alex Franz. 2006. Web 1T 5-
gram Corpus version 1.1. Technical report, Google
Research.
Mauro Cettolo, Christian Girardi, and Marcello Fed-
erico. 2012. WIT3 : Web Inventory of Transcribed
and Translated Talks. In Proceedings of the 16th
Conference of the European Associattion for Ma-
chine Translation (EAMT), pages 261?268, Trent,
Italy, May.
Daniel Dahlmeier and Hwee Tou Ng. 2011. Cor-
recting semantic collocation errors with l1-induced
paraphrases. In Proceedings of the 2011 Conference
on Empirical Methods in Natural Language Pro-
cessing, pages 107?117, Edinburgh, Scotland, UK.,
July.
Ayako Hoshino and Hiroshi Nakagawa. 2005. A Real-
TimeMultiple-Choice Question Generation for Lan-
guage Testing ? A Preliminary Study ?. In Pro-
ceedings of the 2nd Workshop on Building Educa-
tional Applications Using NLP, pages 17?20, Ann
Arbor, June.
Claudia Leacock, Martin Chodorow, Michael Gamon,
and Joel R. Tetreault. 2010. Automated Grammat-
ical Error Detection for Language Learners. Syn-
thesis Lectures on Human Language Technologies.
Morgan & Claypool Publishers.
Chao-Lin Liu, Chun-Hung Wang, Zhao-Ming Gao,
and Shang-Ming Huang. 2005. Applications of
Lexical Information for Algorithmically Composing
Multiple-Choice Cloze Items. In Proceedings of the
2ndWorkshop on Building Educational Applications
Using NLP, pages 1?8, Ann Arbor, June.
Ruslan Mitkov, Le An Ha, and Nikiforos Karamanis.
2006. A Computer-Aided Environment for Generat-
ing Multiple-Choice Test Items. Natural Language
Engineering, 12:177?194, 5.
Eiichiro Sumita, Fumiaki Sugaya, and Seiichi Ya-
mamoto. 2005. Measuring Non-native Speak-
ers? Proficiency of English by Using a Test with
Automatically-Generated Fill-in-the-Blank Ques-
tions. In Proceedings of the 2nd Workshop on Build-
ing Educational Applications Using NLP, pages 61?
68, Ann Arbor, June.
Vladimir Vapnik. 1995. The Nature of Statistical
Learning Theory. Springer.
242
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 708?713,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
A Learner Corpus-based Approach to Verb Suggestion for ESL
Yu Sawai Mamoru Komachi?
Graduate School of Information Science
Nara Institute of Science and Technology
8916-5 Takayama, Ikoma, Nara, 630-0192, Japan
{yu-s, komachi, matsu}@is.naist.jp
Yuji Matsumoto
Abstract
We propose a verb suggestion method
which uses candidate sets and domain
adaptation to incorporate error patterns
produced by ESL learners. The candi-
date sets are constructed from a large scale
learner corpus to cover various error pat-
terns made by learners. Furthermore, the
model is trained using both a native cor-
pus and the learner corpus via a domain
adaptation technique. Experiments on two
learner corpora show that the candidate
sets increase the coverage of error patterns
and domain adaptation improves the per-
formance for verb suggestion.
1 Introduction
In this study, we address verb selection errors in
the writing of English learners. Selecting the right
verb based on the context of a sentence is difficult
for the learners of English as a Second Language
(ESL). This error type is one of the most common
errors in various learner corpora ranging from ele-
mentary to proficient levels1.
They ?connect/communicate with other
businessmen and do their jobs with the
help of computers.2
This sentence is grammatically acceptable with
either verb. However, native speakers of En-
glish would less likely use ?connect?, which
means ?forming a relationship (with other busi-
nessmen)?, whereas ?communicate? means ?ex-
changing information or ideas?, which is what the
sentence is trying to convey.
?Now at Tokyo Metropolitan University.
1For example, in the CLC-FCE dataset, the replacement
error of verbs is the third most common out of 75 error types.
In the KJ corpus, lexical choice of verb is the sixth most com-
mon out of 47 error types.
2This sentence is taken from the CLC-FCE dataset.
Previous work on verb selection usually treats
the task as a multi-class classification problem
(Wu et al, 2010; Wang and Hirst, 2010; Liu et
al., 2010; Liu et al, 2011). In this formaliza-
tion, it is important to restrict verbs by a candi-
date set because verb vocabulary is more numer-
ous than other classes, such as determiners. Can-
didate sets for verb selection are often extracted
from thesauri and/or round-trip translations. How-
ever, these resources may not cover certain error
patterns found in actual learner corpora, and suffer
from low-coverage. Furthermore, all the existing
classifier models are trained only using a native
corpus, which may not be adequate for correcting
learner errors.
In this paper, we propose to use error patterns
in ESL writing for verb suggestion task by using
candidate sets and a domain adaptation technique.
First, to increase the coverage, candidate sets are
extracted from a large scale learner corpus derived
from a language learning website. Second, a do-
main adaptation technique is applied to the model
to fill the gap between two domains: native cor-
pus and ESL corpus. Experiments are carried out
on publicly available learner corpora, the Cam-
bridge Learner Corpus First Certificate of English
dataset (CLC-FCE) and the Konan JIEM corpus
(KJ). The results show that the proposed candidate
sets improve the coverage, compared to the base-
line candidate sets derived from the WordNet and
a round-trip translation table. Domain adaptation
also boosts the suggestion performance.
To our knowledge, this is the first work for
verb suggestion that uses (1) a learner corpus as
a source of candidate sets and (2) the domain
adaptation technique to take learner errors into ac-
count.
708
2 Verb Suggestion Considering Error
Patterns
The proposed verb suggestion system follows the
standard approach in related tasks (Rozovskaya
and Roth, 2011; Wu et al, 2010), where the candi-
date selection is formalized as a multi-class classi-
fication problem with predefined candidate sets.
2.1 Candidate Sets
For reflecting tendency of learner errors to the can-
didate sets, we use a large scale corpus obtained
from learners? writing on an SNS (Social Net-
working Service), Lang-83. An advantage of using
the learner corpus from such website is the size of
annotated portion (Mizumoto et al, 2011). This
SNS has over 1 million manually annotated En-
glish sentences written by ESL learners. We have
collected the learner writings on the site, and re-
leased the dataset for research purpose4.
First, we performed POS tagging for the dataset
using the treebank POS tagger in the NLTK toolkit
2.10. Second, we extracted the correction pairs
which have ?VB*? tag. The set of correction pairs
given an incorrect verb is considered as a candi-
date set for the verb.
We then performed the following preprocessing
for the dataset because we focus on lexical selec-
tion of verbs:
? Lemmatize verbs to reduce data sparseness.
? Remove non-English verbs using WordNet.
? Remove incorrect verbs which occur only
once in the dataset.
The target verbs are limited to the 500 most
common verbs in the CLC-FCE corpus5. There-
fore, verbs that do not appear in the target list are
not included in the candidate sets. The topmost
500 verbs cover almost 90 percent of the vocabu-
lary of verbs in the CLC-FCE corpus6.
The average number of candidates in a set is
20.37. Note that the number of candidates varies
across each target verb8.
3http://lang-8.com
4Further details can be found at http://cl.naist.
jp/nldata/lang-8/. Candidate sets will also be avail-
able at the same URL.
5They are extracted from all ?VB? tagged tokens, and
they contain 1,292 unique verbs after removing non-English
words.
6This number excludes ?be?.
7In this paper, we limit the maximum number of candi-
dates in each set to 50.
8For instance, the candidate set for ?get? has 315 correc-
tion pairs, whereas ?refund? has only 4.
2.2 Suggestion Model
The verb suggestion model consists of multi-class
classifiers for each target verb; and based on the
classifiers? output, it suggests alternative verbs.
Instances are in a fill-in-the-blank format, where
the labels are verbs. Features in this format are
extracted from the surrounding context of a verb.
When testing on the learner corpus, the model sug-
gests a ranking of the possible verbs for the blank
corresponding to a given context. Note that un-
like the fill-in-the-blank task, the candidate sets
and domain adaptation can be applied to this task
to take the original word into account.
The model is trained on a huge native corpus,
namely the ukWaC corpus, because the data-size
of learner corpora is limited compared to native
corpora. It is then adapted to the target domain,
i.e., learner writing. In our experiment, the Lang-
8 corpus is used as the target domain corpus, since
we assume that it shares the same characteristics
with the CLC-FCE and the KJ corpora used for
testing.
2.3 Domain Adaptation
To adapt the models to the learner corpus, we em-
ploy a domain adaptation technique to emphasize
the importance of learner domain information. Al-
though there are many studies on domain adap-
tation, we chose to use Feature Augmentation
technique introduced by (Daume? III, 2007) for its
simplicity. Recently, (Imamura et al, 2012) pro-
posed to apply this method to grammatical error
correction for writings of Japanese learners and
confirmed that this is more effective for correct-
ing learner errors than simply adding the target do-
main instances.
In this study, the source domain is the native
writing, and the target domain is the ESL writing.
Our motivation is to use the ESL corpus together
with the huge native corpus to employ both an ad-
vantage of the size of training data and the ESL
writing specific features.
In this method, adapting a model to another
model is achieved by extending the feature space.
Given a feature vector of F dimensions as x ?
RF(F > 0), using simple mapping, the aug-
mented feature vectors for source and target do-
mains are obtained as follows,
Source domain: < xS, xS, 0 > (1)
Target domain: < xT, 0, xT > (2)
709
where 0 denotes a zero-vector of F dimensions.
The three partitions mean a common, a source-
specific, and a target-specific feature space. When
testing on the ESL corpora, the target-specific fea-
tures are emphasized.
2.4 Features
In previous work, various features were used: lex-
ical and POS n-grams, dependencies, and argu-
ments in the verb context. (Liu et al, 2011) has
shown that shallow parse features, such as lexi-
cal n-grams and chunks, work well in realistic set-
tings, in which the input sentence may not be cor-
rectly parsed. Considering this, we use shallow
parse features as context features for robustness.
The features include lexical and POS n-grams,
and lexical head words of the nearest NPs, and
clustering features of these head words. An ex-
ample of extracted features is shown in Table 2.4.
Note that those features are also used when ex-
tracting examples from the target domain dataset
(the learner domain corpus). As shown in Table
2.4, the n-gram features are 3-gram and extracted
from ?2 context window. The nearest NP?s head
features are divided into two (Left, Right).
The additional clustering features are used for
reducing sparseness, because the NP?s head words
are usually proper nouns. To create the word clus-
ters, we employ Brown clustering, a hierarchical
clustering algorithm proposed by (Brown et al,
1992). The structure of clusters is a complete bi-
nary tree, in which each node is represented as a
bit-string. By varying the length of the prefix of
bit-string, it is possible to change the granularity
of cluster representation. As illustrated in Table
2.4, we use the clustering features with three lev-
els of granularity: 256, 128, and 64 dimensions.
We used Percy Liang?s implementation9 to create
256 dimensional model from the ukWaC corpus,
which is used as the native corpus.
3 Experiments
Performance of verb suggestion is evaluated on
two error-tagged learner corpora: CLC-FCE and
KJ. In the experiments, we assume that the tar-
get verb and its context for suggestion are already
given.
For the experiment on the CLC-FCE dataset,
the targets are all words tagged with ?RV? (re-
9https://github.com/percyliang/
brown-cluster
Feature Example
n-grams they-*V*-with
(surface) <S>-they-*V*
*V*-with-other
n-grams PRP-*V*-IN
(POS) <S>-PRP-*V*
*V*-IN-JJ
NP head L they, L PRP
(Left, Right) R businessmen, R NNS
NP head cluster L 01110001, L 0111000, L 011100
(Left, Right) R 11011001, R 1101100, R 110110
(e.g., They (communicate) with other businessmen and do
their jobs with the help of computers.)
?<S>? denotes the beginning of the sentence, ?*V*?
denotes the blanked out verb.
Table 1: Example of extracted features as the fill-
in-the-blank form.
placement error of verbs). We assume that all the
verb selection errors are covered with this error
tag. All error tagged parts with nested correction
or multi-word expressions are excluded. The re-
sulting number of ?true? targets is 1,083, which
amounts to 4% of all verbs. Therefore the dataset
is highly skewed to correct usages, though this set-
ting expresses well the reality of ESL writing, as
shown in (Chodorow et al, 2012).
We carried out experiments with a variety of re-
sources used for creating candidate sets.
? WordNet
Candidates are retrieved from the synsets and
verbs sharing the same hypernyms in the
WordNet 3.0.
? LearnerSmall
Candidates are retrieved from following
learner corpora: NUS corpus of learner
English (NUCLE), Konan-JIEM (KJ), and
NICT Japanese learner English (JLE) corpus.
? Roundtrip
Candidates are collected by performing
?round-trip? translation, which is similar to
(Bannard and Callison-Burch, 2005) 10.
? WordNet+Roundtrip
A combination of the thesaurus-based and the
translation table-based candidate sets, similar
to (Liu et al, 2010) and (Liu et al, 2011).
? Lang-8
The proposed candidate sets obtained from a
large scale learner corpus.
? Lang-8+DA
Lang-8 candidate sets with domain adapta-
10Our roundtrip translation lexicons are built using a subset
of the WIT3 corpus (Cettolo et al, 2012), which is available
at http://wit3.fbk.eu.
710
Settings Candidates/set (Avg.)
WordNet 14.8
LearnerSmall 5.1
Roundtrip 50
Roundtrip (En-Ja-En) 50
WordNet+Roundtrip 50
Lang-8 20.3
Table 2: Comparison of candidate set size for each
setting.
tion via feature augmentation.
Table 3 shows a comparison of the average
number of candidates in each setting. In all config-
urations above, the parameters of the models un-
derlying the system are identical. We used a L2-
regularized generalized linear model with log-loss
function via Scikit-learn ver. 0.13.
Inter-corpus Evaluation
We also evaluate the suggestion performance on
the KJ corpus. The corpus contains diary-style
writing by Japanese university students. The pro-
ficiency of the learners ranges from elementary to
intermediate, so it is lower than that of the CLC-
FCE learners. The targets are all verbs tagged with
?v lxc? (lexical selection error of verbs).
To see the effect of L1 on the verb sugges-
tion task, we added an alternative setting for
the Roundtrip using only the English-Japanese
and Japanese-English round-trip translation tables
(En-Ja-En). For the experiment on this test-
corpus, the LearnerSmall is not included.
Datasets
The ukWaC web-corpus (Ferraresi et al, 2008) is
used as a native corpus for training the suggestion
model. Although this corpus consists of over 40
million sentences, 20,000 randomly selected sen-
tences are used for each verb11.
The Lang-8 learner corpus is used for domain
adaptation of the model in the Lang-8+DA config-
uration. The portion of data is the same as that
used for constructing candidate sets.
Metrics
Mean Reciprocal Rank (MRR) is used for evalu-
ating the performance of alternative suggestions.
The mean reciprocal rank is calculated by taking
11e.g., a classifier with a candidate set containing 50 verbs
is trained with 1 million sentences in total.
the average of the reciprocal ranks for each in-
stance. Given r goldi as the position of the gold
correction candidate in the suggestion list Si for i-
th checkpoint, the reciprocal rank RRi is defined
as,
RRi =
?
?
?
1
r goldi (goldi ? Si)
0 (otherwise)
(3)
4 Results
Tables 5 and 5 show the results of suggestion per-
formance on the CLC-FCE dataset and the KJ cor-
pus, respectively. In both cases, the Lang-8 and its
domain adaptation variant outperformed the oth-
ers. The coverage of error patterns in the tables
is the percentage of the cases where the sugges-
tion list includes the gold correction. Generally,
the suggestion performance and the coverage im-
prove as the size of the candidate sets increases.
5 Discussions
Although the expert-annotated learner corpora
contain candidates which are more reliable than
a web-crawled Lang-8 corpus, the Lang-8 setting
performed better as shown in Table 5. This can be
explained by the broader coverage by the Lang-8
candidate sets than that of the LearnerSmall. Sim-
ilarly, the WordNet performed the worst because
it contains only synonym-like candidates. We can
conclude that, for the verb suggestion task, the
coverage (recall) of candidate sets is more impor-
tant than the quality (precision).
We see little influence of learners? L1 in the re-
sults of Table 5, since the Roundtrip performed
better than the Roundtrip (En-Ja-En). As already
mentioned, the number of error patterns contained
in the candidate sets seems to have more impor-
tance than the quality.
As shown in Tables 5 and 5, a positive ef-
fect of domain adaptation technique appeared in
both test-corpora. In the case of the CLC-FCE,
280 out of 624 suggestions were improved com-
pared to the setting without domain adaptation.
For instance, confusions between synonyms such
as ??live/stay?, ??say/tell?, and ??solve/resolve?
are improved, because sentences containing these
confusions appear more frequently in the Lang-
8 corpus. Although the number of test-cases for
the KJ corpus is smaller than the CLC-FCE, we
can see the improvements for 33 out of 66 sug-
711
Settings MRR Coverage
WordNet 0.066 14.0 %
LearnerSmall 0.128 23.5 %
Roundtrip 0.185 48.1 %
WordNet+Roundtrip 0.173 48.1 %
Lang-8 0.220 57.6 %
Lang-8+DA 0.269* 57.6 %
The value marked with the asterisk indicates statistically sig-
nificant improvement over the baselines, where p < 0.05
bootstrap test.
Table 3: Suggestion performance on the CLC-
FCE dataset.
Settings MRR Coverage
WordNet 0.044 5.0 %
Roundtrip 0.241 53.8 %
Roundtrip (En-Ja-En) 0.188 38.8 %
WordNet+Roundtrip 0.162 53.8 %
Lang-8 0.253 68.9 %
Lang-8+DA 0.412* 68.9 %
The value marked with the asterisk indicates statistically sig-
nificant improvement over the baselines, except ?Roundtrip?,
where p < 0.05 bootstrap test.
Table 4: Suggestion performance on the KJ cor-
pus.
gestions. The improvements appeared for fre-
quent confusions of Japanese ESL learners such
as ??see/watch? and ??tell/teach?.
Comparing the results of the Lang-8+DA on
both test-corpora, the domain adaptation tech-
nique worked more effectively on the KJ cor-
pus than on the CLC-FCE. This can be explained
by the fact that the style of writing of the addi-
tional data, i.e., the Lang-8 corpus, is closer to
KJ than it is to CLC-FCE. More precisely, unlike
the examination-type writing style of CLC-FCE,
the KJ corpus consists of diary writing similar in
style to the Lang-8 corpus, and it expresses more
closely the proficiency of the learners.
We think that the next step is to refine the sug-
gestion models, since we currently take a simple
fill-in-the-blank approach. As future work, we
plan to extend the models as follows: (1) use both
incorrect and correct sentences in learner corpora
for training, and (2) employ ESL writing specific
features such as learners? L1 for domain adapta-
tion.
Acknowledgments
We thank YangYang Xi of Lang-8, Inc. for kindly
allowing us to use the Lang-8 learner corpus. We
also thank the anonymous reviewers for their in-
sightful comments. This work was partially sup-
ported by Microsoft Research CORE Project.
References
Colin Bannard and Chris Callison-Burch. 2005. Para-
phrasing with bilingual parallel corpora. In Pro-
ceedings of the 43rd Annual Meeting of the Associa-
tion for Computational Linguistics, pages 597?604.
Peter F Brown, Vincent J Della Pietra, Peter V DeS-
ouza, Jenifer C Lai, Robert L Mercer, and Vincent
J Della Pietra. 1992. Class-Based n-gram Models
of Natural Language. Computational Linguistics,
18(4):467?479, December.
M Cettolo, Christian Girardi, and Marcello Federico.
2012. WIT3: Web Inventory of Transcribed and
Translated Talks. In Proceedings of the 16th Con-
ference of the European Association for Machine
Translation (EAMT), pages 28?30.
Martin Chodorow, Markus Dickinson, Ross Israel, and
Joel Tetreault. 2012. Problems in Evaluating Gram-
matical Error Detection Systems. In Proceedings of
the 24th International Conference on Computational
Linguistics (Coling2012), pages 611?628.
Hal Daume? III. 2007. Frustratingly easy domain adap-
tation. In Proceedings of the 45th Annual Meeting of
the Association of Computational Linguistics, pages
256?263.
Adriano Ferraresi, Eros Zanchetta, and Marco Baroni.
2008. Introducing and evaluating ukWaC, a very
large web-derived corpus of English. In Proceed-
ings of the 4th Web as Corpus Workshop (WAC-4),
pages 45?54.
Kenji Imamura, Kuniko Saito, Kugatsu Sadamitsu, and
Hitoshi Nishikawa. 2012. Grammar error correc-
tion using pseudo-error sentences and domain adap-
tation. In Proceedings of the 50th Annual Meet-
ing of the Association for Computational Linguis-
tics, pages 388?392.
Xiaohua Liu, Bo Han, Kuan Li, Stephan Hyeonjun
Stiller, and Ming Zhou. 2010. SRL-based verb se-
lection for ESL. In Proceedings of the 2010 Con-
ference on Empirical Methods in Natural Language
Processing, pages 1068?1076.
Xiaohua Liu, Bo Han, and Ming Zhou. 2011. Cor-
recting verb selection errors for ESL with the per-
ceptron. In 12th International Conference on Intel-
ligent Text Processing and Computational Linguis-
tics, pages 411?423.
712
Tomoya Mizumoto, Mamoru Komachi, Masaaki Na-
gata, and Yuji Matsumoto. 2011. Mining revi-
sion log of language learning SNS for automated
Japanese error correction of second language learn-
ers. In Proceedings of the 5th International Joint
Conference on Natural Language Processing, pages
147?155.
Alla Rozovskaya and Dan Roth. 2011. Algorithm
selection and model adaptation for ESL correction
tasks. In Proceedings of the 49th Annual Meet-
ing of the Association for Computational Linguis-
tics, pages 924?933.
Tong Wang and Graeme Hirst. 2010. Near-synonym
lexical choice in latent semantic space. In Proceed-
ings of the 23rd International Conference on Com-
putational Linguistics (Coling 2010), pages 1182?
1190.
Jian-Cheng Wu, Yu-Chia Chang, Teruko Mitamura,
and Jason S Chang. 2010. Automatic collocation
suggestion in academic writing. In Proceedings of
the 48th Annual Meeting of the Association for Com-
putational Linguistics Short Papers, pages 115?119.
713
Proceedings of the Fifteenth Conference on Computational Natural Language Learning, pages 154?162,
Portland, Oregon, USA, 23?24 June 2011. c?2011 Association for Computational Linguistics
Using the Mutual k-Nearest Neighbor Graphs
for Semi-supervised Classification of Natural Language Data
Kohei Ozaki and Masashi Shimbo and Mamoru Komachi and Yuji Matsumoto
Nara Institute of Science and Technology
8916-5 Takayama, Ikoma, Nara 630-0192, Japan
{kohei-o,shimbo,komachi,matsu}@is.naist.jp
Abstract
The first step in graph-based semi-supervised
classification is to construct a graph from in-
put data. While the k-nearest neighbor graphs
have been the de facto standard method of
graph construction, this paper advocates using
the less well-known mutual k-nearest neigh-
bor graphs for high-dimensional natural lan-
guage data. To compare the performance
of these two graph construction methods, we
run semi-supervised classification methods on
both graphs in word sense disambiguation and
document classification tasks. The experi-
mental results show that the mutual k-nearest
neighbor graphs, if combined with maximum
spanning trees, consistently outperform the k-
nearest neighbor graphs. We attribute better
performance of the mutual k-nearest neigh-
bor graph to its being more resistive to mak-
ing hub vertices. The mutual k-nearest neigh-
bor graphs also perform equally well or even
better in comparison to the state-of-the-art
b-matching graph construction, despite their
lower computational complexity.
1 Introduction
Semi-supervised classification try to take advan-
tage of a large amount of unlabeled data in addi-
tion to a small amount of labeled data, in order to
achieve good classification accuracy while reducing
the cost of manually annotating data. In particular,
graph-based techniques for semi-supervised classi-
fication (Zhou et al, 2004; Zhu et al, 2003; Cal-
lut et al, 2008; Wang et al, 2008) are recognized
as a promising approach. Some of these techniques
have been successfully applied for NLP tasks: word
sense disambiguation (Alexandrescu and Kirchhoff,
2007; Niu et al, 2005), sentiment analysis (Gold-
berg and Zhu, 2006), and statistical machine trans-
lation (Alexandrescu and Kirchhoff, 2009), to name
but a few.
However, the focus of these studies is how to as-
sign accurate labels to vertices in a given graph. By
contrast, there has not been much work on how such
a graph should be built, and graph construction re-
mains ?more of an art than a science? (Zhu, 2005).
Yet, it is an essential step for graph-based semi-
supervised classification and (unsupervised) cluster-
ing, and the input graph affects the quality of final
classification/clustering results.
Both for semi-supervised classification and for
clustering, the k-nearest neighbor (k-NN) graph
construction has been used almost exclusively in the
literature. However, k-NN graphs often produce
hubs, or vertices with extremely high degree (i.e.,
the number of edges incident to a vertex). This ten-
dency is obvious especially if the original data is
high-dimensional?a characteristic typical of natu-
ral language data. In a later section, we demonstrate
that such hub vertices indeed deteriorate the accu-
racy of semi-supervised classification.
While not in the context of graph construction,
Radovanovic? et al (2010) made an insightful obser-
vation into the nature of hubs in high-dimensional
space; in their context, a hub is a sample close to
many other samples in the (high-dimensional) sam-
ple space. They state that such hubs inherently
emerge in high-dimensional data as a side effect of
the ?curse of dimensionality,? and argue that this is a
154
reason nearest neighbor classification does not work
well in high-dimensional space.
Their observation is insightful for graph con-
struction as well. Most of the graph-based semi-
supervised classification methods work by gradu-
ally propagating label information of a vertex to-
wards neighboring vertices in a graph, but the neigh-
borhood structure in the graph is basically deter-
mined by the proximity of data in the original high-
dimensional sample space. Hence, it is very likely
that a hub in the sample space also makes a hub
in the k-NN graph, since k-NN graph construction
greedily connects a pair of vertices if the sample cor-
responding to one vertex is among the k closest sam-
ples of the other sample in the original space. It is
therefore desirable to have an efficient graph con-
struction method for high-dimensional data that can
produce a graph with reduced hub effects.
To this end, we propose to use the mutual k-
nearest neighbor graphs (mutual k-NN graphs),
a less well-known variant of the standard k-NN
graphs. All vertices in a mutual k-NN graph have
a degree upper-bounded by k, which is not usually
the case with standard k-NN graphs. This property
helps not to produce vertices with extremely high
degree (hub vertices) in the graph. A mutual k-NN
graph is easy to build, at a time complexity identical
to that of the k-NN graph construction.
We first evaluated the quality of the graphs apart
from specific classification algorithms using the ?-
edge ratio of graphs. Our experimental results show
that the mutual k-NN graphs have a smaller num-
ber of edges connecting vertices with different la-
bels than the k-NN graphs, thus reducing the possi-
bility of wrong label information to be propagated.
We also compare the classification accuracy of two
standard semi-supervised classification algorithms
on the mutual k-NN graphs and the k-NN graphs.
The results show that the mutual k-NN graphs con-
sistently outperorm the k-NN graphs. Moreover, the
mutual k-NN graphs achieve equally well or bet-
ter classification accuracy than the state-of-the-art
graph construction method called b-matching (Je-
bara et al, 2009), while taking much less time to
construct.
2 Problem Statement
2.1 Semi-supervised Classification
The problem of semi-supervised classification can
be stated as follows. We are given a set of n ex-
amples, X = {x1, . . . ,xn}, but only the labels
of the first l examples are at hand; the remaining
u = n ? l examples are unlabeled examples. Let
S = {1, . . . , c} be the set of possible labels, and
yi ? S the label of xi, for i = 1, . . . , n. Since
we only know the labels of the first l examples, we
do not have access to yl+1, . . . , yn. For later conve-
nience, further let y = (y1, . . . , yn).
The goal of a semi-supervised classification al-
gorithm is to predict the hidden labels yl+1, . . . , yn
of u unlabeled examples xl+1, . . . ,xn, given
these unlabeled examples and l labeled data
(x1, y1), . . . , (xl, yl). A measure of similarity be-
tween examples is also provided to the algorithm.
Stated differently, the classifier has access to an all-
pair similarity matrix W ? of size n ? n, with its
(i, j)-element W ?ij holding the similarity of exam-
ples xi and xj . It is assumed that W ? is a symmetric
matrix, and the more similar two examples are (with
respect to the similarity measure), more likely they
are to have the same label. This last assumption is
the premise of many semi-supervised classification
algorithms and is often called the cluster assumption
(Zhou et al, 2004).
2.2 Graph-based Semi-supervised
Classification
Graph-based approaches to semi-supervised classi-
fication are applicable if examples X are graph ver-
tices. Otherwise, X must first be converted into a
graph. This latter case is the focus of this paper.
That is, we are interested in how to construct a graph
from the examples, so that the subsequent classifica-
tion works well.
Let G denote the graph constructed from the ex-
amples. Naturally, G has n vertices, since vertices
are identified with examples. Instead of graph G it-
self, let us consider its real-valued (weighted) adja-
cency matrix W , of size n ? n. The task of graph
construction then reduces to computing W from all-
pairs similarity matrix W ?.
The simplest way to compute W from W ? is to
let W = W ?, which boils down to using a dense,
155
complete graph G with the unmodified all-pairs sim-
ilarity as its edge weights. However, it has been ob-
served that a sparseW not only save time needed for
classification, but also results in better classification
accuracy1 than the full similarity matrix W ? (Zhu,
2008). Thus, we are concerned with how to sparsify
W ? to obtain a sparseW ; i.e., the strategy of zeroing
out some elements of W ?.
Let the set of binary values be B = {0, 1}. A spar-
sification strategy can be represented by a binary-
valued matrix P ? Bn?n, where Pij = 1 if W ?ij
must be retained as Wij , and Pij = 0 if Wij = 0.
Then, the weighted adjacency matrix W of G is
given by Wij = PijW ?ij . The n ? n matrices W
and P are symmetric, reflecting the fact that most
graph-based algorithms require the input graph to be
undirected.
3 k-Nearest Neighbor Graphs and the
Effect of Hubs
The standard approach to making a sparse graph G
(or equivalently, matrix W ) is to construct a k-NN
graph from the data (Szummer and Jaakkola, 2002;
Niu et al, 2005; Goldberg and Zhu, 2006).
3.1 The k-Nearest Neighbor Graphs
The k-NN graph is a weighted undirected graph con-
necting each vertex to its k-nearest neighbors in the
original sample space. Building a k-NN graph is a
two step process. First we solve the following opti-
mization problem.
max
P??Bn?n
?
i,j
P?ijW
?
ij (1)
s.t.
?
j
P?ij = k, P?ii = 0, ?i, j ? {1, . . . , n}
Note that we are trying to find P? , and not P . This
is an easy problem and we can solve it by greedily
assigning P?ij = 1 only if W ?ij is among the top k
elements in the ith row of W ? (in terms of the mag-
nitude of the elements). After P? is determined, we
let Pij = max(P?ij , P?ji). Thus P is a symmetric
matrix, i.e., Pij = Pji for all i and j, while P? may
1See also the experimental results of Section 6.3.2 in which
the full similarity matrix W ? is used as the baseline.
d 1 2 ? 3 total
# of vertices 1610 1947 164 3721
original 65.9 65.7 69.8 66.0
hub-removed 66.6 66.0 69.8 66.4
Table 1: Classification accuracy of vertices around hubs
in a k-NN graph, before (?original?) and after (?hub-
removed?) hubs are removed. The value d represents the
shortest distance (number of hops) from a vertex to its
nearest hub vertex in the graph.
not. Finally, weighted adjacency matrix W is deter-
mined byWij = PijW ?ij . MatrixW is also symmet-
ric since P and W ? are symmetric.
This process is equivalent to retaining all edges
from each vertex to its k-nearest neighbor vertices,
and then making all edges undirected.
Note the above symmetrization step is necessary
because the k-nearest neighbor relation is not sym-
metric; even if a vertex vi is a k-nearest neighbor of
another vertex vj , vj may or may not be a k-nearest
neighbor of vi. Thus, symmetrizing P and W as
above makes the graph irregular; i.e., the degree of
some vertices may be larger than k, which opens the
possibility of hubs to emerge.
3.2 Effect of Hubs on Classification
In this section, we demonstrate that hubs in k-NN
graphs are indeed harmful to semi-supervised clas-
sification as we claimed earlier. To this end, we
eliminate such high degree vertices from the graph,
and compare the classification accuracy of other ver-
tices before and after the elimination. For this pre-
liminary experiment, we used the ?line? dataset of
a word sense disambiguation task (Leacock et al,
1993). For details of the dataset and the task, see
Section 6.
In this experiment, we randomly selected 10 per-
cent of examples as labeled examples. The remain-
ing 90 percent makes the set of unlabeled examples,
and the goal is to predict the label (word sense) of
these unlabeled examples.
We first built a k-NN graph (with k = 3)
from the dataset, and ran Gaussian Random Fields
(GRF) (Zhu et al, 2003), one of the most widely-
used graph-based semi-supervised classification al-
gorithms. Then we removed vertices with degree
156
greater than or equal to 30 from the k-NN graph,
and ran GRF again on this ?hub-removed? graph.
Table 1 shows the classification accuracy of GRF
on the two graphs. The table shows both the over-
all classification accuracy, and the classification ac-
curacy on the subsets of vertices, stratified by their
distance d from the nearest hub vertices (which were
eliminated in the ?hub-removed? graph). Obvi-
ously, overall classification accuracy has improved
after hub removal. Also notice that the increase in
the classification accuracy on the vertices nearest to
hubs (d = 1, 2). These results suggest that the pres-
ence of hubs in the graph is deteriorating classifica-
tion accuracy.
4 Mutual k-Nearest Neighbor Graphs for
Semi-supervised Classification
As demonstrated in Section 3.2, removing hub ver-
tices in k-NN graphs is an easy way of improv-
ing the accuracy of semi-supervised classification.
However, this method adds another parameter to the
graph construction method, namely, the threshold on
the degree of vertices to be removed. The method
also does not tell us how to assign labels to the re-
moved (hub) vertices. Hence, it is more desirable
to have a graph construction method which has only
one parameter just like the k-NN graphs, but is at the
same time less prone to produce hub vertices.
In this section, we propose to use mutual k-NN
graphs for this purpose.
4.1 Mutual k-Nearest Neighbor Graphs
The mutual k-NN graph is not a new concept and
it has been used sometimes in clustering. Even in
clustering, however, they are not at all as popular as
the ordinary k-NN graphs. A mutual k-NN graph
is defined as a graph in which there is an edge be-
tween vertices vi and vj if each of them belongs to
the k-nearest neighbors (in terms of the original sim-
ilarity metric W ) of the other vertex. By contrast, a
k-NN graph has an edge between vertices vi and vj
if one of them belongs to the k-nearest neighbors of
the other. Hence, the mutual k-NN graph is a sub-
graph of the k-NN graph computed from the same
data with the same value of k. The mutual k-NN
graph first optimizes the same formula as (1), but in
mutual k-NN graphs, the binary-valued symmetric
matrix P is defined as Pij = min(P?ij , P?ji). Since
mutual k-NN graph construction guarantees that all
vertices in the resulting graph have degree at most
k, it is less likely to produce extremely high degree
vertices in comparison with k-NN graphs, provided
that the value of k is kept adequately small.
4.2 Fixing Weak Connectivity
Because the mutual k-NN graph construction is
more selective of edges than the standard k-NN
graphs, the resulting graphs often contain many
small disconnected components. Disconnected
components are not much of a problem for clus-
tering (since its objective is to divide a graph into
discrete components eventually), but can be a prob-
lem for semi-supervised classification algorithms; if
a connected component does not contain a labeled
node, the algorithms cannot reliably predict the la-
bels of the vertices in the component; recall that
these algorithms infer labels by propagating label in-
formation along edges in the graph.
As a simple method for overcoming this problem,
we combine the mutual k-NN graph and the max-
imum spanning tree. To be precise, the minimum
number of edges from the maximum spanning tree
are added to the mutual k-NN graph to ensure that
only one connected component exists in a graph.
4.3 Computational Efficiency
Using a Fibonacci heap-based implementation
(Fredman and Tarjan, 1987), one can construct
the standard k-NN graph in (amortized) O(n2 +
kn log n) time. A mutual k-NN graph can also be
constructed in the same time complexity as the k-
NN graphs. The procedure below transforms a stan-
dard k-NN graph into a mutual k-NN graph. It uses
Fibonacci heaps once again and assumes that the in-
put k-NN graph is represented as an adjacency ma-
trix in sparse matrix representation.
1. Each vertex is associated with its own heap.
For each edge e connecting vertices u and v,
insert e to the heaps associated with u and v.
2. Fetch maximum weighted edges from each
heap k times, keeping globally the record of
the number of times each edge is fetched. No-
tice that an edge can be fetched at most twice,
157
once at an end vertex of the edge and once at
the other end.
3. A mutual k-NN graph can be constructed by
only keeping edges fetched twice in the previ-
ous step.
The complexity of this procedure is O(kn). Hence
the overall complexity of building a mutual k-NN
graph is dominated by the time needed to build
the standard k-NN graph input to the system; i.e.,
O(n2 + kn log n).
If we call the above procedure on an approximate
k-NN graph which can be computed more efficiently
(Beygelzimer et al, 2006; Chen et al, 2009; Ram
et al, 2010; Tabei et al, 2010), it yields an ap-
proximate mutual k-NN graphs. In this case, the
overall complexity is identical to that of the ap-
proximate k-NN graph construction algorithm, since
these approximate algorithms have a complexity at
least O(kn).
5 Related Work
5.1 b-Matching Graphs
Recently, Jebara et al (2009) proposed a new
graph construction method called b-matching. A b-
matching graph is a b-regular graph, meaning that
every vertex has the degree b uniformly. It can be ob-
tained by solving the following optimization prob-
lem.
max
P?Bn?n
?
ij
PijW
?
ij
s.t.
?
j
Pij = b, ?i ? {1, . . . , n} (2)
Pii = 0, ?i ? {1, . . . , n} (3)
Pij = Pji, ?i, j ? {1, . . . , n} (4)
After P is computed, the weighted adjacency matrix
W is determined by Wij = PijW ?ij The constraint
(4) makes the binary matrix P symmetric, and (3) is
to ignore self-similarity (loops). Also, the constraint
(2) ensures that the graph is regular. Note that k-NN
graphs are in general not regular. The regularity re-
quirement of the b-matching graphs can be regarded
as an effort to avoid the hubness phenomenon dis-
cussed by Radovanovic? et al (2010).
Figure 1: Two extreme cases of ?-edge ratio. Vertex
shapes (and colors) denote the class labels. The ?-edge
ratio of the graph on the left is 1, meaning that all edges
connect vertices with different labels. The ?-edge ratio
of the one on the right is 0, because all edges connect
vertices of the same class.
Jebara et al (2009) reported that b-matching
graphs achieve semi-supervised classification accu-
racy higher than k-NN graphs. However, with-
out approximation, building a b-matching graph
is prohibitive in terms of computational complex-
ity. Huang and Jebara (2007) developed a fast im-
plementation based on belief propagation, but the
guaranteed running time of the implementation is
O(bn3), which is still not practical for large scale
graphs. Notice that the k-NN graphs and mutual k-
NN graphs can be constructed with much smaller
time complexity, as we mentioned in Section 4.3.
In Section exp, we empirically compare the per-
formance of mutual k-NN graphs with that of b-
matching graphs.
5.2 Mutual Nearest Neighbor in Clustering
In the clustering context, mutual k-NN graphs have
been theoretically analyzed by Maier et al (2009)
with Random Geometric Graph Theory. Their study
suggests that if one is interested in identifying the
most significant clusters only, the mutual k-NN
graphs give a better clustering result. However, it is
not clear what their results imply in semi-supervised
classification settings.
6 Experiments
We compare the k-NN, mutual k-NN, and b-
matching graphs in word sense disambiguation and
document classification tasks. All of these tasks are
multi-class classification problems.
6.1 Datasets
We used two word sense disambiguation datasets in
our experiment: ?interest? and ?line.? The ?inter-
est? data is originally taken from the POS-tagged
158
interest dataset
number of edges (x 103)
phi?
edg
e ra
tio
0.10
0.15
0.20
0.25
ll
llll
llll
lllll
llllll
lllllll
l
0 5 10 15 20 25 30 35
l bMG
kNNG
MkNNG
line dataset
number of edges (x 103)
phi?
edg
e ra
tio
0.15
0.20
0.25
0.30
0.35
lll
llll
llll
lllllll
llll
0 10 20 30 40
l bMG
kNNG
MkNNG
Reuters dataset
number of edges (x 103)
phi?
edg
e ra
tio
0.10
0.12
0.14
0.16
0.18
0.20
0.22
0.24
ll
lll
lll
llll
llllllll
llllllll
0 10 20 30 40 50
l bMG
kNNG
MkNNG
20 newsgroups dataset
number of edges (x 103)
phi?
edg
e ra
tio
0.15
0.20
0.25
0.30
0.35
0.40
0 50 100 150 200 250
kNNG
MkNNG
Figure 2: ?-edge ratios of the k-NN graph, mutual k-NN graph, and b-matching graphs. The ?-edge ratio of a graph
is a measure of how much the cluster assumption is violated; hence, smaller the ?-edge ratio, the better. The plot for
b-matching graph is missing for the 20 newsgroups dataset, because its construction did not finish after one week for
this dataset.
dataset examples features labels
interest 2,368 3,689 6
line 4,146 8,009 6
Reuters 4,028 17,143 4
20 newsgroups 19,928 62,061 20
Table 2: Datasets used in experiments.
portion of the Wall Street Journal Corpus. Each in-
stance of the polysemous word ?interest? has been
tagged with one of the six senses in Longman Dic-
tionary of Contemporary English. The details of the
dataset are described in Bruce and Wiebe (1994).
The ?line? data is originally used in numerous com-
parative studies of word sense disambiguation. Each
instance of the word ?line? has been tagged with one
of the six senses on the WordNet thesaurus. Further
details can be found in the Leacock et al (1993).
Following Niu et al (2005), we used the following
context features in the word sense disambiguation
tasks: part-of-speech of neighboring words, single
words in the surrounding context, and local colloca-
tion. Details of these context features can be found
in Lee and Ng (2002).
The Reuters dataset is extracted from RCV1-
v2/LYRL2004, a text categorization test collection
(Lewis et al, 2004). In the same manner as Cram-
mer et al (2009), we produced the classification
dataset by selecting approximately 4,000 documents
from 4 general topics (corporate, economic, gov-
ernment and markets) at random. The features de-
scribed in Lewis et al (2004) are used with this
dataset.
The 20 newsgroups dataset is a popular dataset
frequently used for document classification and
clustering. The dataset consists of approximately
20,000 messages on newsgroups and is originally
distributed by Lang (1995). Each message is as-
signed one of the 20 possible labels indicating which
newsgroup it has been posted to, and represented as
binary bag-of-words features as described in Rennie
(2001).
Table 2 summarizes the characteristics of the
datasets used in our experiments.
6.2 Experimental Setup
Our focus in this paper is a semi-supervised classi-
fication setting in which the dataset contains a small
amount of labeled examples and a large amount of
unlabeled examples. To simulate such settings, we
create 10 sets of labeled examples, with each set
consisting of randomly selected l examples from the
original dataset, where l is 10 percent of the total
number of examples. For each set, the remaining
90 percent constitute the unlabeled examples whose
labels must be inferred.
After we build a graph from the data using one
of the graph construction methods discussed earlier,
a graph-based semi-supervised classification algo-
rithm must be run on the resulting graph to infer la-
bels to the unlabeled examples (vertices). We use
two most frequently used classification algorithms:
Gaussian Random Fields (GRF) (Zhu et al, 2003)
and the Local/Global Consistency algorithm (LGC)
(Zhou et al, 2004). Averaged classification accuracy
is used as the evaluation metric. For all datasets, co-
159
interest dataset (GRF)
number of edges (x 103)
av
era
ged
 ac
cur
ac
y
0.79
0.80
0.81
0.82
0.83
ll
llllllllllllllllll
l
ll
lll
0 5 10 15 20 25 30 35
l bMG
kNNG
MkNNG
interest dataset (LGC)
number of edges (x 103)
av
era
ged
 ac
cur
ac
y
0.79
0.80
0.81
0.82
0.83
ll
llllllll
lllllllllll
l
ll
lll
0 5 10 15 20 25 30 35
l bMG
kNNG
MkNNG
line dataset (GRF)
number of edges (x 103)
av
era
ged
 ac
cur
ac
y
0.62
0.64
0.66
0.68
0.70
lllllllllllll
l
llllll
0 10 20 30 40
l bMG
kNNG
MkNNG
line dataset (LGC)
number of edges (x 103)
av
era
ged
 ac
cur
ac
y
0.62
0.64
0.66
0.68
0.70
lllllllllllll
l
llllll
0 10 20 30 40
l bMG
kNNG
MkNNG
Figure 3: Averaged classification accuracies for k-NN graphs, b-matching graphs and mutual k-NN graphs (+ maxi-
mum spanning trees) in the interest and line datasets.
sine similarity is used as the similarity measure be-
tween examples.
In ?interest? and ?line? datasets, we compare the
performance of the graph construction methods over
the broad range of their parameters; i.e., b in b-
matching graphs and k in (mutual) k-NN graphs.
In Reuters and the 20 newsgroups datasets, 2-fold
cross validation is used to determine the hyperpa-
rameters (k and b) of the graph construction meth-
ods; i.e., we split the labeled data into two folds, and
used one fold for training and the other for develop-
ment, and then switch the folds in order to find the
optimal hyperparameter among k, b ? {2, . . . , 50}.
The smoothing parameter ? of LGC is fixed at ? =
0.9.
6.3 Results
6.3.1 Comparison of ?-Edge Ratio
We first compared the ?-edge ratios of k-NN
graphs, mutual k-NN graphs, and b-matching graphs
to evaluate the quality of the graphs apart from spe-
cific classification algorithms.
For this purpose, we define the ?-edge ratio as the
yardstick to measure the quality of a graph. Here, a
?-edge of a labeled graph (G,y) is any edge (vi, vj)
for which yi 6= yj (Cesa-Bianchi et al, 2010), and
we define the ?-edge ratio of a graph as the number
of ?-edges divided by the total number of edges in
the graph. Since most graph-based semi-supervised
classification methods propagate label information
along edges, edges connecting vertices with differ-
ent labels may lead to misclassification. Hence, a
graph with a smaller ?-edge ratio is more desirable.
Figure 1 illustrates two toy graphs with extreme val-
ues of ?-edge ratio.
Figure 2 shows the plots of ?-edge ratios of the
compared graph construction methods when the val-
ues of parameters k (for k-NN and mutual k-NN
graphs) and b (for b-matching graphs) are varied. In
these plots, the y-axes denote the ?-edge ratio of the
constructed graphs. The x-axes denote the number
of edges in the constructed graphs, and not the val-
ues of parameters k or b, because setting parameters
b and k to an equal value does not achieve the same
level of sparsity (number of edges) in the resulting
graphs.
As mentioned earlier, the smaller the ?-edge ra-
tio, the more desirable. As the figure shows, mu-
tual k-NN graphs achieve smaller ?-edge ratio than
other graphs if they are compared at the same level
of graph sparsity.
The plot for b-matching graph is missing for the
20 newsgroups data, because we were unable to
complete its construction in one week2. Meanwhile,
a k-NN graph and a mutual k-NN graph for the same
dataset can be constructed in less than 15 minutes on
the same computer.
6.3.2 Classification Results
Figure 3 shows the classification accuracy of GRF
and LGC on the different types of graphs con-
structed for the interest and line datasets. As in Fig-
ure 2, the x-axes represent the sparsity of the con-
structed graphs measured by the number of edges in
the graph, which can change as the hyperparameter
(b or k) of the compared graph construction methods
2All experiments were run on a machine with 2.3 GHz AMD
Opteron 8356 processors and 256 GB RAM.
160
kNN graph b-matching graph mutual kNN graph
dataset alorithm Dense MST original +MST original +MST original +MST
Reuters GRF 43.65 72.74 81.70 80.89 84.04 84.04 85.01 84.72
Reuters LGC 43.66 71.78 82.60 82.60 84.42 84.42 84.81 84.85
20 newsgroups GRF 10.18 66.96 75.47 75.47 ?? ?? 76.31 76.46
20 newsgroups LGC 14.51 65.82 75.19 75.19 ?? ?? 75.27 75.41
Table 3: Document classification accuracies for k-NN graphs, b-matching graphs, and mutual k-NN graphs. The col-
umn for ?Dense? is the result for the graph with the original similarity matrix W ? as the adjacency matrix; i.e., without
using any graph construction (sparsification) methods. The column for ?MST? is the result the for the maximum span-
ning tree. b-matching graph construction did not complete after one week on the 20 newsgroups data, and hence no
results are shown.
vs. kNNG vs. bMG
dataset (algo) orig +MST orig +MST
Reuters (GRF)   > ?
Reuters (LGC)   ? ?
20 newsgroups (GRF)   ?? ??
20 newsgroups (LGC) ? > ?? ??
Table 4: One-sided paired t-test results of averaged ac-
curacies between using mutual k-NN graphs and other
graphs. ??, ?>?, and ??? correspond to p-value <
0.01, (0.01, 0.05], and > 0.05 respectively.
are varied.
As shown in the figure, the combination of mu-
tual k-NN graphs and the maximum spanning trees
achieves better accuracy than other graph construc-
tion methods in most cases, when they are com-
pared at the same levels of graph sparsity (number
of edges).
Table 3 summarizes the classification accuracy on
the document classification datasets. As a baseline,
the table also shows the results (?Dense?) on the
dense complete graph with the original all-pairs sim-
ilarity matrix W ? as the adjacency matrix (i.e., no
graph sparsification), as well as the results for us-
ing the maximum spanning tree alone as the graph
construction method.
In all cases, mutual k-NN graphs achieve better
classification accuracy than other graphs.
Table 4 reports the one-sided paired t-test results
of averaged accuracies with k-NN graphs and b-
matching graphs against our proposed approach, the
combination of mutual k-NN graphs and maximum
spanning trees. From Table 4, we see that mutual
k-NN graphs perform significantly better than k-
NN graphs. On the other hand, theere is no signifi-
cant difference in the accuracy of the mutual k-NN
graphs and b-matching graphs. However, mutual
k-NN graphs achieves the same level of accuracy
with b-matching graphs, at much less computation
time and are applicable to large datasets. As men-
tioned earlier, mutual k-NN graphs can be computed
with less than 15 minutes in the 20 newsgroups data,
while b-matching graphs cannot be computed in one
week.
7 Conclusion
In this paper, we have proposed to use mutual k-
NN graphs instead of the standard k-NN graphs for
graph-based semi-supervised learning. In mutual k-
NN graphs, all vertices have degree upper bounded
by k. We have demonstrated that this type of
graph construction alleviates the hub effects stated
in Radovanovic? et al (2010), which also makes the
graph more consistent with the cluster assumption.
In addition, we have shown that the weak connectiv-
ity of mutual k-NN graphs is not a serious problem
if we augment the graph with maximum spanning
trees. Experimental results on various natural lan-
guage processing datasets show that mutual k-NN
graphs lead to higher classification accuracy than the
standard k-NN graphs, when two popular label in-
ference methods are run on these graphs.
References
Andrei Alexandrescu and Katrin Kirchhoff. 2007. Data-
driven graph construction for semi-supervised graph-
based learning in NLP. In Proc. of HLT-NAACL.
161
Andrei Alexandrescu and Katrin Kirchhoff. 2009.
Graph-based learning for statistical machine transla-
tion. In Proc. of NAACL-HLT.
Alina Beygelzimer, Sham Kakade, and John Langford.
2006. Cover trees for nearest neighbor. In Proc. of
ICML.
Rebecca Bruce and Janyce Wiebe. 1994. Word-sense
disambiguation using decomposable models. In Proc.
of ACL.
Je?ro?me Callut, Kevin Franc?oisse, Marco Saerens, and
Pierre Dupont. 2008. Semi-supervised classification
from discriminative random walks. In Proc. of ECML-
PKDD.
Nicolo Cesa-Bianchi, Claudio Gentile, Fabio Vitale, and
Giovanni Zappella. 2010. Random spanning trees and
the prediction of weighted graphs. In Proc. of ICML.
Jie Chen, Haw-ren Fang, and Yousef Saad. 2009. Fast
approximate kNN graph construction for high dimen-
sional data via recursive lanczos bisection. Journal of
Machine Learning Research, 10.
Koby Crammer, Mark Dredze, and Alex Kulesza. 2009.
Multi-class confidence weighted algorithms. In Proc.
of EMNLP.
Michael L. Fredman and Robert Endre Tarjan. 1987. Fi-
bonacci heaps and their uses in improved network op-
timization algorithms. J. ACM, 34:596?615, July.
Andrew B. Goldberg and Xiaojin Zhu. 2006. Seeing
stars when there aren?t many stars: graph-based semi-
supervised learning for sentiment categorization. In
Proc. of TextGraphs Workshop on HLT-NAACL.
Bert Huang and Tony Jebara. 2007. Loopy belief prop-
agation for bipartite maximum weight b-matching. In
Proc. of AISTATS.
Tony Jebara, Jun Wang, and Shih-Fu Chang. 2009.
Graph construction and b-matching for semi-
supervised learning. In Proc. of ICML.
Ken Lang. 1995. Newsweeder: Learning to filter net-
news. In Proc. of ICML.
Claudia Leacock, Geoffrey Towell, and Ellen Voorhees.
1993. Corpus-based statistical sense resolution. In
Proc. of ARPA Workshop on HLT.
Yoong Keok Lee and Hwee Tou Ng. 2002. An empir-
ical evaluation of knowledge sources and learning al-
gorithms for word sense disambiguation. In Proc. of
EMNLP.
David D. Lewis, Yiming Yang, Tony G. Rose, Fan Li,
G. Dietterich, and Fan Li. 2004. RCV1: A new bench-
mark collection for text categorization research. Jour-
nal of Machine Learning Research, 5.
Markus Maier, Matthias Hein, and Ulrike von Luxburg.
2009. Optimal construction of k-nearest-neighbor
graphs for identifying noisy clusters. Journal of Theo-
retical Computer Science, 410.
Zheng-Yu Niu, Dong-Hong Ji, and Chew Lim Tan. 2005.
Word sense disambiguation using label propagation
based semi-supervised learning. In Proc. of ACL.
Milos? Radovanovic?, Alexandros Nanopoulos, and Mir-
jana Ivanovic?. 2010. Hub in space: popular nearest
neighbors in high-dimensional data. Journal of Ma-
chine Learning Research, 11.
Parikshit Ram, Dongryeol Lee, William March, and
Alexander Gray. 2010. Linear-time algorithms for
pairwise statistical problems. In Proc. of NIPS.
Jason D. M. Rennie. 2001. Improving multi-class text
classification with naive bayes. Master?s thesis, Mas-
sachusetts Institute of Technology. AITR-2001-004.
Martin Szummer and Tommi Jaakkola. 2002. Partially
labeled classification with markov random walks. In
Proc. of NIPS.
Yasuo Tabei, Takeaki Uno, Masashi Sugiyama, and Koji
Tsuda. 2010. Single versus multiple sorting in all
pairs similarity search. In Proc. of ACML.
Jun Wang, Tony Jebara, and Shih-Fu. Chang. 2008.
Graph transduction via alternating minimization. In
Proc. of ICML.
Dengyong Zhou, Olivier Bousquet, Thomas Navin Lal,
Jason Weston, and Bernhard Scho?lkopf. 2004. Learn-
ing with local and global consistency. In Proc. of
NIPS.
Xiaojin Zhu, Zoubin Ghahramani, and John D. Lafferty.
2003. Semi-supervised learning using gaussian fields
and harmonic functions. In Proc. of ICML.
Xiaojin Zhu. 2005. Semi-Supervised Learning with
Graphs. Ph.D. thesis, Carnegie Mellon University.
CMU-LTI-05-192.
Xiaojin Zhu. 2008. Semi-supervised learning literature
survey. Technical Report 1530, Computer Sciences,
University of Wisconsin-Madison.
162
Proceedings of the 15th Conference on Computational Natural Language Learning: Shared Task, pages 86?92,
Portland, Oregon, 23-24 June 2011. c?2011 Association for Computational Linguistics
Narrative Schema as World Knowledge for Coreference Resolution
Joseph Irwin
Nara Institute of
Science and Technology
Nara Prefecture, Japan
joseph-i@is.naist.jp
Mamoru Komachi
Nara Institute of
Science and Technology
Nara Prefecture, Japan
komachi@is.naist.jp
Yuji Matsumoto
Nara Institute of
Science and Technology
Nara Prefecture, Japan
matsu@is.naist.jp
Abstract
In this paper we describe the system with
which we participated in the CoNLL-2011
Shared Task on modelling coreference. Our
system is based on a cluster-ranking model
proposed by Rahman and Ng (2009), with
novel semantic features based on recent re-
search on narrative event schema (Chambers
and Jurafsky, 2009). We demonstrate some
improvements over the baseline when using
schema information, although the effect var-
ied between the metrics used. We also explore
the impact of various features on our system?s
performance.
1 Introduction
Coreference resolution is a problem for automated
document understanding. We say two segments of
a natural-language document corefer when they re-
fer to the same real-world entity. The segments of
a document which refer to an entity are called men-
tions. In coreference resolution tasks, mentions are
usually restricted to noun phrases.
The goal of the CoNLL-2011 Shared Task (Prad-
han et al, 2011) is to model unrestricted coreference
using the OntoNotes corpus. The OntoNotes cor-
pus is annotated with several layers of syntactic and
semantic information, making it a rich resource for
investigating coreference resolution (Pradhan et al,
2007).
We participated in both the ?open? and ?closed?
tracks. The ?closed? track requires systems to only
use the provided data, while the ?open? track al-
lows use of external data. We created a baseline
system based on the cluster-ranking model proposed
by Rahman and Ng (2009). We then experimented
with adding novel semantic features derived from
co-referring predicate-argument chains. These nar-
rative schema were developed by Chambers and Ju-
rafsky (2009). They are described in more detail in
a later section.
2 Related Work
Supervised machine-learning approaches to corefer-
ence resolution have been researched for almost two
decades. Recently, the state of the art seems to be
moving away from the early mention-pair classifica-
tion model toward entity-based models. Ng (2010)
provides an excellent overview of the history and re-
cent developments within the field.
Both entity-mention and mention-pair models are
formulated as binary classification problems; how-
ever, ranking may be a more natural approach to
coreference resolution (Ng, 2010; Rahman and Ng,
2009). Rahman and Ng (2009) in particular pro-
pose the cluster-ranking model which we used in our
baseline. In another approach, Daume? and Marcu
(2005) apply their Learning as Search Optimization
framework to coreference resolution, and show good
results.
Feature selection is important for good perfor-
mance in coreference resolution. Ng (2010) dis-
cusses commonly used features, and analyses of
the contribution of various features can be found in
(Daume? and Marcu, 2005; Rahman and Ng, 2011;
Ponzetto and Strube, 2006b). Surprisingly, Rahman
and Ng (2011) demonstrated that a system using al-
most exclusively lexical features could outperform
86
systems which used more traditional sets of features.
Although string features have a large effect on
performance, it is recognized that the use of seman-
tic information is important for further improvement
(Ng, 2010; Ponzetto and Strube, 2006a; Ponzetto
and Strube, 2006b; Haghighi and Klein, 2010). The
use of predicate-argument structure has been ex-
plored by Ponzetto and Strube (2006b; 2006a).
3 Narrative Schema for Coreference
Narrative schema are extracted from large-scale cor-
pora using coreference information to identify pred-
icates whose arguments often corefer. Similarity
measures are used to build up schema consisting
of one or more event chains ? chains of typically-
coreferring predicate arguments (Chambers and Ju-
rafsky, 2009). Each chain corresponds to a role in
the schema.
A role defines a class of participants in the
schema. Conceptually, if a schema is present in a
document, than each role in the schema corresponds
to an entity in the document. An example schema is
shown with some typical participants in Figure 1. In
this paper the temporal order of events in the schema
is not considered.
prohibit
require
allow
bar
violate
subj. obj.
law, bill, rule,
amendment
company, mi-
crosoft, govern-
ment, banks
Figure 1: An example narrative schema with two roles.
Narrative schema are similar to the script con-
cept put forth by Schank and Abelson (1977). Like
scripts, narrative schema can capture complex struc-
tured information about events described in natural
language documents (Schank and Abelson, 1977;
Abelson, 1981; Chambers and Jurafsky, 2009).
We hypothesize that narrative schema can be a
good source of information for making coreference
decisions. One reason they could be useful is that
they can directly capture the fact that arguments of
certain predicates are relatively more likely to refer
to the same entity. In fact, they can capture global
information about verbs ranging over the entire doc-
ument, which we expect may lead to greater accu-
racy when combined with the incremental clustering
algorithm we employ.
Additionally, the information that two predicates
often share arguments yields semantic information
about the argument words themselves. For exam-
ple, if the subjects of the verbs eat and drink often
corefer, we may be able to infer that words which
occur in the subject position of these verbs share
some property (e.g., animacy). This last conjec-
ture is somewhat validated by Ponzetto and Strube
(2006b), who reported that including predicate-
argument pairs as features improved the perfor-
mance of a coreference resolver.
4 System Description
4.1 Overview
We built a coreference resolution system based on
the cluster-ranking algorithm proposed by Rahman
and Ng (2009). During document processing main-
tains a list of clusters of coreferring mentions which
are created iteratively. Our system uses a determin-
istic mention-detection algorithm that extracts can-
didate NPs from a document. We process the men-
tions in order of appearance in the document. For
each mention a ranking query is created, with fea-
tures generated from the clusters created so far. In
each query we include a null-cluster instance, to al-
low joint learning of discourse-new detection, fol-
lowing (Rahman and Ng, 2009).
For training, each mention is assigned to its cor-
rect cluster according to the coreference annota-
tion. The resulting queries are used to train a
classification-based ranker.
In testing, the ranking model thus learned is used
to rank the clusters in each query as it is created;
the active mention is assigned to the cluster with the
highest rank.
A data-flow diagram for our system is shown in
Figure 2.
87
Document
Mention
Extraction
Feature
Extraction
Entities
Narrative
Schema
Database
Cluster
Ranking
Figure 2: System execution flow
4.2 Cluster-ranking Model
Our baseline system uses a cluster-ranking model
proposed by Rahman and Ng (2009; 2011). In this
model, clusters are iteratively constructed after con-
sidering each active mention in a document in order.
During training, features are created between the ac-
tive mention and each cluster created so far. A rank
is assigned such that the cluster which is coreferent
to the active mention has the highest value, and each
non-coreferent cluster is assigned the same, lower
rank (The exact values are irrelevant to learning a
ranking; for the experiments in this paper we used
the values 2 and 1). In this way it is possible to
learn to preferentially rank correct clustering deci-
sions higher.
For classification, instances are constructed ex-
actly the same way as for training, except that for
each active mention, a query must be constructed
and ranked by the classifier in order to proceed with
the clustering. After the query for each active men-
tion has been ranked, the mention is assigned to the
cluster with the highest ranking, and the algorithm
proceeds to the next mention.
4.3 Notation
In the following sections, mk is the active mention
currently being considered, mj is a candidate an-
tecedent mention, and cj is the cluster to which it
belongs. Most of the features used in our system ac-
tually apply to a pair of mentions (i.e., mk and mj)
or to a single mention (either mk or mj). To cre-
ate a training or test instance using mk and cj , the
features which apply to mj are converted to cluster-
level features by a procedure described in 4.6.
4.4 Joint Anaphoric Mention Detection
We follow Rahman and Ng (2009) in jointly learn-
ing to detect anaphoric mentions along with resolv-
ing coreference relations. For each active mention
mk, an instance for a ?null? cluster is also created,
with rank 2 if the mention is not coreferent with
any preceding mention, or rank 1 if it has an an-
tecedent. This allows the ranker the option of mak-
ing mk discourse-new. To create this instance, only
the features which involve just mk are used.
4.5 Features
The features used in our system are shown in Table
1. For the NE features we directly use the types from
the OntoNotes annotation. 1
4.6 Making Cluster-Level Features
Each feature which applies to mj must be converted
to a cluster-level feature. We follow the proce-
dure described in (Rahman and Ng, 2009). This
procedure uses binary features whose values corre-
spond to being logically true or false. Multi-valued
features are first converted into equivalent sets of
binary-valued features. For each binary-valued fea-
ture, four corresponding cluster-level features are
created, whose values are determined by four logical
1The set of types is: PERSON, NORP, FACILITY, ORGA-
NIZATION, GPE, LOCATION, PRODUCT, EVENT, WORK,
LAW, LANGUAGE, DATE, TIME, PERCENT, MONEY,
QUANTITY, ORDINAL, CARDINAL
88
Features involving mj only
SUBJECT Y if mj is the grammatical subject of a verb; N otherwise
*NE_TYPE1 the NE label for mj if there is one else NONE
Features involving mk only
DEFINITE Y if the first word of mk is the; N otherwise
DEMONSTRATIVE Y if the first word of mk is one of this, that, these, or those; N otherwise
DEF_DEM_NA Y if neither DEFINITE nor DEMONSTRATIVE is Y; N otherwise
PRONOUN2 Y if mk is a personal pronoun; N otherwise
PROTYPE2 nominative case of mk if mk is a pronoun or NA if it is not (e.g., HE if mk is him)
NE_TYPE2 the NE label for mk if there is one
Features involving both mj and mk
DISTANCE how many sentences separate mj and mk; the values are A) same sentence, B) previous sentence,
and C) two sentences ago or more
HEAD_MATCH Y if the head words are the same; N otherwise
PRONOUN_MATCH if either of mj and mk is not a pronoun, NA; if the nominative case of mj and mk is the same, C; I
otherwise
*NE_TYPE? the concatenation of the NE labels of mj and mk (if either or both are not labelled NEs, the feature
is created using NONE as the corresponding label)
SCHEMA_PAIR_MATCH Y if mj and mk appear in the same role in a schema, and N if they do not
Features involving cj and mk
SCHEMA_CLUSTER_MATCH a cluster-level feature between mk and cj (details in Section 4.7)
Table 1: Features implemented in our coreference resolver. Binary-valued features have values of YES or NO. Multi-
valued features are converted into equivalent sets of binary-valued features before being used to create the cluster-level
features used by the ranker.
predicates: NONE, MOST-FALSE, MOST-TRUE,
and ALL.
To be precise, a feature F may be thought of as a
function taking mj as a parameter, e.g., F (mj). To
simplify notation, features which apply to the pair
mj ,mk take mk as an implicit parameter. The log-
ical predicates then compare the two counts n =
|{mj | F (mj) = true}| and C = |cj |. The re-
sulting features are shown in Table 2.
NONE F TRUE iff n = 0
MOST-FALSE F TRUE iff n < C2
MOST-TRUE F TRUE iff C2 ? n < C
ALL F TRUE iff n = C
Table 2: Cluster-level features created from binary-
valued feature F
The two features marked with * are treated
differently. For each value of NE_TYPE1 and
NE_TYPE?, a new cluster-level feature is cre-
ated whose value is the number of times that fea-
ture/value appeared in the cluster (i.e., if there were
two PERSON NEs in a cluster then the feature
NE_TYPE1_PERSON would have the value 2).
4.7 SCHEMA_CLUSTER_MATCH
The SCHEMA_CLUSTER_MATCH feature is ac-
tually three features, which are calculated over an
entire candidate antecedent cluster cj . First a list is
created of all of the schema roles which the men-
tions in cj participate in, and sorted in decreasing
order according to how many mentions in cj par-
ticipate in each. Then, the value of the feature
SCHEMA_CLUSTER_MATCHn is Y if mention
mk also participates in the nth schema role in the
list, for n = 1, 2, 3. If it does not, or if the corre-
sponding nth schema role has fewer than two partic-
ipants in cj , the value of this feature is N.
4.8 Implementation Details
Our system was implemented in Python, in order to
make use of the NLTK library2. For the ranker we
used SVMrank, an efficient implementation for train-
ing ranking SVMs (Joachims, 2006) 3.
2http://www.nltk.org/
3http://svmlight.joachims.org/
89
R P F1
MUC 12.45% 50.60% 19.98
CLOSED B3 35.07% 89.90% 50.46
CEAF 45.84% 17.38% 25.21
Overall score: 31.88
MUC 18.56% 51.01% 27.21
OPEN B3 38.97% 85.57% 53.55
CEAF 43.33% 19.36% 26.76
Overall score: 35.84
Table 3: Official system results
5 Experiments and Results
5.1 CoNLL System Submission
We submitted two results to the CoNLL-2011
Shared Task. In the ?closed? track we submitted the
results of our baseline system without the schema
features, trained on all documents in both the train-
ing and development portions of the OntoNotes cor-
pus.
We also submitted a result in the ?open? track:
a version of our system with the schema features
added. Due to issues with the implementation of this
second version, however, we were only able to sub-
mit results from a model trained on just the WSJ por-
tion of the training dataset. For the schema features,
we used a database of narrative schema released by
Chambers and Jurafsky (2010) ? specifically the list
of schemas of size 12. 4
The official system scores for our system are
listed in Table 3. We can attribute some of the low
performance of our system to features which are too
noisy, and to having not enough features compared
to the large size of the dataset. It is likely that these
two factors adversely impact the ability of the SVM
to learn effectively. In fact, the features which we in-
troduced partially to provide more features to learn
with, the NE features, had the worst impact on per-
formance according to later analysis. Because of a
problem with our implementation, we were unable
to get an accurate idea of our system?s performance
until after the submission deadline.
4Available at http://cs.stanford.edu/people/nc/schemas/
R P F1
MUC 12.77% 57.66% 20.91
Baseline B3 35.1% 91.05% 50.67
CEAF 47.80% 17.29% 25.40
MUC 12.78% 54.84% 20.73
+SCHEMA B3 35.75% 90.39% 51.24
CEAF 46.62% 17.43% 25.38
Table 4: Schema features evaluated on the development
set. Training used the entire training dataset.
5.2 Using Narrative Schema as World
Knowledge for Coreference Resolution
We conducted an evaluation of the baseline without
schema features against a model with both schema
features added. The results are shown in Table 4.
The results were mixed, with B3 going up and
MUC and CEAF falling slightly. Cross-validation
using just the development set showed a more posi-
tive picture, however, with both MUC and B3 scores
increasing more than 1 point (p = 0.06 and p <
0.01, respectively), and CEAF increasing about 0.5
points as well (although this was not significant at
p > 0.1). 5
One problem with the schema features that we
had anticipated was that they may have a problem
with sparseness. We had originally intended to ex-
tract schema using the coreference annotation in
OntoNotes, predicting that this would help alleviate
the problem; however, due to time constraints we
were unable to complete this effort.
5.3 Feature Analysis
We conducted a feature ablation analysis on our
baseline system to better understand the contribu-
tion of each feature to overall performance. The
results are shown in Table 5. We removed fea-
tures in blocks of related features; -HEAD removes
HEAD MATCH; -DIST removes the DISTANCE
feature; -SUBJ is the baseline system without SUB-
JECT; -PRO is the baseline system without PRO-
NOUN2, PROTYPE2, and PRONOUN MATCH;
-DEF DEM removes DEFINITE, DEMONSTRA-
TIVE, and DEF DEM NA; and -NE removes the
named entity features.
5All significance tests were performed with a two-tailed t-
test.
90
MUC 12.77% 57.66% 20.91
Baseline B3 35.1% 91.05% 50.67
CEAF 47.80% 17.29% 25.40
R P F1 ?F1
MUC 0.00% 33.33% 0.01 -20.90
-HEAD B3 26.27% 99.98% 41.61 -9.06
CEAF 52.88% 13.89% 22.00 -3.40
MUC 0.39% 60.86% 0.79 -20.12
-DIST B3 26.59% 99.72% 41.99 -8.68
CEAF 52.76% 13.99% 22.11 -3.29
MUC 12.47% 47.69% 19.78 -1.13
-SUBJ B3 36.54% 87.80% 51.61 0.94
CEAF 43.75% 17.22% 24.72 -0.68
MUC 18.36% 55.98% 27.65 6.74
-PRO B3 37.45% 85.78% 52.14 1.47
CEAF 47.86% 19.19% 27.40 2.00
MUC 18.90% 51.72% 27.68 6.77
-DEF_DEM B3 41.65% 86.11% 56.14 5.47
CEAF 46.39% 21.61% 29.48 4.08
MUC 22.76% 49.5% 31.18 10.27
-NE B3 46.78% 84.92% 60.33 9.66
CEAF 45.65% 25.19% 32.46 7.06
Table 5: Effect of each feature on performance.
The fact that for three of the features, removing
the feature actually improved performance is trou-
bling. Possibly these features were too noisy; we
need to improve the baseline features for future ex-
periments.
6 Conclusions
Semantic information is necessary for many tasks in
natural language processing. Most often this infor-
mation is used in the form of relationships between
words ? for example, how semantically similar two
words are, or which nouns are the objects of a verb.
However, it is likely that humans make use of much
higher-level information than the similarity between
two concepts when processing language (Abelson,
1981). We attempted to take advantage of recent de-
velopments in automatically aquiring just this sort
of information, and demonstrated the possibility of
making use of it in NLP tasks such as coreference.
However, we need to improve both the implementa-
tion and data for this approach to be practical.
For future work, we intend to investigate avenues
for improving the aquisition and use of the narra-
tive schema information, and also compare narra-
tive schema with other types of semantic informa-
tion in coreference resolution. Because coreference
information is central to the extraction of narrative
schema, the joint learning of coreference resolution
and narrative schema is another area we would like
to explore.
References
Robert P. Abelson. 1981. Psychological status of the
script concept. American Psychologist, 36(7):715?
729.
Nathanael Chambers and Dan Jurafsky. 2009. Unsuper-
vised Learning of Narrative Schemas and their Partic-
ipants. In Proceedings of the Joint Conference of the
47th Annual Meeting of the ACL and the 4th Interna-
tional Joint Conference on Natural Language Process-
ing of the AFNLP, pages 602?610, Suntec, Singapore.
Nathanael Chambers and Dan Jurafsky. 2010. A
database of narrative schemas. In Proceedings of the
Seventh International Conference on Language Re-
sources and Evaluation (LREC 2010), Malta.
Hal Daume? and Daniel Marcu. 2005. A large-scale ex-
ploration of effective global features for a joint en-
tity detection and tracking model. In Proceedings of
the Conference on Human Language Technology and
Empirical Methods in Natural Language Processing -
HLT ?05, pages 97?104, Morristown, NJ, USA.
Aria Haghighi and Dan Klein. 2010. Coreference reso-
lution in a modular, entity-centered model. In Human
Language Technologies: The 2010 Annual Conference
of the North American Chapter of the Association for
Computational Linguistics, pages 385?393.
Thorsten Joachims. 2006. Training linear SVMs in lin-
ear time. In Proceedings of the 12th ACM SIGKDD In-
ternational Conference on Knowledge Discovery and
Data Mining KDD 06, pages 217?226.
Vincent Ng. 2010. Supervised noun phrase coreference
research: the first fifteen years. In Proceedings of the
48th Annual Meeting of the Association for Computa-
tional Linguistics, pages 1396?1411.
Simone Paolo Ponzetto and Michael Strube. 2006a.
Exploiting semantic role labeling, WordNet and
Wikipedia for coreference resolution. In Proceedings
of the main conference on Human Language Technol-
ogy Conference of the North American Chapter of the
Association of Computational Linguistics, pages 192?
199.
Simone Paolo Ponzetto and Michael Strube. 2006b. Se-
mantic role labeling for coreference resolution. In
Proceedings of the Eleventh Conference of the Euro-
pean Chapter of the Association for Computational
91
Linguistics - EACL ?06, pages 143?146, Morristown,
NJ, USA.
Sameer Pradhan, Lance Ramshaw, Ralph Weischedel,
Jessica MacBride, and Linnea Micciulla. 2007. Unre-
stricted Coreference: Identifying Entities and Events
in OntoNotes. In International Conference on Seman-
tic Computing (ICSC 2007), pages 446?453.
Sameer Pradhan, Lance Ramshaw, Mitchell Marcus,
Martha Palmer, Ralph Weischedel, and Nianwen Xue.
2011. CoNLL-2011 Shared Task: Modeling Unre-
stricted Coreference in OntoNotes. In Proceedings
of the Fifteenth Conference on Computational Natural
Language Learning (CoNLL 2011), Portland, Oregon.
Altaf Rahman and Vincent Ng. 2009. Supervised Mod-
els for Coreference Resolution. In Proceedings of
the 2009 Conference on Empirical Methods in Natu-
ral Language Processing, pages 968?977, Singapore.
Altaf Rahman and Vincent Ng. 2011. Narrowing
the Modeling Gap: A Cluster-Ranking Approach to
Coreference Resolution. Journal of Artificial Intelli-
gence Research, 40:469?521.
Roger C. Schank and Robert P. Abelson. 1977. Scripts,
plans, goals and understanding: An inquiry into hu-
man knowledge structures. Lawrence Erlbaum, Ox-
ford, England.
92
The 7th Workshop on the Innovative Use of NLP for Building Educational Applications, pages 281?288,
Montre?al, Canada, June 3-8, 2012. c?2012 Association for Computational Linguistics
NAIST at the HOO 2012 Shared Task
Keisuke Sakaguchi, Yuta Hayashibe, Shuhei Kondo, Lis Kanashiro
Tomoya Mizumoto, Mamoru Komachi, Yuji Matsumoto
Graduate School of Information Science
Nara Institute of Science and Technology
8916-5, Takayama, Ikoma, Nara 630-0192, Japan
{ keisuke-sa, yuta-h, shuhei-k, lis-k, tomoya-m, komachi, matsu }@is.naist.jp
Abstract
This paper describes the Nara Institute of Sci-
ence and Technology (NAIST) error correc-
tion system in the Helping Our Own (HOO)
2012 Shared Task. Our system targets prepo-
sition and determiner errors with spelling cor-
rection as a pre-processing step. The re-
sult shows that spelling correction improves
the Detection, Correction, and Recognition F-
scores for preposition errors. With regard to
preposition error correction, F-scores were not
improved when using the training set with cor-
rection of all but preposition errors. As for
determiner error correction, there was an im-
provement when the constituent parser was
trained with a concatenation of treebank and
modified treebank where all the articles ap-
pearing as the first word of an NP were re-
moved. Our system ranked third in preposi-
tion and fourth in determiner error corrections.
1 Introduction
Researchers in natural language processing have fo-
cused recently on automatic grammatical error de-
tection and correction for English as a Second Lan-
guage (ESL) learners? writing. There have been a lot
of papers on these challenging tasks, and remark-
ably, an independent session for grammatical error
correction took place in the ACL-2011.
The Helping Our Own (HOO) shared task (Dale
and Kilgarriff, 2010) is proposed for improving the
quality of ESL learners? writing, and a pilot run with
six teams was held in 2011.
The HOO 2012 shared task focuses on the cor-
rection of preposition and determiner errors. There
has been a lot of work on correcting preposition and
determiner errors, where discriminative models such
as Maximum Entropy and Averaged Perceptron (De
Felice and Pulman, 2008; Rozovskaya and Roth,
2011) and/or probablistic language models (Gamon,
2010) are generally used.
In addition, it is pointed out that spelling and
punctuation errors often disturb grammatical error
correction. In fact, some teams reported in the
HOO 2011 that they corrected spelling and punc-
tuation errors before correcting grammatical errors
(Dahlmeier et al, 2011).
Our strategy for HOO 2012 follows the above
procedure. In other words, we correct spelling er-
rors at the beginning, and then train classifiers for
correcting preposition and determiner errors. The
result shows our system achieved 24.42% (third-
ranked) in F-score for preposition error correc-
tion, 29.81% (fourth-ranked) for determiners, and
27.12% (fourth-ranked) for their combined.
In this report, we describe our system architec-
ture and the experimental results. Sections 2 to 4
describe the system for correcting spelling, prepo-
sition, and determiner errors. Section 5 shows the
experimental design and results.
2 System Architecture for Spelling
Correction
Spelling errors in second language learners? writing
often disturb part-of-speech (POS) tagging and de-
pendency parsing, becoming an obstacle for gram-
matical error detection and correction tasks. For ex-
ample, POS tagging for learners? writing fails be-
281
e.g. I think it is *verey/very *convent/convenient for the group.
without spelling error correction: ... (?it?, ?PRP?), (?is?, ?VBZ?), (?verey?, ?PRP?), (?convent?, ?NN?), ...
with spelling error correction : ... (?It?, ?PRP?), (?is?, ?VBZ?), (?very?, ?RB?), (?convenient?, ?JJ?), ...
Figure 1: POS tagging for learners? writing without and with spelling error correction.
cause of misspelled words (Figure 1).1
To reduce errors derived from misspelled words,
we conduct spelling error correction as a pre-
processing task. The procedure of spelling error cor-
rection we use is as follows. First of all, we look for
misspelled words and suggest candidates by GNU
Aspell2, an open-source spelling checker. The can-
didates are ranked by the probability of 5-gram lan-
guage model built from Google N-gram (Web 1T
5-gram Version 1)3 (Brants and Franz, 2006) with
IRST LM Toolkit (Federico and Cettolo, 2007).4 Fi-
nally, according to the rank, we changed the mis-
spelled word into the 1-best candidate word.
In a preliminary experiment, where we use the
original CLC FCE dataset,5 our spelling error cor-
rection obtains 52.4% of precision, 72.2% of recall,
and 60.7% of F-score.
We apply the spelling error correction to the train-
ing and test sets provided, and use both spelling-
error and spelling-error-free sets for comparison.
3 System Architecture for Preposition
Error Correction
There are so many prepositions in English. Because
it is difficult to perform multi-class classification,
we focus on twelve prepositions: of, in, for, to, by,
with, at, on, from, as, about, since, which account
for roughly 91% of preposition usage (Chodorow et
al., 2010).
The errors are classified into three categories ac-
cording to their ways of correction. First, replace-
ment error indicates that learners use a wrong
preposition. For instance, with in Example (1) is a
1The example is extracted from the CLC FCE dataset and
part-of-speech tagged by Natural Language Toolkit (NLTK).
http://www.nltk.org/
2GNU Aspell 0.60.6.1 http://aspell.net/
3http://www.ldc.upenn.edu/Catalog/CatalogEntry.jsp?
catalogId=LDC2006T13
4irstlm5.70 http://sourceforge.net/projects/irstlm/
5In the CLC FCE dataset, misspelled words are corrected
and tagged with a label ?S?.
replacement error.
I went there withby bus. (1)
Second, insertion error points out they incor-
rectly inserted a preposition, such as ?about? in Ex-
ample (2).6
We discussed aboutNONE the topic. (2)
Third, deletion error means they fail to write
obligatory prepositions. For example, ?NONE? in
Example (3) is an deletion error.
This is the place to relax NONEin. (3)
Replacement and insertion error correction can be
regarded as a multi-class classification task at each
preposition occurrence. However, deletion errors
differ from the other two types of errors in that they
may occur at any place in a sentence. Therefore, we
build two models, a combined model for replace-
ment and insertion errors and a model for deletion
errors, taking the difference into account.
For the model of replacement and insertion errors,
we simultaneously perform error detection and cor-
rection with a single model.
For the model of deletion errors, we only check
whether direct objects of verbs need prepositions,
because it is time consuming to check all the gaps
between words. Still, it covers most deletion errors.7
We merge the outputs of the two models to get the
final output.
We used two types of training sets extracted from
the original CLC-FCE dataset. One is the ?gold?
set, where training sentences are corrected except
for preposition errors. In the gold set, spelling er-
rors are also corrected to the gold data in the corpus.
The other is the ?original? set, which includes the
6?NONE? means there are no words.
72,407 out of 5,324 preposition errors in CLC-FCE are be-
tween verbs and nouns.
282
Type Name Description (NP and PRED refer a noun phrase and a predicate.)
Lexical Token n-gram Token n-grams in a 2 word window around the preposition
POS n-gram POS n-grams in a 2 word window around the preposition
HEAD PREC VP The head verb in the preceding verb phrase
HEAD PREC NP The head noun in the preceding noun phrase
HEAD FOLLOW NP The head noun in the following noun phrase
Parsing HEAD Head of the preposition
HEAD POS POS of the head
COMP Complement of the preposition
COMPLEMENT POS POS of the complement
HEAD RELATION Prep-Head relation name
COMPLEMENT RELATION Prep-Comp relation name
Phrase Structure PARENT TAG TAG of the preposition?s parent
GRANDPARENT TAG TAG of the preposition?s grandparent
PARENT LEFT Left context of the preposition parent
PARENT RIGHT Right context of the preposition?s parent
Web N-gram COUNT For the frequency fprep,i of i (3 to 5) window size phrase including
the preposition prep, the value of log100(fi + 1)
PROPORTION The proportion pprep,i (i is 3 to 5).
pprep,i = fprep,i?
k?T fk,i
, given the set of target prepositions T .
Semantic WORDNET CATEGORY WordNet lexicographer classes which are about 40 broad semantic
categories for all words used as surface features. As De Felice and
Pulman (2008) did not perform word sense disambiguation, neither
did we.
Table 1: Baseline features for English preposition error correction.
original CLC-FCE plain sentences.
We performed sentence splitting using the im-
plementation of Kiss and Strunk (2006) in NLTK
2.0.1rc2. We conducted dependency parsing by
Stanford parser 1.6.9.8
We used the features described in (Tetreault et al,
2010) as shown in Table 1 with Maximum Entropy
(ME) modeling (Berger et al, 1996) as a multi-class
classifier. We used the implementation of Maximum
Entropy Modeling Toolkit9 with its default parame-
ters. For web n-gram calculation, we used Google
N-gram with a search system for giga-scale n-gram
corpus, called SSGNC 0.4.6.10
4 System Architecture for Determiner
Error Correction
We focused on article error correction in the deter-
miner error correction subtask, because the errors
related to articles significantly outnumber the errors
unrelated to them. Though more than twenty types
of determiners are involved in determiner error cor-
rections of the HOO training set, over 90% of errors
8http://nlp.stanford.edu/software/lex-parser.shtml
9https://github.com/lzhang10/maxent
10http://code.google.com/p/ssgnc/
are related to three articles a, an and the. We defined
article error correction as a multi-class classification
problem with three classes, a, the and null article,
and assumed that target articles are placed at the left
boundary of a noun phrase (NP). The indefinite ar-
ticle an was normalized to a in training and testing,
and restored to an later in an example-based post-
processing step. If the system output was a and the
word immediately after a appeared more frequently
with an than with a in the training corpus, a was re-
stored to an. If the word appeared equally frequently
with a and an or didn?t appear in the training corpus,
a was restored to an if the word?s first character was
one of a, e, i, o, u.
Each input sentence was parsed using the Berke-
ley Parser11 with two models, ?normal? and
?mixed?. The ?normal? model was trained on a tree-
bank of normal English sentences. In preliminary
experiments, the ?normal? model sometimes mis-
judged the span of NPs in ESL writers? sentences
due to missing articles. So we trained the ?mixed?
model on a concatenation of the normal treebank
and a modified treebank in which all the articles ap-
pearing as the first word of an NP were removed. By
11version 1.1, http://code.google.com/p/berkeleyparser/
283
Name Description
HeadNounWord The word form of the head noun
HeadNounTag The POS tag of the head noun
ObjOfPrep Indicates that the head noun is an object of a preposition
PrepWord The word form of the preposition
PrepHeadWord The word form of the preposition?s syntactic parent
PrepHeadTag The POS tag of the preposition?s syntactic parent
ContextWindowTag
The POS tag of the words in a 3 word window
around the candidate position for the article
ContextWindowWord
The word form of the word immediately following
the candidate position for the article
ModByDetWord The word form of the determiner that modifies the head noun
ModByAdjWord The word form of the adjective that modifies the head noun
ModByAdjTag The POS tag of the adjective that modifies the head noun
ModByPrep Indicates that the head noun is modified by a preposition
ModByPrepWord The word form of the preposition that modifies the head noun
ModByPossesive Indicates that the head noun is modified by a possesive
ModByCardinal Indicates that the head noun is modified by a cardinal number
ModByRelative Indicates that the head noun is modified by a relative clause
Table 2: Feature templates for English determiner correction.
augmenting the training data for the parser model
with sentences lacking articles, the span of NPs that
lack an article might have better chance of being cor-
rectly recognized. In addition, dependency informa-
tion was extracted from the parse using the Stanford
parser 1.6.9.
For each NP in the parse, we extracted a feature
vector representation. We used the feature templates
shown in Table 2, which are inspired by (De Felice,
2008) and adapted to the CFG representation.
For the parser models, we trained the ?normal?
model on the WSJ part of Penn Treebank sections
02-21 with the NP annotation by Vadas and Curran
(2007). The ?mixed? model was trained on the con-
catenation of the WSJ part and its modified version.
For the classification model, we used the written part
of the British National Corpus (BNC) in addition to
the CLC FCE Dataset, because the amount of in-
domain data was limited. In examples taken from
the CLC FCE Dataset, the true labels after the cor-
rection were used. In examples taken from the BNC,
the article of each NP was used as the label. We
trained a linear classifier using opal12 with the PA-I
algorithm. We also used the feature augmentation
12http://www.tkl.iis.u-tokyo.ac.jp/?ynaga/opal/
Subsystem Parameters
Run Spelling Preposition Determiner
0 no change gold mixed
1 no change gold normal
2 no change original mixed
3 no change original normal
4 corrected gold mixed
5 corrected gold normal
6 corrected original mixed
7 corrected original normal
Table 3: Distinct configurations of the system.
approach of (Daume? III, 2007) for domain adapta-
tion.
5 Experiment and Result
Previously undisclosed data extracted from the
CLC-FCE dataset was provided as a test set by the
HOO organizers. The test set includes 100 essays
and each contains 180.1 word tokens on average.
We defined eight distinct configurations based
on our subsystem parameters (Table 3). The offi-
cial task evaluation uses three metrics (Detection,
Recognition, and Correction), and three measures
Precision, Recall, and F-score were computed13 for
13For details about the evaluation metrics, see http://
284
Detection Correction Recognition
Run R P F R P F R P F
0 29.58 34.09 31.67 19.86 22.90 21.27 26.71 30.78 28.60
1 28.69 36.41 32.09 19.42 24.64 21.72 25.82 32.77 28.88
2? 28.91 37.21 32.54 20.97 26.98 23.60 26.26 33.80 29.56
3 28.03 40.18 33.02 20.52 29.43 24.18 25.38 36.39 29.90
4 30.24 33.66 31.86 20.75 23.09 21.86 27.37 30.46 28.83
5 29.13 35.57 32.03 19.64 23.98 21.60 26.26 32.07 28.88
6 29.35 36.23 32.43 21.41 26.43 23.65 26.26 32.42 29.02
7 28.25 38.67 32.65 20.30 27.29 23.46 25.16 34.44 29.08
Table 4: Result for preposition and determiner errors combined before revisions.
?We re-evaluated the Run2 because we submitted the Run2 with the same condition as Run0.
Detection Correction Recognition
Spelling Preposition R P F R P F R P F
no change gold 25.00 34.70 29.06 14.40 20.00 16.74 20.76 28.82 24.13
no change original 23.30 42.63 30.13 16.52 30.23 21.36 19.91 36.43 25.75
corrected gold 26.69 34.80 30.21 15.25 19.88 17.26 22.45 29.28 25.41
corrected original 24.57 41.13 30.76 16.52 27.65 20.68 20.33 34.04 25.46
Table 5: Result for preposition errors before revisions.
each metric.
Table 4 to Table 9 show the overall results of our
systems. In terms of the effect of pre-processing,
spelling correction improved the F-score of Detec-
tion, Correction, and Recognition for preposition er-
rors after revision, whereas there were fluctuations
in other conditions. This may be because there were
a few spelling errors corrected in the test set.14 An-
other reason why no stable improvement was found
in determiner error correction is because spelling
correction often produces nouns that affect the de-
terminer error detection and correction more sensi-
tively than prepositions. For example, a misspelled
word *freewho / free who was corrected as freezer.
This type of error may have increased false posi-
tives. The example *National Filharmony / the Na-
tional Philharmony was corrected as National Flem-
ing, where the proper noun Fleming does not need a
determiner and this type of error increased false neg-
atives.
As for preposition error correction, the classifier
performed better when it was trained with the ?origi-
nal? set rather than the error-corrected (all but prepo-
sition errors) ?gold? set. The reason for this is that
the gold set is trained with the test set that contains
correcttext.org/hoo2012/eval.html
14There was one spelling correction per document in average.
several types of errors which the original CLC-FCE
dataset alo contains. Therefore, the ?original? clas-
sifier is more optimised and suitable for the test set
than the ?gold? one.
For determiner error correction, the ?mixed?
model improved precision and F-score in the addi-
tional experiments.
5.1 Error Analysis of Preposition Correction
We briefly analyze some errors in our proposed
model according to the three categories of errors.
First, most replacement errors require deep under-
standing of context. For instance, for in Example (4)
must be changed to to. However, modifications of is
also often used, so it is hard to decide either to or of
is suitable based on the values of N-gram frequen-
cies.
Its great news to hear you have been given
extra money and that you will spend it in
modifications forto the cinema.
(4)
Second, most insertion errors need a grammatical
judgement rather than a semantic one. For instance,
?in? in Example (5) must be changed to ?NONE.?
Their love had always been kept inNONE se-
cret
(5)
In order to correct this error, we need to recog-
285
Detection Correction Recognition
Spelling Determiner R P F R P F R P F
no change mixed 34.10 33.18 33.63 25.80 25.11 25.45 33.17 32.28 32.72
no change normal 32.25 37.43 34.65 24.88 28.87 26.73 31.33 36.36 33.66
corrected mixed 33.64 32.30 32.95 26.72 25.66 26.18 32.71 31.41 32.05
corrected normal 31.33 35.78 33.41 24.42 27.89 26.04 30.41 34.73 32.43
Table 6: Result for determiner errors before revisions.
Detection Correction Recognition
Run R P F R P F R P F
0 31.28 37.65 34.18 22.62 27.22 24.71 28.54 34.35 31.17
1 30.44 40.33 34.69 22.19 29.41 25.30 27.69 36.69 31.56
2? 31.07 41.76 35.63 23.04 30.96 26.42 28.11 30.96 32.24
3 30.23 45.25 36.24 22.62 33.86 27.12 27.27 40.82 32.69
4 31.92 37.10 34.31 23.46 27.27 25.22 29.17 33.90 31.36
5 30.86 39.35 34.59 22.41 28.57 25.11 28.11 35.84 31.51
6 31.71 40.87 35.71 23.89 30.79 26.90 28.75 37.05 32.38
7 30.65 43.80 36.06 22.83 32.62 26.86 27.69 39.57 32.58
Table 7: Result for preposition and determiner errors combined after revisions.
?We re-evaluated the Run2 because we submitted the Run2 with the same condition as Run0.
nize ?keep? takes an object and a complement; in
Example (5) ?love? is the object and ?secret? is
the complement of ?keep? while the former is left-
extraposed. A rule-based approach may be better
suited for these cases than a machine learning ap-
proach.
Third, most deletion errors involve discrimination
between transitive and intransitive. For instance,
?NONE? in Example (6) must be changed to ?for?,
because ?wait? is intransitive.
I?ll wait NONEfor your next letter. (6)
To deal with these errors, we may use rich knowl-
edge about verbs such as VerbNet (Kipper et al,
2000) and FrameNet (Baker et al, 1998) in order
to judge whether a verb is transitive or intransitive.
5.2 Error Analysis of Determiner Correction
We conducted additional experiments for determiner
errors and report the results here because the sub-
mitted system contained a bug. In the submit-
ted system, while the test data were parsed by the
?mixed? model, the training data and the test data
were parsed by the default grammar provided with
Berkeley Parser. Moreover, though there were about
5.5 million sentences in the BNC corpus, only about
2.7 million of them had been extracted. Though
these errors seem to have improved the performance,
it is difficult to specify which errors had positive ef-
fects.
Table 10 shows the result of additional experi-
ments. Unlike the submitted system, the ?mixed?
model contributed toward a higher precision and F-
score. Though the two parser models parsed the
sentences differently, the difference in the syntactic
analysis of test sentences did not always led to dif-
ferent output by the downstream classifiers. On the
contrary, the classifiers often returned different out-
puts even for an identically parsed sentence. In fact,
the major source of the performance gap between the
two models was the number of the wrong outputs
rather than the number of correct ones. While the
?mixed? model without spelling correction returned
146 outputs, of which 83 were spurious, the ?nor-
mal? model without spelling correction produced
209 outputs, of which 143 were spurious. This may
suggest the difference of the two models can be at-
tributed to the difference in the syntactic analysis of
the training data.
One of the most frequent types of errors com-
mon to the two models were those caused by mis-
spelled words. For example, when your letter was
misspelled to be *yours letter, it was regarded as an
286
Detection Correction Recognition
Spelling Preposition R P F R P F R P F
no change gold 26.63 38.23 31.40 17.62 25.29 20.77 23.36 33.52 27.53
no change original 26.22 49.61 34.31 18.44 34.88 24.12 22.54 42.63 29.49
corrected gold 28.27 38.12 32.47 18.44 24.86 21.17 25.00 33.70 28.70
corrected original 27.86 48.22 35.32 19.26 33.33 24.41 24.18 41.84 30.64
Table 8: Result for preposition errors after revisions.
Detection Correction Recognition
Spelling Determiner R P F R P F R P F
no change mixed 35.37 36.32 35.84 27.94 28.69 28.31 34.06 34.97 34.51
no change normal 33.62 41.17 37.01 27.07 33.15 29.80 32.31 39.57 35.57
corrected mixed 34.93 35.39 35.16 28.82 29.20 29.01 33.62 34.07 33.84
corrected normal 32.75 39.47 35.79 26.63 32.10 29.11 31.44 37.89 34.36
Table 9: Result for determiner errors after revisions.
Detection Correction Recognition
Spelling Determiner R P F R P F R P F
no change mixed 27.39 43.15 33.51 23.04 36.30 28.19 27.39 43.15 33.51
no change normal 28.69 31.57 30.06 22.61 24.88 23.69 28.69 31.57 30.06
corrected mixed 27.39 41.44 31.98 22.61 34.21 27.22 26.96 40.79 32.46
corrected normal 30.43 33.33 31.82 24.34 26.67 25.45 30.00 32.86 31.36
Table 10: Result of additional experiments for determiner errors after revisions.
NP without a determiner resulting in a false posi-
tive such as *a yours letter. Among the other types
of errors, several seemed to be caused by the infor-
mation from the context window. For instance, the
system output for It was last month and ... was it
was *the last month and .... It is likely that the word
last triggered the misinsertion here. Such kind of
errors might be avoided by conjunctive features of
context information and the head word. Last but not
least, compound errors were also frequent and prob-
ably the most difficult to solve. For example, it is
quite difficult to correct *for a month to per month
if we are dealing with determiner errors and prepo-
sition errors separately. A more sophisticated ap-
proach such as joint modeling seems necessary to
correct this kind of errors.
6 Conclusion
This report described the architecture of our prepo-
sition and determiner error correction system. The
experimental result showed that spelling correction
advances the performance of Detection, Correction
and Recognition for preposition errors. In terms of
preposition error correction, F-scores were not im-
proved when the error-corrected dataset was used.
As to determiner error correction, there was an im-
provement when the constituent parser was trained
on a concatenation of treebank and modified tree-
bank where all the articles appearing as the first
word of an NP were removed.
Acknowledgements
This work was partly supported by the National In-
stitute of Information and Communications Tech-
nology Japan.
287
References
Collin F Baker, Charles J Fillmore, and John B Lowe.
1998. The Berkeley FrameNet Project. In Proceed-
ings of the 36th Annual Meeting of the Association for
Computational Linguistics, pages 86?90, Montreal,
Quebec, Canada.
Adam L. Berger, Vincent J. Della Pietra, and Stephen
A. Della Pietra. 1996. A Maximum Entropy Ap-
proach to Natural Language Processing. Computa-
tional Linguistics, 22(1):39?71.
Thorsten Brants and Alex Franz. 2006. Web 1T 5-gram
Corpus Version 1.1. Linguistic Data Consortium.
Martin Chodorow, Michael Gamon, and Joel Tetreault.
2010. The Utility of Article and Preposition Error Cor-
rection Systems for English Language Learners: Feed-
back and Assessment. Language Testing, 27(3):419?
436.
Daniel Dahlmeier, Hwee Tou Ng, and Thanh Phu Tran.
2011. NUS at the HOO 2011 Pilot Shared Task. In
Proceedings of the 13th European Workshop on Nat-
ural Language Generation, pages 257?259, Nancy,
France.
Robert Dale and Adam Kilgarriff. 2010. Helping Our
Own: Text Massaging for Computational Linguistics
as a New Shared Task. In Proceedings of the 6th In-
ternational Natural Language Generation Conference,
pages 261?266, Trim, Co. Meath, Ireland.
Hal Daume? III. 2007. Frustratingly Easy Domain Adap-
tation. In Proceedings of the 45th Annual Meeting of
the Association of Computational Linguistics, pages
256?263, Prague, Czech Republic.
Rachele De Felice and Stephen G. Pulman. 2008. A
Classifier-Based Approach to Preposition and Deter-
miner Error Correction in L2 English. In Proceed-
ings of the 22nd International Conference on Compu-
tational Linguistics, pages 169?176, Manchester, UK.
Rachele De Felice. 2008. Automatic Error Detection in
Non-native English. Ph.D. thesis, University of Ox-
ford.
Marcello Federico and Mauro Cettolo. 2007. Efficient
Handling of N-gram Language Models for Statistical
Machine Translation. In Proceedings of the Second
Workshop on Statistical Machine Translation, pages
88?95, Prague, Czech Republic.
Michael Gamon. 2010. Using Mostly Native Data to
Correct Errors in Learners? Writing. In Human Lan-
guage Technologies: The 2010 Annual Conference of
the North American Chapter of the Association for
Computational Linguistics, pages 163?171, Los An-
geles, California.
Karin Kipper, Hoa Trang Dang, and Martha Palmer.
2000. Class-based Construction of a Verb Lexicon. In
Proceedings of the 7th National Conference on Artifi-
cial Intelligence, pages 691?696, Austin, Texas, USA.
Tibor Kiss and Jan Strunk. 2006. Unsupervised Multi-
lingual Sentence Boundary Detection. Computational
Linguistics, 32(4):485?525.
Alla Rozovskaya and Dan Roth. 2011. Algorithm Selec-
tion and Model Adaptation for ESL Correction Tasks.
In Proceedings of the 49th Annual Meeting of the As-
sociation for Computational Linguistics: Human Lan-
guage Technologies, pages 924?933, Portland, Ore-
gon, USA.
Joel Tetreault, Jennifer Foster, and Martin Chodorow.
2010. Using Parse Features for Preposition Selec-
tion and Error Detection. In Proceedings of the 47th
Annual Meeting of the Association for Computational
Linguistics Short Papers, pages 353?358, Uppsala,
Sweden.
David Vadas and James Curran. 2007. Adding Noun
Phrase Structure to the Penn Treebank. In Proceedings
of the 45th Annual Meeting of the Association of Com-
putational Linguistics, pages 240?247, Prague, Czech
Republic.
288
Proceedings of the Eighth Workshop on Innovative Use of NLP for Building Educational Applications, pages 134?139,
Atlanta, Georgia, June 13 2013. c?2013 Association for Computational Linguistics
NAIST at the NLI 2013 Shared Task
Tomoya Mizumoto, Yuta Hayashibe
Keisuke Sakaguchi, Mamoru Komachi, Yuji Matsumoto
Graduate School of Information Science
Nara Institute of Science and Technology
8916-5, Takayama, Ikoma, Nara 630-0192, Japan
{ tomoya-m, yuta-h, keisuke-sa, komachi, matsu }@is.naist.jp
Abstract
This paper describes the Nara Institute of
Science and Technology (NAIST) native lan-
guage identification (NLI) system in the NLI
2013 Shared Task. We apply feature selec-
tion using a measure based on frequency for
the closed track and try Capping and Sampling
data methods for the open tracks. Our system
ranked ninth in the closed track, third in open
track 1 and fourth in open track 2.
1 Introduction
There have been many studies using English as a
second language (ESL) learner corpora. For exam-
ple, automatic grammatical error detection and cor-
rection is one of the most active research areas in this
field. More recently, attention has been paid to na-
tive language identification (NLI) (Brooke and Hirst,
2012; Bykh and Meurers, 2012; Brooke and Hirst,
2011; Wong and Dras, 2011; Wong et al, 2011).
Native language identification is the task of identi-
fying the ESL learner?s L1 given a learner?s essay.
The NLI Shared Task 2013 (Tetreault et al, 2013)
is the first shared task on NLI using the com-
mon dataset ?TOEFL-11? (Blanchard et al, 2013;
Tetreault et al, 2012). TOEFL-11 consists of essays
written by learners of 11 native languages (Arabic,
Chinese, French, German, Hindi, Italian, Japanese,
Koran, Spanish, Telugu, Turkish), and it contains
1,100 essays for each native language. In addition,
the essay topics are balanced, and the number of top-
ics is 8.
In the closed track, we tackle feature selection
for increasing accuracy. We use a feature selection
method based on the frequency of each feature (e.g.,
document frequency, TF-IDF).
In the open tracks, to address the problem of im-
balanced data, we tried two approaches: Capping
and Sampling data in order to balance the size of
training data.
In this paper, we describe our system and exper-
imental results. Section 2 describes the features we
used in the system for NLI. Section 3 and Section 4
describe the systems for closed track and open track
in NLI Shared Task 2013. Section 5 describes the re-
sults for NLI Shared Task 2013. Section 6 describes
the experimental result for 10-fold cross validation
on the data set used by Tetreault et al (2012).
2 Features used in all tracks
In this section, we describe the features in our sys-
tems. We formulate NLI as a multiclass classifica-
tion task. Following previous work, we use LIB-
LINEAR 2 for the classification tool and tune the C
parameter using grid-search.
We select the features based on previous work
(Brooke and Hirst, 2012; Tetreault et al, 2012). All
features used are binary. We treated the features as
shown in Table 1. The example of features in Table
1 shows the case whose input is ?I think not a really
difficult question?.
We use a special symbol for the beginning and
end of sentence (or word) for bigrams and trigrams.
For surface forms, we lowercased all words. POS,
POS-function and dependency features are extracted
1http://www.lextek.com/manuals/onix/stopwords1.html
2http://www.csie.ntu.edu.tw/?cjlin/liblinear/
134
Name Description Example
Word N-gram (N=1,2) Surface form of the word. N=1 i, think, not
N=2 BOS i, i think
POS N-gram (N=2,3) POS tags of the word. N=2 BOS PRP, PRP VBP
N=3 BOS PRP VBP, PRP VBP RB
Character N-gram (N=2,3) N=2 ? t, t h, hi, in, nk, k$
N=3 ? t h, t h i
POS-function N-gram (N=2,3) We use surface form for words in stop
word list 1, otherwise we use POS form.
N=2 RB difficult, difficult NN
N=3 RB difficult NN
Dependency the surface and relation name (i, nsubj)
the surface and the dependend token?s
surface
(think, i)
the surface, relation name and the de-
pendend token?s surface
(nsubj, i, think)
Tree substitution grammer Fragments of TSG (PRP UNK-INITC-
KNOWNLC) (VB think)
(NP RB DT ADJP NN)
(JJ UNK-LC)
Table 1: All features for native language identification.
using the Stanford Parser 2.0.2 3.
We use tree substitution grammars as fea-
tures. TSGs are generalized context-free grammars
(CFGs) that allow nonterminals to re-write to tree
fragments. The fragments reflect both syntactic and
surface structures of a given sentence more effi-
ciently than using several CFG rules. In practice,
efficient Bayesian approaches have been proposed
in prior work (Post and Gildea, 2009). In terms
of the application of TSG to NLI task, (Swanson
and Charniak, 2012) have shown a promising re-
sult. Post (2011) also uses TSG to judge grammat-
icality of a sentence written by language learners.
With these previous findings in mind, we also ex-
tract TSG rules. We use the training settings and
public software from Post (2011)4, obtaining 21,020
unique TSG fragments from the training dataset of
the TOEFL-11 corpus.
3 Closed Track
In this section, we describe our system for the closed
track. We use the tools and features described in
Section 2.
In our system, feature selection is performed us-
ing a measure based on frequency. Although Tsur
3http://nlp.stanford.edu/software/lex-parser.shtml
4https://github.com/mjpost/post2011judging
and Rappoport (2007) used TF-IDF, they use it to
decrease the influence of topic bias rather than for
increasing accuracy. Brooke and Hirst (2012) used
document frequency for feature selection, however
it does not affect accuracy.
We use the native language frequency (hereafter
we refer to this as NLF). NLF is the number of na-
tive languages a feature appears in. Thus, NLF takes
values from 1 to 11. Figure 1 shows an example of
NLF. The word bigram feature ?in Japan? appears
only in essays of which the learners? native language
is Japanese, therefore the NLF is 1.
The assumption behind using this feature is that a
feature which appears in all native languages affects
NLI less, while a feature which appears in few na-
tive language affects NLI more. The features whose
NLFs are 11 include e.g. ?there are?, ?PRP VBP?
and ?a JJ NN?. Table 2 shows some examples of the
features appearing in only 1 native language in the
TOEFL-11 corpus. The features include place-name
or company name such as ?tokyo?, ?korea?, ?sam-
sung?, which are certainly specific for some native
language.
135
Native Language
Chinese Japanese Korean
carry more this : NN samsung
i hus become of tokyo of korea
JJ whole and when i worked debatable whether
striking conclusion usuful NN VBG whether
traffic tools oppotunity for in thesedays
Table 2: Example of feature appearing in 1 native language for Chinese, Japanese and Korean
Figure 1: Example of native language frequency
Native Language # of articles
Japanese 258,320
Mandarin 48,364
Korean 31,188
Spanish 5,106
Italian 2,589
Arabic 1,549
French 1,168
German 832
Turkish 504
Hindi 223
Telugu 19
Table 3: Distribution of native languages in Lang-8
corpus
4 Open tracks
4.1 Lang-8 corpus
For the open tracks, we used Lang-8 as a source to
create a learner corpus tagged with the native lan-
guages of learners. Lang-8 is a language learning
social networking service. 5 Users write articles
in their non-native languages and native speakers
correct them. We used all English articles written
through the end of 2012. We removed all sentences
which contain non-ASCII characters. 6
Almost all users register their native language on
the site. We regard users? registered native language
5http://lang-8.com/
6Some users also add translation in their native languages
for correctors? reference.
as the gold label for each article. We split the learner
corpus extracted from Lang-8 into sub-corpora by
the native languages. The numbers of articles in all
corpora are summarized in Table 3. Unfortunately,
some sub-corpora are too small to train the model.
For example, the Telugu corpus has only 19 articles.
In order to balance the size of the training data,
we tried two approaches: Capping and Sampling.
We confirmed in preliminary experiments that the
model with these approaches work better than the
model with the original sized data.
Capping
In this approach, we limit the size of a sub-corpus
for training to N articles. For a sub-corpus which
contains over N articles, we randomly extract ar-
ticles up to N . We set N = 5000 and adapt this
approach for Run 1 and Run 3 in the open tracks.
Sampling
In this approach, we equalize the size of all sub-
corpora. For corpora which contain less than N ar-
ticles, we randomly copy articles until their size be-
comesN . We setN = 5000 and adapt this approach
for Run 2 and Run 4 in the open tracks.
4.2 Models
We compared two approaches with baseline features
and all features.
The models in Run 1 and Run 3 were trained with
the data created by the Capping approach, and the
models in Run 2 and Run 47 were trained by the
Sampling approach.
We used only word N-grams (N = 1, 2) as base-
line features. As extra features we used the follow-
ing features.
7We did not have time to train the model for Run 4 in the
open 1 track.
136
? POS N-grams (N = 2, 3)
? dependency
? character N-grams (N = 2, 3)
In open track 2, we also add the TOEFL-11
dataset to the training data for all runs.
5 Result for NLI shared Task 2013
Table 4 shows the results of our systems for NLI
Shared Task. Chance accuracy is 0.09. All results
outperform random guessing.
5.1 Closed track
In the closed track, we submitted 5 runs. Run 1
is the system using only word 1,2-grams features.
Run 2 is the system using all features with NLF fea-
ture selection (1 < NLF < 11). Run 3 is the system
using word 1,2-grams and POS 2,3-grams features.
Run 4 is the system using word 1,2-grams, POS 2,3-
grams, character 2,3-grams and dependency features
without parameter tuning. Run 5 is the system us-
ing word 1,2-grams without parameter tuning. The
method using the feature selection method we pro-
posed achieved the best performance of our systems.
5.2 Open tracks
Comparison of the two data balancing
approaches
In open track 1, the method of ?Sampling? out-
performs that of ?Capping? (Run 2 > Run 1). This
means even duplicated training data can improve the
performance.
On the other hand, in open track 2, ?Capping?
works better than ?Sampling? (Run 1 > Run 2 and
Run 3>Run 4). In the first place, the models trained
with both Lang-8 data and TOEFL data do not per-
form better than ones trained with only TOEFL data.
This means the less Lang-8 data we use, the better
performance we obtain.
Comparison on two feature sets
In open track 1, adding extra features seems to
have a bad influence because the result of Run 3
is worse than that of Run 1. This may be because
Lang-8 data is out of domain of the test corpus
(TOEFL).
Closed Open 1 Open 2
Run Accuracy Accuracy Accuracy
1 0.811 0.337 0.699
2 ?0.817 0.356 0.661
3 0.808 0.285 0.703
4 0.771 - 0.665
5 0.783 - -
Table 4: Result for systems which submitted in NLI
2013 ?We re-evaluated the Run2 because we submitted the
Run1 with the same output as Run2.
In open track 2, adding extra features makes the
performance better (Run 3 > Run 1, Run 4 > Run
2). In-domain TOEFL data seem to be effective for
training with extra features. In order to improve the
result with extra features in open track 2, domain
adaptation may be effective.
6 Experiment and Result for 10 fold
Cross-Validation
We conducted an experiment using 10-fold cross
validation on the data set used by Tetreault et al
(2012). Table 5 shows the results for different fea-
ture set. The table consists of 3 blocks; the first
block is results of the system using 1 feature, the
second block is the result of the system using word
1,2-grams feature and another feature, and the third
block is the result of the system using word 1,2-
grams and more features.
In the first block results, the system using the
word 1,2-grams feature achieved 0.8075. It is the
highest accuracy in the first block, and third highest
accuracy in the results of Table 5. From the second
block of results, adding an extra feature does not im-
prove accuracy, however in the third block the sys-
tems in (14) and (15) outperform the system using
only word 1,2-grams.
Table 6 shows the results of using feature selec-
tion by NLF. The table consists of 3 blocks; the
first block is the results of the system using features
whose NLF is smaller than N (N = 11, 10, 9, 8), the
second block is the results of the system using fea-
tures whose NLF is greater than N (N = 1, 2, 3, 4),
and the third block is the results of the system using
features whose NLF is smaller than 11 and greater
than N (N = 1, 2, 3, 4).
The best accuracy is achieved by excluding fea-
137
Feature Accuracy
(1) Word 1,2-gram 0.8075
(2) POS 2,3-gram 0.5555
(3) POS,Function 2,3-gram 0.7080
(4) Chracter 2,3-gram 0.6678
(5) Dependency 0.7236
(6) Tree substitution grammar 0.6455
(7) 1 + 2 0.7825
(8) 1 + 3 0.7913
(9) 1 + 4 0.7953
(10) 1 + 5 0.8020
(11) 1 + 6 0.7999
(12) 1 + 2 + 3 0.7849
(13) 1 + 2 + 3 + 4 0.8000
(14) 1 + 2 + 3 + 4 + 5 0.8097
(15) ALL 0.8088
Table 5: 10-fold cross validation results for each
feature
tures whose NLF is 1 or 11. While the results of the
first block and the second block are intuitive, the re-
sults of the third block are not (looking at the second
block of Table 6, excluding features whose NLF is
greater than N (1, 2, 3, 4) reduces accuracy). One
possible explanation is that features whose NLF is
1 includes features that rarely appear in the training
corpus.
7 Conclusion
In this paper, we described our systems for the NLI
Shared Task 2013. We tried feature selection using
native language frequency for the closed track and
Capping and the Sampling data to balance the size of
training data for the open tracks. The feature selec-
tion we proposed improves the performance for NLI.
The system using our feature selection achieved
0.817 on the test data of NLI Shared Task and 0.821
using 10-fold cross validation. While the Sampling
system outperformed Capping system for open track
1, the Capping system outperformed Sampling sys-
tem in open track 2 (because it reduced the amount
of out of domain data).
References
Daniel Blanchard, Joel Tetreault, Derrick Higgins, Aoife
Cahill, and Martin Chodorow. 2013. Toefl11: A cor-
Accuracy
NLF < 11 0.8176
NLF < 10 0.8157
NLF < 9 0.8123
NLF < 8 0.8098
1 < NLF 0.8062
2 < NLF 0.8062
3 < NLF 0.8057
4 < NLF 0.8053
1 < NLF < 11 0.8209
2 < NLF < 11 0.8206
3 < NLF < 11 0.8201
4 < NLF < 11 0.8195
Table 6: 10-fold cross validation results using
feature selection by NLF. (feature selection is not
applied to word N-grams features.)
pus of non-native english. Technical report, Educa-
tional Testing Service.
Julian Brooke and Graeme Hirst. 2011. Native language
detection with ?cheap? learner corpora. In Proceedings
of LCR 2011.
Julian Brooke and Graeme Hirst. 2012. Robust, lexical-
ized native language identification. In Proceedings of
COLING 2012, pages 391?408.
Serhiy Bykh and Detmar Meurers. 2012. Native lan-
guage identification using recurring n-grams ? inves-
tigating abstraction and domain dependence. In Pro-
ceedings of COLING 2012, pages 425?440.
Matt Post and Daniel Gildea. 2009. Bayesian Learning
of a Tree Substitution Grammar. In Proceedings of the
ACL-IJCNLP 2009, pages 45?48.
Matt Post. 2011. Judging Grammaticality with Tree Sub-
stitution Grammar Derivations. In Proceedings of ACL
2011, pages 217?222.
Ben Swanson and Eugene Charniak. 2012. Native Lan-
guage Detection with Tree Substitution Grammars. In
Proceedings of ACL 2012, pages 193?197.
Joel Tetreault, Daniel Blanchard, Aoife Cahill, and Mar-
tin Chodorow. 2012. Native tongues, lost and
found: Resources and empirical evaluations in native
language identification. In Proceedings of COLING
2012, pages 2585?2602.
Joel Tetreault, Daniel Blanchard, and Aoife Cahill. 2013.
A report on the first native language identification
shared task. In Proceedings of the Eighth Workshop
on Building Educational Applications Using NLP.
Oren Tsur and Ari Rappoport. 2007. Using classifier
features for studying the effect of native language on
the choice of written second language words. In Pro-
ceedings of CACLA, pages 9?16.
138
Sze-Meng Jojo Wong and Mark Dras. 2011. Exploiting
parse structures for native language identification. In
Proceedings of EMNLP 2011, pages 1600?1610.
Sze-Meng Jojo Wong, Mark Dras, and Mark Johnson.
2011. Topic modeling for native language identifi-
cation. In Proceedings of the Australasian Language
Technology Association Workshop 2011, pages 115?
124.
139
Proceedings of the Seventeenth Conference on Computational Natural Language Learning: Shared Task, pages 26?33,
Sofia, Bulgaria, August 8-9 2013. c?2013 Association for Computational Linguistics
NAIST at 2013 CoNLL Grammatical Error Correction Shared Task
Ippei Yoshimoto, Tomoya Kose, Kensuke Mitsuzawa, Keisuke Sakaguchi,
Tomoya Mizumoto, Yuta Hayashibe, Mamoru Komachi, Yuji Matsumoto
Nara Institute of Science and Technology
8916-5 Takayama, Ikoma, Nara 630-0192, Japan
{ippei-y,tomoya-kos,kensuke-mi,keisuke-sa,tomoya-m,yuta-h,komachi,matsu}@is.naist.jp
Abstract
This paper describes the Nara Institute
of Science and Technology (NAIST) er-
ror correction system in the CoNLL 2013
Shared Task. We constructed three sys-
tems: a system based on the Treelet Lan-
guage Model for verb form and subject-
verb agreement errors; a classifier trained
on both learner and native corpora for
noun number errors; a statistical machine
translation (SMT)-based model for prepo-
sition and determiner errors. As for
subject-verb agreement errors, we show
that the Treelet Language Model-based
approach can correct errors in which the
target verb is distant from its subject. Our
system ranked fourth on the official run.
1 Introduction
Grammatical error correction is the task of auto-
matically detecting and correcting grammatical er-
rors in text, especially text written by second lan-
guage learners. Its purpose is to assist learners in
writing and helps them learn languages.
Last year, HOO 2012 (Dale et al, 2012) was
held as a shared task on grammatical error cor-
rection, focusing on prepositions and determiners.
The CoNLL-2013 shared task (Dahlmeier et al,
2013) includes these areas and also noun number,
verb form, and subject-verb agreement errors.
We divide the above 5 error types into three
groups: (1) subject-verb agreement (SVA) and
verb form (Vform) errors, (2) noun number (Nn)
errors, and (3) preposition (Prep) and determiner
(ArtOrDet) errors. For the subject-verb agreement
and verb form errors, we used a syntactic language
model, the Treelet Language Model, because syn-
tactic information is important for verb error cor-
rection. For the noun number errors, we used a
binary classifier trained on both learner and native
corpora. For the preposition and determiner errors,
we adopt a statistical machine translation (SMT)-
based approach, aiming at correcting errors in con-
ventional expressions. After each subsystem cor-
rects the errors of the corresponding error types,
we merge the outputs of all the subsystems.
The result shows our system achieved 21.85
in F-score on the formal run before revision and
28.14 after revision.
The rest of this paper is organized as follows.
Section 2 presents an overview of related work.
Section 3 describes the system architecture of each
of the three subsystems. Section 4 shows experi-
mental settings and results. Section 5 presents dis-
cussion. Section 6 concludes this paper.
2 Related Work
Lee and Seneff (2008) tried correcting English
verb errors including SVA and Vform. They pro-
posed correction candidates with template match-
ing on parse trees and filtered candidates by uti-
lizing n-gram counts. Our system suggests candi-
dates based on the Part-Of-Speech (POS) tag of a
target word and filters them by using a syntactic
language model.
For the noun number errors, we improved the
system proposed by Izumi et al (2003). In
Izumi et al (2003), a noun number error detec-
tion method is a part of an automatic error de-
tection system for transcribed spoken English by
Japanese learners. They used a maximum entropy
method whose features are unigrams, bigrams and
trigrams of surface words, of POS tags and of
the root forms. They trained a classifier on only
a learner corpus. The main difference between
theirs and ours is a domain of the training corpus
and features we used. We trained a classifier on
the mixed corpus of the leaner corpus and the na-
tive corpus. We employ a treepath feature in our
system.
Our SMT system for correcting preposition and
26
determiner errors is based on Mizumoto et al
(2012). They constructed a translation model from
the data of the language-exchange social network
service Lang-81 and evaluated its performance for
18 error types, including preposition and deter-
miner errors in the Konan-JIEM Learner Corpus.
On preposition error correction, they showed that
their SMT system outperformed a system using
a maximum entropy model. The main difference
with this work is that our new corpus collection
here is about three times larger.
3 System Architecture
3.1 Subject-Verb Agreement and Verb Form
For SVA and Vform errors, we used the Treelet
Language Model (Pauls and Klein, 2012) to cap-
ture syntactic information and lexical information
simultaneously. We will first show examples of
SVA and Vform errors and then describe our model
used to correct them. Finally, we explain the pro-
cedure for error correction.
3.1.1 Errors
According to Lee and Seneff (2008), both SVA and
Vform errors are classified as syntactic errors. Ex-
amples are as follows:
Subject-Verb Agreement (SVA) The verb is not
correctly inflected in number and person with
respect to its subject.
They *has been to Nara many times.
In this example, a verb ?has? is wrongly in-
flected. It should be ?have? because its subject is
the pronoun ?they?.
Verb Form (Vform) This type of error mainly
consists of two subtypes,2 one of which includes
auxiliary agreement errors.
They have *be to Nara many times.
Since the ?have? in this sentence is an auxil-
iary verb, the ?be? is incorrectly inflected and it
should be ?been?.
The other subtype includes complementation
1http://lang-8.com
2In the NUCLE (Dahlmeier et al, 2013) corpus, most of
semantic errors related to verbs are included in other error
types such as verb tense errors, not Vform errors.
errors like the following:
They want *go to Nara this summer.
Verbs can be a complement of another verb
and preposition. The ?go? in the above sentence
is incorrect. It should be in the infinitive form, ?to
go?.
3.1.2 Treelet Language Model
We used the Treelet Language Model (Pauls and
Klein, 2012) for SVA and Vform error correction.
Our model assigns probability to a production
rule of the form r = P ? C1 ? ? ?Cd in a con-
stituent tree T , conditioned on a context h consist-
ing of previously generated treelets,3 where P is
the parent symbol of a rule r and Cd1 = C1 ? ? ?Cd
are its children.
p(r) = p(Cd1 |h)
The probability of a constituent tree T is given by
the following equation:
p(T ) =
?
r?T
p(r)
The context h differs depending on whether Cd1 is
a terminal symbol or a sequence of non-terminal
symbols.
Terminal When Cd1 is a terminal symbol w,
p(Cd1 |h) = p(w|P,R, r?, w?1, w?2)
where P is the POS tag of w, R is the right sibling
of P , r? is the production rule which yields P and
its siblings, and w?2 and w?1 are the two words
preceding w.
Non-Terminal When Cd1 is a sequence of non-
terminal symbols,
p(Cd1 |h) = p(Cd1 |P, P ?, r?)
where P is the parent symbol of Cd1 , P ? is the par-
ent symbol of P .
In order to capture a richer context, we apply the
annotation and transformation rules below to parse
trees in order. We use almost the same annota-
tion and transformation rules as those proposed by
3The term treelet is used to refer to an arbitrary connected
subgraph of a tree (Quirk et al, 2005)
27
Original Candidates
am/VBP, are/VBP or is/VBZ {am/VBP, are/VBP, is/VBZ}
was/VBD or were/VBD {was/VBD, were/VBD}
being/VBG {be/VB, being/VBG}
been/VBN {be/VB, been/VBN}
be/VB {be/VB, being/VBG, been/VBN}
Table 1: Examples of candidates in the case of ?be?
ROOT
S
VP
VP
ADVP
NNS
times
JJ
many
PP
NNP
Nara
TO
to
VBN
been
VBP
have
NP
PRP
They
ROOT
S@ROOT-have
VP@S-have
ADVP-NNTS
NNTS
times
JJ
many
PP-to
NNP
Nara
TO-to
to
VBN-been
been
VBP-have
have
PRP-they
They
Figure 1: The tree on the left is before annotations and transformations which convert it to the tree on
the right.
Pauls and Klein (2012). For instance, the common
CFG tree on the left side of Figure 1 is transformed
to the one on the right side.
Temporal NPs Pauls and Klein (2012) marked ev-
ery noun which is the head of an NP-TMP at least
once in the Penn Treebank. For example, NN ?
time is replaced with NNT ? time and NNS ?
times is replaced with NNTS ? times. This rule
seems to be useful for correcting verb tense er-
rors.4
Head Annotations We annotated every non-terminal
and preterminal with its head word.5 If the head
word is not a closed class word,6 we annotated
non-terminal symbols with the head POS tag in-
stead of the head word.
NP Flattening Pauls and Klein (2012) deleted NPs
dominated by other NPs, unless the child NPs are
in coordination or apposition. These NPs typically
4Verb tense (Vt) errors are not covered in this shared task.
5We identified the head with almost the same rules used
in Collins (1999).
6We took the following to be the closed class words: all
inflections of the verbs do, be, and have; and any word tagged
with IN, WDT, PDT, WP, WP$, TO, WRB, RP, DT, SYM,
EX, POS, PRP, AUX, MD or CC.
occur when nouns are modified by PPs. Our model
therefore assigns probability to nouns conditioned
on the head of modifying PPs with prepositions
such as ?in?, ?at? and so on by applying simul-
taneously the NP Flattening and the Head Annota-
tions. However, our model cannot assign probabil-
ity to prepositions conditioned on verbs or nouns
on which the prepositions depend. For this reason
we did not use our model to correct prepositional
errors.
Number Annotations Pauls and Klein (2012) di-
vided numbers into five classes: CD-YR for num-
bers that consist of four digits, which are usually
years; CD-NM for entirely numeric numbers; CD-
DC for numbers that have a decimal; CD-MX for
numbers that mix letters and digits; and CD-AL
for numbers that are entirely alphabetic.
SBAR Flattening They removed any S nodes which
are children of an SBAR.
VP Flattening They removed any VPs immedi-
ately dominated by a VP, unless it is con-
joined with another VP. The chains of verbs
are represented as separated VPs for each verb,
such as (VP (MD will) (VP (VB be) (VP (VBG
28
playing) . . .))). This transformation turns the
above VPs into (VP (MD will) (VB be) (VBG
playing) . . .). This has an effect on the cor-
rection of auxiliary agreement errors because
our model can assign probability to main verbs
strongly conditioned on their auxiliary verbs.
Gapped Sentence Annotation They annotated all S
and SBAR nodes that have a VP before any NP.
Parent Annotation They annotated all VPs and chil-
dren of the ROOT node with their parent symbol.
Unary Deletion All unary rules are deleted except
the root and the preterminal rules. We kept only
the bottom-most symbol of the unary rule chain.
This brings many symbols into the context of a
production rule.
3.1.3 Procedure
Our system for SVA and Vform errors tries to cor-
rect the words in a sentence from left to right. Cor-
rection proceeds in the following steps.
1. If the POS tag of the word is ?VB?, ?VBD?,
?VBG?, ?VBN?, ?VBP? or ?VBZ?, our sys-
tem generates sentences which have the word
replaced with candidates. For example, if the
original word is an inflection of ?be?, the sys-
tem generates candidates as shown in Table
1.
2. The system parses those sentences and ob-
tains the k-best parses for each sentence.
3. The system keeps only the one sentence to
which our language model assigned the high-
est probability in the parses.
4. The system repeats Steps 1 to 3 with the sen-
tence kept in Step 3 until the rightmost word
of that sentence.
Note that the system uses the Berkeley Parser7 in
Step 2.
3.2 Noun Number
3.2.1 Errors
A noun number error is the mistake of using the
singular form for a plural noun, and vice versa, as
in the following:
7http://code.google.com/p/
berkeleyparser/
I saw many *student yesterday.
In this example, the inflection of ?student?
is mistaken. It should be ?students? because it is
modified by ?many?.
To correct such errors, we use a binary classi-
fication approach because the inflection of a noun
is either ?singular? or ?plural?. If the binary clas-
sifier detects an error with a sufficiently high con-
fidence, the system changes the noun form. We
adopt the adaptive regularization of weight vectors
algorithm (AROW) (Crammer et al, 2009). AROW
is a variant of a confidence weighted linear classi-
fication algorithm which is suitable for the classi-
fication of large scale data.
3.2.2 Binary classifier approach
The binary classifier indicates ?singular? or ?plu-
ral? for all nouns except proper and uncountable
nouns. First, if a noun is found in the training cor-
pus, we extract an instance with features created
by the feature template in Table 2.8 Second, we
train a classifier with extracted instances and la-
bels from the training corpus.
We use unigram, bigram, and trigram features
around the target word and the path features be-
tween the target word and all the other nodes in
the NPs that dominate the target word as the right-
most constituent. The path feature is commonly
used in semantic role labeling tasks (Pradhan et
al., 2004). For the path features, we do not use
the right subtree of the NP as the path features be-
cause we assume that right subtrees do not affect
the number of the target word. We limit the maxi-
mum depth of the subtree containing the NP to be
3 because nodes over this limit may be noisy. To
encode the relationship between the target word
and another node in the NP, we append a symbol
which reflects the direction of tree traversal to the
label: ?p? for going up (parent) and ?c? for going
down (child). For example, we show extracted fea-
tures in Table 2 for the phrase ?some interesting
and recent topics about politics and economics?.
In the training corpus, since the proportions of
singular and plural nouns are unequal, we set dif-
ferent thresholds for classifying singular and plu-
ral forms. These thresholds limit the probabilities
which the binary classifier uses for error detection.
We have used a development set to determine the
8Target word refers to a noun whose POS tag is ?NN? or
?NNS? in the Penn Treebank tagset.
29
Feature name Word, Pos used as features Example
surface unigram word?1, word?2 and, recent, about, politics
surface bigram word?2 word?1 and recent, about politics
surface trigram word?3 word?2 word?1 interesting and recent, about politics and
POS unigram POS?1, POS?2 CC, JJ, IN, NN
POS bigram POS?1 POS?2 CC JJ, IN NN
POS trigram POS?3 POS?2 POS?1 JJ CC JJ, IN NN CONJ
lemma unigram lemma?2, lemma?1 and, recent, about, politics
lemma bigram lemma?2 lemma?1 and recent, about politics
lemma trigram lemma?3 lemma?2 lemma?1 interesting and recent, about politics and
lemma target lemma of target word topic
path feature path between the target word p NP, pc JJ, pc recent, pp NP, ppc CC, ppc and,
and the other nodes in NP ppc NP, ppcc DT, ppcc some, ppcc JJ, ppcc interesting
Table 2: Features used for the detection of noun number errors and example features for the phrase ?some
interesting and recent topics about politics and economics?.
best thresholds for singular and plural forms, re-
spectively.
For proper and uncountable nouns, we do not
change number because of the nature of those
nouns. In order to determine whether to change
number or not, we create a list which consists of
words frequently used as singular forms in the na-
tive corpus.
3.3 Prepositions and Determiners
For preposition and determiner errors, we con-
struct a system using a phrase-based statistical
machine translation (Koehn et al, 2003) frame-
work. The SMT-based approach functions well
in corrections of conventional usage of determin-
ers and prepositions such as ?the young? and ?take
care of ?. The characteristic of the SMT-based ap-
proach is its ability to capture tendencies in learn-
ers? errors. This approach translates erroneous
phrases that learners often make to correct phrases.
Hence, it can handle errors in conventional expres-
sions without over-generalization.
The phrase-based SMT framework which we
used is based on the log-linear model (Och and
Ney, 2002), where the decision rule is expressed
as follow:
argmax
e
P (e|f) = argmax
e
M
?
m=1
?mhm(e, f)
Here, f is an input sentence, e are hypotheses,
hm(e, f) feature functions and ?m their weights.
The hypothesis that maximizes the weighted sum
of the feature functions is chosen as an output sen-
tence.
The feature functions encode components of
the phrase-based SMT, including the translation
model and the language model. The translation
model suggests translation hypotheses and the lan-
guage model filters out ill-formed hypotheses.
For an error correction system based on SMT,
the translation model is constructed from pairs of
original sentences and corrected sentences, and the
language model is built on a native corpus (Brock-
ett et al, 2006).
Brockett et al (2006) trained the translation
model on a corpus where the errors are restricted
to mass noun errors. In our case, we trained our
model on a corpus with no restriction on error
types. Consequently, the system corrects all types
of errors. To focus on preposition and determiner
errors, we retain proposed edits that include 48
prepositions and 25 determiners listed in Table 3.
4 Experiments
4.1 Experimental setting
4.1.1 Subject-Verb Agreement and Verb
Form
We describe here the training data and tools used
to train our model. Our model was trained with the
Berkeley LM9 version 1.1.3. We constructed the
training data by concatenating the WSJ sections of
the Penn Treebank and the AFP sections of the En-
glish Gigaword Corpus version 5.10 Our training
data consists of about 27 million sentences. Al-
though human-annotated parses for the WSJ are
available, there is no gold standard for the AFP,
so we parsed the AFP automatically by using the
Berkeley Parser released on October 9, 2012.
9http://code.google.com/p/berkeleylm/
10LDC2011T07
30
Preposition about, across, after, against, along, among, around, as, at, before, behind, below,
beside, besides, between, beyond, but, by, despite, down, during, for, from, in,
inside, into, near, of, off, on, onto, opposite, outside, over, past, round, without,
than, through, to, toward, towards, under, until, up, upon, with, within
Determiner the, a, an, all, these, those, many, much, another, no, some, any, my,
our, their, her, his, its, no, each, every, certain, its, this, that
Table 3: Preposition and determiner lists
4.1.2 Noun Number
We trained a binary classifier on a merged corpus
of the English Gigaword and the NUCLE data.
From the English Gigaword corpus, we used the
New York Times (NYT) as a training corpus. In
order to create the training corpus, we corrected
all but noun number errors in the NUCLE data us-
ing gold annotations.
The AROW++ 11 0.1.2 was used for the binary
classification. For changing noun forms, we used
the pattern.en toolkit.12
The maximum depth of subtrees containing an
NP is set to 3 when we extracted the path features.
We built and used a list of nouns that appear in
singular forms frequently in a native corpus. We
counted the frequency of nouns in entire English
Gigaword. If a noun appears in more than 99%13
of occurrences in singular form, we included it in
the list. The resulting list contains 836 nouns.
4.1.3 Prepositions and Determiners
We used Moses 2010-08-13 with default parame-
ters for our decoder14 and GIZA++ 1.0.515 as the
alignment tool. The grow-diag-final heuristics was
applied for phrase extraction. As a language mod-
eling tool we used IRSTLM version 5.8016 with
Witten-Bell smoothing.
The translation model was trained on the NU-
CLE corpus and our Lang-8 corpus.17 From the
Lang-8 corpus, we filtered out noisy sentences.
Out of 1,230,257 pairs of sentences, 1,217,124
pairs of sentences were used for training. As for
the NUCLE corpus we used 55,151 pairs of sen-
tences from the official data provided as training
11https://code.google.com/p/arowpp/
12http://www.clips.ua.ac.be/pages/
pattern-en
13We tested many thresholds, and set 99% as threshold.
14http://sourceforge.net/projects/
mosesdecoder/
15http://code.google.com/p/giza-pp/
16http://sourceforge.net/projects/
irstlm/
17consisting of entries through 2012.
data. We used a 3-gram language model built on
the entire English Gigaword corpus.
4.2 Result
Table 4 shows the overall results of our submit-
ted systems and the results of an additional ex-
periment. In the additional experiment, we tried
the SMT-based approach described in Section 3.3
for errors in SVA, Vform and Nn. While the sys-
tem based on the Treelet Language Model out-
performed the SMT-based system on the SVA er-
rors and the Vform errors, the binary classifier ap-
proach did not perform as well as the SMT-based
system on the Nn errors.
5 Discussion
5.1 Subject-Verb Agreement and Verb Form
We provide here examples of our system?s output,
beginning with a successful example.
source: This is an age which most people *is re-
tired and *has no sources of incomes.
hypothesis: This is an age which most people are
retired and have no sources of incomes.
The source sentence of this pair includes two SVA
errors. The first is that ?be? should agree with its
subject ?people? and must be ?are?. Our system is
able to correct errors where the misinflected pred-
icate is adjacent to its subject. The second error
is also an agreement error, in this case between
?have? and its subject ?people?. Our model can
assign probability to yields related to predicates
conditioned strongly on their subjects even if the
distance between the predicate and its subject is
long. The same can be said of Vform errors.
One mistake made by our system is miscorrec-
tion due to the negative effect of other errors.
source/hypothesis: The rising life *expectancies
*are like a two side sword to the modern world.
31
submitted system additional experiments
ALL Verb Nn Prep ArtOrDet Verb Nn
Precision 0.2707 0.1378 0.4452 0.2649 0.3118 0.2154 0.3687
original Recall 0.1832 0.2520 0.1641 0.1286 0.2029 0.0569 0.2020
F-score 0.2185 0.1782 0.2399 0.1732 0.2458 0.0900 0.2610
Precision 0.3392 0.1814 0.5578 0.3245 0.4027 0.3846 0.4747
revised Recall 0.2405 0.2867 0.1708 0.1494 0.2497 0.0880 0.2137
F-score 0.2814 0.2222 0.2616 0.2046 0.3082 0.1433 0.2947
Table 4: Results of the submitted system for each type of error and results of additional experiments
with the SMT-based system. The score is evaluated on the m2scorer (Dahlmeier and Ng, 2012). ALL
is the official result of formal run, and each of the others shows the result of the corresponding error
type. Since our system did not distinguish SVA and Vform, we report the combined result for them in the
column Verb.
gold: The rising life expectancy is like a two side
sword to the modern world.
Since the subject of ?are? is ?expectancies?, the
sentence looks correct at first. However, this ex-
ample includes not only an SVA error but also an
Nn error, and therefore the predicate ?are? should
be corrected along with correcting its subject ?ex-
pectancies?.
An example of a Vform error is shown below.
source/hypothesis: Besides, we can try to reduce
the bad effect *cause by the new technology.
gold: Besides, we can try to reduce the bad effect
caused by the new technology.
The word ?cause? is tagged as ?NN? in this sen-
tence. This error is ignored because our system
makes corrections on the basis of the original POS
tag. For a similar example, our system does not
make modifications between the to-infinitive and
the other forms.
5.2 Noun Number
We provide here examples of our system?s output,
beginning with a successful example.
source: many of cell *phone are equipped with
GPS
hypothesis/gold: many of cell phones are
equipped with GPS
In the example, the noun ?phone? should be in the
plural form ?phones?. This is because the phrase
?many of? modifies the noun. In this case, the un-
igrams ?many? and ?are?, and the bigram ?many
of? are features with strong weights for the plural
class as expected.
However, n-gram features sometimes work to
the contrary of our expectations.
source/hypothesis: RFID is not only used to
track products for logistical and storage *purpose,
it is also used to track people
gold: RFID is not only used to track products for
logistical and storage purposes, it is also used to
track people
The ?purpose? is in the PP which is modified by
?products?. Thus, ?purpose? should not be af-
fected by the following words. However, the verb
?is?, which is immediately after ?purpose?, has a
strong influence to keep the word in singular form.
Therefore, it may be better not to use a verb that
the word is not immediately dependent on as a fea-
ture.
5.3 Prepositions and Determiners
While the SMT-based system can capture the
statistics of learners? errors, it cannot correct
phrases that are not found in the training corpus.
(1) source: *with economic situation
gold: in economic situation
(2) source: *with such situation
gold: in such situation
Our system was not able to correct the source
phrase in (1), in spite of the fact that the similar
phrase pair (2) was in the training data. To correct
such errors, we should construct a system that al-
lows a gap in source and target phrases as in Galley
and Manning (2010).
32
6 Conclusion
This paper described the architecture of our cor-
rection system for errors in verb forms, subject
verb agreement, noun number, prepositions and
determiners. For verb form and subject verb
agreement errors, we used the Treelet Language
Model. By taking advantage of rich syntactic in-
formation, it corrects subject-verb agreement er-
rors which need to be inflected according to a dis-
tant subject. For noun number errors, we used a
binary classifier using both learner and native cor-
pora. For preposition and determiner errors, we
built an SMT-based system trained on a larger cor-
pus than those used in prior works. We show that
our subsystems are effective to each error type. On
the other hand, our system cannot handle the er-
rors strongly related to other errors well. In future
work we will explore joint correction of multiple
error types, especially noun number and subject-
verb agreement errors, which are closely related
to each other.
Acknowledgements
We would like to thank Yangyang Xi for giving us
permission to use text from Lang-8 and the anony-
mous reviewers for helpful comments.
References
Chris Brockett, William B. Dolan, and Michael Ga-
mon. 2006. Correcting ESL errors using phrasal
smt techniques. In Proceedings of COLING/ACL
2006, pages 249?256.
Michael Collins. 1999. Head-Driven Statistical Mod-
els for Natural Language Parsing. Ph.D. thesis,
University of Pennsylvania.
Koby Crammer, Alex Kulesza, and Mark Dredze.
2009. Adaptive regularization of weight vectors. In
Proceedings of NIPS 2009, pages 414?422.
Daniel Dahlmeier and Hwee Tou Ng. 2012. Better
evaluation for grammatical error correction. In Pro-
ceedings of NAACL 2012, pages 568?572.
Daniel Dahlmeier, Hwee Tou Ng, and Siew Mei Wu.
2013. Building a large annotated corpus of learner
english: The NUS corpus of learner English. In Pro-
ceedings of BEA 2013, pages 313?330.
Robert Dale, Ilya Anisimoff, and George Narroway.
2012. HOO 2012: a report on the preposition and
determiner error correction shared task. In Proceed-
ings of BEA 2012, pages 54?62.
Michel Galley and Christopher D. Manning. 2010.
Accurate non-hierarchical phrase-based translation.
In Processing of HLT/NAACL 2010, pages 966?974.
Emi Izumi, Kiyotaka Uchimoto, Toyomi Saiga, Thep-
chai Supnithi, and Hitoshi Isahara. 2003. Auto-
matic error detection in the Japanese learners? En-
glish spoken data. In Proceedings of ACL 2003,
pages 145?148.
Philipp Koehn, Franz Josef Och, and Daniel C Marcu.
2003. Statistical phrase-based translation. In Pro-
ceedings of HLT/NAACL 2003, pages 48?54.
John Lee and Stephanie Seneff. 2008. Correcting mis-
use of verb forms. In Proceedings of ACL 2008,
pages 174?182.
Tomoya Mizumoto, Yuta Hayashibe, Mamoru Ko-
machi, Masaaki Nagata, and Yuji Matsumoto. 2012.
The effect of learner corpus size in grammatical er-
ror correction of ESL writings. In Proceedings of
COLING 2012, pages 863?872.
Franz Josef Och and Hermann Ney. 2002. Discrimina-
tive training and maximum entropy models for sta-
tistical machine translation. In Proceedings of ACL
2002, pages 295?302.
Adam Pauls and Dan Klein. 2012. Large-scale syntac-
tic language modeling with treelets. In Proceedings
of ACL 2012, pages 959?968.
Sameer S Pradhan, Wayne H Ward, Kadri Hacioglu,
James H Martin, and Dan Jurafsky. 2004. Shallow
semantic parsing using support vector machines. In
Proceedings of HLT/NAACL 2004, pages 233?240.
Chris Quirk, Arul Menezes, and Colin Cherry. 2005.
Dependency treelet translation: Syntactically in-
formed phrasal SMT. In Proceedings of ACL 2005,
pages 271?279.
33
