Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers,
pages 632?643, Dublin, Ireland, August 23-29 2014.
 Interpretation of Chinese Discourse Connectives  
for Explicit Discourse Relation Recognition 
 
 
Hen-Hsen Huang, Tai-Wei Chang, Huan-Yuan Chen, and Hsin-Hsi Chen 
Department of Computer Science and Information Engineering 
National Taiwan University 
No. 1, Sec. 4, Roosevelt Road, Taipei, 10617 Taiwan 
{hhhuang, twchang}@nlg.csie.ntu.edu.tw; 
{b00902057, hhchen}@ntu.edu.tw 
 
  
 
Abstract 
This paper addresses the specific features of Chinese discourse connectives, including types 
(word-pair and single-word), linking directions (forward and backward linking), positions and 
ambiguous degrees, and discusses how they affect the discourse relation recognition. A semi-
supervised learning method is proposed to learn the probability distributions of discourse func-
tions of connectives from a small labeled dataset and a big unlabeled dataset. The statistics 
learned from the dataset demonstrates some interesting linguistic phenomena such as connec-
tive synonyms sharing similar distributions, multiple discourse functions of connectives, and 
couple-linking elements providing strong clues for discourse relation resolution.  
1 Introduction 
Discourse relation labeling determines how two discourse units cohere to each other. A discourse unit 
may be a clause, a sentence, or a group of sentences. The labeled relation has many potential applica-
tions. Coherence is considered as a metric to evaluate the essay writing by essay scorer (Lin et al., 
2011). Discourse relations are used to order sentences in an event in a summarization system (Der-
czynski and Gaizauskas, 2013). Sentiment transition of two clausal arguments is identified based on 
their discourse relation in sentiment analysis (Hutchinson, 2004; Zhou et al., 2011; Wang et al., 2012; 
Huang et al., 2013).  
The pioneer research of discourse has been established by Hobbs (1985), Polanyi (1988), Hovy and 
Maier (1992), and Asher and Lascarides (1995). Various discourse relation types have been defined in 
the frameworks such as Sanders et al. (1992), Hovy and Maier (1992), RST-DT (Carlson et al., 2002), 
Wolf and Gibson (2005), and PDTB (Prasad et al., 2008). Temporal, Contingency, Comparison, and 
Expansion, the four classes on the top level of PDTB sense hierarchy, are common used in the dis-
course relation labeling tasks. When two arguments are temporally related, they form a Temporal rela-
tion. The Contingency relation talks about the situation that the event in one argument casually affects 
the event in the other argument. Comparison is used to show the difference between two arguments. 
The last one relation, Expansion, is the most common. An Expansion relation either expands the in-
formation for one argument in the other one or continues the narrative flow. 
In the recent years, discourse relation recognition has been studied for different languages (Afan-
tenos et al., 2012, Cartoni et al., 2013). In explicit English discourse relation labeling tasks, the accu-
racy of the approach using just the connectives is already quite high, 93.67%, and incorporating the 
syntactic features raises performance to 94.15% (Pitler and Nenkova, 2009). In our previous work, we 
investigate Chinese intra-sentential relation detection and show an accuracy of 81.63% and an F-score 
of 71.11% in the two-way classification (Contingency vs. Comparison relations) when connectives are 
This work is licensed under a Creative Commons Attribution 4.0 International Licence. Page numbers and proceedings footer 
are added by the organisers. Licence details: http://creativecommons.org/licenses/by/4.0/. 
632
introduced as features (Huang and Chen, 2012a). We also report an accuracy of 27.10% and an F-
score of 24.27% in the four-way inter-sentential relation classification when only connectives are used 
(Huang and Chen, 2011). Sporleder and Lascarides (2008) point out some English connectives are of-
ten ambiguous between multiple discourse relations or between discourse and non-discourse usage, 
and Roze et al. (2010) report the ambiguity of French connectives. This issue also occurs in Chinese. 
Zhou et al. (2012) propose a framework to identify the ambiguous Chinese discourse connectives, and 
report an F-score of 74.81% in the four-way classification at the intra-sentential level. 
The above discourse relation labeling tasks are done on the datasets of different size for different 
languages at the intra-/inter-sentential levels, thus the results cannot be compared directly. However, 
these works show a tendency: discourse connectives are useful clues for explicit discourse relation 
recognition, and the uses of Chinese connectives in discourse relation labeling are more challenging 
than those of English connectives. In comparison with English, the connectives in Chinese are more 
and their parts of speech are diverse. There are 100 English explicit connectives annotated in the 
PDTB 2.0. In Chinese, the linguists report a list of 808 discourse connectives (Cheng and Tian, 1989; 
Cheng, 2006). In addition, the Chinese discourse connectives have a variety of parts-of-speech. For 
example, ?? (ji? sh?, suppose) is a verb and listed as a discourse connective of the Contingency rela-
tion. 
The following examples address some specific features of Chinese discourse connectives. On the 
one hand, the two words, ???? (su? r?n, although) and ???? (d?n sh?, but), which form a word-pair 
connective, appear in the two discourse units shown in (S1), respectively. These two units demonstrate 
a Comparison relation. On the other hand, ???? (su? r?n, although) and ???? (d?n sh?, but) can ap-
pear individually as single-word connectives shown in (S2)-(S6). The two discourse units have differ-
ent discourse relations when the single-word connectives appear at different positions, i.e., (S2): Com-
parison, (S3): Comparison, (S4): Expansion, (S5): Comparison, and (S6): Expansion. Furthermore, 
the short word ??? (?r) can be an individual connective, which is interpreted as ???? (and), ???? 
(but), or ???? (thus), and serves as functions of Expansion, Comparison, and Contingency, respec-
tively. In addition, it can be linked with ???? (su? r?n, although) and ???? (y?n w?i, because) to be 
word-pair connectives, which are interpreted as Comparison and Contingency functions in (S7) and 
(S8), respectively. These examples demonstrate word-pair connectives composed of a same word and 
other words may have different discourse functions, so does the same single-word connective at dif-
ferent positions. 
 
(S1) ????????????????(Although Tom is smart, he doesn?t study hard.) 
(S2) ??????????????(Although Tom is smart, he doesn?t study hard.) 
(S3) ??????????????(He sweated a lot, although he went only a few miles.) 
(S4) ????????????????????(I'll read, even if I really feel spider terrible.) 
(S5) ??????????????(Tom is smart, but he doesn?t study hard.) 
(S6) ?????????????(But in Paris, he gave up studying medicine.) 
(S7) ??????????????(Although you did not say, I knew that smell.) 
(S8) ??????????????(Because he came home late, he was scolded by his mother.)  
 
In this paper, we investigate special features of Chinese discourse connectives and apply the results 
to discourse relation labeling. A semi-supervised learning algorithm is proposed to estimate the proba-
bility distribution of the discourse functions of each connective. We address the issue of ambiguity 
between multiple discourse relations of Chinese connectives. The ambiguity between discourse and 
non-discourse usages is not our focus in this paper. This paper is organized as follows. Section 2 anal-
yses the types of Chinese connectives and their forward/backward linking properties. Section 3 pre-
sents a semi-supervised method to deal with the probability distributions of discourse functions of 
Chinese connectives and discourse relation labeling. The experimental results are shown and discussed. 
In Section 4, we further introduce the discourse relation labeler to annotate 302,293 unlabeled sen-
tences and analyze the linguistic phenomena of discourse connectives. We conclude this work in Sec-
tion 5. 
633
2 Types of Discourse Connectives 
From the surface form, there are three kinds of linking elements in Chinese (Li and Thompson, 1981): 
forward-linking elements, backward-linking elements, and couple-linking elements. Discourse con-
nectives are such kinds of linking elements. A discourse unit containing a forward-linking (backward-
linking) element is linked with its next (previous) discourse unit. A couple-linking element is a pair of 
words that exist in two discourse units (Chen, 1994).  
Figure 1 shows connectives and their linking direction. The word-pair connective ???...??? 
(su? r?n?d?n sh?, although?but) in (S1) is a couple-linking element. A single-word connective may 
function as a forward-linking element and/or a backward-linking element. It may be a word appearing 
in a word-pair connective, e.g., ???? (su? r?n, although), or a word existing individually, e.g., ???? 
(y? j?, and). A single-word connective which is the first (the second) word of a word-pair connective 
may function as a forward-linking (backward-linking) element. The single-word connective ???? 
(su? r?n, although) in (S2) is a typical example. It keeps the major discourse function, i.e., Comparison, 
of the word-pair connective that it belongs when it appears in the first discourse unit. In contrast, it 
may become ambiguous when its position is reversed from the first to the second (i.e., S3 and S4). It 
may link to the previous or the next discourse units. S5 and S6 have the similar behaviors. The single-
word ???? (d?n sh?, but) in (S5) shows a backward-linking. In (S6), it is shifted to the first position 
and becomes ambiguous. It may be linked to the previous, or to the next discourse units. The correct 
interpretation depends on the context. These phenomena show a single-word connective may have dif-
ferent senses when it is not at its original position. 
 
  
   
 
  Figure 1: Examples for forward linkging and backward linking. 
 
In this study, we collect 808 discourse connectives based on Cheng and Tian (1989), Cheng (2006), 
and Lu (2007). The discourse connective lexicon contains 319 single-word and 489 word-pair connec-
tives. Initially, each connective is associated with only one discourse function manually by linguists. 
634
For example, the word-pair connective, ???...??? (su? r?n?d?n sh?, although?but), is assigned a 
Comparison function. The assignment is one-to-one mapping, thus it cannot capture the complete dis-
course functions of Chinese connectives. Table 1 shows an overview of the discourse connective lexi-
con. In this lexicon, Expansion is the majority, and Comparison is the minority. The percentages of 
Contingency and Expansion are close. Temporal is the third largest discourse function. Intuitively, the 
discourse connective lexicon cannot cover all their senses. To learn the probability distribution of the 
discourse functions of a connective needs a large-scale discourse corpus. Compared with RST-DT 
(Carlson et al., 2002) and PDTB (Prasad et al., 2008), Chinese discourse corpora are not publicly 
available (Zhou and Xue, 2012; Huang and Chen, 2012b). 
 
Discourse Function Number of Connectives Examples of Single-Word and Word-Pair Discourse Connectives 
Temporal 151 (18.69%) ?? (ji? zhe, then), ??...?? (zu? ch??xi?n z?i, first...now) 
Contingency 261 (32.30%) ?? (y?n w?i, because), ?...? (r??z?, if ... then) 
Comparison 87 (10.77%) ?? (j? sh?, even if), ???? (j?n gu?n?d?n, although?but)  
Expansion 309 (38.24%) ?? (l?ng w?i, besides), ????? (b? j?n??r qi?, not only?but also) 
Table 1: A Chinese discourse connective lexicon. 
3 Learning Discourse Functions of Connectives 
This section proposes a semi-supervised learning method to learn the interpretation of discourse con-
nectives from an incomplete and sparse dataset. 
3.1 A Semi-Supervised Learning Algorithm 
Given a pair of discourse units ds1 and ds2 containing an explicit connective c, a discourse relation 
classifier drc aims at selecting a relation r from the set {Temporal, Contingency, Comparison, Expan-
sion} to illustrate how ds1 and ds2 cohere to each other. The connective c may be a word-pair c1?c2, 
where c1 and c2 appear in ds1 and ds2, respectively. It may be a single word appearing in ds1 or ds2. 
Each discourse unit is mapped into a representation. Various features from different linguistic levels 
have been explored in the related work (Huang and Chen, 2011; Huang and Chen, 2012a; Zhou et al, 
2011; Zhou et al., 2012). We adopt some of their features shown as follows. Here we focus in particu-
lar on the probability distributions of the discourse functions and the positions of connectives. 
  
Length. This feature includes the word counts of ds1 and ds2. 
Punctuation. The punctuation at the end of ds2 is regarded as a feature. The possible punctuation 
includes a full stop, a question mark, or an exclamation mark. The punctuation at the end of ds1 is 
dropped from the features because it is always a comma.   
Words. The bags of words in ds1 and ds2 are considered.  
Hypernym. The bags of hypernyms of the words in ds1 and ds2 are considered. A Chinese thesau-
rus, Tongyici Cilin1, is consulted. The categorization scheme at the fourth level is adopted. 
Shared Word. The number of words shared in ds1 and ds2 is considered as a feature. 
Collocated Word. Collocated words are word pairs mined from the training set. The first and the 
second words of a pair come from ds1 and ds2, respectively. 
POS. The bags of parts of speech in ds1 and ds2 are considered. 
Polarity. Polarity and discourse relation may be related (Huang et al., 2013; Zhou, et al., 2011). 
For example, a Comparison relation implies its two discourse units are contrasting, and some contrasts 
are presented with different polarities. We estimate the polarity of ds1 and ds2 by a lexicon-based ap-
proach. The polarity score and the existence of negation are taken as features. 
Discourse Connective. A discourse connective c is represented as a probability distribution of dis-
course functions denoted by a quadruple (P(c,temporal), P(c,contingency), P(c,comparison), P(c,expansion)), where 
P(c,temporal), P(c,contingency), P(c,comparison), and P(c,expansion) indicate the probabilities of the four discourse func-
tions of c, such that P(c,temporal)+P(c,contingency)+P(c,comparison)+P(c,expansion)=1. Section 3.3 shows how we as-
sign the probabilities to each connective in different experimental settings.    
Position. The linguistic phenomena discussed in Section 2 show a single-word connective at dif-
ferent position may play different discourse function. Thus, the position of c is considered as a feature. 
                                                 
1 http://ir.hit.edu.cn/ 
635
Because the number of Chinese connectives is large (e.g., 808 Chinese connectives in our lexicon) 
and the large-scale labeled Chinese discourse corpus is not available, how to learn the probability dis-
tribution is a challenging issue. This paper proposes a semi-supervised learning method as follows. Its 
pseudo code is shown in Algorithm 1. 
 
(1) Train a 4-way discourse relation classifier drc with the training set and LIBSVM (Chang and 
Lin, 2011). 
(2) Initialize probability distributions of unknown connectives in the test set (see experiments). 
(3) Use drc to label all the instances in the test set. 
(4) Compute the new probability distribution of discourse functions of each connective based on 
the labeled results in the current run. Maximum likelihood estimation is adopted. 
(5) Repeat (3) and (4) until the number of label changes between two successive runs is below 1%. 
 
Algorithm 1. Probability Estimation for the Discourse Functions of Connectives 
Input:  
D={Temporal, Contingency, Comparison, Expansion}: a set of discourse relations and discourse 
functions for argument pairs and discourse connectives, 
C={c1, c2, ?, cn}: a set of n discourse connectives, 
S={s1, s2, ?, sp}: a set of p labeled argument-pairs [sa1, sa2] containing connective c?CS?C, each 
with a label d?D, where CS is a set of connectives appearing in S, 
T={t1, t2, ?, tq}: a set of q unlabeled argument-pairs [ta1, ta2] containing connective c?CT?C, where 
CT is a set of connectives appearing in T. 
Output:  
Q={q1, q2, ?, qn}: a probability distribution qi for connective ci?C. 
Method: 
1. Initialization 
1) Train a classifier drc using S. 
2) Initialize the probability distribution with equal weight, (0.25, 0.25, 0.25, 0.25), for connec-
tive c ? CT-CS, and build Q(0). 
3) i ? 0 
2. Relation labeling 
For each t ? T, estimate the probabilities of four discourse relations, P(t,temporal), P(t,contingency), 
P(t.comparison), and P(t.expansion), using the classifier drc with Q
(i). 
3. Updating the probability distribution 
1) For each c ? C, compute the average probability of each discourse relation among the argu-
ment-pairs containing c in T:  
P(c,tempora)l ? Average of P(t,temporal) for all t containing c in T. 
P(c,contingency) ? Average of P(t,contingency) for all t containing c in T. 
P(c,comparison) ? Average of P(t,comparison) for all t containing c in T. 
P(c,expansion) ? Average of P(t,expansion) for all t containing c in T. 
2) Form a new Q(i+1) 
3) i ? i+1 
4. Repeat steps 2-3 until the ratio of the number of label changes by previous and current runs is less 
than 1%. 
5. Q ? Q(i) 
 
3.2 Experimental Setup 
For the corpus study of discourse connectives and discourse relations, we refer to a public available 
Chinese Web POS tagged corpus (Yu et al., 2012). This Chinese POS-tagged corpus is developed 
based on the ClueWeb09 dataset (CMU, 2009), where Chinese material is the second largest.  To cap-
ture the discourse functions of individual connectives more accurately, the following three criteria are 
used to sample sentences: 
 
1. A sentence should contain only two clauses. 
2. A sentence should contain exact one discourse connective. 
636
3. The lengths of both clauses in a sentence are no more than 20 Chinese characters.  
 
Total 7,601 sentences composed of two discourse units linked by a connective are sampled from a 
public available Chinese Web POS tagged corpus (Yu et al., 2012). Each sentence is annotated with a 
most likely discourse relation selected from {Comparison, Contingency, Comparison, Expansion} by 
three annotators guided by an instruction manual. The majority is taken as the ground truth. A mentor 
is involved to make a final decision for the tie conditions. The inter-agreement among the annotators is 
0.41 in Fleiss? Kappa values, which is a moderate agreement. The discourse category with the lowest 
inter-annotation agreement is Temporal, which annotators usually confuse with Expansion. It shows 
the difficulty to distinguish Temporal and Expansion even by human. Table 2 shows the statistics of 
the corpus. More than 50% of pairs are annotated with Expansion relation. The second largest group is 
Contingency relation. The percentages of Temporal and Comparison relations are near. Only 359 con-
nectives appear in the corpus. That reflects the incompleteness issue. 
 
Discourse Relation # Instances Percentage 
Temporal 846 11.13% 
Contingency 1,594 20.97% 
Comparison 926 12.18% 
Expansion 4,235 55.72% 
Table 2: Statistics of the experimental discourse corpus. 
 
This Chinese discourse corpus is used for training and testing. We set up the experiments to simu-
late the scenario of estimating the probability distributions of discourse functions of the unknown con-
nectives based on the information in the training set. We evaluate the experimental results by 5-fold 
cross-validation. To ensure the discourse connectives appearing in the test set are mutual exclusive of 
those connectives in the training set, we split the discourse connectives into 5 mutual exclusive sets 
and split all the 7,601 sentences into 5 folds according to the 5 sets of discourse connectives.  
The kernel of our SVM classifier is the radial basis function. The two parameters, cost c and gamma 
g, are optimized by the grid-search algorithm within the range c ? {2-5, 2-3, 2-1, ?, 215} and g ? {2-15, 
2-13, 2-11, ?, 23}.  
3.3 Results and Discussions 
To demonstrate the performance of our proposed semi-supervised learning methods, the following five 
models are experimented and compared. 
 
M0:  Label the relation between two discourse units linked by a connective c based on the c?s dis-
course function defined in the connective lexicon. M0 is considered as a baseline model. 
M1: Train a 4-way discourse relation classifier drc with the training set, then initialize the function 
probability distributions of the unknown connectives to (0.25, 0.25, 0.25, 0.25), and finally la-
bel all the pairs of discourse units by the classifier drc. M1 is a supervised-learning method. 
M2: M2 model is similar to M1 model except that the probability distribution (p(c,temporal), p(c,contingency), 
p(c,comparison), p(c,expansion)) of an unknown connective is initialized based on its setting in the con-
nective lexicon. The probability of the unique function is set to 1, and the others are set to 0. 
M3: M3 is a semi-supervised learning method. In testing, the function probability distributions of 
the unknown connectives are initialized to (0.25, 0.25, 0.25, 0.25). Discourse relation labeling 
and probability distribution updating are done iteratively. Finally, all the test instances are la-
beled, and probability distributions of discourse functions are learned for all test connectives. 
M4: M4 is similar to M3 except that the initial probability distributions are set based on the connec-
tive lexicon. 
 
Table 3 compares the performances of these five models. The average tendency is 
M4>M3>M2>M1>M0. It shows the proposed two semi-supervised learning methods are significantly 
better than the baseline model M0 and the two supervised-learning methods M1 and M2 at p=0.001. 
The best model is M4, but the performance differences between M3 and M4 are not significant. It 
demonstrates that both the two initial assignments, i.e., equal-weight assignment and lexicon-based 
637
assignment, are effective. If a connective is not listed in the lexicon due to its coverage, we can still 
derive its probability distribution starting from the equal-weight approach. 
We further examine the individual performance of each discourse relation. Comparing M1 and M3, 
the semi-supervised classifier (M3) outperforms the supervised classifier (M1) in all three metrics in 
all the four relations except recall and F-score in the Temporal relation. Because more than one half of 
the pairs of discourse units annotated with Temporal relation whose discourse connectives have Ex-
pansion function in the connective lexicon, some discourse-units of Temporal relation are misclassi-
fied as Expansion relation. That is why the recall is dropped by 8.22% in M3. The precisions of all the 
four relations are increased. In particular, the precisions of Temporal, Contingency, and Comparison 
gain more than 10%. The overall F-score is increased 6.61%. 
Moreover, M4 is better than M2 in F-score for all the relations. In particular, the precisions of Tem-
poral, Contingency, and Comparison recognition by M4 are greatly increased. In other words, the 
boosting algorithm tends to correct those instances that are originally misclassified into the Expansion 
relation. The t-test also confirms M4 has a significant improvement over M2 at p=0.001. 
The semi-supervised algorithm learns the probability distributions of discourse functions of the un-
known connectives from the test instances, so that their size may affect the performance. Figure 2 ana-
lyzes how the number of test instances of a connective affects the performance. Each point (x, y) in 
this figure denote a connective, where x is its total occurrences in the test set, and y is its F-score in 
Figure 2(a) and its precision/recall in Figure 2(b). We can find (1) many connectives have good per-
formance, (2) connectives containing more test instances demonstrate better performance, and (3) 
connectives containing fewer instances are sensitive to the evaluation. We treat the probability distri-
bution of discourse functions of each connective as a vector of four real numbers and compute the co-
sine similarity among the distributions of connectives derived by the connective lexicon, human anno-
tators, and our best model M4. When the 114 connectives containing more than 10 instances are 
counted, the average cosine similarity between our model and human is 0.940, and the average cosine 
similarity between the connective lexicon and human is 0.767. 
 
Metric Model Temporal Contingency Comparison Expansion Average 
 M0 0.3933 0.7124 0.5092 0.7364 0.6656 
 M1 0.5618 0.6005 0.5982 0.7147 0.6595 
Precision M2 0.5024 0.7038 0.5332 0.7529 0.6879 
 M3 0.6682 0.7652 0.7749 0.7254 0.7334 
 M4 0.6708 0.7773 0.7869 0.7373 0.7344 
 M0 0.3757 0.6014 0.6588 0.7389 0.6600 
 M1 0.5371 0.5098 0.4154 0.8114 0.6694 
Recall M2 0.4808 0.5808 0.6207 0.7578 0.6731 
 M3 0.4549 0.5387 0.5065 0.9015 0.7276 
 M4 0.4480 0.5803 0.5821 0.8985 0.7299 
 M0 0.3843 0.6522 0.5744 0.7376 0.6606 
 M1 0.5492 0.5515 0.4903 0.7600 0.6644 
F-score M2 0.4913 0.6364 0.5736 0.7553 0.6805 
 M3 0.5413 0.6323 0.6126 0.8039 0.7305 
 M4 0.5372 0.6645 0.6691 0.8099 0.7322 
Table 3: Performance comparisons among models. 
 
       
            (a) F-Score                                                      (b) Precision/Recall 
Figure 2: Effects of the number of test instances for each connective on relation labeling. 
638
4 Further Analyses on a Big Dataset 
We further apply the best model (M4) to predict the probability distributions of discourse functions of 
connectives on a big dataset. For each discourse connective c, up to 500 sentences composed of two 
discourse units linked by c are randomly selected from the Chinese Web POS tagged corpus (Yu et al., 
2012). The limitation of 500 is set to reduce the imbalance among the discourse connectives. Some 
connectives appear quite often in the dataset, e.g., the connective ??? (y?, also). Some connectives 
appear less than 500 times, e.g., ??????? (qi?n w?n?b? r?n, must...otherwise) occurs only 212 
times. Finally, total 302,293 sentences are extracted and predicted. Because the dataset is very large, it 
is not easy to evaluate each pair of discourse units. We examine the linguistic phenomena instead. A 
lexicon of the probability distributions of connectives estimated by M4 is available at 
http://nlg.csie.ntu.edu.tw/ntu-discourse/. 
We sort the discourse connectives by the ratios of their largest relations. In this way, the top connec-
tives in this order almost contain one relation. They can be considered to be less ambiguous. The top 
ten connectives which appear 500 times are shown in Table 4. Note the bracket notation [ds1, ds2] de-
notes the discourse units where connectives appear. The discourse function defined in the discourse 
connective lexicon specified in Section 2 is marked in bold. The probabilities of the major discourse 
function of these connectives are larger than 0.89. The distribution is consistent with the human as-
signment except the last connective ???...??? (ch? f?i...b? r?n, unless...otherwise), which is as-
signed to Contingency in the lexicon. This connective denotes a negated cause-effect relation between 
ds1 and ds2 in which ds2 is the effect when ds1 is not satisfied. In such a case, ds1 and ds2 show clear 
contrast, so that it is reasonable to label this connective with a higher probability of the Comparison 
relation. There are two groups of synonyms in the list: (1) ???...??? (su? r?n?b? gu?, alt-
hough?but) and ???...??? (su? r?n?k? sh?, although?but), and (2) ????? (ji?n y?n zh?, in 
short) and ?????? (ji?n ?r y?n zh?, in short). Table 4 shows that synonyms share similar distribu-
tions. The cosine similarities of their probability distributions are 0.99996 and 0.99952, respectively.  
The probability of each discourse function of each connective c is the average of the probabilities 
estimated by the classifier, thus the distributions reported by our model is not completely identical to 
the empirical distribution. For example, all the instances containing the connective ???...??? (su? 
r?n?b? gu?, although?but) are labeled with the major discourse function Expansion, but the esti-
mated probability of Expansion of this connective is 93.47%. 
We also sort the discourse connectives by the ratio of their second largest relations. In this manner, 
the top connectives in this order may have two major discourse functions. In other words, they are 
ambiguous. Table 5 shows the top ten estimated ambiguous discourse connectives. It is interesting that 
Expansion is one of the two major discourse functions, and the other one shown in bold is the dis-
course function defined in the connective lexicon. The discourse connectives ????? (j?n ji? zhe, 
then), ???? (xi?n z?i, now), ???? (w?i l?i, in the future), and ???? (zh?ng y?, finally), which 
are defined to have Temporal function in the lexicon, frequently occur in the discourse units with Ex-
pansion relation. The estimated distribution of the connective ??? (?r, and; but; thus) is consistent 
with the human interpretation, i.e., it has multiple discourse functions.  
Chinese single-word connectives are usually put together with other words to form word-pair con-
nectives. Tables 6 and 7 show examples for ???? (su? r?n, although) and ???? (su? y?, so),  
 
Discourse Connectives [ds1, ds2] Temporal (%) Contingency (%) Comparison (%) Expansion (%) 
[???, ?] ([in short, ?]) 2.78 2.08 1.67 93.47 
[??, ??] ([although, but]) 0.77 1.80 92.70 4.74 
[???, ?] ([in other words, ?]) 3.63 2.82 1.53 92.02 
[??, ??] ([although, but]) 0.93 2.11 91.58 5.37 
[??, ??] ([since, therefore]) 1.41 91.07 0.97 6.55 
[???, ] ([after all, ?]) 3.17 3.95 2.97 89.91 
[?, ???] ([?, after all]) 3.13 4.34 2.84 89.69 
[????, ] ([in short, ?]) 5.07 3.20 2.25 89.48 
[??, ??] ([or, or]) 3.94 4.51 2.16 89.39 
[??, ??] ([unless, otherwise]) 1.04 3.71 89.33 5.93 
Table 4: Top 10 less-ambiguous connectives estimated by using a big dataset. 
639
 
Discourse Connectives [ds1, ds2] Temporal (%) Contingency (%) Comparison (%) Expansion (%) 
[???, ?] ([then, ?]) 48.71 5.12 1.70 44.46 
[?, ??] ([?, even though]) 3.93 5.23 46.48 44.36 
[??, ?] ([now, ?]) 44.31 7.42 3.42 44.85 
[?, ??] ([?, although]) 3.60 3.68 44.17 48.55 
[??, ?] ([so that, ?]) 3.96 49.83 2.05 44.16 
[??, ??] ([now, in the future]) 47.05 6.41 3.10 43.44 
[??, ?] ([only, then]) 4.34 43.30 9.33 43.03 
[??, ?] ([in the future, ?]) 48.21 6.15 2.85 42.79 
[?, ?] ([?, and; but; thus])  3.72 6.13 42.78 47.37 
[?, ??] ([?, finally]) 42.39 6.13 2.99 48.49 
Table 5: Some ambiguous connectives estimated by using a big dataset. 
 
respectively. The former is often connected with a word in the second discourse unit to form a couple-
linking, while the latter is connected with a word in the first one. We can find word-pair connectives 
are less ambiguous than single-word connectives in different probabilities. The former (????, su? 
r?n, although) tends to have Comparison function. When the word-pair connectives are shorten to sin-
gle-word connectives, the probability to have Comparison function becomes lower. The connective 
???? (su? r?n, although) in the first argument still has probability 0.7639 to have Comparison func-
tion. When ???? (su? r?n, although) is moved to the second argument, the probability to serve as 
Comparison function is decreased to 0.4417, which is even lower than that of Expansion function. It 
shows that couple-linking elements provide strong clue to determine discourse relation. Besides, a sin-
gle-word connective has some tendency to function as either forward linking or backward linking. For 
example, ???? (su? r?n, although) is a forward-linking element. Normally, it will link the first dis-
course unit containing it with the second one. When it appears in the second discourse unit, it becomes 
ambiguous. The connectives containing ???? (su? y?, so) have the similar effects. It tends to be a 
backward linking element, so its companion appears in the first discourse unit. Its probability to have 
Contingency function decreases from a word-pair connective to a single-word connective. When it 
appears in the first discourse unit, it may link to the previous sentence at the inter-sentential level.  
Some Chinese short words like ??? (?r) is often a part of word-pair connectives. Table 8 shows 10 
words which are often connected with ??? (?r) to form word-pair connectives. The word-pair connec-
tives tend to have one major function. When the word-pair connective is ?abbreviated? to a single- 
 
Discourse Connectives [ds1, ds2] Temporal (%) Contingency (%) Comparison (%) Expansion (%) 
[??,??] ([although, but]) 0.77 1.80 92.70 4.74 
[??,??] ([although, but]) 0.93 2.11 91.58 5.37 
[??,??] ([while, however]) 1.04 2.03 90.76 6.17 
[??,??] ([although, but]) 1.14 2.62 88.49 7.74 
[??,?] ([although, but]) 1.48 2.89 87.54 8.09 
[??,?] ([although, still]) 2.70 3.43 85.20 8.68 
[??,?] ([although, still]) 3.06 4.10 81.03 11.81 
[??,?] ([although, while]) 2.86 5.09 79.23 12.82 
[??,??] ([although, still]) 3.68 5.70 77.23 13.39 
[??,??] ([although, still]) 3.51 8.54 75.26 12.69 
[??,?] ([although, still]) 4.24 3.71 74.58 17.47 
[??, ?] ([although, ?]) 3.46 5.28 76.39 14.87 
[?, ??] ([?, although]) 3.60 3.68 44.17 48.55 
Table 6: Effects of single-word and word-pair connectives containing ???? (su? r?n, although). 
 
Discourse Connectives [ds1, ds2] Temporal (%) Contingency (%) Comparison (%) Expansion (%) 
[??, ??] ([because, so]) 1.64 85.25 1.77 11.35 
[?, ??] ([because, so]) 2.26 83.20 1.82 12.72 
[??, ??] ([because, so]) 2.69 78.03 2.35 16.93 
[??, ??] ([since, so]) 1.68 67.32 6.37 24.63 
[?, ??] ([?, so]) 2.82 50.67 5.29 41.22 
[??, ?] ([so, ?]) 5.71 50.61 2.50 41.18 
Table 7: Effects of single-word and word-pair connectives containing ???? (so). 
640
word connective, it becomes ambiguous. The discourse function depends on which word-pair connec-
tive it is mapped. The determination relies on contextual information. 
Table 9 further shows the effects of positions of single-word connectives. The major discourse func-
tion of the first 7 sets of connectives is changed when the connectives are shifted from the first dis-
course unit to the second one. In contrast, the last 3 sets of connectives keep their major discourse 
function no matter whether they are placed in the first or the second discourse unit. The only differ-
ence is the probability to serve as the major discourse function is changed. For example, the probabil-
ity of the connective ????? (zh? b? gu?, only; just; merely) to have Comparison function is in-
creased from 0.6920 to 0.8501 when it is shifted from the first discourse unit to the second one. 
 
Discourse Connectives [ds1, ds2] Temporal (%) Contingency (%) Comparison (%) Expansion (%) 
[??, ?] ([not only, but]) 2.19 4.13 4.92 88.76 
[??, ?] ([not only, but]) 2.41 4.56 10.13 82.89 
[??, ?] ([not only, but]) 3.20 5.14 10.55 81.11 
[??, ?] ([since, but]) 3.99 13.87 13.42 68.72 
[??, ?] ([of course, while]) 1.16 2.76 80.82 15.24 
[??, ?] ([although, while]) 2.86 5.09 79.23 12.82 
[??, ?] ([although, while]) 2.76 43.61 79.16 13.71 
[??, ?] ([because, so]) 2.02 79.01 2.16 16.81 
[?, ?] ([because, so]) 3.21 71.03 2.28 23.49 
[??, ?] ([because, so]) 3.11 49.12 7.52 40.26 
[?, ?] ([?, and; but; thus]) 3.71 6.13 42.78 47.37 
[?, ?] ([and; but; thus, ?]) 5.47 8.55 17.00 68.98 
Table 8: Effects of single-word and word-pair connectives containing ??? (and, but, so). 
 
Discourse Connectives [ds1, ds2] Temporal (%) Contingency (%) Comparison (%) Expansion (%) 
[??, ?] ([therefore, ?]) 6.26 64.30 1.66 27.77 
[?, ??] ([?, therefore]) 3.54 28.32 5.15 62.99 
[??, ?] ([as long as, ?]) 2.68 66.02 5.33 25.98 
[?, ??] ([?, as long as]) 2.57 5.49 4.23 87.71 
[??, ?] ([if, ?]) 3.51 57.15 7.47 31.87 
[?, ??] ([?, if]) 3.31 5.21 5.33 86.16 
[??, ?] ([however, ?]) 8.17 9.20 23.12 59.51 
[?, ??] ([?, however]) 2.26 2.39 80.97 14.38 
[??, ?] ([but, ?]) 8.56 7.72 20.87 62.86 
[?, ??] ([?, but]) 2.32 2.90 75.76 19.02 
[??, ?] ([even though, ?]) 3.55 5.04 75.65 15.75 
[?, ??] ([?, even though]) 3.93 5.23 46.48 44.36 
[??, ?] ([now, ?]) 44.31 7.42 3.42 44.85 
[?, ??] ([?, now]) 8.03 2.88 3.60 85.49 
[?, ?] ([and, ?]) 7.14 8.43 3.14 81.29 
[?, ?] ([?, and]) 4.62 3.79 2.38 89.22 
[??, ?] ([as well as, ?]) 4.83 9.88 2.69 82.60 
[?, ??] ([?, as well as]) 4.20 4.29 2.33 89.18 
[???, ?] ([merely, ?]) 3.54 4.76 69.20 22.50 
[?, ???] ([?, merely]) 1.48 2.00 85.01 11.50 
Table 9: Effects of positions of single-word connectives. 
5 Conclusion 
In this paper, we address the issue of the ambiguous discourse functions of Chinese connectives in 
discourse relation labeling and propose a semi-supervised learning method to estimate the probability 
distribution of discourse functions of connectives. We examine the constructions of Chinese connec-
tives and their effects on the discourse relation recognition. The proposed approach learns the proba-
bility distributions of discourse functions of Chinese connectives from a small labeled dataset and a 
big unlabeled dataset. The results reflect many interesting linguistic phenomena. We compare the am-
biguity degrees of single-word and word-pair connectives, and show the effects of the positions of sin-
gle-word connectives on the discourse functions. The discourse relation recognizer integrating the 
641
probability distributions and contextual information significantly outperforms the approaches without 
the knowledge.  
This methodology can be extended to estimate the probability distribution of discourse functions of 
connectives on much finer relation categories. In the current experiments, we focus on explicit dis-
course relation recognition. The 302,293 labeled sentences in Section 4 can be regarded as a training 
corpus for implicit discourse relation recognition. Those labeled sentences composed of unambiguous 
connectives will be sampled from the reference corpus for training an implicit discourse relation 
recognition system. Furthermore, how to employ the learned probability distributions to deal with dis-
course units containing multiple connectives will be investigated. In the future, we will tell out the dis-
course connective and non-discourse connective uses of words and explore their interpretations on the 
discourse relation recognition. Besides, we will make use of the probability distributions to the relation 
labeling on more than two clauses and further extend the methodology to experiments at the inter-
sentence level. 
Acknowledgements 
This research was partially supported by Ministry of Science and Technology, Taiwan, under the 
grants 101-2221-E-002-195-MY3 and 102-2221-E-002-103-MY3, and 2012 Google Research Award.  
We are also very thankful to the anonymous reviewers for their helpful comments to revise this paper. 
References 
Stergos Afantenos, Nicholas Asher, Farah Benamara, Myriam Bras, C?cile Fabre, Mai Ho-dac, Anne Le Dra-
oulec, Philippe Muller, Marie-Paule P?ry-Woodley, Laurent Pr?vot, Josette Rebeyrolle, Ludovic Tanguy, Ma-
rianne Vergez-Couret, and Laure Vieu. 2012. An Empirical Resource for Discovering Cognitive Principles of 
Discourse Organisation: the ANNODIS Corpus. In Proceedings of the18th International Conference on Lan-
guage Resources and Evaluation (LREC 2012), pages 2727-2734, Istanbul, Turkey. 
Nicholas Asher and Alex Lascarides. 1995. Lexical Disambiguation in a Discourse Context. Journal of Seman-
tics, 12(1):69-108, Oxford University Press. 
Lynn Carlson, Daniel Marcu, and Mary E. Okurowski. 2002. RST Discourse Treebank. Linguistic Data Consor-
tium, Philadelphia. 
Bruno Cartoni, Sandrine Zufferey, and Thomas Meyer. 2013. Annotating the Meaning of Discourse Connectives 
by Looking at their Translation: The Translation Spotting Technique. Dialogue and Discourse, 4(2):65-86. 
Chih-Chung Chang and Chih-Jen Lin. 2011. LIBSVM: A Library for Support Vector Machines. ACM Transac-
tions on Intelligent Systems and Technology, 2:27:1-27:27. 
Hsin-Hsi Chen. 1994. The Contextual Analysis of Chinese Sentences with Punctuation Marks. Literal and Lin-
guistic Computing, 9(4):281-289. 
Shou-Yi Cheng. 2006. Corpus-Based Coherence Relation Tagging in Chinese Discourse. Master Thesis, Na-
tional Chiao Tung University, Hsinchu, Taiwan. 
Xianghui Cheng and Xiaolin Tian. 1989. Xian dai Han yu (????), San lian shu dian (????), Hong 
Kong. 
CMU 2009. ClueWeb09, http://lemurproject.org/clue-web09.php/ 
Leon Derczynski and Robert Gaizauskas. 2013. Temporal Signals Help Label Temporal Relations. In Proceed-
ings of the 51st Annual Meeting of the Association for Computational Linguistics, Volume 2: Short Papers, 
pages 645-650, Sofia, Bulgaria. 
Jerry R. Hobbs. 1985. On the Coherence and Structure of Discourse, Report No. CSLI-85-37, Center for the 
Study of Language and Information, Stanford University. http://www.isi.edu/~hobbs/ocsd.pdf 
Eduard H. Hovy and Elisabeth Maier. 1992. Parsimonious or Profligate: How Many and Which Discourse Struc-
ture Relations? No. ISI/RR-93-373. Information Sciences Institute, University of Southern California, Marina 
del Rey. 
Hen-Hsen Huang and Hsin-Hsi Chen. 2011. Chinese Discourse Relation Recognition. In Proceedings of the 5th 
International Joint Conference on Natural Language Processing (IJCNLP 2011), pages 1442-1446, Chiang 
Mai, Thailand. 
642
Hen-Hsen Huang and Hsin-Hsi Chen. 2012a. Contingency and Comparison Relation Labeling and Structure 
Prediction in Chinese Sentences. In Proceedings of the 13th Annual Meeting of the Special Interest Group on 
Discourse and Dialogue (SIGDIAL 2012), pages 261-269, Seoul, South Korea. 
Hen-Hsen Huang and Hsin-Hsi Chen. 2012b. An Annotation System for Development of Chinese Discourse 
Corpus. In Proceedings of the 24th International Conference on Computational Linguistics (COLING 2012) 
Demonstration Papers, pages 223-230, Mumbai, India. 
Hen-Hsen Huang, Chi-Hsin Yu, Tai-Wei Chang, Cong-Kai Lin, and Hsin-Hsi Chen. 2013. Analyses of the As-
sociation between Discourse Relation and Sentiment Polarity with a Chinese Human-Annotated Corpus. In 
Proceedings of the 7th Linguistic Annotation Workshop and Interoperability with Discourse, pages 70-78, So-
fia, Bulgaria. 
Ben Hutchinson. 2004. Acquiring the Meaning of Discourse Markers. In Proceedings of the 42nd Annual Meet-
ing of the Association for Computational Linguistics (ACL 2004), pages 684-691, Barcelona, Spain. 
Charles N. Li, Sandra A. Thompson. 1981. Mandarin Chinese: A Functional Reference Grammar. University of 
California Press. 
Ziheng Lin, Hwee Tou Ng, and Min-Yen Kan. 2011. Automatically Evaluating Text Coherence Using Discourse 
Relations. In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics (ACL 
2011), pages 997-1006, Portland, Oregon, USA. 
Shuxiang Lu. 2007. Eight Hundred Words of The Contemporary Chinese (Xian dai Han yu Ba bai Ci). China 
Social Sciences Press. 
Emily Pitler and Ani Nenkova. 2009. Using Syntax to Disambiguate Explicit Discourse Connectives in Text. In 
Proceedings of the ACL-IJCNLP 2009 Conference Short Papers, pages 13-16, Suntec, Singapore. 
Livia Polanyi. 1988. A Formal Model of the Structure of Discourse. Journal of Pragmatics, 12(5-6):601-638. 
Rashmi Prasad, Nikhil Dinesh, Alan Lee, Eleni Miltsakaki, Livio Robaldo, Aravind Joshi, and Bonnie Webber. 
2008. The Penn Discourse TreeBank 2.0. In Proceedings of the 6th Language Resources and Evaluation Con-
ference (LREC 2008), pages 2961-2968, Marrakech, Morocco. 
Charlotte Roze, Laurence Danlos, and Philippe Muller. 2010. LEXCONN: a French Lexicon of Discourse Con-
nectives. In Proceedings of the 8th International Workshop on Multidisciplinary Approaches to Discourse 
(MAD 2010), Moissac.  
Ted J. M. Sanders, Wilbert P. M. Spooren, and Leo G. M. Noordman. 1992. Toward a Taxonomy of Coherence 
Relations. Discourse Processes, 15(1):1-35. 
Caroline Sporleder and Alex Lascarides. 2008. Using Automatically Labelled Examples to Classify Rhetorical 
Relations: A Critical Assessment. Natural Language Engineering, 14(3):369-416, Cambridge University 
Press. 
Fei Wang, Yunfang Wu, and Likun Qiu. 2012. Exploiting Discourse Relations for Sentiment Analysis. In Pro-
ceedings of the 24th International Conference on Computational Linguistics (COLING 2012), Posters, pages 
1311-1320, Mumbai, India. 
Florian Wolf and Edward Gibson. 2005. Representing Discourse Coherence: A Corpus-Based Study. Computa-
tional Linguistics, 31(2):249-287. 
Chi-Hsin Yu, Yi-jie Tang and Hsin-Hsi Chen. 2012. Development of a Web-scale Chinese Word N-gram Corpus 
with Parts of Speech Information. In Proceedings the 8th International Conference on Language Resources 
and Evaluation (LREC 2012), pages 320-324, Istanbul, Turkey. 
Lanjun Zhou, Binyang Li, Wei Gao, Zhongyu Wei and Kam-Fai Wong. 2011. Unsupervised Discovery of Dis-
course Relations for Eliminating Intra-sentence Polarity Ambiguities. In Proceedings of Conference on Em-
pirical Methods in Natural Language Processing (EMNLP 2011), pages 162-171, Edinburgh, UK. 
Lanjun Zhou, Wei Gao, Binyang Li, Zhongyu Wei and Kam-Fai Wong. 2012. Cross-lingual Identification of 
Ambiguous Discourse Connectives for Resource-Poor Language. In Proceedings of the 24th International 
Conference on Computational Linguistics (COLING 2012), pages 1409-1418, Mumbai, India. 
Yuping Zhou and Nianwen Xue. 2012. PDTB-style Discourse Annotation of Chinese Text. In Proceedings of the 
50th Annual Meeting of the Association for Computational Linguistics (ACL 2012), pages 67-77, Jeju Island, 
Korea. 
643
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 446?450,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
Modeling Human Inference Process for  
Textual Entailment Recognition 
 
Hen-Hsen Huang Kai-Chun Chang Hsin-Hsi Chen 
Department of Computer Science and Information Engineering 
National Taiwan University 
No. 1, Sec. 4, Roosevelt Road, Taipei, 10617 Taiwan 
{hhhuang, kcchang}@nlg.csie.ntu.edu.tw; hhchen@ntu.edu.tw 
 
 
Abstract 
This paper aims at understanding what hu-
man think in textual entailment (TE) recogni-
tion process and modeling their thinking pro-
cess to deal with this problem. We first ana-
lyze a labeled RTE-5 test set and find that the 
negative entailment phenomena are very ef-
fective features for TE recognition. Then, a 
method is proposed to extract this kind of 
phenomena from text-hypothesis pairs auto-
matically. We evaluate the performance of 
using the negative entailment phenomena on 
both the English RTE-5 dataset and Chinese 
NTCIR-9 RITE dataset, and conclude the 
same findings. 
1 Introduction 
Textual Entailment (TE) is a directional relation-
ship between pairs of text expressions, text (T) 
and hypothesis (H). If human would agree that 
the meaning of H can be inferred from the mean-
ing of T, we say that T entails H (Dagan et al, 
2006). The researches on textual entailment have 
attracted much attention in recent years due to its 
potential applications (Androutsopoulos and Ma-
lakasiotis, 2010). Recognizing Textual Entail-
ment (RTE) (Bentivogli, et al, 2011), a series of 
evaluations on the developments of English TE 
recognition technologies, have been held seven 
times up to 2011. In the meanwhile, TE recogni-
tion technologies in other languages are also un-
derway (Shima, et al, 2011).   
Sammons, et al, (2010) propose an evaluation 
metric to examine the characteristics of a TE 
recognition system. They annotate text-
hypothesis pairs selected from the RTE-5 test set 
with a series of linguistic phenomena required in 
the human inference process. The RTE systems 
are evaluated by the new indicators, such as how 
many T-H pairs annotated with a particular phe-
nomenon can be correctly recognized. The indi-
cators can tell developers which systems are bet-
ter to deal with T-H pairs with the appearance of 
which phenomenon. That would give developers 
a direction to enhance their RTE systems. 
Such linguistic phenomena are thought as im-
portant in the human inference process by anno-
tators. In this paper, we use this valuable re-
source from a different aspect. We aim at know-
ing the ultimate performance of TE recognition 
systems which embody human knowledge in the 
inference process. The experiments show five 
negative entailment phenomena are strong fea-
tures for TE recognition, and this finding con-
firms the previous study of Vanderwende et al 
(2006). We propose a method to acquire the lin-
guistic phenomena automatically and use them in 
TE recognition.  
This paper is organized as follows. In Section 
2, we introduce linguistic phenomena used by 
annotators in the inference process and point out 
five significant negative entailment phenomena. 
Section 3 proposes a method to extract them 
from T-H pairs automatically, and discuss their 
effects on TE recognition. In Section 4, we ex-
tend the methodology to the BC (binary class 
subtask) dataset distributed by NTCIR-9 RITE 
task (Shima, et al, 2011) and discuss their ef-
fects on TE recognition in Chinese. Section 5 
concludes the remarks. 
2 Human Inference Process in TE 
We regard the human annotated phenomena as 
features in recognizing the binary entailment re-
lation between the given T-H pairs, i.e., EN-
TAILMENT and NO ENTAILMENT. Total 210 
T-H pairs are chosen from the RTE-5 test set by 
Sammons et al (2010), and total 39 linguistic 
phenomena divided into the 5 aspects, including 
knowledge domains, hypothesis structures, infer-
ence phenomena, negative entailment phenome-
446
na, and knowledge resources, are annotated on 
the selected dataset. 
2.1 Five aspects as features 
We train SVM classifiers to evaluate the perfor-
mances of the five aspects of phenomena as fea-
tures for TE recognition. LIBSVM RBF kernel 
(Chang and Lin, 2011) is adopted to develop 
classifiers with the parameters tuned by grid 
search. The experiments are done with 10-fold 
cross validation. 
For the dataset of Sammons et al (2010), two 
annotators are involved in labeling the above 39 
linguistic phenomena on the T-H pairs. They 
may agree or disagree in the annotation. In the 
experiments, we consider the effects of their 
agreement. Table 1 shows the results. Five as-
pects are first regarded as individual features, 
and are then merged together. Schemes ?Annota-
tor A? and ?Annotator B? mean the phenomena 
labelled by annotator A and annotator B are used 
as features respectively.  The ?A AND B? 
scheme, a strict criterion, denotes a phenomenon 
exists in a T-H pair only if both annotators agree 
with its appearance. In contrast, the ?A OR B? 
scheme, a looser criterion, denotes a phenome-
non exists in a T-H pair if at least one annotator 
marks its appearance. 
We can see that the aspect of negative entail-
ment phenomena is the most significant feature 
among the five aspects. With only 9 phenomena 
in this aspect, the SVM classifier achieves accu-
racy above 90% no matter which labeling 
schemes are adopted. Comparatively, the best 
accuracy in RTE-5 task is 73.5% (Iftene and 
Moruz, 2009). In negative entailment phenomena 
aspect, the ?A OR B? scheme achieves the best 
accuracy. In the following experiments, we adopt 
this labeling scheme. 
2.2 Negative entailment phenomena 
There is a large gap between using negative en-
tailment phenomena and using the second effec-
tive features (i.e., inference phenomena). Moreo-
ver, using the negative entailment phenomena as 
features only is even better than using all the 39 
linguistic phenomena. We further analyze which 
negative entailment phenomena are more signifi-
cant. 
There are nine linguistic phenomena in the as-
pect of negative entailment. We take each phe-
nomenon as a single feature to do the two-way 
textual entailment recognition. The ?A OR B? 
scheme is applied. Table 2 shows the experi-
mental results. 
 Annotator A Annotator B A AND B A OR B 
Knowledge  
Domains 
50.95% 52.38% 52.38% 50.95% 
Hypothesis  
Structures 
50.95% 51.90% 50.95% 51.90% 
Inference  
Phenomena 
74.29% 72.38% 72.86% 74.76% 
Negative  
Entailment  
Phenomena 
97.14% 95.71% 92.38% 97.62% 
Knowledge  
Resources 
69.05% 69.52% 67.62% 69.52% 
ALL  97.14% 92.20% 90.48% 97.14% 
Table 1: Accuracy of recognizing binary TE rela-
tion with the five aspects as features. 
 
Phenomenon ID Negative entailment  
Phenomenon  
Accuracy 
0 Named Entity mismatch 60.95% 
1 Numeric Quantity mismatch 54.76% 
2 Disconnected argument 55.24% 
3 Disconnected relation 57.62% 
4 Exclusive argument 61.90% 
5 Exclusive relation 56.67% 
6 Missing modifier 56.19% 
7 Missing argument 69.52% 
8 Missing relation 68.57% 
Table 2: Accuracy of recognizing TE relation 
with individual negative entailment phenomena. 
 
The 1st column is phenomenon ID, the 2nd col-
umn is the phenomenon, and the 3rd column is 
the accuracy of using the phenomenon in the bi-
nary classification. Comparing with the best ac-
curacy 97.62% shown in Table 1, the highest 
accuracy in Table 2 is 69.52%, when missing 
argument is adopted. It shows that each phenom-
enon is suitable for some T-H pairs, and merging 
all negative entailment phenomena together 
achieves the best performance.  
We consider all possible combinations of 
these 9 negative entailment phenomena, i.e., 
  
 +?+   
  =511 feature settings, and use each 
feature setting to do 2-way entailment relation 
recognition by LIBSVM. The notation   
  de-
notes a set of 
  
(   )   
 feature settings, each with 
n features.  
The model using all nine phenomena achieves 
the best accuracy of 97.62%. Examining the 
combination sets, we find phenomena IDs 3, 4, 5, 
7 and 8 appear quite often in the top 4 feature 
settings of each combination set. In fact, this set-
ting achieves an accuracy of 95.24%, which is 
the best performance in   
  combination set. On 
the one hand, adding more phenomena into (3, 4, 
5, 7, 8) setting does not have much performance 
difference.  
In the above experiments, we do all the anal-
yses on the corpus annotated with linguistic phe-
nomena by human. We aim at knowing the ulti-
447
mate performance of TE recognition systems 
embodying human knowledge in the inference. 
The human knowledge in the inference cannot be 
captured by TE recognition systems fully correct-
ly. In the later experiments, we explore the five 
critical features, (3, 4, 5, 7, 8), and examine how 
the performance is affected if they are extracted 
automatically. 
3 Negative Entailment Phenomena Ex-
traction 
The experimental results in Section 2.2 show that 
disconnected relation, exclusive argument, ex-
clusive relation, missing argument, and missing 
relation are significant. We follow the definitions 
of Sammons et al (2010) and show them as fol-
lows. 
(a) Disconnected Relation. The arguments and 
the relations in Hypothesis (H) are all matched 
by counterparts in Text (T). None of the argu-
ments in T is connected to the matching relation. 
(b) Exclusive Argument. There is a relation 
common to both the hypothesis and the text, but 
one argument is matched in a way that makes H 
contradict T. 
(c) Exclusive Relation. There are two or more 
arguments in the hypothesis that are also related 
in the text, but by a relation that means H contra-
dicts T. 
(d) Missing Argument. Entailment fails be-
cause an argument in the Hypothesis is not pre-
sent in the Text, either explicitly or implicitly. 
(e) Missing Relation. Entailment fails because 
a relation in the Hypothesis is not present in the 
Text, either explicitly or implicitly. 
To model the annotator?s inference process, 
we must first determine the arguments and the 
relations existing in T and H, and then align the 
arguments and relations in H to the related ones 
in T. It is easy for human to find the important 
parts in a text description in the inference process, 
but it is challenging for a machine to determine 
what words are important and what are not, and 
to detect the boundary of arguments and relations. 
Moreover, two arguments (relations) of strong 
semantic relatedness are not always literally 
identical.  
In the following, a method is proposed to ex-
tract the phenomena from T-H pairs automatical-
ly. Before extraction, the English T-H pairs are 
pre-processed by numerical character transfor-
mation, POS tagging, and dependency parsing 
with Stanford Parser (Marneffe, et al, 2006; 
Levy and Manning, 2003), and stemming with 
NLTK (Bird, 2006). 
3.1 A feature extraction method 
Given a T-H pair, we first extract 4 sets of noun 
phrases based on their POS tags, including {noun 
in H}, {named entity (nnp) in H}, {compound 
noun (cnn) in T}, and {compound noun (cnn) in 
H}.  Then, we extract 2 sets of relations, includ-
ing {relation in H} and {relation in T}, where 
each relation in the sets is in a form of Predi-
cate(Argument1, Argument2).  Some typical ex-
amples of relations are verb(subject, object) for 
verb phrases, neg(A, B) for negations, num(Noun, 
number) for numeric modifier, and tmod(C, tem-
poral argument) for temporal modifier. A predi-
cate has only 2 arguments in this representation. 
Thus, a di-transitive verb is in terms of two rela-
tions. 
Instead of measuring the relatedness of T-H 
pairs by comparing T and H on the predicate-
argument structure (Wang and Zhang, 2009), our 
method tries to find the five negative entailment 
phenomena based on the similar representation. 
Each of the five negative entailment phenomena 
is extracted as follows according to their defini-
tions. To reduce the error propagation which may 
be arisen from the parsing errors, we directly 
match those nouns and named entities appearing 
in H to the text T. Furthermore, we introduce 
WordNet to align arguments in H to T. 
(a) Disconnected Relation. If (1) for each a ? 
{noun in H}?{nnp in H}?{cnn in H}, we can 
find a ? T too, and (2) for each r1=h(a1,a2) ? 
{relation in H}, we can find a relation r2=h(a3,a4) 
? {relation in T} with the same header h, but 
with different arguments, i.e., a3?a1 and a4?a2, 
then we say the T-H pair has the ?Disconnected 
Relation?  phenomenon. 
(b) Exclusive Argument. If there exist a rela-
tion r1=h(a1,a2)?{relation in H}, and a relation 
r2=h(a3,a4)?{relation in T} where both relations 
have the same header h, but either the pair (a1,a3) 
or the pair (a2,a4) is an antonym by looking up 
WordNet, then we say the T-H pair has the ?Ex-
clusive Argument? phenomenon.   
(c) Exclusive Relation. If there exist a relation 
r1=h1(a1,a2)?{relation in T}, and a relation 
r2=h2(a1,a2)?{relation in H} where both relations 
have the same arguments, but h1 and h2 have the 
opposite meanings by consulting WordNet, then 
we say that the T-H pair has the ?Exclusive Rela-
tion? phenomenon. 
448
(d) Missing Argument. For each argument a1 
?{noun in H}?{nnp in H}?{cnn in H}, if there 
does not exist an argument a2?T such that a1=a2, 
then we say that the T-H pair has ?Missing Ar-
gument? phenomenon. 
(e) Missing Relation. For each relation 
r1=h1(a1,a2)?{relation in H}, if there does not 
exist a relation r2=h2(a3,a4)?{relation in T} such 
that h1=h2, then we say that the T-H pair has 
?Missing Relation? phenomenon. 
3.2 Experiments and discussion 
The following two datasets are used in English 
TE recognition experiments. 
(a) 210 pairs from part of RTE-5 test set. The 
210 T-H pairs are annotated with the linguistic 
phenomena by human annotators.  They are se-
lected from the 600 pairs in RTE-5 test set, in-
cluding 51% ENTAILMENT and 49% NO EN-
TAILMENT. 
(b) 600 pairs of RTE-5 test set. The original 
RTE-5 test set, including 50% ENTAILMENT 
and 50% NO ENTAILMENT.  
Table 3 shows the performances of TE recog-
nition. The ?Machine-annotated? and the ?Hu-
man-annotated? columns denote that the phe-
nomena annotated by machine and human are 
used in the evaluation respectively. Using ?Hu-
man-annotated? phenomena can be seen as the 
upper-bound of the experiments. The perfor-
mance of using machine-annotated features in 
210-pair and 600-pair datasets is 52.38% and 
59.17% respectively. 
Though the performance of using the phenom-
ena extracted automatically by machine is not 
comparable to that of using the human annotated 
ones, the accuracy achieved by using only 5 fea-
tures (59.17%) is just a little lower than the aver-
age accuracy of all runs in RTE-5 formal runs 
(60.36%) (Bentivogli, et al, 2009). It shows that 
the significant phenomena are really effective in 
dealing with entailment recognition. If we can 
improve the performance of the automatic phe-
nomena extraction, it may make a great progress 
on the textual entailment. 
 
Phenomena 210 pairs 600 pairs 
Machine- 
annotated 
Human- 
annotated 
Machine- 
annotated 
Disconnected Relation 50.95% 57.62% 54.17% 
Exclusive Argument 50.95% 61.90% 55.67% 
Exclusive Relation 50.95% 56.67% 51.33% 
Missing Argument 53.81% 69.52% 56.17% 
Missing Relation 50.95% 68.57% 52.83% 
All 52.38% 95.24% 59.17% 
Table 3: Accuracy of textual entailment recogni-
tion using the extracted phenomena as features. 
4 Negative Entailment Phenomena in 
Chinese RITE Dataset 
To make sure if negative entailment phenomena 
exist in other languages, we apply the methodol-
ogies in Sections 2 and 3 to the RITE dataset in 
NTCIR-9. We annotate all the 9 negative entail-
ment phenomena on Chinese T-H pairs according 
to the definitions by Sammons et al (2010) and 
analyze the effects of various combinations of 
the phenomena on the new annotated Chinese 
data. The accuracy of using all the 9 phenomena 
as features (i.e.,   
  setting) is 91.11%. It shows 
the same tendency as the analyses on English 
data. The significant negative entailment phe-
nomena on Chinese data, i.e., (3, 4, 5, 7, 8), are 
also identical to those on English data. The mod-
el using only 5 phenomena achieves an accuracy 
of 90.78%, which is very close to the perfor-
mance using all phenomena.  
We also classify the entailment relation using 
the phenomena extracted automatically by the 
similar method shown in Section 3.1, and get a 
similar result. The accuracy achieved by using 
the five automatically extracted phenomena as 
features is 57.11%, and the average accuracy of 
all runs in NTCIR-9 RITE task is 59.36% (Shima, 
et al, 2011). Compared to the other methods us-
ing a lot of features, only a small number of bi-
nary features are used in our method. Those ob-
servations establish what we can call a useful 
baseline for TE recognition. 
5 Conclusion 
In this paper we conclude that the negative en-
tailment phenomena have a great effect in deal-
ing with TE recognition. Systems with human 
annotated knowledge achieve very good perfor-
mance. Experimental results show that not only 
can it be applied to the English TE problem, but 
also has the similar effect on the Chinese TE 
recognition. Though the automatic extraction of 
the negative entailment phenomena still needs a 
lot of efforts, it gives us a new direction to deal 
with the TE problem.  
The fundamental issues such as determining 
the boundary of the arguments and the relations, 
finding the implicit arguments and relations, ver-
ifying the antonyms of arguments and relations, 
and determining their alignments need to be fur-
ther examined to extract correct negative entail-
ment phenomena. Besides, learning-based ap-
proaches to extract phenomena and multi-class 
TE recognition will be explored in the future.  
449
Acknowledgments 
 
This research was partially supported by Excel-
lent Research Projects of National Taiwan Uni-
versity under contract 102R890858 and 2012 
Google Research Award.  
References 
Ion Androutsopoulos and Prodromos Malakasiotis. 
2010. A Survey of Paraphrasing and Textual En-
tailment Methods. Journal of Artificial Intelligence 
Research, 38:135-187.  
Luisa Bentivogli, Peter Clark, Ido Dagan, Hoa Trang 
Dang, and Danilo Giampiccolo. 2011. The seventh 
PASCAL recognizing textual entailment challenge. 
In Proceedings of the 2011 Text Analysis 
Conference (TAC 2011), Gaithersburg, Maryland, 
USA.. 
Luisa Bentivogli, Ido Dagan, Hoa Trang Dang, 
Danilo Giampiccolo, and Bernardo Magnini. 2009. 
The fifth PASCAL recognizing textual entailment 
challenge. In Proceedings of the 2009 Text 
Analysis Conference (TAC 2009), Gaithersburg, 
Maryland, USA. 
Steven Bird. 2006. NLTK: the natural language 
toolkit. In Proceedings of the 21st International 
Conference on Computational Linguistics and 44th 
Annual Meeting of the Association for Computa-
tional Linguistics (COLING-ACL 2006), pages 69-
72. 
Chih-Chung Chang and Chih-Jen Lin. 2011. LIBSVM: 
a Library for Support Vector Machines. ACM 
Transactions on Intelligent Systems and Technolo-
gy, 2:27:1-27:27. Software available at 
http://www.csie.ntu.edu.tw/~cjlin/libsvm. 
Ido Dagan, Oren Glickman, and Bernardo Magnini. 
2006. The PASCAL Recognising Textual Entail-
ment Challenge.  Lecture Notes in Computer Sci-
ence, 3944:177-190. 
Adrian Iftene and Mihai Alex Moruz. 2009. UAIC 
Participation at RTE5. In Proceedings of the 2009 
Text Analysis Conference (TAC 2009), 
Gaithersburg, Maryland, USA. 
Roger Levy and Christopher D. Manning. 2003. Is it 
harder to parse Chinese, or the Chinese Treebank? 
In Proceedings of the 41st Annual Meeting on As-
sociation for Computational Linguistics (ACL 
2003), pages 439-446. 
Marie-Catherine de Marneffe, Bill MacCartney, and 
Christopher D. Manning. 2006. Generating typed 
dependency parses from phrase structure parses. In 
The Fifth International Conference on Language 
Resources and Evaluation (LREC 2006), pages 
449-454. 
Mark Sammons, V.G.Vinod Vydiswaran, and Dan 
Roth. 2010. Ask not what textual entailment can do 
for you... In Proceedings of the 48th Annual Meet-
ing of the Association for Computational Linguis-
tics (ACL 2010), pages 1199-1208, Uppsala, Swe-
den. 
Hideki Shima, Hiroshi Kanayama, Cheng-Wei Lee, 
Chuan-Jie Lin, Teruko Mitamura, Yusuke Miyao,  
Shuming Shi, and Koichi Takeda. 2011. Overview 
of NTCIR-9 RITE: Recognizing inference in text. 
In Proceedings of the NTCIR-9 Workshop Meeting, 
Tokyo, Japan. 
Lucy Vanderwende, Arul Menezes, and Rion Snow. 
2006. Microsoft Research at RTE-2: Syntactic 
Contributions in the Entailment Task: an imple-
mentation. In Proceedings of the Second PASCAL 
Challenges Workshop. 
Rui Wang and Yi Zhang. 2009. Recognizing Textual 
Relatedness with Predicate-Argument Structures. 
In Proceedings of the 2009 Conference on Empiri-
cal Methods in Natural Language Processing, pag-
es 784?792, Singapore.  
 
450
Classical Chinese Sentence Segmentation 
Hen-Hsen Huang?, Chuen-Tsai Sun? and Hsin-Hsi Chen? 
?Department of Computer Science and Information Engineering, National Taiwan University, Taipei, Taiwan 
?Department of Computer Science, National Chiao Tung University, Hsinchu, Taiwan 
hhhuang@nlg.csie.ntu.edu.tw ctsun@cis.nctu.edu.tw hhchen@csie.ntu.edu.tw 
Abstract 
Sentence segmentation is a fundamental 
issue in Classical Chinese language 
processing. To facilitate reading and 
processing of the raw Classical Chinese 
data, we propose a statistical method to 
split unstructured Classical Chinese text 
into smaller pieces such as sentences and 
clauses. The segmenter based on the 
conditional random field (CRF) model is 
tested under different tagging schemes 
and various features including n-gram, 
jump, word class, and phonetic informa-
tion. We evaluated our method on four 
datasets from several eras (i.e., from the 
5th century BCE to the 19th century). 
Our CRF segmenter achieves an F-score 
of 83.34% and can be applied on a varie-
ty of data from different eras. 
1 Introduction 
Chinese word segmentation is a well-known and 
widely studied problem in Chinese language 
processing. In Classical Chinese processing, sen-
tence segmentation is an even more vexing issue. 
Unlike English and other western languages, 
there is no delimiter marking the end of the word 
in Chinese. Moreover, not only is there a lack of 
delimiters between the words, almost all pre-
20th century Chinese is written without any 
punctuation marks. Figure 1 shows photocopies 
of printed and hand written documents from the 
19th century. Within any given paragraph, the 
Chinese characters are printed as evenly spaced 
characters, with nothing to separate words from 
words, phrases from phrases, and sentences from 
sentences. Thus, inside a paragraph, explicit 
boundaries of sentences and clauses are lacking. 
In order to understand the structure, readers of 
Classical Chinese have to manually identify 
these boundaries during the reading. This 
process is called Classical Chinese sentence 
segmentation, or Judo (??). 
For example, the opening lines of the Daoist 
classic Zhuangzi originally lacked segmentation: 
 
?/north ?/ocean ?/have?/fish ?/it ?/name 
?/is ?/Kun (a kind of big fish) ?/Kun ?/of 
?/big ?/not ?/know ?/how ?/thousand ?
/mile  ?/exclamation 
 
The meaning of the text is hard to interpret 
without segmentation. Below is the identical text 
as segmented by a human being. It is clearly 
more readable.  
 
????/in the north ocean there is a fish 
???? /its name is Kun 
???/the size of the Kun 
??????/I don?t know how many  
thousand miles the fish is 
 
However, sentence segmentation in Classical 
Chinese is not a trivial problem. Classical Chi-
nese sentence segmentation, like Chinese word 
segmentation, is inherently ambiguous. Individ-
uals generally perform sentence segmentation in 
instinctive ways. To identify the boundaries of 
sentences and clauses, they primarily rely on 
their experience and sense of the language rather 
than on a systematic procedure. It is thus diffi-
cult to construct a set of rules or practical proce-
dures to specify the segmentation of the infinite 
variety of Classical Chinese sentences. 
 
Figure 1. A Printed Page (Left) and a Hand Written Manuscript (Right) from the 19th Century. 
 
Because of the importance of sentence seg-
mentation, beginning in the 20th century, some 
editions of the Chinese classics have been labor-
intensively segmented and marked with modern 
punctuation. However, innumerable documents 
in Classical Chinese from the centuries of Chi-
nese history remain to be segmented. To aid in 
processing these documents, we propose an au-
tomated Classical Chinese sentence segmenta-
tion approach that enables completion of seg-
mentation tasks quickly and accurately. To con-
struct the sentence segmenter for Classical Chi-
nese, the popular sequence tagging models, con-
ditional random field (CRF) (Lafferty et al, 
2001), are adopted in this study. 
The rest of this paper is organized as follows. 
First, we describe the Classical Chinese sentence 
segmentation problem in Section 2. In Section 3, 
we review the relevant literature, including sen-
tence boundary detection (SBD) and Chinese 
word segmentation. In Section 4, we introduce 
the tagging schemes along with the features, and 
show how the sentence segmentation problem 
can be transformed into a sequence tagging 
problem and decoded with CRFs. In Section 5, 
the experimental setup and data are described. In 
Section 6, we report the experimental results and 
discuss the properties and the challenges of the 
Classical Chinese sentence segmentation prob-
lem. Finally, we conclude the remarks in Section 
7. 
2 Problem Description 
The outcomes of Classical Chinese sentence 
 
segmentation are not well-defined in linguistics 
at present. In general, the results of segmentation 
consist of sentences, clauses, and phrases. For 
instance, in the segmented sentence ???? / 
??? / ?????????, ????? (?the 
mists on the mountains like wild horses?) and 
????? (?the dust in the air?) are phrases, and ?
????????? (?the living creatures blow 
their breaths at each other?) is a clause. A 
sentence such as ?????????? (?I do 
not believe it because it is ridiculous.?) is a short 
sentence itself, and does not require any 
segmentation. For a given text, there is no strict 
rule to determine at which level the 
segmentation should be performed. For instance, 
the opening lines of the Daoist classic Daodejing 
is ?????????????? (?The way 
that can be spoken is not the eternal way. The 
name that can be given is not the eternal name.?) 
which is usually segmented as ???? / ??? 
/ ??? / ????, but may also be segmented 
as ?? / ?? / ??? / ? / ?? / ????. 
Either segmentation is reasonable. 
In this paper, we do not distinguish among the 
three levels of segmentation. Instead, our system 
learns directly from the human-segmented cor-
pus. After training, our system will be adapted to 
perform human-like segmentation automatically. 
Further, we do not distinguish the various out-
comes of Classical Chinese sentence segmenta-
tion. Instead, for the sake of convenience, every 
product of the segmentation process is termed 
?clause? in the following sections. 
  
 
3 Related Work 
Besides Classical Chinese, sentence boundary 
detection (SBD) is also an issue in English and 
other western languages. SBD in written texts 
and speech represents quite different problems. 
For written text, the SBD task is to distinguish 
periods used as the end-of-sentence indicator 
(full stop) from other usages, such as parts of 
abbreviations and decimal points. By contrast, 
the task of SBD in speech is closely related to 
the task of Classical Chinese sentence segmenta-
tion. In speech processing, the outcome of 
speech recognizers is a sequence of words, in 
which the punctuation marks are absence, and 
the sentence boundaries are thus lacking. To re-
cover the syntactic structure of the original 
speech, SBD is required. 
Like Classical Chinese sentence segmentation, 
the task of SBD in speech is to determine which 
of the inter-word boundaries in the stream of 
words should be marked as end-of-sentence, and 
then to divide the entire word sequence into in-
dividual sentences. Empirical methods are com-
monly employed to deal with this problem. Such 
methods involve many different sequence labe-
ling models including HMMs (Shriberg et al, 
2000), maximum entropy (Maxent) models (Liu 
et al, 2004), and CRFs (Liu et al, 2005). 
Among these, a CRF model used in Liu et al
(2005) offered the lowest error rate.  
Chinese word segmentation is a problem 
closely related to Classical Chinese sentence 
segmentation. The former identifies the bounda-
ries of the words in a given text, while the latter 
identifies the boundaries of the sentences, claus-
es, and phrases. In contrast to sentences and 
clauses, the length of Chinese words is shorter, 
and the variety of Chinese words is more limited. 
Despite the minor unknown words, most of the 
frequent words can be handled with a dictionary 
predefined by Chinese language experts or ex-
tracted from the corpus automatically. However, 
it is impossible to maintain a dictionary of the 
infinite number of sentences and clauses. For 
these reasons, the Classical Chinese sentence 
segmentation problem is more challenging. 
Methods of Chinese word segmentation can 
be mainly classified into heuristic rule-based 
approaches, statistical machine learning ap-
proaches, and hybrid approaches. Hybrid ap-
proaches combine the advantages of heuristic 
and statistical approaches to achieve better re-
sults (Gao et al, 2003; Xue, 2003; Peng et al, 
2004). 
Xue (2003) transformed the Chinese word 
segmentation problem into a tagging problem. 
For a given sequence of Chinese characters, the 
author applies a Maxent tagger to assign each 
character one of four positions-of-character 
(POC) tags, and then coverts the tagged se-
quence into a segmented sequence. The four 
POC tags used in Xue (2003) denote the posi-
tions of characters within a word. For example, 
the first character of a word is tagged ?left 
boundary?, the last character of a word is tagged 
?right boundary?, the middle character of a word 
is tagged ?middle?, and a single character that 
forms a word by itself is tagged ?single-
character-word?. Once the given sequence is 
tagged, the boundaries of words are also re-
vealed, and the task of segmentation becomes 
straightforward. However, the Maxent models 
used in Xue (2003) suffer from an inherent the 
label bias problem. Peng et al(2004) uses the 
CRFs to address this issue. The tags used in 
Peng et al(2004) are of only two types, ?start? 
and ?non-start?, in which the ?start? tag denotes 
the first character of a word, and the characters 
in other positions are given the ?non-start? tag. 
The closest previous works to Classic Chinese 
sentence segmentation are Huang (2008) and 
Zhang et al (2009). Huang combined the Xue?s 
tagging scheme (i.e., 4-tag set) and CRFs to ad-
dress the Classical Chinese sentence segmenta-
tion problem and reported an F-score of 80.96% 
averaged over various datasets. A similar work 
by Zhang et al reported an F-score of 71.42%.  
4 Methods 
Conditional random field is our tagging model, 
and the implementation is CrfSgd 1.31 provided 
by L?on Bottou. As denoted by the tool name, 
the parameters in this implementation are opti-
mized using Stochastic Gradient Descent (SGD) 
which convergences much faster than the com-
mon optimization algorithms such as L-BFGS 
and conjugate gradient (Vishwanathan, et al, 
2006). To construct the sentence segmenter on 
                                                 
1 http://leon.bottou.org/projects/sgd 
CRF, the tagging scheme and the feature func-
tions play the crucial roles.  
4.1 Tagging Schemes 
In the previous works (Huang, 2008; Zhang et 
al., 2009), POC tags used in Chinese word seg-
mentation (Xue, 2003) are converted to denote 
the positions of characters within a clause. The 
4-tag set is redefined as L (?the left boundary of 
a clause?), R (?the right boundary of a clause?), 
M (?the middle character of a clause?), and S (?a 
single character forming a clause?). For example, 
the sentence ???????????????
???? should be tagged as follows. 
 
?/L ?/M ?/M ?/R ?/L ?/M ?/M ?/R ?
/L ?/M ?/R ?/L ?/M ?/M ?/M ?/M ?/R 
 
We can easily split the sentence into clauses by 
making a break after each character tagged R 
and S and obtain the final outcome ????? / 
???? / ??? / ???????. 
In this work, more tagging schemes are expe-
rimented. The basic tagging scheme for segmen-
tation is 2-tag set in which only two types of tags, 
?start? and ?non-start?, are used to label the se-
quence. The segmented fragments (clauses) for 
sentence segmentation are usually much longer 
than those for word segmentation. Thus, we add 
more middle states into the 4-tag set to model 
the nature of long fragments. The Markov chain 
of our tagging scheme is shown in Figure 2, 
where L2, L3, ?, Lk are the additional states to 
extend Xue?s 4-tag set. In our experiments, vari-
ous k values are tested. If the k value is 1, the 
scheme is identical to the one used in the two 
previous works (Zhang et al, 2009; Huang, 
2008). The 2-tag set, 4-tag set, 5-tag set and their 
corresponding examples are listed in Table 1. 
With the tagging scheme, the Classical Chinese 
sentence segmentation task is transformed into a 
sequence labeling or tagging task. 
 
4.2 Features 
Due to the flexibility of the feature function in-
terface provided by CRFs, we apply various fea-
ture conjunctions. Besides the n-gram character 
patterns, the phonetic information and the part-  
 
Figure 2. Markov Chain of Our Tagging Scheme.  
 
 
Tag set Tags Example 
2-tag S: Start ??????? 
N: Non-Start ??????? 
4-tag 
(k=1) 
L1: Left-end ??????? 
M: Middle ??????? 
R: Right-end ??????? 
S: Single ? / ???? 
5-tag 
(k=2) 
L1: Left-end ??????? 
L2: Left-2nd ??????? 
M: Middle ??????? 
R: Right-end ??????? 
S: Single ? / ???? 
Table 1. Examples of Tag Sets. 
 
of-speech (POS) are also included. The pronun-
ciation of each Chinese character is labeled in 
three ways. The first one is Mandarin Phonetic 
Symbols (MPS), also known as Bopomofo, 
which is a phonetic system for Modern Chinese. 
The initial/final/tone of each character can be 
obtained from its MPS label.  
However, Chinese pronunciation varies in the 
thousands of years, and the pronunciation of 
Modern Chinese is much different from the 
Classical Chinese. For this reason, two Ancient 
Chinese phonetic systems, Fanqie (??) and 
Guangyun (??), are applied to label the cha-
racters. The pronunciation of a target character is 
represented by two characters in the Fanqie sys-
tem. The first character indicates the initial of 
the target character, and the second character 
indicates the combination of the final and the 
tone. The Guangyun system is in a similar man-
ner with a smaller phonetic symbol set. There 
are 8,157 characters in our phonetic dictionary 
and the statistics are shown in Table 2. 
The POS information is also considered. It is 
difficult to construct a Classical Chinese POS 
System #Initials #Finals #Tones 
MPS 21 36 5 
Fanqie 403 1,054 
Guangyun 43 203 
Table 2. Phonetic System Statistics. 
  
POS # Characters Examples 
Beginning 60 ?, ?, ? 
Middle 50 ?, ? 
End 45 ?, ?, ?, ?  
Interjection 20 ?, ?, ?, ? 
Table 3. Four Types of POS. 
 
tagger at this moment. Instead, we collected 
three types of particles that are usually placed at 
the beginning, at the middle, and at the end of 
Classical Chinese clauses. In addition, the inter-
jections which are usually used at the end of 
clauses are also collected. Some examples are 
given in Table 3. The five feature sets and the 
feature templates are shown in Table 4. 
5 Experiments 
There are three major sets of experiments.  In the 
1st set of experiments, we test different tagging 
schemes for Classical Chinese sentence segmen-
tation. In the 2nd set of experiments, all kinds of 
feature sets and their combinations are tested. 
The performances of the first two sets of expe-
riments are evaluated by 10-fold cross-validation 
on four datasets which cross both eras and con-
texts. In the 3rd set of experiments, we train the 
system on one dataset, and test it on the others. 
In last part of the experiments, the generality of 
the datasets and the toughness of our system are 
tested (Peng et al, 2004). The cut-off threshold 
for the features is set to 2 for all the experiments. 
In other words, the features occur only once in 
the training set will be ignored. The other op-
tions of CrfSgd remain default. 
5.1 Datasets 
The datasets used in the evaluation are collected 
from the corpora of the Pre-Qin and Han Dynas-
ties (the 5th century BCE to the 1st century BCE) 
and the Qing Dynasty (the 17th century CE to 
the 20th century CE). Chinese in the 19th cen-
tury is fairly different from Chinese in the era 
before 0 CE. In ancient Chinese, the syntax is 
much simpler, the sentences are shorter, and the 
words are largely composed of a single character. 
Those are unlike later and more modern Chinese, 
where word segmentation is a serious issue. 
Given these properties, the task of segmenting 
 
Feature Set Template Function 
Character ?? ,?2 ? ? ? 2 Unigrams 
????+1 ,?2 ? ? ? 1 Bigrams 
????+1??+2,?2 ? ? ? 0 Trigrams 
????+2 ,?2 ? ? ? 0 Jumps 
POS ???_?(?0)  Current character serves as a clause-beginning particle. 
???_?(?0) Current character serves as a clause-middle particle. 
???_?(?0) Current character serves as a clause-end particle. 
???_?(?0) Current character serves as an interjection. 
MPS ?_?(?0) The initial of current character in MPS. 
?_?(?0) The final of current character in MPS. 
?_?(?0) The tone of current character in MPS. 
?_?(??1)?_?(??1)?_?(?0) The connection between successive characters. 
Fanqie ?_?(?0) The initial of current character in Fanqie. 
?_?(?0) The final and the tone of current character in Fanqie. 
?_?(??1)?_?(?0) The connection between successive characters. 
Guangyun ?_?(?0) The initial of the current character in Guangyun. 
?_?(?0) The final and the tone of current character in Guan-
gyun. 
?_?(??1)?_?(?0) The connection between successive characters. 
Table 4. Feature Templates. 
Corpus Author Era #  of data 
entries 
# of  
characters 
Size of cha-
racter set 
Average # of  
characters/clause 
Zuozhuan Zuo Qiuming 500 BCE 3,381 195,983 3,238 4.145 
Zhuangzi Zhuangzi 300 BCE 1,128 65,165 2,936 5.183 
Shiji Qian Sima 100 BCE 4,778 503,890 4,788 5.049 
Qing Documents Qing Dynasty  
Officials 
19th  
century 
1,000 111,739 3,147 7.199 
Table 5. Datasets and Statistics. 
 
ancient Chinese sentences is easier than that of 
segmenting later Chinese ones. Thus, we col-
lected texts from the pre-Qin and Han period, 
and from the late Qing Dynasty closer to the 
present, to show that our system can handle 
Classical Chinese as it has evolved across a span 
of two thousand years. 
A summary of the four datasets is listed in 
Table 5. The Zuozhuan is one of earliest histori-
cal works, recording events of China in the 
Spring and Autumn Period (from 722 BCE to 
481 BCE). The book Zhuangzi was named after 
its semi-legendary author, the Daoist philoso-
pher Zhuangzi, who lived around the 4th century 
BCE. The book consists of stories and fables, in 
which the philosophy of the Dao is propounded. 
The Shiji, known in English as The Records of 
the Grand Historian, was written by Qian Sima 
in the 1st century BCE. It narrates Chinese histo-
ry from 2600 BCE to 100 BCE. The Shiji is not 
only an extremely long book of more than 
500,000 characters, but also the chief historical 
work of ancient China, exerting an enormous 
influence on subsequent Chinese literature and 
historiography. 
The three ancient works are the most impor-
tant classics of Chinese literature. We fetched 
well-segmented electronic editions of these 
works from the online database of the Institute 
of History and philology of the Academia Sinica, 
Taiwan.2 Each work was partitioned into para-
graphs forming a single data entry, which acted 
as the basic unit of training and testing. The da-
taset of Qing documents is selected from the 
Qing Palace Memorials (??) related to Taiwan 
written in the 19th century. These documents 
were kindly provided by the Taiwan History 
Digital Library and have also been human-
segmented and stored on electronic media (Chen 
et al, 2007). We randomly selected 1,000 para-
graphs from them as our dataset. 
                                                 
2http://hanji.sinica.edu.tw 
5.2 Evaluation Metrics 
For Classical Chinese sentence segmentation, we 
define the precision P as the ratio of the bounda-
ries of clauses which are correctly segmented to 
all segmented boundaries, the recall R as the ra-
tio of correctly segmented boundaries to all ref-
erence boundaries, and the score F as the har-
monic mean of precision and recall: 
 
? =
? ? ? ? 2
? + ?
 
 
Dataset Precision Recall F-Score 
Zuozhuan 100% 32.80% 42.73% 
Zhuangzi 100% 19.84% 29.83% 
Shiji 100% 14.11% 20.63% 
Qing Doc. 100% 33.08% 41.42% 
Average 100% 24.96% 33.65% 
Table 6. Performance of Majority-Class Baseline. 
 
Tag Set Precision Recall F-Score 
2-tag set 85.00% 82.16% 82.92% 
4-tag set 85.11% 82.13% 82.95% 
5-tag set 85.26% 82.36% 83.18% 
7-tag set 84.47% 82.18% 82.74% 
Baseline 100% 24.96% 33.65% 
Table 7. Comparison between Tagging Schemes. 
 
Features Precision Recall F-Score 
Character 85.26% 82.36% 83.18% 
POS 61.04% 40.35% 43.93% 
MPS 65.31% 54.00% 56.31% 
Fanqie 80.96% 76.80% 77.95% 
Guangyun 73.11% 69.13% 69.59% 
POS + 
Fanqie 
81.07% 74.91% 76.77% 
Character 
+ Fanqie 
85.43% 82.52% 83.34% 
Character 
+ POS + 
Fanqie 
85.67% 81.70% 82.98% 
Table 8. Comparison between Feature Sets. 
Dataset Precision Recall F-Score 
Zuozhuan 92.83% 91.56% 91.79% 
Zhuangzi 81.02% 78.87% 79.34% 
Shiji 80.79% 78.10% 78.99% 
Qing Doc. 87.07% 81.53% 83.24% 
Average 85.43% 82.52% 83.34% 
Table 9. Performance on Four Datasets. 
6 Results 
Our baseline is a majority-class tagger which 
always regards the whole paragraph as a single 
sentence (i.e., never segments). In Table 6, the 
performance of the baseline is given. In the 1st 
set of experiments, four tagging schemes are 
tested while the feature set is Character. The re-
sults are shown in Table 7. In the table, each of 
the precision, the recall, and the F-score are av-
eraged over the four datasets for each scheme. 
The results show that the CRF with the 5-tag set 
is superior to the 4-tag set used in previous 
works. However, the performance is degraded 
when the k is larger.  
In the 2nd set of experiments, the tag scheme 
is fixed to the 5-tag set and a number of feature 
set combinations are tested. The results are 
shown in Table 8. The performance of MPS is 
significantly inferior to the other two phonetic 
systems. As expected, the pronunciation of Clas-
sical Chinese is much different from that of 
Modern Chinese, thus the Ancient Chinese pho-
netic systems are more suitable for this work. 
The Fanqie has a surprisingly performance close 
to the Character. However, performance of the 
combination of Character and Fanqie is similar 
to the performance of Character only model. 
This result indicates that the phonetic informa-
tion is an important clue to Classical Chinese 
sentence segmentation but such information is 
mostly already covered by the characters. Be-
sides, the simple POS features do not help a lot. 
The higher precision and the lower recall of the 
POS features show that the particles such as ?/
?/?/? is indeed a clue to segmentation, but 
does not catch enough cases. 
The best performance comes from the com-
bination of Character and Fanqie with the 5-tag 
set. We use this configuration as our final tagger. 
The performances of our tagger for each dataset 
are given in Table 9. The result shows that our 
tagger achieves fairly good performance on the 
Zuozhuan segmentation, while obtaining accept-
able performance overall. Because the 19th cen-
tury Chinese is more complex than ancient Chi-
nese, what we had assumed was that segmenta-
tion of the Qing documents would more difficult. 
However, the results indicate that our assump-
tion does not seem to be true. Our tagger per-
forms the sentence segmentation on the Qing 
documents well, even better than on the Zhuang-
zi and on the Shiji. The issues of longer clauses 
and word segmentation described earlier in this 
paper do not significantly affect the performance 
of our system. 
In the last experiments, our system is trained 
and tested on different datasets, and the results 
are presented in Table 10, where the training 
datasets are in the rows and the test datasets are 
in the columns, and the F-scores of the segmen-
tation performance are shown in the inner entries. 
As expected, the results of segmentation tasks 
across datasets are significantly poorer than the 
segmentation in the first two experiments. 
These results indicate that our system main-
tains its performance on a test dataset differing 
from the training dataset, but the difference in 
written eras between the test dataset and training 
dataset cannot be very large. Among all datasets, 
Shiji is the best training dataset. As training on 
Shiji and testing on the two other ancient corpo-
ra Zuozhuan and Zhuangzi, the performances of 
our CRF segmenter are not bad. 
 
Training Set Testing Set  
Zuozhuan Zhuangzi Shiji Qing doc. Average 
Zuozhuan  72.04% 59.12% 38.85% 56.67% 
Zhuangzi 63.70%  52.51% 42.75% 52.99% 
Shiji 76.27% 75.46%  44.11% 65.28% 
Qing doc. 52.68% 53.13% 42.61%  49.47% 
Average 64.22% 66.88% 51.41% 41.90%  
Table 10. F-score of Segmentation cross the Datasets.
7 Conclusion 
Our Classical Chinese sentence segmentation is 
important for many applications such as text 
mining, information retrieval, corpora research, 
and digital archiving. To aid in processing such 
kind of data, an automatic sentence segmenta-
tion system is proposed. Different tagging 
schemes and various features are introduced and 
tested. Our system was evaluated using three 
sets of experiments. Five main results are de-
rived. First, the CRF segmenter achieves an F-
score of 91.79% in the best case and 83.34% in 
overall performance. Second, a little longer tag-
ging scheme improves the performance. Third, 
the phonetic information, especially sourced 
from Fanqie, is an important clue for Classical 
Chinese sentence segmentation and may be use-
ful in the related tasks. Fourth, our method per-
forms well on data from various eras. In the ex-
periments, texts from both 500 BCE and the 
19th century were well-segmented. Last, the 
CRF segmenter maintains a certain level of per-
formance in situations which the test data and 
the training data differ in authors, genres, and 
written styles, but eras in which they were pro-
duced are sufficiently close. 
References 
Chen, Szu-Pei, Jieh Hsiang, Hsieh-Chang Tu, and 
Micha Wu. 2007. On Building a Full-Text Digital 
Library of Historical Documents. In Proceedings 
of the 10th International Conference on Asian 
Digital Libraries, Lecture Notes in Computer 
Science, Springer-Verlag 4822:49-60. 
Gao, Jianfeng, Mu Li, and Chang-Ning Huang. 2003. 
Improved Source-Channel Models for Chinese 
Word Segmentation. In Proceedings of the 41st 
Annual Meeting of the Association for Computa-
tional Linguistics, 272-279. 
Huang, Hen-Hsen. 2008. Classical Chinese Sentence 
Division by Sequence Labeling Approaches. Mas-
ter?s Thesis, National Chiao Tung University, 
Hsinchu, Taiwan. 
Lafferty, John, Andrew McCallum, and Fernando 
Pereira. 2001. Conditional Random Fields: Proba-
bilistic Models for Segmentation and Labeling Se-
quence Data. In Proceedings of the 18th Interna-
tional Conference on Machine Learning, 282-289. 
Liu, Yang, Andreas Stolcke, Elizabeth Shriberg, and 
Mary Harper. 2004. Comparing and Combining 
Generative and Posterior Probability Models: 
Some Advances in Sentence Boundary Detection 
in Speech. In Proceedings of the Conference on 
Empirical Methods in Natural Language 
Processing. 
Liu, Yang, Andreas Stolcke, Elizabeth Shriberg, and 
Mary Harper. 2005. Using Conditional Random 
Fields for Sentence Boundary Detection in Speech. 
In Proceedings of the 43rd Annual Meeting of the 
Association for Computational Linguistics, 451-
458. Ann Arbor, Mich., USA. 
Peng, Fuchun, Fangfang Feng, and Andrew McCal-
lum. 2004. Chinese Segmentation and New Word 
Detection using Conditional Random Fields. In 
Proceedings of the 20th International Conference 
on Computational Linguistics, 562-568. 
Shriberg, Elizabeth, Andreas Stolcke, Dilek Hakkani-
T?r, and G?khan T?r. 2000. Prosody-Based Au-
tomatic Segmentation of Speech into Sentences 
and Topics. Speech Communication, 32(1-2):127-
154. 
Vishwanathan, S. V. N., Nicol N. Schraudolph, Mark 
W. Schmidt, and Kevin P. Murphy. 2006. Accele-
rated training of conditional random fields with 
stochastic gradient methods. In Proceedings of the 
23th International Conference on Machine Learn-
ing, 969?976. ACM Press, New York, USA. 
Xue, Nianwen. 2003. Chinese Word Segmentation as 
Character Tagging. Computational Linguistics and 
Chinese Language Processing, 8(1):29-48. 
Zhang, Hel, Wang Xiao-dong, Yang Jian-yu, and 
Zhou Wei-dong. 2009. Method of Sentence Seg-
mentation and Punctuating for Ancient Chinese. 
Application Research of Computers, 26(9):3326-
3329. 
Proceedings of the 13th Annual Meeting of the Special Interest Group on Discourse and Dialogue (SIGDIAL), pages 261?269,
Seoul, South Korea, 5-6 July 2012. c?2012 Association for Computational Linguistics
Contingency and Comparison Relation Labeling and Structure Prediction in Chinese Sentences 
  Hen-Hsen Huang Hsin-Hsi Chen Department of Computer Science and Department of Computer Science and Information Engineering, Information Engineering, National Taiwan University, Taipei, Taiwan National Taiwan University, Taipei, Taiwan hhhuang@nlg.csie.ntu.edu.tw hhchen@csie.ntu.edu.tw      Abstract 
Unlike in English, the sentence boundaries in Chinese are fuzzy and not well-defined. As a result, Chinese sentences tend to be long and consist of complex discourse relations. In this paper, we focus on two important relations, Contingency and Comparison, which occur often inside a sentence. We construct a moderate-sized corpus for the investigation of intra-sentential relations and propose models to label the relation structure. A learning based model is evaluated with various features. Experimental results show our model achieves accuracies of 81.63% in the task of relation labeling and 74.8% in the task of relation structure prediction.  1 Introduction Discourse relation labeling has attracted much attention in recent years due to its potential applications such as opinion mining, question answering, etc. The release of the Penn Discourse Treebank (Joshi and Webber, 2004; Prasad et al, 2008) has advanced the development of English discourse relation recognition (Lin et al, 2009; Pitler et al, 2009; Pitler and Nenkova, 2009; Wang et al, 2010). For Chinese, a discourse corpus is not publicly available yet. Thus, the research on Chinese discourse relation recognition is relatively rare. Most notably, Xue (2005) annotated discourse 
connectives in the Chinese Treebank. Our previous work labeled four types of relations, including temporal, contingency, comparison and expansion, between two successive sentences, and reported an accuracy of 88.28% and an F-score of 62.88% (Huang and Chen, 2011). The major issue of our work is the determination of discourse boundaries. Each Chinese sentence is always treated as one of the two arguments in their annotation and many instances of the Contingency and the Comparison remain uncaught. As suggested by the Penn Discourse Treebank annotation guidelines, an argument is possibly some clauses in a sentence, a sentence, or several successive sentences. In Chinese, the Contingency and the Comparison relations are likely to occur within a sentence. Thus, a lot of the Contingency relations and the Comparison relations are missing from annotation in the corpus used in our previous work, and the classification performance for these two relations, especially the Contingency relation, is especially poor (Huang and Chen, 2011). In contrast to Chinese inter-sentential discourse relation detection (Huang and Chen, 2011) and the study of English coherence evaluation (Lin et al, 2011), this paper focuses on the Contingency relation and the Comparison relations that occur inside a sentence. In Chinese, the relations usually occur in the sentences which contain many clauses. For example, two relations occur in sample (S1).  (S1) ?????????????????????????????  (?Although the 
261
management office tried to make the Yangmingshan area a more natural environment as the long-term garden of Taipei?)???????????????  (?But due to the two-day weekend and the improved economic conditions?)???????????????????? (?The issues of tourists parking, garbage, and other indirect effects become more serious?)?  In (S1), the long sentence consists of three clauses, and such a Chinese sentence is expressed as multiple short sentences in English. Figure 1 shows that a Comparison relation occurs between the first clause and the last two clauses, and a Contingency relation occurs between the second clause and the third clause. An explicit paired discourse marker ? (although) ? ? (but) denotes a Comparison relation in (S1), where the first clause is the first argument of this relation, and the second and the third clauses are the second argument of this relation. In addition, an implicit Contingency relation also occurs between the second and the third clauses. The second clause is the cause argument of this Contingency relation, and the third clause is its effect. It shows a nested relation, which makes relation labeling and relation structure determination challenging. In Chinese, an explicit discourse marker does not always uniquely identify the existence of a particular discourse relation. In sample (S2), a discourse marker ?  ?moreover? appears, but neither Contingency nor Comparison relation exists between the two clauses. The discourse marker ? has many meanings. Here, It has the meaning of ?and? or ?moreover?, which indicates an Expansion relation. In other usages, it may have the meaning of ?but? or ?however?, which indicates a Comparison relation.  (S2) ???????????????????????? (?Moreover, the progress of mainland is more impressive due to its economic openness for the last 10 years.?)  Note that the relation structure of a sentence cannot be exactly derived from the parse tree of the sentence. Shown in Figure 2 is the structure of sample (S3) based on the syntactic tree generated by the Stanford parser. However, it is clear that the 
correct structure of (S3) is the one shown in Figure 3.  (S3) ????????????????? (?Although women only appear in the pictures?)?? ? ? ? ? ? ? ?  (?The contribution of women?)?????????????? (?Will be another major focus in textbooks in the future?)?  This shows that the Stanford parser does not capture the information that the last two clauses form a unit, which in turn is one of the two arguments of a Comparison relation. In this work, we investigate intra-sentential relation detection in Chinese. Given a Chinese sentence, our model will predict if Contingency or Comparison relations exist, and determine their relation structure. In Section 2, the development of a corpus annotated with Contingency and Comparison relations is presented. The methods and the features are proposed in Section 3. In Section 4, the experimental results are shown and discussed. Finally, Section 5 concludes this paper. 
 Figure 1: Relation structure of sample (S1). 
 Figure 2: Structure of sample (S3) based on the syntactic tree generated by the Stanford parser.  
 Figure 3: Correct structure of sample (S3) 
262
2 Dataset  The corpus is based on the Sinica Treebank (Huang et al, 2000). A Total of 81 articles are randomly selected from the Sino and Travel sets. All the sentences that consist of two, three, and four clauses are extracted for relation and structure labeling by native Chinese speakers. A web-based system is developed for annotation. The annotation scheme is designed as follows. An annotator first signs in to the annotation system, and a list of sentences that are assigned to the annotator are given. The annotator labels the sentences one by one in the system. A sentence is split into clauses along commas, and all of its feasible binary tree structures are shown in the interface. The annotator decides if a Contingency/Comparison relation occurs in this sentence. The sentence will be marked as ?Nil? if no relation is found. If there is at least one relation in this sentence, the annotator then chooses the best tree structure of the relations, and the second page is shown. The previously chosen tree structure is presented again, and at this time the annotator has to assign a suitable relation type to each internal node of the tree structure. The relation type includes Contingency ????, Comparison ????, and Nil. For example, in sample (S4), its three internal nodes are annotated with three relation types as shown in Figure 4.  (S4) ?????????? (?Even without the sense of mission of the heritage?)????????????  (?In order to seek better treatments?)?????????????????? (?These medical workers will be driven crossing domain areas?)?????? (?To find resources?)?  The number of feasible relation structures of a sentence may be very large depending on the number of clauses. For a sentence with n clauses, the number of its feasible structures is given as the recursive function f(n) as follows, and the number of its feasible relation structures is 3???? ? .  
? ? = 1, ? = 1? ? ? ? ?(?)?????? , ? > 1 
 Figure 4: Relation structure of sample (S4).  Explicit/ implicit Relations 2-Clause 3-Clause 4-Clause Total % Explicit Both 0 5 6 11 0.89% Contingency 59 72 45 176 14.31% Comparison 41 57 22 120 9.76% Nil 269 249 169 687 55.85% Implicit Both 0 0 0 0 0.00% Contingency 11 8 0 19 1.54% Comparison 6 0 0 6 0.49% Nil 125 56 4 211 17.15% All  511 447 272 1,230 100.00% Table 1: Statistics of the dataset.  For a two-clause sentence, there are only one tree structure and three possible relation tags (Contingency, Comparison, and Nil) for the only one internal node, the root. For a three-clause sentence, there are two candidate tree structures and nine combinations of the relation tags. For a four-clause sentence, there are five candidate tree structures and 27 combinations of the relation tags. There are theoretically 3, 18, and 135 feasible relation structures for the two-, three-, and four- clause sentences, respectively, though only 49 types of relations structures are observed in the dataset. Each sentence is shown to three annotators, and the majority is taken as the ground-truth. The Fleiss-Kappa of the inter-annotator agreement is 0.44 (moderate agreement). A final decider is involved to break ties. The statistics of our corpus are shown in Table 1. The explicit data are those sentences which have at least one discourse marker. The rest of the data are implicit. A total of 11 explicit sentences which contain both Contingency and Comparison relations form complex sentence compositions. The implicit samples are relatively rare. 3 Methods To predict the intra-sentential relations and structures, two learning algorithms, the modern implementation of the decision tree algorithm, 
263
C5.01, and the support vector machine, SVMlight2, are applied. The linguistic features are the crucial part in the learning-based approaches. Various features from different linguistic levels are evaluated in the experiments as shown below. Word: The bags of words in each clause. The Stanford Chinese word segmenter3 is applied to all the sentences to tokenize the Chinese words. In addition, the first word and the last word in each clause are extracted as distinguished features. POS: The bags of parts of speech (POS) of the words in each clause are also taken as features. All the sentences in the dataset are sent to the Stanford parser4 that parses a sentence from a surface form into a syntactic tree, labels POS for each word, and generates all the dependencies among the words. In addition, the POS tags of the first word and the last word in each clause are extracted as distinguished features. Length: Several length features are considered, including the number of clauses in the sentence and the number of words for each clause in the sentence. Connective: In English, some words/phrases called connectives are used as discourse markers. For example, the phrase ?due to? is a typical connective that indicates a Contingency relation, and the word ?however? is a connective that indicates a Comparison relation. Similar to the connectives in English, various words and word pair patterns are usually used as discourse markers in Chinese. A dictionary that contains several types of discourse markers is used. The statistics of the connective dictionary and samples are listed in Table 2. An intra-sentential phrase pair indicates a relation which occurs only inside a sentence. In other words, a relation occurs when the two phrases of an intra-sentential pair exist in the same sentence no matter whether they are in the same clause or not. In contrast, an inter-sentential connective indicates a relation that can occur across neighboring sentences. Some connectives belong to both intra-sentential and inter-sentential types. Each connective in each clause is detected and marked with its corresponding type. For example, the phrase ??
                                                            1 http://www.rulequest.com/see5-unix.html 2 http://svmlight.joachims.org/ 3 http://nlp.stanford.edu/software/segmenter.shtml 4 http://nlp.stanford.edu/software/lex-parser.shtml 
? ?In contrast? will be marked as a connective that belongs to Comparison relation. The number of types and scopes of the connectives in a sentence are used as features. Dependency: The dependencies among all words in a sentence are used as features. The Stanford parser generates dependency pairs from the sentence. A dependency pair consists of two arguments, i.e., the governor and the dependent, and their types. We are interested in those dependency pairs that are across two clauses. That is, the two arguments of a pair are from different clauses. In our assumption, the clauses have a closer connection if some dependencies occur between them. All such dependency pairs and their types are extracted and counted. Structure: Recent research work reported improved performance using syntactic information for English discourse relation detection. In the work of Pilter and Nenkova (2009), the categories of a tree node, its parent, its left sibling, and its right sibling are taken as features. In the work of Wang et al (2010), the entire paragraph is parsed    Relation Type  # Samples Temporal Single Phrase 41 ?? ?now? ?? ?after? 
Intra-Sent Phrase Pair 80 ??...? ?Then...again? ??...? ?At first...ever? 
Inter-Sent Phrase Pair 30 ??...?? ?Initially...Later? ??...??? ?At first...Then? 
Contingency Single Phrase 62 ???? ?As a result? ?? ?If? 
Intra-Sent Phrase Pair 180 ??...? ?If ... then? ??...? ?Whether ...? 
Inter-Sent Phrase Pair 14 ??...?? ?Since... It seems? ??...?? ?Fortunately... otherwise? 
Comparison Single Phrase 34 ??? ?In contrast? ?? ?Unexpectedly? 
Intra-Sent Phrase Pair 38 ??...? ?Even ... but? ??...? ?Although...still? 
Inter-Sent Phrase Pair 15 ??...?? ?Although... In fact? ??...?? ?Although... However? 
Expansion Single Phrase 182 ???? ?in addition? ?? ?moreover? 
Intra-Sent Phrase Pair 106 ??...?? ?Not only...but also? ??...?? ?or...or? 
Inter-Sent Phrase Pair 26 ??...?? ?Firstly...Secondly? ??...?? ?Since...Furthermore? Table 2: Statistics of connectives (discourse markers). 
264
 Figure 5: The upper three level sub-tree of (S1) and the punctuation sub-tree of (S1).  as a syntactic tree, and three levels of tree expansions are extracted as structured syntactic features. To capture syntactic structure, we get the syntactic tree for each sentence using the Stanford parser, and extract the sub-tree of the upper three levels, which represents the fundamental composition of this sentence. In addition, all the paths from the root to each punctuation node in a sentence are extracted. From the paths, the depth of each comma node is counted, and the common parent node of every adjacent clause is also extracted. For example, the upper three level sub-tree of the syntactic tree of (S1) is shown in Figure 5. In addition, the sub-tree in the dotted line forms the structure of the punctuations in the (S1).  Polarity: A Comparison relation implies its two arguments are contrasting, and some contrasts are presented with different polarities in the two arguments. For example, sample (S5) is a case of Comparison.  (S5)  ???????????????????????????????????? (?Despite such favorable natural environment, man-made disasters still make the Khmer people unfortunate to suffer from the pain of war.?)  The first clause in (S5) is positive (?favorable natural environment?), while the last two clauses are negative (?unfortunate to suffer from the pain of war?). Besides the connectives ?? ?despite? and ??  ?still?, the opposing polarity values between the first and the last two clauses is also a strong clue to the existence of a Comparison 
relation. In addition, the same polarity of the last two clauses is also a hint that no Comparison relation occurs between them. To capture polarity information, we estimate the polarity of each clause and detect the negations from the clause. The polarity score is a real number estimated by a sentiment dictionary-based algorithm. For each clause, the polarity score, and the existence of negation are taken as features. 4 Experiments and Discussion 4.1   Experimental Results All the models in the experiments are evaluated by 5-fold cross-validation. The metrics are accuracies and macro-averaged F-scores. The t-test is used for significance testing. We firstly examine our model for the task of two-way classification. In this task, binary classifiers are trained to predict the existence of Contingency and Comparison relations in a given sentence. For meaningful comparison, a majority classifier is used as a baseline model, which always predicts the majority class. In the dataset, 72.6% of the sentences involve neither Contingency nor Comparison. Thus, the major class is ?Nil?, and the accuracy and the F-score of the baseline model is 72.6% and 42.06%, respectively. The experimental results for the two-way classification task are shown in Table 3. In the table, the symbol ? denotes the lowest accuracy which has a significant improvement over the baseline at p=0.05 for the two models. The symbol ? denotes the adding of a single feature yields a significant improvement for the model at p=0.005. The performance of the decision tree and the SVM are similar in terms of accuracy and F-score. Overall, the decision tree model achieves better accuracies. In the two-way classification task, the decision tree model with only the Word feature achieves an accuracy of 76.75%, which is significantly better than the baseline at p=0.05. For both the decision tree and the SVM, Connective is the most useful feature: performance is significantly improved with the addition of Connective.  Besides the binary classification task, we extend our model to tackle the task of finer classification. In the second task, four-way classifiers are trained  
265
  Decision Tree SVM Features Accuracy F-Score Accuracy F-Score Word ?76.75%  58.94% 72.36% 56.54% +POS  77.15% 61.72% 72.28%  60.53% +Length 77.15%  61.72%  72.60% 61.09% +Connective ?81.63%  71.11% ?78.05% 69.17% +Dependency 81.14% 70.79% 77.80% 68.79% +Structure  81.30%  70.78%  ?77.48% 69.08% +Polarity  81.30%  70.78% 77.64% 69.09% Table 3: Performance of the two-way classification.    Decision Tree SVM Features Accuracy F-Score Accuracy F-Score Word ?76.50%  34.72% 73.58% 31.54% +POS  76.99% 36.77% 72.52%  34.44% +Length  76.99%  36.77% 72.36% 34.54% +Connective  79.84%  44.08% ?77.89%  45.26% +Dependency 79.92% 44.47% ?77.07% 44.42% +Structure   79.92%   44.47% 77.15% 44.69% +Polarity  79.92%  44.47% 77.40% 44.80% Table 4: Performance of the four-way classification.   Decision Tree SVM Features Accuracy F-Score Accuracy F-Score Word 73.66%   3.00% 70.00% 3.62% +POS  73.66% 3.00% 69.84% 4.29% +Length 73.66%  3.00% 70.00% 5.08% +Connective  74.80%  4.90% 74.39% 7.66% +Dependency  74.72% 4.61% 72.60% 5.60% +Structure  74.72%  4.61% 73.01% 5.49% +Polarity  74.72%  4.61% 72.76% 5.23% Table 5: Performance of the 49-way classification.   Task Explicit Implicit Accuracy F-score Accuracy F-score 2-way 77.97% 69.26% 88.98% 50.64% 4-way 76.06% 42.54% 88.98% 31.39% 49-way 71.33% 4.88% 89.41% 1.92% Table 6: Performances for explicit cases and implicit cases.  to predict a given sentence with four classes: existence of Contingency relations only, existence of Comparison relations only, existence of Both relations, and Nil. The experimental results of the four-way classification task are shown in Table 4. Consistent with the results of the two-way classification task, the addition of Connective to the SVM yields a significant improvement at p=0.005. The performance between the decision tree and the SVM is still similar, but the SVM achieves a slightly better F-score of 45.26% in comparison with the best F-score of 44.47% achieved by the decision tree. 
We further extend our model to predict the full relation structure of a given sentence as shown in Figure 1 and Figure 4. This is a 49-way classification task because there are 49 types of the full relation structures in the dataset. Not only as many as 49-ways, 72.6% of instances belong to the Nil relation, which yields an unbalanced classification problem. The experimental results are shown in Table 5. In the most challenging case, the SVM achieves a better F-score of 7.66% in comparison with the F-score of 4.90% achieved by the decision tree. Connective is still the most helpful feature. Comparing the F-scores of the SVM in the three tasks with the F-scores of the decision tree, it shows that the SVM performs better for predicting finer classes. 4.2 Explicit versus Implicit We compare the performances between the explicit instances and the implicit instances for the three tasks with the decision tree model trained on all features.  The results are shown in Table 6. The higher accuracies and the lower F-scores of the implicit cases are due to the fact that the classifier tends to predict the sentences as Nil when no connective is found, and most implicit samples are Nil. For example, the relation of Contingency in implicit sample (S6) should be inferred from the meaning of ?? ?brought?.  (S6) ??????????????????????????(?The unique geographical environment, it really brought the infinite wealth to this hundred-year port.?)  In addition, some informal/spoken phrases are useful clues for predicting the relations, but they are not present in our connective dictionary. For example, the phrase ? ?  ?if? implies a Contingency relation in (S7). This issue can be addressed by using a larger connective dictionary that contains informal and spoken phrases.  (S7) ??????????????????????? (?If you want to backpacking, how about an organized tour??)    We regard an instance as explicit if there is at least one connective in the sentence. However, many explicit instances are still not easy to label 
266
even with the connectives. As a result, predicting explicit samples is much more challenging than the task of recognizing explicit discourse relations in English. One reason is the ambiguous usage of connectives as shown in (S2). The following sentence depicts another issue. The word ?? ?however? in (S8) is a connective used as a marker of an inter-sentential relation. That is, the entire sentence is one of the arguments of an inter-sentential Comparison relation, but it does not contain any intra-sentential relation inside the sentence itself.   (S8) ????????????????????????(?However, Fu Wu Kang, who speaks fluent Chinese, openly criticizes this opinion.?)  The fact that connectives possess multiple senses is one of the important reasons for their misclassification. This issue can be addressed by employing contextual information such as the neighboring sentences. 4.3 Number of Clauses We compare the performance among the 2-clause instances, the 3-clause instances, and the 4-clause instances for the three tasks with the decision tree model trained on all the features. The accuracies (A) and F-scores (F) are reported in Table 7.  Comparing the two-way classification and the four-way classification tasks, the performance of the longer instances decreases a little in relation labeling. Although sentence complexity increases with length, a longer sentence provides more information at the same time. In the 49-way classification, the model should predict the sentence structure and the relation tags from the 49 candidate classes. The performances are greatly decreased because the feasible classes are substantially increased along with the number of clauses.  4.4 Contingency versus Comparison The confusion matrix of the decision tree model trained on all features for the four-way classification is shown in Table 8. Each row represents the samples in an actual class, while each column of the matrix represents the samples in a predicted class. The precision (P), recall (R), 
  Task 2-Clause 3-Clause 4-Clause A (%) F (%) A (%) F (%) A (%) F (%) 2-way 81.80 66.39 78.52 70.32 79.41 69.32 4-way 79.84 49.98 75.62 42.64 80.88 46.73 49-way 80.23 29.62 70.02 9.56 69.85 2.25 Table 7: Performances of clauses of different lengths.  Actual Class Predicted Class Performance Cont. Comp. Both Nil P (%) R (%) F (%) Cont. 61 3 0 131 81.33 31.28 45.19 Comp. 3 40 0 83 74.07 31.75 44.44 Both 2 4 0 5 0 0 0 Nil 9 7 0 882 80.11 98.22 88.24 Table 8: Confusion matrix of the best model in the 4-way classification.  Feature instance Category Usages The first token in the third clause is the word? ?but; however? Word 100% The first token in the second clause is the word ? ?but; however? Word 99% The first token in the third clause is a single connective of Contingency Connective 98% The first token in the first clause is the word ?? ?because; due to? Word 96% There is at least one word ?? ?in order to avoid? in the entire sentence Word 95% The first token in the second clause is the word ? ?moreover; while; but? Word 94% The first token in the third clause is a single connective of Comparison Connective 93% The second clause contains a single connective of Contingency Connective 92% The first token in the second clause is a single connective of Contingency Connective 91% The first clause contains a single connective of Contingency Connective 90% Table 9: Instances of the top ten useful features for the decision tree model  and F-score (F) for each class are provided on the right side of the table. The class Both is too small to train the model, thus our model does not correctly predict the samples in the Both class. The confusion matrix shows that the confusions between the classes Contingency and Comparison are very rare. The major issue is to distinguish Contingency and Comparison from the largest class, Nil. The lower recall of the Contingency and Comparison relations also show that our model tends to predict the instances as the largest class. 4.5 Features The top ten useful feature instances reported by the decision tree model in the 49-way classification are shown in Table 9. Word and Connective provide useful information for the classification. Moreover, 
267
seven of the ten feature instances are about the word or the connective category of the first token in each clause. This result shows that it is crucial to employ the information of the first token in each clause as distinguished features. Certain words, for example, ? ?but; however?, ?? ?because; due to?, and ? ?moreover; while; but? are especially useful for deciding the relations. For this reason, labeling these words carefully is necessary. All the synonyms for each of these words should be clustered and assigned the same category. In addition, a dedicated extractor should be involved in accurately fetching these words from the sentence in order to reduce tokenization errors introduced by the Chinese word segmenter.  The advanced features such as Dependency, Structure, and Polarity are not helpful as expected. One possible reason is that the training data is still not enough to model the complex features. In such a case, the surface features are even more useful. Sample (S1) shows an interesting case of the use of polarity information. The first clause of (S1) is positive (?????????????????????????  ?tried to make the Yangmingshan area a more natural state as the long-term garden of Taipei?), the second clause of (S1) is also positive (??????????????  ?the two-day weekend and the improved economic conditions.?), while the last clause of (S1) is negative (???????????????????  ?the issues of tourists parking, garbage, and other indirect effects?). The polarity of the last clause is opposite to those of the second clause, but they do not form a Comparison relation. Instead, a Contingency relation occurs between the last two clauses. Likewise, the polarities of the first and second clauses are both positive, but a Comparison relation occurs after the first clause. In fact, we realize that this is a complex case after performing an in-depth analysis. Because the last clause plays the role of effect in the Contingency relation, the negative polarity of the last clause makes the last two clauses form a negative polarity. For this reason, a Comparison relation occurs between the first argument with positive polarity and the second argument (i.e., the last two clauses) with negative polarity without a doubt. The polarity diagram of sample (S1) is shown in Figure 6.  
 
 Figure 6: Polarity diagram of (S1).  Overall, the interaction among structure, relation, and polarity is complicated. The surface polarity information we extract by using the sentiment dictionary-based algorithm does not capture such complexity well. A dedicated structure-sensitive polarity tagger will be utilized in future work. 5 Conclusion and Future Work In this paper, we addressed the problem of intra-sentential Contingency and Comparison relation detection in Chinese. This is a challenging task because Chinese sentences tend to be very long and therefore contain more clauses. To tackle this problem, we constructed a moderate-sized corpus and proposed a learning-based approach that achieves accuracies of 81.63%, 79.92%, and 74.80% and F-scores of 71.11%, 45.26%, and 7.66% in the two-way, the four-way, and the 49-way classification tasks, respectively. From the experiments, we found that performance could be significantly improved by adding the Connective feature. The next step is to enlarge the connective dictionary automatically by a text mining approach, in particular with those informal connectives, in order to boost performance. The advanced features such as Dependency, Structure, and Polarity are not as helpful as expected due to the small size of the corpus. In future work, we plan to construct a large Chinese discourse Treebank based on the methodology proposed in Section 2 and release the corpus to the public. Naturally, the intra-sentential relations are important cues for discourse relation detection at the inter-sentential level. How to integrate cues from these two levels will be investigated. Besides, relation labeling and structure prediction are tackled at the same time with the same learning algorithm in this study. We will explore different methods to tackle the two problems separately to reduce the complexity.  
268
References Chu-Ren Huang, Feng-Yi Chen, Keh-Jiann Chen, Zhao-ming Gao, and Kuang-Yu Chen. 2000. Sinica Treebank: Design Criteria, Annotation Guidelines, and On-line Interface. In Proceedings of 2nd Chinese Language Processing Workshop (Held in conjunction with the 38th Annual Meeting of the Association for Computational Linguistics, ACL-2000), pages 29-37. Hen-Hsen Huang and Hsin-Hsi Chen. 2011. Chinese Discourse Relation Recognition. In Proceedings of 5th International Joint Conference on Natural Language Processing (IJCNLP 2011), pages 1442-1446. Aravind Joshi and Bonnie L. Webber. 2004. The Penn Discourse Treebank. In Proceedings of the Language and Resources and Evaluation Conference, Lisbon. Ziheng Lin, Min-Yen Kan, and Hwee Tou Ng. 2009. Recognizing Implicit Discourse Relations in the Penn Discourse Treebank. In Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing (EMNLP 2009), Singapore.  Ziheng Lin, Hwee Tou Ng, and Min-Yen Kan. 2011.  Automatically Evaluating Text Coherence Using Discourse Relations. In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics (ACL 2011), pages 997-1006. Emily Pitler and Ani Nenkova. 2009. Using Syntax to Disambiguate Explicit Discourse Connectives in Text. In Proceedings of the ACL-IJCNLP 2009 Conference Short Papers, pages 13-1, Singapore. Emily Pitler, Annie Louis, and Ani Nenkova. 2009. Automatic Sense Prediction for Implicit Discourse Relations in Text. In Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP (ACL-IJCNLP 2009), Singapore. Rashmi Prasad, Nikhil Dinesh, Alan Lee, Eleni Miltsakaki, Livio Robaldo, Aravind Joshi, and Bonnie Webber. 2008. The Penn Discourse Treebank 2.0. In Proceedings of the 6th International Conference on Language Resources and Evaluation (LREC). WenTing Wang, Jian Su, and Chew Lim Tan. 2010. Kernel Based Discourse Relation Recognition with Temporal Ordering Information. In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics (ACL 2010), Uppsala, Sweden, July. 
Nianwen Xue. 2005. Annotating Discourse Connectives in the Chinese Treebank. In Proceedings of the Workshop on Frontiers in Corpus Annotation II: Pie in the Sky, pages 84-91. 
269
Proceedings of the 7th Linguistic Annotation Workshop & Interoperability with Discourse, pages 70?78,
Sofia, Bulgaria, August 8-9, 2013. c?2013 Association for Computational Linguistics
Analyses of the Association between Discourse Relation and Sentiment Polarity with a Chinese Human-Annotated Corpus 
 Hen-Hsen Huang Chi-Hsin Yu Tai-Wei Chang Cong-Kai Lin Hsin-Hsi Chen Department of Computer Science and Information Engineering National Taiwan University, Taipei, Taiwan {hhhuang, jsyu, twchang, cklin}@nlg.csie.ntu.edu.tw; hhchen@ntu.edu.tw     Abstract 
Discourse relation may entail sentiment in-formation. In this work, we annotate both discourse relation and sentiment information on a moderate-sized Chinese corpus extracted from the ClueWeb09. Based on the annota-tion, we investigate the association between the relation type and the sentiment polarity in Chinese and interpret the data from various aspects. Finally, we highlight some language phenomena and give some remarks. 1 Introduction A discourse relation indicates how two argu-ments (i.e., elementary discourse units) cohere to each other. Various discourse relations were de-fined according to different taxonomy (Carlson and Marcu, 2001; Carlson et al, 2002; Prasad et al, 2008). In the work of the Penn Discourse Treebank 2.0 annotation, Prasad et al (2008) labeled four grammatical classes of connectives in English, including subordinating conjunctions, coordinating conjunctions, adverbial connectives, and implicit connectives. Besides, the sense of each connective was also tagged. They defined three levels of sense hierarchy for the connec-tives. The four classes on the top level are Tem-poral, Contingency, Comparison, and Expansion.  There are explicit and implicit uses of dis-course relations. An explicit discourse relation indicates the arguments are connected with an overt discourse marker (i.e., connective). A con-nective joins two discourse units such as phrases, clauses, or sentences together. For example, the word however is a common connective that indi-cates a Comparison relation between two argu-ments. The sense of a discourse marker denotes how its two arguments cohere. In other words, a 
discourse marker presents the relation of its two arguments. In other cases, discourse marker is absent from an implicit relation. However, readers can still infer the relation from its argument pair. To re-solve implicit discourse relations, i.e., without the information from discourse markers, is more challenging (Lin et al, 2009; Zhou et al, 2010).   Hutchinson (2004) pointed out the properties of a discourse marker from three dimensions, including polarity, veridicality, and type. The polarity of a discourse marker indicates the sen-timent transition of its two arguments. Veridi-cality, the second dimension of a discourse marker, specifies whether both the two argu-ments are true or not. Type, similar to the sense which is annotated in the PDTB, is the third di-mension of a discourse marker.  Our previous work (Huang and Chen, 2012a; Huang and Chen, 2012b) addressed the interac-tion between the sentiment polarity and the dis-course structure in Chinese. Consider (S1), which consists of three clauses and forms a nest-ed discourse structure shown in Figure 1.  (S1) ??????????????????????????????? (Although the management office tried to make the Yang-mingshan area a more natural environment as the long-term garden of Taipei)??????????????? (but due to the two-day weekend and the improved economic conditions)??????????????????? (the is-sues of tourist parking, garbage, and other indi-rect effects become more serious)?  The second and the third clauses form a Con-tingency relation with a sentiment polarity transi-tion from Positive to Negative. Furthermore, 
70
 Figure 1: Discourse structure and sentiment po-larities of (S1).  these two clauses also constitute one of the ar-guments of a Positive-Negative Comparison rela-tion. As the PDTB 2.0 annotation manual sug-gests (Prasad, et al, 2007), a Comparison rela-tion is established to emphasize the differences between two arguments. Therefore, it is expected that the two arguments of a Comparison relation are relatively likely to have the opposing polarity states (i.e., Positive-Negative or Negative-Positive). On the other hand, the two arguments of an Expansion relation are relatively likely to belong to the same polarity states (e.g., Positive-Positive or Neutral-Neutral).  Discourse relation recognition (Hernault et al, 2010; Soricut and Marcu, 2003) and sentiment analysis (Pang and Lee, 2008) have attracted much attention recently. Due to the limitation of the resources, the research on Chinese discourse relation analysis is relatively rare. In our previ-ous work, we annotated a collection of Chinese discourse corpora, namely NTU Chinese Dis-course Resources (http://nlg.csie.ntu.edu.tw/ntu-discourse/), for inter-sentential and intra-sentential discourse relation recognition (Huang and Chen, 2011; Huang and Chen, 2012a). How-ever, no sentiment information is labeled in these corpora. In another work (Huang and Chen, 2012b), we proposed an annotation scheme to construct a Chinese discourse corpus with rich information including sentiment polarities, but the corpus is still under construction due to its complexity. Zhou and Xue (2012) did PDTB-style Chinese discourse corpus annotation, but the corpus is also not available yet. In this paper, we annotate a moderate-sized Chinese corpus with the information of discourse relations and sentiment polarities. Total 7,638 sentences are sampled from the ClueWeb09. We review the results of annotation and analyze some language phenomena found in the corpus.  The rest of this paper is organized as follows. In Section 2, we introduce the ClueWeb corpus 
and a dictionary of Chinese discourse markers. In Section 3, the criteria to sample instances and the annotation scheme are shown. We analyze the language phenomena found in the annotated data and discuss the correlation between discourse relations and sentiment polarities in Section 4. Finally, we conclude the remarks in Section 5.  2 Linguistic Resources The PDTB is a popular dataset used in the Eng-lish discourse research. In contrast, no Chinese discourse corpus is publicly available at present. To construct a Chinese discourse corpus, we sample instances from a huge Chinese corpus (Yu et al, 2012). This corpus was developed based on the ClueWeb09 dataset, where Chinese material is the second largest. It contains a total of 9,598,430,559 POS-tagged sentences in 172,298,866 documents.   In this paper, only the explicit discourse rela-tions are concerned. A dictionary of discourse markers is consulted to extract the instances of explicit discourse relations from the ClueWeb. This Chinese discourse marker dictionary is de-veloped based on Cheng and Tian (1989), Cheng (2006) and Lu (2007). Table 1 shows an over-view of the discourse marker dictionary. It con-tains 808 words and word pairs mapped into the PDTB four top-level classes (Cheng and Tian, 1989; Wolf and Gibson, 2005). Besides the types of discourse relations, we further classify the markers into three groups of scopes shown in the second column, including Single word, Intra-sentential, and Inter-sentential, according to their grammatical usages. The Single word group con-tains those individual words used as discourse markers. The Intra-sentential group contains pairs of words that occur inside the same senten-ce and denote a discourse relation. Here, a Chi-nese sentence is defined as a sequence of succes-sive words that is ended by a period, a question mark, or an exclamation mark. The clauses of a sentence are delimited by commas. The Inter-sentential discourse markers are similar to the Intra-sentential ones, but the two words of a pair individually appear in different sentences. Some discourse markers can be used as both Inter-sentential and Intra-sentential. In this work, the Inter-sentential only discourse markers are ex-cluded because we only concern the discourse relation occurring within a sentence. The third column lists the number of discourse markers for each scope under each PDTB class, and the fourth column gives some examples. 
71
PDTB Class Scope # Markers Examples 
Expansion 
Single word 177 ?? (besides), ?? (or), ?? (not only), ?? (such as) Intra-sentential 106 ??????? (on the one hand ... on the other hand), ????? (not ... but), ???? (not only ... also) Inter-sentential 26 ????? (first ... second), ???? (or ... perhaps), ????? (not only ... not only) 
Temporal 
Single word 41 ?? (then) Intra-sentential 80 ????? (first ... finally) Inter-sentential 30 ????? (first ... now) 
Comparison 
Single word 34 ?? (even if) Intra-sentential 38 ???? (although ... but) Inter-sentential 15 ????? (in spite of ... in fact) 
Contingency 
Single word 67 ?? (because), ? (if), ?? (suppose), ?? (in order to avoid) Intra-sentential 180 ??? (because ... then), ??? (if then), ??? (any ... can) Inter-sentential 14 ????? (since ... then), ????? (at least ... otherwise) Table 1: Overview of a Chinese discourse marker dictionary. 3 Annotation Based on the Chinese part of the ClueWeb09 (Yu et al, 2012), we sample a moderate-sized data with some criteria and annotate them with the information of discourse relations and sentiment polarities. 3.1 Sampling a reliable dataset Discourse relations may be explicit or implicit, and a sentence may contain more than one dis-course marker. Multiple discourse relations oc-curring in a sentence will make the annotation more complex. In this work, we focus on the cor-relation between discourse relations and senti-ment polarity. To get a reliable dataset for analy-sis, we sample sentences based on the following three criteria. 1.  A sentence should contain only two clauses. 2. A sentence should contain exact one dis-course marker shown in the Chinese discourse marker dictionary. We match the discourse marker on the word level. For the Single word markers, the marker can appear in either of the clauses. For the pairwise markers, the first word should appear in the first clause, and the second word should appear in the second one. 3. The lengths of both clauses in a sentence are no more than 20 Chinese characters.  As shown in Figure 1, the sentiment polarity determination is more challenging when more than one discourse relation is involved in a sen-tence. In order to facilitate the analysis, we focus on those sentences that contain exact one dis-
course marker. The limitation of clause length is also applied to avoid the noise from implicit dis-course relation. Based on a preliminary statistics, we find that most clauses in the Chinese part of the ClueWeb (Yu et al, 2012) are no longer than 20 Chinese characters shown in Figure 2. 
 Figure 2: Length distribution in the ClueWeb. 3.2 Annotation scheme Using the criteria described in Section 3.1, total 7,638 instances are randomly selected from the ClueWeb, and 87 native speakers annotate these instances. Each instance is shown to three anno-tators. The annotator labels the polarities of the first clause, the second clause, and the whole in-stance with Negative, Neutral, and Positive. In addition, the discourse relation between the two clauses is also labeled with Temporal, Contin-gency, Comparison, and Expansion. For each target sentence, the annotation is based on the information from the sentence only. The sen-tences are not given to annotators. Finally, the majority of each label is taken. For example, the 
72
polarity p1 of the first clause in the instance (S2) is labeled as Positive, the polarity p2 of the sec-ond clause is labeled as Negative, the resulting polarity pw of the whole sentence is also labeled as Negative, and the discourse relation between the two clauses is labeled as Comparison.    (S2) ???????????????????? (Although French brand cars share more than half of the domestic market share)?????????? (but the market share con-tinued to shrink)? The inter-agreements of p1, p2, pw, and dis-course relation among annotators are 0.49, 0.50, 0.47, and 0.41 in Fleiss? Kappa values, respec-tively (all are moderate agreement). The result-ing corpus is publicly available on the website of NTU Chinese Discourse Resources1.  4 Results and Discussion To investigate the corpus annotated with dis-course relation and sentiment polarity, we firstly give an overview of results with respect to these two types of linguistic phenomena. And then, the most frequent discourse markers for each class of discourse relations are discussed. Finally, we reorganize the results to several aspects and dis-cuss the association between discourse relations and sentiment polarities.  4.1 Overview of the annotated corpus The distribution of the discourse relations versus the polarities of whole sentence (pw) is shown in Table 2. Compared to the distributions of dis-course relations in the Penn Discourse Treebank (Prasad et al, 2008) shown in Table 3, the ex-plicit Chinese discourse corpus is more similar to the whole English corpus. The instances of Ex-pansion form the largest set among four dis-course relation classes. In Chinese, the instances of Expansion are even more. Temporal is the most infrequent relation which has close fre-quencies in both corpora. The different charac-teristic is the frequency of Comparison relation. In our Chinese corpus, the frequency of Compar-ison relation is about half of that in the PDTB.  In Table 2, the symbol ? is used to highlight the relatively major polarity of each relation. The symbol ? is marked when the polarity is the ma-jority (i.e., with a frequency greater than 50%). Near half (49.11%) of the instances belong to Neutral. Neutral statements are major in Tem-                                                1 http://nlg.csie.ntu.edu.tw/ntu-discourse/  
poral and Expansion classes. On the other hand, Comparison is the relation which is most in-volved in expressing sentiment, negative senti-ment in particular. Contingency is second to Comparison in expressing sentiment. The distribution of the discourse relations ver-sus (p1, p2), the sentiment polarity transitions be-tween two clauses, is shown in Table 4. Neutral-Neutral is the most frequent polarity transition in all relations. More than half of the Temporal in-stances are Neutral-Neutral. The reason may be that the Temporal relations are usually used in the sentences that describe the objective facts of the past, present, or the future. In such sentences, the sentiments are relatively rare. On the other hand, the sentences of Comparison and Contin-gency occur more in the critical and analytical scenarios. Although the most frequent transition of Com-parison is also Neutral-Neutral (23.14%), the other three types of transitions, Positive-Negative, Neutral-Negative, and Negative-Positive, have close frequencies of 22.71%, 16.90%, and 15.72%, respectively. Moreover, Negative polar-ity is involved in all these three transitions in one of their clauses. The relations between p1, p2, and pw are also interesting. Table 5 shows the top 10 most fre-quent correlations of the polarities (p1, p2, pw) of the first clause, the second clause, and the whole sentence. On the one hand, it is not surprising that most instances belong to (Neutral, Neutral, Neutral). On the other hand, it is worthy of not-ing that p2 and pw are identical in the top eight types of combinations in Table 5. In other words, the resulting sentiment polarity of a two-clause sentence is mostly consistent with the polarity of   Relation # % Neu  (%) Pos  (%) Neg  (%) Temporal 849 11.12 ?60.66 22.38 16.96 Contingency 1,598 20.92 ?44.74 26.97 28.29 Comparison 929 12.16 33.37 27.88 ?38.75 Expansion 4,262 55.80 ?51.88 31.75 16.38 Overall 7,638 100.00 ?49.11 29.24 21.65 Table 2: Distribution of discourse relations vs. polarities of whole sentences.  Relation Only Explicit Cases Total # % # % Temporal 3,612 18.88 4,650 12.71 Contingency 3,581 18.72 8,042 21.98 Comparison 5,516 28.83 8,394 22.94 Expansion 6,424 33.58 15,506 42.38 Overall 19,133 100.00 36,592 100.00 Table 3: Distribution of discourse relations in the Penn Discourse TreeBank 2.0. 
73
PDTB Class # Distribution of each type of sentiment polarity transition (p1, p2) (%) Neu Neu Pos Neu Neg Neu Neu Pos Pos Pos Neg Pos Neu Neg Pos Neg Neg Neg Temporal 849 ?57.01 1.53 2.12 16.37 3.53 2.36 12.72 1.06 3.30 Contingency 1,598 ?35.42 3.69 5.88 13.70 10.45 2.32 11.64 1.81 15.08 Comparison 929 ?23.14 2.69 2.48 8.61 3.12 15.72 16.90 22.71 4.63 Expansion 4,262 ?48.33 2.86 1.92 14.24 16.19 0.59 7.86 0.63 7.37 Overall 7,638 ?43.53 2.87 2.84 13.68 11.99 2.99 10.29 3.61 8.20 Table 4: Distribution of discourse relations vs. types of sentiment transitions.  p1 p2 pw Occurrences Neutral Neutral Neutral 3,268 Neutral Positive Positive 945 Positive Positive Positive 908 Neutral Negative Negative 706 Negative Negative Negative 614 Positive Negative Negative 204 Negative Positive Positive 199 Negative Neutral Neutral 125 Positive Neutral Positive 121 Neutral Positive Neutral 99 Table 5: Most frequent (p1, p2, pw) combinations.   p1 = pw p1 ? pw Total p2 = pw 62.71% 29.79% 92.50% p2 ? pw 5.51% 1.99% 7.50% Total 68.22% 31.78% 100.00% Table 6: Correlations between (p1,pw) and (p2,pw).  the second clause. Table 6 shows the correlations of sentiment polarities between clauses and the whole sentence. Total 92.50% of instances be-long to the case (p2 = pw), where the polarity of the second clause is identical to the polarity of the whole sentence. In Chinese writing, putting the important part of a sentence at the end of the sentence is very common.  4.2 Frequent discourse markers The top discourse markers in our Chinese corpus are shown in Table 7. For each PDTB class, the five most frequent discourse markers are listed. In each row of the table, its number of occur-rences and the distribution of its nine sentiment polarity transitions are given. Note that there are three polarities, i.e., positive, neutral, and nega-tive. The relatively major sentiment polarity tran-sition of each discourser maker is labeled with the symbol ?. The symbol ? is marked when the sentiment polarity is the majority, i.e., its ratio is greater than 50%. Some discourse markers are the top markers in more than one discourse relation such as ? (also) and ? (still). In the discourse marker dictionary, the word ?  (also) is defined as a discourse 
marker of the Expansion relation. However, this word is frequent in the instances of all the four relations. In different relations, the distributions of the sentiment transitions of this word differ. In other words, the word ?  (also), which is a common word in Chinese, is not only used as a discourse marker for emphasizing the Expansion relation, but also has various senses in other us-ages.  For instance, the word ? in (S3) is a dis-course marker to denote an Expansion relation, but it is a particle in (S4). In fact, (S4) is an in-stance of the implicit Contingency relation. We ignore all of instances of the word ? (also) in the following analysis since it is an outlier. (S3) ??????????? (This is an af-firmation of our work)??????????????(and also our encouragement and mo-tivation)? (S4) ??????? (The mind cannot be open to forward progress)???????? (the world becomes narrow)? The word ? (still) is another ambiguous dis-course marker. Besides the Expansion relation defined in the dictionary, it is sometimes used to denote the Temporal relation, especially in the negation context, e.g., ?? (not yet). The two frequent discourse markers of the Contingency relation, ?? (due to) and ?? (because) share the similar sense, and their dis-tributions of sentiment polarity transitions are more consistent than the other markers of the Contingency relation.  The most frequent discourse marker of the Comparison class is ? (but). The other two dis-course markers ? (but) and ?? (but) share the similar sense, however, their polarity distribu-tions differ significantly. Compared to the more general marker ?  (but), the second frequent marker ? (but) is bolder and more critical. (S5) is an example of the marker ? (but). As shown in our data, the marker ? (but) is likely to high-light the negative sentences. 
74
PDTB  Class Discourse Markers # Distribution of each type of sentiment polarity transition (%) Neu Neu Pos Neu Neg Neu Neu Pos Pos Pos Neg Pos Neu Neg Pos Neg Neg Neg Temporal ?? (and then) in Arg1 69 ?50.72 1.45 2.90 15.94 5.80 2.90 8.70 4.35 7.25 ? (also) in Arg2 50 ?44.00 2.00 2.00 18.00 6.00 0.00 20.00 0.00 8.00 ? (again) in Arg2 49 ?71.43 0.00 0.00 12.24 2.04 0.00 10.20 4.08 0.00 ? (still) in Arg2 46 ?58.70 0.00 0.00 10.87 8.70 0.00 17.39 0.00 4.35 ? (again) in Arg2 38 ?78.95 2.63 0.00 10.53 0.00 0.00 2.63 0.00 5.26 Contingency ?? (if) in Arg1 190 ?42.63 4.21 11.58 14.21 3.68 3.16 10.53 1.05 8.95 ?? (due to) in Arg1 82 ?31.71 2.44 2.44 4.88 18.29 3.66 13.41 1.22 21.95 ? (also) in Arg2 77 20.78 0.00 1.30 20.78 19.48 0.00 11.69 2.60 ?23.38 ?? (because ) in Arg1 70 ?28.57 4.29 7.14 7.14 10.00 2.86 18.57 4.29 17.14 ?? (in order to) in Arg1 62 ?50.00 14.52 1.61 6.45 9.68 1.61 8.06 6.45 1.61 Comparison ? (but) in Arg2 176 21.59 4.55 2.84 4.55 3.41 16.48 15.91 ?28.98 1.70 ? (but) in Arg2 85 11.76 0.00 2.35 4.71 1.18 10.59 22.35 ?42.35 4.71 ? (however) in Arg2 77 ?46.75 5.19 0.00 5.19 1.30 3.90 10.39 22.08 5.19 ? (also) in Arg2 44 ?31.82 0.00 2.27 6.82 15.91 13.64 18.18 2.27 9.09 ?? (but) in Arg2 44 15.91 4.55 0.00 0.00 2.27 25.00 11.36 ?40.91 0.00 Expansion ? (also) in Arg2 603 ?43.62 1.66 1.49 15.26 19.07 1.00 7.79 0.33 9.78 ? (still) in Arg2 231 ?50.65 2.60 0.87 11.26 14.72 0.87 9.96 0.43 8.66 ? (say) in Arg1 206 ?48.54 2.43 0.49 18.45 9.22 0.00 16.50 0.49 3.88 ? (and) in Arg2 191 ?54.45 3.14 0.52 10.47 25.65 0.00 4.19 0.00 1.57 ? (also) in Arg1 159 ?37.11 7.55 3.14 11.95 25.16 0.63 3.77 0.63 10.06 Table 7. Five most frequent discourse makers of each PDTB class in our corpus.   (S5) ???????????  (The new type of crime is so startling)????????????(but had never been disclosed before solved)? The other discourser marker ?? (but) is an emphasized version of the marker ? (but) so that it is more likely used in the stronger polarity transitions such as Positive-Negative and Nega-tive-Positive. In addition, the sense of the marker ? (however) is also similar to the sense of ? (but), but it is more frequent to be used in the neutral situations. These linguistic phenomena show that the synonyms may have different sen-timent usages in the real world. 4.3 Association between discourse relation and sentiment polarity To analyze the data at a higher level, we reor-ganize the sentiment transitions into several tran-sition categories from four aspects. The details are shown in Table 8. The first aspect is Polarity Tendency, which classifies the transitions into three categories, including Positive-Tendency, Neutral, and Negative-Tendency. This aspect reflects the overall polarity of both arguments. The Negative-Positive transition is considered as Positive-Tendency because the emphasis of a Chinese sentence is usually placed in the last clause. Similarly, the Positive-Negative transition is considered as Negative-Tendency. The second aspect is Polarity Change, which indicates if the polarities of both arguments are opposite. Only Negative-Positive and Positive-Negative are re-garded as Opposite. All the rest transitions are 
treated as NonOpposite. The third aspect is Di-rection, which captures the movement from the first clause to the second one. To-Positive stands for the transitions in which the polarity of the second clause is more positive than that of the first clause. On the other hand, To-Negative stands for the transitions in which the polarity of the second clause is less positive than that of the first clause. Equal stands for the cases in which the polarities of both clauses are identical. The last aspect is Negativity, which regards the polar-ity of an argument as binary values, i.e., Negative and NonNegative. In this way, we re-classify the nine-way sentiment polarity transitions into four transitions. In other words, both the polarity states Neutral and Positive are merged into one state NonNegative in this aspect. Such a binary scheme is also used in some related work, in which the negative polarity is distinguished and the rest are considered Positive (Kim and Hovy, 2004; Devitt and Ahmad, 2007). For each type of each aspect, five discourse markers that occur more than 10 times in the dataset and have the highest ratio of the corresponding type are listed in the fifth column of Table 8 as significant dis-course markers.  We analyze the annotations according to the four aspects, and the results are shown in Table 9. The chi-squared test is used to test the dependen-cy between the PDTB classes of discourse mark-ers and each aspect of sentiment transitions. The results show that no matter whether the senti-ment polarity transitions are categorized into Po-larity Tendency, Polarity Change, Direction, or Negativity, the classes of discourse relations are 
75
significantly dependent on the sentiment polari-ties of the arguments at p=0.001. In the aspect of Polarity Tendency, the ratios of Neutral in the Temporal and Expansion rela-tions are 57.01% and 48.33%, respectively, which are definitely higher than those of Contin-gency and Comparison relations. In other words, the two arguments of Contingency and Compari-son relations are less likely to be neutral. The ratio of Negative-Tendency of the Comparison relation is 46.72%. It confirms the Comparison relation is likely to be involved in negative statements. As shown in Table 8, three of the five significant discourse markers of Negative-Tendency are the synonyms of ? (but), which are discourse markers of  the Comparison rela-tion. The other two markers, ?? (otherwise) and ? (because), are discourse markers of the Contingency relation. Like the word otherwise in English, ?? (otherwise) is used for introducing what bad scenario will happen if something is not done. The marker ? (because) is not only a significant discourse marker of the category Negative-Tendency, but also a significant marker 
of Negative-Negative from the aspect of Negativ-ity. From the real data, we find this marker is often used in bad cause-and-effect statements. (S6) is an example. The usage of the other dis-course marker ??  (because), which is a syno-nyms of ? (because), is more general.  (S6) ????????? (Because the tow-el is without sunlight for a long time)?????????? (it is easy to breed bacteria and fungi)? The ratio of Opposite of Comparison relation from the aspect of Polarity Change is 38.43%. Although it is not as high as expected, it is the highest among the four PDTB classes and much higher than those of three other classes. Com-pared to the other classes, Comparison is most likely to have a pair of opposite arguments. Four of the five significant discourse markers of Opposite in Table 8 are the synonyms of ? (but). Expansion relation has the highest ratio of NonOpposite. This matches our expectation that the Expansion relation is used to concatenate several events which have similar properties   Aspect  Transition Category Sentiment polarity transitions Explanation Significant Discourse Markers Polarity Tendency Positive-Tendency Pos-Neu, Neu-Pos, Pos-Pos, Neg-Pos The two arguments present an overall positive polarity. ?? ...?  (not only... also), ?? (finally) , ?...? (now that... ), ?? ...?  (as long as... ), ?? (recently) Neutral Neu-Neu Both arguments are neutral.  ?? (and then), ?? (hence) , ?? (at the end), ? (so) , ?? (as well as) Negative-Tendency Pos-Neg, Neg-Neu, Neu-Neg, Neg-Neg The two arguments present an overall negative polarity. ??  (otherwise), ?  (but), ?? (but), ?? (but), ? (because) Polarity Change Opposite Neg-Pos, Pos-Neg The polarities of both arguments are opposite. ?? (but), ??...? (although...) ,  ? (but), ? (but), ?? (but)  NonOpposite Neu-Neu, Pos-Neu, Neg-Neu, Neu-Pos, Pos-Pos, Neu-Neg, Neg-Neg The polarities of both arguments are not opposite. ? (or), ? (as), ?? (moreover), ??...? (if ... may), ?? (say) Direction To-Positive Neg-Neu, Neg-Pos, Neu-Pos The second argument is less negative than the first one. ? ?  (finally), ? ? ... ? (although...) , ?? (recently), ?? ...?  (as long as...) , ?? (seem...) Equal Neg-Neg, Neu-Neu, Pos-Pos Both arguments are the same polarity value. ??...? (Not only... even), ?? (at the end), ?? (in addition), ? (so),  ?...? (now that...) To-Negative Pos-Neu, Pos-Neg, Neu-Neg The second argument is less positive than the first one. ? (but), ?? (but), ?? (but), ?? (otherwise), ??...? (even if...) Negativity NonNegative- NonNegative Neu-Neu, Neu-Pos, Pos-Neu, Pos-Pos Both arguments are not negative.  ??  (as well as), ??  (in the future),  ?? (in order to), ?? (in addition), ?? (and then) NonNegative- Negative Neu-Neg, Pos-Neg The first argument is not negative while the second argument is negative. ?  (but), ??  (otherwise), ?? (but), ??...?  (even if...), ?? (but) Negative-NonNegative Neg-Neu, Neg-Pos The first argument is negative while the second argument is not negative. ??...? (although...), ?? (but), ?? (but), ?? (finally), ? (but) Negative-Negative Neg-Neg Both arguments are negative. ?? (even), ? (but), ? (because), ??...? (if... may), ?? (but) 
76
Table 8: Aspects of sentiment transition. PDTB Class # Polarity Tendency (%) Polarity Change (%) Direction(%) Negativity (%) Pos Tend Neutral Neg Tend Oppo Non Oppo To Pos Eq. To Neg NonNeg-NonNeg NonNeg-Neg Neg-NonNeg Neg-Neg Tem 849 23.79 57.01 19.20 3.42 96.58 20.85 63.84 15.31 78.45 13.78 4.48 3.30 Con 1,598 30.16 35.42 34.42 4.13 95.87 21.90 60.95 17.15 63.27 13.45 8.20 15.08 Com 929 30.14 23.14 46.72 38.43 61.57 26.80 30.89 42.30 37.57 39.61 18.19 4.63 Exp 4,262 33.88 48.33 17.79 1.22 98.78 16.75 71.89 11.36 81.63 8.49 2.51 7.37 Table 9: Statistics of sentiment transition for each PDTB class over the corpus annotated by human.  from certain perspective. The ratio of To-Negative of Comparison rela-tion from the aspect of Direction in Table 9 is 42.30%, which is significantly higher than the ratios of To-Negative of the other classes. This also confirms the Comparison relation is likely to be used to express critical opinions. Further-more, the ratio of Equal of Comparison relations is much lower than those of other classes. This result shows the Comparison relation is more involved in sentiment polarity transitions. The Negativity aspect in Table 9 also shows the NonNegative-Negative is more likely to hap-pen than the Negative-NonNegative in all rela-tions. This statistics reflects a particular phenom-enon ?good words ahead? in Chinese.  That is, speakers tend to express a negative opinion after kind words. The sentiment polarity flips in the instances of the two categories Negative-NonNegative and NonNegative-Negative. However, the significant discourse markers of the two categories are very different. In spite of the general marker ?? (but), the discourse markers ? (but), ?? (oth-erwise), ??...? (even if...), and ?? (but) are often used in NonNegative-Negative, which usu-ally results a negative remark. On the other hand, the discourse markers??...? (although...), ?? (but), ?? (finally), and ? (but) are often used in Negative-NonNegative, which usually results a positive remark. For example, the dis-course marker ?? (finally), which is a dis-course marker of the Temporal relation, is usu-ally used when an event successfully accom-plished after twists and turns such as (S7). (S7) ???????????????	 (Domestic mobile phone giant Ningbo Bird after many tribulations)??????????? (finally successfully fought in the automotive industry)? 
5 Conclusion To investigate the discourse relation and the sen-timent polarity of Chinese discourse markers, we construct a moderate-sized corpus based on the Chinese part of ClueWeb09. In this paper, our annotation scheme and the analysis of the anno-tation results are shown. Total 7,638 instances are annotated by native speakers. The discourse relation distribution of the annotated data is comparable to the distribution of the well-known English discourse corpus PDTB 2.0. Through the data analysis, we validate certain human intui-tions in Chinese language. Near half of instances are in neutral sentiment while the Comparison relation is most likely to be involved in negative sentiment. Furthermore, the high sentiment de-pendency between the last clause and the whole sentence is validated in the data. The data shows the significant association be-tween the discourse relation and the sentiment polarity. The arguments of a Comparison rela-tion or a Contingency relation are more likely to be involved in expressing sentiment. Moreover, the Comparison relation often occurs in the sen-tences with sentiment polarity transitions, and frequently occurs in the instances with the nega-tive sentiment. On the other hand, the arguments of the Temporal and the Expansion relations are relatively objective. The behavior of word choice between synonyms is also observed in the data. Each synonym of a sense may have its own us-age in expressing sentiment. This paper points out the ambiguities of the discourse markers in Chinese.  That is, a marker may suggest more than one discourse relation. Besides, words may have both the functions of discourse connectives and non-discourse ones in their surface forms. These two issues make the interpretation of Chinese discourse markers more challenging. Determination of their correct uses and disambiguation of their discourse functions will be investigated in the future.  
77
Acknowledgments 
This research was partially supported by Excel-lent Research Projects of National Taiwan Uni-versity under contract 102R890858 and 2012 Google Research Award. References Lynn Carlson and Daniel Marcu. 2001. Discourse Tagging Reference Manual.   http://www.isi.edu/~marcu/discourse/tagging-ref-manual.pdf Lynn Carlson, Daniel Marcu, and Mary Ellen Oku-rowski. 2002. RST Discourse Treebank. Linguistic Data Consortium, Philadelphia.  Shou-Yi Cheng. 2006. Corpus-Based Coherence Re-lation Tagging in Chinese Discourse. Master?s Thesis, National Chiao Tung University, Hsinchu, Taiwan. Xianghui Cheng and Xiaolin Tian. 1989. Xian dai Han yu (????), San lian shu dian (????), Hong Kong. Ann Devitt and Khurshid Ahmad. 2007. Sentiment polarity identification in financial news: a cohe-sion-based approach. In Proceedings of the 45th Annual Meeting of the Association of Computa-tional Linguistics (ACL 2007), pages 984-991, Pra-gue, Czech Republic. Hugo Hernault, Helmut Prendinger, David A. duVerle, and Mitsuru Ishizuka. 2010. HILDA: A Discourse Parser Using Support Vector Machine Classifica-tion. Dialogue and Discourse, 1(3): 1-33. Hen-Hsen Huang and Hsin-Hsi Chen. 2011. Chinese discourse relation recognition. In Proceedings of the 5th International Joint Conference on Natural Language Processing (IJCNLP 2011). pages 1442-1446, Chiang Mai, Thailand. Hen-Hsen Huang and Hsin-Hsi Chen. 2012a. Contin-gency and comparison relation labeling and struc-ture prediction in Chinese sentences. In Proceed-ings of the 13th Annual Meeting of the Special In-terest Group on Discourse and Dialogue (SIGDI-AL 2012), pages 261-269, Seoul, South Korea. Hen-Hsen Huang and Hsin-Hsi Chen. 2012b. An An-notation System for Development of Chinese Dis-course Corpus. In Proceedings of the 24th Interna-tional Conference on Computational Linguistics (COLING 2012): Demonstration Papers, pages 223-230, Mumbai, India. Ben Hutchinson. 2004. Acquiring the meaning of dis-course markers. In Proceedings of the 42nd Annual Meeting of the Association for Computational Lin-guistics (ACL 2004), pages 684-691, Barcelona, Spain. 
Soo-Min Kim and Eduard Hovy. 2004. Determining the sentiment of opinions. In Proceedings of the 20th International Conference on Computational Linguistics (COLING-04), pages 1367-1373, Ge-neva, Switzerland. Ziheng Lin, Min-Yen Kan, and Hwee Tou Ng. 2009. Recognizing implicit discourse relations in the Penn Discourse Treebank. In Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing (EMNLP 2009), pages 343-351. Shuxiang Lu. 2007. Eight Hundred Words of The Contemporary Chinese (Xian dai Han yu Ba bai Ci), China Social Sciences Press. Bo Pang and Lillian Lee. 2008. Opinion Mining and Sentiment Analysis. Foundations and Trends in In-formation Retrieval, 2(1-2): 1-135. Rashmi Prasad, Nikhil Dinesh, Alan Lee, Eleni Miltsakaki, Livio Robaldo, Aravind Joshi, and Bonnie Webber. 2007. The Penn Discourse Tree-bank 2.0 Annotation Manual. The PDTB Research Group. Rashmi Prasad, Nikhil Dinesh, Alan Lee, Eleni Miltsakaki, Livio Robaldo, Aravind Joshi, and Bonnie Webber. 2008. The Penn Discourse Tree-Bank 2.0. In Proceedings of the 6th Language Re-sources and Evaluation Conference (LREC 2008), pages 2961-2968, Marrakech, Morocco. Radu Soricut and Daniel Marcu. 2003. Sentence level discourse parsing using syntactic and lexical in-formation. In Proceedings of Human Language Technology Conference of the North American Chapter of the Association for Computational Lin-guistics (HLT/NAACL 2003), pages 149-156, Ed-monton, Canada. Florian Wolf and Edward Gibson. 2005. Representing Discourse Coherence: A Corpus-Based Analysis. Computational Linguistics, 31(2): 249-287. Chi-Hsin Yu, Yi-jie Tang and Hsin-Hsi Chen. 2012. Development of a web-scale Chinese word N-gram corpus with parts of speech information. In Pro-ceedings the 8th International Conference on Lan-guage Resources and Evaluation (LREC 2012), pages 320-324, Istanbul, Turkey. Zhi-Min Zhou, Yu Xu, Zheng-Yu Niu, Man Lan, Jian Su, and Chew Lim Tan. 2010. Predicting discourse connectives for implicit discourse relation recogni-tion. In Proceedings of the 23rd International Con-ference on Computational Linguistics (COLING 2010): Posters, pages 1507-1514. Yuping Zhou and Nianwen Xue. 2012. PDTB-style discourse annotation of Chinese text. In Proceed-ings the 50th Annual Meeting of the Association for Computational Linguistics (ACL 2012), pages 69-77, Jeju, South Korea. 
78
Proceedings of the Second Workshop on Hybrid Approaches to Translation, pages 117?122,
Sofia, Bulgaria, August 8, 2013. c?2013 Association for Computational Linguistics
Uses of Monolingual In-Domain Corpora for Cross-Domain  
Adaptation with Hybrid MT Approaches 
 
 
An-Chang Hsieh, Hen-Hsen Huang and Hsin-Hsi Chen 
Department of Computer Science and Information Engineering 
National Taiwan University 
No. 1, Sec. 4, Roosevelt Road, Taipei, 10617 Taiwan 
{achsieh,hhhuang}@nlg.csie.ntu.edu.tw;hhchen@ntu.edu.tw 
 
  
 
Abstract 
Resource limitation is challenging for cross-
domain adaption. This paper employs patterns 
identified from a monolingual in-domain cor-
pus and patterns learned from the post-edited 
translation results, and translation model as 
well as language model learned from pseudo 
bilingual corpora produced by a baseline MT 
system. The adaptation from a government 
document domain to a medical record 
domain shows the rules mined from the 
monolingual in-domain corpus are useful, 
and the effect of using the selected pseudo 
bilingual corpus is significant.   
1 Introduction 
Bilingual dictionary and corpus are important 
resources for MT applications. They are used for 
lexical choice and model construction. However, 
not all resources are available in bilingual forms 
in each domain. For example, medical records 
are in English only in some countries. In such a 
case, only bilingual dictionary and monolingual 
corpus is available. Lack of bilingual corpus 
makes domain adaptation more challenging.  
A number of adaptation approaches (Civera 
and Juan, 2007; Foster and Kuhn 2007; Foster et al, 
2010, Matsoukas et al, 2009; Zhao et al, 2004) 
have been proposed. They address the reliability 
of a model in a new domain and count the do-
main similarities between a model and the in-
domain development data. The domain relevance 
in different granularities including words, 
phrases, sentences, documents and corpora are 
considered. Ueffing et al (2007) propose semi-
supervised methods which use monolingual data 
in source language to improve translation per-
formance. Schwenk (2008) present lightly-
supervised training to generate additional train-
ing data from the translation results of monolin-
gual data. To deal with the resource-poor issue, 
Bertoldi and Federico (2009) generate a pseudo 
bilingual corpus from the monolingual in-domain 
corpus, and then train a translation model from 
the pseudo bilingual corpus.   
Besides counting similarities and generating 
pseudo bilingual in-domain corpus, text simplifi-
cation (Zhu et al, 2010; Woodsend and Lapata, 
2011; Wubben et al, 2012) is another direction. 
Simplifying a source language text makes the 
translation easier in a background MT system. 
Chen et al (2012a) propose a method to simplify 
a sentence before MT and to restore the transla-
tion of the simplified part after MT. They focus 
on the treatments of input text only, but do not 
consider how to adapt the background MT to the 
specific domain. The translation performance 
depends on the coverage of the simplification 
rules and the quality of the background system. 
This paper adopts the simplification-
translation-restoration methodology (Chen et al, 
2012a), but emphasizes on how to update bilin-
gual translation rules, translation model and lan-
guage model, which are two kernels of rule-
based and statistics-based MT systems, respec-
tively. This paper is organized as follows. Sec-
tion 2 specifies the proposed hybrid MT ap-
proaches to resource-limited domains. The char-
acteristics of available resources including their 
types, their linguality, their belonging domains, 
and their belonging languages are analyzed and 
their uses in translation rule mining and model 
construction are presented. Section 3 discusses 
how to adapt an MT system from a government 
document domain to a medical record domain. 
The experimental setups reflect various settings. 
Section 4 concludes the remarks. 
 
117
 Figure 1: Hybrid MT Approaches 
 
2 Hybrid MT Approaches 
Figure 1 sketches the overall picture of our pro-
posed hybrid MT approaches. A resource is rep-
resented in terms of its linguality, domain, lan-
guage, and type, where MO/BI denotes mono-
lingual/bilingual, ID/OD denotes in-domain/out-
domain, and SL/TL denotes source lan-
guage/target language. For example, an MO-ID-
SL corpus and an MO-ID-TL corpus mean mon-
olingual in-domain corpora in source and in tar-
get languages, respectively. Similarly, a BI-OD 
corpus and a BI-ID dictionary denote a bilingual 
out-domain corpus, and a bilingual in-domain 
dictionary, respectively.   
Resources may be provided by some organi-
zations such as LDC, or collected from hetero-
geneous resources. The MO-ID-SL/TL corpus, 
the BI-OD corpus, and the BI-ID dictionary be-
long to this type. Besides, some outputs generat-
ed by the baseline MT systems are regarded as 
other kinds of resources for enhancing the pro-
posed methods incrementally. Initial translation 
results, selected translation results, and post-
edited translation results, which form pseudo 
bilingual in-domain corpora, belong to this type.   
The following subsections first describe the 
baseline systems with the original resources and 
then specify the advanced systems with the gen-
erated resources. 
2.1 A baseline translation system 
In an extreme case, only a bilingual out-domain 
corpus, a monolingual in-domain corpus in 
source/target language, a bilingual in-domain 
dictionary and a monolingual in-domain thesau-
rus in source language are available. The bilin-
gual out-domain corpus is used to train transla-
tion and language models by Moses. They form 
a background out-domain translation system. 
A pattern miner is used to capture the written 
styles in the monolingual in-domain corpus in 
source language. A monolingual in-domain the-
saurus in source language is looked up to extract 
the class (sense) information of words. Mono-
lingual patterns are mined by counting frequent 
word/class n-grams. Then, the bilingual in-
domain dictionary is introduced to formulate 
translation rules based on the mined monolin-
gual patterns. Here in-domain experts may be 
involved in reviewing the bilingual rules. The 
human cost will affect the number of translation 
rules formulated and thus its coverage. 
The baseline translation system is composed 
of four major steps shown as follows. (1) and (2) 
are pre-processing steps before kernel MT, and 
(4) is a post-processing step after kernel MT. 
(1) Identifying and translating in-domain 
segments from an input sentence by using 
translation rules. 
118
(2) Simplifying the input sentence by replac-
ing the in-domain segments as follows. 
(a) If an in-domain segment is a term in 
the bilingual in-domain dictionary, 
we find a related term (i.e., hypernym 
or synonym) in the in-domain thesau-
rus which has relatively more occur-
rences in the background SMT sys-
tem to replace the term. 
(b) If an in-domain segment is a noun 
phrase, we keep its head only, and 
find a related term of the head as (a). 
(c) If an in-domain segment is a verb 
phrase composed of a verb and a 
noun phrase, we keep the verb and 
simplify the noun phrase as (b). 
(d) If an in-domain segment is a verb 
phrase composed of a verb and a 
prepositional phrase, we keep the 
verb and remove the prepositional 
phrase if it is optional. If the preposi-
tional phrase is mandatory, it is kept 
and simplified as (e). 
(e) If an in-domain segment is a preposi-
tional phrase, we keep the preposition 
and simplify the noun phrase as (b). 
(f) If an in-domain segment is a clause, 
we simplify its children recursively as 
(a)-(e). 
(3) Translating the simplified source sentence 
by using the out-domain background MT 
system. 
(4) Restoring the results of the bilingual in-
domain segments translated in (1) back to 
the translation results generated in (3).  
The restoration is based on the internal 
alignment between the source and the tar-
get sentences. 
2.2 Incremental learning 
There are several alternatives to update the base-
line translation system incrementally. The first 
consideration is the in-domain translation rules.  
They are formed semi-automatically by domain 
experts.  The cost of domain experts results that 
only small portion of n-gram patterns along with 
the corresponding translation are generated. The 
post-editing results suggests more translation 
rules and they are fed back to revise the baseline 
translation system. 
The second consideration is translation model 
and language model in the Moses. In an ideal 
case, the complete monolingual in-domain cor-
pus in source language is translated by the base-
line translation system, then the results are post-
edited by domain experts, and finally the com-
plete post-edited bilingual corpus is fed back to 
revise both translation model and language 
model. However, the post-editing cost by do-
main experts is high. Only some samples of the 
initial translation are edited by domain experts.  
On the one hand, the sampled post-edited in-
domain corpus in target language is used to re-
vise the language model.  On the other hand, the 
in-domain bilingual translation result before 
post-editing is used to revise the translation 
model and the language model. Size and transla-
tion quality are two factors to be considered. We 
will explore the effect of different size of imper-
fect in-domain translation results on refining the 
baseline MT system.  Moreover, a selection 
strategy, e.g., only those translation results com-
pletely in target language are considered, is in-
troduced to sample ?relatively more accurate? 
bilingual translation results. 
In the above incremental learning, translation 
rules, translation model and language model are 
revised individually.  The third consideration is 
to merge some refinements together and exam-
ine their effects on the translation performance. 
3 Cross-Domain Adaptation  
To evaluate the feasibility of the proposed hy-
brid MT approaches, we adapt an English-
Chinese machine translation system from a gov-
ernment document domain to a medical record 
domain. The linguistic resources are described 
first and then the experimental results. 
3.1 Resource description 
Hong Kong parallel text (LDC2004T08), which 
contains official records, law codes, and press 
releases of the Legislative Council, the Depart-
ment of Justice, and the Information Services 
Department of the HKSAR, respectively, and 
UN Chinese-English Parallel Text collection 
(LDC2004E12) is used to train the translation 
model. These two corpora contain total 6.8M 
sentences. The Chinese counterpart of the above 
parallel corpus and the Central News Agency 
part of the Tagged Chinese Gigaword 
(LDC2007T03) are used to train trigram lan-
guage model. These two corpora contain total 
18.8M sentences. The trained models are used in 
Step (3) of the baseline translation system. 
Besides the out-domain corpora for the devel-
opment of translation model and language model, 
we select 60,448 English medical records (1.8M 
sentences) from National Taiwan University 
119
Hospital (NTUH) to learn the n-gram patterns. 
Metathesaurus of the Unified Medical Language 
System (UMLS) provides medical classes of in-
domain words. A bilingual medical domain dic-
tionary composed of 71,687 pairs is collected. 
Total 7.2M word/class 2-grams~5-grams are 
identified. After parsing, there remain 57.2K 
linguistic patterns. A higher order pattern may 
be composed of two lower order patterns. Keep-
ing the covering patterns and ruling out the cov-
ered ones further reduce the size of the extracted 
patterns. The remaining 40.1K patterns are 
translated by dictionary look-up.  Because of the 
high cost of medical record domain experts (i.e., 
physicians), only a small portion is verified. Fi-
nally, 981 translation rules are formulated.  They 
are used in Step (1) of the baseline MT system. 
The detail rule mining and human correction 
process please refer to Chen et al (2012b). 
We further sample 2.1M and 1.1M sentences 
from NTUH medical record datasets, translate 
them by the baseline MT system, and get 2.1M- 
and 1.1M-pseudo bilingual in-domain corpora. 
We will experiment the effects of the corpus size. 
On the other hand, we apply the selection strate-
gy to select 0.95M ?good? translation from 
2.1M-pseudo bilingual in-domain corpus.  Fur-
thermore, some other 1,004 sentences are post-
edited by the domain experts. They are used to 
learn the advanced MT systems. 
To evaluate the baseline and the advanced 
MT systems, we sample 1,000 sentences differ-
ent from the above corpora as the test data, and 
translate them manually as the ground truth.  
3.2 Results and discussion 
Table 1 lists the methods along with the re-
sources they used. B is the baseline MT system. 
Most patterns appearing in the 57.2K learned n-
grams mentioned in Section 3.1 are not reviewed 
by physicians due to their cost. Part of these un-
reviewed patterns may occur in the post-edited 
data. They will be further introduced into M1. In 
the experiments, patterns appearing at least two 
times in the post-edited result are integrated into 
M1. Total 422 new patterns are identified. 
Translation model and language model in M1 is 
the same as those in baseline system.   
In M2-M6, the translation rules are the same 
as those in baseline MT system, only translation 
model and/or language model are re-trained. In 
 
 Translation Rules Translation Model Language Model Tuning Data 
B 981 bilingual translation rules 6.8M government domain bilingual 
sentences 
18.8M government/news domain 
Chinese sentences 
1000 government domain 
bilingual sentences 
M1 981 bilingual translation rules + 
422 mined  rules from post-
editing 
6.8M government domain bilingual 
sentences 
18.8M government/news domain 
Chinese sentences 
200 post-edited medical 
domain sentences 
M2 981 bilingual translation rules 6.8M government domain bilingual 
sentences 
804 post-edited Chinese sentences 200 post-edited medical 
domain sentences 
M3 981 bilingual translation rules 6.8M government domain bilingual 
sentences 
30,000 Chinese sentences selected 
from medical literature 
200 post-edited medical 
domain sentences 
M4 981 bilingual translation rules 1.1M pseudo medical domain bilingual 
sentences generated by M1 
1.1M pseudo medical domain Chinese 
sentences generated by M1 
200 post-edited medical 
domain sentences 
M5 981 bilingual translation rules 2.1M pseudo medical domain bilingual 
sentences generated by M1 
2.1M pseudo medical domain Chinese 
sentences generated by M1 
200 post-edited medical 
domain sentences 
M6 981 bilingual translation rules 0.95M selected pseudo medical do-
main bilingual sentences generated by 
M1 
0.95M selected pseudo medical do-
main Chinese sentences generated by 
M1 
200 post-edited medical 
domain sentences 
M12 981 bilingual translation rules + 
422 mined  rules from post-
editing 
6.8M government domain bilingual 
sentences 
804 post-edited Chinese sentences 200 post-edited medical 
domain sentences 
M13 981 bilingual translation rules + 
422 mined  rules from post-
editing 
6.8M government domain bilingual 
sentences 
30,000 medical domain Chinese sen-
tences 
200 post-edited medical 
domain sentences 
M14 981 bilingual translation rules + 
422 mined  rules from post-
editing 
1.1M pseudo medical domain bilingual 
sentences generated by M1 
1.1M pseudo medical domain Chinese 
sentences generated by M1 
200 post-edited medical 
domain sentences 
M15 981 bilingual translation rules + 
422 mined  rules from post-
editing 
2.1M pseudo medical domain bilingual 
sentences generated by M1 
2.1M pseudo medical domain Chinese 
sentences generated by M1 
200 post-edited medical 
domain sentences 
M16 981 bilingual translation rules + 
422 mined  rules from post-
editing 
0.95M selected pseudo medical do-
main bilingual sentences generated by 
M1 
0.95M selected pseudo medical do-
main Chinese sentences generated by 
M1 
200 post-edited medical 
domain sentences 
Table 1: Resources used in each hybrid MT method 
 
Method Bleu Method Bleu Method Bleu Method Bleu Method Bleu Method Bleu 
B 28.04 M2 39.45 M3 32.03 M4 34.86 M5 35.09 M6 40.48 
M1 39.72 M12 39.72 M13 32.85 M14 35.11 M15 35.52 M16 40.71 
Table 2: BLEU of each hybrid MT method 
120
M2, 804 post-edited sentences are used to train a 
new language model, without changing the 
translation model. In M3, paper abstracts in 
medical domain are used to derive a new lan-
guage model. M4, M5 and M6 are similar except 
that different sizes of corpora are used.  M4 and 
M5 use 1.1M and 2.1M sentences, respectively, 
while M6 uses 0.95M sentences chosen by using 
the selection strategy. M12-M16 are combina-
tions of M1 and M2-M6, respectively. Transla-
tion rules, translation model and language model 
are refined by using different resources. Total 
200 of the 1,004 post-edited sentences are se-
lected to tune the parameters of Moses in the 
advanced methods. 
Table 2 shows the BLEU of various MT 
methods. The BLEU of the MT system without 
employing simplification-translation-restoration 
methodology (Chen et al, 2012a) is 15.24. Ap-
parently, the method B, which employs the 
methodology, achieves the BLEU 28.04 and is 
much better than the original system. All the 
enhanced systems are significantly better than 
the baseline system B by t-test (p<0.05). Com-
paring M1 and M12-M16 with the correspond-
ing systems, we can find that introducing the 
mined patterns has positive effects. M1 is even 
much better than B. Although the number of the 
post-edited sentences is small, M2 and M12 
show such a resource has the strongest effects. 
The results of M3 and M13 depict that 30,000 
sentences selected from medical literature are 
not quite useful for medical record translation. 
Comparing M4 and M5, we can find larger 
pseudo corpus is useful.  M6 shows using the 
selected pseudo subset performs much better. 
Comparing the top 4 methods, the best method, 
M16, is significantly better than M12 and M1 
(p<0.05), but is not different from M6 signifi-
cantly (p=0.1662). 
We further analyze the translation results of 
the best methods M6 and M16 from two per-
spectives.  On the one hand, we show how the 
mined rules improve the translation. The follow-
ing list some examples for reference.  The un-
derlined parts are translated correctly by new 
mined patterns in M16. 
(1) Example: Stenting was done from distal 
IVC through left common iliac vein to ex-
ternal iliac vein. 
M6: ????? ? ? ?? ???? 
?? ? ????? ? ???? ? 
M16: ?? ????? ? ?? ???
? ?? ? ????? ? ???? ? 
(2) Example: We shifted the antibiotic to 
cefazolin. 
M6: ?? ? ??? ???? ? 
M16: ?? ? ??? ?? ? ???
? ? 
(3) Example: Enhancement of right side pleu-
ral, and mild pericardial effusion was not-
ed . 
M6: ?? ?? ? ?? ?? ? ? ?? 
? ??   ? ???? ? 
M16: ?? ? ? ?? ? ?? ???? 
? ??? ? 
On the other hand, we touch on which factors 
affect the translation performance of M16. Three 
factors including word ordering errors, word 
sense disambiguation errors and OOV (out-of-
vocabulary) errors are addressed as follows.  
The erroneous parts are underlined. 
(1) Ordering errors 
Example: Antibiotics were discontinued 
after 8 days of treatment. 
M16: ??? ?? ? 8? ? ?? ? 
Analysis: The correct translation result is 
?8 ? ? ?? ? ??? ????The 
current patterns are 2-5 grams, so that the 
longer patterns cannot be captured. 
(2) Word sense disambiguation errors 
Example: After tracheostomy, he was 
transferred to our ward for post operation 
care. 
M16: ????? ? ? ? ? ??? ?
? ?? ? ?? ?? ?? ? 
Analysis: The correct translation of ?post 
operation care? should be ??????.  
However, the 1,004 post-edited sentences 
are still not large enough to cover the pos-
sible patterns. Incremental update will in-
troduce more patterns and may decrease 
the number of translation errors. 
(3) OOV errors 
Example: Transcatheter intravenous uro-
kinase therapy was started on 1/11 for 24 
hours infusion. 
M16: transcatheter ?? ??? ? 1/11 
?? ?? ?? 24 ?? ?? ? 
Analysis: The word ?transcatheter? is an 
OOV. Its translation should be???".   
4 Conclusion 
This paper considers different types of resources 
in cross-domain MT adaptation. Several meth-
ods are proposed to integrate the mined transla-
121
tion rules, translation model and language model. 
The adaptation experiments show that the rules 
mined from the monolingual in-domain corpus 
are useful, and the effect of using the selected 
pseudo bilingual corpus is significant. 
Several issues such as word ordering errors, 
word sense disambiguation errors, and OOV 
errors still remain for further investigation in the 
future. 
 
Acknowledgments 
This work was partially supported by National 
Science Council (Taiwan) and Excellent Re-
search Projects of National Taiwan University 
under contracts NSC101-2221-E-002-195-MY3 
and 102R890858. We are very thankful to Na-
tional Taiwan University Hospital for providing 
NTUH the medical record dataset. 
References  
N. Bertoldi and M. Federico. 2009. Domain adapta-
tion for statistical machine translation with mono-
lingual resources. In Proceedings of the Fourth 
Workshop on Statistical Machine Translation, 
pages 182?189. 
H.B. Chen, H.H. Huang, H.H. Chen and C.T. Tan. 
2012a. A simplification-translation-restoration 
framework for cross-domain SMT applications. In 
Proceedings of COLING 2012, pages 545?560. 
H.B. Chen, H.H. Huang, J. Tjiu, C.Ti. Tan and H.H. 
Chen. 2012b. A statistical medical summary trans-
lation system. In Proceedings of 2012 ACM 
SIGHIT International Health Informatics Sympo-
sium, pages. 101-110. 
J. Civera and A. Juan. 2007. Domain adaptation in 
statistical machine translation with mixture model-
ing. In Proceedings of the Second Workshop on 
Statistical Machine Translation, pages 177?180. 
G. Foster and R. Kuhn. 2007. Mixture-model adapta-
tion for SMT. In Proceedings of the Second Work-
shop on Statistical Machine Translation, pages 
128?135. 
G. Foster, C. Goutte, and R. Kuhn. 2010. Discrimina-
tive instance weighting for domain adaptation in 
statistical machine translation. In Proceedings of 
EMNLP 2010, pages 451?459. 
S. Matsoukas, A.I. Rosti, and B. Zhang. 2009. Dis-
criminative corpus weight estimation for machine 
translation. In Proceedings of EMNLP 2009, pages 
708?717. 
H. Schwenk. 2008. Investigations on large-scale 
lightly-supervised training. In Proceedings of 
IWSLT 2008, pages 182?189. 
N. Ueffing, G. Haffari and A. Sarkar. 2007. Trans-
ductive learning for statistical machine translation. 
In Proceedings of the 45th Annual Meeting of the 
Association of Computational Linguistics, pages 
25?32, 
K. Woodsend and M. Lapata. 2011. Learning to sim-
plify sentences with quasi-synchronous grammar 
and integer programming. In Proceedings of 
EMNLP 2011, pages 409?420. 
S. Wubben and  A. van den Bosch, and  E. Krahmer. 
2012. Sentence simplification by monolingual ma-
chine translation. In Proceedings of ACL 2012, 
pages 1015?1024. 
B. Zhao, M. Eck, M. and S. Vogel. 2004. Language 
model adaptation for statistical machine translation 
via structured query models. In Proceedings of 
COLING 2004, pages 411?417. 
Z. Zhu, D. Bernhard, and I. Gurevych. 2010. A mon-
olingual tree-based translation model for sentence 
simplification. In Proceedings of COLING 2010, 
pages 1353?1361. 
122
