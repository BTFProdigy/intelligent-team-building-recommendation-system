Proceedings of the 12th European Workshop on Natural Language Generation, pages 82?89,
Athens, Greece, 30 ? 31 March 2009. c?2009 Association for Computational Linguistics
An Alignment-capable Microplanner for Natural Language Generation
Hendrik Buschmeier, Kirsten Bergmann and Stefan Kopp
Sociable Agents Group, CITEC, Bielefeld University
PO-Box 10 01 31, 33501 Bielefeld, Germany
{hbuschme, kbergman, skopp}@TechFak.Uni-Bielefeld.DE
Abstract
Alignment of interlocutors is a well known
psycholinguistic phenomenon of great rel-
evance for dialogue systems in general and
natural language generation in particular.
In this paper, we present the alignment-
capable microplanner SPUD prime. Us-
ing a priming-based model of interactive
alignment, it is flexible enough to model
the alignment behaviour of human speak-
ers to a high degree. This will allow for
further investigation of which parameters
are important to model alignment and how
the human?computer interaction changes
when the computer aligns to its users.
1 Introduction
A well known phenomenon in dialogue situations
is alignment of the interlocutors. An illustrative
example is given by Levelt and Kelter (1982), who
telephoned shops and either asked the question
?What time does your shop close?? or the ques-
tion ?At what time does your shop close??. The
answers were likely to mirror the form of the ques-
tion. When asked ?At what . . . ??, answers tended
to begin with the preposition ?at? (e.g., ?At five
o?clock.?). Conversely, when asked ?What . . . ??,
answers tended to begin without the preposition
(e.g., ?Five o?clock.?). Similar alignment phenom-
ena can be observed in many aspects of speech pro-
duction inter alia in syntactic and lexical choice.
Pickering and Garrod (2004) present the inter-
active alignment model bringing together all align-
ment phenomena of speech processing in dialogue.
According to this model, human language com-
prehension and production are greatly facilitated
by alignment of the interlocutors during conversa-
tion. The process of alignment is explained through
mutual priming of the interlocutors? linguistic rep-
resentations. Thus, it is automatic, efficient, and
non-conscious. A stronger claim of the authors is
that alignment ? in combination with routines and
a dialogue lexicon ? is a prerequisite for fluent
speech production in humans.
Alignment effects also occur in human?com-
puter interaction. Brennan (1991) and Branigan
et al (in press) present evidence that syntactic
structures and lexical items used by a computer
are subsequently adopted by users. For this reason,
alignment is an important concept for natural lan-
guage human?computer interaction in general, and
for dialogue systems with natural language gener-
ation in particular. Integrating ideas from the in-
teractive alignment model into the microplanning
component of natural language generation systems
should be beneficial for several reasons. First, mi-
croplanning may become more efficient since the
subsets of rules or lexical items in the dialogue
lexicon that have been used before can be prefer-
entially searched. Second, due to self-alignment,
the output of the system can become more con-
sistent and therefore easier to understand for the
user. Finally, mutual alignment of user and dia-
logue system might make the conversation itself
more natural and, presumably, cognitively more
lightweight for the user.
In this paper we present a computational model
for parts of the interactive alignment model that
are particularly important in the context of natural
language generation. We describe how this model
has been incorporated into the existing SPUD lite
system (Stone et al, 2003; Stone, 2002) to yield
the alignment-capable microplanner SPUD prime.
In Section 2 we describe previous approaches to
integrate alignment into natural language genera-
tion. In Sections 3 and 4, we present our priming-
based model of alignment and its implementation
in SPUD prime. In Section 5, we describe results
of an evaluation on a corpus of task-oriented dia-
logue, and in Section 6 we conclude our work and
describe possible future directions.
82
2 Related Work
Computational modelling is an important method-
ology for evaluating and testing psycholinguistic
theories. Thus, it is certainly not a new idea to
implement the interactive alignment model compu-
tationally. Indeed, a call for ?explicit computational
models? is made as early as in the open peer com-
mentary on Pickering and Garrod?s (2004) paper.
Brockmann et al (2005) and Isard et al (2006)
present a ?massive over-generation? approach to
modelling alignment and individuality in natural
language generation. Their system generates a
huge number of alternative sentences ? up to
3000 ? and evaluates each of these sentences with
a trigram model consisting of two parts: a default
language model computed from a large corpus and
a cache model which is calculated from the user?s
last utterance. The default language model is lin-
early interpolated with the cache model, whose in-
fluence on the resulting combined language model
is determined by a weighting factor ? ? [0,1] that
controls the amount of alignment the system exhib-
its.
Purver et al (2006) take a more formal approach.
They use an implementation of the Dynamic Syn-
tax formalism, which uses the same representations
and mechanisms for parsing as well as for genera-
tion of natural language, and extend it with a model
of context. In their model, context consists of two
distinct representations: a record of the semantic
trees generated and parsed so far and a record of
the transformation actions used for the construction
of these semantic trees. Re-use of semantic trees
and actions is used to model many dialogue phe-
nomena in Dynamic Syntax and can also explain
alignment. Thus, the authors declare alignment to
be a corollary of context re-use. In particular, re-use
of actions is assumed to have a considerable influ-
ence on alignment in natural language generation.
Instead of looking through the complete lexicon
each time a lexical item is chosen, this kind of lex-
ical search is only necessary if no action ? which
constructed the same meaning in the given con-
text before ? exists in the record. If such an action
exists, it can simply be re-used, which obviously
leads to alignment.
A completely different approach to alignment
in natural language generation is presented by de
Jong et al (2008), whose goal is to make a vir-
tual museum guide more believable by aligning
to the user?s level of politeness and formality. In
order to achieve this, the virtual guide analyses sev-
eral features of the user?s utterance and generates a
reply with the same level of politeness and formal-
ity. According to the authors, lexical and syntactic
alignment occur automatically because the lexical
items and syntactic constructions to choose from
are constrained by the linguistic style adopted.
Finally, Bateman (2006) advocates another pro-
posal according to which alignment in dialogue is
predictable for it is an inherently social activity.
Following the social-semiotic view of language,
Bateman suggests to model alignment as arising
from register and microregister. More specifically,
in his opinion priming of a linguistic representation
is comparable with pre-selecting a microregister
that must be considered when generating an utter-
ance in a particular social context.
The approaches presented above primarily focus
on the linguistic aspects of alignment in natural
language generation. The work of Brockmann et
al. (2005) and Isard et al (2006) concentrates on
the surface form of language, Bateman (2006) sees
alignment arising from social-semiotic aspects, and
Purver et al (2006) are primarily interested in fit-
ting alignment into a formal linguistic framework.
In this paper we adopt a more psycholinguistic and
cognitive stance on alignment. Pickering and Gar-
rod (2004) propose that low-level priming is the
basic mechanism underlying interactive alignment.
Here, we propose that computational modelling of
these priming mechanisms also opens up an inter-
esting and new perspective for alignment in natural
language generation.
3 A Priming-based Model of Alignment
We are interested here in those parts of the inter-
active alignment model that are most relevant for
microplanning in natural language generation and
it is out of our scope to model all the facets and
details of direct/repetition priming in the alignment
of linguistic representations. For instance, exact
timing effects are likely to be not even relevant as,
in an actual system, it does not matter how many
milliseconds faster the retrieval of a primed lexical
item is in contrast to the retrieval of an item that
is not primed. For this reason we adopt an ideal-
ised view, in which priming of linguistic structures
results from two basic activation mechanisms:
Temporary activation This kind of activation
should increase abruptly and then decrease
slowly over time until it reaches zero again.
83
Permanent activation This kind of activation
should increase by a certain quantity and then
maintain the new level.
These two mechanisms of priming are in ac-
cordance with empirical findings. Branigan et al
(1999) present evidence for rapid decay of activa-
tion of primed syntactic structures, whereas Bock
and Griffin (2000) report evidence for their long(er)
term activation. In any case, Reitter (2008) found
both types of priming in his analysis of several
corpora, with temporary activation being the more
important one. The assumption that both mechan-
isms play a role in dialogue is also supported by
Brennan and Clark (1996) whose terminology will
be followed in this paper: temporary priming will
be called ?recency of use effects? and permanent
priming will be called ?frequency of use effects?.
Reitter (2008) assumes the repetition probability
of primed syntactic structures to depend logarith-
mically on the distance between priming and usage.
Here, recency of use effects are modelled by a
more general exponential decay function, modified
to meet the needs for modelling activation decay of
primed structures:
ta(?r) = exp
(
?
?r?1
?
)
, (1)
?r ? N+; ? > 0; ta ? [0,1]
ta(?r) is the temporary activation value of a lin-
guistic structure depending on the distance ?r
between the current time T and the time r at which
the structure was primed. The slope of the function
is determined by the parameter ? . Additionally, the
function is shifted right in order to yield an activa-
tion value of 1 for ?r = 1. This shift is due to the
assumption of discrete time steps with a minimal
distance of 1. A plot of ta(?r) with different values
for ? is given in Figure 1a.
Using exponential decay to model temporary ac-
tivation appears to be a sensible choice that is often
used to model natural processes. The advantage of
this model of temporary activation lies in its flexib-
ility. By changing the slope parameter ? , different
empirical findings as well as variation among hu-
mans can be modelled easily.
Next, a mathematical model for frequency of use
effects is needed. To prevent that frequency effects
lead to an ever increasing activation value, a max-
imum activation level exists. This is also found in
Reitter?s (2008) corpus studies, which indicate that
 0
 0.2
 0.4
 0.6
 0.8
 1
 1  3  5  7  9  11  13  15
T
em
po
ra
ry
 A
ct
iv
at
io
n 
 ta
(?
r)
Recency Distance  ?r
(a)
? = 1
2
4
8
16
 0
 0.2
 0.4
 0.6
 0.8
 1
 1  3  5  7  9  11  13  15
Pe
rm
an
en
t A
ct
iv
at
io
n 
 p
a(
 f 
)
Frequency Counter f
(b)
? = 1
2
4
8
16
Figure 1: Plots of the mathematical functions that
model recency and frequency effects. Plot (a) dis-
plays temporary activation depending on the re-
cency of priming. Plot (b) shows permanent activ-
ation depending on the frequency count. Both are
shown for different values of the slope parameter
? respectively ? .
the frequency effect is inversely connected to the
recency effect. Here, we model recency effects with
a general exponential saturation function, modified
to meet the requirements for modelling permanent
activation of linguistic structures:
pa( f ) = 1? exp
(
?
f ?1
?
)
, (2)
f ? N+; ? > 0; pa ? [0,1]
The most important point to note here is that the
permanent activation value pa( f ) is not a function
of time but a function of the frequency-counter f
attached to each linguistic structure. Whenever a
structure is primed, its counter is increased by the
value of 1. Again, the slope of the function is de-
termined by the parameter ? and the function is
84
shifted right in order to get an activation value of
0 for f = 1. A plot of equation (2) with different
slope parameters is given in Figure 1b. Similar to
the advantages of the model of temporary activa-
tion, this model for frequency effects is very flex-
ible so that different empirical findings and human
individuality can be expressed easily.
Now, both priming models need to be combined
for a model of alignment. We opted for a weighted
linear combination of temporary and permanent
activation:
ca(?r, f ) = ? ? ta(?r)+(1??) ? pa( f ), (3)
0? ? ? 1; ca ? [0,1]
Different values of ? allow different forms of align-
ment. With a value of ? = 0.5 recency and fre-
quency effects are equally important, with a value
of ? = 1 alignment depends on recency only, and
with a value of ? = 0 alignment is governed solely
by frequency. Being able to adjust the influence
of the different sorts of priming on alignment is
crucial as it has not yet been empirically determ-
ined to what extent recency and frequency of use
affect alignment (in Section 5.2 we will exploit this
flexibility for matching empircial data).
In contrast to the models of alignment presented
in Section 2, the computational alignment model
presented here will not only consider alignment
between the interlocutors (interpersonal- or other-
alignment), but also alignment to oneself (intra-
personal- or self-alignment). Pickering et al (2003)
present results from three experiments which sug-
gest self-alignment to be even more important than
other-alignment. In our model, self-alignment is
accounted for with the same priming-based mech-
anisms. To this end, four counters are attached to
each linguistic structure:
? ?rs: recency of use by the system itself
? ?ro: recency of use by the interlocutor
? fs: frequency of use by the system itself
? fo: frequency of use by the interlocutor
The overall activation value of the structure is
a linear combination of the combined activation
value ca(?rs, fs) and the combined activation value
ca(?ro, fo) from equation (3):
act(?rs, fs,?ro, fo) =
? ? (? ? ca(?rs, fs)+(1??) ? ca(?ro, fo)),
(4)
0? ? ,? ? 1; act ? [0,1]
Again, by changing the factor ? , smooth interpola-
tion between pure self-alignment (? = 1) and pure
other-alignment (? = 0) is possible, which can ac-
count for different empirical findings or human
individual differences. Furthermore, the strength
of alignment is modelled with a scaling factor ? ,
which determines whether alignment is considered
during generation (? > 0) or not (? = 0).
4 The Alignment-capable Microplanner
SPUD prime
The previously described priming-based model of
alignment has been implemented by extending
the integrated microplanning system SPUD lite
(Stone, 2002). SPUD lite is a lightweight Prolog
re-implementation of the SPUD microplanning sys-
tem (Stone et al, 2003) based on the context-free
tree rewriting grammar formalism TAGLET. Not
only the microplanner itself, but also the linguistic
structures (the initial TAGLET trees) are represen-
ted as Prolog clauses.
SPUD lite carries out the different microplan-
ning tasks (lexical choice, syntactic choice, refer-
ring expression generation and aggregation) at once
by treating microplanning as a search problem. Dur-
ing generation it tries to find an utterance which is
in accordance with the constraints set by its input
(a grammar, a knowledge base and a query). This is
done by searching the search space spanned by the
linguistic grammar rules and the knowledge base
until a goal state is found. Non-goal search states
are preliminary utterances that are extended by one
linguistic structure in each step until a syntactically
complete utterance is found which conveys all the
specified communicative goals. Since this search
space is large even for relatively small grammars,
a heuristic greedy search strategy is utilised.
Our alignment-capable microplanner SPUD
prime extends SPUD lite in several ways. First, we
altered the predicate for the initial TAGLET trees
by adding a unique identifier ID as well as counters
for self/other-recency/frequency values (rs, fs, ro
and fo; see Section 3). The activation value of an
initial tree is then calculated with equation (4).
Furthermore, we have created a mechanism that
enables SPUD lite to change the recency and fre-
quency information attached to the initial trees on-
line during generation. This is done in three steps
with the help of Prolog?s meta-programming cap-
abilities: Firstly, the clause of a tree is retrieved
85
from the knowledge base. Secondly, it is retrac-
ted from the knowledge base. Finally, the clause
is (re-)asserted in the knowledge base with up-
dated recency and frequency information. As a
welcome side effect of this procedure, primed ini-
tial trees are moved to the top of the knowledge
base and ? since Prolog evaluates clauses and facts
in the order of their appearance in the knowledge
base ? they can be accessed earlier than unprimed
initial trees or initial trees that were primed longer
ago. Thus, in SPUD prime recency of priming dir-
ectly influences the access of linguistic structures.
Most importantly, the activation values of the ini-
tial trees are considered during generation. Thus, in
addition to the evaluation measures used by SPUD
lite?s heuristic state evaluation function, the mean
activation value
act(S) =
?Ni=1 actti(?rsti , fsti ,?roti , foti )
N
of the N initial trees {t1, . . . , tN} of a given search
state S is taken into account as a further evaluation
measure. Hence, when SPUD prime evaluates (oth-
erwise equal) successor search states, the one with
the highest mean activation value is chosen as the
next current state.
5 Evaluation
In order to find out whether our priming-based
alignment model and its implementation work as
intended, we evaluated SPUD prime on a corpus
that was collected in an experiment designed to
investigate the alignment behaviour of humans in
a controlled fashion (Wei? et al, 2008). The part
of the corpus that we used consists of eight recor-
ded and transcribed dialogues between two inter-
locutors that play the ?Jigsaw Map Game?, a task
in which different objects have to be placed cor-
rectly on a table. Speakers take turns in explaining
each other where to place the next object in re-
lation to the objects that are already on the table.
Each speaker has to learn a set of name?object rela-
tions before the game, such that both use the same
names for all but three objects. Due to this precon-
dition, both speakers use the same lexical referring
expressions for most objects and the speaker?s lex-
ical alignment behaviour for the differently named
objects can be observed easily.
In our evaluation, we concentrate on the gener-
ation of nouns by simulating the uses of the three
differently learned nouns in the eight dialogues
from the perspective of all sixteen interlocutors.
In each test, SPUD prime plays the role of one
of the speakers talking to a simulated interlocutor
who behaves exactly as in the real experiment.
With this test setup we examined, first, how well
SPUD prime can model the alignment behaviour
of a real speaker in a real dialogue context and,
second, whether our model is flexible enough to
consistently emulate different speakers with differ-
ent alignment behaviour.
In order to find the best model (i.e., the best
parameter set {?,? ,?,?}) for each speaker, we
simulated all tests with all parameter combinations
and counted the number of mismatches between
our model?s choice and the real speaker?s choice.
To make this exhaustive search possible, we limit
the set of values for the parameters ? and ? to
{1,2,4,6,8,10,14,18,24,30} and the set of values
for the parameters ? and ? to {0,0.1,0.2, ...,1},
resulting in a total of 112?102 = 12100 different
parameter sets. Since we want to investigate align-
ment, ? is constantly set to 1.
5.1 An Illustrative Example
To illustrate our evaluation method, we first present
and discuss the simulation of one particular dia-
logue (from the Jigsaw Map Game corpus) from
the perspective of participant (A). Before the exper-
iment started, both interlocutors learned the name?
object relations ?Raute? (rhombus), ?Ring? (ring),
?Schraube? (bolt) and ?Wu?rfel? (cube), additionally
participant (A) learned ?Spielfigur? (token), ?Ball?
(sphere) and ?Block? (cuboid) and participant (B)
learned ?Ma?nnchen? (token), ?Kugel? (sphere) and
?Klotz? (cuboid). In our simulation, we focus on the
use of the differently learned names (the targets)
and not on the other names (non-targets). Table 1
shows the sequence of target nouns as they oc-
curred in the real dialogue (non-targets omitted).
For each parameter set {?,? ,?,?} the dialogue
is simulated in the following way:
? When participant (A) used a referring non-
target noun in the dialogue, self-priming of
the corresponding rule(s) in SPUD prime?s
knowledge base is simulated (i.e., the recency
and frequency counters are increased).
? When participant (A) used a referring target
noun in the dialogue, SPUD prime is queried
to generate a noun for the target object. Then
it is noted whether the noun actually generated
86
B: der Klotz 14 A: der Klotz
1 A: die Spielfigur 15 A: die Kugel
2 A: der Klotz 16 A: der Klotz
B: das Ma?nnchen B: der Klotz
B: der Klotz B: die Kugel
3 A: die Spielfigur B: der Klotz
B: das Ma?nnchen 17 A: der Klotz
4 A: das Ma?nnchen B: das Ma?nnchen
5 A: das Ma?nnchen B: der Klotz
6 A: das Ma?nnchen 18 A: das Ma?nnchen
7 A: das Ma?nnchen 19 A: der Klotz
8 A: das Ma?nnchen B: das Ma?nnchen
B: das Ma?nnchen 20 A: der Ball
9 A: das Ma?nnchen 21 A: das Ma?nnchen
10 A: der Ball B: der Ball
B: der Ball B: das Ma?nnchen
11 A: der Ball 22 A: die Kugel
12 A: der Ball 23 A: der Ball
B: die Kugel B: der Klotz
B: das Ma?nnchen 24 A: der Ball
13 A: der Ball B: der Klotz
B: die Kugel 25 A: der Klotz
Table 1: Sequence of referring target nouns used by
participants (A) and (B) in our example dialogue.
is the noun used in the actual dialogue (match)
or not (mismatch).
? When participant (B) used a referring noun
(target or non-target), priming of the corres-
ponding rule(s) in SPUD prime?s knowledge
base is simulated.
The evaluation measure for a specific parameter
set is the number of mismatches it produces when
simulating a dialogue. Thus the parameter set (or
rather sets) which produce the least number of mis-
matches are the ones that best model the particular
speaker under consideration. For participant (A)
of our example dialogue the distribution of para-
meter sets p producing m mismatches is shown in
Table 2. Four parameter sets produce only two mis-
matches (in phrase 15 and 22; cf. Table 1) and thus
our priming-based alignment model can account
for 92% of the target nouns produced by speaker
(A). However, it must be noted that these two mis-
matches occur at points in the dialogue where the
alignment behaviour of (A) is not straightforward.
At target noun 15, both interlocutors have already
used the name ?Ball? and then both switch to ?Ku-
gel?. The mismatch at target 22 is a special case: (A)
used ?Kugel? and immediately corrected himself to
?Ball?, the name he learned prior to the experiment.
It seems as if the task instruction, to use the learned
nouns, suddenly became prevalent.
m 0 1 2 3 4 5
# p 0 0 4 833 3777 2248
m 6 7 8 9 10 . . .
# p 3204 1105 478 148 294 0
Table 2: Number of parameter sets p leading to m
mismatches for participant (A) in dialogue 7.
5.2 Simulation Results
To evaluate our alignment-capable microplanner,
we simulated the noun production for each of the
interlocutors from the experiment. One dialogue
has been excluded from the data analysis as the
dialogue partners used nouns that none of them had
learned in the priming phase. For each of the re-
maining 14 interlocutors we varied the parameters
? , ? , ? and ? as described above to identify those
parameter set(s) which result in the least number
of mismatches.
Each interlocutor produced between 18 and 32
target nouns (N=14, M=23.071, SD=3.936). Our
simulation runs contain between 0 and 19 mis-
matches overall (N=169400, M=6.35, SD=3.398).
The minimal number of mismatches for each
speaker simulation ranges between 0 and 6 (N=14,
M=2.286, SD=1.684). That is, our model can sim-
ulate a mean of 89.9% of all target nouns (N=14,
M=.899, Min=.667, Max=1.000, SD=.082), which
is an improvement of 24.6% on the baseline con-
dition (alignment switched off), where 65.3% of
the target nouns are generated correctly (N=14,
M=.653, Min=.360, Max=1.000, SD=.071). As
already illustrated in Section 5.1, mismatches typic-
ally occur at points in the dialogue where the align-
ment behaviour of the interlocutor is not straight-
forward.
As displayed in Table 3 the parameter assign-
ments resulting in least mismatches differ consid-
erably from speaker to speaker. However, there are
some remarkable trends to be observed in the data.
As concerns the parameter ? , which determines
the combination of self- and other-alignment, the
majority of values are in the upper range of the
interval [0,1]. For 8 of 14 speakers the mean is
above 0.7 with relatively low standard deviations.
Only for one speaker (P13) the mean ? is below
0.3. Thus, the parameter values indicate a consider-
able tendency toward self-alignment in contrast to
other-alignment.
For the parameter ? that interpolates between
recency and frequency effects of priming, the res-
87
? ? ? ?
m # p M SD M SD M SD M SD
P13 2 4 3.0 1.155 19.5 9.14 .1 .0 .3 .0
P14 1 72 5.53 1.52 14.32 9.61 .819 .040 .901 .108
P17 1 200 1.66 .823 12.94 9.529 .353 .169 .955 .069
P18 3 2445 15.37 8.758 10.98 9.76 .597 .211 .706 .236
P19 0 4321 11.81 9.492 11.01 8.929 .824 .148 .387 .291
P20 2 8 1.0 .0 15.75 9.285 .737 .052 .388 .146
P23 6 987 6.85 6.681 12.08 9.354 .331 .374 .4 .33
P24 3 256 12.95 9.703 13.63 8.937 .537 .201 .468 .298
P39 5 1 1.0 .0 2.0 .0 .9 .0 .8 .0
P40 0 3504 12.08 9.33 10.30 8.753 .843 .147 .343 .282
P41 2 609 11.37 8.475 15.34 8.921 .770 .106 .655 .213
P42 3 30 6.0 1.486 17.53 9.016 .783 .059 .760 .122
P47 2 326 13.75 7.794 13.53 9.508 .772 .095 .816 .166
P48 2 2478 12.87 9.545 10.74 8.538 .764 .175 .166 .148
Table 3: Mean parameter values for those simulation runs which result in a minimal number of mismatches
for each speaker.
ults are less revealing. For two speaker simulations
(P13 and P48) the mean ? is 0.3 or lower, for an-
other four speaker simulations the mean ? is above
0.7. That is, our model produces good matching be-
haviour in adopting different alignment strategies,
depending either primarily on frequency or recency,
respectively. All other simulations, however, are
characterised by a mean ? in the medium range
along with a relatively high standard deviation.
6 Conclusion
In this paper, we introduced a priming-based model
of alignment which focusses more on the psycho-
linguistic aspects of interactive alignment and mod-
els recency and frequency of use effects ? as pro-
posed by Reitter (2008) and Brennan and Clark
(1996) ? as well as the difference between intraper-
sonal and interpersonal alignment (Pickering et al,
2003; Pickering and Garrod, 2004). The presented
model is fully parameterisable and can account for
different empirical findings and ?personalities?. It
has been implemented in the SPUD prime micro-
planner which activates linguistic rules by changing
its knowledge base on-line and considers the ac-
tivation values of those rules used in constructing
the current utterance by using their mean activation
value as an additional feature in its state evaluation
function.
We evaluated our alignment model and its im-
plementation in SPUD prime on a corpus of task-
oriented dialogue collected in an experimental
setup especially designed for alignment research.
The results of this evaluation show that our priming-
based model of alignment is flexible enough to sim-
ulate the alignment behaviour of different human
speakers (generating target nouns) in the experi-
mental setting. It should be noted, however, that
our model tries to give a purely mechanistic ex-
planation of lexical and syntactic choice and that
it, therefore, cannot explain alignment phenomena
that are due to social factors (e.g., politeness, rela-
tionship, etc.), audience design or cases, in which a
speaker consciously decides whether to align or not
(e.g., whether to use a word or its synonym). While
the evaluation has shown that our model can repro-
duce human alignment behaviour to a high degree,
it remains to be investigated which influence each
parameter exerts and how exactly the parameters
vary across individual speakers.
Nevertheless, the development of the alignment-
capable microplanner is only one step in the dir-
ection of an intuitive natural language human?
computer interaction system. In order to reach this
goal, the next step is to combine SPUD prime with
a natural language understanding system, which
should ideally work with the same linguistic rep-
resentations so that the linguistic structures used
by the interlocutor could be primed automatically.
This work is underway.
Furthermore, user studies should be carried
out in order to evaluate SPUD prime in a more
sophisticated way. Branigan et al (in press) found
that human?computer alignment was even stronger
than human?human alignment. But how would
the alignment behaviour of human interlocutors
change if the computer they are speaking to also
aligns to them? Further, would integration of an
alignment-capable dialogue system into a computer
interface make the interaction more natural? And
would an embodied conversational agent appear
88
more resonant and more sociable (Kopp, 2008) if
it aligned to users during conversation? The work
presented here provides a starting point for the
investigation of these questions.
Acknowledgements ? This research is supported
by the Deutsche Forschungsgemeinschaft (DFG) in
the Center of Excellence in ?Cognitive Interaction
Technology? (CITEC) as well as in the Collabor-
ative Research Center 673 ?Alignment in Commu-
nication?. We also thank Petra Wei? for making
the ?Jigsaw Map Game? corpus available and three
anonymous reviewers for their helpful comments.
References
John A. Bateman. 2006. A social-semiotic view of
interactive alignment and its computational instanti-
ation: A brief position statement and proposal. In
Kerstin Fischer, editor, How People Talk to Com-
puters, Robots and Other Artificial Communication
Partners, SFB/TR 8 Report No. 010-09/2006, pages
157?170, Bremen, Germany.
J. Kathryn Bock and Zenzi M. Griffin. 2000. The per-
sistence of structural priming: Transient activation
or implicit learning? Journal of Experimental Psy-
chology: General, 129(2):177?192.
Holly P. Branigan, Martin J. Pickering, and Alexan-
dra A. Cleland. 1999. Syntactic priming in written
production: Evidence for rapid decay. Psychonomic
Bulletin & Review, 6(4):635?640.
Holly P. Branigan, Martin J. Pickering, Jamie Pearson,
and Janet F. McLean. in press. Linguistic alignment
between people and computers. Journal of Pragmat-
ics.
Susan E. Brennan and Herbert H. Clark. 1996.
Conceptual pacts and lexical choice in conversa-
tion. Journal of Experimental Psychology: Learn-
ing, Memory, and Cognition, 22(6):1482?1493.
Susan E. Brennan. 1991. Conversation with and
through computers. User Modeling and User-Adapt-
ed Interaction, 1(1):67?86.
Carsten Brockmann, Amy Isard, Jon Oberlander, and
Michael White. 2005. Modelling alignment for af-
fective dialogue. In Proc. of the Workshop on Adapt-
ing the Interaction Style to Affective Factors at the
10th Int. Conf. on User Modeling.
Markus A. de Jong, Marie?t Theune, and Dennis Hofs.
2008. Politeness and alignment in dialogues with
a virtual guide. In Proc. of the 7th Int. Conf. on
Autonomous Agents and Multiagent Systems, pages
207?214.
Amy Isard, Carsten Brockmann, and Jon Oberlander.
2006. Individuality and alignment in generated dia-
logues. In Proc. of the 4th Int. Natural Language
Generation Conf., pages 25?32.
Stefan Kopp. 2008. From communicators to reson-
ators ? Making embodied conversational agents so-
ciable. In Proc. of the Speech and Face to Face
Communication Workshop in Memory of Christian
Beno??t, pages 34?36.
Willem J. M. Levelt and Stephanie Kelter. 1982. Sur-
face form and memory in question answering. Cog-
nitive Psychology, 14(1):78?106.
Martin J. Pickering and Simon Garrod. 2004. Toward
a mechanistic psychology of dialogue. Behavioral
and Brain Sciences, 27(2):169?226.
Martin J. Pickering, Holly P. Branigan, and Janet F.
McLean. 2003. Dialogue structure and the activ-
ation of syntactic information. In Proc. of the 9th
Annual Conf. on Architectures and Mechanisms for
Language Processing, page 126.
Matthew Purver, Ronnie Cann, and Ruth Kempson.
2006. Grammars as parsers: Meeting the dialogue
challenge. Research on Language and Computation,
4(2?3):289?326.
David Reitter. 2008. Context Effects in Language Pro-
duction: Models of Syntactic Priming in Dialogue
Corpora. Ph.D. thesis, University of Edinburgh.
Matthew Stone, Christine Doran, Bonnie Webber, To-
nia Bleam, and Martha Palmer. 2003. Microplan-
ning with communicative intentions: The SPUD sys-
tem. Computational Intelligence, 19(4):311?381.
Matthew Stone. 2002. Lexicalized grammar 101. In
Proc. of the ACL-02 Workshop on Effective Tools
and Methodologies for Teaching Natural Language
Processing and Computational Linguistics, pages
77?84.
Petra Wei?, Thies Pfeiffer, Gesche Schaffranietz, and
Gert Rickheit. 2008. Coordination in dialog: Align-
ment of object naming in the Jigsaw Map Game. In
Proc. of the 8th Annual Conf. of the Cognitive Sci-
ence Society of Germany, pages 4?20.
89
Proceedings of the SIGDIAL 2011: the 12th Annual Meeting of the Special Interest Group on Discourse and Dialogue, pages 88?97,
Portland, Oregon, June 17-18, 2011. c?2011 Association for Computational Linguistics
 
Regulating Dialogue with Gestures?Towards an Empirically Grounded 
Simulation with Conversational Agents 
 
Kirsten Bergmann1,2 Hannes Rieser1 Stefan Kopp1,2
 
1 Collaborative Research Center 673 ?Alignment in Communication?, Bielefeld University 
2 Center of Excellence ?Cognitive Interaction Technology?(CITEC), Bielefeld University 
   
{kbergman,skopp}@TechFak.Uni-Bielefeld.DE 
hannes.rieser@Uni-Bielefeld.DE 
 
 
 
Abstract 
Although not very well investigated, a crucial as-
pect of gesture use in dialogues is to regulate the 
organisation of the interaction. People use gestures 
decisively, for example to indicate that they want 
someone to take the turn, to 'brush away' what 
someone else said, or to acknowledge others' con-
tributions. We present first insights from a corpus-
based investigation of how gestures are used to 
regulate dialogue, and we provide first results from 
an account to capture these phenomena in agent-
based communication simulations. By advancing a 
model for autonomous gesture generation to also 
cover gesture interpretation, this account enables a 
full gesture turn exchange cycle of generation, un-
derstanding and acceptance/generation in virtual 
conversational agents. 
1 Motivation 
Research on gestures must combine empirical, 
theoretical and simulation methods to investigate 
form, content and function of gestures in relation 
to speech. Our work is based on a corpus of multi-
modal data, the Bielefeld Speech and Gesture 
Alignment corpus of route-description dialogues 
(SAGA corpus, L?cking et al 2010). The point of 
departure of our research has been work on iconic 
and deictic gestures over many years. In this paper 
we focus on a not very well investigated function 
of gestures which we have repeatedly observed in 
this corpus, namely, the regulation of dialogue.  
Most of current gesture research is oriented to-
wards the semiotics of a Peircean tradition as can  
for instance be seen from McNeill?s ?Kendon?s 
continuum? (McNeill 1992, p. 37). As a conse-
quence of this Peircian orientation, gestures have 
been viewed as single signs interfacing with 
speech. Going beyond the integration of in-
put/output modalities in single speech-gesture 
compositions (Johnston and Bangalore, 2005), lit-
tle effort has been spent on the investigation of 
sequences of gestures and speech-gesture composi-
tion both within and across speakers (Hahn and 
Rieser 2010, Rieser 2010). Furthermore, research 
of gesture meaning was restricted to the contribu-
tion of gesture content to propositional content. An 
exception to this research line has been the work of 
Bavelas et al (1992, 1995). It is characterised by 
two features, a functional perspective on gesture in 
opposition to purely classificatory and typological 
ones and an interest to systematically investigate 
the role of gesture in interaction. In particular, 
Bavelas et al (1992) proposed a distinction be-
tween ?topic gestures? and ?interactive gestures?: 
Topic gestures depict semantic information di-
rectly related to the topic of discourse, while inter-
active gestures refer to some aspect of the process 
of conversing with another person. Interactive ges-
tures include delivery gestures (e.g. marking in-
formation status as new, shared, digression), citing 
gestures (acknowledging others? prior contribu-
tions), seeking gestures (seeking agreement, or 
help in finding a word), and turn coordination ges-
 
88
 tures (e.g. taking or giving the turn). Gill et al 
(1999) noted similar functions of gesture use, add-
ing body movements to the repertoire of pragmatic 
acts used in dialogue act theory (e.g. turn-taking, 
grounding, acknowledgements).  
We aim to find out how gestures are related to and 
help regulate the structure of dialogue. We will call 
these gestures `discourse gestures?. Relevant re-
search questions in this respect are the following: 
How can gesture support next speaker selection if 
this follows regular turn distribution mechanisms 
such as current speaker selects next? From the dia-
logues in SAGA we know that averting next 
speaker?s self-selection is of similar importance as 
handing over the floor to the next speaker. So, how 
can averting self-selection of other be accom-
plished gesturally? A still different problem is how 
gesture is utilised to establish an epistemically 
transparent, reliable common ground, say a tight 
world of mutual belief. A precondition for that is 
how gesture can help to indicate a gesturer?s stance 
to the information he provides. Natural language 
has words to indicate degrees of confidence in in-
formation such as probably, seemingly, approxi-
mately, perhaps, believe, know, guess etc. Can ges-
tures acquire this function as well?  
All these issues can be synopsised as follows: How 
can gestures?apart from their manifest contribu-
tion to propositional content?be used to push the 
dialogue machinery forward? In our research, ges-
ture simulation and theory of speech-gesture inte-
gration are developed in tandem. Up to now, both  
have been tied to occurrences of single gestures 
and their embedding in dialogue acts. In this paper, 
we present first steps along both methodological 
strands to explore the use and function of gesture 
in dialogue. We start with an empirical perspective 
on discourse gestures in section 2. In section 3 we 
briefly describe our gesture simulation model 
which so far simulates gesture use employing the 
virtual agent MAX independent of discourse struc-
tures. Section 4 analyses a corpus example of a 
minimal discourse which is regulated mainly by 
gestures of the two interactants. This provides the 
basis for our proposed extension of the gesture 
generation approach to capture the discourse func-
tion of gestures as described in section 5. This ex-
tension will encompass a novel approach to em-
ploy the very generation model used for gesture 
production, and hence all the heuristic gesture 
knowledge it captures, also for gesture interpreta-
tion in dialogue. Section 6 discusses the difference 
between pure interactive gestures and discourse 
gestures and proposes further steps that need to be 
taken to elucidate how gestures are used as a vehi-
cle for regulating dialogue.  
2 Empirical Work on Discourse Gestures 
In looking for discourse gestures we started from 
the rated annotation of 6000 gestures in the SAGA 
corpus. We managed to annotate and rate about 
5000 of them according to traditional criteria using 
practices and fine-grained gesture morphology like 
hand-shape and wrist-movement. About 1000 ges-
tures could not be easily subsumed under the tradi-
tional gesture types (iconics, deictics, metaphorics, 
beats). Furthermore, they were observed to corre-
late with discourse properties such as current 
speaker?s producing his contribution or non-
regular interruption by other speaker.  
For purposes of the classification of the remaining 
1000 gestures we established the following func-
tional working definition: `Discourse gestures? are 
gestures tied up with properties or functions of 
agents? contributions in dialogue such as success-
fully producing current turn, establishing coher-
ence across different speakers? turns by gestural 
reference or indicating who will be next speaker.  
What did we use for dialogue structure? Being fa-
miliar with dialogue models such as SDRT (Asher 
and Lascarides, 2003), PTT (Poesio and Traum, 
1997), and KoS (Ginzburg, 2011) we soon found 
that these were too restricted to serve descriptive 
purposes. So we oriented our ?classification of dia-
logue gesture enterprise? on the well known turn 
taking organisation model of Sacks et al (1974) 
and Levinson?s (1983) discussion of it. However, it 
soon turned out that even these approaches were 
too normative for the SAGA data: This is due to 
the fact that dialogue participants develop enor-
mous creativity in establishing new rules of con-
tent production and of addressing violations of 
prima facie rules.  
Rules of turn-taking, for example, are not hard and 
fast rules, they can be skirted if the need arises, 
albeit there is a convention that this has to be ac-
knowledged and negotiated. A very clear example 
of an allowed interruption of an on-going produc-
tion is a quickly inserted clarification request serv-
ing the communicative goals of current speaker 
and the aims of the dialogue in general. Another 
 
89
 problem with the Sacks et al model consists in the 
following fact: Since its origination many dialogue 
regularities have been discovered which cannot be 
easily founded on a phenomenological or observa-
tional stratum which is essentially semantics-free. 
This can for example be seen from the develop-
ment of the notion of grounding and common 
ground as originally discussed by Stalnaker (1978), 
Clark (1996) and others. Nevertheless, grounding 
(roughly, coming to agree on the meaning of what 
has been said (see e.g. Traum, 1999; Roque and 
Traum, 2008;  Ginzburg 2011, ch. 4.2 for the op-
tions available) generates verbal structure and ver-
bal structure interfaces with gesture. Other exam-
ples in this class are acknowledgements or accepts 
discussed in more detail below. 
How did we decide on which distinctions of ges-
ture annotation have to be used for characterising 
discourse gestures? In other words, how did we 
conceive of the map between gestures of a certain 
sort and discourse structures? First of all we ob-
served that two types of discourse gestures emerge 
from the SAGA data. Some of them come with 
their own global shape and are close to emblems, 
(i.e. conveyors of stable meaning like the victory 
sign). This is true for example of the ?brush aside 
or brush away? gesture shown in Figure 1 (left), 
indicating a gesturer?s assessment of the down-
rated relevance of information, actions or situa-
tions. Discourse gestures of the second class ex-
ploit the means of, for instance, referring gestures 
or iconic gestures. An example of an iconic gesture 
in this role will be discussed to some extent in sec-
tion 4. Its simulation will be described in sections 3 
and 5.  
Here we explain the phenomenon with respect to 
referring pointing gestures which are easier to fig-
ure out (see Figure 1 (right)). Their usage as under 
focus here is not tied to the information under dis-
cussion but to objects in the immediate discourse 
situation, preferably to the participants of the dia-
logue. These uses have a Gricean flavour in the 
following way: Only considerations of relevance 
and co-occurrence with a turn transition relevance 
place together indicate that prima facie not general 
reference is at stake but indication of next speaker 
role. It wouldn?t make sense to point to the other 
person singling her or him out by indexing, be-
cause her or his identity is clear and well estab-
lished through the on-going interaction. Thus we 
see that a gestural device associated with estab-
lished morphological features, pointing, acquires a 
new function, namely indicating the role of next-
speaker. 
Figure 1: Examples of discourse gestures: the brush-away 
gesture (left) and situated pointing to the upper part of the 
interlocutor?s torso (right) used for next speaker selection in 
a ?Gricean? sense (see text for explanation). 
Now both classes of gestures, ?brush away? used 
to indicate informational or other non-relevance 
and pointing, indicating the role of being next 
speaker exploit the motor equipment of the hands. 
For this reason, annotation of discourse gestures 
can safely be based on the classification schemas 
we have developed for practices like indexing, 
shaping or modelling and for the fine-grained mo-
tor behaviour of the hands as exhibited by palm 
orientation, back-of-hand trajectory etc. In work by 
Hahn & Rieser (2009-2011) the following broad 
classes of discourse gestures were established. We 
briefly comment upon these classes of gestures 
found in the SAGA corpus relevant for dialogue 
structure and interaction:  
? Managing of own turn: A speaker may in-
dicate how successful he is in editing out his 
current production.  
? Mechanisms of next-speaker selection as 
proposed in classical CA research, for in-
stance, pointing to the other?s torso is often 
used as a means to indicate next speaker.  
? In grounding acts and feed-back especially 
iconic gestures are used to convey proposi-
tional content. 
? Clarification requests to work on contribu-
tions: An addressee may indicate the need 
for a quick interruption using a pointing to 
demand a clarification. In contrast, a current 
speaker can ward off the addressee?s incipi-
ent interruption using a palm-up gesture di-
 
90
 rected against the intruder thus setting up a 
?fence?.  
? Evidentials for establishing a confidence 
leve: There are fairly characteristic gestures 
indicating the confidence a speaker has in 
the information he is able to convey. 
? Handling of non-canonical moves by dis-
course participants: Interaction sequences 
consisting of attempts by other speaker to in-
terrupt and to thwart this intention by current 
speaker or to give way to it show how dis-
course participants handle non-canonical 
moves.  
? Assessment of relevance by discourse par-
ticipants: Speakers provide an assessment 
of which information is central and which 
one they want to consider as subsidiary. 
? An indication of topical information with 
respect to time, place or objects is fre-
quently given by pointing or by ?placing ob-
jects? into the gesture space.  
 
We know that this list is open and could, more-
over, depend on the corpus. In this paper the focus 
will be on grounding acts and feedback (see sec-
tions 3-5). The reason is that this way we can pro-
vide an extension of existing work on the simula-
tion of gesture production in a fairly direct manner. 
 
3 Simulating Gesture Use: The Genera-
tion Perspective 
Our starting point to simulate gestural behavior in 
dialogue is a gesture generation system which is 
able to simulate speaker-specific use of iconic ges-
tures given (1) a communicative intention, (2) dis-
course contextual information, and (3) an imagistic 
representation of the object to be described. Our 
approach is based on empirical evidence that 
iconic gesture production in humans is influenced 
by several factors. Apparently, iconic gestures 
communicate through iconicity, that is their physi-
cal form depicts object features such as shape or 
spatial properties. Recent findings indicate that a 
gesture?s form is also influenced by a number of 
contextual constraints such as information struc-
ture (see for instance Cassell and Prevost, 1996), or 
the use of more general gestural representation 
techniques such as shaping or drawing is decisive. 
In addition, inter-subjective differences in gestur-
ing are pertinent. There is, for example, wide vari-
ability in how much individuals gesture when they 
speak. Similarly, inter-subjective differences are 
found in preferences for particular representation 
techniques or low-level morphological features 
such as handshape or handedness (Bergmann & 
Figure 2: Schema of a gesture generation network in which 
gesture production choices are considered either 
probabilistically (chance nodes drawn as ovals) or rule-based 
(decision nodes drawn as rectangles). Each choice is 
depending on a number of contextual variables. The links are 
either learned from speaker-specific corpus data (dotted lines) 
or defined in a set of if-then rules (solid lines). 
 
 
Kopp, 2009).  
To meet the challenge of considering general and 
individual patterns in gesture use, we have pro-
posed GNetIc, a gesture net specialised for iconic 
gestures (Bergmann & Kopp, 2009a), in which we 
model the process of gesture formulation with 
Bayesian decision networks (BDNs) that supple-
ment standard Bayesian networks by decision 
nodes. This formalism provides a representation of 
a finite sequential decision problem, combining 
probabilistic and rule-based decision-making. Each 
decision to be made in the formation of an iconic 
gesture (e.g., whether or not to gesture at all or 
which representation technique to use) is repre-
sented in the network either as a decision node 
(rule-based) or as a chance node with a specific 
probability distribution. Factors which contribute 
to these choices (e.g., visuo-spatial referent fea-
tures) are taken as input to the model (see Figure 2) 
The structure of the network as well as local condi-
tional probability tables are learned from the 
SAGA corpus by means of automated machine 
 
 
91
 learning techniques and supplemented with rule-
based decision making. Individual as well as gen-
eral networks are learned from the SAGA corpus 
by means of automated machine learning tech-
niques and supplemented with rule-based decision 
making. So far, three different factors have been 
incorporated into this model: discourse context, the 
previously performed gesture, and features of the 
referent. The latter are extracted from a hierarchi-
cal representation called Imagistic Description 
Trees (IDT), which is designed to cover all deci-
sive visuo-spatial features of objects one finds in 
iconic gestures (Sowa & Wachsmuth, 2009). Each 
node in an IDT contains an imagistic description  
which holds a schema representing the shape of an 
object or object part. Features extracted from this 
representation in order to capture the main charac-
teristics of a gesture?s referent are whether an ob-
ject can be decomposed into detailed subparts 
(whole-part relations), whether it has any symmet-
rical axes, its main axis, its position in the VR 
stimulus, and its shape properties extracted on the 
 are not only present in the 
ly in terms of likeability, competence and 
communicative intent 
 describe the landmark townhall with respect to 
cular speaker) 
sulting in a posterior distribution of probabilities 
nique is decided to be ?drawing?, to be 
basis of so called multimodal concepts (see Berg-
mann & Kopp, 2008). 
Analyzing the GNetIc modelling results enabled us 
to gain novel insights into the production process 
of iconic gestures: the resulting networks for indi-
vidual speakers differ in their structure and in their 
conditional probability distributions, revealing that  
individual differences
overt gestures, but also in the production process 
they originate from.  
The GNetIc model has been extensively evaluated. 
First, in a prediction-based evaluation, the auto-
matically generated gestures were compared 
against their empirically observed counterparts, 
which yielded very promising results (Bergmann & 
Kopp, 2010). Second, we evaluated the GNetIc 
models in a perception-based evaluation study with 
human addressees. Results showed that GNetIc-
generated gestures actually helped to increase the 
perceived quality of object descriptions given by 
MAX. Moreover, gesturing behaviour generated 
with individual speaker networks was rated more 
positive
human-likeness (Bergmann, Kopp & Eyssel, 
2010). 
GNetIc gesture formulation has been embedded in 
a larger production architecture for speech and ges-
ture production. This architecture comprises mod-
ules that carry out content planning, formulation, 
and realisation for speech and gesture separately, 
but in close and systematic coordination (Berg-
mann & Kopp, 2009). To illustrate gesture genera-
tion on the basis of GNetIc models, consider the 
following example starting upon the arrival of a 
message which specifies the 
to
its characteristic properties:  
 
   lmDescrProperty (townhall-1). 
 
Based on this communicative intention, the imag-
istic description of the involved object gets acti-
vated and the agent adopts a spatial perspective 
towards it from which the object is to be described 
(see Figure 3). The representation is analyzed for 
referent features required by the GNetIc model: 
position, main axis, symmetry, number of subparts, 
and shape properties. Regarding the latter, a unifi-
cation of the imagistic townhall-1 representation 
and a set of underspecified shape property repre-
sentations (e.g. for ?longish?, ?round? etc.) reveals 
?U-shaped? as the most salient property to be de-
picted. All evidence available  (referent features, 
discourse context, previous gesture and linguistic 
context) is propagated through the network 
(learned from the data of  one parti
re
for the values in each chance node.  
 
 
Figure 3: The townhall in the virtual world (left) and sche-
matic of the corresponding IDT content (right); activated parts 
are marked. 
 
This way, it is first decided to generate a gesture in 
the current discourse situation at all, the represen-
ation techt
realized with both hands and the pointing hand-
shape ASL-G. Next, the model?s decision nodes 
are employed to decide on the palm and back of 
hand (BoH) orientation as well as movement type 
and direction: as typical in drawing gestures, the 
palm is oriented downwards and the BoH away 
from the speaker?s body. These gesture features are 
combined with a linear movement  consisting of 
two segments per hand (to the right and backwards 
with the right hand; accordingly mirror-
symmetrical with the left hand) to depict the shape 
of the townhall.  
Accompanying speech is generated from selected 
propositional facts using an NLG engine. Syn-
 
92
 chrony between speech and gesture follows co-
expressivity and is set to hold between the gesture 
 
would approach the town-hall and 
 initial 
sequent 
transition 
ore than a repetition of the word 
 
Router: Das ist dann das Rathaus [placing]. 
This is then the townhall [placing]. 
 Das ist ein u-f?rmiges Geb?ude [drawing].  
That is a U-shaped building [drawing]. 
 Du blickst praktisch da rein [shaping].Y
stroke (depicting the U-shape property) and corre-
sponding linguistic element. These values are used 
to fill the slots of a gesture feature matrix which is 
transformed into an XML representation to be real-
ized with the virtual agent MAX (see Figure 4).  
 
 
Figure 4: Specification (left) and realization (right) of an 
autonomously generated drawing gesture which depicts the U-
haped townhall. s
4 Example of a Minimal Discourse 
To start with the analysis of how gestures are not
only employed to carry referential content but also 
to regulate dialogue and discourse, we first present 
a datum from the SAGA corpus showing how the 
Follower?s gesture aligns with the Router?s gesture 
to indicate acknowledgement or accept. The situa-
tion is as follows: the Router describes to the Fol-
lower that he 
how it looks to him. A transcription of the
dialogue passage by the Router and the sub
crucial speech-gesture annotation, including the 
Follower, in ELAN looks as displayed in Figure 5 
(placing, drawing, and shaping are names of anno-
tated gestural representation techniques). 
A short comment on the data might be in order: 
When introducing the townhall as a U-shaped 
building, the Router draws the boundary of it, 
namely a ?U?. He then goes on to describe how the 
on-looker apprehends the building. This is accom-
panied by a forward-oriented direction gesture with 
both hands, mimicking into it. In principle, all the 
information necessary to identify the townhall 
from a front perspective is given by then. There is 
a short pause and we also have a turn 
relevance place here. However, there is no feed-
back by the Follower at this point. Therefore the 
Router selects a typical pattern for self-repairs or 
continuations in German, a that is construction in 
the guise of a propositional apposition. Overlap-
ping the production of kind, he produces a three-
dimensional partial U-shaped object maintaining 
the same perspective as in his first drawing of the 
U-shaped border.  
Observe that the Follower already gives feedback 
after front. The most decisive contribution is the 
Follower?s acknowledgement, however. She imi-
tates the Router?s gesture but from her perspective 
as a potential observer. Also, at the level of single 
form features, she performs the gesture differently. 
(different movement direction, different symmetry) 
The imitating gesture overlaps with her nod and 
her contribution OK. It is important to see that her 
gesture provides m
townhall could possibly give. It refers at the same 
time to the town-hall (standing for a discourse ref-
erent) and provides the information of a U-shape 
indicating property, in other words, it expresses the 
propositional information ?This building being U-
shaped? with this building acting as a definite 
anaphora to the occurrence of a building in the first 
part of the Router?s contribution. Hence, assessed 
from a dialogue perspective the following happens: 
The grounding process triggered by the Follower?s 
acknowledgement amounts to mutual belief among 
Router and Follower that the town hall is U-shaped 
and the approaching on-looker on the route per-
ceives it from the open side of the U. 
o
look practically there into it  [shaping]. 
 Das heisst, es hat vorne so zwei Buchtungen
That is, it has to the front kind of two bulges. 
 und geht hinten zusammen dann.and closes i
the rear then. 
Figure 5: Example showing the Router?s and the Fol-
lower?s gestures and their crucial exchange in terms of 
the Router?s assertion and the Follower?s acknowl-
edgement.  
 
93
 Figure 6: Overview of the production and understanding cycle in the simulation model.
 
5 Extending the Simulation: The Under-
standing-Acceptance/Generation Cycle 
How can we go beyond the simulation of isolated 
speaker-specific gestures towards the generation of 
gestures in dialogues? We build on our findings in 
 the corpus study, briefly taken up here again (see
list in section 2 and the respective comments): 
Gesture helps in structuring the dialogue support-
n
ment of the current speaker?s (Router?s or Fol-
 
re 5 (R1) and the sub-
e fact that the BDN 
-
ing next speaker selection or indicating non-regular 
co tributions of other speaker. It enables assess-
lower?s) communicative intentions by the ad-
dressee, for example of whether the Router wants 
to keep the turn but indicates current memory and 
recapitulation problems thus appealing to the ad-
dressee?s cooperation. In addition, appraisal of the 
reliability of the information given by the Router 
can be read off from some of the Router?s gestures. 
Finally, as shown in section 4, gestures comple-
menting or even replacing verbal information is 
used in acknowledgements. 
Building on these observations, our goal is to 
simulate such dialogic interaction with two virtual 
agents (Router and Follower), each of whom pro-
vided with a speaker-specific GNetIc model. In the 
minimal discourse example Router and Follower  
use similar gestures which, notably, differ with 
respect to some details (e.g. speaker?s perspective). 
In the simulation we essentially capture the 
Router?s contribution in Figu
sequent acknowledgement by the Follower (F1). In 
order to vary the Router?s gesturing behavior we 
use the representation technique of drawing instead 
of shaping in the simulation. 
What we need to extend the model with is an 
analysis of the Follower?s understanding of the 
Router?s gesture. Psychologically plausible but 
beyond commonly specialised technical ap-
proaches, we want to employ the same model of an 
agent?s ?gesture knowledge? for both generating 
and understanding gestures. For an overview of the 
production and understanding cycle see Figure 6.  
Here we can make use of th
formalism allows for two different types of infer-
ence, causal inferences that follow the causal inter 
actions from cause to effect, and diagnostic infer-
ences that allow for introducing evidence for ef-
fects and infer the most likely causes of these ef-
fects. This bi-directional use of BDNs could be 
complementary to approaches of plan/intention 
recognition such as in Geib and Goldman (2003). 
To model a use of gestures for regulation as ob-
served with the Follower F1, the Router agent?s 
gestural activity is set as evidence for the output 
nodes of the Follower?s BDN. A diagnostic infer-
ence then yields the most likely causes, that is, the 
most likely referent properties and values of dis-
course contextual variables. In other words, we 
employ the same speaker-specific GNetIc model 
for generation and for understanding. That is, in
formation about the physical appearance of the 
 
94
 Router?s gesture (as specified in Figure 4) is pro-
vided as evidence for the Follower?s GNetIc model 
revealing?correctly?that the gesture?s representa-
tion technique is ?drawing? and the shape property 
is ?U-shaped?.  
Notably, just as the gesture generation process has 
to make choices between similarly probable alter-
natives, not all diagnostic inferences which are 
drawn by employing the Follower agent?s GNetIc 
model are necessarily in line with the evidence 
from which the Router agent?s gesture was origi-
nally generated. For instance, the communicative 
goal as inferred by the Follower agent is 
?lmDescrPosition? (with a likelihood of .65) in-
simulate such iconic ges-
nts 
gue structure such as 
ext speaker selection or acknowledgement and 
outer?s 
e? 
posed in classical CA research 
back 
 participants 
stead of ?lmDescrProperty?. Nevertheless, the in-
ferred knowledge reveals an underspecified repre-
sentation of the referent (see Figure 7) as well as 
the most likely specification of the discourse con-
text. That way, the Follower agent develops his 
own hypothesis of the Router agent?s communica-
tive goal and the content being depicted gesturally.  
This hypothesis is forwarded to the follower 
agent?s dialogue manager, which responds to such 
declaratives by the Router with an acknowledge-
ment grounding act. Now the very same generation 
process as described in section 3 sets in. The Fol-
lower agent?s feedback is generated by employing 
his GNetIc model for causal inference. The result-
ing gesture is, notably, different from the Router 
agent?s gesture: it is a two-handed shaping gesture 
with handshape ASL-C. Movement type and 
movement features are the same as in the Router 
agent?s drawing gesture. Palm and BoH orientation 
are different due to representation technique spe-
cific patterns which are implemented in the deci-
sion nodes (see Figure 7). This case of using iconic 
gesture for regulating dialogue has been success-
fully implemented using GNetIc and the overall 
production architecture. 
6 Discussion and further research agenda 
In this paper we addressed the dialogue-regulating 
function of gestures. Based on empirical observa-
tions of interactional patterns from the SAGA cor-
pus, the starting points for the simulation of these 
gestures were non-interactional propositional ones 
such as iconics used to describe routes or land-
marks. We achieved to 
tures used in their function as acknowledgeme
shown in section 3 which clearly transcends their 
mere representational task. 
 
 
Figure 7: Imagistic representation of what the Follower un-
derstood from the Router?s gestural depiction of the townhall 
(left) and the simulation of the Follower?s autonomously gen-
erated shaping gesture used as an acknowledgement. 
 
We first note that we draw a distinction between 
gestures relevant for dialo
n
those which focus on influencing the social climate 
among the dialogue participants. We did not have 
many of the latter in SAGA but observed some 
which we classified as ?calming down? and ?don?t 
bother?. In certain communication cultures also 
touching the other?s body is accepted. 
As for a research agenda to elucidate further the 
functions of gestures in dialogue, we do not go too 
deeply into matters of dialogue theory here. We 
already have shown that gestures accompanying 
base-line information, being part of the R
report or the Follower?s uptake can be modelled in 
PTT (Poesio and Rieser 2009, Rieser and Poesio 
2009), if one assumes a unified representation for 
verbal and gestural meaning. Here we concentrate 
on how the simulation work can be pushed forward 
based on theoretical analyses of empirical data.  
Note that on the list of discourse gestures given in 
section 2 the following items are tied to Router?s 
behaviour and can be generated in an autonomous 
fashion: 
? managing of own turn 
? evidentials for establishing a confidence 
level 
? assessment of relevance by discourse par-
ticipants 
? indication of topicality with respect to time, 
place or objects.  
Observe, however, that these will also have an im-
pact on the mental state of the Follower as is e.g., 
obvious for evidentials or the ?brush away gestur
(Figure 1). Relevant for the sequencing of multi-
modal contributions are clearly the following: 
? mechanisms of next-speaker selection as 
pro
? grounding acts and feed
? handling of non-canonical moves by dis-
course
 
95
 ? clarification requests to work on contribu-
tions. 
Th
of adj ing a current and a next 
cep-
tan
this ki ation. 
Ac
Thi r
the
es-
ogue. Personality and 
lletin, 21(4):394?405 
 
 Kopp, S. (2009). Increasing expres-
siveness for virtual agents?Autonomous generation 
d gesture in spatial description tasks. In 
 
n-
n-
  
Ca
Cla
Ge
Gil
gy 
Gin
ress).  
Ha 9-2011): Dialogue Struc-
Ha ch-
ing Gesture 
Lev 983). Pragmatics. Cambridge Uni-
L?c
 M. Kipp et al (Eds.), 
Mc
Poe
nd et al (Eds.), Proceedings of the 13th Work-
Rie
and Wachsmuth (Eds.), 
Rie io, M. (2009). Interactive Gesture in 
Poe ordi-
, 1?89 
Ro  
d-
Sac
king 
Sta  
Sow 9). A computational 
guage and Dialogue, pages 132?
146. Oxford University Press. 
ese are intrinsically involved in the production 
acency pairs, hav
contribution and it is on these that simulation will 
focus on in future work. In combination with an 
information state-based multimodal discourse re-
cord (Traum & Larsson, 2003), the implementated 
cycle of generation, understanding and ac
ce/generation provides the basis for modeling 
nd of gesture-based discourse regul
knowledgments 
esearch is partially ss upported by the DFG in 
 CRC 673 ?Alignment in Communication? and 
the Center of Excellence ?Cognitive Interaction 
Technology?. 
References  
Asher, N. and Lascarides, A. (2003). The Logic of Con-
versation. Cambridge University Press 
Bavelas, J., Chovil, N., Lawrie, D., and Wade, A. 
(1992). Interactive gestures. Discourse Processes, 
15(4):469?491. 
Bavelas, J., Chovil N., Coated, L., Roe, L. (1995). G
tures Specialised for Dial
Social Psychology Bu
Bergmann, K., & Kopp, S. (2010). Modelling the Pro-
duction of Co-Verbal Iconic Gestures by Learning
Bayesian Decision Networks. Applied Artificial In-
telligence, 24(6):530?551. 
Bergmann, K. &
of speech an
Proceedings of AAMAS 2009, pages 361?368.  
Bergmann, K. & Kopp, S. (2009a). GNetIc?Using
Bayesian Decision Networks for iconic gesture ge
eration. In Proceedings of the 9th International Co
ference on Intelligent Virtual Agents, pages 76?89.
rgmann, K., KoppBe , S., and Eyssel, F. (2010). Indi-
vidualized gesturing outperforms average gesturing?
Evaluating gesture production in virtual humans. In 
Proceedings of IVA 2010, pages 104?117, Ber-
lin/Heidelberg. Springer.  
ssell, J. and S. Prevost (1996). Distribution of Seman-
tic Features Across Speech and Gesture by Humans 
and Computers. Proceedings of the Workshop on the 
Integration of Gesture in Language and Speech. 
rk, H.H. (1996). Using Language. CUP 
ib, C., Goldman, R.,(2003). Recognizing Plan/Goal 
Abandonment. In Proceedings of the International 
Joint Conference on Artificial Intelligence (IJCAI), 
pp. 1515?1517. 
l, S. P., Kawamori, M., Katagiri, Y., and Shimojima, 
A. (1999). Pragmatics of body moves. In Proceed-
ings of the 3rd International Cognitive Technolo
Conference, pages 345?358.  
zburg, J. (2011). The Interactive Stance. Meaning 
for Conversation. Oxford University Press (in p
hn, F. and Rieser, H. (200
ture Gestures and Interactive Gestures. Manual, 1st 
version. CRC 673 Working Paper. Bielefeld Univer-
sity 
hn, F. and Rieser, H. (2010): Explaining Spee
Gesture Alignment in MM Dialogue Us
Typology. In P. Lupowski and M. Purver (Eds.), As-
pects of Semantics and Pragmatics of Dialogue. 
SemDial 2010, pp. 99?111. 
inson, St. C. (1
versity Press. 
king, A., Bergmann, K., Hahn, F., Kopp, S., & Rie-
ser, H. (2010): The Bielefeld Speech and Gesture 
Alignment Corpus (SaGA). In
LREC 2010 Workshop: Multimodal Corpora. 
Neill, D. (1992). Hand and Mind. Chicago Univer-
sity Press. 
sio, M. & Rieser, H. (2009). Anaphora and Direct 
Reference: Empirical Evidence from Pointing. In J. 
Edlu
shop on the Semantics and Pragmatics of Dialogue 
(DiaHolmia) (pp. 35?43). Stockholm, Sweden. 
ser, H. (2010). On Factoring out a Gesture Typology 
from the Bielefeld Speech-And-Gesture-Alignment 
Corpus (SAGA). In Kopp 
Proceedings of GW 2009. Springer, pp. 47?61. 
ser, H. & Poes
Dialogue: a PTT Model. In P. Healey et al (Eds.), 
Proceedings of the SIGDIAL 2009 Conference (pp. 
87?96). London, UK: ACL. 
sio, M. and Rieser, H. (2010). Completions, co
nation and alignment in dialogue. Dialogue and Dis-
course 1(1)
Poesio, M. and Traum, D. (1997). Conversational ac-
tions and discourse situations. Computational Intel-
ligence, 13(3): 309?347 
que, A. and Traum, D. (2008). Degrees of Grounding
Based on Evidence of Understanding. In Procee
ings of the 9th SIGdial Workshop on Discourse and 
Dialogue, pp. 54?63 
ks, H., Schegloff, E., Jefferson, G. (1974). A sim-
plest systematics for the organization of turn-ta
for conversation. Language, 50: 696?735 
lnaker, R. (1978): Assertion. In Cole, P. (Ed.) Syntax
and Semantics 9: Pragmatics, pp. 315?322. 
a, T. and Wachsmuth, I. (200
model for the representation an processing of shape 
in coverbal iconic gestures. In K. Coventry et al 
(Eds.), Spatial Lan
 
96
 Traum, D. (1999). Computational models of groundin
in collaborative systems. In Working Notes of AAAI 
Fall Symposium on Psychol
g 
ogical Models of Com-
Tra e 
elt (Eds.), Current and New 
munication, pp. 124?131. 
um, D., & Larsson, S. (2003). The information stat
approach to dialogue management. In R.W. Smith 
and J.C.J. van Kuppev
Directions in Discourse & Dialogue (pp. 325?353). 
Kluwer Academic Publishers. 
 
97
