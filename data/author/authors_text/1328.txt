R. Dale et al (Eds.): IJCNLP 2005, LNAI 3651, pp. 792 ? 803, 2005. 
? Springer-Verlag Berlin Heidelberg 2005 
Global Path-Based Refinement of Noisy Graphs  
Applied to Verb Semantics 
Timothy Chklovski and Patrick Pantel 
Information Sciences Institute,University of Southern California, 
4676 Admiralty Way,Marina del Rey, CA  90292 
{timc, pantel}@isi.edu 
Abstract. Recently, researchers have applied text- and web-mining algorithms 
to mine semantic resources. The result is often a noisy graph of relations be-
tween words. We propose a mathematically rigorous refinement framework, 
which uses path-based analysis, updating the likelihood of a relation between a 
pair of nodes using evidence provided by multiple indirect paths between the 
nodes. Evaluation on refining temporal verb relations in a semantic resource 
called VERBOCEAN showed a 16.1% error reduction after refinement. 
1   Introduction 
Increasingly, researchers are creating broad-coverage semantic resources by mining 
text corpora [1][5] and the Web [2][6]. These resources typically consist of a noisy 
collection of relations between words. The data is typically extracted on a per link 
basis (i.e., the relation between two nodes is determined without regard to other 
nodes). Yet, little work has taken a global view of the graph of relations, which may 
provide additional information to refine local decisions by identifying inconsistencies, 
updating confidences in specific edges (relations), and suggesting relations between 
additional pairs of nodes. 
For example, observing the temporal verb relations ?discover happens-before re-
fine? and ?refine happens-before exploit? provides evidence for the relation ?discover 
happens-before exploit,? because the happens-before relation is transitive. 
We conceptualize a semantic resource encoding relations between words as a graph 
where words are nodes and binary relations between words are edges. In this paper, we 
investigate the refinement of such graphs by updating the confidence in edges using a 
global analysis relying on link semantics. Our approach is based on the observation that 
some paths (chains of relations) between a pair of nodes xi and xj imply the presence or 
absence of a particular direct relation between xi and xj. Despite each individual path 
being noisy, multiple indirect paths can provide sufficient evidence for adding, remov-
ing, or altering a relation between two nodes. As illustrated by the earlier example, 
inferring a relation based on the presence of an indirect path relies on the semantics of 
the links that make up the path, like transitivity or equivalence classes. 
As an evaluation and a sample practical application, we apply our refinement 
framework to the task of refining the temporal precedence relations in VERBOCEAN,  
a broad-coverage noisy network of semantic relations between verbs extracted by 
mining the Web [2]. Examples of new edges discovered (added) by applying the 
 Global Path-Based Refinement of Noisy Graphs Applied to Verb Semantics 793 
framework include: ?ascertain happens-before evaluate?, ?approve happens-before 
back?, ?coat happens-before bake?, ?plan happens-before complete?, and ?interrogate 
happens-before extradite?. 
Examples of edges that are removed by applying our framework include: ?induce 
happens-before treat?, ?warm happens-before heat?, ?halve happens-before slice?, 
and ?fly happens-before operate?. 
Experiments show that our framework is particularly good at filtering out the in-
correct temporal relations in VERBOCEAN. Removing incorrect relations is particu-
larly important for inference systems. 
2   VerbOcean 
We apply our path-based refinement framework to VERBOCEAN [2], a web-extracted 
lexical semantics resource with potential applications to a variety of natural language 
tasks such as question answering, information retrieval, document summarization, and 
machine translation. VERBOCEAN is a graph of semantic relations between verbs, with 
3,477 verbs (nodes) and 22,306 relations (edges). Although the framework applies 
whenever some paths through the graph imply presence or absence of a relation, for 
the evaluation we focus on the temporal precedence relation in VERBOCEAN, and, in 
an ancillary role, on the similarity relation. Senses are not discriminated and an edge 
indicates that the relation is believed to hold between some senses of the verbs in this 
relation. 
The five semantic relations present in VERBOCEAN are presented in Table 1. Tem-
poral precedence (happens-before) is a transitive asymmetric temporal relation be-
tween verbs. Similarity is a relation that suggests two nodes are likely to be in the 
same equivalence class, although polysemy makes it only weakly transitive. 
Table 1. Types, examples and frequencies of 22,306 semantic relations in VERBOCEAN 
Semantic Relation Example Transitive Symmetric # in VERBOCEAN 
temporal precedence marry :: divorce Y N 4,205 
similarity produce :: create Y Y 11,515 
strength wound :: kill Y N 4,220 
antonymy open :: close N Y 1,973 
enablement fight :: win Y N 393 
In VERBOCEAN, asymmetric relations between two nodes are enforced to be unidi-
rectional (i.e., presence of an edge xi happens-before xj guarantees absence of an edge 
xj happens-before xi). Larger, inconsistent loops are possible, however, as extraction 
is strictly local. Taking advantage of the global picture to refine the edges of the graph 
can improve quality of the resource, helping performance of any algorithms or appli-
cations that rely on the resource. 
794 T. Chklovski and P. Pantel 
3   Global Refinement 
Our approach relies on a global view of the graph to refine a relation between a given 
pair of nodes xi and xj, based on multiple indirect paths between the two nodes.  The 
analysis processes triples <xi, r, xj> for the relation r to output r, its opposite (which 
we will denote q), or neither. The opposite of happens-before is the same relation in 
the reverse direction (happens-after). The refinement is based on evidence provided 
by indirect paths, over a probabilistic representation of the graph. 
Section 3.1 introduces the steps of the refinement, Section 3.2 details which paths 
are used as evidence, and Section 3.3 derives the statistical model used for combining 
evidence from multiple unreliable paths. 
3.1   Overview of the Refinement Algorithm 
We first introduce some notation. Let Ri,j denote the event that the relation r is present 
between nodes xi and xj in the original graph ? i.e., the graph indicates (perhaps spuri-
ously) the presence of the relation r between xi and xj. Let ri,j denote the relation r 
actually holding between xi and xj. Let ?i,j denote an acyclic path from xi to xj of (pos-
sibly distinct) relations {Ri,i+1 .. Rj-1,j}. For example, the path ?x1 similar x2 happens-
before x3? can be denoted ?1,3. If the edges of ?i,j indicate the relation r between the 
nodes xi and xj, we say that ?i,j indicates ri,j. 
Given a triple <xi, r, xj>, we identify the set ?r full of all paths ?i,j such that ?i,j indi-
cates ri,j and ?i,j?s sequence of relations {Ri,i+1 .. Rj-1,j} matches one of the allowed 
sequences.  That is, we only consider certain path types.  The restriction on types of 
paths considered is introduced because identifying and processing all possible paths 
indicating ri,j is too demanding computationally in a large non-sparse graph. The path 
types considered are detailed in Section 3.2. Note that the intermediate nodes of paths 
can range over the entire graph. 
For each ?i,j in the above set ?r full, we compute the estimated probability that ri,j 
holds given the observation of (relations that make up) ?i,j. Each edge in the input 
graph is treated as a probabilistic one, with probabilities P(ri,j) and P(ri,j|Ri,j) estimated 
from human judgments on a representative sample. Generally, longer paths and paths 
made up of less reliable edges will have lower probabilities. Section 3.3 presents the 
full model for estimating these probabilities. 
Next, we form the set ?r by selecting from ?r full only the paths which have no 
common intermediate nodes. This is done greedily, processing all paths in ?r full in 
order of decreasing score, placing each in ?r iff it does not share any intermediate 
nodes with any path already in ?r. This is done to avoid double-counting the available 
evidence in our framework, which operates assuming conditional independence of 
paths. 
Next, we compute P(ri,j | ?r), the probability of ri,j given the evidence provided by 
the paths in ?r.  The model for computing this is described in Section 3.3. Similarly, 
?q and P(qi,j | ?q) are computed for qi,j, the opposite of ri,j. Next, the evidence for r 
and q are reconciled by computing P(ri,j | ?r, ?q) and, similarly, P(qi,j | ?r, ?q). 
Finally, the more probable of the two relations ri,j and qi,j is output if its probability 
exceeds a threshold value Pmin (i.e., ri,j is output if P(ri,j | ?r, ?q) > P(qi,j | ?r, ?q) and 
P(ri,j | ?r, ?q) > Pmin. In Section 4.2, we experiment with varying values of Pmin. 
 Global Path-Based Refinement of Noisy Graphs Applied to Verb Semantics 795 
3.2   Paths Considered 
The enabling observation behind our approach is that in a graph in which edges have 
certain properties such as transitivity, some paths ?i,j indicate the presence of a rela-
tion between the first node xi and the last node xj. In the paths we consider, we rely on 
two kinds of inferences: transitivity and equivalence. Also, we do not consider very 
long paths, as they tend to become unreliable due to accumulation of chance of false 
detection of each edge and sense drift in each intermediate node. The set of paths to 
consider was not rigorously motivated. Rather, we aimed to cover some common 
cases. Refining the sets of paths is a possible fruitful direction for future work. 
For the presence of happens-before, a transitive asymmetric relation, we consid-
ered all 11 path types of length 3 or less which imply happens-before between the end 
nodes based on transitivity and equivalence: 
 
?happens-before? ?similar, similar, happens-before? 
?happens-before, similar? ?happens-before, happens-before, similar? 
?similar, happens-before? ?similar, happens-before, happens-before? 
?happens-before, happens-before? ?happens-before, similar, happens-before? 
?happens-before, similar, similar? ?happens-before, happens-before, happens-before? 
?similar, happens-before, similar?  
3.3   Statistical Model for Combining Evidence 
This section presents a rigorous derivation of the probabilistic model for computing 
and combining probabilities with which indirect paths indicate a given edge. 
3.3.1   Estimating from a Single Path 
We first derive probability of r1,n given single path ? 1,n: 
 
( )nnrP ,1,1 |?
 
If n is 2, i.e. ?1,n has only one edge R1,2, we have simply the probability that the 
edge actually holds given its presence in the graph: 
 ( ) ( )2,12,12,12,1 || RrPrP =?  (1) 
Otherwise, ?1,n has intermediate nodes, in which case P(r1,n | ?1,n) can be estimated 
as follows: 
 
( ) ( ) ( ) ( )
( )( ) ( )( )nnnnnnnnn
nnnnnnnnnnnnnn
RRrrPrrRRrP
RRrrPrrRRrPRRrPrP
,12,1,12,1,12,1,12,1,1
,12,1,12,1,12,1,12,1,1,12,1,1,1,1
...,,|...,,...,,,...,,|
...,,|...,,...,,,...,,|...,,||
????
?????
??
+==?
 
Because r1,n is conditionally independent from Ri,i+1 given ri,i+1 or ?ri,i+1, we can 
simplify: 
 
( ) ( ) ( )
( )( ) ( )( )nnnnnnn
nnnnnnnnn
RRrrPrrrP
RRrrPrrrPrP
,12,1,12,1,12,1,1
,12,1,12,1,12,1,1,1,1
...,,|...,,...,,|
...,,|...,,...,,||
???
???
??
+=?
 
Assuming independence of a given relation ri,i+1 from all edges in ?1,n except for 
the edge Ri,i+1 yields: 
796 T. Chklovski and P. Pantel 
 
( ) ( ) ( )
( )( ) ( )( )?
?
?=
++?
?=
++?
??
+=
1..1 1,1,,12,1,1
1..1 1,1,,12,1,1,1,1
|1...,,|
|...,,||
ni iiiinnn
ni iiiinnnnn
RrPrrrP
RrPrrrPrP ?
 
Let Pmatch denote the probability that there is no significant shift in meaning at a 
given intermediate node.  Then, assume that path r1,2,?, rn-1,n indicates r1,n iff the 
meanings at n ? 2 intermediate nodes match: 
 ( ) 2
,12,1,1 ...,,| ?? = nmatchnnn PrrrP  
Also, when one or more of the relations ri,i+1 do not hold, nothing is generally im-
plied1 about r1,n, thus 
 ( )( ) ( )nnnn rPrrrP ,1,12,1,1 ...,,| =? ?  
Plugging these in, we have: 
 ( ) ( ) ( ) ( )( )??
?=
++
?
?=
++
?
?+=
1..1 1,1,
2
,11..1 1,1,
2
,1,1 |1|| ni iiiinmatchnni iiiinmatchnn RrPPrPRrPPrP ?  
which can be rewritten as: 
 ( ) ( ) ( )( ) ( )?
?=
++
?
?+=
1..1 1,1,
2
,1,1,1,1 |1| ni iiiinmatchnnnn RrPPrPrPrP ?  (2) 
where the prior P(r1,n) and the conditional P(ri,i+1 | Ri,i+1) can be estimated empirically 
by manually tagging the relations Ri,j in a graph as correct or incorrect: P(r1,n) is the 
probability that an edge will be labeled with relation r by a human judge, and 
P(ri,i+1 | Ri,i+1) is the precision with which the system could identify R. While Pmatch 
can be estimated empirically we have not done so. We experimentally set Pmatch = 0.9. 
3.3.2   Combining Estimates from Multiple Paths 
In this subsection we derive an estimate of the validity of inferring r1,n given the set 
?r of m paths ?1,n1, ?1,n2, ?, ?1,nm: 
 ( )mnnnnrP ,12,11,1,1 ,...,,| ???  (3) 
In the case of zero paths, we use simply P(r1,n)=P(r), the probability of observing r 
between a pair of nodes from a sample set with no additional evidence. The case of 
one path has been treated in the previous section. In the case of multiple paths, we 
derive the expression as follows (omitting for convenience subscripts on paths, and 
distinguishing them by their superscripts). We assume conditional independence of 
any two paths ?k and ?l given r or ?r. Using Bayes? rule yields2: 
 ( ) ( ) ( )( )
( ) ( )
( )mmk
k
m
m
m
n P
rPrP
P
rPrP
rP
??
?
??
????
,...,
|
,...,
|,...,
,...,| 1 ..11
1
1
,1
?
=
==
 (4) 
                                                          
1
  This is not the case for paths in which the value of one edge, given the other edges, is corre-
lated with the value of the end-to-end relation. The exception does not apply for happens-
before edges if there are other happens-before edges in the path, nor does it ever apply for 
any similar edges. 
2
  Here and afterward, the denominators must be non-zero; they are always so when we apply 
this model. 
 Global Path-Based Refinement of Noisy Graphs Applied to Verb Semantics 797 
The above denominator can be rewritten as: 
 
( ) ( ) ( ) ( ) ( )
( ) ( ) ( ) ( )??
==
??+
=??+=
mk
k
mk
k
mmm
rPrPrPrP
rPrPrPrPP
..1..1
111
||
|,...,|,...,,...,
??
??????
 (5) 
Using Bayes? rule again, the expressions in the above products can be rewritten as 
follows: 
 ( ) ( ) ( )( )rP
PrP
rP
kk
k ??? || =  (6) 
 ( ) ( ) ( )( )
( )( ) ( )
( )rP
PrP
rP
PrP
rP
kkkk
k
?
?
=
?
?
=?
1
|1|| ?????  (7) 
Substituting into Eq. 5 the Eqs. 6 and 7 yields: 
( ) ( ) ( ) ( ) ( ) ( ) ( ) ( )( ) ( )( )
( )( ) ( )
( )????
====
=???
?
???
?
?
?
?+???
?
???
?
=??+=
mk
kk
mk
kk
mk
k
mk
km
rP
PrP
rP
rP
PrP
rPrPrPrPrPP
..1..1..1..1
1
1
|11|||,..., ????????
 ( )( ) ( )( )( )
( )( )
( )( ) ??
?
?
?
??
?
?
?
?
?
+?
?
=
?
=
=
??
? 1..11..1
..1 1
|1|
m
mk
k
m
mk
k
mk
k
rP
rP
rP
rP
P
??
?
 
Using the above for the denominator of Eq. 4, using Eq. 6 in the numerator of Eq. 
4, and simplifying, we have: 
 
( ) ( ) ( )( )
( )
( )( )( )
( )( )
( )( )
( )( ) 1
..1
1
..1
1
..1
1
..11
1
|1|
|
,...,
|
,...,|
?
=
?
=
?
=
=
?
?
+
== ??
?
?
m
mk
k
m
mk
k
m
mk
k
m
mk
k
m
rP
rP
rP
rP
rP
rP
P
rPrP
rP ??
?
??
?
??
 
which can be rewritten as 
 
( ) ( )
( ) ( )( ) ( )( )??
?
=
?
=
=
????
?
???
?
?
+
=
mk
k
m
mk
k
mk
k
m
rP
rP
rP
rP
rP
rP
..1
1
..1
..11
|1
1
|
|
,...,|
??
?
??
 (8) 
where P(r | ?k) is as in Eq. 2 and P(r) can be estimated empirically. 
3.3.3   Estimating from Supporting and Opposing Paths 
Recall that q denotes the opposite of r. The previous section has shown how to com-
pute P(r | ?r) and, similarly, P(q | ?q). We now derive how to estimate r given both 
?r, ?q: 
 ( )qrrP ?? ,|  (9) 
We assume that r and q are disjoint, P(r,q)= P(r|q)= P(q|r)=0. We also assume that 
q is conditionally independent from ?r given ?r, i.e., 
798 T. Chklovski and P. Pantel 
 ( ) ( )rqPrqP r ?=?? |,|  and ( ) ( )qqr rqPrqP ??=??? ,|,,| , and similarly 
 ( ) ( )qrPqrP q ?=?? |,|  and ( ) ( )rqr qrPqrP ??=??? ,|,,|  
We proceed by deriving the following, each consequent relying on the previous re-
sult: 
LEMMA 1: P(q | ?r), in Eq. 10 
LEMMA 2: P(?q | ?r), in Eq. 12 
LEMMA 3: P(r | ?q, ?r) and P(q | ?r, ?q), in Eqs. 13 and 14 
THEOREM 1: P(r | ?r, ?q), in Eq. 18. 
LEMMA 1. From P(r | q) = 0, we observe: 
 
( ) ( ) ( ) ( ) ( ) ( ) ( )rqPrPrqPrPrqPrPqP ??=??+= |||
 
Solving for P(q | ?r), we obtain: 
 ( ) ( )( )rP
qP
rqP
?
=?|  (10) 
LEMMA 2. Using an approach similar to that of Lemma 1 and noting that P(q | r, ?r) = 
P(q | r) = 0 yields: 
 ( ) ( ) ( ) ( ) ( ) ( ) ( )rrrrrrr rqPrPrqPrPrqPrPqP ????+=????+??=? ,||0,||,|||  
Invoking the assumption P(q | ?r, ?r) = P(q | ?r), we can simplify: 
 ( ) ( ) ( )rqPrPqP
rr
???=? |||  
Substituting the result of Lemma 1 (Eq. 10) into the above yields: 
 ( ) ( ) ( )( )rP
qPrPqP rr
?
??
=? ||  (11) 
And thus 
 ( ) ( ) ( ) ( )( )rP
qPrPrPqP rr
?
????
=?? ||  (12) 
LEMMA 3. We derive P(r | ?q, ?r), using P(?q | r, ?r) = 1: 
 ( ) ( )( )
( ) ( )
( ) ( )
( )
( )
( )
( )r
r
r
r
rr
rr
r
r
r qP
rP
qP
qrP
PqP
PqrP
qP
qrP
qrP
??
?
=
??
??
=
???
???
=
??
??
=?? |
|
|
|,
|
|,
,
,,
,|  
Substituting the result of Lemma 2 (Eq. 12) into the above yields: 
 ( ) ( ) ( )( ) ( ) ( )qPrPrP
rPrPqrP
r
r
r ????
??
=?? |
|
,|  (13) 
Similarly, 
 
( ) ( ) ( )( ) ( ) ( )rPqPqP
qPqP
rqP
q
q
q ????
??
=?? |
|
,|
 (14) 
 Global Path-Based Refinement of Noisy Graphs Applied to Verb Semantics 799 
THEOREM 3 
 ( ) ( ) ( ) ( )( )( ) ( )( ) ( ) ( )( ) ( ) ( )( )qPqPrPrPqPrP
qPrPrP
rP
qr
qr
qr
???????
????
=?? ||11
||
,|  
P(r | ?r, ?q) can be derived using the above Lemmas, as follows: 
 ( ) ( ) ( ) ( ) ( )qrqrqrqrqr qrPqPqrPqPrP ??????+????=?? ,,|,|,,|,|,|  
The assumption P(r | q) = 0 implies P(r | q, ?r, , ?q) = 0. Also, since r is condi-
tionally independent of ?q given ?q, we have P(r |?q, ?r, ?q) = P(r | ?q, ?r).  Thus, 
we can simplify: 
 ( ) ( ) ( ) ( )( ) ( )rqrrqrqr qrPqPqrPqPrP ?????=?????=?? ,|,|1,|,|,|  (15) 
Similarly, 
 ( ) ( ) ( ) ( )( ) ( )qqrqqrqr rqPrPrqPrPqP ?????=?????=?? ,|,|1,|,|,|  (16) 
Substituting, Eq. 16 into Eq. 15 yields: 
 
( ) ( )( ) ( )( ) ( )
( ) ( )( ) ( ) ( ) ( )rqqrqr
rqqrqr
qrPrqPrPrqPqrP
qrPrqPrPrP
??????+?????=
????????=??
,|,|,|,|1,|
,|,|,|11,|
 
Solving for P(r | ?r, ?q), we get: 
 ( ) ( ) ( ) ( )( ) ( )qr
qrr
qr
rqPqrP
rqPqrPqrP
rP
?????
???????
=??
,|,|1
,|,|,|
,|  (17) 
Expanding and simplifying, we establish our Theorem 1: 
 ( ) ( ) ( ) ( )( )( ) ( )( ) ( ) ( )( ) ( ) ( )( )qPqPrPrPqPrP
qPrPrP
rP
qr
qr
qr
???????
????
=?? ||11
||
,|  (18) 
4   Experimental Results 
In this section, we evaluate our refinement framework on the temporal precedence 
relations discovered by VERBOCEAN, and present some observations on applying the 
refinement to other VERBOCEAN relations. 
4.1   Experimental Setup 
Following Chklovski and Pantel [2], we studied 29,165 pairs of verbs obtained from a 
paraphrasing algorithm called DIRT [4]. We applied VERBOCEAN to the 29,165 verb 
pairs, which tagged each pair with the semantic tag happens-before, happens-after 
and no temporal precedence3. 
                                                          
3
  VERBOCEAN actually produces additional relations such as similarity, antonymy, strength and 
enablement. For our purposes, we only consider the temporal relations. 
800 T. Chklovski and P. Pantel 
For our experiments, we randomly sampled 1000 of these verb pairs, and presented 
them to two human judges (without revealing the VERBOCEAN tag). The judges were 
asked to classify each pair among the following tags: 
Happens-before with entailment 
Happens-before without entailment 
Happens-after with entailment 
Happens-after without entailment 
Another semantic relation 
No semantic relation 
For the purposes of our evaluation, tags a and b align with VERBOCEAN?s happens-
before tag, tags c and d align with the happens-after tag, and tags e and f align with 
the no temporal relation tag4. The Kappa statistic [7] for the task was ? = 0.78. 
4.2   Refinement Results 
Table 2 shows the overall accuracy of VERBOCEAN tags on the 1000 verb pairs ran-
domly sampled from DIRT. Each row represents a different refinement. The number 
in parentheses is Pmin, the threshold value for the strength of the relation from Section 
3.1. As the threshold is increased, the refinement algorithm requires greater evidence 
(more supporting paths and absence of opposing evidence) to trigger a temporal rela-
tion between a pair of verbs. 
Table 2. Accuracy (95% confidence) of VERBOCEAN on a random sample of 1000 verb pairs 
tagged by two judges 
 Accuracy 
 
Judge1 Judge2 Total 
Unrefined
 
80.7% 74.8% 77.7% ? 2.0% 
Refined (0.5) 66.0% 63.7% 64.8% ? 2.6% 
Refined (0.66) 75.4% 71.7% 73.5% ? 2.4% 
Refined (0.9) 83.1% 77.2% 80.2% ? 2.1% 
Refined (0.95) 84.5% 78.0% 81.3% ? 1.9% 
Refined (Combo)* 86.8% 81.3% 84.0% ? 2.4% 
* Combo combines the no temporal relation from the 0.5 and the happens-before and 
happens-after from the and 0.95 refinements, where the reported accuracy is com-
puted on the subset of 716 verb pairs for which the algorithm is most confident. 
Table 3 shows the reassignments due to refinement. At the 0.5 level, the refinement 
left 76 of 81 relations unchanged, revising 3 to happens-after and 2 to no temporal 
relation. Similarly, only two of the original happens-after relations were changed 
with  refinement.  However,  of  the  849  originally  tagged  no temporal relation, the  
                                                          
4
  In future work, we plan to use the judges? classifications to evaluate the extraction of entail-
ment relations using VERBOCEAN. 
 Global Path-Based Refinement of Noisy Graphs Applied to Verb Semantics 801 
Fig. 3. Refinement precision on all 1000 verb 
pairs vs. on the 819 verb pairs on which the 
annotators agree on tag 
Overall Precision vs. Precision on Agreed Pairs
60
65
70
75
80
85
90
95
100
Unrefined R 0.5 R 0.66 R 0.9 R 0.95 R Combo
Refinement Algorithm
Pr
e
c
is
io
n
 
(%
)
Overall Agreed Pairs
Table 3. Allocation change between semantic tags due to refinement 
 Happens-Before Happens-After No Temporal Relation 
Unrefined 81 70 849 
Refined (0.5) 190 180 630 
Refined (0.66) 118 124 758 
Refined (0.9) 53 66 881 
Refined (0.95) 40 46 914 
 
refinement moved 113 to happens-before 
and 109 to happens-after. The precision 
of the 0.5 refinement on the no temporal 
relation tag increased by 4%; however, 
the precision on the temporal relations 
decreased by 5.7%. At the 0.95 refine-
ment level, 54 of the 81 relations origi-
nally tagged happens-before and 45 of 
the 70 relations originally tagged hap-
pens-after were changed to no temporal 
relation. Only 34 of the 849 no temporal 
relations were changed. At this level, the 
precision of no temporal relation tag 
decreased by 0.8% and the temporal 
relations? precision increased by 4%. 
Hence, at the 0.5 level, pairs classified as no temporal relation were improved 
while at the 0.95 level, pairs classified as a temporal relation were improved. To lev-
erage benefits of the two, we applied both the 0.5 and 0.95 level refinements and kept 
happens-before and happens-after classifications from the 0.95 level, and kept the no 
temporal relation classification from the 0.5 level.5 284 verb pairs were left unclassi-
fied. On the 716 classified verb pairs, refinement improved accuracy by 6.3%.  
                                                          
5
  This combination is guaranteed to be free of conflicts in classification because it is impossi-
ble for a relation to be classified as temporal at the 0.95 threshold level while being classified 
as non-temporal at the 0.5 level. 
Fig. 1. Refinement precision on each semantic 
tag 
Precision of Semantic Tags
0
20
40
60
80
100
Unrefined R 0.5 R 0.66 R 0.9 R 0.95 R Combo
Refinement Algorithm
Pr
e
c
is
io
n
 (%
)
Happens-Before Happens-After
No Temporal Relation Overall
Fig. 2. Refinement recall on each semantic tag 
Recall of Semantic Tags
0
20
40
60
80
100
Unrefined R 0.5 R 0.66 R 0.9 R 0.95 R Combo
Refinement Algorithm
R
ec
a
ll 
(%
)
Happens-Before Happens-After
No Temporal Relation Overall
802 T. Chklovski and P. Pantel 
Figures 1 and 2 illustrate the refinement precision and recall for each semantic tag. 
Both annotators have agreed on 819 verb pairs, and we examined performance on 
these. Figure 3 shows a higher precision on these pairs as compared to the overall set, 
illustrating that what is easier for the annotators is easier for the system. 
4.3   Observations on Refining Other Relations 
We have briefly investigated refining other semantic relations in VERBOCEAN. The 
extent of the evaluation was limited by availability of human judgments. We ran-
domly sampled 100 pairs from DIRT and presented the classifications to three human 
judges for evaluation [2]. 
Of the 100 pairs, 66 were identified to have a relation. We applied our refinement 
algorithm to VERBOCEAN and inspected the output. On the 37 relations that 
VERBOCEAN got wrong, our system identified six of them. On the remaining 29 that 
VERBOCEAN got correct, only one was identified as incorrect (false positive). Hence, 
on the task of identifying incorrect relations in VERBOCEAN, our system has a preci-
sion of 85.7%, where precision is defined as the percentage of correctly identified 
erroneous relations. However, it only achieved a recall of 16.2%, where recall is the 
percentage of erroneous relations that our system identified. Table 4 presents the 
relations that were refined by our system. The first two columns show the verb pair 
while the next two columns show the original relation in VERBOCEAN. 
Table 4. Seven relations in VERBOCEAN refined by a small test run on other relations 
Verb 1 Verb 2 
VERBOCEAN 
Relation 
Refinement 
Relation 
Judge 1 Relation Judge 2 Relation Judge 3 Relation 
attach use 
happens-before 
similar 
similar none none none 
bounce get weaker than  stronger than none none none 
dispatch defeat opposite none none none happens-before 
doom complicate opposite similar* none stronger-than stronger-than 
flatten level stronger than no relation* similar similar similar 
outlaw codify similar opposite none none opposition 
privatize improve happens-before none happens-before happens-before happens-before 
* only revision of relation to its opposite or ?none? was attempted here 
4.4   Discussion 
Our evaluation focused on the presence or absence of relations after refinement, with-
out exploiting the fact that our framework also updates confidences in a given rela-
tion. The additional information about confidence can benefit probabilistic inference 
approaches (e.g., [3]). 
Possible extensions to the algorithm include a more elaborate inference from graph 
structure, for example treating the absence of certain paths as counter-evidence. Sup-
pose that relations A happens-before B and A similar A' were detected, but the rela-
tion A' happens-before B was not. Then, the absence of a path 
 Global Path-Based Refinement of Noisy Graphs Applied to Verb Semantics 803 
A similar A' happens-before B 
suggests the absence of A happens-before B. 
Other important avenues of future work include applying our framework to other 
relations (e.g., strength in VERBOCEAN) and to better characterize the refinement 
thresholds. 
5   Conclusions 
We presented a method for refining edges in graphs by leveraging the semantics of 
multiple noisy paths. We re-estimated the presence of an edge between a pair of nodes 
from the evidence provided by multiple indirect paths between the nodes. Our ap-
proach applies to a variety of relation types: transitive symmetric, transitive asymmet-
ric, and relations inducing equivalence classes. We applied our model to refining 
temporal verb relations in a semantic resource called VERBOCEAN. Experiments 
showed a 16.1% error reduction after refinement. On the 72% refinement decisions 
that it was most confident, the error reduction was 28.3%. 
The usefulness of a semantic resource is highly dependent on its quality, which is 
often poor in automatically mined resources. With graph refinement frameworks such 
as the one presented here, many of these resources may be improved automatically. 
References 
1. Berland, M. and E. Charniak, 1999. Finding parts in very large corpora. In ACL-1999. pp. 
57-64. College Park, MD. 
2. Chklovski, T., and Pantel, P. 2004. VERBOCEAN: Mining the Web for Fine-Grained 
Semantic Verb Relations. In Proceedings of 2004 Conference on Empirical Methods in 
Natural Language Processing (EMNLP 2004), Barcelona, Spain, July 25-26. 
3. Domingos, P. and Richardson, M. 2004. Markov Logic: A unifying framework for 
statistical relational learning. In Proceedings of ICML Workshop on Statistical Relational 
Learning and its Connections to Other Fields. Banff, Canada. 
4. Lin, D. and Pantel, P. 2001. Discovery of inference rules for question answering. Natural 
Language Engineering, 7(4):343-360. 
5. Pantel, P. and Ravichandran, D. 2004. Automatically labeling semantic classes. In 
Proceedings HLT/NAACL-04. pp. 321-328. Boston, MA. 
6. Shinzato, K. and Torisawa, K. 2004. Acquiring hyponymy relations from web documents. 
In Proceedings of HLT-NAACL-2004. pp. 73-80. Boston, MA. 
7. Siegel, S. and Castellan Jr., N. 1988. Nonparametric Statistics for the Behavioral Sciences. 
McGraw-Hill. 
Proceedings of NAACL HLT 2007, pages 564?571,
Rochester, NY, April 2007. c?2007 Association for Computational Linguistics
ISP: Learning Inferential Selectional Preferences 
 
Patrick Pantel?, Rahul Bhagat?, Bonaventura Coppola?, 
Timothy Chklovski?, Eduard Hovy? 
?Information Sciences Institute 
University of Southern California 
Marina del Rey, CA 
{pantel,rahul,timc,hovy}@isi.edu
?ITC-Irst and University of Trento 
Via Sommarive, 18 ? Povo 38050  
Trento, Italy 
coppolab@itc.it 
  
Abstract 
Semantic inference is a key component 
for advanced natural language under-
standing. However, existing collections of 
automatically acquired inference rules 
have shown disappointing results when 
used in applications such as textual en-
tailment and question answering. This pa-
per presents ISP, a collection of methods 
for automatically learning admissible ar-
gument values to which an inference rule 
can be applied, which we call inferential 
selectional preferences, and methods for 
filtering out incorrect inferences. We 
evaluate ISP and present empirical evi-
dence of its effectiveness. 
1 Introduction 
Semantic inference is a key component for ad-
vanced natural language understanding. Several 
important applications are already relying heavily 
on inference, including question answering 
(Moldovan et al 2003; Harabagiu and Hickl 2006), 
information extraction (Romano et al 2006), and 
textual entailment (Szpektor et al 2004). 
In response, several researchers have created re-
sources for enabling semantic inference. Among 
manual resources used for this task are WordNet 
(Fellbaum 1998) and Cyc (Lenat 1995). Although 
important and useful, these resources primarily 
contain prescriptive inference rules such as ?X di-
vorces Y ? X married Y?. In practical NLP appli-
cations, however, plausible inference rules such as 
?X married Y? ? ?X dated Y? are very useful. This, 
along with the difficulty and labor-intensiveness of 
generating exhaustive lists of rules, has led re-
searchers to focus on automatic methods for build-
ing inference resources such as inference rule 
collections (Lin and Pantel 2001; Szpektor et al 
2004) and paraphrase collections (Barzilay and 
McKeown 2001). 
Using these resources in applications has been 
hindered by the large amount of incorrect infer-
ences they generate, either because of altogether 
incorrect rules or because of blind application of 
plausible rules without considering the context of 
the relations or the senses of the words. For exam-
ple, consider the following sentence: 
Terry Nichols was charged by federal prosecutors for murder 
and conspiracy in the Oklahoma City bombing. 
and an inference rule such as: 
 X is charged by Y ? Y announced the arrest of X (1) 
Using this rule, we can infer that ?federal prosecu-
tors announced the arrest of Terry Nichols?. How-
ever, given the sentence: 
Fraud was suspected when accounts were charged by CCM 
telemarketers without obtaining consumer authorization. 
the plausible inference rule (1) would incorrectly 
infer that ?CCM telemarketers announced the ar-
rest of accounts?. 
This example depicts a major obstacle to the ef-
fective use of automatically learned inference 
rules. What is missing is knowledge about the ad-
missible argument values for which an inference 
rule holds, which we call Inferential Selectional 
Preferences. For example, inference rule (1) 
should only be applied if X is a Person and Y is a 
Law Enforcement Agent or a Law Enforcement 
Agency. This knowledge does not guarantee that 
the inference rule will hold, but, as we show in this 
paper, goes a long way toward filtering out errone-
ous applications of rules. 
In this paper, we propose ISP, a collection of 
methods for learning inferential selectional prefer-
ences and filtering out incorrect inferences. The 
564
presented algorithms apply to any collection of 
inference rules between binary semantic relations, 
such as example (1). ISP derives inferential selec-
tional preferences by aggregating statistics of in-
ference rule instantiations over a large corpus of 
text. Within ISP, we explore different probabilistic 
models of selectional preference to accept or reject 
specific inferences. We present empirical evidence 
to support the following main contribution: 
Claim: Inferential selectional preferences can be 
automatically learned and used for effectively fil-
tering out incorrect inferences. 
2 Previous Work 
Selectional preference (SP) as a foundation for 
computational semantics is one of the earliest top-
ics in AI and NLP, and has its roots in (Katz and 
Fodor 1963).  Overviews of NLP research on this 
theme are (Wilks and Fass 1992), which includes 
the influential theory of Preference Semantics by 
Wilks, and more recently (Light and Greiff 2002). 
Rather than venture into learning inferential 
SPs, much previous work has focused on learning 
SPs for simpler structures. Resnik (1996), the 
seminal paper on this topic, introduced a statistical 
model for learning SPs for predicates using an un-
supervised method. 
Learning SPs often relies on an underlying set of 
semantic classes, as in both Resnik?s and our ap-
proach. Semantic classes can be specified manu-
ally or derived automatically. Manual collections 
of semantic classes include the hierarchies of 
WordNet (Fellbaum 1998), Levin verb classes 
(Levin 1993), and FrameNet (Baker et al 1998). 
Automatic derivation of semantic classes can take 
a variety of approaches, but often uses corpus 
methods and the Distributional Hypothesis (Harris 
1964) to automatically cluster similar entities into 
classes, e.g. CBC (Pantel and Lin 2002). In this 
paper, we experiment with two sets of semantic 
classes, one from WordNet and one from CBC. 
Another thread related to our work includes ex-
tracting from text corpora paraphrases (Barzilay 
and McKeown 2001) and inference rules, e.g. 
TEASE1 (Szpektor et al 2004) and DIRT (Lin and 
Pantel 2001). While these systems differ in their 
approaches, neither provides for the extracted in-
                                                     
1 Some systems refer to inferences they extract as entail-
ments; the two terms are sometimes used interchangeably. 
ference rules to hold or fail based on SPs. Zanzotto 
et al (2006) recently explored a different interplay 
between SPs and inferences. Rather than examine 
the role of SPs in inferences, they use SPs of a par-
ticular type to derive inferences.  For instance the 
preference of win for the subject player, a nomi-
nalization of play, is used to derive that ?win ? 
play?. Our work can be viewed as complementary 
to the work on extracting semantic inferences and 
paraphrases, since we seek to refine when a given 
inference applies, filtering out incorrect inferences. 
3 Selectional Preference Models 
The aim of this paper is to learn inferential selec-
tional preferences for filtering inference rules. 
Let pi ? pj be an inference rule where p is a bi-
nary semantic relation between two entities x and 
y. Let ?x, p, y? be an instance of relation p. 
Formal task definition: Given an inference rule 
 pi ? pj and the instance ?x, pi, y?, our task is to 
determine if ?x, pj, y? is valid. 
Consider the example in Section 1 where we 
have the inference rule ?X is charged by Y? ? ?Y 
announced the arrest of X?. Our task is to auto-
matically determine that ?federal prosecutors an-
nounced the arrest of Terry Nichols? (i.e., 
?Terry Nichols, pj, federal prosecutors?) is valid 
but that ?CCM telemarketers announced the arrest 
of accounts? is invalid. 
Because the semantic relations p are binary, the 
selectional preferences on their two arguments may 
be either considered jointly or independently. For 
example, the relation p = ?X is charged by Y? 
could have joint SPs: 
 ?Person, Law Enforcement Agent? 
 ?Person, Law Enforcement Agency?  (2) 
 ?Bank Account, Organization? 
or independent SPs: 
 ?Person, *? 
 ?*, Organization? (3) 
 ?*, Law Enforcement Agent? 
This distinction between joint and independent 
selectional preferences constitutes the difference 
between the two models we present in this section. 
The remainder of this section describes the ISP 
approach. In Section 3.1, we describe methods for 
automatically determining the semantic contexts of 
each single relation?s selectional preferences. Sec-
tion 3.2 uses these for developing our inferential 
565
selectional preference models. Finally, we propose 
inference filtering algorithms in Section 3.3. 
3.1 Relational Selectional Preferences 
Resnik (1996) defined the selectional preferences 
of a predicate as the semantic classes of the words 
that appear as its arguments. Similarly, we define 
the relational selectional preferences of a binary 
semantic relation pi as the semantic classes C(x) of 
the words that can be instantiated for x and as the 
semantic classes C(y) of the words that can be in-
stantiated for y. 
The semantic classes C(x) and C(y) can be ob-
tained from a conceptual taxonomy as proposed in 
(Resnik 1996), such as WordNet, or from the 
classes extracted from a word clustering algorithm 
such as CBC (Pantel and Lin 2002). For example, 
given the relation ?X is charged by Y?, its rela-
tional selection preferences from WordNet could 
be {social_group, organism, state?} for X and 
{authority, state, section?} for Y. 
Below we propose joint and independent mod-
els, based on a corpus analysis, for automatically 
determining relational selectional preferences. 
Model 1: Joint Relational Model (JRM) 
Our joint model uses a corpus analysis to learn SPs 
for binary semantic relations by considering their 
arguments jointly, as in example (2). 
Given a large corpus of English text, we first 
find the occurrences of each semantic relation p. 
For each instance ?x, p, y?, we retrieve the sets C(x) 
and C(y) of the semantic classes that x and y be-
long to and accumulate the frequencies of the tri-
ples ?c(x), p, c(y)?, where c(x) ? C(x) and  
c(y) ? C(y)2. 
Each triple ?c(x), p, c(y)? is a candidate selec-
tional preference for p. Candidates can be incorrect 
when: a) they were generated from the incorrect 
sense of a polysemous word; or b) p does not hold 
for the other words in the semantic class. 
Intuitively, we have more confidence in a par-
ticular candidate if its semantic classes are closely 
associated given the relation p. Pointwise mutual 
information (Cover and Thomas 1991) is a com-
monly used metric for measuring this association 
strength between two events e1 and e2: 
                                                     
2 In this paper, the semantic classes C(x) and C(y) are ex-
tracted from WordNet and CBC (described in Section 4.2).  
 ( )( ) ( )21
21
21
,
log);(
ePeP
eeP
eepmi =  (3.1) 
We define our ranking function as the strength 
of association between two semantic classes, cx and 
cy3, given the relation p: 
 ( ) ( )( ) ( )pcPpcP pccPpcpcpmi yx yxyx
,
log; =  (3.2) 
Let |cx, p, cy| denote the frequency of observing 
the instance ?c(x), p, c(y)?. We estimate the prob-
abilities of Equation 3.2 using maximum likeli-
hood estimates over our corpus: 
( ) ?? ?= ,, ,,ppcpcP xx
 ( ) ???= ,, ,, pcppcP yy  ( ) ??= ,, ,,, p cpcpccP yxyx   (3.3) 
Similarly to (Resnik 1996), we estimate the 
above frequencies using: 
( )??
?=?
xcw
x wC
pw
pc
,,
,,
 
( )??
?=?
ycw
y wC
wp
cp
,,
,,
 
( ) ( )??? ?= yx cwcwyx wCwC
wpw
cpc
21 , 21
21 ,,,,
 
where |x, p, y| denotes the frequency of observing 
the instance ?x, p, y? and |C(w)| denotes the number 
of classes to which word w belongs. |C(w)| distrib-
utes w?s mass equally to all of its senses cw. 
Model 2: Independent Relational Model (IRM) 
Because of sparse data, our joint model can miss 
some correct selectional preference pairs. For ex-
ample, given the relation  
 Y announced the arrest of X 
we may find occurrences from our corpus of the 
particular class ?Money Handler? for X and ?Law-
yer? for Y, however we may never see both of 
these classes co-occurring even though they would 
form a valid relational selectional preference. 
To alleviate this problem, we propose a second 
model that is less strict by considering the argu-
ments of the binary semantic relations independ-
ently, as in example (3). 
Similarly to JRM, we extract each instance  
?x, p, y? of each semantic relation p and retrieve the 
set of semantic classes C(x) and C(y) that x and y 
belong to, accumulating the frequencies of the tri-
ples ?c(x), p, *? and ?*, p, c(y)?, where  
c(x) ? C(x) and c(y) ? C(y). 
All tuples ?c(x), p, *? and ?*, p, c(y)? are candi-
date selectional preferences for p. We rank candi-
dates by the probability of the semantic class given 
the relation p, according to Equations 3.3. 
                                                     
3 cx and cy are shorthand for c(x) and c(y) in our equations. 
566
3.2 Inferential Selectional Preferences 
Whereas in Section 3.1 we learned selectional 
preferences for the arguments of a relation p, in 
this section we learn selectional preferences for the 
arguments of an inference rule pi ? pj. 
Model 1: Joint Inferential Model (JIM) 
Given an inference rule pi ? pj, our joint model 
defines the set of inferential SPs as the intersection 
of the relational SPs for pi and pj, as defined in the 
Joint Relational Model (JRM). For example, sup-
pose relation pi = ?X is charged by Y? gives the 
following SP scores under the JRM: 
 ?Person, pi, Law Enforcement Agent? = 1.45 
 ?Person, pi, Law Enforcement Agency? = 1.21  
 ?Bank Account, pi, Organization? = 0.97 
and that pj = ?Y announced the arrest of X? gives 
the following SP scores under the JRM: 
 ?Law Enforcement Agent, pj, Person? = 2.01 
 ?Reporter, pj, Person? = 1.98  
 ?Law Enforcement Agency, pj, Person? = 1.61 
The intersection of the two sets of SPs forms the 
candidate inferential SPs for the inference pi ? pj: 
 ?Law Enforcement Agent, Person? 
 ?Law Enforcement Agency, Person? 
We rank the candidate inferential SPs according 
to three ways to combine their relational SP scores, 
using the minimum, maximum, and average of the 
SPs. For example, for ?Law Enforcement Agent, 
Person?, the respective scores would be 1.45, 2.01, 
and 1.73. These different ranking strategies pro-
duced nearly identical results in our experiments, 
as discussed in Section 5. 
Model 2: Independent Inferential Model (IIM) 
Our independent model is the same as the joint 
model above except that it computes candidate in-
ferential SPs using the Independent Relational 
Model (IRM) instead of the JRM. Consider the 
same example relations pi and pj from the joint 
model and suppose that the IRM gives the follow-
ing relational SP scores for pi: 
 ?Law Enforcement Agent, pi, *? = 3.43 
 ?*, pi, Person? = 2.17  
 ?*, pi, Organization? = 1.24 
and the following relational SP scores for pj: 
 ?*, pj, Person? = 2.87 
 ?Law Enforcement Agent, pj, *? = 1.92  
 ?Reporter, pj, *? = 0.89 
The intersection of the two sets of SPs forms the 
candidate inferential SPs for the inference pi ? pj: 
 ?Law Enforcement Agent, *? 
 ?*, Person?  
We use the same minimum, maximum, and av-
erage ranking strategies as in JIM. 
3.3 Filtering Inferences 
Given an inference rule pi ? pj and the instance  
?x, pi, y?, the system?s task is to determine whether 
?x, pj, y? is valid. Let C(w) be the set of semantic 
classes c(w) to which word w belongs. Below we 
present three filtering algorithms which range from 
the least to the most permissive: 
? ISP.JIM, accepts the inference ?x, pj, y? if the 
inferential SP ?c(x), pj, c(y)? was admitted by the 
Joint Inferential Model for some c(x) ? C(x) and 
c(y) ? C(y). 
? ISP.IIM.?, accepts the inference ?x, pj, y? if the 
inferential SPs ?c(x), pj, *? AND ?*, pj, c(y)? were 
admitted by the Independent Inferential Model 
for some c(x) ? C(x) and c(y) ? C(y) . 
? ISP.IIM.?, accepts the inference ?x, pj, y? if the 
inferential SP ?c(x), pj, *? OR ?*, pj, c(y)? was 
admitted by the Independent Inferential Model 
for some c(x) ? C(x) and c(y) ? C(y) . 
Since both JIM and IIM use a ranking score in 
their inferential SPs, each filtering algorithm can 
be tuned to be more or less strict by setting an ac-
ceptance threshold on the ranking scores or by se-
lecting only the top ? percent highest ranking SPs. 
In our experiments, reported in Section 5, we 
tested each model using various values of ?. 
4 Experimental Methodology 
This section describes the methodology for testing 
our claim that inferential selectional preferences 
can be learned to filter incorrect inferences. 
Given a collection of inference rules of the form 
pi ? pj, our task is to determine whether a particu-
lar instance ?x, pj, y? holds given that ?x, pi, y? 
holds4. In the next sections, we describe our collec-
tion of inference rules, the semantic classes used 
for forming selectional preferences, and evaluation 
criteria for measuring the filtering quality. 
                                                     
4 Recall that the inference rules we consider in this paper are 
not necessary strict logical inference rules, but plausible in-
ference rules; see Section 3. 
567
4.1 Inference Rules 
Our models for learning inferential selectional 
preferences can be applied to any collection of in-
ference rules between binary semantic relations. In 
this paper, we focus on the inference rules con-
tained in the DIRT resource (Lin and Pantel 2001). 
DIRT consists of over 12 million rules which were 
extracted from a 1GB newspaper corpus (San Jose 
Mercury, Wall Street Journal and AP Newswire 
from the TREC-9 collection). For example, here 
are DIRT?s top 3 inference rules for ?X solves Y?: 
 ?Y is solved by X?, ?X resolves Y?, ?X finds a solution to Y? 
4.2 Semantic Classes 
The choice of semantic classes is of great impor-
tance for selectional preference. One important 
aspect is the granularity of the classes. Too general 
a class will provide no discriminatory power while 
too fine-grained a class will offer little generaliza-
tion and apply in only extremely few cases. 
The absence of an attested high-quality set of 
semantic classes for this task makes discovering 
preferences difficult. Since many of the criteria for 
developing such a set are not even known, we de-
cided to experiment with two very different sets of 
semantic classes, in the hope that in addition to 
learning semantic preferences, we might also un-
cover some clues for the eventual decisions about 
what makes good semantic classes in general. 
Our first set of semantic classes was directly ex-
tracted from the output of the CBC clustering algo-
rithm (Pantel and Lin 2002). We applied CBC to 
the TREC-9 and TREC-2002 (Aquaint) newswire 
collections consisting of over 600 million words. 
CBC generated 1628 noun concepts and these were 
used as our semantic classes for SPs. 
Secondly, we extracted semantic classes from 
WordNet 2.1 (Fellbaum 1998). In the absence of 
any externally motivated distinguishing features 
(for example, the Basic Level categories from Pro-
totype Theory, developed by Eleanor Rosch 
(1978)), we used the simple but effective method 
of manually truncating the noun synset hierarchy5 
and considering all synsets below each cut point as 
part of the semantic class at that node. To select 
the cut points, we inspected several different hier-
archy levels and found the synsets at a depth of 4 
                                                     
5 Only nouns are considered since DIRT semantic relations 
connect only nouns. 
to form the most natural semantic classes. Since 
the noun hierarchy in WordNet has an average 
depth of 12, our truncation created a set of con-
cepts considerably coarser-grained than WordNet 
itself. The cut produced 1287 semantic classes, a 
number similar to the classes in CBC. To properly 
test WordNet as a source of semantic classes for 
our selectional preferences, we would need to ex-
periment with different extraction algorithms. 
4.3 Evaluation Criteria 
The goal of the filtering task is to minimize false 
positives (incorrectly accepted inferences) and 
false negatives (incorrectly rejected inferences). A 
standard methodology for evaluating such tasks is 
to compare system filtering results with a gold 
standard using a confusion matrix. A confusion 
matrix captures the filtering performance on both 
correct and incorrect inferences: 
  
where A represents the number of correct instances 
correctly identified by the system, D represents the 
number of incorrect instances correctly identified 
by the system, B represents the number of false 
positives and C represents the number of false 
negatives. To compare systems, three key meas-
ures are used to summarize confusion matrices: 
? Sensitivity, defined as CA
A
+ , captures a filter?s 
probability of accepting correct inferences; 
? Specificity, defined as DB
D
+ , captures a filter?s 
probability of rejecting incorrect inferences; 
? Accuracy, defined as DCBA
DA
+++
+ , captures the 
probability of a filter being correct. 
5 Experimental Results 
In this section, we provide empirical evidence to 
support the main claim of this paper. 
Given a collection of DIRT inference rules of 
the form pi ? pj, our experiments, using the meth-
odology of Section 4, evaluate the capability of our 
ISP models for determining if ?x, pj, y? holds given 
that ?x, pi, y? holds. 
GOLD STANDARD   
1 0 
1 A B 
SY
ST
E
M
 
0 C D 
568
5.1 Experimental Setup 
Model Implementation 
For each filtering algorithm in Section 3.3, ISP.JIM, 
ISP.IIM.?, and ISP.IIM.?, we trained their probabil-
istic models using corpus statistics extracted from 
the 1999 AP newswire collection (part of the 
TREC-2002 Aquaint collection) consisting of ap-
proximately 31 million words. We used the Mini-
par parser (Lin 1993) to match DIRT patterns in 
the text. This permits exact matches since DIRT 
inference rules are built from Minipar parse trees. 
For each system, we experimented with the dif-
ferent ways of combining relational SP scores: 
minimum, maximum, and average (see Section 
3.2). Also, we experimented with various values 
for the ? parameter described in Section 3.3. 
Gold Standard Construction 
In order to compute the confusion matrices de-
scribed in Section 4.3, we must first construct a 
representative set of inferences and manually anno-
tate them as correct or incorrect. 
We randomly selected 100 inference rules of the 
form pi ? pj from DIRT. For each pattern pi, we 
then extracted its instances from the Aquaint 1999 
AP newswire collection (approximately 22 million 
words), and randomly selected 10 distinct in-
stances, resulting in a total of 1000 instances. For 
each instance of pi, applying DIRT?s inference rule 
would assert the instance ?x, pj, y?. Our evaluation 
tests how well our models can filter these so that 
only correct inferences are made. 
To form the gold standard, two human judges 
were asked to tag each instance ?x, pj, y? as correct 
or incorrect. For example, given a randomly se-
lected inference rule ?X is charged by Y ? Y an-
nounced the arrest of X? and the instance ?Terry 
Nichols was charged by federal prosecutors?, the 
judges must determine if the instance ?federal 
prosecutors, Y announced the arrest of X, Terry 
Nichols? is correct. The judges were asked to con-
sider the following two criteria for their decision: 
? ?x, pj, y? is a semantically meaningful instance; 
? The inference pi ? pj holds for this instance. 
Judges found that annotation decisions can range 
from trivial to difficult. The differences often were 
in the instances for which one of the judges fails to 
see the right context under which the inference 
could hold. To minimize disagreements, the judges 
went through an extensive round of training. 
To that end, the 1000 instances ?x, pj, y? were 
split into DEV and TEST sets, 500 in each. The 
two judges trained themselves by annotating DEV 
together. The TEST set was then annotated sepa-
rately to verify the inter-annotator agreement and 
to verify whether the task is well-defined. The 
kappa statistic (Siegel and Castellan Jr. 1988) was 
? = 0.72. For the 70 disagreements between the 
judges, a third judge acted as an adjudicator. 
Baselines 
We compare our ISP algorithms to the following 
baselines: 
? B0: Rejects all inferences; 
? B1: Accepts all inferences; 
? Rand: Randomly accepts or rejects inferences. 
One alternative to our approach is admit instances 
on the Web using literal search queries. We inves-
tigated this technique but discarded it due to subtle 
yet critical issues with pattern canonicalization that 
resulted in rejecting nearly all inferences. How-
ever, we are investigating other ways of using Web 
corpora for this task. 
Table 1. Filtering quality of best performing systems according to the evaluation criteria defined in Section 4.3 on 
the TEST set ? the reported systems were selected based on the Accuracy criterion on the DEV set. 
PARAMETERS SELECTED FROM DEV SET 
SYSTEM 
RANKING STRATEGY ? (%) 
SENSITIVITY 
(95% CONF) 
SPECIFICITY 
(95% CONF) 
ACCURACY 
(95% CONF) 
B0 - - 0.00?0.00 1.00?0.00 0.50?0.04 
B1 - - 1.00?0.00 0.00?0.00 0.49?0.04 
Random - - 0.50?0.06 0.47?0.07 0.50?0.04 
ISP.JIM maximum 100 0.17?0.04 0.88?0.04 0.53?0.04 
ISP.IIM.? maximum 100 0.24?0.05 0.84?0.04 0.54?0.04 CBC 
ISP.IIM.? maximum 90 0.73?0.05 0.45?0.06 0.59?0.04? 
ISP.JIM minimum 40 0.20?0.06 0.75?0.06 0.47?0.04 
ISP.IIM.? minimum 10 0.33?0.07 0.77?0.06 0.55?0.04 WordNet 
ISP.IIM.? minimum 20 0.87?0.04 0.17?0.05 0.51?0.05 
? Indicates statistically significant results (with 95% confidence) when compared with all baseline systems using pairwise t-test. 
569
5.2 Filtering Quality 
For each ISP algorithm and parameter combina-
tion, we constructed a confusion matrix on the de-
velopment set and computed the system sensitivity, 
specificity and accuracy as described in Section 
4.3. This resulted in 180 experiments on the devel-
opment set. For each ISP algorithm and semantic 
class source, we selected the best parameter com-
binations according to the following criteria: 
? Accuracy: This system has the best overall abil-
ity to correctly accept and reject inferences. 
? 90%-Specificity: Several formal semantics and 
textual entailment researchers have commented 
that inference rule collections like DIRT are dif-
ficult to use due to low precision. Many have 
asked for filtered versions that remove incorrect 
inferences even at the cost of removing correct 
inferences. In response, we show results for the 
system achieving the best sensitivity while main-
taining at least 90% specificity on the DEV set. 
We evaluated the selected systems on the TEST 
set. Table 1 summarizes the quality of the systems 
selected according to the Accuracy criterion. The 
best performing system, ISP.IIM.?, performed  sta-
tistically significantly better than all three base-
lines. The best system according to the 90%-
Specificity criteria was ISP.JIM, which coinciden-
tally has the highest accuracy for that model as 
shown in Table 16. This result is very promising 
for researchers that require highly accurate infer-
ence rules since they can use ISP.JIM and expect to 
recall 17% of the correct inferences by only ac-
cepting false positives 12% of the time. 
Performance and Error Analysis 
Figures 1a) and 1b) present the full confusion ma-
trices for the most accurate and highly specific sys-
tems, with both systems selected on the DEV set. 
The most accurate system was ISP.IIM.?, which is 
the most permissive of the algorithms. This sug-
                                                     
6 The reported sensitivity of ISP.Joint in Table 1 is below 
90%, however it achieved 90.7% on the DEV set. 
gests that a larger corpus for learning SPs may be 
needed to support stronger performance on the 
more restrictive methods. The system in Figure 
1b), selected for maximizing sensitivity while 
maintaining high specificity, was 70% correct in 
predicting correct inferences. 
Figure 2 illustrates the ROC curve for all our 
systems and parameter combinations on the TEST 
set. ROC curves plot the true positive rate against 
the false positive rate. The near-diagonal line plots 
the three baseline systems. 
Several trends can be observed from this figure. 
First, systems using the semantic classes from 
WordNet tend to perform less well than systems 
using CBC classes. As discussed in Section 4.2, we 
used a very simplistic extraction of semantic 
classes from WordNet. The results in Figure 2 
serve as a lower bound on what could be achieved 
with a better extraction from WordNet. Upon in-
spection of instances that WordNet got incorrect 
but CBC got correct, it seemed that CBC had a 
much higher lexical coverage than WordNet. For 
example, several of the instances contained proper 
names as either the X or Y argument (WordNet has 
poor proper name coverage). When an argument is 
not covered by any class, the inference is rejected. 
Figure 2 also illustrates how our three different 
ISP algorithms behave. The strictest filters, ISP.JIM 
and ISP.IIM.?, have the poorest overall perform-
ance but, as expected, have a generally very low 
rate of false positives. ISP.IIM.?, which is a much 
more permissive filter because it does not require 
ROC on the TEST Set
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1
1-Specificity
S
en
si
tiv
ity
Baselines WordNet CBC ISP.JIM ISP.IIM.AND ISP.IIM.OR
Figure 2. ROC curves for our systems on TEST. 
GOLD STANDARD a)  
1 0 
1 184 139 
SY
ST
E
M
 
0 63 114 
GOLD STANDARD b)  
1 0 
1 42 28 
SY
ST
E
M
 
0 205 225 
Figure 1. Confusion matrices for a) ISP.IIM.? ? best 
Accuracy; and b) ISP.JIM ? best 90%-Specificity.
570
both arguments of a relation to match, has gener-
ally many more false positives but has an overall 
better performance. 
We did not include in Figure 2 an analysis of the 
minimum, maximum, and average ranking strate-
gies presented in Section 3.2 since they generally 
produced nearly identical results. 
For the most accurate system, ISP.IIM.?, we ex-
plored the impact of the cutoff threshold ? on the 
sensitivity, specificity, and accuracy, as shown in 
Figure 3. Rather than step the values by 10% as we 
did on the DEV set, here we stepped the threshold 
value by 2% on the TEST set. The more permis-
sive values of ? increase sensitivity at the expense 
of specificity. Interestingly, the overall accuracy 
remained fairly constant across the entire range of 
?, staying within 0.05 of the maximum of 0.62 
achieved at ?=30%. 
Finally, we manually inspected several incorrect 
inferences that were missed by our filters. A com-
mon source of errors was due to the many incorrect 
?antonymy? inference rules generated by DIRT, 
such as ?X is rejected in Y???X is accepted in Y?. 
This recognized problem in DIRT occurs because 
of the distributional hypothesis assumption used to 
form the inference rules. Our ISP algorithms suffer 
from a similar quandary since, typically, antony-
mous relations take the same sets of arguments for 
X (and Y). For these cases, ISP algorithms learn 
many selectional preferences that accept the same 
types of entities as those that made DIRT learn the 
inference rule in the first place, hence ISP will not 
filter out many incorrect inferences. 
6 Conclusion 
We presented algorithms for learning what we call 
inferential selectional preferences, and presented 
evidence that learning selectional preferences can 
be useful in filtering out incorrect inferences. Fu-
ture work in this direction includes further explora-
tion of the appropriate inventory of semantic 
classes used as SP?s. This work constitutes a step 
towards better understanding of the interaction of 
selectional preferences and inferences, bridging 
these two aspects of semantics. 
References 
Barzilay, R.; and McKeown, K.R. 2001.Extracting Paraphrases from a 
Parallel Corpus. In Proceedings of ACL 2001. pp. 50?57. Toulose, 
France. 
Baker, C.F.; Fillmore, C.J.; and Lowe, J.B. 1998. The Berkeley 
FrameNet Project. In Proceedings of COLING/ACL 1998.  pp. 86-
90. Montreal, Canada. 
Cover, T.M. and Thomas, J.A. 1991. Elements of Information Theory. 
John Wiley & Sons. 
Fellbaum, C. 1998. WordNet: An Electronic Lexical Database. MIT 
Press. 
Harabagiu, S.; and Hickl, A. 2006. Methods for Using Textual 
Entailment in Open-Domain Question Answering. In Proceedings 
of ACL 2006.  pp. 905-912. Sydney, Australia. 
Katz, J.; and Fodor, J.A. 1963. The Structure of a Semantic Theory. 
Language, vol 39. pp.170?210.  
Lenat, D. 1995. CYC: A large-scale investment in knowledge 
infrastructure. Communications of the ACM, 38(11):33?38. 
Levin, B. 1993. English Verb Classes and Alternations: A Preliminary 
Investigation. University of Chicago Press, Chicago, IL. 
Light, M. and Greiff, W.R. 2002. Statistical Models for the Induction 
and Use of Selectional Preferences. Cognitive Science,26:269?281. 
Lin, D. 1993. Parsing Without OverGeneration. In Proceedings of  
ACL-93. pp. 112-120. Columbus, OH. 
Lin, D. and Pantel, P. 2001. Discovery of Inference Rules for 
Question Answering. Natural Language Engineering 7(4):343-360. 
Moldovan, D.I.; Clark, C.; Harabagiu, S.M.; Maiorano, S.J. 2003. 
COGEX: A Logic Prover for Question Answering. In Proceedings 
of HLT-NAACL-03. pp. 87-93. Edmonton, Canada. 
Pantel, P. and Lin, D. 2002. Discovering Word Senses from Text. In 
Proceedings of KDD-02. pp. 613-619. Edmonton, Canada. 
Resnik, P. 1996. Selectional Constraints: An Information-Theoretic 
Model and its Computational Realization. Cognition, 61:127?159. 
Romano, L.; Kouylekov, M.; Szpektor, I.; Dagan, I.; Lavelli, A. 2006. 
Investigating a Generic Paraphrase-Based Approach for Relation 
Extraction. In EACL-2006. pp. 409-416. Trento, Italy. 
Rosch, E. 1978. Human Categorization. In E. Rosch and B.B. Lloyd 
(eds.) Cognition and Categorization. Hillsdale, NJ: Erlbaum.  
Siegel, S. and Castellan Jr., N. J. 1988. Nonparametric Statistics for 
the Behavioral Sciences. McGraw-Hill. 
Szpektor, I.; Tanev, H.; Dagan, I.; and Coppola, B. 2004. Scaling 
web-based acquisition of entailment relations. In Proceedings of 
EMNLP 2004. pp. 41-48. Barcelona,Spain. 
Wilks, Y.; and Fass, D. 1992. Preference Semantics: a family history. 
Computing and Mathematics with Applications, 23(2). A shorter 
version in the second edition of the Encyclopedia of Artificial 
Intelligence, (ed.) S. Shapiro. 
Zanzotto, F.M.; Pennacchiotti, M.; Pazienza, M.T. 2006. Discovering 
Asymmetric Entailment Relations between Verbs using Selectional 
Preferences. In COLING/ACL-06. pp. 849-856. Sydney, Australia. 
Figure 3. ISP.IIM.? (Best System)?s performance 
variation over different values for the ? threshold. 
ISP.IIM.OR (Best System)'s Performance vs. Tau-Thresholds
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0 10 20 30 40 50 60 70 80 90 100
Tau-Thresholds
Sensitivity Specificity Accuracy
571
The SENSEVAL?3 Multilingual English?Hindi Lexical Sample Task
Timothy Chklovski
Information Sciences Institute
University of Southern California
Marina del Rey, CA 90292
timc@isi.edu
Rada Mihalcea
Department of Computer Science
University of North Texas
Dallas, TX 76203
rada@cs.unt.edu
Ted Pedersen
Department of Computer Science
University of Minnesota
Duluth, MN 55812
tpederse@d.umn.edu
Amruta Purandare
Department of Computer Science
University of Minnesota
Duluth, MN 55812
pura0010@d.umn.edu
Abstract
This paper describes the English?Hindi Multilingual
lexical sample task in SENSEVAL?3. Rather than
tagging an English word with a sense from an En-
glish dictionary, this task seeks to assign the most
appropriate Hindi translation to an ambiguous tar-
get word. Training data was solicited via the Open
Mind Word Expert (OMWE) from Web users who
are fluent in English and Hindi.
1 Introduction
The goal of the MultiLingual lexical sample task
is to create a framework for the evaluation of sys-
tems that perform Machine Translation, with a fo-
cus on the translation of ambiguous words. The
task is very similar to the lexical sample task, ex-
cept that rather than using the sense inventory from
a dictionary we follow the suggestion of (Resnik and
Yarowsky, 1999) and use the translations of the tar-
get words into a second language. In this task for
SENSEVAL-3, the contexts are in English, and the
?sense tags? for the English target words are their
translations in Hindi.
This paper outlines some of the major issues that
arose in the creation of this task, and then describes
the participating systems and summarizes their re-
sults.
2 Open Mind Word Expert
The annotated corpus required for this task was
built using the Open Mind Word Expert system
(Chklovski and Mihalcea, 2002), adapted for mul-
tilingual annotations 1.
To overcome the current lack of tagged data and
the limitations imposed by the creation of such data
using trained lexicographers, the Open Mind Word
1Multilingual Open Mind Word Expert can be accessed at
http://teach-computers.org/word-expert/english-hindi
Expert system enables the collection of semantically
annotated corpora over the Web. Tagged examples
are collected using a Web-based application that al-
lows contributors to annotate words with their mean-
ings.
The tagging exercise proceeds as follows. For
each target word the system extracts a set of sen-
tences from a large textual corpus. These examples
are presented to the contributors, together with all
possible translations for the given target word. Users
are asked to select the most appropriate translation
for the target word in each sentence. The selection
is made using check-boxes, which list all possible
translations, plus two additional choices, ?unclear?
and ?none of the above.? Although users are encour-
aged to select only one translation per word, the se-
lection of two or more translations is also possible.
The results of the classification submitted by other
users are not presented to avoid artificial biases.
3 Sense Inventory Representation
The sense inventory used in this task is the set of
Hindi translations associated with the English words
in our lexical sample. Selecting an appropriate
English-Hindi dictionary was a major decision early
in the task, and it raised a number of interesting is-
sues.
We were unable to locate any machine readable
or electronic versions of English-Hindi dictionaries,
so it became apparent that we would need to manu-
ally enter the Hindi translations from printed mate-
rials. We briefly considered the use of Optical Char-
acter Recognition (OCR), but found that our avail-
able tools did not support Hindi. Even after deciding
to enter the Hindi translations manually, it wasn?t
clear how those words should be encoded. Hindi is
usually represented in Devanagari script, which has
a large number of possible encodings and no clear
standard has emerged as yet.
                                             Association for Computational Linguistics
                        for the Semantic Analysis of Text, Barcelona, Spain, July 2004
                 SENSEVAL-3: Third International Workshop on the Evaluation of Systems
We decided that Romanized or transliterated
Hindi text would be the the most portable encoding,
since it can be represented in standard ASCII text.
However, it turned out that the number of English?
Hindi bilingual dictionaries is much less than the
number of Hindi?English, and the number that use
transliterated text is smaller still.
Still, we located one promising candidate, the
English?Hindi Hippocrene Dictionary (Raker and
Shukla, 1996), which represents Hindi in a translit-
erated form. However, we found that many English
words only had two or three translations, making it
too coarse grained for our purposes2 .
In the end we selected the Chambers English?
Hindi dictionary (Awasthi, 1997), which is a high
quality bilingual dictionary that uses Devanagari
script. We identified 41 English words from the
Chambers dictionary to make up our lexical sam-
ple. Then one of the task organizers, who is
fluent in English and Hindi, manually transliter-
ated the approximately 500 Hindi translations of
the 41 English words in our lexical sample from
the Chambers dictionary into the ITRANS format
(http://www.aczone.com/itrans/). ITRANS software
was used to generate Unicode for display in the
OMWE interfaces, although the sense tags used in
the task data are the Hindi translations in transliter-
ated form.
4 Training and Test Data
The MultiLingual lexical sample is made up of 41
words: 18 nouns, 15 verbs, and 8 adjectives. This
sample includes English words that have varying de-
grees of polysemy as reflected in the number of pos-
sible Hindi translations, which range from a low of
3 to a high of 39.
Text samples made up of several hundred in-
stances for each of 31 of the 41 words were drawn
from the British National Corpus, while samples for
the other 10 words came from the SENSEVAL-2 En-
glish lexical sample data. The BNC data is in a
?raw? text form, where the part of speech tags have
been removed. However, the SENSEVAL-2 data in-
cludes the English sense?tags as determined by hu-
man taggers.
After gathering the instances for each word in
the lexical sample, we tokenized each instance and
removed those that contain collocations of the tar-
get word. For example, the training/test instances
for arm.n do not include examples for contact arm,
2We have made available transcriptions of the entries for
approximately 70 Hippocrene nouns, verbs, and adjectives
at http://www.d.umn.edu/?pura0010/hindi.html, although these
were not used in this task.
pickup arm, etc., but only examples that refer to arm
as a single lexical unit (not part of a collocation). In
our experience, disambiguation accuracy on collo-
cations of this sort is close to perfect, and we aimed
to concentrate the annotation effort on the more dif-
ficult cases.
The data was then annotated with Hindi transla-
tions by web volunteers using the Open Mind Word
Expert (bilingual edition). At various points in time
we offered gift certificates as a prize for the most
productive tagger in a given day, in order to spur
participation. A total of 40 volunteers contributed to
this task.
To create the test data we collected two indepen-
dent tags per instance, and then discarded any in-
stances where the taggers disagreed. Thus, each
instance that remains in the test data has complete
agreement between two taggers. For the training
data, we only collected one tag per instance, and
therefore this data may be noisy. Participating sys-
tems could choose to apply their own filtering meth-
ods to identify and remove the less reliably anno-
tated examples.
After tagging by the Web volunteers, there were
two data sets provided to task participants: one
where the English sense of the target word is un-
known, and another where it is known in both the
training and test data. These are referred to as the
translation only (t) data and the translation and sense
(ts) data, respectively. The t data is made up of in-
stances drawn from the BNC as described above,
while the ts data is made up of the instances from
SENSEVAL-2. Evaluations were run separately for
each of these two data sets, which we refer to as the
t and ts subtasks.
The t data contains 31 ambiguous words: 15
nouns, 10 verbs, and 6 adjectives. The ts data con-
tains 10 ambiguous words: 3 nouns, 5 verbs, and 2
adjectives, all of which have been used in the En-
glish lexical sample task of SENSEVAL-2. These
words, the number of possible translations, and the
number of training and test instances are shown in
Table 1. The total number of training instances in
the two sub-tasks is 10,449, and the total number of
test instances is 1,535.
5 Participating Systems
Five teams participated in the t subtask, submitting
a total of eight systems. Three teams (a subset of
those five) participated in the ts subtask, submitting
a total of five systems. All submitted systems em-
ployed supervised learning, using the training ex-
amples provided. Some teams used additional re-
sources as noted in the more detailed descriptions
Table 1: Target words in the SENSEVAL-3 English-Hindi task
Lexical Unit Translations Train Test Lexical Unit Translations Train Test Lexical Unit Translations Train Test
TRANSLATION ONLY (T?DATA)
band.n 8 224 91 bank.n 21 332 52 case.n 13 348 42
different.a 4 320 25 eat.v 3 271 48 field.n 14 300 100
glass.n 8 379 13 hot.a 18 348 32 line.n 39 360 11
note.v 11 220 12 operate.v 9 280 50 paper.n 8 264 73
plan.n 8 210 35 produce.v 7 265 67 rest.v 14 172 10
rule.v 8 160 18 shape.n 8 320 32 sharp.a 16 248 48
smell.v 5 210 17 solid.a 16 327 37 substantial.a 15 250 100
suspend.v 4 370 28 table.n 21 378 16 talk.v 6 341 35
taste.n 6 350 40 terrible.a 4 200 99 tour.n 5 240 9
vision.n 14 318 20 volume.n 9 309 54 watch.v 10 300 100
way.n 16 331 22 TOTAL 348 8945 1336
TRANSLATION AND SENSE ONLY (TS?DATA)
bar.n 19 278 39 begin.v 6 360 15 channel.n 6 92 16
green.a 9 175 26 nature.n 15 71 14 play.v 14 152 10
simple.a 9 166 19 treat.v 7 100 32 wash.v 16 10 11
work.v 24 100 17 TOTAL 125 1504 199
below.
5.1 NUS
The NUS team from the National University of Sin-
gapore participated in both the t and ts subtasks. The
t system (nusmlst) uses a combination of knowledge
sources as features, and the Support Vector Machine
(SVM) learning algorithm. The knowledge sources
used include part of speech of neighboring words,
single words in the surrounding context, local col-
locations, and syntactic relations. The ts system
(nusmlsts) does the same, but adds the English sense
of the target word as a knowledge source.
5.2 LIA-LIDILEM
The LIA-LIDILEM team from the Universite? d?
Avignon and the Universite? Stendahl Grenoble had
two systems which participated in both the t and ts
subtasks. In the ts subtask, only the English sense
tags were used, not the Hindi translations.
The FL-MIX system uses a combination of three
probabilistic models, which compute the most prob-
able sense given a six word window of context. The
three models are a Poisson model, a Semantic Clas-
sification Tree model, and a K nearest neighbors
search model. This system also used a part of speech
tagger and a lemmatizer.
The FC-MIX system is the same as the FL-MIX
system, but replaces context words by more gen-
eral synonym?like classes computed from a word
aligned English?French corpus which number ap-
proximately 850,000 words in each language.
5.3 HKUST
The HKUST team from the Hong Kong University
of Science and Technology had three systems that
participated in both the t and ts subtasks
The HKUST me t and HKUST me ts sys-
tems are maximum entropy classifiers. The
HKUST comb t and HKUST comb ts systems
are voted classifiers that combine a new Kernel
PCA model with a maximum entropy model and
a boosting?based model. The HKUST comb2 t
and HKUST comb2 ts are voted classifiers that
combine a new Kernel PCA model with a maximum
entropy model, a boosting?based model, and a
Naive Bayesian model.
5.4 UMD
The UMD team from the University of Maryland en-
tered (UMD?SST) in the t task. UMD?SST is a su-
pervised sense tagger based on the Support Vector
Machine learning algorithm, and is described more
fully in (Cabezas et al, 2001).
5.5 Duluth
The Duluth team from the University of Minnesota,
Duluth had one system (Duluth-ELSS) that partici-
pated in the t task. This system is an ensemble of
three bagged decision trees, each based on a differ-
ent type of lexical feature. This system was known
as Duluth3 in SENSEVAL-2, and it is described more
fully in (Pedersen, 2001).
6 Results
All systems attempted all of the test instances, so
precision and recall are identical, hence we report
Table 2: t Subtask Results
System Accuracy
nusmlst 63.4
HKUST comb t 62.0
HKUST comb2 t 61.4
HKUST me t 60.6
FL-MIX 60.3
FC-MIX 60.3
UMD-SST 59.4
Duluth-ELSS 58.2
Baseline (majority) 51.9
Table 3: ts Subtask Results
System Accuracy
nusmlsts 67.3
FL-MIX 64.1
FC-MIX 64.1
HKUST comb ts 63.8
HKUST comb2 ts 63.8
HKUST me ts 60.8
Baseline (majority) 55.8
the single Accuracy figure. Tables 2 and 3 show re-
sults for the t and ts subtasks, respectively.
We note that the participating systems all ex-
ceeded the baseline (majority) classifier by some
margin, suggesting that the sense distinctions made
by the translations are clear and provide sufficient
information for supervised methods to learn effec-
tive classifiers.
Interestingly, the average results on the ts data are
higher than the average results on the t data, which
suggests that sense information is likely to be helpful
for the task of targeted word translation. Additional
investigations are however required to draw some fi-
nal conclusions.
7 Conclusion
The Multilingual Lexical Sample task in
SENSEVAL-3 featured English ambiguous words
that were to be tagged with their most appropriate
Hindi translation. The objective of this task is to
determine feasibility of translating words of various
degrees of polysemy, focusing on translation of
specific lexical items. The results of five teams
that participated in this event tentatively suggest
that machine learning techniques can significantly
improve over the most frequent sense baseline.
Additionally, this task has highlighted creation
of testing and training data by leveraging the
knowledge of bilingual Web volunteers. The
training and test data sets used in this exercise are
available online from http://www.senseval.org and
http://teach-computers.org.
Acknowledgments
Many thanks to all those who contributed to the Mul-
tilingual Open Mind Word Expert project, making
this task possible. We are also grateful to all the par-
ticipants in this task, for their hard work and involve-
ment in this evaluation exercise. Without them, all
these comparative analyses would not be possible.
We are particularly grateful to a research grant
from the University of North Texas that provided the
funding for contributor prizes, and to the National
Science Foundation for their support of Amruta Pu-
randare under a Faculty Early CAREER Develop-
ment Award (#0092784).
References
S. Awasthi, editor. 1997. Chambers English?Hindi
Dictionary. South Asia Books, Columbia, MO.
C. Cabezas, P. Resnik, and J. Stevens. 2001. Su-
pervised sense tagging using Support Vector Ma-
chines. In Proceedings of the Senseval-2 Work-
shop, Toulouse, July.
T. Chklovski and R. Mihalcea. 2002. Building a
sense tagged corpus with the Open Mind Word
Expert. In Proceedings of the ACL Workshop on
Word Sense Disambiguation: Recent Successes
and Future Directions, Philadelphia.
T. Pedersen. 2001. Machine learning with lexical
features: The Duluth approach to Senseval-2. In
Proceedings of the Senseval-2 Workshop, pages
139?142, Toulouse, July.
J. Raker and R. Shukla, editors. 1996. Hip-
pocrene Standard Dictionary English-Hindi
Hindi-English (With Romanized Pronunciation).
Hippocrene Books, New York, NY.
P. Resnik and D. Yarowsky. 1999. Distinguish-
ing systems and distinguishing senses: New eval-
uation methods for word sense disambiguation.
Natural Language Engineering, 5(2):113?133.
The SENSEVAL?3 English Lexical Sample Task
Rada Mihalcea
Department of Computer Science
University of North Texas
Dallas, TX, USA
rada@cs.unt.edu
Timothy Chklovski
Information Sciences Institute
University of Southern California
Marina del Rey, CA, USA
timc@isi.edu
Adam Kilgarriff
Information Technology Research Institute
University of Brighton
Brighton, UK
Adam.Kilgarriff@itri.brighton.ac.uk
Abstract
This paper presents the task definition, resources,
participating systems, and comparative results for
the English lexical sample task, which was orga-
nized as part of the SENSEVAL-3 evaluation exer-
cise. The task drew the participation of 27 teams
from around the world, with a total of 47 systems.
1 Introduction
We describe in this paper the task definition, re-
sources, participating systems, and comparative re-
sults for the English lexical sample task, which was
organized as part of the SENSEVAL-3 evaluation ex-
ercise. The goal of this task was to create a frame-
work for evaluation of systems that perform targeted
Word Sense Disambiguation.
This task is a follow-up to similar tasks organized
during the SENSEVAL-1 (Kilgarriff and Palmer,
2000) and SENSEVAL-2 (Preiss and Yarowsky,
2001) evaluations. The main changes in this
year?s evaluation consist of a new methodology for
collecting annotated data (with contributions from
Web users, as opposed to trained lexicographers),
and a new sense inventory used for verb entries
(Wordsmyth).
2 Building a Sense Tagged Corpus with
Volunteer Contributions over the Web
The sense annotated corpus required for this task
was built using the Open Mind Word Expert system
(Chklovski and Mihalcea, 2002) 1. To overcome the
current lack of sense tagged data and the limitations
imposed by the creation of such data using trained
lexicographers, the OMWE system enables the col-
lection of semantically annotated corpora over the
Web. Sense tagged examples are collected using
1Open Mind Word Expert can be accessed at http://teach-
computers.org/
a Web-based application that allows contributors to
annotate words with their meanings.
The tagging exercise proceeds as follows. For
each target word the system extracts a set of sen-
tences from a large textual corpus. These examples
are presented to the contributors, who are asked to
select the most appropriate sense for the target word
in each sentence. The selection is made using check-
boxes, which list all possible senses of the current
target word, plus two additional choices, ?unclear?
and ?none of the above.? Although users are encour-
aged to select only one meaning per word, the se-
lection of two or more senses is also possible. The
results of the classification submitted by other users
are not presented to avoid artificial biases.
Similar to the annotation scheme used for the En-
glish lexical sample at SENSEVAL-2, we use a ?tag
until two agree? scheme, with an upper bound on the
number of annotations collected for each item set to
four.
2.1 Source Corpora
The data set used for the SENSEVAL-3 English
lexical sample task consists of examples extracted
from the British National Corpus (BNC). Ear-
lier versions of OMWE also included data from
the Penn Treebank corpus, the Los Angeles Times
collection as provided during TREC conferences
(http://trec.nist.gov), and Open Mind Common Sense
(http://commonsense.media.mit.edu).
2.2 Sense Inventory
The sense inventory used for nouns and adjec-
tives is WordNet 1.7.1 (Miller, 1995), which
is consistent with the annotations done for the
same task during SENSEVAL-2. Verbs are in-
stead annotated with senses from Wordsmyth
(http://www.wordsmyth.net/). The main reason mo-
tivating selection of a different sense inventory is the
                                             Association for Computational Linguistics
                        for the Semantic Analysis of Text, Barcelona, Spain, July 2004
                 SENSEVAL-3: Third International Workshop on the Evaluation of Systems
Class Nr of Avg senses Avg senses
words (fine) (coarse)
Nouns 20 5.8 4.35
Verbs 32 6.31 4.59
Adjectives 5 10.2 9.8
Total 57 6.47 4.96
Table 1: Summary of the sense inventory
weak verb performance of systems participating in
the English lexical sample in SENSEVAL-2, which
may be due to the high number of senses defined for
verbs in the WordNet sense inventory. By choos-
ing a different set of senses, we hope to gain insight
into the dependence of difficulty of the sense disam-
biguation task on sense inventories.
Table 1 presents the number of words under each
part of speech, and the average number of senses for
each class.
2.3 Multi-Word Expressions
For this evaluation exercise, we decided to isolate the
task of semantic tagging from the task of identifying
multi-word expressions; we applied a filter that re-
moved all examples pertaining to multi-word expres-
sions prior to the tagging phase. Consequently, the
training and test data sets made available for this task
do not contain collocations as possible target words,
but only single word units. This is a somewhat dif-
ferent definition of the task as compared to previous
similar evaluations; the difference may have an im-
pact on the overall performance achieved by systems
participating in the task.
2.4 Sense Tagged Data
The inter-tagger agreement obtained so far is closely
comparable to the agreement figures previously re-
ported in the literature. Kilgarriff (2002) mentions
that for the SENSEVAL-2 nouns and adjectives there
was a 66.5% agreement between the first two tag-
gings (taken in order of submission) entered for
each item. About 12% of that tagging consisted of
multi-word expressions and proper nouns, which are
usually not ambiguous, and which are not consid-
ered during our data collection process. So far we
measured a 62.8% inter-tagger agreement between
the first two taggings for single word tagging, plus
close-to-100% precision in tagging multi-word ex-
pressions and proper nouns (as mentioned earlier,
this represents about 12% of the annotated data).
This results in an overall agreement of about 67.3%
which is reasonable and closely comparable with
previous figures. Note that these figures are col-
lected for the entire OMWE data set build so far,
which consists of annotated data for more than 350
words.
In addition to raw inter-tagger agreement, the
kappa statistic, which removes from the agreement
rate the amount of agreement that is expected by
chance(Carletta, 1996), was also determined. We
measure two figures: micro-average   , where num-
ber of senses, agreement by chance, and   are de-
termined as an average for all words in the set,
and macro-average   , where inter-tagger agreement,
agreement by chance, and   are individually deter-
mined for each of the words in the set, and then
combined in an overall average. With an average of
five senses per word, the average value for the agree-
ment by chance is measured at 0.20, resulting in a
micro-   statistic of 0.58. For macro-   estimations,
we assume that word senses follow the distribution
observed in the OMWE annotated data, and under
this assumption, the macro-   is 0.35.
3 Participating Systems
27 teams participated in this word sense disambigua-
tion task. Tables 2 and 3 list the names of the partic-
ipating systems, the corresponding institutions, and
the name of the first author ? which can be used
as reference to a paper in this volume, with more
detailed descriptions of the systems and additional
analysis of the results.
There were no restrictions placed on the number
of submissions each team could make. A total num-
ber of 47 submissions were received for this task.
Tables 2 and 3 show all the submissions for each
team, gives a brief description of their approaches,
and lists the precision and recall obtained by each
system under fine and coarse grained evaluations.
The precision/recall baseline obtained for this task
under the ?most frequent sense? heuristic is 55.2%
(fine grained) and 64.5% (coarse grained). The per-
formance of most systems (including several unsu-
pervised systems, as listed in Table 3) is significantly
higher than the baseline, with the best system per-
forming at 72.9% (79.3%) for fine grained (coarse
grained) scoring.
Not surprisingly, several of the top performing
systems are based on combinations of multiple clas-
sifiers, which shows once again that voting schemes
that combine several learning algorithms outperform
the accuracy of individual classifiers.
4 Conclusion
The English lexical sample task in SENSEVAL-
3 featured English ambiguous words that were to
be tagged with their most appropriate WordNet or
Wordsmyth sense. The objective of this task was
to: (1) Determine feasibility of reliably finding the
Fine Coarse
System/Team Description P R P R
htsa3 A Naive Bayes system, with correction of the a-priori frequencies, by
U.Bucharest (Grozea) dividing the output confidence of the senses by  	
	 (   ) 72.9 72.9 79.3 79.3
IRST-Kernels Kernel methods for pattern abstraction, paradigmatic and syntagmatic info.
ITC-IRST (Strapparava) and unsupervised term proximity (LSA) on BNC, in an SVM classifier. 72.6 72.6 79.5 79.5
nusels A combination of knowledge sources (part-of-speech of neighbouring words,
Nat.U. Singapore (Lee) words in context, local collocations, syntactic relations), in an SVM classifier. 72.4 72.4 78.8 78.8
htsa4 Similar to htsa3, with different correction function of a-priori frequencies. 72.4 72.4 78.8 78.8
BCU comb An ensemble of decision lists, SVM, and vectorial similarity, improved
Basque Country U. with a variety of smoothing techniques. The features consist 72.3 72.3 78.9 78.9
(Agirre & Martinez) of local collocations, syntactic dependencies, bag-of-words, domain features.
htsa1 Similar to htsa3, but with smaller number of features. 72.2 72.2 78.7 78.7
rlsc-comb A regularized least-square classification (RLSC), using local and topical
U.Bucharest (Popescu) features, with a term weighting scheme. 72.2 72.2 78.4 78.4
htsa2 Similar to htsa4, but with smaller number of features. 72.1 72.1 78.6 78.6
BCU english Similar to BCU comb, but with a vectorial space model learning. 72.0 72.0 79.1 79.1
rlsc-lin Similar to rlsc-comb, with a linear kernel, and a binary weighting scheme. 71.8 71.8 78.4 78.4
HLTC HKUST all A voted classifier combining a new kernel PCA method, a Maximum Entropy
HKUST (Carpuat) model, and a boosting-based model, using syntactic and collocational features 71.4 71.4 78.6 78.6
TALP A system with per-word feature selection, using a rich feature set. For
U.P.Catalunya learning, it uses SVM, and combines two binarization procedures: 71.3 71.3 78.2 78.2
(Escudero et al) one vs. all, and constraint learning.
MC-WSD A multiclass averaged perceptron classifier with two components: one
Brown U. trained on the data provided, the other trained on this data, and on 71.1 71.1 78.1 78.1
(Ciaramita & Johnson) WordNet glosses. Features consist of local and syntactic features.
HLTC HKUST all2 Similar to HLTC HKUST all, also adds a Naive Bayes classifier. 70.9 70.9 78.1 78.1
NRC-Fine Syntactic and semantic features, using POS tags and pointwise mutual infor-
NRC (Turney) mation on a terabyte corpus. Five basic classifiers are combined with voting. 69.4 69.4 75.9 75.9
HLTC HKUST me Similar to HLTC HKUST all, only with a maximum entropy classifier. 69.3 69.3 76.4 76.4
NRC-Fine2 Similar to NRC-Fine, with a different threshold for dropping features 69.1 69.1 75.6 75.6
GAMBL A cascaded memory-based classifier, using two classifiers based on global
U. Antwerp (Decadt) and local features, with a genetic algorithm for parameter optimization. 67.4 67.4 74.0 74.0
SinequaLex Semantic classification trees, built on short contexts and document se-
Sinequa Labs (Crestan) mantics, plus a decision system based on information retrieval techniques. 67.2 67.2 74.2 74.2
CLaC1 A Naive Bayes approach using a context window around the target word, 67.2 67.2 75.1 75.1
Concordia U. (Lamjiri) which is dynamically adjusted
SinequaLex2 A cumulative method based on scores of surrounding words. 66.8 66.8 73.6 73.6
UMD SST4 Supervised learning using Support Vector Machines, using local and
U. Maryland (Cabezas) wide context features, and also grammatical and expanded contexts. 66.0 66.0 73.7 73.7
Prob1 A probabilistic modular WSD system, with individual modules based on
Cambridge U. (Preiss) separate known approaches to WSD (26 different modules) 65.1 65.1 71.6 71.6
SyntaLex-3 A supervised system that uses local part of speech features and bigrams,
U.Toronto (Mohammad) in an ensemble classifier using bagged decision trees. 64.6 64.6 72.0 72.0
UNED A similarity-based system, relying on the co-occurrence of nouns and
UNED (Artiles) adjectives in the test and training examples. 64.1 64.1 72.0 72.0
SyntaLex-4 Similar to SyntaLex-3, but with unified decision trees. 63.3 63.3 71.1 71.1
CLaC2 Syntactic and semantic (WordNet hypernyms) information of neighboring
words, fed to a Maximum Entropy learner. See also CLaC1 63.1 63.1 70.3 70.3
SyntaLex-1 Bagged decision trees using local POS features. See also SyntaLex-3. 62.4 62.4 69.1 69.1
SyntaLex-2 Similar to SyntaLex-1, but using broad context part of speech features. 61.8 61.8 68.4 68.4
Prob2 Similar to Prob1, but invokes only 12 modules. 61.9 61.9 69.3 69.3
Duluth-ELSS An ensemble approach, based on three bagged decision trees, using
U.Minnesota (Pedersen) unigrams, bigrams, and co-occurrence features 61.8 61.8 70.1 70.1
UJAEN A Neural Network supervised system, using features based on semantic
U.Jae?n (Garc??a-Vega) relations from WordNet extracted from the training data 61.3 61.3 69.5 69.5
R2D2 A combination of supervised (Maximum Entropy, HMM Models, Vector
U. Alicante (Vazquez) Quantization, and unsupervised (domains and conceptual density) systems. 63.4 52.1 69.7 57.3
IRST-Ties A generalized pattern abstraction system, based on boosted wrapper
ITC-IRST (Strapparava) induction, using only few syntagmatic features. 70.6 50.5 76.7 54.8
NRC-Coarse Similar to NRC-Fine; maximizes the coarse score, by training on coarse senses. 48.5 48.5 75.8 75.8
NRC-Coarse2 Similar to NRC-Coarse, with a different threshold for dropping features. 48.4 48.4 75.7 75.7
DLSI-UA-LS-SU A maximum entropy method and a bootstrapping algorithm (?re-training?) with,
U.Alicante (Vazquez) iterative feeding of training cycles with new high-confidence examples. 78.2 31.0 82.8 32.9
Table 2: Performance and short description of the supervised systems participating in the SENSEVAL-3
English lexical sample Word Sense Disambiguation task. Precision and recall figures are provided for both
fine grained and coarse grained scoring. Corresponding team and reference to system description (in this
volume) are indicated for the first system for each team.
Fine Coarse
System/Team Description P R P R
wsdiit An unsupervised system using a Lesk-like similarity between context
IIT Bombay of ambiguous words, and dictionary definitions. Experiments are 66.1 65.7 73.9 74.1
(Ramakrishnan et al) performed for various window sizes, various similarity measures
Cymfony A Maximum Entropy model for unsupervised clustering, using neighboring
(Niu) words and syntactic structures as features. A few annotated instances 56.3 56.3 66.4 66.4
are used to map context clusters to WordNet/Worsmyth senses.
Prob0 A combination of two unsupervised modules, using basic part of speech
Cambridge U. (Preiss) and frequency information. 54.7 54.7 63.6 63.6
clr04-ls An unsupervised system relying on definition properties (syntactic, semantic,
CL Research subcategorization patterns, other lexical information), as given in a dictionary. 45.0 45.0 55.5 55.5
(Litkowski) Performance is generally a function of how well senses are distinguished.
CIAOSENSO An unsupervised system that combines the conceptual density idea with the
U. Genova (Buscaldi) frequency of words to disambiguate; information about domains is also 50.1 41.7 59.1 49.3
taken into account.
KUNLP An algorithm that disambiguates the senses of a word by selecting a substituent
Korea U. (Seo) among WordNet relatives (antonyms, hypernyms, etc.). The selection 40.4 40.4 52.8 52.8
is done based on co-occurrence frequencies, measured on a large corpus.
Duluth-SenseRelate An algorithm that assigns the sense to a word that is most related to the
U.Minnesota (Pedersen) possible senses of its neighbors, using WordNet glosses to measure 40.3 38.5 51.0 48.7
relatedness between senses.
DFA-LS-Unsup A combination of three heuristics: similarity between synonyms and the context,
UNED (Fernandez) according to a mutual information measure; lexico-syntactic patterns extracted 23.4 23.4 27.4 27.4
from WordNet glosses; the first sense heuristic.
DLSI-UA-LS-NOSU An unsupervised method based on (Magnini & Strapparava 2000) WordNet
U.Alicante (Vazquez) domains; it exploits information contained in glosses of WordNet domains, and 19.7 11.7 32.2 19.0
uses ?Relevant Domains?, obtained from association ratio over domains and words.
Table 3: Performance and short description for the Unsupervised systems participating in the SENSEVAL-3
English lexical sample task.
appropriate sense for words with various degrees of
polysemy, using different sense inventories; and (2)
Determine the usefulness of sense annotated data
collected over the Web (as opposed to other tradi-
tional approaches for building semantically anno-
tated corpora).
The results of 47 systems that participated in this
event tentatively suggest that supervised machine
learning techniques can significantly improve over
the most frequent sense baseline, and also that it is
possible to design unsupervised techniques for reli-
able word sense disambiguation. Additionally, this
task has highlighted creation of testing and training
data by leveraging the knowledge of Web volunteers.
The training and test data sets used in this exercise
are available online from http://www.senseval.org
and http://teach-computers.org.
Acknowledgments
Many thanks to all those who contributed to the
Open Mind Word Expert project, making this task
possible. In particular, we are grateful to Gwen
Lenker ? our most productive contributor. We are
also grateful to all the participants in this task, for
their hard work and involvement in this evaluation
exercise. Without them, all these comparative anal-
yses would not be possible.
We are indebted to the Princeton WordNet team,
for making WordNet available free of charge, and to
Robert Parks from Wordsmyth, for making available
the verb entries used in this evaluation.
We are particularly grateful to the National Sci-
ence Foundation for their support under research
grant IIS-0336793, and to the University of North
Texas for a research grant that provided funding for
contributor prizes.
References
J. Carletta. 1996. Assessing agreement on classification tasks:
The kappa statistic. Computational Linguistics, 22(2):249?
254.
T. Chklovski and R. Mihalcea. 2002. Building a sense tagged
corpus with Open Mind Word Expert. In Proceedings of the
ACL 2002 Workshop on ?Word Sense Disambiguation: Re-
cent Successes and Future Directions?, Philadelphia, July.
A. Kilgarriff and M. Palmer, editors. 2000. Computer and
the Humanities. Special issue: SENSEVAL. Evaluating Word
Sense Disambiguation programs, volume 34, April.
G. Miller. 1995. Wordnet: A lexical database. Communication
of the ACM, 38(11):39?41.
J. Preiss and D. Yarowsky, editors. 2001. Proceedings of
SENSEVAL-2, Association for Computational Linguistics
Workshop, Toulouse, France.
An Evaluation Exercise for Romanian Word Sense Disambiguation
Rada Mihalcea
Department of Computer Science
University of North Texas
Dallas, TX, USA
rada@cs.unt.edu
Vivi Na?stase
School of Computer Science
University of Ottawa
Ottawa, ON, Canada
vnastase@site.uottawa.ca
Timothy Chklovski
Information Sciences Institute
University of Southern California
Marina del Rey, CA, USA
timc@isi.edu
Doina Ta?tar
Department of Computer Science
Babes?-Bolyai University
Cluj-Napoca, Romania
dtatar@ubb.ro
Dan Tufis?
Romanian Academy Center
for Artificial Intelligence
Bucharest, Romania
tufis@racai.ro
Florentina Hristea
Department of Computer Science
University of Bucharest
Bucharest, Romania
fhristea@mailbox.ro
Abstract
This paper presents the task definition, resources,
participating systems, and comparative results for a
Romanian Word Sense Disambiguation task, which
was organized as part of the SENSEVAL-3 evaluation
exercise. Five teams with a total of seven systems
were drawn to this task.
1 Introduction
SENSEVAL is an evaluation exercise of the lat-
est word-sense disambiguation (WSD) systems. It
serves as a forum that brings together researchers in
WSD and domains that use WSD for various tasks.
It allows researchers to discuss modifications that
improve the performance of their systems, and an-
alyze combinations that are optimal.
Since the first edition of the SENSEVAL competi-
tions, a number of languages were added to the orig-
inal set of tasks. Having the WSD task prepared for
several languages provides the opportunity to test
the generality of WSD systems, and to detect dif-
ferences with respect to word senses in various lan-
guages.
This year we have proposed a Romanian WSD
task. Five teams with a total of seven systems have
tackled this task. We present in this paper the data
used and how it was obtained, and the performance
of the participating systems.
2 Open Mind Word Expert
The sense annotated corpus required for this task
was built using the Open Mind Word Expert system
(Chklovski and Mihalcea, 2002), adapted to Roma-
nian1.
To overcome the current lack of sense tagged
data and the limitations imposed by the creation of
such data using trained lexicographers, the Open
Mind Word Expert system enables the collection
of semantically annotated corpora over the Web.
Sense tagged examples are collected using a Web-
based application that allows contributors to anno-
tate words with their meanings.
The tagging exercise proceeds as follows. For
each target word the system extracts a set of sen-
tences from a large textual corpus. These examples
are presented to the contributors, who are asked to
select the most appropriate sense for the target word
in each sentence. The selection is made using check-
boxes, which list all possible senses of the current
target word, plus two additional choices, ?unclear?
and ?none of the above.? Although users are en-
couraged to select only one meaning per word, the
selection of two or more senses is also possible. The
results of the classification submitted by other users
are not presented to avoid artificial biases.
3 Sense inventory
For the Romanian WSD task, we have chosen a set
of words from three parts of speech - nouns, verbs
and adjectives. Table 1 presents the number of
words under each part of speech, and the average
number of senses for each class.
The senses were (manually) extracted from a Ro-
manian dictionary (Dict?ionarul EXplicativ al limbii
roma?ne - DEX (Coteanu et al, 1975)). These senses
1Romanian Open Mind Word Expert can be accessed at
http://teach-computers.org/word-expert/romanian
                                             Association for Computational Linguistics
                        for the Semantic Analysis of Text, Barcelona, Spain, July 2004
                 SENSEVAL-3: Third International Workshop on the Evaluation of Systems
Number Avg senses Avg senses
Class words (fine) (coarse)
Nouns 25 8.92 4.92
Verbs 9 8.7 4.6
Adjectives 5 9 4
Total 39 8.875 4.725
Table 1: Sense inventory
and their dictionary definitions were incorporated in
the Open Mind Word Expert. For each annotation
task, the contributors could choose from this list of
39 words. For each chosen word, the system dis-
plays the associated senses, together with their def-
initions, and a short (1-4 words) description of the
sense. After the user gets familiarized with these
senses, the system displays each example sentence,
and the list of senses together with their short de-
scription, to facilitate the tagging process.
For the coarse grained WSD task, we had the op-
tion of using the grouping provided by the dictio-
nary. A manual analysis however showed that some
of the senses in the same group are quite distinguish-
able, while others that were separated were very
similar.
For example, for the word circulatie (roughly, cir-
culation). The following two senses are grouped in
the dictionary:
2a. movement, travel along a communication
line/way
2b. movement of the sap in plants or the cytoplasm
inside cells
Sense 2a fits better with sense 1 of circulation:
1. the event of moving about
while sense 2b fits better with sense 3:
3. movement or flow of a liquid, gas, etc. within a
circuit or pipe.
To obtain a better grouping, a linguist clustered
the similar senses for each word in our list of forty.
The average number of senses for each class is al-
most halved.
Notice that Romanian is a language that uses dia-
critics, and the the presence of diacritics may be cru-
cial for distinguishing between words. For example
peste without diacritics may mean fish or over. In
choosing the list of words for the Romanian WSD
task, we have tried to avoid such situations. Al-
though some of the words in the list do have dia-
critics, omitting them does not introduce new ambi-
guities.
4 Corpus
Examples are extracted from the ROCO corpus, a
400 million words corpus consisting of a collection
of Romanian newspapers collected on the Web over
a three years period (1999-2002).
The corpus was tokenized and part-of-speech
tagged using RACAI?s tools (Tufis, 1999). The to-
kenizer recognizes and adequately segments various
constructs: clitics, dates, abbreviations, multiword
expressions, proper nouns, etc. The tagging fol-
lowed the tiered tagging approach with the hidden
layer of tagging being taken care of by Thorsten
Brants? TNT (Brants, 2000). The upper level of
the tiered tagger removed from the assigned tags all
the attributes irrelevant for this WSD exercise. The
estimated accuracy of the part-of-speech tagging is
around 98%.
5 Sense Tagged Data
While several sense annotation schemes have been
previously proposed, including single or dual anno-
tations, or the ?tag until two agree? scheme used dur-
ing SENSEVAL-2, we decided to use a new scheme
and collect four tags per item, which allowed us
to conduct and compare inter-annotator agreement
evaluations for two-, three-, and four-way agree-
ment. The agreement rates are listed in Table 3.
The two-way agreement is very high ? above 90%
? and these are the items that we used to build the
annotated data set. Not surprisingly, four-way agree-
ment is reached for a significantly smaller number of
cases. While these items with four-way agreement
were not explicitly used in the current evaluation,
we believe that this represents a ?platinum standard?
data set with no precedent in the WSD research com-
munity, which may turn useful for a range of future
experiments (for bootstrapping, in particular).
Agreement type Total (%)
TOTAL ITEMS 11,532 100%
At least two agree 10,890 94.43%
At least three agree 8,192 71.03%
At least four agree 4,812 41.72%
Table 3: Inter-agreement rates for two-, three-, and
four-way agreement
Table 2 lists the target words selected for this task,
together with their most common English transla-
tions. For each word, we also list the number of
senses, as defined in the DEX sense inventory (col-
locations included), and the number of annotated ex-
amples made available to task participants.
Word Main English senses senses Train Test Word Main English senses senses Train Test
translation (fine) (coarse) size size translation (fine) (coarse) size size
NOUNS
ac needle 16 7 127 65 accent accent 5 3 172 87
actiune action 10 7 261 128 canal channel 6 5 134 66
circuit circuit 7 5 200 101 circulatie circulation 9 3 221 114
coroana crown 15 11 252 126 delfin doplhin 5 4 31 15
demonstratie demonstration 6 3 229 115 eruptie eruption 2 2 54 27
geniu genius 5 3 106 54 nucleu nucleus 7 5 64 33
opozitie opposition 12 7 266 134 perie brush 5 3 46 24
pictura painting 5 2 221 111 platforma platform 11 8 226 116
port port 7 3 219 108 problema problem 6 4 262 131
proces process 11 3 166 82 reactie reaction 7 6 261 131
stil style 14 4 199 101 timbru stamp 7 3 231 116
tip type 7 4 263 131 val wave 15 9 242 121
valoare value 23 9 251 125
VERBS
cistiga win 5 4 227 115 citi read 10 4 259 130
cobori descend 11 6 252 128 conduce drive 7 6 265 134
creste grow 14 6 209 103 desena draw 3 3 54 27
desface untie 11 5 115 58 fierbe boil 11 4 83 43
indulci sweeten 7 4 19 10
ADJECTIVES
incet slow 6 3 224 113 natural natural 12 5 242 123
neted smooth 7 3 34 17 oficial official 5 3 185 96
simplu simple 15 6 153 82
Table 2: Target words in the SENSEVAL-3 Romanian Lexical Sample task
Team System name Reference (this volume)
Babes-Bolyai University, Cluj-Napoca (1) ubb nbc ro (Csomai, 2004)
Babes-Bolyai University, Cluj-Napoca (2) UBB (Serban and Tatar, 2004)
Swarthmore College swat-romanian (Wicentowski et al, 2004a)
Swarthmore College / Hong Kong Polytechnic University swat-hk-romanian (Wicentowski et al, 2004b)
Hong Kong University of Science and Technology romanian-swat hk-bo
University of Maryland, College Park UMD SST6 (Cabezas et al, 2004)
University of Minnesota, Duluth Duluth-RomLex (Pedersen, 2004)
Table 4: Teams participating in the SENSEVAL-3 Romanian Word Sense Disambiguation task
In addition to sense annotated examples, partici-
pants have been also provided with a large number
of unlabeled examples. However, among all partici-
pating systems, only one system ? described in (Ser-
ban and Ta?tar 2004) ? attempted to integrate this ad-
ditional unlabeled data set into the learning process.
6 Participating Systems
Five teams participated in this word sense disam-
biguation task. Table 4 lists the names of the par-
ticipating systems, the corresponding institutions,
and references to papers in this volume that provide
detailed descriptions of the systems and additional
analysis of their results.
There were no restrictions placed on the number
of submissions each team could make. A total num-
ber of seven submissions was received for this task.
Table 5 shows all the submissions for each team, and
gives a brief description of their approaches.
7 Results and Discussion
Table 6 lists the results obtained by all participating
systems, and the baseline obtained using the ?most
frequent sense? (MFS) heuristic. The table lists pre-
cision and recall figures for both fine grained and
coarse grained scoring.
The performance of all systems is significantly
higher than the baseline, with the best system per-
forming at 72.7% (77.1%) for fine grained (coarse
grained) scoring, which represents a 35% (38%) er-
ror reduction with respect to the baseline.
The best system (romanian-swat hk-bo) relies on
a Maximum Entropy classifier with boosting, using
local context (neighboring words, lemmas, and their
part of speech), as well as bag-of-words features for
surrounding words.
Not surprisingly, several of the top perform-
ing systems are based on combinations of multi-
ple sclassifiers, which shows once again that voting
System Description
romanian-swat hk-bo Supervised learning using Maximum Entropy with boosting, using bag-of-words
and n-grams around the head word as features
swat-hk-romanian The swat-romanian and romanian-swat hk-bo systems combined with majority voting.
Duluth-RLSS An ensemble approach that takes a vote among three bagged decision trees,
based on unigrams, bigrams and co-occurrence features
swat-romanian Three classifiers: cosine similarity clustering, decision list, and Naive Bayes,
using bag-of-words and n-grams around the head word as features
combined with a majority voting scheme.
UMD SST6 Supervised learning using Support Vector Machines, using contextual features.
ubb nbc ro Supervised learning using a Naive Bayes learning scheme, and features extracted
using a bag-of-words approach.
UBB A k-NN memory-based learning approach, with bag-of-words features.
Table 5: Short description of the systems participating in the SENSEVAL-3 Romanian Word Sense Disam-
biguation task. All systems are supervised.
Fine grained Coarse grained
System P R P R
romanian-swat hk-bo 72.7% 72.7% 77.1% 77.1%
swat-hk-romanian 72.4% 72.4% 76.1% 76.1%
Duluth-RLSS 71.4% 71.4% 75.2% 75.2%
swat-romanian 71.0% 71.0% 74.9% 74.9%
UMD SST6 70.7% 70.7% 74.6% 74.6%
ubb nbc ro 71.0% 68.2% 75.0% 72.0%
UBB 67.1% 67.1% 72.2% 72.2%
Baseline (MFS) 58.4% 58.4% 62.9% 62.9%
Table 6: System results on the Romanian Word Sense Disambiguation task.
schemes that combine several learning algorithms
outperform the accuracy of individual classifiers.
8 Conclusion
A Romanian Word Sense Disambiguation task
was organized as part of the SENSEVAL-3 eval-
uation exercise. In this paper, we presented
the task definition, and resources involved, and
shortly described the participating systems. The
task drew the participation of five teams, and in-
cluded seven different systems. The sense an-
notated data used in this exercise is available
online from http://www.senseval.org and
http://teach-computers.org.
Acknowledgments
Many thanks to all those who contributed to the
Romanian Open Mind Word Expert project, mak-
ing this task possible. Special thanks to Bog-
dan Harhata, from the Institute of Linguistics Cluj-
Napoca, for building a coarse grained sense map.
We are also grateful to all the participants in this
task, for their hard work and involvement in this
evaluation exercise. Without them, all these com-
parative analyses would not be possible.
References
T. Brants. 2000. Tnt - a statistical part-of-speech
tagger. In Proceedings of the 6th Applied NLP
Conference, ANLP-2000, Seattle, WA, May.
T. Chklovski and R. Mihalcea. 2002. Building a
sense tagged corpus with Open Mind Word Ex-
pert. In Proceedings of the Workshop on ?Word
Sense Disambiguation: Recent Successes and Fu-
ture Directions?, ACL 2002, Philadelphia, July.
I. Coteanu, L. Seche, M. Seche, A. Burnei,
E. Ciobanu, E. Contras?, Z. Cret?a, V. Hristea,
L. Mares?, E. St??ngaciu, Z. S?tefa?nescu, T. T?ugulea,
I. Vulpescu, and T. Hristea. 1975. Dict?ionarul
Explicativ al Limbii Roma?ne. Editura Academiei
Republicii Socialiste Roma?nia.
D. Tufis. 1999. Tiered tagging and combined classi-
fiers. In Text, Speech and Dialogue, Lecture Notes
in Artificial Intelligence.
VERBOCEAN: Mining the Web for Fine-Grained Semantic Verb Relations 
Timothy Chklovski and Patrick Pantel 
Information Sciences Institute 
University of Southern California 
4676 Admiralty Way 
Marina del Rey, CA  90292 
{timc, pantel}@isi.edu 
 
Abstract 
Broad-coverage repositories of semantic relations 
between verbs could benefit many NLP tasks. We 
present a semi-automatic method for extracting 
fine-grained semantic relations between verbs. We 
detect similarity, strength, antonymy, enablement, 
and temporal happens-before relations between 
pairs of strongly associated verbs using lexico-
syntactic patterns over the Web. On a set of 29,165 
strongly associated verb pairs, our extraction algo-
rithm yielded 65.5% accuracy. Analysis of  
error types shows that on the relation strength we 
achieved 75% accuracy. We provide the  
resource, called VERBOCEAN, for download at 
http://semantics.isi.edu/ocean/. 
1 Introduction 
Many NLP tasks, such as question answering, 
summarization, and machine translation could 
benefit from broad-coverage semantic resources 
such as WordNet (Miller 1990) and EVCA (Eng-
lish Verb Classes and Alternations) (Levin 1993). 
These extremely useful resources have very high 
precision entries but have important limitations 
when used in real-world NLP tasks due to their 
limited coverage and prescriptive nature (i.e. they 
do not include semantic relations that are plausible 
but not guaranteed). For example, it may be valu-
able to know that if someone has bought an item, 
they may sell it at a later time. WordNet does not 
include the relation ?X buys Y? happens-before ?X 
sells Y? since it is possible to sell something with-
out having bought it (e.g. having manufactured or 
stolen it). 
Verbs are the primary vehicle for describing 
events and expressing relations between entities. 
Hence, verb semantics could help in many natural 
language processing (NLP) tasks that deal with 
events or relations between entities. For tasks 
which require canonicalization of natural language 
statements or derivation of plausible inferences 
from such statements, a particularly valuable re-
source is one which (i) relates verbs to one another 
and (ii) provides broad coverage of the verbs in the 
target language. 
In this paper, we present an algorithm that semi-
automatically discovers fine-grained verb seman-
tics by querying the Web using simple lexico-
syntactic patterns. The verb relations we discover 
are similarity, strength, antonymy, enablement, and 
temporal relations. Identifying these relations over 
29,165 verb pairs results in a broad-coverage re-
source we call VERBOCEAN. Our approach extends 
previously formulated ones that use surface pat-
terns as indicators of semantic relations between 
nouns (Hearst 1992; Etzioni 2003; Ravichandran 
and Hovy 2002). We extend these approaches in 
two ways: (i) our patterns indicate verb conjuga-
tion to increase their expressiveness and specificity 
and (ii) we use a measure similar to mutual infor-
mation to account for both the frequency of the 
verbs whose semantic relations are being discov-
ered as well as for the frequency of the pattern. 
2 Relevant Work 
In this section, we describe application domains 
that can benefit from a resource of verb semantics. 
We then introduce some existing resources and 
describe previous attempts at mining semantics 
from text. 
2.1 Applications 
Question answering is often approached by can-
onicalizing the question text and the answer text 
into logical forms. This approach is taken, inter 
alia, by a top-performing system (Moldovan et al 
2002). In discussing future work on the system?s 
logical form matching component, Rus (2002 p. 
143) points to incorporating entailment and causa-
tion verb relations to improve the matcher?s per-
formance. In other work, Webber et al (2002) 
have argued that successful question answering 
depends on lexical reasoning, and that lexical rea-
soning in turn requires fine-grained verb semantics 
in addition to troponymy (is-a relations between 
verbs) and antonymy. 
In multi-document summarization, knowing verb 
similarities is useful for sentence compression and 
for determining sentences that have the same 
meaning (Lin 1997). Knowing that a particular 
action happens before another or is enabled by 
another is also useful to determine the order of the 
events (Barzilay et al 2002). For example, to order 
summary sentences properly, it may be useful to 
know that selling something can be preceded by 
either buying, manufacturing, or stealing it. Fur-
thermore, knowing that a particular verb has a 
meaning stronger than another (e.g. rape vs. abuse 
and renovate vs. upgrade) can help a system pick 
the most general sentence. 
In lexical selection of verbs in machine transla-
tion and in work on document classification, prac-
titioners have argued for approaches that depend 
on wide-coverage resources indicating verb simi-
larity and membership of a verb in a certain class. 
In work on translating verbs with many counter-
parts in the target language, Palmer and Wu (1995) 
discuss inherent limitations of approaches which 
do not examine a verb?s class membership, and put 
forth an approach based on verb similarity. In 
document classification, Klavans and Kan (1998) 
demonstrate that document type is correlated with 
the presence of many verbs of a certain EVCA 
class (Levin 1993). In discussing future work, Kla-
vans and Kan point to extending coverage of the 
manually constructed EVCA resource as a way of 
improving the performance of the system. A wide-
coverage repository of verb relations including 
verbs linked by the similarity relation will provide 
a way to automatically extend the existing verb 
classes to cover more of the English lexicon. 
2.2 Existing resources 
Some existing broad-coverage resources on 
verbs have focused on organizing verbs into 
classes or annotating their frames or thematic roles. 
EVCA (English Verb Classes and Alternations) 
(Levin 1993) organizes verbs by similarity and 
participation / nonparticipation in alternation pat-
terns. It contains 3200 verbs classified into 191 
classes. Additional manually constructed resources 
include PropBank (Kingsbury et al 2002), Frame-
Net (Baker et al 1998), VerbNet (Kipper et al 
2000), and the resource on verb selectional restric-
tions developed by Gomez (2001). 
Our approach differs from the above in its focus. 
We relate verbs to each other rather than organize 
them into classes or identify their frames or the-
matic roles. WordNet does provide relations be-
tween verbs, but at a coarser level. We provide 
finer-grained relations such as strength, enable-
ment and temporal information. Also, in contrast 
with WordNet, we cover more than the prescriptive 
cases. 
2.3 Mining semantics from text 
Previous web mining work has rarely addressed 
extracting many different semantic relations from 
Web-sized corpus. Most work on extracting se-
mantic information from large corpora has largely 
focused on the extraction of is-a relations between 
nouns. Hearst (1992) was the first followed by 
recent larger-scale and more fully automated ef-
forts (Pantel and Ravichandran 2004; Etzioni et al 
2004; Ravichandran and Hovy 2002). Recently, 
Moldovan et al (2004) present a learning algo-
rithm to detect 35 fine-grained noun phrase rela-
tions. 
Turney (2001) studied word relatedness and 
synonym extraction, while Lin et al (2003) present 
an algorithm that queries the Web using lexical 
patterns for distinguishing noun synonymy and 
antonymy. Our approach addresses verbs and pro-
vides for a richer and finer-grained set of seman-
tics.  Reliability of estimating bigram counts on the 
web via search engines has been investigated by 
Keller and Lapata (2003). 
Semantic networks have also been extracted 
from dictionaries and other machine-readable re-
sources. MindNet (Richardson et al 1998) extracts 
a collection of triples of the type ?ducks have 
wings? and ?duck capable-of flying?. This re-
source, however, does not relate verbs to each 
other or provide verb semantics. 
3 Semantic relations among verbs 
In this section, we introduce and motivate the 
specific relations that we extract. Whilst the natural 
language literature is rich in theories of semantics 
(Barwise and Perry 1985; Schank and Abelson 
1977), large-coverage manually created semantic 
resources typically only organize verbs into a flat 
or shallow hierarchy of classes (such as those de-
scribed in Section 2.2). WordNet identifies synon-
ymy, antonymy, troponymy, and cause. As sum-
marized in Figure 1, Fellbaum (1998) discusses a 
finer-grained analysis of entailment, while the 
WordNet database does not distinguish between, 
e.g., backward presupposition (forget :: know, 
where know must have happened before forget) 
from proper temporal inclusion (walk :: step). In 
formulating our set of relations, we have relied on 
the finer-grained analysis, explicitly breaking out 
the temporal precedence between entities. 
In selecting the relations to identify, we aimed at 
both covering the relations described in WordNet 
and covering the relations present in our collection 
Figure 1. Fellbaum?s (1998) entailment hierarchy.
+Temporal Inclusion
Entailment
-Temporal Inclusion
+Troponymy
 (coextensiveness)
 march-walk
-Troponymy
 (proper inclusion)
 walk-step
Backward
Presupposition
forget-know
Cause
show-see
of strongly associated verb pairs. We relied on the 
strongly associated verb pairs, described in Section 
4.4, for computational efficiency. The relations we 
identify were experimentally found to cover 99 out 
of 100 randomly selected verb pairs. 
Our algorithm identifies six semantic relations 
between verbs. These are summarized in Table 1 
along with their closest corresponding WordNet 
category and the symmetry of the relation (whether 
V1 rel V2 is equivalent to V2 rel V1). 
Similarity. As Fellbaum (1998) and the tradition 
of organizing verbs into similarity classes indicate, 
verbs do not neatly fit into a unified is-a (tro-
ponymy) hierarchy. Rather, verbs are often similar 
or related. Similarity between action verbs, for 
example, can arise when they differ in connota-
tions about manner or degree of action. Examples 
extracted by our system include maximize :: en-
hance, produce :: create, reduce :: restrict. 
Strength. When two verbs are similar, one may 
denote a more intense, thorough, comprehensive or 
absolute action. In the case of change-of-state 
verbs, one may denote a more complete change. 
We identify this as the strength relation. Sample 
verb pairs extracted by our system, in the order 
weak to strong, are: taint :: poison, permit :: au-
thorize, surprise :: startle, startle :: shock. Some 
instances of strength sometimes map to WordNet?s 
troponymy relation. 
Strength, a subclass of similarity, has not been 
identified in broad-coverage networks of verbs, but 
may be of particular use in natural language gen-
eration and summarization applications. 
Antonymy. Also known as semantic opposition, 
antonymy between verbs has several distinct sub-
types. As discussed by Fellbaum (1998), it can 
arise from switching thematic roles associated with 
the verb (as in buy :: sell, lend :: borrow). There is 
also antonymy between stative verbs (live :: die, 
differ :: equal) and antonymy between sibling 
verbs which share a parent (walk :: run) or an en-
tailed verb (fail :: succeed both entail try). 
Antonymy also systematically interacts with the 
happens-before relation in the case of restitutive 
opposition (Cruse 1986). This subtype is exempli-
fied by damage :: repair, wrap :: unwrap. In terms 
of the relations we recognize, it can be stated that 
restitutive-opposition(V1, V2) = happens-
before(V1, V2), and antonym(V1, V2). Examples of 
antonymy extracted by our system include: assem-
ble :: dismantle; ban :: allow; regard :: condemn, 
roast :: fry. 
Enablement. This relation holds between two 
verbs V1 and V2 when the pair can be glossed as V1 
is accomplished by V2. Enablement is classified as 
a type of causal relation by Barker and Szpakowicz 
(1995). Examples of enablement extracted by our 
system include: assess :: review and accomplish :: 
complete. 
Happens-before. This relation indicates that the 
two verbs refer to two temporally disjoint intervals 
or instances. WordNet?s cause relation, between a 
causative and a resultative verb (as in buy :: own), 
would be tagged as instances of happens-before by 
our system. Examples of the happens-before rela-
tion identified by our system include marry :: di-
vorce, detain :: prosecute, enroll :: graduate, 
schedule :: reschedule, tie :: untie. 
4 Approach 
We discover the semantic relations described 
above by querying the Web with Google for 
lexico-syntactic patterns indicative of each rela-
tion. Our approach has two stages. First, we iden-
tify pairs of highly associated verbs co-occurring 
on the Web with sufficient frequency using previ-
ous work by Lin and Pantel (2001), as described in 
Section 4.4. Next, for each verb pair, we tested 
lexico-syntactic patterns, calculating a score for 
each possible semantic relation as described in 
Section 4.2. Finally, as described in Section 4.3, 
we compare the strengths of the individual seman-
tic relations and, preferring the most specific and 
then strongest relations, output a consistent set as 
the final output.  As a guide to consistency, we use 
a simple theory of semantics indicating which se-
mantic relations are subtypes of other ones, and 
which are compatible and which are mutually ex-
clusive. 
4.1 Lexico-syntactic patterns 
The lexico-syntactic patterns were manually se-
lected by examining pairs of verbs in known se-
mantic relations. They were refined to decrease 
capturing wrong parts of speech or incorrect se-
mantic relations. We used 50 verb pairs and the 
overall process took about 25 hours. 
We use a total of 35 patterns, which are listed in 
Table 2 along with the estimated frequency of hits. 
Table 1. Semantic relations identified in VERBOCEAN.  Sib-
lings in the WordNet column refers to terms with the same 
troponymic parent, e.g. swim and fly. 
SEMANTIC 
RELATION 
EXAMPLE 
Alignment with 
WordNet 
Symmetric
similarity transform :: integrate synonyms or siblings Y 
strength wound :: kill synonyms or siblings N 
antonymy open :: close antonymy Y 
enablement fight :: win cause N 
happens-
before 
buy :: sell; 
marry :: divorce 
cause 
entailment, no 
temporal inclusion 
N 
 
Note that our patterns specify the tense of the verbs 
they accept. When instantiating these patterns, we 
conjugate as needed. For example, ?both Xed and 
Yed? instantiates on sing and dance as ?both sung 
and danced?. 
4.2 Testing for a semantic relation 
In this section, we describe how the presence of 
a semantic relation is detected. We test the rela-
tions with patterns exemplified in Table 2. We 
adopt an approach inspired by mutual information 
to measure the strength of association, denoted 
Sp(V1, V2),  between three entities: a verb pair V1 
and V2 and a lexico-syntactic pattern p: 
 
)()()(
),,(),(
21
21
21 VPVPpP
VpVPVVS p ??=
 
The probabilities in the denominator are difficult 
to calculate directly from search engine results. For 
a given lexico-syntactic pattern, we need to esti-
mate the frequency of the pattern instantiated with 
appropriately conjugated verbs. For verbs, we need 
to estimate the frequency of the verbs, but avoid 
counting other parts-of-speech (e.g. chair as a 
noun or painted as an adjective). Another issue is 
that some relations are symmetric (similarity and 
antonymy), while others are not (strength, enable-
ment, happens-before). For symmetric relations 
only, the verbs can fill the lexico-syntactic pattern 
in either order. To address these issues, we esti-
mate Sp(V1,V2) using: 
 
N
CVtohits
N
CVtohits
N
phits
N
VpVhits
VVS
vvest
P ????
? )"(")"(")(
),,(
),(
21
21
21
 
for asymmetric relations and 
N
CVtohits
N
CVtohits
N
phits
N
VpVhits
N
VpVhits
VVS
vvest
P ????
+
? )"(")"(")(*2
),,(),,(
),(
21
1221
21
 
for symmetric relations. 
Here, hits(S) denotes the number of documents 
containing the string S, as returned by Google. N is 
the number of words indexed by the search engine 
(N ? 7.2 ? 1011), Cv is a correction factor to obtain 
the frequency of the verb V in all tenses from the 
frequency of the pattern ?to V?. Based on several 
verbs, we have estimated Cv = 8.5. Because pattern 
counts, when instantiated with verbs, could not be 
estimated directly, we have computed the frequen-
cies of the patterns in a part-of-speech tagged 
500M word corpus and used it to estimate the ex-
pected number of hits hitsest(p) for each pattern.  
We estimated the N with a similar method. 
We say that the semantic relation Sp indicated by 
lexico-syntactic pattern p is present between V1 
and V2 if 
 Sp(V1,V2) > C1 
As a result of tuning the system on a tuning set of 
50 verb pairs, C1 = 8.5. 
Additional test for asymmetric relations.  For 
the asymmetric relations, we require not only that 
),( 21 VVSP exceed a certain threshold, but that 
there be strong asymmetry of the relation: 
 
2
12
21
12
21
),,(
),,(
),(
),(
C
VpVhits
VpVhits
VVS
VVS
p
p >=  
From the tuning set, C2 = 5. 
4.3 Pruning identified semantic relations 
Given a pair of semantic relations from the set 
we identify, one of three cases can arise: (i) one 
Table 2. Semantic relations and the 35 surface patterns used 
to identify them. Total number of patterns for that relation is 
shown in parentheses. In patterns, ?*? matches any single 
word.  Punctuation does not count as words by the search 
engine used (Google). 
SEMANTIC 
RELATION 
 Surface 
Patterns 
Hitsest for 
patterns
narrow  
similarity (2)* 
X ie Y 
Xed ie Yed 219,480
broad  
similarity (2)* 
Xed and Yed 
to X and Y 154,518,326
strength (8) 
X even Y 
Xed even Yed 
X and even Y 
Xed and even Yed 
Y or at least X 
Yed or at least Xed 
not only Xed but Yed 
not just Xed but Yed  
1,016,905
enablement (4) 
Xed * by Ying the 
Xed * by Ying or 
to X * by Ying the 
to X * by Ying or 
2,348,392
antonymy (7) 
either X or Y 
either Xs or Ys 
either Xed or Yed 
either Xing or Ying 
whether to X or Y 
Xed * but Yed 
to X * but Y 
18,040,916
happens-before 
(12) 
to X and then Y 
to X * and then Y 
Xed and then Yed 
Xed * and then Yed 
to X and later Y 
Xed and later Yed 
to X and subsequently Y 
Xed and subsequently Yed 
to X and eventually Y 
Xed and eventually Yed 
8,288,871
*narrow- and broad- similarity overlap in their coverage 
and are treated as a single category, similarity, when 
postprocessed. Narrow similarity tests for rare patterns 
and hitsest for it had to be approximated rather than 
estimated from the smaller corpus. 
relation is more specific (strength is more specific 
than similarity, enablement is more specific than 
happens-before), (ii) the relations are compatible 
(antonymy and happens-before), where presence of 
one does not imply or rule out presence of the 
other, and (iii) the relations are incompatible (simi-
larity and antonymy). 
It is not uncommon for our algorithm to identify 
presence of several relations, with different 
strengths. To produce the most likely output, we 
use semantics of compatibility of the relations to 
output the most likely one(s).  The rules are as 
follows: 
If the frequency was too low (less than 10 on the 
pattern ?X * Y? OR ?Y * X? OR ?X * * Y? OR ?Y 
* * X?), output that the statements are unrelated 
and stop. 
If happens-before is detected, output presence of 
happens-before (additional relation may still be 
output, if detected). 
If happens-before is not detected, ignore detec-
tion of enablement (because enablement is more 
specific than happens-before, but is sometimes 
falsely detected in the absence of happens-before). 
If strength is detected, score of similarity is ig-
nored (because strength is more specific than simi-
larity). 
Of the relations strength, similarity, opposition 
and enablement which were detected (and not ig-
nored), output the one with highest Sp. 
If nothing has been output to this point, output 
unrelated. 
4.4 Extracting highly associated verb pairs 
To exhaustively test the more than 64 million 
unordered verb pairs for WordNet?s more than 
11,000 verbs would be computationally intractable. 
Instead, we use a set of highly associated verb 
pairs output by a paraphrasing algorithm called 
DIRT (Lin and Pantel 2001).  Since we are able to 
test up to 4000 verb pairs per day on a single ma-
chine (we issue at most 40 queries per test and 
each query takes approximately 0.5 seconds), we 
are able to test several dozen associated verbs for 
each verb in WordNet in a matter of weeks. 
Lin and Pantel (2001) describe an algorithm 
called DIRT (Discovery of Inference Rules from 
Text) that automatically learns paraphrase expres-
sions from text.  It is a generalization of previous 
algorithms that use the distributional hypothesis 
(Harris 1985) for finding similar words.  Instead of 
applying the hypothesis to words, Lin and Pantel 
applied it to paths in dependency trees.  Essen-
tially, if two paths tend to link the same sets of 
words, they hypothesized that the meanings of the 
corresponding paths are similar. It is from paths of 
the form subject-verb-object that we extract our set 
of associated verb pairs. Hence, this paper is con-
cerned only with relations between transitive 
verbs. 
A path, extracted from a parse tree, is an expres-
sion that represents a binary relation between two 
nouns. A set of paraphrases was generated for each 
pair of associated paths.  For example, using a 
1.5GB newspaper corpus, here are the 20 most 
associated paths to ?X solves Y? generated by 
DIRT: 
Y is solved by X, X resolves Y, X finds 
a solution to Y, X tries to solve Y, X 
deals with Y, Y is resolved by X, X ad-
dresses Y, X seeks a solution to Y, X 
does something about Y, X solution to 
Y, Y is resolved in X, Y is solved 
through X, X rectifies Y, X copes with 
Y, X overcomes Y, X eases Y, X tackles 
Y, X alleviates Y, X corrects Y, X is a 
solution to Y, X makes Y worse, X irons 
out Y 
This list of associated paths looks tantalizingly 
close to the kind of axioms that would prove useful 
in an inference system. However, DIRT only out-
puts pairs of paths that have some semantic rela-
tion. We used these as our set to extract finer-
grained relations. 
5 Experimental results 
In this section, we empirically evaluate the accu-
racy of VERBOCEAN1. 
5.1 Experimental setup 
We studied 29,165 pairs of verbs. Applying 
DIRT to a 1.5GB newspaper corpus2, we extracted 
4000 paths that consisted of single verbs in the 
relation subject-verb-object (i.e. paths of the form 
?X verb Y?) whose verbs occurred in at least 150 
documents on the Web. For example, from the 20 
most associated paths to ?X solves Y? shown in 
Section 4.4, the following verb pairs were ex-
tracted: 
solves :: resolves 
solves :: addresses 
solves :: rectifies 
solves :: overcomes 
solves :: eases 
solves :: tackles 
solves :: corrects 
5.2 Accuracy 
We classified each verb pair according to the 
semantic relations described in Section 2. If the 
system does not identify any semantic relation for 
a verb pair, then the system tags the pair as having 
                                                     
1 VERBOCEAN is available for download at 
http://semantics.isi.edu/ocean/. 
2 The 1.5GB corpus consists of San Jose Mercury, 
Wall Street Journal and AP Newswire articles from the 
TREC-9 collection. 
no relation. To evaluate the accuracy of the sys-
tem, we randomly sampled 100 of these verb pairs, 
and presented the classifications to two human 
judges. The adjudicators were asked to judge 
whether or not the system classification was ac-
ceptable (i.e. whether or not the relations output by 
the system were correct). Since the semantic rela-
tions are not disjoint (e.g. mop is both stronger 
than and similar to sweep), multiple relations may 
be appropriately acceptable for a given verb pair. 
The judges were also asked to identify their pre-
ferred semantic relations (i.e. those relations which 
seem most plausible). Table 3 shows five randomly 
selected pairs along with the judges? responses. 
The Appendix shows sample relationships discov-
ered by the system. 
Table 4 shows the accuracy of the system. The 
baseline system consists of labeling each pair with 
the most common semantic relation, similarity, 
which occurs 33 times. The Tags Correct column 
represents the percentage of verb pairs whose sys-
tem output relations were deemed correct. The 
Preferred Tags Correct column gives the percent-
age of verb pairs whose system output relations 
matched exactly the human?s preferred relations. 
The Kappa statistic (Siegel and Castellan 1988) for 
the task of judging system tags as correct and in-
correct is ? = 0.78 whereas the task of identifying 
the preferred semantic relation has ? = 0.72. For 
the latter task, the two judges agreed on 73 of the 
100 semantic relations. 73% gives an idea of an 
upper bound for humans on this task. On these 73 
relations, the system achieved a higher accuracy of 
70.0%. The system is allowed to output the hap-
pens-before relation in combination with other 
relations. On the 17 happens-before relations out-
put by the system, 67.6% were judged correct. 
Ignoring the happens-before relations, we achieved 
a Tags Correct precision of 68%. 
Table 5 shows the accuracy of the system on 
each of the relations. The stronger-than relation is 
a subset of the similarity relation. Considering a 
coarser extraction where stronger-than relations 
are merged with similarity, the task of judging 
system tags and the task of identifying the pre-
ferred semantic relation both jump to 68.2% accu-
racy. Also, the overall accuracy of the system 
climbs to 68.5%. 
As described in Section 2, WordNet contains 
verb semantic relations. A significant percentage 
of our discovered relations are not covered by 
WordNet?s coarser classifications. Of the 40 verb 
pairs whose system relation was tagged as correct 
by both judges in our accuracy experiments and 
whose tag was not ?no relation?, only 22.5% of 
them existed in a WordNet relation. 
5.3 Discussion 
The experience of extracting these semantic rela-
tions has clarified certain important challenges. 
While relying on a search engine allows us to 
query a corpus of nearly a trillion words, some 
issues arise: (i) the number of instances has to be 
approximated by the number of hits (documents); 
(ii) the number of hits for the same query may 
fluctuate over time; and (iii) some needed counts 
are not directly available. We addressed the latter 
issue by approximating these counts using a 
smaller corpus. 
Table 3. Five randomly selected pairs along with the system tag (in bold) and the judges? responses. 
 CORRECT PREFERRED SEMANTIC RELATION 
PAIRS WITH SYSTEM TAG (IN BOLD) JUDGE 1 JUDGE 2 JUDGE 1 JUDGE 2 
X absolve Y is similar to X vindicate Y Yes Yes is similar to is similar to 
X bottom Y has no relation with X abate Y Yes Yes has no relation with has no relation with 
X outrage Y happens-after / is stronger than X shock Y Yes Yes happens-before / is 
stronger than 
happens-before/ is stronger 
than 
X pool Y has no relation with X increase Y Yes No has no relation with can result in 
X insure Y is similar to X expedite Y No No has no relation with has no relation with 
 
Table 4. Accuracy of system-discovered relations. 
 ACCURACY 
 Tags 
Correct 
Preferred Tags 
Correct 
Baseline 
Correct 
Judge 1 66% 54% 24% 
Judge 2 65% 52% 20% 
Average 65.5% 53% 22% 
 
Table 5. Accuracy of each semantic relation. 
SEMANTIC 
RELATION 
SYSTEM 
TAGS 
Tags 
Correct 
Preferred Tags 
Correct 
similarity 41 63.4% 40.2% 
strength 14 75.0% 75.0% 
antonymy 8 50.0% 43.8% 
enablement 2 100% 100% 
no relation 35 72.9% 72.9% 
happens before 17 67.6% 55.9% 
 
We do not detect entailment with lexico-
syntactic patterns.  In fact, we propose that whether 
the entailment relation holds between V1 and V2 
depends on the absence of another verb V1' in the 
same relationship with V2. For example, given the 
relation marry happens-before divorce, we can 
conclude that divorce entails marry. But, given the 
relation buy happens-before sell, we cannot con-
clude entailment since manufacture can also hap-
pen before sell. This also applies to the enablement 
and strength relations. 
Corpus-based methods, including ours, hold the 
promise of wide coverage but are weak on dis-
criminating senses. While we hope that applica-
tions will benefit from this resource as is, an inter-
esting next step would be to augment it with sense 
information. 
6 Future work 
There are several ways to improve the accuracy 
of the current algorithm and to detect relations 
between low frequency verb pairs. One avenue 
would be to automatically learn or manually craft 
more patterns and to extend the pattern vocabulary 
(when developing the system, we have noticed that 
different registers and verb types require different 
patterns).  Another possibility would be to use 
more relaxed patterns when the part of speech 
confusion is not likely (e.g. ?eat? is a common 
verb which does not have a noun sense, and pat-
terns need not protect against noun senses when 
testing such verbs). 
Our approach can potentially be extended to 
multiword paths.  DIRT actually provides two 
orders of magnitude more relations than the 29,165 
single verb relations (subject-verb-object) we ex-
tracted. On the same 1GB corpus described in Sec-
tion 5.1, DIRT extracted over 200K paths and 6M 
unique paraphrases. These provide an opportunity 
to create a much larger corpus of semantic rela-
tions, or to construct smaller, in-depth resources 
for selected subdomains.  For example, we could 
extract that take a trip to is similar to travel to, and 
that board a plane happens before deplane. 
If the entire database is viewed as a graph, we 
currently leverage and enforce only local consis-
tency.  It would be useful to enforce global consis-
tency, e.g. V1 stronger-than V2, and V2 stronger-
than V3 indicates that V1 stronger-than V3, which 
may be leveraged to identify additional relations or 
inconsistent relations (e.g. V3 stronger-than V1). 
Finally, as discussed in Section 5.3, entailment 
relations may be derivable by processing the com-
plete graph of the identified semantic relation. 
7 Conclusions 
We have demonstrated that certain fine-grained 
semantic relations between verbs are present on the 
Web, and are extractable with a simple pattern-
based approach. In addition to discovering rela-
tions identified in WordNet, such as opposition and 
enablement, we obtain strong results on strength 
relations (for which no wide-coverage resource is 
available). On a set of 29,165 associated verb 
pairs, experimental results show an accuracy of 
65.5% in assigning similarity, strength, antonymy, 
enablement, and happens-before. 
Further work may refine extraction methods and 
further process the mined semantics to derive other 
relations such as entailment. 
We hope to open the way to inferring implied, 
but not stated assertions and to benefit applications 
such as question answering, information retrieval, 
and summarization. 
Acknowledgments 
The authors wish to thank the reviewers for their 
helpful comments and Google Inc. for supporting 
high volume querying of their index. This research 
was partly supported by NSF grant #EIA-0205111. 
References  
Baker, C.; Fillmore, C.; and Lowe, J. 1998. The 
Berkeley FrameNet project. In Proceedings of 
COLING-ACL. Montreal, Canada. 
Barker, K.; and Szpakowicz, S. 1995. Interactive 
Semantic Analysis of Clause-Level 
Relationships. In Proceedings of PACLING `95. 
Brisbane.  
Barwise, J. and Perry, J. 1985. Semantic innocence 
and uncompromising situations. In: Martinich, 
A. P. (ed.) The Philosophy of Language. New 
York: Oxford University Press. pp. 401?413. 
Barzilay, R.; Elhadad, N.; and McKeown, K. 2002. 
Inferring strategies for sentence ordering in 
multidocument summarization. JAIR, 17:35?55. 
Cruse, D. 1992 Antonymy Revisited: Some 
Thoughts on the Relationship between Words 
and Concepts, in A. Lehrer and E.V. Kittay 
(eds.), Frames, Fields, and Contrasts, Hillsdale, 
NJ, Lawrence Erlbaum associates, pp. 289-306. 
Etzioni, O.; Cafarella, M.; Downey, D.; Kok, S.; 
Popescu, A.; Shaked, T.; Soderland, S.; Weld, 
D.; and Yates, A. 2004. Web-scale information 
extraction in KnowItAll. To appear in WWW-
2004. 
Fellbaum, C. 1998. Semantic network of English 
verbs. In Fellbaum, (ed). WordNet: An 
Electronic Lexical Database, MIT Press. 
Gomez, F. 2001. An Algorithm for Aspects of 
Semantic Interpretation Using an Enhanced 
WordNet. In NAACL-2001, CMU, Pittsburgh.  
Harris, Z. 1985. Distributional Structure. In: Katz, 
J. J. (ed.), The Philosophy of Linguistics. New 
York: Oxford University Press. pp. 26?47. 
Hearst, M. 1992. Automatic acquisition of 
hyponyms from large text corpora. In COLING-
92. pp. 539?545. Nantes, France. 
Keller, F.and Lapata, M. 2003. Using the Web to 
Obtain Frequencies for Unseen Bigrams. 
Computational Linguistics 29:3.  
Kingsbury, P; Palmer, M.; and Marcus, M. 2002. 
Adding semantic annotation to the Penn 
TreeBank. In Proceedings of HLT-2002. San 
Diego, California. 
Kipper, K.; Dang, H.; and Palmer, M. 2000. Class-
based construction of a verb lexicon. In Pro-
ceedings of AAAI-2000. Austin, TX. 
Klavans, J. and Kan, M. 1998. Document classi-
fication: Role of verb in document analysis. In 
Proceedings COLING-ACL '98. Montreal, 
Canada. 
Levin, B. 1993. English Verb Classes and 
Alternations: A Preliminary Investigation. 
University of Chicago Press, Chicago, IL. 
Lin, C-Y. 1997. Robust Automated Topic 
Identification. Ph.D. Thesis. University of 
Southern California. 
Lin, D. and Pantel, P. 2001. Discovery of inference 
rules for question answering. Natural Language 
Engineering, 7(4):343?360. 
Lin, D.; Zhao, S.; Qin, L.; and Zhou, M. 2003. 
Identifying synonyms among distributionally 
similar words. In Proceedings of IJCAI-03. 
pp.1492?1493. Acapulco, Mexico. 
Miller, G. 1990. WordNet: An online lexical 
database. International Journal of Lexicography, 
3(4). 
Moldovan, D.; Badulescu, A.; Tatu, M.; Antohe, 
D.; and Girju, R. 2004. Models for the semantic 
classification of noun phrases. To appear in 
Proceedings of HLT/NAACL-2004 Workshop on 
Computational Lexical Semantics. 
Moldovan, D.; Harabagiu, S.; Girju, R.; 
Morarescu, P.; Lacatusu, F.; Novischi, A.; 
Badulescu, A.; and Bolohan, O. 2002. LCC tools 
for question answering. In Notebook of the 
Eleventh Text REtrieval Conference (TREC-
2002). pp. 144?154. 
Palmer, M., Wu, Z. 1995. Verb Semantics for 
English-Chinese Translation Machine 
Translation, 9(4). 
Pantel, P. and Ravichandran, D. 2004. 
Automatically labeling semantic classes. To 
appear in Proceedings of HLT/NAACL-2004. 
Boston, MA. 
Ravichandran, D. and Hovy, E., 2002.  Learning 
surface text patterns for a question answering 
system.  In Proceedings of ACL-02. 
Philadelphia, PA. 
Richardson, S.; Dolan, W.; and Vanderwende, L. 
1998. MindNet: acquiring and structuring 
semantic information from text. In Proceedings 
of COLING '98. 
Rus, V. 2002. Logic Forms for WordNet Glosses. 
Ph.D. Thesis. Southern Methodist University. 
Schank, R. and Abelson, R. 1977. Scripts, Plans, 
Goals and Understanding: An Inquiry into 
Human Knowledge Structures. Lawrence 
Erlbaum Associates. 
Siegel, S. and Castellan Jr., N. 1988. 
Nonparametric Statistics for the Behavioral 
Sciences. McGraw-Hill. 
Turney, P. 2001. Mining the Web for synonyms: 
PMI-IR versus LSA on TOEFL. In Proceedings 
ECML-2001. Freiburg, Germany. 
Webber, B.; Gardent, C.; and Bos, J. 2002. 
Position statement: Inference in question 
answering. In Proceedings of LREC-2002. Las 
Palmas, Spain. 
Appendix. Sample relations extracted by our system. 
SEMANTIC 
RELATION EXAMPLES 
SEMANTIC 
RELATION EXAMPLES 
SEMANTIC 
RELATION EXAMPLES 
similarity 
maximize :: enhance 
produce :: create 
reduce :: restrict 
enablement 
assess :: review 
accomplish :: complete 
double-click :: click 
happens 
before 
detain :: prosecute 
enroll :: graduate 
schedule :: reschedule 
strength 
permit :: authorize 
surprise :: startle 
startle :: shock 
antonymy 
assemble :: dismantle 
regard :: condemn 
roast :: fry 
 
 
Proceedings of the 2006 Conference on Empirical Methods in Natural Language Processing (EMNLP 2006), pages 423?430,
Sydney, July 2006. c?2006 Association for Computational Linguistics
Automatically Assessing Review Helpfulness 
 
Soo-Min Kim?, Patrick Pantel?, Tim Chklovski?, Marco Pennacchiotti? 
?Information Sciences Institute 
University of Southern California 
4676 Admiralty Way 
Marina del Rey, CA  90292 
{skim,pantel,timc}@isi.edu 
?ART Group - DISP 
University of Rome ?Tor Vergata? 
Viale del Politecnico 1 
Rome, Italy 
pennacchiotti@info.uniroma2.it
 
 
 
Abstract 
User-supplied reviews are widely and 
increasingly used to enhance e-
commerce and other websites. Because 
reviews can be numerous and varying in 
quality, it is important to assess how 
helpful each review is. While review 
helpfulness is currently assessed manu-
ally, in this paper we consider the task 
of automatically assessing it. Experi-
ments using SVM regression on a vari-
ety of features over Amazon.com 
product reviews show promising results, 
with rank correlations of up to 0.66. We 
found that the most useful features in-
clude the length of the review, its uni-
grams, and its product rating. 
1 Introduction 
Unbiased user-supplied reviews are solicited 
ubiquitously by online retailers like Ama-
zon.com, Overstock.com, Apple.com and Epin-
ions.com, movie sites like imdb.com, traveling 
sites like citysearch.com, open source software 
distributors like cpanratings.perl.org, and count-
less others. Because reviews can be numerous 
and varying in quality, it is important to rank 
them to enhance customer experience. 
In contrast with ranking search results, assess-
ing relevance when ranking reviews is of little 
importance because reviews are directly associ-
ated with the relevant product or service. Instead, 
a key challenge when ranking reviews is to de-
termine which reviews the customers will find 
helpful. 
Most websites currently rank reviews by their 
recency or product rating (e.g., number of stars 
in Amazon.com reviews). Recently, more sophis-
ticated ranking schemes measure reviews by their 
helpfulness, which is typically estimated by hav-
ing users manually assess it. For example, on 
Amazon.com, an interface allows customers to 
vote whether a particular review is helpful or not. 
Unfortunately, newly written reviews and re-
views with few votes cannot be ranked as several 
assessments are required in order to properly es-
timate helpfulness. For example, for all MP3 
player products on Amazon.com, 38% of the 
20,919 reviews received three or fewer helpful-
ness votes. Another problem is that low-traffic 
items may never gather enough votes. Among the 
MP3 player reviews that were authored at least 
three months ago on Amazon.com, still only 31% 
had three or fewer helpfulness votes. 
It would be useful to assess review helpfulness 
automatically, as soon as the review is written. 
This would accelerate determining a review?s 
ranking and allow a website to provide rapid 
feedback to review authors. 
In this paper, we investigate the task of auto-
matically predicting review helpfulness using a 
machine learning approach. Our main contribu-
tions are: 
? A system for automatically ranking reviews 
according to helpfulness; using state of the art 
SVM regression, we empirically evaluate our 
system on a real world dataset collected from 
Amazon.com on the task of reconstructing the 
helpfulness ranking; and 
? An analysis of different classes of features 
most important to capture review helpful-
ness; including structural (e.g., html tags, 
punctuation, review length), lexical (e.g., n-
grams), syntactic (e.g., percentage of verbs and 
nouns), semantic (e.g., product feature men-
tions), and meta-data (e.g., star rating). 
2 Relevant Work 
The task of automatically assessing product re-
view helpfulness is related to these broader areas 
423
of research: automatic analysis of product re-
views, opinion and sentiment analysis, and text 
classification. 
In the thriving area of research on automatic 
analysis and processing of product reviews (Hu 
and Liu 2004; Turney 2002; Pang and Lee 2005), 
little attention has been paid to the important task 
studied here ? assessing review helpfulness. Pang 
and Lee (2005) have studied prediction of prod-
uct ratings, which may be particularly relevant 
due to the correlation we find between product 
rating and the helpfulness of the review (dis-
cussed in Section 5). However, a user?s overall 
rating for the product is often already available. 
Helpfulness, on the other hand, is valuable to 
assess because it is not explicitly known in cur-
rent approaches until many users vote on the 
helpfulness of a review.  
In opinion and sentiment analysis, the focus is 
on distinguishing between statements of fact vs. 
opinion, and on detecting the polarity of senti-
ments being expressed. Many researchers have 
worked in various facets of opinion analysis. 
Pang et al (2002) and Turney (2002) classified 
sentiment polarity of reviews at the document 
level.  Wiebe et al (1999) classified sentence 
level subjectivity using syntactic classes such as 
adjectives, pronouns and modal verbs as features.  
Riloff and Wiebe (2003) extracted subjective 
expressions from sentences using a bootstrapping 
pattern learning process. Yu and Hatzivassi-
loglou (2003) identified the polarity of opinion 
sentences using semantically oriented words. 
These techniques were applied and examined in 
different domains, such as customer reviews (Hu 
and Liu 2004) and news articles (TREC novelty 
track 2003 and 2004).  
In text classification, systems typically use 
bag-of-words models, although there is some 
evidence of benefits when introducing relevant 
semantic knowledge (Gabrilovich and Mark-
ovitch, 2005). In this paper, we explore the use of 
some semantic features for review helpfulness 
ranking. Another potential relevant classification 
task is academic and commercial efforts on de-
tecting email spam messages1, which aim to cap-
ture a much broader notion of helpfulness. For an 
SVM-based approach, see (Drucker et al 1999).  
Finally, a related area is work on automatic es-
say scoring, which seeks to rate the quality of an 
essay (Attali and Burstein 2006; Burstein et al 
2004). The task is important for reducing the 
human effort required in scoring large numbers 
                                                     
1 See http://www.ceas.cc/, http://spamconference.org/  
of student essays regularly written for standard 
tests such as the GRE. The exact scoring ap-
proaches developed in commercial systems are 
often not disclosed. However, more recent work 
on one of the major systems, e-rater 2.0, has fo-
cused on systematizing and simplifying the set of 
features used (Attali and Burstein 2006). Our 
choice of features to test was partially influenced 
by the features discussed by Attali and Burstein. 
At the same time, due to differences in the tasks, 
we did not use features aimed at assessing essay 
structure such as discourse structure analysis fea-
tures. Our observations suggest that even helpful 
reviews vary widely in their discourse structure. 
We present the features which we have used be-
low, in Section 3.2. 
3 Modeling Review Helpfulness 
In this section, we formally define the learning 
task and we investigate several features for as-
sessing review helpfulness. 
3.1 Task Definition 
Formally, given a set of reviews R for a particu-
lar product, our task is to rank the reviews ac-
cording to their helpfulness. We define a review 
helpfulness function, h, as: 
 ( ) ( )( ) ( )rratingrrating
rrating
Rrh
?+
+
+=?  (1) 
where rating+(r) is the number of people that will 
find a review helpful and rating-(r) is the number 
of people that will find the review unhelpful. For 
evaluation, we resort to estimates of h from man-
ual review assessments on websites like Ama-
zon.com, as described in Section 4. 
3.2 Features 
One aim of this paper is to investigate how well 
different classes of features capture the helpful-
ness of a review. We experimented with various 
features organized in five classes: Structural, 
Lexical, Syntactic, Semantic, and Meta-data. Be-
low we describe each feature class in turn. 
Structural Features 
Structural features are observations of the docu-
ment structure and formatting. Properties such as 
review length and average sentence length are 
hypothesized to relate structural complexity to 
helpfulness. Also, HTML formatting tags could 
help in making a review more readable, and con-
sequently more helpful. We experimented with 
the following features: 
424
? Length (LEN): The total number of tokens in a 
syntactic analysis2 of the review. 
? Sentential (SEN): Observations of the sen-
tences, including the number of sentences, the 
average sentence length, the percentage of 
question sentences, and the number of excla-
mation marks. 
? HTML (HTM): Two features for the number of 
bold tags <b> and line breaks <br>. 
Lexical Features 
Lexical features capture the words observed in 
the reviews. We experimented with two sets of 
features: 
? Unigram (UGR): The tf-idf statistic of each 
word occurring in a review. 
? Bigram (BGR): The tf-idf statistic of each bi-
gram occurring in a review. 
For both unigrams and bigrams, we used lemma-
tized words from a syntactic analysis of the re-
views and computed the tf-idf statistic (Salton 
and McGill 1983) using the following formula: 
 ( )
N
idftf
idftf
log?=  
where N is the number of tokens in the review. 
Syntactic Features 
Syntactic features aim to capture the linguistic 
properties of the review. We grouped them into 
the following feature set: 
? Syntax (SYN): Includes the percentage of 
parsed tokens that are open-class (i.e., nouns, 
verbs, adjectives and adverbs), the percentage 
of tokens that are nouns, the percentage of to-
kens that are verbs, the percentage of tokens 
that are verbs conjugated in the first person, 
and the percentage of tokens that are adjectives 
or adverbs. 
Semantic Features 
Most online reviews are fairly short; their spar-
sity suggests that bigram features will not per-
form well (which is supported by our 
experiments described in Section 5.3). Although 
semantic features have rarely been effective in 
many text classification problems (Moschitti and 
Basili 2004), there is reason here to hypothesize 
that a specialized vocabulary of important words 
might help with the sparsity. We hypothesized 
                                                     
2  Reviews are analyzed using the Minipar dependency 
parser (Lin 1994). 
that good reviews will often contain: i) refer-
ences to the features of a product (e.g., the LCD 
and resolution of a digital camera), and ii) men-
tions of sentiment words (i.e., words that express 
an opinion such as ?great screen?). Below we 
describe two families of features that capture 
these semantic observations within the reviews: 
? Product-Feature (PRF): The features of prod-
ucts that occur in the review, e.g., capacity of 
MP3 players and zoom of a digital camera. 
This feature counts the number of lexical 
matches that occur in the review for each prod-
uct feature. There is no trivial way of obtaining 
a list of all the features of a product. In Section 
5.1 we describe a method for automatically ex-
tracting product features from Pro/Con listings 
from Epinions.com. Our assumption is that 
pro/cons are the features that are important for 
customers (and hence should be part of a help-
ful review). 
? General-Inquirer (GIW): Positive and negative 
sentiment words describing products or prod-
uct features (e.g., ?amazing sound quality? and 
?weak zoom?). The intuition is that reviews 
that analyze product features are more helpful 
than those that do not. We try to capture this 
analysis by extracting sentiment words using 
the publicly available list of positive and nega-
tive sentiment words from the General Inquirer 
Dictionaries3. 
Meta-Data Features 
Unlike the previous four feature classes, meta-
data features capture observations which are in-
dependent of the text (i.e., unrelated with linguis-
tic features). We consider the following feature: 
? Stars (STR): Most websites require reviewers 
to include an overall rating for the products 
that they review (e.g., star ratings in Ama-
zon.com). This feature set includes the rating 
score (STR1) as well as the absolute value of 
the difference between the rating score and the 
average rating score given by all reviewers 
(STR2). 
We differentiate meta-data features from seman-
tic features since they require external knowl-
edge that may not be available from certain 
review sites. Nowadays, however, most sites that 
collect user reviews also collect some form of 
product rating (e.g., Amazon.com, Over-
stock.com, and Apple.com). 
                                                     
3 http://www.wjh.harvard.edu/~inquirer/homecat.htm 
425
4 Ranking System 
In this paper, we estimate the helpfulness func-
tion in Equation 1 using user ratings extracted 
from Amazon.com, where rating+(r) is the num-
ber of unique users that rated the review r as 
helpful and rating-(r) is the number of unique 
users that rated r as unhelpful. 
Reviews from Amazon.com form a gold stan-
dard labeled dataset of {review, h(review)} pairs 
that can be used to train a supervised machine 
learning algorithm. In this paper, we applied an 
SVM (Vapnik 1995) package on the features ex-
tracted from reviews to learn the function h. 
Two natural options for learning helpfulness 
according to Equation 1 are SVM Regression and 
SVM Ranking (Joachims 2002). Though learning 
to rank according to helpfulness requires only 
SVM Ranking, the helpfulness function provides 
non-uniform differences between ranks in the 
training set. Also, in practice, many products 
have only one review, which can serve as train-
ing data for SVM Regression but not SVM Rank-
ing. Furthermore, in large sites such as 
Amazon.com, when new reviews are written it is 
inefficient to re-rank all previously ranked re-
views. We therefore choose SVM Regression in 
this paper. We describe the exact implementation 
in Section 5.1. 
After the SVM is trained, for a given product 
and its set of reviews R, we rank the reviews of R 
in decreasing order of h(r), r ? R. 
Table 1 shows four sample reviews for the 
iPod Photo 20GB product from Amazon.com, 
their total number of helpful and unhelpful votes, 
as well as their rank according to the helpfulness 
score h from both the gold standard from Ama-
zon.com and using the SVM prediction of our 
best performing system described in Section 5.2. 
5 Experimental Results 
We empirically evaluate our review model and 
ranking system, described in Section 3 and Sec-
tion 4, by comparing the performance of various 
feature combinations on products mined from 
Amazon.com. Below, we describe our experi-
mental setup, present our results, and analyze 
system performance. 
5.1 Experimental Setup 
We describe below the datasets that we extracted 
from Amazon.com, the implementation of our 
SVM system, and the method we used for ex-
tracting features of reviews. 
Extraction and Preprocessing of Datasets 
We focused our experiments on two products 
from Amazon.com: MP3 Players and Digital 
Cameras. 
Using Amazon Web Services API, we col-
lected reviews associated with all products in the 
MP3 Players and Digital Cameras categories. 
For MP3 Players, we collected 821 products and 
33,016 reviews; for Digital Cameras, we col-
lected 1,104 products and 26,189 reviews. 
In most retailer websites like Amazon.com, 
duplicate reviews, which are quite frequent, skew 
statistics and can greatly affect a learning algo-
rithm. Looking for exact string matches between 
reviews is not a sufficient filter since authors of 
duplicated reviews often make small changes to 
the reviews to avoid detection. We built a simple 
filter that compares the distribution of word bi-
grams across each pair of reviews. A pair is 
deemed a duplicate if more than 80% of their 
bigrams match. 
Also, whole products can be duplicated. For 
different product versions, such as iPods that can 
come in black or white models, reviews on Ama-
zon.com are duplicated between them. We filter 
Table 1. Sample of 4 out of 43 reviews for the iPod Photo 20GB product from Ama-
zon.com along with their ratings as well as their helpfulness ranks (from both the gold 
standard from Amazon.com and the SVM prediction of our best performing system de-
scribed in Section 5.2). 
RANK(h) 
REVIEW TITLE 
HELPFUL 
VOTES 
UNHELPFUL 
VOTES GOLD 
STANDARD 
SVM 
PREDICTION 
?iPod Moves to All-color Line-up? 215 11 7 1 
?iPod: It's NOT Music to My Ears? 11 13 25 30 
?The best thing I ever bought? 22 32 26 27 
?VERY disappointing? 1 18 40 40 
 
426
out complete products where each of its reviews 
is detected as a duplicate of another product (i.e., 
only one iPod version is retained). 
The filtering of duplicate products and dupli-
cate reviews discarded 85 products and 12,097 
reviews for MP3 Players and 38 products and 
3,692 reviews for Digital Cameras. 
In order to have accurate estimates for the 
helpfulness function in Equation 1, we filtered 
out any review that did not receive at least five 
user ratings (i.e., reviews where less than five 
users voted it as helpful or unhelpful are filtered 
out). This filtering was performed before dupli-
cate detection and discarded 45.7% of the MP3 
Players reviews and 32.7% of the Digital Cam-
eras reviews. 
Table 2 describes statistics for the final data-
sets after the filtering steps. 10% of products for 
both datasets were withheld as development cor-
pora and the remaining 90% were randomly 
sorted into 10 sets for 10-fold cross validation. 
SVM Regression 
For our regression model, we deployed the state 
of the art SVM regression tool SVMlight 
(Joachims 1999). We tested on the development 
sets various kernels including linear, polynomial 
(degrees 2, 3, and 4), and radial basis function 
(RBF). The best performing kernel was RBF and 
we report only these results in this paper (per-
formance was measured using Spearman?s corre-
lation coefficient, described in Section 5.2). 
We tuned the RBF kernel parameters C (the 
penalty parameter) and ? (the kernel width hy-
perparameter) performing full grid search over 
the 110 combinations of exponentially spaced 
parameter pairs (C,?) following (Hsu et al 2003). 
Feature Extraction 
To extract the features described in Section 3.2, 
we preprocessed each review using the Minipar 
dependency parser (Lin 1994). We used the 
parser tokenization, sentence breaker, and syn-
tactic categorizations to generate the Length, 
Sentential, Unigram, Bigram, and Syntax feature 
sets. 
In order to count the occurrences of product 
features for the Product-Feature set, we devel-
oped an automatic way of mining references to 
product features from Epinions.com. On this 
website, user-generated product reviews include 
explicit lists of pros and cons, describing the best 
and worst aspects of a product. For example, for 
MP3 players, we found the pro ?belt clip? and 
the con ?Useless FM tuner?. Our assumption is 
that the pro/con lists tend to contain references to 
the product features that are important to cus-
tomers, and hence their occurrence in a review 
may correlate with review helpfulness. We fil-
tered out all single-word entries which were in-
frequently seen (e.g., hold, ever). After splitting 
and filtering the pro/con lists, we were left with a 
total of 9,110 unique features for MP3 Players 
and 13,991 unique features for Digital Cameras. 
The Stars feature set was created directly from 
the star ratings given by each author of an Ama-
zon.com review. 
For each feature measurement f, we applied 
the following standard transformation: 
 ( )1ln +f  
and then scaled each feature between [0, 1] as 
suggested in (Hsu et al 2003). 
We experimented with various combinations 
of feature sets. Our results tables use the abbre-
viations presented in Section 3.2. For brevity, we 
report the combinations which contributed to our 
best performing system and those that help assess 
the power of the different feature classes in cap-
turing helpfulness. 
5.2 Ranking Performance 
Evaluating the quality of a particular ranking is 
difficult since certain ranking intervals can be 
more important than others (e.g., top-10 versus 
bottom-10) We adopt the Spearman correlation 
coefficient ? (Spearman 1904) since it is the 
most commonly used measure of correlation be-
tween two sets of ranked data points4. 
For each fold in our 10-fold cross-validation 
experiments, we trained our SVM system using 9 
folds. For the remaining test fold, we ranked each 
product?s reviews according to the SVM predic-
tion (described in Section 4) and computed the ? 
                                                     
4 We used the version of Spearman?s correlation coeffi-
cient that allows for ties in rankings. See Siegel and Cas-
tellan (1988) for more on alternate rank statistics such as 
Kendall?s tau. 
Table 2. Overview of filtered datasets extracted 
from Amazon.com. 
 MP3 PLAYERS 
DIGITAL 
CAMERAS 
Total Products 736 1066 
Total Reviews 11,374 14,467 
Average Reviews/Product 15.4 13.6 
Min/MaxReviews/Product 1 / 375 1 / 168 
 
427
correlation between the ranking and the gold 
standard ranking from the test fold5. 
Although our task definition is to learn review 
rankings according to helpfulness, as an interme-
diate step the SVM system learns to predict the 
absolute helpfulness score for each review. To 
test the correlation of this score against the gold 
standard, we computed the standard Pearson cor-
relation coefficient. 
Results show that the highest performing fea-
ture combination consisted of the Length, the 
Unigram, and the Stars feature sets. Table 3 re-
ports the evaluation results for every combination 
of these features with 95% confidence bounds. 
Of the three features alone, neither was statisti-
cally more significant than the others. Examining 
each pair combination, only the combination of 
length with stars outperformed the others. Sur-
prisingly, adding unigram features to this combi-
nation had little effect for the MP3 Players. 
Given our list of features defined in Section 
3.2, helpfulness of reviews is best captured with 
a combination of the Length and Stars features. 
Training an RBF-kernel SVM regression model 
does not necessarily make clear the exact rela-
tionship between input and output variables. To 
investigate this relationship between length and 
helpfulness, we inspected their Pearson correla-
tion coefficient, which was 0.45. Users indeed 
tend to find short reviews less helpful than longer 
ones: out of the 5,247 reviews for MP3 Players 
that contained more than 1000 characters, the 
average gold standard helpfulness score was 
82%; the 204 reviews with fewer than 100 char-
acters had on average a score of 23%. The ex-
plicit product rating, such as Stars is also an 
                                                     
5 Recall that the gold standard is extracted directly from 
user helpfulness votes on Amazon.com (see Section 4). 
indicator of review helpfulness, with a Pearson 
correlation coefficient of 0.48. 
The low Pearson correlations of Table 3 com-
pared to the Spearman correlations suggest that 
we can learn the ranking without perfectly learn-
ing the function itself. To investigate this, we 
tested the ability of SVM regression to recover 
the target helpfulness score, given the score itself 
as the only feature. The Spearman correlation for 
this test was a perfect 1.0. Interestingly, the Pear-
son correlation was only 0.798, suggesting that 
the RBF kernel does learn the helpfulness rank-
ing without learning the function exactly. 
5.3 Results Analysis 
Table 3 shows only the feature combinations of 
our highest performing system. In Table 4, we 
report several other feature combinations to show 
why we selected certain features and what was 
the effect of our five feature classes presented in 
Section 3.2. 
In the first block of six feature combinations in 
Table 4, we show that the unigram features out-
perform the bigram features, which seem to be 
suffering from the data sparsity of the short re-
views. Also, unigram features seem to subsume 
the information carried in our semantic features 
Product-Feature (PRF) and General-Inquirer 
(GIW). Although both PRF and GIW perform 
well as standalone features, when combined with 
unigrams there is little performance difference 
(for MP3 Players we see a small but insignificant 
decrease in performance whereas for Digital 
Cameras we see a small but insignificant im-
provement). Recall that PRF and GIW are simply 
subsets of review words that are found to be 
product features or sentiment words. The learn-
ing algorithm seems to discover on its own which 
Table 3. Evaluation of the feature combinations that make up our best performing system 
(in bold), for ranking reviews of Amazon.com MP3 Players and Digital Cameras accord-
ing to helpfulness. 
MP3 PLAYERS DIGITAL CAMERAS 
FEATURE COMBINATIONS 
SPEARMAN? PEARSON? SPEARMAN? PEARSON? 
LEN 0.575 ? 0.037 0.391 ? 0.038 0.521 ? 0.029 0.357 ? 0.029 
UGR 0.593 ? 0.036 0.398 ? 0.038 0.499 ? 0.025 0.328 ? 0.029 
STR1 0.589 ? 0.034 0.326 ? 0.038 0.507 ? 0.029 0.266 ? 0.030 
UGR+STR1 0.644 ? 0.033 0.436 ? 0.038 0.490 ? 0.032 0.324 ? 0.032 
LEN+UGR 0.582 ? 0.036 0.401 ? 0.038 0.553 ? 0.028 0.394 ? 0.029 
LEN+STR1 0.652 ? 0.033 0.470 ? 0.038 0.577 ? 0.029 0.423 ? 0.031 
LEN+UGR+STR1 0.656 ? 0.033 0.476 ? 0.038 0.595 ? 0.028 0.442 ? 0.031 
LEN=Length; UGR=Unigram; STR=Stars 
?95% confidence bounds are calculated using 10-fold cross-validation. 
428
words are most important in a review and does 
not use additional knowledge about the meaning 
of the words (at least not the semantics contained 
in PRF and GIW). 
We tested two different versions of the Stars 
feature: i) the number of star ratings, STR1; and 
ii) the difference between the star rating and the 
average rating of the review, STR2. The second 
block of feature combinations in Table 4 shows 
that neither is significantly better than the other 
so we chose STR1 for our best performing sys-
tem. 
Our experiments also revealed that our struc-
tural features Sentential and HTML, as well as 
our syntactic features, Syntax, did not show any 
significant improvement in system performance. 
In the last block of feature combinations in Table 
4, we report the performance of our best per-
forming features (Length, Unigram, and Stars) 
along with these other features. Though none of 
the features cause a performance deterioration, 
neither of them significantly improves perform-
ance. 
5.4 Discussion 
In this section, we discuss the broader implica-
tions and potential impacts of our work, and pos-
sible connections with other research directions. 
The usefulness of the Stars feature for deter-
mining review helpfulness suggests the need for 
developing automatic methods for assessing pro-
duct ratings, e.g., (Pang and Lee 2005).  
Our findings focus on predictors of helpful-
ness of reviews of tangible consumer products 
(consumer electronics). Helpfulness is also solic-
ited and tracked for reviews of many other types 
of entities: restaurants (citysearch.com), films 
(imdb.com), reviews of open-source software 
modules (cpanratings.perl.org), and countless 
others. Our findings of the importance of Length, 
Unigrams, and Stars may provide the basis of 
comparison for assessing helpfulness of reviews 
of other entity types. 
Our work represents an initial step in assessing 
helpfulness. In the future, we plan to investigate 
other possible indicators of helpfulness such as a 
reviewer?s reputation, the use of comparatives 
(e.g., more and better than), and references to 
other products. 
Taken further, this work may have interesting 
connections to work on personalization, social 
networks, and recommender systems, for in-
stance by identifying the reviews that a particular 
user would find helpful.  
Our work on helpfulness of reviews also has 
potential applications to work on automatic gen-
Table 4. Performance evaluation of various feature combinations for ranking reviews of MP3 Players 
and Digital Cameras on Amazon.com according to helpfulness. The first six lines suggest that uni-
grams subsume the semantic features; the next two support the use of the raw counts of product ratings 
(stars) rather than the distance of this count from the average rating; the final six investigate the impor-
tance of auxiliary feature sets.  
MP3 PLAYERS DIGITAL CAMERAS 
FEATURE COMBINATIONS 
SPEARMAN? PEARSON? SPEARMAN? PEARSON? 
UGR 0.593 ? 0.036 0.398 ? 0.038 0.499 ? 0.025 0.328 ? 0.029 
BGR 0.499 ? 0.040 0.293 ? 0.038 0.434 ? 0.032 0.242 ? 0.029 
PRF 0.591? 0.037 0.400 ? 0.039 0.527 ? 0.030 0.316 ? 0.028 
GIW 0.571 ? 0.036 0.381 ? 0.038 0.524 ? 0.030 0.333 ? 0.028 
UGR+PRF 0.570 ? 0.037 0.375 ? 0.038 0.546 ? 0.029 0.348 ? 0.028 
UGR+GIW 0.554 ? 0.037 0.358 ? 0.038 0.568 ? 0.031 0.324 ? 0.029 
STR1 0.589 ? 0.034 0.326 ? 0.038 0.507 ? 0.029 0.266 ? 0.030 
STR2 0.556 ? 0.032 0.303 ? 0.038 0.504 ? 0.027 0.229 ? 0.027 
LEN+UGR+STR1 0.656 ? 0.033 0.476 ? 0.038 0.595 ? 0.028 0.442 ? 0.031 
LEN+UGR+STR1+SEN 0.653 ? 0.033 0.470 ? 0.038 0.599 ? 0.028 0.448 ? 0.030 
LEN+UGR+STR1+HTM 0.640 ? 0.035 0.459 ? 0.039 0.594 ? 0.028 0.442 ? 0.031 
LEN+UGR+STR1+SYN 0.645 ? 0.034 0.469 ? 0.039 0.595 ? 0.028 0.447 ? 0.030 
LEN+UGR+STR1+SEN+HTM+SYN 0.631 ? 0.035 0.453 ? 0.039 0.600 ? 0.028 0.452 ? 0.030 
LEN+UGR+STR1+SEN+HTM+SYN+PRF+GIW 0.601 ? 0.035 0.396 ? 0.038 0.604 ? 0.027 0.460 ? 0.030 
LEN=Length; SEN=Sentential; HTM=HTML; UGR=Unigram; BGR=Bigram; 
SYN=Syntax; PRF=Product-Feature; GIW=General-Inquirer; STR=Stars 
?95% confidence bounds are calculated using 10-fold cross-validation. 
429
eration of review information, by providing a 
way to assess helpfulness of automatically gener-
ated reviews. Work on generation of reviews in-
cludes review summarization and extraction of 
useful reviews from blogs and other mixed texts. 
6 Conclusions 
Ranking reviews according to user helpfulness is 
an important problem for many online sites such 
as Amazon.com and Ebay.com. To date, most 
websites measure helpfulness by having users 
manually assess how helpful each review is to 
them. In this paper, we proposed an algorithm for 
automatically assessing helpfulness and ranking 
reviews according to it. Exploiting the multitude 
of user-rated reviews on Amazon.com, we 
trained an SVM regression system to learn a 
helpfulness function and then applied it to rank 
unlabeled reviews. Our best system achieved 
Spearman correlation coefficient scores of 0.656 
and 0.604 against a gold standard for MP3 play-
ers and digital cameras. 
We also performed a detailed analysis of dif-
ferent features to study the importance of several 
feature classes in capturing helpfulness. We 
found that the most useful features were the 
length of the review, its unigrams, and its product 
rating. Semantic features like mentions of prod-
uct features and sentiment words seemed to be 
subsumed by the simple unigram features. Struc-
tural features (other than length) and syntactic 
features had no significant impact. 
It is our hope through this work to shed some 
light onto what people find helpful in user-
supplied reviews and, by automatically ranking 
them, to ultimately enhance user experience. 
References 
Attali, Y. and Burstein, J. 2006. Automated Essay 
Scoring With e-rater? V.2. Journal of Technology, 
Learning, and Assessment, 4(3). 
Burstein, J., Chodorow, M., and Leacock, C. 2004. 
Automated essay evaluation: the criterion online 
writing service. AI Magazine. 25(3), pp 27?36.  
Drucker,H., Wu,D. and Vapnik,V. 1999. Support vector 
machines for spam categorization. IEEE Trans. 
Neural Netw., 10, 1048?1054. 
Gabrilovich, E. and Markovitch, S. 2005. 
Feature Generation for Text Categorization Using 
World Knowledge. In Proceedings of IJCAI-2005. 
Hsu, C.-W.; Chang, C.-C.; and Lin, C.-J. 2003. A 
practical guide to SVM classification. Technical 
report, Department of Computer Science and 
Information Technology, National Taiwan University. 
Hu, M. and Liu, B. 2004. Mining and summarizing 
customer reviews. KDD?04. pp.168 ? 177 
Kim, S. and Hovy, E. 2004. Determining the Sentiment 
of Opinions. Proceedings of COLING-04. 
Joachims, T. 1999. Making Large-Scale SVM Learning 
Practical. In B. Sch?lkopf, C. Burges, and A. Smola 
(eds), Advances in Kernel Methods: Support Vector 
Learning. MIT Press. Cambridge, MA. 
Joachims, T. 2002. Optimizing Search Engines Using 
Clickthrough Data. In Proceedings of ACM KDD-02.  
Moschitti, A. and Basili R. 2004. Complex Linguistic 
Features for Text Classification: A Comprehensive 
Study. In Proceedings of ECIR 2004. Sunderland, 
U.K. 
Pang, B, L. Lee, and S. Vaithyanathan. 2001. Thumbs 
up? Sentiment Classification using Machine Learning 
Techniques.  Proceedings of EMNLP 2002. 
Pang, B. and Lee, L. 2005. Seeing stars: Exploiting class 
relationships for sentiment categorization with respect 
to rating scales. In Proceedings of the ACL, 2005. 
Riloff , E. and J. Wiebe. 2003. Learning Extraction 
Patterns for Subjective Expressions. In Proc. of 
EMNLP-03. 
Riloff, E., J. Wiebe, and T. Wilson. 2003. Learning 
Subjective Nouns Using Extraction Pattern 
Bootstrapping. Proceedings of CoNLL-03 
Rose, C., Roque, A., Bhembe, D., and Vanlehn, K. 2003. 
A Hybrid Text Classification Approach for Analysis 
of Student Essays. In Proc. of the HLT-NAACL, 2003. 
Salton, G. and McGill, M. J. 1983. Introduction to 
Modern Information Retrieval. McGraw Hill. 
Siegel, S. and Castellan, N.J. Jr. 1988. Nonparametric 
Statistics for the Behavioral Sciences. McGraw-Hill. 
Spearman C. 1904. The Proof and Measurement of 
Association Between Two Things. American Journal 
of Psychology, 15:72?101. 
Turney, P. 2002. Thumbs Up or Thumbs Down? 
Semantic Orientation Applied to Unsupervised 
Classification of Reviews. Proceedings of the 40th 
Annual Meeting of the ACL, Philadelphia, 417?424. 
Vapnik, V.N. 1995. The Nature of Statistical Learning 
Theory. Springer.  
Wiebe, J, R. Bruce, and T. O?Hara. 1999. Development 
and use of a gold standard data set for subjectivity 
classifications. Proc. of the 37th Annual Meeting of the 
Association for Computational Linguistics(ACL-99), 
246?253. 
Yu, H. and Hatzivassiloglou, V. 2003. Towards 
Answering Opinion Questions: Separating Facts from 
Opinions and Identifying the Polarity of Opinion 
Sentences. Proceedings of EMNLP 2003.
  
430
Building a Sense Tagged Corpus with
Open Mind Word Expert
Timothy Chklovski
Artificial Intelligence Laboratory
Massachusetts Institute of Technology
timc@mit.edu
Rada Mihalcea
Department of Computer Science
University of Texas at Dallas
rada@utdallas.edu
Abstract
Open Mind Word Expert is an imple-
mented active learning system for col-
lecting word sense tagging from the
general public over the Web. It is avail-
able at http://teach-computers.org. We
expect the system to yield a large vol-
ume of high-quality training data at a
much lower cost than the traditional
method of hiring lexicographers. We
thus propose a Senseval-3 lexical sam-
ple activity where the training data is
collected via Open Mind Word Expert.
If successful, the collection process can
be extended to create the definitive cor-
pus of word sense information.
1 Introduction
Most of the efforts in the Word Sense Disam-
biguation (WSD) field have concentrated on su-
pervised learning algorithms. These methods usu-
ally achieve the best performance at the cost of
low recall. The main weakness of these meth-
ods is the lack of widely available semantically
tagged corpora and the strong dependence of dis-
ambiguation accuracy on the size of the training
corpus. The tagging process is usually done by
trained lexicographers, and consequently is quite
expensive, limiting the size of such corpora to a
handful of tagged texts.
This paper introduces Open Mind Word Ex-
pert, a Web-based system that aims at creating
large sense tagged corpora with the help of Web
users. The system has an active learning compo-
nent, used for selecting the most difficult exam-
ples, which are then presented to the human tag-
gers. We expect that the system will yield more
training data of comparable quality and at a sig-
nificantly lower cost than the traditional method
of hiring lexicographers.
Open Mind Word Expert is a newly born project
that follows the Open Mind initiative (Stork,
1999). The basic idea behind Open Mind is to
use the information and knowledge that may be
collected from the existing millions of Web users,
to the end of creating more intelligent software.
This idea has been used in Open Mind Common
Sense, which acquires commonsense knowledge
from people. A knowledge base of about 400,000
facts has been built by learning facts from 8,000
Web users, over a one year period (Singh, 2002).
If Open Mind Word Expert experiences a similar
learning rate, we expect to shortly obtain a cor-
pus that exceeds the size of all previously tagged
data. During the first fifty days of activity, we col-
lected about 26,000 tagged examples without sig-
nificant efforts for publicizing the site. We expect
this rate to gradually increase as the site becomes
more widely known and receives more traffic.
2 Sense Tagged Corpora
The availability of large amounts of semanti-
cally tagged data is crucial for creating successful
WSD systems. Yet, as of today, only few sense
tagged corpora are publicly available.
One of the first large scale hand tagging efforts
is reported in (Miller et al, 1993), where a subset
of the Brown corpus was tagged with WordNet
                   July 2002, pp. 116-122.  Association for Computational Linguistics.
                 Disambiguation: Recent Successes and Future Directions, Philadelphia,
                             Proceedings of the SIGLEX/SENSEVAL Workshop on Word Sense
senses. The corpus includes a total of 234,136
tagged word occurrences, out of which 186,575
are polysemous. There are 88,058 noun occur-
rences of which 70,214 are polysemous.
The next significant hand tagging task was re-
ported in (Bruce and Wiebe, 1994), where 2,476
usages of interest were manually assigned with
sense tags from the Longman Dictionary of Con-
temporary English (LDOCE). This corpus was
used in various experiments, with classification
accuracies ranging from 75% to 90%, depending
on the algorithm and features employed.
The high accuracy of the LEXAS system (Ng
and Lee, 1996) is due in part to the use of large
corpora. For this system, 192,800 word occur-
rences have been manually tagged with senses
from WordNet. The set of tagged words consists
of the 191 most frequently occurring nouns and
verbs. The authors mention that approximately
one man-year of effort was spent in tagging the
data set.
Lately, the SENSEVAL competitions provide a
good environment for the development of su-
pervised WSD systems, making freely available
large amounts of sense tagged data for about
100 words. During SENSEVAL-1 (Kilgarriff and
Palmer, 2000), data for 35 words was made avail-
able adding up to about 20,000 examples tagged
with respect to the Hector dictionary. The size
of the tagged corpus increased with SENSEVAL-2
(Kilgarriff, 2001), when 13,000 additional exam-
ples were released for 73 polysemous words. This
time, the semantic annotations were performed
with respect to WordNet.
Additionally, (Kilgarriff, 1998) mentions the
Hector corpus, which comprises about 300 word
types with 300-1000 tagged instances for each
word, selected from a 17 million word corpus.
Sense tagged corpora have thus been central to
accurate WSD systems. Estimations made in (Ng,
1997) indicated that a high accuracy domain inde-
pendent system for WSD would probably need a
corpus of about 3.2 million sense tagged words.
At a throughput of one word per minute (Ed-
monds, 2000), this would require about 27 man-
years of human annotation effort.
With Open Mind Word Expert we aim at creat-
ing a very large sense tagged corpus, by making
use of the incredible resource of knowledge con-
stituted by the millions of Web users, combined
with techniques for active learning.
3 Open Mind Word Expert
Open Mind Word Expert is a Web-based interface
where users can tag words with their WordNet
senses. Tagging is organized by word. That is,
for each ambiguous word for which we want to
build a sense tagged corpus, users are presented
with a set of natural language (English) sentences
that include an instance of the ambiguous word.
Initially, example sentences are extracted from
a large textual corpus. If other training data is
not available, a number of these sentences are pre-
sented to the users for tagging in Stage 1. Next,
this tagged collection is used as training data, and
active learning is used to identify in the remaining
corpus the examples that are ?hard to tag?. These
are the examples that are presented to the users for
tagging in Stage 2. For all tagging, users are asked
to select the sense they find to be the most appro-
priate in the given sentence, from a drop-down
list that contains all WordNet senses, plus two
additional choices, ?unclear? and ?none of the
above?. The results of any automatic classifica-
tion or the classification submitted by other users
are not presented so as to not bias the contrib-
utor?s decisions. Based on early feedback from
both researchers and contributors, a future version
of Open Mind Word Expert may allow contribu-
tors to specify more than one sense for any word.
A prototype of the system has been imple-
mented and is available at http://www.teach-
computers.org. Figure 1 shows a screen shot from
the system interface, illustrating the screen pre-
sented to users when tagging the noun ?child?.
3.1 Data
The starting corpus we use is formed by a mix
of three different sources of data, namely the
Penn Treebank corpus (Marcus et al, 1993), the
Los Angeles Times collection, as provided during
TREC conferences1 , and Open Mind Common
Sense2, a collection of about 400,000 common-
sense assertions in English as contributed by vol-
unteers over the Web. A mix of several sources,
each covering a different spectrum of usage, is
1http://trec.nist.gov
2http://commonsense.media.mit.edu
Figure 1: Screen shot from Open Mind Word Expert
used to increase the coverage of word senses and
writing styles. While the first two sources are well
known to the NLP community, the Open Mind
Common Sense constitutes a fairly new textual
corpus. It consists mostly of simple single sen-
tences. These sentences tend to be explanations
and assertions similar to glosses of a dictionary,
but phrased in a more common language and with
many sentences per sense. For example, the col-
lection includes such assertions as ?keys are used
to unlock doors?, and ?pressing a typewriter key
makes a letter?. We believe these sentences may
be a relatively clean source of keywords that can
aid in disambiguation. For details on the data and
how it has been collected, see (Singh, 2002).
3.2 Active Learning
To minimize the amount of human annotation ef-
fort needed to build a tagged corpus for a given
ambiguous word, Open Mind Word Expert in-
cludes an active learning component that has the
role of selecting for annotation only those exam-
ples that are the most informative.
According to (Dagan et al, 1995), there are two
main types of active learning. The first one uses
memberships queries, in which the learner con-
structs examples and asks a user to label them. In
natural language processing tasks, this approach
is not always applicable, since it is hard and
not always possible to construct meaningful un-
labeled examples for training. Instead, a second
type of active learning can be applied to these
tasks, which is selective sampling. In this case,
several classifiers examine the unlabeled data and
identify only those examples that are the most in-
formative, that is the examples where a certain
level of disagreement is measured among the clas-
sifiers.
We use a simplified form of active learning
with selective sampling, where the instances to be
tagged are selected as those instances where there
is a disagreement between the labels assigned by
two different classifiers. The two classifiers are
trained on a relatively small corpus of tagged data,
which is formed either with (1) Senseval training
examples, in the case of Senseval words, or (2)
examples obtained with the Open Mind Word Ex-
pert system itself, when no other training data is
available.
The first classifier is a Semantic Tagger with
Active Feature Selection (STAFS). This system
(previously known as SMUls) is one of the top
ranked systems in the English lexical sample task
at SENSEVAL-2. The system consists of an in-
stance based learning algorithm improved with
a scheme for automatic feature selection. It re-
lies on the fact that different sets of features
have different effects depending on the ambigu-
ous word considered. Rather than creating a gen-
eral learning model for all polysemous words,
STAFS builds a separate feature space for each
individual word. The features are selected from a
pool of eighteen different features that have been
previously acknowledged as good indicators of
word sense, including: part of speech of the am-
biguous word itself, surrounding words and their
parts of speech, keywords in context, noun be-
fore and after, verb before and after, and others.
An iterative forward search algorithm identifies
at each step the feature that leads to the highest
cross-validation precision computed on the train-
ing data. More details on this system can be found
in (Mihalcea, 2002b).
The second classifier is a COnstraint-BAsed
Language Tagger (COBALT). The system treats
every training example as a set of soft constraints
on the sense of the word of interest. WordNet
glosses, hyponyms, hyponym glosses and other
WordNet data is also used to create soft con-
straints. Currently, only ?keywords in context?
type of constraint is implemented, with weights
accounting for the distance from the target word.
The tagging is performed by finding the sense that
minimizes the violation of constraints in the in-
stance being tagged. COBALT generates confi-
dences in its tagging of a given instance based on
how much the constraints were satisfied and vio-
lated for that instance.
Both taggers use WordNet 1.7 dictionary
glosses and relations. The performance of the two
systems and their level of agreement were eval-
uated on the Senseval noun data set. The two
systems agreed in their classification decision in
54.96% of the cases. This low agreement level
is a good indication that the two approaches are
fairly orthogonal, and therefore we may hope for
high disambiguation precision on the agreement
Precision
System (fine grained) (coarse grained)
STAFS 69.5% 76.6%
COBALT 59.2% 66.8%
STAFS   COBALT 82.5% 86.3%
STAFS - STAFS   COBALT 52.4% 63.3%
COBALT - STAFS   COBALT 30.09% 42.07%
Table 1: Disambiguation precision for the two in-
dividual classifiers and their agreement and dis-
agreement sets
set. Indeed, the tagging accuracy measured on
the set where both COBALT and STAFS assign
the same label is 82.5%, a figure that is close
to the 85.5% inter-annotator agreement measured
for the SENSEVAL-2 nouns (Kilgarriff, 2002).
Table 1 lists the precision for the agreement
and disagreement sets of the two taggers. The
low precision on the instances in the disagreement
set justifies referring to these as ?hard to tag?. In
Open Mind Word Expert, these are the instances
that are presented to the users for tagging in the
active learning stage.
3.3 Ensuring Quality
Collecting from the general public holds the
promise of providing much data at low cost. It
also makes attending to two aspects of data col-
lection more important: (1) ensuring contribution
quality, and (2) making the contribution process
engaging to the contributors.
We have several steps already implemented and
have additional steps we propose to ensure qual-
ity.
First, redundant tagging is collected for each
item. Open Mind Word Expert currently uses the
following rules in presenting items to volunteer
contributors:
 Two tags per item. Once an item has two
tags associated with it, it is not presented for
further tagging.
 One tag per item per contributor. We allow
contributors to submit tagging either anony-
mously or having logged in. Anonymous
contributors are not shown any items already
tagged by contributors (anonymous or not)
from the same IP address. Logged in con-
tributors are not shown items they have al-
ready tagged.
Second, inaccurate sessions will be discarded.
This can be accomplished in two ways, roughly
by checking agreement and precision:
 Using redundancy of tags collected for each
item, any given session (a tagging done all
in one sitting) will be checked for agreement
with tagging of the same items collected out-
side of this session.
 If necessary, the precision of a given contrib-
utor with respect to a preexisting gold stan-
dard (such as SemCor or Senseval training
data) can be estimated directly by presenting
the contributor with examples from the gold
standard. This will be implemented if there
are indications of need for this in the pilot;
it will help screen out contributors who, for
example, always select the first sense (and
are in high agreement with other contribu-
tors who do the same).
In all, automatic assessment of the quality of
tagging seems possible, and, based on the ex-
perience of prior volunteer contribution projects
(Singh, 2002), the rate of maliciously misleading
or incorrect contributions is surprisingly low.
Additionally, the tagging quality will be esti-
mated by comparing the agreement level among
Web contributors with the agreement level that
was already measured in previous sense tagging
projects. An analysis of the semantic annotation
task performed by novice taggers as part of the
SemCor project (Fellbaum et al, 1997) revealed
an agreement of about 82.5% among novice tag-
gers, and 75.2% among novice taggers and lexi-
cographers.
Moreover, since we plan to use paid, trained
taggers to create a separate test corpus for each
of the words tagged with Open Mind Word Ex-
pert, these same paid taggers could also validate
a small percentage of the training data for which
no gold standard exists.
3.4 Engaging the Contributors
We believe that making the contribution process
as engaging and as ?game-like? for the contrib-
utors as possible is crucial to collecting a large
volume of data. With that goal, Open Mind Word
Expert tracks, for each contributor, the number of
items tagged for each topic. When tagging items,
a contributor is shown the number of items (for
this word) she has tagged and the record number
of items tagged (for this word) by a single user.
If the contributor sets a record, it is recognized
with a congratulatory message on the contribution
screen, and the user is placed in the Hall of Fame
for the site. Also, the user can always access a
real-time graph summarizing, by topic, their con-
tribution versus the current record for that topic.
Interestingly, it seems that relatively sim-
ple word games can enjoy tremendous
user acceptance. For example, WordZap
(http://wordzap.com), a game that pits players
against each other or against a computer to be the
first to make seven words from several presented
letters (with some additional rules), has been
downloaded by well over a million users, and
the reviewers describe the game as ?addictive?.
If sense tagging can enjoy a fraction of such
popularity, very large tagged corpora will be
generated.
Additionally, NLP instructors can use the site
as an aid in teaching lexical semantics. An in-
structor can create an ?activity code?, and then,
for users who have opted in as participants of that
activity (by entering the activity code when cre-
ating their profiles), access the amount tagged by
each participant, and the percentage agreement of
the tagging of each contributor who opted in for
this activity. Hence, instructors can assign Open
Mind Word Expert tagging as part of a homework
assignment or a test.
Also, assuming there is a test set of already
tagged examples for a given ambiguous word, we
may add the capability of showing the increase
in disambiguation precision on the test set, as it
results from the samples that a user is currently
tagging.
4 Proposed Task for SENSEVAL-3
The Open Mind Word Expert system will be used
to build large sense tagged corpora for some of
the most frequent ambiguous words in English.
The tagging will be collected over the Web from
volunteer contributors. We propose to organize a
task in SENSEVAL-3 where systems will disam-
biguate words using the corpus created with this
system.
We will initially select a set of 100 nouns,
and collect for each of them  	
 tagged
samples (Edmonds, 2000), where  is the num-
ber of senses of the noun. It is worth mention-
ing that, unlike previous SENSEVAL evaluations,
where multi-word expressions were considered
as possible senses for an constituent ambiguous
word, we filter these expressions apriori with an
automatic tool for collocation extraction. There-
fore, the examples we collect refer only to single
ambiguous words, and hence we expect a lower
inter-tagger agreement rate and lower WSD tag-
ging precision when only single words are used,
since usually multi-word expressions are not am-
biguous and they constitute some of the ?easy
cases? when doing sense tagging.
These initial set of tagged examples will then
be used to train the two classifiers described in
Section 3.2, and annotate an additional set of
 
examples. From these, the users will be
presented only with those examples where there
is a disagreement between the labels assigned by
the two classifiers. The final corpus for each am-
biguous word will be created with (1) the original
set of
 	

tagged examples, plus (2) the
examples selected by the active learning compo-
nent, sense tagged by users.
Words will be selected based on their frequen-
cies, as computed on SemCor. Once the tag-
ging process of the initial set of 100 words is
completed, additional nouns will be incremen-
tally added to the Open Mind Word Expert inter-
face. As we go along, words with other parts of
speech will be considered as well.
To enable comparison with Senseval-2, the set
of words will also include the 29 nouns used in
the Senseval-2 lexical sample tasks. This would
allow us to assess how much the collected data
helps on the Senseval-2 task.
As shown in Section 3.3, redundant tags will be
collected for each item, and overall quality will be
assessed. Moreover, starting with the initial set of
 	

examples labeled for each word, we
will create confusion matrices that will indicate
the similarity between word senses, and help us
create the sense mappings for the coarse grained
evaluations.
One of the next steps we plan to take is to re-
place the ?two tags per item? scheme with the
?tag until at least two tags agree? scheme pro-
posed and used during the SENSEVAL-2 tagging
(Kilgarriff, 2002). Additionally, the set of mean-
ings that constitute the possible choices for a cer-
tain ambiguous example will be enriched with
groups of similar meanings, which will be de-
termined either based on some apriori provided
sense mappings (if any available) or based on the
confusion matrices mentioned above.
For each word with sense tagged data created
with Open Mind Word Expert, a test corpus will
be built by trained human taggers, starting with
examples extracted from the corpus mentioned in
Section 3.1. This process will be set up indepen-
dently of the Open Mind Word Expert Web in-
terface. The test corpus will be released during
SENSEVAL-3.
5 Conclusions and future work
Open Mind Word Expert pursues the poten-
tial of creating a large tagged corpus. WSD
can also benefit in other ways from the Open
Mind approach. We are considering using a
AutoASC/GenCor type of approach to generate
sense tagged data with a bootstrapping algorithm
(Mihalcea, 2002a). Web contributors can help
this process by creating the initial set of seeds,
and exercising control over the quality of the
automatically generated seeds.
Acknowledgments
We would like to thank the Open Mind Word Ex-
pert contributors who are making all this work
possible. We are also grateful to Adam Kilgar-
riff for valuable suggestions and interesting dis-
cussions, to Randall Davis and to the anonymous
reviewers for useful comments on an earlier ver-
sion of this paper, and to all the Open Mind Word
Expert users who have emailed us with their feed-
back and suggestions, helping us improve this ac-
tivity.
References
R. Bruce and J. Wiebe. 1994. Word sense disam-
biguation using decomposable models. In Proceed-
ings of the 32nd Annual Meeting of the Associa-
tion for Computational Linguistics (ACL-94), pages
139?146, LasCruces, NM, June.
I. Dagan, , and S.P. Engelson. 1995. Committee-
based sampling for training probabilistic classifiers.
In International Conference on Machine Learning,
pages 150?157.
P. Edmonds. 2000. Designing a task for
Senseval-2, May. Available online at
http://www.itri.bton.ac.uk/events/senseval.
C. Fellbaum, J. Grabowski, and S. Landes. 1997.
Analysis of a hand-tagging task. In Proceedings
of ANLP-97 Workshop on Tagging Text with Lexi-
cal Semantics: Why, What, and How?, Washington
D.C.
A. Kilgarriff and M. Palmer, editors. 2000. Com-
puter and the Humanities. Special issue: SENSE-
VAL. Evaluating Word Sense Disambiguation pro-
grams, volume 34, April.
A. Kilgarriff. 1998. Gold standard datasets for eval-
uating word sense disambiguation programs. Com-
puter Speech and Language, 12(4):453?472.
A. Kilgarriff, editor. 2001. SENSEVAL-2, Toulouse,
France, November.
A. Kilgarriff. 2002. English lexical sample task de-
scription. In Proceedings of Senseval-2, ACL Work-
shop.
M.P. Marcus, B. Santorini, and M.A. Marcinkiewicz.
1993. Building a large annotated corpus of en-
glish: the penn treebank. Computational Linguis-
tics, 19(2):313?330.
R. Mihalcea. 2002a. Bootstrapping large sense tagged
corpora. In Proceedings of the Third International
Conference on Language Resources and Evaluation
LREC 2002, Canary Islands, Spain, May. (to ap-
pear).
R. Mihalcea. 2002b. Instance based learning with
automatic feature selection applied to Word Sense
Disambiguation. In Proceedings of the 19th Inter-
national Conference on Computational Linguistics
(COLING-ACL 2002), Taipei, Taiwan, August. (to
appear).
G. Miller, C. Leacock, T. Randee, and R. Bunker.
1993. A semantic concordance. In Proceedings
of the 3rd DARPA Workshop on Human Language
Technology, pages 303?308, Plainsboro, New Jer-
sey.
H.T. Ng and H.B. Lee. 1996. Integrating multiple
knowledge sources to disambiguate word sense: An
examplar-based approach. In Proceedings of the
34th Annual Meeting of the Association for Com-
putational Linguistics (ACL-96), Santa Cruz.
H.T. Ng. 1997. Getting serious about word sense dis-
ambiguation. In Proceedings of the ACL SIGLEX
Workshop on Tagging Text with Lexical Semantics:
Why, What, and How?, pages 1?7, Washington.
P. Singh. 2002. The public acquisition of common-
sense knowledge. In Proceedings of AAAI Spring
Symposium: Acquiring (and Using) Linguistic (and
World) Knowledge for Information Access., Palo
Alto, CA. AAAI.
D. Stork. 1999. The Open Mind initiative. IEEE Ex-
pert Systems and Their Applications, 14(3):19?20.
