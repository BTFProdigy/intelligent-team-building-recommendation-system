Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, pages 820?828,
Honolulu, October 2008. c?2008 Association for Computational Linguistics
Integrating Multi-level Linguistic Knowledge with a Unified Framework for
Mandarin Speech Recognition
Xinhao Wang, Jiazhong Nie, Dingsheng Luo, Xihong Wu?
Speech and Hearing Research Center,
Key Laboratory of Machine Perception (Ministry of Education),
School of Electronics Engineering and Computer Science,
Peking University, Beijing, 100871, China
{wangxh,niejz,wxh,dsluo}@cis.pku.edu.cn
Abstract
To improve the Mandarin large vocabulary
continuous speech recognition (LVCSR), a
unified framework based approach is intro-
duced to exploit multi-level linguistic knowl-
edge. In this framework, each knowledge
source is represented by a Weighted Finite
State Transducer (WFST), and then they are
combined to obtain a so-called analyzer for in-
tegrating multi-level knowledge sources. Due
to the uniform transducer representation, any
knowledge source can be easily integrated into
the analyzer, as long as it can be encoded
into WFSTs. Moreover, as the knowledge in
each level is modeled independently and the
combination is processed in the model level,
the information inherently in each knowledge
source has a chance to be thoroughly ex-
ploited. By simulations, the effectiveness
of the analyzer is investigated, and then a
LVCSR system embedding the presented ana-
lyzer is evaluated. Experimental results reveal
that this unified framework is an effective ap-
proach which significantly improves the per-
formance of speech recognition with a 9.9%
relative reduction of character error rate on
the HUB-4 test set, a widely used Mandarin
speech recognition task.
1 Introduction
Language modeling is essential for large vocabu-
lary continuous speech recognition (LVCSR), which
aims to determine the prior probability of a supposed
word string W , p(W ). Although the word-based n-
gram language model remains the mainstream for
?Corresponding author: Xihong Wu
most speech recognition systems, the utilization of
linguistic knowledge is too limited in this model.
Consequently, many researchers have focused on
introducing more linguistic knowledge in language
modeling, such as lexical knowledge , syntax and
semantics of language (Wang and Vergyri, 2006;
Wang et al, 2004; Charniak, 2001; Roark, 2001;
Chelba, 2000; Heeman, 1998; Chelba et al, 1997).
Recently, structured language models have been
introduced to make use of syntactic hierarchi-
cal characteristics (Roark, 2001; Charniak, 2001;
Chelba, 2000). Nevertheless, the computational
complexity of decoding will be heavily increased, as
they are parser-based models. In contrast, the class-
based language model groups the words that have
similar functions of syntax or semantics into mean-
ingful classes. As a result, it handles the questions of
data sparsity and generalization of unseen event. In
practice, the part-of-speech (POS) information, cap-
turing the syntactic role of words, has been widely
used in clustering words (Wang and Vergyri, 2006;
Maltese et al, 2001; Samuelsson and Reichl, 1999).
In Heeman?s POS language model (Heeman, 1998),
the joint probability of word sequence and associ-
ated POS sequence was estimated directly, which
has been demonstrated to be superior to the condi-
tional probability previously used in the class-based
models (Johnson, 2001). Moreover, a SuperARV
language model was presented (Wang and Harper,
2002), in which lexical features and syntactic con-
straints were tightly integrated into a linguistic struc-
ture of SuperARV serving as a class in the model.
Thus, these knowledge was integrated in the rep-
resentation level, and then the joint probabilities
820
of words and corresponding SuperARVs were esti-
mated. However, in the class-based language mod-
els, words are taken as the model units, while other
units smaller or larger than words are unfeasible for
modeling simultaneously, such as the Chinese char-
acters for Chinese names.
Usually, speech recognition systems can only rec-
ognize the words within a predefined dictionary.
With the increase of unknown words, i.e., out-of-
vocabulary (OOV) words, the performance will de-
grade dramatically. This is because not only those
unknown words cannot be recognized correctly, but
the words surrounding them will be affected. Thus,
many efforts have been made to deal with the is-
sue of OOV words (Martins et al, 2006; Galescu,
2003; Bazzi and Glass, 2001), and various model
units smaller than words have been examined to rec-
ognize OOVs from speech, such as phonemes (Bazzi
and Glass, 2000a), variable-length phoneme se-
quence (Bazzi and Glass, 2001), syllable (Bazzi and
Glass, 2000b) and sub-word (Galescu, 2003). Since
the proper name is a typical category of OOV words
and usually takes a very large proportion among all
kinds of OOV words, it has been specially addressed
in (Hu et al, 2006; Tanigaki et al, 2000).
All those attempts mentioned above succeed in
utilizing linguistic knowledge in language modeling
in some degree respectively. In this study, a uni-
fied framework based approach, which aims to ex-
ploit information from multi-level linguistic knowl-
edge, is presented. Here, the Weighted Finite State
Transducer (WFST) turns to be an ideal choice for
our purpose. WFSTs were formerly introduced to
simplify the integration of models in speech recog-
nition, including acoustic models, phonetic mod-
els and word n-gram (Mohri, 1997; Mohri et al,
2002). In recent years, the WFST has been suc-
cessfully applied in several state-of-the-art speech
recognition systems, such as systems developed by
the AMI project (Hain et al, 2006), IBM (Saon et
al., 2003) and AT&T (Mohri et al, 1996), and in
various fields of natural language processing, such
as smoothed n-gram model, partial parsing (Abney,
1996), named entities recognition (Friburger and
Maurel, 2004), semantic interpretation (Raymond et
al., 2006) and machine translation (Tsukada and Na-
gata, 2004). In (Takaaki Hori and Minami, 2003),
the WFST has been further used for language model
adaptation, where language models of different vo-
cabularies that represented different styles were in-
tegrated through the framework of speech transla-
tion. In WFST-based systems, all of the models are
represented uniformly by WFSTs, and the general
composition algorithm (Mohri et al, 2000) com-
bines these representations flexibly and efficiently.
Thereby, rather than integrating the models step by
step in decoding stage, a complete search network is
constructed in advance. The combined WFST will
be more efficient by optimizing with determiniza-
tion, minimization and pushing algorithms of WF-
STs (Mohri, 1997). Besides, the researches on opti-
mizing the search space and improving WFST-based
speech recognition has been carried out, especially
on how to perform on-the-fly WFSTs composition
more efficiently (Hori et al, 2007; Diamantino Ca-
seiro, 2002).
In this study, we extend the linguistic knowledge
used in speech recognition. As WFSTs provide a
common and natural representation for lexical con-
straints, n-gram language model, Hidden Markov
Model models and context-dependency, multi-level
knowledge sources can be encoded into WFSTs un-
der the uniform transducer representation. Then this
group of WFSTs is flexibly combined together to
obtain an analyzer representing knowledge of per-
son and location names as well as POS information.
Afterwards, the presented analyzer is incorporated
into LVCSR to evaluate the linguistic correctness of
recognition candidates by an n-best rescoring.
Unlike other methods, this approach holds two
distinct features. Firstly, as all multi-level knowl-
edge sources are modeled independently, the model
units such as character, words, phrase, etc., can be
chosen freely. Meanwhile, the integration of these
information sources is conducted in the model level
rather than the representation level. This setup will
help to model each knowledge source sufficiently
and may promote the accuracy of speech recogni-
tion. Secondly, under this unified framework, it is
easy to combine additional knowledge source into
the framework with the only requirement that the
new knowledge source can be represented by WF-
STs. Moreover, since all knowledge sources are fi-
nally represented by a single WFST, additional ef-
forts are not required for decoding the new knowl-
edge source.
821
The remainder of this paper is structured as fol-
lows. In section 2, we introduce our analyzer in de-
tail, and incorporate it into a Mandarin speech recog-
nition system. In section 3, the simulations are per-
formed to evaluate the analyzer and test its effective-
ness when being applied to LVCSR. The conclusion
appears in section 4.
2 Incorporation of Multi-level linguistic
knowledge in LVCSR
In this section, we start by giving a brief descrip-
tion on WFSTs. Then some special characteristics
of Chinese are investigated, and the model units are
fixed. Afterwards, each knowledge source is rep-
resented with WFSTs, and then they are combined
into a final WFST, so-called analyzer. At last, this
analyzer is incorporated into Mandarin LVCSR.
2.1 Weighted Finite State Transducers
The Weighted Finite State Transducer (WFST) is the
generalization of the finite state automata, in which,
besides of an input label, an output label and a
weight are also placed on each transition. With these
labels, a WFST is capable of realizing a weighted re-
lation between strings. In our system, log probabili-
ties are adopted as transition weights and the relation
between two strings is associated with a weight indi-
cating the probability of the mapping between them.
Given a group of WFSTs, each of which models a
stage of a mapping cascade, the composition opera-
tion provides an efficient approach to combine them
into a single one (Mohri et al, 2002; Mohri et al,
1996). In particular, for two WFSTs R and S, the
composition T = RoS represents the composition
of relations realized by R and S. The combination
is performed strictly on R?s output and S?s input. It
means for each path in T, mapping string r to string
s, there must exist a path mapping r to some string
t in R and a path mapping t to s in S. Decoding on
the combined WFST enables to find the joint opti-
mal results for multi-level weighted relations.
2.2 Model Unit Selection
This study primarily takes the person and location
names as well as the POS information into account.
To deal with Chinese OOV words, different from
the western language in which the phoneme, sylla-
ble or sub-word are used as the model units (Bazzi
and Glass, 2000a; Bazzi and Glass, 2000b; Galescu,
2003), Chinese characters are taken as the basic
units. In general, a person name of Han nation-
ality consists of a surname and a given name usu-
ally with one or two characters. Surnames com-
monly come from a fixed set that has been histori-
cally used. According to a recent investigation on
surnames involving 296 million people, 4100 sur-
names are found, and 129 most used surnames ac-
count for 87% (conducted by the Institute of Genet-
ics and Developmental Biology, Chinese Academy
of Sciences). In contrast, the characters used in
given names can be selected freely, and in many situ-
ations, some commonly used words may also appear
in names, such as ???? (victory) and ???? (the
Changjiang River). Therefore, both Chinese charac-
ters and words are considered as model units in this
study, and a word re-segmentation process on recog-
nition hypotheses is necessary, where an n-gram lan-
guage model based on word classes is adopted.
2.3 Representation and Integration of
Multi-level Knowledge
In this work, we ignore the word boundaries of n-
best hypotheses and perform a word re-segmentation
for names recognition. Given an input Chinese
character, it is encoded by a finite state acceptor
FSAinput. For example, the input ???????
(while synthesizing molecule) is represented as in
Figure 1(a). Then a dictionary is represented by a
50 321
0
?:??:?
?:??
?:?
(a)
(b)
4
?:?
?:? ?:? ?:?
?:??
?:?
?:?
?:?
?:? ?:?
?:??
?:??
?? ? ? ?
1 3
6
5
4
2
10
9
8
7
Figure 1: (a) is an example of the FSA representing a
given input; (b) is the FST representing a toy dictionary.
822
transducer with empty weights, denoted as FSTdict.
Figure 1(b) illustrates a toy dictionary listed in Ta-
ble 1, in which a successful path encodes a mapping
from a Chinese character sequence to some word
in the dictionary. In practice, all Chinese charac-
Chinese Words English Words
?? synthesize
?? element
?? molecule
?? the period of the day from11 p.m.to l a.m.
? together
? present
Table 1: The Toy dictionary
ters should appear in the dictionary for further in-
corporating models of names. Then the combination
of FSAinput and FSTdict, FSTseg = FSAinput ?
FSTdict, will result in a WFST embracing all the
possible candidate segmentations. Afterwards an n-
gram language model based on word classes is used
to weight the candidate segmentations. As in Fig-
ure 2, a toy bigram with three words is depicted by
WFSTn?gram, and the word classes are defined in
Table 2. Here, both in the training and test stages,
0
w1/un(w1)
w2/un(w2)
w3/un(w3)
4
w3/un(w3)
?/back(w1)
w1/un(w1)
?/back(w3)
w2/un(w2)
?/back(w2)
w1/bi(w2,w1)
w2/bi(w3,w2)
w2/bi(w1,w2)
w3/bi(w2,w3)
w1/bi(w3,w1)
w3/bi(w1,w3)
2
3
1
Figure 2: The WFST representing a toy bigram language
model, in which un(w1) denotes the unigram of w1;
bi(w1, w2) and back(w1) respectively denotes the bi-
gram of w2 and the backoff weight given the word history
w1.
the strings of numbers or letters in sentences are ex-
Classes Description
wi Each word wi listed in the dictionary
CNAME Person names of Han nationality
TNAME Translated person names
LOC Location names
NUM Number expressions
LETTER Letter strings
NON Other non Chinese character strings
BEGIN Beginning of sentence
END End of sentence
Table 2: The Definition of word classes
tracted according to the rules, and then substituted
with the class tags, ?NUM? and ?LETTER? respec-
tively. At the same time, the words, such as ????
and ?A??, are replaced with ?NUM?? and ?LET-
TER?? in the dictionary. In addition, name classes,
including ?CNAME?, ?TNAME? and ?LOC?, will
be set according to names recognition.
Hidden Markov Models (HMMs) are adopted
both for names recognition and POS tagging. Here,
each HMM is represented with two WFSTs. Tak-
ing the POS tagging as an example, the toy POS
WFSTs with 3 different tags are illustrated in Fig-
ure 3. The emission probability of a word by a POS,
(P (word/pos)), is represented as in Figure 3(a),
and the bigram transition probabilities between POS
tags are represented as in Figure 3(b), similar to the
word n-gram. In terms of names recognition, the
HMM states correspond to 30 role tags of names,
some for model units of Chinese characters, such as
surname, the first or second character of a given per-
son name with two characters, the first or last charac-
ter of a location name and so on, but others for model
units of words, such as the word before or after a
name, the words in a name and so on. When rec-
ognizing the person names, since there is a big dif-
ference between the translated names and the names
of Han nationality, two types of person names are
modeled separately, and substituted with two differ-
ent class tags in the segmentation language model,
as ?TNAME? and ?CNAME?. Some rules, which
can be encoded into WFSTs, are responsible for the
transformation from a role sequence to correspond-
ing name class (for example, a role sequence might
consist of the surname, the first character of the
823
0pos1/un(pos1)
pos2/un(pos2)
pos3/un(pos3)
pos1/bi(pos2,pos1)
pos3/bi(pos2,pos3)
pos2/bi(pos1,pos2)
pos3/bi(pos1,pos3)
pos1/bi(pos3,pos1)pos2/bi(pos3,pos2)
(a)
(b)
word: pos/p(word/pos)
3
2
1
0
Figure 3: The toy POS WFSTs. (a) is the WFST rep-
resenting the relationship between the word and the pos;
(b) is the WFSA representing the bigram transition prob-
abilities between POS tags
given name, and the second character of the given
name, which will be transformed to ?CNAME? in
FSTseg). Hence, taking names recognition into ac-
count, a WFST, including all possible segmentations
as well as recognized candidates of names, can be
obtained as below, denoted as WFSTwords:
FSAinput ? FSTdict ?WFSTne ?WFSAn?gram
(1)
POS information is integrated as follows.
(? ?WFSTwords) ?WFSTPOS (2)
Consequently, the desired analyzer, a combined
WFST that represents multi-level linguistic knowl-
edge sources, has been obtained.
2.4 Incorporation in LVCSR
The presented analyzer models linguistic knowledge
at different levels, which will be useful to find an
optimal words sequence among a large number of
speech recognition hypotheses. Thus in this re-
search, the analyzer is incorporated after the first
pass recognition, and the n-best hypotheses are
reranked according to the total path scores adjusted
with the analyzer scores as follows.
W? = argmax
W
?
??
log (PAM (O|W ))
+? ? log (PLM (W ))
+? ? log (PAnalyzer (W ))
?
??
(3)
where PAM (O|W ) and PLM (W ) are the acoustic
and language scores produced in first pass decoding,
and PAnalyzer (W ) reflects the linguistic correctness
of one hypothesis scored by the analyzer. Through
the reranking paradigm, a new best sentence hypoth-
esis is obtained.
3 Simulation
Under the unified framework, multi-level linguistic
knowledge is represented by the analyzer as men-
tioned above. To guarantee the effectiveness of
the introduced framework in integrating knowledge
sources, the analyzer is evaluated in this section.
Then the experiments using an LVCSR system in
which the analyzer is embedded are performed.
3.1 Analyzer Evaluation
Considering the function of the analyzer, cascaded
subtasks of word segmentation, names recognition
and POS tagging can be processed jointly, while
they are traditionally handled in a pipeline manner.
Hence, a comparison between the analyzer and the
pipeline system can be used to evaluate the effec-
tiveness of the introduced framework for knowledge
integration. As illustrated in Figure 4, two systems
based on the presented analyzer and the pipeline
manner are constructed respectively.
The evaluation data came from the People?s Daily
of China in 1998 from January to June (annotated by
the Institute of Computational Linguistics of Peking
University1), among which the January to May data
was taken as the training set, and the June data was
taken as the test set (consisted of 21,143 sentences
and about 1.2 million words). The first two thou-
sand sentences from the June data were extracted
as the development set, used to fix the composition
weight ? in equation 2. A dictionary including about
113,000 words was extracted from the training data,
1http://icl.pku.edu.cn/icl res/
824
input d ict ne n gramF SA F ST W F ST W F ST q q q
Decode
The best segmentation
posWFST
CCompose ompose
Decode Decode
Pipeline System Presented Analyzer
output output
Figure 4: The pipeline system vs The analyzer
in which a person or location name was accounted
as a word in vocabulary, only when the number of
its appearances was no less than three.
In Figure 5, the analyzer is compared with the
pipeline system, where the analyzer outperforms the
pipeline manner on all the subtasks in terms of F1-
score metric. Furthermore to detect the differences,
the statistical significance test using approximate
randomization approach (Yeh, 2000) is done on the
word segmentation results. Since there are more
than 21,000 sentences in the test set, which is not
appropriate for approximate randomization test, ten
sets (500 sentences for each) are randomly selected
from the test corpus. For each set, we run 1048576
shuffles twice and calculate the significance level p-
value according to the shuffled results. It has been
shown that all p-value are less than 0.001 on the ten
sets. Accordingly the improvement is statistically
significant. Actually, this significant improvement
is reasonable, since the joint processing avoids error
propagation and provides the opportunity of shar-
ing information between different level knowledge
sources. The superiority of this analyzer also shows
that the integration of multi-level linguistic knowl-
edge under the unified framework is effective, which
may lead to improved LVCSR.
95.9
91.1
89.9
96.8
91.8
88.5
90.9
88
92
96 Pipeline Analyzer
Integrated Analyzer
83.3
80
84
Word Segmentation POS Tagging Person Name Recognition Location Name Recognition
Figure 5: The Performance comparison between the
pipeline system and the analyzer. The system perfor-
mances are measured with the F1-score in the tasks
of word segmentation, POS tagging, the person names
recognition and the location names recognition.
3.2 Experimental Setup for Mandarin Speech
Recognition
In the baseline speech recognition system, the
acoustic models consisted of context-dependent
Initial-Final models, in which the left-to-right model
topology was used to represent each unit. Accord-
ing to the phonetic structures, the number of states
in each model was set to 2 or 3 for initials, and 4
or 5 for tonal finals. Each state was trained to have
32 Gaussian mixtures. The used 39-dimension fea-
ture vector comprised 12 MFCC coefficients, en-
ergy, and their first-order and second-order deltas.
Since in this work we focused on modeling knowl-
edge of language in Mandarin LVCSR, only clean
male acoustic models were trained with a speech
database that contained about 360 hours speech of
over 750 male speakers. This training data was
picked up from three continuous Mandarin speech
corpora: the 863-I, 863-II and Intel corpora. The
brief information about these three speech corpora
was listed in Table 3. As in this work, the eval-
uation data was the 1997 HUB-4 Mandarin broad-
cast news evaluation data (HUB-4 test set), to bet-
ter fit this task, the acoustic models were adapted
by the approach of maximum a posterior (MAP)
adaptation. The adaption data was drawn from the
HUB4 training set, excluding the HUB-4 develop-
825
Corpus Speakers Amount of Speech
(hours)
863-I (male) 83 56.67
863-II(male) 120 78.08
Intel (male) 556 227.30
total 759 362.05
Table 3: The information of the speech training data
ing set, where only the cleaned male speech data
(data under condition f0 defined as (Doddington,
1996)) was used. The partition for the clean data
was done with the acoustic segmentation software
CMUseg 0.52 (Siegler et al, 1997), and finally 8.6
hours adaptation data was obtained.
The language model was a word-based trigram
built on 60,000 words entries and trained with a cor-
pus about 1.5 billion characters. The training set
consisted of broadcast news data from the Xinhua
News Agency released by LDC (Xinhua part of Chi-
nese Gigaword), seven years data of People?s Daily
of China from 1995 to 2002 released by People?s
Daily Online3, and some other data from news web-
sites, such as yahoo, sina and so on.
In addition, the analyzer incorporated in speech
recognition was trained with a larger corpus from
People?s Daily of China, including the data in 1998
from January to June and the data in 2000 from
January to November (annotated by the Institute
of Computational Linguistics of Peking University).
The December data in 2000 was taken as the devel-
opment set used to fix the composition weight ? in
equation 2.
3.3 Experimental Results
In our experiments, the clean male speech data from
the Hub-4 test set was used, and 238 sentences were
finally extracted for testing. The weight of the ana-
lyzer was empirically derived from the development
set, including 649 clean male sentences from the de-
vSet of HUB-4 Evaluation. The recognition results
are shown in Table 4. The baseline system has a
character error rate (CER) of 14.85%. When the an-
alyzer is incorporated, a 9.9% relative reduction is
2Acoustic segmentation software downloaded from
http://www.nist.gov/speech/tools/CMUseg 05targz.htm.
3http://www.people.com.cn
System Err. Sub. Del. Ins.
Baseline 14.85 13.02 0.76 1.07
Analyzer 13.38 11.78 1.00 0.60incorporation
Table 4: The Speech recognition results
achieved. Furthermore, we ran the statistical signif-
icance test to detect the performance improvement,
in which the approximate randomization approach
(Yeh, 2000) was modified to output the significance
level, p-value, for the CER metric. The p-levels pro-
duced through two rounds of 1048576 shuffles are
0.0058 and 0.0057 respectively, both less than 0.01.
Thus the performance improvement imposed by the
utilization of the analyzer is statistically significant.
4 Conclusion
Addressing the challenges of Mandarin large vocab-
ulary continuous speech recognition task, within the
unified framework of WFSTs, this study presents
an analyzer integrating multi-level linguistic knowl-
edge. Unlike other methods, model units, such as
characters and words, can be chosen freely in this
approach since multi-level knowledge sources are
modeled independently. As a consequence, the fi-
nal analyzer can be derived from the combination
of better optimized models based on proper model
units. Along with two level knowledge sources, i.e.,
the person and location names as well as the part-of-
speech information, the analyzer is built and evalu-
ated by a comparative simulation. Further evaluation
is also conducted on an LVCSR system in which the
analyzer is embedded. Experimental results consis-
tently reveal that the approach is effective, and suc-
cessfully improves the performance of speech recog-
nition by a 9.9% relative reduction of character error
rate on the HUB-4 test set. Also, the unified frame-
work based approach provides a property of integrat-
ing additional linguistic knowledge flexibly, such as
organization name and syntactic structure. Further-
more, the presented approach has a benefit of ef-
ficiency that additional efforts are not required for
decoding as new knowledge comes, since all knowl-
edge sources are finally encoded into a single WFST.
826
Acknowledgments
The work was supported in part by the National
Natural Science Foundation of China (60435010;
60535030; 60605016), the National High Tech-
nology Research and Development Program of
China (2006AA01Z196; 2006AA010103), the Na-
tional Key Basic Research Program of China
(2004CB318005), and the New-Century Training
Program Foundation for the Talents by the Ministry
of Education of China.
References
Steven Abney. 1996. Partial parsing via finite-state cas-
cades. Natural Language Engineering, 2(4):337?344.
Issam Bazzi and James R. Glass. 2000a. Modeling out-
of-vocabulary words for robust speech recognition. In
Proc. of 6th International Conference on Spoken Lan-
guage Processing, pages 401?404, Beijing, China, Oc-
tober.
Issam Bazzi and James Glass. 2000b. Heterogeneous
lexical units for automatic speech recognition: prelim-
inary investigations. In Proc. of ICASSP, pages 1257?
1260, Istanbul, Turkey, June.
Issam Bazzi and James Glass. 2001. Learning units
for domain-independent out-of-vocabulary word mod-
elling. In Proc. of EUROSPEECH, pages 61?64, Aal-
borg, Denmark, September.
Eugene Charniak. 2001. Immediate-head parsing for
language models. In Proc. of ACL, pages 116?123,
Toulouse, France, July.
Ciprian Chelba, David Engle, Frederick Jelinek, Vic-
tor Jimenez, Sanjeev Khudanpur, Lidia Mangu, Harry
Printz, Eric Ristad, Ronald Rosenfeld, Andreas Stol-
cke, and Dekai Wu. 1997. Structure and performance
of a dependency language model. In Proc. of EU-
ROSPEECH, pages 2775?2778, Rhodes, Greece.
Ciprian Chelba. 2000. Exploiting Syntactic Structure for
Natural Language Modeling. Ph.D. thesis, Johns Hop-
kins University.
Isabel Trancoso Diamantino Caseiro. 2002. Using dy-
namic WFST composition for recognizing broadcast
news. In Proc. of ICSLP, pages 1301?1304, Denver,
Colorado, USA, September.
George Doddington. 1996. The 1996 hub-
4 annotation specification for evaluation of
speech recognition on broadcast news. In
ftp://jaguar.ncsl.nist.gov/csr96/h4/h4annot.ps.
N. Friburger and D. Maurel. 2004. Finite-state trans-
ducer cascades to extract named entities in texts. The-
oretical Computer Science, 313(1):93?104.
Lucian Galescu. 2003. Recognition of out-of-vocabulary
words with sub-lexical language models. In Proc.
of EUROSPEECH, pages 249?252, Geneva, Switzer-
land, September.
Thomas Hain, Lukas Burget, John Dines, Giulia Garau,
Martin Karafiat, Mike Lincoln, Jithendra Vepa, and
Vincent Wan. 2006. The AMI meeting transcription
system: Progress and performance. In Proc. of Rich
Transcription 2006 Spring Meeting Recognition Eval-
uation.
Peter A. Heeman. 1998. Pos tagging versus classes in
language modeling. In Proc. of the 6th Workshop on
very large corpora, pages 179?187, Montreal, Canada.
Takaaki Hori, Chiori Hori, Yasuhiro Minami, and At-
sushi Nakamura. 2007. Efficient WFST-based one-
pass decoding with on-the-fly hypothesis rescoring in
extremely large vocabulary continuous speech recog-
nition. IEEE Transactions on audio, speech, and lan-
guage processing, 15(4):1352?1365.
Xinhui Hu, Hirofumi Yamamoto, Genichiro Kikui, and
Yoshinori Sagisaka. 2006. Language modeling of
chinese personal names based on character units for
continuous chinese speech recognition. In Proc. of
INTERSPEECH, pages 249?252, Pittsburgh, USA,
September.
Mark Johnson. 2001. Joint and conditional estimation of
tagging and parsing models. In Proc. of ACL, pages
322 ? 329, Toulouse, France.
G. Maltese, P. Bravetti, H. Cr?py, B. J. Grainger, M. Her-
zog, and F. Palou. 2001. Combining word- and
class-based language models: A comparative study in
several languages using automatic and manual word-
clustering techniques. In Proc. of EUROSPEECH,
pages 21?24, Aalborg, Denmark, September.
Ciro Martins, Antonio Texeira, and Joao Neto. 2006.
Dynamic vocabulary adaptation for a daily and real-
time broadcast news transcription system. In Proc. of
Spoken Language Technology Workshop, pages 146?
149, December.
Mehryar Mohri, Fernando Pereira, and Michael Riley.
1996. Weighted automata in text and speech process-
ing. In ECAI-96 Workshop.
Mehryar Mohri, Fernando Pereira, and Michael Riley.
2000. The design principles of a weighted finite-
state transducer library. Theoretical Computer Sci-
ence, 231(1):17?32.
Mehryar Mohri, Fernando Pereira, and Michael Ri-
ley. 2002. Weighted finite-state transducers in
speech recognition. Computer Speech and Language,
16(1):69?88.
Mehrya Mohri. 1997. Finite-state transducers in lan-
guage and speech processing. Computational Linguis-
tics, 23(2):269?311.
827
Christan Raymond, Fre de ric Be chet, Renato D. Mori,
and Ge raldine Damnati. 2006. On the use of finite
state transducers for semantic interpretation. Speech
Communication, 48(3-4):288?304.
Brian Roark. 2001. Probabilistic top-down parsing
and language modeling. Computational Linguistics,
27(2):249?276.
Christer Samuelsson and Wolfgang Reichl. 1999. A
class-based language model for large-vocabulary
speechrecognition extracted from part-of-speech
statistics. In Proc. of ICASSP, pages 537?540,
Phoenix, Arizona, USA, March.
George Saon, Geoffrey Zweig, Brain KingsBury, Lidia
Mangu, and Upendra Canudhari. 2003. An architec-
ture for rapid decoding of large vocabulary conversa-
tional speech. In Proc. of Eurospeech, pages 1977?
1980, Geneva, Switzerland, September.
Matthew A. Siegler, Uday Jain, Bhiksha Raj, and
Richard M. Stern. 1997. Automatic segmentation,
classification and clustering of broadcast news audio.
In Proc. of DARPA Speech Recognition Workshop,
pages 97?99, Chantilly, Virginia, February.
Daniel Willett Takaaki Hori and Yasuhiro Minami.
2003. Language model adaptation using WFST-based
speaking-style translation. In Proc. of ICASSP, pages
I.228?I.231, Hong Kong, April.
Koichi Tanigaki, Hirofumi Yamamoto, and Yoshinori
Sagisaka. 2000. A hierarchical language model incor-
porating class-dependent word models for oov words
recognition. In Proc. of 6th International Conference
on Spoken Language Processing, pages 123?126, Bei-
jing, China, October.
Hajime Tsukada and Masaaki Nagata. 2004. Efficient
decoding for statistical machine translation with a fully
expanded WFST model. In Proc. of EMNLP, pages
427?433, Barcelona, Spain, July.
Wen Wang and Mary P. Harper. 2002. The superarv lan-
guage model: investigating the effectiveness of tightly
integrating multiple knowledge sources. In Proc. of
EMNLP, pages 238?247, Philadelphia, USA, July.
Wen Wang and Dimitra Vergyri. 2006. The use of word
n-grams and parts of speech for hierarchical cluster
language modeling. In Proc. of ICASSP, pages 1057?
1060, Toulouse, France, May.
Wen Wang, Andreas Stolcke, and Mary P. Harper. 2004.
The use of a linguistically motivated language model
in conversational speech recognition. In Proc. of
ICASSP, pages 261?264, Montreal, Canada, May.
Alexander Yeh. 2000. More accurate tests for the sta-
tistical significance of result differences. In Proc. of
COLING, pages 947?953, Saarbr?cken, August.
828
An Improved CRF based Chinese Language Processing System for SIGHAN
Bakeoff 2007
Xihong Wu, Xiaojun Lin, Xinhao Wang, Chunyao Wu, Yaozhong Zhang and Dianhai Yu
Speech and Hearing Research Center
State Key Laboratory of Machine Perception,
Peking University, China, 100871
{wxh,linxj,wangxh,wucy,zhangyaoz,yudh}@cis.pku.edu.cn
Abstract
This paper describes three systems: the
Chinese word segmentation (WS) system,
the named entity recognition (NER) sys-
tem and the Part-of-Speech tagging (POS)
system, which are submitted to the Fourth
International Chinese Language Processing
Bakeoff. Here, Conditional Random Fields
(CRFs) are employed as the primary mod-
els. For the WS and NER tracks, the n-
gram language model is incorporated in our
CRFs based systems in order to take into ac-
count the higher level language information.
Furthermore, to improve the performances
of our submitted systems, a transformation-
based learning (TBL) technique is adopted
for post-processing.
1 Introduction
Among 24 closed and open tracks in this bakeoff, we
participated in 23 tracks, except the open NER track
of MSRA. Our systems are ranked 1st in 6 tracks,
and get close to the top level in several other tracks.
Recently, Maximum Entropy model(ME) and
CRFs (Low et al, 2005)(Tseng et al, 2005) (Hai
Zhao et al, 2006) turned out to be promising in natu-
ral language processing tracks, and obtain excellent
performances on most of the test corpora of Bake-
off 2005 and Bakeoff 2006. Compared to the gen-
erative models, like HMM, the primary advantage
of CRFs is that it relaxes the independence assump-
tions, which makes it able to handle multiple inter-
acting features between observation elements (Wal-
lach et al, 2004).
However, the ME and CRFs emphasize the rela-
tion of the basic units of sequence, like the Chinese
characters in these tracks. While, the higher level
information, like the relationship of the words is ig-
nored. From this point of view, the n-gram language
model is incorporated in our CRFs based systems in
order to cover the word level language information.
Based on several pilot-experimental results, we
found that the tagging errors always follow some
patterns. In order to find those error patterns and cor-
rect the similar errors, we integrated the TBL post-
processor in our systems. In addition, extra train-
ing data, which is transformed from People Daily
Corpus (Shiwen Yu et al, 2000) with some auto-
extracted transition rules, is used in each corpus for
the open tracks of WS.
The remainder of this paper is organized as fol-
lows. The scheme of our three developed systems
are described in section 2, 3 and 4, respectively. In
section 5, evaluation results based on these systems
are enumerated and discussed. Finally some conclu-
sions are drawn in section 6.
2 Word Segmentation
The WS system mainly consists of three compo-
nents, CRFs, n-gram language model and post-
processing strategies.
2.1 Conditional Random Fields
Conditional Random Fields, as the statistical se-
quence labeling models, achieve great success in
natural language processing, such as chunking (Fei
Sha et al, 2003) and word segmentation (Hai Zhao
et al, 2006). Different from traditional generative
155
Sixth SIGHAN Workshop on Chinese Language Processing
model, CRFs relax the constraint of the indepen-
dence assumptions, and therefore turn out to be more
suitable for natural language tasks.
CRFs model the conditional distribution p(Y |X)
of the labels Y given the observations X directly
with the formulation:
P?(Y |X) = 1Z(X)exp{
?
c?C
?
k
?kfk(Yc, X, c)}
(1)
Y is the label sequence, X is the observation se-
quence, Z(X) is a normalization term, fk is a fea-
ture function, and c is the set of cliques in Graphic.
In our tasks, C = {(yi?1, yi)}, X is the Chinese
character sequence of a sentence.
To label a Chinese character, we need to define
the label tags. Here we have six types of tags ac-
cording to character position in a word (Hai Zhao et
al., 2006):
tag = {B1, B2, B3, I, E, S}
?B1, B2, B3, I, E? represent the first, second, third,
continue, and end character positions in a multi-
character word, and ?S? is the single-character word
tag.
The unigram feature templates used here are:
Cn (n = ?2,?1, 0, 1, 2)
CnCn+1 (n = ?2,?1, 0)
CnCn+1Cn+2 (n = ?1)
Where C0 refers to the current character and
C?n(Cn) is the nth character to the left(right) of the
current character. We also use the basic bigram fea-
ture template which denotes the dependency on the
previous tag and current tag.
2.2 Multi-Model Integration
In order to integrate multi-model information, we
use a log-linear model(Och et al, 2002) to compute
the posterior probability:
Pr (W |C) = p?M1 (W |C)
= exp[
?M
m=1 ?mhm(W,C)]
?
W ? exp[
?M
m=1 ?mhm(W ?, C)]
(2)
Where W is the word sequence, and C is the char-
acter sequence. The decision rule here is:
W0 = argmaxW {Pr(W |C)}
= argmaxW {
M
?
m=1
?mhm(W,C)} (3)
The parameters ?M1 of this model can be opti-
mized by standard approaches, such as the Mini-
mum Error Rate Training used in machine transla-
tion (Och, 2003). In fact, the CRFs approach is
a special case of this framework when we define
M = 1 and use the following feature function:
h1(W,C) = logP?(Y |X) (4)
In our approach, the logarithms of the scores gen-
erated by the two kinds of models are used as feature
functions:
h1(W,C) = logPcrf (W,C)
= log
?
w
i
P?(wi|C) (5)
h2(W,C) = logPlm(W ) (6)
The first feature function(Eq.5) comes from CRFs.
Instead of computing the score of the whole la-
bel sequence Y with character sequence X through
P?(Y |X) directly, we try to get the posterior prob-
ability of a sub-sequence to be tagged as one whole
word P?(wi|C). Then we combine all the score of
words together. The second feature function(Eq.6)
comes from n-gram language model, which aims to
catch the words information.
The log-linear model with the feature functions
described above allows the dynamic programming
search algorithm for efficient decoding. The system
generates the word lattice with posterior probability
P?(wi|C). Then the best word sequence is searched
on the word lattice with the decision rule(Eq.3).
Since arbitrary sub-sequence can be viewed as a
candidate word in word lattice, we need to deal with
the problem of OOV words. The unigram of an OOV
word is estimated as:
Unigram(OOV Word) = pl (7)
where p is the minimal value of unigram scores in
the language model; l is the length of the OOV
word, which is used as a punishment factor to
avoid overemphasizing the long OOV words (Xin-
hao Wang et al, 2006).
2.3 Post-Processing Strategies
The division and combination rule, which has been
proved to be useful in our system of Bakeoff 2006
(Xinhao Wang et al, 2006), is adopted for the post-
processing in the system.
156
Sixth SIGHAN Workshop on Chinese Language Processing
2.4 Training Data Transition
For the WS open tracks, the unique difference from
closed tracks is that the additional training data is
supplemented for model refinement.
For the Simplified Chinese tracks, the additional
training data are collected from People Daily Cor-
pus with a set of auto-extracted transition rules. This
process is performed in a heuristic strategy and con-
tains five steps as follows:
(1) Segment the raw People Daily texts with the cor-
responding system for the closed track of each cor-
pus.
(2) Compare the result of step 1 with People Daily
Corpus to get the conflict pairs. For example,
{pair1: ??? vs. ???}
(Zhemin Jiang)
{pair2: ??? vs. ???}
(catch with two hands)
In each pair, the left phrase follows the People Daily
Corpus segmentation guideline, while the right one
is the phrase obtained from step 1.
(3) Divide the pairs into two sets: the first set con-
tains the pairs with right phrase appearing in the tar-
get training data; the other pairs are in the second
set.
(4) Select sentences which contain the left phrase of
the pairs in the second set from People Daily Cor-
pus.
(5) Transform these selected sentences by replacing
their phrase in the left side of the pair in the first set
to the right one. This is used as our transition rules.
3 Named Entity Recognition
The named entity recognition track is viewed as a
character sequence tagging problem in our NER sys-
tem and the log-linear model mentioned above is
employed again to integrate multi-model informa-
tion. To find the error patterns and correct them,
a TBL strategy is then used in the post-processing
module.
3.1 Model Description
In this NER track, we employe the log-linear model
and use the logarithms of the scores generated by the
two types of models as feature functions. Besides
CRFs, another model is the class-based n-gram lan-
guage model:
h1(Y, X) = logPcrf (Y, X)
= logP?(Y |X) (8)
h2(Y, X) = logPclm(Y, X) (9)
Y is the label sequence and X is the character se-
quence.
CRFs are used to generate the N-best tagging re-
sults with the scores of whole label sequence Y on
character sequence X by P?(Y |X). And then, the
log-linear model is used to reorder the N-best tag-
ging results by integrating the CRFs score and the
class-based n-gram language model score together.
CRFs
In this track, one Chinese character is labeled by
a tag of ten classes, which denoting the beginning,
continue, ending character of a specified named en-
tity or a non-entity character. There are three types
of named entities in these tracks, including person
name, location name and organization name.
In CRFs, the basic features used here are:
Cn (n = ?2,?1, 0, 1, 2)
CnCn+1 (n = ?2,?1, 0, 1)
CnCn+2 (n = ?1)
Besides basic unigram features, the bigram transi-
tion features considering the previous tag is adopted
with template Cn (n = ?2,?1, 0, 1, 2).
Class-Based N-gram Language Model
For the class-based n-gram language model, we
define that each character is a single class, while
each type of named entity is viewed as a single class.
With the character sequence and label sequence, the
class sequence can be generated. Take this sentence
for instance:
???????????
(But Ibrahimov is not satisfied)
Table 1 shows its class sequence. Class-based n-
gram language model can be trained with class se-
quence.
3.2 TBL
Since the analysis on our experiments shows that the
tagging errors always follow some patterns in NER
track, TBL strategy is adopted in our system to find
these patterns and correct the similar errors.
157
Sixth SIGHAN Workshop on Chinese Language Processing
character sequence ? ? ? ? ? ? ? ? ? ? ?
label sequence N Per-B Per-C Per-C Per-C Per-C Per-E N N N N
class sequence ? PERSON ? ? ? ?
Table 1: A class sequence example
Transformation-based learning is a symbolic ma-
chine learning method, introduced by (Eric Brill,
1995). The main idea in TBL is to generate a set of
transformation rules that can correct tagging errors
produced by the initial process.
There are four main procedures in our TBL
framework: An initial state assignment which is op-
erated by the system we described above; a set of al-
lowable templates for rules, ranging from words in
a 3 positions windows and name entity information
in a 3-word window with their combinations consid-
ered, and rules which are learned according to the
tagging differences between training data and results
generated by our system, at last, those rules are in-
troduced to correct similar errors.
4 POS Tagging
The POS tagging track is to assign the part-of-
speech sequence for the correctly segmented word
sequence. In our system, for the CTB corpus, the
CRFs are adopted; however for the other four cor-
pora, considering the limitations of resources and
time, the ME model is adopted. To improve the per-
formance of ME model, the POS tag of the previous
word is taken as a feature and the dynamic program-
ming strategy is used in decoding.
In the closed track, the features include the basic
features and their combined features. Firstly the pre-
vious and next words of the current word are taken
as the basic features. Secondly, based on the anal-
ysis of the OOV words, the first and last characters
of the current word, as well as the length of the cur-
rent word are proven to be effective features for the
OOV POS. Furthermore since the long distance con-
straint word may impact the POS of current word
(Yan Zhao et al, 2006), in the open track, a Chi-
nese parser is imported and the word depended on
the current word is extracted as feature.
5 Experiments and Results
We have participated in 23 tracks, except the open
NER track of MSRA. CRFs, ME model and n-gram
language model are adopted in these systems. Our
implementation uses the CRF++ package1 provided
by Taku Kudo, the Maximum Entropy Toolkit2 pro-
vided by Zhang Le, and the SRILM Toolkit provided
by Andreas Stolcke (Andreas Stolcke et al, 2002).
5.1 Chinese Word Segmentation
In the closed tracks, CRFs and bigram language
model are trained on the given training data for each
corpus. In order to integrate these two models, it is
necessary to train the corresponding parameter ?M1
with Minimum Error Rate Training approache based
on a development data. Since the development data
is not provided in this bakeoff, a ten-fold cross val-
idation approach is employed to implement the pa-
rameter training. A set of parameters can be trained
independently, and then the mean value is calculated
as the estimation of each parameter.
Table 2 gives the results of our WS system for
closed tracks.
baseline +LM +LM+Post
CTB 94.7 94.7 94.8
NCC 92.6 92.4 92.9
SXU 94.7 95.7 95.8
CITYU 92.9 93.7 93.9
CKIP 93.2 93.7 93.7
Table 2: Word segmentation performance on F-
value with different approach for the closed tracks
In the open tracks, as we do not have enough time
to finish the parameter estimation on the new data,
our system adopt the same parameters ?M1 used in
closed tracks. The unique difference from closed
1http://chasen.org/taku/software/CRF++
2http://homepages.inf.ed.ac.uk/s0450736/maxent
toolkit.html
158
Sixth SIGHAN Workshop on Chinese Language Processing
tracks is that extra training data is added for each
corpus to improve the performance. For the Sim-
plified Chinese tracks, additional data comes from
People Daily Corpus which is transformed by our
transition strategy. At the same time, for the Tra-
ditional Chinese tracks, additional data comes from
the training and testing data used in the early Bake-
off. However, we implement two systems for the
CTB open track. The system (a) takes the training
and testing data used in the early Bakeoff as addi-
tional data, and System (b) takes the translated Peo-
ple Daily Corpus as additional data. Table 3 gives
the results of our open WS system.
baseline +LM +LM+Post
CTB(a) 99.2 99.2 99.3
CTB(b) 95.6 95.1 97.0
NCC 93.7 93.0 92.9
SXU 96.4 87.0 95.8
CITYU 95.8 90.6 91.0
CKIP 94.5 94.8 95.1
Table 3: Word segmentation performance on F-
value with different approach for the open tracks
The result shows that the system performance is
sensitive to the parameters ?M1 . Although we train
the useful parameter for closed tracks, it plays a bad
role in open tracks as we do not adapt it for the ad-
ditional training data.
5.2 Named Entity Recognition
In the closed NER tracks, CRFs and class-based tri-
gram language model are trained on the given train-
ing data for each corpus. The same approach em-
ployed in the WS tracks is adopted to train the corre-
sponding parameter ?M1 in our NER systems. Mean-
while, the TBL rules trained via five-fold cross val-
idation approach are also used in post-processing
procedure. Table 4 reports the results of our closed
NER system.
5.3 POS Tagging
The experiments show that the CRFs/ME method is
superior to the TBL method, and the concurrent er-
rors for these two methods are less than 60%. There-
fore we adopted TBL to correct the output results
of CRFs/ME: If the output tags of CRFs/ME and
baseline +LM +LM+Post
MSRA 89.3 89.7 89.9
CITYU 79.3 80.6 80.5
Table 4: Named entity recognition F-value through
different approaches for the closed tracks
TBL are not consistent and the output probability
of CRFs/ME is below a certain threshold, the TBL
results are fixed. Here the 90% of the training set
is taken as the training data and remained 10% is
separated as the development data to get the thresh-
old, which is 0.60 for the CRFs, and 0.90 for the
ME. In addition, the POS tagged corpus of the Chi-
nese Treebank 5.0 from LDC is added to the training
data for CTB open track. In our system, the Berke-
ley Parser (Slav Petrov et al, 2006) is adopted to
obtain the long distance constraint words. The per-
formance achieved by the methods described above
on each corpus are reported in Table 5.
CRFs/ME CRFs/ME
CRFs/ME TBL +TBL +TBL
+Syntax
CTIYU 88.7 87.7 89.1 89.0
CKIP 91.8 91.4 92.2 92.1
CTB 94.0 92.7 94.3 96.5
NCC 94.6 94.3 94.9 95.0
PKU 93.5 93.2 94.0 94.1
Table 5: POS tagging performance on total-accuracy
with different approach
6 Conclusion
In this paper, we have briefly described our systems
participating in the Bakeoff 2007. In the WS and
NER systems, the log-linear model is adopted to in-
tegrate CRFs and language model, which improves
the system performances effectively. At the same
time, system integration approach used in the POS
system also proves its validity. In addition, a heuris-
tic strategy is imported to generate additional train-
ing data for the open WS tracks. Finally, several
post-processing strategies are used to further im-
prove our systems.
159
Sixth SIGHAN Workshop on Chinese Language Processing
References
Jin Kiat Low, Hwee Tou Ng and Wenyuan Guo. 2005.
A Maximum Entropy Approach to Chinese Word Seg-
mentation. Proceedings of the Fourth SIGHAN Work-
shop on Chinese Language Processing. pp. 161-164.
Jeju Island, Korea.
Huihsin Tseng, Pichuan Chang, Galen Andrew, Daniel
Jurafsky, Christopher Manning. 2005. A Conditional
Random Field Word Segmenter for Sighan Bakeoff
2005. Proceedings of the Fourth SIGHAN Workshop
on Chinese Language Processing. pp. 168-171. Jeju
Island, Korea.
Hai Zhao, Chang-Ning Huang and Mu Li. 2006. An
Improved Chinese Word Segmentation System with
Conditional Random Field. Proceedings of the Fifth
SIGHAN Workshop on Chinese Language Processing.
pp. 162-165. Sydney, Australia.
Hanna M. Wallach. 2004. Conditional Random Fields:
An Introduction. Technical Report, UPenn CIS TR
MS-CIS-04-21.
Shiwen Yu, Xuefeng Zhu and Huiming Duan. 2000.
Specification of large-scale modern Chinese corpus.
Proceedings of ICMLP?2001. pp. 18-24. Urumqi,
China.
Fei Sha and Fernando Pereira. 2003. Shallow Parsing
with Conditional Random Fields. Proceedings of Hu-
man Language Technology/NAACL. pp. 213-220. Ed-
monton, Canada.
Franz Josef Och and Hermann Ney. 2002. Discrimi-
native training and maximum entropy models for sta-
tistical machine translation. Proceedings of the 40th
Annual Meeting of the Association for Computational
Linguistics (ACL). pp. 295-302. Philadelphia, PA.
Franz Josef Och. 2003. Minimum Error Rate Train-
ing in Statistical Machine Translation. Proceedings of
the 41th Annual Meeting of the Association for Com-
putational Linguistics (ACL). pp. 160-167. Sapporo,
Japan.
Xinhao Wang, Xiaojun Lin, Dianhai Yu, Hao Tian, Xi-
hong Wu. 2006. Chinese Word Segmentation with
Maximum Entropy and N-gram Language Model. the
Fifth SIGHAN Workshop on Chinese Language Pro-
cessing. pp. 138-141. Sydney, Australia.
Eric Brill. 1995. Transformation-based error-driven
learning and natural language processing: a case study
in Part-of-Speech tagging. Computational Lingusitics.
21(4).
Yan Zhao, Xiaolong Wang, Bingquan Liu, and Yi Guan.
2006. Fusion of Clustering Trigger-Pair Features for
POS Tagging Based on Maximum Entropy Model.
Journal of Computer Research and Development.
43(2). pp. 268-274.
Andreas Stolcke. 2002. SRILM - An Extensible Lan-
guage Modeling Toolkit. Proceedings of International
Conference on Spoken Language Processing. pp. 901-
904. Denver, Colorado.
Slav Petrov, Leon Barrett, Romain Thibaux and Dan
Klein. 2006. Learning Accurate, Compact, and Inter-
pretable Tree Annotation. Proceedings of the 21st In-
ternational Conference on Computational Linguistics
and the 44th annual meeting of the ACL. pp. 433-440.
Sydney, Australia.
160
Sixth SIGHAN Workshop on Chinese Language Processing
Proceedings of the Fifth SIGHAN Workshop on Chinese Language Processing, pages 138?141,
Sydney, July 2006. c?2006 Association for Computational Linguistics
Chinese Word Segmentation with Maximum Entropy
and N-gram Language Model
Wang Xinhao, Lin Xiaojun, Yu Dianhai, Tian Hao, Wu Xihong
National Laboratory on Machine Perception,
School of Electronics Engineering and Computer Science,
Peking University, China, 100871
{wangxh,linxj,yudh,tianhao,wxh}@cis.pku.edu.cn
Abstract
This paper presents the Chinese word seg-
mentation systems developed by Speech
and Hearing Research Group of Na-
tional Laboratory on Machine Perception
(NLMP) at Peking University, which were
evaluated in the third International Chi-
nese Word Segmentation Bakeoff held by
SIGHAN. The Chinese character-based
maximum entropy model, which switches
the word segmentation task to a classi-
fication task, is adopted in system de-
veloping. To integrate more linguistics
information, an n-gram language model
as well as several post processing strate-
gies are also employed. Both the closed
and open tracks regarding to all four cor-
pora MSRA, UPUC, CITYU, CKIP are
involved in our systems? evaluation, and
good performance are achieved. Espe-
cially, in the closed track on MSRA, our
system ranks 1st.
1 Introduction
Chinese word segmentation is one of the core tech-
niques in Chinese language processing and attracts
lots of research interests in recent years. Sev-
eral promising methods are proposed by previous
researchers, in which Maximum Entropy (ME)
model has turned out to be a successful way for
this task (Hwee Tou Ng et al, 2004; Jin Kiat
Low et al, 2005). By employing Maximum En-
tropy (ME) model, the Chinese word segmentation
task is regarded as a classification problem, where
each character will be classified to one of the four
classes, i.e., the beginning, middle, end of a multi-
character word and a single-character word.
However, in a high degree, ME model pays its
emphasis on Chinese characters while debases the
consideration on the relationship of the context
words. Motivated by this view, several strategies
used for reflecting the context words? relationship
and integrating more linguistics information, are
employed in our systems.
As known, an n-gram language model could ex-
press the relationship of the context words well, it
therefore as a desirable choice is imported in our
system to modify the scoring of the ME model.
An analysis on our preliminary experiments shows
the combination ambiguity is another issue that
should be specially tackled, and a division and
combination strategy is then adopted in our sys-
tem. To handle the numeral words, we also intro-
duce a number conjunction strategy. In addition,
to deal with the long organization names problem
in MSRA corpus, a post processing strategy for
organization name is presented.
The remainder of this paper is organized as fol-
lows. Section 2 describes our system in detail.
Section 3 presents the experiments and results.
And in last section, we draw our conclusions.
2 System Description
With the ME model, n-gram language model, and
several post processing strategies, our systems are
established. And detailed description on these
components are given in following subsections.
2.1 Maximum Entropy Model
The ME model used in our system is based on the
previous works (Jin Kiat Low et al, 2005; Hwee
Tou Ng et al, 2004). As mentioned above, the
ME model based word segmentation is a 4-classes
learning process. Here, we remarked four classes,
i.e. the beginning, middle, end of a multi-character
138
word and a single-character word, as b, m, e and s
respectively.
In ME model, the following features (Jin Kiat
Low et al, 2005) are selected:
a) cn (n = ?2,?1, 0, 1, 2)
b) cncn+1 (n = ?2,?1, 0, 1)
c) c?1c+1
where cn indicates the character in the left or right
position n relative to the current character c0.
For the open track especially, three extended
features are extracted with the help of an external
dictionary as follows:
d) Pu (c0)
e) L and t0
f) cnt0 (n = ?1, 0, 1)
where Pu(c0) denotes whether the current charac-
ter is a punctuation, L is the length of word W that
conjoined from the character and its context which
matching a word in the external dictionary as long
as possible. t0 is the boundary tag of the character
in W.
With the features, a ME model is trained which
could output four scores for each character with
regard to four classes. Based on scores of all char-
acters, a completely segmented semiangle matrix
can be constructed. Each element wji in this ma-
trix represents a word that starts at the ith charac-
ter and ends at jth character, and its value ME(j, i),
the score for these (j ? i+1) characters to form a
word, is calculated as follow:
ME[j, i] = ? log p(w = ci...cj)
= ? log[p(bci)p(mci+1)...
p(mcj?1)p(ecj )]
(1)
As a consequence, the optimal segmentation re-
sults corresponding to the best path with the low-
est overall score could be reached via a dynamic
programming algorithm. For example:
@?c????(I was 19 years old that year)
Table 1 shows its corresponding matrix. In this
example, the ultimate segmented result is:
@ ?c ? ???
2.2 Language Model
N-gram language model, a widely used method
in natural language processing, can represent the
context relation of words. In our systems, a bi-
gram model is integrated with ME model in the
phase of calculating the path score. In detail, the
score of a path will be modified by adding the bi-
gram of words with a weight ? at the word bound-
aries. The approach used for modifying path score
is based on the following formula.
V [j, i] = ME[j, i]
+mini?1k=1{[(V [i ? 1, k]
+?Bigram(wk,i?1, wi,j)}
(2)
where V[j,i] is the score of local best path which
ends at the jth character and the last word on the
path is wi,j = ci...cj , the parameter ? is optimized
by the test set used in the 2nd International Chi-
nese Word Segmentation Bakeoff. When scoring
the path, if one of the words wk,i?1 and wi,j is out
of the vocabulary, their bigram will backoff to the
unigram. And the unigram of the OOV word will
be calculated as:
Unigram(OOV Word) = pl (3)
where p is the minimal unigram value of words in
vocabulary; l is the length of the word acting as
a punishment factor to avoid overemphasizing the
long OOV words.
2.3 Post Processing Strategies
The analysis on preliminary experiments, where
the ME model and n-gram language model are in-
volved, lead to several post processing strategies
in developing our final systems.
2.3.1 Division and Combination Strategy
To handle the combination ambiguity issue,
we introduce a division and combination strategy
which take in use of unigram and bigram. For
each two words A and B, if their bigrams does
not exist while there exists the unigram of word
AB, then they can be conjoined as one word. For
example, ??ff(August)? and ???(revolution)?
are two segmented words, and in training set the
bigram of ??ff? and ???? is absent, while
the word ??ff??(the August Revolution)? ap-
peares, then the character string ??ff??? is
conjoined as one word. On the other hand, for a
word C which can be divided as AB, if its uni-
gram does not exit in training set, while the bigram
of its subwords A and B exits, then it will be re-
segmented. For example, Taking the word ??L
N?U?(economic system reform)? for instance,
if its corresponding unigram is absent in training
set, while the bigram of two subwords ??LN
139
@ ? c ? ? ? ?
1 2 3 4 5 6 7
@ 1 6.3180e-07
? 2 33.159 7.5801
c 3 26.401 0.0056708 5.2704
? 4 71.617 45.221 49.934 3.1001e-07
? 5 83.129 56.734 61.446 33.869 7.0559
? 6 90.021 63.625 68.337 40.760 12.525 12.534
? 7 77.497 51.101 55.813 28.236 0.0012012 10.077 10.055
Table 1: A completely segmented matrix
?(economic system)? and ?U?(reform)? exists,
as a consequence, it will be segmented into two
words ??LN?? and ?U??.
2.3.2 Numeral Word Processing Strategy
The ME model always segment a numeral
word into several words. For instance, the word
?4.34(RMB Yuan 4.34)?, may be segmented
into two words ?4.? and ?34?. To tackle this
problem, a numeral word processing strategy is
used. Under this strategy, those words that contain
Arabic numerals are manually marked in the train-
ing set firstly, then a list of high frequency charac-
ters which always appear alone between the num-
bers in the training set can be extracted, based on
which numeral word issue can be tackled as fol-
lows. When segmenting one sentence, if two con-
joint words are numeral words, and the last char-
acter of the former word is in the list, then they are
combined as one word.
2.3.3 Long Organization Name Processing
Strategy
Since an organization name is usually an OOV,
it always will be segmented as several words, es-
pecially for a long one, while in MSRA corpus, it
is required to be recognized as one word. In our
systems, a corresponding strategy is presented to
deal with this problem. Firstly a list of organiza-
tion names is manually selected from the training
set and stored in the prefix-tree based on charac-
ters. Then a list of prefixes is extracted by scan-
ning the prefix-tree, that is, for each node, if the
frequencies of its child nodes are all lower than the
predefined threshold k and half of the frequency of
the current node, the string of the current node will
be extracted as a prefix; otherwise, if there exists
a child node whose frequency is higher than the
threshold k, scan the corresponding subtree. In the
same way, the suffixes can also be extracted. The
only difference is that the order of characters is in-
verse in the lexical tree.
During recognizing phase, to a successive
words string that may include 2-5 words, will be
combined as one word, if all of the following con-
ditions are satisfied.
a) Does not include numbers, full stop or comma.
b) Includes some OOV words.
c) Has a tail substring matching some suffix.
d) Appears more than twice in the test data.
e) Has a higher frequency than any of its substring which
is an OOV word or combined by multiple words.
f) Satisfy the condition that for any two successive words
w1 w2 in the strings, freq(w1w2)/freq(w1)?0.1, unless w1
contains some prefix in its right.
3 Experiments and Results
We have participated in both the closed and open
tracks of all the four corpora. For MSRA corpus
and other three corpora, we build System I and
System II respectively. Both systems are based on
the ME model and the Maximum Entropy Toolkit
1, provided by Zhang Le, is adopted.
Four systems are derived from System I with re-
gard to whether or not the n-gram language model
and three post processing strategies are used on the
closed track of MSRA corpus. Table 2 shows the
results of four derived systems.
System R P F ROOV RIV
IA 95.0 95.7 95.3 66.0 96.0
IB 96.0 95.6 95.8 60.3 97.3
IC 96.4 96.0 96.2 60.3 97.7
ID 96.4 96.1 96.3 61.2 97.6
Table 2: The effect of MEmodel, n-gram language
model and three post processing strategies on the
closed track of MSRA corpus.
System IA only adopts the ME model. System
IB integrates the ME model and the bigram lan-
guage model. System IC integrates the division
and combination strategy and the numeral words
1http://homepages.inf.ed.ac.uk/s0450736
/maxent toolkit.html
140
processing strategy. System ID adds the long or-
ganization name processing strategy.
For the open track of MSRA, an external dictio-
nary is utilized to extract the e and f features. The
external dictionary is built from six sources, in-
cluding the Chinese Concept Dictionary from In-
stitute of Computational Linguistics, Peking Uni-
versity(72,716 words), the LDC dictionary(43,120
words), the Noun Cyclopedia(111,633), the word
segmentation dictionary from Institute of Com-
puting Technology, Chinese Academy of Sci-
ences(84,763 words), the dictionary from Insti-
tute of Acoustics, and the dictionary from Insti-
tute of Computational Linguistics, Peking Univer-
sity(68,200 words) and a dictionary collected by
ourselves(63,470 words).
The union of the six dictionaries forms a big
dictionary, and those words appearing in five or
six dictionaries are extracted to form a core dic-
tionary. If a word belongs to one of the following
dictionaries or word sets, it is added into the exter-
nal dictionary.
a) The core dictionary.
b) The intersection of the big dictionary and the training
data.
c) The words appearing in the training data twice or more
times.
Those words in the external dictionaries will be
eliminated, if in most cases they are divided in
the training data. Table 3 shows the effect of ME
model, n-gram language model, three post pro-
cessing strategies on the open track of MSRA.
Here System IO only adopts the basic features,
while the external dictionary based features are
used in four derived systems related to open track:
IA, IB, IC, ID.
System R P F ROOV RIV
IO 96.0 96.5 96.3 71.1 96.9
IA 97.5 96.9 97.2 65.9 98.6
IB 97.6 96.8 97.2 64.8 98.7
IC 97.7 97.0 97.4 66.8 98.8
ID 97.7 97.1 97.4 67.5 98.8
Table 3: The effect of MEmodel, n-gram language
model, three post processing strategies on the open
track of MSRA.
System II only adopts ME model, the division
and combination strategy and the numeral word
processing strategy. In the open track of the cor-
pora CKIP and CITYU, the training set and test set
from the 2nd Chinese Word Segmentation Backoff
are used for training. For the corpora UPUC and
CITYU, the external dictionaries are used, which
is constructed in the same way as that in the open
track of MSRA Corpus. Table 4 shows the official
results of system II on UPUC, CKIP and CITYU.
Corpus R P F ROOV RIV
UPUC-C 93.6 92.3 93.0 68.3 96.1
UPUC-O 94.0 90.7 92.3 56.1 97.6
CKIP-C 95.8 94.8 95.3 64.6 97.2
CKIP-O 95.8 94.8 95.3 64.7 97.2
CITYU-C 96.9 97.0 97.0 77.3 97.8
CITYU-O 97.9 97.6 97.7 81.3 98.5
Table 4: Official results of our systems on UPUC
CKIP and CITYU
On the UPUC corpus, an interesting observation
is that the performance of the open track is worse
than the closed track. The investigation and analy-
sis lead to a possible explanation. That is, the seg-
mentation standard of the dictionaries, which are
used to construct the external dictionary, is differ-
ent from that of the UPUC corpus.
4 Conclusion
In this paper, a detailed description on several Chi-
nese word segmentation systems are presented,
where ME model, n-gram language model as well
as three post processing strategies are involved. In
the closed track of MSRA, the integration of bi-
gram language model greatly improves the recall
ratio of the words in vocabulary, although it will
impairs the performance of system in recognizing
the words out of vocabulary. In addition, three
strategies are introduced to deal with combination
ambiguity, numeral word, long organization name
issues. And the evaluation results reveal the valid-
ity and effectivity of our approaches.
References
Jin Kiat Low, Hwee Tou Ng and Wenyuan Guo.
A maximum Entropy Approach to Chinese Word
Segmentation. 2005. Preceedings of the Fourth
SIGHAN Workshop on Chinese Language Process-
ing, pp. 161-164.
Hwee Tou Ng and Jin Kiat Low. Chinese part-of-
speech tagging: One-at-a-time or all-at-once? word-
based or character-based? 2004. Preceedings of the
2004 Conference on Empirical Methods in Natural
Language Processing(EMNLP), pp. 277-284.
Zhang Huaping and Liu Qun. Model of Chinese
Words Rough Segmentation Based on N-Shortest-
Paths Method. 2002. Journal of Chinese Informa-
tion Processing, 28(1):pp. 1-7.
141
Proceedings of NAACL-HLT 2013, pages 814?819,
Atlanta, Georgia, 9?14 June 2013. c?2013 Association for Computational Linguistics
Coherence Modeling for the Automated Assessment of  
Spontaneous Spoken Responses 
 
Xinhao Wang, Keelan Evanini, Klaus Zechner 
Educational Testing Service 
660 Rosedale Road 
Princeton, NJ 08541, USA 
xwang002,kevanini,kzechner@ets.org 
 
 
 
 
Abstract 
This study focuses on modeling discourse co-
herence in the context of automated assess-
ment of spontaneous speech from non-native 
speakers. Discourse coherence has always 
been used as a key metric in human scoring 
rubrics for various assessments of spoken lan-
guage. However, very little research has been 
done to assess a speaker's coherence in auto-
mated speech scoring systems. To address 
this, we present a corpus of spoken responses 
that has been annotated for discourse coher-
ence quality. Then, we investigate the use of 
several features originally developed for es-
says to model coherence in spoken responses. 
An analysis on the annotated corpus shows 
that the prediction accuracy for human holistic 
scores of an automated speech scoring system 
can be improved by around 10% relative after 
the addition of the coherence features.  Fur-
ther experiments indicate that a weighted F-
Measure of 73% can be achieved for the au-
tomated prediction of the coherence scores. 
1 Introduction 
In recent years, much research has been conducted 
into developing automated assessment systems to 
automatically score spontaneous speech from non-
native speakers with the goals of reducing the bur-
den on human raters, improving reliability, and 
generating feedback that can be used by language 
learners. Various features related to different as-
pects of speaking proficiency have been exploited, 
such as delivery features for pronunciation, proso-
dy, and fluency (Strik and Cucchiarini, 1999; Chen 
et al, 2009; Cheng, 2011; Higgins et al, 2011), as 
well as language use features for vocabulary and 
grammar, and content features (Chen and Zechner, 
2011; Xie et al, 2012). However, discourse-level 
features related to topic development have rarely 
been investigated in the context of automated 
speech scoring. This is despite the fact that an im-
portant criterion in the human scoring rubrics for 
speaking assessments is the evaluation of coher-
ence, which refers to the conceptual relations be-
tween different units within a response. 
Methods for automatically assessing discourse 
coherence in text documents have been widely 
studied in the context of applications such as natu-
ral language generation, document summarization, 
and assessment of text readability. For example, 
Foltz et al (1998) measured the overall coherence 
of a text by utilizing Latent Semantic Analysis 
(LSA) to calculate the semantic relatedness be-
tween adjacent sentences. Barzilay and Lee (2004) 
introduced an HMM-based model for the docu-
ment-level analysis of topics and topic transitions. 
Barzilay and Lapata (2005; 2008) presented an 
approach to coherence modeling which focused on 
the entities in the text and their grammatical transi-
tions between adjacent sentences, and calculated 
the entity transition probabilities on the document 
level. Pitler et al (2010) provided a summary of 
the performance of several different types of 
features for automated coherence evaluation, such 
as cohesive devices, adjacent sentence similarity, 
Coh-Metrix (Graesser et al, 2004), word co-
occurrence patterns, and entity-grid. 
In addition to studies on well-formed text, re-
searchers have also addressed coherence modeling 
on text produced by language learners, which may 
contain many spelling and grammar errors.  
Utilizing LSA and Random Indexing methods, 
Higgins et al (2004) measured the global 
814
coherence of students? essays by calculating the 
semantic relatedness between sentences and the 
corresponding prompts. In addition, Burstein et. al 
(2010) combined entity-grid features with writing 
quality features produced by an automated assess-
ment system of essays to predict the coherence 
scores of student essays. Recently, Yannakoudakis 
and Briscoe (2012) systematically analyzed a vari-
ety of coherence modeling methods within the 
framework of an automated assessment system for 
non-native free text responses and indicated that 
features based on Incremental Semantic Analysis 
(ISA), local histograms of words, the part-of-
speech IBM model, and word length were the most 
effective.   
In contrast to these previous studies involving 
well-formed text or learner text containing errors, 
this paper focuses on modeling coherence in spon-
taneous spoken responses as well as investigating 
discourse features in an attempt to extend the con-
struct coverage of an automated speech scoring 
system. In a related study, Hassanali et al (2012) 
investigated coherence modeling for spoken lan-
guage in the context of a story retelling task for the 
automated diagnosis of children with language im-
pairment. They annotated transcriptions of chil-
dren's narratives with coherence scores as well as 
markers of narrative structure and narrative quali-
ty; furthermore they built models to predict the 
coherence scores based on Coh-Metrix features 
and the manually annotated narrative features. The 
current study differs from this one in that it deals 
with free spontaneous spoken responses provided 
by students at a university level; these responses 
therefore contain more varied and more complicat-
ed information than the child narratives. 
The main contributions of this paper can be 
summarized as follows: First, we obtained coher-
ence annotations on a corpus of spontaneous spo-
ken responses drawn from a university-level 
English language proficiency assessment, and 
demonstrated an improvement of around 10% rela-
tive in the accuracy of the automated prediction of 
human holistic scores with the addition of the co-
herence annotations. Second, we applied the entity-
grid features and writing quality features from an 
automated essay scoring system to predict the co-
herence scores; the experimental results have 
shown promising correlations between some of 
these features and the coherence scores.  
2 Data and Annotation 
2.1 Data 
For this study, we collected 600 spoken responses 
from the international TOEFL? iBT assessment of 
English proficiency for non-native speakers. 100 
responses were drawn from each of 6 different test 
questions comprising two different speaking tasks: 
1) providing an opinion based on personal experi-
ence (N = 200) and 2) summarizing or discussing 
material provided in a reading and/or listening pas-
sage (N = 400). The spoken responses were all 
transcribed by humans with punctuation and capi-
talization. The average number of words contained 
in the responses was 104.4 (st. dev. = 34.4) and the 
average number of sentences was 5.5 (st. dev. = 
2.1).  
The spoken responses were all provided with 
holistic English proficiency scores on a scale of 1 - 
4 by expert human raters in the context of opera-
tional, high-stakes scoring for the spoken language 
assessment. The scoring rubrics address the fol-
lowing three main aspects of speaking proficiency: 
delivery (pronunciation, fluency, prosody), lan-
guage use (grammar and lexical choice), and topic 
development (content and coherence). In order to 
ensure a sufficient quantity of responses from each 
proficiency level for training and evaluating the 
coherence prediction features, the spoken respons-
es selected for this study were balanced based on 
the human scores as follows: 25 responses were 
selected randomly from each of the 4 score points 
(1 - 4) for each of the 6 test questions. In some 
cases, more than one response was selected from a 
given test-taker; in total, 471 distinct test-takers are 
represented in the data set. 
2.2 Annotation and Analysis 
The coherence annotation guidelines used for the 
spoken responses in this study were modified 
based on the annotation guidelines developed for 
written essays described in Burstein et al (2010). 
According to these guidelines, expert annotators 
provided each response with a score on a scale of 1 
- 3. The three score points were defined as follows: 
3 = highly coherent (contains no instances of con-
fusing arguments or examples), 2 = somewhat co-
herent (contains some awkward points in which the 
speaker's line of argument is unclear), 1 = barely 
815
coherent (the entire response was confusing and 
hard to follow; it was intuitively incoherent as a 
whole and the annotators had difficulties in identi-
fying specific weak points). For responses receiv-
ing a coherence score of 2, the annotators were 
required to highlight the specific awkward points 
in the response. In addition, the annotators were 
specifically required to ignore disfluencies and 
grammatical errors as much as possible; thus, they 
were instructed to not label sentences or clauses as 
awkward points solely because of the presence of 
disfluent or ungrammatical speech.  
Two annotators (not drawn from the pool of ex-
pert human raters who provided the holistic scores) 
made independent coherence annotations for all 
600 spoken responses. The distribution of annota-
tions across the three score points is presented in 
Table 1. The two annotators achieved a moderate 
inter-annotator agreement (Landis and Koch, 1977) 
of ? = 0.68 on the 3-point scale. The average of the 
two coherence scores provided by the two annota-
tors correlates with the holistic speaking proficien-
cy scores at r = 0.66, indicating that the overall 
proficiency scores of spoken responses can benefit 
from the discourse coherence annotations. 
 
 1 2 3 
# 1 160 (27%) 278 (46%) 162 (27%) 
# 2 125 (21%) 251 (42%) 224 (37%) 
Table 1. Distribution of coherence annotations from two 
annotators 
 
Furthermore, coherence features based on the 
human annotations were examined within the con-
text of an automated spoken language assessment 
system, SpeechRaterSM (Zechner et al, 2007; 
2009). We extracted 96 features related to pronun-
ciation, prosody, fluency, language use, and con-
tent development using SpeechRater. These 
features were either extracted directly from the 
speech signal or were based on the output of an 
automatic speech recognition system (with a word 
error rate of around 28%1
                                                          
1 Both the training and evaluation sets used to develop the 
speech recognizer consist of similar spoken responses drawn 
from the same assessment. However, there is no response 
overlap between these sets and the corpus used for discourse 
coherence annotation in this study. 
). By utilizing a decision 
tree classifier (the J48 implementation from Weka 
(Hall et al, 2009)), 4-fold cross validation was 
conducted on the 600 responses to train and evalu-
ate a scoring model for predicting the holistic pro-
ficiency scores. The resulting correlation between 
the predicted scores (based on the 96 baseline 
SpeechRater features) and the human holistic pro-
ficiency scores was r = 0.667.  
In order to model a spoken response's coher-
ence, three different features were extracted from 
the human annotations. Firstly, the average of the 
two annotators? coherence scores was directly used 
as a feature with a 5-point scale (henceforth 
Coh_5). Secondly, following the work in Burstein 
et al (2010), we collapsed the average coherence 
scores into a 2-point scale to deal with the 
difficulty in distinguishing somewhat and highly 
coherent responses. For this second feature 
(henceforth Coh_2), scores 1 and 1.5 were mapped 
to score 1, and scores 2, 2.5, and 3 were mapped to 
score 2. Finally, the number of awkward points 
was also counted as a feature (henceforth Awk). 
As shown in Table 2, when these three coherence 
features were combined separately with the 
SpeechRater features, the correlations could be 
improved from r = 0.667 to r > 0.7. Meanwhile, 
the accuracy (i.e., the percentage of correctly pre-
dicted holistic scores) could be improved from 
0.487 to a range between 0.535 and 0.543.  
 
Features r Accuracy 
SpeechRater 0.667 0.487 
SpeechRater+Coh_5 0.714 0.540 
SpeechRater+Coh_2 0.705 0.543 
SpeechRater+Awk 0.702 0.535 
SpeechRater+Coh_5+Awk 0.703 0.537 
SpeechRater+Coh_2+Awk 0.701 0.542 
Table 2. Improvement to an automated speech scoring 
system after the addition of human-assigned coherence 
scores and measures, showing both Pearson r correla-
tions and the ratio of correctly matched holistic scores 
between the system and human experts 
 
These experimental results demonstrate that the 
automatic scoring system can benefit from coher-
ence modeling either by directly using a human-
assigned coherence score or the identified awk-
ward points. However, the use of both kinds of 
annotations does not provide further improvement. 
When collapsing the average scores into a 2-point 
scale, there was a 0.009 correlation drop (not sta-
tistically significant), but the accuracy was slightly 
improved. In addition, due to the relatively small 
816
size of the set of available coherence annotations, 
we adopted the collapsed 2-point scale instead of 
the 5-point scale for the coherence prediction ex-
periments in the next section.  
2.3 Experimental Design 
As demonstrated in Section 2.2, the collapsed av-
erage coherence score can be used to improve the 
performance of an automated speech scoring sys-
tem. Therefore, this study treats coherence predic-
tion as a binary classification task: low-coherent 
vs. high-coherent, where the low-coherent re-
sponses are those with average scores 1 and 1.5, 
and the high-coherent responses are those with av-
erage scores 2, 2.5, and 3.  
For coherence modeling, we again use the J48 
decision tree from the Weka machine learning 
toolkit (Hall et al, 2009) and run 4-fold cross-
validation on the 600 annotated responses. The 
correlation coefficient (r) and the weighted aver-
age F-Measure2
In this experiment, we examine the performance 
of the entity-grid features and a set of features pro-
duced by the e-rater? system (an automated writ-
ing assessment system for learner essays) (Attali 
and Burstein, 2006) to predict the coherence scores 
of the spontaneous spoken responses, where all the 
features are extracted from human transcriptions of 
the responses.  
 are used as evaluation metrics.  
2.4 Entity Grid and e-rater Features 
First, we applied the algorithm from Barzilay and 
Lapata (2008) to extract entity-grid features, which 
calculated the vector of entity transition probabili-
ties across adjacent sentences.  Several different 
methods of representing the entities can be used 
before generating the entity-grid. First, all the enti-
ties can be described by their syntactic roles in-
cluding S (Subject), O (Object), and X (Other). 
Alternatively, these roles can also be reduced to P 
(Present) or N (Absent). Furthermore, entities can 
be defined as salient, when they appear two or 
more times, otherwise as non-salient. In this study, 
                                                          
2 The data distribution in the experimental corpus is unbal-
anced:  71% of the responses are high-coherent and 29% are 
low-coherent. Therefore, we adopt the weighted average F-
Measure to evaluate the performance of coherence prediction: 
first, the F1-Measure of each category is calculated, and then 
the percentages of responses in each category are used as 
weights to obtain the final weighted average F-Measure. 
we generated there basic entity grids: EG_SOX 
(entity grid with the syntactic roles S, O, and X), 
EG_REDUCED (entity grid with the reduced rep-
resentations P and N), and EG_SALIENT (entity 
grid with salient and non-salient entities). In addi-
tion to these entity-grid features, we also used 130 
writing quality features related to grammar, usage, 
mechanics, and style from e-rater to model the co-
herence. 
A baseline system for this task would simply as-
sign the majority class (high-coherent) to all of the 
responses; this baseline achieves an F-Measure of 
0.587. Table 3 shows that the EG_REDUCED and 
e-rater features can obtain F-Measures of 0.677 
and 0.726 as well as correlations with human 
scores of 0.20 and 0.33, respectively. However, the 
combination of the two sets of features only brings 
a very small improvement (from 0.33 to 0.34). In 
addition, our experiments show that by introducing 
the component of co-reference resolution for entity 
grid building, we can only get a very slight im-
provement on EG_SALIENT, but no improvement 
on EG_SOX and EG_REDUCED. That may be 
because it is generally more difficult to parse the 
transcriptions of spoken language than well-
formed text, and more errors are introduced during 
the process of co-reference resolution. 
 
 r F-Measure 
Baseline 0.0 0.587 
EG_SOX 0.16 0.664 
EG_REDUCED 0.2 0.677 
EG_SALIENT 0.2 0.678 
e-rater 0.33 0.726 
EG_SOX +e-rater 0.30 0.714 
EG_REDUCED +e-rater 0.34 0.73 
EG_SALIENT + e-rater 0.26 0.695 
Table 3. Performance of entity grid and e-rater features 
on the coherence modeling task  
2.5 Discussion and Future Work  
In order to further analyze these features, the  cor-
relation coefficients between various features and 
the average coherence scores (on a five-point 
scale) were calculated; Figure 1 shows the histo-
gram of these correlation values. As the figure 
shows, there are a total of approximately 50 fea-
tures with correlations larger than 0.1. Four of the 
entity-grid features have correlations between 0.15 
and 0.29. As for the writing quality features, some 
817
of them show high correlations with the average 
coherence scores, despite the fact that they are not 
explicitly related to discourse coherence, such as 
the number of good lexical collocations.  
Based on the above analysis, we plan to investi-
gate additional superficial features explicitly relat-
ed to discourse coherence, such as the distribution 
of conjunctions, pronouns, and discourse connec-
tives. Moreover, based on the research on well-
formed texts and learner essays, we will attempt to 
examine more effective features and models to bet-
ter cover the discourse aspects of spontaneous 
speech. For example, local semantic features relat-
ed to inter-sentential coherence and the ISA feature 
will be investigated on spoken responses. In addi-
tion, we will apply the features and build coher-
ence models using the output of automatic speech 
recognition in addition to human transcriptions. 
Finally, various coherence features or models will 
be integrated into a practical automated scoring 
system, and further experiments will be performed 
to measure their effect on the performance of au-
tomated assessment of spontaneous spoken re-
sponses.  
 
 
Figure1. Histogram of entity-grid and writing quality 
features based on their correlations with coherence 
scores 
 
3 Conclusion  
In this paper, we present a corpus of coherence 
annotations for spontaneous spoken responses pro-
vided in the context of an English speaking profi-
ciency assessment. Entity-grid features and fea-
tures from an automated essay scoring system were 
examined for coherence modeling of spoken re-
sponses. The analysis on the annotated corpus 
showed promising results for improving the per-
formance of an automated scoring system by 
means of modeling the coherence of spoken re-
sponses.  
Acknowledgments 
The authors wish to express our thanks to the dis-
course annotators Melissa Lopez and Matt Mulhol-
land for their dedicated work and our colleagues 
Jill Burstein and Slava Andreyev for their support 
in generating entity-grid features. 
References   
Yigal Attali and Jill Burstein. 2006. Automated essay 
scoring with e-rater? V.2.0. Journal of Technology, 
Learning, and Assessment. 4(3): 159-174. 
Regina Barzilay and Lillian Lee. 2004. Catching the 
drift: Probabilistic content models, with applications 
to generation and summarization. Proceedings of 
NAACL-HLT, 113-120. 
Regina Barzilay and Mirella Lapata. 2005. Modeling 
local coherence: An entity-based approach. 
Proceedings of ACL, 141-148. 
Regina Barzilay and Mirella Lapata. 2008. Modeling 
local coherence: An entity-based approach. 
Computational Linguistics, 34(1):1-34. 
Jill Burstein, Joel Tetreault and Slava Andreyev. 2010. 
Using entity-based features to model coherence in 
student essays. Proceedings of NAACL-HLT, 681-
684. Los Angeles, California. 
Lei Chen, Klaus Zechner and Xiaoming Xi. 2009. 
Improved pronunciation features for construct-
driven assessment of non-native spontaneous 
speech. Proceedings of NAACL-HLT, 442-449. 
Miao Chen and Klaus Zechner. 2011. Computing and 
evaluating syntactic complexity features for 
automated scoring of spontaneous non-native 
speech. Proceedings of ACL, 722-731. 
Jian Cheng. 2011. Automatic assessment of prosody in 
high-stakes English tests. Proceedings of 
Interspeech , 27-31. 
Peter W. Foltz, Walter Kintsch and Thomas K. 
Landauer. 1998. The measurement of textual 
coherence with Latent Semantic Analysis. Discourse 
Processes, 25(2&3):285-307. 
Arthur C. Graesser,  Danielle S. McNamara,  Max M. 
Louwerse and Zhiqiang Cai. 2004. Coh-Metrix: 
Analysis of text on cohesion and language. Behavior 
818
Research Methods, Instruments, & Computers, 
36(2):193-202. 
Mark Hall, Eibe Frank, Geoffrey Holmes, Bernhard 
Pfahringer, Peter Reutemann and Ian H. Witten. 
2009. The WEKA data mining software: An update. 
SIGKDD Explorations, 11(1):10-18. 
Khairun-nisa Hassanali, Yang Liu and Thamar Solorio. 
2012. Coherence in child language narratives: A 
case study of annotation and automatic prediction of 
coherence. Proceedings of the Interspeech 
Workshop on Child, Computer and Interaction.  
Derrick Higgins, Jill Burstein, Daniel Marcu and 
Claudia Gentile. 2004. Evaluating multiple aspects 
of coherence in student essays. Proceedings of 
NAACL-HLT, 185-192. 
Derrick Higgins, Xiaoming Xi, Klaus Zechner and 
David Williamson.  2011. A three-stage approach to 
the automated scoring of spontaneous. Computer 
Speech and Language, 25:282-306. 
J. Richard Landis and Gary G. Koch. 1977. The 
measurement of observer agreement for categorical 
data. Biometrics, 33(1):159-174. 
Emily Pitler, Annie Louis and Ani Nenkova. 2010. 
Automatic evaluation of linguistic quality in multi-
document summarization. Proceedings of ACL. 
544?554. Uppsala. 
Helmer Strik and Catia Cucchiarini. 1999. Automatic 
assessment of second language learners' fluency. 
Proceedings of the 14th International Congress of 
Phonetic Sciences, 759-762. Berkeley, CA. 
Shasha Xie, Keelan Evanini and Klaus Zechner. 2012. 
Exploring content features for automated speech 
scoring. Proceedings of NAACL-HLT, 103-111. 
Helen Yannakoudakis and Ted Briscoe. 2012. Modeling 
coherence in ESOL learner texts. Proceedings of the 
7th Workshop on the Innovative Use of NLP for 
Building Educational Applications, 33-43. Montreal. 
Klaus Zechner, Derrick Higgins and Xiaoming Xi. 
2007.   SpeechRaterSM
Klaus Zechner, Derrick Higgins, Xiaoming Xi and 
David M. Williamson. 2009. Automatic scoring of 
non-native spontaneous speech in tests of spoken 
English. Speech Communication, 51(10):883-895. 
: A construct-driven approach 
to scoring spontaneous non-native speech. 
Proceedings of the International Speech 
Communication Association Special Interest Group 
on Speech and Language Technology in Education, 
128-131. 
 
819
Proceedings of the Eighth Workshop on Innovative Use of NLP for Building Educational Applications, pages 73?81,
Atlanta, Georgia, June 13 2013. c?2013 Association for Computational Linguistics
Automated Content Scoring of Spoken Responses in an Assessment for 
Teachers of English 
 
Klaus Zechner, Xinhao Wang  
Educational Testing Service 
660 Rosedale Road 
Princeton, NJ 08541, USA 
kzechner@ets.org, xwang002@ets.org 
 
 
 
 
Abstract 
This paper presents and evaluates approaches 
to automatically score the content correctness 
of spoken responses in a new language test for 
teachers of English as a foreign language who 
are non-native speakers of English. Most ex-
isting tests of English spoken proficiency elic-
it responses that are either very constrained 
(e.g., reading a passage aloud) or are of a pre-
dominantly spontaneous nature (e.g., stating 
an opinion on an issue). However, the assess-
ment discussed in this paper focuses on essen-
tial speaking skills that English teachers need 
in order to be effective communicators in their 
classrooms and elicits mostly responses that 
fall in between these extremes and are moder-
ately predictable. In order to automatically 
score the content accuracy of these spoken re-
sponses, we propose three categories of robust 
features, inspired from flexible text matching, 
n-grams, as well as string edit distance met-
rics. The experimental results indicate that 
even based on speech recognizer output, most 
of the feature correlations with human expert 
rater scores are in the range of r = 0.4 to r = 
0.5, and further, that a scoring model for pre-
dicting human rater proficiency scores that in-
cludes our content features can significantly 
outperform a baseline without these features 
(r = 0.56 vs. r = 0.33).  
1 Introduction 
With the increased need for instruction of interna-
tional learners of English as a foreign language 
(EFL), there is a concomitant rise in demand to 
assess the language competence of English teach-
ers who are non-native speakers of English. This 
situation arises because it is neither possible nor 
affordable for countries where English is not spo-
ken as a native language to employ only or even 
mostly native speakers of English as EFL teachers. 
Moreover, as the language of instruction increas-
ingly becomes English in most classrooms, teach-
ers? competence in the productive language 
modality of speaking becomes substantially more 
important than in the past. In order to meet this 
demand for assessing the English language profi-
ciency of teachers of English, a new test, English 
Teachers Language Assessment (ETLA), was de-
veloped recently and piloted in 2012. The test 
comprises items for all four main language modali-
ties: reading, listening, writing and speaking. 
While reading and listening items use a multi-
ple-choice paradigm, test items for speaking and 
writing elicit open responses. For cost and effi-
ciency reasons, we aim to employ automated scor-
ing of written and spoken responses in this test. 
This paper is concerned in particular with the con-
ceptualization, implementation and evaluation of 
features that can assess one aspect of English 
speaking proficiency: the content correctness of a 
test taker?s response. Our automated speech scor-
ing system, SpeechRaterSM
The speaking items in ETLA range in complexi-
ty from reading a text passage aloud to more chal-
lenging tasks requiring multi-sentence responses 
related to typical teaching situations. The items, 
therefore, elicit speech in which predictability 
ranges from high (e.g., reading aloud) to medium 
(e.g., open responses based on teaching material). 
 (Zechner et al, 2009), 
also has features addressing other aspects of speak-
ing proficiency, such as fluency or pronunciation, 
but the details of these features will not be dis-
cussed as part of this paper. 
73
While approaches to capture the content of mostly 
predictable speech have been widely used in the 
past (see, e.g., Alwan et al, 2007; Franco et al, 
2010), this is not the case for responses that exhibit 
considerable variation but are still much shorter 
and more constrained than spontaneous items from 
other language tests, such as TOEFL iBT?
Therefore, the goal of the study reported in this 
paper is to conceptualize, implement and evaluate 
features that can address the subset of ETLA 
speaking items where responses are not strongly 
predictable but are still fairly short and constrained 
by the context of the item stimulus and prompt.
. 
1
To illustrate what an ETLA speaking item may 
look like, we provide a relatively simple example 
here. Suppose the test taker (i.e., an English lan-
guage teacher) is asked to request that the class 
open their textbooks on page 55. We could see a 
range of responses, from ?perfect? (score level 3, 
e.g., ?Please open your textbooks on page 55.? or 
?Please open your textbooks and turn to page 
55.?), to ?good? (score level 2, e.g., ?Please open 
the books on the page 55.?) and to ?poor? (score 
level 1, e.g., ?Open book page 55.?). Again, note 
that for this paper we are not interested in potential 
issues with fluency, such as long pauses or speak-
ing rate, nor with pronunciation or prosody. We 
just look at the content of the test takers? respons-
es, either in idealized form by means of a human 
transcription of what a test taker actually said, or in 
a realistic operational scenario, where we look at 
the output of an ASR system. In both cases, we 
consider the sequence of words only (i.e., a textual 
representation of the test takers? spoken respons-
es). 
 
One important aspect of any features used for con-
tent scoring is that they have to be robust with re-
spect to speech recognition errors. Robustness is 
necessary because we are using an automatic 
speech recognition (ASR) system as a front end, 
and the average word error rate of the system is 
around 27% for moderately predictable item re-
sponses. 
In order to investigate the effectiveness of can-
didate content features in a short-term development 
cycle before a larger amount of pilot data would be 
available, we first conducted a small scale in-house 
                                                          
1 A test item is a basic element of a test, consisting of stimulus 
material, such as text and/or visuals, and a prompt (test ques-
tion) that elicits a response from the test taker. 
data collection effort focusing on the moderately 
predictable spoken items in ETLA. Based on the 
analysis of this mini-corpus, several different cate-
gories of promising features were selected for po-
tential operational use and then evaluated on the 
pilot data. 
The paper is organized as follows: Section 2 
provides an overview on related work; Section 3 
describes the in-house data set, the pilot data and 
the ASR system; the developed features are pre-
sented in Section 4; Section 5 presents our experi-
ments; we then discuss our findings in Section 6 
and we conclude the paper in Section 7. 
2 Related Work  
Related to the automated assessment of writing 
free-text, research to date has concentrated mainly 
on two tasks: (1) scoring of short answers (Mitch-
ell et al, 2002; Leacock and Chodorow, 2003; 
Mohler and Mihalcea, 2009) and (2) scoring of 
essays (Foltz et al, 1999; Kanejiya et al, 2003; 
Attali and Burstein, 2006). For example, Leacock 
and Chodorow (2003) built an automated scoring 
system, c-rater?, to evaluate the short constructed 
or free-text responses, where the concepts given in 
test items were modeled, and the presence of these 
expected concepts in students? answers would be 
detected.  
As for the evaluation of free-text essays, Attali 
and Burstein (2006) used a selected set of mean-
ingful features to measure different constructed 
aspects of writing essays, such as grammar, usage, 
mechanics, style, organization, development, lexi-
cal complexity and prompt-specific vocabulary 
usage. In addition, the Intelligent Essay Assessor 
(Foltz et al, 1999) used Latent Semantic Analysis 
(LSA) to score students? answers by comparing 
them to domain-representative texts. Since LSA is 
based on the bag-of-words model, researchers have 
also tried to expand it by introducing additional 
information, such as part-of-speech (POS) tags 
(Kanejiya et al, 2003).  
In addition, research efforts have also been 
made to evaluate the content relatedness and cor-
rectness for spoken responses. For example, Xie et 
al. (2012) used LSA and Pairwise Mutual Infor-
mation approaches to evaluate the content correct-
ness of unrestricted spontaneous spoken responses. 
Moreover, Chen and Zechner (2011) explored fea-
74
tures related to grammatical complexity in an au-
tomated speech scoring system.  
In order to address the moderately predictable 
speaking test items in the new ETLA, this paper 
presents several different types of features to score 
the content correctness of the elicited spoken re-
sponses. Following a series of experiments and 
comparisons, seven features from three content 
feature categories are selected and evaluated. 
3 Data Sets and ASR System 
This study conducts experiments and evaluations 
based on two different data sets: (1) a small scale 
in-house data collection effort, which was used for 
the design and development of content features; 
and (2) a larger-scale pilot data collection, which 
was used to further evaluate the features selected 
according to the in-house data and to build scoring 
models for the prediction of human proficiency 
scores. 
3.1 In-house Data Collection 
Twenty-two items from ETLA with moderately 
predictable responses were selected for the in-
house data collection.2
                                                          
2 We decided to focus our efforts only on the moderately pre-
dictable items since scoring of highly predictable item types 
has been extensively studied in previous research already. 
 Firstly, 1,053 text responses 
in total for all three score levels (3 = high profi-
ciency, 2 = medium proficiency, 1 = low profi-
ciency) were drafted and collected by human 
experts. In order to simulate the operational scenar-
io with an ASR system in place, a subset of re-
sponses was recorded by a small set of 
predominantly non-native speakers of English. For 
each test item, four responses were randomly se-
lected from each score level, which resulted in 22 
? 3 ? 4 = 264 responses for voice recording. The 
remainder of 789 text responses comprised the set 
for feature development and training. In addition, 
about two thirds of the 264 text responses were 
randomly double-recorded by a second speaker, 
resulting in a speech corpus with 444 spoken re-
sponses in total, used as the evaluation set. Fur-
thermore, all these spoken responses were 
manually transcribed to accommodate the errors 
introduced by reading, such as insertions of various 
speech disfluencies.  
3.2 Pilot Data Collection 
This study uses data from a 2012 pilot administra-
tion of the ETLA assessment. In particular, we fo-
cus on 14 moderately predictable items from the 
pilot, covering 2,308 test takers. In order to build 
the automatic speech recognizer and the scoring 
models, the pilot data were partitioned into five 
different subsets without any speaker and response 
overlaps. The first three data partitions were used 
for training, development and evaluation of the 
speech recognition system (hereafter, ?asrTrain?, 
?asrDev? and ?asrEval?), which included spoken 
responses from both the moderately and highly 
predictable items. The asrTrain partition was fur-
ther used to develop and train the content features 
described below. The remaining two partitions 
were used for training and evaluation of scoring 
models that predicted item scores based on a set of 
features (hereafter, ?smTrain? and ?smEval?), 
where only the spoken responses from 14 moder-
ately predictable items from one pilot form were 
included.   
The detailed partition information is listed in 
Table 1. All these spoken responses have been 
manually transcribed and scored with holistic 
scores from 1 to 3 by trained human expert raters. 
For the smTrain and smEval partitions, there were 
6,367 responses receiving double annotation, and 
the inter-rater correlation was 0.73. Furthermore, 
the average length of responses from smTrain and 
smEval sets was 10.5 words, and the correspond-
ing vocabulary size was 855 (not including partial 
words).  
 
Partitions # Speakers # Responses 
asrTrain 1,658 27,604 
asrDev 25  700 
asrEval 25  700 
smTrain 300  3,452 
smEval 300  3,466 
Table 1. Number of speakers and number of responses 
included within each data partition. 
3.3 System Architecture 
Our automated speech scoring system, 
SpeechRater (Zechner et al, 2009), consists of an 
ASR system described below which generates a 
word hypothesis for every response by a test taker, 
including information about timing, energy and 
pitch, and other information from the input audio 
75
file. Next, the feature computation modules take 
the outputs of the ASR system and compute a set 
of features, related to fluency, pronunciation, pros-
ody, as well as content, the focus of this paper. Fi-
nally, a scoring model (linear regression model) is 
trained based on the smTrain set to predict scores 
and then evaluated on unseen data (smEval set). 
3.4 ASR System 
In this study, a state-of-the-art gender-independent 
Hidden Markov Model speech recognition system 
trained on about 800 hours of non-native speech is 
taken as the baseline recognizer, and its language 
model (LM) is then further adapted using the tran-
scriptions from the asrTrain data partition. The 
language model adaptation weights are tuned on 
the asrDev set, and the resulting word error rate 
(WER) on the asrEval set (with both moderately 
and highly predictable responses) is 11.7%, and its 
WER on the subset of 264 moderately predictable 
responses is 19.7%. This speech recognizer is fur-
ther evaluated on both smTrain and smEval sets as 
shown in Table 2, only including moderately pre-
dictable responses.  
 
Partition WER (%) 
smTrain 26.7 
smEval 26.9 
Table 2. Word error rates (WER) of the speech recog-
nizer on smTrain and smEval3
4 Content Features 
 data sets.  
Following a careful inspection and analysis of the 
collected in-house data (described in Section 3.1 
above), several different categories of content fea-
tures were designed and developed. The initial data 
analysis showed that features need to be able to 
capture very narrow ranges of expressions with 
minor variations, but also should be able to capture 
something like the ?overall accuracy? of expres-
sion, where local word sequences or phrases 
should conform to the expectations of the item de-
sign without requiring that a response follows a 
confined pattern in its entirety. For the former situ-
ation, features like regular expression matches 
                                                          
3 The calculation of WER is based on only the recognized 
outputs with more than one word. Thus, the number of actual-
ly recognized responses is less than that in Table 1, i.e., 3,264 
responses for smTrain and 3,255 responses for smEval. 
 
seem appropriate to be a good match, whereas for 
the latter, more flexible approaches such as n-gram 
models or string edit distance metrics may be more 
appropriate. We list and describe our proposed 
content features in the following section. 
A. Flexible String Matching Metrics 
AI. Regular Expressions 
Since many responses in ETLA are expected to 
follow certain patterns, it is intuitive to construct 
limited regular expressions (RegEx) to match gold 
standard responses for candidates with high profi-
ciency score levels. Accordingly, one type of regu-
lar expression related features, re_match, can be 
extracted to detect whether the test response can be 
matched by any of the pre-built regular expres-
sions. This feature can obtain the values of 0 (does 
not match), 1 (partially matches) and 2 (exactly 
matches). Here, a partial match indicates that a 
RegEx can be matched within a test response that 
also has other spoken material, which is useful 
when the speaker repeats or corrects the answer 
multiple times in a single item response, and the 
compiled RegEx can still be used to match parts of 
the test response. 
This content feature has the advantage of high 
precision, as it can precisely examine the content 
correctness of the test responses. Thus, the RegEx 
should be compiled to match all the example re-
sponses at the highest score level 3 from the train-
ing set. For some test items with relatively short 
and fixed answer patterns, this feature is quite use-
ful; however, it is very time-consuming and diffi-
cult to manually build regular expressions for 
items with longer and more flexible expressions. 
Meanwhile, the mechanism of exact matching can 
make this feature fail in very small variations of 
expression. Especially when applying this feature 
on ASR output, it is difficult to successfully match 
some content-correct responses that have 
disfluencies or recognition errors. 
Therefore, in order to improve the robustness of 
RegEx, another regular expression related feature 
is proposed. In general, for each item in ETLA, 
some pieces of specific expressions are required in 
a test response to represent its content correctness. 
Accordingly, we can segment the reference re-
sponses into several fragments and identify some 
pieces as key fragments. For example, when look-
ing at the reference response ?Please open your 
76
text books and turn to page 55.? two key fragments 
can be extracted with ?Please open your text 
books? and ?turn to page 55.? We group versions 
of these key fragments from the training corpus 
together and construct regular expressions to match 
each group. Afterwards, a feature can be defined to 
count how many key fragments can be matched by 
a test response, namely num_fragments.  
 
AII. Keyword Detection 
For moderately predictable items on ETLA, key-
word lists can be extracted from the stimulus mate-
rial and the item prompt, containing the words that 
need to be included in a test response by test tak-
ers. Then a feature, num_keywords, can be used to 
examine how many keywords appear in a test re-
sponse, which can be further normalized by the 
number of predefined keywords for each item, i.e., 
percent_keywords. In addition, as some keywords 
may be a phrase with multiple words, such as 
?page 55,? we can split all the keywords into sin-
gle words and get another sub-keywords list. Then 
two corresponding features can be extracted as 
num_sub_keywords and percent_sub_keywords. 
B. N-grams 
BI. Word N-grams 
The word n-gram model is introduced here to cap-
ture the similarity of word usage between the test 
and the reference responses. Based on the collected 
training samples, trigrams are trained using the text 
responses from the highest score level 3. Then, the 
LM can be used to score a test response, and the 
resulting probability can be taken as feature, called 
lm_3.  
 
BII. POS Similarity 
This feature measures the syntactic complexity of 
test responses based on the distribution of POS 
tags. First, all the responses from the training data 
set are assigned with POS tag sequences via an 
automatic POS tagger. Then, a POS vector accord-
ing to each score level can be obtained by gather-
ing the POS unigram, bigram or trigram statistics 
from the same score level. 
Given a test response, its corresponding POS 
sequence can be determined by the same POS tag-
ger, and the cosine similarities between the test 
POS n-gram vector and the POS vectors from three 
different score levels can be calculated as pos_1, 
pos_2 and pos_3, where pos_3 is used as a feature 
in our experiments below. Furthermore, by com-
paring these three cosine similarities, the score cat-
egory with the highest similarity can be extracted 
as another feature, i.e., pos_score. 
 
BIII. Machine Translation Evaluation Metric 
(BLEU) 
BLEU (Papineni et al, 2002) is one of the most 
popular metrics for automatic evaluation of ma-
chine translation, where the score is calculated 
based on the modified n-gram precision. In this 
study, the BLEU score is introduced to evaluate 
the content quality of a test response, where three 
different gold standard reference corpora are ex-
tracted from the training set according to each 
score level. Similar to the edit distance and WER 
features described below, three BLEU scores are 
calculated by comparing them with reference re-
sponses from each score level (i.e., bleu_1, bleu_2 
and bleu_3). We decide to use the following two 
features for our experiments below: bleu_3 and 
bleu_score, the score level which receives the 
maximum BLEU score.  
C. String Edit Distance Metrics 
CI. String Edit Distance 
As the edit distance is an effective string metric for 
measuring the amount of difference between two 
word sequences, including insertions, deletions and 
substitutions, we use it to capture the sequence dis-
tance between the test and reference responses.  
Given a test response, we can separately calcu-
late the edit distance by comparing it with training 
responses from each score level. Afterwards, the 
minimum edit distance from each score level can 
be extracted as ed_1, ed_2 and ed_3, where ed_3 is 
selected as feature for our experiments. Further-
more, by comparing these three edit distances, the 
score category with the minimum value is taken as 
another feature, ed_score.  
 
CII. Word Error Rate (WER) 
By dividing the edit distance by the length of the 
reference response, we obtain the word error rate 
(WER) metrics, commonly used in speech recogni-
tion, and two additional features, wer_3 and 
wer_score, similarly as above, can be calculated.  
Compared to the above category of n-gram re-
lated features, which capture the n-gram fragment 
77
matching between the test and reference samples, 
the category of edit distance features try to find the 
most similar reference sample to the test sample at 
the whole-response level.  
Finally, all the proposed features are implement-
ed and then examined based on both the ideal hu-
man transcription and the realistic ASR output. 
The speech recognizer used with the small in-
house data is the same as the ASR system de-
scribed in Section 3.4, but its language model is 
adapted with the much smaller set of 789 training 
text responses. The WER of this system is 17.8%, 
evaluated on 444 spoken responses. 
In addition, in order to increase the robustness of 
the extracted features, a preprocessing stage is in-
troduced to remove all the disfluencies from the 
ASR output, such as filler words, recognized par-
tial words and repeated words. Afterwards, each 
feature is evaluated on both the transcription and 
the ASR output of the 444 collected spoken re-
sponses, and its corresponding Pearson correlation 
coefficient with human scores is presented in Table 
3.  
Based on overall correlation, inter-correlation 
analyses, as well as on construct4
5 Experiments and Results 
 considerations, 
seven content features from three categories are 
selected and will be evaluated on a larger scale on 
ETLA pilot data in the next section: re_match 
(A1), num_fragments (A2), percent_sub_keywords 
(A3), bleu_3 (B1), ed_score (C1), wer_3 (C2) and 
wer_score (C3). 
This section first describes experiments related to 
the performance of the seven selected content fea-
tures on a larger corpus from an ETLA pilot ad-
ministration (described above in Section 3.2). 
Then, a similar analysis is conducted based on hu-
man rater analytic content scores on a subset of 
this data. Finally, the selected content features are 
combined with other features related to pronuncia-
tion, prosody and fluency to build a scoring model 
for the prediction of human scores. 
 
 
                                                          
4 A construct is the set of knowledge, skills and abilities 
measured by a test. The term ?construct considerations? in the 
context of feature selection refers to the process of ensuring 
that the selected feature set obtains a high coverage of all as-
pects of the relevant construct. 
 Feature Trans ASR  
A 
re_match 0.789 0.537 
num_fragments 0.629 0.523 
num_keywords 0.269 0.254 
percent_keywords 0.419 0.375 
num_sub_keywords 0.249 0.239 
percent_sub_keywords 0.482 0.417 
B 
lm_3 0.482 0.461 
pos_3 0.270 0.270 
pos_score 0.315 0.339 
bleu_3 0.531 0.458 
bleu_score 0.144 0.194 
C 
ed_3 -0.362 -0.337 
ed_score 0.642 0.614 
wer_3 -0.573 -0.513 
wer_score 0.585 0.557 
Table 3. Pearson correlation coefficients (r) of content 
features with human holistic scores. 
5.1 Feature Evaluation on Pilot Data 
In the following experiments, we use the asrTrain 
set to train the content features. Then these features 
are examined on the smTrain and smEval data sets. 
In order to extract the edit distance, WER- and 
BLEU-related features for each item, three text 
reference corpora according to different score lev-
els, are needed. Duplicate reference responses with 
the same content are removed within each score 
level.  
Furthermore, we improve two RegEx features 
using the reference responses from the highest 
score level 3 in the asrTrain set. (1) Since the pre-
viously obtained re_match feature based on the in-
house data may not be able to match multiple con-
tent-correct responses in the pilot data, we need to 
augment the set of RegEx for this feature based on 
correct responses from score level 3 in the asrTrain 
set. (2) Since the maximum number of candidate 
fragments varies across different ETLA items, the 
num_fragments feature values are not comparable 
across items. Therefore, we redesign this feature 
by assigning a list of manually selected keywords 
for each fragment. During feature extraction, we 
count the number of distinct keywords associated 
with all the matched fragments and divide this 
number by the number of predefined keywords for 
each item (as in AII. Keyword Detection), which 
results in another feature: perc_fragment_kw (A2).  
Based on the ASR output of smTrain and 
smEval data sets, seven content features are ex-
tracted and their Pearson correlation coefficients 
with the holistic human scores are calculated and 
shown in Table 4. 
78
 Feature 
smTrain (r) smEval (r) 
Trans ASR Trans ASR 
A1 0.53 0.415 0.534 0.441 
A2 0.576 0.458 0.583 0.48 
A3 0.42 0.286 0.419 0.297 
B1 0.597 0.478 0.564 0.452 
C1 0.535 0.412 0.52 0.39 
C2 -0.588 -0.469 -0.564 -0.446 
C3 0.554 0.433 0.51 0.428 
Table 4. Pearson correlation coefficients between con-
tent features and human holistic scores, based on both 
the transcription and the ASR output of smTrain and 
smEval.5
5.2 Evaluations Using Human Rater Analyt-
ic Content Scores 
 Features include A1 (re_match), A2 
(perc_fragment_kw), A3 (percent_sub_keywords), B1 
(bleu_3), C1 (ed_score), C2 (wer_3) and C3 
(wer_score) 
In addition to the human rating of all spoken re-
sponses of the ETLA pilot data set with holistic 
scores that take into account both the dimensions 
of ?delivery? (fluency, pronunciation, prosody) 
and ?content,? a subset of the data was further 
scored by human expert raters in these two dimen-
sions separately, resulting in so-called analytic 
scores for delivery and content. The inter-
correlation for content analytic scores was 0.79. 
1,410 responses from the smTrain set and 1,402 
responses from the smEval set received such ana-
lytic content scores. On this subset, table 5 shows 
the Pearson correlation coefficients between the 
content features and the analytic content scores, as 
well as the holistic scores, for comparison. 
5.3 Scoring Model Comparison 
We further examine these content features by in-
troducing them in a scoring model to predict hu-
man rater holistic proficiency scores, using 
smTrain for training of the models and smEval for 
their evaluation. The baseline system employs 14 
features related to the construct dimension of de-
livery, such as pronunciation, prosody and fluency.  
                                                          
5 The evaluation is conducted on recognition output with more 
than one word. In addition, due to technical problems, such as 
high background noise, some responses are non-scorable for 
human raters, and these responses are removed from the eval-
uation sets. Finally, there are 3176 responses included in 
smTrain, and 3084 responses in smEval.  
 
Feature 
smTrain (r) 
Holistic Content 
Trans ASR Trans ASR 
A1 0.529 0.415 0.563 0.434 
A2 0.564 0.46 0.646 0.525 
A3 0.422 0.283 0.452 0.277 
B1 0.6 0.499 0.654 0.504 
C1 0.527 0.43 0.555 0.46 
C2 -0.588 -0.473 -0.627 -0.488 
C3 0.542 0.434 0.563 0.462 
Feature 
smEval (r) 
Holistic Content 
Trans ASR Trans ASR 
A1 0.525 0.424 0.538 0.436 
A2 0.579 0.472 0.621 0.512 
A3 0.423 0.308 0.454 0.321 
B1 0.563 0.442 0.606 0.471 
C1 0.521 0.4 0.539 0.422 
C2 -0.543 -0.42 -0.584 -0.457 
C3 0.514 0.417 0.529 0.439 
Table 5. Pearson correlation coefficients between con-
tent features and human analytic content scores as well 
as human holistic scores.  
 
Furthermore, an extended scoring model is built by 
adding the selected seven content features to the 
model. Table 6 provides the comparison between 
these two scoring models, reporting both quadratic 
weighted kappa and Pearson correlation coeffi-
cients between automatically predicted scores and 
human holistic scores on the smEval data set. 
 
Scoring Model Kappa r 
Baseline (Delivery only) 0.30 0.33 
Extended (Delivery+Content) 0.53 0.56 
Table 6. Scoring model comparison: quadratic weighted 
kappa and Pearson correlation coefficients between pre-
dicted scores (unrounded) and human holistic scores.  
6 Discussion 
The goal of this paper was to conceptualize, im-
plement and evaluate features that can determine 
the content correctness of spoken item responses in 
an English language test for teachers of English 
who are not native speakers of English. 
Based on observations from a small in-house da-
ta collection, where human test developers and 
content experts created example responses to 22 
test items for three different score levels, we de-
cided to implement a range of features that can 
capture the content correctness of test takers? re-
sponses in varying degree of precision. Our fea-
79
tures belong to three classes: features related to 
fixed expressions, with potential small variations, 
such as regular expressions or keywords; features 
based on n-grams of words or POS tags, including 
the BLEU metrics frequently used for evaluations 
of machine translation output; and features related 
to measures of string edit distance, including the 
WER metrics commonly used in speech recogni-
tion evaluations.  
It should be noted that we use the term ?content? 
in a fairly broad way in this paper, namely, every-
thing in a spoken response that is not related to 
lower-level aspects of speech production such as 
fluency or pronunciation. Since the scoring rubrics 
for ETLA place a high emphasis both on the 
grammatical accuracy, as well as on the correct 
content (in a more narrow sense), this situation is 
reflected by our choice of features that focus both 
on elements traditionally associated with content 
(such as matching of keywords), as well as on ele-
ments more related to correct grammatical expres-
sions (e.g., sequences of POS tags). 
Our initial evaluations on the small in-house da-
ta collection showed that most of these features 
correlate well with human expert scores, both 
when using transcribed speech as well as when 
using ASR output. The absolute correlations for 
human transcriptions of speech range from r = 
0.144 (bleu_score) to r = 0.789 (re_match), and for 
ASR output from r = 0.194 (bleu_score) to r = 
0.614 (ed_score). The relative drop in correlation 
between these two conditions varies across fea-
tures, but is generally around 5%-15%, with 
re_match having a much larger performance drop 
from r = 0.789 for transcribed speech to r = 0.537 
for ASR output (32% relative decrease in perfor-
mance). 6
From this initial set of 15 features, we selected 
seven features based on feature performance, inter-
correlation analyses (i.e., avoiding features that 
have a high inter-correlation and measure a similar 
aspect of content), and considerations of construct, 
i.e., which features are representing content in a 
way that is consistent with what human experts 
would consider important in determining the con-
tent correctness of a response. This subset of seven 
 
                                                          
6 The correlation of one feature, pos_3, remained unchanged 
between the two conditions, and two features, pos_score and 
bleu_score, showed higher correlations for ASR output than 
for human transcriptions. 
features includes three features each from the clas-
ses of flexible string matching and string edit dis-
tance, and one feature (bleu_3) from the n-gram 
class. 
When evaluating these seven features on a larger 
data set, the smTrain and smEval sets of the 2012 
ETLA pilot data, we find absolute correlations be-
tween features and human holistic scores ranging 
from r = 0.286 to r = 0.480 for ASR output, and 
from r = 0.419 to r = 0.597 for transcriptions. The 
relative decrease in correlation between transcrip-
tions and ASR outputs ranges from 16% to 32% in 
these data sets (smTrain and smEval). The magni-
tude of content feature correlations observed in this 
study is similar to that of features related to fluen-
cy and pronunciation computed on spontaneous 
speech, as reported in Zechner et al (2009). In 
fact, due to the brevity of the moderately predicta-
ble responses in ETLA, features related to fluency 
and pronunciation achieve correlations of less than 
0.3 on this data set, making content features crucial 
for the assessment of speech here. 
When comparing the six content features that 
are identical between the original feature set of 15 
features (in-house data collection) and the final 
feature set, we observe a relative drop in feature 
correlation between the in-house data set and the 
smEval pilot data set between 1% (blue_3) and 
36% (ed_score), with an average decrease of 20%. 
This performance decrease can be explained by (1) 
the more challenging data set of the pilot, as indi-
cated, e.g., by a much higher word error rate of the 
ASR system (27% vs. 18%); and (2) the fact that 
the in-house data collection was much more con-
strained in terms of test taker response variation 
compared to the real-world pilot data. 
Since a subset of the ETLA responses was also 
scored analytically by human raters, we could fur-
ther compare the feature correlations between ho-
listic vs. analytic content scores (Section 5.2). We 
find that on smEval, for all features, absolute cor-
relations increase on human analytic content scores 
compared to human holistic scores. Although these 
differences are rather small (0.01 to 0.04), this is 
an indicator that our features are measuring what 
they are supposed to measure, since the holistic 
scores also take other dimensions of speech, such 
as fluency and pronunciation, into account. 
 
 
80
7 Conclusion and Future Work 
This paper presented a study whose aim was to 
conceptualize, implement and evaluate features to 
measure the content correctness of test takers? re-
sponses in a new assessment for EFL teachers 
whose native language is not English. 
We implemented and evaluated an initial set of 
15 content features from three feature classes: flex-
ible string matching, n-grams and string edit dis-
tance metrics. A subset of these features was then 
evaluated on a 2012 ETLA pilot administration, 
and we found correlations between features and 
human holistic scores in the range of r = 0.29 to r 
= 0.48 on ASR output. Correlations increased 
when comparing features with human analytic con-
tent scores. 
Finally, we compared a baseline regression scor-
ing model for prediction of human holistic scores 
without any content features to an extended model 
using seven content features and found that the 
model correlation substantially improved from r = 
0.33 (baseline) to r = 0.56 (extended model). 
Future work will include devising strategies on 
how to obtain RegEx features more quickly in a 
semi-automated way in order to reduce human la-
bor. Further, we plan more in-depth analysis of the 
feature performance across different test items and 
item types which potentially could lead to further 
improvements and refinements of our content fea-
tures. 
References  
Abeer Alwan, Yijian Bai, Matt Black, Larry Casey, 
Matteo Gerosa, Margaret Heritage, Markus Iseli, 
Barbara Jones, Abe Kazemzadeh, Sungbok Lee, 
Shrikanth Narayanan, Patti Price, Joseph Tepperman 
and Shizhen Wang. 2007. A system for technology 
based assessment of language and literacy in young 
children: the role of multiple information sources. 
Proceedings of IEEE International Workshop on 
Multimedia Signal Processing, 26-30. 
Yigal Attali and Jill Burstein. 2006. Automated essay 
scoring with e-rater? V.2.0. Journal of Technology, 
Learning, and Assessment, 4(3): 159-174. 
Miao Chen and Klaus Zechner. 2011. Computing and 
evaluating syntactic complexity features for 
automated scoring of spontaneous non-native 
speech. Proceedings of ACL, 722-731. 
Peter W. Foltz, Darrell Laham and Thomas K. 
Landauer. 1999. The intelligent essay assessor: 
Applications to educational technology. Interactive 
Multimedia Electronic Journal of Computer-
Enhanced Learning, 1(2). 
Horacio Franco, Harry Bratt, Romain Rossier, Venkata 
Rao Gadde, Elizabeth Shriberg, Victor Abrash and 
Kristin Precoda. 2010. EduSpeak?: A speech 
recognition and pronunciation scoring toolkit for 
computer-aided language learning applications. 
Language Testing, 27(3): 401-418. 
Dharmendra Kanejiya, Arun Kumary and Surendra 
Prasad. 2003. Automatic evaluation of students' 
answers using syntactically enhanced LSA. 
Proceedings of Workshop on Building Educational 
Applications Using Natural Language Processing, 
53-60. 
Claudia Leacock and Martin Chodorow. 2003. C-rater: 
Automated scoring of short-answer questions. 
Computers and Humanities,37: 389?405. 
Tom Mitchell, Terry Russell, Peter Broomhead and 
Nicola Aldridge. 2002. Towards robust 
computerised marking of free-text responses. 
Proceedings of International Computer Assisted 
Assessment Conference, 233-249. 
Michael Mohler and Rada Mihalcea. 2009. Text-to-text 
semantic similarity for automatic short answer 
grading. Proceedings of EACL, 567-575. 
Kishore Papineni, Salim Roukos, Todd Ward and Wei-
Jing Zhu. 2002. BLEU: A method for automatic 
evaluation of machine translation. Proceedings of 
ACL, 311-318. 
Shasha Xie, Keelan Evanini and Klaus Zechner. 2012. 
Exploring content features for automated speech 
scoring. Proceedings of NAACL-HLT, 103-111. 
Klaus Zechner, Derrick Higgins, Xiaoming Xi and 
David M. Williamson. 2009. Automatic scoring of 
non-mative spontaneous speech in tests of spoken 
English. Speech Communication, 51: 883-895. 
 
81
Proceedings of the Ninth Workshop on Innovative Use of NLP for Building Educational Applications , pages 22?27,
Baltimore, Maryland USA, June 26, 2014.
c?2014 Association for Computational Linguistics
Automatic detection of plagiarized spoken responses
Keelan Evanini and Xinhao Wang
Educational Testing Service
660 Rosedale Road, Princeton, NJ, USA
{kevanini,xwang002}@ets.org
Abstract
This paper addresses the task of auto-
matically detecting plagiarized responses
in the context of a test of spoken En-
glish proficiency for non-native speakers.
A corpus of spoken responses containing
plagiarized content was collected from a
high-stakes assessment of English profi-
ciency for non-native speakers, and sev-
eral text-to-text similarity metrics were
implemented to compare these responses
to a set of materials that were identified
as likely sources for the plagiarized con-
tent. Finally, a classifier was trained using
these similarity metrics to predict whether
a given spoken response is plagiarized or
not. The classifier was evaluated on a
data set containing the responses with pla-
giarized content and non-plagiarized con-
trol responses and achieved accuracies of
92.0% using transcriptions and 87.1% us-
ing ASR output (with a baseline accuracy
of 50.0%).
1 Introduction
The automated detection of plagiarism has been
widely studied in the domain of written student
essays, and several online services exist for this
purpose.
1
In addition, there has been a series of
shared tasks using common data sets of written
language to compare the performance of a vari-
ety of approaches to plagiarism detection (Potthast
et al., 2013). In contrast, the automated detection
of plagiarized spoken responses has received little
attention from both the NLP and assessment com-
munities, mostly due to the limited application of
1
For example, http://turnitin.com/en_
us/features/originalitycheck, http:
//www.grammarly.com/plagiarism-checker/,
and http://www.paperrater.com/plagiarism_
checker.
automated speech scoring for the types of spo-
ken responses that could be affected by plagiarism.
Due to a variety of factors, though, this is likely to
change in the near future, and the automated detec-
tion of plagiarism in spoken language will become
an increasingly important application.
First of all, English continues its spread as the
global language of education and commerce, and
there is a need to assess the communicative com-
pentance of high volumes of highly proficient non-
native speakers. In order to provide a valid evalua-
tion of the complex linguistic skills that are nec-
essary for these speakers, the assessment must
contain test items that elicit spontaneous speech,
such as the Independent and Integrated Speaking
items in the TOEFL iBT test (ETS, 2012), the
Retell Lecture item in the Pearson Test of English
Academic (Longman, 2010), and the oral inter-
view in the IELTS Academic assessment (Cullen
et al., 2014). However, with the increased em-
phasis on complex linguistic skills in assessments
of non-native speech, there is an increased chance
that test takers will prepare canned answers using
test preparation materials prior to the examination.
Therefore, research should also be conducted on
detecting spoken plagiarized responses in order to
prevent this type of cheating strategy.
In addition, there will also likely be an increase
in spoken language assessments for native speak-
ers in the K-12 domain in the near future. Curricu-
lum developers and assessment designers are rec-
ognizing that the assessment of spoken commu-
nication skills is important for determining a stu-
dent?s college readiness. For example, the Com-
mon Core State Standards include Speaking &
Listening English Language Arts standards for
each grade that pertain to a student?s ability to
communicate information and ideas using spoken
language.
2
In order to assess these standards, it
2
http://www.corestandards.org/
ELA-Literacy/SL/
22
will be necessary to develop standardized assess-
ments for the K-12 domain that contain items elic-
iting spontaneous speech from the student, such as
presentations, group discussions, etc. Again, with
the introduction of these types of tasks, there is a
risk that a test taker?s spoken response will contain
prepared material drawn from an external source,
and there will be a need to automatically detect
this type of plagiarism on a large scale, in order to
provide fair and valid assessments.
In this paper, we present an initial study of au-
tomated plagiarism detection on spoken responses
containing spontaneous non-native speech. A data
set of actual plagiarized responses was collected,
and text-to-text similarity metrics were applied to
the task of classifying responses as plagiarized or
non-plagiarized.
2 Previous Work
A wide variety of techniques have been employed
in previous studies for the task of detecting plagia-
rized written documents, including n-gram over-
lap (Lyon et al., 2006), document fingerprinting
(Brin et al., 1995), word frequency statistics (Shiv-
akumar and Garcia-Molina, 1995), Information
Retrieval-based metrics (Hoad and Zobel, 2003),
text summarization evaluation metrics (Chen et
al., 2010), WordNet-based features (Nahnsen et
al., 2005), and features based on shared syntactic
patterns (Uzuner et al., 2005). This task is also
related to the widely studied task of paraphrase
recognition, which benefits from similar types of
features (Finch et al., 2005; Madnani et al., 2012).
The current study adopts several of these features
that are designed to be robust to the presence of
word-level modifications between the source and
the plagiarized text; since this study focuses on
spoken responses that are reproduced from mem-
ory and subsequently processed by a speech recog-
nizer, metrics that rely on exact matches are likely
to perform sub-optimally. To our knowledge, no
previous work has been reported on automatically
detecting similar spoken documents, although re-
search in the field of Spoken Document Retrieval
(Haputmann, 2006) is relevant.
Due to the difficulties involved in collecting cor-
pora of actual plagiarized material, nearly all pub-
lished results of approaches to the task of plagia-
rism detection have relied on either simulated pla-
giarism (i.e., plagiarized texts generated by experi-
mental human participants in a controlled environ-
ment) or artificial plagiarism (i.e., plagiarized texts
generated by algorithmically modifying a source
text) (Potthast et al., 2010). These results, how-
ever, may not reflect actual performance in a de-
ployed setting, since the characteristics of the pla-
giarized material may differ from actual plagia-
rized responses. To overcome this limitation, the
current study is based on a set of actual plagiarized
responses drawn from a large-scale assessment.
3 Data
The data used in this study was drawn from the
TOEFL
R
?
Internet-based test (TOEFL
R
?
iBT), a
large-scale, high-stakes assessment of English for
non-native speakers, which assesses English com-
munication skills for academic purposes. The
Speaking section of TOEFL iBT contains six
tasks, each of which requires the test taker to pro-
vide an extended response containing spontaneous
speech. Two of the tasks are referred to as In-
dependent tasks; these tasks cover topics that are
familiar to test takers and ask test takers to draw
upon their own ideas, opinions, and experiences in
a 45-second spoken response (ETS, 2012). Since
these two Independent tasks ask questions that are
not based on any stimulus materials that were pro-
vided to the test taker (such as a reading passage,
figure, etc.), the test takers can provide responses
that contain a wide variety of specific examples.
In some cases, test takers may attempt to game
the assessment by memorizing canned material
from an external source and adapting it to a ques-
tion that is asked in one of the Independent tasks.
This type of plagiarism can affect the validity of
a test taker?s speaking score; however, it is often
difficult even for trained human raters to recog-
nize plagiarized spoken responses, due to the large
number and variety of external sources that are
available from online test preparation sites.
In order to better understand the strategies used
by test takers who incorporated material from ex-
ternal sources into their spoken responses and to
develop a capability for automated plagiarism de-
tection for speaking items, a data set of opera-
tional spoken responses containing potentially pla-
giarized material was collected. This data set con-
tains responses that were flagged by human raters
as potentially containing plagiarized material and
then subsequently reviewed by rater supervisors.
In the review process, the responses were tran-
scribed and compared to external source materi-
23
als obtained through manual internet searches; if
it was determined that the presence of plagiarized
material made it impossible to provide a valid as-
sessment of the test taker?s performance on the
task, the response was assigned a score of 0. This
study investigates a set of 719 responses that were
flagged as potentially plagiarized between Octo-
ber 2010 and December 2011; in this set, 239 re-
sponses were assigned a score of 0 due to the pres-
ence of a significant amount of plagiarized con-
tent from an identified source. This set of 239 re-
sponses is used in the experiments described be-
low.
During the process of reviewing potentially pla-
giarized responses, the raters also collected a data
set of external sources that appeared to have been
used by test takers in their responses. In some
cases, the test taker?s spoken response was nearly
identical to an identified source; in other cases,
several sentences or phrases were clearly drawn
from a particular source, although some modifi-
cations were apparent. Table 1 presents a sample
source that was identified for several of the 239 re-
sponses in the data set.
3
Many of the plagiarized
responses contained extended sequences of words
that directly match idiosyncratic features of this
source, such as the phrases ?how romantic it can
ever be? and ?just relax yourself on the beach.?
In total, 49 different source materials were iden-
tified for all of the potentially plagiarized re-
sponses in the corpus.
4
In addition to the source
materials and the plagiarized responses, a set of
non-plagiarized control responses was also ob-
tained in order to conduct classification experi-
ments between plagiarized and non-plagiarized re-
sponses. Since the plagiarized responses were
collected over the course of more than one year,
they were drawn from many different TOEFL iBT
test forms; in total, the 239 plagiarized responses
comprise 103 distinct Independent test questions.
Therefore, it was not practical to obtain control
data from all of the test items that were represented
in the plagiarized set; rather, approximately 300
responses were extracted from each of the four test
3
This source is available from several online test prepara-
tion websites, for example http://www.mhdenglish.
com/eoenglish_article_view_1195.html.
4
A total of 39 sources were identified for the set of 239
responses in the Plagiarized set; however, all 49 identified
sources were used in the experiments in order to make the
experimental design more similar to an operational set-up in
which the exact set of source texts that will be represented in
a given set of plagiarized responses is not known.
Well, the place I enjoy the most is a small
town located in France. I like this small town
because it has very charming ocean view. I
mean the sky there is so blue and the beach
is always full of sunshine. You know how
romantic it can ever be, just relax yourself
on the beach, when the sun is setting down,
when the ocean breeze is blowing and the
seabirds are singing. Of course I like this
small French town also because there are
many great French restaurants. They offer
the best seafood in the world like lobsters and
tuna fishes. The most important, I have been
benefited a lot from this trip to France because
I made friends with some gorgeous French
girls. One of them even gave me a little watch
as a souvenir of our friendship.
Table 1: Sample source passage used in plagia-
rized responses
items that were most frequently represented in the
set of plagiarized responses. Table 2 provides a
summary of the three data sets used in the study,
along with summary statistics about the length of
the responses in each set.
Data Set N
Number of Words
Mean Std. Dev.
Sources 49 122.5 36.5
Plagiarized 239 109.1 18.9
Control 1196 84.9 24.1
Table 2: Summary of the data sets
As Table 2 shows, the plagiarized responses
are on average a little longer than the control re-
sponses. This is likely due to the fact that the pla-
giarized responses contain a large percentage of
memorized material, which the test takers are able
to produce using a fast rate of speech, since they
had likely rehearsed the content several times be-
fore taking the assessment.
4 Methodology
The general approach taken in this study for deter-
mining whether a spoken response is plagiarized
or not was to compare its content to the content of
each of the source materials that had been iden-
tified for the responses in this corpus. Given a
test response, a comparison was made with each
24
of the 49 reference sources using the following 9
text-to-text similarity metrics: 1) Word Error Rate
(WER), or edit distance between the response and
the source; 2) TER, similar to WER, but allowing
shifts of words within the text at a low edit cost
(Snover et al., 2006); 3) TER-Plus, an extension of
TER that includes matching based on paraphrases,
stemming, and synonym substitution (Snover et
al., 2008); 4) a WordNet similarity metric based on
presence in the same synset;
5
5) a WordNet sim-
ilarity metric based on the shortest path between
two words in the is-a taxonomy; 6) a WordNet
similarity metric similar to (5) that also takes into
account the maximum depth of the taxonomy in
which the words occur (Leacock and Chodorow,
1998); 7) a WordNet similarity metric based on the
depth of the Least Common Subsumer of the two
words (Wu and Palmer, 1994); 8) Latent Semantic
Analysis, using a model trained on the British Na-
tional Corpus (BNC, 2007); 9) BLEU (Papineni et
al., 2002). Most of these similarity metrics (with
the exception of WER and TER) are expected to
be robust to modifications between the source text
and the plagiarized response, since they do not rely
on exact string matches.
Each similarity metric was used to compute 4
different features comparing the test response to
each of the 49 source texts: 1) the document-level
similarity between the test response and the source
text; 2) the single maximum similarity value from
a sentence-by-sentence comparison between the
test response and the source text; 3) the average of
the similarity values for all sentence-by-sentence
comparisons between the test response and the
source text; 4) the average of the maximum simi-
larity values for each sentence in the test response,
where the maximum similarity of a sentence is ob-
tained by comparing it with each sentence in the
source text. The intuition behind using the fea-
tures that compare sentence-to-sentence similarity
as opposed to only the document-level similarity
feature is that test responses may contain a combi-
nation of both passages that were memorized from
a source text and novel content. Depending on the
amount of the response that was plagiarized, these
types of responses may also receive a score of 0;
so, in order to also detect these responses as pla-
5
For the WordNet-based similarity metrics, the similarity
scores for pairs of words were combined to obtain document-
and sentence-level similarity scores by taking the average
maximum pairwise similarity values, similar to the sentence-
level similarity feature defined in (4) below.
giarized, a sentence-by-sentence comparison ap-
proach may be more effective.
The experiments described below were con-
ducted using both human transcriptions of the spo-
ken responses as well as the output from an au-
tomated speech recognition (ASR) system. The
ASR system was trained on approximately 800
hours of TOEFL iBT responses; the system?s
WER on the data used in this study was 0.411 for
the Plagiarized set and 0.362 for the Control set.
Since the ASR output does not contain sentence
boundaries, these were obtained using a Maxi-
mum Entropy sentence boundary detection system
based on lexical features (Chen and Yoon, 2011).
Before calculating the similarity features, all of the
texts were preprocessed to normalize case, seg-
ment the text into sentences, and remove disfluen-
cies, including filled pauses (such as uh and um)
and repeated words. No stemming was performed
on the words in the texts for this study.
5 Results
As described in Section 4, 36 similarity features
were calculated between each spoken response
and each of the 49 source texts. In order to exam-
ine the performance of these features in discrim-
inating between plagiarized and non-plagiarized
responses, classification experiments were con-
ducted on balanced sets of Plagiarized and Con-
trol responses, and the results were averaged using
1000 random subsets of 239 responses from the
Control set.
6
In addition, the following different
feature sets were compared: All (all 36 features),
Doc (the 9 document-level features), and Sent (the
27 features based on sentence-level comparisons).
The J48 decision tree model from the Weka toolkit
(with the default parameter settings) was used
for classification, and 10-fold cross-validation was
performed using both transcriptions and ASR out-
put. Table 3 presents the results of these experi-
ments, including the means (and standard devia-
tions) of the accuracy and kappa (?) values (for all
experiments, the baseline accuracy is 50%).
6 Discussion and Future Work
As Table 3 shows, the classifier achieved a higher
accuracy when using the 9 document-level simi-
larity features compared to using the 27 sentence-
6
Experiments were also conducted using the full Control
set, and the results showed a similar relative performance of
the feature sets.
25
Text Features Accuracy ?
Trans.
All 0.903 (0.01) 0.807 (0.02)
Doc 0.920 (0.01) 0.839 (0.02)
Sent 0.847 (0.01) 0.693 (0.03)
ASR
All 0.852 (0.02) 0.703 (0.03)
Doc 0.871 (0.01) 0.742 (0.03)
Sent 0.735 (0.02) 0.470 (0.04)
Table 3: Mean Accuracy and ? values (and stan-
dard deviations) for classification results using the
239 responses in the Plagiarized set and 1000 ran-
dom subsets of 239 responses from the Control set
level similarity features. In addition, the combined
set of 36 features resulted in a slightly lower per-
formance than when only the 9 document-level
features were used. This suggests that the sentence
level features are not as robust as the document-
level features, probably due to the increased like-
lihood of chance similarities between sentences in
the response and a source text. Despite the fact
that the plagiarized spoken responses in this data
set may contain some original content (in particu-
lar, introductory material provided by the test taker
in an attempt to make the plagiarized content seem
more relevant to the specific test question), it ap-
pears that the document-level features are most ef-
fective. Table 3 also indicates that the performance
of the classifier decreases by approximately 5% -
10% when ASR output is used. This indicates that
the similarity metrics are reasonably robust to the
presence of speech recognition errors in the text,
and that the approach is viable in an operational
setting in which transcriptions of the spoken re-
sponses are not available.
A more detailed error analysis indicates that the
precision of the classifier, with respect to the Pla-
giarized class, is higher than the recall: on the
transcriptions, the average precision using the Doc
features was 0.948 (s.d.= 0.01), whereas the av-
erage recall was 0.888 (s.d.=0.01); for the ASR
set, the average precision was 0.904 (s.d.=0.02),
whereas the average recall was 0.831 (s.d.=0.02).
This means that the rate of false positives pro-
duced by this classifier is somewhat lower than the
rate of false negatives. In an operational scenario,
an automated plagiarized spoken response detec-
tion system such as this one would likely be de-
ployed in tandem with human raters to review the
results and provide a final decision about whether
a given spoken response was plagiarized or not. In
that case, it may be desirable to tune the classi-
fier parameters to increase the recall so that fewer
cases of plagiarism would go undetected, assum-
ing that there are suffient human reviewers avail-
able to process the increased number of false pos-
itives that would result from this approach. Im-
proving the classifier?s recall is also important for
practical applications of this approach, since the
distribution of actual responses is heavily imbal-
anced in favor of the non-plagiarized class. The
current set of experiments only used a relatively
small Control set of 1196 responses for which
transcriptions could be obtained in a cost effective
manner in order to be able to compare the system?s
performance using transcriptions and ASR output.
Since there was only a minor degradation in per-
formance when ASR output was used, future ex-
periments will be conducted using a much larger
Control set in order to approximate the distribution
of categories that would be observed in practice.
One drawback of the method described in this
study is that it requires matching source texts in
order to detect a plagiarized spoken response. This
means that plagiarized spoken responses based on
a given source text will not be detected by the
system until the appropriate source text has been
identified, thus limiting the system?s recall. Be-
sides attempting to obtain additional source texts
(either manually, as was done for this study, or by
automated means), this could also be addressed
by comparing a test response to all previously
collected spoken responses for a given popula-
tion of test takers in order to flag pairs of sim-
ilar responses. While this method would likely
produce a high number of false positives when
the ASR output was used, due to chance simi-
larities between two responses in a large pool of
test taker responses resulting from imperfect ASR,
performance could be improved by considering
additional information from the speech recognizer
when computing the similarity metrics, such as
the N-best list. Additional sources of informa-
tion that could be used for detecting plagiarized re-
sponses include stylistic patterns and prosodic fea-
tures; for example, spoken responses that are re-
produced from memory likely contain fewer filled
pauses and have a faster rate of speech than non-
plagiarized responses; these types of non-lexical
features should also be investigated in future re-
search into the detection of plagiarized spoken re-
sponses.
26
Acknowledgments
We would like to thank Beata Beigman Klebanov,
Dan Blanchard, Nitin Madnani, and three anony-
mous BEA-9 reviewers for their helpful com-
ments.
References
BNC. 2007. The British National Corpus, version 3.
Distributed by Oxford University Computing Ser-
vices on behalf of the BNC Consortium, http:
//www.natcorp.ox.ac.uk/.
Sergey Brin, James Davis, and Hector Garcia-Molina.
1995. Copy detection mechanisms for digital docu-
ments. In Proceedings of the ACM SIGMOD Annual
Conference, pages 398?409.
Lei Chen and Su-Youn Yoon. 2011. Detecting
structural events for assessing non-native speech.
In Proceedings of the 6th Workshop on Innovative
Use of NLP for Building Educational Applications,
NAACL-HLT, pages 38?45, Portland, OR. Associa-
tion for Computational Linguistics.
Chien-Ying Chen, Jen-Yuan Yeh, and Hao-Ren Ke.
2010. Plagiarism detection using ROUGE and
WordNet. Journal of Computing, 2(3):34?44.
Pauline Cullen, Amanda French, and Vanessa Jakeman.
2014. The Official Cambridge Guide to IELTS.
Cambridge University Press.
ETS. 2012. The Official Guide to the TOEFL
R
?
Test,
Fourth Edition. McGraw-Hill.
Andrew Finch, Young-Sook Hwang, and Eiichiro
Sumita. 2005. Using machine translation evalua-
tion techniques to determine sentence-level seman-
tic equivalence. In Proceedings of the Third Inter-
national Workshop on Paraphrasing, pages 17?24.
Alexander Haputmann. 2006. Automatic spoken doc-
ument retrieval. In Ketih Brown, editor, Encylclope-
dia of Language and Linguistics (Second Edition),
pages 95?103. Elsevier Science.
Timothy C. Hoad and Justin Zobel. 2003. Methods
for identifying versioned and plagiarised documents.
Journal of the American Society for Information Sci-
ence and Technology, 54:203?215.
Claudia Leacock and Martin Chodorow. 1998. Com-
bining local context and WordNet similarity for
word sense identification. In Christiane Fellbaum,
editor, WordNet: An Electronic Lexical Database,
pages 305?332. MIT Press.
Pearson Longman. 2010. The Official Guide to Pear-
son Test of English Academic. Pearson Education
ESL.
Caroline Lyon, Ruth Barrett, and James Malcolm.
2006. Plagiarism is easy, but also easy to detect.
Plagiary, 1:57?65.
Nitin Madnani, Joel Tetreault, and Martin Chodorow.
2012. Re-examining machine translation metrics
for paraphrase identification. In Proceedings of
the 2012 Conference of the North American Chap-
ter of the Association for Computational Linguis-
tics: Human Language Technologies, pages 182?
190, Montr?eal, Canada, June. Association for Com-
putational Linguistics.
Thade Nahnsen,
?
Ozlem Uzuner, and Boris Katz. 2005.
Lexical chains and sliding locality windows in
content-based text similarity detection. CSAIL
Technical Report, MIT-CSAIL-TR-2005-034.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: A method for automatic
evaluation of machine translation. In Proceedings
of the 40th Annual Meeting of the Association for
Computational Linguistics.
Martin Potthast, Benno Stein, Alberto Barr?on-Cede?no,
and Paolo Rosso. 2010. An evaluation frame-
work for plagiarism detection. In Proceedings of the
23rd International Conference on Computational
Linguistics.
Martin Potthast, Matthias Hagen, Tim Gollub, Martin
Tippmann, Johannes Kiesel, Paolo Rosso, Efstathios
Stamatatos, and Benno Stein. 2013. Overview of
the 5th International Competition on Plagiarism De-
tection. In Pamela Forner, Roberto Navigli, and
Dan Tufis, editors, CLEF 2013 Evaluation Labs and
Workshop ? Working Notes Papers.
Narayanan Shivakumar and Hector Garcia-Molina.
1995. SCAM: A copy detection mechanism for digi-
tal documents. In Proceedings of the Second Annual
Conference on the Theory and Practice of Digital
Libraries.
Matthew Snover, Bonnie Dorr, Richard Schwartz, Lin-
nea Micciulla, and John Makhoul. 2006. A study of
translation edit rate with targeted human annotation.
In Proceedings of Association for Machine Transla-
tion in the Americas, pages 223?231.
Matt Snover, Nitin Madnani, Bonnie Dorr, and Richard
Schwartz. 2008. TERp: A system descrip-
tion. In Proceedings of the First NIST Metrics
for Machine Translation Challenge (MetricsMATR),
Waikiki, Hawaii, October.
?
Ozlem Uzuner, Boris Katz, and Thade Nahnsen. 2005.
Using syntactic information to identify plagiarism.
In Proceedings of the 2nd Workshop on Building Ed-
ucational Applications using NLP. Ann Arbor.
Zhibiao Wu and Martha Palmer. 1994. Verb semantics
and lexical selection. In Proceedings of the 32nd
Annual Mmeeting of the Association for Computa-
tional Linguistics (ACL).
27
Proceedings of the Ninth Workshop on Innovative Use of NLP for Building Educational Applications , pages 134?142,
Baltimore, Maryland USA, June 26, 2014.
c?2014 Association for Computational Linguistics
Automated Scoring of Speaking Items in an Assessment for Teachers of
English as a Foreign Language
Klaus Zechner, Keelan Evanini, Su-Youn Yoon, Lawrence Davis,
Xinhao Wang, Lei Chen, Chong Min Lee, Chee Wee Leong
Educational Testing Service (ETS)
Princeton, NJ 08541, USA
{kzechner,kevanini,syoon,ldavis,xwang002,lchen,clee001,cleong}@ets.org
Abstract
This paper describes an end-to-end proto-
type system for automated scoring of spo-
ken responses in a novel assessment for
teachers of English as a Foreign Language
who are not native speakers of English.
The 21 speaking items contained in the as-
sessment elicit both restricted and moder-
ately restricted responses, and their aim is
to assess the essential speaking skills that
English teachers need in order to be effec-
tive communicators in their classrooms.
Our system consists of a state-of-the-art
automatic speech recognizer; multiple fea-
ture generation modules addressing di-
verse aspects of speaking proficiency, such
as fluency, pronunciation, prosody, gram-
matical accuracy, and content accuracy; a
filter that identifies and flags problematic
responses; and linear regression models
that predict response scores based on sub-
sets of the features. The automated speech
scoring system was trained and evaluated
on a data set involving about 1,400 test
takers, and achieved a speaker-level cor-
relation (when scores for all 21 responses
of a speaker are aggregated) with human
expert scores of 0.73.
1 Introduction
As English has become increasingly important as a
language of international business, trade, science,
and communication, efforts to promote teaching
English as a Foreign Language (EFL) have seen
substantially more emphasis in many non-English-
speaking countries worldwide in recent years. In
addition, the prevailing trend in English pedagogy
has been to promote the use of spoken English in
the classroom, as opposed to the respective native
languages of the EFL learners. However, due to
the high demand for EFL teachers in many coun-
tries, the training of these teachers has not always
caught up with these high expectations, so there is
a need for both governmental and private institu-
tions involved in the employment and training of
EFL teachers to assess their competence in the En-
glish language, as well as in English pedagogy.
Against this background, we developed a lan-
guage assessment for EFL teachers who are not
native speakers of English that addresses the four
basic English language skills of Reading, Listen-
ing, Writing and Speaking. This paper focuses
only on the speaking portion of the English assess-
ment, and, in particular, on the system that we de-
veloped to automatically compute scores for test
takers? spoken responses.
Several significant challenges needed to be ad-
dressed during the course of building this auto-
mated speech scoring system, including, but not
limited to:
? The 21 Speaking items belong to 8 differ-
ent task types with different characteristics;
therefore, we had to select features and build
scoring models for each task type separately.
? The test takers speak a variety of native lan-
guages, and thus have very different non-
native accents in their spoken English. Fur-
thermore, the test takers also exhibit a wide
range of speaking proficiency levels, which
contributes to the diversity of their spoken re-
sponses. Our speech recognizer therefore had
to be trained and adapted to a large database
of non-native speech.
? Since content accuracy is very important for
the types of tasks contained in the test, even
small error rates by the automatic speech
recognition (ASR) system can lead to a no-
ticeable impact on feature performance. This
fact motivated the development of a set of
134
features that are robust to speech recognition
errors.
? A significant amount of responses (more than
7%) exhibit issues that make them hard or
impossible to score automatically, e.g., high
noise levels, background speech, etc. We
therefore implemented a filter to identify
these non-scorable responses automatically.
The paper is organized as follows: Section 2
discusses related work; in Section 3, we present
the data used for system training and evaluation;
Section 4 describes the system architecture of the
automated speech scoring system. We detail the
methods we used to build our system in Section 5,
followed by an overview of the results in Section
6. Section 7 discusses our findings; finally, Sec-
tion 8 concludes the paper.
2 Related Work
Automated speech processing and scoring tech-
nology has been applied to a variety of domains
over the course of the past two decades, includ-
ing evaluation and tutoring of children?s literacy
skills (Mostow et al., 1994), preparation for high
stakes English proficiency tests for institutions of
higher education (Zechner et al., 2009), evalua-
tion of English skills of foreign-based call center
agents (Chandel et al., 2007), and evaluation of
aviation English (Pearson Education, Inc., 2011),
to name a few (for a comprehensive overview, see
(Eskenazi, 2009)).
Most of these applications elicit restricted
speech from the participants, and the most com-
mon item type by far is the Read Aloud, in which
the speaker reads a sentence or collection of sen-
tences out loud. Due to the constrained nature
of this task, it is possible to develop ASR sys-
tems that are relatively accurate, even with heav-
ily accented non-native speech. Several types of
features related to a non-native speaker?s ability
to produce English sounds and speech patterns
effectively have been extracted from these types
of responses. Some of the best performing of
these types of features include pronunciation fea-
tures, such as a phone?s spectral match to na-
tive speaker acoustic models (Witt, 1999) and a
phone?s duration compared to native speaker mod-
els (Neumeyer et al., 2000); fluency features, such
as the rate of speech, mean pause length, and num-
ber of disfluencies (Cucchiarini et al., 2000); and
prosody features, such as F0 and intensity slope
(Hoenig, 2002).
In addition to the large majority of applications
that elicit restricted speech, a small number of ap-
plications have also investigated automated scor-
ing of non-native spontaneous speech, in order
to more fully evaluate a speaker?s communicative
competence (e.g., (Cucchiarini et al., 2002) and
(Zechner et al., 2009)). In these systems, the same
types of pronunciation, fluency, and prosody fea-
tures can be extracted; furthermore, features re-
lated to additional aspects of a speaker?s profi-
ciency in the non-native language can be extracted,
such as vocabulary usage (Yoon et al., 2012), syn-
tactic complexity (Bernstein et al., 2010a; Chen
and Zechner, 2011), and topical content (Xie et al.,
2012).
As described in Section 1, the domain for the
automated speaking assessment investigated in
this study is teachers of EFL around the world.
Based on the fact that many of the item types are
designed to assess the test taker?s ability to pro-
ductively use English constructions and linguis-
tic units that commonly recur in English teach-
ing environments, several of the item types elicit
semi-restricted speech (see Table 1 below for a de-
scription of the different item types). These types
of responses fall somewhere between the heavily
restricted speech elicited by a Read Aloud task
and unconstrained spontaneous speech. In these
semi-restricted responses, the test taker may be
provided with a set of lexical items that should
be used to form a sentence; in addition, the test
taker is often asked to make the sentence conform
to a given grammatical template. Thus, the re-
sponses provided for a given prompt of this type
by multiple different speakers will often overlap
with each other; however, it is not possible to
specify a complete list of all possible responses.
These types of items have only infrequently been
examined in the context of automated speech scor-
ing. Some related item types that have been
explored previously include the Sentence Build
and Short item types described in (Bernstein et
al., 2010b); however, those item types typically
elicited a much narrower range of responses than
the semi-restricted ones in this study.
3 Data
The data used in this study was drawn from a pilot
administration of a language assessment for teach-
135
ers of English as a Foreign Language. This test
is designed to assess the ability of a non-native
teacher of English to use English in classroom set-
tings. The language forms and functions included
in this test are based on the materials included in a
curriculum that the test takers studied prior to tak-
ing the assessment. The assessment includes items
that cover the four language skills: Reading, Lis-
tening, Writing, and Speaking. There are a total of
8 different types of Speaking items included in the
assessment. These can be divided into the follow-
ing two categories, depending on how constrained
the test taker?s response is:
? Restricted Speech: In these item types, all
of the linguistic content expected in the
test taker?s response is presented in the test
prompt, and the test taker is asked to read or
repeat it aloud.
? Semi-restricted Speech: In these item types, a
portion of the linguistic content is presented
in the prompt, and the test taker is required to
provide the remaining content to formulate a
complete response.
Sets of 7 Speaking items are presented to the
test taker in thematic units, called ?lessons?, based
on their instructional goals; in total, each test taker
completed three lessons, and thus responded to 21
Speaking items. Table 1 presents descriptions of
the 8 different item types included in the assess-
ment.
The numbers of responses provided by the test
takers to each type (along with their respective re-
sponse durations) are as follows: four Multiple
Choice (10 seconds each), six Read Aloud (four 40
second responses and two 60 second responses),
two Repeat Aloud (15 seconds each), one Incom-
plete Sentence (20 seconds), one Key Words (15
seconds), five Chart (four 20 seconds and one 40
seconds), one Keyword Chart (15 seconds), and
one Visuals (15 seconds). Thus, each test taker
provided a total of approximately 9 minutes of au-
dio.
The responses were all double-scored by trained
human raters on a three-point scale (1 - 3). For
the Restricted Speech items, the raters assessed
the test taker?s pronunciation, pacing, and intona-
tion. For the Semi-restricted Speech items, the re-
sponses were also scored holistically on a 3-point
scale, but raters were also asked to take into ac-
count the appropriateness of the language used
Restricted Speech
Type Description
Multiple
Choice
(MC)
The test taker selects the correct
option and reads it aloud
Read Aloud
(RA)
The test taker reads aloud a set
of classroom instructions
Repeat
Aloud (RP)
The test taker listens to a student
utterance twice and then repeats
it
Semi-restricted Speech
Type Description
Incomplete
Sentence
(IS)
The test taker is given a sentence
fragment and completes the sen-
tence according to the instruc-
tions
Key Words
(KW)
The test taker uses the key words
provided to speak a sentence as
instructed
Chart (CH) The test taker uses an example
from a language chart and then
formulates a similar sentence us-
ing a given grammatical pattern
Keyword
Chart (KC)
The test taker constructs a sen-
tence using keywords provided
and information in a chart
Visuals (VI) The test taker is given two visu-
als and is asked to give instruc-
tions to students based on the
graphical information
Table 1: Types of speaking items included in the
assessment
(e.g., grammatical accuracy and content correct-
ness) in addition to aspects of fluency and pronun-
ciation. For some responses, the raters were not
able to provide a score on the 1 - 3 scale, e.g.,
because the audio response contained no speech
input, the test taker responded in their native lan-
guage, etc. These responses are labeled NS for
Non-Scoreable.
After receiving scores, all of the responses
were transcribed using standard English orthogra-
phy (disfluencies, such as filled pauses and par-
tial words are also included in the transcriptions).
Then, the responses were partitioned (with no
speaker overlap) into five sets for the training and
evaluation of the ASR system and the linear re-
gression scoring models. The amount of data and
136
human score distributions in each of these parti-
tions are displayed in Table 2.
4 System Architecture
The automated scoring system used for the teach-
ers? spoken language assessment consists of the
following four components, which are invoked
one after the other in a pipeline fashion (ETS
SpeechRater
SM
, (Zechner et al., 2009; Higgins et
al., 2011)):
? an automated speech recognizer, generating
word hypotheses from input audio recordings
of the test takers? responses
? a feature computation module that generates
features based on the ASR output, e.g., mea-
suring fluency, pronunciation, prosody, and
content accuracy
? a filtering model that flags responses that
should not be scored automatically due to is-
sues with audio quality, empty responses, etc.
? linear regression scoring models that predict
the score for each response based on a set of
selected features
Furthermore, we use Praat (Boersma and
Weenick, 2012) to extract power and pitch from
the speech signal; this information is used for
some of the feature computation modules, as well
as for the filtering model.
The ASR is an HMM-based triphone system
trained on approximately 800 hours of non-native
speech from a different data set; a background
Language Model (LM) was also trained on the
same data set. Subsequently, 8 adapted LMs were
trained (with an interpolation weight of 0.9 for the
in-domain data) using the responses in the ASR
Training partition for the 8 different item types
listed in Table 1. The ASR system obtained an
overall word error rate (WER) of 13.0% on the
ASR Evaluation partition and 15.6% on the Model
Evaluation partition. As would be expected, the
ASR system performed best on the responses that
were most restricted by the test item and per-
formed worse on the responses that were less re-
stricted. The WER ranged from 11.4% for the
RA responses to 41.4% for the IS responses in the
Model Evaluation partition.
5 Methodology
5.1 Speech features
The feature computation components of our
speech scoring system compute more than 100
features based on a speaker?s response. They be-
long to the following broad dimensions of speak-
ing proficiency: fluency, pronunciation, prosody,
vocabulary usage, grammatical complexity and
accuracy, and content accuracy (Zechner et al.,
2009; Chen and Yoon, 2012; Chen et al., 2009;
Zechner et al., 2011; Yoon et al., 2012; Yoon and
Bhat, 2012; Zechner and Wang, 2013).
After initial feature generation, we selected a set
of about 10 features for each of the 8 item types,
based on the following considerations
1
(Zechner
et al., 2009; Xi et al., 2008):
? empirical performance, i.e., feature correla-
tion with human scores
? construct
2
relevance, i.e., to what extent the
feature measures aspects of speaking profi-
ciency that are considered to be relevant and
important by content experts
? overall construct coverage, i.e., the feature set
should include features from all relevant con-
struct dimensions
? feature independence, i.e., the inter-
correlation between any two features of the
set should be low
Furthermore, some features were transformed
(e.g., by applying the inverse or log function), in
order to increase the normality of their distribu-
tions (an assumption of linear regression classi-
fiers). All feature values that exceeded a thresh-
old of 4 standard deviations from the mean were
replaced by the respective threshold (outlier trun-
cation).
The composition of feature sets is slightly dif-
ferent for the two item type categories: for the 3
restricted item types, features related to fluency,
pronunciation, prosody and read/repeat accuracy
were chosen, whereas for the 5 semi-restricted
item types, vocabulary and grammar features were
also added to the set. Further, while accuracy
1
While automated feature selection is conceivable in prin-
ciple, in our experience it typically does not result in a feature
set that meets all of these criteria well.
2
A construct is the set of knowledge, skills, and abilities
measured by a test.
137
Partition Spk. Resp. Dur. 1 2 3 NS
ASR Training 773 16,049 116.7 1,587 (9.9) 4,086 (25.5) 8,796 (54.8) 1,580 (9.8)
ASR Development 25 525 3.8 53 (10.1) 133 (25.3) 327 (62.3) 12 (2.3)
ASR Evaluation 25 525 3.8 31 (5.9) 114 (21.7) 326 (62.1) 54 (10.3)
Model Training 300 6,300 45.8 675 (10.7) 1,715 (27.2) 3,577 (56.8) 333 (5.3)
Model Evaluation 300 6,300 45.7 647 (10.3) 1,637 (26.0) 3,487 (55.3) 529 (8.4)
Total 1,423 29,699 215.8 2,993 (9.38) 7,685 (25.14) 16,513 (58.26) 2,508 (7.22)
Table 2: Amount of data contained in each partition (speakers, responses, hours of speech) and distribu-
tion of human scores (percentages of scores per partition in brackets).
features for the restricted items were based only
on string alignment measures, content accuracy
features for the semi-restricted items were more
diverse, e.g., based on regular expressions, key-
words, and language model scores (Zechner and
Wang, 2013). Table 3 lists the features that were
used in the scoring models for restricted and semi-
restricted item types, along with sub-constructs
they measure and their description.
5.2 Filtering model
In order to automatically identify responses that
have technical issues (e.g., loud background noise)
or are otherwise not scorable (e.g., empty re-
sponses), a decision tree-based filtering model was
developed using a combination of features derived
from ASR output and from pitch and energy in-
formation (Yoon et al., 2011; Jeon and Yoon,
2012). The filtering model was tested on the scor-
ing model evaluation data, and obtained an ac-
curacy rate (the exact agreement between the fil-
tering model and a human rater concerning the
distinction between scorable and non-scorable re-
sponses) of 97%; it correctly identified 90% of the
non-scorable responses in the data set with a false
positive rate of 21% (recall=0.90, precision=0.79,
F-score=0.84).
5.3 Scoring models
We used the Model Training set to train 8 linear
regression models for the 8 different item types,
using the previously determined feature sets. We
used the features as independent variables in these
models and the summed scores of two human
raters as the dependent variable. These trained
scoring models were then employed to score re-
sponses of the Model Evaluation data (exclud-
ing responses marked as non-scorable by human
raters) and rounded to the nearest integer to predict
the final scores for each response. These scores
were then evaluated against the first human rater
score (H1).
Item N S-H1 H1-H2 WER (%)
RA 1653 0.34 0.51 11.4
RP 543 0.41 0.73 21.8
MC 1036 0.67 0.83 17.1
CH 1372 0.44 0.67 26.3
KW 275 0.45 0.67 28.7
KC 274 0.57 0.74 28.8
IS 260 0.46 0.69 41.4
VI 272 0.43 0.80 30.4
Table 4: Correlations between system and first hu-
man rater (S-H1) and between two human raters
(H1-H2), for all responses of each item type in the
Model Evaluation partition (N). The last column
provides the average ASR word error rate (WER)
in percent.
Additionally, for responses flagged as non-
scorable by the automatic filtering model, the sec-
ond human rater score (H2) was used as final
item score in order to mimic the operational sce-
nario where human raters score responses that are
flagged by the filtering model.
We also compute the agreement between sys-
tem and human raters based on a set of all 21 re-
sponses of a speaker. Score imputation was used
for responses that were labeled as non-scorable by
both the system and H2; in this case, the response
was given the mean score of the total scorable
responses from the same speaker. Similarly, the
same score imputation rule was applied to the H1
scores.
6 Results
Table 4 presents the Pearson correlation coeffi-
cients between human and automated scores for
the responses from the 8 different item types along
with the human-human correlation for each item
type. Furthermore, we also provide the word error
rates of the ASR system for the same 8 item types
in the last column of the table.
138
Feature Sub-construct Description
Content Ed1 Read/repeat accu-
racy / Fluency
Correctly read words per minute
Content Ed2 Read/repeat accu-
racy
Read/repeat word error rate
Content RegEx Content accuracy Matching of regular expressions
Content WER Content accuracy Response discrepancy from high scoring responses
Content NGram Content accuracy N-grams in response matching high scoring response n-
grams
Fluency Rate Fluency Speaking rate
Fluency Chunk Fluency Average length of contiguous word chunks
Fluency Sil1 Fluency Frequency of long silences
Fluency Sil2 Fluency / Grammar Proportion of long within-clause-silences to all within-
clause-silences
Fluency Sil3 Fluency Mean length of silences within a clause
Fluency Disfl1 Fluency Frequency of interruption points (repair, repetition, false
start)
Fluency Disfl2 Fluency Number of disfluencies per second
Fluency Disfl3 Fluency Frequency of repetitions
Pron Vowels Pronunciation Average vowel duration differences relative to a native-
speaker model
Prosody1 Prosody Percentage of stressed syllables
Prosody2 Prosody Mean deviation of time intervals between stressed syllables
Prosody3 Prosody Mean distance between stressed syllables
Vocab1 Vocabulary / Flu-
ency
Number of word types divided by utterance duration
Grammar POS Grammar Part-of-speech based distributional similarity score be-
tween a response and responses with different score levels
Grammar LM Grammar Global language model score (normalized by response
length)
Table 3: List of features used for item type scoring models, with the sub-constructs they represent and
descriptions.
139
Comparison Pearson r
S-H1 0.725
S-H2 0.742
H1-H2 0.934
Table 5: Speaker-level performance (Pearson r
correlations) computed over the sum of all 21
scores from each speaker, N=272
Sub-construct Restricted Semi-restricted
Content 0.33?0.67 0.34?0.61
Fluency 0.19?0.33 0.20?0.33
Pronunciation 0.20?0.22 0.13?0.31
Prosody 0.18?0.24 0.12?0.27
Grammar ? 0.23?0.49
Vocabulary ? 0.21?0.32
Table 6: Range of Pearson r correlations for dif-
ferent features with human scores (H1) by sub-
construct for restricted and semi-restricted item
types.
Table 5 presents the Pearson correlation coeffi-
cients between the speaker-level scores produced
by the automated scoring system (S) and the two
sets of human scores (H1 and H2). These speaker-
level scores were computed based on the sum of
all 21 scores from each speaker in the Model Eval-
uation partition. Responses that received a non-
scorable rating from the human raters were im-
puted, as described above. Furthermore, 28 speak-
ers were excluded from this analysis because they
had more than 7 non-scorable responses each.
3
Finally, Table 6 provides an overview of Pear-
son correlation ranges with human rater scores
(H1) for the different features used in the scoring
models, summarized by the sub-constructs that the
features represent.
7 Discussion
When looking at Table 4, we see that the inter-rater
reliability for human raters ranges between 0.51
(for RA items) and 0.83 (for MC items). Inter-
rater reliability varies less for the 5 semi-restricted
item types (0.67?0.80), compared to the 3 re-
stricted item types (0.51?0.83). As for automated
score correlations with human raters, the Pearson
r coefficients range from 0.34 (RA) to 0.67 (MC).
3
In an operational setting, these test takers would not re-
ceive a test score; instead, they would have the opportunity to
take the test again.
Again, the variability of Pearson r coefficients is
larger for the 3 restricted item types (0.34?0.67)
than for the 5 semi-restricted item types (0.43?
0.57). The degradation in correlation between the
inter-human results and the machine-human re-
sults varies from 0.16 (MC) to 0.37 (VI).
Speech recognition word error rate does not
seem to have a strong influence on model perfor-
mance (RA items have the lowest WER with S-
H1 r=0.34, but r=0.46 for IS items that have the
highest WER). However, we found other factors
that affect model performance negatively; for ex-
ample, multiple repeats of responses by test tak-
ers contribute to the large performance difference
between S-H1 and H1-H2 for the RP items. In
general, we conjecture that using features for a
larger set of sub-construct areas?in the case of
semi-restricted item types?may contribute to the
lower variation of scoring model performance for
this subset of the data.
As for speaker-level results (Table 5), the over-
all degradation between the inter-human correla-
tion and the system-human correlations is of a
similar magnitude (around 0.2) as observed for
most of the individual item types. Still, the
speaker-level correlation of 0.73 is 0.26 higher
than the average item type correlation between the
system and H1.
When we look into more detail at the Pearson
r correlations between individual features used in
the item type scoring models and human scores
(Table 6), we can see that features related to con-
tent accuracy exhibit a substantially stronger per-
formance (r=0.33?0.67) than features related to
most other sub-constructs of speaking proficiency,
namely fluency, pronunciation, prosody, and vo-
cabulary (r ? 0.2). One exception is features
related to grammar, where correlations with hu-
man scores are as high as 0.49. Since related work
on scoring speech using features indicative of flu-
ency, pronunciation, etc. showed higher correla-
tions (e.g., (Cucchiarini et al., 1997; Franco et al.,
2000; Zechner et al., 2009)), we conjecture that
the reason behind this difference is likely to be
found in the fact that the responses in this assess-
ment for teachers of English are quite short (6?
14 words on average for all items except for Read
Aloud items that are about 46 words on average).
Since content features are less reliant on longer
stretches of speech, they still work fairly well for
most items in our corpus.
140
Finally, while the proportion of words contained
in responses in restricted items is much larger than
those contained in responses in semi-restricted
items, these two item type categories are more
evenly distributed over the whole test, i.e., each
test taker responds to 9 semi-restricted and 12 re-
stricted items, and the item scores are then aggre-
gated for a final score with equal weight given to
each item score.
8 Conclusion
This paper presented an overview of an automated
speech scoring system that was developed for a
language assessment for teachers of English as a
Foreign Language (EFL) whose native language
is not English. We described the main compo-
nents of this prototype system and their perfor-
mance: the ASR system, features generated from
ASR output, a filtering model to flag non-scorable
responses, and finally a set of linear regression
models, one for each of 8 different types of test
items.
We found that overall, the correlation between
our speech scoring system?s predicted scores and
human rater scores range between 0.34 and 0.67,
evaluated on responses from 8 item types. Further-
more, we found that correlations based on com-
plete sets of 21 spoken responses per test taker im-
prove to around r = 0.73.
Given the many significant challenges of this
work, including 8 different item types in the as-
sessment, responses from speakers from different
native languages and speaking proficiency levels,
sub-optimal audio conditions for a part of the data,
and a relatively small data set for both ASR system
adaptation and linear regression model training,
we find that the overall performance achieved by
our automated speech scoring system was a good
starting point for an eventual deployment in a low-
stakes assessment context.
Future work will aim at improving the perfor-
mance of the prediction models by the addition of
more features addressing different aspects of the
construct as well as an improved filtering model
for flagging the different types of problematic re-
sponses. Furthermore, agreement between human
raters, in particular for read-aloud items, could be
improved by refining rater rubrics and additional
rater training and monitoring.
Acknowledgments
The authors would like to thank Anastassia Louk-
ina and Jidong Tao for their comments on an ear-
lier version of this paper, and are also indebted
to the anonymous reviewers of BEA-9 and ASRU
2013 for their valuable comments and suggestions.
References
Jared Bernstein, Jian Cheng, and Masanori Suzuki.
2010a. Fluency and structural complexity as pre-
dictors of L2 oral proficiency. In Proceedings of In-
terspeech.
Jared Bernstein, Alistair Van Moere, and Jian Cheng.
2010b. Validating automated speaking tests. Lan-
guage Testing, 27(3):355?377.
Paul Boersma and David Weenick. 2012. Praat: Doing
phonetics by computer, version 5.3.32. http://
www.praat.org.
Abhishek Chandel, Abhinav Parate, Maymon Ma-
dathingal, Himanshu Pant, Nitendra Rajput, Shajith
Ikbal, Om Deshmuck, and Ashish Verma. 2007.
Sensei: Spoken language assessment for call cen-
ter agents. In Proceedings of the IEEE Workshop on
Automatic Speech Recognition and Understanding
(ASRU).
Lei Chen and Su-Youn Yoon. 2012. Application of
structural events detected on ASR outputs for auto-
mated speaking assessment. In Proceedings of In-
terspeech.
Miao Chen and Klaus Zechner. 2011. Computing
and evaluating syntactic complexity features for au-
tomated scoring of spontaneous non-native speech.
In Proceedings of the 49th Annual Meeting of the As-
sociation for Computational Linguistics, pages 722?
731.
Lei Chen, Klaus Zechner, and Xiaoming Xi. 2009. Im-
proved pronunciation features for construct-driven
assessment of non-native spontaneous speech. In
Proceedings of NAACL-HLT.
Catia Cucchiarini, Helmer Strik, and Lou Boves. 1997.
Automatic evaluation of Dutch pronunciation by us-
ing speech recognition technology. In Proceedings
of the IEEE Workshop on Auotmatic Speech Recog-
nition and Understanding (ASRU).
Catia Cucchiarini, Helmer Strik, and Lou Boves. 2000.
Quantitative assessment of second language learn-
ers? fluency by means of automatic speech recogni-
tion technology. Journal of the Acoustical Society of
America, 107(2):989?999.
Catia Cucchiarini, Helmer Strik, and Lou Boves. 2002.
Quantitative assessment of second language learn-
ers? fluency: Comparisons between read and spon-
taneous speech. Journal of the Acoustical Society of
America, 111(6):2862?2873.
141
Maxine Eskenazi. 2009. An overview of spoken lan-
guage technology for education. Speech Communi-
cation, 51(10):832?844.
Horacio Franco, Leonardo Neumeyer, Vassilios Di-
galakis, and Orith Ronen. 2000. Combination of
machine scores for automatic grading of pronuncia-
tion quality. Speech Communication, 30(1-2):121?
130.
Derrick Higgins, Xiaoming Xi, Klaus Zechner, and
David M. Williamson. 2011. A three-stage ap-
proach to the automated scoring of spontaneous spo-
ken responses. Computer Speech and Language,
25(2):282?306.
Florian Hoenig. 2002. Automatic assessment of non-
native prosody ? Annotation, modelling, and evalu-
ation. In Proceedings of the International Sympo-
sium on Automatic Detection of Errors in Pronun-
ciation Training (ISADEPT), pages 21?30, Stock-
holm, Sweden.
Je Hun Jeon and Su-Youn Yoon. 2012. Acoustic
feature-based non-scorable response detection for an
automated speaking proficiency assessment. In Pro-
ceedings of Interspeech.
Jack Mostow, Steven F. Roth, Alexander G. Haupt-
mann, and Matthew Kane. 1994. A prototype read-
ing coach that listens. In Proceedings of the Twelfth
National Conference on Artificial Intelligence.
Leonardo Neumeyer, Horacio Franco, Vassilios Di-
galakis, and Mitchel Weintraub. 2000. Automatic
scoring of pronunciation quality. Speech Communi-
cation, 30:83?93.
Pearson Education, Inc. 2011. Versant
TM
Aviation English Test. http://www.
versanttest.com/technology/
VersantAviationEnglishTestValidation.
pdf.
Silke Witt. 1999. Use of speech recognition in
computer-assisted language learning. Ph.D. thesis,
Cambridge University.
Xiaoming Xi, Derrick Higgins, Klaus Zechner, and
David M. Williamson. 2008. Automated scoring of
spontaneous speech using SpeechRater v1.0. Edu-
cational Testing Service Research Report RR-08-62.
Shasha Xie, Keelan Evanini, and Klaus Zechner. 2012.
Exploring content features for automated speech
scoring. In Proceedings of the 2012 Conference of
the North American Chapter of the Association for
Computational Linguistics: Human Language Tech-
nologies, pages 103?111, Montr?eal, Canada. Asso-
ciation for Computational Linguistics.
Su-Youn Yoon and Suma Bhat. 2012. Assessment of
ESL learners? syntactic competence based on sim-
ilarity measures. In Proceedings of the 2012 Joint
Conference on Empirical Methods in Natural Lan-
guage Processing and Computational Natural Lan-
guage Learning, pages 600?608, Jeju Island, Korea.
Association for Computational Linguistics.
Su-Youn Yoon, Keelan Evanini, and Klaus Zechner.
2011. Non-scorable response detection for auto-
mated speaking proficiency assessment. In Proceed-
ings of NAACL-HLT Workshop on Innovative Use of
NLP for Building Educational Applications.
Su-Youn Yoon, Suma Bhat, and Klaus Zechner. 2012.
Vocabulary profile as a measure of vocabulary so-
phistication. In Proceedings of the 7th Workshop on
Innovative Use of NLP for Building Educational Ap-
plications, NAACL-HLT, Montr?eal, Canada. Associ-
ation for Computational Linguistics.
Klaus Zechner and Xinhao Wang. 2013. Automated
content scoring of spoken responses in an assess-
ment for teachers of english. In Proceedings of
the 8th Workshop on Innovative Use of NLP for
Building Educational Applications, NAACL-HLT,
Atlanta. Association for Computational Linguistics.
Klaus Zechner, Derrick Higgins, Xiaoming Xi, and
David M. Williamson. 2009. Automatic scoring
of non-native spontaneous speech in tests of spoken
English. Speech Communication, 51(10):883?895.
Klaus Zechner, Xiaoming Xi, and Lei Chen. 2011.
Evaluating prosodic features for automated scoring
of non-native read speech. In Proceedings of the
IEEE Workshop on Automatic Speech Recognition
and Understanding (ASRU).
142
