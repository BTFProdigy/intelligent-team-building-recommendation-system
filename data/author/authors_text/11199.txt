Story Link Detection based on Dynamic Information Extending
Xiaoyan Zhang Ting Wang Huowang Chen
Department of Computer Science and Technology, School of Computer,
National University of Defense Technology
No.137, Yanwachi Street, Changsha, Hunan 410073, P.R.China
{zhangxiaoyan, tingwang, hwchen}@nudt.edu.cn
Abstract
Topic Detection and Tracking refers to au-
tomatic techniques for locating topically re-
lated materials in streams of data. As the
core technology of it, story link detection is
to determine whether two stories are about
the same topic. To overcome the limitation
of the story length and the topic dynamic
evolution problem in data streams, this pa-
per presents a method of applying dynamic
information extending to improve the per-
formance of link detection. The proposed
method uses previous latest related story to
extend current processing story, generates
new dynamic models for computing the sim-
ilarity between the current two stories. The
work is evaluated on the TDT4 Chinese cor-
pus, and the experimental results indicate
that story link detection using this method
can make much better performance on all
evaluation metrics.
1 Introduction
Topic Detection and Tracking (TDT) (Allan, 2002)
refers to a variety of automatic techniques for dis-
covering and threading together topically related
material in streams of data such as newswire or
broadcast news. Such automatic discovering and
threading could be quite valuable in many appli-
cations where people need timely and efficient ac-
cess to large quantities of information. Supported
by such technology, users could be alerted with new
events and new information about known events. By
examining one or two stories, users define the topic
described in them. Then with TDT technologies
they could go to a large archive, find all the stories
about this topic, and learn how it evolved.
Story link detection, as the core technology de-
fined in TDT, is a task of determining whether two
stories are about the same topic, or topically linked.
In TDT, a topic is defined as ?something that hap-
pens at some specific time and place? (Allan, 2002).
Link detection is considered as the basis of other
event-based TDT tasks, such as topic tracking, topic
detection, and first story detection. Since story link
detection focuses on the streams of news stories,
it has its specific characteristic compared with the
traditional Information Retrieval (IR) or Text Clas-
sification task: new topics usually come forth fre-
quently during the procedure of the task, but nothing
about them is known in advance.
The paper is organized as follows: Section 2 de-
scribes the procedure of story link detection; Section
3 introduces the related work in story link detection;
Section 4 explains a baseline method which will
be compared with the proposed dynamic method in
Section 5; the experiment results and analysis are
given in Section 6; finally, Section 7 concludes the
paper.
2 Problem Definition
In the task definition of story link detection (NIST,
2003), a link detection system is given a se-
quence of time-ordered news source files S =
?S1, S2, S3, . . . , Sn? where each Si includes a set
of stories, and a sequence of time-ordered story
pairs P = ?P1, P2, P3, . . . , Pm? where Pi =
40
(si1, si2), si1 ? Sj , si2 ? Sk, 1 ? i ? m, 1 ? j ?
k ? n. The system is required to make decisions on
all story pairs to judge if they describe a same topic.
We formalize the procedure for processing a pair
of stories as follows:
For a story pair Pi = (si1, si2):
1. Get background corpus Bi of Pi. According to
the supposed application situation and the cus-
tom that people usually look ahead when they
browse something, in TDT research the system
is usually allowed to look ahead N (usually 10)
source files when deciding whether the current
pair is linked. So Bi = {S1, S2, S3, . . . , Sl},
where
l =
{
k + 10 , si2 ? Sk and (k + 10) ? n
n , si2 ? Sk and (k + 10) > n .
2. Produce the representation models (Mi1,Mi2)
for two stories in Pi. M = {(fs, ws) | s ? 1},
where fs is a feature extracted from a story and
ws is the weight of the feature in the story. They
are computed with some parameters counted
from current story and the background.
3. Choose a similarity function F and computing
the similarity between two models. If t is a pre-
defined threshold and F (Mi1,Mi2) ? t, then
stories in Pi are topically linked.
3 Related Work
A number of works has been developed on story link
detection. It can be classified into two categories:
vector-based methods and probabilistic-based meth-
ods.
The vector space model is widely used in IR and
Text Classification research. Cosine similarity be-
tween document vectors with tf?idf term weighting
(Connell et al, 2004) (Chen et al, 2004) (Allan et
al., 2003) is also one of the best technologies for link
detection. We have examined a number of similarity
measures in story link detection, including cosine,
Hellinger and Tanimoto, and found that cosine sim-
ilarity produced outstanding results. Furthermore,
(Allan et al, 2000) also confirms this conclusion
among cosine, weighted sum, language modeling
and Kullback-Leibler divergence in its story link de-
tection research.
Probabilistic-based method has been proven to be
very effective in several IR applications. One of its
attractive features is that it is firmly rooted in the the-
ory of probability, thereby allowing the researcher
to explore more sophisticated models guided by the
theoretical framework. (Nallapati and Allan, 2002)
(Lavrenko et al, 2002) (Nallapati, 2003) all ap-
ply probability models (language model or relevance
model) for story link detection. And the experiment
results indicate that the performances are compara-
ble with those using traditional vector space models,
if not better.
On the basis of vector-based methods, this paper
represents a method of dynamic information extend-
ing to improve the performance of story link detec-
tion. It makes use of the previous latest topically re-
lated story to extend the vector model of current be-
ing processed story. New dynamic models are gen-
erated for computing the similarity between two sto-
ries in current pair. This method resolves the prob-
lems of information shortage in stories and topic dy-
namic evolution in streams of data.
Before introducing the proposed method, we first
describe a method which is implemented with vector
model and cosine similarity function. This straight
and classic method is used as a baseline to be com-
pared with the proposed method.
4 Baseline Story Link Detection
The related work in story link detection shows that
vector representation model with cosine function
can be used to build the state-of-the-art story link de-
tection systems. Many research organizations take
this as their baseline system (Connell et al, 2004)
(Yang et al, 2002). In this paper, we make a similar
choice.
The baseline method represents each story as a
vector in term space, where the coordinates repre-
sent the weights of the term features in the story.
Each vector terms (or feature) is a single word plus
its tag which is produced by a segmenter and part of
speech tagger for Chinese. So if two tokens with
same spelling are tagged with different tags, they
will be taken as different terms (or features). It is
notable that in it is independent between processing
any two comparisons the baseline method.
41
4.1 Preprocessing
A preprocessing has been performed for TDT Chi-
nese corpus. For each story we tokenize the text, tag
the generated tokens, remove stop words, and then
get a candidate set of terms for its vector model. Af-
ter that, the term-frequency for each token in the
story and the length of the story will also be ac-
quired. In the baseline and dynamic methods, both
training and test data are preprocessed in this way.
The segmenter and tagger used here is ICTCLAS
1 . The stop word list is composed of 507 terms. Al-
though the term feature in the vector representation
is the word plus its corresponding tag, we will ig-
nore the tag information when filtering stop words,
because almost all the words in the list should be
filtered out whichever part of speech is used to tag
them.
4.2 Feature Weighting
One important issue in the vector model is weight-
ing the individual terms (features) that occur in the
vector. Most IR systems employed the traditional
tf ? idf weighting, which also provide the base for
the baseline and dynamic methods in this paper. Fur-
thermore, this paper adopts a dynamic way to com-
pute the tf ? idf weighting:
wi(fi, d) = tf(fi, d) ? idf(fi)
tf = t/(t+ 0.5 + 1.5dl/dlavg)
idf = log((N + 0.5)/df)/log(N + 1)
where t is the term frequency in a story, dl is the
length of a story, dlavg is the average length of sto-
ries in the background corpus, N is the number of
stories in the corpus, df is the number of the stories
containing the term in the corpus.
The tf shows how much a term represents the
story, while the idf reflects the distinctive ability
of distinguishing current story from others. The
dynamic attribute of the tf ? idf weighting lies in
the dynamic computation of dlavg, N and df . The
background corpus used for statistics is incremen-
tal. As more story pairs are processed, more source
files could be seen, and the background is expand-
ing as well. Whenever the size of the background
1http://sewm.pku.edu.cn/QA/reference/ICTCLAS/FreeICT-
CLAS/
has changed, the values of dlavg, N and df will up-
date accordingly. We call this as incremental tf ?idf
weighting. A story might have different term vectors
in different story pairs.
4.3 Similarity Function
Another important issue in the vector model is de-
termining the right function to measure the similar-
ity between two vectors. We have firstly tried three
functions: cosine, Hellinger and Tanimoto, among
which cosine function performs best for its substan-
tial advantages and the most stable performance. So
we consider the cosine function in baseline method.
Cosine similarity, as a classic measure and con-
sistent with the vector representation, is simply an
inner product of two vectors where each vector is
normalized to the unit length. It represents cosine
of the angle between two vector models M1 =
{(f1i, w1i), i ? 1} and M2 = {(f2i, w2i), i ? 1}.
cos(M1,M2) = (?(w1i ? w2i))/
?
(?w21i)(?w22i)
Cosine similarity tends to perform best at full di-
mensionality, as in the case of comparing two sto-
ries. Performance degrades as one of the vectors be-
comes shorter. Because of the built-in length nor-
malization, cosine similarity is less dependent on
specific term weighting.
5 Dynamic Story Link Detection
5.1 Motivation
Investigation on the TDT corpus shows that news
stories are usually short, which makes that their rep-
resentation models are too sparse to reflect topics
described in them. A possible method of solving
this problem is to extend stories with other related
information. The information can be synonym in
a dictionary, related documents in external corpora,
etc. However, extending with synonym is mainly
adding repetitious information, which can not define
the topics more clearly. On the other hand, topic-
based research should be real-sensitive. The corpora
in the same period as the test corpora are not easy
to gather, and the number of related documents in
previous period is very few. So it is also not feasi-
ble to extend the stories with related documents in
other corpora. We believe that it is more reason-
able that the best extending information may be the
42
story corpus itself. Following the TDT evaluation
requirement, we will not use entire corpus at a time.
Instead, when we process current pair of stories, we
utilize all the stories before the current pair in the
story corpus.
In addition, topics described by stories usually
evolve along with time. A topic usually begins with
a seminal event. After that, it will focus mainly on
the consequence of the event or other directly re-
lated events as the time goes. When the focus in
later stories has changed, the words used in them
may change remarkably. Keeping topic descrip-
tions unchanged from the beginning to the end is
obviously improper. So topic representation mod-
els should also be updated as the topic emphases in
stories has changed. Formerly we have planed to use
related information to extend a story to make up the
information shortage in stories. Considering more
about topic evolution, we extend a story with its lat-
est related story. In addition, up to now almost all
research in story link detection takes the hypothe-
sis that whether two stories in one pair are topically
linked is independent of that in another pair. But we
realize that if two stories in a pair describe a same
topic, one story can be taken as related information
to extend another story in later pairs. Compared with
extending with more than one story, extending only
with its latest related story can keep representation
of the topic as fresh as possible, and avoid extend-
ing too much similar information at the same time,
which makes the length of the extended vector too
long. Since the vector will be renormalized, a too
big length means evidently decreasing the weight
of an individual feature which will instead cause a
lower cosine similarity. This idea has also been con-
firmed by the experiment showing that the perfor-
mance extending with one latest related story is su-
perior to that extending with more than one related
story, as described in section 6.3. The experiment re-
sults also show that this method of dynamic informa-
tion extending apparently improves the performance
of story link detection.
5.2 Method Description
The proposed dynamic method is actually the base-
line method plus dynamic information extending.
The preprocessing, feature weighting and similarity
computation in dynamic method are similar as those
in baseline method. However, the vector representa-
tion for a story here is dynamic. This method needs a
training corpus to get the extending threshold decid-
ing whether a story should be used to extend another
story in a pair. We split the sequence of time-ordered
story pairs into two parts: the former is for training
and the later is for testing. The following is the pro-
cessing steps:
1. Preprocess to create a set of terms for repre-
senting each story as a term vector, which is
same as baseline method.
2. Run baseline system on the training corpora
and find an optimum topically link threshold.
We take this threshold as extending threshold.
The topically link threshold used for making
link decision in dynamic method is another pre-
defined one.
3. Along with the ordered story pairs in the test
corpora, repeat a) and b):
(a) When processing a pair of stories Pi =
(si1, si2), if si1 or si2 has an extending
story, then update the corresponding vec-
tor model with its related story to a new
dynamic one. The generation procedure
of dynamic vector will be described in
next subsection.
(b) Computing the cosine similarity between
the two dynamic term vectors. If it ex-
ceeds the extending threshold, then si1 and
si2 are the latest related stories for each
other. If one story already has an extend-
ing story, replace the old one with the new
one. So a story always has no more than
one extending story at any time. If the
similarity exceeds topically link threshold,
si1 and si2 are topically linked.
From the above description, it is obvious that dy-
namic method needs two thresholds, one for making
extending decision and the other for making link de-
cision. Since in this paper we will focus on the op-
timum performance of systems, the first threshold is
more important. But topically link threshold is also
necessary to be properly defined to approach a bet-
ter performance. In the baseline method, term vec-
tors are dynamic because of the incremental tf ? idf
43
weighting. However, dynamic information extend-
ing is another more important reason in the dynamic
method. Whenever a story has an extending story, its
vector representation will update to include the ex-
tending information. Having the extending method,
the representation model can have more information
to describe the topic in a story and make the topic
evolve along with time. The dynamic method can
define topic description clearer and get a more accu-
rate similarity between stories.
5.3 Dynamic Vector Model
In the dynamic method, we have tried two ways for
the generation of dynamic vector models: increment
model and average model. Supposing we use vector
model M1 = {(f1i, w1i), i ? 1} of story s1 to ex-
tend vector model M2 = {(f2i, w2i), i ? 1} of story
s2, M2 will change to representing the latest evolv-
ing topic described in current story after extending.
1. Increment Model: For each term f1i in M1, if
it also occurs as f2j in M2, then w2j will not
change, otherwise (f1i, w1i) will be added into
M2. This dynamic vector model only takes in-
terest in the new information that occurs only in
M1. For features both occurred in M1 and M2,
the dynamic model will respect to their original
weights.
2. Average Model: For each term f1i in M1, if
it also occurs as f2j in M2, then w2j = 0.5 ?
(w1i+w2j), otherwise (f1i, w1i) will be added
into M2. This dynamic model will take account
of all information in M1. So the difference be-
tween those two dynamic models is the weight
recalculation method of the feature occurred in
both M1 and M2.
Both the above two dynamic models can take ac-
count of information extending and topic evolution.
Increment Model is closer to topic description since
it is more dependent on latest term weights, while
Average Model makes more reference to the cen-
troid concept. The experiment results show that dy-
namic method with Average Model is a little supe-
rior to that with Increment Model.
6 Experiment and Discussion
6.1 Experiment Data
To evaluate the proposed method, we use the Chi-
nese subset of TDT4 corpus (LDC, 2003) devel-
oped by the Linguistic Data Consortium (LDC) for
TDT research. This subset contains 27145 stories
all in Chinese from October 2000 through January
2001, which are gathered from news, broadcast or
TV shows.
LDC totally labeled 40 topics on TDT4 for 2003
evaluation. There are totally 12334 stories pairs
from 1151 source files in the experiment data. The
answers for these pairs are based on 28 topics of
these topics, generated from the LDC 2003 anno-
tation documents. The first 2334 pairs are used for
training and finding extending threshold of dynamic
method. The rest 10000 pairs are testing data used
for comparing performances of baseline and the dy-
namic methods.
6.2 Evaluation Measures
The work is measured by the TDT evaluation soft-
ware, which could be referred to (Hoogma, 2005)
for detail. Here is a brief description. The goal of
link detection is to minimize the cost due to errors
caused by the system. The TDT tasks are evaluated
by computing a ?detection cost?:
Cdet = Cmiss?Pmiss?Ptarget+Cfa?Pfa?Pnon?target
where Cmiss is the cost of a miss, Pmiss is the es-
timated probability of a miss, Ptarget is the prior
probability under which a pair of stories are linked,
Cfa is the cost of a false alarm, Pfa is the estimated
probability of a false alarm, and Pnon?target is the
prior probability under which a pair of stories are
not linked. A miss occurs when a linked story pair is
not identified as being linked by the system. A false
alarm occurs when the pair of stories that are not
linked are identified as being linked by the system.
A target is a pair of linked stories; conversely a non-
target is a pair of stories that are not linked. For the
link detection task these parameters are set as fol-
lows: Cmiss is 1, Ptarget is 0.02, and Cfa is 0.1. The
cost for each topic is equally weighted (usually the
cost of topic-weighted is the mainly evaluation pa-
rameter) and normalized so that for a given system,
the normalized value (Cdet)norm can be no less than
44
one without extracting information from the source
data:
(Cdet)norm = Cdetmin(CmissPtarget, CfaPnon?target)
(Cdet)overall = ?i(Cidet)norm/#topics
where the sum is over topics i. A detection curve
(DET curve) is computed by sweeping a threshold
over the range of scores, and the minimum cost over
the DET curve is identified as the minimum detec-
tion cost or min DET. The topic-weighted DET cost
is dependent on both a good minimum cost and a
good method for selecting an operating point, which
is usually implemented by selecting a threshold. A
system with a very low min DET cost can have a
much larger topic-weighted DET score. Therefore,
we focus on the minimum DET cost for the experi-
ments.
6.3 Experiment Results
In this paper, we have tried three methods for story
link detection: the baseline method described in
Section 4 and two dynamic methods with different
dynamic vectors introduced in Section 5. The fol-
lowing table gives their evaluation results.
metrics baseline dynamic 1 dynamic 2
Pmiss 0.0514 0.0348 0.0345
Pfa 0.0067 0.0050 0.0050
Clinkmin 0.0017 0.0012 0.0012
Clinknorm 0.0840 0.0591 0.0588
Table 1: Experiment Results of Baseline System and
Dynamic Systems
In the table, Clinkmin is the minimum
(Cdet)overall, DET Graph Minimum Detection
Cost (topic-weighted), Clinknorm is the normal-
ized minimum (Cdet)overall, the dynamic 1 is the
dynamic method which uses Increment Model and
the dynamic 2 is the dynamic method which uses
Average Model. We can see that the proposed two
dynamic methods are both much better than base-
line method on all four metrics. The ClinkNorm
of dynamic 1 and 2 are improved individually by
27.2% and 27.8% as compared to that of baseline
method. The difference between two dynamic
methods is due to different in the Pmiss. However,
it is too little to compare the two dynamic systems.
We also make additional experiments in which a
story is extended with all of its previous related
stories. The minimum (Cdet)overall is 0.0614 for
the system using Increment Model, and 0.0608 for
the system using Average Model. Although the
performances are also much superior to baseline, it
is still a little poorer than that with only one latest
related story, which confirm the ideal described in
section 5.1.
Figure 1, 2 and 3 show the detail evaluation in-
formation for individual topic on Minimum Norm
Detection Cost, Pmiss and Pfa. From Figure 1 we
know these two dynamic methods have improved the
performance on almost all the topic, except topic 12,
26 and 32. Note that detection cost is a function of
Pmiss and Pfa. Figure 2 shows that both two dy-
namic methods reduce the false alarm rates on all
evaluation topics. In Figure 3 there are 20 topics
on which the miss rates remain zero or unchange.
The dynamic methods reduce the miss rates on 5
topics. However, dynamic methods get relatively
poorer results on topic 12, 26 and 32 . Altogether
dynamic methods can notably improve system per-
formance on evaluation metrics of both individual
and weighted topic, especially the false alarm rate,
but on some topics, it gets poorer results.
Further investigation shows that topic 12, 26 and
32 are about Presidential election in Ivory Coast
on October 25, 2000, Airplane Crash in Chiang
Kai Shek International Airport in Taiwan on Octo-
ber 31, 2000, and APEC Conference on Novem-
ber 12-15, 2000 at Brunei. After analyzing those
story pairs with error link decision, we can split
them into two sets. One is that two stories in a pair
are general linked but not TDT specific topically
linked. Here general linked means that there are
many common words in two stories, but the events
described in them happened in different times or dif-
ferent places. For example, Airplane Crash is a gen-
eral topic, while Airplane Crash in certain location
at specification time is a TDT topic. The other is
that two stories in a pair are TDT topically linked
while they describe the topic from different perspec-
tives. In this condition they will have few common
words. These may be due to that the information
extracted from stories is still not accurate enough
to represent them. It also may be because of the
45
00.05
0.1
0.15
0.2
0.25
0.3
0.35
1 3 5 7 9 11 13 15 17 19 21 23 25 27 29 31 33 35 37 39
Topic ID
N
o
r
m
 
C
l
i
n
k
Norm Clink(Baseline)
Norm Clink(Dynamic 1)
Norm Clink(Dynamic 2)
 
Figure 1: Normalized Minimum Detection Cost for individual topic
0
0.005
0.01
0.015
0.02
0.025
0.03
0.035
1 3 5 7 9 11 13 15 17 19 21 23 25 27 29 31 33 35 37 39
Topic ID
F
a
l
s
e
 
A
l
a
r
m
 
p
r
o
b
a
b
i
l
i
t
y
Pfa(Baseline)
Pfa(Dynamic 1)
Pfa(Dynamic 2)
 
Figure 2: Pfa for individual topic
0
0.05
0.1
0.15
0.2
0.25
0.3
1 3 5 7 9 11 13 15 17 19 21 23 25 27 29 31 33 35 37 39
Topic ID
M
i
s
s
 
p
r
o
b
a
b
i
l
i
t
y
Pmiss(Baseline)
Pmiss(Dynamic 1)
Pmiss(Dynamic 2)
 
Figure 3: Pmiss for individual topic
46
deficiency of vector model itself. Furthermore, we
know that the extending story is chosen by cosine
similarity, which results that the extending story and
the extended story are usually topically linked from
the same perspectives, seldom from different per-
spectives. Therefore the method of information ex-
tending may sometimes turn the above first problem
worse and have no impact on the second problem.
So mining more useful information or making more
use of other useful resources to solve these problems
will be the next work. In addition, how to repre-
sent this information with a proper model and seek-
ing better or more proper representation models for
TDT stories are also important issues. In a word,
the method of information extending has been veri-
fied efficient in story link detection and can provide a
reference to improve the performance of some other
similar systems whose data must be processed seri-
ally, and it is also hopeful to combined with other
improvement technologies.
7 Conclusion
Story link detection is a key technique in TDT re-
search. Though many approaches have been tried,
there are still some characters ignored. After analyz-
ing the characters and deficiency in TDT stories and
story link detection, this paper presents a method of
dynamic information extending to improve the sys-
tem performance by focus on two problems: infor-
mation deficiency and topic evolution. The exper-
iment results indicate that this method can effec-
tively improve the performance on both miss and
false alarm rates, especially the later one. How-
ever, we should realize that there are still some prob-
lems to solve in story link detection. How to com-
pare general topically linked stories and how to com-
pare stories describing a TDT topic from different
angles will be very vital to improve system perfor-
mance. The next work will focus on mining more
and deeper useful information in TDT stories and
exploiting more proper models to represent them.
Acknowledgement
This research is supported by the National Natu-
ral Science Foundation of China (60403050), Pro-
gram for New Century Excellent Talents in Uni-
versity (NCET-06-0926) and the National Grand
Fundamental Research Program of China under
Grant(2005CB321802).
References
James Allan, Victor Lavrenko, Daniella Malin, and Rus-
sell Swan. 2000. Detections, bounds, and timelines:
Umass and tdt?3. In Proceedings of Topic Detection
and Tracking (TDT?3), pages 167?174.
J. Allan, A. Bolivar, M. Connell, S. Cronen-Townsend,
A Feng, F. Feng, G. Kumaran, L. Larkey, V. Lavrenko,
and H. Raghavan. 2003. Umass tdt 2003 research
summary. In proceedings of TDT workshop.
James Allan, editor. 2002. Topic Detection and Track-
ing: Event-based Information Organization. Kluwer
Academic Publishers, Norvell, Massachusetts.
Francine Chen, Ayman Farahat, and Thorsten Brants.
2004. Multiple similarity measures and source-pair
information in story link detection. In HLT-NAACL,
pages 313?320.
Margaret Connell, Ao Feng, Giridhar Kumaran, Hema
Raghavan, Chirag Shah, and James Allan. 2004.
Umass at tdt 2004. In TDT2004 Workshop.
Niek Hoogma. 2005. The modules and methods of topic
detection and tracking. In 2nd Twente Student Confer-
ence on IT.
Victor Lavrenko, James Allan, Edward DeGuzman,
Daniel LaFlamme, Veera Pollard, and Stephen
Thomas. 2002. Relevance models for topic detec-
tion and tracking. In Proceedings of Human Language
Technology Conference (HLT), pages 104?110.
LDC. 2003. Topic detection and tracking - phase 4.
Technical report, Linguistic Data Consortium.
Ramesh Nallapati and James Allan. 2002. Capturing
term dependencies using a language model based on
sentence trees. In Proceedings of the eleventh interna-
tional conference on Information and knowledge man-
agement, pages 383?390. ACM Press.
Ramesh Nallapati. 2003. Semantic language models for
topic detection and tracking. In HLT-NAACL.
NIST. 2003. The 2003 topic detection and tracking task
definition and evaluation plan. Technical report, Na-
tional Institute of Standards and Technology(NIST).
Yiming Yang, Jian Zhang, Jaime Carbonell, and Chun
Jin. 2002. Topic-conditioned novelty detection. In
Proceedings of the eighth ACM SIGKDD international
conference on Knowledge discovery and data mining,
pages 688?693. ACM Press.
47
Semantic Role Labeling of Chinese Using Transductive SVM and  
Semantic Heuristics 
Yaodong Chen        Ting Wang         Huowang Chen        Xishan Xu 
Department of Computer Science and Technology, School of Computer, 
National University of Defense Technology  
No.137, Yanwachi Street, Changsha, Hunan 410073, P.R.China 
{yaodongchen, tingwang, hwchen}@nudt.edu.cn   xxs@hnmcc.com 
 
Abstract 
Semantic Role Labeling (SRL) as a 
Shallow Semantic Parsing causes more 
and more attention recently. The shortage 
of manually tagged data is one of main 
obstacles to supervised learning, which is 
even serious in SRL. Transductive SVM 
(TSVM) is a novel semi-supervised learn-
ing method special to small mount of 
tagged data. In this paper, we introduce an 
application of TSVM in Chinese SRL. To 
improve the performance of TSVM, some 
heuristics have been designed from the 
semantic perspective. The experiment re-
sults on Chinese Propbank showed that 
TSVM outperforms SVM in small tagged 
data, and after using heuristics, it performs 
further better. 
1 Introduction 
Semantic analysis is one of the fundamental and 
key problems for the research in computational 
linguistics. Traditional semantic research is 
mainly concerned with deep analysis, which pro-
vides a representation of the sentence in predicate 
logic or other formal specification. Recently, shal-
low semantic parsing is becoming a hotspot in 
semantic analysis research. Semantic Role Label-
ing is a shallow semantic parsing technology and 
defined as a shared task in CoNLL-04. It aims at 
recognizing semantic roles (i.e. arguments) for 
each target verb in sentence and labeling them to 
the corresponding syntactic constituents. Many 
SRL research utilizes machine learning methods 
(Park, 2005; Pradhan, 2005; Cohn, 2005), in 
which the high performance reported was attrib-
uted to large tagged dataset (Carreras, 2005). But 
one of the main obstacles to supervised learning is 
the shortage of manually labeled data, which is 
even serious in SRL. It could bring about one 
question: whether these methods perform well 
when large mount of tagged data are not available? 
In this paper, we investigate Transductive SVM 
(Joachims, 1999), a semi-supervised learning 
method, for this question. The proposed method 
uses large untagged data in training with the sup-
port of the linguistic knowledge of semantic roles.  
Generally speaking, not all constituents in syn-
tactic tree could act as argument candidates in 
SRL. Large redundant constituents lead to a high 
training cost and decrease the performance of sta-
tistical model especially when tagged data is small. 
In contrast to the pruning algorithms in Park 
(2005) and Xue (2004) which are based on syntax, 
some argument-specific heuristics, based on word 
semantic features of arguments, make semantic 
restrictions on constituent candidates to optimize 
dataset of statistical models. The experiment re-
sults on Chinese Propbank shows that TSVM out-
performs regular statistical models in small tagged 
data, and after using argument-specific heuristics, 
it performs further better. 
The rest of this paper is organized as follows. 
Section 2 gives the definition, method, and re-
sources about SRL. Section 3 discusses how to 
apply TSVM for SRL. Some argument-specific 
heuristics are introduced in Section 4. And then, 
section 5 shows the experiment results of the pro-
posed methods and compare it with SVM. Finally, 
we conclude our work in section 6. 
919
2 Problem Definitions & Related Works 
Comparing with full parsing, SRL acts on part 
of constituents in sentences in order to achieve 
high performance and robustness, as well as low 
complexity in practices. The SRL problem can be 
described as follows. 
Definition Given a semantic role (or argument) 
collect R and a sentence S, for any substring c of S, 
SRL is a function: c?R?NONE, where NONE is 
the value excluded in R. 
Notice that c usually indicates phrases in a sen-
tence. SRL can be classified to two steps: 
z Identification: c? {NONE, ARG}. It is a 
binary-value function where ARG is assigned 
to c when it should be labeled at some ele-
ment of R, or NONE is assigned. Identifica-
tion separates the argument substrings from 
the rest of sentence, in another words, finds 
the argument candidates. 
z Classification: c?R. It is a multi-value func-
tion which assigns a role value to c, that is, 
labels a role to some candidate. 
Some typical systems, based on inductive learn-
ing, have been evaluated in CoNLL-05 (Carreras, 
2005). It concluded that the performance of SRL 
depends on the combination of several factors in-
cluding models, features, and results of syntactic 
parsing. The best result achieved F1=75.04 1 . 
These systems have strong dependency on large 
tagged data. This paper evaluates the performance 
of a classical supervised learning method--SVM 
in small tagged data and introduces a novel semi-
supervised method to handle this problem. 
There are two tagged corpora available for SRL: 
one is Proposition Bank (Propbank); the other is 
FrameNet. The Propbank annotates the Penn 
Treebank with verb argument structure according 
as Levin class (Levin, 1993). It defines a general 
set of arguments for all types of predicates, and 
these arguments are divided into core and adjunct 
ones. FrameNet, as a linguistic ontology, describe 
the scenario related to each predicates. The sce-
nario (i.e. frame) is filled with specific partici-
pants (i.e. role). In this paper, we use Chinese 
Propbank 1.0 provided by Linguistic Data Consor-
tium (LDC), which is based on Chinese Treebank. 
It consists of 37,183 propositions indexed to the 
                                                 
1 F1 measure computes the harmonic mean of precision 
and recall of SRL systems in CoNLL-2005 
first 250k words in Chinese Treebank 5.1, includ-
ing 4,865 verb types and 5,298 framesets. 
3 TSVM based SRL 
3.1 TSVM 
There are two kinds of learning modes that are 
applied in Artificial Intelligence, i.e. inductive 
inference and transductive inference. In classifica-
tion problems, inductive inference trains a global 
model based on tagged instances from the whole 
problem space and classify new untagged in-
stances by it. The classical statistical models such 
as SVM, ME have been developed in this way. 
Since large mount of tagged data are usually ac-
quired difficultly in practice, and the global mod-
els are hard to get when tagged training data are 
not enough to find the target function in the hy-
pothesis space. In addition, this global model may 
be unnecessary sometimes when we only care for 
specific data. Compared with inductive inference, 
transductive inference classifies untagged in-
stances by a local model based on the clustering 
distribution of these untagged instances. The 
TSVM, a representative of transductive inference 
method, was introduced by Joachims (1999). 
TSVM is a good semi-supervised method special 
to some cases where the tagged data is difficult to 
acquire on a large scale while large untagged data 
is easily available. TSVM can be formulated as an 
optimization problem: 
Minimize Over (y1*..yn*, w, b,?1..?n, ?*..?k*) in  
 **ww
2
1
11
__ ??
==
?+?+
k
i
i
n
i
i
T
CC ?? , subject to: 
0for  -1  b)  xw( y 1      ii
__
i
n
1i ??>+?? == ini ??   
0*for   * -1  b)  *xw( *y k 1iii
__
i
k
1i ??>+?? == i??  
where (x1,y1),?,(xn,yn) ? Strain, y1,?,yn? {-
1,+1}, x1*,?,xn*?Stest, y1*,?,yn* is the labels of 
x1*,?, xn*, C and C* , specified by user, are the 
effect factor of the tagged and untagged examples 
respectively, C*?i* is the effect term of the ith 
untagged example in the above objective function. 
In addition, a cost-factor Ctemp, which indicates the 
ratio of positive untagged examples, should be 
specified experientially by user before training.  
Here we introduce the algorithm briefly, and 
the detail is referred to Joachims (1999). The algo-
rithm starts with training regular SVM with the 
920
tagged examples and then classifies the untagged 
examples by the trained model. Then several cou-
ples of examples (one is positive, the other is 
negative) are switched in class labels according to 
some rule, and the model is retrained to minimum 
the objective function. At the same time, Ctemp will 
increase in consistent way. The iteration will end 
when Ctemp goes beyond C*. The algorithm is 
proved to converge in a finite number of steps. 
3.2 Apply TSVM for SRL 
The SRL using TSVM is related to following 
portions: 
Dataset The principle of TSVM described in 
above section implicitly indicates the performance 
depends deeply on dataset (including tagged and 
untagged data). In particular, tagged data have an 
influence on original regular SVM in the first step 
of training, while the untagged data will affect the 
final performance through the iteration of training. 
It is obvious that the more even the data set distri-
bution is, the better the learning classifier will per-
form. Similar to most practical classification task, 
a serious uneven problem (Li, 2003) exists in SRL. 
For instance, the number of constituents labeled to 
arguments (positive instances) is much less than 
the number of the rest (negative instances). To 
handle this problem, we design some heuristics 
for several kinds of arguments (that is, ARG0, 
ARGM-TMP, ARGM-LOC, ARGM-MNR, 
ARGM-DIR and ARGM-EXT) semantically. 
These heuristics filter out redundant constituents 
and raise the ratio of positive instances in the 
dataset. We will compare these argument-specific 
heuristics with Xue (2004), and some results are 
showed in Section 4. 
Parameters The ratio of positive examples in 
dataset, P, is a key parameter in TSVM and 
should be assigned as one prior value in experi-
ment. In this paper, P is dynamically assigned ac-
cording to different argument since different heu-
ristics could produce different proportion of posi-
tive and negative instances used to training data. 
Features A wide range of features have been 
shown to be useful in previous work on SRL 
(Pradhan, 2005; Xue et al 2004). This paper 
chooses 10 features in classification because of 
two reasons: at first, they are the core features 
considered to have significance on the perform-
ance of SRL (Carreras, 2005); secondly, these 
features provide a standard to evaluate different 
methods of Chinese SRL. These features are listed 
in Table 1, detail description referred in Xue 
(2005). 
Feature Description 
Predicate The predicate lemma 
Subcat-Frame The rule that expands the parent of 
verb 
Path The syntactic path through the parse 
tree from the parse constituent  to 
the predicate being classified 
Position A binary feature identifying whether 
the phrase is before or after the 
predicate 
Phrase Type The syntactic category of the phrase 
corresponding to the argument 
Phrase type of the 
sibling to the left
The syntactic category of the phrase 
is sibling to the argument in the left
Head Word and 
Part Of Speech 
The syntactic head of the phrase 
First and last word 
of the constituent 
in focus 
First and last word of phrase corre-
sponding to the argument 
Syntactic Frame The syntactic frame consists of the 
NPs that surround the predicate 
Table 1. The features of Semantic Role Labeling 
It should be mentioned that we have not con-
sidered the Combination features (Xue et al 2005) 
because the above 10 features have already coded 
them. Verb class is also not be used here since we 
have no idea about the syntactic alternations used 
for verb classification in Xue (2005) and could not 
evaluate them equally. So, the experiment in this 
paper refers to the results without verb class in 
Xue (2005). 
Classifiers Chinese Propbank has 22 argument 
types, in which 7 argument types appearing less 
than ten times or even having no appearance have 
not been considered, that is,ARGM-FRQ, ARGM-
ASP, ARGM-PRD, ARGM-CRD, ARGM-T, and 
ARGM-DGR. So we have developed 15 binary 
classifiers for those 15 type of arguments and ex-
cluded the above 7 because they hardly provide 
useful information for classification, as well as 
have slightly influence on results (account for 
0.02% in all arguments appeared in the corpus). 
4 Heuristics 
In this section, we discuss the principle of the 
designing of the argument-specific heuristics. To 
handle the uneven problem in SRL, six semantic 
heuristics have been designed for six types of ar-
guments, such as ARG0, ARGM-TMP, ARGM-
921
LOC, ARGM-MNR, ARGM-DIR, and ARGM-
EXT. The heuristic is actually some restrictive 
rules which can be viewed as pre-processing of 
identification. (Xue et al 2004) introduced a pri-
mary algorithm for pruning argument non-
candidates. The algorithm still remain large re-
dundant unnecessary constituents yet (correct ar-
guments account for 7.31% in all argument candi-
dates extracted). (Park, 2005) used the clause 
boundary restriction and tree distance restriction 
for extracting candidates based on Government 
and Binding Theory. All of these restrictive rules, 
however, are on the syntax level. Here we con-
sider several semantic features directly extracted 
by the head word of the argument in lexicon. This 
is based on facts that ARG0 contain mostly NPs 
whose head words are animate objects or entities. 
(Yi, 2007) shows agent and experiencer as ARG0 
accounts for 93% in all ARG0s in Propbank. In 
addition, some head words of the constituents la-
beled by ARGM-TMP have temporal sense, 
which is the same as ARGM-LOC whose head 
words usually have spatial sense. The semantic 
information can be extracted from a Chinese-
English bilingual semantic resource: HowNet 
(Dong, 2000). HowNet is an on-line common-
sense knowledge base providing a universal lexi-
cal concept representation mechanism. Word 
sense representations are encoded by a set of ap-
proximately 2,000 primitive concepts, called se-
memes. A word sense is defined by its primary 
sememes. For example, ?? (child) is defined 
with sememes ?human|??, ?young|??; ?? (at 
present) has sememes ?time|???, ?now|??; ?
(street) contains sememes ?location|???, ?route|
??. We considered sememes as the basis of heu-
ristics, and Table 2 shows these heuristics. 
Table 2 shows the argument-specific heuristics 
on the semantics level, for example, only when 
the head word of a PP contains a sememe ?time|
???, it could be a candidate of ARGM-TMP, 
such as ??, ??; only a sememe ?location|?
?? has a head word of one phrase, it may be la-
beled to ARGM-LOC. Furthermore, we make a 
comparison with Xue (2004) in whole argument 
types on Chinese Propbank (the extraction princi-
ple about argument types which are not listed in 
Table 1 is the same as Xue (2004)). We find the 
argument-specific heuristics decrease in uneven 
problem more effectively than Xue (2004). The 
overall coverage 2 rises from 7.31% to 20.30%, that 
is, 65% constituents which have no possibility to 
labeling have been pruned based on six types of 
arguments. And the overall recall of arguments in 
corpus decline slightly from 99.36% to 97.28%. 
Args Def Heuristic Cover
-age 
ARG0 agent,ex
p-eriencer 
the NP whose head 
word has sememe that is 
hyponymy with animate|?
? or whose head word is 
place or organization 
38.90 
ARGM-
TMP 
temporal The NP and LCP whose 
head word has sememe 
time|?? or the PP whose 
prep is from|?, from|?, 
to|?, in|?, or at|? 
58.7 
ARGM-
LOC 
location The NP and LCP whose 
head word has sememe 
location|?? or the PP 
whose prep is in|? ,at|? 
or from|? 
44.4 
ARGM-
MNR
manner The PP whose prep is ?ac-
cording to|??, ?,?, ?
?? or by|??, as|?? 
30.98 
ARGM-
DIR 
directional The PP whose prep is to|? 
or from|?, to|? 
20.56 
ARGM-
EXT 
extent The NP and QP whose head 
word is number 
70.27 
Table 2. The arguments-specific heuristics. 
5 Experiment and discussion 
This section will describe the experiment on the 
SRL in Chinese Treebank, compare TSVM with 
regular SVM, and evaluate the effect of the pro-
posed argument-specific heuristics. 
5.1 Experiment Setting 
SVM-light3 is used as a SVM classifier toolkit 
in the experiment, which includes some sub-tools 
for optimizing performance and reducing training 
time. It also provides an approximate implementa-
tion of transductive SVM. At first, about 80% 
propositions (1711891) has been extracted ran-
domly from the corpus as the dataset, which had 
been divided into tagged set and untagged set ac-
cording to 4:1. Then, for each type of arguments, 
                                                 
2The coverage means the ratio of arguments in all role 
candidates extracted from Chinese Propbank by given 
heuristic. 
3 http://svmlight.joachims.org/ 
922
numeric vectors are extracted from these two sets 
(one proposition could produce many instances) 
as the dataset for the following learning models 
through the heuristics in Table 2. When training 
the classifier, linear kernel function had used, set-
ting the C to 2 experientially. 
5.2 Results and Discussion 
A baseline was developed with 10 features and 
15 SVM classifiers (tagged set for training, 
untagged set for testing) as described in Section 3. 
We made a comparison between the baseline and 
the work in Xue (2005), and then used the argu-
ment-specific heuristics for baseline. Table 3 
shows the performance of these methods. Baseline 
matches Xue approximately despite of the absence 
of combination features. We also find that the ar-
gument-specific heuristics improve the perform-
ance of baseline from 89.97% to 90.86% for F1 
and beyond the Xue. It can be explained that when 
using heuristics, the proportion of positive and 
negative instances in dataset are adjusted reasona-
bly to improve the model. About 1 percent im-
provement attributes to the effectivity of these six 
argument-specific heuristics. 
Systems Precision Recall F1 
Baseline 89.70 90.24 89.97
Xue 90.40 90.30 90.30
Heuristics 91.45 90.28 90.86
Table 3. A comparison among baseline, Xue and 
heuristics through regular SVM 
In order to investigating the learning perform-
ance of SVM, TSVM and TSVM using argument-
specific heuristics in small tagged data, we ex-
tracted randomly different number of propositions 
in Propbank as tagged data and another 5000 
propositions held out as untagged data. Both of 
them are used for training TSVM model. Table 4 
shows the overall performance and the perform-
ances of two arguments--ARG0 and ARGM-
TMP--along with the different training data size. 
As we can see in (a) of Table 4, the TSVM leads 
to an improved performance on overall argument 
types when tagged data less than 100 propositions 
(raising F1 about 10%). It indicates that transduc-
tive inference performs much better than inductive 
inference because it makes use of the additional 
information about the distribution of 5000 
untagged propositions. More important, we find 
that TSVM using argument-specific heuristics, 
comparing to TSVM, has a distinctive improve-
ment (raising about 3%). It confirmed that our 
heuristics have positive influences on transductive 
inference.  
Number of tagged 
propositions 
SVM TSVM TSVM + 
Heuristics
10 36.51 50.51 50.82 
20 41.65 50.52 53.66 
40 41.64 55.42 60.63 
160 76.40 80.84 82.32 
1000 82.00 83.87 84.00 
5000 84.41 85.61 86.45 
 (a). The overall results on all argument types. 
Number of tagged 
propositions 
SVM TSVM TSVM +  
Heuristics
10 20.51 29.51 30.21 
20 22.34 32.45 38.54 
40 35.00 45.42 50.63 
160 45.45 50.45 55.74 
1000 52.43 55.43 57.40 
5000 58.00 60.34 61.45 
(b) The detail results on ARG0 
Number of tagged 
propositions 
SVM TSVM TSVM + 
Heuristics
10 15.98 20.45 19.98 
20 25.34 29.45 35.43 
40 30.32 32.80 39.43 
160 38.31 40.00 45.09 
1000 48.43 50.43 55.45 
5000 60.34 62.34 63.90 
(c) The detail results on ARGM-TMP 
Table 4. A comparison with Regular SVM, TSVM 
and TSVM using argument-specific heuristics hold-
ing 5000 untagged propositions 
Number of untagged 
propositions 
SVM TSVM TSVM + 
Heuristics
500 69.03 68.50 69.44 
1000 70.12 70.22 70.82 
2000 68.64 71.30 73.01 
4000 69.53 72.01 76.50 
5000 68.95 72.54 77.21 
10000 70.28 74.78 79.74 
Table 5. A comparison with Regular SVM, TSVM 
and TSVM using argument-specific heuristics hold-
ing 100 tagged propositions 
We then evaluate the six argument-specific 
heuristics introduced in Section 4 with the same 
5000 untagged propositions. It is noticeable that 
the training time of TSVM doubles that of SVM 
approximately. The (b) and (c) of Table 4 give the 
detail results on ARG0 and ARGM-TMP. Com-
923
pared with (a), it is obvious that the improvement 
between TSVM using heuristics with TSVM for 
ARG0 and ARGM-TMP is larger than the overall 
improvement. That is to say, the more distinctive 
knowledge is embedded in heuristics, the better 
performance can be achieved for the correspond-
ing argument. This observation encourages us to 
investigate more heuristics for more arguments. 
Finally, the influence of untagged data on per-
formance of TSVM has been investigated. We 
extract different size of untagged propositions and 
hold 100 tagged propositions for training TSVM. 
Table 5 shows the results. It should be mention 
that the result of SVM fluctuates slightly, which is 
due to different number of testing examples. On 
the other hand, TSVM and TSVM using argu-
ment-specific heuristics improve highly as the 
increase in untagged data size. The bigger the 
untagged data, the larger the performance gap be-
tween SVM and TSVM and the gap between 
TSVM and TSVM using argument-specific heu-
ristics. It indicates that the argument-specific heu-
ristics, optimizing the dataset, have substantial 
effectivity in the performance of TSVM when 
untagged data is large.  
6 Conclusions 
Most machine learning methods such as SVM, 
ME have a strong dependence on tagged data, 
which lead to a poor generalization when large 
tagged data are not available. This paper intro-
duces a novel semi-supervised method--TSVM for 
this problem. TSVM can effectively use clustering 
information from untagged data for training the 
model. The experiment demonstrated the TSVM 
achieve better performance than regular SVM 
when only very few tagged examples are available. 
Aiming at serious uneven problem in SRL, argu-
ment-specific heuristics are proposed correspond 
to six kinds of arguments. These heuristics are 
developed by extracting semantic features of ar-
guments from HowNet. The experiment proves 
that these heuristics have much effect not only in 
the inductive inference (regular SVM) but also in 
transductive inference (TSVM), especially when 
the untagged data is large. The high performance 
of six heuristics demonstrated that semantic char-
acteristics are significant on SRL, which encour-
ages us to develop more semantic characteristics 
of more arguments in the future. 
 
Acknowledgement This research is supported by 
the National Natural Science Foundation of China 
(60403050), Program for New Century Excellent Tal-
ents in University (NCET-06-0926) and the National 
Grand Fundamental Research Program of China under 
Grant (2005CB321802). 
References 
Levin Beth. 1993. English Verb Class and Alternations: 
A Preliminary Investigation. Chicago: University of 
Chicago Press. 
Xavier Carreras and Llu??s M`arquez, 2005. Introduc-
tion to the CoNLL-2005 Shared Task: Semantic Role 
Labeling. CoNLL-2005. 
Trevor Cohn and Philip Blunsom. 2005. Semantic role 
labeling with tree conditional random fields. CoNLL-
2005. 
Zhendong Dong. 2000. http://www.keenage.com/. 
Thorsten Joachims. 1999. Transductive inference for 
text classification using support vector machines. 
ICML-99, pages 200?209, Bled, Slovenia, Jun. 
Yaoyong Li and John Shawe-Taylor. 2003. The SVM 
with Uneven Margins and Chinese Document Catego-
rization. PACLIC-2003, Singapore. 
Kyung-Mi Park and Hae-Chang Rim. 2005. Maximum 
entropy based semantic role labeling. CoNLL-2005. 
Pradhan S, Hacioglu K, Krugler V, et al 2005. Support 
Vector Learning for Semantic Argument Classification. 
Machine Learning journal. 60(1-3): 11-39. 
Nianwen Xue and Martha Palmer. 2004. Calibrating 
Features for Semantic Role Labeling. EMNLP. 
 Nianwen Xue and Martha Palmer. 2005, Automatic 
Semantic Role Labeling for Chinese Verbs. The IJCAI-
2005, Edinburgh, Scotland. 
Szuting Yi, Edward Loper and Martha Palmer. 2007. 
Can Semantic Roles Generalize Across Genres? 
NAANL-HLT 07, Rochester, N Y. 
924
