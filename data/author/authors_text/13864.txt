Proceedings of the Human Language Technology Conference of the North American Chapter of the ACL, pages 85?88,
New York, June 2006. c?2006 Association for Computational Linguistics
Automatic Recognition of Personality in Conversation
Franc?ois Mairesse
Department of Computer Science
University of Sheffield
Sheffield, S1 4DP, United Kingdom
F.Mairesse@sheffield.ac.uk
Marilyn Walker
Department of Computer Science
University of Sheffield
Sheffield, S1 4DP, United Kingdom
M.A.Walker@sheffield.ac.uk
Abstract
The identification of personality by auto-
matic analysis of conversation has many
applications in natural language process-
ing, from leader identification in meetings
to partner matching on dating websites.
We automatically train models of the main
five personality dimensions, on a corpus
of conversation extracts and personality
ratings. Results show that the models per-
form better than the baseline, and their
analysis confirms previous findings link-
ing language and personality, while re-
vealing many new linguistic and prosodic
markers.
1 Introduction
It is well known that utterances convey information
about the speaker in addition to their semantic con-
tent. One such type of information consists of cues
to the speaker?s personality traits, typically assessed
along five dimensions known as the Big Five (Nor-
man, 1963):
? Extraversion (sociability, assertiveness)
? Emotional stability (vs. neuroticism)
? Agreeableness to other people (friendliness)
? Conscientiousness (discipline)
? Intellect (openness to experience)
Findings include that extraverts talk more, louder,
and faster, with fewer pauses and hesitations, and
more informal language (Scherer, 1979; Furnham,
1990; Heylighen and Dewaele, 2002; Gill and Ober-
lander, 2002). Neurotics use more 1st person sin-
gular pronouns and negative emotion words, while
conscientious people avoid negations and negative
emotion words (Pennebaker and King, 1999). The
use of words related to insight and the avoidance of
past tense indicate intellect, and swearing and neg-
ative emotion words mark disagreeableness. Cor-
relations are higher in spoken language, possibly
especially in informal conversation (Mehl et al, in
press).
Previous work has modeled emotion and person-
ality in virtual agents, and classified emotions from
actor?s speech (Andre? et al, 1999; Liscombe et al,
2003). However, to our knowledge no one has tested
whether it is possible to automatically recognize per-
sonality from conversation extracts of unseen sub-
jects. Our hypothesis is that automatic analysis of
conversation to detect personality has application
in a wide range of language processing domains.
Identification of leaders using personality dimen-
sions could be useful in analyzing meetings and the
conversations of suspected terrorists (Hogan et al,
1994; Tucker and Whittaker, 2004; Nunn, 2005).
Dating websites could analyze text messages to try
to match personalities and increase the chances of a
successful relationship (Donnellan et al, 2004). Di-
alogue systems could adapt to the user?s personality,
like humans do (Reeves and Nass, 1996; Funder and
Sneed, 1993). This work is a first step toward indi-
vidual adaptation in dialogue systems.
We present non-linear statistical models for rank-
ing utterances based on the Big Five personality
traits. Results show that the models perform sig-
nificantly better than a random baseline, and that
prosodic features are good indicators of extraver-
sion. A qualitative analysis confirms previous find-
ings linking language and personality, while reveal-
ing many new linguistic markers.
2 Experimental method
Our approach can be summarized in five steps: (1)
collect individual corpora; (2) collect personality
85
ratings for each participant; (3) extract relevant fea-
tures from the texts; (4) build statistical models of
the personality ratings based on the features; and (5)
test the learned models on the linguistic outputs of
unseen individuals.
2.1 Spoken language and personality ratings
The data consists of daily-life conversation extracts
of 96 participants wearing an Electronically Acti-
vated Recorder (EAR) for two days, collected by
Mehl et al (in press). To preserve the participants?
privacy, random bits of conversation were recorded,
and only the participants? utterances were tran-
scribed, making it impossible to reconstruct whole
conversations. The corpus contains 97,468 words
and 15,269 utterances. Table 1 shows utterances for
two participants judged as introvert and extravert.
Introvert:
- Yeah you would do kilograms. Yeah I see what you?re saying.
- On Tuesday I have class. I don?t know.
- I don?t know. A16. Yeah, that is kind of cool.
- I don?t know. I just can?t wait to be with you and not have
to do this every night, you know?
- Yeah. You don?t know. Is there a bed in there? Well ok just...
Extravert:
- That?s my first yogurt experience here. Really watery. Why?
- Damn. New game.
- Oh.
- Yeah, but he, they like each other. He likes her.
- They are going to end up breaking up and he?s going to be like.
Table 1: Extracts from the corpus, for participants
rated as extremely introvert and extravert.
Between 5 and 7 independent observers scored
each extract using the Big Five Inventory (John and
Srivastava, 1999). Mehl et al (in press) report
strong inter-observer reliabilities for all dimensions
(r = 0.84, p < 0.01). Average observers? ratings
were used as the scores for our experiments.
2.2 Feature selection
Features are automatically extracted from each ex-
tract (see Table 2). We compute the ratio of words
in each category from the LIWC utility (Pennebaker
et al, 2001), as those features are correlated with the
Big Five dimensions (Pennebaker and King, 1999).
Additional psychological characteristics were com-
puted by averaging word feature counts from the
MRC psycholinguistic database (Coltheart, 1981).
In an attempt to capture initiative-taking in conversa-
tion (Walker and Whittaker, 1990; Furnham, 1990),
we introduce utterance type features using heuristics
on the parse tree to tag each utterance as a command,
prompt, question or assertion. Overall tagging accu-
racy over 100 randomly selected utterances is 88%.
As personality influences speech, we also use Praat
LIWC FEATURES (Pennebaker et al, 2001):
? STANDARD COUNTS:
- Word count (WC), words per sentence (WPS), type/token ratio (Unique),
words captured (Dic), words longer than 6 letters (Sixltr), negations
(Negate), assents (Assent), articles (Article), prepositions (Preps), num-
bers (Number)
- Pronouns (Pronoun): 1st person singular (I), 1st person plural (We), total
1st person (Self), total 2nd person (You), total 3rd person (Other)
? PSYCHOLOGICAL PROCESSES:
- Affective or emotional processes (Affect): positive emotions (Posemo),
positive feelings (Posfeel), optimism and energy (Optim), negative
emotions (Negemo), anxiety or fear (Anx), anger (Anger), sadness (Sad)
- Cognitive Processes (Cogmech): causation (Cause), insight (Insight),
discrepancy (Discrep), inhibition (Inhib), tentative (Tentat), certainty
(Certain)
- Sensory and perceptual processes (Senses): seeing (See), hearing (Hear),
feeling (Feel)
- Social processes (Social): communication (Comm), other references to
people (Othref), friends (Friends), family (Family), humans (Humans)
? RELATIVITY:
- Time (Time), past tense verb (Past), present tense verb (Present),
future tense verb (Future)
- Space (Space): up (Up), down (Down), inclusive (Incl), exclusive (Excl)
- Motion (Motion)
? PERSONAL CONCERNS:
- Occupation (Occup): school (School), work and job (Job),
achievement (Achieve)
- Leisure activity (Leisure): home (Home), sports (Sports), television and
movies (TV), music (Music)
- Money and financial issues (Money)
- Metaphysical issues (Metaph): religion (Relig), death (Death), physical
states and functions (Physcal), body states and symptoms (Body),
sexuality (Sexual), eating and drinking (Eating), sleeping (Sleep),
grooming (Groom)
? OTHER DIMENSIONS:
- Punctuation (Allpct): period (Period), comma (Comma), colon
(Colon), semi-colon (Semic), question (Qmark), exclamation (Exclam),
dash (Dash), quote (Quote), apostrophe (Apostro), parenthesis
(Parenth), other (Otherp)
- Swear words (Swear), nonfluencies (Nonfl), fillers (Fillers)
MRC FEATURES (Coltheart, 1981):
Number of letters (Nlet), phonemes (Nphon), syllables (Nsyl), Kucera-
Francis written frequency (K-F-freq), Kucera-Francis number of categories
(K-F-ncats), Kucera-Francis number of samples (K-F-nsamp), Thorndike-
Lorge written frequency (T-L-freql), Brown verbal frequency (Brown-
freq), familiarity rating (Fam), concreteness rating (Conc), imageability
rating (Imag), meaningfulness Colorado Norms (Meanc), meaningfulness
Paivio Norms (Meanp), age of acquisition (AOA)
UTTERANCE TYPE FEATURES:
Ratio of commands (Command), prompts or back-channels (Prompt),
questions (Question), assertions (Assertion)
PROSODIC FEATURES:
Average, minimum, maximum and standard deviation of the voice?s pitch
in Hz (Pitch-mean, Pitch-min, Pitch-max, Pitch-stddev) and intensity in dB
(Int-mean, Int-min, Int-max, Int-stddev), voiced time (Voiced) and speech
rate (Word-per-sec)
Table 2: Description of all features, with feature la-
bels in brackets.
(Boersma, 2001) to compute prosodic features char-
acterizing the voice?s pitch, intensity, and speech
rate.
2.3 Statistical model
By definition, personality evaluation assesses rela-
tive differences between individuals, e.g. one per-
86
son is described as an extravert because the average
population is not. Thus, we formulate personality
recognition as a ranking problem: given two indi-
viduals? extracts, which shows more extraversion?
Personality models are trained using RankBoost,
a boosting algorithm for ranking, for each Big Five
trait using the observers? ratings of personality (Fre-
und et al, 1998). RankBoost expresses the learned
models as rules, which support the analysis of dif-
ferences in the personality models (see section 3).
Each rule modifies the conversation extract?s rank-
ing score by ? whenever a feature value exceeds ex-
perimentally learned thresholds, e.g. Rule 1 of the
extraversion model in Table 4 increases the score of
an extract by ? = 1.43 if the speech rate is above
0.73 words per second. Models are evaluated by a
ranking error function which reports the percentage
of misordered pairs of conversation extracts.
3 Results
The features characterize many aspects of lan-
guage production: utterance types, content and syn-
tax (LIWC), psycholinguistic statistics (MRC), and
prosody. To evaluate how each feature set con-
tributes to the final result, we trained models with
the full feature set and with each set individually.
Results are summarized in Table 3. The baseline is a
model ranking extracts randomly, producing a rank-
ing error of 0.5 on average. Results are averaged
over a 10 fold cross-validation.
Feature set All LIWC MRC Type Pros
Set size 117 88 14 4 11
Extraversion 0.35? 0.36? 0.45 0.55 0.26?
Emot. stability 0.40 0.41 0.39? 0.43 0.45
Agreeableness 0.31? 0.32? 0.44 0.45 0.54
Conscientious. 0.33? 0.36? 0.41? 0.44 0.55
Intellect 0.38? 0.37? 0.41 0.49 0.44
? statistically significant improvement over the random
ordering baseline (two-tailed paired t-test, p < 0.05)
Table 3: Ranking errors over a 10 fold cross-
validation for different feature sets (Type=utterance
type, Pros=prosody). Best models are in bold.
Paired t-tests show that models of extraversion,
agreeableness, conscientiousness and intellect using
all features are better than the random ordering base-
line (two-tailed, p < 0.05)1. Emotional stability is
the most difficult trait to model, while agreeableness
1We also built models of self-reports of personality, but none
of them significantly outperforms the baseline.
and conscientiousness produce the best results, with
ranking errors of 0.31 and 0.33 respectively. Table 3
shows that LIWC features perform significantly bet-
ter than the baseline for all dimensions but emo-
tional stability, while emotional stability is best pre-
dicted by MRC features. Interestingly, prosodic fea-
tures are very good predictors of extraversion, with
a lower ranking error than the full feature set (0.26),
while utterance type features on their own never out-
perform the baseline.
The RankBoost rules indicate the impact of each
feature on the recognition of a personality trait by
the magnitude of the parameter ? associated with
that feature. Table 4 shows the rules with the most
impact on each best model, with the associated ?
values. The feature labels are in Table 2. For ex-
ample, the model of extraversion confirms previous
findings by associating this trait with a high speech
rate (Rules 1 and 4) and longer conversations (Rule
5). But many new markers emerge: extraverts speak
with a high pitch (Rules 2, 6 and 7), while introverts?
pitch varies a lot (Rules 15, 18 and 20). Agreeable
people use longer words but shorter sentences (Rule
1 and 20), while swear words reduce the agreeable-
ness score (Rules 12, 18 and 19). As expected, con-
scientious people talk a lot about their job (Rule 1),
while unconscientious people swear a lot and speak
loudly (Rules 19 and 20). Our models contain many
additional personality cues which aren?t identified
through a typical correlational analysis.
4 Conclusion
We showed that personality can be recognized auto-
matically in conversation. To our knowledge, this is
the first report of experiments testing trained mod-
els on unseen subjects. There are models for each
dimension that perform significantly better than the
baseline. Combinations of these models may be use-
ful to identify important personality types in dif-
ferent NLP applications, e.g. a combination of
extraversion, emotional stability and intellect indi-
cates leadership, while low intellect, extraversion
and agreeableness are correlated with perceptions of
trustworthiness.
One limitation for applications involving speech
recognition is that recognition errors will introduce
noise in all features except prosodic features, and
prosodic features on their own are only effective in
the extraversion model. However, our data set is rel-
atively small (96 subjects) so we expect that more
87
# Extraversion Emotional stability Agreeableness Conscientiousness Intellect
with prosody ? with MRC ? with all ? with all ? with LIWC ?
1 Word-per-sec? 0.73 1.43 Nlet? 3.28 0.53 Nphon? 2.66 0.56 Occup? 1.21 0.37 Colon? 0.03 0.49
2 Pitch-mean? 194.61 0.41 T-L-freq? 28416 0.25 Tentat? 2.83 0.50 Insight? 2.15 0.36 Insight? 1.75 0.37
3 Voiced? 647.35 0.41 Meanc? 384.17 0.24 Colon? 0.03 0.41 Posfeel? 0.30 0.30 Job? 0.29 0.33
4 Word-per-sec? 2.22 0.36 AOA? 277.36 0.24 Posemo? 2.67 0.32 Int-stddev? 7.83 0.29 Music? 0.18 0.32
5 Voiced? 442.95 0.31 K-F-nsamp? 322 0.22 Voiced? 584 0.32 Nlet? 3.29 0.27 Optim? 0.19 0.24
6 Pitch-max? 599.88 0.30 Meanp? 654.57 0.19 Relig? 0.43 0.27 Comm? 1.20 0.26 Inhib? 0.15 0.24
7 Pitch-mean? 238.99 0.26 Conc? 313.55 0.17 Insight? 2.09 0.25 Nphon? 2.66 0.25 Tentat? 2.23 0.22
8 Int-stddev? 6.96 0.24 K-F-ncats? 14.08 0.15 Prompt? 0.06 0.25 Nphon? 2.67 0.22 Posemo? 2.67 0.19
9 Int-max? 85.87 0.24 Nlet? 3.28 0.14 Comma? 4.60 0.23 Nphon? 2.76 0.20 Future? 0.87 0.17
10 Voiced? 132.35 0.23 Nphon? 2.64 0.13 Money? 0.38 0.20 K-F-nsamp? 329 0.19 Certain? 0.92 0.17
11 Pitch-max? 636.35 -0.05 Fam? 601.98 -0.19 Fam? 601.61 -0.16 Swear? 0.20 -0.18 Affect? 5.07 -0.16
12 Pitch-slope? 312.67 -0.06 Nphon? 2.71 -0.19 Swear? 0.41 -0.18 WPS? 6.25 -0.19 Achieve? 0.62 -0.17
13 Int-min? 54.30 -0.06 AOA? 308.39 -0.23 Anger? 0.92 -0.19 Pitch-mean? 229 -0.20 Othref? 7.67 -0.17
14 Word-per-sec? 1.69 -0.06 Brown-freq? 1884 -0.25 Time? 3.71 -0.20 Othref? 7.64 -0.20 I? 7.11 -0.19
15 Pitch-stddev? 115.49 -0.06 Fam? 601.07 -0.25 Negate? 3.52 -0.20 Humans? 0.83 -0.21 WPS? 5.60 -0.20
16 Pitch-max? 637.27 -0.06 K-F-nsamp? 329 -0.26 Fillers? 0.54 -0.22 Swear? 0.93 -0.21 Social? 10.56 -0.20
17 Pitch-slope? 260.51 -0.12 Imag? 333.50 -0.27 Time? 3.69 -0.23 Swear? 0.17 -0.24 You? 3.57 -0.21
18 Pitch-stddev? 118.10 -0.15 Meanp? 642.81 -0.28 Swear? 0.61 -0.27 Relig? 0.32 -0.27 Incl? 4.30 -0.33
19 Int-stddev? 6.30 -0.18 K-F-ncats? 14.32 -0.35 Swear? 0.45 -0.27 Swear? 0.65 -0.31 Physcal? 1.79 -0.33
20 Pitch-stddev? 119.73 -0.47 Nsyl? 1.17 -0.63 WPS? 6.13 -0.45 Int-max? 86.84 -0.50 Family? 0.08 -0.39
Table 4: Best RankBoost models for each trait. Rows 1-10 represent the rules producing the highest score
increase, while rows 11-20 indicate evidence for the other end of the scale, e.g. introversion.
training data would improve model accuracies and
might also make additional features useful. In fu-
ture work, we plan to integrate these models in a di-
alogue system to adapt the system?s language gener-
ation; we will then be able to test whether the accu-
racies we achieve are sufficient and explore methods
for improving them.
Acknowledgements
Thanks to Matthias Mehl and James Pennebaker for
sharing their data.
References
E. Andre?, M. Klesen, P. Gebhard, S. Allen, and T. Rist. 1999.
Integrating models of personality and emotions into lifelike
characters. In Proc. of the International Workshop on Affect
in Interactions, p. 136?149.
P. Boersma. 2001. Praat, a system for doing phonetics by com-
puter. Glot International, 5(9/10):341?345.
M. Coltheart. 1981. The MRC psycholinguistic database.
Quarterly J. of Experimental Psychology, 33A:497?505.
B. Donnellan, R. D. Conger, and C. M. Bryant. 2004. The Big
Five and enduring marriages. J. of Research in Personality,
38:481?504.
Y. Freund, R. Iyer, R. E. Schapire, and Y. Singer. 1998. An
efficient boosting algorithm for combining preferences. In
Proc. of the 15th ICML, p. 170?178.
D. Funder and C. Sneed. 1993. Behavioral manifestations of
personality: An ecological approach to judgmental accuracy.
J. of Personality and Social Psychology, 64(3):479?490.
A. Furnham, 1990. Handbook of Language and Social Psychol-
ogy, chapter Language and Personality. Winley.
A. J. Gill and J. Oberlander. 2002. Taking care of the linguistic
features of extraversion. In Proc. of the 24th Annual Confer-
ence of the Cognitive Science Society, p. 363?368.
F. Heylighen and J.-M. Dewaele. 2002. Variation in the contex-
tuality of language: an empirical measure. Context in Con-
text, Special issue of Foundations of Science, 7:293?340.
R. Hogan, G. J. Curphy, and J. Hogan. 1994. What we know
about leadership: Effectiveness and personality. American
Psychologist, 49(6):493?504.
O. P. John and S. Srivastava. 1999. The Big Five trait taxon-
omy: History, measurement, and theoretical perspectives. In
L. A. Pervin and O. P. John, editors, Handbook of personality
theory and research. New York: Guilford Press.
J. Liscombe, J. Venditti, and J. Hirschberg. 2003. Classifying
subject ratings of emotional speech using acoustic features.
In Proc. of Eurospeech - Interspeech 2003, p. 725?728.
M. R. Mehl, S. D. Gosling, and J. W. Pennebaker. In press.
Personality in its natural habitat: Manifestations and implicit
folk theories of personality in daily life. J. of Personality and
Social Psychology.
W. T. Norman. 1963. Toward an adequate taxonomy of person-
ality attributes: Replicated factor structure in peer nomina-
tion personality rating. J. of Abnormal and Social Psychol-
ogy, 66:574?583.
S. Nunn. 2005. Preventing the next terrorist attack: The theory
and practice of homeland security information systems. J. of
Homeland Security and Emergency Management, 2(3).
J. W. Pennebaker and L. A. King. 1999. Linguistic styles: Lan-
guage use as an individual difference. J. of Personality and
Social Psychology, 77:1296?1312.
J. W. Pennebaker, L. E. Francis, and R. J. Booth, 2001. LIWC:
Linguistic Inquiry and Word Count.
B. Reeves and C. Nass. 1996. The Media Equation. University
of Chicago Press.
K. R. Scherer. 1979. Personality markers in speech. In K. R.
Scherer and H. Giles, editors, Social markers in speech, p.
147?209. Cambridge University Press.
S. Tucker and S. Whittaker. 2004. Accessing multimodal meet-
ing data: Systems, problems and possibilities. Lecture Notes
in Computer Science, 3361:1?11.
M. Walker and S. Whittaker. 1990. Mixed initiative in dia-
logue: an investigation into discourse segmentation. In Proc.
of the 28th Annual Meeting of the ACL, p. 70?78.
88
Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 496?503,
Prague, Czech Republic, June 2007. c?2007 Association for Computational Linguistics
PERSONAGE: Personality Generation for Dialogue
Franc?ois Mairesse
Department of Computer Science
University of Sheffield
Sheffield, S1 4DP, United Kingdom
F.Mairesse@sheffield.ac.uk
Marilyn Walker
Department of Computer Science
University of Sheffield
Sheffield, S1 4DP, United Kingdom
M.A.Walker@sheffield.ac.uk
Abstract
Over the last fifty years, the ?Big Five?
model of personality traits has become a
standard in psychology, and research has
systematically documented correlations be-
tween a wide range of linguistic variables
and the Big Five traits. A distinct line of
research has explored methods for automati-
cally generating language that varies along
personality dimensions. We present PER-
SONAGE (PERSONAlity GEnerator), the
first highly parametrizable language gener-
ator for extraversion, an important aspect
of personality. We evaluate two personal-
ity generation methods: (1) direct genera-
tion with particular parameter settings sug-
gested by the psychology literature; and (2)
overgeneration and selection using statistical
models trained from judge?s ratings. Results
show that both methods reliably generate ut-
terances that vary along the extraversion di-
mension, according to human judges.
1 Introduction
Over the last fifty years, the ?Big Five? model of per-
sonality traits has become a standard in psychology
(extraversion, neuroticism, agreeableness, conscien-
tiousness, and openness to experience), and research
has systematically documented correlations between
a wide range of linguistic variables and the Big Five
traits (Mehl et al, 2006; Norman, 1963; Oberlan-
der and Gill, 2006; Pennebaker and King, 1999). A
distinct line of research has explored methods for
automatically generating language that varies along
personality dimensions, targeting applications such
as computer gaming and educational virtual worlds
(Andre? et al, 2000; Isard et al, 2006; Loyall and
Bates, 1997; Piwek, 2003; Walker et al, 1997) inter
alia. Other work suggests a clear utility for gener-
ating language manifesting personality (Reeves and
Nass, 1996). However, to date, (1) research in gener-
ation has not systematically exploited the psycholin-
guistic findings; and (2) there has been little evalua-
tion showing that automatic generators can produce
language with recognizable personality variation.
Alt Realization Extra
5 Err... it seems to me that Le Marais isn?t as bad
as the others.
1.83
4 Right, I mean, Le Marais is the only restaurant
that is any good.
2.83
8 Ok, I mean, Le Marais is a quite french, kosher
and steak house place, you know and the atmo-
sphere isn?t nasty, it has nice atmosphere. It has
friendly service. It seems to me that the service
is nice. It isn?t as bad as the others, is it?
5.17
9 Well, it seems to me that I am sure you would
like Le Marais. It has good food, the food is
sort of rather tasty, the ambience is nice, the at-
mosphere isn?t sort of nasty, it features rather
friendly servers and its price is around 44 dol-
lars.
5.83
3 I am sure you would like Le Marais, you know.
The atmosphere is acceptable, the servers are
nice and it?s a french, kosher and steak house
place. Actually, the food is good, even if its price
is 44 dollars.
6.00
10 It seems to me that Le Marais isn?t as bad as
the others. It?s a french, kosher and steak house
place. It has friendly servers, you know but it?s
somewhat expensive, you know!
6.17
2 Basically, actually, I am sure you would like Le
Marais. It features friendly service and accept-
able atmosphere and it?s a french, kosher and
steak house place. Even if its price is 44 dollars,
it just has really good food, nice food.
6.17
Table 1: Recommendations along the extraver-
sion dimension, with the average extraversion rating
from human judges on a scale from 1 to 7. Alt-2 and
3 are from the extravert set, Alt-4 and 5 are from the
introvert set, and others were randomly generated.
Our aim is to produce a highly parameterizable
generator whose outputs vary along personality di-
mensions. We hypothesize that such language can
496
be generated by varying parameters suggested by
psycholinguistic research. So, we must first map
the psychological findings to parameters of a natural
language generator (NLG). However, this presents
several challenges: (1) The findings result from
studies of genres of language, such as stream-of-
consciousness essays (Pennebaker and King, 1999),
and informal conversations (Mehl et al, 2006), and
thus may not apply to fixed content domains used in
NLG; (2) Most findings are based on self-reports of
personality, but we want to affect observer?s percep-
tions; (3) The findings consist of weak but signifi-
cant correlations, so that individual parameters may
not have a strong enough effect to produce recog-
nizable variation within a single utterance; (4) There
are many possible mappings of the findings to gen-
eration parameters; and (5) It is unclear whether
only specific speech-act types manifest personality
or whether all utterances do.
Thus this paper makes several contributions.
First, Section 2 summarizes the linguistic reflexes of
extraversion, organized by the modules in a standard
NLG system, and propose a mapping from these
findings to NLG parameters. To our knowledge this
is the first attempt to put forward a systematic frame-
work for generating language manifesting personal-
ity. We start with the extraversion dimension be-
cause it is an important personality factor, with many
associated linguistic variables. We believe that our
framework will generalize to the other dimensions
in the Big Five model. Second, Sections 3 and 4
describe the PERSONAGE (PERSONAlity GEner-
ator) generator and its 29 parameters. Table 1 shows
examples generated by PERSONAGE for recom-
mendations in the restaurant domain, along with
human extraversion judgments. Third, Sections 5
and 6 describe experiments evaluating two genera-
tion methods. We first show that (1) the parame-
ters generate utterances that vary significantly on the
extraversion dimension, according to human judg-
ments; and (2) we can train a statistical model that
matches human performance in assigning extraver-
sion ratings to generation outputs produced with ran-
dom parameter settings. Section 7 sums up and dis-
cusses future work.
2 Psycholinguistic Findings and
PERSONAGE Parameters
We hypothesize that personality can be made man-
ifest in evaluative speech acts in any dialogue do-
main, i.e. utterances responding to requests to REC-
OMMEND or COMPARE domain entities, such as
restaurants or movies (Isard et al, 2006; Stent et al,
2004). Thus, we start with the SPaRKy genera-
tor1, which produces evaluative recommendations
and comparisons in the restaurant domain, for a
database of restaurants in New York City. There
are eight attributes for each restaurant: the name and
address, scalar attributes for price, food quality, at-
mosphere, and service and categorical attributes for
neighborhood and type of cuisine. SPaRKy is based
on the standard NLG architecture (Reiter and Dale,
2000), and consists of the following modules:
1. Content Planning: refine communicative goals, select and
structure content;
2. Sentence planning; choose linguistic resources (lexicon,
syntax) to achieve goals;
3. Realization: use grammar (syntax, morphology) to gen-
erate surface utterances.
Given the NLG architecture, speech-act types,
and domain, the first step then is to summarise psy-
chological findings on extraversion and map them
to this architecture. The column NLG modules of
Table 2 gives the proposed mapping. The first row
specifies findings for the content planning module
and the other rows are aspects of sentence planning.
Realization is achieved with the RealPro surface re-
alizer (Lavoie and Rambow, 1997). An examina-
tion of the introvert and extravert findings in Table 2
highlights the challenges above, i.e. exploiting these
findings in a systematic way within a parameteriz-
able NLG system.
The column Parameter in Table 2 proposes pa-
rameters (explained in Sections 3 and 4) that are ma-
nipulated within each module to realize the findings
in the other columns. Each parameter varies con-
tinuously from 0 to 1, where end points are meant
to produce extreme but plausible output. Given the
challenges above, it is important to note that these
parameters represent hypotheses about how a find-
ing can be mapped into any NLG system. The Intro
and Extra columns at the right hand side of the Pa-
rameter column indicate a range of settings for this
parameter, suggested by the psychological findings,
to produce introverted vs. extraverted language.
SPaRKy produces content plans for restaurant
recommendations and comparisons that are modi-
fied by the parameters. The sample content plan
for a recommendation in Figure 1 corresponds to
the outputs in Table 1. While Table 1 shows that
PERSONAGE?s parameters have various pragmatic
effects, they preserve the meaning at the Gricean in-
tention level (dialogue goal). Each content plan con-
tains a claim (nucleus) about the overall quality of
1Available for download from
www.dcs.shef.ac.uk/cogsys/sparky.html
497
NLG modules Introvert findings Extravert findings Parameter Intro Extra
Content Single topic Many topics VERBOSITY low high
selection Strict selection Think out loud* RESTATEMENTS low high
and REPETITIONS low low
structure Problem talk, Pleasure talk, agreement, CONTENT POLARITY low high
dissatisfaction compliment REPETITIONS POLARITY low high
CLAIM POLARITY low high
CONCESSIONS avg avg
CONCESSIONS POLARITY low high
POLARISATION low high
POSITIVE CONTENT FIRST low high
Syntactic Few self-references Many self-references SELF-REFERENCES low high
templates Elaborated constructions Simple constructions* CLAIM COMPLEXITY high low
selection Many articles Few articles
Aggregation Many words per Few words per RELATIVE CLAUSES high low
Operations sentence/clause sentence/clause WITH CUE WORD high low
CONJUNCTION low high
Many unfilled pauses Few unfilled pauses PERIOD high low
...
Pragmatic
transformations
Many nouns, adjectives, prepo-
sitions (explicit)
Many verbs, adverbs, pronouns
(implicit)
SUBJECT IMPLICITNESS low high
Many negations Few negations NEGATION INSERTION high low
Many tentative words Few tentative words DOWNTONER HEDGES:
?SORT OF, SOMEWHAT, QUITE, RATHER,
ERR, I THINK THAT, IT SEEMS THAT, IT
SEEMS TO ME THAT, I MEAN
high low
?AROUND avg avg
Formal Informal ?KIND OF, LIKE low high
ACKNOWLEDGMENTS:
?YEAH low high
?RIGHT, OK, I SEE, WELL high low
Realism Exaggeration* EMPHASIZER HEDGES:
?REALLY, BASICALLY, ACTUALLY, JUST
HAVE, JUST IS, EXCLAMATION low high
?YOU KNOW low high
No politeness form Positive face redressment* TAG QUESTION INSERTION low high
Lower word count Higher word count HEDGE VARIATION low avg
HEDGE REPETITION low low
Lexical Rich Poor LEXICON FREQUENCY low high
choice Few positive emotion words Many positive emotion words see polarity parameters
Many negative emotion words Few negative emotion words see polarity parameters
Table 2: Summary of language cues for extraversion, based on Dewaele and Furnham (1999); Furnham
(1990); Mehl et al (2006); Oberlander and Gill (2006); Pennebaker and King (1999), as well as PERSON-
AGE?s corresponding generation parameters. Asterisks indicate hypotheses, rather than results. For details
on aggregation parameters, see Section 4.2.
Relations: JUSTIFY (nuc:1, sat:2); JUSTIFY (nuc:1, sat:3);
JUSTIFY (nuc:1, sat:4); JUSTIFY (nuc:1, sat:5);
JUSTIFY (nuc:1, sat:6)
Content: 1. assert(best (Le Marais))
2. assert(is (Le Marais, cuisine (French)))
3. assert(has (Le Marais, food-quality (good)))
4. assert(has (Le Marais, service (good)))
5. assert(has (Le Marais, decor (decent)))
6. assert(is (Le Marais, price (44 dollars)))
Figure 1: A content plan for a recommendation.
the selected restaurant(s), supported by a set of satel-
lite content items describing their attributes. See Ta-
ble 1. Claims can be expressed in different ways,
such as RESTAURANT NAME is the best, while
the attribute satellites follow the pattern RESTAU-
RANT NAME has MODIFIER ATTRIBUTE NAME,
as in Le Marais has good food. Recommendations
are characterized by a JUSTIFY rhetorical relation
associating the claim with all other content items,
which are linked together through an INFER relation.
In comparisons, the attributes of multiple restaurants
are compared using a CONTRAST relation. An op-
tional claim about the quality of all restaurants can
also be expressed as the nucleus of an ELABORATE
relation, with the rest of the content plan tree as a
satellite.
3 Content Planning
Content planning selects and structures the content
to be communicated. Table 2 specifies 10 param-
eters hypothesized to affect this process which are
explained below.
Content size: Extraverts are more talkative than
introverts (Furnham, 1990; Pennebaker and King,
1999), although it is not clear whether they actu-
ally produce more content, or are just redundant and
wordy. Thus various parameters relate to the amount
and type of content produced. The VERBOSITY pa-
rameter controls the number of content items se-
lected from the content plan. For example, Alt-5 in
Table 1 is terse, while Alt-2 expresses all the items in
the content plan. The REPETITION parameter adds
an exact repetition: the content item is duplicated
and linked to the original content by a RESTATE
498
rhetorical relation. In a similar way, the RESTATE-
MENT parameter adds paraphrases of content items
to the plan, that are obtained from the initial hand-
crafted generation dictionary (see Section 4.1) and
by automatically substituting content words with the
most frequent WordNet synonym (see Section 4.4).
Alt-9 in Table 1 contains restatements for the food
quality and the atmosphere attributes.
Polarity: Extraverts tend to be more positive; in-
troverts are characterized as engaging in more ?prob-
lem talk? and expressions of dissatisfaction (Thorne,
1987). To control for polarity, content items are
defined as positive or negative based on the scalar
value of the corresponding attribute. The type of cui-
sine and neighborhood attributes have neutral polar-
ity. There are multiple parameters associated with
polarity. The CONTENT POLARITY parameter con-
trols whether the content is mostly negative (e.g.
X has mediocre food), neutral (e.g. X is a Thai
restaurant), or positive. From the filtered set of
content items, the POLARISATION parameter deter-
mines whether the final content includes items with
extreme scalar values (e.g. X has fantastic staff).
In addition, polarity can also be implied more sub-
tly through rhetorical structure. The CONCESSIONS
parameter controls how negative and positive infor-
mation is presented, i.e. whether two content items
with different polarity are presented objectively, or if
one is foregrounded and the other backgrounded. If
two opposed content items are selected for a con-
cession, a CONCESS rhetorical relation is inserted
between them. While the CONCESSIONS param-
eter captures the tendency to put information into
perspective, the CONCESSION POLARITY parameter
controls whether the positive or the negative content
is concessed, i.e. marked as the satellite of the CON-
CESS relation. The last sentence of Alt-3 in Table 1
illustrates a positive concession, in which the good
food quality is put before the high price.
Content ordering: Although extraverts use more
positive language (Pennebaker and King, 1999;
Thorne, 1987), it is unclear how they position the
positive content within their utterances. Addition-
ally, the position of the claim affects the persuasive-
ness of an argument (Carenini and Moore, 2000):
starting with the claim facilitates the hearer?s under-
standing, while finishing with the claim is more ef-
fective if the hearer disagrees. The POSITIVE CON-
TENT FIRST parameter therefore controls whether
positive content items ? including the claim ? appear
first or last, and the order in which the content items
are aggregated. However, some operations can still
impose a specific ordering (e.g. BECAUSE cue word
to realize the JUSTIFY relation, see Section 4.2).
4 Sentence Planning
Sentence planning chooses the linguistic resources
from the lexicon and the syntactic and discourse
structures to achieve the communicative goals spec-
ified in the input content plan. Table 2 specifies four
sets of findings and parameters for different aspects
of sentence planning discussed below.
4.1 Syntactic template selection
PERSONAGE?s input generation dictionary is made
of 27 Deep Syntactic Structures (DSyntS): 9 for
the recommendation claim, 12 for the comparison
claim, and one per attribute. Selecting a DSyntS re-
quires assigning it automatically to a point in a three
dimensional space described below. All parameter
values are normalized over all the DSyntS, so the
DSyntS closest to the target value can be computed.
Syntactic complexity: Furnham (1990) suggests
that introverts produce more complex constructions:
the CLAIM COMPLEXITY parameter controls the
depth of the syntactic structure chosen to represent
the claim, e.g. the claim X is the best is rated as less
complex than X is one of my favorite restaurants.
Self-references: Extraverts make more self-
references than introverts (Pennebaker and King,
1999). The SELF-REFERENCE parameter controls
whether the claim is made in the first person, based
on the speaker?s own experience, or whether the
claim is reported as objective or information ob-
tained elsewhere. The self-reference value is ob-
tained from the syntactic structure by counting the
number of first person pronouns. For example, the
claim of Alt-2 in Table 1, i.e. I am sure you would
like Le Marais, will be rated higher than Le Marais
isn?t as bad as the others in Alt-5.
Polarity: While polarity can be expressed by con-
tent selection and structure, it can also be directly
associated with the DSyntS. The CLAIM POLARITY
parameter determines the DSyntS selected to realize
the claim. DSyntS are manually annotated for po-
larity. For example, Alt-4?s claim in Table 1, i.e. Le
Marais is the only restaurant that is any good, has a
lower polarity than Alt-2.
4.2 Aggregation operations
SPaRKy aggregation operations are used (See Stent
et al (2004)), with additional operations for conces-
sions and restatements. See Table 2. The probabil-
ity of the operations biases the production of com-
plex clauses, periods and formal cue words for in-
troverts, to express their preference for complex syn-
499
tactic constructions, long pauses and rich vocabulary
(Furnham, 1990). Thus, the introvert parameters fa-
vor operations such as RELATIVE CLAUSE for the
INFER relation, PERIOD HOWEVER CUE WORD for
CONTRAST, and ALTHOUGH ADVERBIAL CLAUSE
for CONCESS, that we hypothesize to result in more
formal language. Extravert aggregation produces
longer sentences with simpler constructions and in-
formal cue words. Thus extravert utterances tend to
use operations such as a CONJUNCTION to realize
the INFER and RESTATE relations, and the EVEN IF
ADVERBIAL CLAUSE for CONCESS relations.
4.3 Pragmatic transformations
This section describes the insertion of markers in the
DSyntS to produce various pragmatic effects.
Hedges: Hedges correlate with introversion (Pen-
nebaker and King, 1999) and affect politeness
(Brown and Levinson, 1987). Thus there are param-
eters for inserting a wide range of hedges, both af-
fective and epistemic, such as kind of, sort of, quite,
rather, somewhat, like, around, err, I think that, it
seems that, it seems to me that, and I mean. Alt-5 in
Table 1 shows hedges err and it seems to me that.
To model extraverts use of more social language,
agreement and backchannel behavior (Dewaele and
Furnham, 1999; Pennebaker and King, 1999), we
use informal acknowledgments such as yeah, right,
ok. Acknowledgments that may affect introversion
are I see, expressing self-reference and cognitive
load, and the well cue word implying reservation
from the speaker (see Alt-9).
To model social connection and emotion we
added mechanisms for inserting emphasizers such as
you know, basically, actually, just have, just is, and
exclamations. Alt-3 in Table 1 shows the insertion
of you know and actually.
Although similar hedges can be grouped together,
each hedge has a unique pragmatic effect. For ex-
ample, you know implies positive-face redressment,
while actually doesn?t. A parameter for each hedge
controls the likelihood of its selection.
To control the general level of hedging, a HEDGE
VARIATION parameter defines how many different
hedges are selected (maximum of 5), while the fre-
quency of an individual hedge is controlled by a
HEDGE REPETITION parameter, up to a maximum
of 2 identical hedges per utterance.
The syntactic structure of hedges are defined as
well as constraints on their insertion point in the ut-
terance?s syntactic structure. Each time a hedge is
selected, it is randomly inserted at one of the inser-
tion points respecting the constraints, until the spec-
ified frequency is reached. For example, a constraint
on the hedge kind of is that it modifies adjectives.
Tag questions: Tag questions are also polite-
ness markers (Brown and Levinson, 1987). They
redress the hearer?s positive face by claiming com-
mon ground. A TAG QUESTION INSERTION param-
eter leads to negating the auxiliary of the verb and
pronominalizing the subject, e.g. X has great food
results in the insertion of doesn?t it?, as in Alt-8.
Negations: Introverts use significantly more
negations (Pennebaker and King, 1999). Although
the content parameters select more negative polarity
content items for introvert utterances, we also ma-
nipulate negations, while keeping the content con-
stant, by converting adjectives to the negative of
their antonyms, e.g. the atmosphere is nice was
transformed to not nasty in Alt-9 in Table 1.
Subject implicitness: Heylighen and Dewaele
(2002) found that extraverts use more implicit lan-
guage than introverts. To control the level of implic-
itness, the SUBJECT IMPLICITNESS parameter deter-
mines whether predicates describing restaurant at-
tributes are expressed with the restaurant in the sub-
ject, or with the attribute itself (e.g., it has good food
vs. the food is tasty in Alt-9).
4.4 Lexical choice
Introverts use a richer vocabulary (Dewaele and
Furnham, 1999), so the LEXICON FREQUENCY pa-
rameter selects lexical items by their normalized fre-
quency in the British National Corpus. WordNet
synonyms are used to obtain a pool of synonyms, as
well as adjectives extracted from a corpus of restau-
rant reviews for all levels of polarity (e.g. the ad-
jective tasty in Alt-9 is a high polarity modifier of
the food attribute). Synonyms are manually checked
to make sure they are interchangeable. For example,
the content item expressed originally as it has decent
service is transformed to it features friendly service
in Alt-2, and to the servers are nice in Alt-3.
5 Experimental Method and Hypotheses
Our primary hypothesis is that language generated
by varying parameters suggested by psycholinguis-
tic research can be recognized as extravert or in-
trovert. To test this hypothesis, three expert judges
evaluated a set of generated utterances as if they had
been uttered by a friend responding in a dialogue to a
request to recommend restaurants. These utterances
had been generated to systematically manipulate ex-
traversion/introversion parameters.
The judges rated each utterance for perceived ex-
traversion, by answering the two questions measur-
500
ing that trait from the Ten-Item Personality Inven-
tory, as this instrument was shown to be psychome-
trically superior to a ?single item per trait? question-
naire (Gosling et al, 2003). The answers are aver-
aged to produce an extraversion rating ranging from
1 (highly introvert) to 7 (highly extravert). Because
it was unclear whether the generation parameters in
Table 2 would produce natural sounding utterances,
the judges also evaluated the naturalness of each ut-
terance on the same scale. The judges rated 240 ut-
terances, grouped into 20 sets of 12 utterances gen-
erated from the same content plan. They rated one
randomly ordered set at a time, but viewed all 12
utterances in that set before rating them. The ut-
terances were generated to meet two experimental
goals. First, to test the direct control of the per-
ception of extraversion. 2 introvert utterances and
2 extravert utterances were generated for each con-
tent plan (80 in total) using the parameter values
in Table 2. Multiple outputs were generated with
both parameter settings normally distributed with a
15% standard deviation. Second, 8 utterances for
each content plan (160 in total) were generated with
random parameter values. These random utterances
make it possible to: (1) improve PERSONAGE?s di-
rect output by calibrating its parameters more pre-
cisely; and (2) build a statistical model that selects
utterances matching input personality values after an
overgeneration phase (see Section 6.2). The inter-
rater agreement for extraversion between the judges
over all 240 utterances (average Pearson?s correla-
tion of 0.57) shows that the magnitude of the differ-
ences of perception between judges is almost con-
stant (? = .037). A low agreement can yield a high
correlation (e.g. if all values differ by a constant
factor), so we also compute the intraclass correla-
tion coefficient r based on a two-way random effect
model. We obtain a r of 0.79, which is significant
at the p < .001 level (reliability of average mea-
sures, identical to Cronbach?s alpha). This is com-
parable to the agreement of judgments of personality
in Mehl et al (2006) (mean r = 0.84).
6 Experimental Results
6.1 Hypothesized parameter settings
Table 1 provides examples of PERSONAGE?s out-
put and extraversion ratings. To assess whether
PERSONAGE generates language that can be rec-
ognized as introvert and extravert, we did a indepen-
dent sample t-test between the average ratings of the
40 introvert and 40 extravert utterances (parameters
with 15% standard deviation as in Table 2). Table 3
Rating Introvert Extravert Random
Extraversion 2.96 5.98 5.02
Naturalness 4.93 5.78 4.51
Table 3: Average extraversion and naturalness rat-
ings for the utterances generated with introvert, ex-
travert, and random parameters.
shows that introvert utterances have an average rat-
ing of 2.96 out of 7 while extravert utterances have
an average rating of 5.98. These ratings are signifi-
cantly different at the p < .001 level (two-tailed).
In addition, if we divide the data into two equal-
width bins around the neutral extravert rating (4 out
of 7), then PERSONAGE?s utterance ratings fall in
the bin predicted by the parameter set 89.2% of the
time. Extravert utterance are also slightly more nat-
ural than the introvert ones (p < .001).
Table 3 also shows that the 160 random parame-
ter utterances produce an average extraversion rating
of 5.02, both significantly higher than the introvert
set and lower than the extravert set (p < .001). In-
terestingly, the random utterances, which may com-
bine linguistic variables associated with both intro-
verts and extraverts, are less natural than the intro-
vert (p = .059) and extravert sets (p < .001).
6.2 Statistical models evaluation
We also investigate a second approach: overgener-
ation with random parameter settings, followed by
ranking via a statistical model trained on the judges?
feedback. This approach supports generating utter-
ances for any input extraversion value, as well as de-
termining which parameters affect the judges? per-
ception.
We model perceived personality ratings (1 . . . 7)
with regression models from the Weka toolbox (Wit-
ten and Frank, 2005). We used the full dataset of
160 averaged ratings for the random parameter utter-
ances. Each utterance was associated with a feature
vector with the generation decisions for each param-
eter in Section 2. To reduce data sparsity, we select
features that correlate significantly with the ratings
(p < .10) with a coefficient higher than 0.1.
Regression models are evaluated using the mean
absolute error and the correlation between the pre-
dicted score and the actual average rating. Table 4
shows the mean absolute error on a scale from 1 to
7 over ten 10-fold cross-validations for the 4 best
regression models: Linear Regression (LR), M5?
model tree (M5), and Support Vector Machines (i.e.
SMOreg) with linear kernels (SMO1) and radial-
501
basis function kernels (SMOr). All models signif-
icantly outperform the baseline (0.83 mean absolute
error, p < .05), but surprisingly the linear model
performs the best with a mean absolute error of 0.65.
The best model produces a correlation coefficient of
0.59 with the judges? ratings, which is higher than
the correlations between pairs of judges, suggesting
that the model performs as well as a human judge.
Metric LR M5 SMO1 SMOr
Absolute error 0.65 0.66 0.72 0.70
Correlation 0.59 0.56 0.54 0.57
Table 4: Mean absolute regression errors (scale from
1 to 7) and correlation coefficients over ten 10-fold
cross-validations, for 4 models: Linear Regression
(LR), M5? model tree (M5), Support Vector Ma-
chines with linear kernels (SMO1) and radial-basis
function kernels (SMOr). All models significantly
outperform the mean baseline (0.83 error, p < .05).
The M5? regression tree in Figure 2 assigns a rat-
ing given the features. Verbosity plays the most im-
portant role: utterances with 4 or more content items
are modeled as more extravert. Given a low ver-
bosity, lexical frequency and restatements determine
the extraversion level, e.g. utterances with less than
4 content items and infrequent words are perceived
as very introverted (rating of 2.69 out of 7). For
verbose utterances, the you know hedge indicates
extraversion, as well as concessions, restatements,
self-references, and positive content. Although rel-
atively simple, these models are useful for identify-
ing new personality markers, as well as calibrating
parameters in the direct generation model.
7 Discussion and Conclusions
We present and evaluate PERSONAGE, a parame-
terizable generator that produces outputs that vary
along the extraversion personality dimension. This
paper makes four contributions:
1. We present a systematic review of psycholinguistic find-
ings, organized by the NLG reference architecture;
2. We propose a mapping from these findings to generation
parameters for each NLG module and a real-time imple-
mentation of a generator using these parameters2. To our
knowledge this is the first attempt to put forward a sys-
tematic framework for generating language that manifests
personality;
3. We present an evaluation experiment showing that we can
control the parameters to produce recognizable linguis-
tic variation along the extraversion personality dimen-
sion. Thus, we show that the weak correlations reported
2An online demo is available at
www.dcs.shef.ac.uk/cogsys/personage.html
in other genres of language, and for self-reports rather
than observers, carry over to the production of single eval-
uative utterances with recognizable personality in a re-
stricted domain;
4. We present the results of a training experiment showing
that given an output, we can train a model that matches
human performance in assigning an extraversion rating to
that output.
Some of the challenges discussed in the introduc-
tion remain. We have shown that evaluative utter-
ances in the restaurant domain can manifest person-
ality, but more research is needed on which speech
acts recognisably manifest personality in a restricted
domain. We also showed that the mapping we hy-
pothesised of findings to generation parameters was
effective, but there may be additional parameters
that the psycholinguistic findings could be mapped
to.
Our work was partially inspired by the ICONO-
CLAST and PAULINE parameterizable generators
(Bouayad-Agha et al, 2000; Hovy, 1988), which
vary the style, rather than the personality, of the gen-
erated texts. Walker et al (1997) describe a gen-
erator intended to affect perceptions of personality,
based on Brown and Levinson?s theory of polite-
ness (Brown and Levinson, 1987), that uses some
of the linguistic constructions implemented here,
such as tag questions and hedges, but it was never
evaluated. Research by Andre? et al (2000); Piwek
(2003) uses personality variables to affect the lin-
guistic behaviour of conversational agents, but they
did not systematically manipulate parameters, and
their generators were not evaluated. Reeves and
Nass (1996) demonstrate that manipulations of per-
sonality affect many aspects of user?s perceptions,
but their experiments use handcrafted utterances,
rather than generated utterances. Cassell and Bick-
more (2003) show that extraverts prefer systems uti-
lizing discourse plans that include small talk. Paiva
and Evans? trainable generator (2005) produces out-
puts that correspond to a set of linguistic variables
measured in a corpus of target texts. Their method
is similar to our statistical method using regression
trees, but provides direct control. The method re-
ported in Mairesse and Walker (2005) for training
individualized sentence planners ranks the outputs
produced by an overgeneration phase, rather than di-
rectly predicting a scalar value, as we do here. The
closest work to ours is probably Isard et al?s CRAG-
2 system (2006), which overgenerates and ranks us-
ing ngram language models trained on a corpus la-
belled for all Big Five personality dimensions. How-
ever, CRAG-2 has no explicit parameter control, and
it has yet to be evaluated.
502
Max BNC Frequency Restatements
Verbosity
> 0.02
2.69 3.52 4.47
4.12
3.26
> 0.1
> 2.5
> 0.64<= 0.64
3.74
Max BNC Frequency
Concessions
> 0.87
Self?references Restatements
> 0.5 > 0.5
5.33 Verbosity
5.08 5.53
> 5.5
5.85
> 0.5
5.00
> 0.5
.
. <= 0.02
<= 2.5
<= 0.5
<= 0.5
<= 0.5
<= 0.5
<= 0.5
<= 5.5
> 0.5
> 3.5
<= 0.87
> 0.5<= 0.1
Max BNC Frequency
Verbosity
Verbosity
> 4.5<= 4.5
.
Infer aggregation:
       Period
<= 0.5
4.52
<= 3.5
Hedge: ?you know?
5.93Content
5.54 5.78
Polarity
Figure 2: M5? regression tree. The output ranges from 1 to 7, where 7 means strongly extravert.
In future work, we hope to directly compare the
direct generation method of Section 6.1 with the
overgenerate and rank method of Section 6.2, and to
use these results to refine PERSONAGE?s parame-
ter settings. We also hope to extend PERSONAGE?s
generation capabilities to other Big Five traits, iden-
tify additional features to improve the model?s per-
formance, and evaluate the effect of personality vari-
ation on user satisfaction in various applications.
References
E. Andre?, T. Rist, S. van Mulken, M. Klesen, and S. Baldes.
2000. The automated design of believable dialogues for
animated presentation teams. In Embodied conversational
agents, p. 220?255. MIT Press, Cambridge, MA.
N. Bouayad-Agha, D. Scott, and R. Power. 2000. Integrating
content and style in documents: a case study of patient in-
formation leaflets. Information Design Journal, 9:161?176.
P. Brown and S. Levinson. 1987. Politeness: Some universals
in language usage. Cambridge University Press.
G. Carenini and J. D. Moore. 2000. A strategy for generating
evaluative arguments. In Proc. of International Conference
on Natural Language Generation, p. 47?54.
J. Cassell and T. Bickmore. 2003. Negotiated collusion: Model-
ing social language and its relationship effects in intelligent
agents. User Modeling and User-Adapted Interaction, 13
(1-2):89?132.
J-M. Dewaele and A. Furnham. 1999. Extraversion: the unloved
variable in applied linguistic research. Language Learning,
49(3):509?544.
A. Furnham. 1990. Language and personality. In Handbook of
Language and Social Psychology. Winley.
S. D. Gosling, P. J. Rentfrow, and W. B. Swann Jr. 2003. A very
brief measure of the big five personality domains. Journal of
Research in Personality, 37:504?528.
F. Heylighen and J-M. Dewaele. 2002. Variation in the con-
textuality of language: an empirical measure. Context in
Context, Foundations of Science, 7(3):293?340.
E. Hovy. 1988. Generating Natural Language under Pragmatic
Constraints. Lawrence Erlbaum Associates.
A. Isard, C. Brockmann, and J. Oberlander. 2006. Individuality
and alignment in generated dialogues. In Proc. of INLG.
B. Lavoie and O. Rambow. 1997. A fast and portable realizer
for text generation systems. In Proc. of ANLP.
A. Loyall and J. Bates. 1997. Personality-rich believable agents
that use language. In Proc. of the First International Confer-
ence on Autonomous Agents, p. 106?113.
F. Mairesse and M. Walker. 2005. Learning to personalize spo-
ken generation for dialogue systems. In Proc. of the Inter-
speech - Eurospeech, p. 1881?1884.
M. Mehl, S. Gosling, and J. Pennebaker. 2006. Personality in
its natural habitat: Manifestations and implicit folk theories
of personality in daily life. Journal of Personality and Social
Psychology, 90:862?877.
W. T. Norman. 1963. Toward an adequate taxonomy of per-
sonality attributes: Replicated factor structure in peer nom-
ination personality rating. Journal of Abnormal and Social
Psychology, 66:574?583.
J. Oberlander and A. Gill. 2006. Language with character: A
stratified corpus comparison of individual differences in e-
mail communication. Discourse Processes, 42:239?270.
D. Paiva and R. Evans. 2005. Empirically-based control of nat-
ural language generation. In Proc. of ACL.
J. W. Pennebaker and L. A. King. 1999. Linguistic styles: Lan-
guage use as an individual difference. Journal of Personality
and Social Psychology, 77:1296?1312.
P. Piwek. 2003. A flexible pragmatics-driven language genera-
tor for animated agents. In Proc. of EACL.
B. Reeves and C. Nass. 1996. The Media Equation. University
of Chicago Press.
E. Reiter and R. Dale. 2000. Building Natural Language Gen-
eration Systems. Cambridge University Press.
A. Stent, R. Prasad, and M. Walker. 2004. Trainable sentence
planning for complex information presentation in spoken di-
alog systems. In Proc. of ACL.
A. Thorne. 1987. The press of personality: A study of conver-
sations between introverts and extraverts. Journal of Person-
ality and Social Psychology, 53:718?726.
M. Walker, J. Cahn, and S. Whittaker. 1997. Improvising lin-
guistic style: Social and affective bases for agent personality.
In Proc. of the Conference on Autonomous Agents.
I. H. Witten and E. Frank. 2005. Data Mining: Practical ma-
chine learning tools and techniques. Morgan Kaufmann.
503
Proceedings of ACL-08: HLT, pages 165?173,
Columbus, Ohio, USA, June 2008. c?2008 Association for Computational Linguistics
Trainable Generation of Big-Five Personality Styles
through Data-driven Parameter Estimation
Franc?ois Mairesse
Cambridge University Engineering Department
Trumpington Street
Cambridge, CB2 1PZ, United Kingdom
farm2@eng.cam.ac.uk
Marilyn Walker
Department of Computer Science
University of Sheffield
Sheffield, S1 4DP, United Kingdom
lynwalker@gmail.com
Abstract
Previous work on statistical language gen-
eration has primarily focused on grammat-
icality and naturalness, scoring generation
possibilities according to a language model
or user feedback. More recent work has
investigated data-driven techniques for con-
trolling linguistic style without overgenera-
tion, by reproducing variation dimensions ex-
tracted from corpora. Another line of work
has produced handcrafted rule-based systems
to control specific stylistic dimensions, such
as politeness and personality. This paper
describes a novel approach that automati-
cally learns to produce recognisable varia-
tion along a meaningful stylistic dimension?
personality?without the computational cost
incurred by overgeneration techniques. We
present the first evaluation of a data-driven
generation method that projects multiple per-
sonality traits simultaneously and on a contin-
uous scale. We compare our performance to a
rule-based generator in the same domain.
1 Introduction
Over the last 20 years, statistical language models
(SLMs) have been used successfully in many tasks
in natural language processing, and the data avail-
able for modeling has steadily grown (Lapata and
Keller, 2005). Langkilde and Knight (1998) first
applied SLMs to statistical natural language genera-
tion (SNLG), showing that high quality paraphrases
can be generated from an underspecified representa-
tion of meaning, by first applying a very undercon-
strained, rule-based overgeneration phase, whose
outputs are then ranked by an SLM scoring phase.
Since then, research in SNLG has explored a range
of models for both dialogue and text generation.
One line of work has primarily focused on gram-
maticality and naturalness, scoring the overgener-
ation phase with a SLM, and evaluating against
a gold-standard corpus, using string or tree-match
metrics (Langkilde-Geary, 2002; Bangalore and
Rambow, 2000; Chambers and Allen, 2004; Belz,
2005; Isard et al, 2006).
Another thread investigates SNLG scoring mod-
els trained using higher-level linguistic features
to replicate human judgments of utterance quality
(Rambow et al, 2001; Nakatsu and White, 2006;
Stent and Guo, 2005). The error of these scoring
models approaches the gold-standard human rank-
ing with a relatively small training set.
A third SNLG approach eliminates the overgen-
eration phase (Paiva and Evans, 2005). It applies
factor analysis to a corpus exhibiting stylistic vari-
ation, and then learns which generation parameters
to manipulate to correlate with factor measurements.
The generator was shown to reproduce intended fac-
tor levels across several factors, thus modelling the
stylistic variation as measured in the original corpus.
Our goal is a generation technique that can tar-
get multiple stylistic effects simultaneously and
over a continuous scale, controlling stylistic di-
mensions that are commonly understood and thus
meaningful to users and application developers.
Our intended applications are output utterances
for intelligent training or intervention systems,
video game characters, or virtual environment
avatars. In previous work, we presented PERSON-
AGE, a psychologically-informed rule-based genera-
tor based on the Big Five personality model, and we
showed that PERSONAGE can project extreme per-
sonality on the extraversion scale, i.e. both intro-
verted and extraverted personality types (Mairesse
and Walker, 2007). We used the Big Five model
to develop PERSONAGE for several reasons. First,
the Big Five has been shown in psychology to ex-
165
Trait High Low
Extraversion warm, assertive, sociable, excitement seeking, active,
spontaneous, optimistic, talkative
shy, quiet, reserved, passive, solitary, moody
Emotional stability calm, even-tempered, reliable, peaceful, confident neurotic, anxious, depressed, self-conscious
Agreeableness trustworthy, considerate, friendly, generous, helpful unfriendly, selfish, suspicious, uncooperative, ma-
licious
Conscientiousness competent, disciplined, dutiful, achievement striving disorganised, impulsive, unreliable, forgetful
Openness to experience creative, intellectual, curious, cultured, complex narrow-minded, conservative, ignorant, simple
Table 1: Example adjectives associated with extreme values of the Big Five trait scales.
plain much of the variation in human perceptions of
personality differences. Second, we believe that the
adjectives used to develop the Big Five model pro-
vide an intuitive, meaningful definition of linguis-
tic style. Table 1 shows some of the trait adjec-
tives associated with the extremes of each Big Five
trait. Third, there are many studies linking person-
ality to linguistic variables (Pennebaker and King,
1999; Mehl et al, 2006, inter alia). See (Mairesse
and Walker, 2007) for more detail.
In this paper, we further test the utility of basing
stylistic variation on the Big Five personality model.
The Big Five traits are represented by scalar val-
ues that range from 1 to 7, with values normally
distributed among humans. While our previous
work targeted extreme values of individual traits,
here we show that we can target multiple person-
ality traits simultaneously and over the continuous
scales of the Big Five model. Section 2 describes
a novel parameter-estimation method that automat-
ically learns to produce recognisable variation for
all Big Five traits, without overgeneration, imple-
mented in a new SNLG called PERSONAGE-PE.
We show that PERSONAGE-PE generates targets for
multiple personality dimensions, using linear and
non-linear parameter estimation models to predict
generation parameters directly from the scalar tar-
gets. Section 3.2 shows that humans accurately per-
ceive the intended variation, and Section 3.3 com-
pares PERSONAGE-PE (trained) with PERSONAGE
(rule-based; Mairesse and Walker, 2007). We delay
a detailed discussion of related work to Section 4,
where we summarize and discuss future work.
2 Parameter Estimation Models
The data-driven parameter estimation method con-
sists of a development phase and a generation phase
(Section 3). The development phase:
1. Uses a base generator to produce multiple utter-
ances by randomly varying its parameters;
2. Collects human judgments rating the personality of
each utterance;
3. Trains statistical models to predict the parameters
from the personality judgments;
7.006.005.004.003.002.001.00 Agreeableness rating
30
20
10
0
Freq
uenc
y
Figure 1: Distribution of average agreeableness ratings
from the 2 expert judges for 160 random utterances.
4. Selects the best model for each parameter via cross-
validation.
2.1 Base Generator
We make minimal assumptions about the input to
the generator to favor domain independence. The
input is a speech act, a potential content pool that
can be used to achieve that speech act, and five scalar
personality parameters (1. . .7), specifying values for
the continuous scalar dimensions of each trait in
the Big Five model. See Table 1. This requires a
base generator that generates multiple outputs ex-
pressing the same input content by varying linguis-
tic parameters related to the Big Five traits. We
start with the PERSONAGE generator (Mairesse and
Walker, 2007), which generates recommendations
and comparisons of restaurants. We extend PER-
SONAGE with new parameters for a total of 67 pa-
rameters in PERSONAGE-PE. See Table 2. These
parameters are derived from psychological studies
identifying linguistic markers of the Big Five traits
(Pennebaker and King, 1999; Mehl et al, 2006, in-
ter alia). As PERSONAGE?s input parameters are
domain-independent, most parameters range contin-
uously between 0 and 1, while pragmatic marker in-
sertion parameters are binary, except for the SUB-
JECT IMPLICITNESS, STUTTERING and PRONOMI-
166
Parameters Description
Content parameters:
VERBOSITY Control the number of propositions in the utterance
RESTATEMENTS Paraphrase an existing proposition, e.g. ?Chanpen Thai has great service, it has fantastic waiters?
REPETITIONS Repeat an existing proposition
CONTENT POLARITY Control the polarity of the propositions expressed, i.e. referring to negative or positive attributes
REPETITIONS POLARITY Control the polarity of the restated propositions
CONCESSIONS Emphasise one attribute over another, e.g. ?even if Chanpen Thai has great food, it has bad service?
CONCESSIONS POLARITY Determine whether positive or negative attributes are emphasised
POLARISATION Control whether the expressed polarity is neutral or extreme
POSITIVE CONTENT FIRST Determine whether positive propositions?including the claim?are uttered first
Syntactic template selection parameters:
SELF-REFERENCES Control the number of first person pronouns
CLAIM COMPLEXITY Control the syntactic complexity (syntactic embedding)
CLAIM POLARITY Control the connotation of the claim, i.e. whether positive or negative affect is expressed
Aggregation operations:
PERIOD Leave two propositions in their own sentences, e.g. ?Chanpen Thai has great service. It has nice decor.?
RELATIVE CLAUSE Aggregate propositions with a relative clause, e.g. ?Chanpen Thai, which has great service, has nice decor?
WITH CUE WORD Aggregate propositions using with, e.g. ?Chanpen Thai has great service, with nice decor?
CONJUNCTION Join two propositions using a conjunction, or a comma if more than two propositions
MERGE Merge the subject and verb of two propositions, e.g. ?Chanpen Thai has great service and nice decor?
ALSO CUE WORD Join two propositions using also, e.g. ?Chanpen Thai has great service, also it has nice decor?
CONTRAST - CUE WORD Contrast two propositions using while, but, however, on the other hand, e.g. ?While Chanpen Thai has great
service, it has bad decor?, ?Chanpen Thai has great service, but it has bad decor?
JUSTIFY - CUE WORD Justify a proposition using because, since, so, e.g. ?Chanpen Thai is the best, because it has great service?
CONCEDE - CUE WORD Concede a proposition using although, even if, but/though, e.g. ?Although Chanpen Thai has great service, it
has bad decor?, ?Chanpen Thai has great service, but it has bad decor though?
MERGE WITH COMMA Restate a proposition by repeating only the object, e.g. ?Chanpen Thai has great service, nice waiters?
CONJ. WITH ELLIPSIS Restate a proposition after replacing its object by an ellipsis, e.g. ?Chanpen Thai has . . . , it has great service?
Pragmatic markers:
SUBJECT IMPLICITNESS Make the restaurant implicit by moving the attribute to the subject, e.g. ?the service is great?
NEGATION Negate a verb by replacing its modifier by its antonym, e.g. ?Chanpen Thai doesn?t have bad service?
SOFTENER HEDGES Insert syntactic elements (sort of, kind of, somewhat, quite, around, rather, I think that, it seems that, it seems
to me that) to mitigate the strength of a proposition, e.g. ?Chanpen Thai has kind of great service? or ?It seems
to me that Chanpen Thai has rather great service?
EMPHASIZER HEDGES Insert syntactic elements (really, basically, actually, just) to strengthen a proposition, e.g. ?Chanpen Thai has
really great service? or ?Basically, Chanpen Thai just has great service?
ACKNOWLEDGMENTS Insert an initial back-channel (yeah, right, ok, I see, oh, well), e.g. ?Well, Chanpen Thai has great service?
FILLED PAUSES Insert syntactic elements expressing hesitancy (like, I mean, err, mmhm, you know), e.g. ?I mean, Chanpen
Thai has great service, you know? or ?Err... Chanpen Thai has, like, great service?
EXCLAMATION Insert an exclamation mark, e.g. ?Chanpen Thai has great service!?
EXPLETIVES Insert a swear word, e.g. ?the service is damn great?
NEAR-EXPLETIVES Insert a near-swear word, e.g. ?the service is darn great?
COMPETENCE MITIGATION Express the speaker?s negative appraisal of the hearer?s request, e.g. ?everybody knows that . . . ?
TAG QUESTION Insert a tag question, e.g. ?the service is great, isn?t it??
STUTTERING Duplicate the first letters of a restaurant?s name, e.g. ?Ch-ch-anpen Thai is the best?
CONFIRMATION Begin the utterance with a confirmation of the restaurant?s name, e.g. ?did you say Chanpen Thai??
INITIAL REJECTION Begin the utterance with a mild rejection, e.g. ?I?m not sure?
IN-GROUP MARKER Refer to the hearer as a member of the same social group, e.g. pal, mate and buddy
PRONOMINALIZATION Replace occurrences of the restaurant?s name by pronouns
Lexical choice parameters:
LEXICAL FREQUENCY Control the average frequency of use of each content word, according to BNC frequency counts
WORD LENGTH Control the average number of letters of each content word
VERB STRENGTH Control the strength of the selected verbs, e.g. ?I would suggest? vs. ?I would recommend?
Table 2: The 67 generation parameters whose target values are learned. Aggregation cue words, hedges, acknowl-
edgments and filled pauses are learned individually (as separate parameters), e.g. kind of is modeled differently than
somewhat in the SOFTENER HEDGES category. Parameters are detailed in previous work (Mairesse and Walker, 2007).
NALIZATION parameters.
2.2 Random Sample Generation and Expert
Judgments
We generate a sample of 160 random utterances by
varying the parameters in Table 2 with a uniform dis-
tribution. This sample is intended to provide enough
training material for estimating all 67 parameters
for each personality dimension. Following Mairesse
and Walker (2007), two expert judges (not the au-
thors) familiar with the Big Five adjectives (Table 1)
evaluate the personality of each utterance using the
Ten-Item Personality Inventory (TIPI; Gosling et al,
2003), and also judge the utterance?s naturalness.
Thus 11 judgments were made for each utterance for
a total of 1760 judgments. The TIPI outputs a rating
on a scale from 1 (low) to 7 (high) for each Big Five
trait. The expert judgments are approximately nor-
167
mally distributed; Figure 1 shows the distribution for
agreeableness.
2.3 Statistical Model Training
Training data is created for each generation
parameter?i.e. the output variable?to train statis-
tical models predicting the optimal parameter value
from the target personality scores. The models are
thus based on the simplifying assumption that the
generation parameters are independent. Any person-
ality trait whose correlation with a generation deci-
sion is below 0.1 is removed from the training data.
This has the effect of removing parameters that do
not correlate strongly with any trait, which are set to
a constant default value at generation time. Since
the input parameter values may not be satisfiable
depending on the input content, the actual genera-
tion decisions made for each utterance are recorded.
For example, the CONCESSIONS decision value is
the actual number of concessions produced in the
utterance. To ensure that the models? output can
control the generator, the generation decision values
are normalized to match the input range (0. . .1) of
PERSONAGE-PE. Thus the dataset consists of 160
utterances and the corresponding generation deci-
sions, each associated with 5 personality ratings av-
eraged over both judges.
Parameter estimation models are trained to predict
either continuous (e.g. VERBOSITY) or binary (e.g.
EXCLAMATION) generation decisions. We compare
various learning algorithms using the Weka toolkit
(with default values unless specified; Witten and
Frank, 2005). Continuous parameters are modeled
with a linear regression model (LR), an M5? model
tree (M5), and a model based on support vector ma-
chines with a linear kernel (SVM). As regression
models can extrapolate beyond the [0, 1] interval, the
output parameter values are truncated if needed?at
generation time?before being sent to the base gen-
erator. Binary parameters are modeled using clas-
sifiers that predict whether the parameter is enabled
or disabled. We test a Naive Bayes classifier (NB), a
j48 decision tree (J48), a nearest-neighbor classifier
using one neighbor (NN), a Java implementation of
the RIPPER rule-based learner (JRIP), the AdaBoost
boosting algorithm (ADA), and a support vector ma-
chines classifier with a linear kernel (SVM).
Figures 2, 3 and 4 show the models learned for
the EXCLAMATION (binary), STUTTERING (contin-
uous), and CONTENT POLARITY (continuous) pa-
rameters in Table 2. The models predict generation
parameters from input personality scores; note that
Condition Class Weight
--------- ----- ------
if extraversion > 6.42 then 1 else 0 1.81
if extraversion > 4.42 then 1 else 0 0.38
if extraversion <= 6.58 then 1 else 0 0.22
if extraversion > 4.71 then 1 else 0 0.28
if agreeableness > 5.13 then 1 else 0 0.42
if extraversion <= 6.58 then 1 else 0 0.14
if extraversion > 4.79 then 1 else 0 0.19
if extraversion <= 6.58 then 1 else 0 0.17
Figure 2: AdaBoost model predicting the EXCLAMATION
parameter. Given input trait values, the model outputs
the class yielding the largest sum of weights for the rules
returning that class. Class 0 = disabled, class 1 = enabled.
(normalized) Content polarity =
0.054
- 0.102 * (normalized) emotional stability
+ 0.970 * (normalized) agreeableness
- 0.110 * (normalized) conscientiousness
+ 0.013 * (normalized) openness to
experience
Figure 3: SVM model with a linear kernel predicting the
CONTENT POLARITY parameter.
sometimes the best performing model is non-linear.
Given input trait values, the AdaBoost model in Fig-
ure 2 outputs the class yielding the largest sum of
weights for the rules returning that class. For ex-
ample, the first rule of the EXCLAMATION model
shows that an extraversion score above 6.42 out of
7 would increase the weight of the enabled class by
1.81. The fifth rule indicates that a target agreeable-
ness above 5.13 would further increase the weight
by .42. The STUTTERING model tree in Figure 4
lets us calculate that a low emotional stability (1.0)
together with a neutral conscientiousness and open-
ness to experience (4.0) yield a parameter value of
.62 (see LM2), whereas a neutral emotional stabil-
ity decreases the value down to .17. Figure 4 also
shows how personality traits that do not affect the
parameter are removed, i.e. emotional stability, con-
scientiousness and openness to experience are the
traits that affect stuttering. The linear model in Fig-
ure 3 shows that agreeableness has a strong effect
on the CONTENT POLARITY parameter (.97 weight),
but emotional stability, conscientiousness and open-
ness to experience also have an effect.
2.4 Model Selection
The final step of the development phase identifies
the best performing model(s) for each generation
parameter via cross-validation. For continuous pa-
168
? 3.875 > 3.875
Conscientiousness
Emotional stability
? 4.375 > 4.375
Stuttering =
-0.0136 * emotional stability
+ 0.0098 * conscientiousness
+ 0.0063 * openness to experience
+ 0.0126
Stuttering =
-0.1531 * emotional stability
+ 0.004 * conscientiousness
+ 0.1122 * openness to experience
+ 0.3129
Stuttering =
-0.0142 * emotional stability
+ 0.004 * conscientiousness
+ 0.0076 * openness to experience
+ 0.0576
Figure 4: M5? model tree predicting the STUTTERING parameter.
Continuous parameters LR M5 SVM
Content parameters:
VERBOSITY 0.24 0.26 0.21
RESTATEMENTS 0.14 0.14 0.04
REPETITIONS 0.13 0.13 0.08
CONTENT POLARITY 0.46 0.46 0.47
REPETITIONS POLARITY 0.02 0.15 0.06
CONCESSIONS 0.23 0.23 0.12
CONCESSIONS POLARITY -0.01 0.16 0.07
POLARISATION 0.20 0.21 0.20
Syntactic template selection:
CLAIM COMPLEXITY 0.10 0.33 0.26
CLAIM POLARITY 0.04 0.04 0.05
Aggregation operations:
INFER - WITH CUE WORD 0.03 0.03 0.01
INFER - ALSO CUE WORD 0.10 0.10 0.06
JUSTIFY - SINCE CUE WORD 0.03 0.07 0.05
JUSTIFY - SO CUE WORD 0.07 0.07 0.04
JUSTIFY - PERIOD 0.36 0.35 0.21
CONTRAST - PERIOD 0.27 0.26 0.26
RESTATE - MERGE WITH COMMA 0.18 0.18 0.09
CONCEDE - ALTHOUGH CUE WORD 0.08 0.08 0.05
CONCEDE - EVEN IF CUE WORD 0.05 0.05 0.03
Pragmatic markers:
SUBJECT IMPLICITNESS 0.13 0.13 0.04
STUTTERING INSERTION 0.16 0.23 0.17
PRONOMINALIZATION 0.22 0.20 0.17
Lexical choice parameters:
LEXICAL FREQUENCY 0.21 0.21 0.19
WORD LENGTH 0.18 0.18 0.15
Table 3: Pearson?s correlation between parameter model
predictions and continuous parameter values, for differ-
ent regression models. Parameters that do not correlate
with any trait are omitted. Aggregation operations are as-
sociated with a rhetorical relation (e.g. INFER). Results
are averaged over a 10-fold cross-validation.
rameters, Table 3 evaluates modeling accuracy by
comparing the correlations between the model?s pre-
dictions and the actual parameter values in the test
folds. Table 4 reports results for binary parameter
classifiers, by comparing the F-measures of the en-
abled class. Best performing models are identified
in bold; parameters that do not correlate with any
trait or that produce a poor modeling accuracy are
omitted.
The CONTENT POLARITY parameter is modeled
Binary parameters NB J48 NN ADA SVM
Pragmatic markers:
SOFTENER HEDGES
kind of 0.00 0.00 0.16 0.11 0.10
rather 0.00 0.00 0.02 0.01 0.01
quite 0.14 0.08 0.09 0.07 0.06
EMPHASIZER HEDGES
basically 0.00 0.00 0.02 0.01 0.01
ACKNOWLEDGMENTS
yeah 0.00 0.00 0.04 0.03 0.03
ok 0.13 0.07 0.06 0.05 0.05
FILLED PAUSES
err 0.32 0.20 0.24 0.22 0.19
EXCLAMATION 0.23 0.34 0.36 0.38 0.34
EXPLETIVES 0.27 0.18 0.24 0.17 0.15
IN-GROUP MARKER 0.40 0.31 0.31 0.24 0.21
TAG QUESTION 0.32 0.21 0.21 0.15 0.13
CONFIRMATION 0.00 0.00 0.07 0.04 0.04
Table 4: F-measure of the enabled class for classifica-
tion models of binary parameters. Parameters that do
not correlate with any trait are omitted. Results are av-
eraged over a 10-fold cross-validation. JRIP models are
not shown as they never perform best.
the most accurately, with the SVM model in Fig-
ure 3 producing a correlation of .47 with the true pa-
rameter values. Models of the PERIOD aggregation
operation also perform well, with a linear regression
model yielding a correlation of .36 when realizing
a justification, and .27 when contrasting two propo-
sitions. CLAIM COMPLEXITY and VERBOSITY are
also modeled successfully, with correlations of .33
and .26 using a model tree. The model tree control-
ling the STUTTERING parameter illustrated in Fig-
ure 4 produces a correlation of .23. For binary pa-
rameters, Table 4 shows that the Naive Bayes classi-
fier is generally the most accurate, with F-measures
of .40 for the IN-GROUP MARKER parameter, and
.32 for both the insertion of filled pauses (err) and
tag questions. The AdaBoost algorithm best predicts
the EXCLAMATION parameter, with an F-measure of
.38 for the model in Figure 2.
169
# Traits End Rating Nat Output utterance
1.a Extraversion high 4.42 4.79 Radio Perfecto?s price is 25 dollars but Les Routiers provides adequate food. Iimagine they?re alright!Agreeableness high 4.94
1.b
Emotional stability high 5.35
5.04
Let?s see, Les Routiers and Radio Perfecto... You would probably appreciate them.
Radio Perfecto is in the East Village with kind of acceptable food. Les Routiers is
located in Manhattan. Its price is 41 dollars.Conscientiousness high 5.21
2.a Extraversion low 3.65 3.21 Err... you would probably appreciate Trattoria Rustica, wouldn?t you? It?s inManhattan, also it?s an italian restaurant. It offers poor ambience, also it?s quite costly.Agreeableness low 4.02
2.b
Emotional stability low 4.13
4.50 Trattoria Rustica isn?t as bad as the others. Err... even if it?s costly, it offers kind ofadequate food, alright? It?s an italian place.Openness to low 3.85experience
Table 5: Example outputs controlled by the parameter estimation models for a comparison (#1) and a recommendation
(#2), with the average judges? ratings (Rating) and naturalness (Nat). Ratings are on a scale from 1 to 7, with 1 = very
low (e.g. neurotic or introvert) and 7 = very high on the dimension (e.g. emotionally stable or extraverted).
3 Evaluation Experiment
The generation phase of our parameter estimation
SNLG method consists of the following steps:
1. Use the best performing models to predict parame-
ter values from the desired personality scores;
2. Generate the output utterance using the predicted
parameter values.
We then evaluate the output utterances using naive
human judges to rate their perceived personality and
naturalness.
3.1 Evaluation Method
Given the best performing model for each genera-
tion parameter, we generate 5 utterances for each
of 5 recommendation and 5 comparison speech acts.
Each utterance targets an extreme value for two traits
(either 1 or 7 out of 7) and neutral values for the re-
maining three traits (4 out of 7). The goal is for each
utterance to project multiple traits on a continuous
scale. To generate a range of alternatives, a Gaus-
sian noise with a standard deviation of 10% of the
full scale is added to each target value.
Subjects were 24 native English speakers (12
male and 12 female graduate students from a range
of disciplines from both the U.K. and the U.S.). Sub-
jects evaluate the naturalness and personality of each
utterance using the TIPI (Gosling et al, 2003). To
limit the experiment?s duration, only the two traits
with extreme target values are evaluated for each
utterance. Subjects thus answered 5 questions for
50 utterances, two from the TIPI for each extreme
trait and one about naturalness (250 judgments in
total per subject). Subjects were not told that the
utterances were intended to manifest extreme trait
values. Table 5 shows several sample outputs and
the mean personality ratings from the human judges.
For example, utterance 1.a projects a high extraver-
sion through the insertion of an exclamation mark
based on the model in Figure 2, whereas utterance
2.a conveys introversion by beginning with the filled
pause err. The same utterance also projects a low
agreeableness by focusing on negative propositions,
through a low CONTENT POLARITY parameter value
as per the model in Figure 3. This evaluation ad-
dresses a number of open questions discussed below.
Q1: Is the personality projected by models trained on
ratings from a few expert judges recognised by a
larger sample of naive judges? (Section 3.2)
Q2: Can a combination of multiple traits within a single
utterance be detected by naive judges? (Section 3.2)
Q3: How does PERSONAGE-PE compare to PERSON-
AGE, a psychologically-informed rule-based gen-
erator for projecting extreme personality? (Sec-
tion 3.3)
Q4: Does the parameter estimation SNLG method pro-
duce natural utterances? (Section 3.4)
3.2 Parameter Estimation Evaluation
Table 6 shows that extraversion is the dimension
modeled most accurately by the parameter estima-
tion models, producing a .45 correlation with the
subjects? ratings (p < .01). Emotional stability,
agreeableness, and openness to experience ratings
also correlate strongly with the target scores, with
correlations of .39, .36 and .17 respectively (p <
.01). Additionally, Table 6 shows that the magni-
tude of the correlation increases when considering
the perception of a hypothetical average subject, i.e.
smoothing individual variation by averaging the rat-
ings over all 24 judges, producing a correlation ravg
up to .80 for extraversion. These correlations are
unexpectedly high; in corpus analyses, significant
correlations as low as .05 to .10 are typically ob-
served between personality and linguistic markers
(Pennebaker and King, 1999; Mehl et al, 2006).
Conscientiousness is the only dimension whose
ratings do not correlate with the target scores. The
170
comparison with rule-based results in Section 3.3
suggests that this is not because conscientiousness
cannot be exhibited in our domain or manifested in
a single utterance, so perhaps this arises from dif-
fering perceptions of conscientiousness between the
expert and naive judges.
Trait r ravg e
Extraversion .45 ? .80 ? 1.89
Emotional stability .39 ? .64 ? 2.14
Agreeableness .36 ? .68 ? 2.38
Conscientiousness -.01 -.02 2.79
Openness to experience .17 ? .41 ? 2.51
? statistically significant correlation
p < .05, ? p = .07 (two-tailed)
Table 6: Pearson?s correlation coefficient r and mean ab-
solute error e between the target personality scores and
the 480 judges? ratings (20 ratings per trait for 24 judges);
ravg is the correlation between the personality scores and
the average judges? ratings.
Table 6 shows that the mean absolute error varies
between 1.89 and 2.79 on a scale from 1 to 7. Such
large errors result from the decision to ask judges to
answer just the TIPI questions for the two traits that
were the extreme targets (See Section 3.1), because
the judges tend to use the whole scale, with approx-
imately normally distributed ratings. This means
that although the judges make distinctions leading to
high correlations, they do so on a compressed scale.
This explains the large correlations despite the mag-
nitude of the absolute error.
Table 7 shows results evaluating whether utter-
ances targeting the extremes of a trait are perceived
differently. The ratings differ significantly for all
traits but conscientiousness (p ? .001). Thus pa-
rameter estimation models can be used in applica-
tions that only require discrete binary variation.
Trait Low High
Extraversion 3.69 5.06 ?
Emotional stability 3.75 4.75 ?
Agreeableness 3.42 4.33 ?
Conscientiousness 4.16 4.15
Openness to experience 3.71 4.06 ?
? statistically significant difference
p ? .001 (two-tailed)
Table 7: Average personality ratings for the utterances
generated with the low and high target values for each
trait on a scale from 1 to 7.
It is important to emphasize that generation pa-
rameters were predicted based on 5 target person-
ality values. Thus, the results show that individ-
ual traits are perceived even when utterances project
other traits as well, confirming that the Big Five the-
ory models independent dimensions and thus pro-
vides a useful and meaningful framework for mod-
eling variation in language. Additionally, although
we do not directly evaluate the perception of mid-
range values of personality target scores, the results
suggest that mid-range personality is modeled cor-
rectly because the neutral target scores do not affect
the perception of extreme traits.
3.3 Comparison with Rule-Based Generation
PERSONAGE is a rule-based personality generator
based on handcrafted parameter settings derived
from psychological studies. Mairesse and Walker
(2007) show that this approach generates utterances
that are perceptibly different along the extraversion
dimension. Table 8 compares the mean ratings of
the utterances generated by PERSONAGE-PE with
ratings of 20 utterances generated by PERSONAGE
for each extreme of each Big Five scale (40 for ex-
traversion, resulting in 240 handcrafted utterances in
total). Table 8 shows that the handcrafted parame-
ter settings project a significantly more extreme per-
sonality for 6 traits out of 10. However, the learned
parameter models for neuroticism, disagreeableness,
unconscientiousness and openness to experience do
not perform significantly worse than the handcrafted
generator. These findings are promising as we dis-
cuss further in Section 4.
Method Rule-based Learned parameters
Trait Low High Low High
Extraversion 2.96 5.98 3.69 ? 5.05 ?
Emotional stability 3.29 5.96 3.75 4.75 ?
Agreeableness 3.41 5.66 3.42 4.33 ?
Conscientiousness 3.71 5.53 4.16 4.15 ?
Openness to experience 2.89 4.21 3.71 ? 4.06
?,? significant increase or decrease of the variation range
over the average rule-based ratings (p < .05, two-tailed)
Table 8: Pair-wise comparison between the ratings of
the utterances generated using PERSONAGE-PE with ex-
treme target values (Learned Parameters), and the ratings
for utterances generated with Mairesse andWalker?s rule-
based PERSONAGE generator, (Rule-based). Ratings are
averaged over all judges.
3.4 Naturalness Evaluation
The naive judges also evaluated the naturalness of
the outputs of our trained models. Table 9 shows
that the average naturalness is 3.98 out of 7, which is
significantly lower (p < .05) than the naturalness of
handcrafted and randomly generated utterances re-
ported by Mairesse and Walker (2007). It is possi-
ble that the differences arise from judgments of ut-
terances targeting multiple traits, or that the naive
171
judges are more critical.
Trait Rule-based Random Learned
All 4.59 4.38 3.98
Table 9: Average naturalness ratings for utterances gen-
erated using (1) PERSONAGE, the rule-based generator,
(2) the random utterances (expert judges) and (3) the out-
puts of PERSONAGE-PE using the parameter estimation
models (Learned, naive judges). The means differ sig-
nificantly at the p < .05 level (two-tailed independent
sample t-test).
4 Conclusion
We present a new method for generating linguis-
tic variation projecting multiple personality traits
continuously, by combining and extending previous
research in statistical natural language generation
(Paiva and Evans, 2005; Rambow et al, 2001; Is-
ard et al, 2006; Mairesse and Walker, 2007). While
handcrafted rule-based approaches are limited to
variation along a small number of discrete points
(Hovy, 1988; Walker et al, 1997; Lester et al, 1997;
Power et al, 2003; Cassell and Bickmore, 2003; Pi-
wek, 2003; Mairesse and Walker, 2007; Rehm and
Andre?, in press), we learn models that predict pa-
rameter values for any arbitrary value on the varia-
tion dimension scales. Additionally, our data-driven
approach can be applied to any dimension that is
meaningful to human judges, and it provides an ele-
gant way to project multiple dimensions simultane-
ously, by including the relevant dimensions as fea-
tures of the parameter models? training data.
Isard et al (2006) and Mairesse and Walker
(2007) also propose a personality generation
method, in which a data-driven personality model
selects the best utterance from a large candidate set.
Isard et al?s technique has not been evaluated, while
Mairesse and Walker?s overgenerate and score ap-
proach is inefficient. Paiva and Evans? technique
does not overgenerate (2005), but it requires a search
for the optimal generation decisions according to
the learned models. Our approach does not require
any search or overgeneration, as parameter estima-
tion models predict the generation decisions directly
from the target variation dimensions. This tech-
nique is therefore beneficial for real-time genera-
tion. Moreover the variation dimensions of Paiva
and Evans? data-driven technique are extracted from
a corpus: there is thus no guarantee that they can
be easily interpreted by humans, and that they gen-
eralise to other corpora. Previous work has shown
that modeling the relation between personality and
language is far from trivial (Pennebaker and King,
1999; Argamon et al, 2005; Oberlander and Now-
son, 2006; Mairesse et al, 2007), suggesting that the
control of personality is a harder problem than the
control of data-driven variation dimensions.
We present the first human perceptual evaluation
of a data-driven stylistic variation method. In terms
of our research questions in Section 3.1, we show
that models trained on expert judges to project mul-
tiple traits in a single utterance generate utterances
whose personality is recognized by naive judges.
There is only one other similar evaluation of an
SNLG (Rambow et al, 2001). Our models perform
only slightly worse than a handcrafted rule-based
generator in the same domain. These findings are
promising as (1) parameter estimation models are
able to target any combination of traits over the full
range of the Big Five scales; (2) they do not benefit
from psychological knowledge, i.e. they are trained
on randomly generated utterances.
This work also has several limitations that should
be addressed in future work. Even though the
parameters of PERSONAGE-PE were suggested by
psychological studies (Mairesse and Walker, 2007),
some of them are not modeled successfully by our
approach, and thus omitted from Tables 3 and 4.
This could be due to the relatively small develop-
ment dataset size (160 utterances to optimize 67 pa-
rameters), or to the implementation of some param-
eters. The strong parameter-independence assump-
tion could also be responsible, but we are not aware
of any state of the art implementation for learn-
ing multiple dependent variables, and this approach
could further aggravate data sparsity issues.
In addition, it is unclear why PERSONAGE per-
forms better for projecting extreme personality
and produces more natural utterances, and why
PERSONAGE-PE fails to project conscientiousness
correctly. It might be possible to improve the pa-
rameter estimation models with a larger sample of
random utterances at development time, or with ad-
ditional extreme data generated using the rule-based
approach. Such hybrid models are likely to perform
better for extreme target scores, as they are trained
on more uniformly distributed ratings (e.g. com-
pared to the normal distribution in Figure 1). In ad-
dition, we have only shown that personality can be
expressed by information presentation speech-acts
in the restaurant domain; future work should assess
the extent to which the parameters derived from psy-
chological findings are culture, domain, and speech
act dependent.
172
References
S. Argamon, S. Dhawle, M. Koppel, and J. Pennebaker.
Lexical predictors of personality type. In Proceedings
of the Joint Annual Meeting of the Interface and the
Classification Society of North America, 2005.
S. Bangalore and O. Rambow. Exploiting a probabilistic
hierarchical model for generation. In Proceedings of
the 18th International Conference on Computational
Linguistics (COLING), pages 42?48, 2000.
A. Belz. Corpus-driven generation of weather forecasts.
In Proceedings of the 3rd Corpus Linguistics Confer-
ence, 2005.
J. Cassell and T. Bickmore. Negotiated collusion: Mod-
eling social language and its relationship effects in in-
telligent agents. User Modeling and User-Adapted In-
teraction, 13:89?132, 2003.
N. Chambers and J. Allen. Stochastic language genera-
tion in a dialogue system: Toward a domain indepen-
dent generator. In Proceedings 5th SIGdial Workshop
on Discourse and Dialogue, 2004.
S. D. Gosling, P. J. Rentfrow, and W. B. Swann. A
very brief measure of the big five personality domains.
Journal of Research in Personality, 37:504?528, 2003.
E. Hovy. Generating Natural Language under Pragmatic
Constraints. Lawrence Erlbaum Associates, 1988.
A. Isard, C. Brockmann, and J. Oberlander. Individuality
and alignment in generated dialogues. In Proceedings
of the 4th International Natural Language Generation
Conference (INLG), pages 22?29, 2006.
I. Langkilde and K. Knight. Generation that exploits
corpus-based statistical knowledge. In Proceedings of
the 36th Annual Meeting of the Association for Com-
putational Linguistics (ACL), pages 704?710, 1998.
I. Langkilde-Geary. An empirical verification of coverage
and correctness for a general-purpose sentence genera-
tor. In Proceedings of the 1st International Conference
on Natural Language Generation, 2002.
M. Lapata and F. Keller. Web-based models for natu-
ral language processing. ACM Transactions on Speech
and Language Processing, 2:1?31, 2005.
J. Lester, S. Converse, S. Kahler, S. Barlow, B. Stone,
and R. Bhogal. The persona effect: affective impact
of animated pedagogical agents. Proceedings of the
SIGCHI conference on Human factors in computing
systems, pages 359?366, 1997.
F. Mairesse and M. A. Walker. PERSONAGE: Personal-
ity generation for dialogue. In Proceedings of the 45th
Annual Meeting of the Association for Computational
Linguistics (ACL), pages 496?503, 2007.
F. Mairesse, M. A. Walker, M. R. Mehl, and R. K. Moore.
Using linguistic cues for the automatic recognition of
personality in conversation and text. Journal of Artifi-
cial Intelligence Research (JAIR), 30:457?500, 2007.
M. R. Mehl, S. D. Gosling, and J. W. Pennebaker. Person-
ality in its natural habitat: Manifestations and implicit
folk theories of personality in daily life. Journal of
Personality and Social Psychology, 90:862?877, 2006.
C. Nakatsu and M. White. Learning to say it well:
Reranking realizations by predicted synthesis quality.
In Proceedings of the 44th Annual Meeting of the As-
sociation for Computational Linguistics (ACL), pages
1113?1120, 2006.
J. Oberlander and S. Nowson. Whose thumb is it any-
way? classifying author personality from weblog text.
In Proceedings of the 44th Annual Meeting of the As-
sociation for Computational Linguistics (ACL), 2006.
D. S. Paiva and R. Evans. Empirically-based control of
natural language generation. In Proceedings of the
43rd Annual Meeting of the Association for Compu-
tational Linguistics (ACL), pages 58?65, 2005.
J. W. Pennebaker and L. A. King. Linguistic styles: Lan-
guage use as an individual difference. Journal of Per-
sonality and Social Psychology, 77:1296?1312, 1999.
P. Piwek. A flexible pragmatics-driven language gener-
ator for animated agents. In Proceedings of Annual
Meeting of the European Chapter of the Association
for Computational Linguistics (EACL), 2003.
R. Power, D. Scott, and N. Bouayad-Agha. Generating
texts with style. In Proceedings of the 4th Interna-
tional Conference on Intelligent Text Processing and
Computational Linguistics, 2003.
O. Rambow, M. Rogati, and M. A. Walker. Evaluating a
trainable sentence planner for a spoken dialogue travel
system. In Proceedings of the 39th Annual Meeting of
the Association for Computational Linguistics (ACL),
2001.
M. Rehm and E. Andre?. From annotated multi-
modal corpora to simulated human-like behaviors.
In I. Wachsmuth and G. Knoblich, editors, Model-
ing Communication with Robots and Virtual Humans.
Springer, Berlin, Heidelberg, in press.
A. Stent and H. Guo. A new data-driven approach
for multimedia presentation generation. In Proc. Eu-
roIMSA, 2005.
M. A. Walker, J. E. Cahn, and S. J. Whittaker. Improvis-
ing linguistic style: Social and affective bases for agent
personality. In Proceedings of the 1st Conference on
Autonomous Agents, pages 96?105, 1997.
I. H. Witten and E. Frank. Data Mining: Practical ma-
chine learning tools and techniques. Morgan Kauf-
mann, San Francisco, CA, 2005.
173
Proceedings of the 9th SIGdial Workshop on Discourse and Dialogue, pages 112?119,
Columbus, June 2008. c?2008 Association for Computational Linguistics
Training and Evaluation of the HIS POMDP Dialogue System in Noise
M. Gas?ic?, S. Keizer, F. Mairesse, J. Schatzmann, B. Thomson, K. Yu, S. Young
Machine Intelligence Laboratory
Engineering Department
Cambridge University
United Kingdom
Abstract
This paper investigates the claim that a di-
alogue manager modelled as a Partially Ob-
servable Markov Decision Process (POMDP)
can achieve improved robustness to noise
compared to conventional state-based dia-
logue managers. Using the Hidden Infor-
mation State (HIS) POMDP dialogue man-
ager as an exemplar, and an MDP-based dia-
logue manager as a baseline, evaluation results
are presented for both simulated and real dia-
logues in a Tourist Information Domain. The
results on the simulated data show that the
inherent ability to model uncertainty, allows
the POMDP model to exploit alternative hy-
potheses from the speech understanding sys-
tem. The results obtained from a user trial
show that the HIS system with a trained policy
performed significantly better than the MDP
baseline.
1 Introduction
Conventional spoken dialogue systems operate by
finding the most likely interpretation of each user
input, updating some internal representation of the
dialogue state and then outputting an appropriate re-
sponse. Error tolerance depends on using confidence
thresholds and where they fail, the dialogue manager
must resort to quite complex recovery procedures.
Such a system has no explicit mechanisms for rep-
resenting the inevitable uncertainties associated with
speech understanding or the ambiguities which natu-
rally arise in interpreting a user?s intentions. The re-
sult is a system that is inherently fragile, especially
in noisy conditions or where the user is unsure of
how to use the system.
It has been suggested that Partially Observable
Markov Decision Processes (POMDPs) offer a nat-
ural framework for building spoken dialogue sys-
tems which can both model these uncertainties
and support policies which are robust to their ef-
fects (Young, 2002; Williams and Young, 2007a).
The key idea of the POMDP is that the underlying
dialogue state is hidden and dialogue management
policies must therefore be based not on a single state
estimate but on a distribution over all states.
Whilst POMDPs are attractive theoretically, in
practice, they are notoriously intractable for any-
thing other than small state/action spaces. Hence,
practical examples of their use were initially re-
stricted to very simple domains (Roy et al, 2000;
Zhang et al, 2001). More recently, however, a num-
ber of techniques have been suggested which do al-
low POMDPs to be scaled to handle real world tasks.
The two generic mechanisms which facilitate this
scaling are factoring the state space and perform-
ing policy optimisation in a reduced summary state
space (Williams and Young, 2007a; Williams and
Young, 2007b).
Based on these ideas, a number of real-world
POMDP-based systems have recently emerged. The
most complex entity which must be represented in
the state space is the user?s goal. In the Bayesian
Update of Dialogue State (BUDS) system, the user?s
goal is further factored into conditionally indepen-
dent slots. The resulting system is then modelled
as a dynamic Bayesian network (Thomson et al,
2008). A similar approach is also developed in
112
(Bui et al, 2007a; Bui et al, 2007b). An alterna-
tive approach taken in the Hidden Information State
(HIS) system is to retain a complete representation
of the user?s goal, but partition states into equiva-
lence classes and prune away very low probability
partitions (Young et al, 2007; Thomson et al, 2007;
Williams and Young, 2007b).
Whichever approach is taken, a key issue in a real
POMDP-based dialogue system is its ability to be
robust to noise and that is the issue that is addressed
in this paper. Using the HIS system as an exem-
plar, evaluation results are presented for a real-world
tourist information task using both simulated and
real users. The results show that a POMDP system
can learn noise robust policies and that N-best out-
puts from the speech understanding component can
be exploited to further improve robustness.
The paper is structured as follows. Firstly, in Sec-
tion 2 a brief overview of the HIS system is given.
Then in Section 3, various POMDP training regimes
are described and evaluated using a simulated user at
differing noise levels. Section 4 then presents results
from a trial in which users conducted various tasks
over a range of noise levels. Finally, in Section 5,
we discuss our results and present our conclusions.
2 The HIS System
2.1 Basic Principles
A POMDP-based dialogue system is shown in Fig-
ure 1 where sm denotes the (unobserved or hidden)
machine state which is factored into three compo-
nents: the last user act au, the user?s goal su and
the dialogue history sd. Since sm is unknown, at
each time-step the system computes a belief state
such that the probability of being in state sm given
belief state b is b(sm). Based on this current belief
state b, the machine selects an action am, receives
a reward r(sm, am), and transitions to a new (un-
observed) state s?m, where s?m depends only on sm
and am. The machine then receives an observation
o? consisting of an N-best list of hypothesised user
actions. Finally, the belief distribution b is updated
based on o? and am as follows:
b?(s?m) = kP (o?|s?m, am)
?
sm?Sm
P (s?m|am, sm)b(sm)
(1)
where k is a normalisation constant (Kaelbling et al,
1998). The first term on the RHS of (1) is called the
observation model and the term inside the summa-
tion is called the transition model. Maintaining this
belief state as the dialogue evolves is called belief
monitoring.
Speech
Understanding
Speech
Generation
User
au
am
~
su
am
Belief
Estimator
Dialog
Policy
sd
b(     )sm
sm = <au,s u,s d>
au
1
.
.

au
N
Figure 1: Abstract view of a POMDP-based spoken dia-
logue system
At each time step t, the machine receives a reward
r(bt, am,t) based on the current belief state bt and the
selected action am,t. Each action am,t is determined
by a policy ?(bt) and building a POMDP system in-
volves finding the policy ?? which maximises the
discounted sum R of the rewards
R =
??
t=0
?tr(bt, am,t) (2)
where ?t is a discount coefficient.
2.2 Probability Models
In the HIS system, user goals are partitioned and
initially, all states su ? Su are regarded as being
equally likely and they are placed in a single par-
tition p0. As the dialogue progresses, user inputs
result in changing beliefs and this root partition is
repeatedly split into smaller partitions. This split-
ting is binary, i.e. p ? {p?, p ? p?} with probability
P (p?|p). By replacing sm by its factors (su, au, sd)
and making reasonable independence assumptions,
it can be shown (Young et al, 2007) that in parti-
113
tioned form (1) becomes
b?(p?, a?u, s?d) = k ? P (o?|a?u)
? ?? ?
observation
model
P (a?u|p?, am)
? ?? ?
user action
model
?
?
sd
P (s?d|p?, a?u, sd, am)
? ?? ?
dialogue
model
P (p?|p)b(p, sd)
? ?? ?
partition
splitting
(3)
where p is the parent of p?.
In this equation, the observation model is approx-
imated by the normalised distribution of confidence
measures output by the speech recognition system.
The user action model allows the observation prob-
ability that is conditioned on a?u to be scaled by the
probability that the user would speak a?u given the
partition p? and the last system prompt am. In the
current implementation of the HIS system, user dia-
logue acts take the form act(a = v) where act is the
dialogue type, a is an attribute and v is its value [for
example, request(food=Chinese)]. The user action
model is then approximated by
P (a?u|p?, am) ? P (T (a?u)|T (am))P (M(a?u)|p?)
(4)
where T (?) denotes the type of the dialogue act
and M(?) denotes whether or not the dialogue act
matches the current partition p?. The dialogue
model is a deterministic encoding based on a simple
grounding model. It yields probability one when the
updated dialogue hypothesis (i.e., a specific combi-
nation of p?, a?u, sd and am) is consistent with the
history and zero otherwise.
2.3 Policy Representation
Policy representation in POMDP-systems is non-
trivial since each action depends on a complex prob-
ability distribution. One of the simplest approaches
to dealing with this problem is to discretise the state
space and then associate an action with each dis-
crete grid point. To reduce quantisation errors, the
HIS model first maps belief distributions into a re-
duced summary space before quantising. This sum-
mary space consists of the probability of the top
two hypotheses plus some status variables and the
user act type associated with the top distribution.
Quantisation is then performed using a simple dis-
tance metric to find the nearest grid point. Ac-
tions in summary space refer specifically to the top
two hypotheses, and unlike actions in master space,
they are limited to a small finite set: greet, ask, ex-
plicit confirm, implicit confirm, select confirm, of-
fer, inform, find alternative, query more, goodbye.
A simple heuristic is then used to map the selected
next system action back into the full master belief
space.
1
Observation
From
User
Ontology Rules
2
N
ua
ma
~
From
System
1
1
2
2
2
1
ds
2
ds
1
ds
2
ds
3
ds
1
up
2
up
3
up
POMDP
Policy
2h
3h
4h
5h
1h







~
a u
~
a u
~
a u
~
a u
~
a u
~
a u
~
a u
Belief
State
Application Database
Action
Refinement
(heuristic)
ma
^
Strategic
Action
Specific
Action
Map to
Summary
Space
ma
b
b^
Summary Space
Figure 2: Overview of the HIS system dialogue cycle
The dialogue manager is able to support nega-
tions, denials and requests for alternatives. When the
selected summary action is to offer the user a venue,
the summary-to-master space mapping heuristics
will normally offer a venue consistent with the most
likely user goal hypothesis. If this hypothesis is then
rejected its belief is substantially reduced and it will
no longer be the top-ranking hypothesis. If the next
system action is to make an alternative offer, then
the new top-ranking hypothesis may not be appro-
priate. For example, if an expensive French restau-
rant near the river had been offered and the user asks
for one nearer the centre of town, any alternative of-
fered should still include the user?s confirmed de-
sire for an expensive French restaurant. To ensure
this, all of the grounded features from the rejected
hypothesis are extracted and all user goal hypothe-
ses are scanned starting at the most likely until an
alternative is found that matches the grounded fea-
tures. For the current turn only, the summary-to-
master space heuristics then treat this hypothesis as
if it was the top-ranking one. If the system then of-
fers a venue based on this hypothesis, and the user
accepts it, then, since system outputs are appended
to user inputs for the purpose of belief updating, the
114
alternative hypothesis will move to the top, or near
the top, of the ranked hypothesis list. The dialogue
then typically continues with its focus on the newly
offered alternative venue.
2.4 Summary of Operation
To summarise, the overall processing performed by
the HIS system in a single dialogue turn (i.e. one cy-
cle of system output and user response) is as shown
in Figure 2. Each user utterance is decoded into an
N-best list of dialogue acts. Each incoming act plus
the previous system act are matched against the for-
est of user goals and partitions are split as needed.
Each user act au is then duplicated and bound to
each partition p. Each partition will also have a
set of dialogue histories sd associated with it. The
combination of each p, au and updated sd forms a
new dialogue hypothesis hk whose beliefs are eval-
uated using (3). Once all dialogue hypotheses have
been evaluated and any duplicates merged, the mas-
ter belief state b is mapped into summary space b?
and the nearest policy belief point is found. The as-
sociated summary space machine action a?m is then
heuristically mapped back to master space and the
machine?s actual response am is output. The cycle
then repeats until the user?s goal is satisfied.
3 Training and Evaluation with a
Simulated User
3.1 Policy optimisation
Policy optimisation is performed in the discrete
summary space described in the previous section us-
ing on-line batch ?-greedy policy iteration. Given
an existing policy ?, dialogs are executed and ma-
chine actions generated according to ? except that
with probability ? a random action is generated. The
system maintains a set of belief points {b?i}. At each
turn in training, the nearest stored belief point b?k to
b? is located using a distance measure. If the distance
is greater than some threshold, b? is added to the set
of stored belief points. The sequence of points b?k
traversed in each dialogue is stored in a list. As-
sociated with each b?i is a function Q(b?i, a?m) whose
value is the expected total reward obtained by choos-
ing summary action a?m from state b?i. At the end
of each dialogue, the total reward is calculated and
added to an accumulator for each point in the list,
discounted by ? at each step. On completion of a
batch of dialogs, the Q values are updated accord-
ing to the accumulated rewards, and the policy up-
dated by choosing the action which maximises each
Q value. The whole process is then repeated until
the policy stabilises.
In our experiments, ? was fixed at 0.1 and ? was
fixed at 0.95. The reward function used attempted
to encourage short successful dialogues by assign-
ing +20 for a successful dialogue and ?1 for each
dialogue turn.
3.2 User Simulation
To train a policy, a user simulator is used to gen-
erate responses to system actions. It has two main
components: a User Goal and a User Agenda. At
the start of each dialogue, the goal is randomly
initialised with requests such as ?name?, ?addr?,
?phone? and constraints such as ?type=restaurant?,
?food=Chinese?, etc. The agenda stores the di-
alogue acts needed to elicit this information in a
stack-like structure which enables it to temporarily
store actions when another action of higher priority
needs to be issued first. This enables the simulator
to refer to previous dialogue turns at a later point. To
generate a wide spread of realistic dialogs, the sim-
ulator reacts wherever possible with varying levels
of patience and arbitrariness. In addition, the sim-
ulator will relax its constraints when its initial goal
cannot be satisfied. This allows the dialogue man-
ager to learn negotiation-type dialogues where only
an approximate solution to the user?s goal exists.
Speech understanding errors are simulated at the di-
alogue act level using confusion matrices trained on
labelled dialogue data (Schatzmann et al, 2007).
3.3 Training and Evaluation
When training a system to operate robustly in noisy
conditions, a variety of strategies are possible. For
example, the system can be trained only on noise-
free interactions, it can be trained on increasing lev-
els of noise or it can be trained on a high noise level
from the outset. A related issue concerns the gener-
ation of grid points and the number of training itera-
tions to perform. For example, allowing a very large
number of points leads to poor performance due to
over-fitting of the training data. Conversely, having
too few point leads to poor performance due to a lack
115
of discrimination in its dialogue strategies.
After some experimentation, the following train-
ing schedule was adopted. Training starts in a
noise free environment using a small number of grid
points and it continues until the performance of the
policy levels off. The resulting policy is then taken
as an initial policy for the next stage where the noise
level is increased, the number of grid points is ex-
panded and the number of iterations is increased.
This process is repeated until the highest noise level
is reached. This approach was motivated by the ob-
servation that a key factor in effective reinforcement
learning is the balance between exploration and ex-
ploitation. In POMDP policy optimisation which
uses dynamically allocated grid points, maintaining
this balance is crucial. In our case, the noise intro-
duced by the simulator is used as an implicit mech-
anism for increasing the exploration. Each time ex-
ploration is increased, the areas of state-space that
will be visited will also increase and hence the num-
ber of available grid points must also be increased.
At the same time, the number of iterations must be
increased to ensure that all points are visited a suf-
ficient number of times. In practice we found that
around 750 to 1000 grid points was sufficient and
the total number of simulated dialogues needed for
training was around 100,000.
A second issue when training in noisy conditions
is whether to train on just the 1-best output from the
simulator or train on the N-best outputs. A limit-
ing factor here is that the computation required for
N-best training is significantly increased since the
rate of partition generation in the HIS model in-
creases exponentially with N. In preliminary tests,
it was found that when training with 1-best outputs,
there was little difference between policies trained
entirely in no noise and policies trained on increas-
ing noise as described above. However, policies
trained on 2-best using the incremental strategy did
exhibit increased robustness to noise. To illustrate
this, Figures 3 and 4 show the average dialogue suc-
cess rates and rewards for 3 different policies, all
trained on 2-best: a hand-crafted policy (hdc), a pol-
icy trained on noise-free conditions (noise free) and
a policy trained using the incremental scheme de-
scribed above (increm). Each policy was tested us-
ing 2-best output from the simulator across a range
of error rates. In addition, the noise-free policy was
also tested on 1-best output.
Figure 3: Average simulated dialogue success rate as a
function of error rate for a hand-crafted (hdc), noise-free
and incrementally trained (increm) policy.
Figure 4: Average simulated dialogue reward as a func-
tion of error rate for a hand-crafted (hdc), noise-free and
incrementally trained (increm) policy.
As can be seen, both the trained policies improve
significantly on the hand-crafted policies. Further-
more, although the average rewards are all broadly
similar, the success rate of the incrementally trained
policy is significantly better at higher error rates.
Hence, this latter policy was selected for the user
trial described next.
4 Evaluation via a User Trial
The HIS-POMDP policy (HIS-TRA) that was incre-
mentally trained on the simulated user using 2-best
lists was tested in a user trial together with a hand-
crafted HIS-POMDP policy (HIS-HDC). The strat-
egy used by the latter was to first check the most
likely hypothesis. If it contains sufficient grounded
116
keys to match 1 to 3 database entities, then offer is
selected. If any part of the hypothesis is inconsis-
tent or the user has explicitly asked for another sug-
gestion, then find alternative action is selected. If
the user has asked for information about an offered
entity then inform is selected. Otherwise, an un-
grounded component of the top hypothesis is identi-
fied and depending on the belief, one of the confirm
actions is selected.
In addition, an MDP-based dialogue manager de-
veloped for earlier trials (Schatzmann, 2008) was
also tested. Since considerable effort has been put in
optimising this system, it serves as a strong baseline
for comparison. Again, both a trained policy (MDP-
TRA) and a hand-crafted policy (MDP-HDC) were
tested.
4.1 System setup and confidence scoring
The dialogue system consisted of an ATK-based
speech recogniser, a Phoenix-based semantic parser,
the dialogue manager and a diphone based speech
synthesiser. The semantic parser uses simple phrasal
grammar rules to extract the dialogue act type and a
list of attribute/value pairs from each utterance.
In a POMDP-based dialogue system, accurate
belief-updating is very sensitive to the confidence
scores assigned to each user dialogue act. Ideally
these should provide a measure of the probability of
the decoded act given the true user act. In the evalu-
ation system, the recogniser generates a 10-best list
of hypotheses at each turn along with a compact con-
fusion network which is used to compute the infer-
ence evidence for each hypothesis. The latter is de-
fined as the sum of the log-likelihoods of each arc
in the confusion network and when exponentiated
and renormalised this gives a simple estimate of the
probability of each hypothesised utterance. Each ut-
terance in the 10-best list is passed to the semantic
parser. Equivalent dialogue acts output by the parser
are then grouped together and the dialogue act for
each group is then assigned the sum of the sentence-
level probabilities as its confidence score.
4.2 Trial setup
For the trial itself, 36 subjects were recruited (all
British native speakers, 18 male, 18 female). Each
subject was asked to imagine himself to be a tourist
in a fictitious town called Jasonville and try to find
particular hotels, bars, or restaurants in that town.
Each subject was asked to complete a set of pre-
defined tasks where each task involved finding the
name of a venue satisfying a set of constraints such
as food type is Chinese, price-range is cheap, etc.,
and getting the value of one or more additional at-
tributes of that venue such as the address or the
phone number.
For each task, subjects were given a scenario to
read and were then asked to solve the task via a di-
alogue with the system. The tasks set could either
have one solution, several solutions, or no solution
at all in the database. In cases where a subject found
that there was no matching venue for the given task,
he/she was allowed to try and find an alternative
venue by relaxing one or more of the constraints.
In addition, subjects had to perform each task at
one of three possible noise levels. These levels cor-
respond to signal/noise ratios (SNRs) of 35.3 dB
(low noise), 10.2 dB (medium noise), or 3.3 dB
(high noise). The noise was artificially generated
and mixed with the microphone signal, in addition
it was fed into the subject?s headphones so that they
were aware of the noisy conditions.
An instructor was present at all times to indicate
to the subject which task description to follow, and
to start the right system with the appropriate noise-
level. Each subject performed an equal number of
tasks for each system (3 tasks), noise level (6 tasks)
and solution type (6 tasks for each of the types 0, 1,
or multiple solutions). Also, each subject performed
one task for all combinations of system and noise
level. Overall, each combination of system, noise
level, and solution type was used in an equal number
of dialogues.
4.3 Results
In Table 1, some general statistics of the corpus re-
sulting from the trial are given. The semantic error
rate is based on substitutions, insertions and dele-
tions errors on semantic items. When tested after the
trial on the transcribed user utterances, the semantic
error rate was 4.1% whereas the semantic error rate
on the ASR input was 25.2%. This means that 84%
of the error rate was due to the ASR.
Tables 2 and 3 present success rates (Succ.) and
average performance scores (Perf.), comparing the
two HIS dialogue managers with the two MDP base-
117
Number of dialogues 432
Number of dialogue turns 3972
Number of words (transcriptions) 18239
Words per utterance 4.58
Word Error Rate 32.9
Semantic Error Rate 25.2
Semantic Error Rate transcriptions 4.1
Table 1: General corpus statistics.
line systems. For the success rates, also the stan-
dard deviation (std.dev) is given, assuming a bino-
mial distribution. The success rate is the percentage
of successfully completed dialogues. A task is con-
sidered to be fully completed when the user is able to
find the venue he is looking for and get al the addi-
tional information he asked for; if the task has no so-
lution and the system indicates to the user no venue
could be found, this also counts as full completion.
A task is considered to be partially completed when
only the correct venue has been given. The results on
partial completion are given in Table 2, and the re-
sults on full completion in Table 3. To mirror the re-
ward function used in training, the performance for
each dialogue is computed by assigning a reward of
20 points for full completion and subtracting 1 point
for the number of turns up until a successful recom-
mendation (i.e., partial completion).
Partial Task Completion statistics
System Succ. (std.dev) #turns Perf.
MDP-HDC 68.52 (4.83) 4.80 8.91
MDP-TRA 70.37 (4.75) 4.75 9.32
HIS-HDC 74.07 (4.55) 7.04 7.78
HIS-TRA 84.26 (3.78) 4.63 12.22
Table 2: Success rates and performance results on partial
completion.
Full Task Completion statistics
System Succ. (std.dev) #turns Perf.
MDP-HDC 64.81 (4.96) 5.86 7.10
MDP-TRA 65.74 (4.93) 6.18 6.97
HIS-HDC 63.89 (4.99) 8.57 4.20
HIS-TRA 78.70 (4.25) 6.36 9.38
Table 3: Success rates and performance results on full
completion.
The results show that the trained HIS dialogue
manager significantly outperforms both MDP based
dialogue managers. For success rate on partial com-
pletion, both HIS systems perform better than the
MDP systems.
4.3.1 Subjective Results
In the user trial, the subjects were also asked for
a subjective judgement of the systems. After com-
pleting each task, the subjects were asked whether
they had found the information they were looking
for (yes/no). They were also asked to give a score
on a scale from 1 to 5 (best) on how natural/intuitive
they thought the dialogue was. Table 4 shows the
results for the 4 systems used. The performance of
the HIS systems is similar to the MDP systems, with
a slightly higher success rate for the trained one and
a slightly lower score for the handcrafted one.
System Succ. Rate (std.dev) Score
MDP-HDC 78 (4.30) 3.52
MDP-TRA 78 (4.30) 3.42
HIS-HDC 71 (4.72) 3.05
HIS-TRA 83 (3.90) 3.41
Table 4: Subjective performance results from the user
trial.
5 Conclusions
This paper has described recent work in training a
POMDP-based dialogue manager to exploit the ad-
ditional information available from a speech under-
standing system which can generate ranked lists of
hypotheses. Following a brief overview of the Hid-
den Information State dialogue manager and pol-
icy optimisation using a user simulator, results have
been given for both simulated user and real user di-
alogues conducted at a variety of noise levels.
The user simulation results have shown that al-
though the rewards are similar, training with 2-best
rather than 1-best outputs from the user simulator
yields better success rates at high noise levels. In
view of this result, we would have liked to inves-
tigate training on longer N-best lists, but currently
computational constraints prevent this. We hope in
the future to address this issue by developing more
efficient state partitioning strategies for the HIS sys-
tem.
118
The overall results on real data collected from the
user trial clearly indicate increased robustness by the
HIS system. We would have liked to be able to
plot performance and success scores as a function
of noise level or speech understanding error rate,
but there is great variability in these kinds of com-
plex real-world dialogues and it transpired that the
trial data was insufficient to enable any statistically
meaningful presentation of this form. We estimate
that we need at least an order of magnitude more
trial data to properly investigate the behaviour of
such systems as a function of noise level. The trial
described here, including transcription and analysis
consumed about 30 man-days of effort. Increasing
this by a factor of 10 or more is not therefore an
option for us, and clearly an alternative approach is
needed.
We have also reported results of subjective suc-
cess rate and opinion scores based on data obtained
from subjects after each trial. The results were only
weakly correlated with the measured performance
and success rates. We believe that this is partly due
to confusion as to what constituted success in the
minds of the subjects. This suggests that for subjec-
tive results to be meaningful, measurements such as
these will only be really useful if made on live sys-
tems where users have a real rather than imagined
information need. The use of live systems would
also alleviate the data sparsity problem noted earlier.
Finally and in conclusion, we believe that despite
the difficulties noted above, the results reported in
this paper represent a first step towards establish-
ing the POMDP as a viable framework for develop-
ing spoken dialogue systems which are significantly
more robust to noisy operating conditions than con-
ventional state-based systems.
Acknowledgements
This research was partly funded by the UK EPSRC
under grant agreement EP/F013930/1 and by the
EU FP7 Programme under grant agreement 216594
(CLASSIC project: www.classic-project.org).
References
TH Bui, M Poel, A Nijholt, and J Zwiers. 2007a. A
tractable DDN-POMDP Approach to Affective Dia-
logue Modeling for General Probabilistic Frame-based
Dialogue Systems. In Proc 5th Workshop on Knowl-
edge and Reasoning in Practical Dialogue Systems,
pages 34?57.
TH Bui, B van Schooten, and D Hofs. 2007b. Practi-
cal dialogue manager development using POMDPs .
In 8th SIGdial Workshop on Discourse and Dialogue,
Antwerp.
LP Kaelbling, ML Littman, and AR Cassandra. 1998.
Planning and Acting in Partially Observable Stochastic
Domains. Artificial Intelligence, 101:99?134.
N Roy, J Pineau, and S Thrun. 2000. Spoken Dialogue
Management Using Probabilistic Reasoning. In Proc
ACL.
J Schatzmann, B Thomson, and SJ Young. 2007. Error
Simulation for Training Statistical Dialogue Systems.
In ASRU 07, Kyoto, Japan.
J Schatzmann. 2008. Statistical User and Error Mod-
elling for Spoken Dialogue Systems. Ph.D. thesis, Uni-
versity of Cambridge.
B Thomson, J Schatzmann, K Weilhammer, H Ye, and
SJ Young. 2007. Training a real-world POMDP-based
Dialog System. In HLT/NAACL Workshop ?Bridging
the Gap: Academic and Industrial Research in Dialog
Technologies?, Rochester.
B Thomson, J Schatzmann, and SJ Young. 2008.
Bayesian Update of Dialogue State for Robust Dia-
logue Systems. In Int Conf Acoustics Speech and Sig-
nal Processing ICASSP, Las Vegas.
JD Williams and SJ Young. 2007a. Partially Observable
Markov Decision Processes for Spoken Dialog Sys-
tems. Computer Speech and Language, 21(2):393?
422.
JD Williams and SJ Young. 2007b. Scaling POMDPs
for Spoken Dialog Management. IEEE Audio, Speech
and Language Processing, 15(7):2116?2129.
SJ Young, J Schatzmann, K Weilhammer, and H Ye.
2007. The Hidden Information State Approach to Dia-
log Management. In ICASSP 2007, Honolulu, Hawaii.
SJ Young. 2002. Talking to Machines (Statistically
Speaking). In Int Conf Spoken Language Processing,
Denver, Colorado.
B Zhang, Q Cai, J Mao, E Chang, and B Guo. 2001.
Spoken Dialogue Management as Planning and Acting
under Uncertainty. In Eurospeech, Aalborg, Denmark.
119
Proceedings of SIGDIAL 2009: the 10th Annual Meeting of the Special Interest Group in Discourse and Dialogue, pages 272?275,
Queen Mary University of London, September 2009. c?2009 Association for Computational Linguistics
k-Nearest Neighbor Monte-Carlo Control Algorithm
for POMDP-based Dialogue Systems
F. Lefe`vre?, M. Gas?ic?, F. Jurc???c?ek, S. Keizer, F. Mairesse, B. Thomson, K. Yu and S. Young
Spoken Dialogue Systems Group
Cambridge University Engineering Department
Trumpington Street, Cambridge CB2 1PZ, UK
{frfl2, mg436, fj228, sk561, farm2, brmt2, ky219, sjy}@eng.cam.ac.uk
Abstract
In real-world applications, modelling di-
alogue as a POMDP requires the use of
a summary space for the dialogue state
representation to ensure tractability. Sub-
optimal estimation of the value func-
tion governing the selection of system re-
sponses can then be obtained using a grid-
based approach on the belief space. In
this work, the Monte-Carlo control tech-
nique is extended so as to reduce training
over-fitting and to improve robustness to
semantic noise in the user input. This tech-
nique uses a database of belief vector pro-
totypes to choose the optimal system ac-
tion. A locally weighted k-nearest neigh-
bor scheme is introduced to smooth the de-
cision process by interpolating the value
function, resulting in higher user simula-
tion performance.
1 Introduction
In the last decade dialogue modelling as a Partially
Observable Markov Decision Process (POMDP)
has been proposed as a convenient way to improve
spoken dialogue systems (SDS) trainability, nat-
uralness and robustness to input errors (Young et
al., 2009). The POMDP framework models dia-
logue flow as a sequence of unobserved dialogue
states following stochastic moves, and provides a
principled way to model uncertainty.
However, to deal with uncertainty, POMDPs
maintain distributions over all possible states. But
then training an optimal policy is an NP hard
problem and thus not tractable for any non-trivial
application. In recent works this issue is ad-
dressed by mapping the dialog state representation
?Fabrice Lefe`vre is currently on leave from the Univer-
sity of Avignon, France.
space (the master space) into a smaller summary
space (Williams and Young, 2007). Even though
optimal policies remain out of reach, sub-optimal
solutions can be found by means of grid-based al-
gorithms.
Within the Hidden Information State (HIS)
framework (Young et al, 2009), policies are rep-
resented by a set of grid points in the summary be-
lief space. Beliefs in master space are first mapped
into summary space and then mapped into a sum-
mary action via the dialogue policy. The resulting
summary action is then mapped back into master
space and output to the user.
Methods which support interpolation between
points are generally required to scale well to large
state spaces (Pineau et al, 2003). In the current
version of the HIS framework, the policy chooses
the system action by associating each new belief
point with the single, closest, grid point. In the
present work, a k-nearest neighbour extension is
evaluated in which the policy decision is based on
a locally weighted regression over a subset of rep-
resentative grid points. This method thus lies be-
tween a strictly grid-based and a point-based value
iteration approach as it interpolates the value func-
tion around the queried belief point. It thus re-
duces the policy?s dependency on the belief grid
point selection and increases robustness to input
noise.
The next section gives an overview of the
CUED HIS POMDP dialogue system which we
extended for our experiments. In Section 3, the
grid-based approach to policy optimisation is in-
troduced followed by a presentation of the k-
nn Monte-Carlo policy optimization in Section 4,
along with an evaluation on a simulated user.
272
2 The CUED Spoken Dialogue System
2.1 System Architecture
The CUED HIS-based dialogue system pipelines
five modules: the ATK speech recogniser, an
SVM-based semantic tuple classifier, a POMDP
dialogue manager, a natural language generator,
and an HMM-based speech synthesiser. During
an interaction with the system, the user?s speech
is first decoded by the recogniser and an N-best
list of hypotheses is sent to the semantic classifier.
In turn the semantic classifier outputs an N-best
list of user dialogue acts. A dialogue act is a se-
mantic representation of the user action headed by
the user intention (such as inform, request,
etc) followed by a list of items (slot-value pairs
such as type=hotel, area=east etc). The
N-best list of dialogue acts is used by the dialogue
manager to update the dialogue state. Based on
the state hypotheses and the policy, a machine ac-
tion is determined, again in the form of a dialogue
act. The natural language generator translates the
machine action into a sentence, finally converted
into speech by the HMM synthesiser. The dia-
logue system is currently developed for a tourist
information domain (Towninfo). It is worth not-
ing that the dialogue manager does not contain any
domain-specific knowledge.
2.2 HIS Dialogue Manager
The unobserved dialogue state of the HIS dialogue
manager consists of the user goal, the dialogue his-
tory and the user action. The user goal is repre-
sented by a partition which is a tree structure built
according to the domain ontology. The nodes in
the partition consist mainly of slots and values.
When querying the venue database using the par-
tition, a set of matching entities can be produced.
The dialogue history consists of the grounding
states of the nodes in the partition, generated us-
ing a finite state automaton and the previous user
and system action. A hypothesis in the HIS ap-
proach is then a triple combining a partition, a user
action and the respective set of grounding states.
The distribution over all hypotheses is maintained
throughout the dialogue (belief state monitoring).
Considering the ontology size for any real-world
problem, the so-defined state space is too large for
any POMDP learning algorithm. Hence to obtain a
tractable policy, the state/action space needs to be
reduced to a smaller scale summary space. The set
of possible machine dialogue acts is also reduced
in summary space. This is mainly achieved by re-
Master Space
Masters Sppp c  uamyppp
eus Sers Sppp us SamypppMMM
Mast er S pr
cr S pr uSr pSm Mayt
Summary Space
Master Spcu rmymty
c
um
a
ycr
y rcsProceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 1552?1561,
Uppsala, Sweden, 11-16 July 2010. c?2010 Association for Computational Linguistics
Phrase-based Statistical Language Generation using
Graphical Models and Active Learning
Franc?ois Mairesse, Milica Gas?ic?, Filip Jurc???c?ek,
Simon Keizer, Blaise Thomson, Kai Yu and Steve Young?
Cambridge University Engineering Department, Trumpington Street, Cambridge, CB2 1PZ, UK
{f.mairesse, mg436, fj228, sk561, brmt2, ky219, sjy}@eng.cam.ac.uk
Abstract
Most previous work on trainable language
generation has focused on two paradigms:
(a) using a statistical model to rank a
set of generated utterances, or (b) using
statistics to inform the generation deci-
sion process. Both approaches rely on
the existence of a handcrafted generator,
which limits their scalability to new do-
mains. This paper presents BAGEL, a sta-
tistical language generator which uses dy-
namic Bayesian networks to learn from
semantically-aligned data produced by 42
untrained annotators. A human evalua-
tion shows that BAGEL can generate nat-
ural and informative utterances from un-
seen inputs in the information presentation
domain. Additionally, generation perfor-
mance on sparse datasets is improved sig-
nificantly by using certainty-based active
learning, yielding ratings close to the hu-
man gold standard with a fraction of the
data.
1 Introduction
The field of natural language generation (NLG) is
one of the last areas of computational linguistics to
embrace statistical methods. Over the past decade,
statistical NLG has followed two lines of research.
The first one, pioneered by Langkilde and Knight
(1998), introduces statistics in the generation pro-
cess by training a model which reranks candi-
date outputs of a handcrafted generator. While
their HALOGEN system uses an n-gram language
model trained on news articles, other systems have
used hierarchical syntactic models (Bangalore and
Rambow, 2000), models trained on user ratings of
?This research was partly funded by the UK EPSRC un-
der grant agreement EP/F013930/1 and funded by the EU
FP7 Programme under grant agreement 216594 (CLASSiC
project: www.classic-project.org).
utterance quality (Walker et al, 2002), or align-
ment models trained on speaker-specific corpora
(Isard et al, 2006).
A second line of research has focused on intro-
ducing statistics at the generation decision level,
by training models that find the set of genera-
tion parameters maximising an objective function,
e.g. producing a target linguistic style (Paiva and
Evans, 2005; Mairesse and Walker, 2008), gener-
ating the most likely context-free derivations given
a corpus (Belz, 2008), or maximising the expected
reward using reinforcement learning (Rieser and
Lemon, 2009). While such methods do not suffer
from the computational cost of an overgeneration
phase, they still require a handcrafted generator to
define the generation decision space within which
statistics can be used to find an optimal solution.
This paper presents BAGEL (Bayesian networks
for generation using active learning), an NLG sys-
tem that can be fully trained from aligned data.
While the main requirement of the generator is to
produce natural utterances within a dialogue sys-
tem domain, a second objective is to minimise the
overall development effort. In this regard, a major
advantage of data-driven methods is the shift of
the effort from model design and implementation
to data annotation. In the case of NLG systems,
learning to produce paraphrases can be facilitated
by collecting data from a large sample of annota-
tors. Our meaning representation should therefore
(a) be intuitive enough to be understood by un-
trained annotators, and (b) provide useful gener-
alisation properties for generating unseen inputs.
Section 2 describes BAGEL?s meaning represen-
tation, which satisfies both requirements. Sec-
tion 3 then details how our meaning representation
is mapped to a phrase sequence, using a dynamic
Bayesian network with backoff smoothing.
Within a given domain, the same semantic
concept can occur in different utterances. Sec-
tion 4 details how BAGEL exploits this redundancy
1552
to improve generation performance on sparse
datasets, by guiding the data collection process
using certainty-based active learning (Lewis and
Catlett, 1994). We train BAGEL in the informa-
tion presentation domain, from a corpus of utter-
ances produced by 42 untrained annotators (see
Section 5.1). An automated evaluation metric is
used to compare preliminary model and training
configurations in Section 5.2, while Section 5.3
shows that the resulting system produces natural
and informative utterances, according to 18 hu-
man judges. Finally, our human evaluation shows
that training using active learning significantly im-
proves generation performance on sparse datasets,
yielding results close to the human gold standard
using a fraction of the data.
2 Phrase-based generation from
semantic stacks
BAGEL uses a stack-based semantic representa-
tion to constrain the sequence of semantic con-
cepts to be searched. This representation can be
seen as a linearised semantic tree similar to the
one previously used for natural language under-
standing in the Hidden Vector State model (He
and Young, 2005). A stack representation provides
useful generalisation properties (see Section 3.1),
while the resulting stack sequences are relatively
easy to align (see Section 5.1). In the context of
dialogue systems, Table 1 illustrates how the input
dialogue act is first mapped to a set of stacks of
semantic concepts, and then aligned with a word
sequence. The bottom concept in the stack will
typically be a dialogue act type, e.g. an utterance
providing information about the object under dis-
cussion (inform) or specifying that the request
of the user cannot be met (reject). Other con-
cepts include attributes of that object (e.g., food,
area), values for those attributes (e.g., Chinese,
riverside), as well as special symbols for negat-
ing underlying concepts (e.g., not) or specifying
that they are irrelevant (e.g., dontcare).
The generator?s goal is thus finding the
most likely realisation given an unordered
set of mandatory semantic stacks Sm derived
from the input dialogue act. For example,
s =inform(area(centre)) is a mandatory stack
associated with the dialogue act in Table 1 (frame
8). While mandatory stacks must all be conveyed
in the output realisation, Sm does not contain the
optional intermediary stacks Si that can refer to
(a) general attributes of the object under discus-
sion (e.g., inform(area) in Table 1), or (b) to
concepts that are not in the input at all, which are
associated with the singleton stack inform (e.g.,
phrases expressing the dialogue act type, or clause
aggregation operations). For example, the stack
sequence in Table 1 contains 3 intermediary stacks
for t = 2, 5 and 7.
BAGEL?s granularity is defined by the semantic
annotation in the training data, rather than external
linguistic knowledge about what constitutes a unit
of meaning, i.e. contiguous words belonging to
the same semantic stack are modelled as an atomic
observation unit or phrase.1 In contrast with word-
level models, a major advantage of phrase-based
generation models is that they can model long-
range dependencies and domain-specific idiomatic
phrases with fewer parameters.
3 Dynamic Bayesian networks for NLG
Dynamic Bayesian networks have been used suc-
cessfully for speech recognition, natural language
understanding, dialogue management and text-to-
speech synthesis (Rabiner, 1989; He and Young,
2005; Lefe`vre, 2006; Thomson and Young, 2010;
Tokuda et al, 2000). Such models provide a
principled framework for predicting elements in a
large structured space, such as required for non-
trivial NLG tasks. Additionally, their probabilistic
nature makes them suitable for modelling linguis-
tic variation, i.e. there can be multiple valid para-
phrases for a given input.
BAGEL models the generation task as finding
the most likely sequence of realisation phrases
R? = (r1...rL) given an unordered set of manda-
tory semantic stacks Sm, with |Sm| ? L. BAGEL
must thus derive the optimal sequence of semantic
stacks S? that will appear in the utterance given
Sm, i.e. by inserting intermediary stacks if needed
and by performing content ordering. Any num-
ber of intermediary stacks can be inserted between
two consecutive mandatory stacks, as long as all
their concepts are included in either the previous
or following mandatory stack, and as long as each
stack transition leads to a different stack (see ex-
ample in Table 1). Let us define the set of possi-
ble stack sequences matching these constraints as
Seq(Sm) ? {S = (s1...sL) s.t. st ? Sm ? Si}.
We propose a model which estimates the dis-
1The term phrase is thus defined here as any sequence of
one or more words.
1553
Charlie Chan is a Chinese restaurant near Cineworld in the centre of town
Charlie Chan Chinese restaurant Cineworld centre
name food type near near area area
inform inform inform inform inform inform inform inform
t = 1 t = 2 t = 3 t = 4 t = 5 t = 6 t = 7 t = 8
Table 1: Example semantic stacks aligned with an utterance for the dialogue act
inform(name(Charlie Chan) type(restaurant) area(centre) food(Chinese) near(Cineworld)). Mandatory
stacks are in bold.
tribution P (R|Sm) from a training set of real-
isation phrases aligned with semantic stack se-
quences, by marginalising over all stack sequences
in Seq(Sm):
P (R|Sm) =
?
S?Seq(Sm)
P (R,S|Sm)
=
?
S?Seq(Sm)
P (R|S,Sm)P (S|Sm)
=
?
S?Seq(Sm)
P (R|S)P (S|Sm) (1)
Inference over the model defined in (1) requires
the decoding algorithm to consider all possible or-
derings over Seq(Sm) together with all possible
realisations, which is intractable for non-trivial do-
mains. We thus make the additional assumption
that the most likely sequence of semantic stacks
S? given Sm is the one yielding the optimal reali-
sation phrase sequence:
P (R|Sm) ? P (R|S
?)P (S?|Sm) (2)
with S? = argmax
S?Seq(Sm)
P (S|Sm) (3)
The semantic stacks are therefore decoded first
using the model in Fig. 1 to solve the argmax
in (3). The decoded stack sequence S? is then
treated as observed in the realisation phase, in
which the model in Fig. 2 is used to find the real-
isation phrase sequence R? maximising P (R|S?)
over all phrase sequences of length L = |S?| in
our vocabulary:
R? = argmax
R=(r1...rL)
P (R|S?)P (S?|Sm) (4)
= argmax
R=(r1...rL)
P (R|S?) (5)
In order to reduce model complexity, we fac-
torise our model by conditioning the realisation
phrase at time t on the previous phrase rt?1,
and the previous, current, and following semantic
stacks. The semantic stack st at time t is assumed
last mandatory 
stack
stack set 
validator
first frame
semantic 
stack s
stack set tracker
repeated frame final frame
Figure 1: Graphical model for the semantic decod-
ing phase. Plain arrows indicate smoothed proba-
bility distributions, dashed arrows indicate deter-
ministic relations, and shaded nodes are observed.
The generation of the end semantic stack symbol
deterministically triggers the final frame.
to depend only on the previous two stacks and the
last mandatory stack su ? Sm with 1 ? u < t:
P (S|Sm) =
?
?
?
?T
t=1 P (st|st?1, st?2, su)
if S ? Seq(Sm)
0 otherwise
(6)
P (R|S?) =
T?
t=1
P (rt|rt?1, s
?
t?1, s
?
t , s
?
t+1) (7)
While dynamic Bayesian networks typically
take sequential inputs, mapping a set of seman-
tic stacks to a sequence of phrases is achieved
by keeping track of the mandatory stacks that
were visited in the current sequence (see stack set
tracker variable in Fig. 1), and pruning any se-
quence that has not included all mandatory input
stacks on reaching the final frame (see observed
stack set validator variable in Fig. 1). Since the
number of intermediary stacks is not known at de-
coding time, the network is unrolled for a fixed
number of frames T defining the maximum num-
ber of phrases that can be generated (e.g., T =
50). The end of the stack sequence is then deter-
mined by a special end symbol, which can only
be emitted within the T frames once all mandatory
stacks have been visited. The probability of the re-
sulting utterance is thus computed over all frames
up to the end symbol, which determines the length
1554
L of S? and R?. While the decoding constraints
enforce that L > |Sm|, the search for S? requires
comparing sequences of different lengths. A con-
sequence is that shorter sequences containing only
mandatory stacks are likely to be favoured. While
future work should investigate length normalisa-
tion strategies, we find that the learned transition
probabilities are skewed enough to favour stack
sequences including intermediary stacks.
Once the topology and the decoding constraints
of the network have been defined, any inference al-
gorithm can be used to search for S? and R?. We
use the junction tree algorithm implemented in the
Graphical Model ToolKit (GMTK) for our exper-
iments (Bilmes and Zweig, 2002), however both
problems can be solved using a standard Viterbi
search given the appropriate state representation.
In terms of computational complexity, it is impor-
tant to note that the number of stack sequences
Seq(Sm) to search over increases exponentially
with the number of input mandatory stacks. Nev-
ertheless, we find that real-time performance can
be achieved by pruning low probability sequences,
without affecting the quality of the solution.
3.1 Generalisation to unseen semantic stacks
In order to generalise to semantic stacks which
have not been observed during training, the re-
alisation phrase r is made dependent on under-
specified stack configurations, i.e. the tail l
and the head h of the stack. For example, the
last stack in Table 1 is associated with the head
centre and the tail inform(area). As a re-
sult, BAGEL assigns non-zero probabilities to re-
alisation phrases in unseen semantic contexts, by
backing off to the head and the tail of the stack.
A consequence is that BAGEL?s lexical realisa-
tion can generalise across contexts. For exam-
ple, if reject(area(centre)) was never ob-
served at training time, P (r = centre of town|s =
reject(area(centre))) will be estimated by
backing off to P (r = centre of town|h =
centre). BAGEL can thus generate ?there are
no venues in the centre of town? if the phrase
?centre of town? was associated with the con-
cept centre in a different context, such as
inform(area(centre)). The final realisation
model is illustrated in Fig. 2:
realisation phrase r
repeated frame final framefirst frame
stack head h
semantic 
stack s
stack tail l
Figure 2: Graphical model for the realisation
phase. Dashed arrows indicate deterministic re-
lations, and shaded node are observed.
!"#$%&& '(")*+
11111 ,,,,,,,| +?+?? ttttttttt sssllrlhr
ttttttt sllrlhr ,,,,,| 111 +??
111 ,,,,| +?? tttttt llrlhr
ttt lhr ,|21,| ?? ttt sss
uttt ssss ,,| 21 ??
tt hr |1| ?tt ss
trts
Figure 3: Backoff graphs for the semantic decod-
ing and realisation models.
P (R|S?) =
L?
t=1
P (rt|rt?1, ht, lt?1, lt, lt+1,
s?t?1, s
?
t , s
?
t+1) (8)
Conditional probability distributions are repre-
sented as factored language models smoothed us-
ing Witten-Bell interpolated backoff smoothing
(Bilmes and Kirchhoff, 2003), according to the
backoff graphs in Fig. 3. Variables which are the
furthest away in time are dropped first, and par-
tial stack variables are dropped last as they are ob-
served the most.
It is important to note that generating unseen se-
mantic stacks requires all possible mandatory se-
mantic stacks in the target domain to be prede-
fined, in order for all stack unigrams to be assigned
a smoothed non-zero probability.
3.2 High cardinality concept abstraction
While one should expect a trainable generator
to learn multiple lexical realisations for low-
cardinality semantic concepts, learning lexical
realisations for high-cardinality database entries
(e.g., proper names) would increase the number of
model parameters prohibitively. We thus divide
pre-terminal concepts in the semantic stacks into
two types: (a) enumerable attributes whose val-
ues are associated with distinct semantic stacks in
1555
our model (e.g., inform(pricerange(cheap))),
and (b) non-enumerable attributes whose values
are replaced by a generic symbol before train-
ing in both the utterance and the semantic stack
(e.g., inform(name(X)). These symbolic values
are then replaced in the surface realisation by the
corresponding value in the input specification. A
consequence is that our model can only learn syn-
onymous lexical realisations for enumerable at-
tributes.
4 Certainty-based active learning
A major issue with trainable NLG systems is the
lack of availability of domain-specific data. It is
therefore essential to produce NLG models that
minimise the data annotation cost.
BAGEL supports the optimisation of the data
collection process through active learning, in
which the next semantic input to annotate is de-
termined by the current model. The probabilis-
tic nature of BAGEL allows the use of certainty-
based active learning (Lewis and Catlett, 1994),
by querying the k semantic inputs for which the
model is the least certain about its output real-
isation. Given a finite semantic input space I
representing all possible dialogue acts in our do-
main (i.e., the set of all sets of mandatory seman-
tic stacks Sm), BAGEL?s active learning training
process iterates over the following steps:
1. Generate an utterance for each semantic input Sm ? I
using the current model.2
2. Annotate the k semantic inputs {S1m...S
k
m} yielding
the lowest realisation probability, i.e. for q ? (1..k)
Sqm = argmin
Sm?I\{S1m...S
q?1
m }
(max
R
P (R|Sm)) (9)
with P (R|Sm) defined in (2).
3. Retrain the model with the additional k data points.
The number of utterances to be queried k should
depend on the flexibility of the annotators and the
time required for generating all possible utterances
in the domain.
5 Experimental method
BAGEL?s factored language models are trained us-
ing the SRILM toolkit (Stolcke, 2002), and de-
coding is performed using GMTK?s junction tree
inference algorithm (Bilmes and Zweig, 2002).
2Sampling methods can be used if I is infinite or too
large.
Since each active learning iteration requires gen-
erating all training utterances in our domain, they
are generated using a larger clique pruning thresh-
old than the test utterances used for evaluation.
5.1 Corpus collection
We train BAGEL in the context of a dialogue
system providing information about restaurants
in Cambridge. The domain contains two dia-
logue act types: (a) inform: presenting infor-
mation about a restaurant (see Table 1), and (b)
reject: informing that the user?s constraints can-
not be met (e.g., ?There is no cheap restaurant
in the centre?). Our domain contains 8 restau-
rant attributes: name, food, near, pricerange,
postcode, phone, address, and area, out of
which food, pricerange, and area are treated
as enumerable.3 Our input semantic space is ap-
proximated by the set of information presentation
dialogue acts produced over 20,000 simulated di-
alogues between our statistical dialogue manager
(Young et al, 2010) and an agenda-based user
simulator (Schatzmann et al, 2007), which results
in 202 unique dialogue acts after replacing non-
enumerable values by a generic symbol. Each di-
alogue act contains an average of 4.48 mandatory
semantic stacks.
As one of our objectives is to test whether
BAGEL can learn from data provided by a large
sample of untrained annotators, we collected a
corpus of semantically-aligned utterances using
Amazon?s Mechanical Turk data collection ser-
vice. A crucial aspect of data collection for
NLG is to ensure that the annotators under-
stand the meaning of the semantics to be con-
veyed. Annotators were first asked to provide
an utterance matching an abstract description
of the dialogue act, regardless of the order in
which the constraints are presented (e.g., Offer
the venue Taj Mahal and provide the information
type(restaurant), area(riverside), food(Indian),
near(The Red Lion)). The order of the constraints
in the description was randomised to reduce the
effect of priming. The annotators were then asked
to align the attributes (e.g., Indicate the region of
the utterance related to the concept ?area?), and
the attribute values (e.g., Indicate only the words
related to the concept ?riverside?). Two para-
phrases were collected for each dialogue act in
our domain, resulting in a total of 404 aligned ut-
3With the exception of areas defined as proper nouns.
1556
rt st ht lt
<s> START START START
The Rice Boat inform(name(X)) X inform(name)
is a inform inform EMPTY
restaurant inform(type(restaurant)) restaurant inform(type)
in the inform(area) area inform
riverside inform(area(riverside)) riverside inform(area)
area inform(area) area inform
that inform inform EMPTY
serves inform(food) food inform
French inform(food(French)) French inform(food)
food inform(food) food inform
</s> END END END
Table 2: Example utterance annotation used to estimate the conditional probability distributions of the
models in Figs. 1 and 2 ( rt=realisation phrase, st=semantic stack, ht=stack head, lt=stack tail).
terances produced by 42 native speakers of En-
glish. After manually checking and normalising
the dataset,4 the layered annotations were auto-
matically mapped to phrase-level semantic stacks
by splitting the utterance into phrases at annotation
boundaries. Each annotated utterance is then con-
verted into a sequence of symbols such as in Ta-
ble 2, which are used to estimate the conditional
probability distributions defined in (6) and (8).
The resulting vocabulary consists of 52 distinct se-
mantic stacks and 109 distinct realisation phrases,
with an average of 8.35 phrases per utterance.
5.2 BLEU score evaluation
We first evaluate BAGEL using the BLEU auto-
mated metric (Papineni et al, 2002), which mea-
sures the word n-gram overlap between the gen-
erated utterances and the 2 reference paraphrases
over a test corpus (with n up to 4). While BLEU
suffers from known issues such as a bias towards
statistical NLG systems (Reiter and Belz, 2009), it
provides useful information when comparing sim-
ilar systems. We evaluate BAGEL for different
training set sizes, model dependencies, and active
learning parameters. Our results are averaged over
a 10-fold cross-validation over distinct dialogue
acts, i.e. dialogue acts used for testing are not seen
at training time,5 and all systems are tested on the
same folds. The training and test sets respectively
contain an average of 181 and 21 distinct dialogue
acts, and each dialogue act is associated with two
paraphrases, resulting in 362 training utterances.
4The normalisation process took around 4 person-hour for
404 utterances.
5We do not evaluate performance on dialogue acts used
for training, as the training examples can trivially be used as
generation templates.
!"#$
!"%
!"%$
!"#
!
"
#
$
%
&
'
(
)
%
*
+
,
-
"
!"$
!"$$
!
"
#
$
%
&
'
(
)
%
*
+
,
-
"
!
"
#
$
%
&
'
(
)
%
*
+
,
-
"
&'(()*+,-(
!".
!".$!"
#
$
%
&
'
(
)
%
*
+
,
-
"
/+)01234)5234+66/+)01234)5234+667)8+)6'1'9-)0-*281:30!";$ <! =! .! #! >! <!! <=! <$! =!! =$! ;!! ;#=
!
"
#
$
%
&
'
(
)
%
*
+
,
-
"
.-#/$/$0%*"1%*/2"
!
"
#
$
%
&
'
(
)
%
*
+
,
-
"
!
"
#
$
%
&
'
(
)
%
*
+
,
-
"
Figure 4: BLEU score averaged over a 10-fold
cross-validation for different training set sizes and
network topologies, using random sampling.
Results: Fig. 4 shows that adding a dependency
on the future semantic stack improves perfor-
mances for all training set sizes, despite the added
model complexity. Backing off to partial stacks
also improves performance, but only for sparse
training sets.
Fig. 5 compares the full model trained using
random sampling in Fig. 4 with the same model
trained using certainty-based active learning, for
different values of k. As our dataset only con-
tains two paraphrases per dialogue act, the same
dialogue act can only be queried twice during the
active learning procedure. A consequence is that
the training set used for active learning converges
towards the randomly sampled set as its size in-
creases. Results show that increasing the train-
ing set one utterance at a time using active learn-
ing (k = 1) significantly outperforms random
sampling when using 40, 80, and 100 utterances
(p < .05, two-tailed). Increasing the number of
utterances to be queried at each iteration to k = 10
results in a smaller performance increase. A possi-
1557
!"#
!"##
!"$
!"$#
!"%
!"%#
!
"
#
$
%
&
'
(
)
%
*
+
,
-
"
&'()*+,-'+./0(1
!"2#
!"3
!"3#
4! 5! 3! $! 6! 4!! 45! 4#! 5!! 5#! 2!! 2$5
!
"
#
$
%
&
'
(
)
%
*
+
,
-
"
.-#/$/$0%*"1%*/2"
7890:;,/;'<(0(1,=>47890:;,/;'<(0(1,=>4!
Figure 5: BLEU score averaged over a 10-fold
cross-validation for different numbers of queries
per iteration, using the full model with the query
selection criterion (9).
!"#
!"##
!"$
!"$#
!"%
!"%#
!
"
#
$
%
&
'
(
)
%
*
+
,
-
"
&'(()(*+,-*.
!"/#
!"0
!"0#
1! 2! 0! $! 3! 1!! 12! 1#! 2!! 2#! /!! /$2
!
"
#
$
%
&
'
(
)
%
*
+
,
-
"
.-#/$/$0%*"1%*/2"
4*+,-*.),5-)6-7854*9+5:;)<9,';)6<-:;
Figure 6: BLEU score averaged over a 10-fold
cross-validation for different query selection cri-
teria, using the full model with k = 1.
ble explanation is that the model is likely to assign
low probabilities to similar inputs, thus any value
above k = 1 might result in redundant queries
within an iteration.
As the length of the semantic stack sequence
is not known before decoding, the active learn-
ing selection criterion presented in (9) is biased
towards longer utterances, which tend to have a
lower probability. However, Fig. 6 shows that
normalising the log probability by the number of
semantic stacks does not improve overall learn-
ing performance. Although a possible explanation
is that longer inputs tend to contain more infor-
mation to learn from, Fig. 6 shows that a base-
line selecting the largest remaining semantic input
at each iteration performs worse than the active
learning scheme for training sets above 20 utter-
ances. The full log probability selection criterion
defined in (9) is therefore used throughout the rest
of the paper (with k = 1).
5.3 Human evaluation
While automated metrics provide useful informa-
tion for comparing different systems, human feed-
back is needed to assess (a) the quality of BAGEL?s
outputs, and (b) whether training models using ac-
tive learning has a significant impact on user per-
ceptions. We evaluate BAGEL through a large-
scale subjective rating experiment using Amazon?s
Mechanical Turk service.
For each dialogue act in our domain, partici-
pants are presented with a ?gold standard? human
utterance from our dataset, which they must com-
pare with utterances generated by models trained
with and without active learning on a set of 20, 40,
100, and 362 utterances (full training set), as well
as with the second human utterance in our dataset.
See example utterances in Table 3. The judges are
then asked to evaluate the informativeness and nat-
uralness of each of the 8 utterances on a 5 point
likert-scale. Naturalness is defined as whether the
utterance could have been produced by a human,
and informativeness is defined as whether it con-
tains all the information in the gold standard utter-
ance. Each utterance is taken from the test folds of
the cross-validation experiment presented in Sec-
tion 5.2, i.e. the models are trained on up to 90%
of the data and the training set does not contain the
dialogue act being tested.
Results: Figs. 7 and 8 compare the naturalness
and informativeness scores of each system aver-
aged over all 202 dialogue acts. A paired t-test
shows that models trained on 40 utterances or
less produce utterances that are rated significantly
lower than human utterances for both naturalness
and informativeness (p < .05, two-tailed). How-
ever, models trained on 100 utterances or more do
not perform significantly worse than human utter-
ances for both dimensions, with a mean difference
below .10 over 202 comparisons. Given the large
sample size, this result suggests that BAGEL can
successfully learn our domain using a fraction of
our initial dataset.
As far as the learning method is concerned, a
paired t-test shows that models trained on 20 and
40 utterances using active learning significantly
outperform models trained using random sam-
pling, for both dimensions (p < .05). The largest
increase is observed using 20 utterances, i.e. the
naturalness increases by .49 and the informative-
ness by .37. When training on 100 utterances, the
effect of active learning becomes insignificant. In-
1558
Input inform(name(the Fountain) near(the Arts Picture House) area(centre) pricerange(cheap))
Human There is an inexpensive restaurant called the Fountain in the centre of town near the Arts Picture House
Rand-20 The Fountain is a restaurant near the Arts Picture House located in the city centre cheap price range
Rand-40 The Fountain is a restaurant in the cheap city centre area near the Arts Picture House
AL-20 The Fountain is a restaurant near the Arts Picture House in the city centre cheap
AL-40 The Fountain is an affordable restaurant near the Arts Picture House in the city centre
Full set The Fountain is a cheap restaurant in the city centre near the Arts Picture House
Input reject(area(Barnwell) near(Saint Mary?s Church))
Human I am sorry but I know of no venues near Saint Mary?s Church in the Barnwell area
Full set I am sorry but there are no venues near Saint Mary?s Church in the Barnwell area
Input inform(name(the Swan)area(Castle Hill) pricerange(expensive))
Human The Swan is a restaurant in Castle Hill if you are seeking something expensive
Full set The Swan is an expensive restaurant in the Castle Hill area
Input inform(name(Browns) area(centre) near(the Crowne Plaza) near(El Shaddai) pricerange(cheap))
Human Browns is an affordable restaurant located near the Crowne Plaza and El Shaddai in the centre of the city
Full set Browns is a cheap restaurant in the city centre near the Crowne Plaza and El Shaddai
Table 3: Example utterances for different input dialogue acts and system configurations. AL-20 = active
learning with 20 utterances, Rand = random sampling.
!"## !"$%
!"&'!"(% !")* *"%% *"%#
*"%'
++"$
!!"$
**"$
$
!
"
#
$
%
$
#
&
'
(
#
)
$
"
*
*
%
*
+
,
(
"
,-./01
##"$ +% *% #%% !(+
!
"
#
$
%
$
#
&
'
(
#
)
$
"
*
*
%
*
+
,
(
"
-(#.$.$/%*"&%*.0"
234567897-:.5.;<=1-.8=447:-.378>8*"%'
Figure 7: Naturalness mean opinion scores for dif-
ferent training set sizes, using random sampling
and active learning. Differences for training set
sizes of 20 and 40 are all significant (p < .05).
terestingly, while models trained on 100 utterances
outperform models trained on 40 utterances using
random sampling (p < .05), they do not signifi-
cantly outperform models trained on 40 utterances
using active learning (p = .15 for naturalness and
p = .41 for informativeness). These results sug-
gest that certainty-based active learning is benefi-
cial for training a generator from a limited amount
of data given the domain size.
Looking back at the results presented in Sec-
tion 5.2, we find that the BLEU score correlates
with a Pearson correlation coefficient of .42 with
the mean naturalness score and .35 with the mean
informativeness score, over all folds of all systems
tested (n = 70, p < .01). This is lower than
previous correlations reported by Reiter and Belz
(2009) in the shipping forecast domain with non-
expert judges (r = .80), possibly because our do-
main is larger and more open to subjectivity.
!"## !"$$
#"%&!"'& !"()
#"%$ #"%#
#"&!
**"+
!!"+
##"+
+
!
"
#
$
%
&
$
'
(
)
*
#
+
&
,
"
$
"
-
-
%
-
.
(
)
"
,-./01
&&"+ *% #% &%% !)*
!
"
#
$
%
&
$
'
(
)
*
#
+
&
,
"
$
"
-
-
%
-
.
(
)
"
/)#&$&$0%-"+%-&1"
234567897-:.5.;<=1-.8=447:-.378>8#"&!
Figure 8: Informativeness mean opinion scores for
different training set sizes, using random sampling
and active learning. Differences for training set
sizes of 20 and 40 are all significant (p < .05).
6 Related work
While most previous work on trainable NLG re-
lies on a handcrafted component (see Section 1),
recent research has started exploring fully data-
driven NLG models.
Factored language models have recently been
used for surface realisation within the OpenCCG
framework (White et al, 2007; Espinosa et al,
2008). More generally, chart generators for
different grammatical formalisms have been
trained from syntactic treebanks (White et al,
2007; Nakanishi et al, 2005), as well as from
semantically-annotated treebanks (Varges and
Mellish, 2001). However, a major difference with
our approach is that BAGEL uses domain-specific
data to generate a surface form directly from se-
mantic concepts, without any syntactic annotation
(see Section 7 for further discussion).
1559
This work is strongly related to Wong and
Mooney?s WASP?1 generation system (2007),
which combines a language model with an in-
verted synchronous CFG parsing model, effec-
tively casting the generation task as a translation
problem from a meaning representation to natu-
ral language. WASP?1 relies on GIZA++ to align
utterances with derivations of the meaning repre-
sentation (Och and Ney, 2003). Although early
experiments showed that GIZA++ did not perform
well on our data?possibly because of the coarse
granularity of our semantic representation?future
work should evaluate the generalisation perfor-
mance of synchronous CFGs in a dialogue system
domain.
Although we do not know of any work on ac-
tive learning for NLG, previous work has used
active learning for semantic parsing and informa-
tion extraction (Thompson et al, 1999; Tang et al,
2002), spoken language understanding (Tur et al,
2003), speech recognition (Hakkani-Tu?r et al,
2002), word alignment (Sassano, 2002), and more
recently for statistical machine translation (Blood-
good and Callison-Burch, 2010). While certainty-
based methods have been widely used, future work
should investigate the performance of committee-
based active learning for NLG, in which examples
are selected based on the level of disagreement be-
tween models trained on subsets of the data (Fre-
und et al, 1997).
7 Discussion and conclusion
This paper presents and evaluates BAGEL, a sta-
tistical language generator that can be trained en-
tirely from data, with no handcrafting required be-
yond the semantic annotation. All the required
subtasks?i.e. content ordering, aggregation, lex-
ical selection and realisation?are learned from
data using a unified model. To train BAGEL in a di-
alogue system domain, we propose a stack-based
semantic representation at the phrase level, which
is expressive enough to generate natural utterances
from unseen inputs, yet simple enough for data to
be collected from 42 untrained annotators with a
minimal normalisation step. A human evaluation
over 202 dialogue acts does not show any differ-
ence in naturalness and informativeness between
BAGEL?s outputs and human utterances. Addition-
ally, we find that the data collection process can
be optimised using active learning, resulting in a
significant increase in performance when training
data is limited, according to ratings from 18 hu-
man judges.6 These results suggest that the pro-
posed framework can largely reduce the develop-
ment time of NLG systems.
While this paper only evaluates the most likely
realisation given a dialogue act, we believe that
BAGEL?s probabilistic nature and generalisation
capabilities are well suited to model the linguis-
tic variation resulting from the diversity of annota-
tors. Our first objective is thus to evaluate the qual-
ity of BAGEL?s n-best outputs, and test whether
sampling from the output distribution can improve
naturalness and user satisfaction within a dialogue.
Our results suggest that explicitly modelling
syntax is not necessary for our domain, possi-
bly because of the lack of syntactic complexity
compared with formal written language. Never-
theless, future work should investigate whether
syntactic information can improve performance in
more complex domains. For example, the reali-
sation phrase can easily be conditioned on syntac-
tic constructs governing that phrase, and the recur-
sive nature of syntax can be modelled by keeping
track of the depth of the current embedded clause.
While syntactic information can be included with
no human effort by using syntactic parsers, their
robustness to dialogue system utterances must first
be evaluated.
Finally, recent years have seen HMM-based
synthesis models become competitive with unit se-
lection methods (Tokuda et al, 2000). Our long
term objective is to take advantage of those ad-
vances to jointly optimise the language genera-
tion and the speech synthesis process, by combin-
ing both components into a unified probabilistic
concept-to-speech generation model.
References
S. Bangalore and O. Rambow. Exploiting a probabilistic hi-
erarchical model for generation. In Proceedings of the
18th International Conference on Computational Linguis-
tics (COLING), pages 42?48, 2000.
A. Belz. Automatic generation of weather forecast texts us-
ing comprehensive probabilistic generation-space models.
Natural Language Engineering, 14(4):431?455, 2008.
J. Bilmes and K. Kirchhoff. Factored language models and
generalized parallel backoff. In Proceedings of HLT-
NAACL, short papers, 2003.
J. Bilmes and G. Zweig. The Graphical Models ToolKit: An
open source software system for speech and time-series
processing. In Proceedings of ICASSP, 2002.
6The full training corpus and the generated
utterances used for evaluation are available at
http://mi.eng.cam.ac.uk/?farm2/bagel.
1560
M. Bloodgood and C. Callison-Burch. Bucking the trend:
Large-scale cost-focused active learning for statistical ma-
chine translation. In Proceedings of the 48th Annual
Meeting of the Association for Computational Linguistics
(ACL), 2010.
D. Espinosa, M. White, and D. Mehay. Hypertagging: Su-
pertagging for surface realization with CCG. In Proceed-
ings of the 46th Annual Meeting of the Association for
Computational Linguistics (ACL), 2008.
Y. Freund, H. S. Seung, E.Shamir, and N. Tishby. Selective
sampling using the query by committee algorithm. Ma-
chine Learning, 28:133?168, 1997.
D. Hakkani-Tu?r, G. Riccardi, and A. Gorin. Active learn-
ing for automatic speech recognition. In Proceedings of
ICASSP, 2002.
Y. He and S. Young. Semantic processing using the Hidden
Vector State model. Computer Speech & Language, 19
(1):85?106, 2005.
A. Isard, C. Brockmann, and J. Oberlander. Individuality and
alignment in generated dialogues. In Proceedings of the
4th International Natural Language Generation Confer-
ence (INLG), pages 22?29, 2006.
I. Langkilde and K. Knight. Generation that exploits corpus-
based statistical knowledge. In Proceedings of the 36th
Annual Meeting of the Association for Computational Lin-
guistics (ACL), pages 704?710, 1998.
F. Lefe`vre. A DBN-based multi-level stochastic spoken lan-
guage understanding system. In Proceedings of the IEEE
Workshop on Spoken Language Technology (SLT), 2006.
D. D. Lewis and J. Catlett. Heterogeneous uncertainty am-
pling for supervised learning. In Proceedings of ICML,
1994.
F. Mairesse and M. A. Walker. Trainable generation of Big-
Five personality styles through data-driven parameter esti-
mation. In Proceedings of the 46th Annual Meeting of the
Association for Computational Linguistics (ACL), 2008.
H. Nakanishi, Y. Miyao, , and J. Tsujii. Probabilistic methods
for disambiguation of an HPSG-based chart generator. In
Proceedings of the IWPT, 2005.
F. J. Och and H. Ney. A systematic comparison of various
statistical alignment models. Computational Linguistics,
29(1):19?51, 2003.
D. S. Paiva and R. Evans. Empirically-based control of nat-
ural language generation. In Proceedings of the 43rd An-
nual Meeting of the Association for Computational Lin-
guistics (ACL), pages 58?65, 2005.
K. Papineni, S. Roukos, T. Ward, and W. J. Zhu. BLEU: a
method for automatic evaluation of machine translation. In
Proceedings of the 40th Annual Meeting of the Association
for Computational Linguistics (ACL), 2002.
L. R. Rabiner. Tutorial on Hidden Markov Models and se-
lected applications in speech recognition. Proceedings of
the IEEE, 77(2):257?285, 1989.
E. Reiter and A. Belz. An investigation into the validity
of some metrics for automatically evaluating natural lan-
guage generation systems. Computational Linguistics, 25:
529?558, 2009.
V. Rieser and O. Lemon. Natural language generation as
planning under uncertainty for spoken dialogue systems.
In Proceedings of the Annual Meeting of the European
Chapter of the ACL (EACL), 2009.
M. Sassano. An empirical study of active learning with sup-
port vector machines for japanese word segmentation. In
Proceedings of the 40th Annual Meeting of the Association
for Computational Linguistics (ACL), 2002.
J. Schatzmann, B. Thomson, K. Weilhammer, H. Ye, and
S. Young. Agenda-based user simulation for bootstrap-
ping a POMDP dialogue system. In Proceedings of HLT-
NAACL, short papers, pages 149?152, 2007.
A. Stolcke. SRILM ? an extensible language modeling
toolkit. In Proceedings of the International Conference
on Spoken Language Processing, 2002.
M. Tang, X. Luo, and S. Roukos. Active learning for statis-
tical natural language parsing. In Proceedings of the 40th
Annual Meeting of the Association for Computational Lin-
guistics (ACL), 2002.
C. Thompson, M. E. Califf, and R. J. Mooney. Active learn-
ing for natural language parsing and information extrac-
tion. In Proceedings of ICML, 1999.
B. Thomson and S. Young. Bayesian update of dialogue state:
A POMDP framework for spoken dialogue systems. Com-
puter Speech & Language, 24(4):562?588, 2010.
Y. Tokuda, T. Yoshimura, T. Masuko, T. Kobayashi, and
T. Kitamura. Speech parameter generation algorithms for
HMM-based speech synthesis. In Proceedings of ICASSP,
2000.
G. Tur, R. E. Schapire, and D. Hakkani-Tu?r. Active learn-
ing for spoken language understanding. In Proceedings of
ICASSP, 2003.
S. Varges and C. Mellish. Instance-based natural language
generation. In Proceedings of the Annual Meeting of the
North American Chapter of the ACL (NAACL), 2001.
M. A. Walker, O. Rambow, and M. Rogati. Training a sen-
tence planner for spoken dialogue using boosting. Com-
puter Speech and Language, 16(3-4), 2002.
M. White, R. Rajkumar, and S. Martin. Towards broad cov-
erage surface realization with CCG. In Proceedings of the
Workshop on Using Corpora for NLG: Language Genera-
tion and Machine Translation, 2007.
Y. W. Wong and R. Mooney. Generation by inverting a se-
mantic parser that uses statistical machine translation. In
Proceedings of HLT-NAACL, 2007.
S. Young, M. Gas?ic?, S. Keizer, F. Mairesse, J. Schatzmann,
B. Thomson, and K. Yu. The Hidden Information State
model: a practical framework for POMDP-based spoken
dialogue management. Computer Speech and Language,
24(2):150?174, 2010.
1561
Proceedings of SIGDIAL 2010: the 11th Annual Meeting of the Special Interest Group on Discourse and Dialogue, pages 116?123,
The University of Tokyo, September 24-25, 2010. c?2010 Association for Computational Linguistics
Parameter estimation for agenda-based user simulation
Simon Keizer, Milica Gas?ic?, Filip Jurc???c?ek, Franc?ois Mairesse,
Blaise Thomson, Kai Yu, and Steve Young ?
University of Cambridge, Department of Engineering, Cambridge (UK)
{sk561,mg436,fj228,farm2,brmt2,ky219,sjy}@cam.ac.uk
Abstract
This paper presents an agenda-based user
simulator which has been extended to be
trainable on real data with the aim of more
closely modelling the complex rational be-
haviour exhibited by real users. The train-
able part is formed by a set of random de-
cision points that may be encountered dur-
ing the process of receiving a system act
and responding with a user act. A sample-
based method is presented for using real
user data to estimate the parameters that
control these decisions. Evaluation results
are given both in terms of statistics of gen-
erated user behaviour and the quality of
policies trained with different simulators.
Compared to a handcrafted simulator, the
trained system provides a much better fit
to corpus data and evaluations suggest that
this better fit should result in improved di-
alogue performance.
1 Introduction
In spoken dialogue systems research, modelling
dialogue as a (Partially Observable) Markov Deci-
sion Process ((PO)MDP) and using reinforcement
learning techniques for optimising dialogue poli-
cies has proven to be an effective method for de-
veloping robust systems (Singh et al, 2000; Levin
et al, 2000). However, since this kind of optimi-
sation requires a simulated user to generate a suffi-
ciently large number of interactions to learn from,
this effectiveness depends largely on the quality
of such a user simulator. An important require-
ment for a simulator is for it to be realistic, i.e., it
should generate behaviour that is similar to that of
real users. Trained policies are then more likely to
perform better on real users, and evaluation results
on simulated data are more likely to predict results
on real data more accurately.
?This research was partly funded by the UK EPSRC un-
der grant agreement EP/F013930/1 and by the EU FP7 Pro-
gramme under grant agreement 216594 (CLASSiC project:
www.classic-project.org).
This is one of the reasons why learning user
simulation models from data on real user be-
haviour has become an important direction of re-
search (Scheffler and Young, 2001; Cuaya?huitl et
al., 2005; Georgila et al, 2006). However, the data
driven user models developed so far lack the com-
plexity required for training high quality policies
in task domains where user behaviour is relatively
complex. Handcrafted models are still the most
effective in those cases.
This paper presents an agenda-based user simu-
lator which is handcrafted for a large part, but ad-
ditionally can be trained with data from real users
(Section 2). As a result, it generates behaviour that
better reflects the statistics of real user behaviour,
whilst preserving the complexity and rationality
required to effectively train dialogue management
policies. The trainable part is formed by a set of
random decision points, which, depending on the
context, may or may not be encountered during
the process of receiving a system act and decid-
ing on a response act. If such a point is encoun-
tered, the simulator makes a random decision be-
tween a number of options which may directly or
indirectly influence the resulting output. The op-
tions for each random decision point are reason-
able in the context in which it is encountered, but
a uniform distribution of outcomes might not re-
flect real user behaviour.
We will describe a sample-based method for es-
timating the parameters that define the probabili-
ties for each possible decision, using data on real
users from a corpus of human-machine dialogues
(Section 3). Evaluation results will be presented
both in terms of statistics on generated user be-
haviour and the quality of dialogue policies trained
with different user simulations (Section 4).
2 Agenda-based user simulation
In agenda-based user simulation, user acts are gen-
erated on the basis of a user goal and an agenda
(Schatzmann et al, 2007a). The simulator pre-
sented here is developed and used for a tourist in-
116
formation application, but is sufficiently generic to
accommodate slot-filling applications in any do-
main.1 The user goal consists of the type of venue,
for example hotel, bar or restaurant, a list
of constraints in the form of slot value pairs, such
as food=Italian or area=east, and a list
of slots the user wants to know the value of, such
as the address (addr), phone number (phone),
or price information (price) of the venue. The
user goals for the simulator are randomly gener-
ated from the domain ontology describing which
combinations of venue types and constraints are
allowed and what are the possible values for each
slot. The agenda is a stack-like structure contain-
ing planned user acts. When the simulator receives
a system act, the status of the user goal is updated
as well as the agenda, typically by pushing new
acts onto it. In a separate step, the response user
act is selected by popping one or more items off
the agenda.
Although the agenda-based user simulator in-
troduced by Schatzmann et al (2007a) was en-
tirely handcrafted, it was realistic enough to suc-
cessfully test a prototype POMDP dialogue man-
ager and train a dialogue policy that outperformed
a handcrafted baseline (Young et al, 2009). A
method to train an agenda-based user simula-
tor from data was proposed by Schatzmann et
al. (2007b). In this approach, operations on
the agenda are controlled by probabilities learned
from data using a variation of the EM algorithm.
However, this approach does not readily scale to
more complex interactions in which users can, for
example, change their goal midway through a dia-
logue.
2.1 Random decision parameters
Each time the user simulator receives a system act,
a complex, two-fold process takes place involving
several decisions, made on the basis of both the
nature of the incoming system act and the infor-
mation state of the user, i.e., the status of the user
goal and agenda. The first phase can be seen as
an information state update and involves actions
like filling requested slots or checking whether the
provided information is consistent with the user
goal constraints. In the second phase, the user de-
cides which response act to generate, based on the
updated agenda. Many of the decisions involved
are deterministic, allowing only one possible op-
tion given the context. Other decisions allow for
some degree of variation in the user behaviour and
are governed by probability distributions over the
1We have to date also implemented systems in appoint-
ment scheduling and bus timetable inquiries.
options allowed in that context. For example, if
the system has offered a venue that matches the
user?s goal, the user can randomly decide to either
change his goal or to accept the venue and ask for
additional information such as the phone number.
The non-deterministic part of the simulator is
formalised in terms of a set of random decision
points (RDPs) embedded in the decision process.
If an RDP is encountered (depending on the con-
text), a random choice between the options de-
fined for that point is made by sampling from a
probability distribution. Most of the RDPs are
controlled by a multinomial distribution, such as
deciding whether or not to change the goal after
a system offer. Some RDPs are controlled by a
geometric distribution, like in the case where the
user is planning to specify one of his constraints
(with an inform act popped from the agenda) and
then repeatedly adds an additional constraint to the
act (by combining it with an additional inform act
popped from the agenda) until it randomly decides
not to add any more constraints (or runs out of
constraints to specify). The parameter for this dis-
tribution thus controls how cautious the user is in
providing information to the system.
Hence, the user simulator can be viewed as
a ?decision network?, consisting of deterministic
and random decision points. This is illustrated in
Figure 1 for the simplified case of a network with
only four RDPs; the actual simulator has 23 RDPs,
with 27 associated parameters in total. Each time
the simulator receives a system act, it follows a
path through the network, which is partly deter-
mined by that system act and the user goal and
agenda, and partly by random decisions made ac-
cording to the probability distributions for each
random decision point i given by its parameters
?i.
3 Training the simulator from data
The parameterisation of the user simulator as de-
scribed in Section 2.1 forms the basis for a method
for training the simulator with real user data. The
parameters describing the probability distributions
for each RDP are estimated in order to generate
user behaviour that fits the user behaviour in the
corpus as closely as possible. In order to do so,
a sample based maximum likelihood approach is
taken, in which the simulator is run repeatedly
against the system acts in the corpus, and the ran-
dom decisions that lead to simulated acts matching
the true act in the corpus are recorded. The param-
eters are then estimated using the counts for each
of the random decision points.
117
incoming
system act
outgoing
user act
user goal + agenda
?2
?1
?3
?4
Figure 1: User simulator viewed as a ?decision network?: square nodes indicate deterministic decision
points; round nodes indicate random decision points, and have associated parameters ?i; the loop on one
of the nodes indicates it has a geometric distribution associated with it.
3.1 Parameter estimation
Before starting the process of matching simulated
acts with true acts and collecting counts for the
RDPs, the parameters are initialised to values cor-
responding to uniform distributions. Then, the
simulator is run against all dialogues in the cor-
pus in such a way that for each turn in a dialogue
(consisting of a system act and a user act), the user
simulator is provided with the system act and is
run repeatedly to generate several simulated user
response acts for that turn. For the first turn of a di-
alogue, the simulator is initialised with the correct
user state (see Section 3.2). For each response, the
simulator may make different random decisions,
generally leading to different user acts. The deci-
sions that lead to a simulated act that matches the
true act are recorded as successful. By generating
a sufficiently large number of simulated acts, all
possible combinations of decisions are explored to
find a matching act. Given the high complexity of
the simulator, this sampling approach is preferred
over directly enumerating all decision combina-
tions to identify the successful ones. If none of
the combinations are successful, then either a) the
processing of the dialogue is ended, or b) the cor-
rect context is set for the next turn and processing
is continued. Whereas the former approach aims at
matching sequences of turns, the latter only aims
at matching each user turn separately. In either
case, after all data is processed, the parameters are
estimated using the resulting counts of successful
decisions for each of the RDPs.
For each RDP i, let DPi represent the decision
taken, and dij the j?th possible decision. Then, for
each decision point i that is controlled by a multi-
nomial distribution, the corresponding parameter
estimates ?ij are obtained as follows from the de-
cision frequencies c(DPi = dij):
?ij =
c(DPi = dij)
?
j c(DPi = dij)
(1)
Random decision points that are controlled
by geometric distributions involve potentially
multiple random decisions between two options
(Bernoulli trials). The parameters for such RDPs
are estimated as follows:
?i =
(
1
n
n
?
k=1
bik
)?1
(2)
where bik is the number of Bernoulli trials re-
quired at the k?th time decision point i was en-
countered. In terms of the decision network, this
estimate is correlated with the average number of
times the loop of the node was taken.
3.2 User goal inference
In order to be able to set the correct user goal
state in any given turn, a set of update rules is
used to infer the user?s goals from a dialogue be-
forehand, on the basis of the entire sequence of
system acts and ?true? user acts (see Section 4.1)
in the corpus. These update rules are based on
the notion of dialogue act preconditions, which
specify conditions of the dialogue context that
must hold for a dialogue agent to perform that
act. For example, a precondition for the act
inform(area=central) is that the speaker
wants a venue in the centre. The user act model
118
of the HIS dialogue manager is designed accord-
ing to this same notion (Keizer et al, 2008). In this
model, the probability of a user act in a certain dia-
logue context (the last system act and a hypothesis
regarding the user goal) is determined by checking
the consistency of its preconditions with that con-
text. This contributes to updating the system?s be-
lief state on the basis of which it determines its re-
sponse action. For the user goal inference model,
the user act is given and therefore its precondi-
tions can be used to directly infer the user goal.
So, for example, in the case of observing the user
act inform(area=central), the constraint
(area=central) is added to the user goal.
In addition to using the inferred user goals, the
agenda is corrected in cases where there is a mis-
match between real and simulated user acts in the
previous turn.
In using this offline goal inference model, our
approach takes a position between (Schatzmann et
al., 2007b), in which the user?s goal is treated as
hidden, and (Georgila et al, 2006), in which the
user?s goal is obtained directly from the corpus an-
notation.
4 Evaluation
The parameter estimation technique for training
the user simulator was evaluated in two differ-
ent ways. The first evaluation involved compar-
ing the statistics of simulated and real user be-
haviour. The second evaluation involved compar-
ing dialogue manager policies trained with differ-
ent simulators.
4.1 Data
The task of the dialogue systems we are develop-
ing is to provide tourist information to users, in-
volving venues such as bars, restaurants and hotels
that the user can search for and ask about. These
venues are described in terms of features such as
price range, area, type of food, phone number,
address, and so on. The kind of dialogues with
these systems are commonly called slot-filling di-
alogues.
Within the range of slot-filling applications the
domain is relatively complex due to its hierarchi-
cal data structure and relatively large number of
slots and their possible values. Scalability is in-
deed one of the primary challenges to be addressed
in statistical approaches to dialogue system devel-
opment, including user simulation.
The dialogue corpus that was used for training
and evaluating the simulator was obtained from
the evaluation of a POMDP spoken dialogue sys-
tem with real users. All user utterances in the
resulting corpus were transcribed and semanti-
cally annotated in terms of dialogue acts. Dia-
logue acts consist of a series of semantic items,
including the type (describing the intention of
the speaker, e.g., inform or request) and a
list of slot value pairs (e.g., food=Chinese or
area=south). An extensive analysis of the an-
notations from three different people revealed a
high level of inter-annotator agreement (ranging
from 0.81 to 0.94, depending on which pair of an-
notations are compared), and a voting scheme for
selecting a single annotation for each turn ensured
the reliability of the ?true? user acts used for train-
ing the simulator.
4.2 Corpus statistics results
A first approach to evaluating user simulations is
to look at the statistics of the user behaviour that
is generated by a simulator and compare it with
that of real users as observed in a dialogue cor-
pus. Several metrics for such evaluations have
been considered in the literature, all of which have
both strong points and weaknesses. For the present
evaluation, a selection of metrics believed to give
a reasonable first indication of the quality of the
user simulations was considered2 .
4.2.1 Metrics
The first corpus-based evaluation metric is the Log
Likelihood (LL) of the data, given the user simu-
lation model. This is what is in fact maximised by
the parameter estimation algorithm. The log like-
lihood can be computed by summing the log prob-
abilities of each user turn du in the corpus data D:
ll(D|{?ij}, {?i}) =
?
u
log P (du|{?ij}, {?i})
(3)
The user turn probability is given by the prob-
ability of the decision paths (directed paths in the
decision network of maximal length, such as the
one indicated in Figure 1 in bold) leading to a sim-
ulated user act in that turn that matches the true
user act. The probability of a decision path is ob-
tained by multiplying the probabilities of the de-
cisions made at each decision point i that was en-
countered, which are given by the parameters ?ij
2Note that not all selected metrics are metrics in the strict
sense of the word; the term should therefore be interpreted as
a more general one.
119
and ?i:
logP (du|{?ij}, {?i}) =
?
i?Im(u)
log
(
?
j
?ij ? ?ij(u)
)
+ (4)
?
i?Ig(u)
log
(
?
k
(1 ? ?i)k?1 ? ?i ? ?ik(u)
)
where Im(u) = {i ? Im|?j ?ij(u) > 0} and
Ig(u) = {i ? Ig|
?
k ?ik(u) > 0} are the subsets
of the multinomial (Im) and geometric (Ig) de-
cision points respectively containing those points
that were encountered in any combination of deci-
sions resulting in the given user act:
?ij(u) =
?
?
?
?
?
1 if decision DPi = dij was
taken in any of the
matching combinations
0 otherwise
(5)
?ik(u) =
?
?
?
?
?
1 if any of the matching
combinations required
k > 0 trials
0 otherwise
(6)
It should be noted that the log likelihood only
represents those turns in the corpus for which the
simulated user can produce a matching simulated
act with some probability. Hence, it is impor-
tant to also take into account the corpus cover-
age when considering the log likelihood in cor-
pus based evaluation. Dividing by the number of
matched turns provides a useful normalisation in
this respect.
The expected Precision (PRE), Recall (RCL),
and F-Score (FS) are obtained by comparing the
simulated user acts with the true user acts in the
same context (Georgila et al, 2006). These scores
are obtained by pairwise comparison of the simu-
lated and true user act for each turn in the corpus
at the level of the semantic items:
PRE = #(matched items)#(items in simulated act) (7)
RCL = #(matched items)#(items in true act) (8)
FS = 2 ? PRE ? RCLPRE + RCL (9)
By sampling a sufficient number of simulated
acts for each turn in the corpus and comparing
them with the corresponding true acts, this results
in an accurate measure on average.
The problem with precision and recall is that
they are known to heavily penalise unseen data.
Any attempt to generalise and therefore increase
the variability of user behaviour results in lower
scores.
Another way of evaluating the user simulator
is to look at the global user act distributions it
generates and compare them to the distributions
found in the real user data. A common metric
for comparing such distributions is the Kullback-
Leibler (KL) distance. In (Cuaya?huitl et al,
2005) this metric was used to evaluate an HMM-
based user simulation approach. The KL dis-
tance is computed by taking the average of the
two KL divergences3 DKL(simulated||true) and
DKL(true||simulated), where:
DKL(p||q) =
?
i
pi ? log2(
pi
qi
) (10)
KL distances are computed for both full user act
distributions (taking into account both the dia-
logue act type and slot value pairs) and user act
type distributions (only regarding the dialogue act
type), denoted by KLF and KLT respectively.
4.2.2 Results
For the experiments, the corpus data was ran-
domly split into a training set, consisting of 4479
user turns in 541 dialogues, used for estimat-
ing the user simulator parameters, and a test set,
consisting of 1457 user turns in 175 dialogues,
used for evaluation only. In the evaluation, the
following parameter settings were compared: 1)
non-informative, uniform parameters (UNIF); 2)
handcrafted parameters (HDC); 3) parameters es-
timated from data (TRA); and 4) deterministic pa-
rameters (DET), in which for each RDP the prob-
ability of the most probable decision according to
the estimated parameters is set to 1, i.e., at all
times, the most likely decision according to the es-
timated parameters is chosen.
For both trained and deterministic parameters,
a distinction is made between the two approaches
to matching user acts during parameter estimation.
Recall that in the turn-based approach, in each
turn, the simulator is run with the corrected con-
text to find a matching simulated act, whereas in
the sequence-based approach, the matching pro-
cess for a dialogue is stopped in case a turn
is encountered which cannot be matched by the
simulator. This results in estimated parameters
TRA-T and deterministic parameters DET-T for
3Before computing the distances, add-one smoothing was
applied in order to avoid zero-probabilities.
120
PAR nLL-T nLL-S PRE RCL FS KLF KLT
UNIF ?3.78 ?3.37 16.95 (?0.75) 9.47 (?0.59) 12.15 3.057 2.318
HDC ?4.07 ?2.22 44.31 (?0.99) 34.74 (?0.95) 38.94 1.784 0.623
TRA-T ?2.97 - 37.60 (?0.97) 28.14 (?0.90) 32.19 1.362 0.336
DET-T ?? - 47.70 (?1.00) 40.90 (?0.98) 44.04 2.335 0.838
TRA-S - ?2.13 43.19 (?0.99) 35.68 (?0.96) 39.07 1.355 0.155
DET-S - ?? 49.39 (?1.00) 43.04 (?0.99) 46.00 2.310 0.825
Table 1: Results of the sample-based user simulator evaluation on the Mar?09 training
corpus (the corpus coverage was 59% for the turn-based and 33% for the sequence-based
matching approach).
PAR nLL-T nLL-S PRE RCL FS KLF KLT
UNIF ?3.61 ?3.28 16.59 (?1.29) 9.32 (?1.01) 11.93 2.951 2.180
HDC ?3.90 ?2.19 45.35 (?1.72) 36.04 (?1.66) 40.16 1.780 0.561
TRA-T ?2.84 - 38.22 (?1.68) 28.74 (?1.57) 32.81 1.405 0.310
DET-T ?? - 49.15 (?1.73) 42.17 (?1.71) 45.39 2.478 0.867
TRA-S - ?2.12 43.90 (?1.72) 36.52 (?1.67) 39.87 1.424 0.153
DET-S - ?? 50.73 (?1.73) 44.41 (?1.72) 47.36 2.407 0.841
Table 2: Results of the sample-based user simulator evaluation on the Mar?09 test corpus
(corpus coverage 59% for the turn-based, and 36% for sequence-based matching).
the turn-based approach and analogously TRA-S
and DET-S for the sequence-based approach. The
corresponding normalised (see Section 4.2.1) log-
likelihoods are indicated by nLL-T and nLL-S.
Tables 1 and 2 give the results on the training
and test data respectively. The results show that in
terms of log-likelihood and KL-distances, the es-
timated parameters outperform the other settings,
regardless of the matching method. In terms of
precision/recall (given in percentages with 95%
confidence intervals), the estimated parameters
are worse than the handcrafted parameters for
turn-based matching, but have similar scores for
sequence-based matching.
The results for the deterministic parameters il-
lustrate that much better precision/recall scores
can be obtained, but at the expense of variability as
well as the KL-distances. It will be easier to train
a dialogue policy on such a deterministic simula-
tor, but that policy is likely to perform significantly
worse on the more varied behaviour generated by
the trained simulator, as we will see in Section 4.3.
Out of the two matching approaches, the
sequence-based approach gives the best results:
TRA-S outperforms TRA-T on all scores, except
for the coverage which is much lower for the
sequence-based approach (33% vs. 59%).
4.3 Policy evaluation results
Although the corpus-based evaluation results give
a useful indication of how realistic the behaviour
generated by a simulator is, what really should be
evaluated is the dialogue management policy that
is trained using that simulator. Therefore, differ-
ent parameter sets for the simulator were used to
train and evaluate different policies for the Hidden
Information State (HIS) dialogue manager (Young
et al, 2009). Four different policies were trained:
one policy using handcrafted simulation param-
eters (POL-HDC); two policies using simulation
parameters estimated (using the sequence-based
matching approach) from two data sets that were
obtained by randomly splitting the data into two
parts of 358 dialogues each (POL-TRA1 and POL-
TRA2); and finally, a policy using a determin-
istic simulator (POL-DET) constructed from the
trained parameters as discussed in Section 4.2.2.
The policies were then each evaluated on the sim-
ulator using the four parameter settings at different
semantic error rates.
The performance of a policy is measured in
terms of a reward that is given for each dialogue,
i.e. a reward of 20 for a successful dialogue, mi-
nus the number of turns. A dialogue is consid-
ered successful if the system has offered a venue
matching the predefined user goal constraints and
has given the correct values of all requested slots
for this venue. During the policy optimisation, in
which a reinforcement learning algorithm tries to
optimise the expected long term reward, this dia-
logue scoring regime was also used.
In Figures 2, 3, and 4, evaluation results are
given resulting from running 3000 dialogues at
each of 11 different semantic error rates. The
curves show average rewards with 95% confidence
intervals. The error rate is controlled by a hand-
121
-2
 0
 2
 4
 6
 8
 10
 12
 0  0.1  0.2  0.3  0.4  0.5
Av
er
ag
e 
re
wa
rd
Error rate
POL-HDC
POL-TRA1
POL-TRA2
POL-DET
Figure 2: Average rewards for each policy when
evaluated on UM-HDC.
-4
-2
 0
 2
 4
 6
 8
 10
 0  0.1  0.2  0.3  0.4  0.5
Av
er
ag
e 
re
wa
rd
Error rate
POL-HDC
POL-TRA1
POL-TRA2
POL-DET
Figure 3: Average rewards for each policy when
evaluated on UM-TRA1.
 2
 4
 6
 8
 10
 12
 14
 16
 0  0.1  0.2  0.3  0.4  0.5
Av
er
ag
e 
re
wa
rd
Error rate
POL-HDC
POL-TRA1
POL-TRA2
POL-DET
Figure 4: Average rewards for each policy when
evaluated on UM-DET.
 0
 1
 2
 3
 4
 5
 6
 7
 0  0.1  0.2  0.3  0.4  0.5
Av
er
ag
e 
re
wa
rd
 lo
ss
Error rate
POL-HDC
POL-TRA2
POL-DET
Figure 5: Average loss in reward for each policy,
across three different simulators.
crafted error model that converts the user act gen-
erated by the simulator into an n-best list of dia-
logue act hypotheses.
The policy that was trained using the hand-
crafted simulator (POL-HDC) outperforms the
other policies when evaluated on that same sim-
ulator (see Figure 2), and both policies trained us-
ing the trained simulators (POL-TRA1 and POL-
TRA2) outperform the other policies when evalu-
ated on either trained simulator (see Figure 3 for
the evaluation on UM-TRA1; the evaluation on
UM-TRA2 is very similar and therefore omitted).
There is little difference in performance between
policies POL-TRA1 and POL-TRA2, which can
be explained by the fact that the two trained
parameter settings are quite similar, in contrast
to the handcrafted parameters. The policy that
was trained on the deterministic parameters (POL-
DET) is competitive with the other policies when
evaluated on UM-DET (see Figure 4), but per-
forms significantly worse on the other parameter
settings which generate the variation in behaviour
that the dialogue manager did not encounter dur-
ing training of POL-DET.
In addition to comparing the policies when eval-
uated on each simulator separately, another com-
parison was made in terms of the average perfor-
mance across all simulators. For each policy and
each simulator, we first computed the difference
between the policy?s performance and the ?maxi-
mum? performance on that simulator as achieved
by the policy that was also trained on that simu-
lator, and then averaged over all simulators. To
avoid biased results, only one of the trained simu-
lators was included. The results in Figure 5 show
that the POL-TRA2 policy is more robust than
POL-DET, and has similar robustness as POL-
HDC. Similar results are obtained when including
UM-TRA1 only.
Given that the results of Section 4.2 show that
the dialogues generated by the trained simulator
more closely match real corpus data, and given
that the above simulation results show that the
POL-TRA policies are at least as robust as the
122
other policies, it seems likely that policies trained
using the trained user simulator will show im-
proved performance when evaluated on real users.
However, this claim can only be properly
demonstrated in a real user evaluation of the di-
alogue system containing different dialogue man-
agement policies. Such a user trial would also be
able to confirm whether the results from evalua-
tions on the trained simulator can more accurately
predict the actual performance expected with real
users.
5 Conclusion
In this paper, we presented an agenda-based user
simulator extended to be trainable on real user
data whilst preserving the necessary rationality
and complexity for effective training and evalu-
ation of dialogue manager policies. The exten-
sion involved the incorporation of random deci-
sion points in the process of receiving and re-
sponding to a system act in each turn. The deci-
sions made at these points are controlled by prob-
ability distributions defined by a set of parameters.
A sample-based maximum likelihood approach
to estimating these parameters from real user data
in a corpus of human-machine dialogues was dis-
cussed, and two kinds of evaluations were pre-
sented. When comparing the statistics of real ver-
sus simulated user behaviour in terms of a selec-
tion of different metrics, overall, the estimated pa-
rameters were shown to give better results than
the handcrafted baselines. When evaluating dia-
logue management policies trained on the simula-
tor with different parameter settings, it was shown
that: 1) policies trained on a particular parame-
ter setting outperform other policies when evalu-
ated on the same parameters, and in particular, 2)
a policy trained on the trained simulator outper-
forms other policies on a trained simulator. With
the general goal of obtaining a dialogue manager
that performs better in practice, these results are
encouraging, but need to be confirmed by an eval-
uation of the policies on real users.
Additionally, there is still room for improving
the quality of the simulator itself. For example,
the variation in user behaviour can be improved by
adding more random decision points, in order to
achieve better corpus coverage. In addition, since
there is no clear consensus on what is the best met-
ric for evaluating user simulations, additional met-
rics will be explored in order to get a more bal-
anced indication of the quality of the user simu-
lator and how the various metrics are affected by
modifications to the simulator. Perplexity (related
to the log likelihood, see (Georgila et al, 2005)),
accuracy (related to precision/recall, see (Zuker-
man and Albrecht, 2001; Georgila et al, 2006)),
and Crame?r-von Mises divergence (comparing di-
alogue score distributions, see (Williams, 2008))
are some of the metrics worth considering.
References
H. Cuaya?huitl, S. Renals, O. Lemon, and H. Shi-
modaira. 2005. Human-computer dialogue sim-
ulation using hidden markov models. In Proc.
ASRU?05, pages 290?295.
K. Georgila, J. Henderson, and O. Lemon. 2005.
Learning user simulations for information state up-
date dialogue systems. In Proc. Interspeech ?05.
K. Georgila, J. Henderson, and O. Lemon. 2006. User
simulation for spoken dialogue systems: Learning
and evaluation. In Proc. Interspeech/ICSLP.
S. Keizer, M. Gas?ic?, F. Mairesse, B. Thomson, K. Yu,
and S. Young. 2008. Modelling user behaviour in
the HIS-POMDP dialogue manager. In Proc. SLT,
Goa, India.
E. Levin, R. Pieraccini, and W. Eckert. 2000. A
stochastic model of human-machine interaction for
learning dialogue strategies. IEEE Transactions on
Speech and Audio Processing, 8(1).
J. Schatzmann, B. Thomson, K. Weilhammer, H. Ye,
and S. Young. 2007a. Agenda-based user simula-
tion for bootstrapping a POMDP dialogue system.
In Proceedings HLT/NAACL, Rochester, NY.
J. Schatzmann, B. Thomson, and S. Young. 2007b.
Statistical user simulation with a hidden agenda. In
Proc. SIGDIAL?07, pages 273?282, Antwerp, Bel-
gium.
K. Scheffler and S. Young. 2001. Corpus-based dia-
logue simulation for automatic strategy learning and
evaluation. In Proceedings NAACL Workshop on
Adaptation in Dialogue.
S. Singh, M. Kearns, D. Litman, and M. Walker. 2000.
Reinforcement learning for spoken dialogue sys-
tems. In S. Solla, T. Leen, and K. Mu?ller, editors,
Advances in Neural Information Processing Systems
(NIPS). MIT Press.
J. Williams. 2008. Evaluating user simulations with
the Crame?r-von Mises divergence. Speech Commu-
nication, 50:829?846.
S. Young, M. Gas?ic?, S. Keizer, F. Mairesse, B. Thom-
son, and K. Yu. 2009. The Hidden Information
State model: a practical framework for POMDP
based spoken dialogue management. Computer
Speech and Language, 24(2):150?174.
I. Zukerman and D. Albrecht. 2001. Predictive statis-
tical models for user modeling. User Modeling and
User-Adapted Interaction, 11:5?18.
123
Proceedings of SIGDIAL 2010: the 11th Annual Meeting of the Special Interest Group on Discourse and Dialogue, pages 201?204,
The University of Tokyo, September 24-25, 2010. c?2010 Association for Computational Linguistics
Gaussian Processes for Fast Policy Optimisation of POMDP-based
Dialogue Managers
M. Gas?ic?, F. Jurc???c?ek, S. Keizer, F. Mairesse, B. Thomson, K. Yu and S. Young
Cambridge University Engineering Department
Trumpington Street, Cambridge CB2 1PZ, UK
{mg436, fj228, sk561, farm2, brmt2, ky219, sjy}@eng.cam.ac.uk
Abstract
Modelling dialogue as a Partially Observ-
able Markov Decision Process (POMDP)
enables a dialogue policy robust to speech
understanding errors to be learnt. How-
ever, a major challenge in POMDP pol-
icy learning is to maintain tractability, so
the use of approximation is inevitable.
We propose applying Gaussian Processes
in Reinforcement learning of optimal
POMDP dialogue policies, in order (1) to
make the learning process faster and (2) to
obtain an estimate of the uncertainty of the
approximation. We first demonstrate the
idea on a simple voice mail dialogue task
and then apply this method to a real-world
tourist information dialogue task.
1 Introduction
One of the main challenges in dialogue manage-
ment is effective handling of speech understand-
ing errors. Instead of hand-crafting the error han-
dler for each dialogue step, statistical approaches
allow the optimal dialogue manager behaviour
to be learnt automatically. Reinforcement learn-
ing (RL), in particular, enables the notion of plan-
ning to be embedded in the dialogue management
criteria. The objective of the dialogue manager is
for each dialogue state to choose such an action
that leads to the highest expected long-term re-
ward, which is defined in this framework by the Q-
function. This is in contrast to Supervised learn-
ing, which estimates a dialogue strategy in such a
way as to make it resemble the behaviour from a
given corpus, but without directly optimising over-
all dialogue success.
Modelling dialogue as a Partially Observable
Markov Decision Process (POMDP) allows action
selection to be based on the differing levels of un-
certainty in each dialogue state as well as the over-
all reward. This approach requires that a distribu-
tion of states (belief state) is maintained at each
turn. This explicit representation of uncertainty in
the POMDP gives it the potential to produce more
robust dialogue policies (Young et al, 2010).
The main challenge in the POMDP approach is
the tractability of the learning process. A dis-
crete state space POMDP can be perceived as a
continuous space MDP where the state space con-
sists of the belief states of the original POMDP.
A grid-based approach to policy optimisation as-
sumes discretisation of this space, allowing for
discrete space MDP algorithms to be used for
learning (Brafman, 1997) and thus approximating
the optimal Q-function. Such an approach takes
the order of 100, 000 dialogues to train a real-
world dialogue manager. Therefore, the training
normally takes place in interaction with a simu-
lated user, rather than real users. This raises ques-
tions regarding the quality of the approximation
as well as the potential discrepancy between sim-
ulated and real user behaviour.
Gaussian Processes have been successfully used
in Reinforcement learning for continuous space
MDPs, for both model-free approaches (Engel et
al., 2005) and model-based approaches (Deisen-
roth et al, 2009). We propose using GP Rein-
forcement learning in a POMDP dialogue man-
ager to, firstly, speed up the learning process and,
secondly, obtain the uncertainty of the approxima-
tion. We opt for the model-free approach since it
has the potential to allow the policy obtained in
interaction with the simulated user to be further
refined in interaction with real users.
In the next section, the core idea of the method is
explained on a toy dialogue problem where differ-
ent aspects of GP learning are examined. Follow-
ing that, in Section 3, it is demonstrated how this
methodology can be effectively applied to a real
world dialogue. We conclude with Section 4.
2 Gaussian Process RL on a Toy Problem
2.1 Gaussian Process RL
A Gaussian Process is a generative model of
Bayesian inference that can be used for function
regression (Rasmussen and Williams, 2005). A
Gaussian Process is fully defined by a mean and a
kernel function. The kernel function defines prior
function correlations, which is crucial for obtain-
ing good posterior estimates with just a few ob-
servations. GP-Sarsa is an on-line reinforcement
learning algorithm for both continuous and dis-
crete MDPs that incorporates GP regression (En-
201
gel et al, 2005). Given the observation of rewards,
it estimates the Q-function utilising its correlations
in different parts of the state and the action space
defined by the kernel function. It also gives a vari-
ance of the estimate, thus modelling the uncer-
tainty of the approximation.
2.2 Voice Mail Dialogue Task
In order to demonstrate how this methodology
can be applied to a dialogue system, we first ex-
plain the idea on the voice mail dialogue prob-
lem (Williams, 2006).
The state space of this task consists of three states:
the user asked for the message either to be saved
or deleted, or the dialogue ended. The system
can take three actions: ask the user what to do,
save or delete the message. The observation of
what the user wants is corrupted with noise, there-
fore we model this as a three-state POMDP. This
POMDP can be viewed as a continuous MDP,
where the MDP state is the POMDP belief state,
a 3-dimensional vector of probabilities. For both
learning and evaluation, a simulated user is used
which makes an error with probability 0.3 and ter-
minates the dialogue after at most 10 turns. In the
final state, it gives a positive reward of 10 or a
penalty of ?100 depending on whether the system
performed a correct action or not. Each interme-
diate state receives the penalty of ?1. In order to
keep the problem simple, a model defining tran-
sition and observation probabilities is assumed so
that the belief can be easily updated, but the policy
optimisation is performed in an on-line fashion.
2.3 Kernel Choice for GP-Sarsa
The choice of kernel function is very important
since it defines the prior knowledge about the Q-
function correlations. They have to be defined on
both states and actions. In the voice mail dialogue
problem the action space is discrete, so we opt for
a simple ? kernel over actions:
k(a, a?) = 1 ? ?a(a?), (1)
where ?a is the Kronecker delta function. The
state space is a 3-dimensional continuous space
and the kernel functions over the state space that
we explore are given in Table 1. Each kernel func-
kernel function expression
polynomial k(x,x?) = ?x,x??
parametrised poly. k(x,x?) =
PD
i=1
xix
?
i
r2i
Gaussian k(x,x?) = p2 exp ? ?x ? x
??2
2?2k
scaled norm k(x,x?) = 1 ? ?x ? x
??2
?x?2?x??2
Table 1: Kernel functions
tion defines a different correlation. The polyno-
mial kernel views elements of the state vector as
features, the dot-product of which defines the cor-
relation. They can be given different relevance ri
in the parametrised version. The Gaussian ker-
nel accounts for smoothness, i.e., if two states are
close to each other the Q-function in these states
is correlated. The scaled norm kernel defines posi-
tive correlations in the points that are close to each
other and a negative correlation otherwise. This
is particularly useful for the voice mail problem,
where, if two belief states are very different, tak-
ing the same action in these states generates a neg-
atively correlated reward.
2.4 Optimisation of Kernel Parameters
Some kernel functions are in a parametrised
form, such as Gaussian or parametrised polyno-
mial kernel. These parameters, also called the
hyper-parameters, are estimated by maximising
the marginal likelihood1 on a given corpus (Ras-
mussen and Williams, 2005). We adapted the
available code (Rasmussen and Williams, 2005)
for the Reinforcement learning framework to ob-
tain the optimal hyper-parameters using a dialogue
corpus labelled with states, actions and rewards.
2.5 Grid-based RL Algorithms
To assess the performance of GP-Sarsa, it was
compared with a standard grid-based algorithm
used in (Young et al, 2010). The grid-based ap-
proach discretises the continuous space into re-
gions with their representative points. This then
allows discrete MDP algorithms to be used for pol-
icy optimisation, in this case the Monte Carlo Con-
trol (MCC) algorithm (Sutton and Barto, 1998).
2.6 Optimal POMDP Policy
The optimal POMDP policy was obtained us-
ing the POMDP solver toolkit (Cassandra, 2005),
which implements the Point Based Value Itera-
tion algorithm to solve the POMDP off-line using
the underlying transition and observation proba-
bilities. We used 300 sample dialogues between
the dialogue manager governed by this policy and
the simulated user as data for optimisation of the
kernel hyper-parameters (see Section 2.4).
2.7 Training set-up and Evaluation
The dialogue manager was trained in interaction
with the simulated user and the performance was
compared between the grid-based MCC algorithm
and GP-Sarsa across different kernel functions
from Table 1.
The intention was, not only to test which algo-
rithm yields the best policy performance, but also
to examine the speed of convergence to the opti-
mal policy. All the algorithms use an ?-greedy
approach where the exploration rate ? was fixed
at 0.1. The learning process greatly depends on
1Also called evidence maximisation in the literature.
202
the actions that are taken during exploration. If
early on during the training, the systems discovers
a path that generates high rewards due to a lucky
choice of actions, then the convergence is faster.
To alleviate this, we adopted the following proce-
dure. For every training set-up, exactly the same
training iterations were performed using 1000 dif-
ferent random generator seedings. After every 20
dialogues the resulting 1000 partially optimised
policies were evaluated. Each of them was tested
on 1000 dialogues. The average reward of these
1000 dialogues provides just one point in Fig. 1.
20 60 100 140 180 220 260 300 340 380 420 460 500 540 580 620
?50
?45
?40
?35
?30
?25
?20
?15
?10
?5
0
Training dialogues
Av
er
ag
e 
re
wa
rd
polynomial kernel ? 
? Gaussian kernel with learned hyper?parameters
? scaled norm kernel
polynomial kernel with learned hyper?parameters
?
 
 
Optimal POMDP Policy
GP?Sarsa
Grid?based Monte Carlo Control
Figure 1: Evaluation results on Voice Mail task
The grid-based MCC algorithm used a Euclid-
ian distance to generate the grid by adding every
point that was further than 0.01 from other points
as a representative of a new region. As can be
seen from Fig 1, the grid-Based MCC algorithm
has a relatively slow convergence rate. GP-Sarsa
with the polynomial kernel exhibited a learning
rate similar to MCC in the first 300 training di-
alogues, continuing with a more upward learning
trend. The parametrised polynomial kernel per-
forms slightly better. The Gaussian kernel, how-
ever, achieves a much faster learning rate. The
scaled norm kernel achieved close to optimal per-
formance in 400 dialogues, with a much higher
convergence rate then the other methods.
3 Gaussian Process RL on a Real-world
Task
3.1 HIS Dialogue Manager on CamInfo
Domain
We investigate the use of GP-Sarsa in a real-
world task by extending the Hidden Information
State (HIS) dialogue manager (Young et al, 2010).
The application domain is tourist information for
Cambridge, whereby the user can ask for informa-
tion about a restaurant, hotel, museum or another
tourist attraction in the local area. The database
consists of more than 400 entities each of which
has up to 10 attributes that the user can query.
The HIS dialogue manager is a POMDP-based di-
alogue manager that can tractably maintain belief
states for large domains. The key feature of this
approach is the grouping of possible user goals
into partitions, using relationships between differ-
ent attributes from possible user goals. Partitions
are combined with possible user dialogue actions
from the N-best user input as well as with the di-
alogue history. This combination forms the state
space ? the set of hypotheses, the probability dis-
tribution over which is maintained during the di-
alogue. Since the number of states for any real-
world problem is too large, for tractable policy
learning, both the state and the action space are
mapped into smaller scale summary spaces. Once
an adequate summary action is found in the sum-
mary space, it is mapped back to form an action in
the original master space.
3.2 Kernel Choice for GP-Sarsa
The summary state in the HIS system is a four-
dimensional space consisting of two elements that
are continuous (the probability of the top two hy-
potheses) and two discrete elements (one relating
the portion of the database entries that matches the
top partition and the other relating to the last user
action type). The summary action space is discrete
and consists of eleven elements.
In order to apply the GP-Sarsa algorithm, a kernel
function needs to be specified for both the sum-
mary state space and the summary action space.
The nature of this space is quite different from the
one described in the toy problem. Therefore, ap-
plying a kernel that has negative correlations, such
as the scaled norm kernel (Table 1) might give un-
expected results. More specifically, for a given
summary action, the mapping procedure finds the
most appropriate action to perform if such an ac-
tion exists. This can lead to a lower reward if
the summary action is not adequate but would
rarely lead to negatively correlated rewards. Also,
parametrised kernels could not be used for this
task, since there was no corpus available for hyper-
parameter optimisation. The polynomial kernel
(Table 1) assumes that the elements of the space
are features. Due to the way the probability is
maintained over this very large state space, the
continuous variables potentially encode more in-
formation than in the simple toy problem. There-
fore, we used the polynomial kernel for the con-
tinuous elements. For discrete elements, we utilise
the ?-kernel (Eq. 2.3).
3.3 Active Learning GP-Sarsa
The GP RL framework enables modelling the un-
certainty of the approximation. The uncertainty
estimate can be used to decide which actions
to take during the exploration (Deisenroth et al,
203
2009). In detail, instead of a random action, the
action in which the Q-function for the current state
has the highest variance is taken.
3.4 Training Set-up and Evaluation
Policy optimisation is performed by interacting
with a simulated user on the dialogue act level.
The simulated user gives a reward at the final state
of the dialogue, and that is 20 if the dialogue was
successful, 0 otherwise, less the number of turns
taken to fulfil the user goal. The simulated user
takes a maximum of 100 turns in each dialogue,
terminating it when all the necessary information
has been obtained or if it looses patience.
A grid-based MCC algorithm provides the base-
line method. The distance metric used ensures
that the number of regions in the grid is small
enough for the learning to be tractable (Young et
al., 2010).
In order to measure how fast each algorithm
learns, a similar training set-up to the one pre-
sented in Section 2.7 was adopted and the aver-
aged results are plotted on the graph, Fig. 2.
200 400 600 800 1000 1200 1400 1600 1800 2000 2200 2400 2600 2800 3000
2
3
4
5
6
7
8
9
Training dialogues
Av
er
ag
e 
re
wa
rd
?  Grid?based Monte Carlo Control
?  GP?Sarsa with polynomial kernel
?  Active learning GP?Sarsa with polynomial kernel
Figure 2: Evaluation results on CamInfo task
The results show that in the very early stage of
learning, i.e., during the first 400 dialogues, the
GP-based method learns faster. Also, the learning
process can be accelerated by adopting the active
learning framework where the actions are selected
based on the estimated uncertainty.
After performing many iterations in an incremen-
tal noise learning set-up (Young et al, 2010) both
the GP-Sarsa and the grid-based MCC algorithms
converge to the same performance.
4 Conclusions
This paper has described how Gaussian Processes
in Reinforcement learning can be successfully ap-
plied to dialogue management. We implemented
a GP-Sarsa algorithm on a toy dialogue prob-
lem, showing that with an appropriate kernel func-
tion faster convergence can be achieved. We also
demonstrated how kernel parameters can be learnt
from a dialogue corpus, thus creating a bridge
between Supervised and Reinforcement learning
methods in dialogue management. We applied
GP-Sarsa to a real-world dialogue task showing
that, on average, this method can learn faster than
a grid-based algorithm. We also showed that the
variance that GP is estimating can be used in an
Active learning setting to further accelerate policy
optimisation.
Further research is needed in the area of kernel
function selection. The results here suggest that
the GP framework can facilitate faster learning,
which potentially allows the use of larger sum-
mary spaces. In addition, being able to learn ef-
ficiently from a small number of dialogues offers
the potential for learning from direct interaction
with real users.
Acknowledgements
The authors would like to thank Carl Rasmussen
for valuable discussions. This research was partly
funded by the UK EPSRC under grant agreement
EP/F013930/1 and by the EU FP7 Programme un-
der grant agreement 216594 (CLASSiC project).
References
RI Brafman. 1997. A Heuristic Variable Grid Solution
Method for POMDPs. In AAAI, Cambridge, MA.
AR Cassandra. 2005. POMDP solver.
http://www.cassandra.org/pomdp/
code/index.shtml.
MP Deisenroth, CE Rasmussen, and J Peters. 2009.
Gaussian Process Dynamic Programming. Neuro-
comput., 72(7-9):1508?1524.
Y Engel, S Mannor, and R Meir. 2005. Reinforcement
learning with Gaussian processes. In ICML ?05:
Proceedings of the 22nd international conference on
Machine learning, pages 201?208, New York, NY.
CE Rasmussen and CKI Williams. 2005. Gaussian
Processes for Machine Learning. MIT Press, Cam-
bridge, MA.
RS Sutton and AG Barto. 1998. Reinforcement Learn-
ing: An Introduction. Adaptive Computation and
Machine Learning. MIT Press, Cambridge, MA.
JD Williams. 2006. Partially Observable Markov De-
cision Processes for Spoken Dialogue Management.
Ph.D. thesis, University of Cambridge.
SJ Young, M Gas?ic?, S Keizer, F Mairesse, J Schatz-
mann, B Thomson, and K Yu. 2010. The Hid-
den Information State Model: a practical frame-
work for POMDP-based spoken dialogue manage-
ment. Computer Speech and Language, 24(2):150?
174.
204
