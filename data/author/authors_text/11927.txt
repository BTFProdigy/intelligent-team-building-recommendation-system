Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 257?266,
Singapore, 6-7 August 2009. c?2009 ACL and AFNLP
Clustering to Find Exemplar Terms for Keyphrase Extraction
Zhiyuan Liu, Peng Li, Yabin Zheng, Maosong Sun
Department of Computer Science and Technology
State Key Lab on Intelligent Technology and Systems
National Lab for Information Science and Technology
Tsinghua University, Beijing 100084, China
{lzy.thu, pengli09, yabin.zheng}@gmail.com, sms@tsinghua.edu.cn
Abstract
Keyphrases are widely used as a brief
summary of documents. Since man-
ual assignment is time-consuming, vari-
ous unsupervised ranking methods based
on importance scores are proposed for
keyphrase extraction. In practice, the
keyphrases of a document should not only
be statistically important in the docu-
ment, but also have a good coverage of
the document. Based on this observa-
tion, we propose an unsupervised method
for keyphrase extraction. Firstly, the
method finds exemplar terms by leverag-
ing clustering techniques, which guaran-
tees the document to be semantically cov-
ered by these exemplar terms. Then the
keyphrases are extracted from the doc-
ument using the exemplar terms. Our
method outperforms sate-of-the-art graph-
based ranking methods (TextRank) by
9.5% in F1-measure.
1 Introduction
With the development of Internet, information on
the web is emerging exponentially. How to effec-
tively seek and manage information becomes an
important research issue. Keyphrases, as a brief
summary of a document, provide a solution to help
organize, manage and retrieve documents, and are
widely used in digital libraries and information re-
trieval.
Keyphrases in articles of journals and books
are usually assigned by authors. However,
most articles on the web usually do not have
human-assigned keyphrases. Therefore, automatic
keyphrase extraction is an important research task.
Existing methods can be divided into supervised
and unsupervised approaches.
The supervised approach (Turney, 1999) re-
gards keyphrase extraction as a classification task.
In this approach, a model is trained to determine
whether a candidate term of the document is a
keyphrase, based on statistical and linguistic fea-
tures. For the supervised keyphrase extraction
approach, a document set with human-assigned
keyphrases is required as training set. However,
human labelling is time-consuming. Therefore, in
this study we focus on unsupervised approach.
As an example of an unsupervised keyphrase
extraction approach, the graph-based ranking (Mi-
halcea and Tarau, 2004) regards keyphrase extrac-
tion as a ranking task, where a document is repre-
sented by a term graph based on term relatedness,
and then a graph-based ranking algorithm is used
to assign importance scores to each term. Existing
methods usually use term cooccurrences within a
specified window size in the given document as an
approximation of term relatedness (Mihalcea and
Tarau, 2004).
As we know, none of these existing works
gives an explicit definition on what are appropri-
ate keyphrases for a document. In fact, the existing
methods only judge the importance of each term,
and extract the most important ones as keyphrases.
From the observation of human-assigned
keyphrases, we conclude that good keyphrases
of a document should satisfy the following
properties:
1. Understandable. The keyphrases are un-
derstandable to people. This indicates the
extracted keyphrases should be grammatical.
For example, ?machine learning? is a gram-
matical phrase, but ?machine learned? is not.
2. Relevant. The keyphrases are semantically
relevant with the document theme. For ex-
ample, for a document about ?machine learn-
ing?, we want the keyphrases all about this
theme.
3. Good coverage. The keyphrases should
257
cover the whole document well. Sup-
pose we have a document describing ?Bei-
jing? from various aspects of ?location?,
?atmosphere? and ?culture?, the extracted
keyphrases should cover all the three aspects,
instead of just a partial subset of them.
The classification-based approach determines
whether a term is a keyphrase in isolation, which
could not guarantee Property 3. Neither does the
graph-based approach guarantee the top-ranked
keyphrases could cover the whole document. This
may cause the resulting keyphrases to be inappro-
priate or badly-grouped.
To extract the appropriate keyphrases for a doc-
ument, we suggest an unsupervised clustering-
based method. Firstly the terms in a document are
grouped into clusters based on semantic related-
ness. Each cluster is represented by an exemplar
term, which is also the centroid of each cluster.
Then the keyphrases are extracted from the docu-
ment using these exemplar terms.
In this method, we group terms based on se-
mantic relatedness, which guarantees a good cov-
erage of the document and meets Property 2 and
3. Moreover, we only extract the keyphrases in ac-
cordance with noun group (chunk) patterns, which
guarantees the keyphrases satisfy Property 1.
Experiments show that the clustering-based
method outperforms the state-of-the-art graph-
based approach on precision, recall and F1-
measure. Moreover, this method is unsupervised
and language-independent, which is applicable in
the web era with enormous information.
The rest of the paper is organized as follows.
In Section 2, we introduce and discuss the re-
lated work in this area. In Section 3, we give an
overview of our method for keyphrase extraction.
From Section 4 to Section 7, the algorithm is de-
scribed in detail. Empirical experiment results are
demonstrated in Section 8, followed by our con-
clusions and plans for future work in Section 9.
2 Related Work
A straightforward method for keyphrase extrac-
tion is to select keyphrases according to frequency
criteria. However, the poor performance of this
method drives people to explore other methods. A
pioneering achievement is carried out in (Turney,
1999), as mentioned in Section 1, a supervised ma-
chine learning method was suggested in this paper
which regards keyphrase extraction as a classifi-
cation task. In this work, parameterized heuristic
rules are combined with a genetic algorithm into a
system for keyphrase extraction. A different learn-
ing algorithm, Naive Bayes method, is applied in
(Frank et al, 1999) with improved results on the
same data used in (Turney, 1999). Hulth (Hulth,
2003; Hulth, 2004) adds more linguistic knowl-
edge, such as syntactic features, to enrich term
representation, which significantly improves the
performance. Generally, the supervised methods
need manually annotated training set, which may
sometimes not be practical, especially in the web
scenario.
Starting with TextRank (Mihalcea and Tarau,
2004), graph-based ranking methods are becom-
ing the most widely used unsupervised approach
for keyphrase extraction. The work in (Litvak
and Last, 2008) applies HITS algorithm on the
word graph of a document under the assumption
that the top-ranked nodes should be the document
keywords. Experiments show that classification-
based supervised method provides the highest key-
word identification accuracy, while the HITS al-
gorithm gets the highest F-measure. Work in
(Huang et al, 2006) also considers each document
as a term graph where the structural dynamics of
these graphs can be used to identify keyphrases.
Wan and Xiao (Wan and Xiao, 2008b) use a
small number of nearest neighbor documents to
provide more knowledge to improve graph-based
keyphrase extraction algorithm for single docu-
ment. Motivated by similar idea, Wan and Xiao
(Wan and Xiao, 2008a) propose to adopt cluster-
ing methods to find a small number of similar doc-
uments to provide more knowledge for building
word graphs for keyword extraction. Moreover,
after our submission of this paper, we find that
a method using community detection on seman-
tic term graphs is proposed for keyphrase extrac-
tion from multi-theme documents (Grineva et al,
2009). In addition, some practical systems, such
as KP-Miner (Elbeltagy and Rafea, 2009), also
do not need to be trained on a particular human-
annotated document set.
In recent years, a number of systems are de-
veloped for extracting keyphrases from web docu-
ments (Kelleher and Luz, 2005; Chen et al, 2005),
email (Dredze et al, 2008) and some other spe-
cific sources, which indicates the importance of
keyphrase extraction in the web era. However,
258
none of these previous works has overall consid-
eration on the essential properties of appropriate
keyphrases mentioned in Section 1.
We should also note that, although the preci-
sion and recall of most current keyphrase extrac-
tors are still much lower compared to other NLP-
tasks, it does not indicate the performance is poor
because even different annotators may assign dif-
ferent keyphrases to the same document. As de-
scribed in (Wan and Xiao, 2008b), when two anno-
tators were asked to label keyphrases on 308 doc-
uments, the Kappa statistic for measuring inter-
agreement among them was only 0.70.
3 Algorithm Overview
The method proposed in this paper is mainly in-
spired by the nature of appropriate keyphrases
mentioned in Section 1, namely understandable,
semantically relevant with the document and high
coverage of the whole document.
Let?s analyze the document describing ?Bei-
jing? from the aspects of ?location?, ?atmosphere?
and ?culture?. Under the bag-of-words assump-
tion, each term in the document, except for func-
tion words, is used to describe an aspect of the
theme. Based on these aspects, terms are grouped
into different clusters. The terms in the same clus-
ter are more relevant with each other than with
the ones in other clusters. Taking the terms ?tem-
perature?, ?cold? and ?winter? for example, they
may serve the aspect ?atmosphere? instead of ?lo-
cation? or some other aspects when talking about
?Beijing?.
Based on above description, it is thus reason-
able to propose a clustering-based method for
keyphrase extraction. The overview of the method
is:
1. Candidate term selection. We first filter out
the stop words and select candidate terms for
keyphrase extraction.
2. Calculating term relatedness. We use some
measures to calculate the semantic related-
ness of candidate terms.
3. Term clustering. Based on term relatedness,
we group candidate terms into clusters and
find the exemplar terms of each cluster.
4. From exemplar terms to keyphrases. Fi-
nally, we use these exemplar terms to extract
keyphrases from the document.
In the next four sections we describe the algo-
rithm in detail.
4 Candidate Term Selection
Not all words in a document are possible to be se-
lected as keyphrases. In order to filter out the noisy
words in advance, we select candidate terms using
some heuristic rules. This step proceeds as fol-
lows. Firstly the text is tokenized for English or
segmented into words for Chinese and other lan-
guages without word-separators. Then we remove
the stop words and consider the remaining single
terms as candidates for calculating semantic relat-
edness and clustering.
In methods like (Turney, 1999; Elbeltagy and
Rafea, 2009), candidate keyphrases were first
found using n-gram. Instead, in this method, we
just find the single-word terms as the candidate
terms at the beginning. After identifying the ex-
emplar terms within the candidate terms, we ex-
tract multi-word keyphrases using the exemplars.
5 Calculating Term Relatedness
After selecting candidate terms, it is important to
measure term relatedness for clustering. In this pa-
per, we propose two approaches to calculate term
relatedness: one is based on term cooccurrence
within the document, and the other by leveraging
human knowledge bases.
5.1 Cooccurrence-based Term Relatedness
An intuitive method for measuring term relat-
edness is based on term cooccurrence relations
within the given document. The cooccurrence
relation expresses the cohesion relationships be-
tween terms.
In this paper, cooccurrence-based relatedness is
simply set to the count of cooccurrences within a
window of maximum w words in the whole doc-
ument. In the following experiments, the window
size w is set from 2 to 10 words.
Each document can be regarded as a word se-
quence for computing cooccurrence-based relat-
edness. There are two types of word sequence
for counting term cooccurrences. One is the origi-
nal word sequence without filtering out any words,
and the other is after filtering out the stop words
or the words with specified part-of-speech (POS)
tags. In this paper we select the first type because
each word in the sequence takes important role for
measuring term cooccurrences, no matter whether
259
it is a stop word or something else. If we filter
out some words, the term relatedness will not be
as precise as before.
In experiments, we will investigate how the
window size influences the performance of
keyphrase extraction.
5.2 Wikipedia-based Term Relatedness
Many methods have been proposed for measuring
the relatedness between terms using external re-
sources. One principled method is leveraging hu-
man knowledge bases. Inspired by (Gabrilovich
and Markovitch, 2007), we adopt Wikipedia, the
largest encyclopedia collected and organized by
human on the web, as the knowledge base to mea-
sure term relatedness.
The basic idea of computing term related-
ness by leveragting Wikipedia is to consider each
Wikipedia article as a concept. Then the se-
mantic meaning of a term could be represented
as a weighted vector of Wikipedia concepts, of
which the values are the term?s TFIDF within cor-
responding Wikipedia articles. We could com-
pute the term relatedness by comparing the con-
cept vectors of the terms. Empirical evaluations
confirm that the idea is effective and practical
for computing term relatedness (Gabrilovich and
Markovitch, 2007).
In this paper, we select cosine similarity, Eu-
clidean distance, Point-wise Mutual Information
and Normalized Google Similarity Distance (Cili-
brasi and Vitanyi, 2007) for measuring term relat-
edness based on the vector of Wikipedia concepts.
Denote the Wikipedia-concept vector of the
term t
i
as C
i
= {c
i1
, c
i2
, ..., c
iN
}, where N in-
dicates the number of Wikipedia articles, and c
ik
is the TFIDF value of w
i
in the kth Wikipedia ar-
ticle. The cosine similarity is defined as
cos(i, j) =
C
i
? C
j
?C
i
??C
j
?
(1)
The definition of Euclidean distance is
euc(i, j) =
?
?
?
?
N
?
k=1
(c
ik
? c
jk
)
2 (2)
Point-wise Mutual Information (PMI) is a com-
mon approach to quantify relatedness. Here we
take three ways to measure term relatedness using
PMI. One is based on Wikipedia page count,
pmi
p
(i, j) = log
2
N ? p(i, j)
p(i) ? p(j)
(3)
where p(i, j) is the number of Wikipedia articles
containing both t
i
and t
j
, while p(i) is the number
of articles which contain t
i
. The second is based
on the term count in Wikipedia articles,
pmi
t
(i, j) = log
2
T ? t(i, j)
t(i) ? t(j)
(4)
where T is the number of terms in Wikipedia,
t(i, j) is the number of t
i
and t
j
occurred adja-
cently in Wikipedia, and t(i) is the number of t
i
in
Wikipedia. The third one is a combination of the
above two PMI ways,
pmi
c
(i, j) = log
2
N ? pt(i, j)
p(i) ? p(j)
(5)
where pt(i, j) indicates the number of Wikipedia
articles containing t
i
and t
j
as adjacency. It is ob-
vious that pmi
c
(i, j) ? pmi
p
(i, j), and pmi
c
(i, j)
is more strict and accurate for measuring related-
ness.
Normalized Google Similarity Distance (NGD)
is a new measure for measuring similarity between
terms proposed by (Cilibrasi and Vitanyi, 2007)
based on information distance and Kolmogorov
complexity. It could be applied to compute term
similarity from the World Wide Web or any large
enough corpus using the page counts of terms.
NGD used in this paper is based on Wikipedia ar-
ticle count, defined as
ngd(i, j) =
max(log p(i), log p(j)) ? logp(i, j)
logN ? min(logp(i), logp(j))
(6)
where N is the number of Wikipedia articles used
as normalized factor.
Once we get the term relatedness, we could then
group the terms using clustering techniques and
find exemplar terms for each cluster.
6 Term Clustering
Clustering is an important unsupervised learning
problem, which is the assignment of objects into
groups so that objects from the same cluster are
more similar to each other than objects from dif-
ferent clusters (Han and Kamber, 2005). In this
paper, we use three widely used clustering algo-
rithms, hierarchical clustering, spectral clustering
and Affinity Propagation, to cluster the candidate
terms of a given document based on the semantic
relatedness between them.
260
6.1 Hierarchical Clustering
Hierarchical clustering groups data over a variety
of scales by creating a cluster tree. The tree is a
multilevel hierarchy, where clusters at one level
are joined as clusters at the next level. The hier-
archical clustering follows this procedure:
1. Find the distance or similarity between every
pair of data points in the dataset;
2. Group the data points into a binary and hier-
archical cluster tree;
3. Determine where to cut the hierarchical tree
into clusters. In hierarchical clustering, we
have to specify the cluster number m in ad-
vance.
In this paper, we use the hierarchical cluster-
ing implemented in Matlab Statistics Toolbox.
Note that although we use hierarchical clustering
here, the cluster hierarchy is not necessary for the
clustering-based method.
6.2 Spectral Clustering
In recent years, spectral clustering has become one
of the most popular modern clustering algorithms.
Spectral clustering makes use of the spectrum of
the similarity matrix of the data to perform dimen-
sionality reduction for clustering into fewer di-
mensions, which is simple to implement and often
outperforms traditional clustering methods such as
k-means. Detailed introduction to spectral cluster-
ing could be found in (von Luxburg, 2006).
In this paper, we use the spectral clustering tool-
box developed by Wen-Yen Chen, et al (Chen et
al., 2008) 1. Since the cooccurrence-based term
relatedness is usually sparse, the traditional eigen-
value decomposition in spectral clustering will
sometimes get run-time error. In this paper, we
use the singular value decomposition (SVD) tech-
nique for spectral clustering instead.
For spectral clustering, two parameters are re-
quired to be set by the user: the cluster number
m, and ? which is used in computing similarities
from object distances
s(i, j) = exp(
?d(i, j)
2
2?
2
) (7)
where s(i, j) and d(i, j) are the similarity and dis-
tance between i and j respectively.
1The package could be accessed via http://www.cs.
ucsb.edu/
?
wychen/sc.html.
6.3 Affinity Propagation
Another powerful clustering method, Affinity
Propagation, is based on message passing tech-
niques. AP was proposed in (Frey and Dueck,
2007), where AP was reported to find clusters with
much lower error than those found by other meth-
ods. In this paper, we use the toolbox developed
by Frey, et al 2.
Detailed description of the algorithm could be
found in (Frey and Dueck, 2007). Here we intro-
duced three parameters for AP:
? Preference. Rather than requiring prede-
fined number of clusters, Affinity Propaga-
tion takes as input a real number p for each
term, so that the terms with larger p are more
likely to be chosen as exemplars, i.e., cen-
troids of clusters. These values are referred
to as ?preferences?. The preferences are usu-
ally be set as the maximum, minimum, mean
or median of s(i, j), i 6= j.
? Convergence criterion. AP terminates if (1)
the local decisions stay constant for I
1
itera-
tions; or (2) the number of iterations reaches
I
2
. In this work, we set I
1
to 100 and I
2
to
1, 000.
? Damping factor. When updating the mes-
sages, it is important to avoid numerical os-
cillations by using damping factor. Each
message is set to ? times its value from the
previous iteration plus 1 ? ? times its pre-
scribed updated value, where the damping
factor ? is between 0 and 1. In this paper we
set ? = 0.9.
7 From Exemplar Terms to Keyphrases
After term clustering, we select the exemplar
terms of each clusters as seed terms. In Affinity
Propagation, the exemplar terms are directly ob-
tained from the clustering results. In hierarchical
clustering, exemplar terms could also be obtained
by the Matlab toolbox. While in spectral cluster-
ing, we select the terms that are most close to the
centroid of a cluster as exemplar terms.
As reported in (Hulth, 2003), most manually
assigned keyphrases turn out to be noun groups.
Therefore, we annotate the document with POS
2The package could be accessed via http://www.
psi.toronto.edu/affinitypropagation/.
261
tags using Stanford Log-Linear Tagger 3, and then
extract the noun groups whose pattern is zero or
more adjectives followed by one or more nouns.
The pattern can be represented using regular ex-
pressions as follows
(JJ) ? (NN |NNS|NNP )+
where JJ indicates adjectives and various forms
of nouns are represented using NN , NNS and
NNP . From these noun groups, we select the
ones that contain one or more exemplar terms to
be the keyphrases of the document.
In this process, we may find single-word
keyphrases. In practice, only a small fraction of
keyphrases are single-word. Thus, as a part of
postprocessing process, we have to use a frequent
word list to filter out the terms that are too com-
mon to be keyphrases.
8 Experiment Results
8.1 Datasets and Evaluation Metric
The dataset used in the experiments is a collec-
tion of scientific publication abstracts from the In-
spec database and the corresponding manually as-
signed keyphrases 4. The dataset is used in both
(Hulth, 2003) and (Mihalcea and Tarau, 2004).
Each abstract has two kinds of keyphrases: con-
trolled keyphrases, restricted to a given dictionary,
and uncontrolled keyphrases, freely assigned by
the experts. We use the uncontrolled keyphrases
for evaluation as proposed in (Hulth, 2003) and
followed by (Mihalcea and Tarau, 2004).
As indicated in (Hulth, 2003; Mihalcea and
Tarau, 2004), in uncontrolled manually assigned
keyphrases, only the ones that occur in the cor-
responding abstracts are considered in evaluation.
The extracted keyphrases of various methods and
manually assigned keyphrases are compared after
stemming.
In the experiments of (Hulth, 2003), for her su-
pervised method, Hulth splits a total of 2, 000 ab-
stracts into 1, 000 for training, 500 for validation
and 500 for test. In (Mihalcea and Tarau, 2004),
due to the unsupervised method, only the test set
was used for comparing the performance of Tex-
tRank and Hulth?s method.
3The package could be accessed via http://http://
nlp.stanford.edu/software/tagger.shtml.
4Many thanks to Anette Hulth for providing us the dataset.
For computing Wikipedia-based relatedness,
we use a snapshot on November 11, 2005 5. The
frequent word list used in the postprocessing step
for filtering single-word phrases is also computed
from Wikipedia. In the experiments of this pa-
per, we add the words that occur more than 1, 000
times in Wikipedia into the list.
The clustering-based method is completely un-
supervised. Here, we mainly run our method on
test set and investigate the influence of relatedness
measurements and clustering methods with differ-
ent parameters. Then we compare our method
with two baseline methods: Hulth?s method and
TextRank. Finally, we analyze and discuss the per-
formance of the method by taking the abstract of
this paper as a demonstration.
8.2 Influence of Relatedness Measurements
We first investigate the influence of semantic re-
latedness measurements. By systematic experi-
ments, we find that Wikipedia-based relatedness
outperforms cooccurrence-based relatedness for
keyphrase extraction, though the improvement is
not significant. In Table 1, we list the perfor-
mance of spectral clustering with various related-
ness measurements for demonstration. In this ta-
ble, the w indicates the window size for counting
cooccurrences in cooccurrence-based relatedness.
cos, euc, etc. are different measures for com-
puting Wikipedia-based relatedness which we pre-
sented in Section 5.2.
Table 1: Influence of relatedness measurements
for keyphrase extraction.
Parameters Precision Recall F1-measure
Cooccurrence-based Relatedness
w = 2 0.331 0.626 0.433
w = 4 0.333 0.621 0.434
w = 6 0.331 0.630 0.434
w = 8 0.330 0.623 0.432
w = 10 0.333 0.632 0.436
Wikipedia-based Relatedness
cos 0.348 0.655 0.455
euc 0.344 0.634 0.446
pmi
p
0.344 0.621 0.443
pmi
t
0.344 0.619 0.442
pmi
c
0.350 0.660 0.457
ngd 0.343 0.620 0.442
5The dataset could be get from http://www.cs.
technion.ac.il/
?
gabr/resources/code/
wikiprep/.
262
We use spectral clustering here because it out-
performs other clustering techniques, which will
be shown in the next subsection. The results in Ta-
ble 1 are obtained when the cluster number m =
2
3
n, where n is the number of candidate terms ob-
tained in Section 5. Besides, for Euclidean dis-
tance and Google distance, we set ? = 36 of For-
mula 7 to convert them to corresponding similari-
ties, where we get the best result when we conduct
different trails with ? = 9, 18, 36, 54, though there
are only a small margin among them.
As shown in Table 1, although the method using
Wikipedia-based relatedness outperforms that us-
ing cooccurrence-based relatedness, the improve-
ment is not prominent. Wikipedia-based related-
ness is computed according to global statistical in-
formation on Wikipedia. Therefore it is more pre-
cise than cooccurrence-based relatedness, which is
reflected in the performance of the keyphrase ex-
traction. However, on the other hand, Wikipedia-
based relatedness does not catch the document-
specific relatedness, which is represented by the
cooccurrence-based relatedness. It will be an in-
teresting future work to combine these two types
of relatedness measurements.
From this subsection, we conclude that, al-
though the method using Wikipedia-based related-
ness performs better than cooccurrence-based one,
due to the expensive computation of Wikipedia-
based relatedness, the cooccurrence-based one is
good enough for practical applications.
8.3 Influence of Clustering Methods and
Their Parameters
To demonstrate the influence of clustering meth-
ods for keyphrase extraction, we fix the relat-
edness measurement as Wikipedia-based pmi
c
,
which has been shown in Section 8.2 to be the best
relatedness measurement.
In Table 2, we show the performance of three
clustering techniques for keyphrase extraction.
For hierarchical clustering and spectral clustering,
the cluster number m are set explicitly as the pro-
portion of candidate terms n, while for Affinity
Propagation, we set preferences as the minimum,
mean, median and maximum of s(i, j) to get dif-
ferent number of clusters, denoted as min, mean,
median and max in the table respectively.
As shown in the table, when cluster number m
is large, spectral clustering outperforms hierarchi-
cal clustering and Affinity Propagation. Among
Table 2: Influence of clustering methods for
keyphrase extraction.
Parameters Precision Recall F1-measure
Hierarchical Clustering
m =
1
4
n 0.365 0.369 0.367
m =
1
3
n 0.365 0.369 0.367
m =
1
2
n 0.351 0.562 0.432
m =
2
3
n 0.346 0.629 0.446
m =
4
5
n 0.340 0.657 0.448
Spectral Clustering
m =
1
4
n 0.385 0.409 0.397
m =
1
3
n 0.374 0.497 0.427
m =
1
2
n 0.374 0.497 0.427
m =
2
3
n 0.350 0.660 0.457
m =
4
5
n 0.340 0.679 0.453
Affinity Propagation
p = max 0.331 0.688 0.447
p = mean 0.433 0.070 0.121
p = median 0.422 0.078 0.132
p = min 0.419 0.059 0.103
these methods, only Affinity Propagation under
some parameters performs poorly.
8.4 Comparing with Other Algorithms
Table 3 lists the results of the clustering-based
method compared with the best results reported
in (Hulth, 2003; Mihalcea and Tarau, 2004) on
the same dataset. For each method, the table lists
the total number of assigned keyphrases, the mean
number of keyphrases per abstract, the total num-
ber of correct keyphrases, and the mean number of
correct keyphrases. The table also lists precision,
recall and F1-measure. In this table, hierarchical
clustering, spectral clustering and Affinity Propa-
gation are abbreviated by ?HC?, ?SC? and ?AP?
respectively.
The result of Hulth?s method listed in this ta-
ble is the best one reported in (Hulth, 2003) on the
same dataset. This is a supervised classification-
based method, which takes more linguistic fea-
tures in consideration for keyphrase extraction.
The best result is obtained using n-gram as candi-
date keyphrases and adding POS tags as candidate
features for classification.
The result of TextRank listed here is the best
one reported in (Mihalcea and Tarau, 2004) on the
same dataset. To obtain the best result, the authors
built an undirected graph using window w = 2
on word sequence of the given document, and ran
263
Table 3: Comparison results of Hulth?s method, TextRank and our clustering-based method.
Assigned Correct
Method Total Mean Total Mean Precision Recall F1-measure
Hulth?s 7,815 15.6 1,973 3.9 0.252 0.517 0.339
TextRank 6,784 13.7 2,116 4.2 0.312 0.431 0.362
HC 7,303 14.6 2,494 5.0 0.342 0.657 0.449
SC 7,158 14.3 2,505 5.0 0.350 0.660 0.457
AP 8,013 16.0 2,648 5.3 0.330 0.697 0.448
PageRank on it.
In this table, the best result of hierarchical clus-
tering is obtained by setting the cluster number
m =
2
3
n and using Euclidean distance for comput-
ing Wikipedia-based relatedness. The parameters
of spectral clustering are the same as in last sub-
section. For Affinity Propagation, the best result
is obtained under p = max and using Wikipedia-
based Euclidean distance as relatedness measure.
From this table, we can see clustering-
based method outperforms TextRank and Hulth?s
method. For spectral clustering, F1-measure
achieves an approximately 9.5% improvement as
compared to TextRank.
Furthermore, since the clustering-based method
is unsupervised, we do not need any set for train-
ing and validation. In this paper, we also carry out
an experiment on the whole Hulth?s dataset with
2, 000 abstracts. The performance is similar to
that on 500 abstracts as shown above. The best
result is obtained when we use spectral clustering
by setting m = 2
3
n with Wikipedia-based pmi
c
relatedness, which is the same in 500 abstracts. In
this result, we extract 29, 517 keyphrases, among
which 9, 655 are correctly extracted. The preci-
sion, recall and F1-measure are 0.327, 0.653 and
0.436 respectively. The experiment results show
that the clustering-based method is stable.
8.5 Analysis and Discussions
From the above experiment results, we can see the
clustering-based method is both robust and effec-
tive for keyphrase extraction as an unsupervised
method.
Here, as an demonstration, we use spectral clus-
tering and Wikipedia-based pmi
c
relatedness to
extract keyphrases from the abstract of this pa-
per. The extracted stemmed keyphrases under var-
ious cluster numbers are shown in Figure 1. In
this figure, we find that when m = 1
4
n,
1
3
n,
1
2
n,
the extracted keyphrases are identical, where the
exemplar terms under m = 1
3
n are marked in
boldface. We find several aspects like ?unsuper-
vised?, ?exemplar term? and ?keyphrase extrac-
tion? are extracted correctly. In fact, ?clustering
technique? in the abstract should also be extracted
as a keyphrase. However, since ?clustering? is
tagged as a verb that ends in -ing, which disagrees
the noun group patterns, thus the phrase is not
among the extracted keyphrases.
When m = 2
3
n, the extracted keyphrases
are noisy with many single-word phrases. As
the cluster number increases, more exemplar
terms are identified from these clusters, and more
keyphrases will be extracted from the document
based on exemplar terms. If we set the cluster
number to m = n, all terms will be selected as
exemplar terms. In this extreme case, all noun
groups will be extracted as keyphrases, which
is obviously not proper for keyphrase extraction.
Thus, it is important for this method to appropri-
ately specify the cluster number.
In the experiments, we also notice that frequent
word list is important for keyphrase extraction.
Without the list for filtering, the best F1-measure
will decrease by about 5 percent to 40%. How-
ever, the solution of using frequent word list is
somewhat too simple, and in future work, we plan
to investigate a better combination of clustering-
based method with traditional methods using term
frequency as the criteria.
9 Conclusion and Future Work
In this paper, we propose an unsupervised
clustering-based keyphrase extraction algorithm.
This method groups candidate terms into clus-
ters and identify the exemplar terms. Then
keyphrases are extracted from the document based
on the exemplar terms. The clustering based on
term semantic relatedness guarantees the extracted
keyphrases have a good coverage of the document.
Experiment results show the method has a good ef-
264
Figure 1: Keyphrases in stemmed form extracted
from this paper?s abstract.
Keyphrases when m = 1
4
n,
1
3
n,
1
2
n
unsupervis method; various unsupervis rank
method; exemplar term; state-of-the-art
graph-bas rank method; keyphras; keyphras
extract
Keyphrases when m = 2
3
n
unsupervis method; manual assign; brief sum-
mari; various unsupervis rank method; exem-
plar term; document; state-of-the-art graph-bas
rank method; experi; keyphras; import score;
keyphras extract
fectiveness and robustness, and outperforms base-
lines significantly.
Future work may include:
1. Investigate the feasibility of clustering di-
rectly on noun groups;
2. Investigate the feasibility of combining
cooccurrence-based and Wikipedia-based re-
latedness for clustering;
3. Investigate the performance of the method on
other types of documents, such as long arti-
cles, product reviews and news;
4. The solution of using frequent word list
for filtering out too common single-word
keyphrases is undoubtedly simple, and we
plan to make a better combination of
the clustering-based method with traditional
frequency-based methods for keyphrase ex-
traction.
Acknowledgments
This work is supported by the National 863 Project
under Grant No. 2007AA01Z148 and the Na-
tional Science Foundation of China under Grant
No. 60621062. The authors would like to thank
Anette Hulth for kindly sharing her datasets.
References
Mo Chen, Jian-Tao Sun, Hua-Jun Zeng, and Kwok-Yan
Lam. 2005. A practical system of keyphrase extrac-
tion for web pages. In Proceedings of the 14th ACM
international conference on Information and knowl-
edge management, pages 277?278.
Wen Y. Chen, Yangqiu Song, Hongjie Bai, Chih J. Lin,
and Edward Chang. 2008. Psc: Paralel spectral
clustering. Submitted.
Rudi L. Cilibrasi and Paul M. B. Vitanyi. 2007. The
google similarity distance. IEEE Transactions on
Knowledge and Data Engineering, 19(3):370?383.
Mark Dredze, Hanna M. Wallach, Danny Puller, and
Fernando Pereira. 2008. Generating summary key-
words for emails using topics. In Proceedings of the
13th international conference on Intelligent user in-
terfaces, pages 199?206.
S. Elbeltagy and A. Rafea. 2009. Kp-miner: A
keyphrase extraction system for english and arabic
documents. Information Systems, 34(1):132?144.
Eibe Frank, Gordon W. Paynter, Ian H. Witten, Carl
Gutwin, and Craig G. Nevill-Manning. 1999.
Domain-specific keyphrase extraction. In Proceed-
ings of the 16th International Joint Conference on
Artificial Intelligence, pages 668?673.
Brendan J J. Frey and Delbert Dueck. 2007. Clustering
by passing messages between data points. Science.
E. Gabrilovich and S. Markovitch. 2007. Computing
semantic relatedness using wikipedia-based explicit
semantic analysis. In Proceedings of the 20th Inter-
national Joint Conference on Artificial Intelligence,
pages 6?12.
M. Grineva, M. Grinev, and D. Lizorkin. 2009. Ex-
tracting key terms from noisy and multi-theme docu-
ments. In Proceedings of the 18th international con-
ference on World wide web, pages 661?670. ACM
New York, NY, USA.
Jiawei Han and Micheline Kamber. 2005. Data Min-
ing: Concepts and Techniques, second edition. Mor-
gan Kaufmann.
Chong Huang, Yonghong Tian, Zhi Zhou, Charles X.
Ling, and Tiejun Huang. 2006. Keyphrase extrac-
tion using semantic networks structure analysis. In
Proceedings of the 6th International Conference on
Data Mining, pages 275?284.
Anette Hulth. 2003. Improved automatic keyword ex-
traction given more linguistic knowledge. In Pro-
ceedings of the 2003 conference on Empirical meth-
ods in natural language processing, pages 216?223.
A. Hulth. 2004. Reducing false positives by expert
combination in automatic keyword indexing. Re-
cent Advances in Natural Language Processing III:
Selected Papers from RANLP 2003, page 367.
Daniel Kelleher and Saturnino Luz. 2005. Automatic
hypertext keyphrase detection. In Proceedings of the
19th International Joint Conference on Artificial In-
telligence.
265
Marina Litvak and Mark Last. 2008. Graph-based
keyword extraction for single-document summariza-
tion. In Proceedings of the workshop Multi-source
Multilingual Information Extraction and Summa-
rization, pages 17?24.
Rada Mihalcea and Paul Tarau. 2004. Textrank:
Bringing order into texts. In Proceedings of the
2004 Conference on Empirical Methods in Natural
Language Processing.
Peter D. Turney. 1999. Learning to Extract Keyphrases
from Text. National Research Council Canada, In-
stitute for Information Technology, Technical Report
ERB-1057.
U. von Luxburg. 2006. A tutorial on spectral clus-
tering. Technical report, Max Planck Institute for
Biological Cybernetics.
Xiaojun Wan and Jianguo Xiao. 2008a. Col-
labrank: Towards a collaborative approach to single-
document keyphrase extraction. In Proceedings of
COLING, pages 969?976.
Xiaojun Wan and Jianguo Xiao. 2008b. Single
document keyphrase extraction using neighborhood
knowledge. In Proceedings of the Twenty-Third
AAAI Conference on Artificial Intelligence, pages
855?860.
266
Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010), pages 1011?1019,
Beijing, August 2010
Explore the Structure of Social Tags by Subsumption Relations
Xiance Si, Zhiyuan Liu, Maosong Sun
Department of Computer Science and Technology
State Key Lab on Intelligent Technology and Systems
National Lab for Information Science and Technology
Tsinghua University
{sixiance,lzy.thu}@gmail.com, sms@tsinghua.edu.cn
Abstract
Thanks to its simplicity, social tagging
system has accumulated huge amount of
user contributed tags. However, user
contributed tags lack explicit hierarchi-
cal structure, while many tag-based ap-
plications would benefit if such a struc-
ture presents. In this work, we explore
the structure of tags with a directed and
easy-to-evaluate relation, named as the
subsumption relation. We propose three
methods to discover the subsumption rela-
tion between tags. Specifically, the tagged
document?s content is used to find the re-
lations, which leads to better result. Be-
sides relation discovery, we also propose
a greedy algorithm to eliminate the re-
dundant relations by constructing a Lay-
ered Directed Acyclic Graph (Layered-
DAG) of tags. We perform quantita-
tive evaluations on two real world data
sets. The results show that our methods
outperform hierarchical clustering-based
approach. Empirical study of the con-
structed Layered-DAG and error analysis
are also provided.
1 Introduction
In this work, we aim at exploring the structure of
social tags. Social tagging is widely used in Web-
based services, in which a user could use any word
to annotate an object. Thanks to its simplicity, ser-
vices with social tagging features have attracted a
lot of users and have accumulated huge amount of
annotations. However, comparing to taxonomies,
social tagging has an inherent shortcoming, that
Figure 1: Examples of (a) flat tag cloud, (b) hier-
archical clusters, and (c) subsumption relations.
there is no explicit hierarchical relations between
tags. Figure 1 (a) shows an example of the com-
monly used flat tag cloud, in which only the pop-
ularity of a tag is concerned. Kome et al (2005)
argued that implicit hierarchical relations exist in
social tags. Previous literature shows that orga-
nizing tags in hierarchical structures will help tag-
based Information Retrieval applications (Begel-
man et al, 2006; Brooks and Montanez, 2006).
Hierarchical clustering could reveal the simi-
larity relations of tags. Figure 1 (b) shows an
example of a typical hierarchical clustering of
tags. While clusters can capture similarity be-
tween tags, problems still remain: First, clusters
mix different relations, such as synonyms and hy-
pernyms. Second, clusters also ignore the direc-
tion of relations, for example, the direction in
browser ? firefox. Third, it is hard to evalu-
ate the correctness of clustering. Specifically, it
is hard to tell if two tags are similar or not. In
practice, directed and easy-to-evaluate relations
between tags are preferred, such as Figure 1 (c).
In this work, we explore the structure of so-
cial tags by discovering a directed and easy-to-
evaluate relation between tags, named subsump-
tion relation. A tag ta subsumes tb, if and only
if wherever tb is used, we can also replace it
1011
with ta. Unlike similar-to, subsumption relation
is asymmetric, and its correctness is easier to as-
sess. Then, we propose three ways to discover the
subsumption relations, through tag-tag, tag-word
and tag-reason co-occurrences respectively. In the
third way, A tag?s reason is defined as the word
in the content that explains the using of the tag.
We employ the Tag Allocation Model (TAM) pro-
posed by Si et al (2010) to find the reason for
each tag. Besides subsumption relation discov-
ery, we also propose a greedy algorithm to remove
the redundant relations. The removal is done by
constructing a Layered Directed Acyclic Graph
(Layered-DAG) of tags with the subsumption re-
lations.
We carried out the experiments on two real
world data sets. The results of quantitative evalu-
ation showed that tag-reason based approach out-
performed other two methods and a commonly
used hierarchical clustering-based method. We
also do empirical study on the output of Layered-
DAG construction.
The contribution of this paper can be summa-
rized as follows:
1. We explore the structure of social tags by
a clearly defined subsumption relation. We
propose methods to discover the subsump-
tion relation automatically, leveraging both
the co-occurred tags and the content of an-
notated document.
2. We propose an algorithm to eliminate the re-
dundant relations by constructing a Layered-
DAG of tags.
3. We perform both empirical and quantitative
evaluation of proposed methods on two real
world data sets.
The rest of the paper is organized as follows:
Section 2 surveys the related work; Section 3 de-
fines the subsumption relation we used, and pro-
poses methods for relation discovery; Section 4
proposes a greedy algorithm for Layered-DAG
construction; Section 5 explains the experimen-
tal settings and shows the evaluation results. Sec-
tion 6 concludes the paper.
2 Related Work
To explore the hierarchical relations between tags,
an intuitive way is to cluster the tags into hier-
archical clusters. Wu et al (2006b) used a fac-
torized model, namely Latent Semantic Analy-
sis, to group tags into non-hierarchical topics for
better recommendation. Brooks et al (2006) ar-
gued that performing Hierarchical Agglomerative
Clustering (HAC) on tags can improve the col-
laborative tagging system. Later, HAC on tags
was also used for improving personalized recom-
mendation (Shepitsen et al, 2008). Heymann et
al. (2006) clustered tags into a tree by a similarity-
based greedy tree-growing method. They evalu-
ated the obtained trees empirically, and reported
that the method is simple yet powerful for orga-
nizing tags with hierarchies. Based on Heymann
et al?s work, Schwarzkopf et al (2007) proposed
an approach for modeling users with the hierarchy
of tags. Begelman et al (2006) used top-down hi-
erarchical clustering, instead of bottom-up HAC,
to organize tags, and argued that tag hierarchies
improve user experiences in their system. Most
of the hierarchical clustering algorithms rely on
the symmetric similarity between tags, while the
discovered relations are hard to evaluate quantita-
tively, since one cannot distinguish similar from
not-similar with a clear boundary.
People have also worked on bridging social tag-
ging systems and ontologies. An ontology defines
relations between entities. Peter Mika (2005) pro-
posed an extended scheme of social tagging that
includes actors, concepts and objects, and used
tag co-occurrences to construct an ontology from
social tags. Wu et al (2006a) used hierarchical
clustering to build ontology from tags that also
use similar-to relations. Later, ontology schemes
that fits social tagging system were proposed, such
as (Van Damme et al, 2007) and (Echarte et
al., 2007), which mainly focused on the relation
between tags, objects and users, rather than be-
tween tags themselves. Alexandre Passant (2007)
mapped tags to domain ontologies manually to
improve information retrieval in social media. To
construct tag ontology automatically, Angeletou
et al (2007) used ontologies built by domain ex-
perts to find relations between tags, but observed
a very low coverage. Specia et al (2007) pro-
posed an integrated framework for organizing tags
by existing ontologies, but no experiment was per-
formed. Kim et al (2008) summarized the state-
1012
of-the-art methods to model tags with semantic
annotations.
Before social tagging was invented, Sanderson
et al (1999) proposed to use subsumption relation
to organize words in text hierarchically. Schmitz
et al (2006) followed the idea to use subsumption
relation for organizing Flickr 1 tag, where tag-tag
co-occurrences are used for discover the relations.
We follow the idea of subsumption relation in this
paper, and explore alternative ways for relation
discovery.
3 Subsumption Relations in Tags
In this section, we define the subsumption relation
used in our study, and propose three methods to
discover the subsumption relations.
3.1 Definitions
First, we introduce the symbols used through out
the paper: A tag is denoted as t ? T , where T is
the set of all tags. To distinguish from words, we
use fixed-width to represent the example tags.
An annotated document is denoted as d ? D,
where D is the set of all documents. The words
in d are denoted as a set {wdi}, where i ? [1, |d|],
and |d| is the number of words in d.
Inspired by (Sanderson and Croft, 1999), we
define the subsumption relation between ta and tb
as follows: ta subsumes tb, means that wherever
the tag tb is used, ta can also be used without
ambiguity. The subsumption relation between ta
and tb is denoted as ta ?s tb.
Subsumption relation is directional, that is,
ta ?s tb does not imply tb ?s ta. For ex-
ample, literature ?s chineseliterature,
since for any document annotated with
chineseliterature, we can also annotate
it with literature. However, if we swapped the
two tags, the statement would not hold.
Subsumption relation is more strict than simi-
larity. For example, during the time of Haiti earth-
quake, the tag earthquake is close to haiti in
similarity, but none of them implies the use of the
other one: document annotated with earthquake
may refer to the earthquake in China, while docu-
1http://www.flickr.com. An image sharing site that allows
users to annotate images with tags
ment annotated with haiti may mean the travel-
ing experience in Haiti.
Note that the subsumption has transitivity prop-
erty, that ta ?s tb and tb ?s tc means ta ?s
tc, which corresponds to our intuition. For in-
stance, naturaldisaster ?s earthquake and
disaster?snaturaldisaster means disaster
?searthquake.
3.2 Discover Subsumption Relation
We discover the subsumption relations by estimat-
ing the probability p(ta|tb). The motivation is, if
ta ?s tb and tb is used, it would be more likely to
see ta. So, by sorting all (ta, tb) pairs by p(ta|tb)
in descending order, top-ranked pairs are more
likely to have subsumption relations.
In this work, we present three methods to esti-
mate the probability p(ta|tb), using tag-tag, tag-
word and tag-reason co-occurrences respectively.
By using tag-word and tag-reason co-occurrences,
we leverage the content of the annotated docu-
ment for subsumption relation discovery.
3.2.1 Tag-Tag Co-occurrences Approach
The most intuitive way to estimate p(ta|tb) is
via tag-tag co-occurrences. Specifically, we use
the following formula:
p(ta|tb) =
Nd(ta, tb)
Nd(tb)
, (1)
where Nd(ta, tb) is the number of documents that
are annotated by both ta and tb, and Nd(tb) is the
number of documents annotated by tb. We de-
note the tag-tag co-occurrences approach as TAG-
TAG.
The use of TAG-TAG can be found in previous
literature for organizing tags for photos(Schmitz,
2006). One of TAG-TAG?s benefits is that it does
not rely on the content of the annotated document,
thus it can be applied to tags for non-text objects,
such as images and music. However, when com-
ing to text documents, this benefit is also a short-
coming, that TAG-TAG makes no use of the con-
tent when it is available.
Using TAG-TAG for subsumption relation dis-
covery relies on an implication, that if a user has
annotated d with tb, he would also annotate all
tags that subsumes tb. The implication may not
always hold in real world situations. For example,
1013
a novel reader would use tags such as scifi and
mystery to organize his collections, but he is not
likely to annotate each of his collection as novel
or book, since they are too obvious for him. We
name the problem as the omitted-tag problem.
3.2.2 Tag-Word Co-occurrences Approach
When the content of the annotated document
is available, using it for estimating p(ta|tb) is a
natural thought. The content is expected to be
complete and information-rich whether or not the
user has omitted any tags. We use the follow-
ing formula to estimate p(ta|tb) by tag-word co-
occurrences:
p(ta|tb) =
?
w?W
p(ta|w)p(w|tb)
=
?
w?W
Nd(ta, w)
Nd(w)
Nd(tb, w)
Nd(tb)
, (2)
where Nd(ta, w) is the number of documents that
contains both tag ta and word w, and Nd(w) is
the number of documents that contains the word
w. We denote this approach as TAG-WORD.
Instead of computing tag-tag co-occurrences
directly, TAG-WORD uses words in the document
as a bridge to estimate p(ta|tb). By introduc-
ing words, the estimation is less affected by the
omitted-tag problem, Take the novel reader exam-
ple again: Although he does not use the tag novel
too often, the words in book descriptions would
suggest the using of novel, according to all other
documents annotated by novel.
While using the content may weaken the
omitted-tag problem, it also brings the noise in
text to the estimation. Not every word in the con-
tent is related to one of the tags. To the oppo-
site, most words are functional words or that about
other aspects of the document. p(ta|tb) estimated
by using all words may largely depends on these
irrelevant words.
3.2.3 Tag-Reason Co-occurrences Approach
To focus on the words that are highly relevant
to the interested tags, we propose the third method
that uses tag-reason co-occurrences. The reason is
defined as the word(s) that can explain the using
of a tag in the document. For example, the tag
scifi for a book could be explained by the words
?robot?, ?Asimov? in the book description. If the
reason of each tag could be identified, the noise in
content-based p(ta|tb) could be reduced.
Si et al (2010) proposed a probabilistic model
for content-based social tags, named Tag Allo-
cation Model (TAM). TAM introduces a latent
variable r for each tag in the data set, known
as the reason variable. The value of r can be a
word in the corresponding document, or a global
noise variable ?. Allowing the reason of tags to
be a global noise makes TAM deal with content-
irrelevant tags and mistakenly annotated tags ef-
fectively. The likelihood that a document d is an-
notated by tag t is given as:
p(t|d) =
?
w?d
p(t|r = w)p(r = w|d)p(s = 0)
+ p(t|?)p(r = ?)p(s = 1), (3)
where r is the reason of the tag t, r ? {wdi|i ?
[0, |d|]} ? {?}, ? is the global noise variable. s is
the source of reason t, s = 0 means the source is
the content of the document, while s = 1 means
the source is the global noise variable ?. TAM
can be trained use Gibbs sampling method. For
the details of TAM, please refer to (Si and Sun,
2010).
With a trained TAM, we can infer p(t|r), the
probability of seeing a tag t when using r as the
reason, and p(r|t), the probability of choosing r
as the reason for tag t. With these probabilities,
we can estimate p(ta|tb) by
p(ta|tb) =
?
r?W
p(ta|r)p(r|tb). (4)
Note that we use only word reasons (r ? W ),
ignoring the noise reason ? completely. We de-
note this approach as TAG-REASON.
With the help of TAM, TAG-REASON cov-
ers the problems of the TAG-WORD method in
two aspects: First, instead of using all words,
TAG-REASON emphasizes on the really relevant
words, which are the reasons identified by TAM.
Second, by ignoring the noise variable ?, TAG-
REASON is less affected by the content-irrelevant
noise tags, such as thingstodo or myown.
After p(ta|tb) is estimated for each (ta, tb) ?
T ?T , we use the top-n pairs with largest p(ta|tb)
1014
Figure 2: DAG and Layered-DAG
as the final set of discovered subsumption rela-
tions.
4 Remove Redundancy with
Layered-DAG Construction
The discovered subsumption relations connect all
tags into a directed graph G = {V,E}, where V
is the set of nodes, with each node is a tag; E is
the set of edges, an edge eta,tb from ta to tb means
ta ?s tb. Furthermore, we define the weight of
each edge we as the probability p(ta|tb).
Recalling that subsumption relation has transi-
tivity property, to avoid the cyclic references in G,
we would like to turn G into a Directed Acyclic
Graph (DAG). Further, DAG may also contains
redundant information. Figure 2 (a) shows a part
of a DAG. Note the edge marked as ?*?, which
is perfectly correct, but does not provide extra
information, since literature ?s novel and
novel?s scifi-novel have already implied that
literature?s novel. We would like to remove
these redundant relations, turning a DAG into the
form of Figure 2 (b).
We define Layered-DAG formally as follows:
For a DAG G, when given any pair of nodes, if ev-
ery path that can connect them has equal length, G
is a Layered-DAG. Layered-DAG prohibits edges
that link cross layers, such like edge ?*? in Fig-
ure 2 (a). Constructing a Layered-DAG from the
discovered relations can eliminate the redundant
information.
Given a set of subsumption relations, multiple
Layered-DAGs may be constructed. In particular,
we want to find the Layered-DAG that maximizes
the sum of all edges? weights. Weight maximiza-
tion implies two concerns: First, when we need
to remove a relation to resolve the conflicts or re-
dundancy, the one with lower weight is prefered.
Layered-DAG Construction Algorithm
Input: A set of weighted relations, R = {ta ?s tb|ta ? T, tb ? T},
wta?stb > 0
Output: A Layered-DAG of tags G? = {V ?, E?}
1: V ? = {}
2: while R 6= ?
3: if V ? = ?
4: choose ta ?s tb ? R with highest weight.
5: E? ? ta ?s tb
6: V ? ? ta, V ? ? tb.
7: remove ta ?s tb from R.
8: else
9: C ? {ta ?s tb|ta ?s tb ? R, {ta, tb} ? V ? 6= ?}
10: for ta ?s tb ? C in descending weight order
11: if adding ta ?s tb to G? keeps G? a Layered-DAG.
12: E? ? ta ?s tb
13: V ? ? ta, V ? ? tb.
14: break
15: endif
16: remove ta ?s tb from R.
17: endfor
18: endif
19: endwhile
20: output G?
Figure 3: A greedy algorithm for constructing
Layered-DAG of tags
Second, when more than one valid Layered-DAGs
are available, we want to use the one that contains
as many edges as possible.
Finding and proving an optimal algorithm for
maximum Layered-DAG construction are beyond
the scope of this paper. Here we present a greedy
algorithm that works well in practice, as described
in Figure 3.
The proposed algorithm starts with a minimal
Layered-DAG G? that contains only the high-
est weighted relation in R (Steps 1-8). Then, it
moves an edge in G to G? once a time, ensuring
that adding the new edge still keeps G? a valid
Layered-DAG (Step 11), and the new edge has the
highest weights among all valid candidates (Steps
9-10).
5 Experiments
In this section, we show the experimental results
of proposed methods. Specifically, we focus on
the following points:
? The quality of discovered subsumption rela-
tions by different methods.
? The characteristics of wrong subsumption re-
lations discovered.
? The effect of Layered-DAG construction on
the quality of relations.
? Empirical study of the resulted Layered-
DAG.
1015
Name N N?tag N?content
BLOG 100,192 2.78 332.87
BOOK 110,371 8.51 204.76
Table 1: Statistics of the data sets. N is the num-
ber of documents. N?tag is the mean number of
tags per document. N?content is the mean number
of words per document.
5.1 Data Sets
We use two real world social tagging data sets.
The first data set, named BLOG, is a collection
of blog posts annotated by blog authors, which
is crawled from the web. The second data set,
named BOOK, is from a book collecting and shar-
ing site2, which contains description of Chinese
books and user contributed tags. Table 1 lists the
basic statistics of the data sets.
The two data sets have different characteristics.
Documents in BLOG are longer, not well written,
and the number of tags per document is small. To
the opposite, documents in BOOK are shorter but
well written, and there are more tags for each doc-
ument.
5.2 Discovered Subsumption Relations
5.2.1 Experimental Settings
For BLOG, we use the tags that have been used
more than 10 times; For BOOK, we use the tags
that have been used more than 50 times. We per-
form 100 iterations of Gibbs sampling when train-
ing the TAM model, with first 50 iterations as
the burn-in iterations. All the estimation meth-
ods require proper smoothing. Here we use ad-
ditive smoothing for all methods, which adds a
very small number (0.001 in our case) to all raw
counts. Sophisticated smoothing method could be
employed, but is out of the scope of this paper.
5.2.2 Evaluation
We use precision and coverage to evaluate the
discovered relations at any given cut-off threshold
n. First, we sort the discovered relations by their
weights in descending order. Then, we take the
top-n relations, discarding the others. For the re-
maining relations, precision is computed as Nc/n,
Nc is the number of correct relations in the top-n
2http://www.douban.com
list; coverage is computed as Nt/|T |, where Nt is
the number of unique tags appeared in the top-n
list, and |T | is the total number of tags.
To get Nc, the number of correct relations, we
need a standard judgement of the correctness of
relations, which involves human labeling. To min-
imize the bias in human assessment, we use pool-
ing, which is a widely accepted method in Infor-
mation Retrieval research (Voorhees and Harman,
2005). Pooling works as follows: First, relations
obtained by different methods are mixed together,
creating a pool of relations. Second, the pool is
shuffled, so that the labeler cannot identify the
source of a single relation. Third, annotators are
requested to label the relations in the pool as cor-
rect or incorrect, based on the definition of sub-
sumption relation. After all relations in the pool
are labeled, we use them as the standard judge-
ment to evaluate each method?s output.
Precision measures the proportion of correct re-
lations, while coverage measures the proportion of
tags that are connected by the relations. The cut-
off threshold n affects both precision and cover-
age: the larger the n, the lower the precision, and
the higher the coverage.
5.2.3 Baseline methods
Besides TAG-TAG, TAG-WORD and TAG-
REASON, we also include the method described
in (Heymann and Garcia-Molina, 2006) as a
baseline, denoted as HEYMANN. HEYMANN
method was designed to find similar-to relation
rather than subsumption relation. The similar-to
relation is symmetric, while subsumption relation
is more strict and asymmetric. In our experiments,
we use the same evaluation process to evalu-
ate TAG-TAG, TAG-WORD, TAG-REASON and
HEYMANN, in which only subsumption relations
will be marked as correct.
5.2.4 Results
For each method, we set the cut-off threshold
n from 1 to 500, so as to plot the psrecision-
coverage curves. The result is shown in Figure 4.
The larger the area under the curve, the better the
method?s performance.
We have three observations from Figure 4.
First, TAG-REASON has the best performance
1016
 0
 0.2
 0.4
 0.6
 0.8
 1
 0  0.05  0.1  0.15  0.2  0.25  0.3  0.35  0.4
Pr
ec
is
io
n
Coverage
TAG-REASON
TAG-TAG
TAG-WORD
HEYMANN
(a) BLOG
 0
 0.2
 0.4
 0.6
 0.8
 1
 0  0.05  0.1  0.15  0.2  0.25  0.3  0.35
Pr
ec
is
io
n
Coverage
TAG-REASON
TAG-TAG
TAG-WORD
HEYMANN
(b) BOOK
Figure 4: The precision and coverage of TAG-TAG, TAG-WORD, TAG-REASON and HEYMANN
methods. The larger the area under the curve, the better the result. The cut-off threshold n ? [1, 500].
BLOG BOOK
Insufficient Reversed Irrelevant Insufficient Reversed Irrelevant
childedu?s father stock?s security travel?sbuilding textbook?s exam English?s foreignlang japan?slightnovel
childedu?s grandma stock?s financial emotion?stime history?s military biography?speople building?stextbook
emotion?swarm delicious?staste emotion?soriginal piano?sscores jpbuilding?s jpculture sales?sO
childedu?schild delicious?sfood culture?sspring history?sculture novel?spureliterature japan?s shower
education?schild earthquake?sdisaster poem?snight novel?slove ancientgreek?sgreek photo?sumbrella
Total 52% Total 14% Total 34% Total 37% Total 48% Total 15%
Table 2: Examples of mistakes and the percentage of each mistake type.
on both data sets: On the BOOK data set, TAG-
REASON outperforms others by a marked mar-
gin; On the BLOG data set, TAG-REASON has
higher precision when coverage is smaller (which
means within top-ranked relations), and has com-
parable precision to TAG-TAG when coverage
increases. Second, similarity-based clustering
method (namely HEYMANN) performed worse
than others, suggesting it may not be adequate for
discovering subsumption relation. Third, while
also using content information, TAG-WORD per-
forms poorer than both TAG-REASON and TAG-
TAG, which suggests that noise in the content
would prevent TAG-WORD from getting the cor-
rect estimation of p(ta|tb).
To summarize, by leveraging relevant con-
tent, TAG-REASON could discover better sub-
sumption relations than just using tag-tag co-
occurrences and similarity-based hierarchical
clustering.
5.2.5 Mistakes in Discovered Relations
We also studied the type of mistakes in sub-
sumption relation discovery. To our observation, a
mistakenly discovered relation ta ?s tb falls into
one of the following categories:
1. insufficient ta relates with tb, but using tb
does not implies the using of ta in all cases.
2. reversed tb ?s ta is correct, while ta ?s tb
is not.
3. irrelevant There is no obvious connection
between ta and tb.
We collected all incorrect relations discovered
by the TAG-REASON method. Then, the type of
mistake for each relation is labeled manually. The
result is shown in Table 2, along with selected ex-
amples of each type.
Table 2 shows different error patterns for
BLOG and BOOK. In BLOG, most of the
mistakes are of the type insufficient. Taking
?education?s child? for example, annotating a
document as child does not imply that it is about
child education, it may about food or clothes for
a child. In BOOK, most of the mistakes are re-
versed mistakes, which is a result of the omitted-
tag problem discussed in Section 3.2.1.
1017
Figure 5: Part of the constructed Layered-DAG from the BOOK data set.
BLOG BOOK
Method Precision Coverage Precision Coverage
TAG-TAG ?4.7% +7.9% ?7.4% +12.5%
TAG-WORD 0% 0% ?9.0% +2.2%
TAG-REASON ?3.6% +5.4% ?0.9% +5.4%
Table 3: The effects on precision and coverage by
Layered-DAG construction
5.3 Layered-DAG Construction
Using the algorithm introduced in Section 4, we
constructed Layered-DAGs from the discovered
relations. Constructing Layered-DAG will re-
move certain relations, which will decrease the
precision and increase the coverage. Table 3
shows the changes of precision and coverage
brought by Layered-DAG construction. In most
of the cases, the increasing of coverage is more
than the decreasing of precision.
As a representative example, we show part of
a constructed Layered-DAG from the BOOK data
set in Figure 5, since the whole graph is too big to
fit in the paper. All tags in Chinese are translated
to English.
6 Conclusion and Future Work
In this paper, we explored the structure of social
tags by discovering subsumption relations. First,
we defined the subsumption relation ta ?s tb
as ta can be used to replace tb without ambigu-
ity. Then, we cast the subsumption relation iden-
tification problem to the estimation of p(ta|tb).
We proposed three methods, namely TAG-TAG,
TAG-WORD and TAG-REASON, while the last
two leverage the content of document to help esti-
mation. We also proposed an greedy algorithm for
constructing a Layered-DAG from the discovered
relations, which helps minimizing redundancy.
We performed experiments on two real world
data sets, and evaluated the discovered subsump-
tion relations quantitatively by pooling. The
results showed that the proposed methods out-
perform similarity-based hierarchical clusteing
in finding subsumption relations. The TAG-
REASON method, which uses only the relevant
content to the tags, has the best performance. Em-
pirical study showed that Layered-DAG construc-
tion works effectively as expected.
The results suggest two directions for future
work: First, more ways for p(ta|tb) estima-
tion could be explored, for example, combining
TAG-TAG and TAG-REASON; Second, external
knowledge, such as the Wikipedia and the Word-
Net, could be exploited as background knowledge
to improve the accuracy.
ACKNOWLEDGEMENTS
This work is supported by the National Science
Foundation of China under Grant No. 60873174
and the National 863 High-Tech Program of China
under Grant No. 2007AA01Z148. We also thank
Douban Inc.(www.douban.com) for providing the
DOUBAN data set, and Shoukun Wang, Guozhu
Wen et al of Douban Inc. for insightful discus-
sion.
1018
References
Angeletou, S., M. Sabou, L. Specia, and E. Motta.
2007. Bridging the gap between folksonomies and
the semantic web: An experience report. In Work-
shop: Bridging the Gap between Semantic Web and
Web, volume 2. Citeseer.
Begelman, Grigory, Keller, and F. Smadja. 2006. Au-
tomated tag clustering: Improving search and explo-
ration in the tag space. In Collaborative Web Tag-
ging Workshop, 15 th International World Wide Web
Conference.
Brooks, Christopher H. and Nancy Montanez. 2006.
Improved annotation of the blogosphere via auto-
tagging and hierarchical clustering. In WWW ?06:
Proceedings of the 15th international conference on
World Wide Web, pages 625?632, New York, NY,
USA. ACM.
Echarte, F., J. J. Astrain, A. Co?rdoba, and J. Villadan-
gos. 2007. Ontology of folksonomy: A New mod-
eling method. Proceedings of Semantic Authoring,
Annotation and Knowledge Markup (SAAKM).
Heymann, Paul and Hector Garcia-Molina. 2006. Col-
laborative creation of communal hierarchical tax-
onomies in social tagging systems. Technical Re-
port 2006-10, Stanford University, April.
Kim, Hak L., Simon Scerri, John G. Breslin, Stefan
Decker, and Hong G. Kim. 2008. The state of the
art in tag ontologies: a semantic model for tagging
and folksonomies. In DCMI ?08: Proceedings of
the 2008 International Conference on Dublin Core
and Metadata Applications, pages 128?137. Dublin
Core Metadata Initiative.
Kome, Sam H. 2005. Hierarchical subject relation-
ships in folksonomies. Master?s thesis, University
of North Carolina at Chapel Hill, November.
Mika, P. 2005. Ontologies are us: A unified model of
social networks and semantics. The Semantic Web?
ISWC 2005, pages 522?536.
Passant, Alexandre. 2007. Using ontologies to
strengthen folksonomies and enrich information re-
trieval in weblogs. In Proceedings of International
Conference on Weblogs and Social Media.
Sanderson, M. and B. Croft. 1999. Deriving concept
hierarchies from text. In Proceedings of the 22nd
annual international ACM SIGIR conference on Re-
search and development in information retrieval,
pages 206?213. ACM.
Schmitz, P. 2006. Inducing ontology from flickr
tags. In Collaborative Web Tagging Workshop at
WWW2006, Edinburgh, Scotland, pages 210?214.
Citeseer.
Schwarzkopf, E., D. Heckmann, and D. Dengler.
2007. In Workshop on Data Mining for User Mod-
eling, ICUM?07, page 63. Citeseer.
Shepitsen, Andriy, Jonathan Gemmell, Bamshad
Mobasher, and Robin Burke. 2008. Personalized
recommendation in collaborative tagging systems
using hierarchical clustering. In Proceedings of
ACM RecSys?08.
Si, Xiance and Maosong Sun. 2010. Tag allocation
model: Modeling noisy social annotations by reason
finding. In Proceedings of 2010 IEEE/WIC/ACM
International Conferences on Web Intelligence and
Intelligent Agent Technology.
Specia, Lucia and Enrico Motta. 2007. Integrating
folksonomies with the semantic web. pages 624?
639.
Van Damme, C., M. Hepp, and K. Siorpaes. 2007.
Folksontology: An integrated approach for turning
folksonomies into ontologies. Bridging the Gap be-
tween Semantic Web and Web, 2:57?70.
Voorhees, E.M. and D.K. Harman. 2005. TREC: Ex-
periment and evaluation in information retrieval.
MIT Press.
Wu, Harris, Mohammad Zubair, and Kurt Maly.
2006a. Harvesting social knowledge from folk-
sonomies. In HYPERTEXT ?06: Proceedings of the
seventeenth conference on Hypertext and hyperme-
dia, pages 111?114, New York, NY, USA. ACM.
Wu, Xian, Lei Zhang, and Yong Yu. 2006b. Exploring
social annotations for the semantic web. In WWW
?06: Proceedings of the 15th international con-
ference on World Wide Web, pages 417?426, New
York, NY, USA. ACM.
1019
Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 366?376,
MIT, Massachusetts, USA, 9-11 October 2010. c?2010 Association for Computational Linguistics
Automatic Keyphrase Extraction via Topic Decomposition
Zhiyuan Liu, Wenyi Huang, Yabin Zheng and Maosong Sun
Department of Computer Science and Technology
State Key Lab on Intelligent Technology and Systems
National Lab for Information Science and Technology
Tsinghua University, Beijing 100084, China
{lzy.thu, harrywy, yabin.zheng}@gmail.com,
sms@tsinghua.edu.cn
Abstract
Existing graph-based ranking methods for
keyphrase extraction compute a single impor-
tance score for each word via a single ran-
dom walk. Motivated by the fact that both
documents and words can be represented by
a mixture of semantic topics, we propose to
decompose traditional random walk into mul-
tiple random walks specific to various topics.
We thus build a Topical PageRank (TPR) on
word graph to measure word importance with
respect to different topics. After that, given
the topic distribution of the document, we fur-
ther calculate the ranking scores of words and
extract the top ranked ones as keyphrases. Ex-
perimental results show that TPR outperforms
state-of-the-art keyphrase extraction methods
on two datasets under various evaluation met-
rics.
1 Introduction
Keyphrases are defined as a set of terms in a doc-
ument that give a brief summary of its content for
readers. Automatic keyphrase extraction is widely
used in information retrieval and digital library (Tur-
ney, 2000; Nguyen and Kan, 2007). Keyphrase ex-
traction is also an essential step in various tasks of
natural language processing such as document cate-
gorization, clustering and summarization (Manning
and Schutze, 2000).
There are two principled approaches to extracting
keyphrases: supervised and unsupervised. The su-
pervised approach (Turney, 1999) regards keyphrase
extraction as a classification task, in which a model
is trained to determine whether a candidate phrase
is a keyphrase. Supervised methods require a doc-
ument set with human-assigned keyphrases as train-
ing set. In Web era, articles increase exponentially
and change dynamically, which demands keyphrase
extraction to be efficient and adaptable. However,
since human labeling is time consuming, it is im-
practical to label training set from time to time.
We thus focus on the unsupervised approach in this
study.
In the unsupervised approach, graph-based rank-
ing methods are state-of-the-art (Mihalcea and Ta-
rau, 2004). These methods first build a word graph
according to word co-occurrences within the docu-
ment, and then use random walk techniques (e.g.,
PageRank) to measure word importance. After that,
top ranked words are selected as keyphrases.
Existing graph-based methods maintain a single
importance score for each word. However, a docu-
ment (e.g., news article or research article) is usu-
ally composed of multiple semantic topics. Taking
this paper for example, it refers to two major top-
ics, ?keyphrase extraction? and ?random walk?. As
words are used to express various meanings corre-
sponding to different semantic topics, a word will
play different importance roles in different topics
of the document. For example, the words ?phrase?
and ?extraction? will be ranked to be more impor-
tant in topic ?keyphrase extraction?, while the words
?graph? and ?PageRank? will be more important in
topic ?random walk?. Since they do not take topics
into account, graph-based methods may suffer from
the following two problems:
1. Good keyphrases should be relevant to the ma-
jor topics of the given document. In graph-
based methods, the words that are strongly con-
nected with other words tend to be ranked high,
366
which do not necessarily guarantee they are rel-
evant to major topics of the document.
2. An appropriate set of keyphrases should also
have a good coverage of the document?s ma-
jor topics. In graph-based methods, the ex-
tracted keyphrases may fall into a single topic
of the document and fail to cover other substan-
tial topics of the document.
To address the problem, it is intuitive to consider
the topics of words and document in random walk
for keyphrase extraction. In this paper, we pro-
pose to decompose traditional PageRank into multi-
ple PageRanks specific to various topics and obtain
the importance scores of words under different top-
ics. After that, with the help of the document topics,
we can further extract keyphrases that are relevant
to the document and at the same time have a good
coverage of the document?s major topics. We call
the topic-decomposed PageRank as Topical PageR-
ank (TPR).
In experiments we find that TPR can extract
keyphrases with high relevance and good cover-
age, which outperforms other baseline methods un-
der various evaluation metrics on two datasets. We
also investigate the performance of TPR with dif-
ferent parameter values and demonstrate its robust-
ness. Moreover, TPR is unsupervised and language-
independent, which is applicable in Web era with
enormous information.
TPR for keyphrase extraction is a two-stage pro-
cess:
1. Build a topic interpreter to acquire the topics of
words and documents.
2. Perform TPR to extract keyphrases for docu-
ments.
We will introduce the two stages in Section 2 and
Section 3.
2 Building Topic Interpreters
To run TPR on a word graph, we have to acquire
topic distributions of words. There are roughly two
approaches that can provide topics of words: (1) Use
manually annotated knowledge bases, e.g., Word-
Net (Miller et al, 1990); (2) Use unsupervised ma-
chine learning techniques to obtain word topics from
a large-scale document collection. Since the vocab-
ulary in WordNet cannot cover many words in mod-
ern news and research articles, we employ the sec-
ond approach to build topic interpreters for TPR.
In machine learning, various methods have been
proposed to infer latent topics of words and docu-
ments. These methods, known as latent topic mod-
els, derive latent topics from a large-scale document
collection according to word occurrence informa-
tion. Latent Dirichlet Allocation (LDA) (Blei et al,
2003) is a representative of topic models. Com-
pared to Latent Semantic Analysis (LSA) (Landauer
et al, 1998) and probabilistic LSA (pLSA) (Hof-
mann, 1999), LDA has more feasibility for inference
and can reduce the risk of over-fitting.
In LDA, each word w of a document d is regarded
to be generated by first sampling a topic z from d?s
topic distribution ?(d), and then sampling a word
from the distribution over words ?(z) that charac-
terizes topic z. In LDA, ?(d) and ?(z) are drawn
from conjugate Dirichlet priors ? and ?, separately.
Therefore, ? and ? are integrated out and the prob-
ability of word w given document d and priors is
represented as follows:
pr(w|d, ?, ?) =
K
?
z=1
pr(w|z, ?)pr(z|d, ?), (1)
where K is the number of topics.
Using LDA, we can obtain the topic distribution
of each word w, namely pr(z|w) for topic z ? K.
The word topic distributions will be used in TPR.
Moreover, using the obtained word topic distribu-
tions, we can infer the topic distribution of a new
document (Blei et al, 2003), namely pr(z|d) for
each topic z ? K, which will be used for ranking
keyphrases.
3 Topical PageRank for Keyphrase
Extraction
After building a topic interpreter to acquire the
topics of words and documents, we can perform
keyphrase extraction for documents via TPR. Given
a document d, the process of keyphrase extraction
using TPR consists of the following four steps which
is also illustrated in Fig. 1:
1. Construct a word graph for d according to word
co-occurrences within d.
367
Figure 1: Topical PageRank for Keyphrase Extraction.
2. Perform TPR to calculate the importance
scores for each word with respect to different
topics.
3. Using the topic-specific importance scores of
words, rank candidate keyphrases respect to
each topic separately.
4. Given the topics of document d, integrate the
topic-specific rankings of candidate keyphrases
into a final ranking, and the top ranked ones are
selected as keyphrases.
3.1 Constructing Word Graph
We construct a word graph according to word co-
occurrences within the given document, which ex-
presses the cohesion relationship between words
in the context of document. The document is re-
garded as a word sequence, and the link weights be-
tween words is simply set to the co-occurrence count
within a sliding window with maximum W words in
the word sequence.
It was reported in (Mihalcea and Tarau, 2004)
the graph direction does not influence the perfor-
mance of keyphrase extraction very much. In this
paper we simply construct word graphs with direc-
tions. The link directions are determined as follows.
When sliding a W -width window, at each position,
we add links from the first word pointing to other
words within the window. Since keyphrases are usu-
ally noun phrases, we only add adjectives and nouns
in word graph.
3.2 Topical PageRank
Before introducing TPR, we first give some formal
notations. We denote G= (V,E) as the graph of a
document, with vertex set V = {w1, w2, ? ? ? , wN}
and link set (wi, wj) ? E if there is a link from
wi to wj . In a word graph, each vertex represents
a word, and each link indicates the relatedness be-
tween words. We denote the weight of link (wi, wj)
as e(wi, wj), and the out-degree of vertex wi as
O(wi)=
?
j:wi?wj e(wi, wj).
Topical PageRank is based on PageRank (Page et
al., 1998). PageRank is a well known ranking al-
gorithm that uses link information to assign global
importance scores to web pages. The basic idea of
PageRank is that a vertex is important if there are
other important vertices pointing to it. This can be
regarded as voting or recommendation among ver-
tices. In PageRank, the score R(wi) of word wi is
defined as
R(wi) = ?
?
j:wj?wi
e(wj , wi)
O(wj)
R(wj) + (1? ?)
1
|V | ,
(2)
where ? is a damping factor range from 0 to 1, and
|V | is the number of vertices. The damping fac-
tor indicates that each vertex has a probability of
(1 ? ?) to perform random jump to another vertex
within this graph. PageRank scores are obtained by
running Eq. (2) iteratively until convergence. The
second term in Eq. (2) can be regarded as a smooth-
ing factor to make the graph fulfill the property of
being aperiodic and irreducible, so as to guarantee
that PageRank converges to a unique stationary dis-
368
tribution. In PageRank, the second term is set to be
the same value 1|V | for all vertices within the graph,
which indicates there are equal probabilities of ran-
dom jump to all vertices.
In fact, the second term of PageRank in Eq. (2)
can be set to be non-uniformed. Suppose we as-
sign larger probabilities to some vertices, the final
PageRank scores will prefer these vertices. We call
this Biased PageRank.
The idea of Topical PageRank (TPR) is to run
Biased PageRank for each topic separately. Each
topic-specific PageRank prefers those words with
high relevance to the corresponding topic. And
the preferences are represented using random jump
probabilities of words.
Formally, in the PageRank of a specific topic
z, we will assign a topic-specific preference value
pz(w) to each word w as its random jump proba-
bility with
?
w?V pz(w) = 1. The words that are
more relevant to topic z will be assigned larger prob-
abilities when performing the PageRank. For topic
z, the topic-specific PageRank scores are defined as
follows:
Rz(wi) = ?
?
j:wj?wi
e(wj , wi)
O(wj)
Rz(wj)+(1??)pz(wi).
(3)
In Fig. 1, we show an example with two topics. In
this figure, we use the size of circles to indicate how
relevant the word is to the topic. In the PageRanks
of the two topics, high preference values will be as-
signed to different words with respect to the topic.
Finally, the words will get different PageRank val-
ues in the two PageRanks.
The setting of preference values pz(w) will have
a great influence to TPR. In this paper we use three
measures to set preference values for TPR:
? pz(w) = pr(w|z), is the probability that word
w occurs given topic z. This indicates how
much that topic z focuses on word w.
? pz(w) = pr(z|w), is the probability of topic z
given word w. This indicates how much that
word w focuses on topic z.
? pz(w) = pr(w|z) ? pr(z|w), is the product of
hub and authority values. This measure is in-
spired by the work in (Cohn and Chang, 2000).
Both PageRank and TPR are all iterative algo-
rithms. We terminate the algorithms when the num-
ber of iterations reaches 100 or the difference of each
vertex between two neighbor iterations is less than
0.001.
3.3 Extract Keyphrases Using Ranking Scores
After obtaining word ranking scores using TPR, we
begin to rank candidate keyphrases. As reported in
(Hulth, 2003), most manually assigned keyphrases
turn out to be noun phrases. We thus select noun
phrases from a document as candidate keyphrases
for ranking.
The candidate keyphrases of a document is ob-
tained as follows. The document is first tokenized.
After that, we annotate the document with part-
of-speech (POS) tags 1. Third, we extract noun
phrases with pattern (adjective)*(noun)+,
which represents zero or more adjectives followed
by one or more nouns. We regard these noun phrases
as candidate keyphrases.
After identifying candidate keyphrases, we rank
them using the ranking scores obtained by TPR.
In PageRank for keyphrase extraction, the ranking
score of a candidate keyphrase p is computed by
summing up the ranking scores of all words within
the phrase: R(p)=
?
wi?p R(wi) (Mihalcea and Ta-
rau, 2004; Wan and Xiao, 2008a; Wan and Xiao,
2008b). Then candidate keyphrases are ranked in
descending order of ranking scores. The top M can-
didates are selected as keyphrases.
In TPR for keyphrase extraction, we first com-
pute the ranking scores of candidate keyphrases sep-
arately for each topic. That is for each topic z we
compute
Rz(p) =
?
wi?p
Rz(wi). (4)
By considering the topic distribution of document,
We further integrate topic-specific rankings of can-
didate keyphrases into a final ranking and extract
top-ranked ones as the keyphrases of the document.
Denote the topic distribution of the document d
as pr(z|d) for each topic z. For each candidate
keyphrase p, we compute its final ranking score as
1In experiments we use Stanford POS Tagger from http:
//nlp.stanford.edu/software/tagger.shtml
with English tagging model left3words-distsim-wsj.
369
follows:
R(p) =
K
?
z=1
Rz(p)? pr(z|d). (5)
After ranking candidate phrases in descending order
of their integrated ranking scores, we select the top
M as the keyphrases of document d.
4 Experiments
4.1 Datasets
To evaluate the performance of TPR for keyphrase
extraction, we carry out experiments on two
datasets.
One dataset was built by Wan and Xiao 2 which
was used in (Wan and Xiao, 2008b). This dataset
contains 308 news articles in DUC2001 (Over et al,
2001) with 2, 488 manually annotated keyphrases.
There are at most 10 keyphrases for each document.
In experiments we refer to this dataset as NEWS.
The other dataset was built by Hulth 3 which was
used in (Hulth, 2003). This dataset contains 2, 000
abstracts of research articles and 19, 254 manually
annotated keyphrases. In experiments we refer to
this dataset as RESEARCH.
Since neither NEWS nor RESEARCH itself is
large enough to learn efficient topics, we use the
Wikipedia snapshot at March 2008 4 to build topic
interpreters with LDA. After removing non-article
pages and the articles shorter than 100 words, we
collected 2, 122, 618 articles. After tokenization,
stop word removal and word stemming, we build the
vocabulary by selecting 20, 000 words according to
their document frequency. We learn LDA models by
taking each Wikipedia article as a document. In ex-
periments we learned several models with different
numbers of topics, from 50 to 1, 500 respectively.
For the words absent in topic models, we simply set
the topic distribution of the word as uniform distri-
bution.
4.2 Evaluation Metrics
For evaluation, the words in both standard and ex-
tracted keyphrases are reduced to base forms using
2http://wanxiaojun1979.googlepages.com.
3It was obtained from the author.
4http://en.wikipedia.org/wiki/Wikipedia_
database.
Porter Stemmer 5 for comparison. In experiments
we select three evaluation metrics.
The first metric is precision/recall/F-measure rep-
resented as follows,
p = ccorrectcextract
, r = ccorrectcstandard
, f = 2prp+ r , (6)
where ccorrect is the total number of correct
keyphrases extracted by a method, cextract the to-
tal number of automatic extracted keyphrases, and
cstandard the total number of human-labeled stan-
dard keyphrases.
We note that the ranking order of extracted
keyphrases also indicates the method performance.
An extraction method will be better than another one
if it can rank correct keyphrases higher. However,
precision/recall/F-measure does not take the order
of extracted keyphrases into account. To address the
problem, we select the following two additional met-
rics.
One metric is binary preference measure
(Bpref) (Buckley and Voorhees, 2004). Bpref is
desirable to evaluate the performance considering
the order in which the extracted keyphrases are
ranked. For a document, if there are R correct
keyphrases within M extracted keyphrases by a
method, in which r is a correct keyphrase and n is
an incorrect keyphrase, Bpref is defined as follows,
Bpref = 1R
?
r?R
1? |n ranked higher than r|M . (7)
The other metric is mean reciprocal rank
(MRR) (Voorhees, 2000) which is used to evaluate
how the first correct keyphrase for each document is
ranked. For a document d, rankd is denoted as the
rank of the first correct keyphrase with all extracted
keyphrases, MRR is defined as follows,
MRR =
1
|D|
?
d?D
1
rankd
, (8)
where D is the document set for keyphrase extrac-
tion.
Note that although the evaluation scores of most
keyphrase extractors are still lower compared to
5http://tartarus.org/
?
martin/
PorterStemmer.
370
other NLP-tasks, it does not indicate the perfor-
mance is poor because even different annotators may
assign different keyphrases to the same document.
4.3 Influences of Parameters to TPR
There are four parameters in TPR that may influence
the performance of keyphrase extraction including:
(1) window size W for constructing word graph, (2)
the number of topics K learned by LDA, (3) dif-
ferent settings of preference values pz(w), and (4)
damping factor ? of TPR.
In this section, we look into the influences of these
parameters to TPR for keyphrase extraction. Except
the parameter under investigation, we set parameters
to the following values: W =10, K=1, 000, ?=0.3
and pz(w) = pr(z|w), which are the settings when
TPR achieves the best (or near best) performance on
both NEWS and RESEARCH. In the following tables,
we use ?Pre.?, ?Rec.? and ?F.? as the abbreviations
of precision, recall and F-measure.
4.3.1 Window Size W
In experiments on NEWS, we find that the perfor-
mance of TPR is stable when W ranges from 5 to 20
as shown in Table 1. This observation is consistent
with the findings reported in (Wan and Xiao, 2008b).
Size Pre. Rec. F. Bpref MRR
5 0.280 0.345 0.309 0.213 0.636
10 0.282 0.348 0.312 0.214 0.638
15 0.282 0.347 0.311 0.214 0.646
20 0.284 0.350 0.313 0.215 0.644
Table 1: Influence of window size W when the num-
ber of keyphrases M=10 on NEWS.
Similarly, when W ranges from 2 to 10, the per-
formance on RESEARCH does not change much.
However, the performance on NEWS will become
poor when W = 20. This is because the abstracts
in RESEARCH (there are 121 words per abstract on
average) are much shorter than the news articles
in NEWS (there are 704 words per article on av-
erage). If the window size W is set too large on
RESEARCH, the graph will become full-connected
and the weights of links will tend to be equal, which
cannot capture the local structure information of ab-
stracts for keyphrase extraction.
4.3.2 The Number of Topics K
We demonstrate the influence of the number of
topics K of LDA models in Table 2. Table 2 shows
the results when K ranges from 50 to 1, 500 and
M =10 on NEWS. We observe that the performance
does not change much as the number of topics
varies until the number is much smaller (K = 50).
The influence is similar on RESEARCH which indi-
cates that LDA is appropriate for obtaining topics of
words and documents for TPR to extract keyphrases.
K Pre. Rec. F. Bpref MRR
50 0.268 0.330 0.296 0.204 0.632
100 0.276 0.340 0.304 0.208 0.632
500 0.284 0.350 0.313 0.215 0.648
1000 0.282 0.348 0.312 0.214 0.638
1500 0.282 0.348 0.311 0.214 0.631
Table 2: Influence of the number of topics K when
the number of keyphrases M=10 on NEWS.
4.3.3 Damping Factor ?
Damping factor ? of TPR reconciles the influ-
ences of graph walks (the first term in Eq.(3)) and
preference values (the second term in Eq.(3)) to the
topic-specific PageRank scores. We demonstrate
the influence of ? on NEWS in Fig. 2. This fig-
ure shows the precision/recall/F-measure when ? =
0.1, 0.3, 0.5, 0.7, 0.9 and M ranges from 1 to 20.
From this figure we find that, when ? is set from 0.2
to 0.7, the performance is consistently good. The
values of Bpref and MRR also keep stable with the
variations of ?.
4.3.4 Preference Values
Finally, we explore the influences of different set-
tings of preference values for TPR in Eq.(3). In Ta-
ble 3 we show the influence when the number of
keyphrases M = 10 on NEWS. From the table, we
observe that pr(z|w) performs the best. The similar
observation is also got on RESEARCH.
In keyphrase extraction task, it is required to find
the keyphrases that can appropriately represent the
topics of the document. It thus does not want to ex-
tract those phrases that may appear in multiple top-
ics like common words. The measure pr(w|z) as-
signs preference values according to how frequently
that words appear in the given topic. Therefore, the
371
1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20
0.2
0.25
0.3
0.35
0.4
0.45
0.5
Keyphrase Number
Pr
ec
is
io
n
 
 
?=0.1
?=0.3
?=0.5
?=0.7
?=0.9
(a) Precision
1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 200.05
0.1
0.15
0.2
0.25
0.3
0.35
0.4
0.45
0.5
Keyphrase Number
R
ec
al
l
 
 
?=0.1
?=0.3
?=0.5
?=0.7
?=0.9
(b) Recall
1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 200.1
0.12
0.14
0.16
0.18
0.2
0.22
0.24
0.26
0.28
0.3
Keyphrase Number
F?
m
ea
su
re
 
 
?=0.1
?=0.3
?=0.5
?=0.7
?=0.9
(c) F-measure
Figure 2: Precision, recall and F-measure of TPR with ?=0.1, 0.3, 0.5, 0.7 and 0.9 when M ranges from 1
to 20 on NEWS.
common words will always be assigned to a rela-
tively large value in each topic-specific PageRank
and finally obtain a high rank. pr(w|z) is thus not a
good setting of preference values in TPR. In the con-
trast, pr(z|w) prefers those words that are focused
on the given topic. Using pr(z|w) to set preference
values for TPR, we will tend to extract topic-focused
phrases as keyphrases.
Pref Pre. Rec. F. Bpref MRR
pr(w|z) 0.256 0.316 0.283 0.192 0.584
pr(z|w) 0.282 0.348 0.312 0.214 0.638
prod 0.259 0.320 0.286 0.193 0.587
Table 3: Influence of three preference value settings
when the number of keyphrases M=10 on NEWS.
4.4 Comparing with Baseline Methods
After we explore the influences of parameters to
TPR, we obtain the best results on both NEWS and
RESEARCH. We further select three baseline meth-
ods, i.e., TFIDF, PageRank and LDA, to compare
with TPR.
The TFIDF computes the ranking scores of words
based on words? tfidf values in the document,
namely R(w) = tfw ? log(idfw). While in PageR-
ank (i.e., TextRank), the ranking scores of words are
obtained using Eq.(2). The two baselines do not use
topic information of either words or documents. The
LDA computes the ranking score for each word us-
ing the topical similarity between the word and the
document. Given the topics of the document d and
a word w, We have used various methods to com-
pute similarity including cosine similarity, predic-
tive likelihood and KL-divergence (Heinrich, 2005),
among which cosine similarity performs the best on
both datasets. Therefore, we only show the results of
the LDA baseline calculated using cosine similarity.
In Tables 4 and 5 we show the compar-
ing results of the four methods on both NEWS
and RESEARCH. Since the average number of
manual-labeled keyphrases on NEWS is larger than
RESEARCH, we set M = 10 for NEWS and M =
5 for RESEARCH. The parameter settings on both
NEWS and RESEARCH have been stated in Section
4.3.
Method Pre. Rec. F. Bpref MRR
TFIDF 0.239 0.295 0.264 0.179 0.576
PageRank 0.242 0.299 0.267 0.184 0.564
LDA 0.259 0.320 0.286 0.194 0.518
TPR 0.282 0.348 0.312 0.214 0.638
Table 4: Comparing results on NEWS when the num-
ber of keyphrases M=10.
Method Pre. Rec. F. Bpref MRR
TFIDF 0.333 0.173 0.227 0.255 0.565
PageRank 0.330 0.171 0.225 0.263 0.575
LDA 0.332 0.172 0.227 0.254 0.548
TPR 0.354 0.183 0.242 0.274 0.583
Table 5: Comparing results on RESEARCHwhen the
number of keyphrases M=5.
From the two tables, we have the following obser-
vations.
372
0.2 0.25 0.3 0.35 0.4 0.45 0.50
0.05
0.1
0.15
0.2
0.25
0.3
0.35
0.4
0.45
0.5
Precision
R
ec
al
l
 
 
TFIDF
PageRank
LDA
TPR
Figure 3: Precision-recall results on NEWS when M
ranges from 1 to 20.
First, TPR outperform all baselines on both
datasets. The improvements are all statistically sig-
nificant tested with bootstrap re-sampling with 95%
confidence. This indicates the robustness and effec-
tiveness of TPR.
Second, LDA performs equal or better than
TFIDF and PageRank under precision/recall/F-
measure. However, the performance of LDA un-
der MRR is much worse than TFIDF and PageR-
ank, which indicates LDA fails to correctly extract
the first keyphrase earlier than other methods. The
reason is: (1) LDA does not consider the local struc-
ture information of document as PageRank, and (2)
LDA also does not consider the frequency infor-
mation of words within the document. In the con-
trast, TPR enjoys the advantages of both LDA and
TFIDF/PageRank, by using the external topic infor-
mation like LDA and internal document structure
like TFIDF/PageRank.
Moreover, in Figures 3 and 4 we show the
precision-recall relations of four methods on NEWS
and RESEARCH. Each point on the precision-recall
curve is evaluated on different numbers of extracted
keyphrases M . The closer the curve to the upper
right, the better the overall performance. The results
again illustrate the superiority of TPR.
4.5 Extracting Example
At the end, in Table 6 we show an example of
extracted keyphrases using TPR from a news arti-
cle with title ?Arafat Says U.S. Threatening to Kill
PLO Officials? (The article number in DUC2001
is AP880510-0178). Here we only show the top
10 keyphrases, and the correctly extracted ones
0.3 0.32 0.34 0.36 0.38 0.4 0.420
0.05
0.1
0.15
0.2
0.25
0.3
Precision
R
ec
al
l
 
 
TFIDF
PageRank
LDA
TPR
Figure 4: Precision-recall results on RESEARCH
when M ranges from 1 to 10.
are marked with ?(+)?. We also mark the num-
ber of correctly extracted keyphrases after method
name like ?(+7)? after TPR. We also illustrate the
top 3 topics of the document with their topic-
specific keyphrases. It is obvious that the top topics,
on ?Palestine?, ?Israel? and ?terrorism? separately,
have a good coverage on the discussion objects of
this article, which also demonstrate a good diversity
with each other. By integrating these topic-specific
keyphrases considering the proportions of these top-
ics, we obtain the best performance of keyphrase ex-
traction using TPR.
In Table 7 we also show the extracted keyphrases
of baselines from the same news article. For TFIDF,
it only considered the frequency properties of words,
and thus highly ranked the phrases with ?PLO?
which appeared about 16 times in this article, and
failed to extract the keyphrases on topic ?Israel?.
LDA only measured the importance of words using
document topics without considering the frequency
information of words and thus missed keyphrases
with high-frequency words. For example, LDA
failed to extract keyphrase ?political assassination?,
in which the word ?assassination? occurred 8 times
in this article.
5 Related Work
In this paper we proposed TPR for keyphrase ex-
traction. A pioneering achievement in keyphrase ex-
traction was carried out in (Turney, 1999) which re-
garded keyphrase extraction as a classification task.
Generally, the supervised methods need manually
annotated training set which is time-consuming and
in this paper we focus on unsupervised method.
373
TPR (+7)
PLO leader Yasser Arafat(+), Abu Jihad, Khalil
Wazir(+), slaying Wazir, political assassina-
tion(+), Palestinian guerrillas(+), particulary
Palestinian circles, Israeli officials(+), Israeli
squad(+), terrorist attacks(+)
TPR, Rank 1 Topic on ?Palestine?
PLO leader Yasser Arafat(+), United States(+),
State Department spokesman Charles Redman,
Abu Jihad, U.S. government document, Palestine
Liberation Organization leader, political assassi-
nation(+), Israeli officials(+), alleged document
TPR, Rank 2 Topic on ?Israel?
PLO leader Yasser Arafat(+), United States(+),
Palestine Liberation Organization leader, Israeli
officials(+), U.S. government document, alleged
document, Arab government, slaying Wazir, State
Department spokesman Charles Redman, Khalil
Wazir(+)
TPR, Rank 3 Topic on ?terrorism?
terrorist attacks(+), PLO leader Yasser Arafat(+),
Abu Jihad, United States(+), alleged docu-
ment, U.S. government document, Palestine Lib-
eration Organization leader, State Department
spokesman Charles Redman, political assassina-
tion(+), full cooperation
Table 6: Extracted keyphrases by TPR.
Starting with TextRank (Mihalcea and Tarau,
2004), graph-based ranking methods are becoming
the most widely used unsupervised approach for
keyphrase extraction. Litvak and Last (2008) ap-
plied HITS algorithm on the word graph of a docu-
ment for keyphrase extraction. Although HITS itself
worked the similar performance to PageRank, we
plan to explore the integration of topics and HITS in
future work. Wan (2008b; 2008a) used a small num-
ber of nearest neighbor documents to provide more
knowledge for keyphrase extraction. Some meth-
ods used clustering techniques on word graphs for
keyphrase extraction (Grineva et al, 2009; Liu et
al., 2009). The clustering-based method performed
well on short abstracts (with F-measure 0.382 on
RESEARCH) but poorly on long articles (NEWS with
F-measure score 0.216) due to two non-trivial is-
sues: (1) how to determine the number of clus-
TFIDF (+5)
PLO leader Yasser Arafat(+), PLO attacks, PLO
offices, PLO officials(+), PLO leaders, Abu Ji-
had, terrorist attacks(+), Khalil Wazir(+), slaying
wazir, political assassination(+)
PageRank (+3)
PLO leader Yasser Arafat(+), PLO officials(+),
PLO attacks, United States(+), PLO offices, PLO
leaders, State Department spokesman Charles
Redman, U.S. government document, alleged
document, Abu Jihad
LDA (+5)
PLO leader Yasser Arafat(+), Palestine Liberation
Organization leader, Khalil Wazir(+), Palestinian
guerrillas(+), Abu Jihad, Israeli officials(+), par-
ticulary Palestinian circles, Arab government,
State Department spokesman Charles Redman,
Israeli squad(+)
Table 7: Extracted keyphrases by baselines.
ters, and (2) how to weight each cluster and select
keyphrases from the clusters. In this paper we fo-
cus on improving graph-based methods via topic de-
composition, we thus only compare with PageRank
as well as TFIDF and LDA and do not compare with
clustering-based methods in details.
In recent years, two algorithms were proposed to
rank web pages by incorporating topic information
of web pages within PageRank (Haveliwala, 2002;
Nie et al, 2006). The method in (Haveliwala, 2002),
is similar to TPR which also decompose PageRank
into various topics. However, the method in (Haveli-
wala, 2002) only considered to set the preference
values using pr(w|z) (In the context of (Haveliwala,
2002), w indicates Web pages). In Section 4.3.4 we
have shown that the setting of using pr(z|w) is much
better than pr(w|z).
Nie et al (2006) proposed a more complicated
ranking method. In this method, topical PageRanks
are performed together. The basic idea of (Nie et al,
2006) is, when surfing following a graph link from
vertex wi to wj , the ranking score on topic z of wi
will have a higher probability to pass to the same
topic of wj and have a lower probability to pass to
a different topic of wj . When the inter-topic jump
probability is 0, this method is identical to (Haveli-
374
wala, 2002). We implemented the method and found
that the random jumps between topics did not help
improve the performance for keyphrase extraction,
and did not demonstrate the results of this method.
6 Conclusion and Future Work
In this paper we propose a new graph-based frame-
work, Topical PageRank, which incorporates topic
information within random walk for keyphrase ex-
traction. Experiments on two datasets show that
TPR achieves better performance than other base-
line methods. We also investigate the influence of
various parameters on TPR, which indicates the ef-
fectiveness and robustness of the new method.
We consider the following research directions as
future work.
1. In this paper we obtained latent topics us-
ing LDA learned from Wikipedia. We de-
sign to obtain topics using other machine learn-
ing methods and from other knowledge bases,
and investigate the influence to performance of
keyphrase extraction.
2. In this paper we integrated topic information
in PageRank. We plan to consider topic infor-
mation in other graph-based ranking algorithms
such as HITS (Kleinberg, 1999).
3. In this paper we used Wikipedia to train
LDA by assuming Wikipedia is an exten-
sive snapshot of human knowledge which can
cover most topics talked about in NEWS and
RESEARCH. In fact, the learned topics are
highly dependent on the learning corpus. We
will investigate the influence of corpus selec-
tion in training LDA for keyphrase extraction
using TPR.
Acknowledgments
This work is supported by the National Natu-
ral Science Foundation of China under Grant No.
60873174. The authors would like to thank Anette
Hulth and Xiaojun Wan for kindly sharing their
datasets. The authors would also thank Xiance Si,
Tom Chao Zhou, Peng Li for their insightful sug-
gestions and comments.
References
David M. Blei, Andrew Y. Ng, and Michael I. Jordan.
2003. Latent dirichlet alocation. Journal of Machine
Learning Research, 3:993?1022, January.
C. Buckley and E.M. Voorhees. 2004. Retrieval evalu-
ation with incomplete information. In Proceedings of
SIGIR, pages 25?32.
David Cohn and Huan Chang. 2000. Learning to prob-
abilistically identify authoritative documents. In Pro-
ceedings of ICML, pages 167?174.
M. Grineva, M. Grinev, and D. Lizorkin. 2009. Extract-
ing key terms from noisy and multi-theme documents.
In Proceedings of WWW, pages 661?670.
Taher H. Haveliwala. 2002. Topic-sensitive pagerank. In
Proceedings of WWW, pages 517?526.
G. Heinrich. 2005. Parameter estimation for text anal-
ysis. Web: http://www. arbylon. net/publications/text-
est.
Thomas Hofmann. 1999. Probabilistic latent semantic
indexing. In Proceedings of SIGIR, pages 50?57.
Anette Hulth. 2003. Improved automatic keyword ex-
traction given more linguistic knowledge. In Proceed-
ings of EMNLP, pages 216?223.
J.M. Kleinberg. 1999. Authoritative sources in a hyper-
linked environment. Journal of the ACM, 46(5):604?
632.
T.K. Landauer, P.W. Foltz, and D. Laham. 1998. An in-
troduction to latent semantic analysis. Discourse Pro-
cesses, 25:259?284.
Marina Litvak and Mark Last. 2008. Graph-based key-
word extraction for single-document summarization.
In Proceedings of the workshop Multi-source Mul-
tilingual Information Extraction and Summarization,
pages 17?24.
Zhiyuan Liu, Peng Li, Yabin Zheng, and Maosong Sun.
2009. Clustering to find exemplar terms for keyphrase
extraction. In Proceedings of EMNLP, pages 257?
266.
C.D. Manning and H. Schutze. 2000. Foundations of
statistical natural language processing. MIT Press.
Rada Mihalcea and Paul Tarau. 2004. Textrank: Bring-
ing order into texts. In Proceedings of EMNLP, pages
404?411.
George A. Miller, Richard Beckwith, Christiane Fell-
baum, Derek Gross, and Katherine Miller. 1990.
WordNet: An on-line lexical database. International
Journal of Lexicography, 3:235?244.
Thuy Nguyen and Min-Yen Kan. 2007. Keyphrase ex-
traction in scientific publications. In Proceedings of
the 10th International Conference on Asian Digital Li-
braries, pages 317?326.
375
Lan Nie, Brian D. Davison, and Xiaoguang Qi. 2006.
Topical link analysis for web search. In Proceedings
of SIGIR, pages 91?98.
P. Over, W. Liggett, H. Gilbert, A. Sakharov, and
M. Thatcher. 2001. Introduction to duc-2001: An in-
trinsic evaluation of generic news text summarization
systems. In Proceedings of DUC2001.
L. Page, S. Brin, R. Motwani, and T. Winograd. 1998.
The pagerank citation ranking: Bringing order to the
web. Technical report, Stanford Digital Library Tech-
nologies Project, 1998.
Peter D. Turney. 1999. Learning to extract keyphrases
from text. National Research Council Canada, In-
stitute for Information Technology, Technical Report
ERB-1057.
Peter D. Turney. 2000. Learning algorithms
for keyphrase extraction. Information Retrieval,
2(4):303?336.
E.M. Voorhees. 2000. The trec-8 question answering
track report. In Proceedings of TREC, pages 77?82.
Xiaojun Wan and Jianguo Xiao. 2008a. Collabrank:
Towards a collaborative approach to single-document
keyphrase extraction. In Proceedings of COLING,
pages 969?976.
Xiaojun Wan and Jianguo Xiao. 2008b. Single document
keyphrase extraction using neighborhood knowledge.
In Proceedings of AAAI, pages 855?860.
376
Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 1577?1588,
Edinburgh, Scotland, UK, July 27?31, 2011. c?2011 Association for Computational Linguistics
A Simple Word Trigger Method for Social Tag Suggestion
Zhiyuan Liu, Xinxiong Chen and Maosong Sun
Department of Computer Science and Technology
State Key Lab on Intelligent Technology and Systems
National Lab for Information Science and Technology
Tsinghua University, Beijing 100084, China
{lzy.thu, cxx.thu}@gmail.com, sms@tsinghua.edu.cn
Abstract
It is popular for users in Web 2.0 era to
freely annotate online resources with tags.
To ease the annotation process, it has been
great interest in automatic tag suggestion. We
propose a method to suggest tags according to
the text description of a resource. By consid-
ering both the description and tags of a given
resource as summaries to the resource written
in two languages, we adopt word alignment
models in statistical machine translation to
bridge their vocabulary gap. Based on the
translation probabilities between the words in
descriptions and the tags estimated on a large
set of description-tags pairs, we build a word
trigger method (WTM) to suggest tags accord-
ing to the words in a resource description.
Experiments on real world datasets show that
WTM is effective and robust compared with
other methods. Moreover, WTM is relatively
simple and efficient, which is practical for
Web applications.
1 Introduction
In Web 2.0, Web users often use tags to collect and
share online resources such as Web pages, photos,
videos, movies and books. Table 1 shows a book
entry annotated with multiple tags by users1. On
the top of Table 1 we list the title and a short
introduction of the novel ?The Count of Monte
Cristo?. The bottom half of Table 1 shows the
annotated tags, each of which is followed by a
number in bracket, the total number of users who
1The original record is obtained from the book review
website Douban (www.douban.com) in Chinese. Here we
translate it to English for comprehension.
use the tag to annotate this book. Since the tags of
a resource are annotated collaboratively by multiple
users, we also name these tags as social tags. For
a resource, we refer to the additional information,
such as the title and introduction of a book, as
description, and the user-annotated social tags as
annotation.
Description
Title: The Count of Monte Cristo
Intro: The Count of Monte Cristo is one of the most
popular fictions by Alexandre Dumas. The writing of
the work was completed in 1844. ...
Annotation
Dumas (2748), Count of Monte Cristo (2716), foreign
literature (1813), novel (1345), France (1096), classic
(1062), revenge (913), famous book (759), ...
Table 1: An example of social tagging. The number
in the bracket after each tag is the total count of users
that annotate the tag on this book.
Social tags concisely indicate the main content
of the given resource, and potentially reflect user
interests. Social tagging has thus been widely
studied and successfully applied in recommender
systems (Eck et al, 2007; Yanbe et al, 2007; Zhou
et al, 2010), trend detection and tracking (Hotho
et al, 2006), personalization (Wetzker et al, 2010),
advertising (Mirizzi et al, 2010), etc.
The task of automatic social tag suggestion is
to automatically recommend tags for a user when
he/she wants to annotate a resource. Social tag
suggestion, as a crucial component for social tag-
ging systems, can help users annotate resources.
Moreover, social tag suggestion is usually consid-
ered as an equivalent problem to modeling social
1577
tagging behaviors, which is playing a more and more
important role in social computing and information
retrieval (Wang et al, 2007).
Most online resources contain descriptions, which
usually contain much resource information. For
example, on a book review website, each book entry
contains a title, the author(s) and an introduction
of the book. Some researchers thus propose
to automatically suggest tags based on resource
descriptions, which are collectively known as the
content-based approach.
One may think to suggest tags by selecting
important words from descriptions. This is far from
enough because descriptions and annotations are
using diverse vocabularies, usually referred to as a
vocabulary gap problem. Take the book entry in
Table 1 for instance, the word ?popular? used in the
description contrasts the tags ?classic? and ?famous
book? in the annotation; the word ?novel? is used in
the description, while most users annotate with the
tag ?fiction?. The vocabulary gap usually reflects in
two main issues:
? Some tags in the annotation do appear in the
corresponding description, but they may not be
statistically significant.
? Some tags may even not appear in the descrip-
tion.
It is not trivial to reduce the vocabulary gap and
find the semantic correspondence between descrip-
tions and annotations. By regarding both the de-
scription and the annotation as parallel summaries
of a resource, we use word alignment models in
statistical machine translation (SMT) (Brown et
al., 1993) to estimate the translation probabilities
between the words in descriptions and annotations.
SMT has been successfully applied in many ap-
plications to bridge vocabulary gap. For detailed
descriptions of related work, readers can refer to
Section 2.2. In this paper, besides employing word
alignment models to social tagging, we also propose
a method to efficiently build description-annotation
pairs for sufficient learning translation probabilities
by word alignment models.
Based on the learned translation probabilities
between words in descriptions and annotations,
we regard the tagging behavior as a word trigger
process:
1. A user reads the resource description to realize
its substance by seeing some important words
in the description.
2. Triggered by these important words, the user
translates them into the corresponding tags, and
annotates the resource with these tags.
Based on this perspective, we build a simple word
trigger method (WTM) for social tag suggestion. In
Fig. 1, we use a simple example to show the basic
idea of using word trigger for social tag suggestion.
In this figure, some words in the first sentence of the
book description in Table 1 are triggered to the tags
in annotation.
Figure 1: An example of the word trigger method
for suggesting tags given a description.
2 Related Work
2.1 Social Tag Suggestion
Previous work has been proposed to automatic
social tag suggestion.
Many researchers built tag suggestion systems
based on collaborative filtering (CF) (Herlocker et
al., 1999; Herlocker et al, 2004), a widely used
technique in recommender systems (Resnick and
Varian, 1997). These collaboration-based methods
typically base their suggestions on the tagging
history of the given resource and user, without con-
sidering resource descriptions. FolkRank (Jaschke
et al, 2008) and Matrix Factorization (Rendle et al,
2009) are representative CF methods for social tag
suggestion. Most of these methods suffer from the
cold-start problem, i.e., they are not able to perform
effective suggestions for resources that no one has
annotated yet.
The content-based approach for social tag sug-
gestion remedies the cold-start problem of the
1578
collaboration-based approach by suggesting tags
according to resource descriptions. Therefore, the
content-based approach plays an important role in
social tag suggestion.
Some researchers regarded social tag suggestion
as a classification problem by considering each tag
as a category label (Ohkura et al, 2006; Mishne,
2006; Lee and Chun, 2007; Katakis et al, 2008;
Fujimura et al, 2008; Heymann et al, 2008).
Various classifiers such as Naive Bayes, kNN, SVM
and neural networks have been explored to solve the
social tag suggestion problem.
There are two issues emerging from the
classification-based methods:
? The annotations provided by users are noisy,
and the classification-based methods can not
handle the issue well.
? The training cost and classification cost of
many classification-based methods are usually
in proportion to the number of classification
labels. These methods may thus be inefficient
for a real-world social tagging system, where
hundreds of thousands of unique tags should be
considered as classification labels.
Inspired by the popularity of latent topic models
such as Latent Dirichlet Allocation (LDA) (Blei et
al., 2003), various methods have been proposed to
model tags using generative latent topic models.
One intuitive approach is assuming that both tags
and words are generated from the same set of latent
topics. By representing both tags and descriptions
as the distributions of latent topics, this approach
suggests tags according to their likelihood given
the description (Krestel et al, 2009; Si and Sun,
2009). Bundschus et al (2009) proposed a joint
latent topic model of users, words and tags. Iwata
et al (2009) proposed an LDA-based topic model,
Content Relevance Model (CRM), which aimed at
finding the content-related tags for suggestion. Em-
pirical experiments showed that CRM outperformed
both classification methods and Corr-LDA (Blei and
Jordan, 2003), a generative topic model for contents
and annotations.
Most latent topic models have to pre-specify the
number of topics before training. We can either use
cross validation to determine the optimal number
of topics or employ the infinite topic models, such
as Hierarchical Dirichlet Process (HDP) (Teh et al,
2006) and nested Chinese Restaurant Process (Blei
et al, 2010), to automatically adjust the number
of topics during training. Both solutions are
usually computationally complicated. What is more
important, topic-based methods suggest tags by
measuring the topical relevance of tags and resource
descriptions. The latent topics are of concept-level
which are usually too general to precisely suggest
those specific tags such as named entities, e.g.,
the tags ?Dumas? and ?Count of Monte Cristo? in
Table 1. To remedy the problem, Si et al (2010)
proposed a generative model, Tag Allocation Model
(TAM), which considers the words in descriptions
as the possible topics to generate tags. However,
TAM assumes each tag can only have at most one
word as its reason. This is against the fact that a tag
may be annotated triggered by multiple words in the
description.
It should also be noted that social tag suggestion is
different from automatic keyphrase extraction (Tur-
ney, 2000; Frank et al, 1999; Liu et al, 2009a; Liu
et al, 2010b; Liu et al, 2011). Keyphrase extraction
aims at selecting terms from the given document
to represent the main topics of the document. On
the contrary, in social tag suggestion, the suggested
tags do not necessarily appear in the given resource
description. We can thus regard social tag sugges-
tion as a task of selecting appropriate tags from
a controlled tag vocabulary for the given resource
description.
2.2 Applications of SMT
SMT techniques have been successfully used in
many tasks of information retrieval and natural
language processing to bridge the vocabulary gap
between two types of objects. Some typical tasks are
document information retrieval (Berger and Laffer-
ty, 1999; Murdock and Croft, 2004; Karimzadehgan
and Zhai, 2010), question answering (Berger et al,
2000; Echihabi and Marcu, 2003; Soricut and Brill,
2006; Riezler et al, 2007; Surdeanu et al, 2008;
Xue et al, 2008), query expansions (Riezler et al,
2007; Riezler et al, 2008; Riezler and Liu, 2010),
paraphrasing (Quirk et al, 2004; Zhao et al, 2010a;
Zhao et al, 2010b), summarization (Banko et al,
2000), collocation extraction (Liu et al, 2009b;
1579
Liu et al, 2010c), keyphrase extraction (Liu et
al., 2011), sentiment analysis (Dalvi et al, 2009),
computational advertising (Ravi et al, 2010), and
image/video annotation and retrieval (Duygulu et
al., 2002; Jeon et al, 2003).
3 Word Trigger Method for Social Tag
Suggestion
3.1 Method Framework
We describe the word trigger method (WTM) for
social tag suggestion as a 3-stage process:
1. Preparing description-annotation pairs.
Given a collection of annotated resources, we first
prepare description-annotation pairs for learning
translation probabilities using word alignment mod-
els.
2. Learning a translation model. Given a
collection of description-annotation pairs, we adopt
IBM Model-1, a widely used word alignment model,
to learn the translation probabilities between words
in descriptions and tags in annotations.
3. Suggesting tags given a resource description.
After building translation probabilities between
words and tags, given a resource description, we
first compute the trigger power of each word in the
description and then suggest tags according to their
translation probabilities from the triggered words.
Before introducing the method in details, we
introduce the notations. In a social tagging system,
a resource is denoted as r ? R, where R is the set of
all resources. Each resource contains a description
and an annotation containing a set of tags. The
description dr of resource r can be regarded as a bag
of words wr = {(wi, ei)}Nri=1, where ei is the count
of word wi and Nr is the number of unique words
in r. The annotation ar of resource r is represented
as tr = {(ti, ei)}Mri=1, where ei is the count of tag ti
and Mr is the number of unique tags for r.
3.2 Preparing Description-Annotation Pairs
Learning translation probabilities requires a parallel
training dataset consisting of a number of aligned
sentence pairs. We assume the description and the
annotation of a resource as being written in two
distinct languages. We thus prepare our parallel
training dataset by pairing descriptions with anno-
tations.
The annotation of a resource is a bag of tags with
no position information. We thus select IBM Model-
1 (Brown et al, 1993) for training, which does not
take word position information into account on both
sides for each aligned pair.
In a social tagging system, the length of a
resource description is usually limited to hundreds
of words. Meanwhile, it is common that some
popular resources are annotated by multiple users
with thousands of tags. For example, the tag
Dumas is annotated by 2, 748 users for the book
in Table 1. We have to deal with the length-
unbalance between a resource description and its
corresponding annotation for two reasons.
? It is impossible to list all annotated tags on
the annotation side of a description-annotation
pair. The performance of word alignment
models will also suffer from the unbalanced
length of sentence pairs in the parallel training
data set (Och and Ney, 2003).
? Moreover, the annotated tags may have differ-
ent importance for the resource. It would be
unfair to treat these tags without distinction.
Here we propose a sampling method to pre-
pare length-balanced description-annotation pairs
for word alignment. The basic idea is to sample
a bag of tags from the annotation according to tag
weights and make the generated bag of tags with
comparable length with the description.
We consider two parameters when sampling tags.
First, we have to select a tag weighting type for
sampling. In this paper, we investigate two straight-
forward sampling types, including tag frequen-
cy (TFt) within the annotation and tag-frequency
inverse-resource-frequency (TF-IRFt). Given re-
source r, TFt and TF-IRFt of tag t are defined
as TFt = et/
?
t et and TF-IRFt = et/
?
t et ?
log
(
|R|/|?r?R Iet>0|
)
, where |?r?R Iet>0| in-
dicates the number of resources that have been
annotated with tag t.
Another parameter is the length ratio between the
description and the sampled annotation. We denote
the ratio as ? = |wr|/|tr|, where |wr| is the number
of words in the description and |tr| is the number of
tags in the annotation.
1580
3.3 Learning Translation Probabilities Using
Word Alignment Models
Suppose the source language is resource description
and the target language is resource annotation.
In IBM Model-1, the relationship of the source
language w = wJ1 and the target language t = tI1
is connected via a hidden variable describing an
alignment mapping from source position j to target
position aj :
Pr(wJ1 |tI1) =
?
aJ1
Pr(wJ1 , aJ1 |tI1). (1)
The alignment aJ1 also contains empty-word align-
ments aj = 0 which align source words to the
an empty word. IBM Model-1 can be trained
using Expectation-Maximization (EM) algorithm in
an unsupervised fashion, and obtains the translation
probabilities of two vocabularies, i.e., Pr(w|t),
where t is a tag and w is a word.
IBM Model-1 only produces one-to-many align-
ments from source language to target language.
The learned model is thus asymmetric. We will
learn translation models on two directions: one is
regarding descriptions as the source language and
annotations as the target language, and the other is
in reverse direction of the pairs. We denote the first
model as Prd2a and the latter as Pra2d. We further
define Pr(t|w) as the harmonic mean of the two
models:
Pr(t|w) ?
(
?/Pr d2a(t|w)+(1??)/Pr a2d(t|w)
)?1
,
(2)
where ? is the harmonic factor to combine the two
models. When ? = 1 or ? = 0, it simply uses model
Prd2a or Pra2d correspondingly.
3.4 Tag Suggestion Using Triggered Words and
Translation Probabilities
When given the description of a resource, we can
rank tags by computing the scores:
Pr(t|d = wd) =
?
w?wd
Pr(t|w) Pr(w|d), (3)
in which Pr(w|d) is the trigger power of the word w
in the description, which indicates the importance of
the word. According to the ranking scores, we can
suggest the top-ranked tags to users.
Here we explore three methods to compute the
trigger power of a word in a resource description:
TF-IRFw, TextRank and their product. TF-IRFw and
TextRank are two most widely adopted methods for
keyword extraction.
Similar to TF-IRFt mentioned in Section 3.2, TF-
IRFw considers both the local importance (TFw) and
global specification (IRFw).
TextRank (Mihalcea and Tarau, 2004) is a graph-
based method to compute term importance. Given
a resource description, TextRank first builds a term
graph by connecting the terms in the description
according to their semantic relations, and then run
PageRank algorithm (Page et al, 1998) to measure
the importance of each term in the graph. Readers
can refer to (Mihalcea and Tarau, 2004) for detailed
information.
We also use the product of TF-IRFw and Tex-
tRank to weight terms, which potentially takes both
global information and term relations into account.
Emphasize Tags Appearing In Description for
WTM (EWTM) In some social tagging systems,
the tags that appear in the resource description are
more likely to be selected by users for annotation.
Therefore, we propose to emphasize the tags in the
description by ranking tags as follows
Pr(t|d) =
?
w?wd
(
?It(w)+(1??) Pr(t|w)
)
Pr(w|d),
(4)
where It(w) is an indicator function which gets
value 1 when t = w and 0 when t 6= w; and ? is
the smooth factor with range ? ? [0.0, 1.0]. When
? = 1.0, it suggests tags simply according to their
trigger powers within the description, while when
? = 0.0, it does not emphasize the tags appearing in
the description and just suggests according to their
translation probabilities. In Section 4.4, we will
show the performance of EWTM.
4 Experiments
4.1 Datasets and Evaluation Metrics
Datasets In our experiments, we select two real
world datasets which are of diverse properties to
evaluate our methods. In Table 2 we show the
detailed statistical information of the two datasets.
1581
Data R W T N?w N?t
BOOK 70, 000 174, 748 46, 150 211.6 3.5
BIBTEX 158, 924 91, 277 50, 847 5.8 2.7
Table 2: Statistical information of two datasets. R,
W , T , N?w and N?t are the number of resources, the
vocabulary of descriptions, the vocabulary of tags,
the average number of words in each description
and the average number of tags in each resource,
respectively.
The first dataset, denoted as BOOK, is obtained
from a popular Chinese book review website www.
douban.com, which contains the descriptions of
books and the tags collaboratively annotated by
users. The second dataset, denoted as BIBTEX, is
obtained from an English online bibliography web-
site www.bibsonomy.org2. The dataset contains
the descriptions for academic papers (including the
title and note for each paper) and the tags annotated
by users. As shown in Table 2, the average length of
descriptions in the BIBTEX dataset is much shorter
than the BOOK dataset. Moreover, the BIBTEX
dataset does not provide how many times each tag
is annotated to a resource.
Evaluation Metrics We use precision, recall and
F-measure to evaluate the performance of tag sug-
gestion methods. For a resource, we denote the
original tags (gold standard) as Ta, the suggested
tags as Ts, and the correctly suggested tags as Ts ?
Ta. Precision, recall and F-measure are defined as
p = |Ts ? Ta||Ts|
, r = |Ts ? Ta||Ta|
, F = 2pr(p + r) .
(5)
The final evaluation scores are computed by micro-
averaging (i.e., averaging on resources of test set).
We perform 5-fold cross validation for each method
on all two datasets. In experiments, the number of
suggested tags M ranges from 1 to 10.
4.2 Comparing Results
Baseline Methods We select four content-based
algorithms as the baselines for comparison: Naive
Bayes (NB) (Manning et al, 2008), k nearest
neighbor algorithm (kNN) (Manning et al, 2008),
2The dataset can be obtained from http://www.kde.
cs.uni-kassel.de/bibsonomy/dumps
Content Relevance (CRM) model (Iwata et al,
2009) and Tag Allocation Model (TAM) (Si et al,
2010).
NB and kNN are two representative classification
methods. NB is a simple generative model, which
models the probability of each tag t given descrip-
tion d as
Pr(t|d) ? Pr(t)
?
w?d
Pr(w|t). (6)
Pr(t) is estimated by the frequency of the resources
annotated with the tag t. Pr(w|t) is estimated by the
frequency of the word w in the resource descriptions
annotated with the tag t. kNN is a widely used
classification method for tag suggestion, which
recommends tags to a resource according to the
annotated tags of similar resources measured using
vector space models (Manning et al, 2008).
CRM and TAM are selected to represent topic-
based methods for tag suggestion. CRM is an LDA-
based generative model. The number of latent topics
K is the key parameter for CRM. In experiments, we
evaluated the performance of CRM with different K
values, and here we only show the best one obtained
by setting K = 1, 024. TAM is also a generative
model which considers the words in descriptions as
the topics to further generate tags for the resource.
We set parameters for TAM as in (Si et al, 2010).
For comparison, we denote our method as WTM.
Complexity Analysis We compare the complexity
of these methods. We denote the number of training
iterations in CRM, TAM and WTM as I 3, and
the number of topics in CRM as K. For the
training phase, the complexity of NB is O(RN?wN?t),
kNN is O(1), TAM is O(IRN?wN?t), CRM is
O(IKRN?wN?t), and WTM is O(IRN?wN?t)4. When
suggesting for a given resource description with
length Nw, the complexity of NB is O(NwT ),
kNN is O(RN?wN?t), CRM is O(IKNwT ), TAM
3In fact, the numbers of iterations of the three methods are
different from each other. For simplicity, here we denote them
using the same notation.
4In more detail, the training phase of WTM contains
preparing parallel training dataset with O(RN?t) and learning
translation probabilities using word alignment models with
O(IRN?wN?t), where I is the number of iterations for learning
translation probabilities, and N?t is the average number of tags
for each resource after sampling.
1582
is O(INwT ) and WTM is O(NwT ). From the
analysis, we can see that WTM is a relatively simple
method for both training and suggestion. This is
especially valuable because WTM also shows good
effectiveness for tag suggestion compared with other
methods as we will shown later.
Parameter Settings We use GIZA++ (Och and
Ney, 2003)5 as IBM Model-1 to learn transla-
tion probabilities using description-annotation pairs
for WTM. The experimental results of WTM are
obtained by setting parameters as follows: tag
weighting type as TF-IRFt, length ratio ? = 1,
harmonic factor ? = 0.5 and the type of word trigger
strength as TF-IRFw. The influence of parameters to
WTM can be found in Section 4.3.
Experiment Results and Analysis In Fig. 2 we
show the precision-recall curves of NB, kNN, CRM
and WTM on two datasets. Each point of a
precision-recall curve represents different numbers
of suggested tags from M = 1 (bottom right, with
higher precision and lower recall) to M = 10
(upper left, with higher recall but lower precision)
respectively. The closer the curve to the upper right,
the better the overall performance of the method.
From Fig. 2, we observe that:
? WTM consistently performs the best on both
datasets. This indicates that WTM is robust and
effective for tag suggestion.
? The advantage of WTM is more significant on
the BOOK dataset. The reason is that WTM
can take a good advantage of annotation count
information of tags compared to other methods.
? The average length of resource descriptions is
short in the BIBTEX dataset, which makes
it difficult to determine the trigger powers of
words. But even on the BIBTEX dataset
with no count information of tags, WTM still
outperforms other methods especially when
recommending first several tags.
To further demonstrate the performance of WTM
and other baseline methods, in Table 3 we show the
5GIZA++ is freely available on code.google.com/p/
giza-pp. The toolkit is widely used for word alignment in
SMT. In this paper, we use the default setting of parameters for
training.
 0.1
 0.15
 0.2
 0.25
 0.3
 0.35
 0.4
 0.45
 0.5
 0.55
 0.6
 0.65
 0.15  0.2  0.25  0.3  0.35  0.4  0.45  0.5  0.55  0.6  0.65
R
ec
al
l
Precision
 WAM
 NB
 kNN
 CRM
 TAM
(a) BOOK
 0.1
 0.15
 0.2
 0.25
 0.3
 0.35
 0.4
 0.45
 0.5
 0.55
 0.05  0.1  0.15  0.2  0.25  0.3  0.35  0.4
R
ec
al
l
Precision
 WAM
 NB
 kNN
 CRM
 TAM
(b) BIBTEX
Figure 2: Performance comparison between NB,
kNN, CRM, TAM and WTM on two datasets.
precision, recall and F-measure of NB, kNN, CRM,
TAM and WTM on BOOK dataset when suggesting
M = 3 tags6. Due to the limit of space, we only
show the variance of F-measure. In fact, WTM
achieves its best performance when M = 2, where
the F-measure of WTM is 0.370, outperforming
both CRM (F = 0.263) and TAM (F = 0.277) by
about 10%.
An Example In Table 4 we show top 10 tags
suggested by NB, CRM, TAM and WTM for the
book in Table 1. The number in bracket after
the name of each method is the count of correctly
suggested tags. The correctly suggested tags are
marked in bold face. We select not to show
6We select to show this number because it is near the average
number of tags for BOOK dataset
1583
Method Precision Recall F-measure
NB 0.271 0.302 0.247? 0.004
kNN 0.280 0.314 0.258? 0.002
CRM 0.292 0.323 0.266? 0.004
TAM 0.310 0.344 0.283? 0.001
WTM 0.368 0.452 0.355? 0.002
Table 3: Comparing results of NB, kNN, CRM,
TAM and WTM on BOOK dataset when suggesting
M = 3 tags.
the results of kNN because the tags suggested by
kNN are totally unrelated to the book due to the
insufficient finding of nearest neighbors.
From Table 4, we observe that NB, CRM and
TAM, as generative models, tend to suggest general
tags such as ?novel?, ?literature?, ?classic? and
?France?, and fail in suggesting specific tags such as
?Alexandre Dumas? and ?Count of Monte Cristo?.
On the contrary, WTM succeeds in suggesting both
general and specific tags related to the book.
NB (+6): novel, foreign literature, literature, his-
tory, Japan, classic, France, philosophy, America,
biography
CRM (+5): novel, foreign literature, literature, bi-
ography, philosophy, culture, France, British, comic,
history
TAM (+5): novel, sociology, finance, foreign liter-
ature, France, literature, biography, France litera-
ture, comic, China
WTM (+7): novel, Alexandre Dumas, history,
Count of Monte Cristo, foreign literature, biogra-
phy, suspense, comic, America, France
Table 4: Top 10 tags suggested by NB, CRM, TAM
and WTM for the book in Table 1.
In Table 5, we list four important words (using
TF-IRFw as weighting metric) of the description and
their corresponding tags with the highest translation
probabilities. The values in brackets are the proba-
bility of tag t given word w, Pr(t|w). For each word,
we eliminated the tags with the probability less than
0.1. We can see that the translation probabilities can
map the words in descriptions to their semantically
corresponding tags in annotations.
Count of Monte Cristo: Count of Monte Cristo
(0.728), Alexandre Dumas (0.270), . . .
Alexandre Dumas: Alexandre Dumas (0.966), . . .
revenge: foreign literature (0.168), classic (0.130),
martial arts (0.123), Alexandre Dumas (0.122), . . .
France: France (0.99), . . .
Table 5: Four important words (in bold face) in the
book description in Table 1 and their corresponding
tags with the highest translation probabilities.
4.3 Parameter Influences
We explore the parameter influences to WTM for
social tag suggestion. The parameters include
harmonic factor, length ratio, tag weighting types,
and types of word trigger strength. When inves-
tigating one parameter, we set other parameters
to be the values inducing the best performance
as mentioned in Section 4.2. Finally, we also
investigate the influence of training data size for
suggestion performance. In experiments we find
that WTM reveals similar trends on both the BOOK
dataset and the BIBTEX dataset. We thus only show
the experimental results on the BOOK dataset for
analysis.
Harmonic Factor In Fig. 3 we investigate the
influence of harmonic factor via the curves of F-
measure of WTM versus the number of suggested
tags on the BOOK dataset when harmonic factor ?
ranges from 0.0 to 1.0. As shown in Section 3.3,
harmonic factor ? controls the proportion between
model Prd2a and Pra2d.
From Fig. 3, we observe that neither single model
Prd2a (? = 1.0) nor Pra2d (? = 0.0) achieves
the best performance. When the two models are
combined by harmonic mean, the performance is
consistently better, especially when ? ranges from
0.2 to 0.6. This is reasonable because IBM Model-
1 constrains that only the term in source language
can be aligned to multiple terms in target language,
which makes the translation probability learned by a
single model be asymmetric.
Length Ratio Fig. 4 shows the influence of length
ratios on the BOOK dataset. From the figure, we
observe that the performance for tag suggestion is
robust as the length ratio varies, except when the
ratio breaks the default restriction of GIZA++ (i.e.,
1584
? = 10)7.
 0.22
 0.24
 0.26
 0.28
 0.3
 0.32
 0.34
 0.36
 0.38
1 2 3 4 5 6 7 8 9 10
F-
m
ea
su
re
Number of Suggested Tags
? = 0.0
? = 0.2
? = 0.4
? = 0.5
? = 0.6
? = 0.8
? = 1.0
Figure 3: F-measure of WTM versus the number of
suggested tags on the BOOK dataset when harmonic
factor ? ranges from 0.0 to 1.0.
 0.2
 0.22
 0.24
 0.26
 0.28
 0.3
 0.32
 0.34
 0.36
 0.38
1 2 3 4 5 6 7 8 9 10
F-
m
ea
su
re
Number of Suggested Tags
? = 10/1
? = 10/3
? = 10/5
? = 1/1
? = 1/2
? = 1/5
Figure 4: F-measure of WTM versus the number of
suggested tags on the BOOK dataset when length
ratio ? ranges from 10/1 to 1/5.
Tag Weighting Types The influence of two
weighting types, TFt and TF-IRFt, on social tag
suggestion when M = 3 on the BOOK dataset
is shown in Table 6. TF-IRFt tends to select the
tags more specific to the resource while TFt tends
to select the most popular tags, because the latter
does not consider global information (the IRFt part).
7GIZA++ restricts the values of length ratio within [ 19 , 9] by
setting parameter maxfertility=10. From Fig. 4, we can
see when ? = 10, the performance becomes much worse since
GIZA++ will cut off the sentences out of range.
Table 6 verifies the analysis, where TF-IRFt is
slightly better than TFt.
Weighting Precision Recall F-measure
TFt 0.356 0.437 0.342? 0.002
TF-IRFt 0.368 0.452 0.355? 0.002
Table 6: Evaluation results for different tag weight-
ing types when M = 3 on the BOOK dataset.
Methods for Computing Word Trigger Power
In Table 7, we show the performance of social tag
suggestions on the BOOK dataset with different
methods for computing word trigger power. From
the table, we can see that there is not significant
difference between TF-IRFw and the product of TF-
IRFw and TextRank, while TextRank itself performs
the worst. This indicates that TextRank is less
competitive to measure word trigger power since it
does not take global information into consideration.
Weighting Precision Recall F-measure
TF-IRFw 0.368 0.452 0.355? 0.002
TextRank 0.345 0.424 0.332? 0.002
Product 0.368 0.451 0.354? 0.002
Table 7: Evaluation results for different methods for
computing word trigger powers when M = 3 on the
BOOK dataset.
Training Data Size We investigate the influence
of training data size for social tag suggestion. As
shown in Fig. 5, we increased the training data size
from 8, 000 to 56, 000 step by 8, 000, and carried
out evaluation on 4, 000 resources. The figure shows
that:
? When the training data size is small (e.g.,
8, 000), WTM can still achieve good sugges-
tion performance.
? As the training data size increases, the perfor-
mance of WTM improves, while the improve-
ment speed declines.
The observation indicates that WTM does not
require huge-size dataset to achieve good perfor-
mance.
1585
 0.2
 0.25
 0.3
 0.35
 0.4
 0.45
 0.5
 0.55
 0.6
 0.65
 0.1  0.2  0.3  0.4  0.5  0.6  0.7
R
ec
al
l
Precision
  8,000
 16,000
 24,000
 32,000
 40,000
 48,000
 56,000
Figure 5: Precision-recall curves when the training
data size increases from 8, 000 thousand to 56, 000
thousand on the BOOK dataset.
Conclusion By analyzing the influences of pa-
rameters on WTM, we find that WTM is robust to
parameter variations.
4.4 Performance of EWTM
At the end of this section, we investigate the
performance of EWTM for social tag suggestion.
Here we simply set the smooth factor ? = 0.5.
As shown in Table 8, EWTM improves the
performance of WTM (in Table 7) on the BOOK
dataset when using TF-IRFw and the product as the
methods for computing the word trigger powers,
but decays when using TextRank. This verifies
that TF-IRFw is the best method to measure word
trigger powers for WTM. Table 8 indicates that
emphasizing the tags appearing in the descriptions
may enhance the suggestion power of the word
trigger method.
Weighting Precision Recall F-measure
TF-IRFw 0.385 0.472 0.371? 0.001
TextRank 0.344 0.423 0.332? 0.002
Product 0.374 0.457 0.360? 0.001
Table 8: The evaluation results of EWTM with dif-
ferent methods for computing word trigger powers
when M = 3 on the BOOK dataset.
However, the performance of EWTM on the
BIBTEX dataset decays much compared to WTM.
The F-measure of EWTM is only F = 0.229
compared with WTM F = 0.267. The main reason
of the decay is that: the resource descriptions in
the BIBTEX dataset are usually too short to provide
sufficient information to precisely emphasize tags.
In this case, EWTM may emphasize wrong tags and
drop correct tags.
The experimental results on EWTM suggest that,
the performance of EWTM is heavily influenced by
the length of resource descriptions. Therefore, we
have to analyze the characteristics of social tagging
systems to decide whether to emphasize the tags that
appear in the corresponding resource descriptions.
As future work, we will investigate the influence
of the smooth factor ? to EWTM. It is also worth
to investigate the problem when combining with
collaboration-based methods for social tag sugges-
tion.
5 Conclusion and Future Work
In this paper, we present a new perspective to social
tagging and propose the word trigger method for
social tag suggestion based on word alignment in
statistical machine translation. Experiments show
that our method is effective and efficient for social
tag suggestion compared to other baselines.
There are still several open problems that should
be further investigated:
1. We can exploit other word alignment methods
like log-linear models (Liu et al, 2010a) for
social tag suggestion.
2. We will ensemble WTM with other content-
based and collaboration-based methods to build
a practical social tag suggestion system.
3. WTM and EWTM can only suggest the tags
that have appeared in translation models. In
future, we plan to incorporate keyphrase ex-
traction in social tag suggestion to make it
suggest more appropriate tags not only from
translation models but also from the resource
descriptions.
Acknowledgments
This work is supported by the National Natural
Science Foundation of China (NSFC) under Grant
No. 60873174. The authors would like to thank
Peng Li for his insightful suggestions and thank the
anonymous reviewers for their helpful comments.
1586
References
M. Banko, V.O. Mittal, and M.J. Witbrock. 2000.
Headline generation based on statistical translation. In
Proceedings of ACL, pages 318?325.
A. Berger and J. Lafferty. 1999. Information retrieval as
statistical translation. In Proceedings of SIGIR, pages
222?229.
A. Berger, R. Caruana, D. Cohn, D. Freitag, and
V. Mittal. 2000. Bridging the lexical chasm: statistical
approaches to answer-finding. In Proceedings of
SIGIR, pages 192?199.
D.M. Blei and M.I. Jordan. 2003. Modeling annotated
data. In Proceedings of SIGIR, pages 127?134.
D.M. Blei, A.Y. Ng, and M.I. Jordan. 2003. Latent
dirichlet alocation. JMLR, 3:993?1022.
D.M. Blei, T.L. Griffiths, and M.I. Jordan. 2010.
The nested chinese restaurant process and bayesian
nonparametric inference of topic hierarchies. Journal
of the ACM, 57(2):7.
P.F. Brown, V.J.D. Pietra, S.A.D. Pietra, and R.L.
Mercer. 1993. The mathematics of statistical machine
translation: Parameter estimation. Computational
linguistics, 19(2):263?311.
M. Bundschus, S. Yu, V. Tresp, A. Rettinger, M. Dejori,
and H.P. Kriegel. 2009. Hierarchical bayesian models
for collaborative tagging systems. In Proceedings of
ICDM, pages 728?733.
N. Dalvi, R. Kumar, B. Pang, and A. Tomkins. 2009. A
translation model for matching reviews to objects. In
Proceeding of CIKM, pages 167?176.
P. Duygulu, K. Barnard, J. De Freitas, and D. Forsyth.
2002. Object recognition as machine translation:
Learning a lexicon for a fixed image vocabulary.
Proceedings of ECCV, pages 97?112.
A. Echihabi and D. Marcu. 2003. A noisy-channel
approach to question answering. In Proceedings of
ACL, pages 16?23.
D. Eck, P. Lamere, T. Bertin-Mahieux, and S. Green.
2007. Automatic generation of social tags for music
recommendation. In Proceedings of NIPS, pages 385?
392.
E. Frank, G.W. Paynter, I.H. Witten, C. Gutwin, and C.G.
Nevill-Manning. 1999. Domain-specific keyphrase
extraction. In Proceedings of IJCAI, pages 668?673.
S. Fujimura, KO Fujimura, and H. Okuda. 2008.
Blogosonomy: Autotagging any text using bloggers?
knowledge. In Proceedings of WI, pages 205?212.
J.L. Herlocker, J.A. Konstan, A. Borchers, and J. Riedl.
1999. An algorithmic framework for performing
collaborative filtering. In Proceedings of SIGIR, pages
230?237.
J.L. Herlocker, J.A. Konstan, L.G. Terveen, and J.T.
Riedl. 2004. Evaluating collaborative filtering recom-
mender systems. ACM Transactions on Information
Systems, 22(1):5?53.
P. Heymann, D. Ramage, and H. Garcia-Molina. 2008.
Social tag prediction. In Proceedings of SIGIR, pages
531?538.
A. Hotho, R. Jaschke, C. Schmitz, and G. Stumme.
2006. Trend detection in folksonomies. Semantic
Multimedia, pages 56?70.
T. Iwata, T. Yamada, and N. Ueda. 2009. Modeling
social annotation data with content relevance using a
topic model. In Proceedings of NIPS, pages 835?843.
R. Jaschke, L. Marinho, A. Hotho, L. Schmidt-Thieme,
and G. Stumme. 2008. Tag recommendations in
social bookmarking systems. AI Communications,
21(4):231?247.
J. Jeon, V. Lavrenko, and R. Manmatha. 2003. Automat-
ic image annotation and retrieval using cross-media
relevance models. In Proceedings of SIGIR, pages
119?126.
M. Karimzadehgan and C.X. Zhai. 2010. Estimation of
statistical translation models based on mutual informa-
tion for ad hoc information retrieval. In Proceedings
of SIGIR, pages 323?330.
I. Katakis, G. Tsoumakas, and I. Vlahavas. 2008. Mul-
tilabel text classification for automated tag suggestion.
ECML PKDD Discovery Challenge 2008, page 75.
R. Krestel, P. Fankhauser, and W. Nejdl. 2009. Latent
dirichlet alocation for tag recommendation. In
Proceedings of ACM RecSys, pages 61?68.
S.O.K. Lee and A.H.W. Chun. 2007. Automatic
tag recommendation for the web 2.0 blogosphere
using collaborative tagging and hybrid ann semantic
structures. In Proceedings of WSEAS, pages 88?93.
Z. Liu, P. Li, Y. Zheng, and M. Sun. 2009a. Clustering
to find exemplar terms for keyphrase extraction. In
Proceedings of EMNLP, pages 257?266.
Z. Liu, H. Wang, H. Wu, and S. Li. 2009b. Collocation
extraction using monolingual word alignment method.
In Proceedings of EMNLP, pages 487?495.
Y. Liu, Q. Liu, and S. Lin. 2010a. Discriminative
word alignment by linear modeling. Computational
Linguistics, 36(3):303?339.
Z. Liu, W. Huang, Y. Zheng, and M. Sun. 2010b. Au-
tomatic keyphrase extraction via topic decomposition.
In Proceedings of EMNLP, pages 366?376.
Z. Liu, H. Wang, H. Wu, and S. Li. 2010c. Improving
statistical machine translation with monolingual collo-
cation. In Proceedings of ACL, pages 825?833.
Z. Liu, X. Chen, Y. Zheng, and M. Sun. 2011. Automatic
keyphrase extraction by bridging vocabulary gap. In
Proceedings of CoNLL, pages 135?144.
1587
C.D. Manning, P. Raghavan, and H. Schtze. 2008.
Introduction to information retrieval. Cambridge
University Press New York, NY, USA.
R. Mihalcea and P. Tarau. 2004. TextRank: Bringing
order into texts. In Proceedings of EMNLP, pages
404?411.
R. Mirizzi, A. Ragone, T. Di Noia, and E. Di Sciascio.
2010. Semantic tags generation and retrieval for
online advertising. In Proceedings of CIKM, pages
1089?1098.
G. Mishne. 2006. Autotag: a collaborative approach
to automated tag assignment for weblog posts. In
Proceedings of WWW, pages 953?954.
V. Murdock and W.B. Croft. 2004. Simple translation
models for sentence retrieval in factoid question an-
swering. In Proceedings of SIGIR.
F.J. Och and H. Ney. 2003. A systematic comparison of
various statistical alignment models. Computational
linguistics, 29(1):19?51.
T. Ohkura, Y. Kiyota, and H. Nakagawa. 2006. Browsing
system for weblog articles based on automated folk-
sonomy. In Proceedings of WWW.
L. Page, S. Brin, R. Motwani, and T. Winograd. 1998.
The pagerank citation ranking: Bringing order to
the web. Technical report, Stanford Digital Library
Technologies Project.
C. Quirk, C. Brockett, and W. Dolan. 2004. Monolingual
machine translation for paraphrase generation. In
Proceedings of EMNLP, volume 149.
S. Ravi, A. Broder, E. Gabrilovich, V. Josifovski,
S. Pandey, and B. Pang. 2010. Automatic generation
of bid phrases for online advertising. In Proceedings
of WSDM, pages 341?350.
S. Rendle, L. Balby Marinho, A. Nanopoulos, and
L. Schmidt-Thieme. 2009. Learning optimal ranking
with tensor factorization for tag recommendation. In
Proceedings of KDD, pages 727?736.
P. Resnick and H.R. Varian. 1997. Recommender
systems. Communications of the ACM, 40(3):56?58.
S. Riezler and Y. Liu. 2010. Query rewriting using
monolingual statistical machine translation. Compu-
tational Linguistics, 36(3):569?582.
S. Riezler, A. Vasserman, I. Tsochantaridis, V. Mittal, and
Y. Liu. 2007. Statistical machine translation for query
expansion in answer retrieval. In Proccedings of ACL,
pages 464?471.
S. Riezler, Y. Liu, and A. Vasserman. 2008. Translating
queries into snippets for improved query expansion. In
Proceedings of COLING, pages 737?744.
X. Si and M. Sun. 2009. Tag-LDA for scalable real-
time tag recommendation. Journal of Computational
Information Systems, 6(1):23?31.
X. Si, Z. Liu, and M. Sun. 2010. Modeling social
annotations via latent reason identification. IEEE
Intelligent Systems, 25(6):42 ? 49.
R. Soricut and E. Brill. 2006. Automatic question
answering using the web: Beyond the factoid. Infor-
mation Retrieval, 9(2):191?206.
M. Surdeanu, M. Ciaramita, and H. Zaragoza. 2008.
Learning to rank answers on large online qa collec-
tions. In Proceedings of ACL, pages 719?727.
Y.W. Teh, M.I. Jordan, M.J. Beal, and D.M. Blei.
2006. Hierarchical dirichlet processes. Journal of
the American Statistical Association, 101(476):1566?
1581.
P.D. Turney. 2000. Learning algorithms for keyphrase
extraction. Information Retrieval, 2(4):303?336.
F.Y. Wang, K.M. Carley, D. Zeng, and W. Mao. 2007.
Social computing: From social informatics to social
intelligence. IEEE Intelligent Systems, 22(2):79?83.
R. Wetzker, C. Zimmermann, C. Bauckhage, and S. Al-
bayrak. 2010. I tag, you tag: translating tags for
advanced user models. In Proceedings of WSDM,
pages 71?80.
X. Xue, J. Jeon, and W.B. Croft. 2008. Retrieval models
for question and answer archives. In Proceedings of
SIGIR, pages 475?482.
Y. Yanbe, A. Jatowt, S. Nakamura, and K. Tanaka. 2007.
Can social bookmarking enhance search in the web?
In Proceedings of JCDL, pages 107?116.
S. Zhao, H. Wang, X. Lan, and T. Liu. 2010a. Leverag-
ing multiple mt engines for paraphrase generation. In
Proceedings of COLING, pages 1326?1334.
S. Zhao, H. Wang, and T. Liu. 2010b. Paraphrasing with
search engine query logs. In Proceedings of COLING,
pages 1317?1325.
T.C. Zhou, H. Ma, M.R. Lyu, and I. King. 2010.
UserRec: A user recommendation framework in social
tagging systems. In Proceedings of AAAI, pages
1486?1491.
1588
Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1025?1035,
October 25-29, 2014, Doha, Qatar.
c?2014 Association for Computational Linguistics
A Unified Model for Word Sense Representation and Disambiguation
Xinxiong Chen, Zhiyuan Liu, Maosong Sun
State Key Laboratory of Intelligent Technology and Systems
Tsinghua National Laboratory for Information Science and Technology
Department of Computer Science and Technology
Tsinghua University, Beijing 100084, China
cxx.thu@gmail.com, {lzy, sms}@tsinghua.edu.cn
Abstract
Most word representation methods assume
that each word owns a single semantic vec-
tor. This is usually problematic because
lexical ambiguity is ubiquitous, which is
also the problem to be resolved by word
sense disambiguation. In this paper, we
present a unified model for joint word
sense representation and disambiguation,
which will assign distinct representation-
s for each word sense.
1
The basic idea is
that both word sense representation (WS-
R) and word sense disambiguation (WS-
D) will benefit from each other: (1) high-
quality WSR will capture rich informa-
tion about words and senses, which should
be helpful for WSD, and (2) high-quality
WSD will provide reliable disambiguat-
ed corpora for learning better sense rep-
resentations. Experimental results show
that, our model improves the performance
of contextual word similarity compared to
existing WSR methods, outperforms state-
of-the-art supervised methods on domain-
specific WSD, and achieves competitive
performance on coarse-grained all-words
WSD.
1 Introduction
Word representation aims to build vectors for each
word based on its context in a large corpus, usually
capturing both semantic and syntactic information
of words. These representations can be used as
features or inputs, which are widely employed in
information retrieval (Manning et al., 2008), doc-
ument classification (Sebastiani, 2002) and other
NLP tasks.
1
Our sense representations can be downloaded at http:
//pan.baidu.com/s/1eQcPK8i.
Most word representation methods assume each
word owns a single vector. However, this is usual-
ly problematic due to the homonymy and polyse-
my of many words. To remedy the issue, Reisinger
and Mooney (2010) proposed a multi-prototype
vector space model, where the contexts of each
word are first clustered into groups, and then each
cluster generates a distinct prototype vector for a
word by averaging over all context vectors with-
in the cluster. Huang et al. (2012) followed this
idea, but introduced continuous distributed vectors
based on probabilistic neural language models for
word representations.
These cluster-based models conduct unsuper-
vised word sense induction by clustering word
contexts and, thus, suffer from the following is-
sues:
? It is usually difficult for these cluster-based
models to determine the number of cluster-
s. Huang et al. (2012) simply cluster word
contexts into static K clusters for each word,
which is arbitrary and may introduce mis-
takes.
? These cluster-based models are typically off-
line , so they cannot be efficiently adapted to
new senses, new words or new data.
? It is also troublesome to find the sense that
a word prototype corresponds to; thus, these
cluster-based models cannot be directly used
to perform word sense disambiguation.
In reality, many large knowledge bases have
been constructed with word senses available
online, such as WordNet (Miller, 1995) and
Wikipedia. Utilizing these knowledge bases to
learn word representation and sense representation
is a natural choice. In this paper, we present a uni-
fied model for both word sense representation and
disambiguation based on these knowledge bases
and large-scale text corpora. The unified model
1025
can (1) perform word sense disambiguation based
on vector representations, and (2) learn continu-
ous distributed vector representation for word and
sense jointly.
The basic idea is that, the tasks of word sense
representation (WSR) and word sense disam-
biguation (WSD) can benefit from each other: (1)
high-quality WSR will capture rich semantic and
syntactic information of words and senses, which
should be helpful for WSD; (2) high-quality WS-
D will provide reliable disambiguated corpora for
learning better sense representations.
By utilizing these knowledge bases, the prob-
lem mentioned above can be overcome:
? The number of senses of a word can be de-
cided by the expert annotators or web users.
? When a new sense appears, our model can be
easily applied to obtain a new sense represen-
tation.
? Every sense vector has a corresponding sense
in these knowledge bases.
We conduct experiments to investigate the per-
formance of our model for both WSR and WS-
D. We evaluate the performance of WSR using a
contextual word similarity task, and results show
that out model can significantly improve the cor-
relation with human judgments compared to base-
lines. We further evaluate the performance on
both domain-specific WSD and coarse-grained all-
words WSD, and results show that our model
yields performance competitive with state-of-the-
art supervised approaches.
2 Methodology
We describe our method as a 3-stage process:
1. Initializing word vectors and sense vectors.
Given large amounts of text data, we first use
the Skip-gram model (Mikolov et al., 2013),
a neural network based language model, to
learn word vectors. Then, we assign vector
representations for senses based on their def-
initions (e.g, glosses in WordNet).
2. Performing word sense disambiguation.
Given word vectors and sense vectors, we
propose two simple and efficient WSD algo-
rithms to obtain more relevant occurrences
for each sense.
3. Learning sense vectors from relevant oc-
currences. Based on the relevant occur-
rences of ambiguous words, we modify the
training objective of Skip-gram to learn word
vectors and sense vectors jointly. Then, we
obtain the sense vectors directly from the
model.
Before illustrating the three stages of our
method in Sections 2.2, 2.3 and 2.4, we briefly
introduce our sense inventory, WordNet, in Sec-
tion 2.1. Note that, although our experiments will
use the WordNet sense inventory, our model is not
limited to this particular lexicon. Other knowledge
bases containing word sense distinctions and defi-
nitions can also serve as input to our model.
2.1 WordNet
WordNet (Miller, 1995) is the most widely used
computational lexicon of English where a concep-
t is represented as a synonym set, or synset. The
words in the same synset share a common mean-
ing. Each synset has a textual definition, or gloss.
Table 1 shows the synsets and the corresponding
glosses of the two common senses of bank.
Before introducing the method in detail, we in-
troduce the notations. The unlabeled texts are de-
noted as R, and the vocabulary of the texts is de-
noted as W . For a word w in W , w
s
i
is the ith
sense in WordNet WN. Each sense w
s
i
has a gloss
gloss(w
s
i
) in WN. The word embedding of w is
denoted as vec(w), and the sense embedding of its
ith sense w
s
i
is denoted as vec(w
s
i
).
2.2 Initializing Word Vectors and Sense
Vectors
Initializing word vectors. First, we use Skip-
gram to train the word vectors from large amounts
of text data. We choose Skip-gram for its sim-
plicity and effectiveness. The training objective of
Skip-gram is to train word vector representations
that are good at predicting its context in the same
sentence (Mikolov et al., 2013).
More formally, given a sequence of training
words w
1
, w
2
, w
3
,...,w
T
, the objective of Skip-
gram is to maximize the average log probability
1
T
T
?
t=1
(
?
?k? j?k, j 6=0
log p(w
t+ j
|w
t
)
)
(1)
where k is the size of the training window. The
inner summation spans from ?k to k to compute
1026
Sense Synset Gloss
bank
s
1
(sloping land (especially the slope beside a body of water))
bank ?they pulled the canoe up on the bank?;
?he sat on the bank of the river and watched the currents?
bank
s
2
depository institution, (a financial institution that accepts deposits and channels the
bank, money into lending activities)
banking concern, ?he cashed a check at the bank?;
banking company ?that bank holds the mortgage on my home?
Table 1: Example of a synset in WordNet.
the log probability of correctly predicting the word
w
t+ j
given the word in the middle w
t
. The outer
summation covers all words in the training data.
The prediction task is performed via softmax, a
multiclass classifier. There, we have
p(w
t+ j
|w
t
) =
exp(vec
?
(w
t+ j
)
>
vec(w
t
))
?
W
w=1
exp(vec
?
(w)
>
vec(w
t
))
(2)
where vec(w) and vec
?
(w) are the ?input? and
?output? vector representations of w. This formu-
lation is impractical because the cost of comput-
ing p(w
t+ j
|w
t
) is proportional to W , which is often
large( 10
5
?10
7
terms).
Initializing sense vectors. After learning the
word vectors using the Skip-gram model, we ini-
tialize the sense vectors based on the glosses of
senses. The basic idea of the sense vector initial-
ization is to represent the sense by using the sim-
ilar words in the gloss. From the content words
in the gloss, we select those words whose cosine
similarities with the original word are larger than
a similarity threshold ? . Formally, for each sense
w
s
i
in WN, we first define a candidate set from
gloss(w
s
i
)
cand(w
s
i
) = {u|u ? gloss(w
s
i
),u 6= w,
POS(u) ?CW,cos(vec(w),vec(u)) > ?} (3)
where POS(u) is the part-of-speech tagging of the
word u and CW is the set of all possible part-of-
speech tags that content words could have. In this
paper, CW contains the following tags: noun, verb,
adjective and adverb.
Then the average of the word vectors in
cand(w
s
i
) is used as the initialization value of the
sense vector vec(w
s
i
).
vec(w
s
i
) =
1
|cand(w
s
i
)|
?
u?cand(w
s
i
)
vec(u) (4)
For example, in WordNet, the gloss of the sense
bank
s
1
is ?sloping land (especially the slope beside
a body of water)) they pulled the canoe up on the
bank; he sat on the bank of the river and watched
the currents?. The gloss contains a definition of
the sense and two examples of the sense. The
content words and the cosine similarities with the
word ?bank? are listed as follows: (sloping, 0.12),
(land, 0.21), (slope, 0.17), (body, 0.01), (water,
0.10), (pulled, 0.01), (canoe, 0.09), (sat, 0.06),
(river, 0.43), (watch, -0.11), (currents, 0.01). If
the threshold, ? , is set to 0.05, then cand(bank
s
1
)
is {sloping, land, slope, water, canoe, sat, riv-
er}. Then the average of the word vectors in
cand(bank
s
i
) is used as the initialization value of
vec(bank
s
i
).
2.3 Performing Word Sense Disambiguation.
One of the state-of-the-art WSD results can be
obtained using exemplar models, i.e., the word
meaning is modeled by using relevant occurrences
only, rather than merging all of the occurrences in-
to a single word vector (Erk and Pado, 2010). In-
spired by this idea, we perform word sense disam-
biguation to obtain more relevant occurrences.
Here, we perform knowledge-based word sense
disambiguation for training data on an all-words
setting, i.e., we will disambiguate all of the con-
tent words in a sentence. Formally, the sentence S
is a sequence of words (w
1
,w
2
,...,w
n
), and we will
identify a mapping M from words to senses such
that M(i) ? Senses
WN
(w
i
), where Senses
WN
(w
i
) is
the set of senses encoded in the WN for word w
i
.
For sentence S, there are
?
n
i=1
|Sense
WN
(w
i
)| pos-
sible mapping answers, which are impractical to
compute. Thus, we design two simple algorithms,
L2R (left to right) algorithm and S2C (simple to
complex) algorithm, for word sense disambigua-
tion based on the sense vectors.
The main difference between L2R and S2C is
1027
the order of words when performing word sense
disambiguation. When given a sentence, the L2R
algorithm disambiguates the words from left to
right (the natural order of a sentence), whereas the
S2C algorithm disambiguates the words with few-
er senses first. The main idea of S2C algorithm
is that the words with fewer senses are easier to
disambiguate, and the disambiguation result can
be helpful to disambiguate the words with more
senses. Both of the algorithms have three steps:
Context vector initialization. Similar to the ini-
tialization of sense vectors, we use the average of
all of the content words? vectors in a sentence as
the initialization vector of context.
vec(context) =
1
|cand(S)|
?
u?cand(S)
vec(u) (5)
where cand(S) is the set of content words
cand(S) = {u|u ? S,POS(u) ?CW}.
Ranking words. For L2R, we do nothing in this
step. For S2C, we rank the words based on the
ascending order of |Senses
WN
(w
i
)|.
Word sense disambiguation. For both L2R and
S2C, we denote the order of words as L and per-
form word sense disambiguation according to L.
First, we skip a word if the word is not
a content word or the word is monosemous
(|Senses
WN
(w
i
)| = 1). Then, for each word in
L, we can compute the cosine similarities be-
tween the context vector and its sense vectors. We
choose the sense that yields the maximum cosine
similarity as its disambiguation result. If the s-
core margin between the maximum and the sec-
ond maximum is larger than the threshold ? , we
are confident with the disambiguation result of w
i
and then use the sense vector to replace the word
vector in the context vector. Thus, we obtain a
more accurate context vector for other words that
are still yet to be disambiguated.
For example, given a sentence ?He sat on the
bank of the lake?, we first explain how S2C work-
s. In the sentence, there are three content word-
s, ?sat?, ?bank? and ?lake?, to be disambiguated.
First, the sum of the three word vectors is used as
the initialization of the context vector. Then we
rank the words by |Senses
W
N(w
i
)|, in ascending
order, that is, lake (3 senses), bank (10 senses), sat
(10 senses). We first disambiguate the word ?lake?
based on the similarities between its sense vectors
and context vector. If the score margin is larger
bankinput
projection
output sat on the of the lakesit lake1 1
Figure 1: The architecture of our model. The
training objective of Skip-gram is to train word
vector representations that are not only good at
predicting its context words but are also good at
predicting its context words? senses. The center
word ?bank? is used to predict not only its context
words but also the sense ?sit
1
? and ?lake
1
?.
than the threshold ? , then we are confident with
this disambiguation result and replace the word
vector with the sense vector to update the contex-
t vector. It would be helpful to disambiguate the
next word, ?bank?. We repeat this process until all
three words are disambiguated.
For L2R, the order of words to be disambiguat-
ed will be ?sat?, ?bank? and ?lake?. In this time,
when disambiguating ?bank? (10 senses), we still
don?t know the sense of ?lake? (3 senses).
2.4 Learning Sense Vectors from Relevant
Occurrences.
Based on the disambiguation result, we modify the
training objective of Skip-gram and train the sense
vectors directly from the large-scale corpus. Our
training objective is to train the vector representa-
tions that are not only good at predicting its con-
text words but are also good at predicting its con-
text words? senses. The architecture of our model
is shown in Figure 1.
More formally, given the disambiguation result
M(w
1
), M(w
2
), M(w
3
),...,M(w
T
), the training ob-
jective is modified to
1
T
T
?
t=1
(
k
?
j=?k
log{p(w
t+ j
|w
t
)p(M(w
t+ j
)|w
t
)}
)
(6)
where k is the size of the training window. The
inner summation spans from ?k to k to compute
the log probability of correctly predicting the word
w
t+ j
and the log probability of correctly predicting
1028
the sense M(w
t+ j
) given the word in the middle
w
t
. The outer summation covers all words in the
training data.
Because not all of the disambiguation results are
correct, we only disambiguate the words that we
are confident in. Similar to step 3 of our WSD
algorithm, we only disambiguate words under the
condition that the score margin between the max-
imum and the second maximum is larger than the
score margin threshold, ? .
We also use the softmax function to define
p(w
t+ j
|w
t
) and p(M(w
t+ j
)|w
t
). Then, we use hi-
erarchical softmax (Morin and Bengio, 2005) to
greatly reduce the computational complexity and
learn the sense vectors directly from the relevant
occurrences.
3 Experiments
In this section, we first present the nearest neigh-
bors of some words and their senses, showing that
our sense vectors can capture the semantics of
words. Then, we use three tasks to evaluate our u-
nified model: a contextual word similarity task to
evaluate our sense representations, and two stan-
dard WSD tasks to evaluate our knowledge-based
WSD algorithm based on the sense vectors. Ex-
perimental results show that our model not only
improves the correlation with human judgments
on the contextual word similarity task but also out-
performs state-of-the-art supervised WSD system-
s on domain-specific datasets and competes with
them in a coarse-grained all-words setting.
We choose Wikipedia as the corpus to train
the word vectors because of its wide coverage
of topics and words usages. We use an English
Wikipedia database dump from October 2013
2
,
which includes roughly 3 million articles and 1
billion tokens. We use Wikipedia Extractor
3
to
preprocess the Wikipedia pages and only save the
content of the articles.
We use word2vec
4
to train Skip-gram. We use
the default parameters of word2vec and the dimen-
sion of the vector representations is 200.
We use WordNet
5
as our sense inventory. The
datasets for different tasks are tagged with differ-
ent versions of WordNet. The version of WordNet
2
http://download.wikipedia.org.
3
The tool is available from http://medialab.di.
unipi.it/wiki/Wikipedia_Extractor.
4
The code is available from https://code.
google.com/p/word2vec/.
5
http://wordnet.princeton.edu/.
Word or sense Nearest neighbors
bank banks, IDBI, CitiBank
bank
s
1
river, slope, Sooes
bank
s
2
mortgage, lending, loans
star stars, stellar, trek
star
s
1
photosphere, radiation,
gamma-rays
star
s
2
someone, skilled, genuinely
plant plants, glavaticevo, herbaceous
plant
s
1
factories, machinery,
manufacturing
plant
s
2
locomotion, organism,
organisms
Table 2: Nearest neighbors of word vectors and
sense vectors learned by our model based on co-
sine similarity. The subscript of each sense label
corresponds to the index of the sense in Word-
Net. For example, bank
s
2
is the second sense of
the word bank in WordNet.
is 1.7 for the domain-specific WSD task and 2.1
for the coarse-grained WSD task.
We use the S2C algorithm described in Section
2.3 to perform word sense disambiguation to ob-
tain more relevant occurrences for each sense. We
compare S2C and L2R on the coarse-grained WS-
D task in a all-words setting.
The experimental results of our model are ob-
tained by setting the similarity threshold as ? = 0
and the score margin threshold as ? = 0.1. The in-
fluence of parameters on our model can be found
in Section 3.5.
3.1 Examples for Sense Vectors
Table 2 shows the nearest neighbors of word vec-
tors and sense vectors based on cosine similari-
ty. We see that our sense representations can i-
dentify different meanings of a word, allowing our
model to capture more semantic and syntactic re-
lationships between words and senses. Note that
each sense vector in our model corresponds to a
sense in WordNet; thus, our sense vectors can be
used to perform knowledge-based word sense dis-
ambiguation, whereas the vectors of cluster-based
models cannot.
3.2 Contextual Word Similarity
Experimental setting. A standard dataset for e-
valuating a vector-space model is the WordSim-
353 dataset (Finkelstein et al., 2001), which con-
1029
Model ??100
C&W-S 57.0
Huang-S 58.6
Huang-M AvgSim 62.8
Huang-M AvgSimC 65.7
Our Model-S 64.2
Our Model-M AvgSim 66.2
Our Model-M AvgSimC 68.9
Table 3: Spearman?s ? on the SCWS dataset. Our
Model-S uses one representation per word to com-
pute similarities, while Our Model-M uses one
representation per sense to compute similarities.
AvgSim calculates the similarity with each sense
contributing equally, while AvgSimC weighs the
sense according to the probability of the word
choosing that sense in context c.
sists of 353 pairs of nouns. However, each pair of
nouns in WordSim-353 is presented without con-
text. This is problematic because the meanings
of homonymous and polysemous words depend
highly on the words? contexts. Thus we choose the
Stanford?s Contextual Word Similarities (SCWS)
dataset from (Huang et al., 2012)
6
. The SCWS
dataset contains 2003 pairs of words and each pair
is associated with 10 human judgments on similar-
ity on a scale from 0 to 10. In the SCWS dataset,
each word in a pair has a sentential context.
In our experiments, the similarity between a
pair of words (w, w
?
) is computed as follows:
AvgSimC(w,w
?
) =
1
MN
M
?
i=1
N
?
j=1
p(i|w,c)p( j|w
?
,c
?
)d(vec(w
s
i
),vec(w
?
s
j
)) (7)
where p(i|w,c) is the likelihood that word w
chooses its ith sense given context c. d(vec,vec
?
)
is a function computing the similarity between two
vectors, and here we use cosine similarity.
Results and discussion. For evaluation, we
compute the Spearman correlation between a
model?s computed similarity scores and human
judgements. Table 3 shows our results com-
pared to previous methods, including (Collobert
and Weston, 2008)?s language model (C&W), and
Huang?s model which utilize the global context
and multi-prototype to improve the word represen-
tations.
6
The dataset can be downloaded at http://ai.
stanford.edu/
?
ehhuang/.
From Table 3, we observe that:
? Our single-vector version outperforms
Huang?s single-vector version. This indi-
cates that, by training the word vectors and
sense vectors jointly, our model can better
capture the semantic relationships between
words and senses.
? With one representation per sense, our mod-
el can outperform the single-vector version
without using context (66.2 vs. 64.2).
? Our model obtains the best performance
(68.9) by using AvgSimC, which takes con-
text into account.
3.3 Domain-Specific WSD
Experimental setting. We use Wikipedia as
training data because of its wide coverage for spe-
cific domains. To test our performance on do-
main word sense disambiguation, we evaluated
our system on the dataset published in (Koeling
et al., 2005). This dataset consists of examples
retrieved from the Sports and Finance sections of
the Reuters corpus. 41 words related to the Sports
and Finance domains were selected, with an aver-
age polysemy of 6.7 senses, ranging from 2 to 13
senses.
Approximately 100 examples for each word
were annotated with senses from WordNet v.1.7
by three reviewers, yielding an inter-tagger agree-
ment of 65%. (Koeling et al., 2005) did not clarify
how to select the ?correct? sense for each word, so
we followed the work of (Agirre et al., 2009) and,
used the sense chosen by the majority of taggers
as the correct answer.
Baseline methods. As a baseline, we use the
most frequent WordNet sense (MFS), as well as
a random sense assignment. We also compare our
results with four systems
7
: Static PageRank (A-
girre et al., 2009), the k nearest neighbor algorith-
m (k-NN), Degree (Navigli and Lapata, 2010) and
Personalized PageRank (Agirre et al., 2009).
Static PageRank applies traditional PageRank
over the semantic graph based on WordNet and
obtains a context-independent ranking of word
senses.
k-NN is a widely used classification method,
where neighbors are the k labeled examples most
7
We compare only with those systems performing token-
based WSD, i.e., disambiguating each instance of a target
word separately.
1030
Algorithm
Sports Finance
Recall Recall
Random BL 19.5 19.6
MFS BL 19.6 37.1
k-NN 30.3 43.4
Static PR 20.1 39.6
Personalized PR 35.6 46.9
Degree 42.0 47.8
Our Model 57.3 60.6
Table 4: Performance on the Sports and Finance
sections of the dataset from (Koeling et al., 2005).
similar to the test example. The k-NN system is
trained on SemCor (Miller et al., 1993), the largest
publicly available annotated corpus.
Degree and Personalized PageRank are state-
of-the-art systems that exploit WordNet to build
a semantic graph and exploit the structural proper-
ties of the graph in order to choose the appropriate
senses of words in context.
Results and discussion. Similar to other work
on this dataset, we use recall (the ratio of correct
sense labels to the total labels in the gold standard)
as our evaluation measure. Table 4 shows the re-
sults of different WSD systems on the dataset, and
the best results are shown in bold. The differences
between other results and the best result in each
column of the table are statistically significant at
p < 0.05.
The results show that:
? Our model outperforms k-NN on the t-
wo domains by a large margin, support-
ing the findings from (Agirre et al., 2009)
that knowledge-based systems perform bet-
ter than supervised systems when evaluated
across different domains.
? Our model also achieves better results than
the state-of-the-art system (+15.3% recall on
Sports and +12.8% recall on Finance against
Degree). The reason for this is that when
dealing with short sentences or context words
that are not in WordNet, our model can still
compute similarity based on the context vec-
tor and sense vectors, whereas Degree will
have difficulty building the semantic graph.
? Moreover, our model achieves the best per-
formance by only using the unlabeled text da-
ta and the definitions of senses, whereas other
Algorithm Type
Nouns only All words
F
1
F
1
Random BL U 63.5 62.7
MFS BL Semi 77.4 78.9
SUSSX-FR Semi 81.1 77.0
NUS-PT S 82.3 82.5
SSI Semi 84.1 83.2
Degree Semi 85.5 81.7
Our Model
L2R
U 79.2 73.9
Our Model
S2C
U 81.6 75.8
Our Model
L2R
Semi 82.5 79.6
Our Model
S2C
Semi 85.3 82.6
Table 5: Performance on Semeval-2007 coarse-
grained all-words WSD. In the type column,
U, Semi and S stand for unsupervised, semi-
supervised and supervised, respectively. The dif-
ferences between the results in bold in each col-
umn of the table are not statistically significant at
p < 0.05.
methods rely greatly on high-quality seman-
tic relations or annotated data, which are hard
to acquire.
3.4 Coarse-grained WSD
Experimental setting. We also evaluate our
WSD model on the Semeval-2007 coarse-grained
all-words WSD task (Navigli et al., 2007). There
are multiple reasons that we perform experiments
in a coarse-grained setting: first, it has been ar-
gued that the fine granularity of WordNet is one
of the main obstacles to accurate WSD (cf. the
discussion in (Navigli, 2009)); second, the train-
ing corpus of word representations is Wikipedia,
which is quite different from WordNet.
Baseline methods. We compare our model with
the best unsupervised system SUSSX-FR (Koel-
ing and McCarthy, 2007), and the best supervised
system, NUS-PT (Chan et al., 2007), participat-
ing in the Semeval-2007 coarse-grained all-words
task. We also compare with SSI (Navigli and Ve-
lardi, 2005) and the state-of-the-art system De-
gree (Navigli and Lapata, 2010). We use different
baseline methods for the two WSD tasks because
we want to compare our model with the state-
of-the-art systems that are applicable to different
datasets and show that our WSD method can per-
form robustly in these different WSD tasks.
1031
Results and discussion. We report our results in
terms of F
1
-measure on the Semeval-2007 coarse-
grained all-words dataset (Navigli et al., 2007).
Table 5 reports the results for nouns (1,108 words)
and all words (2,269 words). The difference be-
tween unsupervised and semi-supervised methods
is whether the method uses MFS as a back-off s-
trategy.
We can see that the S2C algorithm outperforms
the L2R algorithm no matter on the nouns subset
or on the entire set. This indicates that words with
fewer senses are easier to disambiguate, and it can
be helpful to disambiguate the words with more
senses.
On the nouns subset, our model yields compa-
rable performance to SSI and Degree, and it out-
performs NUS-PT and SUSSX-FR. Moreover, our
unsupervised WSD method (S2C) beats the MF-
S baseline, which is notably a difficult competitor
for knowledge-based systems.
On the entire set, our semi-supervised model is
significantly better than SUSSX-FR, and it is com-
parable with SSI and Degree. In contrast to SSI,
our model is simple and does not rely on a cost-
ly annotation effort to engineer the set of semantic
relations.
Overall, our model achieves state-of-the-art per-
formance on the Semeval-2007 coarse-grained all-
words dataset compared to other systems, with a
simple WSD algorithm that only relies on a large-
scale unlabeled text corpora and a sense inventory.
3.5 Parameter Influence
We investigate the influence of parameters on our
model with coarse-grained all-words WSD task.
The parameters include the similarity threshold, ? ,
and the score margin threshold, ? .
Similarity threshold. In Table 6, we show the
performance of domain WSD when the similari-
ty threshold ? ranges from ?0.1 to 0.3. The co-
sine similarity interval is [-1, 1], and we focus on
the performance in the interval [-0.1, 0.3] for two
reasons: first, no words are removed from glosses
when ? < ?0.1; second, nearly half of the word-
s are removed when ? > 0.3 and the performance
drops significantly for the WSD task. From table
6, we can see that our model achieves the best per-
formance when ? = 0.0.
Score margin threshold. In Table 7, we show
the performance on the coarse-grained all-words
Parameter Nouns only All words
? =?0.10 79.8 74.3
? =?0.05 81.0 74.6
? = 0.00 81.6 75.8
? = 0.05 81.3 75.4
? = 0.10 80.8 75.2
? = 0.15 80.0 75.0
? = 0.20 77.1 73.3
? = 0.30 75.0 72.1
Table 6: Evaluation results on the coarse-grained
all-words WSD when the similarity threshold ?
ranges from ?0.1 to 0.3.
Parameter Nouns only All words
? = 0.00 78.2 72.9
? = 0.05 79.5 74.5
? = 0.10 81.6 75.8
? = 0.15 81.2 74.7
? = 0.20 80.9 75.1
? = 0.25 80.2 74.8
? = 0.30 80.4 74.9
Table 7: Evaluation results on the coarse-grained
all-words WSD when the score margin threshold
? ranges from 0.0 to 0.3.
WSD when the score margin threshold ? ranges
from 0.0 to 0.3. When ? = 0.0, we use every
disambiguation result to update the context vec-
tor. When ? 6= 0, we only use the confident disam-
biguation results to update the context vector if the
score margin is larger than ? . Our model achieves
the best performance when ? = 0.1.
4 Related Work
4.1 Word Representations
Distributed representations for words were pro-
posed in (Rumelhart et al., 1986) and have been
successfully used in language models (Bengio et
al., 2006; Mnih and Hinton, 2008) and many nat-
ural language processing tasks, such as word rep-
resentation learning (Mikolov, 2012), named enti-
ty recognition (Turian et al., 2010), disambigua-
tion (Collobert et al., 2011), parsing and tag-
ging (Socher et al., 2011; Socher et al., 2013).
They are very useful in NLP tasks because they
can be used as inputs to learning algorithms or as
extra word features in NLP systems. Hence, many
NLP applications, such as keyword extraction (Li-
1032
u et al., 2010; Liu et al., 2011b; Liu et al., 2012),
social tag suggestion (Liu et al., 2011a) and text
classification (Baker and McCallum, 1998), may
also potentially benefit from distributed word rep-
resentation. The main advantage is that the rep-
resentations of similar words are close in vector
space, which makes generalization to novel pat-
terns easier and model estimation more robust.
Word representations are hard to train due to the
computational complexity. Recently, (Mikolov et
al., 2013) proposed two particular models, Skip-
gram and CBOW, to learn word representations in
large amounts of text data. The training objective
of the CBOW model is to combine the representa-
tions of the surrounding words to predict the word
in the middle, while the Skip-gram model?s is to
learn word representations that are good at predict-
ing its context in the same sentence (Mikolov et
al., 2013). Our paper uses the model architecture
of Skip-gram.
Most of the previous vector-space models use
one representation per word. This is problematic
because many words have multiple senses. The
multi-prototype approach has been widely stud-
ied. (Reisinger and Mooney, 2010) proposed the
multi-prototype vector-space model. (Huang et
al., 2012) used the multi-prototype models to learn
the vector for different senses of a word. All of
these models use the clustering of contexts as a
word sense and can not be directly used in word
sense disambiguation.
After our paper was submitted, we perceive the
following recent advances: (Tian et al., 2014) pro-
posed a probabilistic model for multi-prototype
word representation. (Guo et al., 2014) explored
bilingual resources to learn sense-specific word
representation. (Neelakantan et al., 2014) pro-
posed an efficient non-parametric model for multi-
prototype word representation.
4.2 Knowledge-based WSD
The objective of word sense disambiguation (WS-
D) is to computationally identify the meaning of
words in context (Navigli, 2009). There are t-
wo approaches of WSD that assign meaning of
words from a fixed sense inventory, supervised and
knowledge-based methods. Supervised approach-
es require large labeled training sets, which are
time consuming to create. In this paper, we on-
ly focus on knowledge-based word sense disam-
biguation.
Knowledge-based approaches exploit knowl-
edge resources (such as dictionaries, thesauri, on-
tologies, collocations, etc.) to determine the
senses of words in context. However, it has
been shown in (Cuadros and Rigau, 2006) that
the amount of lexical and semantic information
contained in such resources is typically insuf-
ficient for high-performance WSD. Much work
has been presented to automatically extend ex-
isting resources, including automatically linking
Wikipedia to WordNet to include full use of the
first WordNet sense heuristic (Suchanek et al.,
2008), a graph-based mapping of Wikipedia cat-
egories to WordNet synsets (Ponzetto and Nav-
igli, 2009), and automatically mapping Wikipedia
pages to WordNet synsets (Ponzetto and Navigli,
2010).
It was recently shown that word representation-
s can capture semantic and syntactic information
between words (Mikolov et al., 2013). Some re-
searchers tried to incorporate WordNet senses in a
neural model to learn better word representation-
s (Bordes et al., 2011). In this paper, we have pro-
posed a unified method for word sense representa-
tion and disambiguation to extend the information
contained in the vector representations to the ex-
isting resources. Our method only requires a large
amount of unlabeled text to train sense representa-
tions and a dictionary to provide the definitions of
word meanings, which makes it easily applicable
to other resources.
5 Conclusion
In this paper, we present a unified model for word
sense representation and disambiguation that us-
es one representation per sense. Experimental re-
sults show that our model improves the perfor-
mance of contextual word similarity compared to
existing WSR methods, outperforms state-of-the-
art supervised methods on domain-specific WSD,
and achieves competitive performance on coarse-
grained all-words WSD. Our model only requires
large-scale unlabeled text corpora and a sense in-
ventory for WSD, thus it can be easily applied to
other corpora and tasks.
There are still several open problems that
should be investigated further:
1. Because the senses of words change over
time (new senses appear), we will incorpo-
rate cluster-based methods in our model to
find senses that are not in the sense inventory.
1033
2. We can explore other WSD methods based
on sense vectors to improve our performance.
For example, (Li et al., 2010) used LDA to
perform data-driven WSD in a manner simi-
lar to our model. We may integrate the advan-
tages of these models and our model together
to build a more powerful WSD system.
3. To learn better sense vectors, we can exploit
the semantic relations (such as the hypernym
and hyponym relations defined in WordNet)
between senses in our model.
Acknowledgments
This work is supported by National Key Ba-
sic Research Program of China (973 Program
2014CB340500) and National Natural Science
Foundation of China (NSFC 61133012).
References
Eneko Agirre, Oier Lopez De Lacalle, Aitor Soroa, and
Informatika Fakultatea. 2009. Knowledge-based
wsd and specific domains: Performing better than
generic supervised wsd. In Proceedings of IJCAI,
pages 1501?1506.
L Douglas Baker and Andrew Kachites McCallum.
1998. Distributional clustering of words for text
classification. In Proceedings of SIGIR, pages 96?
103.
Yoshua Bengio, Holger Schwenk, Jean-S?ebastien
Sen?ecal, Fr?ederic Morin, and Jean-Luc Gauvain.
2006. Neural probabilistic language models. In In-
novations in Machine Learning, pages 137?186.
Antoine Bordes, Jason Weston, Ronan Collobert,
Yoshua Bengio, et al. 2011. Learning structured
embeddings of knowledge bases. In Proceedings of
AAAI, pages 301?306.
Yee Seng Chan, Hwee Tou Ng, and Zhi Zhong. 2007.
Nus-pt: exploiting parallel texts for word sense dis-
ambiguation in the english all-words tasks. In Pro-
ceedings of SemEval, pages 253?256.
Ronan Collobert and Jason Weston. 2008. A unified
architecture for natural language processing: Deep
neural networks with multitask learning. In Pro-
ceedings of ICML, pages 160?167.
Ronan Collobert, Jason Weston, L?eon Bottou, Michael
Karlen, Koray Kavukcuoglu, and Pavel Kuksa.
2011. Natural language processing (almost) from
scratch. JMLR, 12:2493?2537.
Montse Cuadros and German Rigau. 2006. Quality
assessment of large scale knowledge resources. In
Proceedings of EMNLP, pages 534?541.
Katrin Erk and Sebastian Pado. 2010. Exemplar-based
models for word meaning in context. In Proceedings
of ACL, pages 92?97.
Lev Finkelstein, Evgeniy Gabrilovich, Yossi Matias, E-
hud Rivlin, Zach Solan, Gadi Wolfman, and Eytan
Ruppin. 2001. Placing search in context: The con-
cept revisited. In Proceedings of WWW, pages 406?
414.
Jiang Guo, Wanxiang Che, Haifeng Wang, and Ting Li-
u. 2014. Learning sense-specific word embeddings
by exploiting bilingual resources. In Proceedings of
COLING, pages 497?507.
Eric H Huang, Richard Socher, Christopher D Man-
ning, and Andrew Y Ng. 2012. Improving word
representations via global context and multiple word
prototypes. In Proceedings of ACL, pages 873?882.
Rob Koeling and Diana McCarthy. 2007. Sussx: Ws-
d using automatically acquired predominant senses.
In Proceedings of SemEval, pages 314?317.
Rob Koeling, Diana McCarthy, and John Carroll.
2005. Domain-specific sense distributions and pre-
dominant sense acquisition. In Proceedings of HLT-
EMNLP, pages 419?426.
Linlin Li, Benjamin Roth, and Caroline Sporleder.
2010. Topic models for word sense disambiguation
and token-based idiom detection. In Proceedings of
ACL, pages 1138?1147.
Zhiyuan Liu, Wenyi Huang, Yabin Zheng, and
Maosong Sun. 2010. Automatic keyphrase extrac-
tion via topic decomposition. In Proceedings of
EMNLP, pages 366?376.
Zhiyuan Liu, Xinxiong Chen, and Maosong Sun.
2011a. A simple word trigger method for social tag
suggestion. In Proceedings of EMNLP, pages 1577?
1588.
Zhiyuan Liu, Xinxiong Chen, Yabin Zheng, and
Maosong Sun. 2011b. Automatic keyphrase extrac-
tion by bridging vocabulary gap. In Proceedings of
CoNLL, pages 135?144.
Zhiyuan Liu, Xinxiong Chen, and Maosong Sun. 2012.
Mining the interests of chinese microbloggers via
keyword extraction. Frontiers of Computer Science,
6(1):76?87.
Christopher D Manning, Prabhakar Raghavan, and
Hinrich Sch?utze. 2008. Introduction to information
retrieval. Cambridge University Press Cambridge.
Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey
Dean. 2013. Efficient estimation of word represen-
tations in vector space. Proceedings of ICLR.
Tomas Mikolov. 2012. Statistical Language Model-
s Based on Neural Networks. Ph.D. thesis, Ph. D.
thesis, Brno University of Technology.
1034
George A Miller, Claudia Leacock, Randee Tengi, and
Ross T Bunker. 1993. A semantic concordance. In
Proceedings of the workshop on HLT, pages 303?
308.
George A Miller. 1995. Wordnet: a lexical
database for english. Communications of the ACM,
38(11):39?41.
Andriy Mnih and Geoffrey E Hinton. 2008. A s-
calable hierarchical distributed language model. In
Proceedings of NIPS, pages 1081?1088.
Frederic Morin and Yoshua Bengio. 2005. Hierarchi-
cal probabilistic neural network language model. In
Proceedings of the international workshop on artifi-
cial intelligence and statistics, pages 246?252.
Roberto Navigli and Mirella Lapata. 2010. An ex-
perimental study of graph connectivity for unsu-
pervised word sense disambiguation. IEEE PAMI,
32(4):678?692.
Roberto Navigli and Paola Velardi. 2005. Structural
semantic interconnections: a knowledge-based ap-
proach to word sense disambiguation. IEEE PAMI,
27(7):1075?1086.
Roberto Navigli, Kenneth C Litkowski, and Orin Har-
graves. 2007. Semeval-2007 task 07: Coarse-
grained english all-words task. In Proceedings of
SemEval, pages 30?35.
Roberto Navigli. 2009. Word sense disambiguation: A
survey. CSUR, 41(2):10.
Arvind Neelakantan, Jeevan Shankar, Alexandre Pas-
sos, and Andrew McCallum. 2014. Efficient non-
parametric estimation of multiple embeddings per
word in vector space. In Proceedings of EMNLP.
Simone Paolo Ponzetto and Roberto Navigli. 2009.
Large-scale taxonomy mapping for restructuring
and integrating wikipedia. In Proceedings of IJCAI,
volume 9, pages 2083?2088.
Simone Paolo Ponzetto and Roberto Navigli. 2010.
Knowledge-rich word sense disambiguation rivaling
supervised systems. In Proceedings of ACL, pages
1522?1531.
Joseph Reisinger and Raymond J Mooney. 2010.
Multi-prototype vector-space models of word mean-
ing. In Proceedings of HLT-NAACL, pages 109?
117.
David E Rumelhart, Geoffrey E Hintont, and Ronald J
Williams. 1986. Learning representations by back-
propagating errors. Nature, 323(6088):533?536.
Fabrizio Sebastiani. 2002. Machine learning in auto-
mated text categorization. CSUR, 34(1):1?47.
Richard Socher, Cliff C Lin, Andrew Ng, and Chris
Manning. 2011. Parsing natural scenes and natural
language with recursive neural networks. In Pro-
ceedings of ICML, pages 129?136.
Richard Socher, John Bauer, Christopher D Manning,
and Andrew Y Ng. 2013. Parsing with composi-
tional vector grammars. In Proceedings of ACL.
Fabian M Suchanek, Gjergji Kasneci, and Gerhard
Weikum. 2008. Yago: A large ontology from
wikipedia and wordnet. Web Semantics: Sci-
ence, Services and Agents on the World Wide Web,
6(3):203?217.
Fei Tian, Hanjun Dai, Jiang Bian, Bin Gao, Rui Zhang,
Enhong Chen, and Tie-Yan Liu. 2014. A probabilis-
tic model for learning multi-prototype word embed-
dings. In Proceedings of COLING, pages 151?160.
Joseph Turian, Lev Ratinov, and Yoshua Bengio. 2010.
Word representations: a simple and general method
for semi-supervised learning. In Proceedings of A-
CL, pages 384?394.
1035
Proceedings of the ACL 2010 Student Research Workshop, pages 49?54,
Uppsala, Sweden, 13 July 2010. c?2010 Association for Computational Linguistics
Growing Related Words from Seed via User Behaviors: A Re-ranking 
Based Approach 
Yabin Zheng Zhiyuan Liu Lixing Xie 
State Key Laboratory on Intelligent Technology and Systems 
Tsinghua National Laboratory for Information Science and Technology 
Department of Computer Science and Technology, Tsinghua University, Beijing 100084,China 
{yabin.zheng, lzy.thu, lavender087}@gmail.com 
 
Abstract 
Motivated by Google Sets, we study the prob-
lem of growing related words from a single 
seed word by leveraging user behaviors hiding 
in user records of Chinese input method. Our 
proposed method is motivated by the observa-
tion that the more frequently two words co-
occur in user records, the more related they are. 
First, we utilize user behaviors to generate 
candidate words. Then, we utilize search en-
gine to enrich candidate words with adequate 
semantic features. Finally, we reorder candi-
date words according to their semantic rela-
tedness to the seed word. Experimental results 
on a Chinese input method dataset show that 
our method gains better performance. 
1 Introduction 
What is the relationship between ??????
?? (Natural Language Processing) and ????
? ? (Artificial Intelligence)? We may regard 
NLP as a research branch of AI. Problems arise 
when we want to find more words related to the 
input query/seed word. For example, if seed 
word ??????? ? (Natural Language 
Processing) is entered into Google Sets (Google, 
2010), Google Sets returns an ordered list of re-
lated words such as  ?????? (Artificial In-
telligence) and ????? (Computer). Generally 
speaking, it performs a large-scale clustering al-
gorithm that can gather related words. 
In this paper, we want to investigate the ad-
vantage of user behaviors and re-ranking frame-
work in related words retrieval task using Chi-
nese input method user records. We construct a 
User-Word bipartite graph to represent the in-
formation hiding in user records. The bipartite 
graph keeps users on one side and words on the 
other side. The underlying idea is that the more 
frequently two words co-occur in user records, 
the more related they are. For example, ????
?? (Machine Translation) is quite related to ??
???? (Chinese Word Segmentation) because 
the two words are usually used together by re-
searchers in natural language processing com-
munity. As a result, user behaviors offer a new 
perspective for measuring relatedness between 
words. On the other hand, we can also recom-
mend related words to users in order to enhance 
user experiences. Researchers are always willing 
to accept related terminologies in their research 
fields. 
However, the method is purely statistics based 
if we only consider co-occurrence aspect. We 
want to add semantic features. Sahami and Hel-
man (2006) utilize search engine to supply web 
queries with more semantic context and gains 
better results for query suggestion task. We bor-
row their idea in this paper. User behaviors pro-
vide statistic information to generate candidate 
words. Then, we can enrich candidate words 
with additional semantic features using search 
engine to retrieve more relevant candidates earli-
er. Statistical and semantic features can comple-
ment each other. Therefore, we can gain better 
performance if we consider them together. 
The contributions of this paper are threefold. 
First, we introduce user behaviors in related 
word retrieval task and construct a User-Word 
bipartite graph from user behaviors. Words are 
used by users, and it is reasonable to measure 
relatedness between words by analyzing user 
behaviors. Second, we take the advantage of se-
mantic features using search engine to reorder 
candidate words. We aim to return more relevant 
candidates earlier. Finally, our method is unsu-
pervised and language independent, which means 
that we do not require any training set or manual 
labeling efforts. 
The rest of the paper is organized as follows. 
Some related works are discussed in Section 2. 
Then we introduce our method for related words 
retrieval in Section 3. Experiment results and 
discussions are showed in Section 4. Finally, 
Section 5 concludes the whole paper and gives 
some future works. 
49
2 Related Work 
For related words retrieval task, Google Sets 
(Google, 2010) provides a remarkably interesting 
tool for finding words related to an input word. 
As stated in (Zheng et al, 2009), Google Sets 
performs poor results for input words in Chinese 
language. Bayesian Sets (Ghahramani and Heller, 
2006) offers an alternative method for related 
words retrieval under the framework of Bayesian 
inference. It computes a score for each candidate 
word by comparing the posterior probability of 
that word given the input, to the prior probability 
of that candidate word. Then, it returns a ranked 
list of candidate words according to their com-
puted scores. 
Recently, Zheng et al (2009) introduce user 
behaviors in new word detection task via a colla-
borative filtering manner. They extend their me-
thod to related word retrieval task. Moreover, 
they prove that user behaviors provide a new 
point for new word detection and related word 
retrieval tasks. However, their method is purely 
statistical method without considering semantic 
features. 
We can regard related word retrieval task as 
problem of measuring the semantic relatedness 
between pairs of very short texts. Sahami and 
Helman (2006) introduce a web kernel function 
for measuring semantic similarities using snip-
pets of search results. This work is followed by 
Metzler et al, (2007), Yih and Meek, (2007). 
They combine the web kernel with other metrics 
of similarity between word vectors, such as Jac-
card Coefficient and KL Divergence to enhance 
the result. 
In this paper, we follow the similar idea of us-
ing search engine to enrich semantic features of a 
query word. We regard the returned snippets as 
the context of a query word. And then we reorder 
candidate words and expect more relevant candi-
date words can be retrieved earlier. More details 
are given in Section 3. 
3 Related Words Retrieval 
In this section, we will introduce how to find 
related words from a single seed word via user 
behaviors and re-ranking framework. 
First, we introduce the dataset utilized in this 
paper. All the resource used in this paper comes 
from Sogou Chinese pinyin input method (Sogou, 
2006). We use Sogou for abbreviation hereafter. 
Users can install Sogou on their computers and 
the word lists they have used are kept in their 
user records. Volunteers are encouraged to upl-
oad their anonymous user records to the server 
side. In order to preserve user privacy, user-
names are hidden using MD5 hash algorithm. 
Then we demonstrate how to build a User-
Word bipartite graph based on the dataset. The 
construction can be accomplished while travers-
ing the dataset with linear time cost. We will 
give more details in Section 3.1. 
Second, we adopt conditional probability 
(Deshpande and Karypis, 2004) to measure the 
relatedness of two words. Intuitively, two words 
are supposed to be related if there are a lot of 
users who have used both of them. In other 
words, the two words always co-occur in user 
records. Starting from a single seed word, we can 
generate a set of candidate words. This is the 
candidate generation step. 
Third, in order to take the advantage of seman-
tic features, we carry out feature extraction tech-
niques to represent generated candidate words 
with enriched semantic context. In this paper, we 
generally make use of search engine to conduct 
the feature extraction step. After this step, input 
seed word and candidate words are represented 
as feature vectors in the vector space. 
Finally, we can reorder generated candidate 
words according to their semantic relatedness of 
the input seed word. We expect to retrieve more 
relevant candidate words earlier. We will make 
further explanations about the mentioned steps in 
the next subsections. 
3.1 Bipartite Graph Construction 
As stated before, we first construct a User-Word 
bipartite graph from the dataset. The bipartite 
graph has two layers, with users on one side and 
the words on the other side. We traverse the user 
records, and add a link between user u and word 
w if w appears in the user record of u. Thus this 
procedure can be accomplished in linear time. 
In order to give better explanations of bipartite 
graph construction step, we show some user 
records in Figure 1 and the corresponding bipar-
tite graph in Figure 2. 
 
 
Fig. 1. User Records Sample 
User1 Word1 ????(Natural Language) 
Word2????(Artificial Intelligence) 
Word3 ????(Machine Translation) 
Word2????(Artificial Intelligence) 
Word4 ????(Information Retrieval) 
Word3 ????(Machine Translation) 
Word1 ????(Natural Language) 
 
User2 
User3 
50
 
Fig. 2. Corresponding Bipartite Graph 
 
From Figure 1, we can see that Word1 and 
Word2 appear in User1?s record, which indicates 
that User1 has used Word1 and Word2. As a result, 
in Figure 2, node User1 is linked with node 
Word1 and Word2. The rest can be done in the 
same manner. 
3.2 Candidates Generation 
After the construction of bipartite graph, we can 
measure the relatedness of words from the bipar-
tite graph. Intuitively, if two words always co-
occur in user records, they are related to each 
other. Inspired by (Deshpande and Karypis, 
2004), we adopt conditional probability to meas-
ure the relatedness of two words. 
In particular, the conditional probability of 
word j occurs given that word i has already ap-
peared is the number of users that used both 
word i and word j divided by the total number of 
users that used word i. 
 
( )( | )         (1)( )
Freq ijP j i Freq i?
 
 
In formula 1, Freq(X) is the number of users 
that have used words in the set X. We can clearly 
see that P(j|i) ? P(i|j), which means that condi-
tional probability leads to asymmetric relations. 
The disadvantage is that each word i tends to 
have a close relationship with stop words that are 
used quite frequently in user records, such as 
??? (of) and ???? (a). 
In order to alleviate this problem, we consider 
the conditional probabilities P(j|i) and P(i|j) to-
gether. Word i and word j is said to be quite re-
lated if conditional probabilities P(j|i) and P(i|j) 
are both relatively high. We borrow the idea pro-
posed in (Li and Sun, 2007). In their paper, a 
weighted harmonic averaging is used to define 
the relatedness score between word i and word j 
because either P(j|i) or P(i|j) being too small is a 
severe detriment. 
 
11( , )    (2)( | ) ( | )Score i j P i j P j i
? ? ?? ??? ?? ?? ?
 
In formula 2, parameter [0,1]? ?  is the weight 
for P(i|j), which denotes how much P(i|j) should 
be emphasized. We carry out some comparative 
experiments when parameter ? varies from 0 to 1 
stepped by 0.1. We also tried other co-
occurrence based measures like mutual informa-
tion, Euclidean and Jaccard distance, and found 
that weight harmonic averaging gives relatively 
better results. Due to space limitation, we are not 
able to report detailed results. 
So far, we have introduced how to calculate 
the relatedness Score(i, j) between word i and 
word j. When a user enters an input seed word w, 
we can compute Score(w,c) between seed word 
w and each candidate word c, and then sort can-
didate words in a descending order. Top N can-
didate words are kept for re-ranking, we aim to 
reorder top N candidate words and return the 
more related candidate words earlier. Alterna-
tively, we can also set a threshold for Score(w,c), 
which keeps the candidate word c with Score(w,c) 
larger than the threshold. We argue that this thre-
shold is difficult to set because different seed 
words have different score thresholds. 
Note that this candidate generation step is 
completely statistical method as we only consid-
er the co-occurrence of words. We argue that 
semantic features can be a complement of statis-
tical method. 
3.3 Semantic Feature Representation and 
Re-ranking 
As stated before, we utilize search engine to 
enrich semantic features of the input seed word 
and top N candidate words. To be more specific, 
we issue a word to a search engine (Sogou, 2004) 
and get top 20 returned snippets. We regard 
snippets as the context and the semantic repre-
sentation of this word. 
For an input seed word w, we can generate top 
N candidate words using formula (2). We issue 
each word to search engine and get returned 
snippets. Then, each word is represented as a 
feature vector using bag-of-words model. Fol-
lowing the conventional approach, we calculate 
the relatedness between the input seed word w 
and a candidate word c as the cosine similarity 
between their feature vectors. Intuitively, if we 
introduce more candidate words, we are more 
likely to find related words in the candidate sets. 
However, noisy words are inevitably included. 
We will show how to tune parameter N in the 
experiment part. 
W1 
U1 
U2 
U3 
W2 
W3 
W4 
51
As a result, candidate words with higher se-
mantic similarities can be returned earlier with 
enriched semantic features. Re-ranking can be 
regarded as a complementary step after candidate 
generation. We can improve the performance of 
related word retrieval task if we consider user 
behaviors and re-ranking together. 
4 Experiment 
In this section, we demonstrate our experiment 
results. First, we introduce the dataset used in 
this paper and some statistics of the dataset. Then, 
we build our ground truth for related word re-
trieval task using Baidu encyclopedia. Third, we 
give some example of related word retrieval task. 
We show that more related words can be re-
turned earlier if we consider semantic features. 
Finally, we make further analysis of the parame-
ter tuning mentioned before. 
4.1 Experiment Settings 
We carry out our experiment on Sogou Chinese 
input method dataset. The dataset contains 
10,000 users and 183,870 words, and the number 
of edges in the constructed bipartite graph is 
42,250,718. As we can see, the dataset is quite 
sparse, because most of the users tend to use only 
a small number of words. 
For related word retrieval task, we need to 
judge whether a candidate word is related to the 
input seed word. We can ask domain experts to 
answer this question. However, it needs a lot of 
manual efforts. To alleviate this problem, we 
adopt Baidu encyclopedia (Baidu, 2006) as our 
ground truth. In Baidu encyclopedia, volunteers 
give a set of words that are related to the particu-
lar seed word. As related words are provided by 
human, we are confident enough to use them as 
our ground truth. 
We randomly select 2,000 seed words as our 
validation set. However, whether two words are 
related is quite subjective. In this paper, Baidu 
encyclopedia is only used as a relatively accurate 
standard for evaluation. We just want to investi-
gate whether user behaviors and re-ranking 
framework is helpful in the related word retrieval 
task under various evaluation metrics. 
We give a simple example of our method in 
Table 1. The input seed word is ?????? 
(Machine Learning). Generally speaking, all 
these returned candidate words are relevant to 
the seed word to certain degree, which indicates 
the effectiveness of our method. 
 
????(feature vector) ???(kernel function) 
???(training set) ???(decision tree) 
???(classifier) ???(test set) 
??(dimension reduc-
tion) 
????(feature ex-
traction) 
Table 1. Words Related to ?Machine Learning? 
4.2 Evaluation Metrics 
In this paper, we use three evaluation metrics to 
validate the performance of our method: 
1. Precision@N (P@N). P@N measures how 
much percent of the topmost results returned 
are correct. We consider P@5 and P@10. 
2. Binary preference measure (Bpref) (Buck-
ley and Voorhees, 2004). As we cannot list 
all the related words of an input seed word, 
we use Bpref to evaluate our method. For an 
input seed word with R judged candidate 
words where r is a related word and n is a 
nonrelated word. Bpref is defined as follow: 
 
1 |     |1     (3)
r
n ranked higher than rBpref R R? ??
 
3. Mean reciprocal rank of the first retrieved 
result (MRR). For a sample of input seed 
words W, ranki is the rank of the first related 
candidate word for the input seed word wi, 
MRR is the average of the reciprocal ranks 
of results, which is defined as follow: 
 
1 1      (4) i iMRR W rank? ?
 
4.3 Candidate Re-ranking 
In order to show the effectiveness of semantic 
features and re-ranking framework, we give an 
example in Table 2. The input seed word is ??
??? (Ericsson), and if we only take user beha-
viors into consideration, top 5 words returned are 
shown on the left side. After using search engine 
and semantic representation, we reorder the can-
didate words as shown on the right side. 
 
Input Seed Word: ??? (Ericsson) 
Top 5 Candidates After Re-ranking 
?? (Nortel) ????? (Sony 
Ericsson) 
?? (ZTE Corporation) ?? (Sony Ericsson) 
?? (Base Station) ???? (Alcatel) 
???? (Alcatel) ?? (Sony) 
??? (Core Network) ?? (Huawei) 
Table 2. Candidate Re-ranking 
52
As shown in Table 2, we can clearly see that 
we return the most related candidate words such 
as ??????? (Sony Ericsson) and ???? 
(the abbreviation of Sony Ericsson in Chinese) in 
the first two places. Moreover, after re-ranking, 
top candidate words are some famous brands that 
are quite related to query word ????? (Erics-
son). Some words like ????? (Core Network) 
that are not quite related to the query word are 
removed from the top list. From this observation, 
we can see that semantic features and re-ranking 
framework can improve the performance. 
4.4 Parameter Tuning 
As discussed in Section 3, we have introduced 
two parameters in this paper. The first is the pa-
rameter ? in the candidate generation step, and 
the other is the parameter N in the re-ranking 
step. We show how these two parameters affect 
the performance. In addition, we should emphas-
ize that the ground truth is not a complete answer, 
so all the results are only useful for comparisons. 
The absolute value is not very meaningful. 
As we have shown in Section 3.2, parameter ? 
adjusts the weight of conditional probability be-
tween two word i, j. The parameter ? is varied 
from 0 to 1 stepped by 0.1. We record the cor-
responding values of P@5, P@10, Bpref and 
MRR. The results are shown in Figure 3. 
We can clearly see that all the values increase 
when ? increases first. And then all the values 
decrease dramatically when ? is close to 1. This 
indicates that either P(j|i) or P(i|j) being too 
small is a severe detriment. The result reaches 
peak value when ?=0.5, i.e. we should treat P(j|i) 
and P(i|j)equally to get the best result. Therefore, 
we use ?=0.5 to generate candidate words, those 
candidates are used for re-ranking. 
 
 
Fig. 3. Parameter ? for Candidate Generation 
 
We also carry out the comparisons with Baye-
sian Sets, which is shown in Table 3. It is clear 
that our method gains better results than Baye-
sian Sets with different values of parameter ?. 
Results of Google Sets are omitted here because 
Zheng et al (2009) have already showed that 
Google Sets performs worse than Bayesian Sets 
with query words in Chinese. 
 
 Bpref MRR P@5 P@10 
? = 0.4 0.2057 0.4267 0.2352 0.195 
? = 0.5 0.2035 0.4322 0.2414 0.2019 
? = 0.6 0.2038 0.4292 0.2408 0.2009 
Bayesian Sets 0.2033 0.3291 0.1842 0.1512 
Table 3. Comparisons with Bayesian Sets 
 
To investigate the effectiveness of re-ranking 
framework, we also conduct experiments on the 
parameter N that is used for re-ranking. The ex-
perimental results are shown in Figure 4. 
 
 
Fig. 4. Top N Candidates for Re-ranking 
 
We can observe that more candidates tend to 
harm the performance as noisy words are intro-
duced inevitably. For example, Bpref drops to 
less than 0.25 when N = 100. More comparative 
results are shown in Table 4. We can see that N = 
20 gives relatively best results, which indicates 
that we should select Top 20 candidate words for 
re-ranking. 
 
 Bpref MRR P@5 P@10 
Non Re-ranking 0.2035 0.4322 0.2414 0.2019 
N = 10 0.3208 0.456 0.2752 0.2019 
N = 20 0.3047 0.4511 0.2769 0.2301 
N = 30 0.2899 0.4444 0.272 0.2305 
Table 4. Comparisons with Re-ranking Method 
5 Conclusions and Future Work 
In this paper, we have proposed a novel method 
for related word retrieval task. Different from 
other method, we consider user behaviors, se-
mantic features and re-ranking framework to-
gether. We make a reasonable assumption that if 
two words always co-occur in user records, then 
53
they tend to have a close relationship with each 
other. Based on this assumption, we first gener-
ate a set of candidate words that are related to an 
input seed word via user behaviors. Second, we 
utilize search engine to enrich candidates with 
semantic features. Finally, we can reorder the 
candidate words to return more related candi-
dates earlier. Experiment results show that our 
method is effective and gains better results. 
However, we also observed some noisy words 
in the returned results. As our dataset is generat-
ed from Chinese input method, users can type 
whatever they want, which will bring some noise 
in the dataset. We plan to remove noisy words in 
the future. Furthermore, we want to take the ad-
vantage of learning to rank literature (Liu, 2009) 
to further improve the performance of related 
word retrieval task. We may need to extract more 
features to represent the word pairs and build a 
labeled training set. Then various machine learn-
ing techniques can be used in this task. 
Another important issue is how to build a 
complete and accurate ground truth for related 
word retrieval task. People may have different 
opinions about whether two words are related or 
not, which makes this problem complicate. 
Thirdly, our method can only process a single 
seed word, so we aim to extend our method to 
process multiple seed words. In addition, we 
want to build a network of Chinese word associa-
tion. We can discover how words are organized 
and connected within this network. And this 
word association network will be quite useful for 
foreigners to learn Chinese. 
Fourthly, how to deal with ambiguous query 
word is also left as our future work. For example, 
query word ?apple? can refer to a kind of fruit or 
an IT company. As a result, we are expected to 
return two groups of related words instead of 
mixing them together. 
Finally, our dataset provides a new perspective 
for many interesting research tasks like new 
word detection, social network analysis, user be-
havior analysis, and so on. We are trying to re-
lease our dataset for research use in the future. 
Acknowledgement 
We thank Xiance Si and Wufeng Ke for provid-
ing the Baidu encyclopedia corpus for evaluation. 
We also thank the anonymous reviewers for their 
helpful comments and suggestions. This work is 
supported by a Tsinghua-Sogou joint research 
project. 
References 
Baidu. 2006. Baidu Encyclopedia. Available at 
http://baike.baidu.com 
Chris Buckley and Ellen M. Voorhees. 2004. Retriev-
al Evaluation with Incomplete Information. In Pro-
ceedings of the 27th annual international ACM 
SIGIR conference on Research and development in 
information retrieval, pp 25-32  
Mukund Deshpande and George Karypis. 2004. Item-
Based Top-N Recommendation Algorithms, ACM 
Trans. Information Systems, 22(1): 143-177 
Zoubin Ghahramani and Katherine A. Heller. 2005. 
Bayesian Sets. In Advances in Neural Information 
Processing Systems 
Google. Google Sets. Accessed on Feb. 9th, 2010, 
available at: http://labs.google.com/sets 
Jingyang Li and Maosong Sun. 2007. Scalable term 
selection for text categorization, In Proceedings of 
the 2007 Joint Conference on Empirical Methods 
in Natural Language Processing and Computa-
tional Natural Language Learning, pp. 774-782 
Tie-Yan Liu. 2009. Learning to Rank for Information 
Retrieval, Foundation and Trends on Information 
Retrieval, Now Publishers 
Donald Metzler, Susan T. Dumais, and Christopher 
Meek. 2007. Similarity measures for short seg-
ments of text. In Proceeding of the 29th European 
Conference on Information Retrieval, pp 16-27  
Mehran Sahami and Timothy D. Heilman. 2006. A 
web-based kernel function for measuring the simi-
larity of short text snippets. In Proceedings of the 
15th International Conference on World Wide Web, 
pp 377-386 
Sogou. 2006. Sogou Chinese Pinyin Input Method. 
Available at http://pinyin.sogou.com/ 
Sogou. 2004. Sogou Search Engine. Available at 
http://www.sogou.com  
Wen-Tau Yih and Christopher Meek. 2007. Improv-
ing similarity measures for short segments of text. 
In Proceedings of AAAI 2007, pp 1489-1494 
Yabin Zheng, Zhiyuan Liu, Maosong Sun, Liyun Ru, 
and Yang Zhang. 2009. Incorporating User Beha-
viors in New Word Detection. In Proceedings of 
the Twenty-First International Joint Conference on 
Artificial Intelligence, pp 2101-2106 
54
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:shortpapers, pages 485?490,
Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational Linguistics
Why Press Backspace? Understanding User Input Behaviors in Chinese
Pinyin Input Method
Yabin Zheng1, Lixing Xie1, Zhiyuan Liu1, Maosong Sun1, Yang Zhang2, Liyun Ru1,2
1State Key Laboratory of Intelligent Technology and Systems
Tsinghua National Laboratory for Information Science and Technology
Department of Computer Science and Technology
Tsinghua University, Beijing 100084, China
2Sogou Inc., Beijing 100084, China
{yabin.zheng,lavender087,lzy.thu,sunmaosong}@gmail.com
{zhangyang,ruliyun}@sogou-inc.com
Abstract
Chinese Pinyin input method is very impor-
tant for Chinese language information pro-
cessing. Users may make errors when they
are typing in Chinese words. In this paper, we
are concerned with the reasons that cause the
errors. Inspired by the observation that press-
ing backspace is one of the most common us-
er behaviors to modify the errors, we collect
54, 309, 334 error-correction pairs from a real-
world data set that contains 2, 277, 786 user-
s via backspace operations. In addition, we
present a comparative analysis of the data to
achieve a better understanding of users? input
behaviors. Comparisons with English typos
suggest that some language-specific properties
result in a part of Chinese input errors.
1 Introduction
Unlike western languages, Chinese is unique due
to its logographic writing system. Chinese users
cannot directly type in Chinese words using a QW-
ERTY keyboard. Pinyin is the official system to
transcribe Chinese characters into the Latin alpha-
bet. Based on this transcription system, Pinyin input
methods have been proposed to assist users to type
in Chinese words (Chen, 1997).
The typical way to type in Chinese words is
in a sequential manner (Wang et al, 2001). As-
sume users want to type in the Chinese word ??
?(what)?. First, they mentally generate and type
in corresponding Pinyin ?shenme?. Then, a Chinese
Pinyin input method displays a list of Chinese words
which share that Pinyin, as shown in Fig. 1. Users
Figure 1: Typical Chinese Pinyin input method for a
correct Pinyin (Sogou-Pinyin).
Figure 2: Typical Chinese Pinyin input method for a
mistyped Pinyin (Sogou-Pinyin).
visually search the target word from candidates and
select numeric key ?1? to get the result. The last t-
wo steps do not exist in typing process of English
words, which indicates that it is more complicated
for Chinese users to type in Chinese words.
Chinese users may make errors when they are typ-
ing in Chinese words. As shown in Fig. 2, a user
may mistype ?shenme? as ?shenem?. Typical Chi-
nese Pinyin input method can not return the right
word. Users may not realize that an error occurs and
select the first candidate word ????? (a mean-
ingless word) as the result. This greatly limits us-
er experience since users have to identify errors and
modify them, or cannot get the right word.
In this paper, we analyze the reasons that cause
errors in Chinese Pinyin input method. This analy-
sis is helpful in enhancing the user experience and
the performance of Chinese Pinyin input method. In
practice, users press backspace on the keyboard to
modify the errors, they delete the mistyped word and
re-type in the correct word. Motivated by this ob-
485
servation, we can extract error-correction pairs from
backspace operations. These error-correction pairs
are of great importance in Chinese spelling correc-
tion task which generally relies on sets of confusing
words.
We extract 54, 309, 334 error-correction pairs
from user input behaviors and further study them.
Our comparative analysis of Chinese and English ty-
pos suggests that some language-specific properties
of Chinese lead to a part of input errors. To the best
of our knowledge, this paper is the first one which
analyzes user input behaviors in Chinese Pinyin in-
put method.
The rest of this paper is organized as follows.
Section 2 discusses related works. Section 3 intro-
duces how we collect errors in Chinese Pinyin input
method. In Section 4, we investigate the reasons that
result in these errors. Section 5 concludes the whole
paper and discusses future work.
2 Previous Work
For English spelling correction (Kukich, 1992;
Ahmad and Kondrak, 2005; Chen et al, 2007;
Whitelaw et al, 2009; Gao et al, 2010), most ap-
proaches make use of a lexicon which contains a list
of well-spelled words (Hirst and Budanitsky, 2005;
Islam and Inkpen, 2009). Context features (Ro-
zovskaya and Roth, 2010) of words provide useful
evidences for spelling correction. These features
are usually represented by an n-gram language mod-
el (Cucerzan and Brill, 2004; Wilcox-O?Hearn et
al., 2010). Phonetic features (Toutanova and Moore,
2002; Atkinson, 2008) are proved to be useful in En-
glish spelling correction. A spelling correction sys-
tem is trained using these features by a noisy channel
model (Kernighan et al, 1990; Ristad et al, 1998;
Brill and Moore, 2000).
Chang (1994) first proposes a representative ap-
proach for Chinese spelling correction, which re-
lies on sets of confusing characters. Zhang et al
(2000) propose an approximate word-matching al-
gorithm for Chinese to solve Chinese spell detec-
tion and correction task. Zhang et al (1999) present
a winnow-based approach for Chinese spelling cor-
rection which takes both local language features and
wide-scope semantic features into account. Lin and
Yu (2004) use Chinese frequent strings and report
an accuracy of 87.32%. Liu et al (2009) show that
about 80% of the errors are related to pronunciation-
s. Visual and phonological features are used in Chi-
nese spelling correction (Liu et al, 2010).
Instead of proposing a method for spelling cor-
rection, we mainly investigate the reasons that cause
typing errors in both English and Chinese. Some
errors are caused by specific properties in Chinese
such as the phonetic difference between Mandarin
and dialects spoken in southern China. Meanwhile,
confusion sets of Chinese words play an importan-
t role in Chinese spelling correction. We extract a
large scale of error-correction pairs from real user
input behaviors. These pairs contain important ev-
idence about confusing Pinyins and Chinese words
which are helpful in Chinese spelling correction.
3 User Input Behaviors Analysis
We analyze user input behaviors from anonymous
user typing records in a Chinese input method. Data
set used in this paper is extracted from Sogou Chi-
nese Pinyin input method1. It contains 2, 277, 786
users? typing records in 15 days. The numbers of
Chinese words and characters are 3, 042, 637, 537
and 5, 083, 231, 392, respectively. We show some
user typing records in Fig. 3.
[20100718 11:10:38.790ms] select:2 zhe? WINWORD.exe 
[20100718 11:10:39.770ms] select:1 shi? WINWORD.exe 
[20100718 11:10:40.950ms] select:1 shenem??? WINWORD.exe 
[20100718 11:10:42.300ms] Backspace WINWORD.exe 
[20100718 11:10:42.520ms] Backspace WINWORD.exe 
[20100718 11:10:42.800ms] Backspace WINWORD.exe 
[20100718 11:10:45.090ms] select:1 shenme ?? WINWORD.exe 
 
Figure 3: Backspace in user typing records.
From Fig. 3, we can see the typing process of a
Chinese sentence ?????? (What is this). Each
line represents an input segment or a backspace op-
eration. For example, word ???? (what) is type-
d in using Pinyin ?shenme? with numeric selection
?1? at 11:10am in Microsoft Word application.
The user made a mistake to type in the third
Pinyin (?shenme? is mistyped as ?shenem?). Then,
he/she pressed the backspace to modify the errors
he has made. the word ????? is deleted and re-
placed with the correct word ???? using Pinyin
1Sogou Chinese Pinyin input method, can be accessed from
http://pinyin.sogou.com/
486
?shenme?. As a result, we compare the typed-
in Pinyins before and after backspace operations.
We can find the Pinyin-correction pairs ?shenem-
shenme?, since their edit distance is less than a
threshold. Threshold is set to 2 in this paper, as
Damerau (1964) shows that about 80% of typos are
caused by a single edit operation. Therefore, using a
threshold of 2, we should be able to find most of the
typos. Furthermore, we can extract corresponding
Chinese word-correction pairs ????-??? from
this typing record.
Using heuristic rules discussed above, we extrac-
t 54, 309, 334 Pinyin-correction and Chinese word-
correction pairs. We list some examples of extracted
Pinyin-correction and Chinese word-correction pairs
in Table 1. Most of the mistyped Chinese words are
meaningless.
Pinyin-correction Chinese word-correction
shenem-shenme ???-??(what)
dianao-diannao ??-??(computer)
xieixe-xiexie ????-??(thanks)
laing-liang ???-?(two)
ganam-ganma ???-??(what?s up)
zhdiao-zhidao ??-??(know)
lainxi-lianxi ???-??(contact)
zneme-zenme ???-??(how)
dainhua-dianhua ???-??(phone)
huiali-huilai ???-??(return)
Table 1: Typical Pinyin-correction and Chinese
word-correction pairs.
We want to evaluate the precision and recall of
our extraction method. For precision aspect, we ran-
domly select 1, 000 pairs and ask five native speak-
ers to annotate them as correct or wrong. Annota-
tion results show that the precision of our method is
about 75.8%. Some correct Pinyins are labeled as
errors because we only take edit distance into con-
sideration. We should consider context features as
well, which will be left as our future work.
We choose 15 typical mistyped Pinyins to evalu-
ate the recall of our method. The total occurrences
of these mistyped Pinyins are 259, 051. We success-
fully retrieve 144, 020 of them, which indicates the
recall of our method is about 55.6%. Some errors
are not found because sometimes users do not modi-
fy the errors, especially when they are using Chinese
input method under instant messenger softwares.
4 Comparisons of Pinyin typos and
English Typos
In this section, we compare the Pinyin typos and En-
glish typos. As shown in (Cooper, 1983), typing er-
rors can be classified into four categories: deletions,
insertions, substitutions, and transpositions. We aim
at studying the reasons that result in these four kinds
of typing errors in Chinese Pinyin and English, re-
spectively.
For English typos, we generate mistyped word-
correction pairs from Wikipedia2 and SpellGood.3,
which contain 4, 206 and 10, 084 common mis-
spellings in English, respectively. As shown in Ta-
ble 2, we reach the first conclusion: about half
of the typing errors in Pinyin and English are
caused by deletions, which indicates that users are
more possible to omit some letters than other three
edit operations.
Deletions Insertions Substitutions Transpositions
Pinyin 47.06% 28.17% 19.04% 7.46%
English 43.38% 18.89% 17.32% 18.70%
Table 2: Different errors in Pinyin and English.
Table 3 and Table 4 list Top 5 letters that produce
deletion errors (users forget to type in some letters)
and insertion errors (users type in extra letters) in
Pinyin and English.
Pinyin Examples English Examples
i xianza-xianzai e achive-achieve
g yingai-yinggai i abilties-abilities
e shenm-shenme c acomplish-accomplish
u pengyo-pengyou a agin-again
h senme-shenme t admited-admitted
Table 3: Deletion errors in Pinyin and English.
Pinyin Examples English Examples
g yingwei-yinwei e analogeous-analogous
i tiebie-tebie r arround-around
a xiahuan-xihuan s asside-aside
o huijiao-huijia i aisian-asian
h shuibian-suibian n abandonned-abandoned
Table 4: Insertion errors in Pinyin and English.
2http://en.wikipedia.org/wiki/Wikipedia:
Lists_of_common_misspellings/For_machines
3http://www.spellgood.net/
487
We can see from Table 3 and Table 4 that: (1)
vowels (a, o, e, i, u) are deleted or inserted more fre-
quently than consonants in Pinyin. (2) some specific
properties in Chinese lead to insertion and deletion
errors. Many users in southern China cannot dis-
tinguish the front and the back nasal sound (?ang? -
?an?, ?ing? - ?in?, ?eng? - ?en?) as well as the retroflex
and the blade-alveolar (?zh? - ?z?, ?sh? - ?s?, ?ch? -
?c?). They are confused about whether they should
add letter ?g? or ?h? under these situations. (3) the
same letters can occur continuously in English, such
as ?acomplish-accomplish? and ?admited-admitted?
in our examples. English users sometimes make in-
sertion or deletion errors in these cases. We also
observe this kind of errors in Chinese Pinyin, such
as ?yingai-yinggai?, ?liange-liangge? and ?dianao-
diannao?.
For transposition errors, Table 5 lists Top 10 pat-
terns that produce transposition errors in Pinyin and
English. Our running example ?shenem-shenme?
belongs to this kind of errors. We classify the let-
ters of the keyboard into two categories, i.e. ?left?
and ?right?, according to their positions on the key-
board. Letter ?e? is controlled by left hand while ?m?
is controlled by right hand. Users mistype ?shenme?
as ?shenem? because they mistake the typing order
of ?m? and ?e?.
Fig. 4 is a graphic representation, in which we add
a link between ?m? and ?e?. The rest patterns in Ta-
ble 5 can be done in the same manner. Interestingly,
from Fig. 4, we reach the second conclusion: most
of the transposition errors are caused by mistak-
ing the typing orders across left and right hands.
For instance, users intend to type in a letter (?m?)
controlled by right hand. But they type in a letter
(?e?) controlled by left hand instead.
Pinyin Examples English Examples
ai xaing-xiang ei acheive-achieve
na xinag-xiang ra clera-clear
em shenem-shenme re vrey-very
ia xianzia-xianzai na wnat-want
ne zneme-zenme ie hieght-height
oa zhidoa-zhidao er befoer-before
ei jiejei-jiejie it esitmated-estimated
hs haihsi-haishi ne scinece-science
ah sahng-shang el littel-little
ou rugou-ruguo si epsiode-episode
Table 5: Transpositions errors in Pinyin and English.
Letters Controlled 
by Left Hand
Letters Controlled 
by Right Hand
r a
e
s
t
i
n
m
o
h
l
u
Figure 4: Transpositions errors on the keyboard.
For substitution errors, we study the reason why
users mistype one letter for another. In the Pinyin-
correction pairs, users always mistype ?a? as ?e? and
vice versa. The reason is that they have similar pro-
nunciations in Chinese. As a result, we add two di-
rected edges ?a? and ?e? in Fig. 5. Some letters are
mistyped for each other because they are adjacent
on the keyboard although they do not share similar
pronunciations, such as ?g? and ?f?.
We summarize the substitution errors in English
in Fig. 6. Letters ?q?, ?k? and ?c? are often mixed up
with each other because they sound alike in English
although they are apart on the keyboard. However,
the three letters are not connected in Fig. 5, which
indicates that users can easily distinguish them in
Pinyin.
Figure 5: Substitutions errors in Pinyin.
488
Figure 6: Substitutions errors in English.
Mistyped
letter
pairs
Similar
pronunciations
in Chinese
Similar
pronunciations
in English
Adjacent
on
keyboard
(m,n) X X X
(b,p);(d,t) X X ?
(z,c,s);(g,k,h) X ? X
(j,q,x);(u,v) X ? ?
(i,y) ? X X
(q,k,c) ? X ?
(j,h);(z,x) ? ? X
Table 6: Pronunciation properties and keyboard dis-
tance in Chinese Pinyin and English
We list some examples in Table 6. For example,
letters ?m? and ?n? have similar pronunciations in
both Chinese and English. Moreover, they are adja-
cent on the keyboard, which leads to interferences or
confusion in both Chinese and English. Letters ?j?,
?q? and ?x? are far from each other on the keyboard.
But they sound alike in Chinese, which makes them
connected in Fig. 5. In Fig. 6, letters ?b? and ?p?
are connected to each other because they have simi-
lar pronunciations in English, although they are not
adjacent on the keyboard.
Finally, we summarize the third conclusion: sub-
stitution errors are caused by language specific
similarities (similar pronunciations) or keyboard
neighborhood (adjacent on the keyboard).
All in all, we generally classify typing errors in
English and Chinese into four categories and investi-
gate the reasons that result in these errors respective-
ly. Some language specific properties, such as pro-
nunciations in English and Chinese, lead to substitu-
tion, insertion and deletion errors. Keyboard layouts
play an important role in transposition errors, which
are language-independent.
5 Conclusions and Future Works
In this paper, we study user input behaviors in Chi-
nese Pinyin input method from backspace opera-
tions. We aim at analyzing the reasons that cause
these errors. Users signal that they are very likely
to make errors if they press backspace on the key-
board. Then they modify the errors and type in the
correct words they want. Different from the previous
research, we extract abundant Pinyin-correction and
Chinese word-correction pairs from backspace op-
erations. Compared with English typos, we observe
some language-specific properties in Chinese have
impact on errors. All in all, user behaviors (Zheng
et al, 2009; Zheng et al, 2010; Zheng et al, 2011b)
in Chinese Pinyin input method provide novel per-
spectives for natural language processing tasks.
Below we sketch three possible directions for the
future work: (1) we should consider position fea-
tures in analyzing Pinyin errors. For example, it is
less likely that users make errors in the first letter
of an input Pinyin. (2) we aim at designing a self-
adaptive input method that provide error-tolerant
features (Chen and Lee, 2000; Zheng et al, 2011a).
(3) we want to build a Chinese spelling correction
system based on extracted error-correction pairs.
Acknowledgments
This work is supported by a Tsinghua-Sogou join-
t research project and the National Natural Science
Foundation of China under Grant No. 60873174.
References
F. Ahmad and G. Kondrak. 2005. Learning a spelling
error model from search query logs. In Proceedings of
the conference on Human Language Technology and
Empirical Methods in Natural Language Processing,
pages 955?962.
K. Atkinson. 2008. Gnu aspell 0.60.6.
http://aspell.sourceforge.net.
E. Brill and R.C. Moore. 2000. An improved error model
for noisy channel spelling correction. In Proceedings
of the 38th Annual Meeting on Association for Com-
putational Linguistics, pages 286?293.
C.H. Chang. 1994. A pilot study on automatic Chinese
spelling error correction. Communication of COLIPS,
4(2):143?149.
Z. Chen and K.F. Lee. 2000. A new statistical ap-
proach to Chinese Pinyin input. In Proceedings of the
489
38th Annual Meeting on Association for Computation-
al Linguistics, pages 241?247.
Q. Chen, M. Li, and M. Zhou. 2007. Improving query
spelling correction using web search results. In Pro-
ceedings of the 2007 Joint Conference on Empirical
Methods in Natural Language Processing and Compu-
tational Natural Language Learning, pages 181?189.
Y. Chen. 1997. Chinese Language Processing. Shanghai
Education publishing company.
W.E. Cooper. 1983. Cognitive aspects of skilled type-
writing. Springer-Verlag.
S. Cucerzan and E. Brill. 2004. Spelling correction as an
iterative process that exploits the collective knowledge
of web users. In Proceedings of the 2004 Conference
on Empirical Methods in Natural Language Process-
ing, pages 293?300.
F.J. Damerau. 1964. A technique for computer detection
and correction of spelling errors. Communications of
the ACM, 7(3):171?176.
J. Gao, X. Li, D. Micol, C. Quirk, and X. Sun. 2010.
A large scale ranker-based system for search query
spelling correction. In Proceedings of the 23rd In-
ternational Conference on Computational Linguistics,
pages 358?366.
G. Hirst and A. Budanitsky. 2005. Correcting real-word
spelling errors by restoring lexical cohesion. Natural
Language Engineering, 11(01):87?111.
A. Islam and D. Inkpen. 2009. Real-word spelling cor-
rection using GoogleWeb 1T 3-grams. In Proceedings
of the 2009 Conference on Empirical Methods in Nat-
ural Language Processing, pages 1241?1249.
M.D. Kernighan, K.W. Church, and W.A. Gale. 1990.
A spelling correction program based on a noisy chan-
nel model. In Proceedings of the 13th conference on
Computational linguistics, pages 205?210.
K. Kukich. 1992. Techniques for automatically cor-
recting words in text. ACM Computing Surveys,
24(4):377?439.
Y.J. Lin and M.S. Yu. 2004. The properties and further
applications of Chinese frequent strings. Computa-
tional Linguistics and Chinese Language Processing,
9(1):113?128.
C.L. Liu, K.W. Tien, M.H. Lai, Y.H. Chuang, and S.H.
Wu. 2009. Capturing errors in written Chinese word-
s. In Proceedings of the Joint Conference of the 47th
Annual Meeting of the ACL and the 4th International
Joint Conference on Natural Language Processing of
the AFNLP, pages 25?28.
C.L. Liu, M.H. Lai, Y.H. Chuang, and C.Y. Lee. 2010.
Visually and phonologically similar characters in in-
correct simplified chinese words. In Proceedings of
the 23rd International Conference on Computational
Linguistics, pages 739?747.
E.S. Ristad, P.N. Yianilos, M.T. Inc, and NJ Princeton.
1998. Learning string-edit distance. IEEE Transac-
tions on Pattern Analysis and Machine Intelligence,
20(5):522?532.
A. Rozovskaya and D. Roth. 2010. Generating confu-
sion sets for context-sensitive error correction. In Pro-
ceedings of the Conference on Empirical Methods in
Natural Language Processing, pages 961?970.
K. Toutanova and R.C. Moore. 2002. Pronunciation
modeling for improved spelling correction. In Pro-
ceedings of the 40th Annual Meeting on Association
for Computational Linguistics, pages 144?151.
J. Wang, S. Zhai, and H. Su. 2001. Chinese input with
keyboard and eye-tracking: an anatomical study. In
Proceedings of the SIGCHI conference on Human fac-
tors in computing systems, pages 349?356.
C. Whitelaw, B. Hutchinson, G.Y. Chung, and G. El-
lis. 2009. Using the web for language independent
spellchecking and autocorrection. In Proceedings of
the 2009 Conference on Empirical Methods in Natural
Language Processing, pages 890?899.
A. Wilcox-O?Hearn, G. Hirst, and A. Budanitsky. 2010.
Real-word spelling correction with trigrams: A recon-
sideration of the Mays, Damerau, and Mercer model.
Computational Linguistics and Intelligent Text Pro-
cessing, pages 605?616.
L. Zhang, M. Zhou, C. Huang, and HH Pan. 1999.
Multifeature-based approach to automatic error detec-
tion and correction of Chinese text. In Proceedings of
the First Workshop on Natural Language Processing
and Neural Networks.
L. Zhang, C. Huang, M. Zhou, and H. Pan. 2000. Auto-
matic detecting/correcting errors in Chinese text by an
approximate word-matching algorithm. In Proceed-
ings of the 38th Annual Meeting on Association for
Computational Linguistics, pages 248?254.
Y. Zheng, Z. Liu, M. Sun, L. Ru, and Y. Zhang. 2009. In-
corporating user behaviors in new word detection. In
Proceedings of the 21st International Joint Conference
on Artificial Intelligence, pages 2101?2106.
Y. Zheng, Z. Liu, and L. Xie. 2010. Growing relat-
ed words from seed via user behaviors: a re-ranking
based approach. In Proceedings of the ACL 2010 Stu-
dent Research Workshop, pages 49?54.
Y. Zheng, C. Li, and M. Sun. 2011a. CHIME: An ef-
ficient error-tolerant chinese pinyin input method. In
Proceedings of the 22nd International Joint Confer-
ence on Artificial Intelligence (accepted).
Y. Zheng, Z. Liu, L. Xie, M. Sun, L. Ru, and Y. Zhang.
2011b. User Behaviors in Related Word Retrieval
and New Word Detection: A Collaborative Perspec-
tive. ACM Transactions on Asian Language Informa-
tion Processing, Special Issue on Chinese Language
Processing (accepted).
490
Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 839?849,
Baltimore, Maryland, USA, June 23-25 2014.
c?2014 Association for Computational Linguistics
Distant Supervision for Relation Extraction with Matrix Completion
Miao Fan
?,?,?
, Deli Zhao
?
, Qiang Zhou
?
, Zhiyuan Liu
,?
, Thomas Fang Zheng
?
, Edward Y. Chang
?
?
CSLT, Division of Technical Innovation and Development,
Tsinghua National Laboratory for Information Science and Technology, Tsinghua University, China.

Department of Computer Science and Technology, Tsinghua University, China.
?
HTC Beijing Advanced Technology and Research Center, China.
?
fanmiao.cslt.thu@gmail.com
Abstract
The essence of distantly supervised rela-
tion extraction is that it is an incomplete
multi-label classification problem with s-
parse and noisy features. To tackle the s-
parsity and noise challenges, we propose
solving the classification problem using
matrix completion on factorized matrix of
minimized rank. We formulate relation
classification as completing the unknown
labels of testing items (entity pairs) in a s-
parse matrix that concatenates training and
testing textual features with training label-
s. Our algorithmic framework is based on
the assumption that the rank of item-by-
feature and item-by-label joint matrix is
low. We apply two optimization model-
s to recover the underlying low-rank ma-
trix leveraging the sparsity of feature-label
matrix. The matrix completion problem is
then solved by the fixed point continuation
(FPC) algorithm, which can find the glob-
al optimum. Experiments on two wide-
ly used datasets with different dimension-
s of textual features demonstrate that our
low-rank matrix completion approach sig-
nificantly outperforms the baseline and the
state-of-the-art methods.
1 Introduction
Relation Extraction (RE) is the process of gen-
erating structured relation knowledge from un-
structured natural language texts. Traditional su-
pervised methods (Zhou et al, 2005; Bach and
Badaskar, 2007) on small hand-labeled corpora,
such as MUC
1
and ACE
2
, can achieve high pre-
cision and recall. However, as producing hand-
labeled corpora is laborius and expensive, the su-
pervised approach can not satisfy the increasing
1
http://www.itl.nist.gov/iaui/894.02/related projects/muc/
2
http://www.itl.nist.gov/iad/mig/tests/ace/
Figure 1: Training corpus generated by the basic
alignment assumption of distantly supervised re-
lation extraction. The relation instances are the
triples related to President Barack Obama in the
Freebase, and the relation mentions are some sen-
tences describing him in the Wikipedia.
demand of building large-scale knowledge reposi-
tories with the explosion of Web texts. To address
the lacking training data issue, we consider the dis-
tant (Mintz et al, 2009) or weak (Hoffmann et al,
2011) supervision paradigm attractive, and we im-
prove the effectiveness of the paradigm in this pa-
per.
The intuition of the paradigm is that one
can take advantage of several knowledge bases,
such as WordNet
3
, Freebase
4
and YAGO
5
, to
automatically label free texts, like Wikipedia
6
and New York Times corpora
7
, based on some
heuristic alignment assumptions. An example
accounting for the basic but practical assumption
is illustrated in Figure 1, in which we know
that the two entities (<Barack Obama,
U.S.>) are not only involved in the rela-
tion instances
8
coming from knowledge bases
(President-of(Barack Obama, U.S.)
and Born-in(Barack Obama, U.S.)),
3
http://wordnet.princeton.edu
4
http://www.freebase.com
5
http://www.mpi-inf.mpg.de/yago-naga/yago
6
http://www.wikipedia.org
7
http://catalog.ldc.upenn.edu/LDC2008T19
8
According to convention, we regard a structured triple
r(e
i
, e
j
) as a relation instance which is composed of a pair of
entities <e
i
, e
j
>and a relation name r with respect to them.
839
Error MatrixCompleted Low?rank Matrix
?
Observed Sparse Matrix
TrainingItems
TestingItems
Incomplete LabelsNoisy Features
Figure 2: The procedure of noise-tolerant low-rank matrix completion. In this scenario, distantly super-
vised relation extraction task is transformed into completing the labels for testing items (entity pairs) in
a sparse matrix that concatenates training and testing textual features with training labels. We seek to
recover the underlying low-rank matrix and to complete the unknown testing labels simultaneously.
but also co-occur in several relation mentions
9
appearing in free texts (Barack Obama is
the 44th and current President of
the U.S. and Barack Obama was born
in Honolulu, Hawaii, U.S., etc.). We
extract diverse textual features from all those
relation mentions and combine them into a rich
feature vector labeled by the relation names
(President-of and Born-in) to produce a
weak training corpus for relation classification.
This paradigm is promising to generate large-
scale training corpora automatically. However, it
comes up against three technical challeges:
? Sparse features. As we cannot tell what
kinds of features are effective in advance, we
have to use NLP toolkits, such as Stanford
CoreNLP
10
, to extract a variety of textual fea-
tures, e.g., named entity tags, part-of-speech
tags and lexicalized dependency paths. Un-
fortunately, most of them appear only once in
the training corpus, and hence leading to very
sparse features.
? Noisy features. Not all relation mentions
express the corresponding relation instances.
For example, the second relation mention in
Figure 1 does not explicitly describe any rela-
tion instance, so features extracted from this
sentence can be noisy. Such analogous cases
commonly exist in feature extraction.
? Incomplete labels. Similar to noisy fea-
9
The sentences that contain the given entity pair are called
relation mentions.
10
http://nlp.stanford.edu/downloads/corenlp.shtml
tures, the generated labels can be in-
complete. For example, the fourth re-
lation mention in Figure 1 should have
been labeled by the relation Senate-of.
However, the incomplete knowledge base
does not contain the corresponding relation
instance (Senate-of(Barack Obama,
U.S.)). Therefore, the distant supervision
paradigm may generate incomplete labeling
corpora.
In essence, distantly supervised relation extrac-
tion is an incomplete multi-label classification task
with sparse and noisy features.
In this paper, we formulate the relation-
extraction task from a novel perspective of using
matrix completion with low rank criterion. To the
best of our knowledge, we are the first to apply this
technique on relation extraction with distant super-
vision. More specifically, as shown in Figure 2, we
model the task with a sparse matrix whose rows
present items (entity pairs) and columns contain
noisy textual features and incomplete relation la-
bels. In such a way, relation classification is trans-
formed into a problem of completing the unknown
labels for testing items in the sparse matrix that
concatenates training and testing textual features
with training labels, based on the assumption that
the item-by-feature and item-by-label joint matrix
is of low rank. The rationale of this assumption
is that noisy features and incomplete labels are
semantically correlated. The low-rank factoriza-
tion of the sparse feature-label matrix delivers the
low-dimensional representation of de-correlation
for features and labels.
840
We contribute two optimization models, DRM-
C
11
-b and DRMC-1, aiming at exploiting the s-
parsity to recover the underlying low-rank matrix
and to complete the unknown testing labels simul-
taneously. Moreover, the logistic cost function is
integrated in our models to reduce the influence of
noisy features and incomplete labels, due to that
it is suitable for binary variables. We also modify
the fixed point continuation (FPC) algorithm (Ma
et al, 2011) to find the global optimum.
Experiments on two widely used datasets
demonstrate that our noise-tolerant approaches
outperform the baseline and the state-of-the-art
methods. Furthermore, we discuss the influence of
feature sparsity, and our approaches consistently
achieve better performance than compared meth-
ods under different sparsity degrees.
2 Related Work
The idea of distant supervision was firstly pro-
posed in the field of bioinformatics (Craven and
Kumlien, 1999). Snow et al (2004) used Word-
Net as the knowledge base to discover more h-
pyernym/hyponym relations between entities from
news articles. However, either bioinformatic
database or WordNet is maintained by a few ex-
perts, thus hardly kept up-to-date.
As we are stepping into the big data era, the
explosion of unstructured Web texts simulates us
to build more powerful models that can automat-
ically extract relation instances from large-scale
online natural language corpora without hand-
labeled annotation. Mintz et al (2009) adopt-
ed Freebase (Bollacker et al, 2008; Bollacker
et al, 2007), a large-scale crowdsourcing knowl-
edge base online which contains billions of rela-
tion instances and thousands of relation names, to
distantly supervise Wikipedia corpus. The basic
alignment assumption of this work is that if a pair
of entities participate in a relation, all sentences
that mention these entities are labeled by that rela-
tion name. Then we can extract a variety of textu-
al features and learn a multi-class logistic regres-
sion classifier. Inspired by multi-instance learn-
ing (Maron and Lozano-P?erez, 1998), Riedel et al
(2010) relaxed the strong assumption and replaced
all sentences with at least one sentence. Hoff-
mann et al (2011) pointed out that many entity
pairs have more than one relation. They extend-
11
It is the abbreviation for Distant supervision for Relation
extraction with Matrix Completion
ed the multi-instance learning framework (Riedel
et al, 2010) to the multi-label circumstance. Sur-
deanu et al (2012) proposed a novel approach to
multi-instance multi-label learning for relation ex-
traction, which jointly modeled all the sentences in
texts and all labels in knowledge bases for a giv-
en entity pair. Other literatures (Takamatsu et al,
2012; Min et al, 2013; Zhang et al, 2013; Xu
et al, 2013) addressed more specific issues, like
how to construct the negative class in learning or
how to adopt more information, such as name en-
tity tags, to improve the performance.
Our work is more relevant to Riedel et al?s
(2013) which considered the task as a matrix fac-
torization problem. Their approach is composed
of several models, such as PCA (Collins et al,
2001) and collaborative filtering (Koren, 2008).
However, they did not concern about the data noise
brought by the basic assumption of distant super-
vision.
3 Model
We apply a new technique in the field of ap-
plied mathematics, i.e., low-rank matrix comple-
tion with convex optimization. The breakthrough
work on this topic was made by Cand`es and Recht
(2009) who proved that most low-rank matrices
can be perfectly recovered from an incomplete
set of entries. This promising theory has been
successfully applied on many active research ar-
eas, such as computer vision (Cabral et al, 2011),
recommender system (Rennie and Srebro, 2005)
and system controlling (Fazel et al, 2001). Our
models for relation extraction are based on the
theoretic framework proposed by Goldberg et al
(2010), which formulated the multi-label trans-
ductive learning as a matrix completion problem.
The new framework for classification enhances the
robustness to data noise by penalizing differen-
t cost functions for features and labels.
3.1 Formulation
Suppose that we have built a training corpus for
relation classification with n items (entity pairs),
d-dimensional textual features, and t labels (rela-
tions), based on the basic alignment assumption
proposed by Mintz et al (2009). Let X
train
?
R
n?d
and Y
train
? R
n?t
denote the feature matrix
and the label matrix for training, respectively. The
linear classifier we adopt aims to explicitly learn
the weight matrix W ? R
d?t
and the bias column
841
vector b ? R
t?1
with the constraint of minimizing
the loss function l,
arg min
W,b
l(Y
train
,
[
1 X
train
]
[
b
T
W
]
), (1)
where 1 is the all-one column vector. Then we can
predict the label matrix Y
test
? R
m?t
of m testing
items with respect to the feature matrix X
test
?
R
m?d
. Let
Z =
[
X
train
Y
train
X
test
Y
test
]
.
This linear classification problem can be trans-
formed into completing the unobservable entries
in Y
test
by means of the observable entries in
X
train
, Y
train
and X
test
, based on the assumption
that the rank of matrix Z ? R
(n+m)?(d+t)
is low.
The model can be written as,
arg min
Z?R
(n+m)?(d+t)
rank(Z)
s.t. ?(i, j) ? ?
X
, z
ij
= x
ij
,
(1 ? i ? n+m, 1 ? j ? d),
?(i, j) ? ?
Y
, z
i(j+d)
= y
ij
,
(1 ? i ? n, 1 ? j ? t),
(2)
where we use ?
X
to represent the index set of ob-
servable feature entries in X
train
and X
test
, and
?
Y
to denote the index set of observable label en-
tries in Y
train
.
Formula (2) is usually impractical for real prob-
lems as the entries in the matrix Z are corrupted
by noise. We thus define
Z = Z
?
+ E,
where Z
?
as the underlying low-rank matrix
Z
?
=
[
X
?
Y
?
]
=
[
X
?
train
Y
?
train
X
?
test
Y
?
test
]
,
and E is the error matrix
E =
[
E
X
train
E
Y
train
E
X
test
0
]
.
The rank function in Formula (2) is a non-convex
function that is difficult to be optimized. The sur-
rogate of the function can be the convex nucle-
ar norm ||Z||
?
=
?
?
k
(Z) (Cand`es and Recht,
2009), where ?
k
is the k-th largest singular val-
ue of Z. To tolerate the noise entries in the error
matrix E, we minimize the cost functions C
x
and
C
y
for features and labels respectively, rather than
using the hard constraints in Formula (2).
According to Formula (1), Z
?
? R
(n+m)?(d+t)
can be represented as [X
?
,WX
?
] instead of
[X
?
, Y
?
], by explicitly modeling the bias vector
b. Therefore, this convex optimization model is
called DRMC-b,
arg min
Z,b
?||Z||
?
+
1
|?
X
|
?
(i,j)??
X
C
x
(z
ij
, x
ij
)
+
?
|?
Y
|
?
(i,j)??
Y
C
y
(z
i(j+d)
+ b
j
, y
ij
),
(3)
where ? and ? are the positive trade-off weights.
More specifically, we minimize the nuclear norm
||Z||
?
via employing the regularization terms, i.e.,
the cost functions C
x
and C
y
for features and la-
bels.
If we implicitly model the bias vector b,
Z
?
? R
(n+m)?(1+d+t)
can be denoted by
[1, X
?
,W
?
X
?
] instead of [X
?
, Y
?
], in which W
?
takes the role of [b
T
; W] in DRMC-b. Then we
derive another optimization model called DRMC-
1,
arg min
Z
?||Z||
?
+
1
|?
X
|
?
(i,j)??
X
C
x
(z
i(j+1)
, x
ij
)
+
?
|?
Y
|
?
(i,j)??
Y
C
y
(z
i(j+d+1)
, y
ij
)
s.t. Z(:, 1) = 1,
(4)
where Z(:, 1) denotes the first column of Z.
For our relation classification task, both features
and labels are binary. We assume that the actual
entry u belonging to the underlying matrix Z
?
is
randomly generated via a sigmoid function (Jor-
dan, 1995): Pr(u|v) = 1/(1 + e
?uv
), given the
observed binary entry v from the observed sparse
matrix Z. Then, we can apply the log-likelihood
cost function to measure the conditional probabil-
ity and derive the logistic cost function for C
x
and
C
y
,
C(u, v) = ? logPr(u|v) = log(1 + e
?uv
),
After completing the entries in Y
test
, we adop-
t the sigmoid function to calculate the conditional
probability of relation r
j
, given entity pair p
i
per-
taining to y
ij
in Y
test
,
Pr(r
j
|p
i
) =
1
1 + e
?y
ij
, y
ij
? Y
test
.
Finally, we can achieve Top-N predicted relation
instances via ranking the values of Pr(r
j
|p
i
).
842
4 Algorithm
The matrix rank minimization problem is NP-
hard. Therefore, Cand?es and Recht (2009) sug-
gested to use a convex relaxation, the nuclear nor-
m minimization instead. Then, Ma et al (2011)
proposed the fixed point continuation (FPC) algo-
rithm which is fast and robust. Moreover, Gold-
frab and Ma (2011) proved the convergence of the
FPC algorithm for solving the nuclear norm mini-
mization problem. We thus adopt and modify the
algorithm aiming to find the optima for our noise-
tolerant models, i.e., Formulae (3) and (4).
4.1 Fixed point continuation for DRMC-b
Algorithm 1 describes the modified FPC algorithm
for solving DRMC-b, which contains two steps for
each iteration,
Gradient step: In this step, we infer the ma-
trix gradient g(Z) and bias vector gradient g(b) as
follows,
g(z
ij
) =
?
?
?
?
?
?
?
1
|?
X
|
?x
ij
1+e
x
ij
z
ij
, (i, j) ? ?
X
?
|?
Y
|
?y
i(j?d)
1+e
y
i(j?d)
(z
ij
+b
j
)
, (i, j ? d) ? ?
Y
0, otherwise
and
g(b
j
) =
?
|?
Y
|
?
i:(i,j)??
Y
?y
ij
1 + e
y
ij
(z
i(j+d)
+b
j
)
.
We use the gradient descents A = Z ? ?
z
g(Z)
and b = b ? ?
b
g(b) to gradually find the global
minima of the cost function terms in Formula (3),
where ?
z
and ?
b
are step sizes.
Shrinkage step: The goal of this step is to min-
imize the nuclear norm ||Z||
?
in Formula (3). We
perform the singular value decomposition (SVD)
(Golub and Kahan, 1965) for A at first, and then
cut down each singular value. During the iteration,
any negative value in ?? ?
z
? is assigned by zero,
so that the rank of reconstructed matrix Z will be
reduced, where Z = Umax(?? ?
z
?, 0)V
T
.
To accelerate the convergence, we use a con-
tinuation method to improve the speed. ? is ini-
tialized by a large value ?
1
, thus resulting in the
fast reduction of the rank at first. Then the conver-
gence slows down as ? decreases while obeying
?
k+1
= max(?
k
?
?
, ?
F
). ?
F
is the final value of
?, and ?
?
is the decay parameter.
For the stopping criteria in inner iterations, we
define the relative error to measure the residual of
matrix Z between two successive iterations,
Algorithm 1 FPC algorithm for solving DRMC-b
Input:
Initial matrix Z
0
, bias b
0
; Parameters ?, ?;
Step sizes ?
z
, ?
b
.
Set Z = Z
0
, b = b
0
.
foreach ? = ?
1
> ?
2
> ... > ?
F
do
while relative error > ? do
Gradient step:
A = Z? ?
z
g(Z),b = b? ?
b
g(b).
Shrinkage step:
U?V
T
= SVD(A),
Z = U max(?? ?
z
?, 0) V
T
.
end while
end foreach
Output: Completed Matrix Z, bias b.
||Z
k+1
? Z
k
||
F
max(1, ||Z
k
||
F
)
? ?,
where ? is the convergence threshold.
4.2 Fixed point continuation for DRMC-1
Algorithm 2 is similar to Algorithm 1 except for
two differences. First, there is no bias vector b.
Second, a projection step is added to enforce the
first column of matrix Z to be 1. In addition, The
matrix gradient g(Z) for DRMC-1 is
g(z
ij
) =
?
?
?
?
?
?
?
1
|?
X
|
?x
i(j?1)
1+e
x
i(j?1)
z
ij
, (i, j ? 1) ? ?
X
?
|?
Y
|
?y
i(j?d?1)
1+e
y
i(j?d?1)
z
ij
, (i, j ? d? 1) ? ?
Y
0, otherwise
.
Algorithm 2 FPC algorithm for solving DRMC-1
Input:
Initial matrix Z
0
; Parameters ?, ?;
Step sizes ?
z
.
Set Z = Z
0
.
foreach ? = ?
1
> ?
2
> ... > ?
F
do
while relative error > ? do
Gradient step: A = Z? ?
z
g(Z).
Shrinkage step:
U?V
T
= SVD(A),
Z = U max(?? ?
z
?, 0) V
T
.
Projection step: Z(:, 1) = 1.
end while
end foreach
Output: Completed Matrix Z.
843
Dataset # of training
tuples
# of testing
tuples
% with more
than one label
# of features # of relation
labels
NYT?10 4,700 1,950 7.5% 244,903 51
NYT?13 8,077 3,716 0% 1,957 51
Table 1: Statistics about the two widely used datasets.
Model NYT?10 (?=2) NYT?10 (?=3) NYT?10 (?=4) NYT?10 (?=5) NYT?13
DRMC-b 51.4 ? 8.7 (51) 45.6 ? 3.4 (46) 41.6 ? 2.5 (43) 36.2 ? 8.8(37) 84.6 ? 19.0 (85)
DRMC-1 16.0 ? 1.0 (16) 16.4 ? 1.1(17) 16 ? 1.4 (17) 16.8 ? 1.5(17) 15.8 ? 1.6 (16)
Table 2: The range of optimal ranks for DRMC-b and DRMC-1 through five-fold cross validation. The
threshold ? means filtering the features that appear less than ? times. The values in brackets pertaining to
DRMC-b and DRMC-1 are the exact optimal ranks that we choose for the completed matrices on testing
sets.
5 Experiments
In order to conduct reliable experiments, we adjust
and estimate the parameters for our approaches,
DRMC-b and DRMC-1, and compare them with
other four kinds of landmark methods (Mintz et
al., 2009; Hoffmann et al, 2011; Surdeanu et al,
2012; Riedel et al, 2013) on two public datasets.
5.1 Dataset
The two widely used datasets that we adopt are
both automatically generated by aligning Freebase
to New York Times corpora. The first dataset
12
,
NYT?10, was developed by Riedel et al (2010),
and also used by Hoffmann et al (2011) and Sur-
deanu et al (2012). Three kinds of features, name-
ly, lexical, syntactic and named entity tag fea-
tures, were extracted from relation mentions. The
second dataset
13
, NYT?13, was also released by
Riedel et al (2013), in which they only regarded
the lexicalized dependency path between two enti-
ties as features. Table 1 shows that the two datasets
differ in some main attributes. More specifically,
NYT?10 contains much higher dimensional fea-
tures than NYT?13, whereas fewer training and
testing items.
5.2 Parameter setting
In this part, we address the issue of setting param-
eters: the trade-off weights ? and ?, the step sizes
?
z
and ?
b
, and the decay parameter ?
?
.
We set ? = 1 to make the contribution of the
cost function terms for feature and label matrices
equal in Formulae (3) and (4). ? is assigned by a
series of values obeying ?
k+1
= max(?
k
?
?
, ?
F
).
12
http://iesl.cs.umass.edu/riedel/ecml/
13
http://iesl.cs.umass.edu/riedel/data-univSchema/
We follow the suggestion in (Goldberg et al,
2010) that ? starts at ?
1
?
?
, and ?
1
is the largest
singular value of the matrix Z. We set ?
?
= 0.01.
The final value of ?, namely ?
F
, is equal to 0.01.
Ma et al (2011) revealed that as long as the non-
negative step sizes satisfy ?
z
< min(
4|?
Y
|
?
, |?
X
|)
and ?
b
<
4|?
Y
|
?(n+m)
, the FPC algorithm will guaran-
tee to converge to a global optimum. Therefore,
we set ?
z
= ?
b
= 0.5 to satisfy the above con-
straints on both two datasets.
5.3 Rank estimation
Even though the FPC algorithm converges in iter-
ative fashion, the value of ? varying with different
datasets is difficult to be decided. In practice, we
record the rank of matrix Z at each round of iter-
ation until it converges at a rather small threshold
? = 10
?4
. The reason is that we suppose the opti-
mal low-rank representation of the matrix Z con-
veys the truly effective information about underly-
ing semantic correlation between the features and
the corresponding labels.
We use the five-fold cross validation on the val-
idation set and evaluate the performance on each
fold with different ranks. At each round of itera-
tion, we gain a recovered matrix and average the
F1
14
scores from Top-5 to Top-all predicted rela-
tion instances to measure the performance. Figure
3 illustrates the curves of average F1 scores. After
recording the rank associated with the highest F1
score on each fold, we compute the mean and the
standard deviation to estimate the range of optimal
rank for testing. Table 2 lists the range of optimal
ranks for DRMC-b and DRMC-1 on NYT?10 and
NYT?13.
14
F1 =
2?precision?recall
precision+recall
844
0 100 200 300 400 5000.122
0.124
0.126
0.128
0.13
0.132
0.134
0.136
0.138
0.14
Rank
Ave
rag
e?F
1
 
 Fold 1Fold 2Fold 3Fold 4Fold 5
(a) DRMC-b on NYT?10 validation set (? = 5).
0 100 200 300 400 5000.124
0.126
0.128
0.13
0.132
0.134
0.136
0.138
0.14
Rank
Ave
rag
e?F
1
 
 Fold 1Fold 2Fold 3Fold 4Fold 5
(b) DRMC-1 on NYT?10 validation set (? = 5).
0 100 200 300 400 500
0.104
0.106
0.108
0.11
0.112
0.114
Rank
Ave
rag
e?F
1
 
 Fold 1Fold 2Fold 3Fold 4Fold 5
(c) DRMC-b on NYT?13 validation set.
0 100 200 300 400 5000.106
0.108
0.11
0.112
0.114
0.116
0.118
0.12
0.122
0.124
Rank
Ave
rag
e?F
1
 
 Fold 1Fold 2Fold 3Fold 4Fold 5
(d) DRMC-1 on NYT?13 validation set.
Figure 3: Five-fold cross validation for rank estimation on two datasets.
On both two datasets, we observe an identical
phenomenon that the performance gradually in-
creases as the rank of the matrix declines before
reaching the optimum. However, it sharply de-
creases if we continue reducing the optimal rank.
An intuitive explanation is that the high-rank ma-
trix contains much noise and the model tends to be
overfitting, whereas the matrix of excessively low
rank is more likely to lose principal information
and the model tends to be underfitting.
5.4 Method Comparison
Firstly, we conduct experiments to compare our
approaches with Mintz-09 (Mintz et al, 2009),
MultiR-11 (Hoffmann et al, 2011), MIML-12 and
MIML-at-least-one-12 (Surdeanu et al, 2012) on
NYT?10 dataset. Surdeanu et al (2012) released
the open source code
15
to reproduce the experi-
mental results on those previous methods. More-
over, their programs can control the feature spar-
15
http://nlp.stanford.edu/software/mimlre.shtml
sity degree through a threshold ? which filters the
features that appears less than ? times. They set
? = 5 in the original code by default. Therefore,
we follow their settings and adopt the same way
to filter the features. In this way, we guarantee
the fair comparison for all methods. Figure 4 (a)
shows that our approaches achieve the significant
improvement on performance.
We also perform the experiments to compare
our approaches with the state-of-the-art NFE-13
16
(Riedel et al, 2013) and its sub-methods (N-13,
F-13 and NF-13) on NYT?13 dataset. Figure 4 (b)
illustrates that our approaches still outperform the
state-of-the-art methods. In practical application-
s, we also concern about the precision on Top-N
predicted relation instances. Therefore, We com-
pare the precision of Top-100s, Top-200s and Top-
500s for DRMC-1, DRMC-b and the state-of-the-
16
Readers may refer to the website,
http://www.riedelcastro.org/uschema for the details of
those methods. We bypass the description due to the
limitation of space.
845
0 0.1 0.2 0.3 0.4 0.50
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
Recall
Pre
cisi
on
 
 
Mintz?09
MultiR?11
MIML?12
MIML?at?least?one?12
DRMC?1(Rank=17)
DRMC?b(Rank=37)
(a) NYT?10 testing set (? = 5).
0 0.2 0.4 0.6 0.8 10
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
Recall
Pre
cisi
on
 
 N?13F?13NF?13NFE?13DRMC?1(Rank=16)DRMC?b(Rank=85)
(b) NYT?13 testing set.
Figure 4: Method comparison on two testing sets.
0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 10
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
Recall
Prec
ision
 
 DRMC?1(Rank=1879)DRMC?b(Rank=1993)DRMC?1(Rank=1169)DRMC?b(Rank=1307)DRMC?1(Rank=384)DRMC?b(Rank=464)DRMC?1(Rank=17)DRMC?b(Rank=37)
(a) NYT?10 testing set (? = 5).
0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 10
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
Recall
Prec
ision
 
 DRMC?1(Rank=1378)DRMC?b(Rank=1861)DRMC?1(Rank=719)DRMC?b(Rank=1703)DRMC?1(Rank=139)DRMC?b(Rank=655)DRMC?1(Rank=16)DRMC?b(Rank=85)
(b) NYT?13 testing set.
Figure 5: Precision-Recall curve for DRMC-b and DRMC-1 with different ranks on two testing sets.
Top-N NFE-13 DRMC-b DRMC-1
Top-100 62.9% 82.0% 80.0%
Top-200 57.1% 77.0% 80.0%
Top-500 37.2% 70.2% 77.0%
Average 52.4% 76.4% 79.0%
Table 3: Precision of NFE-13, DRMC-b and
DRMC-1 on Top-100, Top-200 and Top-500 pre-
dicted relation instances.
art method NFE-13 (Riedel et al, 2013). Table 3
shows that DRMC-b and DRMC-1 achieve 24.0%
and 26.6% precision increments on average, re-
spectively.
6 Discussion
We have mentioned that the basic alignment as-
sumption of distant supervision (Mintz et al,
2009) tends to generate noisy (noisy features and
incomplete labels) and sparse (sparse features) da-
ta. In this section, we discuss how our approaches
tackle these natural flaws.
Due to the noisy features and incomplete label-
s, the underlying low-rank data matrix with tru-
ly effective information tends to be corrupted and
the rank of observed data matrix can be extremely
high. Figure 5 demonstrates that the ranks of da-
ta matrices are approximately 2,000 for the initial
optimization of DRMC-b and DRMC-1. Howev-
er, those high ranks result in poor performance.
As the ranks decline before approaching the op-
timum, the performance gradually improves, im-
plying that our approaches filter the noise in data
and keep the principal information for classifica-
tion via recovering the underlying low-rank data
matrix.
Furthermore, we discuss the influence of the
feature sparsity for our approaches and the state-
846
0 100 200 300 400 5000.122
0.124
0.126
0.128
0.13
0.132
0.134
0.136
0.138
0.14
Rank
Avera
ge?F1
 
 Fold 1Fold 2Fold 3Fold 4Fold 5
0 100 200 300 400 5000.124
0.126
0.128
0.13
0.132
0.134
0.136
0.138
0.14
Rank
Avera
ge?F1
 
 Fold 1Fold 2Fold 3Fold 4Fold 5
0 0.1 0.2 0.3 0.4 0.50
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
Recall
Precis
ion
 
 
Mintz?09MultiR?11MIML?12MIML?at?least?one?12DRMC?1(Rank=17)DRMC?b(Rank=43)
0 0.2 0.4 0.6 0.8 10
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
Recall
Precis
ion
 
 DRMC?1(Rank=2148)DRMC?b(Rank=2291)DRMC?1(Rank=1285)DRMC?b(Rank=1448)DRMC?1(Rank=404)DRMC?b(Rank=489)DRMC?1(Rank=17)DRMC?b(Rank=43)
0 100 200 300 400 5000.122
0.124
0.126
0.128
0.13
0.132
0.134
0.136
0.138
0.14
Rank
Avera
ge?F1
 
 Fold 1Fold 2Fold 3Fold 4Fold 5
0 100 200 300 400 5000.124
0.126
0.128
0.13
0.132
0.134
0.136
0.138
0.14
Rank
Avera
ge?F1
 
 Fold 1Fold 2Fold 3Fold 4Fold 5
0 0.1 0.2 0.3 0.4 0.50
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
Recall
Precis
ion
 
 
Mintz?09MultiR?11MIML?12MIML?at?least?one?12DRMC?1(Rank=17)DRMC?b(Rank=46)
0 0.2 0.4 0.6 0.8 10
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
Recall
Precis
ion
 
 DRMC?1(Rank=2539)DRMC?b(Rank=2730)DRMC?1(Rank=1447)DRMC?b(Rank=1644)DRMC?1(Rank=433)DRMC?b(Rank=531)DRMC?1(Rank=17)DRMC?b(Rank=46)
0 100 200 300 400 5000.124
0.126
0.128
0.13
0.132
0.134
0.136
0.138
0.14
Rank
Avera
ge?F1
 
 Fold 1Fold 2Fold 3Fold 4Fold 5
0 100 200 300 400 5000.124
0.126
0.128
0.13
0.132
0.134
0.136
0.138
0.14
Rank
Avera
ge?F1
 
 Fold 1Fold 2Fold 3Fold 4Fold 5
0 0.1 0.2 0.3 0.4 0.50
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
Recall
Precis
ion
 
 
Mintz?09MultiR?11MIML?12MIML?at?least?one?12DRMC?1(Rank=16)DRMC?b(Rank=51)
0 0.2 0.4 0.6 0.8 10
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
Recall
Precis
ion
 
 DRMC?1(Rank=3186)DRMC?b(Rank=3444)DRMC?1(Rank=1728)DRMC?b(Rank=1991)DRMC?1(Rank=489)DRMC?b(Rank=602)DRMC?1(Rank=16)DRMC?b(Rank=51)
Figure 6: Feature sparsity discussion on NYT?10 testing set. Each row (from top to bottom, ? = 4, 3, 2)
illustrates a suite of experimental results. They are, from left to right, five-fold cross validation for
rank estimation on DRMC-b and DRMC-1, method comparison and precision-recall curve with different
ranks, respectively.
of-the-art methods. We relax the feature filtering
threshold (? = 4, 3, 2) in Surdeanu et al?s (2012)
open source program to generate more sparse fea-
tures from NYT?10 dataset. Figure 6 shows that
our approaches consistently outperform the base-
line and the state-of-the-art methods with diverse
feature sparsity degrees. Table 2 also lists the
range of optimal rank for DRMC-b and DRMC-
1 with different ?. We observe that for each ap-
proach, the optimal range is relatively stable. In
other words, for each approach, the amount of tru-
ly effective information about underlying seman-
tic correlation keeps constant for the same dataset,
which, to some extent, explains the reason why our
approaches are robust to sparse features.
7 Conclusion and Future Work
In this paper, we contributed two noise-tolerant
optimization models
17
, DRMC-b and DRMC-1,
for distantly supervised relation extraction task
from a novel perspective. Our models are based on
matrix completion with low-rank criterion. Exper-
17
The source code can be downloaded from https://
github.com/nlpgeek/DRMC/tree/master
iments demonstrated that the low-rank represen-
tation of the feature-label matrix can exploit the
underlying semantic correlated information for re-
lation classification and is effective to overcome
the difficulties incurred by sparse and noisy fea-
tures and incomplete labels, so that we achieved
significant improvements on performance.
Our proposed models also leave open question-
s for distantly supervised relation extraction task.
First, they can not process new coming testing
items efficiently, as we have to reconstruct the data
matrix containing not only the testing items but al-
so all the training items for relation classification,
and compute in iterative fashion again. Second,
the volume of the datasets we adopt are relatively
small. For the future work, we plan to improve our
models so that they will be capable of incremental
learning on large-scale datasets (Chang, 2011).
Acknowledgments
This work is supported by National Program on
Key Basic Research Project (973 Program) under
Grant 2013CB329304, National Science Founda-
tion of China (NSFC) under Grant No.61373075.
847
References
Nguyen Bach and Sameer Badaskar. 2007. A review
of relation extraction. Literature review for Lan-
guage and Statistics II.
Kurt Bollacker, Robert Cook, and Patrick Tufts. 2007.
Freebase: A shared database of structured general
human knowledge. In Proceedings of the nation-
al conference on Artificial Intelligence, volume 22,
page 1962. AAAI Press.
Kurt Bollacker, Colin Evans, Praveen Paritosh, Tim S-
turge, and Jamie Taylor. 2008. Freebase: a collab-
oratively created graph database for structuring hu-
man knowledge. In Proceedings of the 2008 ACM
SIGMOD international conference on Management
of data, pages 1247?1250. ACM.
Ricardo S Cabral, Fernando Torre, Jo?ao P Costeira, and
Alexandre Bernardino. 2011. Matrix completion
for multi-label image classification. In Advances in
Neural Information Processing Systems, pages 190?
198.
Emmanuel J Cand`es and Benjamin Recht. 2009. Exact
matrix completion via convex optimization. Foun-
dations of Computational mathematics, 9(6):717?
772.
Edward Y Chang. 2011. Foundations of Large-Scale
Multimedia Information Management and Retrieval.
Springer.
Michael Collins, Sanjoy Dasgupta, and Robert E
Schapire. 2001. A generalization of principal com-
ponents analysis to the exponential family. In Ad-
vances in neural information processing systems,
pages 617?624.
Mark Craven and Johan Kumlien. 1999. Construct-
ing biological knowledge bases by extracting infor-
mation from text sources. In ISMB, volume 1999,
pages 77?86.
Maryam Fazel, Haitham Hindi, and Stephen P Boyd.
2001. A rank minimization heuristic with applica-
tion to minimum order system approximation. In
American Control Conference, 2001. Proceedings of
the 2001, volume 6, pages 4734?4739. IEEE.
Andrew Goldberg, Ben Recht, Junming Xu, Robert
Nowak, and Xiaojin Zhu. 2010. Transduction with
matrix completion: Three birds with one stone. In
Advances in neural information processing systems,
pages 757?765.
Donald Goldfarb and Shiqian Ma. 2011. Conver-
gence of fixed-point continuation algorithms for ma-
trix rank minimization. Foundations of Computa-
tional Mathematics, 11(2):183?210.
Gene Golub and William Kahan. 1965. Calculat-
ing the singular values and pseudo-inverse of a ma-
trix. Journal of the Society for Industrial & Ap-
plied Mathematics, Series B: Numerical Analysis,
2(2):205?224.
Raphael Hoffmann, Congle Zhang, Xiao Ling,
Luke Zettlemoyer, and Daniel S. Weld. 2011.
Knowledge-based weak supervision for information
extraction of overlapping relations. In Proceedings
of the 49th Annual Meeting of the Association for
Computational Linguistics: Human Language Tech-
nologies, pages 541?550, Portland, Oregon, USA,
June. Association for Computational Linguistics.
Michael Jordan. 1995. Why the logistic function? a
tutorial discussion on probabilities and neural net-
works. Computational Cognitive Science Technical
Report.
Yehuda Koren. 2008. Factorization meets the neigh-
borhood: a multifaceted collaborative filtering mod-
el. In Proceedings of the 14th ACM SIGKDD in-
ternational conference on Knowledge discovery and
data mining, pages 426?434. ACM.
Shiqian Ma, Donald Goldfarb, and Lifeng Chen. 2011.
Fixed point and bregman iterative methods for ma-
trix rank minimization. Mathematical Program-
ming, 128(1-2):321?353.
Oded Maron and Tom?as Lozano-P?erez. 1998. A
framework for multiple-instance learning. Advances
in neural information processing systems, pages
570?576.
Bonan Min, Ralph Grishman, Li Wan, Chang Wang,
and David Gondek. 2013. Distant supervision for
relation extraction with an incomplete knowledge
base. In Proceedings of the 2013 Conference of
the North American Chapter of the Association for
Computational Linguistics: Human Language Tech-
nologies, pages 777?782, Atlanta, Georgia, June.
Association for Computational Linguistics.
Mike Mintz, Steven Bills, Rion Snow, and Dan Ju-
rafsky. 2009. Distant supervision for relation ex-
traction without labeled data. In Proceedings of the
Joint Conference of the 47th Annual Meeting of the
ACL and the 4th International Joint Conference on
Natural Language Processing of the AFNLP: Vol-
ume 2-Volume 2, pages 1003?1011. Association for
Computational Linguistics.
Jasson DM Rennie and Nathan Srebro. 2005. Fast
maximum margin matrix factorization for collabora-
tive prediction. In Proceedings of the 22nd interna-
tional conference on Machine learning, pages 713?
719. ACM.
Sebastian Riedel, Limin Yao, and Andrew McCal-
lum. 2010. Modeling relations and their mention-
s without labeled text. In Machine Learning and
Knowledge Discovery in Databases, pages 148?163.
Springer.
Sebastian Riedel, Limin Yao, Andrew McCallum, and
Benjamin M. Marlin. 2013. Relation extraction
with matrix factorization and universal schemas. In
Proceedings of the 2013 Conference of the North
American Chapter of the Association for Computa-
tional Linguistics: Human Language Technologies,
848
pages 74?84, Atlanta, Georgia, June. Association
for Computational Linguistics.
Rion Snow, Daniel Jurafsky, and Andrew Y Ng. 2004.
Learning syntactic patterns for automatic hypernym
discovery. Advances in Neural Information Process-
ing Systems 17.
Mihai Surdeanu, Julie Tibshirani, Ramesh Nallapati,
and Christopher D Manning. 2012. Multi-instance
multi-label learning for relation extraction. In Pro-
ceedings of the 2012 Joint Conference on Empirical
Methods in Natural Language Processing and Com-
putational Natural Language Learning, pages 455?
465. Association for Computational Linguistics.
Shingo Takamatsu, Issei Sato, and Hiroshi Nakagawa.
2012. Reducing wrong labels in distant supervision
for relation extraction. In Proceedings of the 50th
Annual Meeting of the Association for Computation-
al Linguistics: Long Papers-Volume 1, pages 721?
729. Association for Computational Linguistics.
Wei Xu, Raphael Hoffmann, Le Zhao, and Ralph Gr-
ishman. 2013. Filling knowledge base gaps for dis-
tant supervision of relation extraction. In Proceed-
ings of the 51st Annual Meeting of the Association
for Computational Linguistics (Volume 2: Short Pa-
pers), pages 665?670, Sofia, Bulgaria, August. As-
sociation for Computational Linguistics.
Xingxing Zhang, Jianwen Zhang, Junyu Zeng, Jun
Yan, Zheng Chen, and Zhifang Sui. 2013. Towards
accurate distant supervision for relational facts ex-
traction. In Proceedings of the 51st Annual Meet-
ing of the Association for Computational Linguistic-
s (Volume 2: Short Papers), pages 810?815, Sofi-
a, Bulgaria, August. Association for Computational
Linguistics.
Guodong Zhou, Jian Su, Jie Zhang, and Min Zhang.
2005. Exploring various knowledge in relation ex-
traction. In Proceedings of the 43rd Annual Meeting
on Association for Computational Linguistics, pages
427?434. Association for Computational Linguistic-
s.
849
Proceedings of the Fifteenth Conference on Computational Natural Language Learning, pages 135?144,
Portland, Oregon, USA, 23?24 June 2011. c?2011 Association for Computational Linguistics
Automatic Keyphrase Extraction by Bridging Vocabulary Gap ?
Zhiyuan Liu, Xinxiong Chen, Yabin Zheng, Maosong Sun
State Key Laboratory of Intelligent Technology and Systems
Tsinghua National Laboratory for Information Science and Technology
Department of Computer Science and Technology
Tsinghua University, Beijing 100084, China
{lzy.thu, cxx.thu, yabin.zheng}@gmail.com, sms@tsinghua.edu.cn
Abstract
Keyphrase extraction aims to select a set of
terms from a document as a short summary
of the document. Most methods extract
keyphrases according to their statistical prop-
erties in the given document. Appropriate
keyphrases, however, are not always statis-
tically significant or even do not appear in
the given document. This makes a large
vocabulary gap between a document and its
keyphrases. In this paper, we consider that
a document and its keyphrases both describe
the same object but are written in two different
languages. By regarding keyphrase extraction
as a problem of translating from the language
of documents to the language of keyphrases,
we use word alignment models in statistical
machine translation to learn translation proba-
bilities between the words in documents and
the words in keyphrases. According to the
translation model, we suggest keyphrases giv-
en a new document. The suggested keyphrases
are not necessarily statistically frequent in the
document, which indicates that our method
is more flexible and reliable. Experiments
on news articles demonstrate that our method
outperforms existing unsupervised methods
on precision, recall and F-measure.
1 Introduction
Information on the Web is emerging with the
development of Internet. It is becoming more and
more important to effectively search and manage
information. Keyphrases, as a brief summary of a
document, provide a solution to help organize and
?Zhiyuan Liu and Xinxiong Chen have equal contribution
to this work.
retrieve documents, which have been widely used
in digital libraries and information retrieval (Turney,
2000; Nguyen and Kan, 2007). Due to the explosion
of information, it is ineffective for professional
human indexers to manually annotate documents
with keyphrases. How to automatically extract
keyphrases from documents becomes an important
research problem, which is usually referred to as
keyphrase extraction.
Most methods for keyphrase extraction try to
extract keyphrases according to their statistical prop-
erties. These methods are susceptible to low perfor-
mance because many appropriate keyphrases may
not be statistically frequent or even not appear in the
document, especially for short documents. We name
the phenomenon as the vocabulary gap between
documents and keyphrases. For example, a research
paper talking about ?machine transliteration? may
less or even not mention the phrase ?machine
translation?. However, since ?machine transliter-
ation? is a sub-field of ?machine translation?, the
phrase ?machine translation? is also reasonable to
be suggested as a keyphrase to indicate the topics
of this paper. Let us take another example: in a
news article talking about ?iPad? and ?iPhone?, the
word ?Apple? may rarely ever come up. However,
it is known that both ?iPad? and ?iPhone? are the
products of ?Apple?, and the word ?Apple? may thus
be a proper keyphrase of this article.
We can see that, the essential challenge of
keyphrase extraction is the vocabulary gap between
documents and keyphrases. Therefore, the task of
keyphrase extraction is how to capture the semantic
relations between the words in documents and in
keyphrases so as to bridge the vocabulary gap.
In this paper, we provide a new perspective to
135
documents and their keyphrases: each document
and its keyphrases are descriptions to the same
object, but the document is written using one lan-
guage, while keyphrases are written using another
language. Therefore, keyphrase extraction can be
regarded as a translation problem from the language
of documents into the language of keyphrases.
Based on the idea of translation, we use word
alignment models (WAM) (Brown et al, 1993) in
statistical machine translation (SMT) (Koehn, 2010)
and propose a unified framework for keyphrase
extraction: (1) From a collection of translation pairs
of two languages, WAM learns translation probabil-
ities between the words in the two languages. (2)
According to the translation model, we are able to
bridge the vocabulary gap and succeed in suggesting
appropriate keyphrases, which may not necessarily
frequent in their corresponding documents.
As a promising approach to solve the problem
of vocabulary gap, SMT has been widely ex-
ploited in many applications such as information
retrieval (Berger and Lafferty, 1999; Karimzade-
hgan and Zhai, 2010), image and video anno-
tation (Duygulu et al, 2002), question answer-
ing (Berger et al, 2000; Echihabi and Marcu, 2003;
Murdock and Croft, 2004; Soricut and Brill, 2006;
Xue et al, 2008), query expansion and rewrit-
ing (Riezler et al, 2007; Riezler et al, 2008; Riezler
and Liu, 2010), summarization (Banko et al, 2000),
collocation extraction (Liu et al, 2009b; Liu et al,
2010b) and paraphrasing (Quirk et al, 2004; Zhao
et al, 2010). Although SMT is a widely adopted
solution to vocabulary gap, for various applications
using SMT, the crucial and non-trivial problem is
to find appropriate and enough translation pairs for
SMT.
The most straightforward translation pairs for
keyphrase extraction is document-keyphrase pairs.
In practice, however, it is time-consuming to anno-
tate a large collection of documents with keyphrases
for sufficient WAM training. In order to solve
the problem, we use titles and summaries to build
translation pairs with documents. Titles and sum-
maries are usually accompanying with the corre-
sponding documents. In some special cases, titles
or summaries may be unavailable. We are also able
to extract one or more important sentences from
the corresponding documents to construct sufficient
translation pairs.
2 State of the Art
Some researchers (Frank et al, 1999; Witten et al,
1999; Turney, 2000) regarded keyphrase extraction
as a binary classification problem (is-keyphrase or
non-keyphrase) and learned models for classifica-
tion using training data. These supervised methods
need manually annotated training set, which is time-
consuming. In this paper, we focus on unsupervised
methods for keyphrase extraction.
The most simple unsupervised method for
keyphrase extraction is using TFIDF (Salton and
Buckley, 1988) to rank the candidate keyphrases and
select the top-ranked ones as keyphrases. TFIDF
ranks candidate keyphrases only according to their
statistical frequencies, which thus fails to suggest
keyphrases with low frequencies.
Starting with TextRank (Mihalcea and Tarau,
2004), graph-based ranking methods are becoming
the state-of-the-art methods for keyphrase extrac-
tion (Liu et al, 2009a; Liu et al, 2010a). Given
a document, TextRank first builds a word graph,
in which the links between words indicate their
semantic relatedness, which are estimated by the
word co-occurrences in the document. By executing
PageRank (Page et al, 1998) on the graph, we obtain
the PageRank score for each word to rank candidate
keyphrases.
In TextRank, a low-frequency word will benefit
from its high-frequency neighbor words and thus be
ranked higher as compared to using TFIDF. This
alleviates the problem of vocabulary gap to some
extent. TextRank, however, still tends to extract
high-frequency words as keyphrases because these
words have more opportunities to get linked with
other words and obtain higher PageRank scores.
Moreover, TextRank usually constructs a word
graph simply according to word co-occurrences as
an approximation of the semantic relations between
words. This will introduce much noise because of
connecting semantically unrelated words and highly
influence extraction performance.
Some methods have been proposed to improve
TextRank, of which ExpandRank (Wan and Xi-
ao, 2008b; Wan and Xiao, 2008a) uses a smal-
l number, namely k, of neighbor documents to
136
provide more information of word relatedness for
the construction of word graphs. Compared to
TextRank, ExpandRank performs better when facing
the vocabulary gap by borrowing the information on
document level. However, the finding of neighbor
documents are usually arbitrary. This process may
introduce much noise and result in topic drift when
the document and its so-called neighbor documents
are not exactly talking about the same topics.
Another potential approach to alleviate vocabu-
lary gap is latent topic models (Landauer et al,
1998; Hofmann, 1999; Blei et al, 2003), of which
latent Dirichlet alocation (LDA) (Blei et al, 2003)
is most popular. Latent topic models learn topics
from a collection of documents. Using a topic
model, we can represent both documents and words
as the distributions over latent topics. The semantic
relatedness between a word and a document can be
computed using the cosine similarities of their topic
distributions. The similarity scores can be used as
the ranking criterion for keyphrase extraction (Hein-
rich, 2005; Blei and Lafferty, 2009). On one hand,
latent topic models use topics instead of statistical
properties of words for ranking, which abates the
vocabulary gap problem on topic level. On the other
hand, the learned topics are usually very coarse, and
topic models tend to suggest general words for a
given document. Therefore, the method usually fails
to capture the specific topics of the document.
In contract to the above-mentioned methods, our
method addresses vocabulary gap on word level,
which prevents from topic drift and works out better
performance. In experiments, we will show our
method can better solve the problem of vocab-
ulary gap by comparing with TFIDF, TextRank,
ExpandRank and LDA.
3 Keyphrase Extraction by Bridging
Vocabulary Gap Using WAM
First, we give a formal definition of keyphrase
extraction: given a collection of documents D, for
each document d ? D, keyphrase extraction aims
to rank candidate keyphrases according to their
likelihood given the document d, i.e., Pr(p|d) for all
p ? P, where P is the candidate keyphrase set. Then
we select top-Md ones as keyphrases, where Md can
be fixed or automatically determined by the system.
The document d can be regarded as a sequence of
words wd = {wi}Nd1 , where Nd is the length of d.
In Fig. 1, we demonstrate the framework of
keyphrase extraction using WAM. We divide the
algorithm into three steps: preparing translation
pairs, training translation models and extracting
keyphrases for a given document. We will introduce
the three steps in details from Section 3.1 to
Section 3.3.
Input: A large collection of documents D for keyphrase
extraction.
Step 1: Prepare Translation Pairs. For each d ? D, we
may prepare two types of translation pairs:
? Title-based Pairs. Use the title td of each document
d and prepare translation pairs, denote as ?D,T ?.
? Summary-based Pairs. Use the summary sd of
each document d and prepare translation pairs,
denote as ?D,S?.
Step 2: Train Translation Model. Given translation
pairs, e.g., ?D,T ?, train word-word translation model
Pr?D,T ?(t|w) using WAM, where w is the word in docu-
ment language and t is the word in title language.
Step 3: Keyphrase Extraction. For a document d,
extract keyphrases according to a trained translation
model, e.g., Pr?D,T ?(t|w).
1. Measure the importance score Pr(w|d) of each word
w in document d.
2. Compute the ranking score of candidate keyphrase
p by
Pr(p|d) =
?t?p?w?d Pr?D,T ?(t|w)Pr(w|d) (1)
3. Select top-Md ranked candidate keyphrases accord-
ing to Pr(p|d) as the keyphrases of document d.
Figure 1: WAM for keyphrase extraction.
3.1 Preparing Translation Pairs
Training dataset for WAM consists of a number
of translation pairs written in two languages. In
keyphrase extraction task, we have to construct
sufficient translation pairs to capture the semantic
relations between documents and keyphrases. Here
we propose to construct two types of translation
pairs: title-based pairs and summary-based pairs.
137
3.1.1 Title-based Pairs
Title is usually a short summary of the given doc-
ument. In most cases, documents such as research
papers and news articles have corresponding titles.
Therefore, we can use title to construct translation
pairs for a document.
WAM assumes each translation pair should be of
comparable length. However, a document is usually
much longer than title. It will hurt the performance
if we fill the length-unbalanced pairs for WAM
training. We propose two methods to address the
problem: sampling method and split method.
In sampling method, we perform word sampling
for each document to make it comparable to the
length of its title. Suppose the lengths of a document
and its title are Nd and Nt , respectively. For
document d, we first build a bag of words bd =
{(wi,ei)}Wdi=1, where Wd is the number of unique
words in d, and ei is the weights of word wi in d.
In this paper, we use TFIDF scores as the weights
of words. Using bd , we sample words for Nt
times with replacement according to the weights of
words, and finally form a new bag with Nt words
to represent document d. In the sampling result,
we keep the most important words in document d.
We can thus construct a document-title pair with
balanced length.
In split method, we split each document into
sentences which are of comparable length to its
title. For each sentence, we compute its semantic
similarity with the title. There are various methods
to measure semantic similarities. In this paper, we
use vector space model to represent sentences and
titles, and use cosine scores to compute similarities.
If the similarity is smaller than a threshold ? , we
will discard the sentence; otherwise, we will regard
the sentence and title as a translation pair.
Sampling method and split method have their
own characteristics. Compared to split method,
sampling method loses the order information of
words in documents. While split method generates
much more translation pairs, which leads to longer
training time of WAM. In experiment section, we
will investigate the performance of the two methods.
3.1.2 Summary-based Pairs
For most research articles, authors usually pro-
vide abstracts to summarize the articles. Many news
articles also have short summaries. Suppose each
document itself has a short summary, we can use
the summary and document to construct translation
pairs using either sampling method or split method.
Because each summary usually consists of multiple
sentences, split method for constructing summary-
based pairs has to split both the document and
summary into sentences, and the sentence pairs with
similarity scores above the threshold are filled in
training dataset for WAM.
3.2 Training Translation Models
Without loss of generality, we take title-based pairs
as the example to introduce the training process
of translation models, and suppose documents are
written in one language and titles are written in
another language. In this paper, we use IBM Model-
1 (Brown et al, 1993) for WAM training. IBM
Model-1 is a widely used word alignment algorithm
which does not require linguistic knowledge for two
languages 1.
In IBM Model-1, for each translation pair
?wd ,wt?, the relationship of the document language
wd = {wi}Ldi=0 and the title language wt = {ti}
Lt
i=0
is connected via a hidden variable a = {ai}Ldi=1
describing an alignment mapping from words of
documents to words of titles,
Pr(wd |wt) = ?aPr(wd ,a|wt) (2)
For example, a j = i indicates word w j in wd at
position j is aligned to word ti in wt at position i.
The alignment a also contains empty-word align-
ments a j = 0 which align words of documents to
an empty word. IBM Model-1 can be trained using
Expectation-Maximization (EM) algorithm (Demp-
ster et al, 1977) in an unsupervised fashion. Using
IBM Model-1, we can obtain the translation prob-
abilities of two language-vocabularies, i.e., Pr(t|w)
and Pr(w|t), where w is a word in document
vocabulary and t is a word in title vocabulary.
IBM Model-1 will produce one-to-many align-
ments from one language to another language, and
the trained model is thus asymmetric. Hence, we can
1We have also employed more sophisticated WAM al-
gorithms such as IBM Model-3 for keyphrase extraction.
However, these methods did not achieve better performance
than the simple IBM Model-1. Therefore, in this paper we only
demonstrate the experimental results using IBM Model-1.
138
train two different translation models by assigning
translation pairs in two directions, i.e., (document?
title) and (title ? document). We denote the former
model as Prd2t and the latter as Prt2d. We define
Pr?D,T ?(t|w) in Eq.(1) as the harmonic mean of the
two models:
Pr?D,T ?(t|w) ?
(
?
Prd2t(t|w) +
(1?? )
Prt2d(t|w)
)?1
(3)
where ? is the harmonic factor to combine the two
models. When ? = 1.0 or ? = 0.0, it simply uses
model Prd2t or Prt2d, correspondingly. Using the
translation probabilities Pr(t|w) we can bridge the
vocabulary gap between documents and keyphrases.
3.3 Keyphrase Extraction
Given a document d, we rank candidate keyphrases
by computing their likelihood Pr(p|d). Each can-
didate keyphrase p may be composed of multiple
words. As shown in (Hulth, 2003), most keyphrases
are noun phrases. Following (Mihalcea and Tarau,
2004; Wan and Xiao, 2008b), we simply select
noun phrases from the given document as candidate
keyphrases with the help of POS tags. For each
word t, we compute its likelihood given d, Pr(t|d) =
?w?d Pr(t|w)Pr(w|d), where Pr(w|d) is the weight
of the word w in d, which is measured using
normalized TFIDF scores. Pr(t|w) is the translation
probabilities obtained from WAM training.
Using the scores of all words in candidate
keyphrases, we compute the ranking score of each
candidate keyphrase by summing up the scores
of each word in the candidate keyphrase, i.e.,
Pr(p|d) =
?t?pPr(t|d). In all, the ranking scores
of candidate keyphrases is formalized in Eq. (1)
of Fig. 1. According to the ranking scores, we can
suggest top-Md ranked candidates as the keyphrases,
where Md is the number of suggested keyphrases to
the document d pre-specified by users or systems.
We can also consider the number of words in the
candidate keyphrase as a normalization factor to Eq.
(1), which will be our future work.
4 Experiments
To perform experiments, we crawled a collection of
13,702 Chinese news articles 2 from www.163.
2The dataset can be obtained from http://nlp.csai.
tsinghua.edu.cn/?lzy/datasets/ke_wam.html.
com, one of the most popular news websites in Chi-
na. The news articles are composed of various topics
including science, technology, politics, sports, arts,
society and military. All news articles are manually
annotated with keyphrases by website editors, and
all these keyphrases come from the corresponding
documents. Each news article is also provided with
a title and a short summary.
In this dataset, there are 72,900 unique words in
documents, and 12,405 unique words in keyphrases.
The average lengths of documents, titles and sum-
maries are 971.7 words, 11.6 words, and 45.8 words,
respectively. The average number of keyphrases
for each document is 2.4. In experiments, we
use the annotated titles and summaries to construct
translation pairs.
In experiments, we select GIZA++ 3 (Och and
Ney, 2003) to train IBM Model-1 using translation
pairs. GIZA++, widely used in various applications
of statistical machine translation, implements IBM
Models 1-5 and an HMM word alignment model.
To evaluate methods, we use the annotated
keyphrases by www.163.com as the standard
keyphrases. If one suggested keyphrase exact-
ly matches one of the standard keyphrases, it
is a correct keyphrase. We use precision p =
ccorrect/cmethod , recall r = ccorrect/cstandard and F-
measure f = 2pr/(p + r) for evaluation, where
ccorrect is the number of keyphrases correctly sug-
gested by the given method, cmethod is the number
of suggested keyphrases, and cstandard is the number
of standard keyphrases. The following experiment
results are obtained by 5-fold cross validation.
4.1 Evaluation on Keyphrase Extraction
4.1.1 Performance Comparison and Analysis
We use four representative unsupervised methods
as baselines for comparison: TFIDF, TextRank (Mi-
halcea and Tarau, 2004), ExpandRank (Wan and
Xiao, 2008b) and LDA (Blei et al, 2003). We
denote our method as WAM for short.
In Fig. 2, we demonstrate the precision-recall
curves of various methods for keyphrase extraction
including TFIDF, TextRank, ExpandRank, LDA
and WAM with title-based pairs prepared using
3The website for GIZA++ package is http://code.
google.com/p/giza-pp/.
139
sampling method (Title-Sa) and split method (Title-
Sp), and WAM with summary-based pairs prepared
using sampling method (Summ-Sa) and split method
(Summ-Sp). For WAM, we set the harmonic factor
? = 1.0 and threshold ? = 0.1, which is the optimal
setting as shown in the later analysis on parameter
influence. For TextRank, LDA and ExpandRank, we
report their best results after parameter tuning, e.g.,
the number of topics for LDA is set to 400, and the
number of neighbor documents for ExpandRank is
set to 5 .
The points on a precision-recall curve represent
different numbers of suggested keyphrases from
Md = 1 (bottom right) to Md = 10 (upper left),
respectively. The closer the curve is to the upper
right, the better the overall performance of the
method is. In Table 1, we further demonstrate the
precision, recall and F-measure scores of various
methods when Md = 2 4. In Table 1, we also show
the statistical variances after ?. From Fig. 2 and
Table 1, we have the following observations:
 0.1
 0.2
 0.3
 0.4
 0.5
 0.6
 0.7
 0.05  0.1  0.15  0.2  0.25  0.3  0.35  0.4
Rec
all
Precision
TFIDFTextRankLDAExpandRankTitle-SaTitle-SpSumm-SaSumm-Sp
Figure 2: The precision-recall curves of various
methods for keyphrase extraction.
First, our method outperforms all baselines. It
indicates that the translation perspective is valid
for keyphrase extraction. When facing vocabu-
lary gap, TFIDF and TextRank have no solutions,
ExpandRank adopts the external information on
document level which may introduce noise, and
LDA adopts the external information on topic level
which may be too coarse. In contrast to these
baselines, WAM aims to bridge the vocabulary gap
on word level, which avoids topic drift effectively.
4We select Md = 2 because WAM gains the best F-measure
score when Md = 2, which is close to the average number of
annotated keyphrases for each document 2.4.
Method Precision Recall F-measure
TFIDF 0.187 0.256 0.208?0.005
TextRank 0.217 0.301 0.243?0.008
LDA 0.181 0.253 0.203?0.002
ExpandRank 0.228 0.316 0.255?0.007
Title-Sa 0.299 0.424 0.337?0.008
Title-Sp 0.300 0.425 0.339?0.010
Summ-Sa 0.258 0.361 0.289?0.009
Summ-Sp 0.273 0.384 0.307?0.008
Table 1: Precision, recall and F-measure of various
methods for keyphrase extraction when Md = 2.
Therefore, our method can better solve the problem
of vocabulary gap in keyphrase extraction.
Second, WAM with title-based pairs performs
better than summary-based pairs consistently, no
matter prepared using sampling method or split
method. This indicates the titles are closer to
the keyphrase language as compared to summaries.
This is also consistent with the intuition that titles
are more important than summaries. Meanwhile, we
can save training efforts using title-based pairs.
Last but not least, split method achieves better or
comparable performance as compared to sampling
method on both title-based pairs and summary-
based pairs. The reasons are: (1) the split method
generates more translation pairs for adequate train-
ing than sampling method; and (2) split method
also keeps the context of words, which helps to
obtain better word alignment, unlike bag-of-words
in sampling method.
4.1.2 Influence of Parameters
We also investigate the influence of parameters
to WAM with title-based pairs prepared using split
method, which achieves the best performance as
shown in Fig. 2. The parameters include: harmonic
factor ? (described in Eq. 3) and threshold factor
? . Harmonic factor ? controls the weights of the
translation models trained in two directions, i.e.,
Prd2t(t|w) and Prt2d(t|w) as shown in Eq. (3). As
described in Section 3.1.1, using threshold factor ?
we filter out the pairs with similarities lower than ? .
In Fig. 3, we show the precision-recall curves
of WAM for keyphrase extraction when harmonic
factor ? ranges from 0.0 to 1.0 stepped by 0.2. From
the figure, we observe that the translation model
Prd2t(t|w) (i.e., when ? = 1.0) performs better than
140
Prt2d(t|w) (i.e., when ? = 0.0). This indicates that
it is sufficient to simply train a translation model
in one direction (i.e., Prd2t(t|w)) for keyphrase
extraction.
 0.25
 0.3
 0.35
 0.4
 0.45
 0.5
 0.55
 0.6
 0.65
 0.7
 0.05  0.1  0.15  0.2  0.25  0.3  0.35  0.4
Rec
all
Precision
? = 0.0? = 0.2? = 0.4? = 0.6? = 0.8? = 1.0
Figure 3: Precision-recall curves of WAM when
harmonic factor ? ranges from 0.0 to 1.0.
In Fig. 4, we show the precision-recall curves
of WAM for keyphrase extraction when threshold
factor ? ranges from 0.01 to 0.90. In title-
based pairs using split method, the total number
of pairs without filtering any pairs (i.e., ? = 0)
is 347,188. When ? = 0.01, 0.10 and 0.90, the
numbers of retained translation pairs are 165,023,
148,605 and 41,203, respectively. From Fig. 4,
we find that more translation pairs result in better
performance. However, more translation pairs also
indicate more training time of WAM. Fortunately,
we can see that the performance does not drop much
when discarding more translation pairs with low
similarities. Even when ? = 0.9, our method can
still achieve performance with precision p = 0.277,
recall r = 0.391 and F-measure f = 0.312 when
Md = 2. Meanwhile, we reduce the training efforts
by about 50% as compared to ? = 0.01.
In all, based on the above analysis on two
parameters, we demonstrate the effectiveness and
robustness of our method for keyphrase extraction.
4.1.3 When Titles/Summaries Are Unavailable
Suppose in some special cases, the titles or sum-
maries are unavailable, how can we construct trans-
lation pairs? Inspired by extraction-based document
summarization (Goldstein et al, 2000; Mihalcea and
Tarau, 2004), we can extract one or more important
sentences from the given document to construct
translation pairs. Unsupervised sentence extraction
 0.25
 0.3
 0.35
 0.4
 0.45
 0.5
 0.55
 0.6
 0.65
 0.7
 0.05  0.1  0.15  0.2  0.25  0.3  0.35  0.4  0.45
Rec
all
Precision
? = 0.01? = 0.05? = 0.10? = 0.30? = 0.50? = 0.70? = 0.90
Figure 4: Precision-recall curves of WAM when
threshold ? ranges from 0.01 to 0.90.
for document summarization is a well-studied task
in natural language processing. As shown in Table 2,
we only perform two simple sentence extraction
methods to demonstrate the effectiveness: (1) Select
the first sentence of a document (denoted as ?First?);
and (2) Compute the cosine similarities between
each sentence and the whole document represented
as two bags-of-words (denoted as ?Importance?).
It is interesting to find that the method of using
the first sentence performs similar to using titles.
This profits from the characteristic of news articles
which tend to give a good summary for the whole
article using the first sentence. Although the second
method drops much on performance as compared to
using titles, it still outperforms than other existing
methods. Moreover, the second method will im-
prove much if we use more effective measures to
identify the most important sentence.
Method Precision Recall F-measure
First 0.290 0.410 0.327?0.013
Importance 0.260 0.367 0.293?0.010
Table 2: Precision, recall and F-measure of
keyphrase extraction when Md = 2 by extracting one
sentence to construct translation pairs.
4.2 Beyond Extraction: Keyphrase Generation
In Section 4.1, we evaluate our method on keyphrase
extraction by suggesting keyphrases from docu-
ments. In fact, our method is also able to suggest
keyphrases that have not appeared in the content of
given document. The ability is important especially
when the length of each document is short, which
141
itself may not contain appropriate keyphrases. We
name the new task keyphrase generation. To
evaluate these methods on keyphrase generation,
we perform keyphrase generation for the titles of
documents, which are usually much shorter than
their corresponding documents. The experiment
setting is as follows: the training phase is the
same to the previous experiment, but in the test
phase we suggest keyphrases only using the titles.
LDA and ExpandRank, similar to our method, are
also able to select candidate keyphrases beyond the
titles. We still use the annotated keyphrases of the
corresponding documents as standard answers. In
this case, about 59% standard keyphrases do not
appear in titles.
In Table 3 we show the evaluation results of vari-
ous methods for keyphrase generation when Md = 2.
ForWAM, we only show the results using title-based
pairs prepared with split method. From the table,
we have three observations: (1) WAM outperforms
other methods on keyphrase generation. Moreover,
there are about 10% correctly suggested keyphrases
by WAM do not appear in titles, which indicates the
effectiveness of WAM for keyphrase generation. (2)
The performance of TFIDF and TextRank is much
lower as compared to Table 1, because the titles are
so short that they do not provide enough candidate
keyphrases and even the statistical information to
rank candidate keyphrases. (3) LDA, ExpandRank
and WAM roughly keep comparable performance as
in Table 1 (The performance of ExpandRank drops
a bit). This indicates the three methods are able to
perform keyphrase generation, and verifies again the
effectiveness of our method.
Method Precision Recall F-measure
TFIDF 0.105 0.141 0.115?0.004
TextRank 0.107 0.144 0.118?0.005
LDA 0.180 0.256 0.204?0.008
ExpandRank 0.194 0.268 0.216?0.012
WAM 0.296 0.420 0.334?0.009
Table 3: Precision, recall and F-measure of various
methods for keyphrase generation when Md = 2.
To demonstrate the features of our method for
keyphrase generation, in Table 4 we list top-5
keyphrases suggested by LDA, ExpandRank and
WAM for a news article entitled Israeli Military
Claims Iran Can Produce Nuclear Bombs and
Considering Military Action against Iran (We trans-
late the original Chinese title and keyphrases into
English for comprehension.). We have the following
observations: (1) LDA suggests general words like
?negotiation? and ?sanction? as keyphrases because
the coarse-granularity of topics. (2) ExpandRank
suggests some irrelevant words like ?Lebanon? as
keyphrases, which are introduced by neighbor doc-
uments talking about other affairs related to Israel.
(3) Our method can generate appropriate keyphrases
with less topic-drift. Moreover, our method can find
good keyphrases like ?nuclear weapon? which even
do not appear in the title.
LDA: Iran, U.S.A., negotiation, Israel, sanction
ExpandRank: Iran, Israel, Lebanon, U.S.A., Israeli
Military
WAM: Iran, military action, Israeli Military, Israel,
nuclear weapon
Table 4: Top-5 keyphrases suggested by LDA,
ExpandRank and WAM.
5 Conclusion and Future Work
In this paper, we provide a new perspective to
keyphrase extraction: regarding a document and its
keyphrases as descriptions to the same object written
in two languages. We use IBM Model-1 to bridge
the vocabulary gap between the two languages for
keyphrase generation. We explore various methods
to construct translation pairs. Experiments show
that our method can capture the semantic relations
between words in documents and keyphrases. Our
method is also language-independent, which can be
performed on documents in any languages.
We will explore the following two future work:
(1) Explore our method on other types of articles
and on other languages. (2) Explore more com-
plicated methods to extract important sentences for
constructing translation pairs.
Acknowledgments
This work is supported by the National Natural
Science Foundation of China (NSFC) under Grant
No. 60873174. The authors would like to thank
Peng Li and Xiance Si for their suggestions.
142
References
M. Banko, V.O. Mittal, and M.J. Witbrock. 2000.
Headline generation based on statistical translation. In
Proceedings of ACL, pages 318?325.
A. Berger and J. Lafferty. 1999. Information retrieval as
statistical translation. In Proceedings of SIGIR, pages
222?229.
A. Berger, R. Caruana, D. Cohn, D. Freitag, and
V. Mittal. 2000. Bridging the lexical chasm: statistical
approaches to answer-finding. In Proceedings of
SIGIR, pages 192?199.
D.M. Blei and J.D. Lafferty, 2009. Text mining:
Classification, Clustering, and Applications, chapter
Topic models. Chapman & Hall.
D.M. Blei, A.Y. Ng, and M.I. Jordan. 2003. Latent
dirichlet alocation. Journal of Machine Learning
Research, 3:993?1022, January.
P.F. Brown, V.J.D. Pietra, S.A.D. Pietra, and R.L.
Mercer. 1993. The mathematics of statistical machine
translation: Parameter estimation. Computational
linguistics, 19(2):263?311.
A.P. Dempster, N.M. Laird, D.B. Rubin, et al 1977.
Maximum likelihood from incomplete data via the em
algorithm. Journal of the Royal Statistical Society.
Series B (Methodological), 39(1):1?38.
P. Duygulu, Kobus Barnard, J. F. G. de Freitas, and
David A. Forsyth. 2002. Object recognition as
machine translation: Learning a lexicon for a fixed
image vocabulary. In Proceedings of ECCV, pages
97?112.
A. Echihabi and D. Marcu. 2003. A noisy-channel
approach to question answering. In Proceedings of
ACL, pages 16?23.
E. Frank, G.W. Paynter, I.H. Witten, C. Gutwin, and C.G.
Nevill-Manning. 1999. Domain-specific keyphrase
extraction. In Proceedings of IJCAI, pages 668?673.
J. Goldstein, V. Mittal, J. Carbonell, and M. Kantrowitz.
2000. Multi-document summarization by sentence
extraction. In Proceedings of NAACL-ANLP 2000
Workshop on Automatic summarization, pages 40?48.
G. Heinrich. 2005. Parameter estimation for text anal-
ysis. Web: http://www. arbylon. net/publications/text-
est.
T. Hofmann. 1999. Probabilistic latent semantic
indexing. In Proceedings of SIGIR, pages 50?57.
A. Hulth. 2003. Improved automatic keyword extraction
given more linguistic knowledge. In Proceedings of
EMNLP, pages 216?223.
M. Karimzadehgan and C.X. Zhai. 2010. Estimation of
statistical translation models based on mutual informa-
tion for ad hoc information retrieval. In Proceedings
of SIGIR, pages 323?330.
P. Koehn. 2010. Statistical Machine Translation.
Cambridge University Press.
T.K. Landauer, P.W. Foltz, and D. Laham. 1998. An
introduction to latent semantic analysis. Discourse
Processes, 25:259?284.
Z. Liu, P. Li, Y. Zheng, and M. Sun. 2009a. Clustering
to find exemplar terms for keyphrase extraction. In
Proceedings of EMNLP, pages 257?266.
Z. Liu, H. Wang, H. Wu, and S. Li. 2009b. Collocation
extraction using monolingual word alignment method.
In Proceedings of EMNLP, pages 487?495.
Z. Liu, W. Huang, Y. Zheng, and M. Sun. 2010a. Au-
tomatic keyphrase extraction via topic decomposition.
In Proceedings of EMNLP, pages 366?376.
Z. Liu, H. Wang, H. Wu, and S. Li. 2010b. Improving
statistical machine translation with monolingual collo-
cation. In Proceedings of ACL, pages 825?833.
R. Mihalcea and P. Tarau. 2004. Textrank: Bringing
order into texts. In Proceedings of EMNLP, pages
404?411.
V. Murdock and W.B. Croft. 2004. Simple translation
models for sentence retrieval in factoid question an-
swering. In Proceedings of SIGIR.
T. Nguyen and M.Y. Kan. 2007. Keyphrase extraction
in scientific publications. In Proceedings of the 10th
International Conference on Asian Digital Libraries,
pages 317?326.
F.J. Och and H. Ney. 2003. A systematic comparison of
various statistical alignment models. Computational
linguistics, 29(1):19?51.
L. Page, S. Brin, R. Motwani, and T. Winograd. 1998.
The pagerank citation ranking: Bringing order to
the web. Technical report, Stanford Digital Library
Technologies Project, 1998.
C. Quirk, C. Brockett, andW. Dolan. 2004. Monolingual
machine translation for paraphrase generation. In
Proceedings of EMNLP, volume 149.
S. Riezler and Y. Liu. 2010. Query rewriting using
monolingual statistical machine translation. Compu-
tational Linguistics, 36(3):569?582.
S. Riezler, A. Vasserman, I. Tsochantaridis, V. Mittal, and
Y. Liu. 2007. Statistical machine translation for query
expansion in answer retrieval. In Proccedings of ACL,
pages 464?471.
S. Riezler, Y. Liu, and A. Vasserman. 2008. Translating
queries into snippets for improved query expansion. In
Proceedings of COLING, pages 737?744.
G. Salton and C. Buckley. 1988. Term-weighting
approaches in automatic text retrieval. Information
processing and management, 24(5):513?523.
R. Soricut and E. Brill. 2006. Automatic question
answering using the web: Beyond the factoid. Infor-
mation Retrieval, 9(2):191?206.
143
P.D. Turney. 2000. Learning algorithms for keyphrase
extraction. Information Retrieval, 2(4):303?336.
X. Wan and J. Xiao. 2008a. Collabrank: towards a
collaborative approach to single-document keyphrase
extraction. In Proceedings of COLING, pages 969?
976.
X. Wan and J. Xiao. 2008b. Single document
keyphrase extraction using neighborhood knowledge.
In Proceedings of AAAI, pages 855?860.
I.H. Witten, G.W. Paynter, E. Frank, C. Gutwin, and
C.G. Nevill-Manning. 1999. Kea: Practical automatic
keyphrase extraction. In Proceedings of DL, pages
254?255.
X. Xue, J. Jeon, and W.B. Croft. 2008. Retrieval models
for question and answer archives. In Proceedings of
SIGIR, pages 475?482.
S. Zhao, H. Wang, and T. Liu. 2010. Paraphrasing with
search engine query logs. In Proceedings of COLING,
pages 1317?1325.
144
