Proceedings of the Human Language Technology Conference of the NAACL, Companion Volume, pages 273?275,
New York City, June 2006. c?2006 Association for Computational Linguistics
Knowtator: A Prot?g? plug-in for annotated corpus construction 
 
Philip V. Ogren 
Division of Biomedical Informatics 
Mayo Clinic 
Rochester, MN, USA 
Ogren.Philip@mayo.edu 
 
 
 
 
Abstract 
A general-purpose text annotation tool 
called Knowtator is introduced.  Knowtator 
facilitates the manual creation of annotated 
corpora that can be used for evaluating or 
training a variety of natural language proc-
essing systems.  Building on the strengths 
of the widely used Prot?g? knowledge rep-
resentation system, Knowtator has been 
developed as a Prot?g? plug-in that lever-
ages Prot?g??s knowledge representation 
capabilities to specify annotation schemas.  
Knowtator?s unique advantage over other 
annotation tools is the ease with which 
complex annotation schemas (e.g. schemas 
which have constrained relationships be-
tween annotation types) can be defined and 
incorporated into use.  Knowtator is avail-
able under the Mozilla Public License 1.1 
at http://bionlp.sourceforge.net/Knowtator.   
1 Introduction 
Knowtator is a general-purpose text annotation tool 
for creating annotated corpora suitable for evaluat-
ing Natural Language Processing (NLP) systems.  
Such corpora consist of texts (e.g. documents, ab-
stracts, or sentences) and annotations that associate 
structured information (e.g. POS tags, named enti-
ties, shallow parses) with extents of the texts.  An 
annotation schema is a specification of the kinds of 
annotations that can be created.  Knowtator pro-
vides a very flexible mechanism for defining anno-
tation schemas.  This allows it to be employed for 
a large variety of corpus annotation tasks.    
Prot?g? is a widely used knowledge representa-
tion system that facilitates construction and visu-
alization of knowledge-bases (Noy, 2003)1.  A 
Prot?g? knowledge-base typically consists of class, 
instance, slot, and facet frames.  Class definitions 
represent the concepts of a domain and are organ-
ized in a subsumption hierarchy.  Instances corre-
spond to individuals of a class.  Slots define 
properties of a class or instance and relationships 
between classes or instances.  Facets constrain the 
values that slots can have.   
Prot?g? has garnered widespread usage by pro-
viding an architecture that facilitates the creation 
of third-party plug-ins such as visualization tools 
and inference engines.  Knowtator has been im-
plemented as a Prot?g? plug-in and runs in the Pro-
t?g? environment.  In Knowtator, an annotation 
schema is defined with Prot?g? class, instance, 
slot, and facet definitions using the Prot?g? knowl-
edge-base editing functionality.  The defined anno-
tation schema can then be applied to a text 
annotation task without having to write any task 
specific software or edit specialized configuration 
files.  Annotation schemas in Knowtator can model 
both syntactic (e.g. shallow parses) and semantic 
phenomena (e.g. protein-protein interactions).   
2 Related work 
There exists a plethora of manual text annotation 
tools for creating annotated corpora.  While it has 
been common for individual research groups to 
build customized annotation tools for their specific 
                                                          
1 http://protege.stanford.edu 
273
 
Figure 1  Simple co-reference annotations in Knowtator 
 
annotation tasks, several text annotation tools have 
emerged in the last few years that can be employed 
to accomplish a wide variety of annotation tasks.  
Some of the better general-purpose annotation 
tools include Callisto2, WordFreak3 (Morton and 
LaCivita, 2003), GATE4, and MMAX25.  Each of 
these tools is distributed with a limited number of 
annotation tasks that can be used ?out of the box.?  
Many of the tasks that are provided can be custom-
ized to a limited extent to suit the requirements of a 
user?s annotation task via configuration files.   In 
Callisto, for example, a simple annotation schema 
can be defined with an XML DTD that allows the 
creation of an annotation schema that is essentially 
a tag set augmented with simple (e.g. string) attrib-
utes for each tag.  In addition to configuration files, 
WordFreak provides a plug-in architecture for cre-
ating task specific code modules that can be inte-
grated into the user interface.   
A complex annotation schema might include hi-
erarchical relationships between annotation types 
and constrained relationships between the types. 
Creating such an annotation schema can be a for-
midable challenge for the available tools either 
                                                          
2 http://callisto.mitre.org  
3 http://wordfreak.sourceforge.net  
4 http://gate.ac.uk/.  GATE is a software architecture for NLP that has, as one of 
its many components, text annotation functionality.  
5http://mmax.eml-research.de/.  
because configuration options are too limiting or 
because implementing a new plug-in is too expen-
sive or time consuming.   
3 Implementation 
3.1 Annotation schema 
Knowtator approaches the definition of an annota-
tion schema as a knowledge engineering task by 
leveraging Prot?g??s strengths as a knowledge-
base editor.  Prot?g? has user interface components 
for defining class, instance, slot, and facet frames.  
A Knowtator annotation schema is created by de-
fining frames using these user interface compo-
nents as a knowledge engineer would when 
creating a conceptual model of some domain.  For 
Knowtator the frame definitions model the phe-
nomena that the annotation task seeks to capture.   
As a simple example, the co-reference annota-
tion task that comes with Callisto can be modeled 
in Prot?g? with two class definitions called mark-
able and chain.  The chain class has two slots ref-
erences and primary_reference which are 
constrained by facets to have values of type mark-
able.  This simple annotation schema can now be 
used to annotate co-reference phenomena occur-
274
ring in text using Knowtator.  Annotations in 
Knowtator created using this simple annotation 
schema are shown in Figure 1.   
A key strength of Knowtator is its ability to re-
late annotations to each other via the slot defini-
tions of the corresponding annotated classes.  In 
the co-reference example, the slot references of the 
class chain relates the markable annotations for the 
text extents ?the cat? and ?It? to the chain annota-
tion.  The constraints on the slots ensure that the 
relationships between annotations are consistent.   
Prot?g? is capable of representing much more 
sophisticated and complex conceptual models 
which can be used, in turn, by Knowtator for text 
annotation.  Also, because Prot?g? is often used to 
create conceptual models of domains relating to 
biomedical disciplines, Knowtator is especially 
well suited for capturing named entities and rela-
tions between named entities for those domains.   
3.2 Features 
In addition to its flexible annotation schema defini-
tion capabilities, Knowtator has many other fea-
tures that are useful for executing text annotation 
projects.  A consensus set creation mode allows 
one to create a gold standard using annotations 
from multiple annotators.  First, annotations from 
multiple annotators are aggregated into a single 
Knowtator annotation project.  Annotations that 
represent agreement between the annotators are 
consolidated such that the focus of further human 
review is on disagreements between annotators. 
Inter-annotator agreement (IAA) metrics pro-
vide descriptive reports of consistency between 
two or more annotators.  Several different match 
criteria (i.e. what counts as agreement between 
multiple annotations) have been implemented.  
Each gives a different perspective on how well 
annotators agree with each other and can be useful 
for uncovering systematic differences.  IAA can 
also be calculated for selected annotation types 
giving very fine grained analysis data.   
Knowtator provides a pluggable infrastructure 
for handling different kinds of text source types.  
By implementing a simple interface, one can anno-
tate any kind of text (e.g. from xml or a relational 
database) with a modest amount of coding.   
Knowtator provides stand-off annotation such 
that the original text that is being annotated is not 
modified.  Annotation data can be exported to a 
simple XML format.   
Annotation filters can be used to view a subset 
of available annotations.  This may be important if, 
for example, viewing only named entity annota-
tions is desired in an annotation project that also 
contains many part-of-speech annotations.  Filters 
are also used to focus IAA analysis and the export 
of annotations to XML.   
Knowtator can be run as a stand-alone system 
(e.g. on a laptop) without a network connection.  
For increased scalability, Knowtator can be used 
with a relational database backend (via JDBC).   
Knowtator and Prot?g? are provided under the 
Mozilla Public License 1.1 and are freely available 
with source code at http://bionlp.sourceforge.net/ 
Knowtator and http://protege.stanford.edu, respec-
tively.  Both applications are implemented in the 
Java programming language and have been suc-
cessfully deployed and used in the Windows, Ma-
cOS, and Linux environments.   
4 Conclusion 
Knowtator has been developed to leverage the 
knowledge representation and editing capabilities 
of the Prot?g? system.  By modeling syntactic 
and/or semantic phenomena using Prot?g? frames, 
a wide variety of annotation schemas can be de-
fined and used for annotating text.  New annotation 
tasks can be created without writing new software 
or creating specialized configuration files.  Know-
tator also provides additional features that make it 
useful for real-world multi-person annotation tasks.   
References  
Thomas Morton and Jeremy LaCivita.  2003. Word-
Freak: An Open Tool for Linguistic Annotation, Pro-
ceedings of NLT-NAACL, pp. 17-18.   
Noy, N. F., M. Crubezy, et al  2003. Protege-2000: an 
open-source ontology-development and knowledge-
acquisition environment. AMIA Annual Symposium 
Proceedings: 953. 
 
 
275
Proceedings of the ACL-ISMB Workshop on Linking Biological Literature, Ontologies and Databases: Mining
Biological Semantics, pages 38?45, Detroit, June 2005. c?2005 Association for Computational Linguistics
Corpus design for biomedical natural language processing
K. Bretonnel Cohen
Center for Computational Pharmacology
U. of Colorado School of Medicine
Aurora, Colorado
kevin.cohen@gmail.com
Lynne Fox
Denison Library
U. of Colorado Health Sciences Center
Denver, Colorado
lynne.fox@uchsc.edu
Philip V. Ogren
Center for Computational Pharmacology
U. of Colorado School of Medicine
Aurora, Colorado
philip.ogren@uchsc.edu
Lawrence Hunter
Center for Computational Pharmacology
U. of Colorado School of Medicine
Aurora, Colorado
larry.hunter@uchsc.edu
Abstract
This paper classifies six publicly avail-
able biomedical corpora according to var-
ious corpus design features and charac-
teristics. We then present usage data for
the six corpora. We show that corpora
that are carefully annotated with respect
to structural and linguistic characteristics
and that are distributed in standard for-
mats are more widely used than corpora
that are not. These findings have implica-
tions for the design of the next generation
of biomedical corpora.
1 Introduction
A small number of data sets for evaluating the per-
formance of biomedical language processing (BLP)
systems on a small number of task types have been
made publicly available by their creators (Blaschke
et al 19991, Craven and Kumlein 19992, Puste-
jovsky et al 20023, Franze?n et al 20024, Collier
et al 19995, Tanabe et al 20056). From a biolog-
ical perspective, a number of these corpora (PDG,
GENIA, Medstract, Yapex, GENETAG) are excep-
tionally well curated. From the perspective of sys-
1We refer to this corpus as the Protein Design Group (PDG)
corpus.
2We refer to this as the University of Wisconsin corpus.
3The Medstract corpus.
4The Yapex corpus.
5The GENIA corpus.
6Originally the BioCreative Task 1A data set, now known as
the GENETAG corpus.
tem evaluation, a number of these corpora (Wiscon-
sin, GENETAG) are very well designed, with large
numbers of both positive and negative examples for
system training and testing. Despite the positive at-
tributes of all of these corpora, they vary widely in
their external usage rates: some of them have been
found very useful in the natural language process-
ing community outside of the labs that created them,
as evinced by their high rates of usage in system
construction and evaluation in the years since they
have been released. In contrast, others have seen lit-
tle or no use in the community at large. These data
sets provide us with an opportunity to evaluate the
consequences of a variety of approaches to biomed-
ical corpus construction. We examine these corpora
with respect to a number of design features and other
characteristics, and look for features that character-
ize widely used?and infrequently used?corpora.
Our findings have implications for how the next gen-
eration of biomedical corpora should be constructed,
and for how the existing corpora can be modified to
make them more widely useful.
2 Materials and methods
Table 1 lists the publicly available biomedical cor-
pora of which we are aware. We omit discussion
here of the corpus currently in production by the
University of Pennsylvania and the Children?s Hos-
pital of Philadelphia (Kulick et al 2004), since it is
not yet available in finished form. We also omit text
collections from our discussion. By text collection
we mean textual data sets that may include metadata
about documents, but do not contain mark-up of the
document contents. So, the OHSUMED text collec-
38
Table 1: Name, date, genre, and size for the six cor-
pora. Size is in words.
Name date genre size
PDG 1999 Sentences 10,291
Wisconsin 1999 Sentences 1,529,731
GENIA 1999 Abstracts 432,560
MEDSTRACT 2001 Abstracts 49,138
Yapex 2002 Abstracts 45,143
GENETAG 2004 Sentences 342,574
Table 2: Low- and high-level tasks to which the six
corpora are applicable. SS is sentence segmentation,
T is tokenization, and POS is part-of-speech tagging.
EI is entity identification, IE is information extrac-
tion, A is acronym/abbreviation definition, and C is
coreference resolution.
Name SS T POS EI IE A C
PDG    
Wisconsin    
GENIA        
Medstract      
Yapex  
GENETAG  
tion (Hersh et al 1994) and the TREC Genomics
track data sets (Hersh and Bhupatiraju 2003, Hersh
et al 2004) are excluded from this work, although
their utility in information retrieval is clear.
Table 1 lists the corpora, and for each corpus,
gives its release date (or the year of the correspond-
ing publication), the genre of the contents of the cor-
pus, and the size of the corpus7 .
The left-hand side of Table 2 lists the data sets
and, for each one, indicates the lower-level general
language processing problems that it could be ap-
plied to, either as a source of training data or for
evaluating systems that perform these tasks. We
considered here sentence segmentation, word tok-
enization, and part-of-speech (POS) tagging.
The right-hand side of Table 2 shows the higher-
7Sizes are given in words. Published descriptions of
the corpora don?t generally give size in words, so this
data is based on our own counts. See the web site at
http://compbio.uchsc.edu/corpora for details on how we did the
count for each corpus.
level tasks to which the various corpora can be
applied. We considered here entity identifica-
tion, information (relation) extraction, abbrevia-
tion/acronym definition, and coreference resolution.
(Information retrieval is approached via text collec-
tions, versus corpora.) These tasks are directly re-
lated to the types of semantic annotation present
in each corpus. The three EI-only corpora (GE-
NIA, Yapex, GENETAG) are annotated with seman-
tic classes of relevance to the molecular biology do-
main. In the case of the Yapex and GENETAG cor-
pora, this annotation uses a single semantic class,
roughly equivalent to the gene or gene product. In
the case of the GENIA corpus, the annotation re-
flects a more sophisticated, if not widely used, on-
tology. The Medstract corpus uses multiple seman-
tic classes, including gene, protein, cell type, and
molecular process. In all of these cases, the se-
mantic annotation was carefully curated, and in one
(GENETAG) it includes alternative analyses. Two
of the corpora (PDG, Wisconsin) are indicated in Ta-
ble 2 as being applicable to both entity identification
and information extraction tasks. From a biologi-
cal perspective, the PDG corpus has exceptionally
well-curated positive examples. From a linguistic
perspective, it is almost unannotated. For each sen-
tence, the entities are listed, but their locations in
the text are not indicated, making them applicable
to some definitions of the entity identification task
but not others. The Wisconsin corpus contains both
positive and negative examples. For each example,
entities are listed in a normalized form, but without
clear pointers to their locations in the text, making
this corpus similarly difficult to apply to many defi-
nitions of the entity identification task.
The Medstract corpus is unique among these in
being annotated with coreferential equivalence sets,
and also with acronym expansions.
All six corpora draw on the same subject matter
domain?molecular biology?but they vary widely
with respect to their level of semantic restriction
within that relatively broad category. One (GE-
NIA) is restricted to the subdomain of human
blood cell transcription factors. Another (Yapex)
combines data from this domain with abstracts
on protein binding in humans. The GENETAG
corpus is considerably broader in topic, with all
of PubMed/MEDLINE serving as a potential data
39
Table 3: External usage rates. The systems column
gives the count of the number of systems that actu-
ally used the dataset, as opposed to publications that
cited the paper but did not use the data itself. Age is
in years as of 2005.
Name age systems
GENIA 6 21
GENETAG 1 8
Yapex 3 6
Medstract 4 3
Wisconsin 6 1
PDG 6 0
source. The Medstract corpus contains biomedical
material not apparently related to molecular biology.
The PDG corpus is drawn from a very narrow subdo-
main on protein-protein interactions. The Wiscon-
sin corpus is composed of data from three separate
sub-domains: protein-protein interactions, subcellu-
lar localization of proteins, and gene/disease associ-
ations.
Table 3 shows the number of systems built out-
side of the lab that created the corpus that used each
of the data sets described in Tables 1 and 2. The
counts in this table reflect work that actually used
the datasets, versus work that cites the publication
that describes the data set but doesn?t actually use
the data set. We assembled the data for these counts
by consulting with the creators of the data sets and
by doing our own literature searches8 . If a system is
described in multiple publications, we count it only
once, so the number of systems is slightly smaller
than the number of publications.
3 Results
Even without examining the external usage data, two
points are immediately evident from Tables 1 and 2:
  Only one of the currently publicly available
corpora (GENIA) is suitable for evaluating per-
formance on basic preprocessing tasks.
8In the cases of the two corpora for which we found only
zero or one external usage, this search was repeated by an expe-
rienced medical librarian, and included reviewing 67 abstracts
or full papers that cite Blaschke et al (1999) and 37 that cite
Craven and Kumlein (1999).
  These corpora include only a very limited range
of genres: only abstracts and roughly sentence-
sized inputs are represented.
Examination of Table 3 makes another point im-
mediately clear. The currently publicly available
corpora fall into two groups: ones that have had a
number of external applications (GENIA, GENE-
TAG, and Yapex), and ones that have not (Medstract,
Wisconsin, and PDG). We now consider a number
of design features and other characteristics of these
corpora that might explain these groupings9 .
3.1 Effect of age
We considered the very obvious hypothesis that it
might be length of time that a corpus has been avail-
able that determines the amount of use to which it
has been put. (Note that we use the terms ?hypothe-
sis? and ?effect? in a non-statistical sense, and there
is no significance-testing in the work reported here.)
Tables 1 and 3 show clearly that this is not the case.
The age of the PDG, Wisconsin, and GENIA data
is the same, but the usage rates are considerably
different?the GENIA corpus has been much more
widely used. The GENETAG corpus is the newest,
but has a relatively high usage rate. Usage of a cor-
pus is determined by factors other than the length of
time that it has been available.
3.2 Effect of size
We considered the hypothesis that size might be the
determinant of the amount of use to which a corpus
is put?perhaps smaller corpora simply do not pro-
vide enough data to be helpful in the development
and validation of learning-based systems. We can
9Three points should be kept in mind with respect to this
data. First, although the sample includes all of the corpora that
we are aware of, it is small. Second, there is a variety of po-
tential confounds related to sociological factors which we are
aware of, but do not know how to quantify. One of these is the
effect of association between a corpus and a shared task. This
would tend to increase the usage of the corpus, and could ex-
plain the usage rates of GENIA and GENETAG, although not
that of Yapex. Another is the effect of association between a
corpus and an influential scientist. This might tend to increase
the usage of the corpus, and could explain the usage rate of
GENIA, although not that of GENETAG. Finally, there may
be interactions between any of these factors, or as a reviewer
pointed out, there may be a separate explanation for the usage
rate of each corpus in this study. Nevertheless, the analysis of
the quantifiable factors presented above clearly provides useful
information about the design of successful corpora.
40
reject this hypothesis: the Yapex corpus is one of
the smallest (a fraction of the size of the largest, and
only roughly a tenth of the size of GENIA), but has
achieved fairly wide usage. The Wisconsin corpus
is the largest, but has a very low usage rate.
3.3 Effect of structural and linguistic
annotation
We expected a priori that the corpus with the most
extensive structural and linguistic annotation would
have the highest usage rate. (In this context, by
structural annotation we mean tokenization and sen-
tence segmentation, and by linguistic annotation we
mean POS tagging and shallow parsing.) There isn?t
a clear-cut answer to this.
The GENIA corpus is the only one with curated
structural and POS annotation, and it has the highest
usage rate. This is consistent with our initial hypoth-
esis.
On the other hand, the Wisconsin corpus could
be considered the most ?deeply? linguistically an-
notated, since it has both POS annotation and?
unique among the various corpora?shallow pars-
ing. It nevertheless has a very low usage rate. How-
ever, the comparison is not clearcut, since both the
POS tagging and the shallow parsing are fully au-
tomatic and not manually corrected. (Additionally,
the shallow parsing and the tokenization on which
it is based are somewhat idiosyncratic.) It is clear
that the Yapex corpus has relatively high usage de-
spite the fact that it is, from a linguistic perspective,
very lightly annotated (it is marked up for entities
only, and nothing else). To our surprise, structural
and linguistic annotation do not appear to uniquely
determine usage rate.
3.4 Effect of format
Annotation format has a large effect on usage. It
bears repeating that these six corpora are distributed
in six different formats?even the presumably sim-
ple task of populating the Size column in Table 1
required writing six scripts to parse the various data
files. The two lowest-usage corpora are annotated in
remarkably unique formats. In contrast, the three
more widely used corpora are distributed in rela-
tively more common formats. Two of them (GENIA
and Yapex) are distributed in XML, and one of them
(GENIA) offers a choice for POS tagging informa-
tion between XML and the whitespace-separated,
one-token-followed-by-tags-per-line format that is
common to a number of POS taggers and parsers.
The third (GENETAG) is distributed in the widely
used slash-attached format (e.g. sense/NN).
3.5 Effect of semantic annotation
The data in Table 2 and Table 3 are consistent with
the hypothesis that semantic annotation predicts us-
age. The claim would be that corpora that are
built specifically for entity identification purposes
are more widely used than corpora of other types,
presumably due to a combination of the importance
of the entity identification task as a prerequisite to
a number of other important applications (e.g. in-
formation extraction and retrieval) and the fact that
it is still an unsolved problem. There may be some
truth to this, but we doubt that this is the full story:
there are large differences in the usage rates of the
three EI corpora, suggesting that semantic annota-
tion is not the only relevant design feature. If this
analysis is in fact correct, then certainly we should
see a reduction in the use of all three of these corpora
once the EI problem is solved, unless their semantic
annotations are extended in new directions.
3.6 Effect of semantic domain
Both the advantages and the disadvantages of re-
stricted domains as targets for language processing
systems are well known, and they seem to balance
out here. The scope of the domain does not affect
usage: both the low-use and higher-use groups of
corpora contain at least one highly restricted domain
(GENIA in the high-use group, and PDG in the low-
use group) and one broader domain (GENETAG in
the high-use group, and Wisconsin in the lower-use
group).
4 Discussion
The data presented in this paper show clearly that ex-
ternal usage rates vary widely for publicly available
biomedical corpora. This variability is not related
to the biological relevance of the corpora?the PDG
and Wisconsin corpora are clearly of high biologi-
cal relevance as evinced by the number of systems
that have tackled the information extraction tasks
that they are meant to support. Additionally, from a
biological perspective, the quality of the data in the
41
PDG corpus is exceptionally high. Rather, our data
suggest that basic issues of distribution format and
of structural and linguistic annotation seem to be the
strongest predictors of how widely used a biomed-
ical corpus will be. This means that as builders of
of data sources for BLP, we can benefit from the ex-
tensive experience of the corpus linguistics world.
Based on that experience, and on the data that we
have presented in this paper, we offer a number of
suggestions for the design of the next generation of
biomedical corpora.
We also suggest that the considerable invest-
ments already made in the construction of the less-
frequently-used corpora can be protected by modify-
ing those corpora in accordance with these sugges-
tions.
Leech (1993) and McEnery and Wilson (2001),
coming from the perspective of corpus linguistics,
identify a number of definitional issues and design
maxims for corpus construction. Some of these are
quite relevant to the current state of biomedical cor-
pus construction. We frame the remainder of our
discussion in terms of these issues and maxims.
4.1 Level of annotation
From a definitional point of view, annotation is one
of the distinguishing points of a corpus, as opposed
to a text collection. Perhaps the most salient char-
acteristic of the currently publicly available corpora
is that from a linguistic or language processing per-
spective, with the exception of GENIA and GENE-
TAG, they are barely annotated at all. For example,
although POS tagging has possibly been the sine qua
non of the usable corpus since the earliest days of
the modern corpus linguistic age, five of the six cor-
pora listed in Table 2 either have no POS tagging
or have only automatically generated, uncorrected
POS tags. The GENIA corpus, with its carefully cu-
rated annotation of sentence segmentation, tokeniza-
tion, and part-of-speech tagging, should serve as a
model for future biomedical corpora in this respect.
It is remarkable that with just these levels of anno-
tation (in addition to its semantic mark-up), the GE-
NIA corpus has been applied to a wide range of task
types other than the one that it was originally de-
signed for. Eight papers from COLING 2004 (Kim
et al 2004) used it for evaluating entity identifica-
tion tasks. Yang et al (2002) adapted a subset of
the corpus for use in developing and testing a coref-
erence resolution system. Rinaldi et al (2004) used
it to develop and test a question-answering system.
Locally, it has been used in teaching computational
corpus linguistics for the past two years. We do not
claim that it has not required extension for some of
these tasks?our claim is that it is its annotation on
these structural and linguistic levels, in combination
with its format, that has made these extensions prac-
tical.
4.1.1 Formatting choices and formatting
standardization
A basic desideratum for a corpus is recoverabil-
ity: it should be possible to map from the annotation
to the raw text. A related principle is that it should
be easy for the corpus user to extract all annotation
information from the corpus, e.g. for external stor-
age and processing: ?in other words, the annotated
corpus should allow the maximum flexibility for ma-
nipulation by the user? (McEnery and Wilson, p.
33). The extent to which these principles are met
is a function of the annotation format. The currently
available corpora are distributed in a variety of one-
off formats. Working with any one of them requires
learning a new format, and typically writing code
to access it. At a minimum, none of the non-XML
corpora meet the recoverability criterion. None10 of
these corpora are distributed in a standoff annotation
format. Standoff annotation is the strategy of stor-
ing annotation and raw text separately (Leech 1993).
Table 4 contrasts the two. Non-standoff annota-
tion at least obscures?more frequently, destroys?
important aspects of the structure of the text itself,
such as which textual items are and are not imme-
diately adjacent. Using standoff annotation, there is
no information loss whatsoever. Furthermore, in the
standoff annotation strategy, the original input text
is immediately available in its raw form. In contrast,
in the non-standoff annotation strategy, the original
must be retrieved independently or recovered from
the annotation (if it is recoverable at all). The stand-
off annotation strategy was relatively new at the time
that most of the corpora in Table 1 were designed,
but by now has become easy to implement, in part
10The semantic annotation of the GENETAG corpus is in a
standoff format, but neither the tokenization nor the POS tag-
ging is.
42
Table 4: Contrasting standoff and non-standoff an-
notation
Raw text
MLK2 has a role in vesicle formation
Non-standoff annotation
MLK2/NN has/VBZ a/DT role/NN in/IN
vesicle/NN formation/NN
Standoff annotation
  POS=?NN? start=0 end=3 
  POS=?VBZ? start=5 end=7 
  POS=?DT? start=9 end=9 
  POS=?NN? start=11 end=14 
  POS=?IN? start=16 end=17 
  POS=?NN? start=19 end=25 
  POS=?NN? start=27 end=35 
due to the availability of tools such as the University
of Pennsylvania?s WordFreak (Morton and LaCivita
2003).
Crucially, this annotation should be based on
character offsets, avoiding a priori assumptions
about tokenization. See Smith et al (2005) for an
approach to refactoring a corpus to use character off-
sets.
4.1.2 Guidelines
The maxim of documentation suggests that anno-
tation guidelines should be published. Further, ba-
sic data on who did the annotations and on their
level of agreement should be available. The cur-
rently available datasets mostly lack assessments of
inter-annotator agreement, utilize a small or unspec-
ified number of annotators, and do not provide pub-
lished annotation guidelines. (We note the Yang et
al. (2002) coreference annotation guidelines, which
are excellent, but the corresponding corpus is not
publicly available.) This situation can be remedied
by editors, who should insist on publication of all
of these. The GENETAG corpus is notable for the
detailed documentation of its annotation guidelines.
We suspect that the level of detail of these guidelines
contributed greatly to the success of some rule-based
approaches to the EI task in the BioCreative compe-
tition, which utilized an early version of this corpus.
4.1.3 Balance and representativeness
Corpus linguists generally strive for a well-
structured stratified sample of language, seeking to
?balance? in their data the representation of text
types, different sorts of authors, and so on. Within
the semantic domain of molecular biology texts,
an important dimension on which to balance is the
genre or text type.
As is evident from Table 1, the extant datasets
draw on a very small subset of the types of genres
that are relevant to BLP: we have not done a good
job yet of observing the principle of balance or rep-
resentativeness. The range of genres that exist in the
research (as opposed to clinical) domain alone in-
cludes abstracts, full-text articles, GeneRIFs, defini-
tions, and books. We suggest that all of these should
be included in future corpus development efforts.
Some of these genres have been shown to have
distinguishing characteristics that are relevant to
BLP. Abstracts and isolated sentences from them
are inadequate, and also unsuited to the opportuni-
ties that are now available to us for text data mining
with the recent announcement of the NIH?s new pol-
icy on availability of full-text articles (NIH 2005).
This policy will result in the public availability of
a large and constantly growing archive of current,
full-text publications. Abstracts and sentences are
inadequate in that experience has shown that signifi-
cant amounts of data are not found in abstracts at all,
but are present only in the full texts of articles, some-
times not even in the body of the text itself, but rather
in tables and figure captions (Shatkay and Feldman
2003). They are not suited to the upcoming opportu-
nities in that it is not clear that practicing on abstracts
will let us build the necessary skills for dealing with
the flood of full-text articles that PubMedCentral
is poised to deliver to us. Furthermore, there are
other types of data?GeneRIFs and domain-specific
dictionary definitions, for instance?that are fruit-
ful sources of biological knowledge, and which may
actually be easier to process automatically than ab-
stracts. Space does not permit justifying the impor-
tance of all of these genres, but we discuss the ratio-
nale for including full text at some length due to the
recent NIH announcement and due to the large body
of evidence that can currently be brought to bear on
the issue. A growing body of recent research makes
43
two points clear: full-text articles are different from
abstracts, and full-text articles must be tapped if we
are to build high-recall text data mining systems.
Corney et al (2004) looked directly at the effec-
tiveness of information extraction from full-text ar-
ticles versus abstracts. They found that recall from
full-text articles was more than double that from ab-
stracts. Analyzing the relative contributions of the
abstracts and the full articles, they found that more
than half of the interactions that they were able to
extract were found in the full text and were absent in
the abstract.
Tanabe and Wilbur (2002) looked at the perfor-
mance on full-text articles of an entity identification
system that had originally been developed and tested
using abstracts. They found different false positive
rates in the Methods sections compared to other sec-
tions of full-text articles. This suggests that full-text
articles, unlike abstracts, will require parsing of doc-
ument structure. They also noted a range of prob-
lems related to the wider range of characters (includ-
ing, e.g., superscripts and Greek letters) that occurs
in full-text articles, as opposed to abstracts.
Schuemie et al (2004) examined a set of 3902
full-text articles from Nature Genetics and BioMed
Central, along with their abstracts. They found that
about twice as many MeSH concepts were men-
tioned in the full-text articles as in the abstracts.
They also found that full texts contained a larger
number of unique gene names than did abstracts,
with an average of 2.35 unique gene names in the
full-text articles, but an average of only 0.61 unique
gene names in the abstracts.
It seems clear that for biomedical text data min-
ing systems to reach anything like their full poten-
tial, they will need to be able to handle full-text in-
puts. However, as Table 1 shows, no publicly avail-
able corpus contains full-text articles. This is a defi-
ciency that should be remedied.
5 Conclusion
5.1 Best practices in biomedical corpus
construction
We have discussed the importance of recoverabil-
ity, publication of guidelines, balance and represen-
tativeness, and linguistic annotation. Corpus main-
tenance is also important. Bada et al (2004) point
out the role that an organized and responsive main-
tenance plan has played in the success of the Gene
Ontology. It seems likely that the continued devel-
opment and maintenance reflected in the three ma-
jor releases of GENIA (Ohta et al 2002, Kim et al
2003) have contributed to its improved quality and
continued use over the years.
5.2 A testable prediction
We have interpreted the data on the characteristics
and usage rates of the various datasets discussed in
this paper as suggesting that datasets that are devel-
oped in accordance with basic principles of corpus
linguistics are more useful, and therefore more used,
than datasets that are not.
A current project at the University of Pennsyl-
vania and the Children?s Hospital of Philadelphia
(Kulick et al 2004) is producing a corpus that fol-
lows many of these basic principles. We predict that
this corpus will see wide use by groups other than
the one that created it.
5.3 The next step: grounded references
The logical next step for BLP corpus construction
efforts is the production of corpora in which entities
and concepts are grounded with respect to external
models of the world (Morgan et al 2004).
The BioCreative Task 1B data set construction ef-
fort provides a proof-of-concept of the plausibility
of building BLP corpora that are grounded with re-
spect to external models of the world, and in partic-
ular, biological databases. These will be crucial in
taking us beyond the stage of extracting information
about text strings, and towards mining knowledge
about known, biologically relevant entities.
6 Acknowledgements
This work was supported by NIH grant R01-
LM008111. The authors gratefully acknowledge
helpful discussions with Lynette Hirschman, Alex
Morgan, and Kristofer Franze?n, and thank Sonia
Leach and Todd A. Gibson for LATEXassistance.
Christian Blaschke, Mark Craven, Lorraine Tanabe,
and again Kristofer Franze?n provided helpful data.
We thank all of the corpus builders for their gen-
erosity in sharing their valuable resources.
44
References
Bada, Michael; Robert Stevens; et al 2004. A short
study on the success of the Gene Ontology. Journal of
web semantics 1(2):235-240.
Blaschke, Christian; Miguel A. Andrade; Christos
Ouzounis; and Alfonso Valencia. 1999. Automatic
extraction of biological information from scientific
text: protein-protein interactions. ISMB-99, pp. 60-67.
AAAI Press.
Collier, Nigel, Hyun Seok Park, Norihiro Ogata, Yuka
Tateisi, Chikashi Nobata, Takeshi Sekimizu, Hisao
Imai and Jun?ichi Tsujii. 1999. The GENIA project:
corpus-based knowledge acquisition and information
extraction from genome research papers. EACL 1999.
Corney, David P.A.; Bernard F. Buxton; William B.
Langdon; and David T. Jones. 2004. BioRAT: ex-
tracting biological information from full-length pa-
pers. Bioinformatics 20(17):3206-3213.
Craven, Mark; and Johan Kumlein. 1999. Constructing
biological knowledge bases by extracting information
from text sources. ISMB-99, pp. 77-86. AAAI Press.
Franze?n, Kristofer; Gunnar Eriksson; Fredrik Olsson;
Lars Asker Per Lide?n; and Joakim Co?ster. 2002. Pro-
tein names and how to find them. International Jour-
nal of Medical Informatics, 67(1-3), pp. 49-61.
Hersh, William; Chris Buckley; TJ Leone; and David
Hickam. 1994. OHSUMED: an interactive retrieval
evaluation and new large test collection for research.
SIGIR94, pp. 192-201.
Hersh, William; and Ravi Teja Bhupatiraju. 2003. TREC
genomics track overview. TREC 2003, pp. 14-23.
Hersh et al 2004. TREC 2004 genomics track overview.
TREC Notebook.
Kim, Jin-Dong; Tomoko Ohta; Yuka Tateisi; and Jun?ichi
Tsujii. 2003. GENIA corpus?a semantically an-
notated corpus for bio-textmining. Bioinformatics
19(Suppl. 1):i180-i182.
Kim, Jin-Dong; Tomoko Ohta; Yoshimasa Tsuruoka;
and Yuka Tateisi. 2004. Introduction to the bio-
entity recognition task at JNLPBA. Proc. interna-
tional joint workshop on natural language processing
in biomedicine and its applications, pp. 70-75.
Kulick, Seth; Ann Bies; Mark Liberman; Mark Mandel;
Ryan McDonald; Martha Palmer; Andrew Schein; and
Lyle Ungar. 2004. Integrated annotation for biomedi-
cal information extraction. BioLink 2004, pp. 61-68.
Leech, G. 1993. Corpus annotation schemes. Literary
and linguistic computing 8(4):275-281.
McEnery, Tony; and Andrew Wilson. 2001. Corpus lin-
guistics, 2nd edition. Edinburgh University Press.
Morgan, Alexander A.; Lynette Hirschman; Marc
Colosimo; Alexander S. Yeh; and Jeff B. Colombe.
2004. Gene name identification and normalization us-
ing a model organism database. JBMI 37:396-410.
Morton, Thomas; and Jeremy LaCivita. 2003. Word-
Freak: an open tool for linguistic annotation.
HLT/NAACL 2003: demonstrations, pp. 17-18.
NIH (National Institutes of Health). 2005.
http://www.nih.gov/news/pr/feb2005/od-03.htm
Ohta, Tomoko; Yuka Tateisi; and Jin-Dong Kim. 2002.
The GENIA corpus: an annotated research abstract
corpus in molecular biology domain. HLT 2002, pp.
73-77.
Pustejovsky, James; Jose? Castan?o; R. Saur?i; A.
Rumshisky; J. Zhang; and W. Luo. 2002. Medstract:
creating large-scale information servers for biomedical
libraries. Proc. workshop on natural language pro-
cessing in the biomedical domain, pp. 85-92. Associa-
tion for Computational Linguistics.
Rinaldi, Fabio; James Dowdall; Gerold Schneider; and
Andreas Persidis. 2004. Answering questions in the
genomics domain. Proc. ACL 2004 workshop on ques-
tion answering in restricted domains, pp. 46-53.
Schuemie, M.J.; M. Weeber; B.J. Schijvenaars; E.M.
van Mulligen; C.C. van der Eijk; R. Jelier; B. Mons;
and J.A. Kors. 2004. Distribution of information in
biomedical abstracts and full-text publications. Bioin-
formatics 20(16):2597-2604.
Shatkay, Hagit; and Ronen Feldman. 2003. Mining the
biomedical literature in the genomic era: an overview.
Journal of computational biology 10(6):821-855.
Smith, Lawrence H.; Lorraine Tanabe; Thomas Rind-
flesch; and W. John Wilbur. 2005. MedTag: a col-
lection of biomedical annotations. BioLINK 2005, this
volume.
Tanabe, Lorraine; and L. John Wilbur. 2002. Tagging
gene and protein names in full text articles. Proc.
ACL workshop on natural language processing in the
biomedical domain, pp. 9-13.
Tanabe, Lorraine; Natalie Xie; Lynne H. Thom; Wayne
Matten; and W. John Wilbur. 2005. GENETAG: a
tagged corpus for gene/protein named entity recogni-
tion. BMC Bioinformatics 6(Suppl. 1):S3.
Yang, Xiaofeng; Guodong Zhou; Jian Su; and Chew Lim
Tan. Improving noun phrase coreference resolution by
matching strings. 2002. IJCNLP04, pp. 326-333.
45
Proceedings of the Workshop on BioNLP: Shared Task, pages 50?58,
Boulder, Colorado, June 2009. c?2009 Association for Computational Linguistics
High-precision biological event extraction with a concept recognizer
K. Bretonnel Cohen?, Karin Verspoor?, Helen L. Johnson, Chris Roeder,
Philip V. Ogren, William A. Baumgartner Jr., Elizabeth White, Hannah Tipney, and Lawrence Hunter
Center for Computational Pharmacology
University of Colorado Denver School of Medicine
PO Box 6511, MS 8303, Aurora, CO 80045 USA
kevin.cohen@gmail.com, karin.verspoor@ucdenver.edu, helen.linguist@gmail.com,
chris.roeder@ucdenver.edu, philip@ogren.info, william.baumgartner@ucdenver.edu,
elizabeth.white@colorado.edu, hannah.tipney@ucdenver.edu, larry.hunter@ucdenver.edu
Abstract
We approached the problems of event detec-
tion, argument identification, and negation and
speculation detection as one of concept recog-
nition and analysis. Our methodology in-
volved using the OpenDMAP semantic parser
with manually-written rules. We achieved
state-of-the-art precision for two of the three
tasks, scoring the highest of 24 teams at pre-
cision of 71.81 on Task 1 and the highest of 6
teams at precision of 70.97 on Task 2.
The OpenDMAP system and the rule set are
available at bionlp.sourceforge.net.
*These two authors contributed equally to the
paper.
1 Introduction
We approached the problem of biomedical event
recognition as one of concept recognition and anal-
ysis. Concept analysis is the process of taking a
textual input and building from it an abstract rep-
resentation of the concepts that are reflected in it.
Concept recognition can be equivalent to the named
entity recognition task when it is limited to locat-
ing mentions of particular semantic types in text, or
it can be more abstract when it is focused on recog-
nizing predicative relationships, e.g. events and their
participants.
2 BioNLP?09 Shared Task
Our system was entered into all three of the
BioNLP?09 (Kim et al, 2009) shared tasks:
? Event detection and characterization This
task requires recognition of 9 basic biological
events: gene expression, transcription, protein
catabolism, protein localization, binding, phos-
phorylation, regulation, positive regulation and
negative regulation. It requires identification
of the core THEME and/or CAUSE participants
in the event, i.e. the protein(s) being produced,
broken down, bound, regulated, etc.
? Event argument recognition This task builds
on the previous task, adding in additional argu-
ments of the events, such as the site (protein or
DNA region) of a binding event, or the location
of a protein in a localization event.
? Recognition of negations and speculations
This task requires identification of negations of
events (e.g. event X did not occur), and specu-
lation about events (e.g. We claim that event X
should occur).
3 Our approach
We used the OpenDMAP system developed at the
University of Colorado School of Medicine (Hunter
et al, 2008) for our submission to the BioNLP
?09 Shared Task on Event Extraction. OpenDMAP
is an ontology-driven, integrated concept analysis
system that supports information extraction from
text through the use of patterns represented in a
classic form of ?semantic grammar,? freely mixing
text literals, semantically typed basal syntactic con-
stituents, and semantically defined classes of enti-
ties. Our approach is to take advantage of the high
50
quality ontologies available in the biomedical do-
main to formally define entities, events, and con-
straints on slots within events and to develop pat-
terns for how concepts can be expressed in text that
take advantage of both semantic and linguistic char-
acteristics of the text. We manually built patterns for
each event type by examining the training data and
by using native speaker intuitions about likely ways
of expressing relationships, similar to the technique
described in (Cohen et al, 2004). The patterns char-
acterize the linguistic expression of that event and
identify the arguments (participants) of the events
according to (a) occurrence in a relevant linguistic
context and (b) satisfaction of appropriate semantic
constraints, as defined by our ontology. Our solution
results in very high precision information extraction,
although the current rule set has limited recall.
3.1 The reference ontology
The central organizing structure of an OpenDMAP
project is an ontology. We built the ontology
for this project by combining elements of several
community-consensus ontologies?the Gene Ontol-
ogy (GO), Cell Type Ontology (CTO), BRENDA
Tissue Ontology (BTO), Foundational Model of
Anatomy (FMA), Cell Cycle Ontology (CCO), and
Sequence Ontology (SO)?and a small number of
additional concepts to represent task-specific aspects
of the system, such as event trigger words. Combin-
ing the ontologies was done with the Prompt plug-in
for Prote?ge?.
The ontology included concepts representing each
event type. These were represented as frames, with
slots for the various things that needed to be re-
turned by the system?the trigger word and the var-
ious slot fillers. All slot fillers were constrained to
be concepts in some community-consensus ontol-
ogy. The core event arguments were constrained in
the ontology to be of type protein from the Sequence
Ontology (except in the case of regulation events,
where biological events themselves could satisfy the
THEME role), while the type of the other event argu-
ments varied. For instance, the ATLOC argument
of a gene expression event was constrained to be
one of tissue (from BTO), cell type (from CTO), or
cellular component (from GO-Cellular Component),
while the BINDING argument of a binding event was
constrained to be one of binding site, DNA, domain,
or chromosome (all from the SO and all tagged by
LingPipe). Table 1 lists the various types.
3.2 Named entity recognition
For proteins, we used the gold standard annota-
tions provided by the organizers. For other seman-
tic classes, we constructed a compound named en-
tity recognition system which consists of a LingPipe
GENIA tagging module (LingPipe, (Alias-i, 2008)),
and several dictionary look-up modules. The dictio-
nary lookup was done using a component from the
UIMA (IBM, 2009; Ferrucci and Lally, 2004) sand-
box called the ConceptMapper.
We loaded the ConceptMapper with dictionar-
ies derived from several ontologies, including the
Gene Ontology Cellular Component branch, Cell
Type Ontology, BRENDA Tissue Ontology, and
the Sequence Ontology. The dictionaries contained
the names and name variants for each concept in
each ontology, and matches in the input documents
were annotated with the relevant concept ID for the
match. The only modifications that we made to
these community-consensus ontologies were to re-
move the single concept cell from the Cell Type On-
tology and to add the synonym nuclear to the Gene
Ontology Cell Component concept nucleus.
The protein annotations were used to constrain the
text entities that could satisfy the THEME role in the
events of interest. The other named entities were
added for the identification of non-core event partic-
ipants for Task 2.
3.3 Pattern development strategies
3.3.1 Corpus analysis
Using a tool that we developed for visualizing the
training data (described below), a subset of the gold-
standard annotations were grouped by event type
and by trigger word type (nominalization, passive
verb, active verb, or multiword phrase). This orga-
nization helped to suggest the argument structures of
the event predicates and also highlighted the varia-
tion within argument structures. It also showed the
nature of more extensive intervening text that would
need to be handled for the patterns to achieve higher
recall.
Based on this corpus analysis, patterns were de-
veloped manually using an iterative process in which
individual patterns or groups of patterns were tested
51
Table 1: Semantic restrictions on Task 2 event arguments. CCO = Cell Cycle Ontology, FMA = Foundational Model
of Anatomy, other ontologies identified in the text.
Event Type Site AtLoc ToLoc
binding protein domain (SO),
binding site (SO), DNA
(SO), chromosome (SO)
gene expression gene (SO), biological
entity (CCO)
tissue (BTO), cell type
(CTO), cellular compo-
nent (GO)
localization cellular component
(GO)
cellular component
(GO)
phosphorylation amino acid (FMA),
polypeptide region (SO)
protein catabolism cellular component
(GO)
transcription gene (SO), biological
entity (CCO)
on the training data to determine their impact on per-
formance. Pattern writers started with the most fre-
quent trigger words and argument structures.
3.3.2 Trigger words
In the training data, we were provided annotations
of all relevant event types occurring in the training
documents. These annotations included a trigger
word specifying the specific word in the input text
which indicated the occurrence of each event. We
utilized the trigger words in the training set as an-
chors for our linguistic patterns. We built patterns
around the generic concept of, e.g. an expression
trigger word and then varied the actual strings that
were allowed to satisfy that concept. We then ran ex-
periments with our patterns and these varying sets of
trigger words for each event type, discarding those
that degraded system performance when evaluated
with respect to the gold standard annotations.
Most often a trigger word was removed from an
event type trigger list because it was also a trig-
ger word for another event type and therefore re-
duced performance by increasing the false positive
rate. For example, the trigger words ?level? and
?levels? appear in the training data trigger word lists
of gene expression, transcription, and all three regu-
lation event types.
The selection of trigger words was guided by a
frequency analysis of the trigger words provided in
the task training data. In a post-hoc analysis, we find
that a different proportion of the set of trigger words
was finally chosen for each different event type. Be-
tween 10-20% of the top frequency-ranked trigger
words were used for simple event types, with the
exception that phosphorylation trigger words were
chosen from the top 30%. For instance, for gene ex-
pression all of the top 15 most frequent trigger words
were used (corresponding to the top 16%). For com-
plex event types (the regulations) better performance
was achieved by limiting the list to between 5-10%
of the most frequent trigger words.
In addition, variants of frequent trigger words
were included. For instance, the nominalization ?ex-
pression? is the most frequent gene expression trig-
ger word and the verbal inflections ?expressed? and
?express? are also in the top 20%. The verbal inflec-
tion ?expresses? is ranked lower than the top 30%,
but was nonetheless included as a trigger word in the
gene expression patterns.
3.3.3 Patterns
As in our previous publications on OpenDMAP,
we refer to our semantic rules as patterns. For
this task, each pattern has at a minimum an event
argument THEME and an event-specific trigger
word. For example, {phosphorylation} :=
52
[phosphorylation nominalization][Theme],
where [phosphorylization nominalization]
represents a trigger word. Both elements are defined
semantically. Event THEMEs are constrained by
restrictions placed on them in the ontology, as
described above.
The methodology for creating complex event pat-
terns such as regulation was the same as for sim-
ple events, with the exception that the THEMEs
were defined in the ontology to also include bio-
logical processes. Iterative pattern writing and test-
ing was a little more arduous because these pat-
terns relied on the success of the simple event pat-
terns, and hence more in-depth analysis was re-
quired to perform performance-increasing pattern
adjustments. For further details on the pattern lan-
guage, the reader is referred to (Hunter et al, 2008).
3.3.4 Nominalizations
Nominalizations were very frequent in the train-
ing data; for seven out of nine event types, the most
common trigger word was a nominalization. In writ-
ing our grammars, we focused on these nominaliza-
tions. To write grammars for nominalizations, we
capitalized on some of the insights from (Cohen et
al., 2008). Non-ellided (or otherwise absent) argu-
ments of nominalizations can occur in three basic
positions:
? Within the noun phrase, after the nominaliza-
tion, typically in a prepositional phrase
? Within the noun phrase, immediately preceding
the nominalization
? External to the noun phrase
The first of these is the most straightforward to
handle in a rule-based approach. This is particu-
larly true in the case of a task definition like that
of BioNLP ?09, which focused on themes, since an
examination of the training data showed that when
themes were post-nominal in a prepositional phrase,
then that phrase was most commonly headed by of.
The second of these is somewhat more challeng-
ing. This is because both agents and themes can
occur immediately before the nominalization, e.g.
phenobarbital induction (induction by phenobarbi-
tal) and trkA expression (expression of trkA). To de-
cide how to handle pre-nominal arguments, we made
use of the data on semantic roles and syntactic posi-
tion found in (Cohen et al, 2008). That study found
that themes outnumbered agents in the prenominal
position by a ratio of 2.5 to 1. Based on this obser-
vation, we assigned pre-nominal arguments to the
theme role.
Noun-phrase-external arguments are the most
challenging, both for automatic processing and for
human interpreters; one of the major problems is
to differentiate between situations where they are
present but outside of the noun phrase, and situations
where they are entirely absent. Since the current im-
plementation of OpenDMAP does not have robust
access to syntactic structure, our only recourse for
handling these arguments was through wildcards,
and since they mostly decreased precision without a
corresponding increase in recall, we did not attempt
to capture them.
3.3.5 Negation and speculation
Corpus analysis of the training set revealed two
broad categories each for negation and speculation
modifications, all of which can be described in terms
of the scope of modification.
Negation
Broadly speaking, an event itself can be negated
or some aspect of an event can be negated. In other
words, the scope of a negation modification can be
over the existence of an event (first example below),
or over an argument of an existing event (second ex-
ample).
? This failure to degrade IkappaBalpha ...
(PMID 10087185)
? AP-1 but not NF-IL-6 DNA binding activity ...
(PMID 10233875)
Patterns were written to handle both types of
negation. The negation phrases ?but not? and ?but
neither? were appended to event patterns to catch
those events that were negated as a result of a
negated argument. For event negation, a more ex-
tensive list of trigger words was used that included
verbal phrases such as ?failure to? and ?absence of.?
The search for negated events was conducted in
two passes. Events for which negation cues fall out-
side the span of text that stretches from argument to
53
event trigger word were handled concurrently with
the search for events. A second search was con-
ducted on extracted events for negation cues that fell
within the argument to event trigger word span, such
as
. . . IL-2 does not induce I kappa B alpha degrada-
tion (PMID 10092783)
This second pass allowed us to capture one addi-
tional negation (6 rather than 5) on the test data.
Speculation
The two types of speculation in the training data
can be described by the distinction between ?de re?
and ?de dicto? assertions. The ?de dicto? assertions
of speculation in the training data are modifications
that call into question the degree of known truth of
an event, as in
. . . CTLA-4 ligation did not appear to affect the
CD28 - mediated stabilization (PMID 10029815)
The ?de re? speculation address the potential ex-
istence of an event rather that its degree of truth. In
these cases, the event is often being introduced in
text by a statement of intention to study the event, as
in
. . . we investigated CTCF expression
. . . [10037138]
To address these distinct types of speculation, two
sets of trigger words were developed. One set con-
sisted largely of verbs denoting research activities,
e.g. research, study, examine investigate, etc. The
other set consisted of verbs and adverbs that denote
uncertainty, and included trigger words such as sug-
gests, unknown, and seems.
3.4 Handling of coordination
Coordination was handled using the OpenNLP con-
stituent parser along with the UIMA wrappers that
they provide via their code repository. We chose
OpenNLP because it is easy to train a model, it in-
tegrates easily into a UIMA pipeline, and because
of competitive parsing results as reported by Buyko
(Buyko et al, 2006). The parser was trained using
500 abstracts from the beta version of the GENIA
treebank and 10 full-text articles from the CRAFT
corpus (Verspoor et al, In press). From the con-
stituent parse we extracted coordination structures
into a simplified data structure that captures each
conjunction along with its conjuncts. These were
provided to downstream components. The coordi-
nation component achieves an F-score of 74.6% at
the token level and an F-score of 57.5% at the con-
junct level when evaluated against GENIA. For both
measures the recall was higher than the precision by
4% and 8%, respectively.
We utilized the coordination analysis to identify
events in which the THEME argument was expressed
as a conjoined noun phrase. These were assumed to
have a distributed reading and were post-processed
to create an individual event involving each con-
junct, and further filtered to only include given (A1)
protein references. So, for instance, analysis of the
sentence in the example below should result in the
detection of three separate gene expression events,
involving the proteins HLA-DR, CD86, and CD40,
respectively.
NAC was shown to down-regulate the
production of cytokines by DC as well
as their surface expression of HLA-
DR, CD86 (B7-2), and CD40 molecules
. . . (PMID 10072497)
3.5 Software infrastructure
We took advantage of our existing infrastructure
based on UIMA (The Unstructured Information
Management Architecture) (IBM, 2009; Ferrucci
and Lally, 2004) to support text processing and data
analysis.
3.5.1 Development tools
We developed a visualization tool to enable the
linguistic pattern writers to better analyze the train-
ing data. This tool shows the source text one sen-
tence at a time with the annotated words highlighted.
A list following each sentence shows details of the
annotations.
3.6 Errors in the training data
In some cases, there were discrepancies between the
training data and the official problem definitions.
This was a source of problems in the pattern devel-
opment phase. For example, phosphorylation events
are defined in the task definition as having only a
THEME and a SITE. However, there were instances
in the training data that included both a THEME and
a CAUSE argument. When those events were identi-
fied by our system and the CAUSE was labelled, they
54
were rejected during a syntactic error check by the
test server.
4 Results
4.1 Official Results
We are listed as Team 13. Table 2 shows our re-
sults on the official metrics. Our precision was the
highest achieved by any group for Task 1 and Task
2, at 71.81 for Task 1 and 70.97 for task 2. Our re-
calls were much lower and adversely impacted our
F-measure; ranked by F-measure, we ranked 19th
out of 24 groups.
We noted that our results for the exact match met-
ric and for the approximate match metric were very
close, suggesting that our techniques for named en-
tity recognition and for recognizing trigger words
are doing a good job of capturing the appropriate
spans.
4.2 Other analysis: Bug fixes and coordination
handling
In addition to our official results, we also report in
Table 3 (see last page) the results of a run in which
we fixed a number of bugs. This represents our cur-
rent best estimate of our performance. The precision
drops from 71.81 for Task 1 to 67.19, and from 70.97
for Task 2 to 65.74, but these precisions are still
well above the second-highest precisions of 62.21
for Task 1 and 56.87 for Task 2. As the table shows,
we had corresponding small increases in our recall
to 17.38 and in our F-measure to 27.62 for Task 1,
and in our recall to 17.07 and F-measure to 27.10 for
Task 2.
We evaluated the effects of coordination handling
by doing separate runs with and without this ele-
ment of the processing pipeline. Compared to our
unofficial results, which had an overall F-measure
for Task 1 of 27.62 and for Task 2 of 27.10, a ver-
sion of the system without handling of coordination
had an overall F-measure for Task 1 of 24.72 and for
Task 2 of 24.21.
4.3 Error Analysis
4.3.1 False negatives
To better understand the causes of our low recall,
we performed a detailed error analysis of false neg-
atives using the devtest data. (Note that this section
includes a very small number of examples from the
devtest data.) We found five major causes of false
negatives:
? Intervening material between trigger words and
arguments
? Coordination that was not handled by our coor-
dination component
? Low coverage of trigger words
? Anaphora and coreference
? Appositive gene names and symbols
Intervening material For reasons that we detail
in the Discussion section, we avoided the use of
wildcards. This, and the lack of syntactic analy-
sis in the version of the system that we used (note
that syntactic analyses can be incorporated into an
OpenDMAP workflow), meant that if there was text
intervening between a trigger word and an argument,
e.g. in to efficiently [express] in developing thymo-
cytes a mutant form of the [NF-kappa B inhibitor]
(PMID 10092801), where the bracketed text is the
trigger word and the argument, our pattern would
not match.
Unhandled coordination Our coordination system
only handled coordinated protein names. Thus, in
cases where other important elements of the utter-
ance, such as the trigger word transcription in tran-
scription and subsequent synthesis and secretion
of galectin-3 (PMID 8623933) were in coordinated
structures, we missed the relevant event arguments.
Low coverage of trigger words As we discuss in
the Methods section, we did not attempt to cover
all trigger words, in part because some less-frequent
trigger words were involved in multiple event types,
in part because some of them were extremely low-
frequency and we did not want to overfit to the train-
ing data, and in part due to the time constraints of the
shared task.
Anaphora and coreference Recognition of some
events in the data would require the ability to do
anaphora and coreference resolution. For example,
in Although 2 early lytic transcripts, [BZLF1] and
[BHRF1], were also detected in 13 and 10 cases,
respectively, the lack of ZEBRA staining in any case
indicates that these lytic transcripts are most likely
55
Tasks 1 and 3 Task 2
Event class GS answer R P F R P F
Localization 174 (18) 18 (18) 10.34 100.00 18.75 9.77 94.44 17.71
Binding 347 (44) 110 (44) 12.68 40.00 19.26 12.32 39.09 18.74
Gene expression 722 (263) 306 (263) 36.43 85.95 51.17 36.43 85.95 51.17
Transcription 137 (18) 20 (18) 13.14 90.00 22.93 13.14 90.00 22.93
Protein catabolism 14 (4) 6 (4) 28.57 66.67 40.00 28.57 66.67 40.00
Phosphorylation 135 (30) 30 (30) 22.22 100.00 36.36 20.14 93.33 33.14
EVENT TOTAL 1529 (377) 490 (377) 24.66 76.94 37.35 24.30 76.12 36.84
Regulation 291 (9) 19 (9) 3.09 47.37 5.81 3.08 47.37 5.79
Positive regulation 983 (32) 65 (32) 3.26 49.23 6.11 3.24 49.23 6.08
Negative regulation 379 (10) 22 (10) 2.64 45.45 4.99 2.37 40.91 4.49
REGULATION TOTAL 1653 (51) 106 (51) 3.09 48.11 5.80 3.02 47.17 5.67
Negation 227 (4) 76 (4) 1.76 5.26 2.64
Speculation 208 (14) 105 (14) 6.73 13.33 8.95
MODIFICATION TOTAL 435 (18) 181 (18) 4.14 9.94 5.84
ALL TOTAL 3182 (428) 596 (428) 13.45 71.81 22.66 13.25 70.97 22.33
Table 2: Official scores for Tasks 1 and 2, and modification scores only for Task 3, from the approximate span
matching/approximate recursive matching table. GS = gold standard (true positives) (given for Tasks 1/3 only), answer
= all responses (true positives) (given for tasks 1/3 only), R = recall, P = precision, F = F-measure. All results are as
calculated by the official scoring application.
[expressed] by rare cells in the biopsies entering
lytic cycle (PMID 8903467), where the bracketed
text is the arguments and the trigger word, the syn-
tactic object of the verb is the anaphoric noun phrase
these lytic transcripts, so even with the addition of
a syntactic component to our system, we still would
not have recognized the appropriate arguments with-
out the ability to do anaphora resolution.
Appositives The annotation guidelines for proteins
apparently specified that when a gene name was
present in an appositive with its symbol, the symbol
was selected as the gold-standard argument. For this
reason, in examples like [expression] of Fas ligand
[FasL] (PMID 10092076), where the bracketed text
is the trigger word and the argument, the gene name
constituted intervening material from the perspec-
tive of our patterns, which therefore did not match.
We return to a discussion of recall and its implica-
tions for systems like ours in the Discussion section.
4.3.2 False positives
Although our overall rate of false positives was
low, we sampled 45 false positive events distributed
across the nine event types and reviewed them with
a biologist.
We noted two main causes of error. The most
common was that we misidentified a slot filler or
were missing a slot filler completely for an actual
event. The other main reason for false positives was
when we erroneously identified a (non)event. For
example, in coexpression of NF-kappa B/Rel and
Sp1 transcription factors (PMID 7479915), we mis-
takenly identified Sp1 transcription as an event.
5 Discussion
Our results demonstrate that it is possible to achieve
state-of-the art precision over a broad range of tasks
and event types using our approach of manually
constructed, ontologically typed rules?our preci-
sion of 71.81 on Task 1 was ten points higher than
the second-highest precision (62.21), and our preci-
sion of 70.97 on Task 2 was 14 points higher than
the second-highest precision (56.87). It remains the
case that our recall was low enough to drop our F-
measure considerably. Will it be the case that a sys-
tem like ours can scale to practical performance lev-
els nonetheless? Four factors suggest that it can.
The first is that there is considerable redundancy
in the data; although we have not quantified it for
this data set, we note that the same event is often
56
Tasks 1 and 3 Task 2
Event class GS answer R P F R P F
Localization 174 (33) 41 (33) 18.97 80.49 30.70 16.67 69.05 26.85
Binding 347 (62) 152 (62) 17.87 40.79 24.85 17.48 40.13 24.35
Gene expression 722 (290) 344 (290) 40.17 84.30 54.41 40.17 84.30 54.41
Transcription 137 (28) 31 (28) 20.44 90.32 33.33 20.44 90.32 33.33
Protein catabolism 14 (4) 6 (4) 28.57 66.67 40.00 28.57 66.67 40.00
Phosphorylation 135 (47) 48 (47) 34.81 97.92 51.37 32.37 84.91 46.88
EVENT TOTAL 1529 (464) 622 (464) 30.35 74.60 43.14 29.77 72.77 42.26
Regulation 291 (11) 31 (11) 3.78 35.48 6.83 3.77 35.48 6.81
Positive regulation 983 (60) 129 (60) 6.10 46.51 10.79 6.08 46.51 10.75
Negative regulation 379 (18) 41 (18) 4.75 43.90 8.57 4.49 41.46 8.10
REGULATION TOTAL 1653 (89) 201 (89) 5.38 44.28 9.60 5.31 43.78 9.47
Negation 227 (6) 129 (6) 2.64 4.65 3.37
Speculation 208 (25) 165 (25) 12.02 15.15 13.40
MODIFICATION TOTAL 435 (31) 294 (31) 7.13 10.54 8.50
ALL TOTAL 3182 (553) 823 (553) 17.38 67.19 27.62 17.07 65.74 27.10
Table 3: Updated results on test data for Tasks 1-3, with important bug fixes in the code base. See key above.
mentioned repeatedly, but for knowledge base build-
ing and other uses of the extracted information, it is
only strictly necessary to recognize an event once
(although multiple recognition of the same assertion
may increase our confidence in its correctness).
The second is that there is often redundancy
across the literature; the best-supported assertions
will be reported as initial findings and then repeated
as background information.
The third is that these recall results reflect an ap-
proach that made no use of syntactic analysis be-
yond handling coordination. There is often text
present in the input that cannot be disregarded with-
out either using wildcards, which generally de-
creased precision in our experiments and which
we generally eschewed, or making use of syntac-
tic information to isolate phrasal heads. Syntactic
analysis, particularly when combined with analysis
of predicate-argument structure, has recently been
shown to be an effective tool in biomedical infor-
mation extraction (Miyao et al, 2009). There is
broad need for this?for example, of the thirty lo-
calization events in the training data whose trigger
word was translocation, a full eighteen had inter-
vening textual material that made it impossible for
simple patterns like translocationof [Theme] or
[ToLoc]translocation to match.
Finally, our recall numbers reflect a very short de-
velopment cycle, with as few as four patterns writ-
ten for many event types. A less time-constrained
pattern-writing effort would almost certainly result
in increased recall.
Acknowledgments
We gratefully acknowledge Mike Bada?s help in
loading the Sequence Ontology into Prote?ge?.
This work was supported by NIH
grants R01LM009254, R01GM083649, and
R01LM008111 to Lawrence Hunter and
T15LM009451 to Philip Ogren.
References
Alias-i. 2008. LingPipe 3.1.2.
Ekaterina Buyko, Joachim Wermter, Michael Poprat, and
Udo Hahn. 2006. Automatically mapping an NLP
core engine to the biology domain. In Proceedings
of the ISMB 2006 joint BioLINK/Bio-Ontologies meet-
ing.
K. B. Cohen, L. Tanabe, S. Kinoshita, and L. Hunter.
2004. A resource for constructing customized test
suites for molecular biology entity identification sys-
tems. BioLINK 2004, pages 1?8.
K. Bretonnel Cohen, Martha Palmer, and Lawrence
Hunter. 2008. Nominalization and alternations in
biomedical language. PLoS ONE, 3(9).
57
D. Ferrucci and A. Lally. 2004. Building an example
application with the unstructured information manage-
ment architecture. IBM Systems Journal, 43(3):455?
475, July.
Lawrence Hunter, Zhiyong Lu, James Firby, William
A. Baumgartner Jr., Helen L. Johnson, Philip V. Ogren,
and K. Bretonnel Cohen. 2008. OpenDMAP: An
open-source, ontology-driven concept analysis engine,
with applications to capturing knowledge regarding
protein transport, protein interactions and cell-specific
gene expression. BMC Bioinformatics, 9(78).
IBM. 2009. UIMA Java framework. http://uima-
framework.sourceforge.net/.
Jin-Dong Kim, Tomoko Ohta, Sampo Pyysalo, Yoshi-
nobu Kano, and Jun?ichi Tsujii. 2009. Overview
of BioNLP?09 shared task on event extraction. In
Proceedings of Natural Language Processing in
Biomedicine (BioNLP) NAACL 2009 Workshop. To
appear.
Yusuke Miyao, Kenji Sagae, Rune Saetre, Takuya Mat-
suzaki, and Jun?ichi Tsujii. 2009. Evaluating contri-
butions of natural language parsers to protein-protein
interaction extraction. Bioinformatics, 25(3):394?400.
Karin Verspoor, K. Bretonnel Cohen, and Lawrence
Hunter. In press. The textual characteristics of tradi-
tional and Open Access scientific journals are similar.
BMC Bioinformatics.
58
Proceedings of the NAACL HLT Workshop on Software Engineering, Testing, and Quality Assurance for Natural Language Processing, pages 1?4,
Boulder, Colorado, June 2009. c?2009 Association for Computational Linguistics
Building Test Suites for UIMA Components 
 
 
Philip V. Ogren Steven J. Bethard 
Center for Computational Pharmacology Department of Computer Science 
University of Colorado Denver Stanford University 
Denver, CO 80217, USA Stanford, CA 94305, USA 
philip@ogren.info bethard@stanford.edu 
 
 
 
 
 
 
Abstract 
We summarize our experiences building a 
comprehensive suite of tests for a statistical 
natural language processing toolkit, ClearTK. 
We describe some of the challenges we en-
countered, introduce a software project that 
emerged from these efforts, summarize our re-
sulting test suite, and discuss some of the les-
sons learned.  
1 Introduction 
We are actively developing a software toolkit for 
statistical natural processing called ClearTK (Og-
ren et al, 2008) 1, which is built on top of the Un-
structured Information Management Architecture 
(UIMA) (Ferrucci and Lally, 2004). From the be-
ginning of the project, we have built and main-
tained a comprehensive test suite for the ClearTK 
components. This test suite has proved to be inva-
luable as our APIs and implementations have 
evolved and matured. As is common with early-
stage software projects, our code has undergone 
number of significant refactoring changes and such 
changes invariably break code that was previously 
working. We have found that our test suite has 
made it much easier to identify problems intro-
duced by refactoring in addition to preemptively 
discovering bugs that are present in new code. We 
have also observed anecdotally that code that is 
                                                          
1 http://cleartk.googlecode.com 
more thoroughly tested as measured by code cov-
erage has proven to be more reliable and easier to 
maintain. 
While this test suite has been an indispensable 
resource for our project, we have found creating 
tests for our UIMA components to be challenging 
for a number of reasons. In a typical UIMA 
processing pipeline, components created by devel-
opers are instantiated by a UIMA container called 
the Collection Processing Manager (CPM) which 
decides at runtime how to instantiate components 
and what order they should run via configuration 
information provided in descriptor files. This pat-
tern is typical of programming frameworks: the 
developer creates components that satisfy some 
API specification and then these components are 
managed by the framework. This means that the 
developer rarely directly instantiates the compo-
nents that are developed and simple programs con-
sisting of e.g. a main method are uncommon and 
can be awkward to create. This is indeed consistent 
with our experiences with UIMA. While this is 
generally a favorable approach for system devel-
opment and deployment, it presents challenges to 
the developer that wants to isolate specific compo-
nents (or classes that support them) for unit or 
functional testing purposes. 
2 Testing UIMA Components 
UIMA coordinates data generated and consumed 
by different components using a data structure 
called the Common Analysis Structure (CAS). The 
1
CAS represents the current state of analysis that 
has been performed on the data being analyzed. As 
a simple example, a UIMA component that per-
forms tokenization on text would add token anno-
tations to the CAS. A subsequent component such 
as a part-of-speech tagger would read the token 
annotations from the CAS and update them with 
part-of-speech labels. We have found that many of 
our tests involve making assertions on the contents 
of the CAS after a component or series of compo-
nents has been executed for a given set of configu-
ration parameters and input data. As such, the test 
must obtain an instance of a CAS after it has been 
passed through the components relevant to the 
tests.  
For very simple scenarios a single descriptor file 
can be written which specifies all the configuration 
parameters necessary to instantiate a UIMA com-
ponent, create a CAS instance, and process the 
CAS with the component. Creating and processing 
a CAS from such a descriptor file takes 5-10 lines 
of Java code, plus 30-50 lines of XML for the de-
scriptor file. This is not a large overhead if there is 
a single test per component, however, testing a 
variety of parameter settings for each component 
results in a proliferation of descriptor files. These 
descriptor files can be difficult to maintain in an 
evolving codebase because they are tightly coupled 
with the Java components they describe, yet most 
code refactoring tools fail to update the XML de-
scriptor when they modify the Java code. As a re-
sult, the test suite can become unreliable unless 
substantial manual effort is applied to maintain the 
descriptor files. 
Thus, for ease of refactoring and to minimize the 
number of additional files required, it made sense 
to put most of the testing code in Java instead of 
XML. But the UIMA framework does not make it 
easy to instantiate components or create CAS ob-
jects without an XML descriptor, so even for rela-
tively simple scenarios we found ourselves writing 
dozens of lines of setup code before we could even 
start to make assertions about the expected con-
tents of a CAS. Fortunately, much of this code was 
similar across test cases, so as the ClearTK test 
suite grew, we consolidated the common testing 
code. The end result was a number of utility 
classes which allow UIMA components to be in-
stantiated and run over CAS objects in just 5-10 
lines of Java code. We decided that these utilities 
could also ease testing for projects other than 
ClearTK, so we created the UUTUC project, which 
provides our UIMA unit test utility code. 
3 UUTUC  
UUTUC2 provides a number of convenience 
classes for instantiating, running, and testing 
UIMA components without the overhead of the 
typical UIMA processing pipeline and without the 
need to provide XML descriptor files. 
Note that UUTUC cannot isolate components 
entirely from UIMA ? it is still necessary, for ex-
ample, to create AnalysisEngine objects, JCas ob-
jects, Annotation objects, etc. Even if it were 
possible to isolate components entirely from 
UIMA, this would generally be undesirable as it 
would result in testing components in a different 
environment from that of their expected runtime. 
Instead, UUTUC makes it easier to create UIMA 
objects entirely in Java code, without having to 
create the various XML descriptor files that are 
usually required by UIMA.  
Figure 1 provides a complete code listing for a 
test of a UIMA component we wrote that provides 
a simple wrapper around the widely used Snowball 
stemmer3. A complete understanding of this code 
would require detailed UIMA background that is 
outside the scope this paper. In short, however, the 
code creates a UIMA component from the Snow-
ballStemmer class, fills a CAS with text and to-
kens, processes this CAS with the stemmer, and 
checks that the tokens were stemmed as expected. 
Here are some of the highlights of how UUTUC 
made this easier: 
Line 3 uses TypeSystemDescriptionFactory 
to create a TypeSystemDescription from the 
user-defined annotation classes Token and Sen-
tence. Without this factory, a 10 line XML de-
scriptor would have been required. 
Line 5 uses AnalysisEngineFactory to create 
an AnalysisEngine component from the user-
defined annotator class SnowballStemmer and 
the type system description, setting the stemmer 
name parameter to "English". Without this 
factory, a 40-50 line XML descriptor would 
have been required (and near duplicate descrip-
                                                          
2 http://uutuc.googlecode.com ? provided under BSD license 
3 http://snowball.tartarus.org 
2
tor files would have been required for each ad-
ditional parameter setting tested). 
Line 11 uses TokenFactory to set the text of 
the CAS object and to populate it with Token 
and Sentence annotations. Creating these anno-
tations and adding them to the CAS manually 
would have taken about 20 lines of Java code, 
including many character offsets that would 
have to be manually adjusted any time the test 
case was changed. 
While a Python programmer might not be im-
pressed with the brevity of this code, anyone who 
has written Java test code for UIMA components 
will appreciate the simplicity of this test over an 
approach that does not make use of the UUTUC 
utility classes. 
4 Results  
The test suite we created for ClearTK was built 
using UUTUC and JUnit version 44 and consists of 
92 class definitions (i.e. files that end in .java) con-
taining 258 tests (i.e. methods with the marked 
with the annotation @Test). These tests contain a 
total of 1,943 individual assertions. To measure 
code coverage of our unit tests we use EclEmma5, 
a lightweight analysis tool available for the Eclipse 
development environment, which counts the num-
ber of lines that are executed (or not) when a suite 
of unit tests are executed. While this approach pro-
                                                          
4 http://junit.org 
5 http://www.eclemma.org 
vides only a rough approximation of how well the 
unit tests ?cover? the source code, we have found 
anecdotally that code with higher coverage re-
ported by EclEmma proves to be more reliable and 
easier to maintain. Overall, our test suite provides 
74.3% code coverage of ClearTK (5,391 lines cov-
ered out of 7,252) after factoring out automatically 
generated code created by JCasGen.  Much of the 
uncovered code corresponds to the blocks catching 
rare exceptions. While it is important to test that 
code throws exceptions when it is expected to, 
forcing test code to throw all exceptions that are 
explicitly caught can be tedious and sometimes 
technically quite difficult.  
5 Discussion  
We learned several lessons while building our test 
suite. We started writing tests using Groovy, a dy-
namic language for the Java Virtual Machine. The 
hope was to simplify testing by using a less ver-
bose language than Java. While Groovy provides a 
great syntax for creating tests that are much less 
verbose, we found that creating and maintaining 
these unit tests was cumbersome using the Eclipse 
plug-in that was available at the time (Summer 
2007). In particular, refactoring tasks such as 
changing class names or method names would suc-
ceed in the Java code, but the Groovy test code 
would not be updated, a similar problem to that of 
UIMA?s XML descriptor files. We also found that 
Eclipse became less responsive because user ac-
tions would often wait for the Groovy compiler to 
 1 @Test 
 2 public void testSimple() throws UIMAException { 
 3     TypeSystemDescription typeSystemDescription = TypeSystemDescriptionFactory 
 4         .createTypeSystemDescription(Token.class, Sentence.class); 
 5     AnalysisEngine engine = AnalysisEngineFactory.createAnalysisEngine( 
 6         SnowballStemmer.class, typeSystemDescription, 
 7         SnowballStemmer.PARAM_STEMMER_NAME, "English"); 
 8     JCas jCas = engine.newJCas(); 
 9     String text =   "The brown foxes jumped quickly over the lazy dog."; 
10     String tokens = "The brown foxes jumped quickly over the lazy dog ."; 
11     TokenFactory.createTokens(jCas, text, Token.class, Sentence.class, tokens); 
12     engine.process(jCas); 
13     List<String> actual = new ArrayList<String>(); 
14     for (Token token: AnnotationRetrieval.getAnnotations(jCas, Token.class)) { 
15         actual.add(token.getStem()); 
16     } 
17     String expected = "the brown fox jump quick over the lazi dog ."; 
18     Assert.assertEquals(Arrays.asList(expected.split(" ")), actual); 
19 } 
Figure 1: A complete test case using UUTUC. 
3
complete. Additionally, Groovy tests involving 
Java?s Generics would sometimes work on one 
platform (Windows) and fail on another (Linux or 
Mac). For these reasons we abandoned using 
Groovy and converted our tests to Java. It should 
be noted that the authors are novice users of 
Groovy and that Groovy (and the Eclipse Groovy 
plug-in) may have matured significantly in the in-
tervening two years.  
Another challenge we confronted while building 
our test suite was the use of licensed data. For ex-
ample, ClearTK contains a component for reading 
and parsing PennTreebank formatted data. One of 
our tests reads in and parses the entire PennTree-
bank corpus, but since we do not have the rights to 
redistribute the PennTreeBank, we could not in-
clude this test as part of the test suite distributed 
with ClearTK. So as not to lose this valuable test, 
we created a sibling project of ClearTK which is 
not publicly available, but from which we could 
run tests on ClearTK. This sibling project now 
contains all of our unit tests which use data we 
cannot distribute. We are considering making this 
project available separately for those who have 
access to the relevant data sets.  
We have begun to compile a growing list of best 
practices for our test suite. These include: 
Reuse JCas objects. In UIMA, creating a JCas 
object is expensive. Instead of creating a new 
JCas object for each test, a single JCas object 
should be reused for many tests where possible. 
Refer to descriptors by name, not location. 
UIMA allows descriptors to be located by either 
?location? (a file system path) or ?name? (a Ja-
va-style dotted package name). Descriptors re-
ferred to by ?name? can be found in a .jar file, 
while descriptors referred to by ?location? can-
not. This applies to imports of both type system 
descriptions (e.g. in component descriptors) and 
to imports of CAS processors (e.g. in collection 
processing engine descriptors). 
Test loading of descriptor files. As discussed, 
XML descriptor files can become stale in an 
evolving codebase. Simply loading each de-
scriptor in UIMA and verifying that the para-
meters are as expected is often enough to keep 
the descriptor files working if the actual com-
ponent code is being properly checked through 
other tests. 
Test copyright and license statements. We 
found it useful to add unit tests that search 
through our source files (both Java code and 
descriptor files) and verify that appropriate 
copyright and license statements are present. 
Such statements were a requirement of the 
technology transfer office we were working 
with, and were often accidentally omitted when 
new source files were added to ClearTK. Add-
ing a unit test to check for this meant that we 
caught such omissions much earlier. 
As ClearTK has grown in size and complexity its 
test suite has proven many times over to be a vital 
instrument in detecting bugs introduced by extend-
ing or refactoring existing code. We have found 
that the code in UUTUC has greatly decreased the 
burden of maintaining and extending this test suite, 
and so we have made it available for others to use. 
References  
Philip V. Ogren, Philipp G. Wetzler, and Steven Be-
thard. 2008. ClearTK: a UIMA toolkit for statistical 
natural language processing. In UIMA for NLP 
workshop at LREC. 
David Ferrucci and Adam Lally. 2004. UIMA: an archi-
tectural approach to unstructured information 
processing in the corporate research environment. 
Natural Language Engineering, 10(3-4):327?348. 
4
Proceedings of the NAACL HLT 2010 Student Research Workshop, pages 1?6,
Los Angeles, California, June 2010. c?2010 Association for Computational Linguistics
Improving Syntactic Coordination Resolution Using Language Modeling
Philip V. Ogren
Center for Computational Pharmacology
University of Colorado Denver
12801 E. 17th Ave
Aurora, CO 80045, USA
philip@ogren.info
Abstract
Determining the correct structure of coordi-
nating conjunctions and the syntactic con-
stituents that they coordinate is a difficult task.
This subtask of syntactic parsing is explored
here for biomedical scientific literature. In
particular, the intuition that sentences contain-
ing coordinating conjunctions can often be
rephrased as two or more smaller sentences
derived from the coordination structure is ex-
ploited. Generating candidate sentences cor-
responding to different possible coordination
structures and comparing them with a lan-
guage model is employed to help determine
which coordination structure is best. This
strategy is used to augment a simple baseline
system for coordination resolution which out-
performs both the baseline system and a con-
stituent parser on the same task.
1 Introduction
For this work, coordination resolution (CR) refers to
the task of automatically identifying the correct co-
ordination structure of coordinating conjunctions. In
this study the conjunctions and and or and the con-
juncts they coordinate are examined. CR is an im-
portant subtask of syntactic parsing in the biomed-
ical domain because many information extraction
tasks require correct syntactic structures to perform
well, in particular coordination structures. For ex-
ample, (Cohen et al, 2009) showed that using a con-
stituent parser trained on biomedical data to provide
coordination structures to a high-precision protein-
protein interaction recognition system resulted in
a significant performance boost from an overall F-
measure of 24.7 to 27.6. Coordination structures are
the source of a disproportionate number of parsing
errors for both constituent parsers (Clegg and Shep-
herd, 2007) and dependency parsers (Nivre and Mc-
Donald, 2008).
CR is difficult for a variety of reasons related to
the linguistic complexity of the phenomenon. There
are a number of measurable characteristics of coor-
dination structures that support this claim including
the following: constituent types of conjuncts, num-
ber of words per conjunct, number of conjuncts per
conjunction, and the number of conjunctions that are
nested inside the conjunct of another conjunction,
among others. Each of these metrics reveal wide
variability of coordination structures. For example,
roughly half of all conjuncts consist of one or two
words while the other half consist of three or more
words including 15% of all conjuncts that have ten
or more words. There is also an increased preva-
lence of coordinating conjunctions in biomedical lit-
erature when compared with newswire text. Table 1
lists three corpora in the biomedical domain that are
annotated with deep syntactic structures; CRAFT
(described below), GENIA (Tateisi et al, 2005), and
Penn BIOIE (Bies et al, 2005). The number of co-
ordinating conjunctions they contain as a percentage
of the number of total tokens in each corpus are com-
pared with the Penn Treebank corpus (Marcus et al,
1994). The salient result from this table is that there
are 50% more conjunctions in biomedical scientific
text than in newswire text. It is also interesting to
note that 15.4% of conjunctions in the biomedical
corpora are nested inside a conjunct of another con-
1
junction as compared with 10.9% for newswire.
Table 1: Biomedical corpora that provide coordination
structures compared with the Penn Treebank corpus.
Corpus Tokens Conjunctions
CRAFT 246,008 7,115 2.89%
GENIA 490,970 14,854 3.03%
BIOIE 188,341 5,036 2.67%
subtotal 925,319 27,005 2.92%
PTB 1,173,766 22,888 1.95%
The Colorado Richly Annotated Full-Text
(CRAFT) Corpus being developed at the Univer-
sity of Colorado Denver was used for this work.
Currently, the corpus consists of 97 full-text open-
access scientific articles that have been annotated by
the Mouse Genome Institute1 with concepts from
the Gene Ontology2 and Mammalian Phenotype
Ontology3. Thirty-six of the articles have been
annotated with deep syntactic structures similar
to that of the Penn Treebank corpus described in
(Marcus et al, 1994). As this is a work in progress,
eight of the articles have been set aside for a final
holdout evaluation and results for these articles
are not reported here. In addition to the standard
treebank annotation, the NML tag discussed in
(Bies et al, 2005) and (Vadas and Curran, 2007)
which marks nominal subconstituents which do
not observe the right-branching structure common
to many (but not all) noun phrases is annotated.
This is of particular importance for coordinated
noun phrases because it provides an unambiguous
representation of the correct coordination structure.
The coordination instances in the CRAFT data
were converted to simplified coordination structures
consisting of conjunctions and their conjuncts using
a script that cleanly translates the vast majority of
coordination structures.
2 Related Work
There are two main approaches to CR. The first ap-
proach considers CR as a task in its own right where
1http://www.informatics.jax.org/
2http://geneontology.org/
3http://www.informatics.jax.org/
searches/MP_form.shtml
the solutions are built specifically to perform CR.
Often the task is narrowly defined, e.g. only coor-
dinations of the pattern noun-1 conjunction noun-2
noun-3 are considered, and relies on small training
and testing data sets. Generally, such research ef-
forts do not attempt to compare their results with
previous results other than in the broadest and most-
qualified way. Studies by (Chantree et al, 2005),
(Nakov and Hearst, 2005), and (Resnik, 1999) are
representative examples of such work. A study by
(Shimbo and Hara, 2007) performed CR on sen-
tences from the GENIA corpus containing one in-
stance of the word ?and? coordinating noun phrases.
They used a sequence alignment algorithm modified
for CR drawing on the intuition that conjuncts have
similar syntactic constructs. In each of these studies,
promising results were achieved by careful applica-
tion of their respective approaches. However, each
study is limited in important respects because they
narrowly constrain the problem, use limited train-
ing data, and make certain unrealistic assumptions
in their experimental setup that make general appli-
cation of their solutions problematic. For example,
in the study by (Shimbo and Hara, 2007) they chose
only sentences that have one instance of ?and? be-
cause their algorithm does not handle nested con-
junctions. Additionally, they assume an oracle that
provides the system with only sentences that contain
coordinated noun phrases.
The work most similar to this study was done by
(Hara et al, 2009) in that they define the CR task
essentially the same as is done here. Their approach
involves a grammar tailored for coordination struc-
tures that is coupled with a sequence alignment al-
gorithm that uses perceptrons for learning feature
weights of an edit graph. The evaluation metric they
use is slightly less strict than the metric used for
this study in that they require identification of the
left boundary of the left-most conjunct and the right
boundary of the right-most conjunct to be counted
correct. Two other important differences are that
the evaluation data comes from the GENIA corpus
and they use gold-standard part-of-speech tags for
the input data. Regardless of these relatively minor
differences, their performance of 61.5 F-measure far
outperforms what is reported below and experiments
that are directly comparable to their work will be
performed.
2
The second main approach considers CR within
the broader task of syntactic parsing. Any syntac-
tic parser that generates constituents or dependen-
cies must necessarily perform CR to perform well.
Typically, a syntactic parser will have a single, cen-
tral algorithm that is used to determine all con-
stituents or dependencies. However, this does not
preclude parsers from giving special attention to CR
by adding CR-specific rules and features. For exam-
ple, (Nilsson et al, 2006) show that for dependency
parsing it is useful to transform dependency struc-
tures that make conjunctions the head of their con-
juncts into structures in which coordination depen-
dencies are chained. (Charniak and Johnson, 2005)
discusses a constituent-based parser that adds two
features to the learning model that directly address
coordination. The first measures parallelism in the
labels of the conjunct constituents and their children
and the second measures the lengths of the conjunct
constituents. The work done by (Hogan, 2007) fo-
cuses directly on coordination of noun phrases in
the context of the Collins parser (Collins, 2003) by
building a right conjunct using features from the al-
ready built left conjunct.
3 Using a Language Model
Consider the following sentence:
Tyr mutation results in increased IOP and
altered diurnal changes.
By exploiting the coordination structure we can
rephrase this sentence as two separate sentences:
? Tyr mutation results in increased IOP.
? Tyr mutation results in altered diurnal changes.
Using this simple rewrite strategy a candidate sen-
tence for each possible conjunct can be composed.
For this sentence there are six possible left conjuncts
corresponding to each word to the left of the con-
junction. For example, the candidate conjunct cor-
responding to the third word is results in increased
IOP and the corresponding sentence rewrite is Tyr
mutation altered diurnal changes. The resulting
candidate sentences can be compared by calculat-
ing a sentence probability using a language model.
Ideally, the candidate sentence corresponding to the
correct conjunct boundary will have a higher proba-
bility than the other candidate sentences. One prob-
lem with this approach is that the candidate sen-
tences are different lengths. This has a large and
undesirable (for this task) impact on the probability
calculation. A simple and effective way to normal-
ize for sentence length is by adding4 the probability
of the candidate conjunct (also computed by using
the language model) to the probability of the candi-
date sentence. The probability of each candidate is
calculated using this simple metric and then rank or-
dered. Because the number of candidate conjuncts
varies from one sentence to the next (as determined
by the token index of the conjunction) it is useful to
translate the rank into a percentile. The rank per-
centile of the candidate conjuncts will be applied to
the task of CR as described below. However, it is
informative to directly evaluate how good the rank
percentile scores of the correct conjuncts are.
To build a language model a corpus of more than
80,000 full-text open-access scientific articles were
obtained from PubMed Central5. The articles are
provided in a simple XML format which was parsed
to produce plain text documents using only sections
of the articles containing contentful prose (i.e. by
excluding sections such as e.g. acknowledgments
and references.) The plain text documents were
automatically sentence segmented, tokenized, and
part-of-speech tagged resulting in nearly 13 million
sentences and over 250 million tagged words. A lan-
guage model was then built using this data with the
SRILM toolkit described in (Stolcke, 2002). De-
fault options were used for creating the language
model except that the order of the model was set to
four and the ?-tagged? option was used. Thus, a 4-
gram model with Good-Turing discounting and Katz
backoff for smoothing was built.
For each token to the left of a conjunction a candi-
date conjunct/sentence pair is derived, its probabil-
ity calculated, and a rank percentile score is assigned
to it relative to the other candidates. Because mul-
tiple conjuncts can appear on the left-hand-side of
the conjunction, the left border of the leftmost con-
junct is considered here. The same is done for tokens
4logprobs are used here
5http://www.ncbi.nlm.nih.gov/pmc/about/
ftp.html. The corpus was downloaded in September of
2008.
3
Figure 1: The first column can be read as ?The correct
conjunct candidate had the highest rank percentile 32.1%
of the time.? The second column can be read as ?The
correct conjunct candidate had a rank percentile of 90%
or greater 17.6% of the time.? The columns add to one.
on the right-hand-side of the conjunction. Figure 1
shows a histogram of the rank percentile scores for
the correct left conjunct. The height of the bars cor-
respond to the percentage of the total number of con-
junctions in which the correct candidate was ranked
within the percentile range. Thus, the columns add
to one and generalizations can be made by adding
the columns together. For example, 66.7% of the
conjunctions (by adding the first three columns) fall
above the eightieth percentile. The overall average
rank percentage for all of the left-hand-side con-
juncts was 81.1%. The median number of candi-
dates on the left-hand-side is 17 (i.e. the median to-
ken index of the conjunction is 17). Similar results
were obtained for the right-hand-side data but were
withheld for space considerations. The overall av-
erage rank percentage for right-hand-side conjuncts
was 82.2%. This slightly better result is likely due
to the smaller median number of candidates on the
right-hand-side of 12 (i.e. the median token index
of the conjunction is 12 from the end of the sen-
tence.) These data suggest that the rank percentile of
the candidate conjuncts calculated in this way could
be an effective feature to use for CR.
4 Coordination Resolution
Table 2 reports the performance of two CR systems
that are described below. Results are reported as F-
Measure at both the conjunct and conjunction lev-
els where a true positive requires all boundaries to
be exact. That is, for conjunct level evaluation a
conjunct generated by the system must have exactly
the same extent (i.e. character offsets) as the con-
junct in the gold-standard data in addition to be-
ing attached to the same conjunction. Similarly, at
the conjunction level a true positive requires that a
coordination structure generated by the system has
the same number of conjuncts each with extents ex-
actly the same as the corresponding conjunct in the
gold-standard coordination structure. Where 10-fold
cross-validation is performed, training is performed
on roughly 90% of the data and testing on the re-
maining 10% with the results micro-averaged. Here,
the folds are split at the document level to avoid the
unfair advantage of training and testing on different
sections of the same document.
Table 2: Coordination resolution results at the conjunct
and conjunction levels as F-Measure.
Conjunct Conjunction
OpenNLP + PTB 55.46 36.56
OpenNLP + CRAFT 58.87 39.50
baseline 59.75 40.99
baseline + LM 64.64 46.40
The first system performs CR within the broader
task of syntactic parsing. Here the constituent parser
from the OpenNLP project6 is applied. This parser
was chosen because of its availability and ease of
use for both training and execution. It has also
been shown by (Buyko et al, 2006) to perform well
on biomedical data. The output of the parser is
processed by the same conversion script described
above. The parser was trained and evaluated on
both the Penn Treebank and CRAFT corpora. For
the latter, 10-fold cross-validation was performed.
Preliminary experiments that attempted to add ad-
ditional training data from the GENIA and Penn
BIOIE corpora proved to be slightly detrimental to
performance in both cases. Table 2 shows that CR
improves at the conjunction level by nearly three
points (from 36.56 to 39.50) by simply training on
biomedical data rather than using a model trained
on newswire.
The second system that performs CR as a separate
6http://opennlp.sf.net
4
task by using token-level classification to determine
conjunct boundaries is introduced and evaluated. In
brief, each token to the left of a conjunction is clas-
sified as being either a left-hand border of a conjunct
for that conjunction or not. Similarly, tokens to the
right of a conjunction are classified as either a right-
hand border of a conjunct or not. From these token-
level classifications and some simple assumptions
about the right-hand and left-hand borders of left
and right conjuncts, respectively,7 a complete coor-
dination structure can be constructed. The classifier
used was SVMlight described in (Joachims, 1999)
using a linear kernel. The baseline system uses a
number of shallow lexical features (many common
to named entity recognition systems) including part-
of-speech tags, word and character n-grams, the dis-
tance between the focus token and the conjunction,
and word-level features such as whether the token
is a number or contains a hyphen. A more detailed
description of the baseline system is avoided here as
this remains a major focus of current and future re-
search efforts and the final system will likely change
considerably. Table 2 shows the results of 10-fold
cross-validation for the baseline system. This sim-
ple baseline system performs at 40.99 F-measure at
the conjunction level which is modestly better than
the syntactic parser trained on CRAFT.
The baseline system as described above was aug-
mented using the language modeling approach de-
scribed in Section 3 by adding a simple feature to
each token being classified whose value is the rank
percentile of the probability of the corresponding
conjunct candidate. Again, 10-fold cross-validation
was performed. Table 2 shows that this augmented
baseline system performs at 46.40 F-measure at the
conjunction level which out-performs the baseline
system and the CRAFT-trained parser by 5.4 and 6.9
points, respectively. This increase in performance
demonstrates that a language model can be effec-
tively purposed for CR.
While the use of a language model to improve CR
results is promising, the results in Table 2 also speak
to how difficult this task is for machines to perform.
In contrast, the task is comparatively easy for hu-
mans to perform consistently. To calculate inter-
7For example, the left-hand border of the conjunct to the
right of a conjunction will always be the first word following
the conjunction.
annotator agreement on the CR task, 500 sentences
containing either the word ?and? or ?or? were ran-
domly chosen from the 13 million sentence corpus
described in Section 3 and annotated with coordi-
nation structures by two individuals, the author and
another computer scientist with background in biol-
ogy. Our positive specific agreement8 was 91.93 and
83.88 at the conjunct and conjunction level, respec-
tively, for 732 conjunctions. This represents a dra-
matic gulf between system and human performance
on this task but also suggests that large improve-
ments for automated CR should be expected.
5 Future Work
There is much that can be done to move this work
forward. Creating comparable results to the study
discussed in Section 2 by (Hara et al, 2009) is a top
priority. As alluded to earlier, there is much that can
be done to improve the baseline system. For exam-
ple, constraining coordination structures to not over-
lap except where one is completely nested within
the conjunct of another should be enforced as par-
tially overlapping coordination structures never oc-
cur in the training data. Similarly, a conjunction that
appears inside parentheses should have a coordina-
tion structure that is completely contained inside the
parentheses. Thorough error analysis should also
be performed. For example, it would be interesting
to characterize the conjuncts that have a low rank
percentile for their calculated probability. Also, it
would be useful to measure performance across a
number of metrics such as phrase type of the con-
juncts, length of conjuncts, whether a coordination
structure is nested inside another, etc. Demonstrat-
ing that CR can improve syntactic parsing perfor-
mance and improve the performance of an informa-
tion extraction system would give this work greater
significance.
Conclusion
This work has demonstrated that a language model
can be used to improve performance of a simple CR
system. This is due to the high rank percentile of the
probability of the correct conjunct compared with
other possible conjuncts.
8This measure is directly comparable with F-measure.
5
References
Ann Bies, Seth Kulick, and Mark Mandel. 2005. Parallel
entity and treebank annotation. In CorpusAnno ?05:
Proceedings of the Workshop on Frontiers in Corpus
Annotations II, pages 21?28, Morristown, NJ, USA.
Association for Computational Linguistics.
Ekaterina Buyko, Joachim Wermter, Michael Poprat, and
Udo Hahn. 2006. Automatically adapting an NLP
core engine to the biology domain. In Proceedings
of the Joint BioLINK-Bio-Ontologies Meeting. A Joint
Meeting of the ISMB Special Interest Group on Bio-
Ontologies and the BioLINK Special Interest Group on
Text Data M ining in Association with ISMB, pages
65?68. Citeseer.
Francis Chantree, Adam Kilgarriff, Anne De Roeck, and
Alistair Willis. 2005. Disambiguating coordinations
using word distribution information. Proceedings of
RANLP2005.
Eugene Charniak and Mark Johnson. 2005. Coarse-to-
fine n-best parsing and maxent discriminative rerank-
ing. In Proceedings of the 43rd Annual Meeting of the
Association for Computational Linguistics (ACL?05),
pages 173?180, Ann Arbor, Michigan, June. Associa-
tion for Computational Linguistics.
Andrew Clegg and Adrian Shepherd. 2007. Bench-
marking natural-language parsers for biological appli-
cations using dependency graphs. BMC Bioinformat-
ics, 8(1):24.
Kevin B. Cohen, Karin Verspoor, Helen L. Johnson,
Chris Roeder, Philip V. Ogren, William A. Baumgart-
ner Jr, Elizabeth White, Hannah Tipney, and Lawrence
Hunter. 2009. High-precision biological event extrac-
tion with a concept recognizer. In Proceedings of the
Workshop on BioNLP: Shared Task, pages 50?58. As-
sociation for Computational Linguistics.
Michael Collins. 2003. Head-driven statistical models
for natural language parsing. Computational linguis-
tics, 29(4):589?637.
Kazuo Hara, Masashi Shimbo, Hideharu Okuma, and
Yuji Matsumoto. 2009. Coordinate structure analysis
with global structural constraints and alignment-based
local features. In ACL-IJCNLP ?09: Proceedings of
the Joint Conference of the 47th Annual Meeting of
the ACL and the 4th International Joint Conference on
Natural Language Processing of the AFNLP: Volume
2, pages 967?975, Morristown, NJ, USA. Association
for Computational Linguistics.
Deirdre Hogan. 2007. Coordinate noun phrase disam-
biguation in a generative parsing model. In Proceed-
ings of the 45th Annual Meeting of the Association
of Computational Linguistics, pages 680?687, Prague,
Czech Republic, June. Association for Computational
Linguistics.
Thorsten Joachims. 1999. Making large scale SVM
learning practical.
Mitchell P. Marcus, Beatrice Santorini, and Mary Ann
Marcinkiewicz. 1994. Building a large annotated cor-
pus of English: The Penn Treebank. Computational
linguistics, 19(2):313?330.
Preslav Nakov and Marti Hearst. 2005. Using the web as
an implicit training set: Application to structural ambi-
guity resolution. In Proceedings of Human Language
Technology Conference and Conference on Empirical
Methods in Natural Language Processing, pages 835?
842, Vancouver, British Columbia, Canada, October.
Association for Computational Linguistics.
Jens Nilsson, Joakim Nivre, and Johan Hall. 2006.
Graph transformations in data-driven dependency
parsing. In Proceedings of the 21st International Con-
ference on Computational Linguistics and 44th Annual
Meeting of the Association for Computational Linguis-
tics, pages 257?264, Sydney, Australia, July. Associa-
tion for Computational Linguistics.
Joakim Nivre and Ryan McDonald. 2008. Integrating
graph-based and transition-based dependency parsers.
In Proceedings of ACL-08: HLT, pages 950?958,
Columbus, Ohio, June. Association for Computational
Linguistics.
Philip Resnik. 1999. Semantic similarity in a Taxonomy:
An Information-Based Measure and its Application to
Problems of Ambiguity in Natural Language. Journal
of Artificial Intelligence, 11(11):95?130.
Masashi Shimbo and Kazuo Hara. 2007. A discrim-
inative learning model for coordinate conjunctions.
In Proceedings of the 2007 Joint Conference on Em-
pirical Methods in Natural Language Processing and
Computational Natural Language Learning (EMNLP-
CoNLL), pages 610?619.
Andreas Stolcke. 2002. SRILM-an extensible language
modeling toolkit. In Seventh International Conference
on Spoken Language Processing, volume 3. Citeseer.
Yuka Tateisi, Akane Yakushiji, Tomoko Ohta, and Ju-
nichi Tsujii. 2005. Syntax Annotation for the GE-
NIA corpus. In Second International Joint Conference
on Natural Language Processing (IJCNLP05), pages
222?227.
David Vadas and James Curran. 2007. Adding noun
phrase structure to the penn treebank. In Proceed-
ings of the 45th Annual Meeting of the Association
of Computational Linguistics, pages 240?247, Prague,
Czech Republic, June. Association for Computational
Linguistics.
6
