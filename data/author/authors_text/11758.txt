Proceedings of the ACL-IJCNLP 2009 Student Research Workshop, pages 63?71,
Suntec, Singapore, 4 August 2009. c?2009 ACL and AFNLP
 
Optimizing Language Model Information Retrieval System with  
Expectation Maximization Algorithm 
 
 
Justin Liang-Te Chiu 
Department of Computer Science  
and Information Engineering,  
National Taiwan University  
#1 Roosevelt Rd. Sec. 4, Taipei,  
Taiwan 106, ROC 
b94902009@ntu.edu.tw 
Jyun-Wei Huang 
Department of Computer Science  
and Engineering,  
Yuan Ze University  
#135 Yuan-Tung Road, Chungli, 
Taoyuan,Taiwan,ROC 
s976017@mail.yzu.edu.tw 
 
  
 
Abstract 
 
Statistical language modeling (SLM) has 
been used in many different domains for dec-
ades and has also been applied to information 
retrieval (IR) recently.  Documents retrieved 
using this approach are ranked according 
their probability of generating the given 
query. In this paper, we present a novel ap-
proach that employs the generalized Expecta-
tion Maximization (EM) algorithm to im-
prove language models by representing their 
parameters as observation probabilities of 
Hidden Markov Models (HMM). In the expe-
riments, we demonstrate that our method out-
performs standard SLM-based and tf.idf-
based methods on TREC 2005 HARD Track 
data. 
1 Introduction 
In 1945, soon after the computer was invented, 
Vannevar Bush wrote a famous article---?As we 
may think? (V. Bush, 1996), which formed the 
basis of research into Information Retrieval (IR). 
The pioneers in IR developed two models for 
ranking: the vector space model (G. Salton and 
M. J. McGill, 1986) and the probabilistic model 
(S. E. Robertson and S. Jones, 1976). Since then, 
the research of classical probabilistic models of 
relevance has been widely studied. For example, 
Robertson (S. E. Robertson and S. Walker, 1994; 
S. E. Robertson, 1977) modeled word occur-
rences into relevant or non-relevant classes, and 
ranked documents according to the probabilities 
they belong to the relevant one. In 1998, Ponte 
and Croft (1998) proposed a language modeling 
framework which opens a new point of view in 
IR. In this approach, they gave up the model of 
relevance; instead, they treated query generation 
as random sampling from every document model. 
The retrieval results were based on the probabili-
ties that a document can generate the query string. 
Several improvements were proposed after their 
work. Song and Croft (1999), for example, was 
the first to bring up a model with bi-grams and 
Good Turing re-estimation to smooth the docu-
ment models. Latter, Miller et al (1999) used 
Hidden Markov Model (HMM) for ranking, 
which also included the use of bigrams.  
HMM, firstly introduced by Rabiner and Juain 
(1986) in 1986, has been successfully applied 
into many domains, such as named entity recog-
nition (D. M. Bikel et al, 1997), topic classifica-
tion (R. Schwartz et al, 1997), or speech recog-
nition (J. Makhoul and R. Schwartz, 1995). In 
practice, the model requires solving three basic 
problems. Given the parameters of the model, 
computing the probability of a particular output 
sequence is the first problem. This process is of-
ten referred to as decoding. Both Forward and 
Backward procedure are solutions for this prob-
lem. The second problem is finding the most 
possible state sequence with the parameters of 
the model and a particular output sequence. This 
is usually completed with Viterbi algorithm. The 
third problem is the learning problem of HMM 
models. It is often solved by Baum-Welch algo-
rithm (L. E. Bmjm et al, 1970). Given training 
63
data, the algorithm computes the maximum like-
lihood estimates and posterior mode estimate. It 
is in essence a generalized Expectation Maximi-
zation (EM) algorithm which was first explained 
and given name by Dempster, Laird and Rubin 
(1977) in 1977. EM can estimate the maximum 
likelihood of parameters in probabilistic models 
which has unseen variables. Nonetheless, in our 
knowledge, the EM procedure in HMM has nev-
er been used in IR domain. 
In this paper, we proposed a new language 
model approach which models the user query 
and documents as HMM models. We then used 
EM algorithm to maximize the probability of 
query words in our model. Our assumption is 
that if the word?s probability in a document is 
maximized, we can estimate the probability of 
generating the query word from documents more 
confidently. Because they not only been calcu-
lated by language modeling view features, but 
also been maximized with statistical methods. 
Therefore the imprecise cases caused by special 
distribution in language modeling approach can 
be further prevented in this way. 
The remainders of this paper are organized as 
follows. We review two related works in Section 
2. In Section 3, we introduce our EM IR ap-
proach. Section 4 compares our results to two 
other approaches proposed by Song and Corft 
(1999) and Robertson (1995) based on the data 
from TREC HARD track (J. Allan, 2005). Sec-
tion 5 discusses the effectiveness of our EM 
training and the EM-based document weighting 
we proposed. Finally, we conclude our paper in 
Section 6 and provide some future directions at 
Section 7. 
2 Related Works 
Even if we only focus on the probabilistic ap-
proach to IR, it is still impossible to discuss all 
up-to-date research. Instead we focus on two 
previous works which have inspired the work 
reported in this paper: the first is a general lan-
guage model approach proposed by Song and 
Croft (1999) and the second is a HMM approach 
by Miller et al (1999). 
2.1 A General Language Model for IR 
In 1999, Song and Croft (1999) introduced a lan-
guage model based on a range of data smoothing 
technique. The following are some of the fea-
tures they used:  
Good-Turing estimate: Since the effect of 
Good-Turing estimate was verified as one of the 
best discount methods (C. D. Manning and H. 
Schutze, 1999), Song and Croft used Good-
Turing estimate for allocating proper probability 
for the missing terms in the documents. The 
smoothed probability for term t in document d 
can be obtained with the following formula: 
 
| 	 
  1  
 
where Ntf is the number of terms with frequency 
tf in a document. Nd is the total number of terms 
occurred in document d, and a powerful smooth-
ing function S(Ntf), which is used for calculating 
the expected value of Ntf regardless of the Ntf ap-
pears in the corpus or not. 
Expanding document model: The document 
model can be viewed as a smaller part of whole 
corpus. Due to its limited size, there is a large 
number of missing terms in documents, and can 
lead to incorrect distributions of known terms. 
For dealing with the problem, documents can be 
expanded with the following weighted 
sum/product approach: 
 | 	   