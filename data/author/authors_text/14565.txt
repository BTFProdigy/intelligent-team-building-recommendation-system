Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010), pages 1182?1190,
Beijing, August 2010
Near-synonym Lexical Choice in Latent Semantic Space
Tong Wang
Department of Computer Science
University of Toronto
tong@cs.toronto.edu
Graeme Hirst
Department of Computer Science
University of Toronto
gh@cs.toronto.edu
Abstract
We explore the near-synonym lexical
choice problem using a novel representa-
tion of near-synonyms and their contexts
in the latent semantic space. In contrast to
traditional latent semantic analysis (LSA),
our model is built on the lexical level
of co-occurrence, which has been empir-
ically proven to be effective in provid-
ing higher dimensional information on the
subtle differences among near-synonyms.
By employing supervised learning on the
latent features, our system achieves an ac-
curacy of 74.5% in a ?fill-in-the-blank?
task. The improvement over the current
state-of-the-art is statistically significant.
We also formalize the notion of subtlety
through its relation to semantic space di-
mensionality. Using this formalization
and our learning models, several of our
intuitions about subtlety, dimensionality,
and context are quantified and empirically
tested.
1 Introduction
Lexical choice is the process of selecting content
words in language generation. Consciously or
not, people encounter the task of lexical choice
on a daily basis ? when speaking, writing, and
perhaps even in inner monologues. Its applica-
tion also extends to various domains of natural
language processing, including Natural Language
Generation (NLG, Inkpen and Hirst 2006), writ-
ers? assistant systems (Inkpen, 2007), and sec-
ond language (L2) teaching and learning (Ouyang
et al, 2009).
In the context of near-synonymy, the process
of lexical choice becomes profoundly more com-
plicated. This is partly because of the subtle nu-
ances among near-synonyms, which can arguably
differ along an infinite number of dimensions.
Each dimension of variation carries differences in
style, connotation, or even truth conditions into
the discourse in question (Cruse, 1986), all mak-
ing the seemingly intuitive problem of ?choosing
the right word for the right context? far from triv-
ial even for native speakers of a language. In
a widely-adopted ?fill-in-the-blank? task, where
the goal was to guess missing words (from a set
of near-synonyms) in English sentences, two hu-
man judges achieved an accuracy of about 80%
(Inkpen, 2007). The current state-of-the-art accu-
racy for an automated system is 69.9% (Islam and
Inkpen, 2010).
When the goal is to make plausible or even
elegant lexical choices that best suit the con-
text, the representation of that context becomes a
key issue. We approach this problem in the la-
tent semantic space, where transformed local co-
occurrence data is capable of implicitly inducing
global knowledge (Landauer and Dumais, 1997).
A latent semantic space is constructed by reduc-
ing the dimensionality of co-occurring linguistic
units ? typically words and documents as in La-
tent Semantic Analysis (LSA). We refer to this
level of association (LoA) as document LoA here-
after. Although document LoA can benefit topical
level classification (e.g., as in document retrieval,
Deerwester et al 1990), it is not necessarily suit-
able for lexical-level tasks which might require in-
formation on a more fine-grained level (Edmonds
and Hirst, 2002). Our experimental results show
1182
noticeable improvement when the co-occurrence
matrix is built on a lexical LoA between words
within a given context window.
One intuitive explanation for this improvement
is that the lexical-level co-occurrence might have
helped recover the high-dimensional subtle nu-
ances between near-synonyms. This conjecture
is, however, as imprecise as it is intuitive. The
notion of subtlety has mostly been used qualita-
tively in the literature to describe the level of dif-
ficulty involved in near-synonym lexical choice.
Hence, we endeavor to formalize the concept of
subtlety computationally by using our observa-
tions regarding the relationship between ?subtle?
concepts and their lexical co-occurrence patterns.
We introduce related work on near-synonymy,
lexical choice, and latent semantic space models
in the next section. Section 3 elaborates on lexical
and contextual representations in latent semantic
space. In Section 4, we formulate near-synonym
lexical choice as a learning problem and report our
system performance. Section 5 formalizes the no-
tion of subtlety and its relation to dimensionality
and context. Conclusions and future work are pre-
sented in Section 6.
2 Related Work
2.1 Near-Synonymy and Nuances
Near-synonymy is a concept better explained by
intuition than by definition ? which it does not
seem to have in the existing literature. We thus
borrow Table 1 from Edmonds and Hirst (2002) to
illustrate some basic ideas about near-synonymy.
Cruse (1986) compared the notion of plesionymy
to cognitive synonymy in terms of mutual entail-
ment and semantic traits, which, to the best of our
knowledge, is possibly the closest to a textbook
account of near-synonymy.
There has been a substantial amount of inter-
est in characterizing the nuances between near-
synonyms for a computation-friendly representa-
tion of near-synonymy. DiMarco et al (1993)
discovered 38 dimensions for differentiating near-
synonyms from dictionary usage notes and cat-
egorized them into semantic and stylistic varia-
tions. Stede (1993) focused on the latter and fur-
ther decomposed them into seven scalable sub-
Table 1: Examples of near-synonyms and dimen-
sion of variations (Edmonds and Hirst, 2002).
Types of variation Examples
Continuous, intermittent seep:drip
Emphasis enemy:foe
Denotational, indirect error:mistake
Denotational, fuzzy woods:forest
Stylistic, formality pissed:drunk:inebriated
Stylistic, force ruin:annihilate
Expressed attitude skinny:thin:slim:slender
Emotive daddy:dad:father
Collocational task:job
Selectional pass away:die
Sub-categorization give:donate
categories. By organizing near-synonym vari-
ations into a tree structure, Inkpen and Hirst
(2006) combined stylistic and attitudinal varia-
tion into one class parallel to denotational differ-
ences. They also incorporated this knowledge of
near-synonyms into a knowledge base and demon-
strated its application in an NLG system.
2.2 Lexical Choice Evaluation
Due to their symbolic nature, many of the early
studies were only able to provide ?demo runs? in
NLG systems rather than any empirical evalua-
tion. The study of near-synonym lexical choice
had remained largely qualitative until a ?fill-in-
the-blank? (FITB) task was introduced by Ed-
monds (1997). The task is based on sentences col-
lected from the 1987 Wall Street Journal (WSJ)
that contain any of a given set of near-synonyms.
Each occurrence of the near-synonyms is removed
from the sentence to create a ?lexical gap?, and the
goal is to guess which one of the near-synonyms is
the missing word. Presuming that the 1987 WSJ
authors have made high-quality lexical choices,
the FITB test provides a fairly objective bench-
mark for empirical evaluation for near-synonym
lexical choice. The same idea can be applied to
virtually any corpus to provide a fair amount of
gold-standard data at relatively low cost for lexi-
cal choice evaluation.
The FITB task has since been frequently
adopted for evaluating the quality of lexical choice
systems on a standard dataset of seven near-
synonym sets (as shown in Table 2). Edmonds
1183
(1997) constructed a second-order lexical co-
occurrence network on a training corpus (the 1989
WSJ). He measured the word-word distance us-
ing t-score inversely weighted by both distance
and order of co-occurrence in the network. For
a sentence in the test data (generated from the
1987 WSJ), the candidate near-synonym minimiz-
ing the sum of its distance from all other words in
the sentence (word-context distance) was consid-
ered the correct answer. Average accuracy on the
standard seven near-synonym sets was 55.7%.
Inkpen (2007) modeled word-word distance
using Pointwise Mutual Information (PMI) ap-
proximated by word counts from querying the
Waterloo Multitext System (Clarke et al, 1998).
Word-context distance was the sum of PMI scores
between a candidate and its neighboring words
within a window-size of 10. An unsuper-
vised model using word-context distance directly
achieved an average accuracy of 66.0%, while a
supervised method with lexical features added to
the word-context distance further increased the
accuracy to 69.2%.
Islam and Inkpen (2010) developed a system
which completed a test sentence with possible
candidates one at a time. The candidate gener-
ating the most probable sentence (measured by
a 5-gram language model) was proposed as the
correct answer. N-gram counts were collected
from Google Web1T Corpus and smoothed with
missing counts, yielding an average accuracy of
69.9%.
2.3 Lexical Choice Outside the
Near-synonymy Domain
The problem of lexical choice also comes in many
flavors outside the near-synonymy domain. Reiter
and Sripada (2002) attributed the variation in lexi-
cal choice to cognitive and vocabulary differences
among individuals. In their meteorology domain
data, for example, the term by evening was inter-
preted as before 00:00 by some forecasters but
before 18:00 by others. They claimed that NLG
systems might have to include redundancy in their
output to tolerate cognitive differences among in-
dividuals.
2.4 Latent Semantic Space Models and LoA
LSA has been widely applied in various fields
since its introduction by Landauer and Dumais
(1997). In their study, LSA was conducted on
document LoA on encyclopedic articles and the
latent space vectors were used for solving TOEFL
synonym questions. Rapp (2008) used LSA
on lexical LoA for the same task and achieved
92.50% in accuracy in contrast to 64.38% given
by Landauer and Dumais (1997). This work con-
firmed our early postulation that document LoA
might not be tailored for lexical level tasks, which
might require lower LoAs for more fine-grained
co-occurrence knowledge. Note, however, that
confounding factors might also have led to the dif-
ference in performance, since the two studies used
different weighting schemes and different corpora
for the co-occurrence model1. In Section 3.2 we
will compare models on the two LoAs in a more
controlled setting to show their difference in the
lexical choice task.
3 Representing Words and Contexts in
Latent Semantic Space
We first formalize the FITB task to facili-
tate later discussions. A test sentence t =
{w1, . . . ,w j?1,si,w j+1, . . . ,wm} contains a near-
synonym si which belongs to a set of synonyms
S = {s1, . . . ,sn},1 ? i ? n. A FITB test case is
created by removing si from t, and the context (the
incomplete sentence) c = t?{si} is presented to
subjects with a set of possible choices S to guess
which of the near-synonyms in S is the missing
word.
3.1 Constructing the Latent Space
Representation
The first step in LSA is to build a co-occurrence
matrix M between words and documents, which is
further decomposed by Singular Value Decompo-
sition (SVD) according to the following equation:
Mv?d = Uv?k?k?kV Tk?d
1The former used Groliers Academic American Encyclo-
pedia with weights divided by word entropy, while the latter
used the British National Corpus with weights multiplied by
word entropy.
1184
Here, subscripts denote matrix dimensions, U , ?,
and V together create a decomposition of M, v and
d are the number of word types and documents,
respectively, and k is the number of dimensions
for the latent semantic space. A word w is repre-
sented by the row in U corresponding to the row
for w in M. For a context c, we construct a vector c
of length v with zeros and ones, each correspond-
ing to the presence or absence of a word wi with
respect to c, i.e.,
ci =
{ 1 if wi ? c
0 otherwise
We then take this lexical space vector cv?1 as a
pseudo-document and transform it into a latent se-
mantic space vector c?:
c? = ??1UT c (1)
An important observation is that this represen-
tation is equivalent to a weighted centroid of the
context word vectors: when c is multiplied by
??1UT in Equation (1), the product is essentially
a weighted sum of the rows in U corresponding to
the context words. Consequently, simple modifi-
cations on the weighting can yield other interest-
ing representations of context. Consider, for ex-
ample, the weighting vector wk?1 = (?1, ? ? ? ,?k)T
with
?i = 1|2(pgap? i)?1|
where pgap is the position of the ?gap? in the test
sentence. Multiplying w before ??1 in Equation
(1) is equivalent to giving the centroid gradient-
decaying weights with respect to the distance be-
tween a context word and the near-synonym. This
is a form of a Hyperspace Analogue to Language
(HAL) model, which is sensitive to word order, in
contrast to a bag-of-words model.
3.2 Dimensionality and Level of Association
The number of dimensions k is an important
choice to make in latent semantic space mod-
els. Due to the lack of any principled guideline
for doing otherwise, we conducted a brute force
grid search for a proper k value for each LoA, on
the basis of the performance of the unsupervised
model (Section 4.1 below).
Figure 1: FITB Performance on different LoAs as
a function of the latent space dimensionality.
In Figure 1, performance on FITB using this
unsupervised model is plotted against k for doc-
ument and lexical LoAs. Document LoA is very
limited in the available number of dimensions2;
higher dimensional knowledge is simply unavail-
able from this level of co-occurrence. In contrast,
lexical LoA stands out around k = 550 and peaks
around k = 700. Although the advantage of lexi-
cal LoA in the unsupervised setting is not signif-
icant, later we show that lexical LoA nonetheless
makes higher-dimensional information available
for other learning methods.
Note that the scale on the y-axis is stretched to
magnify the trends. On a zero-to-one scale, the
performance of these unsupervised methods is al-
most indistinguishable, indicating that the unsu-
pervised model is not capable of using the high-
dimensional information made available by lexi-
cal LoA. We will elaborate on this point in Section
5.2.
2The dimensions for document and lexical LoAs on our
development corpus are 55,938?500 and 55,938?55,938,
respectively. The difference is measured between v? d and
v? v (Section 3.1).
1185
4 Learning in the Latent Semantic Space
4.1 Unsupervised Vector Space Model
When measuring distance between vectors, LSA
usually adopts regular vector space model dis-
tance functions such as cosine similarity. With the
context being a centroid of words (Section 3.1),
the FITB task then becomes a k-nearest neighbor
problem in the latent space with k = 1 to choose
the best near-synonym for the context:
s? = argmax
si
cos(UrowId(v(si),M), c?)
where v(si) is the corresponding row for near-
synonym si in M, and rowId(v,M) gives the row
number of a vector v in a matrix M containing v
as a row.
In a model with a cosine similarity distance
function, it is detrimental to use ??1 to weight the
context centroid c?. This is because elements in ?
are the singular values of the co-occurrence matrix
along its diagonal, and the amplitude of a singular
value (intuitively) corresponds to the significance
of a dimension in the latent space; when the in-
verted matrix is used to weight the centroid, it will
?misrepresent? the context by giving more weight
to less-significantly co-occurring dimensions and
thus sabotage performance. We thus use ? instead
of ??1 in our experiments. As shown in Figure
1, the best unsupervised performance on the stan-
dard FITB dataset is 49.6%, achieved on lexical
LoA at k = 800.
4.2 Supervised Learning on the Latent
Semantic Space Features
In traditional latent space models, the latent space
vectors have almost invariantly been used in the
unsupervised setting discussed above. Although
the number of dimensions has been reduced in the
latent semantic space, the inter-relations between
the high-dimension data points may still be com-
plex and non-linear; such problems lend them-
selves naturally to supervised learning.
We therefore formulate the near-synonym lex-
ical choice problem as a supervised classification
problem with latent semantic space features. For
a test sentence in the FITB task, for example, the
context is represented as a latent semantic space
vector as discussed in Section 3.1, which is then
paired with the correct answer (the near-synonym
removed from the sentence) to form one training
case.
We choose Support Vector Machines (SVMs) as
our learning algorithm for their widely acclaimed
classification performance on many tasks as well
as their noticeably better performance on the lex-
ical choice task in our pilot study. Table 2 lists
the supervised model performance on the FITB
task together with results reported by other related
studies. The model is trained on the 1989 WSJ
and tested on the 1987 WSJ to ensure maximal
comparability with other results. The optimal k
value is 415. Context window size3 around the
gap in a test sentence also affects the model per-
formance. In addition to using the words in the
original sentence, we also experiment with enlarg-
ing the context window to neighboring sentences
and shrinking it to a window frame of n words
on each side of the gap. Interestingly, when mak-
ing the lexical choice, the model tends to favor
more-local information ? a window frame of size
5 gives the best accuracy of 74.5% on the test.
Based on binomial exact test4 with a 95% confi-
dence interval, our result outperforms the current
state-of-the-art with statistical significance.
5 Formalizing Subtlety in the Latent
Semantic Space
In this section, we formalize the notion of sub-
tlety through its relation to dimensionality, and
use the formalization to provide empirical support
for some of the common intuitions about subtlety
and its complexity with respect to dimensionality
and size of context.
5.1 Characterizing Subtlety Using
Collocating Differentiator of Subtlety
In language generation, subtlety can be viewed as
a subordinate semantic trait in a linguistic realiza-
3Note that the context window in this paragraph is im-
plemented on FITB test cases, which is different from the
context size we compare in Section 5.3 for building co-
occurrence matrix.
4The binomial nature of the outcome of an FITB test case
(right or wrong) makes binomial exact test a more suitable
significance test than the t-test used by Inkpen (2007).
1186
Table 2: Supervised performance on the seven standard near-synonym sets in the FITB task. 95%
Confidence based on Binomial Exact Test.
Near-synonyms
Co-occur. SVMs 5-gram SVMs on
network & PMI language model latent vectors
(Edmonds, 1997) (Inkpen, 2007) (Islam and Inkpen, 2010) (Section 4.2)
difficult, hard, tough 47.9% 57.3% 63.2% 61.7%
error, mistake, oversight 48.9% 70.8% 78.7% 82.5%
job, task, duty 68.9% 86.7% 78.2% 82.4%
responsibility, burden, 45.3% 66.7% 72.2% 63.5%
obligation, commitment
material, stuff, substance 64.6% 71.0% 70.4% 78.5%
give, provide, offer 48.6% 56.1% 55.8% 75.4%
settle, resolve 65.9% 75.8% 70.8% 77.9%
Average 55.7% 69.2% 69.9% 74.5%
Data size 29,835 31,116 31,116 30,300
95% confidence 55.1?56.3% 68.7?69.7% 69.3?70.4% 74.0?75.0%
tion of an intention5. A key observation regard-
ing subtlety is that it is non-trivial to characterize
subtle differences between two linguistic units by
their collocating linguistic units. More interest-
ingly, the difficulty in such characterization can
be approximated by the difficulty in finding a third
linguistic unit satisfying the following constraints:
1. The unit must collocate closely with at least
one of the two linguistic units under differ-
entiation;
2. The unit must be characteristic of the differ-
ence between the pair.
Such approximation is meaningful in that it trans-
forms the abstract characterization into a concrete
task of finding this third linguistic unit. For ex-
ample, suppose we want to find out whether the
difference between glass and mug is subtle. The
approximation boils the answer down to the dif-
ficulty of finding a third word satisfying the two
constraints, and we may immediately conclude
that the difference between the pair is not subtle
since it is relatively easy to find wine as the quali-
fying third word, which 1) collocates closely with
glass and 2) characterizes the difference between
5The same principle applies when we replace ?genera-
tion? with ?understanding? and ?an intention? with ?a cogni-
tion?.
the pair by instantiating one of their major differ-
ences ? the purpose of use. The same reasoning
applies to concluding non-subtlety for word pairs
such as pen and pencil with sharpener, weather
and climate with forecast, watch and clock with
wrist, etc.
In contrast, for the pair forest and woods, it
might be easy to find words satisfying one but not
both constraints. Consequently, the lack of such
qualifying words ? or at least the relative diffi-
culty for finding one ? makes the difference be-
tween this pair more subtle than in the previous
examples.
We call a linguistic unit satisfying both con-
straints a collocating differentiator of subtlety
(CDS). Notably, the second constraint puts an im-
portant difference between CDSs and the conven-
tional sense of collocation. On the lexical level,
CDSs are not merely words that collocate more
with one word in a pair than with the other; they
have to be characteristic of the differences be-
tween the pair. In the example of forest and
woods, one can easily find a word exclusively col-
locating with one but not the other ? such as na-
tional forest but not *national woods; however,
unlike the CDSs in the previous examples, the
word national does not characterize any of the dif-
ferences between the pair in size, primitiveness,
1187
proximity to civilization, or wildness (Edmonds
and Hirst, 2002), and consequently fails to satisfy
the second constraint.
5.2 Relating Subtlety to Latent Space
Dimensionality6
As mentioned in Section 4.1, elements of a latent
space vector are in descending order in terms of
co-occurrence significance, i.e., the information
within the first few dimensions is obtained from
more closely collocating linguistic units. From
the two constraints in the previous section, it fol-
lows that it should be relatively easier to find a
CDS for words that can be well distinguished in a
lower-dimensional sub-space of the latent seman-
tic space, and the difference among such words
should not be considered subtle.
We thus claim that co-occurrence-based infor-
mation capable of characterizing subtle differ-
ences must then reside in higher dimensions in
the latent space vectors. Furthermore, our intu-
ition on the complexity of subtlety can also be
empirically tested by comparing the performance
of supervised and unsupervised models at differ-
ent k values. One of the differences between the
two types of models is that supervised models are
better at unraveling the convoluted inter-relations
between high-dimensional data points. Under this
assumption, if we hypothesize that subtlety is a
certain form of complex, high-dimensional rela-
tion between semantic elements, then the differ-
ence in performance between the supervised and
unsupervised model should increase as the former
recovers subtle information in higher dimensions.
As shown in Figure 2, performance of both
models is positively correlated to the number of
dimensions in the latent semantic space (with cor-
relation coefficient ? = 0.95 for supervised model
and ? = 0.81 for unsupervised model). This sug-
gests that the lexical choice process is indeed
?picking up? implicit information about subtlety
in the higher dimensions of the latent vectors.
Meanwhile, the difference between the perfor-
mance of the two models correlates strongly to k
with ? = 0.95. Significance tests on the ?differ-
6In order to keep the test data (1987 WSJ) unseen before
producing the results in Table 2, models in this section were
trained on The Brown Corpus and tested on 1988?89 WSJ.
Figure 2: Supervised performance increasing fur-
ther from unsupervised performance in higher di-
mensions.
ence of difference?7 between their performances
further reveal increasing difference in growth rate
of their performance. Significance is witnessed in
both the F-test and the paired t-test,8 indicating
that the subtlety-related information in the higher
dimensions exhibits complex clustering patterns
that are better recognized by SVMs but beyond
the capability of the KNN model.
5.3 Subtlety and the Level of Context
Our previous models on lexical LoA associated
words within the same sentence to build the co-
occurrence matrix. Lexical LoA also allows us
to associate words that co-occur in different lev-
els of context (LoC) such as paragraphs or docu-
ments. This gives an approximate measurement
of how much context a lexical LoA model uses
for word co-occurrence. Intuitively, by looking at
more context, higher LoC models should be better
at differentiating more subtle differences.
We compare the performance of models with
different LoCs in Figure 3. The sentence LoC
model constantly out-performs the paragraph LoC
model after k = 500, indicating that, by inter-
model comparison, larger LoC models do not
necessarily perform better on higher dimensions.
However, there is a noticeable difference in the
optimal dimensionality for the model perfor-
mances. Sentence LoC performance peaks around
7The italicized difference is used in its mathematical
sense as the discrete counterpart of derivative.
8F-test: f (1,16) = 9.13, p < 0.01. Paired t-test: t(8) =
4.16 with two-tailed p = 0.0031. Both conducted on 10 data
points at k = 50 to 500 with a step of 50.
1188
Figure 3: LoC in correlation to latent space di-
mensionality for optimal model performance.
k = 700 ? much lower than that of paragraph
LoC which is around k = 1,100. Such differ-
ence may suggest that, by intra-model compari-
son, each model may have its own ?comfort zone?
for the degree of subtlety it differentiates; models
on larger LoC are better at differentiating between
more subtle nuances, which is in accordance with
our intuition.
One possible explanation for sentence LoC
models outperforming paragraph LoC models is
that, although the high-dimensional elements are
weighed down by ? due to their insignificance in
the latent space, their contribution to the output
of distance function is larger in paragraph LoC
models because the vectors are much denser than
that in the sentence LoC model; since the unsuper-
vised method is incapable of recognizing the clus-
tering patterns well in high-dimensional space,
the ?amplified? subtlety information is eventually
taken as noise by the KNN model. An interesting
extension to this discussion is to see whether a su-
pervised model can consistently perform better on
higher LoC in all dimensions.
6 Conclusions and Future Work
We propose a latent semantic space representa-
tion of near-synonyms and their contexts, which
allows a thorough investigation of several aspects
of the near-synonym lexical choice problem. By
employing supervised learning on the latent space
features, we achieve an accuracy of 74.5% on the
?fill-in-the-blank? task, outperforming the current
state-of-the-art with statistical significance.
In addition, we formalize the notion of subtlety
by relating it to the dimensionality of the latent se-
mantic space. Our empirical analysis suggests that
subtle differences between near-synonyms reside
in higher dimensions in the latent semantic space
in complex clustering patterns, and that the degree
of subtlety correlates to the level of context for co-
occurrence. Both conclusions are consistent with
our intuition.
As future work, we will make better use of the
easy customization of the context representation
to compare HAL and other models with bag-of-
words models. The correlation between subtlety
and dimensionality may lead to many interesting
tasks, such as measuring the degree of subtlety for
individual near-synonyms or near-synonym sets.
With regard to context representation, it is also
intriguing to explore other dimensionality reduc-
tion methods (such as Locality Sensitive Hashing
or Random Indexing) and to compare them to the
SVD-based model.
Acknowledgment
This study is supported by the Natural Sciences
and Engineering Research Council of Canada
(NSERC). We greatly appreciate the valuable sug-
gestions and feedback from all of our anonymous
reviewers and from our colleagues Julian Brooke,
Frank Rudzicz, and George Dahl.
1189
References
Charles L. A. Clarke, Gordon Cormack, and
Christopher Palmer. An overview of MultiText.
ACM SIGIR Forum, 32(2):14?15, 1998.
D. A. Cruse. Lexical Semantics. Cambridge Uni-
versity Press, 1986.
Scott Deerwester, Susan Dumais, George Furnas,
Thomas Landauer, and Richard Harshman. In-
dexing by latent semantic analysis. Journal of
the American Society for Information Science,
41(6):391?407, 1990.
Chrysanne DiMarco, Graeme Hirst, and Manfred
Stede. The semantic and stylistic differentiation
of synonyms and near-synonyms. AAAI Spring
Symposium on Building Lexicons for Machine
Translation, pages 114?121, 1993.
Philip Edmonds. Choosing the word most typi-
cal in context using a lexical co-occurrence net-
work. In Proceedings of the 35th annual meet-
ing of the Association for Computational Lin-
guistics and Eighth Conference of the European
Chapter of the Association for Computational
Linguistics, pages 507?509, 1997.
Philip Edmonds and Graeme Hirst. Near-
synonymy and lexical choice. Computational
Linguistics, 28(2):105?144, 2002.
Diana Inkpen. A statistical model for near-
synonym choice. ACM Transactions on Speech
and Language Processing, 4(1):1?17, 2007.
Diana Inkpen and Graeme Hirst. Building and us-
ing a lexical knowledge-base of near-synonym
differences. Computational Linguistics, 32(2):
223?262, 2006.
Aminul Islam and Diana Inkpen. Near-synonym
choice using a 5-gram language model. Re-
search in Computing Sciences, 46:41?52, 2010.
Thomas Landauer and Susan Dumais. A solution
to Plato?s problem: the latent semantic analy-
sis theory of acquisition, induction, and repre-
sentation of knowledge. Psychological Review,
104(2):211?240, 1997.
Shixiao Ouyang, Helena Hong Gao, and
Soo Ngee Koh. Developing a computer-
facilitated tool for acquiring near-synonyms
in Chinese and English. In Proceedings
of the Eighth International Conference on
Computational Semantics, pages 316?319,
2009.
Reinhard Rapp. The automatic generation of the-
sauri of related words for English, French, Ger-
man, and Russian. International Journal of
Speech Technology, 11(3):147?156, 2008.
Ehud Reiter and Somayajulu Sripada. Human
variation and lexical choice. Computational
Linguistics, 28(4):545?553, 2002.
Manfred Stede. Lexical choice criteria in lan-
guage generation. In Proceedings of the sixth
conference of the European Chapter of the As-
sociation for Computational Linguistics, pages
454?459, 1993.
1190
Coling 2010: Poster Volume, pages 90?98,
Beijing, August 2010
Automatic Acquisition of Lexical Formality
Julian Brooke, Tong Wang, and Graeme Hirst
Department of Computer Science
University of Toronto
{jbrooke,tong,gh}@cs.toronto.edu
Abstract
There has been relatively little work fo-
cused on determining the formality level
of individual lexical items. This study
applies information from large mixed-
genre corpora, demonstrating that signif-
icant improvement is possible over simple
word-length metrics, particularly when
multiple sources of information, i.e. word
length, word counts, and word associ-
ation, are integrated. Our best hybrid
system reaches 86% accuracy on an En-
glish near-synonym formality identifica-
tion task, and near perfect accuracy when
comparing words with extreme formality
differences. We also test our word as-
sociation method in Chinese, a language
where word length is not an appropriate
metric for formality.
1 Introduction
The derivation of lexical resources for use in
computational applications has been focused pri-
marily on the denotational relationships among
words, e.g. the synonym and hyponym relation-
ships encapsulated in WordNet (Fellbaum, 1998).
Largely missing from popular lexical resources
such as WordNet and the General Inquirer (Stone
et al, 1966) is stylistic information; there are,
for instance, no resources which provide com-
prehensive information about the formality level
of words, which relates to the appropriateness
of a word in a given context. Consider, for
example, the problem of choice among near-
synonyms: there are only minor denotational dif-
ferences among synonyms such as get, acquire,
obtain, and snag, but it is difficult to construct a
situation where any choice would be equally suit-
able. The key difference between these words is
their formality, with acquire the most formal and
snag the most informal.
In this work, we conceive of formality as
a continuous property. This approach is in-
spired by resources such as Choose The Right
Word (Hayakawa, 1994), in which differences be-
tween synonyms are generally described in rela-
tive rather than absolute terms, as well as linguis-
tic literature in which the quantification of stylis-
tic differences among genres is framed in terms of
dimensions rather than discrete properties (Biber,
1995). We begin by defining the formality score
for a word as a real number value in the range 1
to ?1, with 1 representing an extremely formal
word, and ?1 an extremely informal word. A
formality lexicon, then, gives a FS score to every
word within its coverage.
The core of our approach to the problem of
classifying lexical formality is the automated cre-
ation of formality lexicons from large corpora. In
this paper, we focus on the somewhat low-level
task of identifying the relative formality of word
pairs; we believe, however, that a better under-
standing of lexical formality is relevant to a num-
ber of problems in computational linguistics, in-
cluding sub-fields such as text generation, error
correction of (ESL) writing, machine translation,
text classification, text simplification, word-sense
disambiguation, and sentiment analysis. One con-
clusion of our research is that formality variation
is omnipresent in natural corpora, but it does not
follow that the identification of these differences
on the lexical level is a trivial one; nevertheless,
90
we are able to make significant progress using the
methods presented here, in particular the applica-
tion of latent semantic analysis to blog corpora.
2 Related Work
As far as we are aware, there are only a few
lines of research explicitly focused on the ques-
tion of linguistic formality. In linguistics proper,
the study of register and genre usually involves
a number of dimensions or clines, sometimes
explicitly identified as formality (Leckie-Tarry,
1995; Carter, 1998), or decomposed into notions
such as informational versus interpersonal con-
tent (Biber, 1995). Heyligen and Dewaele (1998)
provide a part-of-speech based quantification of
textual contextuality (which they argue is funda-
mental to the notion of formality); their metric
has been used, for instance, in a computational
investigation of the formality of online encyclo-
pedias (Emigh and Herring, 2005). In this kind
of quantification, however, there is little, if any,
focus on individual elements of the lexicon. In
computational linguistics, formality has received
attention in the context of text generation (Hovy,
1990); of particular note relevant to our research
is the work of Inkpen and Hirst (2006), who de-
rive boolean formality tags from Choose the Right
Word (Hayakawa, 1994). Like us, their focus was
improved word choice, though the approach was
much broader, also including dimensions such as
polarity. An intriguing example of formality rel-
evant to text classification is the use of infor-
mal language (slang) to help distinguish true news
from satire (Burfoot and Baldwin, 2009).
Our approach to this task is inspired and in-
formed by automatic lexical acquisition research
within the field of sentiment analysis (Turney
and Littman, 2003; Esuli and Sebastiani, 2006;
Taboada and Voll, 2006; Rao and Ravichandra,
2009). Turney and Littman (2003) apply latent
semantic analysis (LSA) (Landauer and Dumais,
1997) and pointwise mutual information (PMI) to
derive semantic orientation ratings for words us-
ing large corpora; like us, they found that LSA
was a powerful technique for deriving this lexical
information. The lexical database SentiWordNet
(Esuli and Sebastiani, 2006) provides 0?1 rank-
ings for positive, negative, and neutral polarity,
derived automatically using relationships between
words in WordNet (Fellbaum, 1998). Unfortu-
nately, WordNet synsets tend to cut across the for-
mal/informal distinction, and so the resource is
not obviously useful for our task.
The work presented here builds directly on a pi-
lot study (Brooke et al, 2010), the focus of which
was the construction of formality score (FS) lex-
icons. In that work, we employed less sophis-
ticated forms of some of the methods used here
in a relatively small dataset (the Brown Corpus),
providing a proof of concept, but with poor cov-
erage, and with no attempt to combine the meth-
ods to maximize performance. However, the small
dataset alowed us to do a thorough test of certain
options associated with our task. In particular we
found that using a similarity metric based on LSA
gave good performance across our test sets, es-
pecially when the term-document matrix was bi-
nary (unweighted), the k-value used for LSA was
small, and the method used to derive a formality
score was cosine similarity to our seed terms. A
metric using total word counts in corpora with di-
vergent formality also showed promise, with both
methods performing above our word-length base-
line for words within their coverage. PMI, by
comparison, proved less effective, and we do not
pursue it further here.
3 Data and Resources
3.1 Word Lists
All the word lists discussed here are publicly
available.1 We begin with two, one formal and
one informal, that we use both as seeds for our
lexicon construction methods and as test sets for
evaluation (our gold standard). We assume that
all slang terms are by their very nature informal
and so our 138 informal seeds were taken primar-
ily from an online slang dictionary2 (e.g. wuss,
grubby) and also include some contractions and
interjections (e.g. cuz, yikes). The 105 formal
seeds were selected from a list of discourse mark-
ers (e.g. moreover, hence) and adverbs from a sen-
timent lexicon (e.g. preposterously, inscrutably);
these sources were chosen to avoid words with
1 http://www.cs.toronto.edu/?jbrooke/FormalityLists.zip
2 http://onlineslangdictionary.com/
91
overt topic, and to ensure that there was some
balance of sentiment across formal and informal
seed sets. Part of speech, however, is not balanced
across our seed sets.
Another test set we use to evaluate our methods
is a collection of 399 pairs of near-synonyms from
Choose the Right Word (CTRW), a manual for as-
sisting writers with synonym word choice; each
pair was either explicitly or implicitly compared
for formality in the book. Implicit comparison in-
cluded statements such as this is the most formal
of these words; in those cases, and more gener-
ally, we avoided words appearing in more than
one comparison (there are no duplicate words in
our CTRW set), as well as multiword expressions
and words whose formality is strongly ambigu-
ous (i.e. word-sense dependent). An example of
this last phenomenon is the word cool, which is
used colloquially in the sense of good but more
formally as in the sense of cold. Partly as a re-
sult of this polysemy, which is clearly more com-
mon among informal words, our pairs are biased
toward the formal end of the spectrum; although
there are some informal comparisons, e.g. belly-
ache/whine, wisecrack/joke, more typical pairs
include determine/ascertain and hefty/ponderous.
Despite this imbalance, one obvious advantage
of using near-synonyms in our evaluation is that
factors other than linguistic formality (e.g. topic,
opinion) are less likely to influence performance.
In general, the CTRW allows for a more objective,
fine-grained evaluation of our methods, and is ori-
ented towards our primary interest, near-synonym
word choice.
To test the performance of our unsupervised
method beyond English, one of the authors (a na-
tive speaker of Mandarin Chinese) created two
sets of Chinese two-character words, one formal,
one informal, based on but not limited to the
words in the English sets. The Chinese seeds in-
clude 49 formal seeds and 43 informal seeds.
3.2 Corpora
Our corpora fall generally into three categories:
formal (written) copora, informal (spoken) cor-
pora, and mixed corpora. The Brown Corpus
(Francis and Kuc?era, 1982), our development cor-
pus, is used here both as a formal and mixed cor-
pus. Although extremely small by modern cor-
pus standards (only 1 million words), the Brown
Corpus has the advantage of being compiled ex-
plicitly to represent a range of American English,
though it is all of the published, written variety.
The Switchboard (SW) Corpus is a collection of
American telephone conversations (Godfrey et al,
1992), which contains roughly 2400 conversations
with over 2.6 million word tokens; we use it as an
informal counterpart to the Brown Corpus. Like
the Brown Corpus, The British National Corpus
(Burnard, 2000) is a manually-constructed mixed-
genre corpus; it is, however, much larger (roughly
100 million words). It contains a written portion
(90%), which we use as a formal corpus, and a
spontaneous spoken portion (4.3%), which we use
as an informal corpus. Our other mixed corpora
are two blog collections available to us: the first,
which we call our development blog corpus (Dev-
Blog) contains a total of over 900,000 English
blogs, with 216 million tokens.3 The second is the
?first tier? English blogs included in the publicly
available ICSWM 2009 Spinn3r Dataset (Burton
et al, 2009), a total of about 1.3 billion word to-
kens in 7.5 million documents. For our investiga-
tions in Chinese, we use the Chinese portion of the
ICSWM blogs, approximately 25.4 million char-
acter tokens in 86,000 documents.
4 Methods
4.1 Simple Formality Measures
The simplest kind of formality measure is based
on word length, which is often used directly as
an indicator of formality for applications such as
genre classification (Karlgren and Cutting, 1994).
Here, we use logarithmic scaling to derive a FS
score based on word length. Given a maximum
word length L4 and a word w of length l, the for-
mality score function, FS(w), is given by:
FS(w) =?1+2 log llogL
3These blogs were gathered by the University of Toronto
Blogscope project (www.blogscope.net) over a week in May
2008.
4We use an upper bound of 28 characters, which is
the length of antidisestablishmentarianism, the prototypical
longest word in English; this value of L provides an appropri-
ate formality/informality threshold, between 5- and 6-letter
words
92
For hyphenated terms, the length of each compo-
nent is averaged. Though this metric works rela-
tively well for English, we note that it is problem-
atic in a language with significant word aggluti-
nation (e.g. German) or without an alphabet (e.g.
Chinese, see below).
Another straightforward method is the assump-
tion that Latinate prefixes and suffixes are indica-
tors of formality in English (Kessler et al, 1997),
i.e. informal words will not have Latinate affixes
such as -ation and intra-. Here, we simply assign
words that appear to have such a prefix or suffix
an FS of 1, and all other words an FS of ?1.
Our frequency methods derive FS from word
counts in corpora. Our first, naive approach as-
sumes a single corpus, where either formal words
are common and informal words are rare, or vice
versa. To smooth out the Zipfian distribution, we
use the frequency rank of words as exponentials;
for a corpus with R frequency ranks, the FS for a
word of rank r under the formal is rare assumption
is given by:
FS(w) =?1+2 e
(r?1)
e(R?1)
Under the informal is rare assumption:
FS(w) = 1?2 e
(r?1)
e(R?1)
We have previously shown that these methods are
not particularly effective on their own (Brooke et
al., 2010), but we note that they provide useful
information for a hybrid system.
A more sophisticated method is to use two cor-
pora that are known to vary with respect to for-
mality and use the relative appearance of words in
each corpus as the metric. If word appears n times
in a (relatively) formal corpus and m times in an
informal corpus (and one of m, n is not zero), we
derive:
FS(w) =?1+2 nm?N +n
Here, N is the ratio of the size (in tokens) of the
informal corpus (IC) to the formal corpus (FC).
We need the constant N so that an imbalance in
the size of the corpora does not result in an equiv-
alently skewed distribution of FS.
4.2 Latent Semantic Analysis
Next, we turn to LSA, a technique for extracting
information from a large corpus of texts by (dras-
tically) reducing the dimensionality of a term?
document matrix, i.e. a matrix where the row vec-
tors correspond to the appearance or (weighted)
frequency of words in a set of texts. In essence,
LSA simplifies the variation of words across a col-
lection of texts, exploiting document?document
correlation to produce information about the k
most important dimensions of variation (k < to-
tal number of documents), which are generally
thought to represent semantic concepts, i.e. topic.
The mathematical basis for this transformation is
singular value decomposition5; for the details of
the matrix transformations, we refer the reader to
the discussion of Turney and Littman (2003). The
factor k, the number of columns in the compacted
matrix, is an important variable in any application
of LSA, one is generally determined by trial and
error (Turney and Littman, 2003).
LSA is computationally intensive; in order to
apply it to extremely large blog corpora, we need
to filter the documents and terms before build-
ing our term?document matrix. We adopt the
following strategy: to limit the number of docu-
ments in our term?document matrix, we first re-
move documents less than 100 tokens in length,
with the rationale that these documents provide
less co-occurrence information. Second, we re-
move documents that either do not contain any
target words (i.e. one of our seeds or CTRW test
words), or contain only target words which are
among the most common 20 in the corpus; these
documents are less likely to provide us with use-
ful information, and the very common target terms
will be well represented regardless. We further
shrink the set of terms by removing all hapax
legomena; a single appearance in a corpus is not
enough to provide reliable co-occurrence informa-
tion, and roughly half the words in our blog cor-
pora appear only once. Finally, we remove sym-
bols and all words which are not entirely lower
5We use the implementation included in Matlab; we take
the rows of the decomposed U matrix weighted by the sin-
gular values in ? for our word vectors. Using no weights
or ??1 generally resulted in worse performance, particularly
with the CTRW sets.
93
case; we are not interested, for instance, in num-
bers, acronyms, and proper nouns. We can esti-
mate the effect this filtering has on performance
by testing it both ways in a development corpus.
Once a k-dimensional vector for each relevant
word is derived using LSA, a standard method is
to use the cosine of the angle between a word vec-
tor and the vectors of seed words to identify how
similar the distribution of the word is to the distri-
bution of the seeds. To begin, each formal seed is
assigned a FS value of 1, each informal seed a FS
value of ?1, and then a raw seed similarity score
(FS?) is calculated for each word w:
FS?(w) = ?
s?S,s6=w
Ws?FS(s)? cos(?(w,s))
S is the set of all seeds. Note that seed terms are
excluded from their own FS calculation, this is
equivalent to leave-one-out cross-validation. Ws
is a weight that depends on whether s is a formal
or informal seed, Wi (for informal seeds) is calcu-
lated as:
Wi = ? f?F FS( f )|?i?I FS(i)|+? f?F FS( f )
and Wf (for formal seeds) is:
Wf = |?i?I FS(i)||?i?I FS(i)|+? f?F FS( f )
Here, I is the set of all informal seeds, and F is the
set of all formal seeds. These weights have the ef-
fect of countering any imbalance in the seed set,
as formal and informal seeds ultimately have the
same (potential) influence on each word, regard-
less of their count. This weighting is necessary for
the iterative extension of this method discussed in
the next section.
We calculate the final FS score as follows:
FS(w) = FS
?(w)?FS?(r)
Nw
The word r is a reference term, a common func-
tion word that has no formality.6 This has the ef-
fect of countering any (moderate) bias that might
6The particular choice of this word is relatively unimpor-
tant; common function words all have essentially the same
LSA vectors because they appear at least once in nearly ev-
ery document of any size. For English, we chose r = and,
and for Chinese, r = yinwei (because); there does not seem
to be an obvious two-character, formality-neutral equivalent
to and in Chinese.
exist in the corpus; in the Brown Corpus, for in-
stance, function words have positive formality be-
fore this step, simply because formal words oc-
curred more often in the corpus. Nw is a normal-
ization factor, either
Nw = maxwi?I? |FS
?(wi)?FS?(r)|
for all wi ? I? or
Nw = maxw f?F ? |FS
?(w f )?FS?(r)|
for all w f ? F ?. I? contains all words w such that
FS?(w)?FS?(r) < 0, and F ? contains all words w
such that FS?(w)?FS?(r) > 0. This ensures that
the resulting lexicon has terms exactly in the range
1 to?1, with the reference word r at the midpoint.
We also tested the LSA method in Chinese.
The only major relevant difference between Chi-
nese and English is word segmentation: Chinese
does not have spaces between words. To sidestep
this problem, we simply included all character bi-
grams found in our corpus. The drawback of this
approach in the inclusion of a huge number of
nonsense ?words? (1.3 million terms in just 86,000
documents), however we are at least certain to
identify all instances of our seeds.
4.3 Hybrid Methods
There are a number of ways to leverage the infor-
mation we derive from our basic methods. One
intriguing option is to use the basic FS measures
as the starting point for an iterative process using
the LSA cosine similarity. Under this paradigm,
all words in the starting FS lexicon are potential
seed words; we choose a cutoff value for inclu-
sion in the seed word set (e.g. words which have
at least .5 or ?.5 FS), and then carry out the co-
sine calculations, as above, to derive new FS val-
ues (a new FS lexicon). We can repeat this process
as many times as required, with the idea that the
connections between various words (as reflected
in their LSA-derived vectors) will cause the sys-
tem to converge towards the true FS values.
A simple hybrid method that combines the two
word count models uses the ratio of word counts
in two corpora to define the center of the FS spec-
trum, but single corpus methods to define the ex-
tremes. Formally, if m and n (word counts for the
94
informal corpus IC and formal corpus FC, respec-
tively) are both non-zero, then FS is given by:
FS(w) =?0.5+ nm?N +n
However, if n is zero, FS is given by:
FS(w) =?1+0.5 e
?rIC?1
e?RIC?1
where rIC is the frequency rank of the word in IC,
and RIC is the total number of ranks in IC. If m is
zero, FS is given by:
FS(w) = 1?0.5 e
?rFC?1
e?RFC?1
where i is the rank of the word in IC, and RIC is the
total number of frequency ranks in IC). This func-
tion is undefined in the case where m and n are
both zero. Intuitively, this is a kind of backoff, re-
lying on the idea that words of extreme formality
are rare even in a corpus of corresponding formal-
ity, whereas words in the core vocabulary (Carter,
1998), which are only moderately formal, will ap-
pear in all kinds of corpora, and thus are amenable
to the ratio method.
Finally, we explore a number of ways to com-
bine lexicons directly. The motivation for this
is that the lexicons have different strengths and
weaknesses, representing partially independent
information. An obvious method is an averag-
ing or other linear combination of the scores, but
we also investigate vote-based methods (requiring
agreement among n dictionaries). Beyond these
simple options, we test support vector machines
and naive Bayes classification using the WEKA
software suite (Witten and Frank, 2005), applying
10-fold cross-validation using default WEKA set-
tings for each classifier. The features here are task
dependent (see Section 5); for the pairwise task,
we use the difference between the FS value of the
words in each lexicon, rather than their individ-
ual scores. Finally, we can use the weights from
the SVM model of the CTRW (pairwise) task to
interpolate an optimal formality lexicon.
5 Evaluation
We evaluate our methods using the gold standard
judgments from the seed sets and CTRW word
pairs. To differentiate the two, we continue to use
the term seed for the former; in this context, how-
ever, these ?seed sets? are being viewed as a test
set (recall that our LSA method is equivalent to
leave-one-out cross-validation).
We derive the following measures: first, the
coverage (Cov.) is the percentage of words in the
set that are covered under the method. The class-
based accuracy (C-Acc.) of our seed sets is the
percentage of covered words which are correctly
classified as formal (FS > 0) or informal (FS <
0). The pair-based accuracy (P-Acc.) is the result
of exhaustively pairing words in the two seed sets
and testing their relative formality; that is, for all
wi ? I and w f ? F , the percentage of wi/w f pairs
where FS(wi) < FS(w f ). For the CTRW pairs
there are only two metrics, the coverage and the
pair-based accuracy; since the CTRW pairs repre-
sent relative formality of varying degrees, it is not
possible to calculate a class-based accuracy.
The first section of Table 1 provides the re-
sults for the basic methods in various corpora.
The word length (1) and morphology-based (2)
methods provide good coverage, but poor accu-
racy, while the word count ratio methods (3?4) are
fairly accurate, but suffer from low coverage. The
LSA results in Table 1 are the best for each corpus
across the k values we tested. When both cover-
age and accuracy are considered, there is a clear
benefit associated with increasing the amount of
data, though the difference between the Dev-Blog
and ICWSM suggests diminishing returns. The
performance of the filtered Dev-Blog is actually
slightly better than the unfiltered versions (though
there is a drop in coverage), suggesting that filter-
ing is a good strategy.
In our previous work (Brooke et al, 2010), we
noted that CTRW set performance in the Brown
dropped for k > 3, while performance on the seed
set was mostly steady as k increased. Figure 1
shows the pairwise performance of each test set
for various corpora across various k. The results
here are similar; all three corpora reach a CTRW
maximum at a relatively low k values (though
higher than Brown Corpus); however the seed set
performance in each corpus continues to improve
(though marginally) as k increases, while CTRW
performance drops. An explanation for this is that
95
Table 1: Seed coverage, class-based accuracy, pairwise accuracy, CTRW coverage, and pairwise accu-
racy for various FS lexicons and hybrid methods (%).
Seed set CTRW set
Method Cov. C-Acc. P-Acc. Cov. P-Acc.
Simple
(1) Word length 100 86.4 91.8 100 63.7
(2) Latinate affix 100 74.5 46.3 100 32.6
(3) Word count ratio, Brown and Switchboard 38.0 81.5 85.7 36.0 78.2
(4) Word count ratio, BNC Written vs. Spoken 60.9 89.2 97.3 38.8 74.3
(5) LSA (k=3), Brown 51.0 87.1 94.2 59.6 73.9
(6) LSA (k=10), BNC 94.7 83.0 98.3 96.5 69.4
(7) LSA (k=20), Dev-Blog 100 91.4 96.8 99.0 80.5
(8) LSA (k=20), Dev-Blog, filtered 99.0 92.1 97.0 97.7 80.5
(9) LSA (k=20), ICWSM, filtered 100 93.0 98.4 99.7 81.9
Hybrid
(10) BNC ratio with backoff (4) 97.1 78.8 75.7 97.0 78.8
(11) Combined ratio with backoff (3 + 4) 97.1 79.2 79.9 97.5 79.9
(12) BNC weighted average (10,6), ratio 2:1 97.1 83.5 90.0 97.0 83.2
(13) Blog weighted average (9,7), ratio 4:1 100 93.8 98.5 99.7 83.4
(14) Voting, 3 agree (1, 6, 7, 9, 11) 92.6 99.1 99.9 87.0 91.6
(15) Voting, 2 agree (1, 11, 13) 86.8 99.1 100 81.5 96.9
(16) Voting, 2 agree (1, 12, 13) 87.7 98.6 100 82.7 97.3
(17) SVM classifier (1, 2, 6, 7, 9, 11) 100 97.9 99.9 100 84.2
(18) Naive Bayes classifier (1, 2, 6, 7, 9, 11) 100 97.5 99.8 100 83.9
(19) SVM (Seed, class) weighted (1, 2, 6, 7, 9, 11) 100 98.4 99.8 100 80.5
(20) SVM (CTRW) weighted (1, 6, 7, 9, 11) 100 93.0 99.0 100 86.0
(21) Average (1, 6, 7, 9, 11) 100 95.9 99.5 100 84.5
Figure 1: Seed and CTRW pairwise accuracy,
LSA method for large corpora k, 10? k ? 200.
the seed terms represent extreme examples of for-
mality; thus there are numerous semantic dimen-
sions to distinguish them. However, the CTRW
set includes near-synonyms, many with only rel-
atively subtle differences in formality; for these
pairs, it is important to focus on the core di-
mensions relevant to formality, which are among
the first discovered in a factor analysis of mixed-
register texts (Biber, 1995).
With regards to hybrid methods, we first briefly
summarize our testing with the iterative model,
which included extensive experiments using ba-
sic lexicons and the LSA vectors derived from
the Brown Corpus, and some targeted testing with
the blog corpora (iteration on these corpora is
extraordinarily time-consuming). In general, we
found only that there were only small, inconsis-
tent benefits to be gained from the iterative ap-
96
proach. More generally, the intuition behind the
iterative method, i.e. that performance would in-
crease with an drastic increase in the number of
seeds, was found to be flawed: in other testing,
we found that we could randomly remove most
of the seeds without negatively affecting perfor-
mance. Even at relatively high k values, it seems
that a few seeds are enough to calibrate the model.
The ratio (with backoff) hybrid built from the
BNC (10) provides CTRW performance that is
comparable the best LSA models, though perfor-
mance in the seed sets is somewhat poor; supple-
menting with word counts from the Brown Cor-
pus and Switchboard Corpus provides a small im-
provement (11). The weighed hybrid dictionar-
ies in (12,13) demonstrate that it is possible to ef-
fectively combine lexicons built using two differ-
ent methods on the same corpus (12) or the same
method on different corpora (13); the former, in
particular, provides an impressive boost to CTRW
accuracy, indicating that word count and word as-
sociation methods are partially independent.
The remainder of Table 1 shows the best re-
sults using voting, averaging, and weighting. The
voting results (14?16) indicate that it is possible
to sacrifice some coverage for very high accu-
racy in both sets, including a near-perfect score
in the seed sets and significant gains in CTRW
performance. In general, the best accuracy with-
out a significant loss of coverage came from 2
of 3 voting (15?16), using dictionaries that rep-
resented our three basic sources of information
(word length, word count, and word associa-
tion). The machine learning hybrids (17?18) also
demonstrate a marked improvement over any sin-
gle lexicon, though it is important to note that
each accuracy score here reflects a different task-
specific model. Hybrid FS lexicons built with the
weights learned by the SVM models (19?20) pro-
vide superior performance on the task correspond-
ing to the model used, though the simple averag-
ing of the best dictionaries (21) also provides good
performance across all evaluation metrics.
Finally, the LSA results for Chinese are mod-
est but promising, given the relatively small scale
of our experiments: we saw a pairwise accuracy of
82.2%, with 79.3% class-based accuracy (k = 10).
We believe that the main reason for the generally
lower performance in Chinese (as compared to
English) is the modest size of the corpus, though
our simplistic character bigram term extraction
technique may also play a role. As mentioned,
smaller seed sets do not seem to be an issue. Inter-
estingly, the class-based accuracy is 10.8% lower
if no reference word is used to calibrate the divide
between formal and informal, suggesting a rather
biased corpus (towards informality); in English,
by comparison, the reference-word normalization
had a slightly negative effect on the LSA results,
though the effect mostly disappeared after hy-
bridization. The obvious next step is to integrate a
Chinese word segmenter, and use a larger corpus.
We could also try word count methods, though
finding appropriate (balanced) resouces similar to
the BNC might be a challenge; (mixed) blog cor-
pora, on the other hand, are easily collected.
6 Conclusion
In this work, we have experimented with a number
of different methods and source corpora for deter-
mining the formality level of lexical items, with
the implicit goal of distinguishing the formality of
near-synonym pairs. Our methods show marked
improvement over simple word-length metrics;
when multiple sources of information, i.e. word
length, word counts, and word association, are in-
tegrated, we are able to reach over 85% perfor-
mance on the near-synonym task, and close to
100% accuracy when comparing words with ex-
treme formality differences; our voting methods
show that even higher precision is possible. We
have also demonstrated that our LSA word associ-
ation method can be applied to a language where
word length is not an appropriate metric of for-
mality, though the results here are preliminary.
Other potential future work includes addressing a
wider range of phenomena, for instance assign-
ing formality scores to morphological elements,
syntactic cues, and multi-word expressions, and
demonstrating that a formality lexicon can be use-
fully applied to other NLP tasks.
Acknowledgements
This work was supported by Natural Sciences and
Engineering Research Council of Canada. Thanks
to Paul Cook for his ICWSM corpus API.
97
References
Biber, Douglas. 1995. Dimensions of Register Vari-
ation: A cross-linguistic comparison. Cambridge
University Press.
Brooke, Julian, Tong Wang, and Graeme Hirst. 2010.
Inducing lexicons of formality from corpora. In
Proceedings of the Language Resources and Eval-
uation Conference (LREC ?10), Workshop on Meth-
ods for the automatic acquisition of Language Re-
sources and their evaluation methods.
Burfoot, Clint and Timothy Baldwin. 2009. Auto-
matic satire detection: Are you having a laugh? In
Proceedings of the Joint Conference of the 47th An-
nual Meeting of the Association for Computationsl
Linguistics and the 4th International Joint Confer-
ence on Nautral Language Processing of the Asian
Federation of Natural Language Processing (ACL-
IJCNLP ?09), Short Papers, Singapore.
Burnard, Lou. 2000. User reference guide for British
National Corpus. Technical report, Oxford Univer-
sity.
Burton, Kevin, Akshay Java, and Ian Soboroff. 2009.
The ICWSM 2009 Spinn3r Dataset. In Proceedings
of the Third Annual Conference on Weblogs and So-
cial Media (ICWSM 2009), San Jose, CA.
Carter, Ronald. 1998. Vocabulary: applied linguistic
perspectives. Routledge, London.
Emigh, William and Susan C. Herring. 2005. Col-
laborative authoring on the web: A genre analysis
of online encyclopedias. In Proceedings of the 38th
Annual Hawaii International Conference on System
Sciences (HICSS ?05).
Esuli, Andrea and Fabrizio Sebastiani. 2006. Senti-
WordNet: A publicly available lexical resource for
opinion mining. In Proceedings of the 5th Interna-
tion Conference on Language Resources and Eval-
uation(LREC), Genova, Italy.
Fellbaum, Christiane, editor. 1998. WordNet: An
Electronic Lexical Database. The MIT Press.
Francis, Nelson and Henry Kuc?era. 1982. Frequency
Analysis of English Usage: Lexicon and Grammar.
Houghton Mifflin, Boston.
Godfrey, J.J., E.C. Holliman, and J. McDaniel. 1992.
Switchboard: telephone speech corpus for research
and development. IEEE International Confer-
ence on Acoustics, Speech, and Signal Processing,
1:517?520.
Hayakawa, S.I., editor. 1994. Choose the Right Word.
HarperCollins Publishers, second edition. Revised
by Eugene Ehrlich.
Heylighen, Francis and Jean-Marc Dewaele. 2002.
Variation in the contextuality of language: An em-
pirical measure. Foundations of Science, 7(3):293?
340.
Hovy, Eduard H. 1990. Pragmatics and natural lan-
guage generation. Artificial Intelligence, 43:153?
197.
Inkpen, Diana and Graeme Hirst. 2006. Building and
using a lexical knowledge base of near-synonym
differences. Computational Linguistics, 32(2):223?
262.
Karlgren, Jussi and Douglas Cutting. 1994. Recog-
nizing text genres with simple metrics using dis-
criminant analysis. In Proceedings of the 15th Con-
ference on Computational Linguistics, pages 1071?
1075.
Kessler, Brett, Geoffrey Nunberg, and Hinrich
Schu?tze. 1997. Automatic detection of text genre.
In Proceedings of the 35th Annual Meeting of the
Association for Computational Linguistics, pages
32?38.
Landauer, Thomas K. and Susan Dumais. 1997. A so-
lution to Plato?s problem: The latent semantic anal-
ysis theory of the acquisition, induction, and rep-
resentation of knowledge. Psychological Review,
104:211?240.
Leckie-Tarry, Helen. 1995. Language Context: a
functional linguistic theory of register. Pinter.
Rao, Delip and Deepak Ravichandra. 2009. Semi-
supervised polarity lexicon induction. In Pro-
ceedings of the 12th Conference of the European
Chapter of the Association for Computational Lin-
gusitics, Athens, Greece.
Stone, Philip J., Dexter C. Dunphy, Marshall S. Smith,
and Daniel M. Ogilivie. 1966. The General In-
quirer: A Computer Approach to Content Analysis.
MIT Press.
Taboada, Maite and Kimberly Voll. 2006. Methods
for creating semantic orientation dictionaries. In
Proceedings of the 5th International Conference on
Language Resources and Evaluation (LREC), Gen-
ova, Italy.
Turney, Peter and Michael Littman. 2003. Measuring
praise and criticism: Inference of semantic orienta-
tion from association. ACM Transactions on Infor-
mation Systems, 21:315?346.
Witten, Ian H. and Eibe Frank. 2005. Data Mining:
Practical Machine Learning Tools and Techniques.
Morgan Kaufmann, San Francisco.
98
Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 1003?1011,
Edinburgh, Scotland, UK, July 27?31, 2011. c?2011 Association for Computational Linguistics
Refining the Notions of Depth and Density in WordNet-based
Semantic Similarity Measures
Tong Wang
Department of Computer Science
University of Toronto
tong@cs.toronto.edu
Graeme Hirst
Department of Computer Science
University of Toronto
gh@cs.toronto.edu
Abstract
We re-investigate the rationale for and the ef-
fectiveness of adopting the notions of depth
and density in WordNet-based semantic sim-
ilarity measures. We show that the intuition
for including these notions in WordNet-based
similarity measures does not always stand up
to empirical examination. In particular, the
traditional definitions of depth and density
as ordinal integer values in the hierarchical
structure of WordNet does not always corre-
late with human judgment of lexical semantic
similarity, which imposes strong limitations
on their contribution to an accurate similarity
measure. We thus propose several novel defi-
nitions of depth and density, which yield sig-
nificant improvement in degree of correlation
with similarity. When used in WordNet-based
semantic similarity measures, the new defini-
tions consistently improve performance on a
task of correlating with human judgment.
1 Introduction
Semantic similarity measures are widely used in
natural language processing for measuring distance
between meanings of words. There are currently
two mainstream approaches to deriving such mea-
sures, i.e., distributional and lexical resource-based
approaches. The former usually explores the co-
occurrence patterns of words in large collections
of texts such as text corpora (Lin, 1998) or the
Web (Turney, 2001). The latter takes advantage of
mostly handcrafted information, such as dictionar-
ies (Chodorow et al, 1985; Kozima and Ito, 1997)
or thesauri (Jarmasz and Szpakowicz, 2003).
Another important resource in the latter stream is
semantic taxonomies such as WordNet (Fellbaum,
1998). Despite their high cost of compilation and
limited availability across languages, semantic tax-
onomies have been widely used in similarity mea-
sures, and one of the main reasons behind this is that
the often complex notion of lexical semantic simi-
larity can be approximated with ease by the distance
between words (represented as nodes) in their hier-
archical structures, and this approximation appeals
much to our intuition. Even methods as simple as
?hop counts? between nodes (e.g., that of Rada et al
1989 on the English WordNet) can take us a long
way. Meanwhile, taxonomy-based methods have
been constantly refined by incorporating various
structural features such as depth (Sussna, 1993; Wu
and Palmer, 1994), density (Sussna, 1993), type of
connection (Hirst and St-Onge, 1998; Sussna, 1993),
word class (sense) frequency estimates (Resnik,
1999), or a combination these features (Jiang and
Conrath, 1997). Most of these algorithms are fairly
self-contained and easy to implement, with off-the-
shelf toolkits such as that of Pedersen et al (2004).
With the existing literature focusing on carefully
weighting these features to construct a better seman-
tic similarity measure, however, the rationale for
adopting these features in calculating semantic sim-
ilarity remains largely intuitive. To the best of our
knowledge, there is no empirical study directly in-
vestigating the effectiveness of adopting structural
features such as depth and density. This serves as
the major motivation for this study.
The paper is organized as follows. In Section
2 we review the basic rationale for adopting depth
1003
and density in WordNet-based similarity measures
as well as existing literature on constructing such
measures. In Section 3, we show the limitations of
the current definitions of depth and density as well as
possible explanations for these limitations.1 We then
propose new definitions to avoid such limitations in
Section 4. The effectiveness of the new definitions
is evaluated by applying them in semantic similar-
ity measures in Section 5 and conclusions made in
Section 6.
2 Related Work
The following are the current definitions of depth
and density which we aim at improving. Given a
node/concept c in WordNet, depth refers to the num-
ber of nodes between c and the root of WordNet,
(i.e., the root has depth zero, its hyponyms depth
one, and so on). There are more variations in the
definition of density, but it is usually defined as the
number of edges leaving c (i.e., its number of child
nodes) or leaving its parent node(s) (i.e., its number
of sibling nodes). We choose to use the latter since
it is used by most of the existing literature.
2.1 The Rationale for Depth and Density
The rationale for using the notions of depth and den-
sity in WordNet-based semantic similarity measures
is based on the following assumption:
Assumption 1 Everything else being equal, two
nodes are semantically closer if (a) they reside
deeper in the WordNet hierarchy, or (b) they are
more densely connected locally.
This is the working assumption for virtually all
WordNet-based semantic similarity studies using
depth and/or density. For depth, the intuition is
that adjacent nodes deep down the hierarchy are
likely to be conceptually close, since the differen-
tiation is based on finer details (Jiang and Conrath,
1997). Sussna (1993) termed the use of depth as
depth-relative scaling, claiming that ?only-siblings
deep in a tree are more closely related than only-
siblings higher in the tree?. Richardson and Smeaton
(1995) gave an hypothetical example illustrating
this ?only-siblings? situation, where plant?animal
1Since the works we review in this section have different
definitions of depth and density, we defer our formal definitions
to Section 3.
are the only two nodes under living things, and
wolfhound?foxhound under hound. They claimed
the reason that the former pair can be regarded as
conceptually farther apart compared to the latter is
related to the difference in depth.
As for the relation between density and similar-
ity, the intuition is that if the overall semantic mass
for a given node is constant (Jiang and Conrath,
1997), then the more neighboring nodes there are in
a locally connected subnetwork, the closer its mem-
bers are to each other. For example, animal, per-
son, and plant are more strongly connected with life
form than aerobe and plankton because the first three
words all have high density in their local network
structures (Richardson and Smeaton, 1995). Note
that the notion of density here is not to be con-
fused with the conceptual density used by Agirre
and Rigau (1996), which is essentially a semantic
similarity measure by itself.
In general, both observations on depth and density
conform to intuition and are supported qualitatively
by several existing studies. The main objective of
this study is to empirically examine the validity of
this assumption.
2.2 Semantic Similarity Measures Using Depth
and/or Density
One of the first examples of using depth and den-
sity in WordNet-based similarity measures is that of
Sussna (1993). The weight on an edge between two
nodes c1 and c2 with relation r in WordNet is given
as:
w(c1,c2) = w(c1?r c2)+w(c2?r c1)2d
where d is the depth of the deeper of the two nodes.
As depth increases, weight decreases and similarity
in turn increases, conforming to Assumption 1. The
edge weight was further defined as
w(c1?r c2) = maxr? maxr?minrnr(c1)
where nr(X) is ?the number of relations of type r
leaving node X?, which is essentially an implicit
form of density, and maxr and minr are the maxi-
mum and minimum of nr, respectively. Note that
this formulation of density contradicts Assumption
1004
1 since it is proportional to edge weight (left-hand-
side) and thus negatively correlated to similarity.
Wu and Palmer (1994) proposed a concept simi-
larity measure between two concepts c1 and c2 as:
sim(c1,c2) = 2 ?dep(c)len(c1,c)+ len(c2,c)+2 ?dep(c)(1)
where c is the lowest common subsumer (LCS) of c1
and c2, and len(?, ?) is the number of edges between
two nodes. The rationale is to adjust ?hop count?
(the first two terms in the denominator) with the
depth of LCS: similarity between nodes with same-
level LCS is in negative proportion to hop counts,
while given the same hop count, a ?deeper? LCS
pulls the similarity score closer to 1.
Jiang and Conrath (1997) proposed a hybrid
method incorporating depth and density information
into an information-content-based model (Resnik,
1999):
w(c, p) =(dep(p)+1dep(p) )
?
? [?+(1??) E?den(p) ]
? [IC(c)? IC(p)]T (c, p) (2)
Here, p and c are parent and child nodes in Word-
Net, dep(?) and den(?) denote the depth and den-
sity of a node, respectively, E? is the average density
over the entire network of WordNet, and ? and ? are
two parameters controlling the contribution of depth
and density values to the similarity score. IC(?) is
the information content of a node based on proba-
bility estimates of word classes from a small sense-
tagged corpus (Resnik, 1999), and T (c, p) is a link-
type factor differentiating different types of relations
between c and p.
3 Limitations on the Current Definitions of
Depth and Density
To what extent do the notions of depth and density
help towards an accurate semantic similarity mea-
sure? Our empirical investigation below suggests
that more often than not, they fail our intuition.
A direct assessment of the effectiveness of us-
ing depth and density is to examine their correla-
tion with similarity. Empirical results in this section
Figure 1: Correlation between depth and similarity.
are achieved by the following experimental setting.
Depth is defined as the number of edges between the
root of the hierarchy and the lowest common sub-
sumer (LCS) of two nodes under comparison, and
density as the number of siblings of the LCS.2 Sim-
ilarity is measured by human judgment on similar-
ity between word pairs. Commonly used data sets
for such judgments include that of Rubenstein and
Goodenough (1965), Miller and Charles (1991), and
Finkelstein et al (2001) (denoted RG, MC, and FG,
respectively). RG is a collection of similarity ratings
of 65 word pairs averaged over judgments from 51
human subjects on a scale of 0 to 4 (from least to
most similar). MC is a subset of 30 pairs out of the
RG data set. These pairs were chosen to have evenly
distributed similarity ratings in the original data set,
and similarity judgment was elicited from 38 human
judges with the same instruction as used for RG. FG
is a much larger set consisting of 353 word pairs,
and the rating scale is from 0 to 10. We combine the
RG and FG data sets in order to maximize data size.
Human ratings r on individual sets are normalized to
rn on 0 to 1 scale by the following formula:
rn = r? rminrmax? rmin
where rmax and rmin are the maximum and minimum
of the original ratings, respectively. Correlation is
evaluated using Spearman?s ?.
2We also tried several other variants of these definitions,
e.g., using the maximum or minimum depth of the two nodes
instead of the LCS. With respect to statistical significance tests,
these variants all gave the same results as our primary definition.
1005
Figure 2: Histogram of depth of WordNet noun synsets.
3.1 Depth
The distribution of similarity of the combined data
set over depth is plotted in Figure 1. For depth val-
ues under 5, similarity scores are fairly evenly dis-
tributed over depth, showing no statistical signifi-
cance in correlation. For depth 5 and above, the
shape of distribution resembles an upper-triangle,
suggesting that (1) correlation with similarity be-
comes stronger in this range of depth value, and (2)
data points with higher depth values tend to have
higher similarity scores, but the reverse of the claim
does not hold, i.e., word pairs with ?shallower? LCS
can also be judged quite similar by humans.
There are many more data points with lower depth
values than with higher depth values in the com-
bined data set. In order to have a fair comparison of
statistical significance tests on the two value ranges
for depth, we randomly sample an equal number
(100) of data points from each value range, and the
correlation coefficient between depth and similarity
is averaged over 100 of such samplings. Correla-
tion coefficients for depth value under 5 versus 5 and
above are ? = 0.0881, p ? 0.1 and ? = 0.3779, p <
0.0001, respectively, showing an apparent difference
in degree of correlation.
Two interesting observations can be made from
these results. Firstly, the notion of depth is relative
to the distribution of number of nodes over depth
value. For example, depth 20 by itself is virtually
meaningless since it might be quite high if the ma-
jority of nodes in WordNet are of depth 10 or less,
or quite low if the majority depth value are 50 or
more. According to the histogram of depth values
in WordNet (Figure 2), the distribution of number of
nodes over depth value approximately conforms to a
normal distribution N (8,2). It is visually quite no-
ticeable that the actual quantity denoting how deep a
node resides in WordNet is conflated at depth values
below 5 or above 14. In other words, the distribution
makes it rather inaccurate to say, for instance, that a
node of depth 4 is twice as deep as a node of depth 2.
This might explain the low degree of correlation be-
tween similarity and depth under 5 in Figure 1 (man-
ifested by the long, vertical stripes across the entire
range of similarity scores (0 to 1) for depth 4 and
under), and also how the correlation increases with
depth value. Unfortunately, we do not have enough
data for depth above 14 to draw any conclusion on
this higher end of the depth spectrum.
Secondly, even on the range of depth values with
higher correlation with similarity, there is no defini-
tive sufficient and necessary relation between depth
and similarity (hence the upper triangle instead of
a sloped line or band). Particularly, semantically
more similar words are not necessarily deeper in the
WordNet hierarchy. Data analysis reveals that the
LCS of highly similar words can be quite close to
the hierarchical root. Examples include coast?shore,
which is judged to be very similar by humans (9 on
a scale of 0?10 in both data sets). The latter is a hy-
pernym of the former and thus the LCS of the pair,
yet it is only four levels below the root node entity
(via geological formation, object, and physical en-
tity). Another situation is when the human judges
confused relatedness with similarity, and WordNet
fails to capture the relatedness with its hierarchical
structure of lexical semantics: the pair software?
computer can only be related by the root node en-
tity as their LCS, although the pair is judged quite
?similar? by humans (8.5 on 0 to 10 scale).
The only conclusive claim that can be made here
is that word pairs with deeper LCS?s tend to be more
similar. However, since only word forms (rather
than senses) are available in these psycho-linguistic
experiments, the one similarity rating given by hu-
man judges sometimes fails to cover multiple senses
for polysemous words. In the pair stock?jaguar of
the FG set, for example, one sense of stock (live-
stock, stock, farm animal: any animals kept for use
or profit) is closely connected to jaguar through a
depth-10 LCS (placental, placental mammal, eu-
therian, eutherian mammal). However, the pair re-
ceived a low similarity rating (0.92 on 0?10), prob-
1006
Figure 3: Correlation between density and similarity.
MC RG FG
dep 0.7056*** 0.6909*** 0.3701***
den 0.2268 0.2660* 0.1023
Table 1: Correlation between depth/density and similar-
ity on individual data sets. Number of asterisks indicates
different confidence intervals (?*? for p < 0.05, ?***? for
p < 0.0001).
ably because judges associated the word form stock
with its financial sense, especially when there was
an abundant presence of pairs indicating this particu-
lar sense of the word (e.g., stock?market, company?
stock).
3.2 Density
Comparing to depth, density exhibits much lower
correlation with similarity (Figure 3-a and 3-b). We
conducted correlation experiments between density
and similarity with the same setting as for depth and
similarity above. Data points with extremely high
density values (up to over 400) are mostly idiosyn-
cratic to the densely connected regions in WordNet
and are numerically quite harmful. We thus ex-
cluded outliers with density values above 100 in the
experiment.
Evaluation on the combined data set shows no
correlation between density and similarity. To con-
firm the result, we break the experiments down to the
three individual data sets, and the results are listed in
Table 1. The correlation coefficient between density
and similarity ranges from 0.10 to 0.27 There is no
statistical significance of correlation on two of the
three data sets (MC and FG), and the significance
on RG is close to marginal with p = 0.0366.
Data analysis suggests that density values are of-
ten biased by particular fine-grainedness of local
structures in WordNet. Qualitatively, Richardson
and Smeaton (1995) previously observed that ?the
irregular densities of links between concepts results
in unexpected conceptual distance measures?. Em-
pirically, on the one hand, more than 90% of Word-
Net nodes have density values less than or equal to
3. This means that for 90% of the LCS?s, there are
only three integer values for density to distinguish
the varying degrees of similarity. In other words,
such a range might be too narrow to have any real
distinguishing power over similarity. On the other
hand, there are outliers with extreme density values
particular to the perhaps overly fine-grained subcat-
egorization of some WordNet concepts, and these
nodes can be LCS?s of word pairs of drastically dif-
ferent similarity. The node person, individual, for
example, can be the LCS of similar pairs such as
man?woman, as well as quite dissimilar ones such
as boy?sage, where the large density value does not
necessarily indicate high degree of similarity.
Another crucial limitation of the definition of den-
sity is the information loss on specificity. In the ex-
isting literature, density is often adopted as a proxy
for the degree of specificity of a concept, i.e., nodes
in densely connected regions in WordNet are taken
to be more specific and thus closer to each other.
This information of a given node should be inher-
ited by its hierarchical descendants, since specificity
should monotonically increase as one descends the
hierarchy. For example, the node piano has a den-
sity value of 15 under the node percussion instru-
ment. However, the density value of its hyponyms
Grand piano, upright piano, and mechanical piano,
is only 3. Due to the particular structure of this sub-
network in WordNet, the grand?upright pair might
be incorrectly regarded as less specific (and thus less
similar) than, say, between piano?gong, both as per-
cussion instruments.
4 New Definitions of Depth and Density
In this section, we formalize new definitions of
depth and density to correct for their current limi-
1007
MC RG FG
depu 0.7201*** 0.6798*** 0.3751***
denu 0.2268 0.2660* 0.1019
deni 0.7338*** 0.6751*** 0.3445***
Table 2: Correlation between new definitions of
depth/density and similarity.
tations discussed in Section 3.
4.1 Depth
The major problem with the current definition of
depth is its failure to take into account the uneven
distribution of number of nodes over the depth value.
As seen in previous examples, the distribution is
rather ?flat? on both ends of depth value, which does
not preserve the linearity of using the ordinal values
of depth and thus introduces much inaccuracy.
To avoid this problem, we ?re-curve? depth value
to the cumulative distribution. Specifically, if we
take the histogram distribution of depth value in Fig-
ure 2 as a probability density function, our approach
is to project cardinal depth values onto its cumula-
tive distribution function. The new depth is denoted
depu and is defined as:
depu(c) = ?c??WN |{c
? : dep(c?)? dep(c)}|
|WN|
Here, dep(?) is the original depth value, and WN is
the set of all nodes in WordNet. The resulting depth
values not only reflect the flat ends, but also preserve
linearity for the depth value range in the middle. In
comparison with Table 1), correlation between depu
and similarity increases over the original depth val-
ues on two of the three data sets (first row in Table
2 and decreases on the RG set. Later, in Section 5,
we show how these marginal improvements translate
into better similarity measures with statistical signif-
icance.
4.2 Density
In theory, a procedure analogous to the above cumu-
lative definition can also be applied to density, i.e.,
by projecting the original values onto the cumula-
tive distribution function. However, due to the Zip-
fian nature of density?s histogram distribution (Fig-
ure 4, in contrast to Gaussian for depth in Figure
2), this is essentially to collapse most density values
Figure 4: Histogram of density in WordNet.
into a very small number of discrete values (which
correspond to the original density of 1 to 3). Ex-
periments show that it does not help in improving
correlation with similarity scores (second row in Ta-
ble 2 for denu): correlation remains the same on MC
and RG, and decreases slightly on FG.
We therefore resort to addressing the issue of in-
formation loss on specificity by inheritance. Intu-
itively, the idea is to ensure that a node be assigned
no less density mass than its parent node(s). In the
?piano? example (Section 3.2), the concept piano is
highly specific due to its large number of siblings
under the parent node percussion instruments. Con-
sequently, the density of its child nodes upright pi-
ano and grand piano should inherit its specificity on
top of their own.
Formally, we redefine density recursively as fol-
lows:
deni(r) = 0
deni(c) = ?h?hyper(c) deni(h)|hyper(c)| +den(c)
where r is the root of WordNet hierarchy (with no
hypernym), and hyper(?) is the set of hypernyms of a
given concept. The first term is the inheritance part,
normalized over all hypernyms of c in case of mul-
tiple inheritance, and the second term is the original
value of density.
The resulting density values correlate signifi-
cantly better with similarity. As shown in row 3
in Table 2, the correlation coefficients are about
tripled on all three data sets with the new density
definition deni, and the significance of correlation
is greatly improved as well (from non-correlating or
1008
marginally correlating to strongly significantly cor-
relating on all three data sets).
5 Using the New Definitions in Semantic
Similarity Measures
In this section, we test the effectiveness of the new
definitions of depth and density by using them in
WordNet-based semantic similarity measures. The
two similarity measures we experiment with are that
of Wu and Palmer (1994) and Jiang and Conrath
(1997). The first one used depth only, and the second
one used both depth and density.
The task is to correlate the similarity measures
with human judgment on similarity between word
pairs. We use the same three data sets as in Section
3. despite the fact that MC is a subset of RG data
set, we include both in order to compare with exist-
ing studies.
Correlation coefficient is calculated using Spear-
man?s ?, although results reported by some earlier
studies used parametric tests such as the Pearson
Correlation Coefficient. The reason for our choice
is that the similarity scores of the word pairs in
these data sets do not necessarily conform to nor-
mal distributions. Rather, we are interested in testing
whether the artificial algorithms would give higher
scores to pairs that are regarded closer in meaning
by human judges. A non-parametric test suits better
for this scenario. And this partly explains why our
re-implementations of the models have lower corre-
lation coefficients than in the original studies.
Note that there are other WordNet-based similar-
ity measures using depth and/or density that we opt
to omit for various reasons. Some of them were not
designed for the particular task at hand (e.g., that of
Sussna, 1993, which gives very poor correlation in
this task). Others use depth of the entire WordNet
hierarchy instead of individual nodes as a scaling
factor (e.g., that of Leacock and Chodorow, 1998),
which is unsuitable for illustrating the improvement
brought about by the new depth and density defini-
tions.
Parameterization of the weighting of depth and
density is a common practice to control their indi-
vidual contribution to the final similarity score (e.g.,
? and ? in Equation (2)). Jiang and Conrath already
had separate weights in their original study. In or-
Best Average
MC RB GR MC RB GR
dep 0.7671 0.7824 0.3773 0.7612 0.7686 0.3660
depu 0.7824 0.7912 0.3946 0.7798 0.7810 0.3787
Table 3: Correlation between human judgment and simi-
larity score by Wu and Palmer (1994) using two versions
of depth.
Best AverageMC RB GR MC RB GR
dep,den 0.7875 0.8111 0.3720 0.7689 0.7990 0.3583depu, den 0.8009 0.8181 0.3804 0.7885 0.8032 0.3669dep,deni 0.7882 0.8199 0.3803 0.7863 0.8102 0.3689depu,deni 0.8065 0.8202 0.3818 0.8189 0.8194 0.3715
Table 4: Correlation between human judgment and sim-
ilarity score by Jiang and Conrath (1997) using different
definitions of depth and density.
der to parameterize depth used by Wu and Palmer in
their similarity measure, we also modify Equation
(1) as follows:
sim(c1,c2) = 2 ?dep
?(c)
len(c1,c)+ len(c2,c)+2 ?dep?(c)
where depth is raised to the power of ? to vary its
contribution to the similarity score.
For a number of combinations of the weighting
parameters, we report both the best performance
and the averaged performance over all the param-
eter combinations. The latter number is meaningful
in that it is a good indication of numerical stability of
the parameterization. In addition, parameterization
is able to generate multiple correlation coefficients,
on which statistical tests can be run in order to show
the significance of improvement. We use the range
from 0 to 5 with step 1 for ? and from 0 to 1 with
step 0.1 for ?.
Table 3 and 4 list the experiment results. In both
models, the cumulative definition of depth depu con-
sistently improve the performance of the similarity
measures. In the Jiang and Conrath (1997) model,
where density is applicable, the inheritance-based
definition of density deni also results in better cor-
relation with human judgments. The optimal result
is achieved when combining the new definitions of
depth and density (row 4 in Table 4). For average
performance, the improvement of all the new def-
initions over the original definitions is statistically
1009
significant on all three data sets according to paired
t-test.
6 Conclusions
This study explored effective uses of depth and/or
density in WordNet-based similarity measures. We
started by examining how well these two structural
features correlate with human judgment on word
pair similarities. This direct comparison showed that
depth correlates with similarity only on certain value
ranges, while density does not correlate with human
judgment at all.
Further investigation revealed that the problem for
depth lies in the simplistic representation as its ordi-
nal integer values. The linearity in this representa-
tion fails to take into account the conflated quantity
of depth in the two extreme ends of the depth spec-
trum. For density, a prominent issue is the informa-
tion loss on specificity of WordNet concepts, which
gives an inaccurate density value that is biased by
the idiosyncratic constructions in densely connected
regions in the hierarchy.
We then proposed new definitions of depth and
density to address these issues. For depth, linear-
ity in different value ranges is realistically reflected
by projecting the depth value to its cumulative dis-
tribution function. The loss of specificity informa-
tion in density, on the other hand, is corrected by
allowing concepts to inherit specificity information
from their parent nodes. The new definitions show
significant improvement in correlation of semantic
similarity given by human judges. In addition, when
used in existing WordNet-based similarity measures,
they consistently improve performance and numeri-
cal stability of the parameterization of the two fea-
tures.
The notions of depth and density pertain to any
hierarchical structure like WordNet, which suggests
various extensions of this work. A natural next step
of the current work is to apply the same idea to se-
mantic taxonomies in languages other than English
with available similarity judgments are also avail-
able. Extrinsic tasks using WordNet-based semantic
similarity can potentially benefit from these refined
notions of depth and density as well.
Acknowledgments
This study was inspired by lectures given by Profes-
sor Gerald Penn of the University of Toronto, and
was financially supported by the Natural Sciences
and Engineering Research Council of Canada.
References
Eneko Agirre and German Rigau. Word sense dis-
ambiguation using conceptual density. In Pro-
ceedings of the 16th Conference on Computa-
tional Linguistics, pages 16?22. Association for
Computational Linguistics, 1996.
Martin Chodorow, Roy Byrd, and George Heidorn.
Extracting semantic hierarchies from a large on-
line dictionary. In Proceedings of the 23rd An-
nual Meeting of the Association for Computa-
tional Linguistics, pages 299?304, Chicago, Illi-
nois, USA, 1985.
Christiane Fellbaum. WordNet: An Electronic Lexi-
cal Database. MIT Press, Cambridge, MA, 1998.
Lev Finkelstein, Evgeniy Gabrilovich, Yossi Matias,
Ehud Rivlin, Zach Solan, Gadi Wolfman, and Ey-
tan Ruppin. Placing search in context: The con-
cept revisited. In Proceedings of the 10th Inter-
national Conference on World Wide Web, pages
406?414. ACM, 2001.
Graeme Hirst and David St-Onge. Lexical chains
as representations of context for the detection and
correction of malapropisms. In Christiane Fell-
baum, editor, WordNet: An Electronic Lexical
Database, pages 305?332. 1998.
Mario Jarmasz and Stan Szpakowicz. Roget?s the-
saurus and semantic similarity. In Proceedings
of International Conference on Recent Advances
in Natural Language Processing, pages 212?219,
Borovets, Bulgaria, 2003.
Jay Jiang and David Conrath. Semantic similarity
based on corpus statistics and lexical taxonomy.
Proceedings of International Conference on Re-
search in Computational Linguistics, 33, 1997.
Hideki Kozima and Akira Ito. Context-sensitive
measurement of word distance by adaptive scal-
ing of a semantic space. Recent Advances in Nat-
ural Language Processing: Selected Papers from
RANLP, 95:111?124, 1997.
1010
Claudia Leacock and Martin Chodorow. Combin-
ing local context and WordNet similarity for word
sense identification. WordNet: An electronic lexi-
cal database, 49(2):265?283, 1998.
Dekang Lin. Automatic retrieval and clustering of
similar words. In Proceedings of the 17th Interna-
tional Conference on Computational Linguistics,
pages 768?774, Montreal, Canada, 1998.
Goerge Miller and Walter Charles. Contextual cor-
relates of semantic similarity. Language and Cog-
nitive Processes, 6(1):1?28, 1991.
Ted Pedersen, Siddharth Patwardhan, and Jason
Michelizzi. WordNet::Similarity: measuring the
relatedness of concepts. In Demonstration Papers
at Human Language Technologies - North Ameri-
can Chapter of the Association for Computational
Linguistics, pages 38?41. Association for Com-
putational Linguistics, 2004.
Roy Rada, Hafedh Mili, Ellen Bicknell, and Maria
Blettner. Development and application of a metric
on semantic nets. IEEE Transactions on Systems,
Man and Cybernetics, 19(1):17?30, 1989.
Philip Resnik. Semantic similarity in a taxon-
omy: an information-based measure and its ap-
plication to problems of ambiguity in natural lan-
guage. Journal of Artificial Intelligence Research,
11(11):95?130, 1999.
R. Richardson and A.F. Smeaton. Using WordNet
in a knowledge-based approach to information re-
trieval. In Proceedings of the BCS-IRSG Collo-
quium, Crewe. Citeseer, 1995.
Herbert Rubenstein and John Goodenough. Contex-
tual correlates of synonymy. Communications of
the ACM, 8(10):627?633, 1965.
Michael Sussna. Word sense disambiguation for
free-text indexing using a massive semantic net-
work. In Proceedings of the second international
conference on Information and knowledge man-
agement, pages 67?74. ACM, 1993.
Peter Turney. Mining the web for synonyms: PMI-
IR versus LSA on TOEFL. Proceedings of the
Twelfth European Conference on Machine Learn-
ing, pages 491?502, 2001.
Zhibiao Wu and Martha Palmer. Verb semantics
and lexical selection. In Proceedings of the 32nd
Annual Meeting of the Association for Compu-
tational Linguistics, pages 133?138. Association
for Computational Linguistics, 1994.
1011
Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 531?537,
Baltimore, Maryland, USA, June 23-25 2014.
c?2014 Association for Computational Linguistics
Applying a Naive Bayes Similarity Measure to
Word Sense Disambiguation
Tong Wang
University of Toronto
tong@cs.toronto.edu
Graeme Hirst
University of Toronto
gh@cs.toronto.edu
Abstract
We replace the overlap mechanism of the
Lesk algorithm with a simple, general-
purpose Naive Bayes model that mea-
sures many-to-many association between
two sets of random variables. Even with
simple probability estimates such as max-
imum likelihood, the model gains signifi-
cant improvement over the Lesk algorithm
on word sense disambiguation tasks. With
additional lexical knowledge from Word-
Net, performance is further improved to
surpass the state-of-the-art results.
1 Introduction
To disambiguate a homonymous word in a given
context, Lesk (1986) proposed a method that mea-
sured the degree of overlap between the glosses
of the target and context words. Known as the
Lesk algorithm, this simple and intuitive method
has since been extensively cited and extended in
the word sense disambiguation (WSD) commu-
nity. Nonetheless, its performance in several WSD
benchmarks is less than satisfactory (Kilgarriff
and Rosenzweig, 2000; Vasilescu et al, 2004).
Among the popular explanations is a key limita-
tion of the algorithm, that ?Lesk?s approach is very
sensitive to the exact wording of definitions, so the
absence of a certain word can radically change the
results.? (Navigli, 2009).
Compounding this problem is the fact that many
Lesk variants limited the concept of overlap to
the literal interpretation of string matching (with
their own variants such as length-sensitive match-
ing (Banerjee and Pedersen, 2002), etc.), and it
was not until recently that overlap started to take
on other forms such as tree-matching (Chen et al,
2009) and vector space models (Abdalgader and
Skabar, 2012; Raviv et al, 2012; Patwardhan and
Pedersen, 2006). To address this limitation, a
Naive Bayes model (NBM) is proposed in this
study as a novel, probabilistic treatment of over-
lap in gloss-based WSD.
2 Related Work
In the extraordinarily rich literature on WSD, we
focus our review on those closest to the topic of
Lesk and NBM. In particular, we opt for the ?sim-
plified Lesk? (Kilgarriff and Rosenzweig, 2000),
where inventory senses are assessed by gloss-
context overlap rather than gloss-gloss overlap.
This particular variant prevents proliferation of
gloss comparison on larger contexts (Mihalcea
et al, 2004) and is shown to outperform the origi-
nal Lesk algorithm (Vasilescu et al, 2004).
To the best of our knowledge, NBMs have been
employed exclusively as classifiers in WSD ?
that is, in contrast to their use as a similarity mea-
sure in this study. Gale et al (1992) used NB
classifier resembling an information retrieval sys-
tem: a WSD instance is regarded as a document d,
and candidate senses are scored in terms of ?rel-
evance? to d. When evaluated on a WSD bench-
mark (Vasilescu et al, 2004), the algorithm com-
pared favourably to Lesk variants (as expected
for a supervised method). Pedersen (2000) pro-
posed an ensemble model with multiple NB clas-
sifiers differing by context window size. Hristea
(2009) trained an unsupervised NB classifier using
the EM algorithm and empirically demonstrated
the benefits of WordNet-assisted (Fellbaum, 1998)
feature selection over local syntactic features.
Among Lesk variants, Banerjee and Pedersen
(2002) extended the gloss of both inventory senses
and the context words to include words in their re-
lated synsets in WordNet. Senses were scored by
the sum of overlaps across all relation pairs, and
the effect of individual relation pairs was evalu-
ated in a later work (Banerjee and Pedersen, 2003).
Overlap was assessed by string matching, with the
number of matching words squared so as to assign
531
higher scores to multi-word overlaps.
Breaking away from string matching, Wilks
et al (1990) measured overlap as similarity be-
tween gloss- and context-vectors, which were ag-
gregated word vectors encoding second order co-
occurrence information in glosses. An extension
by Patwardhan and Pedersen (2006) differentiated
context word senses and extended shorter glosses
with related glosses in WordNet. Patwardhan et al
(2003) measured overlap by concept similarity
(Budanitsky and Hirst, 2006) between each inven-
tory sense and the context words. Gloss overlaps
from their earlier work actually out-performed all
five similarity-based methods.
More recently, Chen et al (2009) pro-
posed a tree-matching algorithm that measured
gloss-context overlap as the weighted sum of
dependency-induced lexical distance. Abdalgader
and Skabar (2012) constructed a sentential simi-
larity measure (Li et al, 2006) using lexical simi-
larity measures (Budanitsky and Hirst, 2006), and
overlap was measured by the cosine of their re-
spective sentential vectors. A related approach
(Raviv et al, 2012) also used Wikipedia-induced
concepts to encoded sentential vectors. These sys-
tems compared favourably to existing methods in
WSD performance, although by using sense fre-
quency information, they are essentially super-
vised methods.
Distributional methods have been used in many
WSD systems in quite different flavours than the
current study. Kilgarriff and Rosenzweig (2000)
proposed a Lesk variant where each gloss word is
weighted by its idf score in relation to all glosses,
and gloss-context association was incremented by
these weights rather than binary, overlap counts.
Miller et al (2012) used distributional thesauri as a
knowledge base to increase overlaps, which were,
again, assessed by string matching.
In conclusion, the majority of Lesk variants
focused on extending the gloss to increase the
chance of overlapping, while the proposed NBM
aims to make better use of the limited lexical
knowledge available. In contrast to string match-
ing, the probabilistic nature of our model offers
a ?softer? measurement of gloss-context associa-
tion, resulting in a novel approach to unsupervised
WSD with state-of-the-art performance in more
than one WSD benchmark (Section 4).
3 Model and Task Descriptions
3.1 The Naive Bayes Model
Formally, given two sets e = {e
i
} and f = { f
j
}
each consisting of multiple random events, the
proposed model measures the probabilistic asso-
ciation p(f|e) between e and f. Under the assump-
tion of conditional independence among the events
in each set, a Naive Bayes treatment of the mea-
sure can be formulated as:
p(f|e) =
?
j
p( f
j
|{e
i
}) =
?
j
p({e
i
}| f
j
)p( f
j
)
p({e
i
})
=
?
j
[p( f
j
)
?
i
p(e
i
| f
j
)]
?
j
?
i
p(e
i
)
,
(1)
In the second expression, Bayes?s rule is applied
not only to take advantage of the conditional inde-
pendence among e
i
?s, but also to facilitate proba-
bility estimation, since p({e
i
}| f
j
) is easier to esti-
mate in the context of WSD, where sample spaces
of e and f become asymmetric (Section 3.2).
3.2 Model Application in WSD
In the context of WSD, e can be regarded as an
instance of a polysemous word w, while f repre-
sents certain lexical knowledge about the sense s
of w manifested by e.
1
WSD is thus formulated as
identifying the sense s
?
in the sense inventory S
of w s.t.:
s
?
= argmax
s?S
p(f|e) (2)
In one of their simplest forms, e
i
?s correspond
to co-occurring words in the instance of w, and
f
j
?s consist of the gloss words of sense s. Conse-
quently, p(f|e) is essentially measuring the asso-
ciation between context words of w and definition
texts of s, i.e., the gloss-context association in the
simplified Lesk algorithm (Kilgarriff and Rosen-
zweig, 2000). A major difference, however, is that
instead of using hard, overlap counts between the
two sets of words from the gloss and the context,
this probabilistic treatment can implicitly model
the distributional similarity among the elements e
i
and f
j
(and consequently between the sets e and
f) over a wider range of contexts. The result is a
?softer? proxy of association than the binary view
of overlaps in existing Lesk variants.
The foregoing discussion offers a second mo-
tivation for applying Bayes?s rule on the second
1
Think of the notations e and f mnemonically as exem-
plars and features, respectively.
532
Senses Hypernyms Hyponyms Synonyms
factory building
complex,
complex
brewery,
factory,
mill, ...
works,
industrial
plant
life form organism,
being
perennial,
crop...
flora,
plant life
Table 1: Lexical knowledge for the word plant un-
der its two meanings factory and life form.
expression in Equation (1): it is easier to estimate
p(e
i
| f
j
) than p( f
j
|e
i
), since the vocabulary for the
lexical knowledge features ( f
j
) is usually more
limited than that of the contexts (e
i
) and hence esti-
mation of the former suffices on a smaller amount
of data than that of the latter.
3.3 Incorporating Additional Lexical
Knowledge
The input of the proposed NBM is bags of words,
and thus it is straightforward to incorporate var-
ious forms of lexical knowledge (LK) for word
senses: by concatenating a tokenized knowledge
source to the existing knowledge representation f,
while the similarity measure remains unchanged.
The availability of LK largely depends on the
sense inventory used in a WSD task. WordNet
senses are often used in Senseval and SemEval
tasks, and hence senses (or synsets, and possibly
their corresponding word forms) that are seman-
tic related to the inventory senses under WordNet
relations are easily obtainable and have been ex-
ploited by many existing studies.
As pointed out by Patwardhan et al (2003),
however, ?not all of these relations are equally
helpful.? Relation pairs involving hyponyms were
shown to result in better F-measure when used
in gloss overlaps (Banerjee and Pedersen, 2003).
The authors attributed the phenomenon to the the
multitude of hyponyms compared to other rela-
tions. We further hypothesize that, beyond sheer
numbers, synonyms and hyponyms offer stronger
semantic specification that helps distinguish the
senses of a given ambiguous word, and thus are
more effective knowledge sources for WSD.
Take the word plant for example. Selected hy-
pernyms, hyponyms, and synonyms pertaining to
its two senses factory and life form are listed in
Table 1. Hypernyms can be overly general terms
(e.g., being). Although conceptually helpful for
humans in coarse-grained WSD, this generality is
likely to inflate the hypernyms? probabilistic esti-
mation. Hyponyms, on the other hand, help spec-
ify their corresponding senses with information
that is possibly missing from the often overly brief
glosses: the many technical terms as hyponyms
in Table 1 ? though rare ? are likely to occur
in the (possibly domain-specific) contexts that are
highly typical of the corresponding senses. Par-
ticularly for the NBM, the co-occurrence is likely
to result in stronger gloss-definition associations
when similar contexts appear in a WSD instance.
We also observe that some semantically related
words appear under rare senses (e.g., still as an
alcohol-manufacturing plant, and annual as a one-
year-life-cycle plant; omitted from Table 1). This
is a general phenomenon in gloss-based WSD and
is beyond the scope of the current discussion.
2
Overall, all three sources of LK may complement
each other in WSD tasks, with hyponyms particu-
larly promising in both quantity and quality com-
pared to hypernyms and synonyms.
3
3.4 Probability Estimation
A most open-ended question is how to estimate the
probabilities in Equation (1). In WSD in particu-
lar, the estimation concerns the marginal and con-
ditional probabilities of and between word tokens.
Many options are available to this end in statis-
tical machine learning (MLE, MAP, etc.), infor-
mation theory (Church and Hanks, 1990; Turney,
2001), as well as the rich body of research in lex-
ical semantic similarity Resnik, 1995; Jiang and
Conrath, 1997; Budanitsky and Hirst, 2006).
Here we choose maximum likelihood ? not
only for its simplicity, but also to demonstrate
model strength with a relatively crude probability
estimation. To avoid underflow, Equation (1) is
estimated as the following log probability:
?
i
log
c( f
j
)
c(?)
+
?
i
?
j
log
c(e
i
, f
j
)
c( f
j
)
?|f|
?
j
log
c(e
i
)
c(?)
=(1?|e|)
?
i
logc( f
j
)?|f|
?
j
logc(e
i
)
+
?
i
?
j
logc(e
i
, f
j
)+ |f|(|e|?1) logc(?),
where c(x) is the count of word x, c(?) is the corpus
2
We do, however, refer curious readers to the work of Ra-
viv et al (2012) for a novel treatment of a similar problem.
3
Note that LK expansion is a feature of our model rather
than a requirement. What type of knowledge to include is
eventually a decision made by the user based on the applica-
tion and LK availability.
533
size, c(x,y) is the joint count of x and y, and |v| is
the dimension of vector v.
Nonetheless, we do investigate how model per-
formance responds to estimation quality. Specif-
ically in WSD, a source corpus is defined as the
source of the majority of the WSD instances in a
given dataset, and a baseline corpus of a smaller
size and less resemblance to the instances is used
for all datasets. The assumption is that a source
corpus offers better estimates for the model than
the baseline corpus, and difference in model per-
formance is expected when using probability esti-
mation of different quality.
4 Evaluation
4.1 Data, Scoring, and Pre-processing
Various aspects of the model discussed in Section
3 are evaluated in the English lexical sample tasks
from Senseval-2 (Edmonds and Cotton, 2001) and
SemEval-2007 (Pradhan et al, 2007). Training
sections are used as development data and test
sections held out for final testing. Model perfor-
mance is evaluated in terms of WSD accuracy us-
ing Equation (2) as the scoring function. Accu-
racy is defined as the number of correct responses
over the number of instances. Because it is a rare
event for the NBM to produce identical scores,
4
the model always proposes a unique answer and
accuracy is thus equivalent to F-score commonly
used in existing reports.
Multiword expressions (MWEs) in the
Senseval-2 sense inventory are not explicitly
marked in the contexts. Several of the top-ranking
systems implemented their own MWE detection
algorithms (Kilgarriff and Rosenzweig, 2000;
Litkowski, 2002). Without digressing to the
details of MWE detection ? and meanwhile,
to ensure fair comparison with existing systems
? we implement two variants of the prediction
module, one completely ignorant of MWE and
defaulting to INCORRECT for all MWE-related
answers, while the other assuming perfect MWE
detection and performing regular disambiguation
algorithm on the MWE-related senses (not de-
faulting to CORRECT). All results reported for
Senseval-2 below are harmonic means of the two
outcomes.
Each inventory sense is represented by a set of
LK tokens (e.g., definition texts, synonyms, etc.)
4
This has never occurred in the hundreds of thousands of
runs in our development process.
from their corresponding WordNet synset (or in
the coarse-grained case, a concatenation of tokens
from all synsets in a sense group). The MIT-JWI
library (Finlayson, 2014) is used for accessing
WordNet. Usage examples in glosses (included by
the library by default) are removed in our experi-
ments.
5
Basic pre-processing is performed on the con-
texts and the glosses, including lower-casing, stop-
word removal, lemmatization on both datasets,
and tokenization on the Senseval-2 instances.
6
Stanford CoreNLP
7
is used for lemmatization and
tokenization. Identical procedures are applied to
all corpora used for probability estimation.
Binomial test is used for significance testing,
and with one exception explicitly noted in Sec-
tion 4.3, all differences presented are statistically
highly significant (p < 0.001).
4.2 Comparing Lexical Knowledge Sources
To study the effect of different types of LK in
WSD (Section 3.3), for each inventory sense, we
choose synonyms (syn), hypernyms (hpr), and hy-
ponyms (hpo) as extended LK in addition to its
gloss. The WSD model is evaluated with gloss-
only (glo), individual extended LK sources, and
the combination of all four sources (all). The re-
sults are listed in Table 2 together with existing re-
sults (1st and 2nd correspond to the results of the
top two unsupervised methods in each dataset).
8
By using only glosses, the proposed model
already shows statistically significant improve-
ment over the basic Lesk algorithm (92.4%
and 140.5% relative improvement in Senseval-
2 coarse- and fine-grained tracks, respectively).
9
Moreover, comparison between coarse- and fine-
grained tracks reveals interesting properties of dif-
ferent LK sources. Previous hypotheses (Section
3.3) are empirically confirmed that WSD perfor-
5
We also compared the two Lesk baselines (with and
without usage examples) on the development data but did not
observe significant differences as reported by Kilgarriff and
Rosenzweig (2000).
6
The SemEval-2007 instances are already tokenized.
7
http://nlp.stanford.edu/software/
corenlp.shtml.
8
We excluded the results of UNED (Fern?andez-Amor?os
et al, 2001) in Senseval-2 because, by using sense frequency
information that is only obtainable from sense-annotated cor-
pora, it is essentially a supervised system.
9
Comparisons are made against the simplified Lesk al-
gorithm (Kilgarriff and Rosenzweig, 2000) without usage
examples. The comparison is unavailable in SemEval2007
since we have not found existing experiments with this exact
configuration.
534
Dataset glo syn hpr hpo all 1st 2nd Lesk
Senseval-2 Coarse .475 .478 .494 .518 .523 .469 .367 .262
Senseval-2 Fine .362 .371 .326 .379 .388 .412 .293 .163
SemEval-2007 .494 .511 .507 .550 .573 .538 .521 ?
Table 2: Lexical knowledge sources and WSD performance (F-measure) on the Senseval-2 (fine- and
coarse-grained) and the SemEval-2007 dataset.
Figure 1: Model response to probability esti-
mates of different quality on the SemEval-2007
dataset. Error bars indicate confidence intervals
(p < .001), and the dashed line corresponds to the
best reported result.
mance benefits most from hyponyms and least
from hypernyms. Specifically, highly similar, fine-
grained sense candidates apparently share more
hypernyms in the fine-grained case than in the
coarse-grained case; adding to the generality of
hypernyms (both semantic and distributional), we
postulate that their probability in the NBM is uni-
formly inflated among many sense candidates, and
hence they decrease in distinguishability. Syn-
onyms might help with regard to semantic spec-
ification, though their limited quantity also limits
their benefits. These patterns on the LK types are
consistent in all three experiments.
When including all four LK sources, our model
outperforms the state-of-the-art systems with sta-
tistical significance in both coarse-grained tasks.
For the fine-grained track, it achieves 2nd place
after that of Tugwell and Kilgarriff (2001), which
used a decision list (Yarowsky, 1995) on manu-
ally selected corpora evidence for each inventory
sense, and thus is not subject to loss of distin-
guishability in the glosses as Lesk variants are.
4.3 Probability Estimation
To evaluate model response to probability esti-
mation of different quality (Section 3.4), source
corpora are chosen as the majority value of the
doc-source attribute of instances in each dataset,
namely, the British National Corpus for Senseval-
2 (94%) and the Wall Street Journal for SemEval-
2007 (86%). The Brown Corpus is shared by both
datasets as the baseline corpus. Figure 1 shows the
comparison on the SemEval-2007 dataset. Across
all experiments, higher WSD accuracy is consis-
tently witnessed using the source corpus; differ-
ences are statistically highly significant except for
hpo (which is significant with p < 0.01).
5 Conclusions and Future Work
We have proposed a general-purpose Naive Bayes
model for measuring association between two sets
of random events. The model replaced string
matching in the Lesk algorithm for word sense dis-
ambiguation with a probabilistic measure of gloss-
context overlap. The base model on average more
than doubled the accuracy of Lesk in Senseval-2
on both fine- and coarse-grained tracks. With ad-
ditional lexical knowledge, the model also outper-
formed state of the art results with statistical sig-
nificance on two coarse-grained WSD tasks.
For future work, we plan to apply the model
in other shared tasks, including open-text WSD,
so as to compare with more recent Lesk variants.
We would also like to explore how to incorpo-
rate syntactic features and employ alternative sta-
tistical methods (e.g., parametric models) to im-
prove probability estimation and inference. Other
NLP problems involving compositionality in gen-
eral might also benefit from the proposed many-
to-many similarity measure.
Acknowledgments
This study is funded by the Natural Sciences and
Engineering Research Council of Canada. We
thank Afsaneh Fazly, Navdeep Jaitly, and Varada
Kolhatkar for the many inspiring discussions, as
well as the anonymous reviewers for their con-
structive advice.
535
References
Khaled Abdalgader and Andrew Skabar. Unsupervised
similarity-based word sense disambiguation using context
vectors and sentential word importance. ACM Transac-
tions on Speech and Language Processing, 9(1):2:1?2:21,
May 2012.
Satanjeev Banerjee and Ted Pedersen. An adapted Lesk al-
gorithm for word sense disambiguation using WordNet.
In Computational Linguistics and Intelligent Text Process-
ing, pages 136?145. Springer, 2002.
Satanjeev Banerjee and Ted Pedersen. Extended gloss over-
laps as a measure of semantic relatedness. In Proceedings
of the 18th International Joint Conference on Artificial In-
telligence, volume 3, pages 805?810, 2003.
Alexander Budanitsky and Graeme Hirst. Evaluating
WordNet-based measures of lexical semantic relatedness.
Computational Linguistics, 32(1):13?47, 2006.
Ping Chen, Wei Ding, Chris Bowes, and David Brown. A
fully unsupervised word sense disambiguation method us-
ing dependency knowledge. In Proceedings of Human
Language Technologies: The 2009 Annual Conference of
the North American Chapter of the Association for Com-
putational Linguistics, pages 28?36, Stroudsburg, PA,
USA, 2009.
Kenneth Ward Church and Patrick Hanks. Word association
norms, mutual information, and lexicography. Computa-
tional Linguistics, 16(1):22?29, 1990.
Philip Edmonds and Scott Cotton. Senseval-2: Overview. In
Proceedings of the 2nd International Workshop on Eval-
uating Word Sense Disambiguation Systems, pages 1?5.
Association for Computational Linguistics, 2001.
Christiane Fellbaum. WordNet: An Electronic Lexical
Database. MIT Press, Cambridge, MA, 1998.
David Fern?andez-Amor?os, Julio Gonzalo, and Felisa Verdejo.
The UNED systems at Senseval-2. In The Proceedings of
the Second International Workshop on Evaluating Word
Sense Disambiguation Systems, pages 75?78. Association
for Computational Linguistics, 2001.
Mark Alan Finlayson. Java libraries for accessing the Prince-
ton WordNet: Comparison and evaluation. In Proceed-
ings of the 7th Global Wordnet Conference, Tartu, Estonia,
2014.
William Gale, Kenneth Church, and David Yarowsky. A
method for disambiguating word senses in a large corpus.
Computers and the Humanities, 26(5-6):415?439, 1992.
Florentina Hristea. Recent advances concerning the usage of
the Na??ve Bayes model in unsupervised word sense dis-
ambiguation. International Review on Computers & Soft-
ware, 4(1), 2009.
Jay Jiang and David Conrath. Semantic similarity based on
corpus statistics and lexical taxonomy. Proceedings of
International Conference on Research in Computational
Linguistics, 1997.
Adam Kilgarriff and Joseph Rosenzweig. Framework and
results for English Senseval. Computers and the Humani-
ties, 34(1-2):15?48, 2000.
Michael Lesk. Automatic sense disambiguation using ma-
chine readable dictionaries: how to tell a pine cone from
an ice cream cone. In Proceedings of the 5th Annual In-
ternational Conference on Systems Documentation, pages
24?26, New York, New York, USA, 1986.
Yuhua Li, David McLean, Zuhair A Bandar, James D
O?Shea, and Keeley Crockett. Sentence similarity based
on semantic nets and corpus statistics. IEEE Transactions
on Knowledge and Data Engineering, 18(8):1138?1150,
2006.
Kenneth C. Litkowski. Sense information for disambigua-
tion: Confluence of supervised and unsupervised methods.
In Proceedings of the ACL-02 Workshop on Word Sense
Disambiguation: Recent Successes and Future Directions,
pages 47?53. Association for Computational Linguistics,
July 2002.
Rada Mihalcea, Paul Tarau, and Elizabeth Figa. PageRank on
semantic networks, with application to word sense disam-
biguation. In Proceedings of the 20th International Con-
ference on Computational Linguistics, 2004.
Tristan Miller, Chris Biemann, Torsten Zesch, and Iryna
Gurevych. Using distributional similarity for lexical ex-
pansion in knowledge-based word sense disambiguation.
In Proceedings of the 24th International Conference on
Computational Linguistics, pages 1781?1796, 2012.
Roberto Navigli. Word sense disambiguation: A survey.
ACM Computing Surveys, 41(2):10:1?10:69, 2009.
Siddharth Patwardhan and Ted Pedersen. Using WordNet-
based context vectors to estimate the semantic relatedness
of concepts. Proceedings of the EACL 2006 Workshop
Making Sense of Sense-Bringing Computational Linguis-
tics and Psycholinguistics Together, 1501:1?8, 2006.
Siddharth Patwardhan, Satanjeev Banerjee, and Ted Peder-
sen. Using measures of semantic relatedness for word
sense disambiguation. In Proceedings of the 4th Interna-
tional Conference on Intelligent Text Processing and Com-
putational Linguistics, pages 241?257, 2003.
Ted Pedersen. A simple approach to building ensembles of
Naive Bayesian classifiers for word sense disambiguation.
In Proceedings of the 1st Conference of North American
Chapter of the Association for Computational Linguistics,
pages 63?69. Association for Computational Linguistics,
2000.
Sameer Pradhan, Edward Loper, Dmitriy Dligach, and
Martha Palmer. SemEval-2007 task 17: English lexical
sample, SRL and all words. In Proceedings of the 4th
International Workshop on Semantic Evaluations, pages
87?92. Association for Computational Linguistics, 2007.
Ariel Raviv, Shaul Markovitch, and Sotirios-Efstathios
Maneas. Concept-based approach to word sense disam-
biguation. In Proceedings of the 26th Conference on Arti-
ficial Intelligence, 2012.
Philip Resnik. Using information content to evaluate seman-
tic similarity in a taxonomy. In Proceedings of the 14th
International Joint Conference on Artificial Intelligence -
Volume 1, IJCAI?95, pages 448?453, San Francisco, CA,
USA, 1995.
David Tugwell and Adam Kilgarriff. Wasp-bench: a lex-
icographic tool supporting word sense disambiguation.
In The Proceedings of the Second International Work-
shop on Evaluating Word Sense Disambiguation Systems,
pages 151?154. Association for Computational Linguis-
tics, 2001.
Peter Turney. Mining the web for synonyms: PMI-IR versus
LSA on TOEFL. In Proceedings of the 12th European
Conference on Machine Learning, pages 491?502, 2001.
Florentina Vasilescu, Philippe Langlais, and Guy Lapalme.
Evaluating variants of the Lesk approach for disambiguat-
ing words. In Proceedings of the 4th International Con-
ference on Language Resources and Evaluation, 2004.
536
Yorick Wilks, Dan Fass, Cheng-Ming Guo, James E. McDon-
ald, Tony Plate, and Brian M. Slator. Providing machine
tractable dictionary tools. Machine Translation, 5(2):99?
154, 1990.
David Yarowsky. Unsupervised word sense disambiguation
rivaling supervised methods. In Proceedings of the 33rd
annual meeting on Association for Computational Lin-
guistics, pages 189?196, 1995.
537
