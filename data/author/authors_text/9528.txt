Software Engineering, Testing, and Quality Assurance for Natural Language Processing, pages 31?39,
Columbus, Ohio, USA, June 2008. c?2008 Association for Computational Linguistics
Building a BIOWORDNET by Using WORDNET?s Data Formats
and WORDNET?s Software Infrastructure ? A Failure Story
Michael Poprat Elena Beisswanger
Jena University Language & Information Engineering (JULIE) Lab
Friedrich-Schiller-Universita?t Jena
D-07743 Jena, Germany
{poprat,beisswanger,hahn}@coling-uni-jena.de
Udo Hahn
Abstract
In this paper, we describe our efforts to build
on WORDNET resources, using WORDNET
lexical data, the data format that it comes with
and WORDNET?s software infrastructure in
order to generate a biomedical extension of
WORDNET, the BIOWORDNET. We began
our efforts on the assumption that the soft-
ware resources were stable and reliable. In
the course of our work, it turned out that this
belief was far too optimistic. We discuss the
stumbling blocks that we encountered, point
out an error in the WORDNET software with
implications for research based on it, and con-
clude that building on the legacy of WORD-
NET data structures and its associated soft-
ware might preclude sustainable extensions
that go beyond the domain of general English.
1 Introduction
WORDNET (Fellbaum, 1998) is one of the most au-
thoritative lexical resources for the general English
language. Due to its coverage ? currently more than
150,000 lexical items ? and its lexicological rich-
ness in terms of definitions (glosses) and semantic
relations, synonymy via synsets in particular, it has
become a de facto standard for all sorts of research
that rely on lexical content for the English language.
Besides this perspective on rich lexicological
data, over the years a software infrastructure has
emerged around WORDNET that was equally ap-
proved by the NLP community. This included,
e.g., a lexicographic file generator, various editors
and visualization tools but also meta tools rely-
ing on properly formated WORDNET data such as
a library of similarity measures (Pedersen et al,
2004). In numerous articles the usefulness of this
data and software ensemble has been demonstrated
(e.g., for word sense disambiguation (Patwardhan
et al, 2003), the analysis of noun phrase conjuncts
(Hogan, 2007), or the resolution of coreferences
(Harabagiu et al, 2001)).
In our research on information extraction and text
mining within the field of biomedical NLP, we sim-
ilarly recognized an urgent need for a lexical re-
source comparable to WORDNET, both in scope and
size. However, the direct usability of the original
WORDNET for biomedical NLP is severely ham-
pered by a (not so surprising) lack of coverage of the
life sciences domain in the general-language English
WORDNET as was clearly demonstrated by Burgun
and Bodenreider (2001).
Rather than building a BIOWORDNET by hand,
as was done for the general-language English
WORDNET, our idea to set up a WORDNET-style
lexical resource for the life sciences was different.
We wanted to link the original WORDNET with
various biomedical terminological resources vastly
available in the life sciences domain. As an obvious
candidate for this merger, we chose one of the ma-
jor high-coverage umbrella systems for biomedical
ontologies, the OPEN BIOMEDICAL ONTOLOGIES
(OBO).1 These (currently) over 60 OBO ontologies
provide domain-specific knowledge in terms of hi-
erarchies of classes that often come with synonyms
and textual definitions for lots of biomedical sub-
domains (such as genes, proteins, cells, sequences,
1http://www.bioontology.org/
repositories.html#obo
31
etc.).2 Given these resources and their software in-
frastructure, our plan was to create a biomedically
focused lexicological resource, the BIOWORDNET,
whose coverage would exceed that of any of its com-
ponent resources in a so far unprecedented man-
ner. Only then, given such a huge combined re-
source advanced NLP tasks such as anaphora res-
olution seem likely to be tackled in a feasible way
(Hahn et al, 1999; Castan?o et al, 2002; Poprat and
Hahn, 2007). In particular, we wanted to make di-
rect use of available software infrastructure such as
the library of similarity metrics without the need for
re-programming and hence foster the reuse of exist-
ing software as is.
We began our efforts on the assumption that the
WORDNET software resources were stable and reli-
able. In the course of our work, it turned out that this
belief was far too optimistic. We discuss the stum-
bling blocks that we encountered, point out an er-
ror in the WORDNET software with implications for
research based on it, and conclude that building on
the legacy of WORDNET data structures and its as-
sociated software might preclude sustainable exten-
sions that go beyond the domain of general English.
Hence, our report contains one of the rare failure sto-
ries (not only) in our field.
2 Software Around WORDNET Data
While the stock of lexical data assembled in the
WORDNET lexicon was continuously growing over
time,3 its data format and storage structures, the so-
called lexicographic file, by and large, remained un-
altered (see Section 2.1). In Section 2.2, we will deal
with two important software components with which
the lexicographic file can be created and browsed.
Over the years, together with the continuous exten-
sion of the WORDNET lexicon, a lot of software
tools have been developed in various programming
languages allowing browsing and accessing WORD-
NET as well as calculating semantic similarities on
it. We will discuss the most relevant of these tools
in Section 2.3.
2Bodenreider and Burgun (2002) point out that the structure
of definitions in WORDNET differ to some degree from more
domain-specialized sources such as medical dictionaries.
3The latest version 3.0 was released in December 2006
2.1 Lexicon Organization of WORDNET and
Storage in Lexicographic Files
At the top level, WORDNET is organized accord-
ing to four parts of speech, viz. noun, verb, adjec-
tive and adverb. The most recent version 3.0 cov-
ers more than 117,000 nouns, 11,500 verbs, 21,400
adjectives and 4,400 adverbs, interlinked by lexical
relations, mostly derivations. The basic semantic
unit for all parts of speech are sets of synonymous
words, so-called synsets. These are connected by
different semantic relations, imposing a thesaurus-
like structure on WORDNET. In this paper, we dis-
cuss the organization of noun synsets in WORDNET
only, because this is the relevant part of WORD-
NET for our work. There are two important seman-
tic relation types linking noun synsets. The hyper-
nym / hyponym relation on which the whole WORD-
NET noun sense hierarchy is built links more spe-
cific to more general synsets, while the meronym /
holonym relation describes partonomic relations be-
tween synsets, such as part of the whole, member of
the whole or substance of the whole.
From its very beginning, WORDNET was built
and curated manually. Lexicon developing experts
introduced new lexical entries into WORDNET,
grouped them into synsets and defined appropriate
semantic and lexical relations. Since WORDNET
was intended to be an electronic lexicon, a data
representation format had to be defined as well.
When the WORDNET project started more than two
decades ago, markup languages such as SGML or
XML were unknown. Because of this reason, a
rather idiosyncratic, fully text-based data structure
for these lexicographic files was defined in a way to
be readable and editable by humans ? and survived
until to-day. This can really be considered as an
outdated legacy given the fact that the WORDNET
community has been so active in the last years in
terms of data collection, but has refrained from
adapting its data formats in a comparable way to
to-day?s specification standards. Very basically,4
each line in the lexicographic file holds one synset
that is enclosed by curly brackets. Take as an
example the synset for ?monkey?:
4A detailed description can be found in the WORDNET
manual wninput(5WN), available from http://wordnet.
princeton.edu/man/wninput.5WN.
32
{ monkey, primate,@ (any of various
long-tailed primates (excluding the
prosimians)) }
Within the brackets at the first position synonyms
are listed, separated by commas. In the exam-
ple, there is only one synonym, namely ?monkey?.
The synonyms are followed by semantic relations to
other synsets, if available. In the example, there is
only one hypernym relation (denoted by ?@?) point-
ing to the synset ?primate?. The final position is
reserved for the gloss of the synset encapsulated in
round brackets. It is important to notice that there
are no identifiers for synsets in the lexicographic file.
Rather, the string expressions themselves serve as
identifiers. Given the fundamental idea of synsets ?
all words within a synset mean exactly the same in
a certain context ? it is sufficient to relate one word
in the synset in order to refer to the whole synset.
Still, there must be a way to deal with homonyms,
i.e., lexical items which share the same string, but
have different meanings. WORDNET?s approach to
distinguish different senses of a word is to add num-
bers from 0 to 15, called lexical identifiers. Hence,
in WORDNET, a word cannot be more than 16-fold
ambiguous. This must be kept in mind when one
wants to build a WORDNET for highly ambiguous
sublanguages such as the biomedical one.
2.2 Software Provided with WORDNET
To guarantee fast access to the entries and their rela-
tions, an optimized index file must be created. This
is achieved through the easy-to-use GRIND software
which comes with WORDNET. It simply consumes
the lexicographic file(s) as input and creates two
plain-text index files,5 namely data and index.
Furthermore, there is a command line tool, WN, and
a graphical browser, WNB, for data visualization that
require the specific index created by GRIND (as all
the other tools that query the WORDNET data do as
well). These tools are the most important (and only)
means of software support for WORDNET creation
by checking the syntax as well as allowing the (man-
ual) inspection of the newly created index.
5Its syntax is described in http://wordnet.
princeton.edu/man/wndb.5WN.
2.3 Third-Party WORDNET Tools
Due to the tremendous value of WORDNET for the
NLP and IR community and its usefulness as a
resource for coping with problems requiring mas-
sive amounts of lexico-semantic knowledge, the
software-developing community was and continues
to be quite active. Hence, in support of WORDNET
several APIs and software tools were released that
allow accessing, browsing and visualizing WORD-
NET data and measuring semantic similarity on the
base of the WORDNET?s lexical data structures.6
The majority of these APIs are maintained well
and kept up to date, such as JAWS7 and JWNL,8
and enable connecting to the most recent ver-
sion of WORDNET. For the calculation of vari-
ous similarity measures, the PERL library WORD-
NET::SIMILARITY initiated and maintained by Ted
Pedersen9 can be considered as a de facto stan-
dard and has been used in various experimental set-
tings and applications. This availability of well-
documented and well-maintained software is defi-
nitely a strong argument to rely on WORDNET as
a powerful lexico-semantic knowledge resource.
3 The BIOWORDNET Initiative
In this section, we describe our approach to extend
WORDNET towards the biomedical domain by in-
corporating terminological resources from the OBO
collection. The most obvious problems we faced
were to define a common data format and to map
non-compliant data formats to the chosen one.
3.1 OBO Ontologies
OBO is a collection of publicly accessible biomed-
ical ontologies.10 They cover terms from
many biomedical subdomains and offer structured,
domain-specific knowledge in terms of classes
(which often come with synonyms and textual defi-
nitions) and class hierarchies. Besides the hierarchy-
defining relation is-a, some OBO ontologies provide
6For a comprehensive overview of available WORDNET
tools we refer to WORDNET?s ?related project? website (http:
//wordnet.princeton.edu/links).
7http://engr.smu.edu/
?
tspell/
8http://jwordnet.sourceforge.net/
9http://wn-similarity.sourceforge.net/
10http://www.bioontology.org/
33
WordNet
Index
{ histoblast, simple_col...
{  laborinth_support ing .. .
{ structural_cell, cell_by...
{  mesangial_phagocyte, . . .
{ ito_cell, perisinusoida_ ...
{  . . .  }
   ...
OBO ontology 
in OWL-format
extracted data BioWordNet
lexicographic
fi le
Step 1 :
data  ex t rac t ion
from OBO
Step 2:  
convers ion to  WordNet
lexicographic f i le
 fo rmat
Step 3:  
bui ld ing WordNet  index 
using ?grind?
WordNet Browser
WordNet APIInformation Retrieval
Similarity MeasuringAnaphora Resolution
Document Clustering
Step 4:  
B ioWordNet
 index can be 
used by var ious 
sof tware  
components  
and APIs . . .
Step 5:  
. . .  and fur ther  be processed 
in  NLP compontents
BioWordNet
index f i le
IR and NLP 
applications
Figure 1: From OBO ontologies to BIOWORDNET? towards a domain-specific WORDNET for biomedicine
additional semantic relation types such as sequence-
of or develops-from to express even more complex
and finer-grained domain-specific knowledge. The
ontologies vary significantly in size (up to 60,000
classes with more than 150,000 synonyms), the
number of synonyms per term and the nature of
terms.
The OBO ontologies are available in various for-
mats including the OBO flat file format, XML and
OWL. We chose to work with the OWL version for
our purpose,11 since for the OWL language also ap-
propriate tools are available facilitating the extrac-
tion of particular information from the ontologies,
such as taxonomic links, labels, synonyms and tex-
tual definitions of classes.
3.2 From OBO to BIOWORDNET
Our plan was to construct a BIOWORDNET by con-
verting, in the first step, the OBO ontologies into a
WORDNET hierarchy of synsets, while keeping to
the WORDNET lexicographic file format, and build-
ing a WORDNET index. As a preparatory step, we
defined a mapping from the ontology to WORDNET
items as shown in Table 1.
The three-stage conversion approach is depicted
in Figure 1. First, domain specific terms and tax-
11http://www.w3.org/TR/owl-semantics/
OBO ontology BIOWORDNET
ontology class synset
class definition synset gloss
class name word in synset
synonym of class name word in synset
Ci is-a Cj Si hyponym of Sj
Cj has-subclass Ci Sj hypernym of Si
Table 1: Mapping between items from OBO and from
BIOWORDNET (Ci and Cj denote ontology classes, Si
and Sj the corresponding BIOWORDNET synsets)
onomic links between terms were extracted sepa-
rately from each of the OBO ontologies. Then
the extracted data was converted according to the
syntax specifications of WORDNET?s lexicographic
file. Finally for each of the converted ontologies the
WORDNET-specific index was built using GRIND.
Following this approach we ran into several prob-
lems, both regarding the WORDNET data structure
and the WORDNET-related software that we used
for the construction of the BIOWORDNET. Con-
verting the OBO ontologies turned out to be cum-
bersome, especially the conversion of the CHEBI
ontology12 (long class names holding many special
characters) and the NCI thesaurus13 (large number
12http://www.ebi.ac.uk/chebi/
13http://nciterms.nci.nih.gov/
34
of classes and some classes that also have a large
number of subclasses). These and additional prob-
lems will be addressed in more detail in Section 4.
4 Problems with WORDNET?s Data
Format and Software Infrastructure
We here discuss two types of problems we found
for the data format underlying the WORDNET lex-
icon and the software that helps building a WORD-
NET file and creating an index for this file. First,
WORDNET?s data structure puts several restrictions
on what can be expressed in a WORDNET lexicon.
For example, it constrains lexical information to a
fixed number of homonyms and a fixed set of rela-
tions. Second, the data structure imposes a number
of restrictions on the string format level. If these
restrictions are violated the WORDNET processing
software throws error messages which differ consid-
erably in terms of informativeness for error tracing
and detection or even do not surface at all at the lex-
icon builder?s administration level.
4.1 Limitations of Expressiveness
The syntax on which the current WORDNET lex-
icographic file is based imposes severe limitations
on what can be expressed in WORDNET. Although
these limitations might be irrelevant for representing
general-language terms, they do affect the construc-
tion of a WORDNET-like resource for biomedicine.
To give some examples, the WORDNET format al-
lows a 16-fold lexical ambiguity only (lexical IDs
that are assigned to ambiguous words are restricted
to the numbers 0-15, see Section 2). This forced us
to neglect some of the OBO ontology class names
and synonyms that were highly ambiguous.14
Furthermore, the OBO ontologies excel in a richer
set of semantic relations than WORDNET can of-
fer. Thus, a general problem with the conversion
of the OBO ontologies into WORDNET format was
that except from the taxonomic is-a relation (which
corresponds to the WORDNET hyponym relation)
and the part-of relation (which corresponds to the
WORDNET meronym relation) all remaining OBO-
specific relations (such as develops-from, sequence-
of, variant-of and position-of ) could not be rep-
14This is a well-known limitation that is already mentioned
in the WORDNET documentation.
resented in the BIOWORDNET. The structure of
WORDNET neither contains such relations nor is
it flexible enough to include them so that we face
a systematic loss of information in BIOWORDNET
compared to the original OBO ontologies. Al-
though these restrictions are well-known, their re-
moval would require extending the current WORD-
NET data structure fundamentally. This, in turn,
would probably necessitate a full re-programming of
all of WORDNET-related software.
4.2 Limitations of Data Format and Software
When we tried to convert data extracted from the
OBO ontologies into WORDNET?s lexicographic
file format (preserving its syntactic idiosyncrasies
for the sake of quick and straightforward reusability
of software add-ons), we encountered several intri-
cacies that took a lot of time prior to building a valid
lexicographic file.
First, we had to replace 31 different charac-
ters with unique strings such as ?(? with ?-LRB-
? and ?+? with ?-PLU-? before GRIND was able
to process the lexicographic file. The reason is
that many of such special characters occurring
in domain specific terms, especially in designa-
tors of chemical compounds such as ?methyl es-
ter 2,10-dichloro-12H-dibenzo(d,g)(1,3)dioxocin-6-
carboxylic acid? (also known as ?treloxinate? with
the CAS registry number 30910-27-1), are reserved
symbols in the WORDNET data formatting syntax.
If these characters are not properly replaced GRIND
throws an exact and useful error message (see Table
2, first row).
Second, we had to find out that we have to replace
all empty glosses by at least one whitespace charac-
ter. Otherwise, GRIND informs the user in terms of
a rather cryptic error message that mentions the po-
sition of the error though not its reason (see Table 2,
second row).
Third, numbers at the end of a lexical item need to
be escaped. In WORDNET, the string representation
of an item is used as its unique identifier. To dis-
tinguish homonyms (words with the same spelling
but different meaning, such as ?cell? as the func-
tional unit of all organisms, on the one hand, and
as small compartment, on the other hand) accord-
ing to the WORDNET format different numbers from
0 to 15 (so-called lexical IDs) have to be appended
35
Problem Description Sample Error Message Usefulness of Er-
ror Message
Problem Solution
illegal use of key characters noun.cell, line 7: Illegal
character %
high replace illegal characters
empty gloss sanity error - actual pos
2145 != assigned pos
2143!
moderate add gloss consisting of at least
one whitespace character
homonyms (different words
with identical strings)
noun.rex, line 5: Syn-
onym ?electrochem-
ical reaction? is not
unique in file
high distinguish word senses by
adding lexical identifiers (use
the numbers 1-15)
lexical ID larger than 15 noun.rex, line 4: ID must
be less than 16: cd25
high quote trailing numbers of
words, only assign lexical
identifiers between 1-15, omit
additional word senses
word with more than 425
characters
Segmentation fault (core
dumped)
low omit words that exceed the max-
imal length of 425 characters
synset with more than 998
direct hyponymous synsets
Segmentation fault (core
dumped)
low omit some hyponymous synsets
or introduce intermediate
synsets with a limited number
of hyponymous synsets
no query result though the
synset is in the index, access
software crashes
none ? not known
Table 2: Overview of the different kinds of problems that we encountered when creating a BIOWORDNET keeping to
the WORDNET data structure and the corresponding software. Each problem description is followed by a sample error
message that GRIND had thrown, a statement about how useful the error message was to detect the source of the error
and a possible solution for the problems, if available. The last row documents a special experience with data viewers
for data from the NCI thesaurus.
to the end of each homonym. If in a lexicographic
file two identical strings occur that have not been as-
signed different lexical identifiers (it does not mat-
ter whether this happens within or across synsets)
GRIND emits an error message that mentions both,
the position and the lexical entry which caused this
error (cf. Table 2, third row).
Numbers that appear at the end of a lexical item as
an integral part of it (such as ?2? in ?IL2?, a special
type of cytokine (protein)) have to be escaped in or-
der to avoid their misinterpretation as lexical identi-
fiers. This, again, is a well-documented shortcoming
of WORDNET?s data specification rules.
In case such numbers are not escaped prior to pre-
senting the lexicographic file to GRIND the word
closing numbers are always interpreted as lexical
identifiers. Closing numbers that exceed the num-
ber 15 cause GRIND to throw an informative error
message (see Table 2, fourth row).
4.3 Undocumented Restrictions and
Insufficient Error Messages
In addition to the more or less documented re-
strictions of the WORDNET data format mentioned
above we found additional restrictions that lack doc-
umentation up until now, to the best of our knowl-
edge.
First, it seems that the length of a word is re-
stricted to 425 characters. If a word in the lexico-
graphic file exceeds this length, GRIND is not able to
create an index and throws an empty error message,
namely the memory error ?segmentation fault? (cf.
Table 2, fifth row). As a consequence of this restric-
tion, some very long CHEBI class names could not
have been included in the BIOWORDNET.
Second, it seems that synsets are only allowed to
group up to 988 direct hyponymous synsets. Again,
GRIND is not able to create an index, if this restric-
tion is not obeyed and throws the null memory er-
36
ror message ?segmentation fault? (cf. Table 2, sixth
row). An NCI thesaurus class that had more than
998 direct subclasses thus could not have been in-
cluded in the BIOWORDNET.
Due to insufficient documentation and utterly
general error messages the only way to locate the
problem causing the ?segmentation fault? errors was
to examine the lexicographic files manually. We had
to reduce the number of synset entries in the lexico-
graphic file, step by step, in a kind of trial and error
approach until we could resolve the problem. This
is, no doubt, a highly inefficient and time consum-
ing procedure. More informative error messages of
GRIND would have helped us a lot.
4.4 Deceptive Results from WORDNET
Software and Third-Party Components
After getting rid of all previously mentioned errors,
valid index files were compiled. It was possible to
access these index files using the WORDNET query-
ing tools WN and WNB, indicating the index files
were ?valid?. However, when we tried to query
the index file that was generated by GRIND for the
NCI thesaurus we got strange results. While WN
did not return any query results, the browser WNB
crashed without any error message (cf. Table 2, sev-
enth row). The same holds for the Java APIs JAWS
and JWNL.
Since a manual examination of the index file re-
vealed that the entries that we were searching for, in
fact, were included in the file, some other, up to this
step unknown error must have prevented the soft-
ware tools from finding the targeted entries. Hence,
we want to point out that although we have exam-
ined this error for the NCI thesaurus only, the risk
is high that this ?no show? error is likely to bias
any other application as well which makes use of
the the same software that we grounded our ex-
periments on. Since the NCI thesaurus is a very
large resource, even worse, further manual error
search is nearly impossible. At this point, we
stopped our attempt building a WORDNET resource
for biomedicine based on the WORDNET formatting
and software framework.
5 Related Work
In the literature dealing with WORDNET and its
structures from a resource perspective (rather than
dealing with its applications), two directions can
be distinguished. On the one hand, besides the
original English WORDNET and the various vari-
ant WORDNETs for other languages (Vossen, 1998),
extensions to particular domains have already been
proposed (for the medical domain by Buitelaar and
Sacaleanu (2002) and Fellbaum et al (2006); for the
architectural domain Bentivogli et al (2004); and
for the technical report domain by Vossen (2001)).
However, none of these authors neither mentions im-
plementation details of the WORDNETs or perfor-
mance pitfalls we have encountered, nor is supple-
mentary software pointed out that might be useful
for our work.
On the other hand, there are suggestions concern-
ing novel representation formats of next-generation
WORDNETs. For instance in the BALKANET
project (Tufis? et al, 2004), an XML schema plus
a DTD was proposed (Smrz?, 2004) and an editor
called CISDIC with basic maintenance functionali-
ties and consistency check was released (Hora?k and
Smrz?, 2004). The availability of APIs or software to
measure similarity though remains an open issue.
So, our approach to reuse the structure and the
software for building a BIOWORDNET was moti-
vated by the fact that we could not find any al-
ternatives coming with a software ensemble as de-
scribed in Section 2. Against all expectations, we
did not manage to reuse the WORDNET data struc-
ture. However, there are no publications that report
on such difficulties and pitfalls we were confronted
with.
6 Discussion and Conclusion
We learnt from our conversion attempt that the cur-
rent WORDNET representation format of WORD-
NET suffers from several limitations and idiosyn-
crasies that cannot be by-passed by a simple, yet
ad hoc work-around. Many of the limitations and
pitfalls we found limiting (in the sense what can be
expressed in WORDNET) are due to the fact that its
data format is out-of-date and not really suitable for
the biomedical sublanguage. In addition, though we
do not take into doubt that the WORDNET software
37
works fine for the official WORDNET release, our
experiences taught us that it fails or gives limited
support in case of building and debugging a new
WORDNET resource. Even worse, we have evidence
from one large terminological resource (NCI) that
WORDNET?s software infrastructure (GRIND) ren-
ders deceptive results.
Although WORDNET might no longerbe the one
and only lexical resource for NLP each year a con-
tinuously strong stream of publications on the use of
WORDNET illustrates its importance for the com-
munity. On this account we find it remarkable that
although improvements in content and structure of
WORDNET have been proposed (e.g., Boyd-Graber
et al (2006) propose to add (weighted) connec-
tions between synsets, Oltramari et al (2002) sug-
gest to restructure WORDNET?s taxonomical struc-
ture, and Mihalcea and Moldovan (2001) recom-
mend to merge synsets that are too fine-grained)
to the best of our knowledge, no explicit proposals
have been made to improve the representation for-
mat of WORDNET in combination with the adaption
of the WORDNET-related software.
According to our experiences the existing WORD-
NET software is hardly (re)usable due to insufficient
error messages that the software throws and limited
documentation. From our point of view it would be
highly preferable if the software would be improved
and made more user-supportive (more meaningful
error messages would already improve the useful-
ness of the software). In terms of the actual rep-
resentation format of WORDNET we found that us-
ing the current format is not only cumbersome and
error-prone, but also limits what can be expressed in
a WORDNET resource.
From our perspective this indicates the need for
a major redesign of WORDNET?s data structure
foundations to keep up with the standards of to-
day?s meta data specification languages (e.g., based
on RFD (Graves and Gutierrez, 2006), XML or
OWL (Lu?ngen et al, 2007)). We encourage the re-
implementation of WORDNET resources based on
such a state-of-the-art markup language (for OWL in
particular a representation of WORDNET is already
available, cf. van Assem et al (2006)). Of course, if
a new representation format is used for a WORDNET
resource also the software accessing the resource has
to be adapted to the new format. This may require
substantial implementation efforts that we think are
worth to be spent, if the new format overcomes the
major problems that are due to the original WORD-
NET format.
Acknowledgments
This work was funded by the German Ministry
of Education and Research within the STEMNET
project (01DS001A-C) and by the EC within the
BOOTSTREP project (FP6-028099).
References
Luisa Bentivogli, Andrea Bocco, and Emanuele Pianta.
2004. ARCHIWORDNET: Integrating WORDNET
with domain-specific knowledge. In Petr Sojka, Karel
Pala, Christiane Fellbaum, and Piek Vossen, editors,
GWC 2004 ? Proceedings of the 2nd International
Conference of the Global WordNet Association, pages
39?46. Brno, Czech Republic, January 20-23, 2004.
Olivier Bodenreider and Anita Burgun. 2002. Character-
izing the definitions of anatomical concepts in WORD-
NET and specialized sources. In Proceedings of the 1st
International Conference of the Global WordNet Asso-
ciation, pages 223?230. Mysore, India, January 21-25,
2002.
Jordan Boyd-Graber, Christiane Fellbaum, Daniel Osh-
erson, and Robert Schapire. 2006. Adding dense,
weighted connections to WORDNET. In Petr Sojka,
Key-Sun Choi, Christiane Fellbaum, and Piek Vossen,
editors, GWC 2006 ? Proceedings of the 3rd Inter-
national WORDNET Conference, pages 29?35. South
Jeju Island, Korea, January 22-26, 2006.
Paul Buitelaar and Bogdan Sacaleanu. 2002. Extend-
ing synsets with medical terms WORDNET and spe-
cialized sources. In Proceedings of the 1st Interna-
tional Conference of the Global WordNet Association.
Mysore, India, January 21-25, 2002.
Anita Burgun and Olivier Bodenreider. 2001. Compar-
ing terms, concepts and semantic classes in WORD-
NET and the UNIFIED MEDICAL LANGUAGE SYS-
TEM. In Proceedings of the NAACL 2001 Workshop
?WORDNET and Other Lexical Resources: Applica-
tions, Extensions and Customizations?, pages 77?82.
Pittsburgh, PA, June 3-4, 2001. New Brunswick, NJ:
Association for Computational Linguistics.
Jose? Castan?o, Jason Zhang, and James Pustejovsky.
2002. Anaphora resolution in biomedical literature. In
Proceedings of The International Symposium on Ref-
erence Resolution for Natural Language Processing.
Alicante, Spain, June 3-4, 2002.
Christiane Fellbaum, Udo Hahn, and Barry Smith. 2006.
Towards new information resources for public health:
38
From WORDNET to MEDICAL WORDNET. Journal
of Biomedical Informatics, 39(3):321?332.
Christiane Fellbaum, editor. 1998. WORDNET: An Elec-
tronic Lexical Database. Cambridge, MA: MIT Press.
Alvaro Graves and Caludio Gutierrez. 2006. Data rep-
resentations for WORDNET: A case for RDF. In Petr
Sojka, Key-Sun Choi, Christiane Fellbaum, and Piek
Vossen, editors, GWC 2006 ? Proceedings of the 3rd
International WORDNET Conference, pages 165?169.
South Jeju Island, Korea, January 22-26, 2006.
Udo Hahn, Martin Romacker, and Stefan Schulz. 1999.
Discourse structures in medical reports ? watch out!
The generation of referentially coherent and valid text
knowledge bases in the MEDSYNDIKATE system. In-
ternational Journal of Medical Informatics, 53(1):1?
28.
Sanda M. Harabagiu, Ra?zvan C. Bunescu, and Steven J.
Maiorano. 2001. Text and knowledge mining for
coreference resolution. In NAACL?01, Language Tech-
nologies 2001 ? Proceedings of the 2nd Meeting of
the North American Chapter of the Association for
Computational Linguistics, pages 1?8. Pittsburgh, PA,
USA, June 2-7, 2001. San Francisco, CA: Morgan
Kaufmann.
Deirdre Hogan. 2007. Coordinate noun phrase disam-
biguation in a generative parsing model. In ACL?07 ?
Proceedings of the 45th Annual Meeting of the Asso-
ciation of Computational Linguistics, pages 680?687.
Prague, Czech Republic, June 28-29, 2007. Strouds-
burg, PA: Association for Computational Linguistics.
Ales? Hora?k and Pavel Smrz?. 2004. New features of
wordnet editor VisDic. Romanian Journal of Infor-
mation Science and Technology (Special Issue), 7(1-
2):201?213.
Harald Lu?ngen, Claudia Kunze, Lothar Lemnitzer, and
Angelika Storrer. 2007. Towards an integrated
OWL model for domain-specific and general language
WordNets. In Attila Tana?cs, Dora? Csendes, Veronika
Vincze, Christiane Fellbaum, and Piek Vossen, editors,
GWC 2008 ? Proceedings of the 4th Global WORD-
NET Conference, pages 281?296. Szeged, Hungary,
January 22-25, 2008.
Rada Mihalcea and Dan Moldovan. 2001.
EZ.WORDNET: Principles for automatic genera-
tion of a coarse grained WORDNET. In Proceedings
of the 14th International Florida Artificial Intelli-
gence Research Society (FLAIRS) Conference, pages
454?458.
Alessandro Oltramari, Aldo Gangemi, Nicola Guarino,
and Claudio Madolo. 2002. Restructuring WORD-
NET?s top-level. In Proceedings of ONTOLEX 2002
@ LREC 2002.
Siddharth Patwardhan, Satanjeev Banerjee, and Ted Ped-
ersen. 2003. Using measures of semantic related-
ness for word sense disambiguation. In Alexander F.
Gelbukh, editor, CICLing 2003 ? Computational Lin-
guistics and Intelligent Text Processing. Proceedings
of the 4th International Conference, volume 2588 of
Lecture Notes in Computer Science, pages 241?257.
Mexico City, Mexico, February 16-22, 2003. Berlin
etc.: Springer.
Ted Pedersen, Siddharth Patwardhan, and Jason Miche-
lizzi. 2004. WORDNET::Similarity: Measuring the
relatedness of concepts. In AAAI?04 ? Proceedings
of the 19th National Conference on Artificial Intelli-
gence & IAAI?04 ? Proceedings of the 16th Innova-
tive Applications of Artificial Intelligence Conference,
pages 1024?1025. San Jose?, CA, USA, July 25-29,
2004. Menlo Park, CA; Cambridge, MA: AAAI Press
& MIT Press.
Michael Poprat and Udo Hahn. 2007. Quantitative data
on referring expressions in biomedical abstracts. In
BioNLP at ACL 2007 ? Proceedings of the Workshop
on Biological, Translational, and Clinical Language
Processing, pages 193?194. Prague, Czech Republic,
June 29, 2007. Stroudsburg, PA: Association for Com-
putational Liguistics.
Pavel Smrz?. 2004. Quality control and checking for
wordnets development: A case study of BALKANET.
Romanian Journal of Information Science and Tech-
nology (Special Issue), 7(1-2):173?181.
D. Tufis?, D. Christea, and S. Stamou. 2004. BALKA-
NET: Aims, methods, results and perspectives. a gen-
eral overview. Romanian Journal of Information Sci-
ence and Technology (Special Issue), 7(1-2):9?43.
Mark van Assem, Aldo Gangemi, and Guus Schreiber.
2006. Conversion of WORDNET to a standard
RDF/OWL representation. In LREC 2006 ? Proceed-
ings of the 5th International Conference on Language
Resources and Evaluation. Genoa, Italy, May 22-28,
2006. Paris: European Language Resources Associa-
tion (ELRA), available on CD.
Piek Vossen, editor. 1998. EUROWORDNET: A Mul-
tilingual Database with Lexical Semantic Networks.
Dordrecht: Kluwer Academic Publishers.
Piek Vossen. 2001. Extending, trimming and fusing
WORDNET for technical documents. In Proceedings
of the NAACL 2001 Workshop ?WORDNET and Other
Lexical Resources: Applications, Extensions and Cus-
tomizations?. Pittsburgh, PA, June 3-4, 2001. New
Brunswick, NJ: Association for Computational Lin-
guistics.
39
Proceedings of the Fourth Linguistic Annotation Workshop, ACL 2010, pages 235?242,
Uppsala, Sweden, 15-16 July 2010. c?2010 Association for Computational Linguistics
A Proposal for a Configurable Silver Standard
Udo Hahn, Katrin Tomanek, Elena Beisswanger and Erik Faessler
Jena University Language & Information Engineering (JULIE) Lab
Friedrich-Schiller-Universita?t Jena
Fu?rstengraben 30, 07743 Jena, Germany
http://www.julielab.de
Abstract
Among the many proposals to promote al-
ternatives to costly to create gold stan-
dards, just recently the idea of a fully au-
tomatically, and thus cheaply, to set up sil-
ver standard has been launched. However,
the current construction policy for such a
silver standard requires crucial parameters
(such as similarity thresholds and agree-
ment cut-offs) to be set a priori, based on
extensive testing though, at corpus com-
pile time. Accordingly, such a corpus is
static, once it is released. We here propose
an alternative policy where silver stan-
dards can be dynamically optimized and
customized on demand (given a specific
goal function) using a gold standard as an
oracle.
1 Introduction
Training natural language systems which rely on
(semi-)supervised machine learning algorithms,
or measuring the systems? performance requires
some standardized ground truth from which one
can learn or against which one evaluate, respec-
tively. Usually, a manually crafted gold stan-
dard is provided that is generated by human lan-
guage or domain experts after lots of iterative,
guideline-based training rounds. This procedure is
expensive, slow and yields only small, yet highly
trustable, amounts of meta data ? because human
experts are in the loop.
In the CALBC project,1 an alternative ap-
proach is currently under investigation (Rebholz-
Schuhmann et al, 2010a). The basic idea is to
generate the much needed ground truth automati-
cally. This is achieved by letting a flock of named
entity taggers run on a corpus, without impos-
ing any restriction on the type(s) being annotated.
1http://www.calbc.eu
The (most likely) heterogeneous results are auto-
matically homogenized subsequently, thus yield-
ing a consensus-based, machine-generated ground
truth. Considering the possible benefits (e.g., the
positive experience from boosting-style machine
learners (Freund, 1990)), but also being aware of
the possible drawbacks (varying quality of the dif-
ferent systems, skewed coverage of entity types,
different types of guidelines on which they were
trained, etc.), the CALBC consortium refers to
the outcome of this process as a silver standard
(Rebholz-Schuhmann et al, 2010a). This proce-
dure is inexpensive, fast, yields huge amounts of
meta data ? because computers are in the loop ?
but after all its applicability and validity has yet to
be determined experimentally.
The first silver standard corpus (SSC) that came
out of the CALBC project was generated by the
four main partners? named entity taggers.2 The
various contributions covered, among others, an-
notations for genes and proteins, chemicals, dis-
eases, etc (Rebholz-Schuhmann et al, 2010b). Af-
ter the submission of their runs, the SSC was gen-
erated by, first, harmonizing stretches of text in
terms of entity mention identification and, second,
by mapping these normalized mentions to agreed-
upon type systems (such as the MESH Semantic
Groups as described by Bodenreider and McCray
(2003) for entity type normalization). Basically,
the harmonization steps included rules when en-
tity mentions were considered to match or overlap
(using a cosine-based similarity criterion) and en-
tity types referred to the same class. For consensus
generation, finally, simple rules for majority votes
were established.
The CALBC consortium is fully aware of the
fact that the value of an SSC can only be assessed
2The CALBC consortium consists the Rebholz Group
from EBI (Hinxton, U.K.), the Biosemantics Group from
Erasmus (Rotterdam, The Netherlands), the JULIE Lab (Jena,
Germany), and LINGUAMATICS (Cambridge, U.K.).
235
by comparing, e.g., systems trained on such a sil-
ver standard with systems trained on a gold stan-
dard (preferably, though not necessarily, one that
is a subset of the document set which makes up the
SSC).
In the absence of such a gold standard, the
CALBC consortium has spent enormous efforts to
find out the most reasonable parameter settings
for, e.g., the cosine threshold (setting similar men-
tions apart from dissimilar ones) or the consen-
sus constraint (where a certain number of entity
types equally assigned by different taggers makes
one type the consensual silver one and discards all
alternative annotations). Once these criteria are
made effective, the SSC is completely fixed.
As an alternative, we are looking for a more
flexible solution. Our investigation was fuelled by
the following observations:
? The idiosyncrasies of guidelines (on which
(some) taggers were trained) do not necessar-
ily lead to semantically totally different enti-
ties although they differ literally to some de-
gree. Some guidelines prefer, e.g., ?human
IL-7 protein?, others favor ?human IL-7?,
and some lean towards ?IL-7?. As the cosine
measure tends to penalize a pair such as ?hu-
man IL-7 protein? and ?IL-7?, we intended
to avoid such a prescriptive mode and just
look at the type assignment for single tokens
as (parts of) entity mentions. thus avoiding
inconclusive mention boundary discussions.
? While we were counting, for all tokens of
the document set, the votes a single token re-
ceived from different taggers in terms of an-
notating this token with respect to some type,
we generated confidence data for meta data
assignments. Incorporating the distribution
of confidence values into the configuration
process, this allows us to get rid of a pri-
ori fixed majority criteria (e.g., two or three
out of five systems must agree on this token)
which are hard to justify in an absolute way.
Summarizing, we believe that the nature of di-
verging tasks to be solved, the levels of entity type
specificity to be reached, the sort of guidelines be-
ing preferred, etc. should allow prospective users
of a silver standard to customize one on their own
and not stick to one that is already prefabricated
without concrete application in mind.3
3There may be tasks where a ?long? entity such as ?hu-
As such an enterprise would be quite arbitrary
without a reference standard, we even go one step
further. We determine the suitability of, say, dif-
ferent voting scores and varying lexical extensions
of mentions by comparison to a gold standard so
that the ?optimal? configuration of a silver stan-
dard, given a set of goal-derived requirements,
can be automatically learned. In real-world ap-
plications, such gold standard annotations would
be delivered only for a fraction of the documents
contained in the entire corpus being tagged by a
flock of taggers. The gold standard is used to op-
timize parameters which are subsequently applied
to the aggregation of automatically annotated data.
Note that the gold standard is used for optimiza-
tion only, not for training. We call such a flexible,
dynamically adjustable silver standard a config-
urable Silver Standard Corpus (conSSC). In a sec-
ond step, we split the various conSSCs, re-trained
our NER tagger on these data sets and, by compar-
ison with the gold standard, were able to identify
the optimal conSSC for this task (which is not the
one (SSC I) made available by the CALBC consor-
tium for the first challenge round).4
2 Optimizing Silver Standards
In this section, we describe the constituent param-
eters of a wide spectrum of SSCs. Mostly, these
parameters were taken over from the design of the
SSC as developed by the CALBC project members.
Differing from that fixed SSC, we investigate the
impact of different parameter settings on the con-
struction of a collection of SSCs, and, first, eval-
uate their direct usefulness on a gold standard for
protein-gene annotations. Second, we also assess
their indirect usefulness by training NER classi-
fiers on these SSCs and evaluate the NERs? perfor-
mance on the gold standard. Thus, our approach
is entirely data-driven without the need for human
intervention in terms of choosing suitable param-
eter settings.
Technically, we first aggregate the votes from
the flock of taggers (in our experiments, we used
the four taggers from the CALBC project members
plus a second tagger of one of the members) for
each text token (for confidence-based decisions)
or at the entity level (for cosine-based decisions),
then we determine the confidence values of these
man IL-7 protein? may be appropriate, while for another task
a short one such as ?IL-7? is entirely sufficient.
4http://www.ebi.ac.uk/Rebholz-srv/
CALBC/challenge.html
236
aggregated votes, and, finally, we compute the
similarity of the various SSCs with the gold stan-
dard data in terms of F-scores (both exact and open
boundaries) and accuracy on the token level.
2.1 Calibrating Consensus
The metrical interpretation of consensus will be
based on thresholded votes for semantic groups at
the token level (cf. Section 2.1.1) and a cosine-
based measure to determine contiguous stretches
of entity mentions in the text (cf. Section 2.1.2).
2.1.1 Type Confidence and Type Voting
For each text token, we determine the entity type
assignment as generated by each NER tagger
which is part of the flock of CALBC taggers.5 We
count and aggregate these votes such that each en-
tity type has an associated type count value.
We then compute the ratio of systems agree-
ing on the same single type assignment and call
this the confidence attributed to a particular type
for some token. The confidence value will sub-
sequently be interpreted against the confidence
threshold [0, 1] that defines a measure of certainty
a type assignment should have in order to be ac-
cepted as consensual.
2.1.2 Cosine-based Similarity of Phrasal
Entity Mentions
As the above policy of token-wise annotation de-
couples contiguous entity mentions spanning over
more than one token, we also want to restitute this
phrasal structure. This is achieved by constructing
contiguous sequences of tokens that characterize a
phrasal entity mention at the text level to which the
same type label has been assigned. Since differ-
ent taggers tend to identify different spans of text
for the same entity type (as shown in the exam-
ple from Section 1) we have to account for similar
phrasal forms of named entity mentions.
This is achieved by constructing vectors which
represent entity mentions and by computing the
cosine between the different entity mention vec-
tors. Let E1 = T1T2T3 be an entity mention com-
prised of three tokens T1 to T3. Let E2 = T2T3 be
5Due to time constraints when we performed our experi-
ments, we make an extremely simplifying assumption: From
the whole range of possible entity types NER taggers may as-
sign to some token (cf. (Bodenreider and McCray, 2003)) we
have chosen the PRotein/GEne group for testing. Still, this
assumption does not do harm to the core of our hypotheses.
See also our discussion in Section 5.
an entity mention overlapping with E1 in the to-
kens T2 and T3. To decide whether E1 and E2 are
considered similar, we first construct two vectors
representing the entity mentions:
v(E1) = (f1, f2, f3)T
with fi = IDF (Ti) being the inverse document
frequency of the token Ti. We compute the in-
verse document frequency of tokens based on the
corpus which is subject to analysis. Analogously,
we construct the vector for E2
v(E2) = (0, f2, f3)T
filling in a zero for the IDF of T1 since it is not
covered by E2. The entity mentions E1 and E2
are considered equal or similar, if the cosine of
the two vectors is greater or equal a given thresh-
old, cos(v(E1), v(E2)) ? threshold.6 We then
compute the number of systems considering an en-
tity annotation as similar in the manner described
above. The annotation is accepted and thus en-
tered into the SSC, if a particular number of sys-
tems agree on one annotation. This approach was
previously developed by the CALBC project part-
ners (Rebholz-Schuhmann et al, 2010a).
The number of agreeing systems and the thresh-
old are the free parameters of this method and thus
subject to optimization.
2.2 Optimization of Silver Standard Corpora
In the experiments described in the next section,
we will consider alternative parametrizations for
Silver Standard Corpora, i.e., the required confi-
dence threshold or cosine threshold and the num-
ber of agreeing systems. We will then discuss two
variants for optimizing this collection of SSCs.
The first one directly uses the gold standard for op-
timization. The task will be to find that particular
parameter setting for an SSC which best fits the
data contained in the gold standard. Once these
parameters are determined they can be applied to
the complete CALBC document set (composed of
100,000 documents) to produce the final, quasi-
optimal SSC.
In another variant, we insert a classifier into this
loop. First, we train a classifier on a particular
6For final corpus creation, it must be decided which of the
matching entity mentions is entered into the reference SSC,
e.g. the longest or shortest entity annotation. In our exper-
iments, we always chose the shortest entity mention. How-
ever, preliminary experiments showed that the differences to
taking the longest entity mention were marginal.
237
SSC that is built from a particular parameter com-
bination. Next, this classifier is tested against the
gold standard. This is iterated through all parame-
ter combinations. Obviously, the best performing
classifier relative to the gold standard selects the
optimal SSC.
3 Experimental Setting
3.1 Gold Standard
We generated a new broad-coverage corpus com-
posed of 3,236 MEDLINE abstracts (35,519 sen-
tences or 941,890 tokens) dealing with gene
and protein mentions. Altogether, it comprises
57,889 named entity type annotations annotated
by one expert biologist. We created this new re-
source to have a consistent and (as far as pos-
sible) subdomain-independent protein-annotated
corpus.7
MEDLINE abstracts were annotated with (pro-
tein coding) genes, mRNAs and proteins. A
distinction was made between dedicated proteins
as they are recorded in the protein database
UNIPROT,8 protein complexes consisting of sev-
eral protein subunits (e.g., IL-2 receptor consist-
ing of ?, ?, and ? chain), and protein families or
groups (e.g., ?transcription factors?). Also enu-
merations of proteins and protein variants were an-
notated. Discontinuous annotations were avoided
as well as nested annotations (annotations embed-
ded in other annotations). However, gene/protein
mentions nested in terms other than gene/protein
mentions were annotated (e.g., protein mentions
nested in protein function descriptions such as
?ligase? in ?ligase activity?). Modifiers such as
species designators were excluded from annota-
tions whenever possible. Gene segments or pro-
tein fragments were also not annotated.
For our experiments, we did not distinguish be-
tween the different annotation classes (see Table
1) but merged all available annotations into one
class, viz. PRotein/GEne (PRGE).
3.2 Automatic Annotation of the Gold Standard
We then asked all four sites participating in the
CALBC project to automatically annotate the given
gold standard (made available without gold data,
7We are aware of other gene/protein-annotated corpora
such as PENNBIOIE (http://bioie.ldc.upenn.
edu/) or GENIA (http://www-tsujii.is.s.
u-tokyo.ac.jp/GENIA/home/wiki.cgi) that will
have to be taken into account in future studies as well.
8http://www.uniprot.org/
semantic type description
T028 Gene or Genome
T086 Nucleotide Sequence
T087 Amino Acid Sequence,
Amino Acid, Peptide
T116 Protein
T126 Enzyme
T192 Receptor
Table 1: Semantic types defining the PRGE group
(semantic type codes refer to the UMLS).
of course) using the same type of named entity tag-
ging machinery as was used to annotate CALBC?s
canonical SSC. The performance results of each
group?s system evaluated against the gold standard
are reported in Table 2. The data of each system
constitute the reference data sets and raw data for
all subsequent experiments on the configuration
and optimization of the silver standard.
The resulting raw material does thus not only
contain gene/protein annotations but also any
other entity types as supplied by the partners.
For our experiments on the gold standard, how-
ever, only the entity types subsumed by the PRGE
group (see Table 1) were considered and annota-
tions of all other types were discarded. The def-
inition of the PRGE group is identical to the one
proposed by Rebholz-Schuhmann et al (2010a).
For the experiments, the specific semantic types
(e.g., the UMLS concepts)9 were not considered,
only the semantic group PRGE was.
3.3 Evaluation Metrics
The following metrics were used to evaluate how
good the silver standard(s) fit(s) the provided gold
standard:
? segment-level recall, precision, and F-score
values with exact boundaries, the standard
way to evaluate NER taggers,
? segment-level recall, precision, and F-score,
but with relaxed boundary constraints. This
means that two entity mentions are consid-
ered to match when they overlap with at least
one token and have the same entity type as-
signed to them,
? accuracy measured on the token level.
These metrics can be considered as optimization
criteria.
9http://www.nlm.nih.gov/research/umls/
238
3.4 Tokenization
The CALBC partners? data do not necessarily
come with tokenization information and, more-
over, different partners/systems might have differ-
ent tokenizations. Since a common ground for
comparison is thus lacking we added a new, con-
sistent tokenization based on the JULIE Lab tok-
enizer (Tomanek et al, 2007b). This tokenizer is
optimized for biomedical documents with intrinsic
focus to keep complex biological terminological
units (such as ?IL-2?) unsegmented, but to split
up tokens that are not terminologically connected
(such as dividing ?IL-2-related? up into ?IL-2?,
?-? and ?related?). As a matter of fact, entity
boundaries do not necessarily coincide with token
boundaries. Our solution to this problem is as fol-
lows: Whenever a token partially overlaps with an
entity name, the full form of that token is consid-
ered to be associated with this entity. All data on
which we report here (silver and gold standards)
obey to this tokenization scheme.
3.5 Parameters Being Tested
The following parameter settings were considered
in our experiments:
? Four different values for confidence thresh-
olds indicating that 20% (0.2), 40% (0.4),
60% (0.6) or 80% (0.8) of all taggers agreed
on the same type annotation, viz. PRGE,
? Five different values for cosine thresholds
to identify overlapping entity mentions, viz.
(0.7, 0.8, 0.9, 0.95, 0.975), and two different
values for the number n of agreeing taggers,
viz. n ? 2 and n ? 3,
? Two tagger crowd scenarios, viz. one where
all five systems were involved, the other
where subsets of cardinality 2 of these
crowds were re-combined.10
4 Results
As already described in Section 2.2, we performed
two types of experiments. In the first experiment
(Section 4.1), we intend to find proper calibrations
of parameters for an optimal SSC as described in
Section 3.5. In the second experiment (Section
4.2), we incorporate an extrinsic task, training an
NER classifier on different parameter settings, as
a selector for the optimal SSC.
10We refrained from also testing combinations of 3 and 4
systems due to time constraints.
4.1 Intrinsic Calibration of Parameters
Full Merger of All Taggers. In this scenario,
we tested the merged results of the entire crowd of
CALBC taggers when compared to the gold stan-
dard and determined their performance scores (see
Table 3). We will discuss the results with respect
to the overlapping F-score, if not explicitly stated
otherwise.
Looking at the results of the runs involving dif-
ferent cosine thresholds, we witness a systematic
drawback when more than two systems are re-
quired to agree. Although precision is boosted in
this setting, recall is decreasing strongly which re-
sults in overall lower F-scores. When only two
systems are required to agree a comparatively
higher recall comes at the cost of lower preci-
sion. Yet, the F-score (both under exact as well
as overlap conditions) is always superior (ranging
between 75% and 73%) when compared to the 3-
agreement scenario. Note that the 2-agreement
condition for the highest threshold being tested
yields, without exception, better scores than the
best single system (cf. Table 2).
The best performing run in terms of F-score for
the confidence method results from a threshold of
0.2 with an F-score of 76%. Note that this F-
score lies 4 percentage points above the best per-
formance of a single system (cf. Table 2).
A threshold of 0.2 with five contributing sys-
tems results in a union of all annotations. Conse-
quently, this run benefits from a high recall com-
pared with the other runs. However, the run ex-
hibits the lowest precision rating (both for the ex-
act and overlap condition), which is due to the low
threshold being chosen. As can also be seen with
the confidence method at a threshold of 0.80, a
very high precision can be reached (99%) but at
the cost of extremely low recall.11 The methods
performing best in terms of overlapping F-score
also perform best in terms of exact F-score.
Selected Tagger Combinations: Twin Taggers.
In this scenario, we evaluated all twin combina-
tions of taggers against the gold standard regard-
ing the confidence criterion. In Table 4 we contrast
the two best performing and the two worst per-
forming tagger pairs for the confidence method.
The table reveals that there are some cases where
the taggers seem to complement each other, e.g.,
the twins SYS-1 and SYS-3, as well as SYS-3 and
11Exactly these kinds of alternatives offer flexibility for
choosing the most appropriate SSC given a specific task.
239
exactR exactP exactF overlapR overlapP overlapF systems
0.55 0.74 0.63 0.63 0.84 0.72 SYS-1
0.36 0.53 0.43 0.46 0.68 0.55 SYS-2
0.48 0.77 0.59 0.59 0.95 0.72 SYS-3
0.44 0.83 0.58 0.49 0.91 0.64 SYS-4
0.34 0.61 0.44 0.41 0.74 0.53 SYS-5
Table 2: Performance of single systems (SYS-1 to SYS-5) as evaluated against the gold standard (best
performance scores in bold face). Measurements are taken both for exact as well as overlapping recall
(R), precision (P) and F-score (F).
method ACC exactR exactP exactF overlapR overlapP overlapF threshold agr. systems
cosine 0.94 0.53 0.71 0.61 0.66 0.87 0.75 0.70 2.00
cosine 0.93 0.40 0.79 0.53 0.49 0.96 0.65 0.70 3.00
cosine 0.94 0.54 0.71 0.61 0.65 0.87 0.74 0.80 2.00
cosine 0.93 0.41 0.80 0.54 0.48 0.95 0.64 0.80 3.00
cosine 0.94 0.54 0.72 0.62 0.65 0.86 0.74 0.90 2.00
cosine 0.93 0.41 0.81 0.54 0.48 0.95 0.64 0.90 3.00
cosine 0.94 0.54 0.73 0.62 0.64 0.86 0.74 0.95 2.00
cosine 0.93 0.41 0.83 0.55 0.47 0.95 0.63 0.95 3.00
cosine 0.94 0.55 0.75 0.64 0.64 0.86 0.73 0.97 2.00
cosine 0.93 0.42 0.85 0.56 0.47 0.95 0.63 0.97 3.00
confidence 0.95 0.58 0.73 0.65 0.68 0.85 0.76 0.20
confidence 0.94 0.44 0.83 0.58 0.50 0.94 0.66 0.40
confidence 0.93 0.32 0.88 0.47 0.35 0.97 0.52 0.60
confidence 0.91 0.16 0.91 0.27 0.17 0.99 0.30 0.80
Table 3: Merged annotations of the entire crowd of CALBC taggers (best performance scores per param-
eter setting in bold face). Parameters: threshold (confidence or cosine) and number of agreeing systems
(agr. systems).
SYS-4. In both cases, a confidence threshold of
0.2 yields the best F-score. Additionally, these F-
scores (81% and 78%) are even higher than the
single system?s F-scores (+9% up to +14%). This
comes with a significant increase in recall over
both systems (+13% to +28%) though at the cost
of lowered precision relative to the system with
the higher precision (?1% to ?10%). These re-
sults also outperform the best results of the exper-
imental runs where all systems were involved (see
Table 3). This indicates that a subset of all systems
might yield a better SSC than a combination of all
systems? outputs.
4.2 Extrinsic Calibration of Parameters
We employed a standard named entity tagger to as-
sess the impact of the different merging strategies
on a scenario near to a real-world application.12
12This tagger is based on Conditional Random Fields (Laf-
ferty et al, 2001) and employs a standard feature set used for
Each SSC variant (and thus each parameter com-
bination) was evaluated with this tagger in a 10-
fold cross validation. The SSC and the gold corpus
were split into ten parts of equal size. Nine parts of
the SSC constituted the training data of one cross
validation round, the corresponding tenth part of
the gold standard was used for evaluation. This
way, we tested how adequate a merged corpus was
with respect to the training of a classifier. Because
the cross validation has been very time consum-
ing, we did not consider specific combinations of
systems but always merged the annotations of all
five systems. The results are displayed in Table 5.
Interestingly, the highest recall, precision, and
F-score values (both for the exact and overlap con-
dition) are shared by the same parameter combi-
nations which also performed best in Section 4.1.
Hence, the use of a named entity tagger supports
the evaluation results when comparing the various
biomedical entity recognition (Settles, 2004).
240
ACC exactR exactP exactF overlapR overlapP overlapF systems threshold
0.95 0.62 0.69 0.65 0.76 0.85 0.81 SYS-1 + SYS-3 0.20
0.92 0.22 0.69 0.34 0.26 0.81 0.39 SYS-2 + SYS-5 0.60
0.95 0.55 0.75 0.63 0.67 0.91 0.78 SYS-3 + SYS-4 0.20
0.92 0.30 0.85 0.45 0.34 0.94 0.50 SYS-4 + SYS-5 0.60
Table 4: Twin pairs of taggers, contrasting the two best (in bold face) and the two worst performing pairs
obtained by the confidence method.
method ACC exactR exactP exactF overlapR overlapP overlapF threshold agr. systems
cosine 0.94 0.46 0.69 0.56 0.58 0.86 0.69 0.70 2.00
cosine 0.93 0.32 0.77 0.45 0.39 0.94 0.55 0.70 3.00
cosine 0.94 0.46 0.69 0.56 0.57 0.86 0.69 0.80 2.00
cosine 0.93 0.32 0.78 0.46 0.39 0.94 0.55 0.80 3.00
cosine 0.94 0.46 0.70 0.56 0.57 0.85 0.68 0.90 2.00
cosine 0.93 0.32 0.79 0.46 0.38 0.93 0.54 0.90 3.00
cosine 0.94 0.47 0.71 0.56 0.56 0.85 0.68 0.95 2.00
cosine 0.93 0.33 0.80 0.47 0.38 0.93 0.54 0.95 3.00
cosine 0.94 0.47 0.73 0.57 0.56 0.85 0.67 0.97 2.00
cosine 0.93 0.33 0.82 0.47 0.38 0.93 0.54 0.97 3.00
confidence 0.94 0.50 0.72 0.59 0.60 0.85 0.70 0.20
confidence 0.93 0.36 0.82 0.50 0.41 0.93 0.56 0.40
confidence 0.92 0.25 0.87 0.39 0.28 0.95 0.43 0.60
confidence 0.91 0.12 0.89 0.20 0.12 0.96 0.22 0.80
Table 5: Performance of an NER tagger trained on an SSC, 10-fold cross validation, and all systems.
Parameters: threshold (confidence or cosine) and number of agreeing systems (agr. systems).
SSCs directly to the gold standard corpus. How-
ever, this result may be due to our particular exper-
imental setting and should not be taken as a gen-
eral rule. Instead, this issue should be studied on
additional gold standard corpora (cf. Section 5).
5 Discussion and Conclusions
The experiments reported in this paper strengthen
the empirical basis of the novel idea of a silver
standard corpus (SSC). While the originators of
the SSC have come up with a fixed SSC, our ex-
periments show that different parametrizations of
SSCs allow to dynamically configure or select an
optimal one given a gold standard for comparison
during this optimization.
Our experimental data reveals that the boosting
hypothesis (the combination of several classifiers
outperforms weaker single ones in terms of perfor-
mance) is confirmed for complete mergers as well
as selected twin pairs of taggers. We also have
evidence that boosting within the SSC paradigm
tends to increase precision whereas it seems to de-
crease recall. This general observation becomes
stronger and stronger when the size of the commit-
tees (i.e., the number of submitting classifiers) in-
creases. It is also particularly interesting that both
the intrinsic evaluation (groups of classifiers vs.
gold standard), as well as the extrinsic evaluation
of SSCs (groups of classifiers trained and tested on
mutually exclusive partitions of the gold standard)
reveal parallel patterns in terms of performance ?
this indicates a surprising level of stability of the
entire SSC approach.
In our view, the strongest finding from our ex-
periments is the possibility to calibrate an SSC ac-
cording to requirements derived from the goal of
annotation campaigns. In particular, one can adapt
parameters to a specific use case, e.g., building a
corpus with high precision when compared to the
gold standard. Through the evaluation of the pa-
rameter space, one can assess the costs of reach-
ing a specific goal. For instance, a precision of
99% can be reached, yet at the cost of the F-score
plunging to 30%; only slightly lowering the preci-
sion to 97% boosts the F-score by 22 points (see
last two rows in Table 3).
241
Also, when increasingly more annotation sets
become available (e.g., through the CALBC chal-
lenges) the problem of adversarial or extremely
bad performing systems is no longer a pressing is-
sue since with the optimization approach such sys-
tems are automatically sorted out when optimizing
over the set of possible system combinations.
While our experiments are but a first step to-
wards the consolidation of the SSC paradigm
some obvious limitations of our work have to be
overcome:
? experiments with different gold standards
have to be run as one might hypothesize that
different gold standards require different pa-
rameter settings for the optimal SSC,
? experiments with different NER taggers have
to be run (e.g., we plan to use an NER tag-
ger which prefers recall over precision, while
the one used for these experiments generally
yields higher precision than recall scores),
? test with crowds of taggers which generate
higher recall than precision.13
In our approach, a gold standard is needed to
find good parameters to build an SSC. A ques-
tion not addressed so far is how huge such a gold
standard must be to offer an appropriate size for
the optimization step. Finally, it might be particu-
larly rewarding to join efforts in reducing the de-
velopment costs for such a gold standards ? Active
Learning (e.g., Tomanek et al (2007a)) might be
one promising approach to break this bottleneck.
Since effective calibration of SSCs is in need of
reasonably sized and densely populated gold stan-
dards, by combining these lines of research we
claim that additional benefits for SSCs become vi-
able.
6 Acknowledgments
We wish to thank Kerstin Hornbostel for stim-
ulating and corrective remarks on the biological
grounding of this investigation. This research was
partially funded by the EC?s 7th Framework Pro-
gramme within the CALBC project (FP7-231727)
and the GERONTOSYS research initiative from the
13We used a gold standard in which some unusual entities
(e.g., protein families) had been annotated for which most
named entity taggers have not been trained. This might also
explain the generally overall low recall among the crowd of
taggers yielded in our experiments.
German Federal Ministry of Education and Re-
search (BMBF) under grant 0315581D within the
JENAGE project.
References
Olivier Bodenreider and Alexa T. McCray. 2003. Ex-
ploring semantic groups through visual approaches.
Journal of Biomedical Informatics, 36(6):414?432.
Yoav Freund. 1990. Boosting a weak learning algo-
rithm by majority. In COLT?90 ? Proceedings of the
3rd Annual Workshop on Computational Learning
Theory, pages 202?216.
John D. Lafferty, Andrew McCallum, and Fernando
Pereira. 2001. Conditional random fields: Prob-
abilistic models for segmenting and labeling se-
quence data. In ICML?01 ? Proceedings of the
18th International Conference on Machine Learn-
ing, pages 282?289.
Dietrich Rebholz-Schuhmann, Antonio Jose? Jimeno
Yepes, Erik van Mulligen, Ning Kang, Jan Kors,
David Milward, Peter Corbett, Ekaterina Buyko,
Elena Beisswanger, and Udo Hahn. 2010a. CALBC
Silver Standard Corpus. Journal of Bioinformatics
and Computational Biology, 8:163?179.
Dietrich Rebholz-Schuhmann, Antonio Jose? Jimeno
Yepes, Erik M. van Mulligen, Ning Kang, Jan
Kors, Peter Milward, David Corbett, Ekaterina
Buyko, Katrin Tomanek, Elena Beisswanger, and
Udo Hahn. 2010b. The CALBC Silver Standard
Corpus for biomedical named entities: A study in
harmonizing the contributions from four indepen-
dent named entity taggers. In LREC 2010 ? Pro-
ceedings of the 7th International Conference on
Language Resources and Evaluation.
Burr Settles. 2004. Biomedical named entity recog-
nition using conditional random fields and rich fea-
ture sets. In NLPBA/BioNLP 2004 ? COLING
2004 International Joint Workshop on Natural Lan-
guage Processing in Biomedicine and its Applica-
tions, pages 107?110.
Katrin Tomanek, Joachim Wermter, and Udo Hahn.
2007a. An approach to text corpus construction
which cuts annotation costs and maintains corpus
reusability of annotated data. In EMNLP-CoNLL?07
? Proceedings of the 2007 Joint Conference on Em-
pirical Methods in Natural Language Processing
and Computational Language Learning, pages 486?
495.
Katrin Tomanek, Joachim Wermter, and Udo Hahn.
2007b. A reappraisal of sentence and token splitting
for life sciences documents. In K. A. Kuhn, J. R.
Warren, and T. Y. Leong, editors, MEDINFO?07 ?
Proceedings of the 12th World Congress on Medical
Informatics, number 129 in Studies in Health Tech-
nology and Informatics, pages 524?528. IOS Press.
242
