A Very Large Dictionary with Paradigmatic, Syntagmatic, and Paronymic Links between Entries 
Igor A. Bolshakov and Alexander Gelbukh Center for Computing Research, National Polytechnic Institute, 07738 DF, Mexico igor@cic.ipn.mx, gelbukh|@|gelbukh.|com; www.gelbukh|.com 
 
Abstract 
A very large Russian dictionary is described. It contains currently 3.6 million links between its 120,000 entries. The links are syntagmatic (collocations), paradigmatic (WordNet-like), or paronymic (words similar in letters or in morphs). The entries of the dictionary are sin-gle- or multiwords belonging to four main POS. The entries represent so-called gram-memes rather than lexemes: e.g., nouns are represented as singular and plural; verbs are split into ?finite forms + infinitive?, ?participle?, and ?gerund?. The multiword entries in turn can be collocations?idiomatic free?whose parts are also entries of the same dictionary. 
1 Introduction 
The entries of modern dictionaries are usually lex-emes, and semantic relations between lexemes are only morphological derivates of the entry. E.g., Webster?s Universal College Dictionary of English gives with manufacture also manufacturable, manufactural, and manufacturer, having no entry. A rather specialized exception is WordNet (Fell-baum, 1998). It gives paradigmatic semantic links: synonymous (common gloss), antonymous (com-mon presupposed meaning, opposite assertion), XPOS links (same meaning, different POS), etc.  Other exceptions are BBI (Benson et al, 1986) and much vaster OCDSE (OCDSE, 2003), both existing only in printed form. They give colloca-tions, i.e. stable and idiomatic word combinations connected by both syntactic (immediate or through auxiliary words) and semantic links, specifically of syntagmatic type (e.g., a verb and its valence fillers, a noun with its modifiers, etc.). The entries are usually single words, whereas general linguistics also suggests multiwords, which may be both inseparable idiomatic concepts like hot dog or point of view and closely tied and fre-quently used terms like routs of communications.   This paper shortly outlines a very large elec-tronic dictionary with the following main features: 
? It adheres to the grammemic principle of selec-tion of entries. Grammemes are subsets of mor-
phological paradigms of lexemes, i.e. they are between lexemes and wordforms. E.g., noun lexemes are represented by two grammemes: singular and plural; verb lexemes are split into finite forms + infinitive, participle, and gerund. 
? Its entries are only content (semantically autonomous) single and multiwords; the latter can be additionally represented by links be-tween their components (this can be called de-composition principle of entry formation). Auxiliary words are stored as an auxiliary part of the dictionary, e.g., as a list of prepositions. 
? It includes links of three vast classes: semantic syntagmatic (as in OCDSE), semantic para-digmatic (as in WordNet), and paronymic. The latter class is new in dictionaries: it connects entries similar in letters (e.g., sigh vs. sign) or in morphs (e.g., sensible vs. sensitive).  The dictionary is the result of CrossLexica pro-ject (Bolshakov, 2004; Bolshakov and Gelbukh, 2000) and is mainly oriented to collocations.  Specifically, the objectives of this paper are: 
? To describe possible options and features for the dictionary entries; 
? To characterize types of the links: syntagmatic, paradigmatic, and paronymic; 
? To prove language independence of the CrossLexica structure; 
? To statistically characterize CrossLexica dic-tionary in its present state. We use English examples for the illustrations. 
2 Types and Features of Entries 
Wordforms are grouped together into dictionary entries basing on several important features: Parts of speech It can be any of the four main POS. The POS is defined according to the syntac-tic role: participles are considered adjectival; a prepositional phrase can be adjectival or adverbial, e.g., in substance is adjectival in unconstitutional-ity in substance (? ?substantial?) and adverbial in to verify in substance (? ?substantially?). We consider such two-functional entries homonymous. Grammemes For a Russian noun, the singular and plural have their own collocations. Printed dic-
tionaries mark this as mainly plural or the like. So we split the morphoparadigm of a noun to singular and plural, calling such sub-paradigms grammemes. Based on their syntactic roles, we divide mor-phoparadigms of verbs into the grammemes of par-ticiples (adjectival), gerunds (adverbial), and per-sonal forms plus infinitives (predicates). Russian verbs have two aspects differing in their combinability: the perfect tends to collocate with singular nouns, the imperfect being indifferent to number; the perfect is usually modified with ?con-centrated? adverbials like suddenly, at once or straightway, the imperfect preferring ?spread? ad-verbials like gradually, continuously or repeatedly. So we split verbs into aspectual grammemes. Homonyms We consider various homonyms separately. Their combinatorial differences are es-pecially useful for word sense disambiguation. Idioms Idiomatic collocations like point of view are entries, since combinability of an idiom is always different from that of its head. Their com-ponents can, though, be also entries on their own. Multiwords If a non-idiomatic multiword has a single-word synonym, we treat it as an entry, since its combinability differs from that of its head. E.g., Rus. puti soob??enija ?routes of communica-tions? has a synonym kommunikacii. Cf. a similar problem in EuroWordNet (Vossen, 2000). Absolute synonyms, abbreviations, and mor-phological variants Absolute synonyms (sofa = settee) are very rare in any language, but there are other types of equivalence: abbreviations (United States of America = USA = United States) and the so-called morphological variants (e.g., Rus. nul? = nol? ?zero? or mu?at? = mu?it? ?to torture?). Since all their collocations are the same, we store them as one entry, selecting one of them as a representative. Paste-ups Many Russian noun-headed con-cepts are used in two equivalent forms: (1) a bi-gram consisting of a modifier with the stem S1 plus its head noun with the stem S2, or (2) a single noun containing the stems S1 and S2, or their initial parts, or only S1: elektri?eskij tok ?electrical current? = elektrotok; fizi?eskij fakul?tet ?physical faculty? = fizfak; komi?eskij akter ?comical actor? = komik. The number of the paste-ups grows, especially in the newswire and everyday speech, but in diction-aries they are scarce. Our dictionary stores about three thousand of them in both forms.  Compound pairs Russian has numerous sta-ble pairs of nouns separated by a dash, usually with both parts declinable in parallel: strana-u?astnica ?participant country?, let?ik-ispytatel? ?test pilot?, zavod-izgotovitel? ?manufacturing plant?. A com-pound pair is considered an entry. Coordinated pairs Dependency links within multiwords can be of stable coordinative type: 
mother and father, safe and sound, sooner or later. We consider such pairs as both collocations (with syntagmatic links) and separate entries. E.g., each bracketed item of the term [[[probability] [theory]] and [[mathematical] [statistics]]] is an entry. Synonyms, hyperonyms/hyponyms, and anto-nyms These are semantically paradigmatic links. We take their participants as entries. Proper names We consider as entries those names that are a part of everyday life and encyclo-paedic knowledge: names of geographic objects, countries, famous persons, large organizations, etc. They are linked to their hyperonyms: country, mountains, island, writer, organization, etc. Semantic derivates These are lexemes of any POS with same basic meaning, e.g., to marry, mar-riage, bride, bridegroom, and matrimonial (XPOS in WordNet). We take such words as entries. Idiomaticity in general All complete idioms are included as collocations, e.g., sest? | v galo?u ?to get | into a fix?, lit. ?to sit | into a galosh?. In rarer cases of tripartite idioms the dichotomy was merely a practical step; e.g. in byt? | bez carja v golove ?to be stupid?, lit. ?to be | without the Tsar in one?s head?, we regard the right part as a modi-fier. Two marks are used: idiom and possible id-iom, the latter for collocations with both figurative and direct senses, e.g., sest? v lu?u means ?to get into a mess? or ?to sit down into a puddle?. Usage marks Special, bookish or obsolete: the use in writing is recommended if the meaning is clear to the writer; colloquial: the use in official writing is not recommended; vulgar: both written and oral use are prohibited; and incorrect: used sometimes but contradicts language norms.  
3 Types of Syntagmatic Links 
We define a collocation as a syntactically con-nected and semantically compatible pair of content words, like full-length dress, well expressed, to briefly expose, to pick up the knife or to listen to the radio (collocation components are underlined). Syntactical connectedness is understood as in dependency grammars (Mel??uk, 1995) (maybe through an auxiliary word), not as co-occurrence (Bentivogli and Pianta, 2002); the components can be distant in the sentence. We consider colloca-tions from absolutely free to purely idiomatic. The following are collocation types. Modifiers These are modifying or attributive components: great ? country; man ? of letters; eat ? quickly; enormously  ? big; very ? well. Verbs with their subjects The subject is a specific dependent of a predicate verb: soldier ? died; bus ? arrives. A specifically Russian type of the subject-to-predicate link is a predicate contain-
ing the copula byt? ?to be? (omitted in Russian in present tense) and an adjectival in short form: god 
? zaver?en (participle) ?the year is over?; vek ? korotok (adjective) ?the lifetime is short?. Verbs with their noun complements Noun complements of a verb are all nouns dependent on the verb as direct, indirect, or prepositional object: to read a book; to strive for peace. We also con-sider as complements circumstantial phrases like to travel by train. A word can have several comple-ments; each collocation reflects one of them, while the omission of other obligatory complement(s) is marked with the ellipsis: to give ... to the boy. Nouns / adjectivals with their noun comple-ments All POS can have noun complements, e.g., nouns the capital of the country, the struggle against poverty; adjectives blind with rage, men-tioned by the observer or going to the cinema. Verbs / nouns / adjectivals with their infini-tive complements E.g. to stop to talk or to per-mit to enter; permission to enter or cream to pro-tect; forced to return or ready to appeal.  Adverbials with their infinitive complements These are purely Russian collocations: xolodno idti ?it is cold to go?, lit. ?coldly to go?; reshiv (gerund) idti ?after having decided to go?. They are possible only with some predicative adverbs or gerunds.  Adverbials with noun complements Purely Russian: xolodno (adverb) bez pal?to ?it is cold without a coat?, lit. ?coldly without a coat?; poby-vav (gerund) v centre ?after visiting the center?.  Verbs / adjectivals with their adjectival com-plements E.g., to remain silent or to consider... stupid; remaining silent or considering ... stupid.  Coordinated pairs E.g. mom and dad, safe and sound, or sooner or later, cf. (Bolshakov et al, 2003b) for details. 
4 Types of Paradigmatic Links 
These semantic (WordNet-like) links are: Synonyms Unlike WordNet synsets, our syn-onymy groups have a dominant member and may include member(s) marked as its absolute syno-nyms. Synonyms can be periphrastic multiwords or even short definitions: to help ? to give help; fall ? quick descent; suffocation ? lack of fresh air. Non-absolute synonyms can be used for heuristic infer-ences of new collocations from those existing in the dictionary (Bolshakov and Gelbukh, 2002a). Hyponyms vs. hyperonyms Hyperonyms are also used for such inferences. Antonyms Together with standard antonyms (good?bad, vanguard?rearguard), we consider opposite notions: missiles?antimissiles. Meronyms vs. holonyms E.g. finger?hand, motor?car. 
Semantic derivates They connect parts of the morphoparadigms split into grammemes. Also, they describe the same idea from various aspects, thus compensating for the lack of glosses. 
5 Types of Paronymic Links 
The types of such links are as follows: Literal paronyms They are at the distance of few editing operations (replacement, omission, insertion, permutation of adjacent letters) from each other. E.g., for sign: sigh, sin, sing. They are useful, e.g., to correct the malapropisms (Bolsha-kov and Gelbukh, 2003a). Morphemic paronyms They are of the same POS and radix but have different prefixes and/or suffixes, e.g., sens-ation-al, sens-ible, in-sens-ible, sens-itive, sens-less, sens-ual. Foreigners? mala-propisms are often confusion of morphemic paro-nyms, so that we can immediately propose candi-dates for correcting such errors. 
Auxiliary parts of CrossLexica is a Russian-English-Russian dictionary (e.g., by two English words, the user can find a fluent Russian colloca-tion), and a generator of all morphological forms. 
6 Interlingual Structural Universality 
The system operates with two main data structures: a list of entries and a set of links between them. An entry contains a list of its morphological categories. This structure is language-independent. The specific links between entries can, however, be language specific. Let us outline grammatical peculiarities of Russian that influence these links. Nouns and adjectivals declinable In English this problem does not exist.  Too few tenses Russian verbs have only three tenses, whereas English has many. No articles For other languages, it is important to specify the forms of articles in collocations. Nouns cannot modify nouns In English the collocations like book review are quite common. A special attributive type of syntagmatic links should be introduced for such English collocations. Thus the Cross-Lexica structure is (almost) lin-guistically universal. 
7 CrossLexica Statistics and Some Discussion 
As of April 2004, the dictionary contains more than 120,000 entries. Collocations are divided into three classes: primary, secondary, and inferred.  The primary collocations are collected manually. The secondary collocations result from automatic morphological transformations of the primary ones. For example, verbs with their noun complements are transformed into adjectivals with their noun 
complements, e.g., to participate in the meeting gives participating in the meeting. Table 1 shows the statistics of the collocations. 
Type of collocations Primary Secondary Words with modifiers 281,000 99,500 Verbs + subjects 106,400 27,500 Verbs + noun complements 204,900  Nouns + noun complements 129,000  Adjectivals w/ noun compl.  13,800       173,300 Adverbials w/ noun compl. 100 148,900 Other types 29,100  Total 764,300 449,200 
Table 1: Statistics of collocations 
The inferences are performed with constraints (Bolshakov and Gelbukh, 2002a), e.g., the source collocation cannot be an idiom, to avoid the infer-ence like (hot dog)idiom & (poodle IS_A dog) ? *(hot poodle). The total of the inferred collocations never exceeded 6 to 8% of the primaries and is declining because the rare species are getting a full description within the primaries. In Table 2, other link statistics are given. All links are counted, e.g., n antonyms pairs give 2n unilateral links, and a group of n synonyms gives n(n?1)/2. The total is more than 1.2 million. Thus, the total of links of the three classes is 3.6 million.  No. Type of links Amt. of links 1 Semantic derivates 821,600 2 Synonyms 226,200 3 Meronyms vs. holonyms 20,800 4 Hyponyms vs. hyperonyms 13,100 5 Antonyms 10,500 6 Morphemic paronyms 86,600 7 Literal paronyms 24,200  Total 1,203,000 
Table 2: Statistics of paradigmatic/paronymic links 
8 Conclusion 
A dictionary of a new type was developed. Its main features are: 
? Entries belong only to nouns, verbs, adjectives and adverbs; they are grammemes (i.e. parts of lexemes) and can be multiwords. 
? The links between entries are of semantic (syn-tagmatic or paradigmatic) or paronymic class.  
? Its structure is interlingually universal.  Such dictionaries have a vast specter of applica-tions: language learning (Bolshakov and Gelbukh, 2002b), word processing, syntactic analysis, word sense disambiguation; semantic error detection and correction (2003a); text translation (2001b), gen-
eration and segmentation (2001a); revealing text cohesion (Gelbukh et al, 2000), steganography. 
9 Acknowledgements 
Work was done under partial support of Mexican Government (CONACyT, SNI) and CGPI-IPN. 
References  
M. Benson, E. Benson, R. Ilson. 1986. The BBI Combinatory Dictionary of English, JBP. L. Bentivogli, E. Pianta. 2002. Detecting Hidden Multiwords in Bilingual Dictionaries.  EURALEX-2002, p. 14?17. I. A. Bolshakov. 2004. Getting One?s First Mil-lion? Collocations. CICLing-2004. LNCS 2945:229?242, Springer. I. A. Bolshakov, A. Gelbukh. 2000. A Very Large Database of Collocations and Semantic Links. NLDB-2000. LNCS 1959:103?114, Springer. I. A. Bolshakov, A. Gelbukh. 2001a. Text segmen-tation into paragraphs based on local text cohe-sion. TSD-2001. LNAI 2166:158?166, Springer. I. A. Bolshakov, A. Gelbukh. 2001b. A Large Da-tabase of Collocations and Semantic Refer-ences: Interlingual Applications. International J. of Translation 13(1-2):167?187. I. A. Bolshakov, A. Gelbukh. 2002a. Heuristics-Based Replenishment of Collocation Databases. PorTAL-2002. LNAI 2389:25?32, Springer. I. A. Bolshakov, A. Gelbukh. 2002b. Ense?ando idiomas extranjeros con una base de coloca-ciones. In: La telem?tica y su aplicaci?n en la educaci?n a distancia y en la informatizaci?n de la sociedad, p. 632?638. F?lix Varela. I. A. Bolshakov, A. Gelbukh. 2003a. On Detection of Malapropisms by Multistage Collocation Testing. NLDB-2003. LNI 41:28?41. Bonner K?llen. I. A. Bolshakov, A. Gelbukh, S. Galicia-Haro. 2003b. Stable Coordinated Pairs in Text Proc-essing. TSD-2003. LNAI 2807:27?34, Springer. A. Gelbukh, G. Sidorov, I. A. Bolshakov. 2000. Dictionary-based Method for Coherence Main-tenance in Man-Machine Dialogue with Indirect Antecedents and Ellipses. TSD-2000. LNAI 1902:357?362, Springer.  Ch. Fellbaum (Ed.). 1998. WordNet: An Electronic Lexical Database. MIT Press. I. Mel??uk. 1995. Phrasemes in Language and Phraseology in Linguistics. In: Idioms: Struc-tural and Psychological Perspectives, p. 169?252. Lawrence Erlbaum Publ., UK, 1995. OCDSE. 2003. Oxford Collocations Dictionary for Students of English. Oxford University Press. P. Vossen (ed.). 2000. EuroWordNet General Do-cument. Ver. 3, www.hum.uva.nl/~ewn. 
 
Proceedings of the ACL 2014 Student Research Workshop, pages 78?85,
Baltimore, Maryland USA, June 22-27 2014.
c
?2014 Association for Computational Linguistics
Open Information Extraction for Spanish Language
based on Syntactic Constraints
Alisa Zhila
Centro de Investigaci?on
en Computaci?on,
Instituto Polit?ecnico Nacional,
07738, Mexico City, Mexico
alisa.zhila@gmail.com
Alexander Gelbukh
Centro de Investigaci?on
en Computaci?on,
Instituto Polit?ecnico Nacional,
07738, Mexico City, Mexico
gelbukh@gelbukh.com
Abstract
Open Information Extraction (Open IE)
serves for the analysis of vast amounts of
texts by extraction of assertions, or rela-
tions, in the form of tuples ?argument 1;
relation; argument 2?. Various approaches
to Open IE have been designed to per-
form in a fast, unsupervised manner. All
of them require language specific infor-
mation for their implementation. In this
work, we introduce an approach to Open
IE based on syntactic constraints over POS
tag sequences targeted at Spanish lan-
guage. We describe the rules specific for
Spanish language constructions and their
implementation in EXTRHECH, an Open
IE system for Spanish. We also discuss
language-specific issues of implementa-
tion. We compare EXTRHECH?s perfor-
mance with that of REVERB, a similar
Open IE system for English, on a paral-
lel dataset and show that these systems
perform at a very similar level. We also
compare EXTRHECH?s performance on a
dataset of grammatically correct sentences
against its performance on a dataset of ran-
dom texts extracted from the Web, drasti-
cally different in their quality from the first
dataset. The latter experiment shows ro-
bustness of EXTRHECH on texts from the
Web.
1 Introduction
Open IE is a rapidly developing area in text pro-
cessing, with its own applications and approaches
that are different from traditional IE (Etzioni et
al., 2008; Banko and Etzioni, 2008; Etzioni,
2011). Unlike traditional IE, where systems are
targeted at extraction of instances of particular re-
lations with arguments restricted to certain seman-
tic classes, e.g., to be born in(HUMAN; LOCA-
TION), Open IE serves for extraction of all pos-
sible relations with arbitrary arguments. For ex-
ample, in ?Woman who drove van full of kids is
charged with attempted murder? two relations can
be identified: ?Woman; drove; van full of kids? and
?Woman; is charged with; attempted murder?.
The ability to extract arbitrary relations from
text allows applications of Open IE that are not
possible in the frame of traditional IE. Among
them are fact extraction at sentence level (e.g.,
?Mozart; was born in; Salzburg?), new perspective
on search as question answering (e.g., Where was
Mozart born?) (Etzioni, 2011), or assessment of
the quality of text documents at Web scale (Horn
et al., 2013). Additionally, the output of Open IE
systems can serve for ontology population (Soder-
land et al., 2010) and acquisition of common sense
knowledge (Lin et al., 2010).
Although all Open IE systems are targeted at the
extraction of arbitrary relations, the approaches to
this task vary significantly. The pilot approach
suggested by Banko et al. (2007) is based on
semi-supervised learning of general relation pat-
terns that then serve for extraction of arbitrary
relations. However, the output of such systems
contains many incoherent and inconsistent extrac-
tions, and the training stage is quite computation-
ally complex. Fader et al. (2011) suggested an-
other approach where syntactic and lexical con-
straints were applied over POS-tagged input. This
approach has proven to be robust and fast enough
for relation extraction at Web scale.
Although Open IE is targeted at extraction of
arbitrary relations without any semantic restric-
tions, all approaches have strong language de-
pendent restrictions and require language spe-
cific information to be introduced in the corre-
sponding systems. For Spanish language, the
apporach based on rules over dependency trees
has been implemented both using full parsing
78
(Aguilar-Galicia, 2012) and using shallow depen-
dency parsing (Gamallo et al., 2012). The for-
mer work shows that this approach is too com-
putationally costly and is not always robust even
on grammatically correct texts. The latter work
does not report any results for Spanish language or
discusses any details specific to implementations
for languages other than English. Further, we are
not aware of any existing research on whether the
approach based on syntactic constraints over POS
tags can be generalized to other languages. Ad-
ditionally, although Open IE is claimed to be use-
ful for information extraction from the Web, we
are not aware of any research on its applicability
to texts randomly extracted from the Internet, i.e.,
those that have not been verified for grammatical
correctness by peers or editors.
In this paper we discuss Open IE based on syn-
tactic constraints over POS tag sequences, aimed
at Spanish language. We describe its implemen-
tation and introduce EXTRHECH, an Open IE sys-
tem for Spanish. We also compare its performance
with that of REVERB (Fader et al., 2011) on a
parallel dataset. Additionally, we evaluate perfor-
mance of our system over a dataset of texts ran-
domly extracted from the Internet and discuss the
issues that arise when processing random Internet
texts. We also give a brief analysis of errors.
The paper is organized as follows. Related work
is reviewed in Section 2. Section 3 presents our
approach to Open IE for Spanish and describes
the EXTRHECH system. Section 4 describes the
experiments for a parallel English-Spanish dataset
and for a Spanish dataset of texts randomly ex-
tracted from the Internet. In Section 5, a brief
analysis of errors is presented. Section 6 draws
the conclusions and outlines future work.
2 Related Work
There exist several approaches to Open IE.
Chronologically the first one was introduced
in the pilot works on Open IE by Banko et al.
(2007) and Etzioni et al. (2008). Their approach is
based on semi-supervised machine learning prin-
ciples and includes three main steps: (1) man-
ual labeling of a training corpus for seed relation
phrases and features; (2) further semi-supervised
learning of relations; (3) automatic extractions of
relations and their arguments. This approach is
implemented in TEXTRUNNER (Banko and Et-
zioni, 2008), WOE
pos
, and WOE
parse
, both (Wu
and Weld, 2010). In these systems, the detection
of a relation triple starts from the potential argu-
ments expressed as noun phrases, i.e., before the
connecting relation phrase is detected. Once de-
tected, neither the argument phrases nor the rela-
tion phrase can be backtracked, which makes the
approach prone to incoherent and uninformative
extractions. For example, in ?to make a deal with?,
deal can be erroneously extracted as an argument,
although it is a part of the relation phrase.
The group of rule-based approaches includes
systems based on rules applied over linguisti-
cally annotated texts. FES-2012 system (Aguilar-
Galicia, 2012) applies rules to the fully parsed sen-
tences. However, in the same work the authors
show that this approach is too slow to be scaled to
a Web-sized corpus and that it is not robust. An-
other system implementing rule-based approach is
DEPOE (Gamallo et al., 2012). In this system, the
rules are applied to the output of shallow depen-
dency parsing. In REVERB system (Fader et al.,
2011), syntactic constraints are applied over POS
tags and syntactic chunks. The last two systems
show better results in terms of precision/recall and
speed, and, consequently, scalability to a Web-
sized corpus.
Finally, the approach based on the deep au-
tomatic linguistic analysis is implemented in
OLLIE (Mausam et al., 2012). This system com-
bines various approaches: it uses output of a rule-
based Open IE system to bootstrap learning of the
relation patterns and then additionally applies lex-
ical and semantic patterns to extract relations that
are not expessed through verb phrases. Such a
complex approach leads to high-precision results
with a high yield. However, there is a tradeoff be-
tween accuracy of the output and cost of imple-
mentation and computation and complexity of the
training stage.
All these approaches require language-
dependent information for their implementation.
The third approach directly uses lexical infor-
mation for the context analysis. The other two
approaches employ language-specific morpholog-
ical and syntactic information. Of the described
systems, only two have been implemented for
languages other than English. FES-2012 system
is implemented for Spanish language; however,
its use of the full syntactic parsing does not scale
to a Web-sized corpus. DEPOE system, based on
rules over shallow dependency parsing, is claimed
79
to have its variants for Spanish, Portuguese, and
Galician languages (Gamallo et al., 2012). How-
ever, the authors do not report any experimental
results on languages other than English or any
language-specific details.
The approach based on syntactic constraints
over POS tags has not been applied to languages
other than English, in spite of that this method
can be easily adapted to other languages because it
only requires a reliable POS tagger. The basic al-
gorithm for relation extraction, according to Fader
et al. (2011), is as follows:
? First, search for a verb-containing relation
phrase in a sentence;
? If detected, search for a noun phrase to the
left of the relation phrase;
? If a noun phrase detected, search for another
noun phrase to the right of the relation phrase.
Additionally, the experiments for Open IE sys-
tems have been conducted only on texts that came
from verified sources, i.e., Wikipedia, news, or
textbooks (Banko and Etzioni, 2008; Fader et al.,
2011; Mausam et al., 2012). However, Open IE is
meant to work with Web text data that may come
from any source including those that have not been
edited or verified for grammar errors.
3 System Description
In this section we introduce EXTRHECH,
1
a sys-
tem for Open IE in Spanish. It takes a POS-tagged
text as input, applies syntactic constraints over se-
quences of POS-tags, and returns a list of extracted
relations as triples ?argument 1; relation; argu-
ment 2? that correspond to each sentence.
3.1 Basic Processing
The system takes as input a POS-tagged text. In
our experiemnts, we used a morphological ana-
lyzer from Freeling-2.2 (Padr?o et al., 2010). For
Spanish language, it returns POS tags accoridng to
EAGLES POS tag set (Leech and Wilson, 1999).
Consequently, our system is designed to work with
this POS tag set.
Spanish uses a number of non-ASCII charac-
ters, such as ?a, ?e, ?n, etc. These characters can
come in different encodings. To be able to cor-
rectly analyze text with these characters, Freeling
1
All materials are available on the page
http://www.gelbukh.com/resources/
spanish-open-fact-extraction.
analyzer should receive the input in ISO encod-
ing. Thus, the input text needs an additional pre-
processing stage to be converted into this encod-
ing. Though this might look as a minor technical
issue, guessing the original encoding becomes a
significant problem when working with texts from
arbitrary sources on the Web. We discuss encoding
related issues in Section 4.2.
After the text has been properly POS-tagged,
we feed it into EXTRHECH system, which ap-
plies the fact extraction algorithm described in
Section 2 to each sentence, one sentence at a time.
We use the same basic algorithm as in (Fader et
al., 2011) but with different triple matching rules
as appropriate for Spanish grammar.
The original POS-tag sequences for English
would produce nonsense results on Spanish input
due to substantial difference in grammars: infini-
tives are not preceded by ?to?, adjectives usually
follow nouns, and oblique case pronouns precede
verbs instead of following them, just to name a few
peculiarities of Spanish.
First, the system looks for a verb-containing
phrase in a sentence by matching it against the fol-
lowing expression:
VREL? (V W* P) | (V),
where V stands either for a single verb optionally
preceded by a reflexive pronoun (se realizaron,
?were carried out?), or a participle (calificado,
?qualified?). V W* P matches a verb with depen-
dent words, where W stands for either a noun, an
adjective, an adverb, a pronoun, or an article, and
P stands either for a preposition optionally imme-
diately followed by an infinitive, or for a gerund
(sigue siendo, ?continues to be?). The symbol
* denotes zero or more matches. Here and fur-
ther, the whole match is referred to as verb phrase
(though it is not a verb phrase in linguistic sense).
After detecting a verb phrase, EXTRHECH
looks for a noun phrase to the left from the be-
ginning of the verb phrase. This noun phrase is a
potential first argument of the relation. If a match
is found, then the system looks for another noun
phrase to the right from the end of the verb phrase.
The noun on the right side is treated as the second
argument.
Noun phrases are searched for with the follow-
ing regular expression:
NP? Np (PREP Np)?,
where Np matches a noun optionally preceded by
either an article (la din?amica, ?the dynamics?),
80
an adjective, an ordinal number (los primeros
ganadores, ?the first winners?), a number (3 casas,
?3 houses?), or their combination, and optionally
followed by either a single adjective (un esfuerzo
criminal, ?a criminal effort?), a single participle,
or both (los documentos escritos antiguos, ?the
ancient written documents?). The whole expres-
sion matched by Np can be preceded by an indef-
inite determinant construction, e.g., uno de, ?one
of ?. PREP matches a single preposition. Hence,
an entire noun phrase is either a single noun with
optional modifiers or a noun with optional modi-
fiers followed by a prepositional phrase that is a
preposition and another noun with its correspond-
ing optional modifiers (una larga lista de proble-
mas actuales, ?a long list of current problems?).
The symbol ? denotes 0 or 1 matches.
If noun phrases are matched on both sides of the
verb phrase, all three components are considered
to represent a relation and are extracted in the form
of a triple.
As an output unit, EXTRHECH returns a triple
consisting of ?argument 1; relation; argument 2?,
where argument 1 semantically is, e.g., an agent
or experiencer of the relation and argument 2 is a
general object or circumstance of the relation.
3.2 Additional Processing
Above we described the core rules and the basic
sequence for relation extraction. In addition to
them, we also implemented several optional rules
for processing of certain language constructions
that can be turned on and off with the input pa-
rameters.
First, participle clauses that follow a noun can
be searched for a relational triple if they terminate
with a noun. For example, from a phrase
Precios del caf?e suministrados por la OIC
(?Coffee prices provided by International Coffee
Organization?)
EXTRHECH returns the relation:
?Precios del caf?e; suministrados por; la OIC?.
Second, EXTRHECH also approaches resolu-
tion of coordinating conjunctions between verb
phrases and between noun phrases into corre-
sponding separate relations. Here follows the ex-
ample of a sentence with a coordinating conjunc-
tion between verb phrases:
El cerebro almacena enormes cantidades de informaci?on y
realiza millones de actividades todos los d??as
(?The brain stores vast amounts of information and performs
millions of activities every day?)
. Two facts are detected:
?El cerebro; almacena enormes cantidades de; informaci?on?
and
?El cerebro; realiza millones de; actividades todos los d??as?.
Third, relative clauses introduced by single rel-
ative pronouns (e.g., que (?that?, ?who?), cual
(?which?)) as in las partes que conforman un
trabajo de investigaci?on (?parts that make up a
research work?) are also searched for relations.
However, relative pronoun phrases with preposi-
tions, e.g. en el cual (?in which?) are not taken into
consideration for relation extraction due to their
coreferential complexity.
3.3 Limitations
The implementation of basic processing per-
formed by EXTRHECH system follows the algo-
rithm introduced in (Fader et al., 2011). This
means that extracted facts are limited to the rela-
tions expressed through a verb phrase. This limi-
tation is discussed in the cited paper.
In our apporach to Open IE in Spanish, we do
not allow pronouns to be potential arguments of
a relation. It was mainly done because of a wide
use of a neutral pronoun lo (?this?, ?which?, or no
direct translation) as a head of relative clauses in
Spanish language, e.g., lo que dio valor al poder
judicial (? that gave value to the judiciary?). In-
cluding pronouns for potential argument matches
would return a lot of uninformative relations as
?lo; dio valor a; el poder judicial?. This issue can
be solved only by introducing anaphora resolution
techniques which involves processing on a super-
sentence level. Although seemingly feasible, this
modification will necessarily slow down the ex-
traction speed which is critical while working with
large scale corpora. As mentioned in Section 2,
high speed performance is one of the main advan-
tages of the approach to Open IE based on syntac-
tis constraints compared to the others. Hence, any
modifications that would affect its speed should be
considered with caution.
Another language dependent limitation is re-
lated to the order of the processing. As
earlier described in Section 3.1, an extracted
triple is expected to correspond semantically
to ?agent/experiencer; relation; general ob-
ject/circumstance?. This is expected to be cor-
rect for a direct word order, i.e., Subject ? Verb
? (Indirect) Object, which is a dominant word or-
der for Spanish. Yet the inverted word order, i.e.
81
(Indirect) Object ? Verb ? Subject (e.g., De la
m?edula espinal nacen los nervios perif?ericos, i.e.,
literally *?From the spinal cord arise peripheral
nerves?), also occasionally takes place in gram-
matically correct and stylistically neutral Spanish
texts. However, the occurence of this construction
is less then 10% according to (Clements, 2006).
4 Experiments and Evaluation
In this section we describe the experiments con-
ducted with EXTRHECH system.
4.1 Experiment on parallel news dataset
We compare EXTRHECH?s performance with that
of REVERB, an Open IE system for English based
on the same algorithm (Fader et al., 2011). Since
these systems are designed for different languages,
we ran our experiment on a parallel dataset.
1
We took 300 parallel sentences from the
English-Spanish part of News Commentary Cor-
pus (Callison-Burch et al., 2011). Then, we ran
the extractors over the corresponding languages.
After that, two human annotators labeled each ex-
traction as correct or incorrect. For the Spanish
part of the dataset, the annotators agreed on 80%
of extractions (Cohen?s kappa ? = 0.60), whereas
for the English part they agreed on 85% of extrac-
tions with ? = 0.68. For both datasets their respec-
tive ? coefficients indicate substantial agreement
between the annotators.
Precision was calculated as a fraction of correct
extractions among all returned extractions. We
calculated Recall as a fraction of all returned cor-
rect extractions among all possible (i.e., expected)
correct extractions. By manual revision of the sen-
tences in the datasets, we made a list of all ex-
pected correct extractions. Their number was used
to estimate the recall.
In contrast to REVERB, our system does not
have a confidence score mechnaism at this point.
To make the comparison between the systems ap-
propriate, we ran REVERB extractor with the con-
fidence score level set to 0 that means that the sys-
tem returns all relations that match the rules, i.e.,
in the same way as EXTRHECH does. Hence, the
systems were in equivalent conditions. The results
of the experiment are shown in Table 1.
As we see, on a parallel dataset of texts from
News Commentary Corpus, both systems show a
very similar performance. Based on this observa-
tion, we can conclude that the algorithm suggested
System Precision Recall
Correct Returned
Extractions Extractions
EXTRHECH 0.59 0.48 218 368
REVERB 0.56 0.44 201 358
Table 1: Performance comparison of REVERB and
EXTRHECH systems over a parallel dataset.
in (Fader et al., 2011) can be easily adopted for
other languages with dominating SVO word order
and an available POS-tagger.
4.2 Experiment on Raw Web dataset
One of the most important goals of Open IE sys-
tems is to be able to process large amounts of texts
directly from the Web. This requires high per-
formance speed and robustness on texts that of-
ten lack grammatical and orthographical correct-
ness or coherence. The study showing the ap-
proach?s advantage in speed was already presented
in (Fader et al., 2011). In this work we focused on
robustness. We evaluated the performance of our
system on a dataset of sentences extracted from
the Internet ?as is?. For this dataset, we took
200 random data chunks detected by a sentence
splitter from CommonCrawl 2012 corpus (Kirk-
patrick, 2011), which is a collection of web texts
crawled from over 5 billion web pages. However,
41 from those 200 chunks were not samples of
textual information in human language but rather
pieces of programming codes or numbers. We
took out these chunks because they are not rele-
vant for our research. In a real life scenario they
could be easily detected and eliminated from the
Web data stream. After this, our dataset consisted
of 159 sentences written in human language. We
will refer to this dataset as Raw Web text dataset.
1
Of 159 sentences of the dataset, 36 sentences (22%
of the dataset) were grammatically incorrect or in-
coherent, as evaluated by a professional linguist.
We ran EXTRHECH system over this dataset
and asked two human judges to label extractions
as correct or incorrect. The annotators agreed on
70% of extractions with Cohen?s ? = 0.40, which
indicates the lower bound of moderate agreement
between judges.
Precision and Recall were calculated in the
same manner as described in Section 4.1. We com-
pare these numbers to the results obtained for the
dataset of grammatically correct sentences from
News Commentary Corpus in Table 2.
We can observe that system?s performence has
82
Dataset Precision Recall
News Commentary 0.59 0.48
Raw Web 0.55 0.49
Table 2: Performance of EXTRHECH on the gram-
matically correct dataset and the dataset of noisy
sentences extracted from the Web
not lowered significantly when processing ?noisy?
texts compared to edited newspaper texts. An in-
teresting observation is that texts from the Internet
are poorer in facts than the news texts. The num-
ber of expected extractions was manually evalu-
ated by a human expert for both datasets. The ra-
tio of extractions to sentences for the news dataset
was 1.5:1, while for the Raw Web dataset it was
only 1.03:1.
Now we will briefly discuss the issue arising
due to various encoding standards used for non-
ASCII characters, e.g., of ?a, ?e, ?n, etc. While apply-
ing Freeling morphological analyzer to the dataset,
we encountered an issue that the sentences came
in various encodings. As we mentioned in Sec-
tion 3, Freeling-2.2 analyzer works properly only
with ISO encoded input. Therefore, we had to
convert each sentence from the dataset into ISO
encoding. While most of the sentences were in
UTF-8 encoding and were converted in a single
pass, the encoding of about 3% of the sentences
was initially corrupted, therefore, they were not
processed correctly by the POS-tagger. Although
the issue is manageable at the scale of a small
dataset, it might affect the speed and quality of fact
extraction when working at Web scale.
5 Error Analysis
After running EXTRHECH on the datasets, we an-
alyzied the errors in the output. We followed
the classifications of the types of errors and their
causes suggested in (Zhila and Gelbukh, 2014).
The distribution of the errors in EXTRHECH?s out-
put over the types of errors is shown in Table 3.
The data about error types was gathered over ex-
tractions from Raw Web dataset. When errors are
present both in the arguments and in the relation
phrase, they are likely to have the same cause.
Based on the analysis of the outputs over Raw
Web dataset, the following causes for errors have
been observed:
? Underspecified noun phrase
? Overspecified verb phrase
? Non-contiguous verb phrase
Type of errors Percentage
Incorrect relation phrase 21%
Incorrect argument(s) 45%
of them, with also incorrect relation 19%
Incorrect argument order 6%
Table 3: Distribution of errors in output by
the basic error types in relation extraction for
EXTRHECH system run over Raw Web dataset
? N-ary relation or preposition (e.g., entre, ?be-
tween?)
? Conditional subordinate clause
? Incorrectly resolved relative clause
? Incorrectly resolved conjunction
? Inverse word order
? Incorrect POS-tagging
? Grammatical errors in original sentences
Inverse word order is one of the main causes for
the incorrect order of arguments in extracted rela-
tions. However, as it can be seen in Table 3, this
is the least common type of errors, which is in ac-
cordance to the low frecuency of the inverse word
order (Clements, 2006). A more detailed analysis
of the issues that cause the errors can be found in
(Zhila and Gelbukh, 2014).
6 Conclusions
We have introduced an approach to Open IE based
on syntactic constraints over POS tag sequences
targeted at Spanish language. We described the
rules for relation phrases and their arguments in
Spanish and their implementation in EXTRHECH
system. Further, we presented a series of ex-
periments with EXTRHECH and showed (1) that
the performance of this approach to Open IE
is similar for English and Spanish, and (2) that
EXTRHECH?s performance is robust on texts of
varying quality. We also gave a brief classification
of errors by their types and causes.
Our future plans include implementation of
shallow parsing and syntactic n-grams (Sidorov
et al., 2012; Sidorov et al., 2013; Sidorov et al.,
2014; Sidorov, 2013a; Sidorov, 2013b), as well as
learning techniques, and analysis of their influence
on the system?s performance.
Acknowledgments
The work was partially supported by the Gov-
ernment of Mexico: SIP-IPN 20144534 and
20144274, PIFI-IPN, and SNI. We thank Yahoo!
for travel and conference support for this paper.
83
References
Honorato Aguilar-Galicia. 2012. Extracci?on au-
tom?atica de informaci?on sem?antica basada en estruc-
turas sint?acticas. Master?s thesis, Center for Com-
puting Research, Instituto Polit?ecnico Nacional,
Mexico City, D.F., Mexico.
Michele Banko and Oren Etzioni. 2008. The tradeoffs
between open and traditional relation extraction. In
Proceedings of ACL-08: HLT, pages 28?36. Associ-
ation for Computational Linguistics, June.
Michele Banko, Michael J. Cafarella, Stephen Soder-
land, Matt Broadhead, and Oren Etzioni. 2007.
Open information extraction from the web. In IJ-
CAI, pages 2670?2676.
Chris Callison-Burch, Philipp Koehn, Christof Monz,
and Omar Zaidan. 2011. Findings of the 2011
workshop on statistical machine translation. In Pro-
ceedings of the Sixth Workshop on Statistical Ma-
chine Translation, pages 22?64, Edinburgh, Scot-
land, July. Association for Computational Linguis-
tics.
Joseph Clancy Clements. 2006. Primary and sec-
ondary object marking in Spanish. In J. Clancy
Clements and Jiyoung Yoon, editors, Functional ap-
proaches to Spanish syntax: Lexical semantics, dis-
course, and trasitivity, pages 115?133. London: Pal-
grave MacMillan.
Oren Etzioni, Michele Banko, Stephen Soderland, and
Daniel S. Weld. 2008. Open information extraction
from the web. Commun. ACM, 51(12):68?74, De-
cember.
Oren Etzioni. 2011. Search Needs a Shake-Up. Na-
ture, 476(7358):25?26, August.
Anthony Fader, Stephen Soderland, and Oren Etzioni.
2011. Identifying relations for open information ex-
traction. In Proceedings of the Conference on Em-
pirical Methods in Natural Language Processing,
EMNLP ?11, pages 1535?1545, Stroudsburg, PA,
USA. Association for Computational Linguistics.
Pablo Gamallo, Marcos Garcia, and Santiago
Fern?andez-Lanza. 2012. Dependency-based
open information extraction. In Proceedings of
the Joint Workshop on Unsupervised and Semi-
Supervised Learning in NLP, ROBUS-UNSUP ?12,
pages 10?18, Stroudsburg, PA, USA. Association
for Computational Linguistics.
Christopher Horn, Alisa Zhila, Alexander Gelbukh,
Roman Kern, and Elisabeth Lex. 2013. Using fac-
tual density to measure informativeness of web doc-
uments. In Proceedings of the 19th Nordic Confer-
ence on Computational Linguistics, NoDaLiDa.
Marshall Kirkpatrick. 2011. New 5 billion
page web index with page rank now avail-
able for free from common crawl foundation.
http://readwrite.com/2011/11/07/
common_crawl_foundation_announces_
5_billion_page_w, November. [last visited on
25/01/2013].
Geoffrey Leech and Andrew Wilson. 1999. Standards
for tagsets. In Syntactic Wordclass Tagging, pages
55?80. Springer Netherlands.
Thomas Lin, Mausam, and Oren Etzioni. 2010. Iden-
tifying functional relations in web text. In Proceed-
ings of the 2010 Conference on Empirical Methods
in Natural Language Processing, pages 1266?1276.
Association for Computational Linguistics, October.
Mausam, Michael Schmitz, Stephen Soderland, Robert
Bart, and Oren Etzioni. 2012. Open language learn-
ing for information extraction. In EMNLP-CoNLL,
pages 523?534. ACL.
Llu??s Padr?o, Samuel Reese, Eneko Agirre, and Aitor
Soroa. 2010. Semantic services in freeling 2.1:
Wordnet and ukb. In Pushpak Bhattacharyya, Chris-
tiane Fellbaum, and Piek Vossen, editors, Princi-
ples, Construction, and Application of Multilingual
Wordnets, pages 99?105, Mumbai, India, February.
Global Wordnet Conference 2010, Narosa Publish-
ing House.
Grigori Sidorov, Francisco Velasquez, Efstathios Sta-
matatos, Alexander Gelbukh, and Liliana Chanona-
Hern?andez. 2012. Syntactic dependency-based n-
grams as classification features. In M. Gonz?alez-
Mendoza and I. Batyrshin, editors, Advances in
Computational Intelligence. Proceedings of MICAI
2012, volume 7630 of Lecture Notes in Artificial In-
telligence, pages 1?11. Springer.
Grigori Sidorov, Francisco Velasquez, Efstathios Sta-
matatos, Alexander Gelbukh, and Liliana Chanona-
Hern?andez. 2013. Syntactic dependency-based n-
grams: More evidence of usefulness in classifica-
tion. In Alexander Gelbukh, editor, Computational
Linguistics and Intelligent Text Processing. Proceed-
ings of International Conference on Intelligent Text
Processing and Computational Linguistics, CICLing
2013, volume 7816 of Lecture Notes in Artificial In-
telligence, pages 13?24. Springer.
Grigori Sidorov, Francisco Velasquez, Efstathios Sta-
matatos, Alexander Gelbukh, and Liliana Chanona-
Hern?andez. 2014. Syntactic n-grams as machine
learning features for natural language processing.
Expert Systems with Applications, 41(3):853?860.
Grigori Sidorov. 2013a. Non-continuous syntactic n-
grams. Polibits, 48:67?75.
Grigori Sidorov. 2013b. Syntactic dependency based
n-grams in rule based automatic english as second
language grammar correction. International Jour-
nal of Computational Linguistics and Applications,
4(2):169?188.
Stephen Soderland, Brendan Roof, Bo Qin, Shi Xu,
Mausam, and Oren Etzioni. 2010. Adapting open
information extraction to domain-specific relations.
AI Magazine, 31(3):93?102.
84
Fei Wu and Daniel S. Weld. 2010. Open information
extraction using wikipedia. In Proceedings of the
48th Annual Meeting of the Association for Com-
putational Linguistics, ACL ?10, pages 118?127,
Stroudsburg, PA, USA. Association for Computa-
tional Linguistics.
Alisa Zhila and Alexander Gelbukh. 2014. Automatic
identification of facts in real internet texts in Spanish
using lightweight syntactic constraints: Problems,
their causes, and ways for improvement. Submitted.
85
First Joint Conference on Lexical and Computational Semantics (*SEM), pages 449?453,
Montre?al, Canada, June 7-8, 2012. c?2012 Association for Computational Linguistics
Soft Cardinality: A Parameterized Similarity Function for Text Comparison
Sergio Jimenez
Universidad Nacional
de Colombia, Bogota,
Ciudad Universitaria
edificio 453, oficina 220
sgjimenezv@unal.edu.co
Claudia Becerra
Universidad Nacional
de Colombia, Bogota
cjbecerrac@unal.edu.co
Alexander Gelbukh
CIC-IPN
Av. Juan Dios B?tiz,
Av. Mendiz?bal, Col.
Nueva Industrial Vallejo,
CP 07738, DF, M?xico
gelbukh@gelbukh.com
Abstract
We present an approach for the construction of text
similarity functions using a parameterized resem-
blance coefficient in combination with a softened
cardinality function called soft cardinality. Our ap-
proach provides a consistent and recursive model,
varying levels of granularity from sentences to char-
acters. Therefore, our model was used to compare
sentences divided into words, and in turn, words di-
vided into q-grams of characters. Experimentally,
we observed that a performance correlation func-
tion in a space defined by all parameters was rel-
atively smooth and had a single maximum achiev-
able by ?hill climbing.? Our approach used only sur-
face text information, a stop-word remover, and a
stemmer to tackle the semantic text similarity task
6 at SEMEVAL 2012. The proposed method ranked
3rd (average), 5th (normalized correlation), and 15th
(aggregated correlation) among 89 systems submit-
ted by 31 teams.
1 Introduction
Similarity is the intrinsic ability of humans and some
animals to balance commonalities and differences when
comparing objects that are not identical. Although there
is no direct evidence of how this process works in liv-
ing organisms, some models have been proposed from
the cognitive perspective (Sj?berg, 1972; Tversky, 1977;
Navarro and Lee, 2004). On the other hand, several simi-
larity models have been proposed in mathematics, statis-
tics, and computer science among other fields. Particu-
larly in AI, similarity measures play an important role in
the construction of intelligent systems that are required
to exhibit behavior similar to humans. For instance, in
the field of natural language processing, text similarity
functions provide estimates of the human similarity judg-
ments related to language. In this paper, we combine el-
ements from the perspective of cognitive psychology and
computer science to propose a model for building simi-
larity functions suitable for the task of semantic text sim-
ilarity.
We identify four main families of text similarity func-
tions: i) resemblance coefficients based on sets (e.g. Jac-
card?s (1901) and Dice?s (1945) coefficients) ii) functions
in metric spaces (e.g. cosine tf-idf similarity (Salton et
al., 1975)); iii) the edit distance family of measures (e.g.
Levenstein (1966) distance, LCS (Hirschberg, 1977));
and iv) hybrid approaches ((Monge and Elkan, 1996; Co-
hen et al, 2003; Corley and Mihalcea, 2005; Jimenez et
al., 2010)). All of these measures use a subdivision of
the texts in different granularity levels, such as q-grams
of words, words, q-grams of characters, syllables, and
characters. Among hybrid approaches, Monge-Elkan?s
measure and soft cardinality methods are recursive and
can be used to build similarity functions at any arbitrary
range of granularity. For instance, it is possible to con-
struct a similarity function to compare sentences based
on a function that compares words, which in turn can be
constructed based on a function that compares bigrams of
characters. Furthermore, hybrid approaches can integrate
similarity functions that are not based on the representa-
tion of the surface of text, such as semantic relatedness
measures (Pedersen et al, 2004).
Text similarity measures can be static or adaptive
whether they are binary functions using only surface in-
formation of the two texts, or are functions that suit
to a wider set of texts. For instance, measures using
tf-idf weights adapt their results to the set of texts in
which those weights were obtained. Other approaches
learn parameters of the similarity function from a set of
texts to optimize a particular task. For instance, Ris-
tad and Yianilos (1998) and Bikenko and Mooney (2003)
learned the costs of edit operations for all characters for
an edit-distance function in a name-matching task. Other
machine-learning approaches have also been proposed to
build adaptive measures in name-matching (Bilenko and
449
Mooney, 2003) and textual-entailment tasks.
However, those machine-learning-based methods for
adaptive similarity suffer from sparseness and the ?curse
of dimensionality?. For example, the method of Ristad
and Yianilos learns n2 + 2n parameters, where n is the
size of the character set. Similarly, dimensionality in the
method of Bilenko and Mooney is the size of the data
set vocabulary. This issue is addressed primarily through
machine-learning algorithms, which reduce the dimen-
sionality of the problem regularizating to achieve enough
generalization to get an acceptable performance differ-
ence between training and test data. Although machine-
learning solutions have proven effective for many appli-
cations, the principle of Occam?s razor suggests that it
should be preferable to have a model that explains the
data with a smaller number of significant parameters. In
this paper, we seek a simpler adaptive similarity model
with few meaningful parameters.
Our proposed similarity model starts with a
cardinality-based resemblance coefficient (i.e. Dice?s
coefficient 2|A?B|/|A|+|B|) and generalizes it to model
the effect of asymmetric selection of the referent. This
effect is a human factor discovered by Tversky (1977)
that affects judgments of similarity, i.e. humans tends
to select the more prominent stimulus as the referent
and the less salient stimulus as the object. Some of
Tversky?s examples are ?the son resembles the father?
rather than ?the father resembles the son?, ?an ellipse is
like a circle? not ?a circle is like an ellipse?, and ?North
Korea is like Red China? rather than ?Red China is like
North Korea?. Generally speaking, ?the variant is more
similar to the prototype than vice versa?. In the previous
example, stimulus salience is associated with the promi-
nence of the country; for text comparison we associate
word salience with tf-idf weights. At the text level, we
associate salience with a combination of word-salience,
inter-word similarity, and text length provided by soft
cardinality. Experimentally, we observed that this effect
also occurs when comparing texts, but not necessarily
in the same direction suggested by Tversky. We used
this effect to improve the performance of our similarity
model. In addition, we proposed a parameter that biases
the function to generate greater or lower similarity
scores.
Finally, in our model we used a soft cardinality func-
tion (Jimenez et al, 2010) instead of the classical set car-
dinality. Just as classical cardinality counts the number
of elements which are not identical in a set, soft cardi-
nality uses an auxiliary inter-element similarity function
to make a soft count. For instance, the soft cardinality of
a set with two very similar (but not identical) elements
should be a real number closer to 1.0 instead of 2.0.
The rest of the paper is organized as follows. In Sec-
tion 2 we briefly present soft cardinality. In Section 3 the
proposed parameterized similarity model is presented. In
Section 4 experimental validation is provided using 8 data
sets annotated with human similarity judgments from the
?Semantic-Text-Similarity? task at SEMEVAL-2012. Fi-
nally, a brief discussion is provided in Section 5 and con-
clusions are presented in Section 6.
2 Soft Cardinality
Let A =
{
a1, a2, . . . , a|A|
}
and B =
{
b1, b2, . . . , b|B|
}
be two sets being compared. When each element of ai
or bj has an associated weight wai or wbj the problem
of comparing those sets becomes a weighted similarity
problem. This means that such model has to take into
account not only the commonalities and diferences, but
also their weights. Also, if an (|A ? B|) ? (|A ? B|)
similarity matrix S is available, the problem becomes a
weighted soft similarity problem because the common-
ality between A and B has to be computed not only
with identical elements, but also with elements with a
degree of similarity. The values of S can be obtained
from an auxiliary similarity function sim(a, b) that sat-
isfies at least non-negativity (?a, b, sim(a, b) ? 0) and
reflexivity (?a, sim(a, a) = 1). Other postulates such as
symmetry (?a, b, sim(a, b) = sim(b, a)) and triangle in-
equality1 (?a, b, c, sim(a, c) ? sim(a, b) + sim(b, c)?
1) are not strictly necessary.
Jimenez et al (2010) proposed a set-based weighted
soft-similarity model using resemblance coefficients and
the soft cardinality function instead of classical set car-
dinality. The idea of calculating the soft cardinality is
to treat elements ai in set the A as sets themselves and
to treat inter-element similarities as the intersections be-
tween the elements sim(ai, aj) = |ai ? aj |. Therefore,
the soft cardinality of set A becomes |A|
?
=
?
?
?
?|A|
i=1ai
?
?
?.
Since it is not feasible to calculate this union, they pro-
posed the following weighted approximation using |ai| =
wai :
|A|
?
sim '
|A|?
i
wai
?
?
|A|?
j
sim(ai, aj)
p
?
?
?1
(1)
Parameter p ? 0 in eq.1 controls the ?softeness? of
the cardinality, taking p = 1 its no-effect value and leav-
ing element similarities unchanged for the calculation of
soft cardinality. When p is large, all sim(?, ?) results
lower than 1 are transformed into a number approaching
0. As a result, the soft cardinality behaves like the clas-
sical cardinality, returning the addition of all the weights
of the elements, i.e |A|
?
sim '
?|A|
i wai . When p is close
to 0, all sim(?.?) results are transformed approaching
1triangle inequality postulate for similarity is derived from its coun-
terpart for dissimilarity (distance) distance(a, b) = 1? sim(a, b).
450
into a number approaching 1, making the soft cardinal-
ity returns the average of the weights of the elements, i.e.
|A|
?
sim '
1
|A|
?|A|
i wai . Jimenez et al used p = 2 and
idf weights in the same name-matching task proposed by
Cohen et al (Cohen et al, 2003).
3 A Parameterized Similarity Model
As we mentioned above, Tvesky proposed that humans
tends to select more salient stimulus as referent and less
salient stimulus as object when comparing two objects A
and B. Based on the idea of Tvesrky, the similarity be-
tween two objects can be measured as the ratio between
the salience of commonalities and the salience of the less
salient object. Drawing an analogy between objects as
sets and salience as the cardinality of a set, the salience
of commonalities is |A ? B|, and the salience of the less
salient object is min(|A|, |B|). This ratio is known as the
overlap coefficient Overlap(A,B) = |A?B|min(|A|,|B|) . How-
ever, whether |A| < |B| or whether |A|  |B|, the sim-
ilarity obtained by Overlap(A,B) is the same. Hence,
we propose to model the selecction of the referent using
a parameter ? that makes a weighted average between
min(|A|, |B|) and max(|A|, |B|), controling the degree
to which the asymmetric referent-selection effect is con-
sidered in the similarity measure.
SIM(A,B) =
|A ?B|+ bias
?max (|A|, |B|) + (1? ?)min (|A|, |B|)
(2)
The parameter ? controls the degree to which the
asymmetric referent-selection effect is considered in the
similarity measure. Its no-effect value is ? = 0.5, so
the eq.2 becomes the Dice coefficient. Moreover, when
? = 0 the eq.2 becomes the overlap coefficient, other-
wise when ? = 1 the opposite effect is modeled.
In addition, we introduced a bias parameter in eq. 2
that increases the commonalities of each object pair by
the same amount, and so it measures the degree to which
all of the objects have commonalities among each other.
Clearly, the non-effect value for the bias parameter is 0.
Besides, the bias parameter has the effect of biasing
SIM(A,B) by considering any pair ?A,B? more sim-
ilar if bias > 0 and their cardinalities are small. Con-
versely, the similarity between pairs with large cardinal-
ities is promoted if bias < 0. However, as higher values
of biasmay result in similarity scores outside the interval
[0, 1], additional post-procesing to limit the similarities in
this interval may be required.
The proposed parameterized text similarity measure is
constructed by combining the proposed resemblance co-
efficient in eq.2 and the soft cardinality in eq.1. The
resulting measure has three parameters: ?, bias, and p.
Weights wai can be idf weights. This measure takes two
? Asymetric referent selection at text level
bias Bias parameter at text level
p Soft cardinality exponent at word level
wai Element weights at word level
q1, q2 q1-grams or [q1 : q2]spectra word division
?sim Asymetric referent selection at q-gram level
biassim Bias parameter q-gram level
Table 1: Parameters of the proposed similarity model
texts represented as sets of words and returns their simi-
larity. The auxiliary similarity function sim(a, b) neces-
sary for calculating the soft cardinality is another param-
eter of the model. This auxiliary function is any function
that can compare two words and return a similarity score
in [0, 1].
To build this sim(a, b) function, we chose to reuse the
eq.2 but representing words as sets of q-grams or ranges
of q-grams of different sizes, i.e. [q1 : q2] spectra. Q-
grams are consecutive overlapped substrings of size q.
For instance, the word ?saturday? divided into trigrams
is {/sa, sat, atu, tur, urd, rda, day, ay.}. The character
?.? is a padding character added to differenciate q-grams
at the begining or end of the string. A [2 : 4]spectra
is the combined representation of a word using ?in this
example? bigrams, trigrams and quadgrams (Jimenez and
Gelbukh, 2011). The cardinality function for sim() was
the classical set cardinality. Clearly, the soft cardinal-
ity could be used again if an auxiliary similarity func-
tion for character comparison and a q-gram weighting
mechanism are provided to allow another level of recur-
sion. Therefore, the parameters of sim(a, b) are: ?sim,
biassim. Finally, the entire set of parameters of the pro-
posed similarity model is shown in Table 1.
4 Experimental Setup and Results
The aim of these experiments is to observe the behavior
of the parameters of our similarity model and verify if the
hypothesis that motivated these parameters can be con-
firmed experimentally. The experimental data are 8 data
sets (3 for training and 5 for test) proposed in the ?Seman-
tic Text Similarity? task at SEMEVAL-2012. Each data
set consist of a set of pairs of text annotated with human-
similarity judgments on a scale of 0 to 5. Each similarity
judgment is the average of the judgments provided by 5
human judges. For a comprehensible description of the
task see(Agirre et al, 2012).
For the experiments, all data sets were pre-processed
by converting to lowercase characters, English stop-
words removal and stemming using Porter stemmer
(Porter, 1980). The performance measure used for all ex-
periments was the Pearson correlation r.
451
4.1 Model Parameters
In order to make an initial exploration of the parame-
ters in Table 1, we set q1 = 2 (i.e. bigrams) and used
wai = idf(ai). For other parameters, we started with all
the non-effect values, i.e. ? = 0.5, bias = 0, p = 1,
?sim = 0.5 and biassim = 0. Plots in Figure 1 show
the Pearson correlation measured in each of the data sets.
For each graph, the non-effect configuration was used and
each parameter varies in the range indicated in each hor-
izontal axis. For best viewing, the non-effect values on
each graph are represented by a vertical line.
In this exploration of the parameters it was noted that
each parameter defines a function for the performance
measure that is smooth and with an unique global maxi-
mum. Therefore, we assumed that the join performance
function in the space defined by the 5 parameters also
had the same properties. The parameters for each data set
shown in Table 2 were found using a simple hill-climbing
algorithm. Different q-gram and spectra configurations
were tested manually.
5 Discussion
It is possible to observe from the results in Figure 1 and
Table 2 that the behavior of the parameters is similar in
pairs of data sets that have training and test parts. This
behavior is evident in both MSRvid and MSRpar data
sets, but it is less evident in SMTeuroparl. Furthermore,
the optimal parameters for training data sets MSRvid and
MSRpar were similar to those of their test data sets. In
conclusion, the proposed set of parameters provides a set
of features that characterize a data set for the text similar-
ity task.
Regarding the effect of asymmetry in referent selecc-
tion proposed by Tvesrky, it was observed that ?at text
level? the MSRvid data sets were the only ones that sup-
ported this hypothesis (? = 0.32, 0.42). The remaining
data sets showed the opposite effect (? > 0.5). That is,
annotators chose the most salient document (the longer)
as the referent when a pair of texts is being compared.
The Table 2 also shows that the optimal parameters
for all data sets were different from the no-effect values
combination. This result can also be seen in Figure 1,
where curves crossed the vertical line of no-effect value
?in most of the cases? in values different to the optimum.
Clearly, the proposed set of parameters is useful for ad-
justing the similarity function for a particular data set and
task.
6 Conclusions
We have proposed a new parameterized similarity func-
tion for text comparison and a method for finding the op-
timal values of the parameter set when training data is
available. In addition, the parameter ?, which was moti-
vated by the similarity model of Tversky, proved effective
in obtaining better performance, but we could not con-
firm the Tvesky?s hypothesis that humans tends to select
the object (text) with less stimulus salience (text length)
as the referent. This result might have occurred because
either the stimulus salience is not properly represented by
the length of the text, or Tversky?s hypothesis cannot be
extended to text comparison.
The proposed similarity function proved effective in
the task of ?Semantic Text Similarity? in SEMEVAL
2012. Our method obtained the third best average cor-
relation on the 5 test data sets. This result is remarkable
because our method only used data from the surface of
the texts, a stop-word remover, and a stemmer, which can
be even be considered as a baseline method.
Acknowledgments
This research was funded by the Systems and Industrial
Engineering Department, the Office of Student Welfare
of the National University of Colombia, Bogot?, and
throught a grant from the Colombian Department for
Science, Technology and Innovation Colciencias, proj.
110152128465. The second author recognizes the sup-
port from Mexican Government (SNI, COFAA-IPN, SIP
20113295, CONACYT 50206-H) and CONACYT?DST
India (proj. ?Answer Validation through Textual Entail-
ment?).
References
Eneko Agirre, Daniel Cer, Mona Diab, and Gonzalez-Agirre
Aitor. 2012. SemEval-2012 task 6: A pilot on semantic
textual similarity. In Proc. of the 6th International Workshop
on Semantic Evaluation (SemEval 2012), in conjunction with
the First Joint Conference on Lexical and Computational Se-
mantics (*SEM 2012)., Montreal,Canada.
Mikhail Bilenko and Raymond J. Mooney. 2003. Adaptive du-
plicate detection using learnable string similarity measures.
In Proc. of the ninth ACM SIGKDD international confer-
ence on Knowledge discovery and data mining, pages 39?48,
Washington, D.C. ACM.
William W Cohen, Pradeep Ravikumar, and Stephen E Fien-
berg. 2003. A comparison of string distance metrics for
Name-Matching tasks. In Proc. of the IJCAI2003 Workshop
on Information Integration on the Web II Web03.
Courtney Corley and Rada Mihalcea. 2005. Measuring the se-
mantic similarity of texts. In Proc. of the ACL Workshop
on Empirical Modeling of Semantic Equivalence and Entail-
ment, Stroudsburg, PA.
Lee R. Dice. 1945. Measures of the amount of ecologic associ-
ation between species. Ecology, pages 297?302.
Daniel S. Hirschberg. 1977. Algorithms for the longest com-
mon subsequence problem. J. ACM, 24(4):664?675.
452
0.6
0.7
0.8
correlati
on
p
0.3
0.4
0.5
0 1 2 3 4 5 6 7 8
Pearson
correlati
on
?
0 0.5 1 1.5
bias
-15 -5 5 15
?sim
-0.5 0.5 1.5
biassim
-4 -2 0 2 4
MSRvid(tr) MSRvid(te) MSRpar(tr) MSRpar(te) SMTeur(tr) SMTeur(te) OnWN SMTnews no effect
-5 -3 -1 1 3 5
Figure 1: Exploring similarity model parameters around their no-effect values (tr=training, te=test)
Parameters correl. Official Results
Data set [q1 : q2] ? bias p ?sim biassim r SoftCard Best
MSRpar.training [4] 0.62 1.14 0.77 -0.04 -0.38 0.6598 n/a n/a
MSR.par.test [4] 0.60 1.02 0.9 -0.02 -0.4 0.6335 0.64051 0.7343
MSRvid.training [1:4] 0.42 -0.80 2.28 0.18 0.08 0.8323 n/a n/a
MSRvid.test [1:4] 0.32 -0.80 1.88 1.08 0.08 0.8579 0.8562 0.8803
SMTeuroparl.training [2:4] 0.74 -0.06 0.91 1.88 2.90 0.6193 n/a n/a
SMTeuroparl.test [2:4] 0.84 -0.16 0.71 1.78 3.00 0.5178 0.51522 0.5666
OnWN.test [2:5] 0.88 -0.62 1.36 -0.02 -0.70 0.7202 0.71091 0.7273
SMTnews.test [1:4] 0.88 0.88 1.57 0.80 3.21 0.5344 0.48331 0.6085
1Result obtained using Jaro-Winkler (Winkler, 1990) measure as sim(a, b) function between words.
2Result obtained using generalized Monge-Elkan measure p = 4, no stop-words removal and no term weights
(Jimenez et al, 2009).
Table 2: Results with optimized parameters and official SEMEVAL 2012 results
Paul Jaccard. 1901. Etude comparative de la distribution florare
dans une portion des alpes et des jura. Bulletin de la Soci?t?
Vaudoise des Sciences Naturelles, pages 547?579.
Sergio Jimenez and Alexander Gelbukh. 2011. SC spectra: a
linear-time soft cardinality approximation for text compari-
son. In Proc. of the 10th international conference on Artifi-
cial Intelligence, MICAI?11, Puebla, Mexico.
Sergio Jimenez, Claudia Becerra, Alexander Gelbukh, and
Fabio Gonzalez. 2009. Generalized Monge-Elkan method
for approximate text string comparison. In Computational
Linguistics and Intelligent Text Processing, volume 5449 of
LNCS, pages 559?570.
Sergio Jimenez, Fabio Gonzalez, and Alexander Gelbukh.
2010. Text comparison using soft cardinality. In String Pro-
cessing and Information Retrieval, volume 6393 of LNCS,
pages 297?302.
Vladimir I. Levenshtein. 1966. Binary codes capable of cor-
recting deletions, insertions, and reversals. Soviet Physics
Doklady, 10(8):707?710.
Alvaro E. Monge and Charles Elkan. 1996. The field matching
problem: Algorithms and applications. In Proc. KDD-96,
pages 267?270, Portland, OR.
Daniel Navarro and Michael D. Lee. 2004. Common and dis-
tinctive features in stimulis representation: A modified ver-
sion of the contrast model. Psychonomic Bulletin & Review,
11:961?974.
Ted Pedersen, Siddharth Patwardhan, and Jason Michelizzi.
2004. WordNet::Similarity: measuring the relatedness of
concepts. In Proc. HLT-NAACL?Demonstration Papers,
Stroudsburg, PA.
Martin Porter. 1980. An algorithm for suffix stripping. Pro-
gram, 3(14):130?137.
Eric S. Ristad and Peter N. Yianilos. 1998. Learning string
edit distance. IEEE Transactions on Pattern Analysis and
Machine Intelligence, 20(5):522?532.
Gerard Salton, A. Wong, and C.S. Yang. 1975. A vector space
model for automatic indexing. Com. ACM, 18(11):613?620.
L. Sj?berg. 1972. A cognitive theory of similarity. G?teborg
Psychological Reports.
Amos Tversky. 1977. Features of similarity. Psychological
Review, 84(4):327?352.
William E. Winkler. 1990. String comparator metrics and en-
hanced decision rules in the Fellegi-Sunter model of record
linkage. In Proc. of the Section on Survey Research Methods.
453
First Joint Conference on Lexical and Computational Semantics (*SEM), pages 571?574,
Montre?al, Canada, June 7-8, 2012. c?2012 Association for Computational Linguistics
JU_CSE_NLP: Multi-grade Classification of Semantic Similarity  
Between Text Pairs 
 
    Snehasis Neogi1, Partha Pakray2, Sivaji Bandyopadhyay1            Alexander Gelbukh 
                           1Computer Science & Engineering Department                        Center for Computing Research 
                           Jadavpur University, Kolkata, India                                 National Polytechnic Institute         
                  2Computer Science & Engineering Department                               Mexico City, Mexico 
                           Jadavpur University, Kolkata, India                                 gelbukh@gelbukh.com 
                        Intern at Xerox Research Centre Europe 
                                          Grenoble, France 
                  {snehasis1981,parthapakray}@gmail.com 
            sbandyopadhyay@cse.jdvu.ac.in 
 
Abstract 
This article presents the experiments car-
ried out at Jadavpur University as part of 
the participation in Semantic Textual Si-
milarity (STS) of Task 6 @ Semantic 
Evaluation Exercises (SemEval-2012). 
Task-6 of SemEval- 2012 focused on se-
mantic relations of text pair. Task-6 pro-
vides five different text pair files to 
compare different semantic relations and 
judge these relations through a similarity 
and confidence score. Similarity score is 
one kind of multi way classification in the 
form of grade between 0 to 5. We have 
submitted one run for the STS task. Our 
system has two basic modules - one deals 
with lexical relations and another deals 
with dependency based syntactic relations 
of the text pair. Similarity score given to a 
pair is the average of the scores of the 
above-mentioned modules. The scores 
from each module are identified using rule 
based techniques. The Pearson Correlation 
of our system in the task is 0.3880. 
1 Introduction 
Task-61 [1] of SemEval-2012 deals with seman-
tic similarity of text pairs. The task is to find the 
similarity between the sentences in the text pair 
(s1 and s2) and return a similarity score and an 
optional confidence score. There are five datasets 
                                                          
1 http://www.cs.york.ac.uk/semeval-2012/task6/ 
in the test data and with tab separated text pairs. 
The datasets are as follows: 
 
? MSR-Paraphrase, Microsoft Research Pa-
raphrase Corpus (750 pairs of sentences.) 
? MSR-Video, Microsoft Research Video De-
scription Corpus (750 pairs of sentences.) 
? SMTeuroparl: WMT2008 development data-
set (Europarl section) (459 pairs of sen-
tences.)  
? SMTnews: news conversation sentence pairs 
from WMT.(399 pairs of sentences.) 
? OnWN: pairs of sentences where the first 
comes from Ontonotes and the second from a 
WordNet definition. (750 pairs of sentences.) 
 
Similarity score ranges from 0 to 5 and confi-
dence score from 0 to 100. An s1-s2 pair gets a 
similarity score of 5 if they are completely 
equivalent. Similarity score 4 is allocated for 
mostly equivalent s1-s2 pair. Similarly, score 3 is 
allocated for roughly equivalent pair. Score 2, 1 
and 0 are allocated for non-equivalent details 
sharing, non-equivalent topic sharing and totally 
different pairs respectively. Major challenge of 
this task is to find the similarity score based simi-
larity for the text pair. Generally text entailment 
tasks refer whether sentence pairs are entailed or 
not: binary classification (YES, NO) [2] or multi-
classification (Forward, Backward, bidirectional 
or no entailment) [3][4]. But multi grade classifi-
cation of semantic similarity assigns a score to 
the sentence pair. Our system considers lexical 
and dependency based syntactic measures for 
semantic similarity. Similarity scores are the ba-
sic average of these module scores. A subsequent 
571
section describes the system architecture. Section 
2 describes JU_NLP_CSE system for STS task. 
Section 3 describes evaluation and experimental 
results. Conclusions are drawn in Section 4.  
2 System Architecture  
The system of Semantic textual similarity task 
has two main modules: one is lexical module and 
another one is dependency parsing based syntac-
tic module. Both these module have some pre-
processing tasks such as stop word removal, co-
reference resolution and dependency parsing etc. 
Figure 1 displays the architecture of the system.   
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
Figure 1: System Architecture 
2.1 Pre-processing Module 
The system separates the s1-s2 sentence pairs 
contained in the different STS task datasets. 
These separated pairs are then passed through the 
following sub modules: 
i. Stop word Removal: Stop words are removed 
from s1 - s2 sentence pairs. 
ii. Co-reference: Co-reference resolutions are 
carried out on the datasets before passing through 
the TE module. The objective is to increase the 
score of the entailment percentage. A word or 
phrase in the sentence is used to refer to an entity 
introduced earlier or later in the discourse and 
both having same things then they have the same 
referent or co reference. When the reader must 
look back to the previous context, reference is 
called "Anaphoric Reference". When the reader 
must look forward, it is termed "Cataphoric Ref-
erence". To address this problem we used a tool 
called JavaRAP2 (A java based implementation 
of Anaphora Procedure (RAP) - an algorithm by 
Lappin and Leass (1994)). 
iii. Dependency Parsing: Separated s1 ? s2 sen-
tences are parsed using Stanford dependency 
parser3 to produce the dependency relations in 
the texts. These dependency relations are used 
for WordNet based syntactic matching.     
2.2 Lexical Matching Module 
In this module the TE system calculates different 
matching scores such as N ? Gram match, Text 
Similarity, Chunk match, Named Entity match 
and POS match.  
 
i. N-Gram Match module: The N-Gram match 
basically measures the percentage match of the 
unigram, bigram and trigram of hypothesis 
present in the corresponding text. These scores 
are simply combined to get an overall N ? Gram 
matching score for a particular pair.  
 
ii. Chunk Match module: In this sub module 
our system evaluates the key NP-chunks of both 
text (s1) and hypothesis (s2) using NP Chunker 
v1.13 (The University of Sheffield). The hypo-
thesis NP chunks are matched in the text NP 
chunks. System calculates an overall value for 
the chunk matching, i.e., number of text NP 
chunks that match the hypothesis NP chunks. If 
the chunks are not similar in their surface form 
then our system goes for wordnet synonyms 
matching for the words and if they match in 
wordnet synsets information, it will be encoun-
tered as a similar chunk. WordNet [5] is one of 
most important resource for lexical analysis. The 
WordNet 2.0 has been used for WordNet based 
chunk matching. The API for WordNet Search-
ing (JAWS)4 is an API that provides Java appli-
cations with the ability to retrieve data from the 
WordNet synsets. 
iii. Text Similarity Module: System takes into 
consideration several text similarities calculated 
                                                          
2 http://aye.comp.nus.edu.sg/~qiu/NLPTools/JavaRAP.html 
3 http://www.dcs.shef.ac.uk/~mark/phd/software/ 
4 http://lyle.smu.edu/~tspell/jaws/index.html 
 
 
572
over the s1-s2 pair. These text similarity values 
are summed up to produce a total score for a par-
ticular s1-s2 pair. Major Text similarity measures 
that our system considers are: 
 
? Cosine Similarity 
? Lavenstine Distance 
? Euclidean Distance 
? MongeElkan Distance 
? NeedlemanWunch Distance 
? SmithWaterman Distance 
? Block Distance 
? Jaro Similarity 
? MatchingCoefficient Distance 
? Dice Similarity 
? OverlapCoefficient 
? QGrams Distance 
 
iv. Named Entity Matching: It is based on the 
detection and matching of Named Entities in the 
s1-s2 pair. Stanford Named Entity Recognizer5 is 
used to tag the named entities in both s1 and s2. 
System simply maps the number of hypothesis 
(s2) NEs present in the text (s1). A score is allo-
cated for the matching. 
 
NE_match = (Number of common NEs in Text 
and Hypothesis) / (Number of NE in Hypothesis). 
 
v. Part ?of ? Speech (POS) Matching: This 
module basically deals with matching the com-
mon POS tags between s1 and s2 sentences. 
Stanford POS tagger6 is used to tag the part of 
speech in both s1 and s2. System matches the 
verb and noun POS words in the hypothesis that 
match in the text. A score is allocated based on 
the number of POS matching. 
 
POS_match = (Number of common verb and 
noun POS in Text and Hypothesis) / (Total num-
ber of verb and noun POS in hypothesis). 
 
System calculates the sum of the entire sub mod-
ule (modules described in section 2.2) scores and 
forms a single percentage score for the lexical 
matching. This score is then compared with some 
predetermined threshold value to assign a final 
lexical score for each pair. If percentage value is 
                                                          
5 http://nlp.stanford.edu/software/CRF-NER.shtml 
6 http://nlp.stanford.edu/software/tagger.shtml 
above 0.80 then lexical score 5 is allocated. If the 
value is between 0.60 to 0.80 then lexical score 4 
is allocated. Similarly, lexical score 3 is allocated 
for percentage score of 0.40 to 0.60 and so on. 
One lexical score is finally generated for each 
text pair.     
2.3. Syntactic Matching Module: 
TE system considers the preprocessed dependen-
cy parsed text pairs (s1 ? s2) and goes for word 
net based matching technique. After parsing the 
sentences, they have some attributes like subject, 
object, verb, auxiliaries and prepositions tagged 
by the dependency parser tag set. System uses 
these attributes for the matching procedure and 
depending on the nature of matching a score is 
allocated to the s1-s2 pair. Matching procedure is 
basically done through comparison of the follow-
ing features that are present in both the text and 
the hypothesis.    
? Subject ? Subject comparison. 
? Verb ? Verb Comparison. 
? Subject ? Verbs Comparison. 
? Object ? Object Comparison. 
? Cross Subject ? Object Comparison. 
? Object ? Verbs Comparison. 
? Prepositional phrase comparison. 
 
Each of these comparisons produces one match-
ing score for the s1-s2 pair that are finally com-
bined with previously generated lexical score to 
generate the final similarity score by taking sim-
ple average of lexical and syntactic matching 
scores. The basic heuristics are as follows: 
(i) If the feature of the text (s1) directly matches 
the same feature of the hypothesis (s2), matching 
score 5 is allocated for the text pair. 
(ii) If the feature of either text (s1) or hypothesis 
(s2) matches with the wordnet synsets of the cor-
responding text (s1) or hypothesis (s2), matching 
score 4 is allocated.     
(iii) If wordnet synsets of the feature of the text 
(s1) match with one of the synsets of the feature 
of the hypothesis (s2), matching score 3 is given 
to the pair. 
(iv) If wordnet synsets of the feature of either 
text (s1) or hypothesis (s2) match with the syn-
sets of the corresponding text (s1) or hypothesis 
(s2) then matching score 2 is allocated for the 
pair. 
573
(v) Similarly if in both the cases match occurs in 
the second level of wordnet synsets, matching 
score 1is allocated. 
(vi) Matching score 0 is allocated for the pair 
having no match in their features. 
After execution of the module, system generates 
some scores. Lexical module generates one lexi-
cal score and wordnet based syntactic matching 
module generates seven matching scores. At the 
final stage of the system all these scores are 
combined and the mean is evaluated on this 
combined score. This mean gives the similarity 
score for a particular s1-s2 pair of different data-
sets of STS task. Optional confidence score is 
also allocated which is basically the similarity 
score multiplied by 10, i.e., if the similarity score 
is 5.22, the confidence score will be 52.2.     
3. Experiments on Dataset and Result  
We have submitted one run in SemEval-2012 
Task 6. The results for Run on STS Test set are 
shown in Table 1. 
 
task6-JU_CSE_NLP-
Semantic_Syntactic_Approach 
Correlations 
ALL    0.3880 
ALLnrm 0.6706 
Mean 0.4111 
MSRpar  0.3427 
MSRvid 0.3549 
SMT-eur 0.4271 
On-WN 0.5298 
SMT-news 0.4034 
Table 1: Results of Test Set 
ALL: Pearson correlation with the gold standard 
for the five datasets and the corresponding rank 
82. 
ALLnrm: Pearson correlation after the system 
outputs for each dataset are fitted to the gold 
standard using least squares and the correspond-
ing rank 86. 
Mean: Weighted mean across the 5 datasets, 
where the weight depends on the number of pairs 
in the dataset and the corresponding rank 76. 
The subsequent rows show the pearson correla-
tion scores for each of the individual datasets. 
 
4. Conclusion 
Our JU_CSE_NLP system for the STS task 
mainly focus on lexical and syntactic approaches. 
There are some limitations in the lexical match-
ing module that shows a correlation that is not 
higher in the range. In case of simple sentences 
lexical matching is helpful for entailment but for 
complex and compound sentences the lexical 
matching module loses its accuracy. Semantic 
graph matching or conceptual graph implementa-
tion can improve the system. That is not consi-
dered in our present work. Machine learning 
tools can be used to learn the system based on the 
features. It can also improve the correlation. In 
future work our system will include semantic 
graph matching and a machine-learning module.  
Acknowledgments 
The work was done under support of the DST 
India-CONACYT Mexico project ?Answer Vali-
dation through Textual Entailment? funded by 
DST, Government of India.  
References  
[1] Eneko Agirre, Daniel Cer, Mona Diab and Aitor 
Gonzalez.  SemEval-2012 Task 6: A Pilot on Se-
mantic Textual Similarity. In Proceedings of the 
6th International Workshop on Semantic Evalua-
tion (SemEval 2012), in conjunction with the First 
Joint Conference on Lexical and Computational 
Semantics (*SEM 2012). (2012) 
[2] Dagan, I., Glickman, O., Magnini, B.: The 
PASCAL Recognising Textual Entailment Chal-
lenge. Proceedings of the First PASCAL Recogniz-
ing Textual Entailment Workshop. (2005). 
[3] H. Shima, H. Kanayama, C.-W. Lee, C.-J. Lin,T. 
Mitamura, S. S. Y. Miyao, and K. Takeda. Over-
view of ntcir-9 rite: Recognizing inference in text. 
In NTCIR-9 Proceedings,2011. 
[4] Pakray, P., Neogi, S., Bandyopadhyay, S., Gel-
bukh, A.: A Textual Entailment System using Web 
based Machine Translation System. NTCIR-9, Na-
tional Center of Sciences, Tokyo, Japan. December 
6-9, 2011. (2011). 
[5] Fellbaum, C.: WordNet: An Electronic Lexical 
Database. MIT Press (1998). 
574
First Joint Conference on Lexical and Computational Semantics (*SEM), pages 684?688,
Montre?al, Canada, June 7-8, 2012. c?2012 Association for Computational Linguistics
Soft Cardinality + ML: Learning Adaptive Similarity Functions
for Cross-lingual Textual Entailment
Sergio Jimenez
Universidad Nacional
de Colombia, Bogota,
Ciudad Universitaria
edificio 453, oficina 220
sgjimenezv@unal.edu.co
Claudia Becerra
Universidad Nacional
de Colombia, Bogota
cjbecerrac@unal.edu.co
Alexander Gelbukh
CIC-IPN
Av. Juan Dios B?tiz,
Av. Mendiz?bal, Col.
Nueva Industrial Vallejo,
CP 07738, DF, M?xico
gelbukh@gelbukh.com
Abstract
This paper presents a novel approach for building
adaptive similarity functions based on cardinality us-
ing machine learning. Unlike current approaches
that build feature sets using similarity scores, we
have developed these feature sets with the cardinal-
ities of the commonalities and differences between
pairs of objects being compared. This approach al-
lows the machine-learning algorithm to obtain an
asymmetric similarity function suitable for direc-
tional judgments. Besides using the classic set cardi-
nality, we used soft cardinality to allow flexibility in
the comparison between words. Our approach used
only the information from the surface of the text,
a stop-word remover and a stemmer to address the
cross-lingual textual entailment task 8 at SEMEVAL
2012. We have the third best result among the 29
systems submitted by 10 teams. Additionally, this
paper presents better results compared with the best
official score.
1 Introduction
Adaptive similarity functions are those functions that, be-
yond using the information of two objects being com-
pared, use information from a broader set of objects
(Bilenko and Mooney, 2003). Therefore, the same sim-
ilarity function may return different results for the same
pair of objects, depending on the context of where the
objects are. Adaptability is intended to improve the per-
formance of the similarity function in relation to the task
in question associated with the entire set of objects. For
example, adaptiveness improves relevance of documents
retrieved for a query in an information retrieval task for a
particular document collection.
In text applications there are mainly three methods
to provide adaptiveness to similarity functions: term
weighting, adjustment or learning the parameters of the
similarity function, and machine learning. Term weight-
ing is a common practice that assigns a degree of im-
portance to each occurrence of a term in a text collec-
tion (Salton and Buckley, 1988; Lan et al, 2005). Sec-
ondly, if a similarity function has parameters, these can
be adjusted or learned to adapt to a particular data set.
Depending on the size of the search space defined by
these parameters, they can be adjusted either manually
or using a technique of AI. For instance, Jimenez et
al. manually adjusted a single parameter in the gener-
alized measure of Monge-Elkan (1996) (Jimenez et al,
2009) and Ristrad and Yanilios (1998) learned the costs
of editing operations between particular characters for
the Levenshtein distance (1966) using HMMs. Thirdly,
the machine-learning approach aims to learn a similar-
ity function based on a vector representation of texts us-
ing a subset of texts for training and a learning func-
tion (Bilenko and Mooney, 2003). The three methods
of adaptability can also be used in a variety of combina-
tions, e.g. term weighting in combination with machine
learning (Debole and Sebastiani, 2003; Lan et al, 2005).
Finally, to achieve adaptability, other approaches use data
sets considerably larger, such as large corpora or the Web,
e.g. distributional similarity (Lee, 1999).
In the machine-learning approach, a vector representa-
tion of texts is used in conjunction with an algorithm of
classification or regression (Alpaydin, 2004). Each vec-
tor of features ?f1, f2, . . . , fm? is associated to each pair
?Ti, Tj? of texts. Thus, Bilenko et al (2003) proposed a
set of features indexed by the data set vocabulary, simi-
lar to Zanzotto et al, (2009) who used fragments of parse
trees. However, a more common approach is to select as
features the scores of different similarity functions. Using
these features, the machine-learning algorithm discovers
the relative importance of each feature and a combina-
tion mechanism that maximizes the alignment of the final
result with a gold standard for the particular task.
In this paper, we propose a novel approach to extract
feature sets for a machine-learning algorithm using car-
684
dinalities rather than scores of similarity functions. For
instance, instead of using as a feature the score obtained
by the Dice?s coefficient (i.e. 2?|Ti?Tj |/|Ti|+|Tj |), we use
|Ti|, |Tj | and |Ti ? Tj | as features. The rationale behind
this idea is that despite the similarity scores being suitable
for learning a combined function of similarity, they hide
the information imbalance between the original pair of
texts. Our hypothesis is that the information coded in this
imbalance could provide the machine-learning algorithm
with better information to generate a combined similar-
ity score. For instance, consider these pairs of texts: ?
?The beach house is white.?, ?The house was completely
empty.? ? and ? ?The house?, ?The beach house was com-
pletely empty and isolated? ?. Both pairs have the same
similarity score using the Dice coefficient, but it is evi-
dent that the latter has an imbalance of information lost in
that single score. This imbalance of information is even
more important if the task requires to identify directional
similarities, such as ?T1 is more similar to T2, than T2 is
to T1?.
However, unlike the similarity functions, which are
numerous, there is only one set cardinality. This issue
can be addressed using the soft cardinality proposed by
Jimenez et al (2010), which uses an auxiliary function of
similarity between elements to make a soft count of the
elements in a set. For instance, the classic cardinality of
the set A = { ?Sunday?, ?Saturday? } is |A| = 2; and the
soft cardinality of the same set, using a normalized edit-
distance as auxiliary similarity function, is |A|
?
sim = 1.23
because of the commonalities between both words. Fur-
thermore, soft cardinality allows weighting of elements
giving it additional capacity to adapt.
We used the proposed approach to participate in the
cross-lingual textual-entailment task 8 at SEMEVAL
2012. The task was to recognize bidirectional, forward,
backward or lack of entailment in pairs of texts written
in five languages. We built a system based on the pro-
posed method and the use of surface information of the
text, a stop-word remover and a stemmer. Our system
achieved the third best result in official classification and,
after some debugging, we are reporting better results than
the best official scores.
This paper is structured as follows. Section 2 briefly
describes soft cardinality and other cardinalities for text
applications. Section 3 presents the proposed method.
Experimental validation is presented in Section 4. A brief
discussion is presented in Section 5. Finally, conclusions
are drawn in Section 6.
2 Cardinalities for text
Cardinality is a measure of counting the number of el-
ements in a set. The cardinality of classical set theory
represents the number of non-repeated elements in a set.
However, this cardinality is rigid because it counts in the
same manner very similar or highly differentiated ele-
ments. In text applications, text can be modeled as a
set of words and a desirable cardinality function should
take into account the similarities between words. In this
section, we present some methods to soften the classical
concept of cardinality.
2.1 Lemmatizer Cardinality
The simplest approach is to use a stemmer that collapses
words with common roots in a single lemma. Consider
the sentence: ?I loved, I am loving and I will love you?.
The plain word counting of this sentence is 10 words. The
classical cardinality collapses the three occurrences of the
pronoun ?I? giving a count of 8. However, a lemmatizer
such as Porter?s stemmer (1980) also collapses the words
?loved?, ?loving? and ?love? in a single lemma ?love? for
a count of 6. Thus, when a text is lemmatized, it induces
a relaxation of the classical cardinality of a text. In ad-
dition, to provide corpus adaptability, a weighted version
of this cardinality can add weights associated with each
word occurrence instead of adding 1 for each word (e.g.
tf-idf).
2.2 LCS cardinality
Longest common subsequence (LCS) length is a measure
of the commonalities between two texts, unlike set in-
tersection, taking into account the order. Therefore, a
cardinality function of a pair of texts A and B could
be |A ? B| = len(LCS(A,B)), |A| = len(A) and
|B| = len(B). Functions len(?) and LCS(?, ?) calcu-
late length and LCS respectively, either in character or
word granularity.
2.3 Soft Cardinality
Soft cardinality is a function that uses an auxiliary simi-
larity function to make a soft count of the elements (i.e.
words) in a set (i.e. text) (Jimenez et al, 2010). The aux-
iliary similarity function can be any measure or metric
that returns scores in the interval [0, 1], with 0 being the
lowest degree of similarity and 1 the highest (i.e. identi-
cal words). Clearly, if the auxiliary similarity function is
a rigid comparator that returns 1 for identical words and
0 otherwise, the soft cardinality becomes the classic set
cardinality.
The soft cardinality of a set A = {a1, a2, . . . , a|A|}
can be calculated by the following expression: |A|
?
sim '
?|A|
i wai
(?|A|
j sim(ai, aj)
p
)?1
. Where sim(?, ?) is
the auxiliary similarity function for approximate word
comparison, wai are weights associated with each word
ai, and p is a tuning parameter that controls the degree
of smoothness of the cardinality, i.e. if 0 ? p all ele-
ments in a set are considered identical and if p?? soft
cardinality becomes classic cardinality.
685
2.4 Dot-product VSM ?Cardinality?
Resemblance coefficients are cardinality-based simi-
larity functions. For instance, the Dice coefficient
is the ratio between the cardinality of the intersec-
tion divided by the arithmetic mean of individual
cardinalities:2?|A?B|/|A|+|B|. The cosine coefficient is
similar but instead of using the arithmetic mean it uses
the geometric mean: |A?B|/
?
|A|?
?
|B|. Furthermore, the
cosine similarity is a well known metric used in the vec-
tor space model (VSM) proposed by Salton et al (1975)
cosine(A,B) =
?
wai?wbi??
w2ai?
??
w2bi
. Clearly, this expres-
sion can be compared with the cosine coefficient inter-
preting the dot-product operation in the cosine similar-
ity as a cardinality. Thus, the obtained cardinalities are:
|A ? B|vsm =
?
wpai ? w
p
bi
, |A|vsm =
?
w2pai and
|B|vsm =
?
w2pbi . The exponent p controls the effect
of weighting providing no effect if 0? p or emphasising
the weights if p > 0. In a similar application, Gonza-
lez and Caicedo (2011) used p = 0.5 and normalization
justified by the quantum information retrieval theory.
3 Learning Similarity Functions from
Cardinalities
Different similarity measures use different knowledge,
identify different types of commonalities, and compare
objects with different granularity. In many of the auto-
matic text-processing applications, the qualities of sev-
eral similarity functions may be required to achieve the
final task. The combination of similarity scores with a
machine-learning algorithm to obtain a unified effect for
a particular task is a common practice (Bilenko et al,
2003; Malakasiotis and Androutsopoulos, 2007; Malaka-
siotis, 2009). For each pair of texts for comparison, there
is provided a vector representation based on multiple sim-
ilarity scores as a set of features. In addition, a class at-
tribute is associated with each vector which contains the
objective of the task or the gold standard to be learned by
the machine-learning algorithm.
However, the similarity scores conceal important in-
formation when the task requires dealing with directional
problems, i.e. whenever the order of comparing each pair
of texts is related with the class attribute. For instance,
textual entailment is a directional task since it is neces-
sary to recognize whether the first text entails the second
text or vice versa. This problem can be addressed us-
ing asymmetric similarity functions and including scores
for sim(A,B) and sim(B,A) in the resulting vector for
each pair ?A,B?. Nevertheless, the similarity measures
that are more commonly used are symmetric, e.g. edit-
distance (Levenshtein, 1966), LCS (Hirschberg, 1977),
cosine similarity, and many of the current semantic re-
latedness measures (Pedersen et al, 2004). Although,
there are asymmetric measures such as the Monge-Elkan
measure (1996) and the measure proposed by Corley and
Mihalcea (Corley and Mihalcea, 2005), they are outnum-
bered by the symmetric measures. Clearly, this situation
restricts the use of the machine learning as a method of
combination for directional problems.
Alternatively, we propose the construction of a vector
for each pair of texts using cardinalities instead of sim-
ilarity scores. Moreover, using cardinalities rather than
similarity scores allows the machine-learning algorithm
to discover patterns to cope with directional tasks.
Basically, we propose to use a set with six features for
each cardinality function: |A|, |B|, |A ? B|, |A ? B|,
|A?B| and |B ?A|.
4 Experimental Setup
4.1 Cross-lingual Textual Entailment (CLTE) Task
This task consist of recognizing in a pair of topically re-
lated text fragments T1 and T2 in different languages, one
of the following possible entailment relations: i) bidi-
rectional T1 ? T2 ? T1 ? T2, i.e. semantic equiv-
alence; ii) forward T1 ? T2 ? T1 : T2; iii) back-
ward T1 ; T2 ? T1 ? T2; and iv) no entailment
T1 ; T2 ? T1 : T2. Besides, both T1 and T2 are as-
sumed to be true statements; hence contradictory pairs
are not allowed.
Data sets consist of a collection of 1,000 text pairs
(500 for training and 500 for testing) each one labeled
with one of the possible entailment types. Four balanced
data sets were provided using the following language
pairs: German-English (deu-eng), French-English (fra-
eng), Italian-English (ita-eng) and Spanish-English (spa-
eng). The evaluation measure for experiments was accu-
racy, i.e. the ratio of correctly predicted pairs by the total
number of predictions. For a comprehensive description
of the task see (Negri et al, 2012).
4.2 Experiments
Given that each pair of texts ?T1, T2? are in different lan-
guages, a pair of translations ?T t1 , T
t
2? were provided us-
ing Google Translate service. Thus, each one of the text
pairs ?T1, T t2? and ?T
t
1 , T2? were in the same language.
Then, all produced pairs were pre-processed by remov-
ing stop-words in their respective languages. Finally, all
texts were lemmatized using Porter?s stemmer (1980) for
English and Snowball stemmers for other languages us-
ing an implementation provided by the NLTK (Loper and
Bird, 2002).
Then, different set of features were generated using
similarity scores or cardinalities. While each symmet-
ric similarity function generates 2 features i)sim(T1, T t2)
and ii)sim(T t1 , T2), asymmetric functions generate two
additional features iii)sim(T t2 , T1) and iv)sim(T2, T
t
1).
686
On the other hand, each cardinality function generates
12 features: i) |T1|, ii) |T t2 |, iii) |T1 ? T
t
2 |, iv) |T1 ? T
t
2 |,
v) |T1 ? T t2 |, vi) |T
t
2 ? T1|, vii) |T
t
1 |, viii) |T2|, ix)
|T t1 ? T2|, x) |T
t
1 ? T2|, xi) |T
t
1 ? T2|, and xii) |T2 ? T
t
1 |.
Various combinations of cardinalities, symmetric and
asymmetric functions were used to generate the follow-
ing feature sets:
Sym.simScores: scores of the following symmetric
similarity functions: Jaccard, Dice, and cosine coef-
ficients using classical cardinality and soft cardinality
(edit-distance as auxiliar sim. function). In addition, co-
sine similarity, softTFIDF (Cohen et al, 2003) and edit-
distance (total 18 features).
Asym.LCS.sim: scores of the following asymmetric
similarity functions: sim(T1, T2) = lcs(T1,T2)/len(T1)
and sim(T1, T2) = lcs(T1,T2)/len(T2) at character level (4
features).
Classic.card: cardinalities using classical set cardinal-
ity (12 features).
Dot.card.w: dot-product cardinality using idf weights
as described in Section 2.4, using p = 1 (12 features).
LCS.card: LCS cardinality at word-level using idf
weights as described in Section 2.1 (12 features).
SimScores: combined features sets from
Sym.SimScores, Asym.LCS.sim and the general-
ized Monge-Elkan measure (Jimenez et al, 2009) using
p = 1, 2, 3 (30 features).
Dot.card.w.0.5: same as Dot.card.w using p = 0.5.
Classic.card.w: classical cardinality using idf weights
(12 features).
Soft.card.w: soft cardinality using idf weights as de-
scribed in Section 2.3 using p = 1, 2, 3, 4, 5 (60 features).
The machine-learning classification algorithm for all
feature sets was SVM (Cortes and Vapnik, 1995) with the
complexity parameter C = 1.5 and a linear polynomial
kernel. All experiments were conducted using WEKA
(Hall et al, 2009).
4.3 Results
In Semeval 2012 exercise, participants were given a par-
ticular subdivision into training and test subsets for each
data set. For official results, participants received only the
gold-standard labels for the subset of training, and accu-
racies of each system in the test subset was measured by
the organizers. In Table 1, the results for that particular
division are shown. At the bottom of that table, the of-
ficial results for the first three systems are shown. Our
system, ?3rd.Softcard? was configured using soft cardi-
nality with edit-distance as auxiliary similarity function
and p = 2. Erroneously, at the time of the submission,
all texts in the 5 languages were lemmatized using an En-
glish stemmer and stop-words in all languages were ag-
gregated into a single set before the withdrawal. In spite
of these bugs, our system was the third best score.
FEATURES SPA ITA FRA DEU avg.
Sym.simScores 0.404 0.410 0.410 0.410 0.409
Asym.LCS.sim 0.490 0.492 0.482 0.474 0.485
Classic.card 0.560 0.534 0.570 0.542 0.552
Dot.card.w 0.562 0.568 0.550 0.548 0.557
LCS.card 0.606 0.566 0.568 0.558 0.575
SimScores 0.600 0.562 0.568 0.572 0.576
Dot.card.w.0.5 0.584 0.574 0.586 0.572 0.579
Classic.card.w 0.584 0.576 0.588 0.590 0.585
Soft.card.w 0.598 0.602 0.624 0.604 0.607
SEMEVAL 2012 OFFICIAL RESULTS
1st.HDU.run2 0.632 0.562 0.570 0.552 0.579
2nd.HDU.run1 0.630 0.554 0.564 0.558 0.577
3rd.Softcard 0.552 0.566 0.570 0.550 0.560
Table 1: Accuracy results for Semeval2012 task 8
Soft.card.w 60.174(1.917)% imprv. Sign.
Sym.simScore 39.802(1.783)% 51.2% <0.001
Asym.LCS.sim 48.669(1.820)% 23.6% <0.001
Classic.card 55.278(2.422)% 8.9% 0.010
Dot.card.w 54.906(2.024)% 9.6% 0.004
LCS.card 55.131(2.471) % 9.1% 0.015
SimScores 56.889(2.412) % 5.8% 0.124
Dot.card.w.0.5 57.114(2.141)% 5.4% 0.059
Classic.card.w 56.708(2.008)% 6.1% 0.017
Table 2: Average accuracy comparison vs. Soft.card.w in 100
runs
To compare our approach of using feature sets based
on soft cardinality versus other approaches, we gener-
ated 100 random training-test subdivisions (50%-50%) of
each data set. The average results were compared and
tested statistically with the paired T-tested corrected test.
Results, deviations, the percentage of improvement, and
its significance in comparison with the Soft.card.w sys-
tem are shown in Table2.
5 Discusion
Results in Table 2 show that our hypothesis that fea-
ture sets obtained from cardinalities should outperform
features sets obtained from similarity scores was de-
mostrated when compared versus similarity functions al-
ternatively symmetrical or asymetrical. However, when
our approach is compared with a feature set obtained by
combining symmetric and asymmetric functions, we ob-
tained an improvement of 5.8% but only with a signif-
icance of 0.124. Regarding soft cardinality compared
to alternative cardinalities, soft cardinality outperformed
others in all cases with significance <0.059.
687
6 Conclusions
We have proposed a new method to compose feature sets
using cardinalities rather than similarity scores. Our ap-
proach proved to be effective for directional text compar-
ison tasks such as textual entailment. Furthermore, the
soft cardinality function proved to be the best for obtain-
ing such sets of features.
Acknowledgments
This research was funded by the Systems and Industrial
Engineering Department, the Office of Student Welfare
of the National University of Colombia, Bogot?, and
throught a grant from the Colombian Department for
Science, Technology and Innovation Colciencias, proj.
110152128465. The second author recognizes the sup-
port from Mexican Government (SNI, COFAA-IPN, SIP
20113295, CONACYT 50206-H) and CONACYT?DST
India (proj. ?Answer Validation through Textual Entail-
ment?).
References
Ethem Alpaydin. 2004. Introduction to Machine Learning.
MIT press.
Mikhail Bilenko and Raymond J. Mooney. 2003. Adaptive du-
plicate detection using learnable string similarity measures.
In Proc. of the ninth ACM SIGKDD international conference
on Knowledge discovery and data mining, Washington, D.C.
Mikhail Bilenko, Raymond Mooney, William Cohen, Pradeep
Ravikumar, and Stephen Fienberg. 2003. Adaptive name
matching in information integration. IEEE Intelligent Sys-
tems, 18(5):16?23.
William W Cohen, Pradeep Ravikumar, and Stephen E Fien-
berg. 2003. A comparison of string distance metrics for
Name-Matching tasks. In Proc. of the IJCAI2003 Workshop
on Information Integration on the Web II Web03.
Courtney Corley and Rada Mihalcea. 2005. Measuring the se-
mantic similarity of texts. In Proceedings of the ACL Work-
shop on Empirical Modeling of Semantic Equivalence and
Entailment, Stroudsburg, PA.
Corinna Cortes and Vladimir N. Vapnik. 1995. Support-Vector
networks. Machine Learning, 20(3):273?297.
Franca Debole and Fabrizio Sebastiani. 2003. Supervised term
weighting for automated text categorization. In Proc. of the
2003 ACM symposium on applied computing, New York,
NY.
Fabio A. Gonzalez and Juan C. Caicedo. 2011. Quantum la-
tent semantic analysis. In Proc. of the Third international
conference on Advances in information retrieval theory.
Mark Hall, Frank Eibe, Geoffrey Holmes, and Bernhard
Pfahringer. 2009. The WEKA data mining software: An
update. SIGKDD Explorations, 11(1):10?18.
Daniel S. Hirschberg. 1977. Algorithms for the longest com-
mon subsequence problem. J. ACM, 24(4):664?675.
Sergio Jimenez, Claudia Becerra, Alexander Gelbukh, and
Fabio Gonzalez. 2009. Generalized Monge-Elkan method
for approximate text string comparison. In Computational
Linguistics and Intelligent Text Processing, volume 5449 of
LNCS, pages 559?570.
Sergio Jimenez, Fabio Gonzalez, and Alexander Gelbukh.
2010. Text comparison using soft cardinality. In String Pro-
cessing and Information Retrieval, volume 6393 of LNCS,
pages 297?302.
Man Lan, Chew-Lim Tan, Hwee-Boon Low, and Sam-Yuan
Sung. 2005. A comprehensive comparative study on term
weighting schemes for text categorization with support vec-
tor machines. In Special interest tracks and posters of the
14th international conference on World Wide Web, New
York, NY.
Lillian Lee. 1999. Measures of distributional similarity. In
Proc. of the 37th annual meeting of the Association for Com-
putational Linguistics on Computational Linguistics, Col-
lege Park, Maryland.
Vladimir I. Levenshtein. 1966. Binary codes capable of cor-
recting deletions, insertions, and reversals. Soviet Physics
Doklady, 10(8):707?710.
Edward Loper and Steven Bird. 2002. NLTK: the natural lan-
guage toolkit. In In Proceedings of the ACL Workshop on
Effective Tools andMethodologies for Teaching Natural Lan-
guage Processing and Computational Linguistics, Philadel-
phia, PA.
Prodromos Malakasiotis and Ion Androutsopoulos. 2007.
Learning textual entailment using SVMs and string similarity
measures. In Proc. of the ACL-PASCALWorkshop on Textual
Entailment and Paraphrasing, Stroudsburg, PA.
Prodromos Malakasiotis. 2009. Paraphrase recognition using
machine learning to combine similarity measures. In Proc. of
the ACL-IJCNLP 2009 Student Research Workshop, Strouds-
burg, PA.
Alvaro E. Monge and Charles Elkan. 1996. The field matching
problem: Algorithms and applications. In Proc. KDD-96,
Portland, OR.
Matteo Negri, Alessandro Marchetti, Yashar Mehdad, Luisa
Bentivogli, and Danilo Giampiccolo. 2012. 2012. semeval-
2012 task 8: Cross-lingual textual entailment for content syn-
chronization. In In Proc. of the 6th International Workshop
on Semantic Evaluation (SemEval 2012), Montreal, Canada.
Ted Pedersen, Siddharth Patwardhan, and Jason Michelizzi.
2004. WordNet::Similarity: measuring the relatedness of
concepts. In Proc. HLT-NAACL?Demonstration Papers,
Stroudsburg, PA.
Martin Porter. 1980. An algorithm for suffix stripping. Pro-
gram, 3(14):130?137.
Eric S. Ristad and Peter N. Yianilos. 1998. Learning string
edit distance. IEEE Transactions on Pattern Analysis and
Machine Intelligence, 20(5):522?532.
Gerard Salton and Christopher Buckley. 1988. Term-weighting
approaches in automatic text retrieval. Information Process-
ing & Management, 24(5):513?523.
Gerard Salton, Andrew K. C. Wong, and Chung-Shu Yang.
1975. A vector space model for automatic indexing. Com-
mun. ACM, 18(11):613?620.
Fabio Massimo Zanzotto, Marco Pennacchiotti, and Alessan-
dro Moschitti. 2009. A machine learning approach to tex-
tual entailment recognition. Natural Language Engineering,
15(Special Issue 04):551?582.
688
First Joint Conference on Lexical and Computational Semantics (*SEM), pages 689?695,
Montre?al, Canada, June 7-8, 2012. c?2012 Association for Computational Linguistics
JU_CSE_NLP: Language Independent Cross-lingual  
Textual Entailment System 
 
Snehasis Neogi1, Partha Pakray2, Sivaji Bandyopadhyay1, 
Alexander Gelbukh3 
1
Computer Science & Engineering Department 
Jadavpur University, Kolkata, India 
2
Computer Science & Engineering Department 
Jadavpur University, Kolkata, India 
Intern at Xerox Research Centre Europe 
Grenoble, France 
3
Center for Computing Research 
National Polytechnic Institute 
Mexico City, Mexico 
{snehasis1981,parthapakray}@gmail.com 
sbandyopadhyay@cse.jdvu.ac.in 
gelbukh@gelbukh.com 
 
Abstract 
This article presents the experiments car-
ried out at Jadavpur University as part of 
the participation in Cross-lingual Textual 
Entailment for Content Synchronization 
(CLTE) of task 8 @ Semantic Evaluation 
Exercises (SemEval-2012). The work ex-
plores cross-lingual textual entailment as a 
relation between two texts in different lan-
guages and proposes different measures 
for entailment decision in a four way clas-
sification tasks (forward, backward, bidi-
rectional and no-entailment). We set up 
different heuristics and measures for eva-
luating the entailment between two texts 
based on lexical relations. Experiments 
have been carried out with both the text 
and hypothesis converted to the same lan-
guage using the Microsoft Bing translation 
system. The entailment system considers 
Named Entity, Noun Chunks, Part of 
speech, N-Gram and some text similarity 
measures of the text pair to decide the en-
tailment judgments. Rules have been de-
veloped to encounter the multi way 
entailment issue. Our system decides on 
the entailment judgment after comparing 
the entailment scores for the text pairs. 
Four different rules have been developed 
for the four different classes of entailment. 
The best run is submitted for Italian ? 
English language with accuracy 0.326. 
1 Introduction 
Textual Entailment (TE) (Dagan and Glick-
man, 2004) is one of the recent challenges of 
Natural Language Processing (NLP). The Task 
8 of SemEval-20121 [1] defines a textual en-
tailment system that specifies two major as-
pects: the task is based on cross-lingual 
corpora and the entailment decision must be 
four ways. Given a pair of topically related text 
fragments (T1 and T2) in different languages, 
the CLTE task consists of automatically anno-
tating it with one of the following entailment 
judgments: 
i. Bidirectional (T1 ->T2 & T1 <- T2): the two 
fragments entail each other (semantic equiva-
lence)  
ii. Forward (T1 -> T2 & T1!<- T2): unidirec-
tional  entailment from T1 to T2 . 
iii. Backward (T1! -> T2 & T1 <- T2): unidirec-
tional entailment from T2 to T1.  
iv. No Entailment (T1! -> T2 & T1! <- T2): 
there is no entailment between T1 and T2. 
CLTE (Cross Lingual Textual Entailment) task 
consists of 1,000 CLTE dataset pairs (500 for 
                                                          
1http://www.cs.york.ac.uk/semeval2012/index.php?id=tasks 
689
training and 500 for test) available for the fol-
lowing language combinations: 
     - Spanish/English (spa-eng)  
     - German/English (deu-eng). 
     - Italian/English (ita-eng)  
     - French/English (fra-eng) 
 
Seven Recognizing Textual Entailment (RTE) 
evaluation tracks have already been held: RTE-1 
in 2005 [2], RTE-2 [3] in 2006, RTE-3 [4] in 
2007, RTE-4 [5] in 2008, RTE-5 [6] in 2009, 
RTE-6 [7] in 2010 and RTE-7 [8] in 2011. RTE 
task produces a generic framework for entail-
ment task across NLP applications. The RTE 
challenges have moved from 2 ? way entailment 
task (YES, NO) to 3 ? way task (YES, NO, 
UNKNOWN). EVALITA/IRTE [9] task is simi-
lar to the RTE challenge for the Italian language. 
So far, TE has been applied only in a monolin-
gual setting. Cross-lingual Textual Entailment 
(CLTE) has been proposed ([10], [11], [12]) as 
an extension of Textual Entailment. In 2010, 
Parser Training and Evaluation using Textual 
Entailment [13] was organized by SemEval-2. 
Recognizing Inference in Text (RITE)2 orga-
nized by NTCIR-9 in 2011 is the first to expand 
TE as a 5-way entailment task (forward, back-
ward, bi-directional, contradiction and indepen-
dent) in a monolingual scenario [14].  
We have participated in RTE-5 [15], RTE-6 
[16], RTE-7 [17], SemEval-2 Parser Training 
and Evaluation using Textual Entailment Task 
and RITE [18]. 
Section 2 describes our Cross-lingual Textual 
Entailment system. The various experiments 
carried out on the development and test data sets 
are described in Section 3 along with the results. 
The conclusions are drawn in Section 4. 
2 System Architecture  
Our system for CLTE task is based on a set of 
heuristics that assigns entailment scores to a text 
pair based on lexical relations. The text and the 
hypothesis in a text pair are translated to the 
same language using the Microsoft Bing ma-
chine translation system. The system separates 
the text pairs (T1 and T2) available in different 
languages and preprocesses them. After prepro-
                                                          
2 http://artigas.lti.cs.cmu.edu/rite/Main_Page 
cessing we have used several techniques such as 
Word Overlaps, Named Entity matching, Chunk 
matching, POS matching to evaluate the sepa-
rated text pairs. These modules return a set of 
score statistics, which helps the system to go for 
multi-class entailment decision based on the 
predefined rules. We have submitted 3 runs for 
each language pair for the CLTE task and there 
are some minor differences in the architectures 
that constitute the 3 runs. The three system ar-
chitectures are described in section 2.1, section 
2.2 and section 2.3. 
2.1 System Architecture 1: CLTE Task 
with  Translated English Text  
The system architecture of Cross-lingual textual 
entailment consists of various components such 
as Preprocessing Module, Lexical Similarity 
Module, Text Similarity Module. Lexical Simi-
larity module again is divided into subsequent 
modules like POS matching, Chunk matching 
and Named Entity matching. Our system calcu-
lates these measures twice once considering T1 
as text and T2 as hypothesis and once T2 as text 
and T1 as hypothesis. The mapping is done in 
both directions T1-to-T2 and T2-to-T1 to arrive 
at the appropriate four way entailment decision 
using a set of rules. Each of these modules is 
now being described in subsequent subsections. 
Figure 1 shows our system architecture where 
the text sentence is translated to English. 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
Figure 1: System Architecture 
 
CLTE Task Data 
(T1, T2) 
T1.txt 
T2.txt 
 
Translated in 
Eng.  Using Bing 
Translator 
Preprocessing 
(Stop word removal, 
Co referencing) 
 
N-Gram Module 
Preprocessing 
(Stop word removal, 
Co referencing) 
Chunking Module 
Text Similarity Module Named Entity POS Module 
? Lexical Score (S1) 
S1 
? Lexical Score (S2) 
S1 
If (S1>S2) Then Entailment = ?forward? 
If (S1<S2) Then Entailment = ?backward? 
If (S1=S2) or (abs (S1-S2) <Threshold) Then Entailment = ?bidirectional? 
(fra, ita, deu, 
spa language) 
T1- Text 
T2- Hypothesis 
 
T1 ? Hypothesis 
T2 - Text 
 
If (S1=S2 and (S1=S2) <Threshold) Then Entailment = ?no_entailment? 
(English 
language) 
 
690
2.1.1 Preprocessing Module 
The system separates the T1 and T2 pair from 
the CLTE task data. T1 sentences are in differ-
ent languages (In French, Italian, German and 
Spanish) where as T2 sentences are in English. 
Microsoft Bing translator3 API for Bing transla-
tor (microsoft-translator-java-api-0.4-jar-with-
dependencies.jar) is being used to translate the 
T1 text sentences into English. The translated 
T1 and T2 sentences are passed through the two 
sub modules. 
i. Stop word Removal: Stop words are removed 
from the T1 and T2 sentences. 
ii. Co-reference: Co?reference chains are eva-
luated for the datasets before passing them to the 
TE module. The objective is to increase the en-
tailment score after substituting the anaphors 
with their antecedents. A word or phrase in the 
sentence is used to refer to an entity introduced 
earlier or later in the discourse and both having 
same things then they have the same referent or 
co-reference. When the reader must look back to 
the previous context, co-reference is called 
"Anaphoric Reference". When the reader must 
look forward, it is termed "Cataphoric Refer-
ence". To address this problem we used a tool 
called JavaRAP4 (A java based implementation 
of Anaphora Procedure (RAP) - an algorithm by 
Lappin and Leass (1994)). It has been observed 
that the presence of co ? referential expressions 
are very small in sentence based paradigm.   
2.1.2 Lexical Based Textual Entailment 
(TE) Module 
T1 - T2 pairs are the inputs to the system. The 
TE module is executed once by considering T1 
as text and T2 as hypothesis and again by consi-
dering T2 as text and T1 as hypothesis. The 
overall TE module is a collection of several lex-
ical based sub modules.  
i. N-Gram Match module: The N-Gram match 
basically measures the percentage match of the 
unigram, bigram and trigram of hypothesis 
present in the corresponding text. These scores 
are simply combined to get an overall N ? Gram 
matching score for a particular pair. By running 
                                                          
3 http://code.google.com/p/microsoft-translator-java-api/ 
4 http://aye.comp.nus.edu.sg/~qiu/NLPTools/JavaRAP.html 
the module we get two scores, one for T1-T2 
pair and another for T2-T1 pair. 
       
ii. Chunk Similarity module: In this sub mod-
ule our system evaluates the key NP-chunks of 
both text and hypothesis identified using NP 
Chunker v1.15. Then our system checks the 
presence of NP-Chunks of hypothesis in the cor-
responding text. System calculates the overall 
value for the chunk matching, i.e., number of 
text NP-chunks that match with hypothesis NP-
chunks. If the chunks are not similar in their sur-
face form then our system goes for WordNet 
matching for the words and if they match in 
WordNet synsets information, the chunks are 
considered as similar. 
WordNet [19] is one of most important resource 
for lexical analysis. The WordNet 2.0 has been 
used for WordNet based chunk matching. The 
API for WordNet Searching (JAWS)6 is an API 
that provides Java applications with the ability 
to retrieve data from the WordNet database. Let 
us consider the following example taken from 
training data: 
 
T1: Due/JJ to/TO [an/DT error/NN of/IN com-
munication/NN] between/IN [the/DT police/NN] 
? 
T2: On/IN [Tuesday/NNP] [a/DT failed/VBN 
communication/NN] between/IN? 
 
The chunk in T1 [error communication] matches 
with T2 [failed communication] via WordNet 
based synsets information. A weight is assigned 
to the score depending upon the nature of chunk 
matching. 
 
 
 
                   M[i] = Wm[i] * ? / Wc[i] 
Where N= Total number of chunk containing 
hypothesis. 
M[i] = Match Score of the ith  Chunk. 
Wm[i] = Number of words matched in the i
th 
chunk. 
Wc[i] = Total words in the i
th chunk. 
                    1 if surface word matches. 
and ? = 
                ? if matche via WordNet 
                                                          
5 http://www.dcs.shef.ac.uk/~mark/phd/software/ 
6 http://lyle.smu.edu/~tspell/jaws/index.html 
691
System takes into consideration several text si-
milarity measures calculated over the T1-T2 
pair. These text similarity measures are summed 
up to produce a total score for a particular text 
pair. Similar to the Lexical module, text simi-
larity module is also executed for both T1-T2 
and T2-T1 pairs.   
iii. Text Distance Module: The following major 
text similarity measures have been considered 
by our system. The text similarity measure 
scores are added to generate the final text dis-
tance score. 
 
?   Cosine Similarity 
?   Levenshtein Distance 
?   Euclidean Distance 
?   MongeElkan Distance 
?   NeedlemanWunch Distance 
?   SmithWaterman Distance 
?   Block Distance 
?   Jaro Similarity 
?   MatchingCoefficient Similarity 
?   Dice Similarity 
?   OverlapCoefficient 
?   QGrams Distance 
 
iv. Named Entity Matching: It is based on the 
detection and matching of Named Entities in the 
T1-T2 pair. Stanford Named Entity Recognizer7 
(NER) is used to tag the Named Entities in both 
T1 and T2. System simply matches the number 
of hypothesis NEs present in the text. A score is 
allocated for the matching. 
NE_match = (Number of common NEs in Text 
and Hypothesis)/(Number of NEs in Hypothe-
sis). 
v. Part-of-Speech (POS) Matching: This mod-
ule basically deals with matching the common 
POS tags between T1 and T2 pair. Stanford POS 
tagger8 is used to tag the part of speech in both 
T1 and T2. System matches the verb and noun 
POS words in the hypothesis that match in the 
text. A score is allocated based on the number of 
POS matching.  
 
POS_match = (Number of verb and noun                            
POS in Text and Hypothesis)/(Total number of 
verb and noun POS in hypothesis).    
                                                          
7 http://nlp.stanford.edu/software/CRF-NER.shtml 
8 http://nlp.stanford.edu/software/tagger.shtml 
System adds all the lexical matching scores to 
evaluate the total score for a particular T1- T2 
pair, i.e.,  
    Pair1:  (T1 ? Text and T2 ? Hypothesis) 
    Pair2:   (T1 ? Hypothesis and T2 - Text). 
Total lexical score for each pair can be mathe-
matically represented by: 
 
 
where S1 represents the score for the pair with 
T1 as text and T2 as hypothesis while S2 
represents the score from T1 to T2. The figure 2 
shows the sample output values of the TE mod-
ule. 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
Figure 2: output values of this module 
 
The system finally compares the above two val-
ues S1 and S2 as obtained from the lexical mod-
ule to go for four-class entailment decision. If 
score S1, i.e., the mapping score with T1 as text 
and T2 as hypothesis is greater than the score 
S2, i.e., mapping score with T2 as text and T1 as 
hypothesis, then the entailment class will be 
?forward?. Similarly if S1 is less than S2, i.e., 
T2 now acts as the text and T1 acts as the hypo-
thesis then the entailment class will be ?back-
ward?. Similarly if both the scores S1 and S2 are 
equal the entailment class will be ?bidirectional? 
(entails in both directions). Measuring ?bidirec-
tional? entailment is much more difficult than 
any other entailment decision due to combina-
tions of different lexical scores. As our system 
produces a final score (S1 and S2) that is basi-
cally the sum over different similarity measures, 
 
692
the tendency of identical S1 ? S2 will be quite 
small. As a result we establish another heuristic 
for ?bidirectional? class. If the absolute value 
difference between S1 and S2 is below the thre-
shold value, our system recognizes the pair as 
?bidirectional? (abs (S1 ? S2) < threshold). This 
threshold has been set as 5 based on observation 
from the training file. If the individual scores S1 
and S2 are below a certain threshold, again set 
based on the observation in the training file, then 
system concludes the entailment class as 
?no_entailment?. This threshold has been set as 
20 based on observation from the training file. 
2.2 System Architecture 2: CLTE Task 
with translated hypothesis  
System Architecture 2 is based on lexical match-
ing between the text pairs (T1, T2) and basically 
measures the same attributes as in the architec-
ture 1. In this architecture, the English hypothe-
sis sentences are translated to the language of 
the text sentence (French, Italian, Spanish and 
German) using the Microsoft Bing Translator. 
The CLTE dataset is preprocessed after separat-
ing the (T1, T2) pairs. Preprocessing module 
includes stop word removal and co-referencing. 
After preprocessing, the system executes the TE 
module for lexical matching between the text 
pairs. This module comprises N-Gram matching, 
Text Similarity, Named Entity Matching, POS 
matching and Chunking. The TE module is ex-
ecuted once with T1 as text and T2 as hypothe-
sis and again with T1 as hypothesis and T2 as 
text. But in this architecture N-Gram matching 
and text similarity modules differ from the pre-
vious architecture. In system architecture 1, the 
N-Gram matching and text similarity values are 
calculated on the English text translated from T1 
(i.e., Text in Spanish, German, French and Ital-
ian languages). In system architecture 2, the Mi-
crosoft Bing translator is used to translate T2 
texts (in English) to different languages (i.e. in 
Spanish, German, French and Italian) and calcu-
late N ? Gram matching and Text Similarity 
values on these (T1 ? newly translated T2) pairs. 
Other lexical sub modules are executed as be-
fore. These lexical matching scores are stored 
and compared according to the heuristic defined 
in section 2.1.    
2.3 System Architecture 3: CLTE task 
using Voting 
The system considers the output of the previous 
two systems (Run 1 from System architecture 1 
and Run 2 from System architecture 2) as input. 
If the entailment decision of both the runs agrees 
then this is output as the final entailment label. 
Otherwise, if they do not agree, the final entail-
ment label will be ?no_entailment?. The voting 
rule can be defined as the ANDing rule where 
logical AND operation of the two inputs are 
considered to arrive at the final evaluation class. 
3 Experiments on Datasets and Results   
Three runs (Run 1, Run 2 and Run 3) for each 
language were submitted for the SemEval-3 
Task 8. The descriptions of submissions for the 
CLTE task are as follows: 
 
? Run1: Lexical matching between text pairs 
(Based on system Architecture ? 1). 
? Run2: Lexical matching between text pairs  
    (Based on System Architecture ? 2). 
? Run3: ANDing Module between Run1 and  
          Run2. (Based on System Architecture ?3). 
 
The CLTE dataset consists of 500 training 
CLTE pairs and 500 test CLTE pairs. The re-
sults for Run 1, Run 2 and Run 3 for each lan-
guage on CLTE Development set are shown in 
Table 1.  
 
Run Name Accuracy 
JU-CSE-NLP_deu-eng_run1 0.284 
JU-CSE-NLP_deu-eng_run2 0.268 
JU-CSE-NLP_deu-eng_run3 0.270 
JU-CSE-NLP_fra-eng_run1 0.290 
JU-CSE-NLP_fra-eng_run2 0.320 
JU-CSE-NLP_fra-eng_run3 0.278 
JU-CSE-NLP_ita-eng_run1 0.302 
JU-CSE-NLP_ita-eng_run2 0.298 
JU-CSE-NLP_ita-eng_run3 0.298 
JU-CSE-NLP_spa-eng_run1 0.270 
JU-CSE-NLP_spa-eng_run2 0.262 
JU-CSE-NLP_spa-eng_run3 0.262 
 
Table 1: Results on Development set 
 
693
The comparison of the runs for different lan-
guages shows that in case of deu-eng language 
pair system architecture ? 1 is useful for devel-
opment data whereas system architecture ? 2 is 
more accurate for test data. For fra-eng language 
pair, system architecture - 2 is more accurate for 
development data whereas voting helps to get 
more accurate results for test data. Similar to the 
deu-eng language pair, ita-eng language pair 
shows same trends, i.e., system architecture ? 1 
is more helpful for development data and system 
architecture ? 2 is more accurate for test data. In 
case of spa-eng language pair system architec-
ture ? 1 is helpful for both development and test 
data. 
 
The results for Run 1, Run 2 and Run 3 for each 
language on CLTE Test set are shown in Table 
2. 
 
Run Name Accuracy 
JU-CSE-NLP_deu-eng_run1 0.262 
JU-CSE-NLP_deu-eng_run2 0.296 
JU-CSE-NLP_deu-eng_run3 0.264 
JU-CSE-NLP_fra-eng_run1 0.288 
JU-CSE-NLP_fra-eng_run2 0.294 
JU-CSE-NLP_fra-eng_run3 0.296 
JU-CSE-NLP_ita-eng_run1 0.316 
JU-CSE-NLP_ita-eng_run2 0.326 
JU-CSE-NLP_ita-eng_run3 0.314 
JU-CSE-NLP_spa-eng_run1 0.274 
JU-CSE-NLP_spa-eng_run2 0.266 
JU-CSE-NLP_spa-eng_run3 0.272 
 
Table 2: Results on Test Set 
4 Conclusions and Future Works 
We have participated in Task 8 of Semeval-2012 
named Cross Lingual Textual Entailment mainly 
based on lexical matching and translation of text 
and hypothesis sentences in the cross lingual 
corpora. Both lexical matching and translation 
have their limitations. Lexical matching is useful 
for simple sentences but fails to retain high ac-
curacy for complex sentences with number of 
clauses. Semantic graph matching or conceptual 
graph is a good substitution to overcome these 
limitations. Machine learning technique is 
another important tool for multi-class entailment 
task. Features can be trained by some machine 
learning tools (such as SVM, Na?ve Bayes or 
Decision tree etc.) with multi-way entailment 
(forward, backward, bi-directional, no-
entailment) as its class. Works have been started 
in these directions. 
Acknowledgments 
The work was carried out under partial support 
of the DST India-CONACYT Mexico project 
?Answer Validation through Textual Entail-
ment? funded by DST, Government of India and 
partial support of the project CLIA Phase II 
(Cross Lingual Information Access) funded by 
DIT, Government of India. 
References  
[1] Negri, M., Marchetti, A., Mehdad, Y., Bentivogli, 
L., and Giampiccolo, D.: Semeval-2012 Task 8: 
Cross-lingual Textual Entailment for Content Syn-
chronization. In Proceedings of the 6th International 
Workshop on Semantic Evaluation (SemEval 2012). 
[2]  Dagan, I., Glickman, O., Magnini, B.: The 
PASCAL Recognising Textual Entailment Chal-
lenge. Proceedings of the First PASCAL Recog-
nizing Textual Entailment Workshop. (2005). 
[3] Bar-Haim, R., Dagan, I., Dolan, B., Ferro, L., 
Giampiccolo, D., Magnini, B., Szpektor, I.: The-
Seond PASCAL Recognising Textual Entailment 
Challenge. Proceedings of the Second PASCAL 
Challenges Workshop on Recognising Textual En-
tailment, Venice, Italy (2006). 
[4] Giampiccolo, D., Magnini, B., Dagan, I., Dolan, 
B.: The Third PASCAL Recognizing Textual En-
tailment Challenge. In Proceedings of the ACL-
PASCAL Workshop on Textual Entailment and 
Paraphrasing, Prague, Czech Republic. (2007). 
[5] Giampiccolo, D., Dang, H. T., Magnini, B., Da-
gan, I., Cabrio, E.: The Fourth PASCAL Recogniz-
ing Textual Entailment Challenge. In TAC 2008 
Proceedings. (2008) 
[6] Bentivogli, L., Dagan, I., Dang. H.T., Giampicco-
lo, D., Magnini, B.: The Fifth PASCAL Recogniz-
ing Textual Entailment Challenge. In TAC 2009 
Workshop, National Institute of Standards and 
Technology Gaithersburg, Maryland USA. (2009). 
[7] Luisa Bentivogli, Peter Clark, Ido Dagan, Hoa 
Trang Dang,Danilo Giampiccolo: The Sixth 
PASCAL Recognizing Textual Entailment Chal-
694
lenge. In TAC 2010 Notebook Proceedings. 
(2010) 
[8] Bentivogli, L., Clark, P., Dagan, I., Dang, H., 
Giampiccolo, D.: The Seventh PASCAL Recogniz-
ing Textual Entailment Challenge. In TAC 2011 
Notebook Proceedings. (2011) 
[9] Bos, Johan, Fabio Massimo Zanzotto, and Marco 
Pennacchiotti. 2009. Textual Entailment at 
EVALITA 2009: In Proceedings of EVALITA 
2009. 
[10] Mehdad, Yashar, Matteo Negri, and Marcello 
Federico.2010. Towards Cross-Lingual Textual 
entailment. In Proceedings of the 11th Annual 
Conference of the North American Chapter of the 
Association for Computational Linguistics, 
NAACL-HLT 2010. LA, USA. 
[11] Negri, Matteo, and Yashar Mehdad. 2010. 
Creating a Bilingual Entailment Corpus through 
Translations with Mechanical Turk: $100 for a 
10-day Rush. In Proceedings of the NAACL-HLT 
2010, Creating Speech and Text Language Data 
With Amazon's Mechanical Turk Workshop. LA, 
USA. 
[12] Mehdad, Yashar, Matteo Negri, Marcello Fede-
rico. 2011. Using Bilingual Parallel Corpora for 
Cross-Lingual Textual Entailment. In Proceedings 
of ACL 2011. 
[13] Yuret, D., Han, A., Turgut, Z.: SemEval-2010 
Task 12: Parser Evaluation using Textual Entail-
ments. Proceedings of the SemEval-2010 Evalua-
tion Exercises on Semantic Evaluation. (2010).  
 
[14] H. Shima, H. Kanayama, C.-W. Lee, C.-J. Lin,T. 
Mitamura, S. S. Y. Miyao, and K. Takeda. Over-
view of ntcir-9 rite: Recognizing inference in text. 
In NTCIR-9 Proceedings,2011. 
[15]  Pakray, P., Bandyopadhyay, S., Gelbukh, A.: 
Lexical based two-way RTE System at RTE-5. Sys-
tem Report, TAC RTE Notebook. (2009) 
 
[16] Pakray, P., Pal, S., Poria, S., Bandyopadhyay, S., 
, Gelbukh, A.: JU_CSE_TAC: Textual Entailment 
Recognition System at TAC RTE-6. System Re-
port, Text Analysis Conference Recognizing Tex-
tual Entailment Track (TAC RTE) Notebook. 
(2010) 
 
[17] Pakray, P., Neogi, S., Bhaskar, P., Poria, S., 
Bandyopadhyay, S., Gelbukh, A.: A Textual En-
tailment System using Anaphora Resolution. Sys-
tem Report. Text Analysis Conference 
Recognizing Textual Entailment Track Notebook, 
November 14-15. (2011) 
 
[18] Pakray, P., Neogi, S., Bandyopadhyay, S., Gel-
bukh, A.: A Textual Entailment System using Web 
based Machine Translation System. NTCIR-9, Na-
tional Center of Sciences, Tokyo, Japan. Decem-
ber 6-9, 2011. (2011) 
 
[19]  Fellbaum, C.: WordNet: An Electronic Lexical 
Database. MIT Press (1998). 
695
Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 1: Proceedings of the Main Conference
and the Shared Task, pages 194?201, Atlanta, Georgia, June 13-14, 2013. c?2013 Association for Computational Linguistics
SOFTCARDINALITY-CORE: Improving Text Overlap with
Distributional Measures for Semantic Textual Similarity
Sergio Jimenez, Claudia Becerra
Universidad Nacional de Colombia
Ciudad Universitaria,
edificio 453, oficina 114
Bogot?, Colombia
sgjimenezv@unal.edu.co
cjbecerrac@unal.edu.co
Alexander Gelbukh
CIC-IPN
Av. Juan Dios B?tiz, esq. Av. Mendiz?bal,
Col. Nueva Industrial Vallejo,
CP 07738, DF, M?xico
www.gelbukh.com
Abstract
Soft cardinality has been shown to be a very
strong text-overlapping baseline for the task of
measuring semantic textual similarity (STS),
obtaining 3rd place in SemEval-2012. At
*SEM-2013 shared task, beside the plain text-
overlapping approach, we tested within soft
cardinality two distributional word-similarity
functions derived from the ukWack corpus.
Unfortunately, we combined these measures
with other features using regression, obtain-
ing positions 18th, 22nd and 23rd among the
90 participants systems in the official rank-
ing. Already after the release of the gold stan-
dard annotations of the test data, we observed
that using only the similarity measures with-
out combining them with other features would
have obtained positions 6th, 7th and 8th; more-
over, an arithmetic average of these similarity
measures would have been 4th(mean=0.5747).
This paper describes both the 3 systems as
they were submitted and the similarity mea-
sures that would obtained those better results.
1 Introduction
The task of textual semantic similarity (STS) con-
sists in providing a similarity function on pairs of
texts that correlates with human judgments. Such
a function has many practical applications in NLP
tasks (e.g. summarization, question answering, tex-
tual entailment, paraphrasing, machine translation
evaluation, among others), which makes this task
particularly important. Numerous efforts have been
devoted to this task (Lee et al, 2005; Mihalcea et al,
2006) and major evaluation campaigns have been
held at SemEval-2012 (Agirre et al, 2012) and in
*SEM-2013 (Agirre et al, 2013).
The experimental setup of STS in 2012 consisted
of three data sets, roughly divided in 50% for train-
ing and for testing, which contained text pairs manu-
ally annotated as a gold standard. Furthermore, two
data sets were provided for surprise testing. The
measure of performance was the average of the cor-
relations per data set weighted by the number of
pairs in each data set (mean). The best performing
systems were UKP (B?r et al, 2012) mean=0.6773,
TakeLab (?aric et al, 2012) mean=0.6753 and soft
cardinality (Jimenez et al, 2012) mean=0.6708.
UKP and TakeLab systems used a large number of
resources (see (Agirre et al, 2012)) such as dictio-
naries, a distributional thesaurus, monolingual cor-
pora, Wikipedia, WordNet, distributional similar-
ity measures, KB similarity, POS tagger, machine
learning and others. Unlike those systems, the soft
cardinality approach used mainly text overlapping
and conventional text preprocessing such as remov-
ing of stop words, stemming and idf term weighting.
This shows that the additional gain in performance
from using external resources is small and that the
soft cardinality approach is a very challenging base-
line for the STS task. Soft cardinality has been
previously shown (Jimenez and Gelbukh, 2012) to
be also a good baseline for other applications such
as information retrieval, entity matching, paraphrase
detection and recognizing textual entailment.
Soft cardinality approach to constructing similar-
ity functions (Jimenez et al, 2010) consists in using
any cardinality-based resemblance coefficient (such
as Jaccard or Dice) but substituting the classical set
194
cardinality with a softened counting function called
soft cardinality. For example, the soft cardinality of
a set containing three very similar elements is close
to (though larger than) 1, while for three very dif-
ferent elements it is close to (though less than) 3.
To use the soft cardinality with texts, they are repre-
sented as sets of words, and a word-similarity func-
tion is used for the soft counting of the words. For
the sake of completeness, we give a brief overview
of the soft-cardinality method in Section 3.
The resemblance coefficient used in our participa-
tion is a modified version of Tversky?s ratio model
(Tversky, 1977). Apart from the two parameters of
this coefficient, a new parameter was included and
functions max and min were used to make it sym-
metrical. The rationale for this new coefficient is
given in Section 2.
Three word similarity features used in our sys-
tems are described in Section 4. The one is a mea-
sure of character q-gram overlapping, which reuses
the coefficient proposed in Section 2; this measure is
described in subsection 4.1. The other two ones are
distributional measures obtained from the ukWack
corpus (Baroni et al, 2009), which is a collection of
web-crawled documents containing about 1.9 billion
words in English. The second measure is, again, a
reuse of the coefficient specified in Section 2, but us-
ing instead sets of occurrences (and co-occurrences)
of words in sentences in the ukWack corpus; this
measure is described in subsection 4.2. Finally, the
third one, which is a normalized version of point-
wise mutual information (PMI), is described in sub-
section 4.3.
The parameters of the three text-similarity func-
tions derived from the combination of the proposed
coefficient of resemblance (Section 2), the soft car-
dinality (Section 3) and the three word-similarity
measures (Section 4) were adjusted to maximize the
correlation with the 2012 STS gold standard data.
At this point, these soft-cardinality similarity func-
tions can provide predictions for the test data. How-
ever, we decided to test the approach of learning a
resemblance function from the training data instead
of using a preset resemblance coefficient. Basically,
most resemblance coefficients are ternary functions
F (x, y, z) where x = |A|, y = |B| and z = |A?B|:
e.g. Dice coefficient is F (x, y, z) = 2z/x+y and Jac-
card is F (x, y, z) = z/x+y?z. Thus, this function
can be learned using a regression model, providing
cardinalities x, y and z as features and the gold stan-
dard value as the target function. The results ob-
tained for the text-similarity functions and the re-
gression approach are presented in Section 7.
Unfortunately, when using a regressor trained
with 2012 STS data and tested with 2013 surprise
data we observed that the results worsened rather
than improved. A short explanation of this is over-
fitting. A more detailed discussion of this, together
with an assessment of the performance gain obtained
by the use of distributional measures is provided in
Section 8.
Finally, in Section 9 the conclusions of our partic-
ipation in this evaluation campaign are presented.
2 Symmetrical Tversky?s Ratio Model
In the field of mathematical psychology Tversky
proposed the ratio model (TRM) (Tversky, 1977)
motivated by the imbalance that humans have on
the selection of the referent to compare things. This
model is a parameterized resemblance coefficient to
compare two sets A and B given by the following
expression:
trm(A,B) =
|A ?B|
?|A \B|+ ?|B \A|+ |A ?B|
,
Having ?, ? ? 0. The numerator represents the
commonality between A and B, and the denomina-
tor represents the referent for comparison. Parame-
ters ? and ? represent the preference in the selection
of A or B as referent. Tversky associated the set
cardinality, to the stimuli of the objects being com-
pared. Let us consider a Tversky?s example of the
70s: A is North Corea, B is red China and stimuli
is the prominence of the country. When subjects as-
sessed the similarity between A and B, they tended
to select the country with less prominence as ref-
erent. Tversky observed that ? was larger than ?
when subjects compared countries, symbols, texts
and sounds. Our motivation is to use this model by
adjusting the parameters ? and ? for better modeling
human similarity judgments for short texts.
However, this is not a symmetric model and the
parameters ? and ?, have the dual interpretation of
modeling the asymmetry in the referent selection,
while controlling the balance between |A ? B| and
195
|A?B|+ |B ?A| as well. The following reformu-
lation, called symmetric TRM (strm), is intended to
address these issues:
strm(A,B) =
c
? (?a+ (1? ?) b) + c
, (1)
a = min(|A ? B|, |B ? A|), b = max(|A ?
B|, |B ? A|) and c = |A ? B| + bias. In strm, ?
models only the balance between the differences in
the cardinalities of A and B, and ? models the bal-
ance between |A?B| and |A?B|+|B?A|. Further-
more, the use of functions min and max makes the
measure to be symmetric. Although the motivation
for the bias parameter is empirical, we believe that
this reduces the effect of the common features that
are frequent and therefore less informative, e.g. stop
words. Note that for ? = 0.5,? = 1 and bias = 0,
strm is equivalent to Dice?s coefficient. Similarity,
for ? = 0.5,? = 2 and bias = 0, strm is equivalent
to the Jaccard?s coefficient.
3 Soft Cardinality
The cardinality of a set is its number of elements. By
definition, the sets do not allow repeated elements,
so if a collection of elements contains repetitions its
cardinality is the number of different elements. The
classical set cardinality does not take into account
similar elements, i.e. only the identical elements
in a collection counted once. The soft cardinality
(Jimenez et al, 2010) considers not only identical
elements but also similar using an auxiliary similar-
ity function sim, which compares pairs of elements.
This cardinality can be calculated for a collection of
elements A with the following expression:
|A|? =
n?
i=1
wi
?
?
n?
j=1
sim(ai, aj)p
?
?
?1
(2)
A ={a1, a2, . . . , an}; wi ? 0; p ? 0; 1 >
sim(x, y) ? 0, x 6= y; and sim(x, x) = 1. The
parameter p controls the degree of "softness" of
the cardinality. This formulation has the property
of reproducing classical cardinality when p is large
and/or when sim is a rigid function that returns 1
only for identical elements and 0 otherwise. The co-
efficients wi are the weights associated with each el-
ement. In text applications elements ai are words
and weights wi represent the importance or infor-
mative character of each word (e.g. idf weights).
The apostrophe is used to differentiate soft cardinal-
ity from the classic set cardinality.
4 Word Similarity
Analogous to the STS, the word similarity is the task
of measuring the relationship of a couple of words
in a way correlated with human judgments. Since
when Rubenstein and Goodenough (1965) provided
the first data set, this task has been addressed pri-
marily through semantic networks (Resnik, 1999;
Pedersen et al, 2004) and distributional measures
(Agirre et al, 2009). However, other simpler ap-
proaches such as edit-distance (Levenshtein, 1966)
and stemming (Porter, 1980) can also be used. For
instance, the former identifies the similarity between
"song" and "sing", and later that between "sing" and
"singing". This section presents three approaches
for word similarity that can be plugged into the soft
cardinality expression in eq. 2.
4.1 Q-grams similarity
Q-grams are the collection of consecutive-
overlapped sub-strings of length q obtained
from the character string in a word. For instance,
the 2-grams (bi-grams) and 3-grams (trigrams) rep-
resentation of the word ?sing? are {?#s?, ?si?, ?in?,
?ng?, ?g#?} and {?#si?, ?sin?, ?ing?, ?ng#?} respec-
tively. The character ?#? is a padding character that
distinguishes q-grams at the beginning and ending
of a word. If the number of characters in a word is
greater or equal than q its representation in q-grams
is the word itself (e.g. the 6-grams in ?sing? are
{?sing?}). Moreover, the 1-grams (unigrams) and
0-grams representations of ?sing? are {?s?, ?i?, ?n?,
?g?} and {?sing?}. A word can also be represented
by combining multiple representations of q-grams.
For instance, the combined representation of ?sing?
using 0-grams, unigrams, and bi-grams is {?sing?,
?s?, ?i?, ?n?, ?g?, ?#s?, ?si?, ?in?, ?ng?, ?g#?}, denoted
by [0:2]-grams. In practice a range [q1 : q2] of
q-grams can be used having 0 ? q1 < q2.
The proposed word-similarity function (named
qgrams) first represents a pair of words using
[q1 : q2]-grams and then compares them reusing
the strm coefficient (eq.1). The parameters of the
196
qgrams function are q1, q2, ?qgrams, ?qgrams, and
biasqgrams. These parameters are sub-scripted to
distinguish them from their counterparts at the text-
similarity functions.
4.2 Context-Set Distributional Similarity
The hypothesis of this measure is that the co-
occurrence of two words in a sentence is a hint of
the possible relationship between them. Let us de-
fine sf(t) as the sentence frequency of a word t in
a corpus. The sentence frequency is equivalent to
the well known document frequency but uses sen-
tences instead of documents. Similarly sf(tA ? tB)
is the number of sentences where words tA and tB
co-occur. The idea is to compute a similarity func-
tion between tA and tB representing them as A and
B, which are sets of the sentences where tA and tB
occur. Similarly, A?B is the set of sentences where
both words co-occur. The required cardinalities can
be obtained from the sentence frequencies by: |A| =
sf(tA); |B| = sf(tB) and |A ? B| = sf(tA ? tB).
These cardinalities are combined reusing again the
strm coefficient (eq. 1) to obtain a word-similarity
function. The parameters of this function, which we
refer to it as csds, are ?csds, ?csds and biascsds.
4.3 Normalized Point-wise Mutual Information
The pointwise mutual information (PMI) is a mea-
sure of relationship between two random variables.
PMI is calculated by the following expression:
pmi(tA, tB) = log2
(
P (tA ? tB)
P (tA) ? P (tB)
)
PMI has been used to measure the relatedness of
pairs of words using the number of the hits returned
by a search engine (Turney, 2001; Bollegala et al,
2007). However, PMI cannot be used directly as
sim function in eq.2. The alternative is to normal-
ize it dividing it by log2(P (tA ? tB)) obtaining a
value in the [1,?1] interval. This measure returns
1 for complete co-occurrence, 0 for independence
and -1 for ?never? co-occurring. Given that the re-
sults in the interval (0,-1] are not relevant, the final
normalized-trimmed expression is:
npmi(tA, tB) = max
[
pmi(tA, tB)
log2(P (tA ? tB))
, 0
]
(3)
The probabilities required by PMI can be obtained
by MLE using sentence frequencies in a large cor-
pus: P (tA) ?
sf(tA)
S , P (tB) ?
sf(tB)
S ,and P (tA ?
tB) ?
sf(tA?tB)
S . Where S is the total number of
sentences in the corpus.
5 Text-similarity Functions
The ?building blocks? proposed in sections 2,
3 and 4, are assembled to build three text-
similarity functions, namely STSqgrams, STScsds
and STSnpmi. The first component is the strm re-
semblance coefficient (eq. 1), which takes as argu-
ments a pair of texts represented as bags of words
with importance weights associated with each word.
In the following subsection 5.1 a detailed descrip-
tion of the procedure for obtaining such weighted
bag-of-words is provided.
The strm coefficient is enhanced by replac-
ing the classical cardinality by the soft cardinality,
which exploits two resources: importance weights
associated with each word (weights wi) and pair-
wise comparisons among words (sim). Unlike
STSqgrams measure, STScsds and STSnpmi mea-
sures require statistics from a large corpus. A brief
description of the used corpus and the method for
obtaining such statistics is described in subsection
5.2. Finally, the three proposed text-similarity func-
tions contain free parameters that need to be ad-
justed. The method used to get those parameters is
described in subsection 5.3.
5.1 Preprocessing and Term Weighting
All training and test texts were preprocessed with
the following sequence of actions: i) text strings
were tokenized, ii) uppercase characters are con-
verted into lower-cased equivalents, iii) stop-words
were removed, iv) punctuation marks were removed,
and v) words were stemmed using Porter?s algorithm
(1980). Then each stemmed word was weighted
with idf (Jones, 2004) calculated using the entire
collection of texts.
5.2 Sentence Frequencies from Corpus
The sentence frequencies sf(t) and sf(tA ? tB) re-
quired by csds and npmi word-similarity func-
tions were obtained from the ukWack corpus (Ba-
roni et al, 2009). This corpus has roughly 1.9 bil-
197
lion words, 87.8 millions of sentences and 2.7 mil-
lions of documents. The corpus was iterated sen-
tence by sentence with the same preprocessing that
was described in the previous section, looking for
all occurrences of words and word pairs from the
full training and test texts. The target words were
stored in a trie, making the entire corpus iteration
took about 90 minutes in a laptop with 4GB and a
1.3Ghz processor.
5.3 Parameter optimization
The three proposed text-similarity functions have
several parameters: p exponent in the soft car-
dinality; ?, ?, and bias in strm coefficient;
their sub-scripted versions in qgrams and csds
word-similarity functions; and finally q1and q2 for
qgrams function. Parameter sets for each of the
three text-similarity functions were optimized us-
ing the full STS-SemEval-2012 data. The function
to maximize was the correlation between similar-
ity scores against the gold standard in the training
data. The set of parameters for each similarity func-
tion were optimized using a greedy hill-climbing ap-
proach by using steps of 0.01 for all parameters ex-
cept q1 and q2 that used 1 as step. The initial values
were p = 1, ? = 0.5, ? = 1, bias = 0, q1 = 2 and
q2 = 3. All parameters were optimized until im-
provement in the function to maximize was below
0.0001. The obtained values are :
STSqgrams p = 1.32,? = 0.52, ? = 0.64, bias =
?0.45, q1 = 0, q2 = 2, ?qgrams = 0.95,
?qgrams = 1.44, biasqgrams = ?0.44.
STScsds p = 0.5, ? = 0.63, ? = 0.69, bias =
?2.05, ?csds = 1.34, ?csds = 2.57, biascsds =
?1.22 .
STSnpmi p = 6.17,? = 0.83, ? = 0.64, bias =
?2.11.
6 Regression for STS
The use of regression is motivated by the follow-
ing experiment. First, a synthetic data set with
1,000 instances was generated with the following
three features: |A| = RandomBetween(1, 100),
|B| = RandomBetween(1, 100) and |A ? B| =
RandomBetween(0,min[|A|, |B|]). Secondly, a
#1 STSsim #11 |A?B|
?/|A|?
#2 |A|? #12 |A?B|?/|B|?
#3 |B|? #13 |A|? ? |B|?
#4 |A ?B|? #14 |A?B|?/|A?B|?
#5 |A ?B|? #15 2?|A?B|?/|A|?+|B|?
#6 |A \B|? #16 |A?B|/min[|A|,|B|]
#7 |B \A|? #17 |A?B|?/max[|A|?,|B|?]
#8 |A ?B ?A ?B|? #18 |A?B|?/
?
|A|??|B|?
#9 |A?B|?/|A|? #19 |A?B|
?+|A|?+|B|?
2?|A|??|B|?
#10 |B?A|?/|B|? #20 gold standard
Table 1: Feature set for regression
linear regressor was trained using the Dice?s coef-
ficient (i.e. 2|A ? B|/|A| + |B|) as target function.
The Pearson correlation obtained using 4-fold cross-
validation as method of evaluation was r = 0.93.
Besides, a Reduced Error Pruning (REP) tree (Wit-
ten and Frank, 2005) boosted with 30 iterations of
Bagging (Breiman, 1996) was used instead of the
linear regressor obtaining r = 0.99. We concluded
that a particular resemblance coefficient can be ac-
curately approximated using a nonlinear regression
algorithm and training data.
This approach can be used for replacing the strm
coefficient by a similarity function learned from STS
training data. The three features used in the previ-
ous experiment were extended to a total of 19 (see
table 1) plus the gold standard as target. The feature
#1 is the score of the corresponding text-similarity
function described in the previous section. Three
sets of features were constructed, each with 19 fea-
tures using the soft cardinality in combination with
the word-similarity functions qgrams, csds and
npmi. Let us name these feature sets as fs:qgrams,
fs:csds and fs:npmi. The submission labeled run1
was obtained using the feature set fs:qgrams (19 fea-
tures). The submission labeled run2 was obtained
using the aggregation of fs:qgrams and fs:csds (19?
2 = 38 features). Finally, run3 was the aggregation
of fs:grams, fs:csds and fs:npmi (19 ? 3 = 57 fea-
tures).
7 Results in *SEM 2013 Shared Task
In this section three groups of systems are described
by using the functions and models proposed in the
previous sections. The first group (and simplest)
198
Data set STSqgrams STScsds STSnpmi average
headlines 0.7625 0.7243 0.7379 0.7562
OnWN 0.7022 0.7050 0.6832 0.7063
FNWM 0.2704 0.3713 0.4215 0.3940
SMT 0.3151 0.3325 0.3408 0.3402
mean 0.5570 0.5592 0.5653 0.5747
rank 8 7 6 4
Table 2: Unofficial results using text-similarity functions
Data set run1 run2 run3
headlines 0.7591 0.7632 0.7640
OnWN 0.7159 0.7239 0.7485
FNWM 0.2806 0.3679 0.3487
SMT 0.2820 0.2786 0.2952
mean 0.5491 0.5586 0.5690
rank 14 8 4
Table 3: Unofficial results using linear regression
of systems consist in using the scores of the three
text-similarity functions STSqgrams, STScsds and
STSnpmi. Table 2 shows the unofficial results of
these three systems. The bottom row shows the posi-
tions that these systems would have obtained if they
had been submitted to the *SEM shared task 2013.
The last column shows the results of a system that
combines the scores of three measures on a single
score calculating the arithmetic mean. This is the
best performing system obtained with the methods
described in this paper.
Tables 3 and 4 show unofficial and official re-
sults of the method described in section 6 using
linear regression and Bagging (30 iterations)+REP
tree respectively. These results were obtained using
WEKA (Hall et al, 2009).
8 Discussion
Contrary to the observation we made in training
data, the methods that used regression to predict the
gold standard performed poorly compared with the
text similarity functions proposed in Section 5. That
is, the results in Table 2 overcome those in Tables 3
and 4. Also in training data, Bagging+REP tree sur-
passed linear regression, but, as can be seen in tables
3 and 4 the opposite happened in test data. This is
a clear symptom of overfitting. However, the OnWN
Data set run1 run2 run3
headlines 0.6410 0.6713 0.6603
OnWN 0.7360 0.7412 0.7401
FNWM 0.3442 0.3838 0.3347
SMT 0.3035 0.2981 0.2900
mean 0.5273 0.5402 0.5294
rank 23 18 22
Table 4: Official results of the submitted runs to STS
*SEM 2013 shared task using Bagging + REP tree for
regression
data set was an exception, which obtained the best
results using linear regression. OnWN was the only
one among the 2013 data sets that was not a sur-
prise data set. Probably the 5.97% relative improve-
ment obtained in run3 by the linear regression versus
the best result in Table 2 may be justified owing to
some patterns discovered by the linear regressor in
the OnWN?2012 training data which are projected
on the OnWN?2013 test data.
It is worth noting that in all three sets of results,
the lowest mean was consistently obtained by the
text-overlapping methods, namely STSqgrams and
run1. The relative improvement in mean due to
the use of distributional measures against the text-
overlapping methods was 3.18%, 3.62% and 2.45%
in each set of results (see Tables 2, 3 and 4). In
FNWM data set, the biggest improvements achieved
55.88%, 31.11% and 11.50% respectively in the
three groups of results, followed by SMT data set.
Both in FNWN data set as in SMT, the texts are sys-
tematically longer than those found in OnWN and
headlines. This result suggests that the improvement
due to distributional measures is more significant in
longer texts than in the shorter ones.
Lastly, it is also important to notice that
the STSqgrams text-similarity function obtained
mean = 0.5570, which proved again to be a very
strong text-overlapping baseline for the STS task.
9 Conclusions
We participated in the CORE-STS shared task in
*SEM 2013 with satisfactory results obtaining po-
sitions 18th, 22nd, and 23rd in the official ranking.
Our systems were based on a new parameterized
resemblance coefficient derived from the Tversky?s
199
ratio model in combination with the soft cardinal-
ity. The three proposed text-similarity functions
used q-grams overlapping and distributional mea-
sures obtained from the ukWack corpus. These text-
similarity functions would have been attained posi-
tions 6th, 7th and 8th in the official ranking, besides
a simple average of them would have reached the
4thplace. Another important conclusion was that the
plain text-overlapping method was consistently im-
proved by the incremental use of the proposed distri-
butional measures. This result was most noticeable
in long texts.
In conclusion, the proposed text-similarity func-
tions proved to be competitive despite their simplic-
ity and the few resources used.
Acknowledgments
This research was funded in part by the Systems
and Industrial Engineering Department, the Office
of Student Welfare of the National University of
Colombia, Bogot?, and through a grant from the
Colombian Department for Science, Technology
and Innovation, Colciencias, proj. 1101-521-28465
with funding from ?El Patrimonio Aut?nomo Fondo
Nacional de Financiamiento para la Ciencia, la Tec-
nolog?a y la Innovaci?n, Francisco Jos? de Caldas.?
The Section 2 was proposed during the first author?s
internship at Microsoft Research in 2012. The third
author recognizes the support from Mexican Gov-
ernment (SNI, COFAA-IPN, SIP 20131702, CONA-
CYT 50206-H) and CONACYT?DST India (proj.
122030 ?Answer Validation through Textual Entail-
ment?). Entailment?).
References
Eneko Agirre, Enrique Alfonseca, Keith Hall, Jana
Kravalova, Marius Pasca, and Aitor Soroa. 2009.
A study on similarity and relatedness using distribu-
tional and WordNet-based approaches. In Proceedings
of Human Language Technologies: The 2009 Annual
Conference of the North American Chapter of the As-
sociation for Computational Linguistics, NAACL ?09,
pages 19?27, Stroudsburg, PA, USA. Association for
Computational Linguistics.
Eneko Agirre, Daniel Cer, Mona Diab, and Gonzalez-
Agirre Aitor. 2012. SemEval-2012 task 6: A pilot on
semantic textual similarity. In Proceedings of the 6th
International Workshop on Semantic Evaluation (Se-
mEval@*SEM 2012), Montreal,Canada. Association
for Computational Linguistics.
Eneko Agirre, Daniel Cer, Mona Diab, Aitor Gonzalez-
Agirre, and Weiwei Guo. 2013. *SEM 2013 shared
task: Semantic textual similarity, including a pilot on
typed-similarity. Atlanta, Georgia, USA. Association
for Computational Linguistics.
Daniel B?r, Chris Biemann, Iryna Gurevych, and Torsten
Zesch. 2012. UKP: computing semantic textual
similarity by combining multiple content similarity
measures. In Proceedings of the 6th International
Workshop on Semantic Evaluation (SemEval *SEM
2012), Montreal, Canada. Association for Computa-
tional Linguistics.
Marco Baroni, Silvia Bernardini, Adriano Ferraresi, and
Eros Zanchetta. 2009. The WaCky wide web: a
collection of very large linguistically processed web-
crawled corpora. Language resources and evaluation,
43(3):209?226.
Danushka Bollegala, Yutaka Matsuto, and Mitsuru
Ishizuka. 2007. Measuring semantic similarity be-
tween words using web search engines. In Proceed-
ings of the 16th international conference on World
Wide Web, WWW ?07, pages 757?766, New York,
NY, USA. ACM.
Leo Breiman. 1996. Bagging predictors. Machine
Learning, 24(2):123?140.
Mark Hall, Frank Eibe, Geoffrey Holmes, and Bernhard
Pfahringer. 2009. The WEKA data mining software:
An update. SIGKDD Explorations, 11(1):10?18.
Sergio Jimenez and Alexander Gelbukh. 2012. Baselines
for natural language processing tasks. Appl. Comput.
Math., 11(2):180?199.
Sergio Jimenez, Fabio Gonzalez, and Alexander Gel-
bukh. 2010. Text comparison using soft cardinality.
In Edgar Chavez and Stefano Lonardi, editors, String
Processing and Information Retrieval, volume 6393 of
LNCS, pages 297?302. Springer, Berlin, Heidelberg.
Sergio Jimenez, Claudia Becerra, and Alexander Gel-
bukh. 2012. Soft cardinality: A parameterized sim-
ilarity function for text comparison. In Proceedings of
the 6th International Workshop on Semantic Evalua-
tion (SemEval *SEM 2012), Montreal, Canada.
Karen Sp?rck Jones. 2004. A statistical interpretation of
term specificity and its application in retrieval. Jour-
nal of Documentation, 60(5):493?502, October.
Michael D Lee, B.M. Pincombe, and Matthew Welsh.
2005. An empirical evaluation of models of text docu-
ment similarity. IN COGSCI2005, pages 1254?1259.
Vladimir I. Levenshtein. 1966. Binary codes capable of
correcting deletions, insertions, and reversals. Soviet
Physics Doklady, 10(8):707?710.
200
Rada Mihalcea, Courtney Corley, and Carlo Strapparava.
2006. Corpus-based and knowledge-based measures
of text semantic similarity. In In AAAI?06, pages 775?
780.
Ted Pedersen, Siddharth Patwardhan, and Jason Miche-
lizzi. 2004. WordNet::Similarity: measuring the re-
latedness of concepts. In Proceedings HLT-NAACL?
Demonstration Papers, Stroudsburg, PA, USA. Asso-
ciation for Computational Linguistics.
Martin Porter. 1980. An algorithm for suffix stripping.
Program, 3(14):130?137, October.
Phillip Resnik. 1999. Semantic similarity in a taxonomy:
An information-based measure and its application to
problems of ambiguity in natural language. Journal of
Artificial Intelligence Research, 11:95?130.
Herbert Rubenstein and John B. Goodenough. 1965.
Contextual correlates of synonymy. Commun. ACM,
8(10):627?633, October.
Frane ?aric, Goran Glava?, Mladen Karan, Jan ?najder,
and Bojana Dalbelo Ba?ic. 2012. TakeLab: systems
for measuring semantic text similarity. In Proceedings
of the 6th International Workshop on Semantic Eval-
uation (SemEval *SEM 2012), Montreal, Canada. As-
sociation for Computational Linguistics.
Peter D. Turney. 2001. Mining the web for synonyms:
PMI-IR versus LSA on TOEFL. In Luc De Raedt and
Peter Flach, editors, Machine Learning: ECML 2001,
number 2167 in Lecture Notes in Computer Science,
pages 491?502. Springer Berlin Heidelberg, January.
Amos Tversky. 1977. Features of similarity. Psycholog-
ical Review, 84(4):327?352, July.
I.H. Witten and E. Frank. 2005. Data Mining: Prac-
tical Machine Learning Tools and Techniques. Mor-
gan Kaufmann Publishers Inc., San Francisto, CA, 2nd
edition.
201
Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 2: Seventh International Workshop on Semantic
Evaluation (SemEval 2013), pages 34?38, Atlanta, Georgia, June 14-15, 2013. c?2013 Association for Computational Linguistics
SOFTCARDINALITY: Learning to Identify Directional
Cross-Lingual Entailment from Cardinalities and SMT
Sergio Jimenez, Claudia Becerra
Universidad Nacional de Colombia
Ciudad Universitaria,
edificio 453, oficina 114
Bogot?, Colombia
sgjimenezv@unal.edu.co
cjbecerrac@unal.edu.co
Alexander Gelbukh
CIC-IPN
Av. Juan Dios B?tiz, Av. Mendiz?bal,
Col. Nueva Industrial Vallejo
CP 07738, DF, M?xico
gelbukh@gelbukh.com
Abstract
In this paper we describe our system submit-
ted for evaluation in the CLTE-SemEval-2013
task, which achieved the best results in two
of the four data sets, and finished third in av-
erage. This system consists of a SVM clas-
sifier with features extracted from texts (and
their translations SMT) based on a cardinality
function. Such function was the soft cardinal-
ity. Furthermore, this system was simplified
by providing a single model for the 4 pairs
of languages obtaining better (unofficial) re-
sults than separate models for each language
pair. We also evaluated the use of additional
circular-pivoting translations achieving results
6.14% above the best official results.
1 Introduction
The Cross-Lingual Textual Entailment (CLTE) task
consists in determining the type of directional en-
tailment (i.e. forward, backward, bidirectional or
no-entailment) between a pair of texts T1 and T2,
each one written in different languages (Negri et al,
2013). The texts and reference annotations for this
task were obtained through crowdsourcing applied
to simpler sub-tasks (Negri et al, 2011). CLTE has
as main applications content synchronization and
aggregation in different languages (Mehdad et al,
2012; Duh et al, 2013). We participated in the first
evaluation of this task in 2012 (Negri et al, 2012),
achieving third place on average among 29 partici-
pating systems (Jimenez et al, 2012).
Since in the CLTE task text pairs are in different
languages, in our system, all comparisons made be-
tween two texts imply that one of them was written
by a human and the other is a translation provided by
statistical machine translation (SMT). Our approach
is based on an SVM classifier (Cortes and Vapnik,
1995) whose features were cardinalities combined
with similarity scores. That system was motivated
by the fact that most text similarity functions are
symmetric, e.g. Edit Distance (Levenshtein, 1966),
longest common sub-sequence (Hirschberg, 1977),
Jaro-Winkler similarity (Winkler, 1990), cosine sim-
ilarity (Salton et al, 1975). Thus, the use of these
functions as only resource seems counter-intuitive
since CLTE task is asymmetric for the forward and
backward entailment classes.
Moreover, cardinality is the central component of
the resemblance coefficients such as Jaccard, Dice,
overlap, etc. For instance, if T1 and T2 are texts
represented as bag of words, it is only necessary to
know the cardinalities |T1|, |T2| and |T1 ? T2| to ob-
tain a similarity score using a resemblance coeffi-
cient such as the Dice?s coefficient (i.e. 2 ? |T1 ?
T2|/(|T1| + |T2|)). Therefore, the idea is to use the
individual cardinalities to enrich a set of features ex-
tracted from texts.
Cardinality gives a rough idea of the amount of
information in a collection of elements (i.e. words)
providing the number of different elements therein.
That is, in a collection of elements whose majority
are repetitions contains less information than a col-
lection whose elements are mostly different. How-
ever, the classical sets cardinality is a rigid mea-
sure as do not take account the degree of similarity
among the elements. Unlike the sets cardinality, soft
cardinality (Jimenez et al, 2010) uses the similari-
ties among the elements providing a more flexible
34
measurement of the amount of information in a col-
lection. In the 2012 CLTE evaluation campaign, it
was noted that the soft cardinality overcame classi-
cal cardinality in the task at hand. All the models
used in our participation and proposed in this paper
are based on the soft cardinality. A brief descrip-
tion of the soft cardinality is presented in Section 2,
along with a description of the functions used to pro-
vide the similarities between words. Besides, the set
of features that are derived from all pairs of texts and
their cardinalities are presented in Section 3.
Section 4 provides a detailed description for each
of the 4 models (one for each language pair) used
to get the predictions submitted for evaluation. In
Section 5 a simplified-multilingual model is tested
with several word-similarity functions and circular-
pivoting translations.
In sections 6 and 7 a brief discussion of the results
and conclusions of our participation in this evalua-
tion campaign are presented.
2 Soft Cardinality
The soft cardinality (Jimenez et al, 2010) of a col-
lection of words T is calculated with the following
expression:
|T |? =
n?
i=1
wi
?
?
n?
j=1
sim(ti, tj)p
?
?
?1
(1)
Having T ={t1, t2, . . . , tn}; wi ? 0; p ? 0; 1 >
sim(x, y) ? 0, x 6= y; and sim(x, x) = 1. The
parameter p controls the degree of "softness" of the
cardinality (the larger the ?harder?). The coefficients
wi are weights associated with each word (or term)
t, which can represent the importance or informative
character of each word (e.g. idf weights). The func-
tion sim is a word-similarity function. Three such
functions are considered in this paper:
Q-grams: each word ai is represented as a col-
lection of character q-grams (Kukich, 1992). In-
stead of single length q-grams, a combination of
a range of lengths q1 to q2 was used. Next,
a couple of words are compared with the fol-
lowing resemblance coefficient: sim(ti, tj) =
|ti?tj |+bias
??max(|ti|,|tj |)+(1??)?min(|ti|,|tj |)
. The parameters of
this word-similarity function are q1, q2, ? and bias.
Group 1: basic cardinalities
#1 |T1|? #4 |T1 ? T2|?
#2 |T2|? #5 |T1 ? T2|?
#3 |T1 ? T2|? #6 |T2 ? T1|?
Group 2: asymmetrical ratios
#7 |T1?T2|
?/|T1|? #8 |T1?T2|
?/|T2|?
Group 3: similarity and arithmetical* scores
#9 |T1?T2|
?/|T1?T2|? #10
2?|T1?T2|
?
|T1|?+|T2|?
#11 |T1?T2|
?/
?
|T1|??|T2|? #12
|T!?T2|
?
min[|T1|?,|T2|?]
#13 |T1?T2|
?+|T1|
?+|T2|
?
2?|T1|??|T2|?
#14* |T1|? ? |T2|?
Table 1: Set of features derived from texts T1 and T2
Edit-Distance: a similarity score for a pair of
words can be obtained from their Edit Distance
(Levenshtein, 1966) by normalizing and converting
distance to similarity with the following expression:
sim(ti, tj) = 1?
EditDistance(ti,tj)
max[len(ti),len(tj)]
.
Jaro-Winkler: this measure is based on the Jaro
(1989) similarity, which is given by this expression
Jaro(ti, tj) = 13
(
c
len(ti)
+ clen(tj) +
c?m
c
)
, where c
is the number of characters in common within a slid-
ing window of length max[len(ti),len(tj)]2 ?1. To avoid
division by 0, when c = 0 then Jaro(ti, tj) = 0. The
number of transpositions m is obtained sorting the
common characters according to their occurrence
in each of the words and counting the number of
non-matching characters. Winkler (1990) proposed
an extension to this measure taking into account
the common prefix length l through this expression:
sim(ti, tj) = Jaro(ti, tj) + l10 (1? Jaro(ti, tj)).
3 Features from Cardinalities
For a pair of texts T1 and T2 represented as bags
of words three basic soft cardinalities can be cal-
culated: |T1|?, |T2|? and |T1 ? T2|?. The soft car-
dinality of their union is calculated using the con-
catenation of T1 and T2. More additional features
can be derived from these three basic features, e.g.
|T1?T2|? = |T1|?+|T2|??|T1?T2|? and |T1?T2|? =
|T1|?? |T1 ? T2|?. The complete set of features clas-
sified into three groups are shown in Table 1.
4 Submitted Runs Description
The data for the 2013 CLTE task consists of 4 data
sets (spa-eng, ita-eng, fra-eng and deu-eng) each
35
Data set q1 q2 ? bias
deu-eng 2 2 0.5 0.0
fra-eng 2 3 0.5 0.0
ita-eng 2 4 0.6 0.0
spa-eng 1 3 0.5 0.1
Table 2: Parameters of the q-grams word-similarity func-
tion for each language pair
with 1,000 pairs of texts for training and 500 for
testing. For each pair of texts T1 and T2 written
in two different languages, two translations are pro-
vided using the Google?s translator1. Thus, T t1 is a
translation of T1 into the language of T2 and T t2 is
a translation of T2 into the language of T1. Using
these pivoting translations, two pairs of texts can be
compared: T1 with T t2 and T
t
1 with T2.
Then all training and testing texts and their trans-
lations were pre-processed with the following se-
quence of actions: i) text strings were tokenized,
ii) uppercase characters are converted into lower-
case equivalents, iii) stop words were removed, iv)
punctuation marks were removed, and v) words were
stemmed using the Snowball2 multilingual stem-
mers provided by the NLTK Toolkit (Loper and
Bird, 2002). Then every stemmed word is tagged
with its idf weight (Jones, 2004) calculated with the
complete collection of texts and translations in the
same language.
Five instances of the soft cardinality are provided
using 1, 2, 3, 4 and 5 as values of the parameter
p. Therefore, the total number of features for each
pair of texts is the multiplication of the number of
features in the feature set (i.e. 14, see Table 1) by
the number of soft cardinality functions (5) and by 2,
corresponding to the two pairs of comparable texts.
That is, 14? 5? 2 = 140 features.
The sim function used was q-grams, whose pa-
rameters were adjusted for each language pair.
These parameters, which are shown in Table 2, were
obtained by manual exploration using the training
data.
Four vector data sets for training (one for each
language pair) were built by extracting the 140 fea-
tures from the 1,000 training instances and using
1https://translate.google.com
2http://snowball.tartarus.org
ECNUCS-team?s system
spa-eng ita-eng fra-eng deu-eng average
run4 0.422 0.416 0.436 0.452 0.432
run3 0.408 0.426 0.458 0.432 0.431
SOFTCARDINALITY-team?s system
spa-eng ita-eng fra-eng deu-eng average
run1 0.434 0.454 0.416 0.414 0.430
run2 0.432 0.448 0.426 0.402 0.427
Table 3: Official results for our system and the top per-
forming system ECNUCS (accuracies)
their gold-standard annotations as class attribute.
Predictions for the 500 test cases were obtained
through a SVM classifier trained with each data set.
For the submitted run1, this SVM classifier used a
linear kernel with its complexity parameter set to its
default value C = 1. For the run2, this parameter
was adjusted for each pair of languages with the fol-
lowing values: Cspa?eng = 2.0, Cita?eng = 1.5,
Cfra?eng = 2.3 and Cdeu?eng = 2.0. The imple-
mentation of the SVM used is that which is available
in WEKA v.3.6.9 (SMO) (Hall et al, 2009). Official
results for run1, run2 and best accuracies obtained
among all participant systems are shown in Table 3.
5 A Single Multilingual Model
This section presents the results of our additional ex-
periments in search for a simplified model and in
turn to respond to the following questions: i) Can
one simplified-multilingual model overcome the ap-
proach presented in Section 4? ii) Does using addi-
tional circular-pivoting translations improve perfor-
mance? and iii) Do other word-similarity functions
work better than the q-grams measure?
First, it is important to note that the approach
described in Section 4 used only patterns discov-
ered in cardinalities. This means, that no language-
dependent features was used, with the exception of
the stemmers. Therefore, we wonder whether the
patterns discovered in a pair of languages can be use-
ful in other language pairs. To answer this question,
a single prediction model was built by aggregating
instances from each of the vector data sets into one
data set with 4,000 training instances. Afterward,
this model was used to provide predictions for the
2,000 test cases.
36
Moreover, customization for each pair of lan-
guages in the word-similarity function, which is
show in Table 2, was set on the following unique set
of parameters: q1 = 1, q2 = 3, ? = 0.5, bias = 0.0.
Thus, the words are compared using q-grams and
the Dice coefficient. In addition to the measure of
q-grams, two "off-the-shelf" measures were used as
nonparametric alternatives, namely: Edit Distance
(Levenshtein, 1966) and the Jaro-Winkler similarity
(Winkler, 1990).
In another attempt to simplify this model, we
evaluated the predictive ability of each of the three
groups of features shown in Table 1. The combi-
nation of groups 2 and 3, consistently obtained bet-
ter results when the evaluation with 10 fold cross-
validation was used in the training data. This result
was consistent with the simple training versus test
data evaluation. The sum of all previous simplifica-
tions significantly reduced the number of parameters
and features in comparison with the model described
in Section 4. That is, only one SVM and 4 parame-
ters, namely: ?, bias, q1 and q2.
Besides, the additional use of circular-pivoting
translations was tested. In the original model, for
every pair of texts (T1, T2) their pivot translations
(T t1 , T
t
2) were provided allowing the calculation of
|T1 ? T t2| and |T
t
1 ? T2|. Translations T
t
1 and T
t
2 can
also be translated back to their original languages
obtaining T tt1 and T
tt
2 . These additional transla-
tions in turn allows the calculation of |T tt1 ? T
t
2|
and |T t1 ? T
tt
2 |. This procedure can be repeated
again to obtain T ttt1 and T
ttt
2 , which in turn provides
|T1 ? T ttt2 |, |T
ttt
1 ? T2|, |T
tt
1 ? T
ttt
2 | and |T
ttt
1 ? T
tt
2 |.
The original feature set is denoted as t. The extended
feature sets using double-pivoting translations and
triple-pivot translations are denoted respectively as
tt and ttt.
The results obtained with this simplified model
using single, double and triple pivot translations are
shown in Table 4. The first column indicates the
word-similarity function used by the soft cardinal-
ity and the second column indicates the number of
pivoting translations.
6 Discussion
In spite of the customization of the parameter C in
the run2, the run1 obtained better results than run2
Soft C. #t spa-e ita-e fra-e deu-e avg.
Ed.Dist. t 0.444 0.450 0.440 0.410 0.436
Ed.Dist. tt 0.452 0.464 0.434 0.432 0.446
Ed.Dist. ttt 0.464 0.468 0.440 0.424 0.449
Jaro-W. t 0.422 0.450 0.426 0.406 0.426
Jaro-W. tt 0.430 0.456 0.444 0.400 0.433
Jaro-W. ttt 0.426 0.458 0.430 0.430 0.436
q-grams t 0.428 0.456 0.456 0.432 0.443
q-grams tt 0.436 0.478 0.444 0.430 0.447
q-grams ttt 0.452 0.474 0.464 0.442 0.458
Table 4: Single-multilingual model results (accuracies)
(see Table 3). This result indicates that the simpler
model produced better predictions in unseen data.
It is also important to note that two of the three
multilingual systems proposed in Section 5 achieved
higher scores than the best official results (see rows
containing ?t? in Table 4). This indicates that the
proposed simplified model is able to discover pat-
terns in the cardinalities of a pair of languages and
project them into the other language pairs.
Regarding the use of additional circular-pivoting
translations, Table 4 shows that t was overcome on
average by tt and tt by ttt in all cases of the three
sets of results. The relative improvement obtained
by comparing t versus ttt for each group was 3.0% in
Edit Distance, 2.3% for Jaro-Winkler and 3.4% for
the q-gram measure. This same trend holds roughly
for each language pair.
7 Conclusions
We described the SOFTCARDINALITY system
that participated in the SemEval CLTE evaluation
campaign in 2013, obtaining the best results in data
sets spa-eng and ita-eng, and achieving the third
place on average. This result was obtained using
separate models for each language pair. It was also
concluded that a single-multilingual model outper-
forms that approach. Besides, we found that the
use of additional pivoting translations provide bet-
ter results. Finally, the measure based on q-grams of
characters, used within the soft cardinality, resulted
to be the best option among other measures of word
similarity. In conclusion, the soft cardinality method
used in combination with SMT and SVM classifiers
is a competitive method for the CLTE task.
37
Acknowledgments
This research was funded in part by the Systems
and Industrial Engineering Department, the Office
of Student Welfare of the National University of
Colombia, Bogot?, and through a grant from the
Colombian Department for Science, Technology
and Innovation, Colciencias, proj. 1101-521-28465
with funding from ?El Patrimonio Aut?nomo Fondo
Nacional de Financiamiento para la Ciencia, la Tec-
nolog?a y la Innovaci?n, Francisco Jos? de Caldas.?
The third author recognizes the support from Mexi-
can Government (SNI, COFAA-IPN, SIP 20131702,
CONACYT 50206-H) and CONACYT?DST India
(proj. 122030 ?Answer Validation through Textual
Entailment?).
References
Corinna Cortes and Vladimir N. Vapnik. 1995. Support-
vector networks. Machine Learning, 20(3):273?297.
Kevin Duh, Ching-Man Au Yeung, Tomoharu Iwata, and
Masaaki Nagata. 2013. Managing information dispar-
ity in multilingual document collections. ACM Trans.
Speech Lang. Process., 10(1):1:1?1:28, March.
Mark Hall, Frank Eibe, Geoffrey Holmes, and Bernhard
Pfahringer. 2009. The WEKA data mining software:
An update. SIGKDD Explorations, 11(1):10?18.
Daniel S. Hirschberg. 1977. Algorithms for the longest
common subsequence problem. J. ACM, 24(4):664?
675, October.
M.A. Jaro. 1989. Advances in record-linkage methodol-
ogy as applied to matching the 1985 census of tampa,
florida. Journal of the American Statistical Associa-
tion, pages 414?420, June.
Sergio Jimenez, Fabio Gonzalez, and Alexander Gel-
bukh. 2010. Text comparison using soft cardinality.
In Edgar Chavez and Stefano Lonardi, editors, String
Processing and Information Retrieval, volume 6393 of
LNCS, pages 297?302. Springer, Berlin, Heidelberg.
Sergio Jimenez, Claudia Becerra, and Alexander Gel-
bukh. 2012. Soft cardinality+ ML: learning adaptive
similarity functions for cross-lingual textual entail-
ment. In Proceedings of the 6th International Work-
shop on Semantic Evaluation (SemEval, *SEM 2012),
Montreal, Canada. ACL.
Karen Sp?rck Jones. 2004. A statistical interpretation of
term specificity and its application in retrieval. Jour-
nal of Documentation, 60(5):493?502, October.
Karen Kukich. 1992. Techniques for automatically
correcting words in text. ACM Computing Surveys,
24:377?439, December.
Vladimir I. Levenshtein. 1966. Binary codes capable of
correcting deletions, insertions, and reversals. Soviet
Physics Doklady, 10(8):707?710.
Edward Loper and Steven Bird. 2002. NLTK: the natu-
ral language toolkit. In Proceedings of the ACL Work-
shop on Effective Tools and Methodologies for Teach-
ing Natural Language Processing and Computational
Linguistics. Philadelphia. Association for Computa-
tional Linguistics.
Yashar Mehdad, Matteo Negri, and Marcello Federico.
2012. Detecting semantic equivalence and informa-
tion disparity in cross-lingual documents. In Proceed-
ings of the 50th Annual Meeting of the Association for
Computational Linguistics: Short Papers - Volume 2,
ACL ?12, page 120?124, Stroudsburg, PA, USA. As-
sociation for Computational Linguistics.
Matteo Negri, Luisa Bentivogli, Yashar Mehdad, Danilo
Giampiccolo, and Alessandro Marchetti. 2011. Di-
vide and conquer: crowdsourcing the creation of cross-
lingual textual entailment corpora. In Proceedings
of the Conference on Empirical Methods in Natural
Language Processing, EMNLP ?11, page 670?679,
Stroudsburg, PA, USA. Association for Computational
Linguistics.
Matteo Negri, Alessandro Marchetti, Yashar Mehdad,
Luisa Bentivogli, and Danilo Giampiccolo. 2012.
2012. semeval-2012 task 8: Cross-lingual textual en-
tailment for content synchronization. In Proceedings
of the 6th International Workshop on Semantic Evalu-
ation (SemEval 2012), Montreal, Canada.
Matteo Negri, Alessandro Marchetti, Yashar Mehdad,
and Luisa Bentivogli. 2013. Semeval-2013 task
8: Cross-lingual textual entailment for content syn-
chronization. In Proceedings of the 7th International
Workshop on Semantic Evaluation (SemEval 2013).
Gerard Salton, Andrew K. C. Wong, and Chung-Shu
Yang. 1975. A vector space model for automatic in-
dexing. Commun. ACM, 18(11):613?620.
William E. Winkler. 1990. String comparator metrics
and enhanced decision rules in the fellegi-sunter model
of record linkage. In Proceedings of the Section on
Survey Research Methods, pages 354?359. American
Statistical Association.
38
Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 2: Seventh International Workshop on Semantic
Evaluation (SemEval 2013), pages 114?117, Atlanta, Georgia, June 14-15, 2013. c?2013 Association for Computational Linguistics
UNAL: Discriminating between Literal and Figurative
Phrasal Usage Using Distributional Statistics and POS tags
Sergio Jimenez, Claudia Becerra
Universidad Nacional de Colombia
Ciudad Universitaria,
edificio 453, oficina 114
Bogot?, Colombia
sgjimenezv@unal.edu.co
cjbecerrac@unal.edu.co
Alexander Gelbukh
CIC-IPN
Av. Juan Dios B?tiz, Av. Mendiz?bal,
Col. Nueva Industrial Vallejo
CP 07738, DF, M?xico
gelbukh@gelbukh.com
Abstract
In this paper we describe the system used to
participate in the sub task 5b in the Phrasal Se-
mantics challenge (task 5) in SemEval 2013.
This sub task consists in discriminating lit-
eral and figurative usage of phrases with
compositional and non-compositional mean-
ings in context. The proposed approach is
based on part-of-speech tags, stylistic features
and distributional statistics gathered from the
same development-training-test text collec-
tion. The system obtained a relative improve-
ment in accuracy against the most-frequent-
class baseline of 49.8% in the ?unseen con-
texts? (LexSample) setting and 8.5% in ?un-
seen phrases? (AllWords).
1 Introduction
The Phrasal Semantics task-5b in SemEval 2013
consisted in the discrimination of literal of figura-
tive usage of phrases in context (Korkontzelos et al,
2013). For instance, the occurrence in a text of the
phrase ?a piece of cake? can be used whether to re-
fer to something that is pretty easy or to an actual
piece of cake. The motivation for this task is that
such discrimination could improve the quality and
performance of other tasks like machine translation
and information retrieval.
This problem has been studied in the past. Lin
(1999) observed that the distributional characteris-
tics of the literal and figurative usage are different.
Katz and Giesbrecht (2006) showed that the similar-
ities among contexts are correlated with their literal
or figurative usage. Birke and Sarkar (2006) clus-
tered literal and figurative contexts using a word-
sense-disambiguation approach. Fazly et al (2009)
showed that literal and figurative usages are related
to particular syntactical forms. Sporleder and Li
(2009) showed that for a particular phrase the con-
texts of its literal usages are more cohesive than
those of its figurative usages. Inspired by these
works and in a new observation, we proposed a set or
features based on cohesiveness, syntax and stylom-
etry (Section 2), which are used to train a machine
learning classifier.
The cohesiveness between a phrase an its context
can be measured aggregating the relatedness of the
context words against the target phrase. This cohe-
siveness should be high for phrases used literally.
Conversely, figurative usages can occur in a large
variety of contexts implying low cohesiveness. For
instance, the cohesiveness of the phrase ?a piece of
cake? against context words such as ?coffee?, ?birth-
day? and ?bakery? should be high. The distribu-
tional measures used to obtain the needed related-
ness scores and the proposed measures of cohesive-
ness are presented in subsection 2.1.
Moreover, we observed a stylistic trend in the
training data set. That is, figurative usage tends to
occur later in the document in comparison with the
literal usage. Consequently, a small set of features
that exploits this particular observation is proposed
in subsection 2.2.
Fazly et al (2009) showed that idiomatic phrases
composed of a verb and a noun (e.g. ?break a leg?)
differ from their literal usages in the use of some
syntactic structures. For instance, idiomatic phrases
are less flexible in the use of determiners, pluraliza-
114
tion and passivization. In order to capture that no-
tion in a simple way, a set of features form a part-
of-speech tagger was included in the feature set (see
subsection 2.3).
In Section, additional details of the proposed sys-
tem are provided jointly with the obtained official
results. Finally, in sections 4 and 5 a brief discus-
sion of the results and some concluding remarks are
presented.
2 Features
Each instance of the training and test sets consist of a
short document d where one or more occurrences of
its target phase pd are annotated. For each particular
phrase p, several instances are provided correspond-
ing to literal or figurative usages. In this section, the
set of features that was extracted from each instance
to provide a vectorial representation is presented.
2.1 Cohesiveness Features
Let?s start with some definitions borrowed from the
information retrieval field: D is a collection of doc-
uments, df(w) is the number of documents in D
where the word w occurs (document frequency),
df(w ? pd) is the number of documents where w
and a target phrase pd co-occur, tf(w, d) is the num-
ber of occurrences of w in a document d ? D (term
frequency), and idf(w) = log2
df(w)
|D| is the inverse
document frequency of w (Jones, 2004).
A simple distributional measure of relatedness be-
tween w and p can be obtained with the following
ratio:
R(w, p) =
df(w ? pd)
df(w)
(1)
Pointwise mutual information (PMI) (Church and
Hanks, 1990) is another distributional measure that
can be used for measuring the relatedness of w and
p. The probabilities needed for its calculation can be
obtained by maximum likelihood estimation (MLE):
P (w) ? df(w)|D| , P (pd) ?
df(pd)
|D| and P (w ? pd) ?
df(w?pd)
|D| .
Thus, PMI is given by this expression:
PMI(w, pd) = log2
(
P (w ? pd)
P (w) ? P (pd)
)
(2)
F1:
?
w?d? R(w, pd)
F2:
?
w?d? tf(w, d)
F3:
?
w?d? idf(w)
F4:
?
w?d? PMI(w, pd)
F5:
?
w?d? NPMI(w, pd)
F6:
?
w?d? (tf(w,d) ? R(w, pd))
F7:
?
w?d? (idf(w) ? R(w, pd))
F8
?
w?d? (R(w, pd) ? PMI(w, pd))
F9:
?
w?d? (R(w, pd) ?NPMI(w, pd))
F10:
?
w?d? (tf(w, d) ? idf(w))
F11:
?
w?d? (tf(w, pd) ? PMI(w, pd))
F12:
?
w?d? (tf(w, pd) ?NPMI(w, pd))
F13:
?
w?d? (idf(w) ? PMI(w, pd))
F14:
?
w?d? (idf(w) ?NPMI(w, pd))
F15:
?
w?d? (PMI(w, pd) ?NPMI(w, pd))
F16:
?
w?d? (tf(w, d) ? idf(w) ? R(w,pd))
F17:
?
w?d? (tf(w, d) ? R(w, pd) ? PMI(w, pd))
F18:
?
w?d? (tf(w, d) ? R(w, pd) ?NPMI(w, pd))
F19:
?
w?d? (tf(w, d) ? idf(w) ? PMI(w,pd))
F20:
?
w?d? (tf(w, d) ? idf(w) ?NPMI(w,pd))
Table 1: Cohesiveness features
Furthermore, the scores obtained through eq. 2
can be normalized in the interval [+2,0] with the fol-
lowing expression:
NPMI(w, pd) =
PMI(w, pd)
? log2(P (w ? pd))
+ 1 (3)
A measure of the cohesiveness between a docu-
ment d against its target phrase pd, can be obtained
by aggregating the pairwise relatedness scores be-
tween all the words in d and pd. For instance, us-
ing eq. 1 that measure is
?
w?d? R(w, pd), where d
?
is the set of different words in d. The equations 1,
2 and 3 can be used as weights associated to each
word, which can also be combined among them and
with tf and idf weights. Such weight combinations
produce measures that can be used as cohesiveness
features for a document. The set of 20 features ob-
tained using this approach is shown in Table 1.
2.2 Stylistic Features
The set of stylistic features related to the document
length, vocabulary size and relative position of the
occurrence of the target phrase in a document is
shown in Table 2.
115
F21: Relative position of pd in d
F22: Document length in characters
F23: Document length in tokens
F24: Number of different words
Table 2: Stylistic features
2.3 Syntactic Features
The features F25 to F67 correspond to the set of 43
part-of-speech tags of the NLTK English POS tag-
ger (Loper and Bird, 2002). Each feature contains
the frequency of occurrence of each POS-tag in a
document d.
3 Experimental Setup and Results
The data provided for this task consists of two data
sets LexSample and AllWords, which are divided
into development, training and test sets. Neverthe-
less, we considered a single training set aggregat-
ing the development and training parts from both
data sets for a total of 3,230 instances. Each train-
ing instance has a class label whether ?literally? or
?figuratively? depending on the usage or the tar-
get phrase. Similarly, the aggregated test set con-
tains 1,112 instances, but with unknown values in
the class attribute.
Firstly, the syntactic features for each text were
obtained using the POS tagger included in the NLTK
v.2.0.4 (Loper and Bird, 2002). Secondly, all texts
were preprocessed by tokenizing, lowecasing, stop-
word removing, punctuation removing and stem-
ming using the Porter?s algorithm (1980). This pre-
processed version of the texts was used to obtain the
remaining cohesiveness and stylistic features. The
resulting vectorial data set was used to produce the
predictions labeled ?UNAL.RUN1? through a Lo-
gistic classifier (Cessie and Houwelingen, 1992).
The implementation used for this classifier was the
included in WEKA v.3.6.9 (Hall et al, 2009). The
accuracies obtained by the different feature groups
in the training set using 10-fold cross validation are
shown in Table 3. The last column shows the per-
centage of relative improvement of different feature
sets combinations from the most frequent class base-
line to our best system using all features.
The predictions labeled ?UNAL.RUN2? were ob-
tained with the same vectorial data set but adding
Features Accuracy % improv.
All features 0.7272 100.0%
Cohesiveness+Syntactic 0.7034 87.1%
Cohesiveness 0.6833 76.2%
Syntactic 0.6229 43.5%
Stylistic 0.5492 3.5%
Baseline MFC 0.5427 0.0%
Table 3: Results by group of features in the training set
using 10-fold cross validation
System LexSample AllWords Both
UNAL.RUN1 0.7222 0.6680 0.6970
UNAL.RUN2 0.7542 0.6448 0.7032
Baseline MFC 0.5034 0.6158 0.5558
Best SemEval?13 0.7795 0.6680 0.7276
# test instances 594 518 1,112
Table 4: Official results in the test set (accuracy)
as a nominal feature the target phrase of each in-
stance. The official results obtained by both sub-
mitted runs are shown in Table 4. Note that official
results in the test set are reported separately for the
data sets LexSample and AllWords. The LexSample
test set contains instances whose target phrases were
seen in the training set (i.e. unseen contexts). Un-
like LexSample, AllWords contains instances whose
target phrases were unseen in the training set (i.e.
unseen phrases).
4 Discussion
As it was expected, the results obtained in the ?un-
seen context? setting were consistently better than
in ?unseen phrases?. This result suggests that the
discrimination of literal and figurative usage heavily
depends on particular idiomatic phrases. This can
also be confirmed by the best accuracy obtained by
RUN2 compared with RUN1 in LexSample. Clearly,
the classifier used in RUN2 exploited the identifica-
tion of the phrase to leverage a priori information
about the phrase such as the most frequent usage.
Another factor that could undermine the results in
the ?unseen phrases? setting is the low number of in-
stances per phrase in the AllWords test set, roughly a
third in comparison with LexSample. Given that the
effectiveness of the cohesiveness features depends
116
on the number of documents where the idiomatic
phrase occurs, the predictions for this test set relied
mainly on the less effective features, namely syn-
tactic and stylistic features (see Table 3). However,
this problem could be alleviated obtaining the distri-
butional statistics from a large corpus with enough
occurrences of the unseen phrases.
Besides it is important to note, that in spite of the
low individual contribution of the stylistic features
to the overall accuracy (3.5%), when these are com-
bined with the remaining features they provide an
improvement of 12.9% (see Table 3).
5 Conclusions
We participated in the Phrasal Semantics sub task 5b
in SemEval 2013. Our system proved the effective-
ness of the use of cohesiveness, stylistic and syn-
tactic features for discriminating literal from figura-
tive usage of idiomatic phrases. The most-frequent-
class baseline was overcame by 49.8% in the ?un-
seen contexts? setting (LexSample) and 8.5% in ?un-
seen phrases? (AllWords).
Acknowledgments
This research was funded in part by the Systems
and Industrial Engineering Department, the Office
of Student Welfare of the National University of
Colombia, Bogot?, and through a grant from the
Colombian Department for Science, Technology
and Innovation, Colciencias, proj. 1101-521-28465
with funding from ?El Patrimonio Aut?nomo Fondo
Nacional de Financiamiento para la Ciencia, la Tec-
nolog?a y la Innovaci?n, Francisco Jos? de Caldas.?
The third author recognizes the support from Mexi-
can Government (SNI, COFAA-IPN, SIP 20131702,
CONACYT 50206-H) and CONACYT?DST India
(proj. 122030 ?Answer Validation through Textual
Entailment?).
References
Julia Birke and Anoop Sarkar. 2006. A clustering ap-
proach for nearly unsupervised recognition of nonlit-
eral language. In Proceedings of the 11th Conference
of the European Chapter of the Association for Com-
putational Linguistics, Trento, Italy.
S. Le Cessie and J. C. Van Houwelingen. 1992. Ridge
estimators in logistic regression. Applied Statistics,
41(1):191.
Kenneth Ward Church and Patrick Hanks. 1990. Word
association norms, mutual information, and lexicogra-
phy. Comput. Linguist., 16(1):22?29, March.
Afsaneh Fazly, Paul Cook, and Suzanne Stevenson.
2009. Unsupervised type and token identifica-
tion of idiomatic expressions. Comput. Linguist.,
35(1):61?103, March.
Mark Hall, Frank Eibe, Geoffrey Holmes, and Bernhard
Pfahringer. 2009. The WEKA data mining software:
An update. SIGKDD Explorations, 11(1):10?18.
Karen Sp?rck Jones. 2004. A statistical interpretation of
term specificity and its application in retrieval. Jour-
nal of Documentation, 60(5):493?502, October.
Graham Katz and Eugenie Giesbrecht. 2006. Automatic
identification of non-compositional multi-word ex-
pressions using latent semantic analysis. In Proceed-
ings of the Workshop on Multiword Expressions: Iden-
tifying and Exploiting Underlying Properties, MWE
?06, pages 12?19, Stroudsburg, PA, USA. Association
for Computational Linguistics.
Ioannis Korkontzelos, Torsten Zesch, Fabio Massimo
Zanzotto, and Chris Biemann. 2013. SemEval-2013
task 5: Evaluating phrasal semantics. In Proceedings
of the 7th International Workshop on Semantic Evalu-
ation (SemEval 2013).
Dekang Lin. 1999. Automatic identification of non-
compositional phrases. In Proceedings of the 37th
annual meeting of the Association for Computational
Linguistics on Computational Linguistics, ACL ?99,
page 317?324, Stroudsburg, PA, USA. Association for
Computational Linguistics.
Edward Loper and Steven Bird. 2002. NLTK: the natu-
ral language toolkit. In Proceedings of the ACL Work-
shop on Effective Tools and Methodologies for Teach-
ing Natural Language Processing and Computational
Linguistics. Philadelphia. Association for Computa-
tional Linguistics.
Martin Porter. 1980. An algorithm for suffix stripping.
Program, 3(14):130?137, October.
Caroline Sporleder and Linlin Li. 2009. Unsupervised
recognition of literal and non-literal use of idiomatic
expressions. In Proceedings of the 12th Conference of
the European Chapter of the Association for Computa-
tional Linguistics, EACL ?09, page 754?762, Strouds-
burg, PA, USA. Association for Computational Lin-
guistics.
117
Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 2: Seventh International Workshop on Semantic
Evaluation (SemEval 2013), pages 280?284, Atlanta, Georgia, June 14-15, 2013. c?2013 Association for Computational Linguistics
SOFTCARDINALITY: Hierarchical Text Overlap
for Student Response Analysis
Sergio Jimenez, Claudia Becerra
Universidad Nacional de Colombia
Ciudad Universitaria,
edificio 453, oficina 114
Bogot?, Colombia
sgjimenezv@unal.edu.co
cjbecerrac@unal.edu.co
Alexander Gelbukh
CIC-IPN
Av. Juan Dios B?tiz, Av. Mendiz?bal,
Col. Nueva Industrial Vallejo
CP 07738, DF, M?xico
gelbukh@gelbukh.com
Abstract
In this paper we describe our system used to
participate in the Student-Response-Analysis
task-7 at SemEval 2013. This system is based
on text overlap through the soft cardinality and
a new mechanism for weight propagation. Al-
though there are several official performance
measures, taking into account the overall ac-
curacy throughout the two availabe data sets
(Beetle and SciEntsBank), our system ranked
first in the 2 way classification task and sec-
ond in the others. Furthermore, our sys-
tem performs particularly well with ?unseen-
domains? instances, which was the more chal-
lenging test set. This paper also describes an-
other system that integrates this method with
the lexical-overlap baseline provided by the
task organizers obtaining better results than
the best official results. We concluded that the
soft cardinality method is a very competitive
baseline for the automatic evaluation of stu-
dent responses.
1 Introduction
The Student-Response-Analysis (SRA) task consist
in provide assessments of the correctness of student
answers (A), considering their corresponding ques-
tions (Q) and reference answers (RA) (Dzikovska
et al, 2012). SRA is the task-7 in the SemEval
2013 evaluation campaign (Dzikovska et al, 2013).
The method used in our participation was basically
text overlap based on the soft cardinality (Jimenez
et al, 2010) plus a machine learning classifier. This
method did not use any information external to the
data sets except for a stemmer and a list of stop
words.
The soft cardinality is a general model for object
comparison that has been tested at text applications.
Particularly, this text overlap approach has provided
strong baselines for several applications, i.e. entity
resolution (Jimenez et al, 2010), semantic textual
similarity (Jimenez et al, 2012a), cross-lingual tex-
tual entailment (Jimenez et al, 2012b), information
retrieval, textual entailment and paraphrase detec-
tion (Jimenez and Gelbukh, 2012). A brief descrip-
tion of the soft cardinality is presented in the next
section.
The data for SRA consist of two data sets Bee-
tle (5,199 instances) and SciEntsBank (10,804 in-
stances) divided into training and test sets (76%-
24% for Beetle and 46%-54% SciEntsBank). In ad-
dition, the test part of Beetle data set was divided
into two test sets: ?unseen answers? (35%) and ?un-
seen questions? (65%). Similarity, SciEntsBank test
part is divided into ?unseen answers? (9%), ?unseen
questions? (13%) and ?unseen domains? (78%). All
texts are in English.
The challenge consists in predicting for each in-
stance triple (Q, A, RA) an assessment of correct-
ness for the student?s answer. Three levels of detail
are considered for this assessment: 2 way (correct
and incorrect), 3 way (correct, contradictory and in-
correct) and 5 way (correct, incomplete, contradic-
tory, irrelevant and non-in-the-domain).
Section 3 presents the method used for the extrac-
tion of features from texts using the soft cardinal-
ity to provide a vector representation. In Section 4,
the details of the system used to produce our predic-
280
tions are presented. Besides, in that section a system
that integrates our system with the lexical-overlap
baseline proposed by the task organizers is also pre-
sented. This combined system was motivated by the
observation that our system performed well in the
SciEntsBank data set but poorly in Beetle in compar-
ison with the lexical-overlap baseline. The results
obtained by both systems are also presented in that
section.
Finally in Section 5 the conclusions of our partic-
ipation in this evaluation campaign are presented.
2 Soft Cardinality
The soft cardinality (Jimenez et al, 2010) of a col-
lection of elements S is calculated with the follow-
ing expression:
|S|? =
n?
i=1
wi ?
?
?
n?
j=1
sim(si, sj)p
?
?
?1
(1)
Having S ={s1, s2, . . . , sn}; wi ? 0; p ? 0;
1 > sim(x, y) ? 0, x 6= y; and sim(x, x) = 1.
The parameter p controls the degree of "softness"
of the cardinality (the larger the ?harder?). In fact,
when p ? ? the soft cardinality is equivalent to
classical set cardinality. The default value for this
parameter is p = 1. The coefficients wi are weights
associated with each element, which can represent
the importance or informative character of each ele-
ment. The function sim is a similarity function that
compares pairs of elements in the collection S.
3 Features from Cardinalities
It is commonly accepted that it is possible to make
a fair comparison of two objects if they are of the
same nature. If the objects are instances of a com-
positional hierarchy, they should belong to the same
class to be comparable. Clearly, a house is compa-
rable with another house, a wall with another wall
and a brick with another brick, but walls and bricks
are not comparable (at least not directly). Similarly,
in text applications documents should be compared
with documents, sentences with sentences, words
with words, and so on.
However, a comparison measure between a sen-
tence and a document can be obtained with different
approaches. First, using the information retrieval ap-
proach, the document is considered like a very long
sentence and the comparison is then straight for-
ward. Another approach is to make pairwise com-
parisons between the sentence and each sentence in
the document. Then, the similarity scores of these
comparisons can be aggregated in a single score
using average, max or min functions. These ap-
proaches have issues, the former ignores the sen-
tence subdivision of the document and the later ig-
nores the similarities among the sentences in the
document.
In the task at hand, each instance is composed of
a question Q, a student answer A, which are sen-
tences, and a collection of reference answers RA,
which could be considered as a multi-sentence doc-
ument. The soft cardinality can be used to provide
values for |Q|?, |A|?, |RA|?, |Q?A|?, |A?RA|? and
|Q?RA|?. The intersections that involve RA require
a special treatment to tackle the aforementioned is-
sues.
Let?s start defining a word-similarity function.
Two words (or terms) t1 and t2 can be compared di-
viding them into character q-grams (Kukich, 1992).
The representation in q-grams of ti can be denoted
as t[q]i . Similarly, a combined representation us-
ing a range of q-grams of different length can be
denoted as t[q1:q2]i . For instance, if t1 =?home?
then t[2:3]1 ={?ho?,?om?,?me?,?hom?,?ome?}. Thus,
t[q1:q2]1 and t
[q1:q2]
2 representations can be com-
pared using the Dice?s coefficient to build a word-
similarity function:
simwords(t1, t2) =
2 ?
?
?
?t[q1:q2]1 ? t
[q1:q2]
2
?
?
?
?
?
?t[q1:q2]1
?
?
?+
?
?
?t[q1:q2]1
?
?
?
(2)
Note that in eq. 2 the classical set cardinality was
used, i.e |x| means classical cardinality and |x|? soft
cardinality.
The function simwords can be plugged in eq.1 to
obtain the soft cardinality of a sentence S (using uni-
tary weights wi = 1 and p = 1):
|S|? =
|S|?
i=1
?
?
|S|?
j=1
simword(ti, tj)
?
?
?1
(3)
281
|X| |Y | |X ? Y |
BF1: |Q|? BF2: |A|? BF3: |Q ?A|?
BF2: |A|? BF4: |RA|?? BF5: |RA ?A|??
BF1: |Q|? BF4: |RA|?? BF6: |RA ?Q|??
Table 1: Basic feature set
Where ti are the words in the sentence S .
The sentence-soft-cardinality function can be
used to build a sentence-similarity function to com-
pare two sentences S1 and S2 using again the Dice?s
coefficient:
simsent.(S1, S2) =
2 ? (|S1|? + |S2|? ? |S1 ? S2|?)
|S1|+ |S2|
(4)
In this formulation S1?S2 is the concatenation of
both sentences.
The eq. 4 can be plugged again into eq. 1 to obtain
the soft cardinality of a ?document? RA, which is a
collection of sentences RA = {S1, S2. . . . , S|RA|}:
|RA|?? =
|RA|?
i=1
|Si|
? ?
?
?
|RA|?
j=1
sim(Si, Sj)
?
?
?1
(5)
Note that the soft cardinalities of the sentences
|Si|? were re-used as importance weights wi in eq.
1. These weights are propagations of the unitary
weights assigned to the words, which in turn were
aggregated by the soft cardinality at sentence level
(eq. 3). This soft cardinality is denoted with double
apostrophe because is a function recursively based
in the single-apostrophized soft cardinality.
The proposed soft cardinality expressions are
used to obtain the basic feature set presented in Ta-
ble 1. The soft cardinalities of |Q|?, |A|? and |Q?A|?
are calculated with eq. 3. The soft cardinalities
|RA|??, |RA?A|?? and |RA?Q|?? are calculated with
eq. 5. Recall that Q ? A is the concatenation of the
question and answer sentences. Similarly, RA ? A
and RA ?Q are the collection of reference answers
adding A xor Q .
Starting from the basic feature set, an extended
set, showed in Table 2, can be obtained from each
one of the three rows in Table 1. Recall that |X ?
Y | = |X|+ |Y |?|X?Y | and |X \Y | = |X|?|X?
EF1: |X ? Y | EF2: |X \ Y |
EF3: |Y \X| EF4: |X?Y ||X|
EF5:
|X?Y |
|Y | EF6:
|X?Y |
|X?Y |
EF7:
2?|X?Y |
|X|+|Y | EF8:
|X?Y |?
|X|?|Y |
EF9:
|X?Y |
min(|X|,|Y |) EF10:
|X?Y |
max(|X|,|Y |)
EF11:
|X?Y |?(|X|+|Y |)
2?|X|?|Y | EF12: |X ? Y | ? |X ? Y |
Table 2: Extended feature set
Y |. Consequently, the total number of features is 6
basic features plus 12 extended features multiplied
by 3, i.e. 42 features.
4 Systems Description
4.1 Submitted System
First, each text in the SRA data was preprocessed by
tokenizing, lowercasing, stop-words1 removing and
stemming with the Porter?s algorithm (Porter, 1980).
Second, each stemmed word t was represented in
q-grams: t[3:4] for Beetle and t[4] for SciEntsBank.
These representations obtained the best accuracies
in the training data sets.
Two vector data sets were obtained extracting the
42 features?described in Section 3?for each instance
in Beetle and SciEntsBank separately. Then, three
classification models (2 way, 3way and 5 way) were
learned from the training partitions on each vector
data set using a J48 graft tree (Webb, 1999). All
6 resulting classification models were boosted with
15 iterations of bagging (Breiman, 1996). The used
implementation of this classifier was that included
in WEKA v.3.6.9 (Hall et al, 2009). The results
obtained by this system are shown in Table 3 in the
rows labeled with ?Soft Cardinality-run1?.
4.2 An Improved System
At the time when the official results were released,
we observed that our submitted system performed
pretty well in SciEntsBank but poorly in Beetle.
Moreover, the lexical-overlap baseline outperformed
our system in Beetle. Firstly, we decided to include
in our feature set the 8 features of the lexical over-
lap baseline described by Dzikovska et al (2012)
1those provided by nltk.org
282
Beetle SciEntsBank
Task System UA1 UQ2 All UA1 UQ2 UD3 All All Rank
2 way
Soft Cardinality-unofficial 0.797 0.725 0.750 0.717 0.733 0.726 0.726 0.730 -
Soft Cardinality-run1 0.781 0.667 0.707 0.724 0.745 0.711 0.716 0.715 1
ETS-run1 0.811 0.741 0.765 0.722 0.711 0.698 0.702 0.713 2
CU-run1 0.786 0.718 0.742 0.656 0.674 0.693 0.687 0.697 3
Lexical overlap baseline 0.797 0.740 0.760 0.661 0.674 0.676 0.674 0.690 6
3 way
Soft Cardinality-unofficial 0.608 0.532 0.559 0.656 0.671 0.646 0.650 0.634 -
ETS-run1 0.633 0.551 0.580 0.626 0.663 0.632 0.635 0.625 1
Soft Cardinality-run1 0.624 0.453 0.513 0.659 0.652 0.637 0.641 0.618 2
CoMeT-run1 0.731 0.518 0.592 0.713 0.546 0.579 0.587 0.588 3
Lexical overlap baseline 0.595 0.512 0.541 0.556 0.540 0.577 0.570 0.565 8
5way
Soft Cardinality-unofficial 0.572 0.476 0.510 0.552 0.520 0.534 0.534 0.530 -
ETS-run1 0.574 0.560 0.565 0.543 0.532 0.501 0.509 0.519 1
Soft Cardinality-run1 0.576 0.451 0.495 0.544 0.525 0.512 0.517 0.513 2
ETS-run2 0.715 0.621 0.654 0.631 0.401 0.476 0.481 0.512 3
Lexical overlap baseline 0.519 0.480 0.494 0.437 0.413 0.415 0.417 0.430 11
Total number of test instances 439 819 1,258 540 733 4,562 5,835 7,093
TEST SETS: unseen answers1, unseen questions2, unseen domains3.
Table 3: Official results for the top-3 performing systems (among 15), the lexical overlap baseline in the SRA task
SemEval 2013 and unofficial results of the soft cardinality system combined with the lexical overlap (in italics).
Performance measure used: overall accuracy.
(see Text::Similarity::Overlaps2 package for more
details).
Secondly, the lexical overlap baseline aggregates
the pairwise scores between each reference answer
and the student answer by taking the maximum
value of the pairwise scores. So, we decided to use
this aggregation mechanism instead of the aggrega-
tion proposed through eq. 3.
Thirdly, only at that time we realized that, unlike
Beetle, in SciEntsBank all instances have only one
reference answer. Consequently, the only effect of
eq. 5 in SciEntsBank was in the calculation of |RA?
A|?? (and |RA?Q|??) by |X?Y |?? = |X|
?+|Y |?
1+simsent.(X,Y )
.
As a result, this transformation induced a boosting
effect in X?Y making |X?Y |?? ? |X?Y |? for any
X , Y . We decided to use this intersection-boosting
effect not only in RA ? A, RA ? Q, but in Q ?
A. This intersecton boosting effect works similarly
to the Lesk?s measure (Lesk, 1986) included in the
lexical overlap baseline.
The individual effect in the performance of each
2http://search.cpan.org/dist/Text-
Similarity/lib/Text/Similarity/Overlaps.pm
of the previous decisions was positive in all three
cases. The results obtained using an improved
system that implemented those three decisions are
shown in Table 3?in italics. This system would have
obtained the best general overall accuracy in the of-
ficial ranking.
5 Conclusions
We participated in the Student-Response-Analysis
task-7 in SemEval 2013 with a text overlap system
based on the soft cardinality. This system obtained
places 1st (2 way task) and 2nd (3 way and 5 way)
considering the overall accuracy across all data sets
and test sets. Particularly, our system was the best
in the largest and more challenging test set, namely
?unseen domains?. Moreover, we integrated the lex-
ical overlap baseline to our system obtaining even
better results.
As a conclusion, the text overlap method based on
the soft cardinality is very challenging base line for
the SRA task.
283
Acknowledgments
This research was funded in part by the Systems
and Industrial Engineering Department, the Office
of Student Welfare of the National University of
Colombia, Bogot?, and through a grant from the
Colombian Department for Science, Technology
and Innovation, Colciencias, proj. 1101-521-28465
with funding from ?El Patrimonio Aut?nomo Fondo
Nacional de Financiamiento para la Ciencia, la Tec-
nolog?a y la Innovaci?n, Francisco Jos? de Caldas.?
The third author recognizes the support from Mexi-
can Government (SNI, COFAA-IPN, SIP 20131702,
CONACYT 50206-H) and CONACYT?DST India
(proj. 122030 ?Answer Validation through Textual
Entailment?).
References
Leo Breiman. 1996. Bagging predictors. Machine
Learning, 24(2):123?140.
Myroslava O. Dzikovska, Rodney D. Nielsen, and Chris
Brew. 2012. Towards effective tutorial feedback for
explanation questions: a dataset and baselines. In Pro-
ceedings of the 2012 Conference of the North Amer-
ican Chapter of the Association for Computational
Linguistics: Human Language Technologies, NAACL
HLT ?12, page 200?210, Stroudsburg, PA, USA. As-
sociation for Computational Linguistics.
Myroslava O. Dzikovska, Rodney D. Nielsen, Chris
Brew, Claudia Leacock, Danilo Giampiccolo, Luisa
Bentivogli, Peter Clark, Ido Dagan, and Hoa Trang
Dang. 2013. SemEval-2013 task 7: The joint stu-
dent response analysis and 8th recognizing textual en-
tailment challenge. In Proceedings of the 7th Inter-
national Workshop on Semantic Evaluation (SemEval
2013), in conjunction with the Second Joint Confer-
ence on Lexical and Computational Semantcis (*SEM
2013), Atlanta, Georgia, USA, June. Association for
Computational Linguistics.
Mark Hall, Frank Eibe, Geoffrey Holmes, and Bernhard
Pfahringer. 2009. The WEKA data mining software:
An update. SIGKDD Explorations, 11(1):10?18.
Sergio Jimenez and Alexander Gelbukh. 2012. Baselines
for natural language processing tasks. Appl. Comput.
Math., 11(2):180?199.
Sergio Jimenez, Fabio Gonzalez, and Alexander Gel-
bukh. 2010. Text comparison using soft cardinality.
In Edgar Chavez and Stefano Lonardi, editors, String
Processing and Information Retrieval, volume 6393 of
LNCS, pages 297?302. Springer, Berlin, Heidelberg.
Sergio Jimenez, Claudia Becerra, and Alexander Gel-
bukh. 2012a. Soft cardinality: A parameterized simi-
larity function for text comparison. In Proceedings of
the 6th International Workshop on Semantic Evalua-
tion (SemEval, *SEM 2012), Montreal, Canada.
Sergio Jimenez, Claudia Becerra, and Alexander Gel-
bukh. 2012b. Soft cardinality+ ML: learning adaptive
similarity functions for cross-lingual textual entail-
ment. In Proceedings of the 6th International Work-
shop on Semantic Evaluation (SemEval, *SEM 2012),
Montreal, Canada. ACL.
Karen Kukich. 1992. Techniques for automatically
correcting words in text. ACM Computing Surveys,
24:377?439, December.
Michael Lesk. 1986. Automatic sense disambiguation
using machine readable dictionaries: how to tell a pine
cone from an ice cream cone. In Proceedings of the
5th annual international conference on Systems docu-
mentation, SIGDOC ?86, page 24?26, New York, NY,
USA. ACM.
Martin Porter. 1980. An algorithm for suffix stripping.
Program, 3(14):130?137, October.
Geoffrey I. Webb. 1999. Decision tree grafting from the
all-tests-but-one partition. In Proceedings of the 16th
international joint conference on Artificial intelligence
- Volume 2, IJCAI?99, pages 702?707, San Francisco,
CA, USA. Morgan Kaufmann Publishers Inc.
284
Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014), pages 732?742,
Dublin, Ireland, August 23-24, 2014.
UNAL-NLP: Combining Soft Cardinality Features for Semantic
Textual Similarity, Relatedness and Entailment
Sergio Jimenez, George Due
?
nas,
and Julia Baquero
Universidad Nacional de Colombia
Ciudad Universitaria, edificio 453,
oficina 114, Bogot?a, Colombia
[sgjimenezv,geduenasl,
jmbaquerov]@unal.edu.co
Alexander Gelbukh
Center for Computing Research (CIC),
Instituto Polit?ecnico Nacional (IPN),
Av. Juan Dios B?atiz, Av. Mendiz?abal,
Col. Nueva Industrial Vallejo,
Mexico City, Mexico
www.gelbukh.com
Abstract
This paper describes our participation in
the SemEval-2014 tasks 1, 3 and 10. We
used an uniform approach for addressing
all the tasks using the soft cardinality for
extracting features from text pairs, and
machine learning for predicting the gold
standards. Our submitted systems ranked
among the top systems in all the task and
sub-tasks in which we participated. These
results confirm the results obtained in pre-
vious SemEval campaigns suggesting that
the soft cardinality is a simple and useful
tool for addressing a wide range of natural
language processing problems.
1 Introduction
The semantic textual similarity is a core prob-
lem in the computational linguistic field. Con-
sequently, the previous evaluation campaigns of
this task in SemEval have attracted the attention
of many research groups worldwide (Agirre et al.,
2012; Agirre et al., 2013).This year, 3 tasks related
to this problem have been proposed exploring dif-
ferent facets such as semantic relatedness, entail-
ment , multilingualism, lack of training data and
imbalance in the amount of information.
The soft cardinality (Jimenez et al., 2010) is a
simple concept that generalizes the classical set
cardinality by considering the similarities among
the elements in a collection for a more intuitive
quantification of the number of elements in that
collection. This approach can be applied to text
applications representing texts as collections of
words and providing a similarity function that
compares two words. Varying this word-to-word
similarity function the soft cardinality can reflect
This work is licensed under a Creative Commons At-
tribution 4.0 International Licence. Page numbers and pro-
ceedings footer are added by the organisers. Licence details:
http://creativecommons.org/licenses/by/4.0/
notions of syntactic similarity, semantic related-
ness, among others. We (and others) have used
this approach to address with success the semantic
textual similarity and other tasks in previous Se-
mEval editions (Jimenez et al., 2012b; Jimenez et
al., 2012a; Jimenez et al., 2013a; Jimenez et al.,
2013b; Jimenez et al., 2013c; Croce et al., 2013).
In this paper we describe our participating sys-
tems in the SemEval-2014 tasks 1, 3, and 10,
which used the soft cardinality as core approach.
2 Features from Soft Cardinalities
The cardinality of a collection of elements is the
counting of non-repeated elements in it. This def-
inition is intrinsically associated with the notion
of set, which is a collection of non-repeated ele-
ments.Thus, the cardinality of a collection or set
A is denoted as |A|. Clearly, the cardinality of a
collection with repeated elements treats groups of
identical elements as a single instance contribut-
ing only with a unit (1) to the element counting.
Jimenez et al. (2010) proposed the soft cardinal-
ity that uses a notion of similarity among elements
for grouping not only identical elements but simi-
lar too. That notion of similarity among elements
is provided by a similarity function that compares
two elements a
i
and a
j
and returns a score in [0,1]
interval, having sim(a
i
, a
i
) = 1. Although, it
is not necessary that sim fulfills another metric
properties aside of identity, symmetry is also de-
sirable. Thus, the soft cardinality of a collection
A, whose elements a
1
, a
2
, . . . , a
|A|
are compara-
ble with a similarity function sim(a
i
, a
j
), is de-
noted as |A|
sim
. This soft cardinality is given by
the following expression:
|A|
sim
=
|A|
?
i=1
w
a
i
?
|A|
j=1
sim(a
i
, a
j
)
p
(1)
It is trivial to see that |A| = |A|
sim
either if
p ? ? or when the function sim is a crisp com-
732
Basic Derived
|A| |A ?B| = |A|+ |B| ? |A ?B|
|B| |A4B| = |A ?B| ? |A ?B||
|A ?B| |A \B| = |A| ? |A ?B|
|B \A| = |B| ? |A ?B|
Table 1: The 7 basic and derived cardinalities for
two sets comparison.
parator, i.e. one that returns 1 for identical ele-
ments and 0 otherwise. This property shows that
the soft cardinality generalizes the classical cardi-
nality and that the parameter p controls its degree
of ?softness?, whose default value is 1. The values
w
a
i
are optional ?importance? weights associated
with each element a
i
, by default those weights can
be assigned to 1.
For the tasks at hand, we represent each short
text (lets say A) as a collection of words a
i
and
the sim function can be any operator that com-
pares pairs of words. The motivation for using the
soft cardinality is that the sim function can reflect
any dimension of word similarity (e.g. syntactic,
semantic) and the soft cardinality projects that no-
tion at sentence level. For instance, if sim pro-
vides the degree of semantic relatedness between
two words using WordNet, two texts A and B
could be compared by computing |A|
sim
, |B|
sim
and |A?B|
sim
. Given that A?B could be empty,
the soft cardinality of the intersection must be ap-
proximated by |A ? B|
sim
? |A|
sim
+ |B|
sim
?
|A ? B|
sim
instead of being computed directly
from A ? B using equation 1. Using that approx-
imation, the commonality (intersection) between
A and B is induced by the pair-wise similarities
provided by sim among the words in A and B.
Since more than a century when Jaccard (1901)
proposed his well-known index, the classical set
cardinality has been used to build similarity func-
tions for set comparison. Any binary-cardinality-
based similarity function is an algebraic combina-
tion of |A|, |B| and either |A ? B| or |A ? B|
(e.g. Jaccard, Dice, Tversky, overlap and cosine
indexes). These three cardinalities describes un-
ambiguously all the regions in the Venn?s diagram
when comparing two sets. Thus, in this scenario 4
possible cardinalities can be derived from these 3
basic cardinalities, see Table 1. Clearly, the same
set of cardinalities can be obtained for the soft car-
dinality.
When training data is available, which is the
# Feature expression
1
|A|
/|A?B|
2
|A|?|A?B|
/|A|
3
|A|?|A?B|
/|A?B|
4
|B|
/|A?B|
5
|B|?|A?B|
/|B|
6
|B|?|A?B|
/|A?B|
7
|A?B|
/|A|
8
|A?B|
/|B|
9
|A?B|
/|A?B|
10
|A?B|?|A?B|
/|A?B|
Table 2: Extended set of 10 rational features.
case for tasks 1, 3 and 10 in SemEval 2014, it
is possible to think that instead of using an ad-
hoc expression (e.g. Jaccard, Dice) the similar-
ity function can be obtained using the cardinalities
in Table 1 as features for a machine-learning re-
gression algorithm. Our hypothesis is that such
learnt function should predict in a more accurate
way the gold standard variable than any other ad-
hoc function. However, these cardinality features
are intrinsically correlated with the length of the
texts where they were obtained. This correlation
makes that the performance of the learnt similar-
ity function could be dependent of the length of
the texts. For instance, if the function was trained
using long texts it is plausible to think that this
function would be more effective when tested with
long texts than with shorter ones. Having this in
mind, an extended set of rational features is pro-
posed, whose values are standardized in [0,1] in-
terval aiming to reduce the effect of the length of
the texts. These features are presented in Table 2.
The soft cardinality has proven to overcome
the classic cardinality in the semantic textual
similarity (STS) task in previous SemEval cam-
paigns (Jimenez et al., 2012b; Jimenez et al.,
2013a). Even using a simplistic function sim
based on q-grams of characters, the soft cardinal-
ity method ranked third among 89 participating
systems (Agirre et al., 2012). Thus, our participat-
ing systems in the SemEval 2014 campaign were
based on the previously described set of 17 fea-
tures, obtained from the soft cardinality with dif-
ferent sim functions for comparing pairs of words.
Each sim function produced a different set of fea-
tures, which were combined with a regression al-
gorithm for similarity and relatedness tasks. Sim-
ilarly, a classification algorithm was used for the
733
entailment task.
3 Systems Description
In this section the different feature sets used for
each submitted system to the different task and
subtask are described. Besides, the data used for
training, parameters and other preprocessing de-
tails are described for each system.
3.1 Task 1: Textual Relatedness and
Entailment
The task 1 is based on the SICK (Sentences
Involving Compositional Knowledge) data set
(Marelli et al., 2014), which contains nearly
10,000 pairs of sentences manually labeled by re-
latedness and entailment. The relatedness gold la-
bels range from 1 to 5, having 1 the minimum level
of relatedness between the texts and 5 for the max-
imum. The entailment labels have three categori-
cal values: neutral, contradiction and entailment.
The two sub tasks consist of predicting the related-
ness and entailment gold standards using approxi-
mately the 50% of the text pairs as training and the
other part as test bed.
Our overall approach consists in extracting 4
different sets of features using the method pre-
sented in section 2 and training a machine learn-
ing algorithm for predicting the gold standard la-
bels in the test data. Each feature set is described
in the following 4 subsections and the subsection
3.1.6 provides details of the used combination of
features, machine learning algorithm and prepro-
cessing details.
3.1.1 String-Matching Features
First, all texts in the SICK data set where prepro-
cessed by lower casing, tokenizing and stop-word
removal (using the NLTK
1
). Then each word was
reduced to its stem using the Porter?s algorithm
(Porter, 1980) and a idf weight (Jones, 2004) was
associated to each stem (w
a
i
weights in eq. 1) us-
ing the very SICK data set as document collec-
tion. Next, for each instance in the data, which
is composed of two texts A and B, the 17 fea-
tures listed in Tables 1 and 2 where extracted using
eq.1. The used word-to-word similarity function
sim decomposes each word in bags of 3-grams
of characters, which are compared using the sym-
metrical Tversky?s index (Tversky, 1977; Jimenez
et al., 2013a). Thus, the similarity between two
1
http://www.nltk.org/
pairs of words w
1
and w
2
, represented each one as
a collection of 3-grams of characters, is given by
the following expression:
sim(w
1
, w
2
) =
|c|
?(?|w
min
|+ (1? ?)|w
max
|) + |c|
(2)
|c| = |w
1
? w
2
|+ bias
sim
,
|w
min
| = min[|w
1
\ w
2
|, |w
2
\ w
1
|],
|w
max
| = max[|w
1
\ w
2
|, |w
2
\ w
1
|].
The values used for the parameters were ? =
1.9, ? = 2.36, bias = ?0.97, and p = 0.39
(where p corresponds to eq.1). The motivation and
justification for these parameters can be found in
(Jimenez et al., 2013a). These values were ob-
tained by building a text similarity function us-
ing the Dice?s coefficient and the soft cardinali-
ties plugging eq.2 in eq.1. Next, this text similar-
ity function is evaluated in the 5,000 training text
pairs and the obtained scores are compared against
the relatedness gold-standard using the Pearson?s
correlation.
w
a
i
are not training parameters, but they are
weights associated with the words. These weights
could have been obtained from a larger corpus,
but we use the training texts to obtain them. This
process is repeated iteratively exploring the search
space defined by these 4 parameters using a hill-
climbing approach until a maximum correlation is
reached. We observe that the optimal values of the
parameters p, ?, ?, and bias vary considerably be-
tween the data sets and for the different sim func-
tions of word-to-word similarity. We do not yet
understand from which factors of the data and the
sim functions depend on these parameters. This
issue will be the objective of further research.
Henceforth, the set of 17 string-based features
described in this subsection will be referred as
SM.
3.1.2 ESA Features
For this set of features we used the idea proposed
by Gabrilovich and Markovitch (2007) of enrich-
ing the representation of a text by representing
each word by its textual definition in a knowl-
edge base, i.e. explicit semantic analysis (ESA).
For that, we used as knowledge base the synset?s
textual definitions provided by WordNet. First,
in order to determine the textual definition asso-
ciated to each word, the texts were tagged using
734
the maximum entropy POS tagger included in the
NLTK. Next, the adapted Lesk algorithm (Baner-
jee and Pedersen, 2002) for word sense disam-
biguation was applied in the texts disambiguating
one word at the time. The software package used
for this disambiguation process was pywsd
2
. The
arguments needed for the disambiguation of each
word are the POS tag of the target word and the
entire sentence as context. Once all the words are
disambiguated with their corresponding WordNet
synsets, each word is replaced by all the words in
their textual definition jointly with the same word
and its lemma. The final result of this stage is that
each text in the data set is replaced by a longer
text including the original text and some related
words. The motivation of this procedure is that the
extended versions of each pair of texts have more
chance of sharing common words that the original
texts.
The extended versions of these texts were used
to obtain another 17 features with the same proce-
dure described in the previous subsection (3.1.1).
This feature subset will henceforth be referred as
ESA.
3.1.3 Features for each part-of-speech
category
This set of features is motivated by the idea pro-
posed by Corley and Mihalcea (2005) of group-
ing words by their POS category before being
compared for semantic textual similarity. Our ap-
proach consist in provide a version of each text
pair in the data set for each POS category in-
cluding only the words belonging to that cate-
gory. For instance, the pair of texts {?A beauti-
ful girl is playing tennis?, ?A nice and handsome
boy is playing football?} produce new pairs such
as: {?beautiful?, ?nice handsome?} for the ADJ
tag, {?girl tennis?, ?boy football?} for NOUN and
{?is playing?, ?is playing?} for VERB.
Again, the POS tags were provided by the
NLTK?s max entropy tagger. The 28 POS cate-
gories were simplified to 9 categories in order to
avoid an excessive number of features and hence
sparseness; the used mapping is shown in Table 3.
Next, for each one of the 9 new POS categories a
set of 17 features (SM) is extracted reusing again
the method proposed in subsection 3.1.1. The only
difference with the method described in that sub-
section is that the stop-words were not removed
2
https://github.com/alvations/pywsd
Reduced tag set NLTK?s POS tag set
ADJ JJ,JJR,JJS
NOUN NN,NNP,NNPS,NNS
ADV RB,RBR,RBS,WRB
VERB VB,VBD,VBG,VBN,VBP,VBZ
PRO WP,WP$,PRP,PRP$
PREP RP,IN
DET PDT,DT,WDT
EX EX
CC CC
Table 3: Mapping reduction of the POS tag set.
and the stemming process was not performed. The
motivation for generating this feature sets by POS
category is that the machine learning algorithms
could weight differently each category. The intu-
ition behind this is that it is reasonable that cat-
egories such as VERB and NOUN could play a
more important role for the task at hand than oth-
ers such as ADJ or PREP. Using these categorized
features, such discrimination among POS cate-
gories can be discovered from the training data.
Finally, the total number of features in this set is
153 (17 features? 9 POS categories). This feature
set will be referred as POS.
3.1.4 Features From Dependencies
The syntactic soft cardinality (Croce et al., 2012;
Croce et al., 2013) extend the soft cardinality
approach by representing texts as bags of de-
pendencies instead of bags of words. Each de-
pendency is a 3-tuple composed of two syntac-
tically related words and the type of their rela-
tionship. For instance, the sentence ?The boy
plays football? can be represented with 3 depen-
dencies: [det,?boy?,?The?], [subj,?plays?,?boy?]
and [obj,?plays?,?football?]. Clearly, this repre-
sentation distinguish pairs of texts such as {?The
dog bites a boy?,?The boy bites a dog?}, which
are indistinguishable when they are represented as
bags of words. This representation can be obtained
automatically using the Stanford Parser (De Marn-
effe et al., 2006), which in addition provides a de-
pendency identifying the root word in a sentence.
We used the version 3.3.1
3
of that parser to obtain
such representation.
Once the texts are represented as bags of de-
pendencies, it is necessary to provide a similar-
ity function between two dependency tuples in or-
3
http://nlp.stanford.edu/software/lex-parser.shtml
735
der to use the soft cardinality (eq. 1) and hence
to obtain the 17 cardinality features in Tables 1
and 2. Such function can be obtained using the
sim function (eq. 2) for comparing the first and
second words between the dependencies and even
the labels of the dependency types. Let?s consider
two dependencies tuples d = [d
dep
, d
w
1
, d
w
2
] and
p = [p
dep
, p
w
1
, p
w
2
] where d
dep
and p
dep
are the
labels of the dependency type; d
w
1
and p
w
1
are
the first words on each dependency tuple; and d
w
2
and p
w
2
are the second words. The similarity func-
tion for comparing two dependency tuples can be a
linear combination of the sim scores between the
corresponding elements of the dependency tuples
by the following expression:
sim
dep
(d, p) =
?sim(d
dep
, p
dep
) + ?sim(d
w
1
, p
w
2
) + ?sim(d
w
2
, p
w
2
)
Although, it is unusual to compare the depen-
dencies? type labels d
dep
and p
dep
with a similar-
ity function designed for words, we observed ex-
perimentally that this approach yield better overall
performance in the relatedness task in comparison
with a simple crisp comparison. The optimal val-
ues for the parameters ? = ?3, ? = 10 and ? = 3
were determined with the same methodology used
in subsection 3.1.1 for determining ?, ? and bias.
Clearly, the fact that ? > ? means that the first
words in the dependency tuples plays a more im-
portant role than the second ones for the task at
hand. However, the fact that ? < 0 is counter intu-
itive because it means that the lower the similarity
between the dependency type labels is, the larger
the similarity between the two dependencies. Up
to date we have been unable to find a plausible ex-
planation for this phenomenon. This set of 17 fea-
tures will be referred hereinafter as DEP.
3.1.5 Additional Features
In addition to the feature sets based in soft car-
dinality, we designed some features aimed to ad-
dress linguistic phenomena such as antonymy, hy-
pernymy and negation.
Antonymy: Consider the following text pair
from the test data {?A man is emptying a container
made of plastic?,?A man is filling a container
made of plastic? }, which is labeled as a contra-
diction with a relatedness score of 3.91. Clearly,
these labels are explained by the antonymy rela-
tion between ?emptying? and ?filling?. Given that
none of the features presented above address this
issue, a list of 11,028 pairs of antonym words was
gathered from several web sites (see Table 4) and
from the antonymy relationships in WordNet, in
order to detect these cases. That list was used to
count the number of occurrences of pairs antonym
words between pairs of texts and in each one of
the texts. Thus, for any pair of texts A and B (rep-
resented as sets of words), three features (referred
henceforth as ANT) were extracted:
antonym AB Counts the number of occurrences
of pairs of antonyms in A ? B (Cartesian
product) or in B ?A .
antonym AA Counts the number of occurrences
of pairs of antonyms in A?A.
antonym BB Counts the number of occurrences
of pairs of antonyms in B ?B.
Hypernymy: Consider the following text pair
from the test data {?A man is sitting comfortably
at a table?,?A person is sitting comfortably at the
table? }, which is labeled as an entailment with
a relatedness score of 3.96. In this case, the en-
tailment is based on the hypernymy between ?per-
son? and ?man?. In order to capture this linguis-
tic factor 3 features similar to the previously de-
scribed antonym features were proposed. First,
word sense disambiguation was performed (as de-
scribed in subsection 3.1.2) for obtaining a synset
label for each word. Secondly, we build a bi-
nary function hyp(ss
1
, ss
2
) that takes two Word-
Net synsets as arguments and returns 1 if ss
1
is
a hypernym of ss
2
with a maximum depth in the
WordNet?s is-a hierarchy of 6 steps, and 0 oth-
erwise. This hypernymy function was build us-
ing the WordNet interface provided by the NLTK.
Next, based on that synset-to-synset function, a
text-to-text function that captures the degree or hy-
pernymy in a text or in a pair of texts was build us-
ing the Monge-Elkan measure (Monge and Elkan,
1996). Thus, for two texts A and B represented
as sets of synset labels, the following expression
measures their degree of hypernymy:
HY P (A,B) =
1
|A|
|A|
?
i=1
|B|
max
j=1
hyp(a
i
, b
j
)
Using the function HY P (?, ?), 3 features are
extracted from each pair of text (referred hence-
forth as HYP):
hypernym AB from HY P (A,B)
736
http://www.myenglishpages.com/site php files/vocabulary-lesson-opposites-adjectives.php
http://www.allaboutspace.com/wordlist/opposites.shtml
http://www.michigan-proficiency-exams.com/antonym-list.html
http://examples.yourdictionary.com/examples-of-antonyms.html
http://www.synonyms-antonyms.com/antonyms.html
http://englishwilleasy.com/word-must-know/vocabulary/vocabulary-list-by-opposites-or-antonyms/
http://www.meridianschools.org/staff/districtcurriculum/moreresources/languagearts/all grades/antonyms.doc
http://mrsbrower.weebly.com/uploads/1/3/2/4/13243672/antonymlist.pdf
https://foxhugh.wordpress.com/word-lists/list-of-antonyms/
http://www.paulnoll.com/Books/Clear-English/English-antonyms-1.html
http://wordnet.princeton.edu/wordnet/download/
Table 4: URLs used for the list of 11,028 antonym pairs (accessed on March 20, 2014).
hypernym AA from HY P (A,A)
hypernym BB from HY P (B,B)
Negation: Negations play an important role in
the task at hand. For instance, consider this pair
of texts {?A person is rinsing a steak with wa-
ter?,?A man is not rinsing a large steak?} labeled
as a contradiction. In that example the negation of
the verb ?rising? is the main factor of contradic-
tion. In order to capture this linguistic feature we
build a simple function that detects the occurrence
of a verb negation if the text contains one of the
following words: ?not?, ?n?t?, ?nor?, ?null?, ?nei-
ther?, ?either?, ?barely?, ?scarcely? and ?hardly?.
Similarly, noun negation is detected looking for
the words: ?no?, ?none?, ?nobody?, ?nowhere?,
?nothing? and ?never?. Thus, for two texts A and
B, 4 features are extracted (referred henceforth as
NEG):
verb neg A if verb negation is detected in A
verb neg B if verb negation is detected in B
noun neg A if noun negation is detected in A
noun neg B if noun negation is detected in B
3.1.6 Submitted Runs and Results
RUN1 (PRIMARY) This system produced pre-
dictions by extracting all the features described
previously (SM, ESA, POS, DEP, ANT,
HYP and NEG) from all the texts in the SICK
data set. Next, two machine learning models were
obtained (WEKA (Hall et al., 2009) was used
for that) using the training part of SICK, one for
regression (relatedness) and another for classifi-
cation (entailment). The regression model was
a reduced-error pruning tree (REPtree) (Quin-
lan, 1987) boosted with 20 iterations of bagging
(Breiman, 1996). The classification model was a
J48Graft tree also boosted with 20 bagging itera-
tions. These two models produced the predictions
for the test part of SICK.
RUN2 This system is similar to the one used
in RUN1, but it used only the feature sets SM and
NEG. Another difference is that a linear regres-
sion was used instead of the REPtree and no bag-
ging was performed.
RUN3 The same as RUN1, but again, linear
regression was used instead of the REPtree and no
bagging was performed.
RUN4 The same as RUN2, but the models
were boosted with 20 iterations of bagging.
RUN5 The same as RUN3, but 30 iterations of
bagging were used instead of 20.
The official results obtained by these systems
(prefixed UNAL-NLP) are shown in Table 5
jointly with those obtained by other 3 top sys-
tems among the 18 participating systems. Our
primary run (RUN1) obtained pretty competitive
results ranking 3th and 4th in the entailment and
relatedness tasks. The RUN4 obtained a remark-
able performance (it would be ranked 6th for en-
tailment and 8th for relatedness) in spite of the
fact that is a system purely based on string match-
ing. The comparison of our runs 1, 3 and 5, which
mainly differs by the use of bagging, shows that
this boosting method provides considerable im-
provements. In fact, comparing RUN3 (all fea-
tures, no bagging) and RUN4 (SM and NEG fea-
ture sets boosted with bagging), they performed
similarly in spite of the considerable larger num-
ber of features used in RUN3. Besides, the RUN5
slightly outperformed our primary run (RUN1) us-
737
Entailment Relatedness
system accuracy official rank Pearson Spearman MSE official rank
UNAL-NLP run1 (primary) 83.05% 3rd/18 0.8043 0.7458 0.3593 4th/17
UNAL-NLP run2 79.81% - 0.7482 0.7033 0.4487 -
UNAL-NLP run3 80.15% - 0.7747 0.7286 0.4081 -
UNAL-NLP run4 80.21% - 0.7662 0.7142 0.4210 -
UNAL-NLP run5 83.24% - 0.8070 0.7489 0.3550 -
ECNU run1 83.64% 2nd/18 0.8280 0.7689 0.3250 1st/17
Stanford run5 74.49% 12th/18 0.8272 0.7559 0.3230 2nd/17
Illinois-LH run1 84.58% 1st/18 0.7993 0.7538 0.3692 5th/17
Table 5: Results for task 1.
ing 10 additional iterations of bagging.
3.1.7 Error Analysis
Our primary run for the task 1 failed in 835 pairs of
sentences out of 4,927 in the entailment subtask.
We wanted to understand in why our system failed
in these 835 instances, so we classified manually
these instances in 4 error categories (each instance
could be assigned to several categories).
Paraphrase not detected (NP): exam-
ple={?Two groups of people are playing football?,
?Two teams are competing in a football match?},
gold standard=entailment, prediction=neutral,
number of occurrences= 420 (50.3%). The system
failed to detect the paraphrase between ?groups of
people? and ?teams?.
Negation not detected (NN) : exam-
ple={?There is no one playing the guitar?,
?Someone is playing the guitar?}, gold stan-
dard=contradiction, prediction=neutral, number
of occurrences=94 (11.3%). The system failed to
detect that the contradiction is due to the negation
in the first text.
False similarity between words (NSS) : ex-
ample={?Two dogs are playing by a tree?,
?Two dogs are sleeping by a tree?}, gold stan-
dard=neutral, prediction=entailment, number of
occurrences=413 (49.5%). The only difference
between these 2 sentences is the gerund ?playing?
vs. ?sleeping?, which the system erroneously con-
sidered as similar.
Antonym not detected (NA): exam-
ple={?Three children are running down hill?,
?Three children are running up hill?}, gold
standard=contradiction, prediction=entailment,
number of occurrences=40 (4.8%). The only
difference between these 2 sentences is the
words ?down? vs. ?up?. In spite that this pair
of antonyms was included in the antonym list,
Error category NP NN NSS NA
NP 420 5 125 0
NN - 94 1 0
NSS - - 413 22
NA - - - 40
Table 6: Co-ocurrences of types of errors in RUN1
(task1).
the system failed to distinguish the contradiction
between the texts.
The matrix in Table 6 reports the number of
co-occurrences of error categories in the 835 in-
stances erroneously classified.
3.2 Task 3: Cross-level Semantic Similarity
The SemEval 2014 task 3 (cross-level semantic
similarity) (Jurgens et al., 2014) proposed the se-
mantic textual similarity task but across differ-
ent textual levels, namely paragraph-to-sentence,
sentence-to-phrase, phrase-to-word and word-to-
sense. As usual, the goal is to predict the gold sim-
ilarity scores for each pair of texts. For each one
of these cross-level comparison types there were
proposed a separated training and test data sets.
Basically, we addressed this task using the set of
features SM presented in subsection 3.1.1 in com-
bination with a text expansion approach similar to
the method presented in subsection 3.1.2.
3.2.1 Paragraph-to-sentence and
Sentence-to-phrase
For these two cross-level comparison types we
extracted the SM feature set using the pro-
vided texts. The model parameters obtained for
paragraph-to-sentence were ? = 0.1, ? = 1.75,
bias = ?1.35, p = 1.55; and for sentence-to-
phrase were ? = 0.68, ? = 0.92, bias = ?0.92,
p = 2.49.
738
The system for the RUN2 used the SM fea-
ture set and a machine learning model build with
the provided training data for generating the simi-
larity score predictions for the test data. For the
paragraph-to-sentence data set the model was a
REPtree for regression boosted with 40 bagging it-
erations. Similarly, the model for the sentence-to-
phrase data set was a linear regressor also boosted
with 40 bagging iterations.
Unlike RUN2, RUN1 does not make use of any
machine learning algorithm. Instead, we used the
only the basic cardinalities (see Table 1) from the
SM feature set in combination with an ad-hoc re-
semblance coefficient, i.e. the Dice?s coefficient
2|A?B|
/|A|+|B| for the paragraph-to-sentence data
set. In turn, for sentence-to-phrase the overlap co-
efficient, i.e.
|A?B|
/min[|A|,|B|], was used.
3.2.2 Phrase-to-word and Word-to-sense
Before applying the same procedure used in the
previous subsection, the texts in the phrase-to-
word and word-to-sense data sets were expanded
with a similar approach to that was used in subsec-
tion 3.1.2.
Phrase-to-word expansion: First, the ?word?
was expanded finding its corresponding WordNet
synset using the adapted Lesk?s algorithm provid-
ing as context the ?phrase?. Then, once the word?s
synset is obtained, the ?word? text is extended
with the textual definition of the synset. Simi-
larity, this procedure is repeated for each word in
the ?phase? obtaining and extended version of the
phrase. Finally, these two texts are used for ex-
tracting the SM feature set. The model param-
eters were ? = 0.8, ? = 1.9, bias = ?0.8,
p = 1.5.
Word-to-sense expansion: First, the ?sense?
(i.e. synset) is replaced by its textual definition
and its lemma. At this point the pair word-sense
becomes a pair word-sentence. Then, the synset
of the ?word? is obtained performing the adapted
Lesk?s algorithm. Next, the ?word? is extended
with textual definition of the synset. Finally, these
two texts are used for extracting the SM feature
set obtaining the following model parameters were
? = 0.59, ? = 0.9, bias = ?0.89, p = 3.91.
3.2.3 Results
The official results obtained by the two submitted
runs jointly with other 3 top systems are shown in
Table 7. Our submissions (prefixed with UNAL-
NLP) ranked 3rd and 5th among 38 participating
test data train data
OnWN (en) OnWN 2012/2013 test
headlines (en) headlines 2013 test
images (en) MSRvid 2012 train and test
deft-news (en) MSRpar 2013 train and test
deft-forum (en)
MSRvid 2012 train and test
OnWN 2012/2013 test
tweet-news (en)
SMTeuroparl 2012 test
SMTnews 2012 test
Wikipedia (es) SMTeuroparl 2012 train
news (es) SMTeuroparl 2012 train
Table 8: Training data used for the STS-2014 data
sets (task 10).
systems, showing that the SM (string-matching)
feature set is effective for the prediction of sim-
ilarity scores. Particularly, in the paragraph-to-
sentence data set, which has the longest text,
RUN2 obtained the best official score. In contrast,
the scores obtained for the phrase-to-word and
word-to-sense data sets were considerably lower
in comparison with the top system, but still com-
petitive against most of the other participating sys-
tems.
3.3 Task 10: Multilingual Semantic
Similarity
The SemEval-2014 task 10 (multilingual seman-
tic similarity) (Agirre et al., 2014) is the sequel of
the semantic textual similarity (STS) evaluations
at SemEval in the past two years (Agirre et al.,
2012; Agirre et al., 2013). This year 6 test data
sets were proposed in English and 2 data sets in
Spanish. Similarly to the 2013 campaign, there is
not explicit training data for each data set. Conse-
quently, different data sets from the previous STS
evaluations were selected to be used as training
data for the new data sets. The selection criterion
was the average character length and type of the
texts. The Table 8 shows the training data used for
each test data set.
3.3.1 English Subtask
The RUN1 for the English data sets was produced
with a parameterized similarity function based on
the SM feature set and the symmetrized Tversky?s
index (Tversky, 1977; Jimenez et al., 2013a). For
a detailed description of this function and its pa-
rameters, please refer to the STS
sim
feature in
the system description paper of the NTNU team
(Lynum et al., 2014). The parameters used in that
739
System Para-2-Sent Sent-2-Phr Phr-2-Word Word-2-Sense Official Rank
SimCompass run1 0.811 0.742 0.415 0.356 1st/38
ECNU run1 0.834 0.771 0.315 0.269 2nd/38
UNAL-NLP run2 0.837 0.738 0.274 0.256 3rd/38
SemantiKLUE run1 0.817 0.754 0.215 0.314 4th/38
UNAL-NLP run1 0.817 0.739 0.252 0.249 5th/38
Table 7: Official results for task 3 (Pearson?s correlation).
Data ? ? bias p ?
?
?
?
bias
?
OnWN 0.53 -0.53 1.01 1.00 -4.89 0.52 0.46
headlines 0.36 -0.29 4.17 0.85 -4.50 0.43 0.19
images 1.12 -1.11 0.93 0.64 -0.98 0.50 0.11
deft-news 3.36 -0.64 1.37 0.44 2.36 0.72 0.02
deft-forum 1.01 -1.01 0.24 0.93 -2.71 0.42 1.63
tweet-news 0.13 0.14 2.80 0.01 2.66 1.74 0.45
Table 9: Optimal parameters used for task 10 in
English.
function are reported in Table 9. Unlike subsec-
tion 3.1.1 where the Dice?s coefficient was used as
the text similarity function, here the symmetrical
Tversky?s index (eq. 2) was reused generating the
three additional parameters marked with apostro-
phe (?
?
, ?
?
and bias
?
).
For the RUN2 the SM feature set was extracted
from all the data sets in English (en) listed in Table
8. Then, a REPtree (Quinlan, 1987) boosted with
50 bagging iterations (Breiman, 1996) was trained
using the training data sets selected for each test
data set. Finally, these machine learning models
produced the similarity score predictions for each
test data set.
The RUN3 was identical to the RUN2 but in-
cluded additional feature sets apart from SM,
namely: ESA, POS and WN. The WN feature
set is the same as SM, but replacing the word-to-
word similarity function in eq. 2 by the path mea-
sure from the WordNet::Similarity package (Ped-
ersen et al., 2004).
3.3.2 Spanish Subtask
The Spanish system was based entirely in the SM
feature set with some small changes for adapt-
ing the system to Spanish. Basically, the list of
English stop-words was replaced by the Spanish
stop-words provided by the NLTK. In addition,
the Porter stemmer was replaced by its Spanish
equivalent, i.e. the Snowball stemmer for Span-
ish. The RUN1 is equivalent to the RUN1 for the
data set run1 run2 run3
deft-forum 0.5043 0.3826 0.4607
deft-news 0.7205 0.7305 0.7216
headlines 0.7616 0.7645 0.7605
images 0.8071 0.7706 0.7782
OnWN 0.7823 0.8268 0.8426
tweet-news 0.6145 0.4028 0.6583
mean (en) 0.7113 0.6573 0.7209
official rank (en) 12th/38 22th/38 9th/38
Wikipedia 0.7804 0.7566 0.6894
news 0.8154 0.7829 0.7965
mean (es) 0.8013 0.7723 0.7533
official rank (es) 3rd/22 9th/22 12th/22
Table 10: Official results for the task 10 (Pearson?s
correlation).
English subtask described in the previous subsec-
tion. The parameters used for the text similarity
function were ? = 1.16, ? = 1.08, bias = 0.02,
p = 1.02, ?
?
= 1.54, ?
?
= 0.08 and bias
?
= 1.37.
The description and meaning of these parameters
can be found in (Lynum et al., 2014) associated to
the STS
sim
feature.
The RUN2 was obtained using the SM feature
set and a linear regressor for generating the simi-
larity score predictions. Similarity, RUN3 used the
same feature set SM in combination with a REP-
tree boosted with 30 bagging iterations.
3.3.3 Results
The results for the 3 submitted runs correspond-
ing to the 2 sub tasks (English and Spanish) are
shown in Table 10. It is important to note that
the RUN1 for the Wikipedia data set in Spanish
was the top system among 22 participating sys-
tems. This result is remarkable given that this sys-
tem was trained with a data set in English showing
the domain adaptation ability of the soft cardinal-
ity approach.
740
4 Conclusions
We participated in the SemEval-2014 task 1, 3 and
10 with an uniform approach based on soft cardi-
nality features, obtaining pretty satisfactory results
in all data sets, tasks and sub tasks. This approach
has been used since SemEval-2012 in all versions
of the following tasks: semantic textual similar-
ity (Jimenez et al., 2012b; Jimenez et al., 2013a),
typed similarity (Croce et al., 2013), cross-lingual
textual entailment (Jimenez et al., 2012a; Jimenez
et al., 2013c), student response analysis (Jimenez
et al., 2013b), and multilingual semantic textual
similarity (Lynum et al., 2014). In the majority
of the cases, the systems based on soft cardinality,
built by us and other teams, have been among the
top systems. Given the uniformity of the approach,
the consistency of the results, the few computa-
tional resources required and the overall concep-
tual simplicity, the soft cardinality is established
as a useful tool for a wide spectrum of applications
in natural language processing.
References
Eneko Agirre, Daniel Cer, Mona Diab, and Gonzalez-
Agirre Aitor. 2012. SemEval-2012 task 6: A pilot
on semantic textual similarity. In Proceedings of the
6th International Workshop on Semantic Evaluation
(SemEval@*SEM 2012), Montreal,Canada.
Eneko Agirre, Daniel Cer, Mona Diab, Aitor Gonzalez-
Agirre, and Weiwei Guo. 2013. *SEM 2013 shared
task: Semantic textual similarity, including a pilot
on typed-similarity. Atlanta, Georgia, USA.
Eneko Agirre, Carmen Banea, Claire Cardie, Daniel
Cer, Mona Diab, Aitor Gonzalez-Agirre, Weiwei
Guo, Rada Mihalcea, German Rigau, and Janyce
Weibe. 2014. SemEval-2014 task 10: Multilingual
semantic textual similarity. In Proceedings of the
8th International Workshop on Semantic Evaluation
(SemEval-2014), Dublin, Ireland, August.
Satanjeev Banerjee and Ted Pedersen. 2002. An
adapted lesk algorithm for word sense disambigua-
tion using WordNet. In Computational linguis-
tics and intelligent text processing, page 136?145.
Springer.
Leo Breiman. 1996. Bagging predictors. Machine
Learning, 24(2):123?140.
Courtney Corley and Rada Mihalcea. 2005. Measur-
ing the semantic similarity of texts. In Proceedings
of the ACL Workshop on Empirical Modeling of Se-
mantic Equivalence and Entailment, EMSEE ?05,
page 13?18, Stroudsburg, PA, USA.
Danilo Croce, Valerio Storch, P. Annesi, and Roberto
Basili. 2012. Distributional compositional seman-
tics and text similarity. In 2012 IEEE Sixth Interna-
tional Conference on Semantic Computing (ICSC),
pages 242?249, September.
Danilo Croce, Valerio Storch, and Roberto Basili.
2013. UNITOR-CORE TYPED: Combining text
similarity and semantic filters through SV regres-
sion. In Second Joint Conference on Lexical and
Computational Semantics (*SEM), Volume 1: Pro-
ceedings of the Main Conference and the Shared
Task: SemanticTextual Similarity, page 59, Atlanta,
Georgia, USA.
Marie-Catherine De Marneffe, Bill MacCartney, and
Christopher D. Manning. 2006. Generating typed
dependency parses from phrase structure parses. In
Proceedings of LREC, volume 6, page 449?454,
Genoa, Italy, May.
Evgeniy Gabrilovich and Shaul Markovitch. 2007.
Computing semantic relatedness using wikipedia-
based explicit semantic analysis. In Proceedings of
the 20th International Joint Conference on Artifical
Intelligence, IJCAI?07, page 1606?1611, San Fran-
cisco, CA, USA. Morgan Kaufmann Publishers Inc.
Mark Hall, Frank Eibe, Geoffrey Holmes, and Bern-
hard Pfahringer. 2009. The WEKA data min-
ing software: An update. SIGKDD Explorations,
11(1):10?18.
Paul Jaccard. 1901. Etude comparative de la distribu-
tion florare dans une portion des alpes et des jura.
Bulletin de la Soci?et?e Vaudoise des Sciences Na-
turelles, pages 547?579.
Sergio Jimenez, Fabio Gonzalez, and Alexander Gel-
bukh. 2010. Text comparison using soft cardi-
nality. In Edgar Chavez and Stefano Lonardi, ed-
itors, String Processing and Information Retrieval,
volume 6393 of LNCS, pages 297?302. Springer,
Berlin, Heidelberg.
Sergio Jimenez, Claudia Becerra, and Alexander Gel-
bukh. 2012a. Soft cardinality: A parameterized
similarity function for text comparison. In SemEval
2012, Montreal, Canada.
Sergio Jimenez, Claudia Becerra, and Alexander Gel-
bukh. 2012b. Soft cardinality+ ML: Learning adap-
tive similarity functions for cross-lingual textual en-
tailment. In SemEval 2012, Montreal, Canada.
Sergio Jimenez, Claudia Becerra, and Alexander Gel-
bukh. 2013a. SOFTCARDINALITY-CORE: Im-
proving text overlap with distributional measures for
semantic textual similarity. In *SEM 2013, Atlanta,
Georgia, USA, June.
Sergio Jimenez, Claudia Becerra, and Alexander Gel-
bukh. 2013b. SOFTCARDINALITY: Hierarchical
text overlap for student response analysis. In Se-
mEval 2013, Atlanta, Georgia, USA, June.
741
Sergio Jimenez, Claudia Becerra, and Alexander Gel-
bukh. 2013c. SOFTCARDINALITY: Learning
to identify directional cross-lingual entailment from
cardinalities and SMT. In SemEval 2013, Atlanta,
Georgia, USA, June.
Karen Sp?arck Jones. 2004. A statistical interpretation
of term specificity and its application in retrieval.
Journal of Documentation, 60(5):493?502, October.
David Jurgens, Mohammad T. Pilehvar, and Roberto
Navigli. 2014. SemEval-2014 task 3: Cross-
level semantic similarity. In Proceedings of the
8th International Workshop on Semantic Evaluation
(SemEval-2014), Dublin, Ireland, August.
Andr?e Lynum, Partha Pakray, Bj?orn Gamb?ack, and
Sergio Jimenez. 2014. NTNU: Measuring se-
mantic similarity with sublexical feature represen-
tations and soft cardinalty. In Proceedings of the
8th International Workshop on Semantic Evaluation
(SemEval-2014), Dublin, Ireland, August.
Marco Marelli, Stefano Menini, Marco Baroni, Lucia
Bentivogli, Raffaella Bernardi, and Roberto Zam-
parelli. 2014. A SICK cure for the evaluation of
compositional distributional semantic models. In
Proceedings of LREC, Reykjavik, Iceland, May.
Alvaro E. Monge and Charles Elkan. 1996. The field
matching problem: Algorithms and applications. In
Proceeding of the 2nd International Conference on
Knowledge Discovery and Data Mining (KDD-96),
pages 267?270, Portland, OR.
Ted Pedersen, Siddharth Patwardhan, and Jason Miche-
lizzi. 2004. WordNet::similarity: measuring the
relatedness of concepts. In Proceedings HLT-
NAACL?Demonstration Papers, Stroudsburg, PA,
USA.
Martin Porter. 1980. An algorithm for suffix stripping.
Program, 3(14):130?137, October.
J. Ross Quinlan. 1987. Simplifying decision
trees. International journal of man-machine studies,
27(3):221?234.
Amos Tversky. 1977. Features of similarity. Psycho-
logical Review, 84(4):327?352, July.
742
Proceedings of the Second Workshop on Natural Language Processing for Social Media (SocialNLP), pages 28?37,
Dublin, Ireland, August 24 2014.
A Rule-Based Approach to Aspect Extraction from Product Reviews
Soujanya Poria
Dept of Computing Science & Maths
University of Stirling
soujanya.poria@cs.stir.ac.uk
Erik Cambria
School of Computer Engineering
Nanyang Technological University
cambria@ntu.edu.sg
Lun-Wei Ku
Institute of Information Science
Academia Sinica
lwku@iis.sinica.edu.tw
Chen Gui
SenticNet
chen@sentic.net
Alexander Gelbukh
Center for Computing Research
National Polytechnic Institute
gelbukh@cic.ipn.mx
Abstract
Sentiment analysis is a rapidly growing research field that has attracted both academia and in-
dustry because of the challenging research problems it poses and the potential benefits it can
provide in many real life applications. Aspect-based opinion mining, in particular, is one of the
fundamental challenges within this research field. In this work, we aim to solve the problem of
aspect extraction from product reviews by proposing a novel rule-based approach that exploits
common-sense knowledge and sentence dependency trees to detect both explicit and implicit as-
pects. Two popular review datasets were used for evaluating the system against state-of-the-art
aspect extraction techniques, obtaining higher detection accuracy for both datasets.
1 Introduction
In opinion mining, different levels of granularity analysis have been proposed, each one having its own
advantages and disadvantages. Aspect-based opinion mining (Hu and Liu, 2004; Ding et al., 2008)
focuses on the extraction of aspects (or product features) from opinionated text and on the inference of
polarity values associated with these. For example, a sentence like ?I love the touchscreen of my phone
but the battery life is so short? contains two aspects or opinion targets, namely touchscreen and battery
life. In this case, applying a sentence level polarity detection technique would mistakenly result in a
polarity value close to neutral, since the two opinions expressed by the users are opposite. Hence, aspect
extraction is necessary to first deconstruct sentences into product features and then assign a separate
polarity value to each of these features.
There are two types of aspects defined in aspect-based opinion mining: explicit and implicit. Explicit
aspects are concepts that explicitly denote targets in the opinionated sentence. For instance, in the above
example, touchscreen and battery life are explicit aspects as they are explicitly mentioned in the sentence.
On the other hand, an aspect can also be expressed indirectly through an implicit aspect clue (IAC), e.g.,
in the sentence ?This camera is sleek and very affordable?, which implicitly provides a positive opinion
about the aspects appearance and price of the entity camera.
Explicit aspect extraction has been widely researched and there exists several approaches for this
task. Still, limited work has been done in extracting implicit aspects. This task is very difficult yet very
important because the phenomenon of implicit aspects is present in nearly every opinionated document.
For example, the following document extracted from the corpus (Hu and Liu, 2004) uses only implicit
aspects:
This work is licensed under a Creative Commons Attribution 4.0 International Licence. Page numbers and proceedings
footer are added by the organisers. Licence details: http://creativecommons.org/licenses/by/4.0
28
This is the best phone one could have. It has all the features one would need in a cellphone: It
is lightweight, sleek and attractive. I found it very user-friendly and easy to manipulate; very
convenient to scroll in menu etc.
Here, the word ?lightweight? refers to the weight of the phone; the words ?sleek? and ?attractive? to
its appearance; the compound ?user-friendly? to its interface; the phrase ?easy to manipulate? to its
functionality; finally, the phrase ?to scroll in menu? can be interpreted as a reference to the interface
of the phone or its menu. Even though the aspects appearance, weight and interface do not appear
in the sentence, the context contains clues that permit us to infer them. Namely, the words ?sleek,?
?lightweight,? and ?user-friendly? that do occur in the context suggest these aspects.
In contrast to the task of identification of explicit aspects, the general scheme for identification of
implicit aspects, a task called implicit aspect extraction, typically involves two steps:
1. Identify IACs (e.g., ?sleek?) in the opinionated document.
2. Map them to the corresponding aspects (e.g., appearance).
In this paper, we propose a novel approach to detect explicit aspects and IACs from opinionated
documents. We also map IACs to their respective aspect categories. IACs are either single words,
such as ?sleek,? or multi-word expressions, such as ?easy to manipulate? as in the above example. Each
IAC can be represented by a different part-of-speech (POS): in the example ?This MP3 player is really
expensive,? the IAC ?expensive? suggesting the aspect price is an adjective; in ?This camera looks
great,? the IAC ?look? suggesting appearance is a verb; in ?I hate this phone. It only lasted less than six
months!?, the IAC ?lasted? suggesting durability of the phone is a verb. In the following examples, IACs
are nouns or noun phrases: ?Even if I had paid full price I would have considered this phone a good deal,?
?Not to mention the sleekness of this phone?, ?The player keeps giving random errors?, ?This phone is a
piece of crap.?
In different contexts, the same implicit aspect can be implied by different IACs, as shown below for
the implicit aspect price:
? This mp3 player is very affordable.
? This mp3 player also costs a lot less than the ipod.
? This mp3 player is quite cheap.
? This mp3 is inexpensive.
? I bought this mp3 for almost nothing!
? This mp3 player has been fairly innovative and reasonably priced.
A common approach for IAC identification is to assume that sentiments or polarity words are good
candidates for IACs: for example, in ?This MP3 player is really expensive,? the word ?expensive?,
which bears negative polarity, is also the IAC for the aspect price. However, this is not always true.
For example, in ?This camera looks great,? the word ?looks? implies the appearance of the phone,
while polarity is conveyed through the word ?great.? In ?I hate this phone. It only lasted less than six
months!?, the word ?lasted? is the IAC for durability of the phone, while polarity is indicated by ?hate.?
Furthermore, the second sentence of this example could appear without the first one: ?This phone only
lasted less than six months? and still constitute a negative opinion of the phone?s durability, but not
expressed by any specific word.
This phenomenon is known in opinion mining as desirable fact: communicating fact that by common-
sense are good or bad, which indirectly implies polarity. For example, the objective fact ?The camera can
hold lots of pictures? does not contain any sentiment or polarity word yet gives a positive opinion about
the camera?s memory capacity (IAC ?hold?), because it is desirable for a camera to hold many pictures.
29
In this paper, we present a rule-based approach that exploits common-sense knowledge and sentence
dependency trees to detect both implicit and explicit aspects. In particular, the approach draws lessons
from recent developments in common-sense reasoning (Cambria et al., 2011; Cambria et al., 2014a)
and concept-level sentiment analysis (Xia et al., 2013; Poria et al., 2014) to first obtain the dependency
structure of each sentence and, hence, exploit external knowledge to extract aspects and infer the polarity
associated with them. The paper is organized as follows: Section 2 presents the literature in aspect ex-
traction; Section 3 explains the features used for the labeler; Section 4 discusses novelty of the proposed
methodology; Section 5 describes in detail the aspect extraction approach and results of the experimental
evaluation; finally, Section 6 concludes the paper.
2 Related Work
Aspect extraction from opinionated text was first studied by Hu and Liu (Hu and Liu, 2004), who also
introduced the distinction between explicit and implicit aspects. However, the authors only dealt with
explicit aspects by adopting a set of rules based on statistical observations. Hu and Liu?s method was im-
proved by Popescu and Etzioni (Popescu and Etzioni, 2005) and by Blair-Goldensonh (Blair-Goldensohn
et al., 2008). Popescu and Etzioni assumed the product class to be known as priori. Their algorithm
detects whether a noun or noun phrase is a product feature or not by computing PMI between the noun
phrase and the product class. Scaffidi et al. (Scaffidi et al., 2007) presented a method that uses a language
model to identify product features. They assumed that product features are more frequent in product re-
views than in general natural language text. However, their method seems to be very inaccurate in terms
of precision as the retrieved aspects extracted by their method were very noisy.
Aspect extraction can be seen as a general information extraction problem, for which techniques based
on sequential labeling are generally used. The most popular methods in this context, in particular, are
Hidden Markov Models (HMM) and Conditional Random Fields (CRF) (Lafferty et al., 2001). Jin and
Ho (Jin and Ho, 2009) used a lexicalized HMM for joint extraction of opinions along with their explicit
aspects. Niklas and Gurevych (Niklas and Gurevych, 2010) used CRF to extract explicit aspects in a
custom corpus with data of different domains. Li et al. (Li et al., 2010), Choi and Cardie (Choi and
Cardie, 2010) and Huang et al. (Huang et al., 2012) also used CRF for extraction of explicit aspects.
As to the implicit aspects, the OPINE extraction system developed by Popescu and Etzioni (Popescu
and Etzioni, 2005) was the first that leveraged on the extraction of this type of aspects to improve polarity
classification. However, their system is not described in detail and is not publicly available. To the
best of our knowledge, all existing methods for implicit aspect extraction are based on the use, in one
or another way, of what we term IAC. Su (Su et al., 2008) proposed a clustering method to map IACs
(which were assumed to be sentiment words) to their corresponding explicit aspects. The method exploits
the mutual reinforcement relationship between an explicit aspect and a sentiment word forming a co-
occurring pair in a sentence. Hai (Zhen et al., 2011) proposed a two-phase co-occurrence association
rule mining approach to match implicit aspects (which were also assumed to be sentiment words) with
explicit aspects. Finally, Zeng and Li (Zeng and Li, 2013) proposed a rule-based method to extract
explicit aspects and mapped implicit features by using a set of sentiment words and by clustering explicit
feature-word pairs.
3 Method
3.1 Corpus for aspect extraction
In order to evaluate the explicit aspect extraction algorithm, we use the corpus provided by (Hu and
Liu, 2004) and the Semeval 2014 dataset
1
(Table 1). As for the implicit aspect extraction algorithm and
lexicon, we use the corpus developed by Cruz-Garcia et al. (Cruz-Garcia et al., 2014), who manually
labeled each IAC and their corresponding aspects in a well-known corpus for opinion mining (Hu and
Liu, 2004). The corpus is publicly available for research purposes.
2
1
http://alt.qcri.org/semeval2014/task4/index.php?id=data-and-tools
2
Available from www.gelbukh.com/resources/implicit-aspect-extraction-corpus, visited on
March 19, 2014.
30
Table 1: Description of Semeval 2014 dataset
Sentences Containing n aspect terms
Domain Name n = 0 n ? 1 n ? 2 total(n ? 0)
Restaurants 1,732 2,212 881 3,944
Laptops 1,883 2,065 456 3,948
3.2 Pre-Processing
Pre-processing is a key step for aspect parsing. The pre-processing module of the proposed framework
consists of two major steps: firstly, the sentence dependency tree is obtained through Stanford Depen-
dency Parser
3
; secondly, dependency structure elements are processed by means of Stanford Lemmatizer
for each sentence. It is important to build the dependency tree before lemmatization as swapping the two
steps results in several imprecisions caused by the lower grammatical accuracy of lemmatized sentences.
3.3 Aspect Parser
3.3.1 Implicit aspect lexicon
We use the implicit aspect corpus developed by Cruz-Garcia et al. (Cruz-Garcia et al., 2014), where
IACs are indicated and manually labeled by their corresponding aspect categories. For our task, we
extracted the sentences having implicit aspects and then extracted IACs for each of them, along with
their corresponding labeled categories. For example, in ?The car is expensive? the IAC is expensive and
it is labeled by the category price. Below is the list of the aspect categories extracted from the corpus:
? functionality
? weight
? price
? appearance
? behavior
? performance
? quality
? service
? size
For each IAC under every aspect category, synonyms and antonyms were obtained from WordNet (Fell-
baum, 1998) and stored under the same aspect category. For example, expensive and its antonym inex-
pensive both have the same category price. Semantics extracted from SenticNet (Cambria et al., 2014b)
have also been exploited to enlarge the set of conceptually related IACs. Thus, a lexicon of 1,128 IACs
categorized into the above categories was built.
3.3.2 Opinion Lexicon
We use SenticNet 3 as a concept-level opinion lexicon. The common-sense knowledge base contains
30,000 multi-word expressions labeled by their polarity scores. The proposed aspect parser is based on
two general rules:
? Rules for the sentences having subject verb.
? Rules for the sentences which do not have subject verb.
3
http://nlp.stanford.edu:8080/parser
31
A dependency relation is a binary relation characterized by the following features:
? The type of the relation that specifies the nature of the (syntactic) link between the two elements in
the relation.
? The head of the relation: this is the element that is the pivot of the relation. Core syntactic and
semantics properties (e.g., agreement) are inherited from the head.
? The dependent is the element that depends on the head and which usually inherits some of its
characteristics (e.g., number, gender in the case of agreement).
Most of the times, the active token is considered in a relation if it acts as the head of the relation, although
there are exceptions. Once the active token has been identified as the trigger for a rule, there are several
ways to compute its contribution, depending on how the dependency relation and the properties of the
tokens match with the rules. The preferred way is not to consider the contribution of the token alone,
but in combination with the other elements in the dependency relation. First of all, Stanford parser is
used to obtain the dependency parse structure of each sentence. Then, hand-crafted dependency rules are
employed on the parse trees to extract aspects.
3.3.3 Subject Noun Rule
Trigger: when the active token is found to be the syntactic subject of a token. Behavior: if an active
token h is in a subject noun relationship with a word t then:
1. if t has any adverbial or adjective modifier and the modifier exists in SenticNet, then t is extracted
as an aspect.
2. if the sentence does not have auxiliary verb, i.e., is, was, would, should, could, then:
? if the verb t is modified by an adjective or an adverb or it is in adverbial clause modifier relation
with another token, then both h and t are extracted as aspects. In (1), battery is in a subject
relation with lasts and lasts is modified by the adjective modifier little, hence both the aspects
last and battery are extracted.
(1) The battery lasts little.
? if t has any direct object relation with a token n and the POS of the token is Noun and n is not
in SenticNet, then n is extracted as an aspect. In (2), like is in direct object relation with lens
so the aspect lens is extracted.
(2) I like the lens of this camera.
? if t has any direct object relation with a token n and the POS of the token n is Noun and n exists
in SenticNet, then the token n extracted as aspect term. In the dependency parse tree of the
sentence, if another token n
1
is connected to n using any dependency relation and the POS of
n
1
is Noun, then n
1
is extracted as an aspect. In (3), like is in direct object relation with beauty
which is connected to screen via a preposition relation. So the aspects screen and beauty are
extracted.
(3) I like the beauty of the screen.
? if t is in open clausal complement relation with a token t
1
, then the aspect t-t
1
is extracted if t-t
1
exists in the opinion lexicon. If t
1
is connected with a token t
2
whose POS is Noun, then t
2
is
extracted as an aspect. In (4), like and comment is in clausal complement relation and comment
is connected to camera using a preposition relation. Here, the POS of camera is Noun and,
hence, camera is extracted as an aspect.
(4) I would like to comment on the camera of this phone.
32
3. A copula is the relation between the complement of a copular verb and the copular verb. If the
token t is in copula relation with a copular verb and the copular verb exists in the implicit aspect
lexicon, then t is extract as aspect term. In (5), expensive is extracted as an aspect.
(5) The car is expensive.
4. If the token t is in copula relation with a copular verb and the POS of h is Noun, then h is extracted
as an explicit aspect. In (6), camera is extracted as an aspect.
(6) The camera is nice.
5. If the token t is in copula relation with a copular verb and the copular verb is connected to a token
t
1
using any dependency relation and t
1
is a verb, then both t
1
and t are extracted as implicit aspect
terms, as long as they exist in the implicit aspect lexicon. In (7), lightweight is in copula relation
with is and lightweight is connected to the word carry by open clausal complement relation. Here,
both lightweight and carry are extracted as aspects.
(7) The phone is very lightweight to carry.
3.3.4 Sentences which do not have subject noun relation in their parse tree
For sentences that do not have noun subject relation in their parse trees, aspects are extracted using the
following rules:
1. if an adjective or adverb h is in infinitival or open clausal complement relation with a token t and h
exists in the implicit aspect lexicon, then h is extracted as an aspect. In (8), big is extracted as an
aspect as it is connected to hold using a clausal complement relation.
(8) Very big to hold.
2. if a token h is connected to a noun t using a prepositional relation, then both h and t are extracted as
aspects. In (9) sleekness is extracted as an aspect.
(9) Love the sleekness of the player.
3. if a token h is in a direct object relation with a token t, t is extracted as aspect. In (10), mention is in
a direct object relation with price, hence price is extracted as an aspect.
(10) Not to mention the price of the phone.
3.3.5 Additional Rules
? For each aspect term extracted above, if an aspect term h is in co-ordination or conjunct relation
with another token t, then t is also extracted as an aspect. In (11), amazing is firstly extracted as an
aspect term. As amazing is in conjunct relation with easy, then use is also extracted as an aspect.
(11) The camera is amazing and easy to use.
? A noun compound modifier of an NP is any noun that serves to modify the head noun. If t is
extracted as an aspect and t has noun compound modifier h, then the aspect h-t is extracted and t
is removed from the aspect list. In (12), as chicken and casserole are in noun compound modifier
relation, only chicken casserole is extracted as an aspect.
(12) We ordered the chicken casserole, but what we got were a few small pieces of chicken, all
dark meat and on the bone.
33
4 Novelty of the proposed work
First of all, the proposed method is fully unsupervised and depends on the accuracy of the dependency
parser and the opinion lexicon, rather then a training corpus and supervised learning accuracy. Only
(Qiu et al., 2011) follow an unsupervised learning approach but the proposed method uses an enhanced
set of rules and opinion lexicon. The proposed method also outperforms (Qiu et al., 2011) on the same
dataset they used. Implicit aspects extracted through the proposed method differ from implicit aspect
expressions defined by Liu (Liu, 2012) as ?aspect expressions that are not nouns or noun phrases? in that
implicit aspects extracted by the proposed algorithm semantically refer to the values of the pre-defined
aspects, irrespective of their own surface POS. Below are listed some examples where the implicit aspect
terms are either noun or noun phrases.
In (13), the IAC deal is extracted.
(13) Even if I had paid full price I would have considered this phone a good deal.
In (14), sleekness is extracted as an IAC.
(14) Not to mention the sleekness of this phone.
In (15), the IAC errors is extracted by the algorithm.
(15) The player keeps giving random errors.
In (16), piece of crap is a noun phrase and is extracted as an IAC by the proposed algorithm.
(16) This phone is a piece of crap.
A demo of the developed aspect parser is freely available at http://sentic.net/demo.
Table 2: Results on the DVD-player review dataset provided by (Hu and Liu, 2004)
Algorithm Precision Recall
Hu and Liu 75.00% 82.00%
Popescu and Etzioni 89.00% 80.00%
Dependency propagation method 87.00% 81.00%
Proposed approach 89.25% 91.25%
Table 3: Results on the Canon G3 review dataset provided by (Hu and Liu, 2004)
Algorithm Precision Recall
Hu and Liu 71.00% 79.00%
Popescu and Etzioni 87.00% 74.00%
Dependency propagation method 90.00% 81.00%
Proposed approach 90.15% 92.25%
Table 4: Results on the Jukebox review dataset provided by (Hu and Liu, 2004)
Algorithm Precision Recall
Hu and Liu 72.00% 76.00%
Popescu and Etzioni 89.00% 74.00%
Dependency propagation method 90.00% 86.00%
Proposed approach 92.25% 94.15%
34
Table 5: Results on the Nikon Coolpix review dataset provided by (Hu and Liu, 2004)
Algorithm Precision Recall
Hu and Liu 69.00% 82.00%
Popescu and Etzioni 86.00% 80.00%
Dependency propagation method 81.00% 84.00%
Proposed approach 82.15% 86.15%
Table 6: Results on the Nokia-6610 review dataset provided by (Hu and Liu, 2004)
Algorithm Precision Recall
Hu and Liu 74.00% 80.00%
Popescu and Etzioni 90.00% 78.00%
Dependency propagation method 92.00% 86.00%
Proposed approach 93.25% 93.32%
5 Experiments and Results
5.1 Experiment on the dataset provided by (Hu and Liu, 2004)
Experimental evaluation was carried out on the dataset derived from (Hu and Liu, 2004). As discussed
in Section 3, the proposed method is able to extract both explicit and implicit aspects. To the best of our
knowledge, there is no state-of-the-art benchmark to evaluate implicit aspect extraction.
We compare the proposed framework with those in Hu and Liu (Hu and Liu, 2004), Qiu et al. (Qiu et
al., 2011), and Popescu and Etzioni (Popescu and Etzioni, 2005) (which only carried out explicit aspect
extraction). Table 2, Table 3, Table 4, Table 5 and Table 6 show that the proposed framework outperforms
all existing methods in terms of both precision and recall.
6 Conclusion
We have illustrated a method for extracting both explicit and implicit aspects from opinionated text.
The proposed framework only leverages on common-sense knowledge and on the dependency structure
of sentences and, hence, is unsupervised. As future work, we aim to discover more rules for aspect
extraction. Another key future effort is to combine existing rules for complex aspect extraction. To
obtain the aspect categories of IACs, we have developed an aspect knowledge base using WordNet and
SenticNet. We will focus on extending the scalability of such knowledge base and on making it as much
noise-free as possible.
6.1 Experiment on Semeval 2014 dataset
We also carried out experiments on Semeval 2014 aspect based sentiment analysis data obtained from
http://alt.qcri.org/semeval2014/task4/index.php?id=data-and-tools. Re-
sults are shown in Table 7. We cannot perform a comparative evaluation of such experimental results as
there is no state-of-art approach yet which used this dataset for the same kind of experiment. Overall,
results show high accuracy.
Table 7: Results on the Semeval 2014 dataset
Domain Precision Recall
Laptop 82.15% 84.32%
Restaurants 85.21% 88.15%
35
References
Sasha Blair-Goldensohn, Kerry Hannan, Ryan McDonald, Tyler Neylon, George A. Reis, and Jeff Reynar. 2008.
Building a sentiment summarizer for local service reviews. In Proceedings of WWW-2008 workshop on NLP in
the Information Explosion Era, page 14.
Erik Cambria, Thomas Mazzocco, Amir Hussain, and Chris Eckl. 2011. Sentic medoids: Organizing affective
common sense knowledge in a multi-dimensional vector space. In D Liu, H Zhang, M Polycarpou, C Alippi,
and H He, editors, Advances in Neural Networks, volume 6677 of Lecture Notes in Computer Science, pages
601?610, Berlin. Springer-Verlag.
Erik Cambria, Paolo Gastaldo, Federica Bisio, and Rodolfo Zunino. 2014a. An ELM-based model for affective
analogical reasoning. Neurocomputing.
Erik Cambria, Daniel Olsher, and Dheeraj Rajagopal. 2014b. SenticNet 3: A common and common-sense knowl-
edge base for cognition-driven sentiment analysis. AAAI, pages 1515?1521.
Yejin Choi and Claire Cardie. 2010. Hierarchical sequential learning for extracting opinions and their attributes. In
Proceedings of Annual Meeting of the Association for Computational Linguistics (ACL-2010), pages 268?274.
Ivan Cruz-Garcia, Alexander Gelbukh, and Grigori Sidorov. 2014. Implicit aspect indicator extraction for aspect-
based opinion mining. submitted.
Xiaowen Ding, Bing Liu, and Philip S. Yu. 2008. A holistic lexicon-based approach to opinion mining. In
Proceedings of First ACM International Conference on Web Search and Data Mining (WSDM-2008), pages
231?240, Stanford University, Stanford, California, USA, Feb.
Christiane Fellbaum. 1998. WordNet: An Electronic Lexical Database (Language, Speech, and Communication).
The MIT Press.
Minqing Hu and Bing Liu. 2004. Mining and summarizing customer reviews. In Proceedings of the ACM
SIGKDD International Conference on Knowledge Discovery & Data Mining, pages 168?177, Aug.
Sheng Huang, Xinlan Liu, Xueping Peng, and Zhendong Niu. 2012. Fine-grained product features extraction and
categorization in reviews opinion mining. In Proceedings of the IEEE 12th International Conference on Data
Mining Workshops, pages 680?686.
Wei Jin and Hung Hay Ho. 2009. A novel lexicalized HMM-based learning framework for web opinion mining.
In Proceedings of International Conference on Machine Learning (ICML-2009), pages 465?472.
John Lafferty, Andrew McCallum, and Fernando C.N. Pereira. 2001. Conditional random fields: probabilistic
models for segmenting and labeling sequence data. In Proceedings of the 18th International Conference on
Machine Learning, pages 282?289. Morgan Kaufmann Publishers.
Fangtao Li, Chao Han, Minlie Huang, Xiaoyan Zhu, Ying-Ju Xia, Shu Zhang, and Hao Yu. 2010. Structure-aware
review mining and summarization. In Proceedings of the 23rd International Conference on Computational
Linguistics (COLING-2010), pages 653?661.
Bing Liu. 2012. Sentiment Analysis and Opinion Mining. Morgan & Claypool Publishers.
Jakob Niklas and Iryna Gurevych. 2010. Extracting opinion targets in a single and cross-domain setting with con-
ditional random fields. In Proceedings of Conference on Empirical Methods in Natural Language Processing
(EMNLP-2010), pages 1035?1045.
Ana-Maria Popescu and Oren Etzioni. 2005. Extracting product features and opinions from reviews. In Proceed-
ings of Conference on Empirical Methods in Natural Language Processing (EMNLP-2005), pages 3?28.
Soujanya Poria, Erik Cambria, Gregoire Winterstein, and Guang-Bin Huang. 2014. Sentic patterns: Dependency-
based rules for concept-level sentiment analysis. Knowledge-Based Systems.
Guang Qiu, Bing Liu, Jiajun Bu, and Chun Chen. 2011. Opinion word expansion and target extraction through
double propagation. Computational linguistics, 37(1):9?27.
Christopher Scaffidi, Kevin Bierhoff, Eric Chang, Mikhael Felker, Herman Ng, and Chun Jin. 2007. Red opal:
product-feature scoring from reviews. In Proceedings of the 8th ACM conference on Electronic commerce,
pages 182?191. ACM.
36
Qi Su, Xinying Xu, Honglei Guo, Zhili Guo, Xian Wu, Xiaoxun Zhang, Bin Swen, and Zhong Su. 2008. Hidden
sentiment association in chinese web opinion mining. In Proceedings of International Conference on World
Wide Web (WWW-2008), pages 959?968.
Rui Xia, Chengqing Zong, Xuelei Hu, and Erik Cambria. 2013. Feature ensemble plus sample selection: A
comprehensive approach to domain adaptation for sentiment classification. IEEE Intelligent Systems, 28(3):10?
18.
Lingwei Zeng and Fang Li. 2013. A classification-based approach for implicit feature identification. In Chinese
Computational Linguistics and Natural Language Processing Based on Naturally Annotated Big Data. 12th
China National Conference, CCL 2013 and First International Symposium, NLP-NABD 2013, Suzhou, China,
October 10?12, 2013, Proceedings, volume 8202 of Lecture Notes in Computer Science, pages 190?202.
Hai Zhen, Kuiyu Chang, and Jung-jae Kim. 2011. Implicit feature identification via co-occurrence association rule
mining. In Computational Linguistics and Intelligent Text Processing. 12th International Conference, CICLing
2011, Tokyo, Japan, February 20?26, 2011. Proceedings, Part I, volume 6608 of Lecture Notes in Computer
Science, pages 393?404.
37
