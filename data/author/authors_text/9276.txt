Proceedings of the 22nd International Conference on Computational Linguistics (Coling 2008), pages 1121?1128
Manchester, August 2008
Diagnostic Evaluation of Machine Translation Systems Using Auto-
matically Constructed Linguistic Check-Points 
Ming Zhou1, Bo Wang2, Shujie Liu2, Mu Li1, Dongdong Zhang1, Tiejun Zhao2 
1Microsoft Research Asia 
Beijing, China 
{mingzhou,muli,dozhang} 
@microsoft.com 
 
2Harbin Institute of Technology 
Harbin, China 
{bowang,Shujieliu,tjzhao} 
@mtlab.hit.edu.cn 
 
 
?Abstract 
We present a diagnostic evaluation plat-
form which provides multi-factored eval-
uation based on automatically con-
structed check-points. A check-point is a 
linguistically motivated unit (e.g. an am-
biguous word, a noun phrase, a verb~obj 
collocation, a prepositional phrase etc.), 
which are pre-defined in a linguistic tax-
onomy. We present a method that auto-
matically extracts check-points from pa-
rallel sentences. By means of check-
points, our method can monitor a MT 
system in translating important linguistic 
phenomena to provide diagnostic evalua-
tion. The effectiveness of our approach 
for diagnostic evaluation is verified 
through experiments on various types of 
MT systems. 
1 Introduction 
Automatic MT evaluation is a crucial issue for 
MT system developers. The state-of-the-art me-
thods for automatic MT evaluation are using an 
n-gram based metric represented by BLEU (Pa-
pineni et al, 2002) and its variants. Ever since its 
invention, the BLEU score has been a widely 
accepted benchmark for MT system evaluation. 
Nevertheless, the research community has been 
aware of the deficiencies of the BLEU metric 
(Callison-Burch et al, 2006). For instance, 
BLEU fails to sufficiently capture the vitality of 
natural languages: all grams of a sentence are 
                                                 
? 2008. Licensed under the Creative Commons Attribution-
Noncommercial-Share Alike 3.0 Unported license 
(http://creativecommons.org/licenses/by-nc-sa/3.0/). Some 
rights reserved. 
treated equally ignoring their linguistic signific-
ance; only consecutive grams are considered ig-
noring the skipped grams of certain linguistic 
relations; candidate translation gets acknowl-
edged only if it uses exactly the same lexicon as 
the reference ignoring the variation in lexical 
choice. Furthermore, BLEU is useful for opti-
mizing and improving statistical MT systems but 
it has shown to be ineffective in comparing sys-
tems with different architectures (e.g., rule-based 
vs. phrase-based) (Callison-Burch et al,  2006).  
    Another common deficiency of the state-of-
the-art evaluation approaches is that they cannot 
clearly inform MT developers on the detailed 
strengths and flaws of an MT system, and there-
fore there is no way for us to understand the ca-
pability of certain modules of an MT system, and 
the capability of translating certain kinds of lan-
guage phenomena. For the purpose of system 
development, MT developers need a diagnostic 
evaluation approach to provide the feedback on 
the translation ability of an MT system with re-
gard to various important linguistic phenomena.     
    We propose a novel diagnostic evaluation ap-
proach. Instead of assigning a general score to an 
MT system we evaluate the capability of the sys-
tem in handling various important linguistic test 
cases called Check-Points. A check-point is a 
linguistically motivated unit, (e.g. an ambiguous 
word, a noun phrase, a verb~obj collocation, a 
prepositional phrase etc.) which are pre-defined 
in a linguistic taxonomy for diagnostic evalua-
tion. The reference of a check-point is its corres-
ponding part in the target sentence. The evalua-
tion is performed by matching the candidate 
translation corresponding to the references of the 
check-points. The extraction of the check-points 
is an automatic process using word aligner and 
parsers. We control the noise of the word aligner 
and parsers within tolerable scope by selecting 
1121
reliable subset of the check-points and weighting 
the references with confidence.  
    The check-points of various kinds extracted in 
this way have shown to be effective in perform-
ing diagnostic evaluation of MT systems. In ad-
dition, scores of check-points are also approved 
to be useful to improve the ranking of MT sys-
tems as additional features at sentence level and 
document level. 
The rest of the paper is structured in the fol-
lowing way:  Section 2 gives the overview of the 
process of the diagnostic evaluation. Section 3 
introduces the design of check-point taxonomy. 
Section 4 explains the details of construction of 
check-point database and the methods of reduc-
ing the noise of aligner and parsers. Section 5 
explains the matching approach. In Section 6, we 
introduce the experiments on different MT sys-
tems to demonstrate the capability of the diag-
nostic evaluation. In Section 7, we show that the 
check-points can be used to improve the current 
ranking methods of MT systems. Section 8 com-
pares our approach with related evaluation ap-
proaches. We conclude this work in Section 9. 
2 Overview of Diagnostic Evaluation 
In our implementation, we first build a check-
point database encoded in XML by associating a 
test sentence with qualified check-points it con-
tains. This process can be described as following 
steps: 
 
? Collect a large amount of parallel sen-
tences from the web or book collections. 
? Parse the sentences of source language 
and target language. 
? Perform the word alignments between 
each sentence pair. 
? For each category of check-points, extract 
the check-points from the parsed sentence 
pairs. 
? Determine the references of each check-
point in source language based on the 
word alignment.  
 
   With the extracted check-point database, the 
diagnostic evaluation of an MT system is per-
formed with the following steps: 
 
? The test sentences are selected from the 
database based on the selected categories 
of check-points to be evaluated. 
? For each check-point, we calculate the 
number of matched n-grams of the refer-
ences against the translated sentence of 
the MT system.  The credit of the MT sys-
tem in translating this check-point is ob-
tained after necessary normalization. 
? The credit of a category can be obtained 
by summing up the credits of all check-
points of this category. Then the credit of 
an MT system can be obtained by sum-
ming up the credits of all categories. 
? Finally, scores of system, category groups 
(e.g. Words), single category (e.g. Noun), 
and detail information of n-gram matching 
of each check-point are all provided to the 
developers to diagnose the MT system. 
3 Linguistic Check-Point Taxonomy 
The taxonomy of automatic diagnostic evaluation 
should be widely accepted so that the diagnostic 
results can be explained and shared with each 
other. We will also need to remove the sophisti-
cated categories that are out of the capability of 
current NLP tools to recognize.  
In light of this consideration, for Chinese-
English machine translation, we adopted the ma-
nual taxonomy introduced by (Lv, 2000; Liu, 
2002) and removed items that are beyond the 
capability of our parsers. The taxonomy includes 
typical check-pints at word, phrase and sentence 
levels. Some examples of the representative 
check-points at different levels are provided be-
low: 
 
?  Word level check-points: 
    a. Preposition word e.g., ?(in), ?(at) 
    b. Ambiguous word e.g., ?(play) 
    c. New word1 e.g., ??(Punk)  
?  Phrase level check-points: 
    a. Collocation. e.g., ??-??(fired ? food)  
    b. Repetitive word combination. e.g., ??
(have a look) 
    c. Subjective-predicate phrase e.g., ?*?, 
(he*said) 
     ? Sentence level check-points:  
a. ?BA? sentence 2 : ?? (BA)???? . 
(He took away the book.) 
b. ?BEI? sentence3????(BEI)???. 
(The vase was broken.)   
                                                 
1 New words are the terms extracted from web which can be 
a name entity or popular words emerging recently.   
2 In a ?BA? sentence, the object which normally follows the 
verb occurs preverbally, marked by word ?BA?. 
3 ?BEI? sentence is a kind of passive voice in Chinese 
marked by word ?BEI?. 
1122
    Our implementation of Chinese-English 
check-point taxonomy contains 22 categories and 
English-Chinese check-point taxonomy contains 
20 categories. Table 1 and 2 show the two 
taxonomies. In practice, any tag in parsers (e.g. 
NP) can be easily added as new category. 
 
Word level 
Ambiguous word New word Idiom 
Noun Verb Adjective 
Pronoun Adverb Preposition 
Quantifier Repetitive word Collocation 
Phrase level 
Subject-predicate 
phrase 
Predicate-object 
 phrase 
Preposition-
object phrase 
Measure phrase Location phrase  
Sentence level 
BA sentence BEI sentence SHI sentence 
YOU sentence Compound sentence 
Table 1:  Chinese check-point taxonomy 
 
Word level 
Noun Verb (with Tense) Modal verb 
Adjective Adverb Pronoun 
Preposition Ambiguous word Plurality 
Possessive Comparative & Superlative  degree 
Phrase level 
Noun phrase Verb phrase Adjective 
phrase 
Adverb phrase Preposition phrase  
Sentence level 
Attribute clause Adverbial clause Noun clause 
Hyperbaton  
Table 2: English check-point taxonomy 
4 Construction of Check-Point Data-
base 
Given a bilingual corpus with word alignment, 
the construction of check-point database consists 
of following two steps. First, the information of 
pos-tag, dependency structure and constituent 
structure can be identified with parsers. Then 
check-points of different categories are identified. 
Check-points of word-level categories such as 
Chinese idiom and English ambiguous words are 
extracted with human-made dictionaries, and the 
check-points of New-Word are extracted with a 
new word list mined from the web. A set of hu-
man-made rules are employed to extract certain 
categories involving sentence types such as com-
pound sentence.  
    Second, for a check-point, with the word 
alignment information, the corresponding target 
language portion is identified as the reference of 
this check-point. The following example illu-
strates the process of extracting check-points 
from a parallel sentence pair.  
?  A Chinese-English sentence pair: 
  ?????????. 
    They opposed the building of reserve funds. 
?  Word segmentation and pos-tagging: 
  ??/R ??/V ??/V ???/N ./P 
?  Parsing result (e.g.  a dependency result): 
    (SUB, 1/??, 0/??)  (OBJ, 1/??, 2/?
?) (OBJ, 2/??, 3/???) 
?  Word alignment: 
     (1; 1); (2; 2); (3; 4); (4; 6,7);   
?   The check-points in table 3 are extracted: 
 
Table 3: Example of check-point extraction 
 
    To extract the categories of check-points of 
different schema of syntactic analysis such as 
constitute structure and dependency structure, 
three parsers including a Chinese skeleton parser 
(a kind of dependency parser) (Zhou, 2000), 
Stanford statistical parser and Berkeley statistical 
parser (Klein 2003) are used to parse the Chinese 
and English sentences.  As explained in next sec-
tion, these multiple parsers are also used to select 
high confident check-points. To get word align-
ment, an existing tool GIZA++ (Och 2003) is 
used.  
4.1 Reducing the Noise of the Parser 
The reliability of the check-points mainly 
depends on the accuracy of the parsers. We can 
achieve high quality word level check-points 
with the state-of-the-art POS tagger (94% 
precision) and dictionaries of various purposes. 
For sentence level categories, the parser tags and 
manually compiled rules can also achieve 95% 
accuracy. For some kinds of categories at phrase 
level which parsers cannot produce high 
accuracy, we only select the check-points which 
can be identified by multiple parsers, that is, 
adopt the intersection of the parsers results. 
Table 4 shows the improvement brought by this 
approach. Column 1 and 2 shows the precision of 
6 major types of phrases in Stanford and 
Berkeley parser. Column 3 shows the precision 
of intersection and column 4 shows the reduction 
of the number of check-points when adopting the 
intersection results. The test corpus is a part of 
Category Check-point Reference 
New word ??? reserve funds 
Ambiguous word ?? building 
Predicate ? object 
phrase 
????? the building of  
reserve funds 
Subject-predicate 
phrase 
???? They opposed 
1123
Penn Chinese Treebank which is not contained in 
the training corpus of two statistical parsers. 
(Klein 2003).  
 
 Stf% Brk% Inter% Tpts redu% 
NP 87.37 86.03 95.83 17.06 
VP 87.34 82.87 95.23 19.68 
PP 90.60 88.56 96.00 11.50 
QP 98.12 92.90 99.21 6.31 
ADJP 91.95 90.87 96.41 10.20 
ADVP 95.21 94.25 92.64 3.92 
Table 4:  Precision of parsers and their intersec-
tion (Stf is Stanford, Brk is Berkelry) 
 
4.2 Alleviating the Impact of Alignment 
Noise 
Except for sentence level check-points whose 
references are the whole sentences and New 
Word, Idiom check-points whose references are 
extracted from dictionary, the quality of the ref-
erences are impacted by the alignment accuracy. 
To alleviate the noise of aligner we use the lexi-
cal dictionary to check the reliability of refer-
ences. Suppose c is a check-point, for each refer-
ence c.r of c we calculate the dictionary match-
ing degree DM(c.r) with the source side c.s of c: 
 
)1()).(
)).(,.(,1.0().( rcWordCnt
scDicrcCoCntMaxrcDM ?  
 
    Where Dic(x) is a word bag contains all words 
in the dictionary translations of each source word 
in x. CoCnt(x, y) is the count of the common 
words in x and y. WordCnt(x) is the count of 
words in x. Specially, if c.r is not obtained based 
on alignment DM(c.r) will be 1. Because the li-
mitation of dictionary, a zero DM score not al-
ways means the reference is completely wrong, 
so we force the DM score to be not less than a 
minimum value (e.g. 0.1). DM score will further 
be used in evaluation in section 5.  
    To better understand the reliability of the ref-
erences and explore whether increasing the num-
ber of check-points could also alleviate the im-
pact of noise, we built two check-point databases 
from a human-aligned corpus (with 60,000 sen-
tence pairs) and an automatically aligned corpus 
(using GIZA++) respectively and tested 10 dif-
ferent SMT systems4 with them. The Spearman 
correlation is calculated between two ranked lists 
of the 10 evaluation results against the two data-
                                                 
4 These systems are derived from an in-house phrase based 
SMT engine with different parameter sets. 
bases. A higher correlation score means that the 
impact of the mistakes in word alignment is 
weaker. The experiment is repeated on 6 subsets 
of the database with the size from 500 sentences 
to 16K sentences to check the impact of the cor-
pus size. 
    At system level, high correlations are found at 
different corpus sizes. At category level, correla-
tions are found to be low for some categories at 
small size and become higher at larger corpus 
size. The results indicate that the impact of the 
alignment quality can be ignored if the corpus 
size is at large scale. As the check-points can be 
extracted fully automatically, increasing the size 
of check-point database will not bring extra cost 
and efforts. Empirically, the proper scale is set to 
be 2000 or more sentences according to the Ta-
ble 6. 
 
Table 6: Impact of word alignment at different 
sizes of test corpus. 
5 Matching Check-Points for Evalua-
tion 
Evaluation can be carried out at multiple options: 
for certain linguistic category, a group of catego-
ries or entire taxonomy. For instance, in Chinese-
English translation task, if a MT developer 
would like to know the ability to translate idiom, 
then a number of parallel sentences containing 
idiom check-points are selected from the data-
base. Then the system translation sentences are 
matched to the references of the check-points of 
idioms.  
 500 1K 2K 4K 8K 16K 
Ambiguous 
word 
0.98 0.98 0.98 0.98 0.96 0.98 
Noun 0.93 0.99 0.99 0.89 0.8 0.86 
Verb 0.97 0.97 0.99 0.99 0.95 0.92 
Adjective 0.16 0.19 0.57 0.75 0.77 0.97 
Pronoun 0.96 1 0.93 0.99 0.97 0.99 
Adverb 0.38 0.77 0.8 0.96 0.72 0.84 
Preposition 0.56 0.86 0.9 0.9 0.97 0.96 
Quantifier 1 0.46 0.46 0.98 0.85 0.96 
Repetitive 
Word 
0.99 0.99 0.97 0.89 0.73 0.95 
Collocation 0.42 0.77 0.77 0.77 0.73 0.88 
Subject-
predicate 
phrase 
0.06 0.8 0.95 1 0.96 0.84 
Predicate-
object phrase 
0.84 0.96 0.78 0.7 0.78 0.88 
Preposition-
object phrase 
0.51 0.5 0.93 0.95 0.87 0.99 
Measure 
phrase 
0.91 0.67 0.95 0.95 0.87 0.97 
Location 
phrase 
0.62 0.54 0.55 0.55 0.85 0.89 
SYSTEM 0.95 0.95 0.98 0.99 0.97 0.98 
1124
To calculate the credit at different occasions of 
matching, similar to BLEU, we split each refer-
ence of a check-point into a set of n-grams and 
sum up the gains over all grams as the credit of 
this check-point. Especially, if the check-point is 
not consecutive we use a special token (e.g. ?*?) 
to represent a component which can be wildcard 
matched by any word sequence. We use the fol-
lowing examples to demonstrate the splitting and 
matching of grams.  
 
?  Consecutive check-point: 
    Check- point: ??? 
    Reference: playing a drum 
    Candidate translation:  He is playing a drum.  
    Matched n-grams: playing; a; drum; playing a; 
a drum; playing a drum  
 
?   Not consecutive check-point: 
    Check- point: ??*?   
    Reference: They*playing   
    Candidate translation: They are playing cop 
per drum. 
    Matched n-grams: They; playing; They * play-
ing 
    Additionally, to match word inflections, 3 dif-
ferent options of matching granularity are de-
fined as follows.  
?  Normal: matching with exact form. 
?  Lower-case: matching with lowercase. 
?  Stem: matching with the stem of the word. 
 
    For a check-point c and references set R of c, 
we select the r* in R which matches the transla-
tion best based on formula (2).  
 
 
 
    
 
When we calculate the recall of a set of check-
points C (C can be a single check-point, a cate-
gory or a category group), r* of each check-point 
c in C are merged into one reference set R* and 
the recall of C is obtained using formula (3) on 
R*. 
 
 
 
 
 
 
A penalty is also introduced to punish the re-
dundancy of candidate sentences, where length(T) 
is the average length of all translation sentences 
and length(R) is the average length of all refer-
ence sentences. 
 
 
 
 
 
Then, the final score of C will be: 
 
)5()(Re)( PenaltyCCScore ??
 
6 Experiments on MT System Diagnos-
es 
In this section, to demonstrate the ability of our 
approach in the diagnoses of MT systems, we 
apply diagnostic evaluation to 3 statistical MT 
(SMT) systems and a rule-based MT (RMT) sys-
tem respectively. We compare two SMT systems 
to understand the strength and shortcoming of 
each of them, and also compare a SMT system 
with the RMT system. The test corpus is NIST05 
test data with 54852 check-points. 
    First SMT system (system A) is an implemen-
tation of classical phrase based SMT. The second 
SMT system (system B) shares the same decoder 
with system A and introduces a preprocess to 
reorder the long phrases in source sentences ac-
cording to the syntax structure before decoding 
(Chiho Li et al, 2007). The third SMT system 
(system C) is a popular internet service and the 
RMT system (system D) is a popular commercial 
system.  
    In the first experiment, we diagnose the sys-
tem A and B and compare the results as shown in 
table 7. When evaluated using BLEU, system B 
achieved a 0.005 points increase on top of system 
A which is not a very significant difference. The 
diagnostic results in table 7 provide much richer 
information. The results indicate that two sys-
tems perform similar at the word level categories 
while at all phrase level categories, system B 
performs better. This result reflects the benefit 
from the reordering of complex phrases in sys-
tem B. Paired t-statistic score for each pair of 
category scores is also calculated by repeating 
the evaluation on a random copy of the test set 
with replacement (Koehn 2004). An absolute 
score beyond 2.17 of paired t-statistic means the 
difference of the samples is statistically signifi-
cant (above 95%). Table 8 and 9 show an in-
stance of the check-point and its evaluation in 
this experiment. 
)2()
)'(
)(
)((maxarg
'
*
?
?
??
??
? ?
?
??
rgramn
rgramn
Rr gramnCount
gramnMatch
rDMr
)4(
1
)()(
)(
)(
??
?
?
? ?
?
Otherwise
RlengthTlengthif
Tlength
Rlength
Penalty
)3(
))'()((
))()((
)Re(
*
*
' ''
'? ?
? ?
? ??
? ??
??
??
?
Rr rgran
Rr rgramn
gramnCountrDM
gramnMatchrDM
C
1125
 System A System B T score 
WORDs 
Idiom 0.1933 0.2370 13.38 
Adjective 0.5836 0.5577 -17.43 
Pronoun 0.7566 0.7344 -13.49 
Adverb 0.5365 0.5433 7.11 
Preposition 0.6529 0.6456 -6.21 
Repetitive word 0.3363 0.3958 9.86 
PHRASEs 
Subject-predicate 0.5117 0.5206 7.36 
Predicate-object 0.4041 0.4180 15.52 
Predicate-complement 0.4409 0.5125 9.51 
Measure phrase 0.5030 0.5092 3.56 
Location phrase 0.5245 0.5338 2.83 
GROUPs 
WORDs 0.4839 0.4855 2.03 
PHRASEs 0.4744 0.4964 13.97 
SYSTEM (Linguistic) 0.4263 0.4370 16.50 
SYSTEM (BLEU) 0.3564 0.3614 7.91 
Table 7: Diagnose of SMT systems 
 
Source Sentence ????????????????
??????? 
Category Preposition_Object_Phrase 
Check-Point ? ? ?? 
Reference 1 in this country  DM = 0.5 
Reference 2 in his country  DM = 0.5 
System A Translation but the prime minister of thailand Dex-
in vowed to continue in domestic the 
search. 
System B Translation but the prime minister of thailand Dex-
in vowed to continue the search in his 
country. 
Table 8: An instance of the check-point. 
 
 System A System B 
Ref 1: Match/Total 1/6 2/6 
Ref 2: Match/Total 1/6 6/6 
Score 0.17 1 
Table 9: N-gram matching rate and scores. 
 
Table 10: Diagnose of SMT and RMT. 
 
In the second experiment, we diagnose system 
C and D and compare the results. The BLEU 
score of system C is 0.3005 and system D is 
0.2606. Table 10 shows the diagnostic results on 
categories with significant differences. Scores 
calculated with 3 matching options described in 
section 5 are given (?Lower? means Lowercase. 
The scores are listed in the form ?SMT 
score/RMT score?). The diagnostic results indi-
cate that system C performs better on most cate-
gories than system D, but system D performs 
better on categories like idiom, pronoun and pre-
position. This result reveals a key difference be-
tween two types of MT systems: the SMT works 
well on the open categories that can be handled 
by context, while the RMT works well on closed 
categories which are easily translated by linguis-
tic rules. 
    As the results of two experiments demonstrate, 
the diagnostic evaluation provides rich informa-
tion of the capability of translating various im-
portant linguistic categories beyond a single sys-
tem score. It successfully distinguishes the spe-
cific difference between the MT systems whose 
system level performance is similar. It can also 
diagnose the MT system with different architec-
tures. Diagnostic evaluation tells the developers 
about the direction to improve the system. Along 
with the scores of categories, the diagnostic 
evaluation provides the system translation and 
references at every check-point so that the devel-
opers can trace and understand about how the 
MT system works on every single instance. 
7 Experiments on Ranking MT Systems 
Offering a general evaluation at system level is 
the major goal of state-of-the-art evaluation me-
thods including widely accepted n-gram metrics. 
The absence of linguistic knowledge in BLEU 
motivated many work to integrate linguistic fea-
tures into evaluation metric. In (Yang 2007), the 
evaluation of SMT systems is alternately formu-
lated as a ranking problem. Different linguistic 
features are combined with BLEU such as 
matching rate of dependency relations of transla-
tion candidates against the reference sentences. 
The experiments demonstrate that the dependen-
cy matching rate feature can increase the ranking 
accuracy in some cases. Compared to dependen-
cy structure, the linguistic categories in our ap-
proach showcase more extensive features. It 
would be interesting to see whether the linguistic 
categories can be used to further improve the 
ranking of SMT systems.  
    In experiments, we use the scores of linguistic 
categories, dependency matching rate, scores of 
BLEU and other popular metrics as ranking fea-
tures of MT systems and trained by Ranking 
SVM of SVMlight (Joachims, 1998). We per-
formed the ranking experiments on ACL 2005 
workshop data, ranking 7 MT translations with 
three-fold cross-validation both on sentence level 
and document level. The Spearman score is used 
Type Normal Lower Stem 
Ambiguous word 0.49/0.42 0.50/0.42 0.53/0.46 
New word 0.13/0.13 0.37/0.32 0.42/0.35 
Idiom 0.43/0.66 0.46/0.67 0.51/0.71 
Pronoun 0.60/0.68 0.69/0.75 0.66/0.75 
Preposition 0.38/0.42 0.42/0.45 0.43/0.46 
Collocation 0.66/0.54 0.66/0.55 0.70/0.56 
Subject-predicate 
phrase 
0.46/0.30 0.51/0.36 0.58/0.42 
Predicate-object 
phrase 
0.37/0.25 0.37/0.26 0.47/0.29 
Compound sentence 0.22/0.16 0.23/0.16 0.23/0.17 
1126
to calculate the correlation with human assess-
ments. Table 11 and 12 show the results of the 
different feature sets on sentence level and doc-
ument level respectively. 
As shown in experiment results linguistic cat-
egories (LC), when used alone, are better related 
with human assessments than BLEU and GTM. 
When combined with the baseline metrics 
(BLEU & NIST), LC scores further improve the 
correlation score, better than dependence match-
ing rate (DP). LC scores are obtained by match-
ing the exact form of the words as ME-
TEOR(exact) does. NIST+LC combination score 
is better than METEOR(exact) at sentence and 
document level, and also better than ME-
TEOR(exact&syn) (syn means wn_synonymy 
module in METEOR) at document level. This 
results indicate the ability of linguistic features in 
improving the performance of ranking task. 
 
 Mean Correlation 
BLEU 4 0.245 
NIST 5 0.307 
GTM (e=2) 0.251 
METEOR(exact) 0.306 
METEOR(exact&syn) 0.327 
DP 0.246 
LC 0.263 
BLEU+DP 0.270 
BLEU+ LC 0.288 
BLEU+ DP +LC 0.307 
NIST+ LC 0.322 
NIST+ DP +LC 0.333 
 Table11: Sentence level ranking (DP means 
dependency and LC means linguistic categories)  
 
 Mean Correlation 
BLEU 4 0.305 
NIST 5 0.373 
GTM (e=2) 0.327 
METEOR(exact) 0.363 
METEOR(exact&syn) 0.394 
DP 0.323 
LC 0.369 
BLEU+DP 0.325 
BLEU+ LC 0.387 
BLEU+ DP +LC 0.332 
NIST+ LC 0.409 
NIST+ DP +LC 0.359 
Table 12: Document level ranking 
8 Comparison with Related Work 
This work is inspired by (Yu, 1993) with many 
extensions. (Yu, 1993) proposed MTE evaluation 
system based on check-points for English-
Chinese machine translation systems with human 
craft linguistic taxonomy including 3,200 pairs of 
sentences containing 6 classes of check-points. 
Their check-points were manually constructed by 
human experts, therefore it will be costly to build 
new test corpus while the check-points in our 
approach are constructed automatically. Another 
limitation of their work is that only binary score 
is used for credits while we use n-gram matching 
rate which provides a broader coverage of differ-
ent levels of matching.   
    There are many recent work motivated by n-
gram based approach. (Callison-Burch et al, 
2006) criticized the inadequate accuracy of eval-
uation at the sentence level. (Lin and Och, 2004) 
used longest common subsequence and skip-
bigram statistics. (Banerjee and Lavie, 2005) cal-
culated the scores by matching the unigrams on 
the surface forms, stemmed forms and senses. 
(Liu et al, 2005) used syntactic features and un-
labeled head-modifier dependencies to evaluate 
MT quality, outperforming BLEU on sentence 
level correlations with human judgment. (Gime-
nez and Marquez, 2007) showed that linguistic 
features at more abstract levels such as depen-
dency relation may provide more reliable system 
rankings. (Yang et al, 2007) formulates MT 
evaluation as a ranking problems leading to 
greater correlation with human assessment at the 
sentence level.  
There are many differences between these n-
gram based methods and our approach. In n-
gram approach, a sentence is viewed as a collec-
tion of n-grams with different length without dif-
ferentiating the specific linguistic phenomena. In 
our approach, a sentence is viewed as a collec-
tion of check-points with different types and 
depth, conforming to a clear linguistic taxonomy. 
Furthermore, in n-gram approach, only one gen-
eral score at the system level is provided which 
make it not suitable for system diagnoses, while 
in our approach we can give scores of linguistic 
categories and provide much richer information 
to help developers to find the concrete strength 
and flaws of the system, in addition to the gener-
al score. The n-gram based metric is not very 
effective when comparing the systems with dif-
ferent architectures or systems with similar gen-
eral score, while our approach is more effective 
in both cases by digging into the multiple lin-
guistic levels and disclosing the latent differenc-
es of the systems. 
9 Conclusion and Future Work 
This paper presents an automatically diagnostic 
evaluation methods on MT based on linguistic 
check-points automatically constructed. In con-
trast with the metrics which only give a general 
score, our evaluation system can give developers 
1127
feedback about the faults and strength of an MT 
system regarding specific linguistic category or 
category group. Different with the existing work 
based on check-points, our work presents an ap-
proach to automatically generate the check-point 
database. We show that although there is some 
noise brought from word alignment and parsing, 
we can effectively alleviate the problem by refin-
ing the parser results, weighting the reference 
with confidence score and providing large quan-
tity of check-points.  
    The experiments demonstrate that this method 
can uncover the specific difference between MT 
systems with similar architectures and different 
architectures. It is also demonstrated that the lin-
guistic check-points can be used as new features 
to improve the ranking task of MT systems.   
    Although we present the diagnostic evaluation 
method with Chinese-English language pair, our 
approach can be applied to other language pair if 
syntax parser and word aligner are available. 
    The taxonomy used in current proposal is 
based on the human-made linguistic system. An 
interesting problem to be explored in the future is 
whether the taxonomy could be constructed au-
tomatically from the parsing results.  
References 
Statanjeev Banerjee, Alon Lavie. 2005. METEOR: An 
Automatic Metric for MT Evaluation with Im-
proved Correlation with Human Judgements. In 
Proceedings of the ACL Workshop on Intrinsic and 
Extrinsic Evaluation Measures for Machine Trans-
lation and/or Summarization 2005. 
Chris Callison-Burch, Miles Osborne, Philipp Koehn. 
2006. Re-evaluating the Role of Bleu in Machine 
Translation Research. In Proceedings of the Euro-
pean Chapter of the ACL 2006. 
Martin Chodorow, Claudia Leacock. 2000. An unsu-
pervised method for detecting grammatical errors, 
In 1st Meeting of the North America Chapter of the 
ACL, pp.140?147, 2000. 
Thorsten Joachims. 1998. Making Large-scale Sup-
port Vector Machine Learning Practical, In B. 
Scholkopf, C. Burges, A. Smola. Advances in Ker-
nel Methods: Support VectorMachines, MIT Press, 
Cambridge, MA, December. 
Jesus Gimenez and Llis Marquez. 2007. Linguistic 
features for automatic evaluation of heterogeneous 
MT systems, Workshop of statistical machine trans-
lation in conjunction with 45th ACL, 2007. 
Dan Klein, Christopher Manning. 2003. Accurate 
Unlexicalized Parsing, Proceedings of the 41th 
Meeting of the ACL, pp. 423-430. 
Philipp Koehn. 2004. Statistical Significance Tests for 
Machine Translation Evaluation. In Proc. of the 
EMNLP, Barcelona, Spain. 
Chiho Li, Minghui Li, Dongdong Zhang, Mu Li, 
Ming Zhou, Yi Guan. 2007. A Probabilistic Ap-
proach to Syntax-based Reor-dering for SMT. In 
Proceedings of the 45th  ACL, 2007. 
Chin-Yew Lin and Franz Josef Och. 2004. Automatic 
evaluation of machine translation quality using 
longest common subsequence and skip-bigram sta-
tistics. In Proceedings of the 42th ACL 2004.  
Ding Liu, Daniel Gildea. 2005. Syntactic Features for 
Evaluation of Machine Translation, ACL Work-
shop on Intrinsic and Extrinsic Evaluation Meas-
ures for Machine Translation and/or Summariza-
tion. 
Shuxin Liu. 2002. Linguistics of Contemporary Chi-
nese Language (in Chinese), Advanced Education 
Publisher. 
Jiping Lv. 2000. Foundation of Mandarin Grammar 
(in Chinese), Shangwu Publisher. 
Franz Josef Och, Hermann Ney. 2003. A Systematic 
Comparison of Various Statistical Alignment Mod-
els, Computational Linguistics, volume 29, number 
1, pp. 19-51 March 2003. 
Kishore Papieni, Salim Roukos, Todd Ward, Wei-Jing 
Zhu. 2002. BLEU: a method for automatic evalua-
tion of machine translation, In Proceedings of the 
ACL 2002. 
Shiwen Yu. 1993. Automatic evaluation of output 
quality for machine translation systems, In Pro-
ceedings of the evaluators? forum, April 21-24, 
1991, Les Rasses, Vaud, 1993.  
Yang Ye, Ming Zhou, Chinyew Lin. 2007. Sentence 
level machine translation evaluation as a ranking 
problem: one step aside from BLEU, In Workshop 
of statistical machine translation, in conjunction 
with 45th ACL, 2007. 
Ming Zhou. 2000, A Block-Based Robust Dependency 
Parser for Unrestricted Chinese Text. Proceedings 
of Second Chinese Language Processing Workshop, 
2000, held in conjunction with ACL, 2000.  
 
1128
Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational
Natural Language Learning, pp. 533?540, Prague, June 2007. c?2007 Association for Computational Linguistics
 
Phrase Reordering Model Integrating Syntactic Knowledge for SMT 
Dongdong Zhang, Mu Li, Chi-Ho Li, Ming Zhou 
 
Microsoft Research Asia 
Beijing, China 
{dozhang,muli,chl,mingzhou}@microsoft.com 
 
 
Abstract 
Reordering model is important for the sta-
tistical machine translation (SMT). Current 
phrase-based SMT technologies are good at 
capturing local reordering but not global 
reordering. This paper introduces syntactic 
knowledge to improve global reordering 
capability of SMT system. Syntactic know-
ledge such as boundary words, POS infor-
mation and dependencies is used to guide 
phrase reordering. Not only constraints in 
syntax tree are proposed to avoid the reor-
dering errors, but also the modification of 
syntax tree is made to strengthen the capa-
bility of capturing phrase reordering. Fur-
thermore, the combination of parse trees 
can compensate for the reordering errors 
caused by single parse tree. Finally, expe-
rimental results show that the performance 
of our system is superior to that of the 
state-of-the-art phrase-based SMT system. 
1 Introduction 
In the last decade, statistical machine translation 
(SMT) has been widely studied and achieved good 
translation results. Two kinds of SMT system have 
been developed, one is phrase-based SMT and the 
other is syntax-based SMT.  
In phrase-based SMT systems (Koehn et al, 
2003; Koehn, 2004), foreign sentences are firstly 
segmented into phrases which consists of adjacent 
words. Then source phrases are translated into tar-
get phrases respectively according to knowledge 
usually learned from bilingual parallel corpus. Fi-
nally the most likely target sentence based on a 
certain statistical model is inferred by combining 
and reordering the target phrases with the aid of 
search algorithm. On the other hand, syntax-based 
SMT systems (Liu et al, 2006; Yamada et al, 
2001) mainly depend on parse trees to complete 
the translation of source sentence.  
 
 
 
 
 
 
 
 
 
 
 
 
 
 
Figure 1: A reordering example 
 
As studied in previous SMT projects, language 
model, translation model and reordering model are 
the three major components in current SMT sys-
tems. Due to the difference between the source and 
target languages, the order of target phrases in the 
target sentence may differ from the order of source 
phrases in the source sentence.  To make the trans-
lation results be closer to the target language style, 
a mathematic model based on the statistic theory is 
constructed to reorder the target phrases. This sta-
tistic model is called as reordering model. As 
shown in Figure 1, the order of the translations of 
???? and ??? is changed. The order of the 
IP 
VP 
ADVP 
NP 
DNP VP 
NP 
NN 
AD DEG 
?? 
VV 
? ?? ?? 
the Euro the significant appreciation of 
533
translation of ???/?? and ???/??? is al-
tered as well. The former reordering case with the 
smaller distance is usually referred as local reor-
dering and the latter with the longer distance reor-
dering as global reordering. Phrase-based SMT 
system can effectively capture the local word reor-
dering information which is common enough to be 
observed in training data. But it is hard to model 
global phrase reordering. Although syntactic 
knowledge used in syntax-based SMT systems can 
help reorder phrases, the resulting model is usually 
much more complicated than a phrase-based sys-
tem. 
There have been considerable amount of efforts 
to improve the reordering model in SMT systems, 
ranging from the fundamental distance-based dis-
tortion model (Och and Ney, 2004; Koehn et al, 
2003), flat reordering model (Wu, 1996; Zens et al, 
2004; Kumar et al, 2005), to lexicalized reordering 
model (Tillmann, 2004; Kumar et al, 2005; Koehn 
et al, 2005), hierarchical phrase-based model 
(Chiang, 2005), and maximum entropy-based 
phrase reordering model (Xiong et al, 2006). Due 
to the absence of syntactic knowledge in these sys-
tems, the ability to capture global reordering know-
ledge is not powerful. Although syntax-based SMT 
systems (Yamada et al, 2001; Quirk et al, 2005; 
Liu et al, 2006) are good at modeling global reor-
dering, their performance is subject to parsing er-
rors to a large extent. 
In this paper, we propose a new method to im-
prove reordering model by introducing syntactic 
information. Syntactic knowledge such as boun-
dary of sub-trees, part-of-speech (POS) and depen-
dency relation is incorporated into the SMT system 
to strengthen the ability to handle global phrase 
reordering. Our method is different from previous 
syntax-based SMT systems in which the translation 
process was modeled based on specific syntactic 
structures, either phrase structures or dependency 
relations. In our system, syntactic knowledge is 
used just to decide where we should combine adja-
cent phrases and what their reordering probability 
is. For example, according to the syntactic infor-
mation in Figure 1, the phrase translation combina-
tion should take place between ???? and ???? 
rather than between ??? and ????. Moreover, 
the non-monotone phrase reordering should occur 
between ???/?? and ???/??? rather than 
between ???/?? and ????. We train a maxi-
mum entropy model, which is able to integrate rich 
syntactic knowledge, to estimate phrase reordering 
probabilities. To enhance the performance of 
phrase reordering model, some modification on the 
syntax trees are also made to relax the phrase reor-
dering constraints. Additionally, the combination 
of other kinds of syntax trees is introduced to over-
come the deficiency of single parse tree. The expe-
rimental results show that the performance of our 
system is superior to that of the state-of-art phrase-
based SMT system.  
The roadmap of this paper is: Section 2 gives the 
related work. Section 3 introduces our model.  Sec-
tion 4 explains the generalization of reordering 
knowledge. The procedures of training and decod-
ing are described in Section 5 and Section 6 re-
spectively. The experimental results are shown in 
Section 7. Section 8 concludes the paper. 
2 Related Work  
The Pharaoh system (Koehn et al, 2004) is well 
known as the typical phrase-based SMT system. Its 
reordering model is designed to penalize transla-
tion according to jump distance regardless of lin-
guistic knowledge. This method just works well for 
language pairs that trend to have similar word-
orders and it has nothing to do with global reorder-
ing. 
A straightforward reordering model used in (Wu, 
1996; Zens et al, 2004; Kumar et al, 2005) is to 
assign constant probabilities to monotone reorder-
ing and non-monotone reordering, which can be 
flexible depending on the different language pairs. 
This method is also adopted in our system for non-
peer phrase reordering. 
The lexicalized reordering model was studied in 
(Tillmann, 2004; Kumar et al, 2005; Koehn et al, 
2005). Their work made a step forward in integrat-
ing linguistic knowledge to capture reordering. But 
their methods have the serious data sparseness 
problem. 
Beyond standard phrase-based SMT system, a 
CKY style decoder was developed in (Xiong et al, 
2006). Their method investigated the reordering of 
any two adjacent phrases. The limited linguistic 
knowledge on the boundary words of phrases is 
used to construct the phrase reordering model.  The 
basic difference to our method is that no syntactic 
knowledge is introduced to guide the global phrase 
reordering in their system. Besides boundary 
534
words, our phrase reordering model also integrates 
more significant syntactic knowledge such as POS 
information and dependencies from the  syntax tree, 
which can avoid some intractable phrase reorder-
ing errors. 
A hierarchical phrase-based model was pro-
posed by (Chiang, 2005). In his method, a syn-
chronous CFG is used to reorganize the phrases 
into hierarchical ones and grammar rules are auto-
matically learned from corpus. Different from his 
work, foreign syntactic knowledge is introduced 
into the synchronous grammar rules in our method 
to restrict the arbitrary phrase reordering.   
Syntax-based SMT systems (Yamada et al, 
2001; Quirk et al, 2005; Liu et al, 2006) totally 
depend on syntax structures to complete phrase 
translation. They can capture global reordering by 
simply swapping the children nodes of a parse tree. 
However, there are also reordering cases which do 
not agree with syntactic structures. Furthermore, 
their model is usually much more complex than a 
phrase-based system. Our method exactly attempts 
to integrate the advantages of phrase-based SMT 
system and syntax-based SMT system to improve 
the phrase reordering model. Phrase translation in 
our system is independent of syntactic structures. 
3 The Model 
In our work, we focus on building a better reorder-
ing model with the help of source parsing informa-
tion. Although we borrow some fundamental ele-
ments from a phrase-based SMT system such as 
the use of bilingual phrases as basic translation unit, 
we are more interested in introducing syntactic 
knowledge to strengthen the ability to handle glob-
al reordering phenomena in translation.  
3.1 Definitions 
Given a foreign sentence f and its syntactic parse 
tree T, each leaf in T corresponds to a single word 
in f and each sub-tree of T exactly covers a phrase 
fi in f which is called as linguistic phrase.  Except 
linguistic phrases, any other phrase is regarded as 
non-linguistic phrase. The height of phrase fi is 
defined as the distance between the root node of T 
and the root node of the maximum sub-tree which 
exactly covers fi. For example, in Figure 1 the 
phrase ???? has the maximum sub-tree rooting 
at ADJP and its height is 3. The height of phrase 
??? is 4 since its maximum sub-tree roots at 
ADBP instead of AD. If two adjacent phrases have 
the same height, we regard them as peer phrases.  
In our model, we make use of bilingual phrases 
as well, which refer to source-target algned phrase 
pairs extracted using the same criterion as most 
phrase-based systems (Och and Ney, 2004). 
3.2 Model 
Similar to the work in Chiang (2005), our transla-
tion model can be formulated as a weighted syn-
chronous context free grammar derivation process. 
Let D be a derivation that generates a bilingual 
sentence pair ?f, e?, in which f is the given source 
sentence, the statistical model that is used to pre-
dict the translation probability p(e|f) is defined over 
Ds as follows: 
? ? ? ? ? ? ? ???  ? 
???
?  ?? ? ? ??,?? 
??
???? ,????
 
?
 
where plm(e) is the language model, ?i(X ???,??) 
is a feature function defined over the derivation 
rule X???,??, and ?i is its weight.  
Although theoretically it is ideal for translation 
reorder modeling by constructing a synchronous 
context free grammar based on bilingual linguistic 
parsing trees, it is generally a very difficult task in 
practice. In this work we propose to use a small 
synchronous grammar constructed on the basis of 
bilingual phrases to model translation reorder 
probability and constraints by referring to the 
source syntactic parse trees. In the grammar, the 
source / target words serve as terminals, and the 
bilingual phrases and combination of bilingual 
phrases are presented with non-terminals. There 
are two non-terminals in the grammar except the 
start symbol S: Y and Z. The general derivation 
rules are defined as follows: 
a) Derivations from non-terminal to non-
terminals are restricted to binary branching 
forms; 
b) Any non-terminals that derives a list of termin-
als, or any combination of two non-terminals, 
if the resulting source string won?t cause any 
cross-bracketing problems in the source parse 
tree (it exactly corresponds to a linguistic 
phrase in binary parse trees), are reduced to Y; 
c) Otherwise, they are reduced to Z. 
Table 1 shows a complete list of derivation rules 
in our synchronous context grammar. The first nine 
grammar rules are used to constrain phrase reor-
535
dering during phrase combination. The last two 
rules are used to represent bilingual phrases. Rule 
(10) is the start grammar rule to generate the entire 
sentence translation.  
 
Rule Name Rule Content 
Rule (1) Y??Y1Y2, Y1Y2? 
Rule (2) Y??Y1Y2, Y2Y1? 
Rule (3) Y??Z1Z2, Z1Z2? 
Rule (4) Y??Y1Z2,Y1Z2? 
Rule (5) Y??Z1Y2, Z1Y2? 
Rule (6) Z??Y1Z2, Y1Z2? 
Rule (7) Z??Z1Y2, Z1Y2? 
Rule (8) Z??Z1Z2, Z1Z2? 
Rule (9) Z??Y1Y2, Y1Y2? 
Rule (10) S??Y1,Y1? 
Rule (11) Z??Z1, Z1? 
Rule (12) Y??Y1,Y1? 
 
Table 1: Synchronous grammar rules 
 
Rule (1) and Rule (2) are only applied to two ad-
jacent peer phrases. Note that, according to the 
constraints of foreign syntactic structures, only 
Rule (2) among all rules in Table 1 can be applied 
to conduct non-monotone phrase reordering in our 
framework. This can avoid arbitrary phrase reor-
dering. For example, as shown in Figure 1, Rule (1) 
is applied to the monotone combination of phrases 
???? and ???, and Rule (2) is applied to the 
non-monotone combination of phrases ???/?? 
and ??? /???. However, the non-monotone 
combination of ??? and ???? is not allowed in 
our method since there is no proper rule for it.  
Non-linguistic phrases are involved in Rule 
(3)~(9). We do not allow these grammar rules for 
non-monotone combination of non-peer phrases, 
which really harm the translation results as proved 
in experimental results. Although these rules vi-
olate the syntactic constraints, they not only pro-
vide the option to leverage non-linguistic transla-
tion knowledge to avoid syntactic errors but also 
take advantage of phrase local reordering capabili-
ties. Rule (3) and Rule (8) are applied to the com-
bination of two adjacent non-linguistic phrases. 
Rule (4)~(7) deal with the situation where one is a 
linguistic phrase and the other is a non-linguistic 
phrase. Rule (9) is applied to the combination of 
two adjacent linguistic phrases but their combina-
tion result is not a linguistic phrase.  
Rule (11) and Rule (12) are applied to generate 
bilingual phrases learned from training corpus. 
Table 2 demonstrates an example how these 
rules are applied to translate the foreign sentence 
???/?/??/??? into the English sentence 
?the significant appreciation of the Euro?. 
 
Step Partial derivations Rule 
1 S??Y1, Y1?   (10) 
2 ??Y2Y3, Y3Y2? (2) 
3 ??Y4Y5Y3, Y3Y5Y4? (2) 
4 ???? Y5Y3, Y3Y5 the Euro? (12) 
5 ???? ? Y3, Y3 of the Euro? (12) 
6 ???? ? Y6Y7, Y6Y7 of the Euro? (1) 
7 ???? ? ?? Y7, the significant 
Y7 of  the Euro? 
(12) 
8 ???? ? ?? ??, the signifi-
cant appreciation of  the Euro? 
(12) 
 
Table 2: Example of application for rules  
 
However, there are always other kinds of bilin-
gual phrases extracted directly from training cor-
pus, such as ???, the Euro? and ?? ?? ?
?, ?s significant appreciation?, which can produce 
different candidate sentence translations. Here, the 
phrase ?? ?? ??? is a non-linguistic phrase. 
The above derivations can also be rewritten as 
S??Y1, Y1???Y2Z3,Y2Z3??? ?? Z3, the Euro 
Z3?????? ?? ??, the Euro ?s significant 
appreciation?, where Rule (10), (4), (12) and (11) 
are applied respectively. 
3.3 Features 
Similar to the default features in Pharaoh (Koehn, 
Och and Marcu 2003), we used following features 
to estimate the weight of our grammar rules. Note 
536
that different rules may have different features in 
our model. 
? The lexical weights plex(?|?) and plex(?|?) esti-
mating how well the words in ? translate the 
words in ?. This feature is only applicable to 
Rule (11) and Rule (12). 
? The phrase translation weights pphr(?|?) and 
pphr(?|?) estimating how well the terminal 
words of ? translate the terminal words of ?, 
This feature is only applicable to Rule (11) and 
Rule (12). 
? A word penalty exp(|?|), where |?| denotes the 
count of terminal words of ?. This feature is 
only applicable to Rule (11) and Rule (12). 
? A penalty exp(1) for grammar rules analogous 
to Pharaoh?s penalty which allows the model to 
learn a preference for longer or shorter deriva-
tions. This feature is applicable to all rules in 
Table 1. 
? Score for applying the current rule. This feature 
is applicable to all rules in Table 1. We will ex-
plain the score estimation in detail in Section 
3.4. 
3.4 Scoring of Rules 
Based on the syntax constraints and involved non-
terminal types, we separate the grammar rules into 
three groups to estimate their application scores 
which are also treated as reordering probabilities.  
For Rule (1) and Rule (2), they strictly comply 
with the syntactic structures. Given two peer 
phrases, we have two choices to use one of them. 
Thus, we use maximum entropy (ME) model algo-
rithm to estimate their reordering probabilities sep-
arately, where the boundary words of foreign 
phrases and candidate target translation phrases, 
POS information and dependencies are integrated 
as features. As listed in Table 3, there are totally 
twelve categories of features used to train the ME 
model. In fact, the probability of Rule (1) is just 
equal to the supplementary probability of Rule (2), 
and vice versa. 
For Rule (3)~(9), according to the syntactic 
structures, their application is determined since 
there is only one choice to complete reordering, 
which is similar to the ?glue rules? in Chiang 
(2005). Due to the appearance of non-linguistic 
phrases, non-monotone phrase reordering is not 
allowed in these rules. We just assign these rules a 
constant score trained using our implementation of 
Minimum Error Rate Training (Och, 2003b), 
which is 0.7 in our system. 
For Rule (10)~(12), they are also determined 
rules since there is no other optional rules compet-
ing with them. Constant score is simply assigned to 
them as well, which is 1.0 in our system. 
 
Fea. Description 
LS1 First word of first foreign phrase 
LS2 First word of second foreign phrase 
RS1 Last word of first foreign phrase 
RS2 Last word of second foreign phrase 
LT1 First word of first target phrase 
LT2 First word of second target phrase 
RT1 Last word of first target phrase 
RT2 Last word of second target phrase 
LPos 
POS of the node covering first foreign 
phrase 
RPos 
POS of the node covering second foreign 
phrase  
Cpos 
POS of the node covering the combina-
tion of foreign phrases 
DP 
Dependency between the nodes covering 
two single foreign phrases respectively 
 
Table 3: Feature categories used for ME model 
4 The Generalization of Reordering 
Knowledge 
4.1 Enriching Parse Trees 
The grammar rules proposed in Section 3 are only 
applied to binary syntax tree nodes. For n-ary syn-
tax trees (n>2), some modification is needed to 
generate more peer phrases. As shown in Figure 
2(a), the syntactic tree of Chinese sentence ???
? /???? /?? /?? ? (Guangdong/high-
tech/products/export), parsed by the Stanford Pars-
er (Klein, 2003), has a 3-ary sub-tree. Referring to 
its English translation result ?export of high-tech 
products in Guangdong?, we understand there 
should be a non-monotone combination between 
the phrases ????? and ?????/???. How-
ever, ?????/??? is not a linguistic phrase 
537
though its component phrases ?????? and ??
?? are peer phrases. To avoid the conflict with the 
Rule (2), we just add some extra virtual nodes in 
the n-ary sub-trees to make sure that only binary 
sub-trees survive in the modified parse tree. Figure 
2(b) is the modification result of the syntactic tree 
from Figure 2(a), where two virtual nodes with the 
new distinguishable POS of M are added.  
In general, we add virtual nodes for each set of 
the continuous peer phrases and let them have the 
same height. Thus, for a n-ary sub-tree, there are 
? ?? ?11 )(ni in
= (n?1)2/2 virtual nodes being added 
where n>2. The phrases exactly covered by the 
virtual nodes are called as virtual peer phrases.  
 
 
 
 
 
 
 
 
 
 
 
 
 
Figure 2: Example of syntax tree modification 
4.2 Combination of Parse Trees 
It is well known that parse errors in syntactic trees 
always are inescapable even if the state-of-the-art 
parser is used.  Incorrect syntactic knowledge may 
harm the reordering probability estimation. To mi-
nimize the impact of parse error of a single tree, 
more parse trees are introduced. To support the 
combination of parse trees, the synchronous 
grammar rules are applied independently, but they 
will compete against each other with the effect of 
other models such as language model. 
In our system, we combine the parse trees gen-
erated respectively by Stanford parser (Klein, 2003) 
and a dependency parser developed by (Zhou, 
2000). Compared with the Stanford parser, the de-
pendency parser only conducts shallow syntactic 
analysis. It is powerful to identify the base NPs and 
base VPs and their dependencies. Additionally, 
dependency parser runs much faster. For example, 
it took about three minutes for the dependency 
parser to parse one thousand sentences with aver-
age length of 25 words, but the Stanford parser 
needs about one hour to complete the same work. 
More importantly, as shown in the experimental 
results, the dependency parser can achieve the 
comparable quality of final translation results with 
Stanford parser in our system.  
5 The Decoder 
We developed a CKY style decoder to complete 
the sentence translation. A two-dimension array 
CA is constructed to store all the local candidate 
phrase translation and each valid cell CAij in CA 
corresponds to a foreign phrase where i is the 
phrase start position and j is the phrase end posi-
tion. The cells in CA are filled in a bottom-up way. 
Firstly we fill in smaller cells with the translation 
in bilingual phrases learned from corpus. Then the 
candidate translation in the larger cell CAij is gen-
erated based on the content in smaller adjacent 
cells CAik and CAk+1j with the monotone combina-
tion and non-monotone combination, where i?k?j. 
To reduce the cost of system resources, the well 
known pruning methods, such as histogram prun-
ing, threshold pruning and recombination, are used 
to only keep the top N candidate translation in each 
cell.  
6 Training 
Similar to most state-of-the-art phrase-based SMT 
systems, we use the SRI toolkit (Stolcke, 2002) for 
language model training and Giza++ toolkit (Och 
and Ney, 2003) for word alignment. For reordering 
model training, two kinds of parse trees for each 
foreign sentence in the training corpus were ob-
tained through the Stanford parser (Klein, 2003) 
and a dependency parser (Zhou, 2000). After that, 
we picked all the foreign linguistic phrases of the 
same sentence according to syntactic structures. 
Based on the word alignment results, if the aligned 
target words of any two adjacent foreign linguistic 
phrases can also be formed into two valid adjacent 
phrase according to constraints proposed in the 
phrase extraction algorithm by Och (2003a), they 
will be extracted as a reordering training sample. 
Finally, the ME modeling toolkit developed by 
Zhang (2004) is used to train the reordering model 
over the extracted samples. 
?? 
NP 
NP 
NP 
NN 
NP 
NR JJ 
ADJP 
NN 
NP 
??
? 
?? 
M M 
?? 
NP 
NP 
NP 
NN 
NP 
NR JJ 
ADJP 
NN 
NP 
??
? 
??
?? 
?? 
(a) (b) 
??
?? 
538
7 Experimental Results and Analysis 
We conducted our experiments on Chinese-to-
English translation task of NIST MT-05 on a 
3.0GHz system with 4G RAM memory. The bilin-
gual training data comes from the FBIS corpus. 
The Xinhua news in GIGAWORD corpus is used 
to train a four-gram language model. The devel-
opment set used in our system is the NIST MT-02 
evaluation test data.  
For phrase extraction, we limit the maximum 
length of foreign and English phrases to 3 and 5 
respectively. But there is no phrase length con-
straint for reordering sample extraction. About 
1.93M and 1.1M reordering samples are extracted 
from the FBIS corpus based on the Stanford parser 
and the dependency parser respectively. To reduce 
the search space in decoder, we set the histogram 
pruning threshold to 20 and relative pruning thre-
shold to 0.1.  
In the following experiments, we compared our 
system performance with that of the other state-of-
the-art systems. Additionally, the effect of some 
strategies on system performance is investigated as 
well. Case-sensitive BLEU-4 score is adopted to 
evaluate system performance.  
7.1 Comparing with Baseline SMT system 
Our baseline system is Pharaoh (Koehn, 2004). 
Xiong?s system (Xiong, et al, 2006) which used 
ME model to train the reordering model is also 
regarded as a competitor. To have a fair compari-
son, we used the same language model and transla-
tion model for these three systems. The experimen-
tal results are showed in Table 4. 
 
System Bleu Score 
Pharaoh 0.2487 
Xiong?s System 0.2616 
Our System 0.2737 
Table 4: Performance against baseline system 
 
These three systems are the same in that the fi-
nal sentence translation results are generated by the 
combination of local phrase translation. Thus, they 
are capable of local reordering but not global reor-
dering. The phrase reordering in Pharaoh depends 
only on distance distortion information which does 
not contain any linguistic knowledge. The experi-
mental result shows that the performance of both 
Xiong?s system and our system is better than that 
of Pharaoh. It proves that linguistic knowledge can 
help the global reordering probability estimation. 
Additionally, our system is superior to Xiong?s 
system in which only use phrase boundary words 
to guide global reordering. It indicates that syntac-
tic knowledge is more powerful to guide global 
reordering than boundary words. On the other hand, 
it proves the importance of syntactic knowledge 
constraints in avoiding the arbitrary phrase reorder-
ing.  
7.2 Syntactic Error Analysis 
Rule (3)~(9) in Section 3 not only play the role to 
compensate for syntactic errors, but also take the 
advantage of the capability of capturing local 
phrase reordering. However, the non-monotone 
combination for non-peer phrases is really harmful 
to system performance. To prove these ideas, we 
conducted experiments with different constrains.  
 
Constraints Bleu Score 
All rules in Table 1 used  0.2737 
Allowing the non-monotone 
combination of non-peer phrases 
0.2647 
Rule (3)~(9) are prohibited 0.2591 
Table 5:  About non-peer phrase combination 
 
From the experimental results shown in Table 5, 
just as claimed in other previous work, the combi-
nation between non-linguistic phrases is useful and 
cannot be abandoned. On the other hand, if we re-
lax the constraint of non-peer phrase combination 
(that is, allowing non-monotone combination for 
on-peer phrases), some more serious errors in non-
syntactic knowledge is introduced, thereby degrad-
ing performance from 0.2737 to 0.2647. 
7.3 Effect of Virtual Peer Phrases 
As discussed in Section 4, for n-ary nodes (n>2) in 
the original syntax trees, the relationship among n-
ary sub-trees is always not clearly captured. To 
give them the chance of free reordering, we add the 
virtual peer nodes to make sure that the combina-
tion of a set of peer phrases can still be a peer 
phrase. An experiment was done to compare with 
the case where the virtual peer nodes were not 
added to n-ary syntax trees. The Bleu score 
539
dropped to 26.20 from 27.37, which shows the vir-
tual nodes have great effect on system performance. 
7.4 Effect of Mixed Syntax Trees 
In this section, we conducted three experiments to 
investigate the effect of constituency parse tree and 
dependency parse tree. Over the same platform, we 
tried to use only one of them to complete the trans-
lation task. The experimental results are shown in 
Table 6.  
Surprisingly, there is no significant difference in 
performance. The reason may be that both parsers 
produce approximately equivalent parse results. 
However, the combination of syntax trees outper-
forms merely only one syntax tree. This suggests 
that the N-best syntax parse trees may enhance the 
quality of reordering model. 
 
Situation Bleu Score 
Dependency parser only 0.2667 
Stanford parser only 0.2670 
Mixed parsing trees 0.2737 
 
Table 6: Different parsing tree 
8 Conclusion and Future Work 
In this paper, syntactic knowledge is introduced 
to capture global reordering of SMT system. This 
method can not only inherit the advantage of local 
reordering ability of standard phrase-based SMT 
system, but also capture the global reordering as 
the syntax-based SMT system. The experimental 
results showed the effectiveness of our method. 
In the future work, we plan to improve the reor-
dering model by introducing N-best syntax trees 
and exploiting richer syntactic knowledge. 
References 
David Chiang. 2005. A hierarchical phrase-based mod-
el for statistical machine translation. In Proceedings 
of ACL 2005. 
Franz Josef Och. 2003a. Statistical Machine Translation: 
From Single-Word Models to Alignment Templates 
Thesis. 
Franz Josef Och. 2003b. Minmum Error Rate Training 
in Statistical Machine Translation. In Proceedings 
for ACL 2003. 
Franz Josef Och, Hermann Ney. 2003. A Systematic 
Comparison of Various Statistical Alignment Models. 
Computational Linguistics, 29:19-51. 
Franz Josef Och and Hermann Ney. 2004. The align-
ment template approach to statistical machine trans-
lation. Computational Linguistics, 30:417-449. 
Dan Klein and Christopher D. Manning. 2003. Accurate 
Unlexicalized Parsing. In Proceedings of ACL 2003. 
Philipp Koehn, Franz Joseph Och, and Daniel Marcu. 
2003. Statistical Phrase-Based Translation. In Pro-
ceedings of HLT/NAACL 2003. 
Philipp Koehn. 2004. Pharaoh: a Beam Search Decoder 
for Phrased-Based Statistical Machine Translation 
Models. In Proceedings of AMTA 2004. 
Shankar Kumar and William Byrne. 2005. Local phrase 
reordering models for statistical machine translation. 
In Proceedings of HLT-EMNLP 2005. 
Yang Liu, Qun Liu, Shouxun Lin. 2006. Tree-to-String 
Alignment Template for Statistical Machine Transla-
tion. In Proceedings of COLING-ACL 2006. 
Chris Quirk, Arul Menezes, and Colin Cherry. 2005. 
Dependency treelet translation: Syntactically in-
formed phrasal SMT. In Proceedings of ACL 2005. 
Andreas Stolcke. 2002. SRILM-An Extensible Language 
Modeling Toolkit. In Proceedings of ICSLP 2002. 
Christoph Tillmann. 2004. A block orientation model 
for statistical machine translation. In Proceedings of 
HLT-NAACL 2004. 
Dekai Wu. 1996. A Polynomial-Time Algorithm for Sta-
tistical Machine Translation. In Proceedings of ACL 
1996. 
Deyi Xiong, Qun Liu, and Shouxun Lin. 2006. Maxi-
mum Entropy Based Phrase Reordering Model for 
Statistical Machine Translation. In Proceedings of 
COLING-ACL 2006. 
Kenji Yamada and Kevin Knight. 2001. A syntax based 
statistical translation model. In Proceedings of ACL 
2001. 
Le Zhang. 2004. Maximum Entropy Modeling Toolkit 
for Python and C++. Available at http://homepa 
ges.inf.ed.ac.uk/s0450736/maxent_toolkit.html. 
R. Zens, H. Ney, T. Watanabe, and E. Sumita. 2004. 
Reordering Constraints for Phrase-Based Statistical 
Machine Translation. In Proceedings of CoLing 2004. 
Ming Zhou. 2000. A block-based robust dependency 
parser for unrestricted Chinese text. The second 
Chinese Language Processing Workshop attached to 
ACL2000. 
540
Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 362?370,
Singapore, 6-7 August 2009. c?2009 ACL and AFNLP
Better Synchronous Binarization for Machine Translation 
 
 
Tong Xiao*, Mu Li+, Dongdong Zhang+, Jingbo Zhu*, Ming Zhou+ 
 
*Natural Language Processing Lab 
Northeastern University 
Shenyang, China, 110004 
xiaotong@mail.neu.edu.cn 
zhujingbo@mail.neu.edu.cn 
 
+Microsoft Research Asia 
Sigma Center 
Beijing, China, 100080 
muli@microsoft.com 
dozhang@microsoft.com 
mingzhou@microsoft.com 
 
  
 
Abstract 
Binarization of Synchronous Context Free 
Grammars (SCFG) is essential for achieving 
polynomial time complexity of decoding for 
SCFG parsing based machine translation sys-
tems. In this paper, we first investigate the 
excess edge competition issue caused by a left-
heavy binary SCFG derived with the method 
of Zhang et al (2006). Then we propose a new 
binarization method to mitigate the problem 
by exploring other alternative equivalent bi-
nary SCFGs. We present an algorithm that ite-
ratively improves the resulting binary SCFG, 
and empirically show that our method can im-
prove a string-to-tree statistical machine trans-
lations system based on the synchronous bina-
rization method in Zhang et al (2006) on the 
NIST machine translation evaluation tasks. 
1 Introduction 
Recently Statistical Machine Translation (SMT) 
systems based on Synchronous Context Free 
Grammar (SCFG) have been extensively investi-
gated (Chiang, 2005; Galley et al, 2004; Galley 
et al, 2006) and have achieved state-of-the-art 
performance. In these systems, machine transla-
tion decoding is cast as a synchronous parsing 
task. Because general SCFG parsing is an NP-
hard problem (Satta and Peserico, 2005), practic-
al SMT decoders based on SCFG parsing re-
quires an equivalent binary SCFG that is directly 
learned from training data to achieve polynomial 
time complexity using the CKY algorithm (Ka-
sami, 1965; Younger, 1967) borrowed from CFG 
parsing techniques. Zhang et al (2006) proposed 
synchronous binarization, a principled method to 
binarize an SCFG in such a way that both the 
source-side and target-side virtual non-terminals 
have contiguous spans. This property of syn-
chronous binarization guarantees the polynomial 
time complexity of SCFG parsers even when an 
n-gram language model is integrated, which has 
been proved to be one of the keys to the success 
of a string-to-tree syntax-based SMT system. 
However, as shown by Chiang (2007), SCFG-
based decoding with an integrated n-gram lan-
guage model still has a time complexity of  
?(?3 ? 4(??1)), where m is the source sentence 
length, and  ?  is the vocabulary size of the lan-
guage model. Although it is not exponential in 
theory, the actual complexity can still be very 
high in practice. Here is an example extracted 
from real data. Given the following SCFG rule: 
     VP   ?   VB  NP  ?  JJR  , 
               VB  NP  will be  JJR 
we can obtain a set of equivalent binary rules 
using the synchronous binarization method 
(Zhang et al, 2006)  as follows: 
        VP ? V1  JJR ,   V1  JJR 
            V1 ? VB  V2 ,   VB  V2 
        V2 ? NP ? ,   NP  will be 
This binarization is shown with the solid lines as 
binarization (a) in Figure 1. We can see that bi-
narization (a) requires that ?NP ?? should be 
reduced at first. Data analysis shows that ?NP ?? 
is a frequent pattern in the training corpus, and 
there are 874 binary rules of which the source 
language sides are ?NP ??. Consequently these 
binary rules generate a large number of compet-
ing edges in the chart when ?NP ?? is matched 
in decoding. To reduce the number of edges pro-
362
posed in decoding, hypothesis re-combination is 
used to combine the equivalent edges in terms of 
dynamic programming. Generally, two edges can 
be re-combined if they satisfy the following two 
constraints:  1) the LHS (left-hand side) non-
terminals are identical and the sub-alignments 
are the same (Zhang et al, 2006); and 2) the 
boundary words 1  on both sides of the partial 
translations are equal between the two edges 
(Chiang, 2007). However, as shown in Figure 2, 
the decoder still generates 801 edges after the 
hypothesis re-combination. As a result, aggres-
sive pruning with beam search has to be em-
ployed to reduce the search space to make the 
decoding practical. Usually in beam search only 
a very small number of edges are kept in the 
beam of each chart cell (e.g. less than 100). 
These edges have to compete with each other to 
survive from the pruning. Obviously, more com-
peting edges proposed during decoding can lead 
to a higher risk of making search errors.  
 
VB NP ? JJR
(a)(b)
V2
V1
V2'
V1'
VP
VB NP will be JJR
 
Figure 1: Two different binarizations (a) and 
(b) of the same SCFG rule distinguished by the 
solid lines and dashed lines 
 
??   ??   ??   ?   ?? ?
(We hope the situation will be better .)
??   ??   NP   ?   JJR   ?
decoding
match 874 rules match 62 rules
competing edges: 801 competing edges: 57
Figure 2: Edge competitions caused by different 
binarizations 
 
The edge competition problem for SMT de-
coding is not addressed in previous work (Zhang 
et al, 2006; Huang, 2007) in which each SCFG 
rule is binarized in a fixed way. Actually the re-
sults of synchronous binarization may not be the 
only solution. As illustrated in Figure 1, the rule 
                                                 
1 For the case of n-gram language model integration, 
2 ? (? ? 1) boundary words needs to be examined. 
can also be binarized as binarization (b) which is 
shown with the dashed lines.  
We think that this problem can be alleviated 
by choosing better binarizations for SMT decod-
ers, since there is generally more than one bina-
rization for a SCFG rule. In our investigation, 
about 96% rules that need to be binarized have 
more than one binarization under the contiguous 
constraint. As shown in binarization (b) (Figure 
1), ?? JJR? is reduced first. In the decoder, the 
number of binary rules with the source-side ?? 
JJR? is 62, and the corresponding number of 
edges is 57 (Figure 2). The two numbers are both 
much smaller than those of ?NP ?? in (a). This 
is an informative clue that the binarization (b) 
could be better than the binarization (a) based on 
the following: the probability of pruning the rule 
in (a) is higher than that in (b) as the rule in (b) 
has fewer competitors and has more chances to 
survive during pruning. 
In this paper we propose a novel binarization 
method, aiming to find better binarizations to 
improve an SCFG-based machine translation 
system. We formulate the binarization optimiza-
tion as a cost reduction process, where the cost is 
defined as the number of rules sharing a common 
source-side derivation in an SCFG. We present 
an algorithm, iterative cost reduction algorithm, 
to obtain better binarization for the SCFG learnt 
automatically from the training corpus. It can 
work with an efficient CKY-style binarizer to 
search for the lowest-cost binarization. We apply 
our method into a state-of-the-art string-to-tree 
SMT system. The experimental results show that 
our method outperforms the synchronous binari-
zation method (Zhang et al, 2006) with over 0.8 
BLEU scores on both NIST 2005 and NIST 2008 
Chinese-to-English evaluation data sets. 
2 Related Work 
The problem of binarization originates from the 
parsing problem in which several binarization 
methods are studied such as left/right binariza-
tion (Charniak et al, 1998; Tsuruoka and Tsujii, 
2004) and head binarization (Charniak et al, 
2006). Generally, the pruning issue in SMT de-
coding is unnecessary for the parsing problem, 
and the accuracy of parsing does not rely on the 
binarization method heavily. Thus, many efforts 
on the binarization in parsing are made for the 
efficiency improvement instead of the accuracy 
improvement (Song et al, 2008). 
Binarization is also an important topic in the 
research of syntax-based SMT. A synchronous 
363
binarization method is proposed in (Zhang et al, 
2006) whose basic idea is to build a left-heavy 
binary synchronous tree (Shapiro and Stephens, 
1991) with a left-to-right shift-reduce algorithm. 
Target-side binarization is another binarization 
method which is proposed by Huang (2007). It 
works in a left-to-right way on the target lan-
guage side. Although this method is compara-
tively easy to be implemented, it just achieves 
the same performance as the synchronous binari-
zation method (Zhang et al, 2006) for syntax-
based SMT systems. In addition, it cannot be 
easily integrated into the decoding of some syn-
tax-based models (Galley et al, 2004; Marcu et 
al., 2006), because it does not guarantee conti-
guous spans on the source language side. 
3 Synchronous Binarization Optimiza-
tion by Cost Reduction 
As discussed in Section 1, binarizing an SCFG in 
a fixed (left-heavy) way (Zhang et al, 2006) may 
lead to a large number of competing edges and 
consequently high risk of making search errors. 
Fortunately, in most cases a binarizable SCFG 
can be binarized in different ways, which pro-
vides us with an opportunity to find a better solu-
tion than the default left-heavy binarization. An 
ideal solution to this problem could be that we 
define an exact edge competition estimation 
function and choose the best binary SCFG based 
on it. However, even for the rules with a com-
mon source-side, generally it is difficult to esti-
mate the exact number of competing edges in the 
dynamic SCFG parsing process for machine 
translation, because in order to integrate an n-
gram language model, the actual number of 
edges not only depends on SCFG rules, but also 
depends on language model states which are spe-
cific to input sentences. Instead, we have to em-
ploy certain kinds of approximation of it. First 
we will introduce some notations frequently used 
in later discussions. 
3.1 Notations 
We use ? = {?? ?  ?? ? ?? ,??}  to denote an 
SCFG, where ??  is the ?
??  rule in ? ; ??  is the 
LHS (left hand side) non-terminal of ?? ; ??  and 
??  are the source-side and target-side RHS (right 
hand side) derivations of ??  respectively. We use 
? ?  to denote the set of equivalent binary 
SCFG of ?. The goal of SCFG binarization is to 
find an appropriate binary SCFG ?? ? ? ? . For 
?? , ? ?? = {??? } ? ?? ? ? ?  is the set of 
equivalent binary rules based on ?? , where ???  is 
the ???  binary rule in ? ?? . Figure 3 illustrates 
the meanings of these notations with a sample 
grammar. 
 
VP ?  VB NP ? JJR  ,   VB NP will be JJR
S   ?  NP ? VP  ,           NP will VP
R1 :
R2 :
G
VP ? V
12
 JJR ,    V
12
 JJR
 (R1)
G? 
V
12
 ? VB V
13
 ,     VB V
13
V
13
 ? NP ? ,       NP  will be
v
11 
:
v
12 
:
v
13 
:
S   ? V
22
 VP ,      V
22
 VP
V
22
 ? NP ? ,      NP will
v
21 
:
v
22 
:
 (R2)
binarization
...
v
11 
v
12 
v
22 
S(?VB NP ? JJR ?, G?) S(?VB NP ??, G?) S(?NP ??, G?)
L(v12)=?VB NP ??
v
13 
rule bucket
 
 
Figure 3: Binarization on a sample grammar 
 
The function ?(?) is defined to map a result-
ing binary rule ??? ??? to the sub-sequence in ??  
derived from ??? . For example, as shown in Fig-
ure 3, the binary rule ?13 covers the source sub-
sequence ?NP ?? in ?1 , so ? ?13 = "NP ?". 
Similarly, ? ?12 = "VB NP ?".  
The function ?(?) is used to group the rules in 
?? with a common right-hand side derivation for 
source language. Given a binary rule ? ? ??, we 
can put it into a bucket in which all the binary 
rules have the same source sub-sequence ?(?). 
For example (Figure 3), as ? ?12 = "VB NP ?", 
?12 is put into the bucket indexed by ?VB NP ??. 
And ?13  and ?22  are put into the same bucket, 
since they have the same source sub-sequence 
?NP ??. Obviously, ?? can be divided into a set 
of mutual exclusive rule buckets by ?(?). 
In this paper, we use ?(?(?),??) to denote the 
bucket for the binary rules having the source sub-
sequence ?(?). For example, ?("?? ?",??) de-
notes the bucket for the binary rules having the 
source-side ?NP ??. For simplicity, we also use 
?(?,??) to denote ? ? ? ,?? .  
3.2 Cost Reduction for SCFG Binarization 
Given a binary SCFG ??, it can be easily noticed 
that if a rule ? in  the bucket ?(?,??) can be ap-
plied to generate one or more new edges in 
SCFG parsing, any other rules in this bucket can 
also be applied because all of them can be re-
duced from the same underlying derivation ?(?). 
364
Each application of other rules in the bucket 
?(?,??) can generate competing edges with the 
one based on ? . Intuitively, the size of bucket 
can be used to approximately indicate the actual 
number of competing edges on average, and re-
ducing the size of bucket could help reduce the 
edges generated in a parsing chart by applying 
the rules in the bucket. Therefore, if we can find 
a method to greedily reduce the size of each 
bucket ?(?,??), we can reduce the overall ex-
pected edge competitions when parsing with ??. 
However, it can be easily proved that the 
numbers of binary rules in any ?? ? ? ?  are 
same, which implies that we cannot reduce the 
sizes of all buckets at the same time ? removing 
a rule from one bucket means adding it to anoth-
er. Allowing for this fact, the excess edge com-
petition example shown in Section 1 is essential-
ly caused by the uneven distribution of rules 
among different buckets ? ? . Accordingly, our 
optimization objective should be a more even 
distribution of rules among buckets. 
In the following, we formally define a metric 
to model the evenness of rule distribution over 
buckets. Given a binary SCFG ?? and a binary 
SCFG rule ? ? ?? , ?(?) is defined as the cost 
function that maps ?  to the size of the bucket  
? ?,?? : 
? ? =  ? ?,??   (1) 
Obviously, all the binary rules in ? ?,??  share a 
common cost value  ? ?,??  . For example (Fig-
ure 3), both ?13  and ?22  are put into the same 
bucket ? "?? ?",?? , so ? ?13 = ? ?22 = 2. 
The cost of the SCFG ??  is computed by 
summing up all the costs of SCFG rules in it: 
? ?? = ?(?)
??? ?
 (2) 
Back to our task, we are to find an equivalent 
binary SCFG ??  of ?  with the lowest cost in 
terms of the cost function ?(. ) given in Equation 
(2): 
?? = argmin???? ? ?(??) (3) 
Next we will show how ??  is related to the 
evenness of rule distribution among different 
buckets. Let ? ?? = {?1,? , ??}  be the set of 
rule buckets containing rules in ??, then the value 
of ?(??) can also be written as: 
? ?? =  ?? 
2
1????
 (4) 
Assume ?? =  ??  is an empirical distribution of a 
discrete random variable ?, then the square devi-
ation of the empirical distribution is: 
?2 =
1
?
 ( ?? ? ? )
2
?
 (5) 
Noticing that ? ?? =  ?
?   and ? =  ?? /?, Equ-
ation (5) can be written as: 
?2 =
1
?
 ? ? ? ?
 ?? 2
?
  (6) 
Since both ? and |??| are constants, minimizing 
the cost function ?(??) is equivalent to minimiz-
ing the square deviation of the distribution of 
rules among different buckets. A binary SCFG 
with the lower cost indicates the rules are more 
evenly distributed in terms of derivation patterns 
on the source language side. 
3.3 Static Cost Reduction 
Before moving on discussing the algorithm 
which can optimize Equation (3) based on rule 
costs specified in Equation (1), we first present 
an algorithm to find the optimal solution to Eq-
uation (3) if we have known the cost setting of 
?? and can use the costs as static values during 
binarization. Using this simplification, the prob-
lem of finding the binary SCFG  ?? with minim-
al costs can be reduced to find the optimal bina-
rization ??(??) for each rule ??  in ?. 
To obtain ??(??) , we can employ a CKY-
style binarization algorithm which builds a com-
pact binarization forest for the rule ??  in bottom-
up direction. The algorithm combines two adja-
cent spans of ??  each time, in which two spans 
can be combined if and only if they observe the 
BTG constraints? their translations are either 
sequentially or reversely adjacent in ?? , the tar-
get-side derivation of ?? . The key idea of this 
algorithm is that we only use the binarization tree 
with the lowest cost of each span for later com-
bination, which can avoid enumerating all the 
possible binarization trees of ??  using dynamic 
programming. 
Let ??
?
 be the sub-sequence spanning from p 
to q on the source-side, ?[?, ?] be optimal bina-
rization tree spanning ??
?
, ??[?, ?] be the cost of 
?[?, ?], and ?? [?, ?] be the cost of any binary 
rules whose source-side is ??
?
, then the cost of 
optimal binarization tree spanning ??
?
 can be 
computed as: 
??[?, ?] = min
??????1
(?? [?, ?] + ??[?,?] + ??[? + 1, ?]) 
365
The algorithm is shown as follows: 
CYK-based binarization algorithm 
Input: a SCFG rule ??  and the cost function ?(. ).  
Output: the lowest cost binarization on ??  
1:  Function CKYBINARIZATION(?? , ?) 
2:      for l = 2 to n do  ?  Length of span 
3:        for p = 1 to n ? l + 1 do ?  Start of span 
4:               q = p + l  ?  End of span 
5:             for k = p to q ? 1 do ?  Partition of span  
6:               if not CONSECUTIVE(? ?, ? , ? ? + 1,? )  
                         then next loop 
7:                   ?? [?, ?] ? ?(??
?)    
8:                   curCost ? ?? ?, ? +?? ?, ? +??[? + 1,?] 
9:                 if curCost  <  minCost then 
10:                   minCost ? curCost 
11:                    ?[?, ?] ? COMBINE(?[?, ?], ?[? + 1,?]) 
12:             ?? ?, ?  ? minCost 
13:    return ?[1,?]     
14: Function CONSECUTIVE(( a, b), (c, d)) 
15:    return (b = c ? 1) or (d = a ? 1)   
where n is the number of tokens (consecutive 
terminals are viewed as a single token) on the 
source-side of ?? . COMBINE(?[?, ?], ?[? + 1,?]) 
combines the two binary sub-trees into a larger 
sub-tree over ??
?
. ? ?, ? = (?, ?) means that the 
non-terminals covering ??
?
 have the consecutive 
indices ranging from a to b on the target-side. If 
the target non-terminal indices are not consecu-
tive, we set ? ?, ? = (?1,?1). ? ??
?
 = ?(??) 
where ?? is any rule in the bucket ? ??
? ,?? . 
In the algorithm, lines 9-11 implement dynam-
ic programming, and the function CONSECUTIVE 
checks whether the two spans can be combined. 
VB NP ?
V[1,2] V[3,4]
VP
JJR
V[2,3]
V[1,3] V[2,4]
c=6619 c=874 c=62
c=884 c=876 c=64c=6629
c=885
c=6682
c=65
VB NP will be JJR
lowest cost
c=0 c=0 c=0 c=0
 
Figure 4: Binarization forest for an SCFG rule 
 
?(?) ?(?) ?(?) ?(?) 
 VB NP 6619 VB NP ? 10 
 NP ? 874 NP ? JJR 2 
 ? JJR 62 VB NP ? JJR 1 
Table 1: Sub-sequences and corresponding costs 
Figure 4 shows an example of the compact 
forest the algorithm builds, where the solid lines 
indicate the optimal binarization of the rule, 
while other alternatives pruned by dynamic pro-
gramming are shown in dashed lines. The costs 
for binarization trees are computed based on the 
cost table given in Table 1. 
The time complexity of the CKY-based bina-
rization algorithm is ?(n3), which is higher than 
that of the linear binarization such as the syn-
chronous binarization (Zhang et al, 2006). But it 
is still efficient enough in practice, as there are 
generally only a few tokens (n < 5) on the 
source-sides of SCFG rules. In our experiments, 
the linear binarization method is just 2 times 
faster than the CKY-based binarization. 
3.4 Iterative Cost Reduction 
However, ?(?) cannot be easily predetermined in 
a static way as is assumed in Section 3.3 because 
it depends on ?? and should be updated whenever 
a rule in ? is binarized differently. In our work 
this problem is solved using the iterative cost 
reduction algorithm, in which the update of ?? 
and the cost function ?(?) are coupled together. 
Iterative cost reduction algorithm 
Input: An SCFG ? 
Output: An equivalent binary SCFG ?? of ? 
1: Function ITERATIVECOSTREDUCTION(?) 
2:   ?? ? ?0 
3:   for each ? ? ?0do 
4:        ?(?) =  ? ?,?0   
5:   while ?(??) does not converge do 
6:        for each ?? ? ? do 
7:            ?[???] ? ?? ?  ?(??) 
8:            for each ? ? ?(??) do 
9:                for each ?? ? ? ?,??  do 
10:                  ? ?? ? ? ?? ? 1 
11:          ?(??) ? CKYBINARIZATION(?? , ?) 
12:          ?? ? ?[???] ?  ?(??) 
13:          for each ? ? ?(??) do 
14:              for each ?? ? ? ?,??  do 
15:                  ? ?? ? ? ?? + 1 
16: return ?? 
In the iterative cost reduction algorithm, we 
first obtain an initial binary SCFG ?0 using the 
synchronous binarization method proposed in 
(Zhang et al, 2006). Then ?0 is assigned to an 
iterative variable ??. The cost of each binary rule 
in ?0 is computed based on ?0 according to Equ-
ation (1) (lines 3-4 in the algorithm). 
After initialization, ?? is updated by iteratively 
finding better binarization for each rule in ?. The 
basic idea is: for each ??  in ? , we remove the 
current binarization result for ??  from ?? (line 7), 
while the cost function ?(?)  is updated accor-
dingly since the removal of binary rule ? ? 
?(??) results in the reduction of the size of the 
corresponding bucket ? ?,?? . Lines 8-10 im-
366
plement the cost reduction of each binary rule in 
the bucket ? ?,? ? . 
Next, we find the lowest cost binarization for 
??  based on the updated cost function ?(?) with 
the CKY-based binarization algorithm presented 
in Section 3.3 (line 11).  
At last, the new binarization for ??  is added 
back to ?? and ?(?) is re-updated to synchronize 
with this change (lines 12-15). Figure 5 illu-
strates the differences between the static cost 
reduction and the iterative cost reduction. 
Ri
Ri-1
Ri+1
...
...
the i
th
 
rule
G
binarizer
Q(?)
binarize
(a) static cost reduction
Ri
Ri-1
Ri+1
...
...
the i
th
 
rule
G
binarizer
Q(?)
G0
(b) iterative cost reduction
update
static
dynamic
binarize
 
Figure 5: Comparison between the static cost 
reduction and the iterative cost reduction 
 
The algorithm stops when ?(??) does not de-
crease any more. Next we will show that ?(??)  
is guaranteed not to increase in the iterative 
process. 
For any ?(??) on ?? , we have 
               ?  ?[???] ?  ? ??   
        = 2 ? ? ? ??  +  ? ??  + ? ?[???]  
As both  ? ??   and ? ?[???]  are constants with 
respect to ?(? ?? ), ?  ?[???] ?  ? ??   is a li-
near function of ?(? ?? ), and the correspond-
ing slope is positive. Thus ?  ?[???] ?  ? ??   
reaches the lowest value only when ?(? ?? ) 
reaches the lowest value. So ?  ?[???] ?  ? ??   
achieves the lowest cost when we replace the 
current binarization with the new binarization  
??(??)  (line 12). Therefore ?  ?[???] ?  ? ??   
does not increase in the processing on each ??  
(lines 7-15), and ?(??) will finally converge to a 
local minimum when the algorithm stops. 
4 Experiments 
The experiments are conducted on Chinese-to-
English translation in a state-of-the-art string-to-
tree SMT system. All the results are reported in 
terms of case-insensitive BLEU4(%). 
4.1 Experimental Setup 
Our bilingual training corpus consists of about 
350K bilingual sentences (9M Chinese words + 
10M English words)2 . Giza++ is employed to 
perform word alignment on the bilingual sen-
tences. The parse trees on the English side are 
generated using the Berkeley Parser3. A 5-gram 
language model is trained on the English part of 
LDC bilingual training data and the Xinhua part 
of Gigaword corpus. Our development data set 
comes from NIST2003 evaluation data in which 
the sentences of more than 20 words are ex-
cluded to speed up the Minimum Error Rate 
Training (MERT). The test data sets are the 
NIST evaluation sets of 2005 and 2008. 
Our string-to-tree SMT system is built based 
on the work of (Galley et al, 2006; Marcu et al, 
2006), where both the minimal GHKM and 
SPMT rules are extracted from the training cor-
pus, and the composed rules are generated by 
combining two or three minimal GHKM and 
SPMT rules. Before the rule extraction, we also 
binarize the parse trees on the English side using 
Wang et al (2007) ?s method to increase the 
coverage of GHKM and SPMT rules. There are 
totally 4.26M rules after the low frequency rules 
are filtered out. The pruning strategy is similar to 
the cube pruning described in (Chiang, 2007). To 
achieve acceptable translation speed, the beam 
size is set to 50 by default. The baseline system 
is based on the synchronous binarization (Zhang 
et al, 2006).  
4.2 Binarization Schemes 
Besides the baseline (Zhang et al, 2006) and 
iterative cost reduction binarization methods, we 
also perform right-heavy and random synchron-
ous binarizations for comparison. In this paper, 
the random synchronous binarization is obtained 
by: 1) performing the CKY binarization to build 
the binarization forest for an SCFG rule; then 2) 
performing a top-down traversal of the forest. In 
the traversal, we randomly pick a feasible binari-
zation for each span, and then go on the traversal 
in the two branches of the picked binarization. 
Table 2 shows the costs of resulting binary 
SCFGs generated using different binarization 
methods. The costs of the baseline (left-heavy) 
                                                 
2 LDC2003E14, LDC2003E07, LDC2005T06 and 
LDC2005T10 
3 http://code.google.com/p/berkeleyparser/ 
367
and right-heavy binarization are similar, while 
the cost of the random synchronous binarization 
is lower than that of the baseline method4. As 
expected, the iterative cost reduction method ob-
tains the lowest cost, which is much lower than 
that of the other three methods.  
 
Method cost of binary SCFG ?? 
Baseline 4,897M 
Right-heavy 5,182M 
Random 3,479M 
Iterative cost reduction    185M 
Table 2: Costs of the binary SCFGs generated 
using different binarization methods. 
4.3 Evaluation of Translations 
Table 3 shows the performance of SMT systems 
based on different binarization methods. The 
iterative cost reduction binarization method 
achieves the best performance on the test sets as 
well as the development set. Compared with the 
baseline method, it obtains gains of 0.82 and 
0.84 BLEU scores on NIST05 and NIST08 test 
sets respectively. Using the statistical signific-
ance test described by Koehn (2004), the im-
provements are significant  (p < 0.05). 
 
Method Dev NIST05 NIST08 
Baseline 40.02 37.90 27.53  
Right-heavy 40.05 37.87 27.40 
Random 40.10 37.99 27.58 
Iterative cost 
reduction 
40.97* 38.72* 28.37* 
Table 3: Performance (BLUE4(%)) of different 
binarization methods. * = significantly better than 
baseline (p < 0.05).  
 
The baseline method and the right-heavy bina-
rization method achieve similar performance, 
while the random synchronous binarization me-
thod performs slightly better than the baseline 
method, which agrees with the fact of the cost 
reduction shown in Table 2. A possible reason 
that the random synchronous binarization me-
thod can outperform the baseline method lies in 
that compared with binarizing SCFG in a fixed 
way, the random synchronous binarization tends 
to give a more even distribution of rules among 
buckets, which alleviates the problem of edge 
competition. However, since the high-frequency 
source sub-sequences still have high probabilities 
to be generated in the binarization and lead to the 
                                                 
4 We perform random synchronous binarization for 5 
times and report the average cost. 
excess competing edges, it just achieves a very 
small improvement. 
4.4 Translation Accuracy vs. Cost of Binary 
SCFG 
We also study the impacts of cost reduction on 
translation accuracy over iterations in iterative 
cost reduction. Figure 6 and Figure 7 show the 
results on NIST05 and NIST08 test sets. We can 
see that the cost of the resulting binary SCFG 
drops greatly as the iteration count increases, 
especially in the first iteration, and the BLEU 
scores increase as the cost decreases. 
 
Figure 6: Cost of binary SCFG vs. BLEU4 (NIST05) 
 
 
Figure 7: Cost of binary SCFG vs. BLEU4 (NIST08) 
4.5 Impact of Beam Size 
In this section, we study the impacts of beam 
sizes on translation accuracy as well as compet-
ing edges. To explicitly investigate the issue un-
der large beam sizes, we use a subset of NIST05 
and NIST08 test sets for test, which has 50 Chi-
nese sentences of no longer than 10 words. 
Figure 8 shows that the iterative cost reduction 
method is consistently better than the baseline 
method under various beam settings. Besides the 
experiment on the test set of short sentences, we 
also conduct the experiment on NIST05 test set. 
To achieve acceptable decoding speed, we range 
the beam size from 10 to 70. As shown in Figure 
9, the iterative cost reduction method also out-
performs the baseline method under various 
beam settings on the large test set. 
Though enlarging beam size can reduce the 
search errors and improve the system perfor-
mance, the decoding speed of string-to-tree SMT 
drops dramatically when we enlarge the beam 
size. The problem is more serious when long 
1.0E+08
1.0E+09
1.0E+10
37.8
38
38.2
38.4
38.6
38.8
0 1 2 3 4 5
performance(BLEU4) cost
iteration
BLEU4(%) cost of G'
1.0E+08
1.0E+09
1.0E+10
27.4
27.6
27.8
28
28.2
28.4
0 1 2 3 4 5
performance(BLEU4) cost
BLEU4(%) cost of G'
iteration
368
sentences are translated. For example, when the 
beam size is set to a larger number (e.g. 200), our 
decoder takes nearly one hour to translate a sen-
tence whose length is about 20 on a 3GHz CPU. 
Decoding on the entire NIST05 and NIST08 test 
sets with large beam sizes is impractical. 
 
Figure 8: BLEU4 against beam size (small test set) 
 
 
Figure 9: BLEU4 against beam size (NIST05) 
 
Figure 10 compares the baseline method and 
the iterative cost reduction method in terms of 
translation accuracy against the number of edges 
proposed during decoding. Actually, the number 
of edges proposed during decoding can be re-
garded as a measure of the size of search space. 
We can see that the iterative cost reduction me-
thod outperforms the baseline method under var-
ious search effort.  
 
Figure 10: BLEU4 against competing edges  
 
The experimental results of this section show 
that compared with the baseline method, the iter-
ative cost reduction method can lead to much 
fewer edges (about 25% reduction) as well as the 
higher BLEU scores under various beam settings. 
4.6 Edge Competition vs. Cost of Binary 
SCFG 
In this section, we study the impacts of cost re-
duction on the edge competition in the chart cells 
of our CKY-based decoder. Two metrics are 
used to evaluate the degree of edge competition. 
They are the variance and the mean of the num-
ber of competing edges in the chart cells, where 
high variance means that in some chart cells the 
rules have high risk to be pruned due to the large 
number of competing edges. The same situation 
holds for the mean as well. Both of the two me-
trics are calculated on NIST05 test set, varying 
with the span length of chart cell. 
Figure 11 shows the cost of resulting binary 
SCFG and the variance of competing edges 
against iteration count in iterative cost reduction. 
We can see that both the cost and the variance 
reduce greatly as the iteration count increases. 
Figure 12 shows the case for mean, where the 
reduction of cost also leads to the reduction of 
the mean value. The results shown in Figure 11 
and Figure 12 indicate that the cost reduction is 
helpful to reduce edge competition in the chart 
cells.  
 
Figure 11: Cost of binary SCFG vs. variance of 
competing edge number (NIST05) 
 
 
Figure 12: Cost of binary SCFG vs. mean of 
competing edge number (NIST05) 
 
We also perform decoding without pruning 
(i.e. beam size = ?) on a very small set which 
has 20 sentences of no longer than 7 words. In 
this experiment, the baseline system and our iter-
ative cost reduction based system propose 
14,454M and 10,846M competing edges respec-
tively. These numbers can be seen as the real 
numbers of the edges proposed during decoding 
instead of an approximate number observed in 
the pruned search space. It suggests that our me-
thod can reduce the number of the edges in real 
search space effectively. A possible reason to 
32
34
36
38
40
42
10 50 100 500 1000 5000
baseline
cost reduction
BLEU4(%)
beam 
size 
35
36
37
38
39
10 20 30 40 50 70
baseline
cost reduction
beam
size
BLEU4(%)
32
34
36
38
40
42
1E+07 1E+08 1E+09 1E+10
baseline
cost reduction
BLEU4(%)
# of
edges
1.0E+5
1.0E+6
1.0E+7
1.0E+8
1.0E+9
1.0E+10
1.0E+7
1.0E+8
1.0E+9
1.0E+10
0 1 2 3 4 5
span=2
span=3
span=5
span=7
span=10
span=20
cost
iteration
variance cost of G'
1.0E+6
1.0E+7
1.0E+8
1.0E+9
1.0E+10
8.0E+3
1.0E+5
0 1 2 3 4 5
span=2
span=3
span=5
span=7
span=10
span=20
cost
iteration
mean cost of G'
369
this result is that the cost reduction based binari-
zation could reduce the probability of rule mis-
matching caused by binarization, which results in 
the reduction of the number of edges proposed 
during decoding. 
5 Conclusion and Future Work 
This paper introduces a new binarization method, 
aiming at choosing better binarization for SCFG-
based SMT systems. We demonstrate the effec-
tiveness of our method on a state-of-the-art 
string-to-tree SMT system. Experimental results 
show that our method can significantly outper-
form the conventional synchronous binarization 
method, which indicates that better binarization 
selection is very beneficial to SCFG-based SMT 
systems. 
In this paper the cost of a binary rule is de-
fined based on the competition among the binary 
rules that have the same source-sides. However, 
some binary rules with different source-sides 
may also have competitions in a chart cell. We 
think that the cost of a binary rule can be better 
estimated by taking the rules with different 
source-sides into account. We intend to study 
this issue in our future work. 
Acknowledgements 
The authors would like to thank the anonymous 
reviewers for their pertinent comments, and Xi-
nying Song, Nan Duan and Shasha Li for their 
valuable suggestions for improving this paper. 
References 
Eugene Charniak,  Mark Johnson, Micha Elsner, Jo-
seph Austerweil, David Ellis, Isaac Haxton, Cathe-
rine Hill, R. Shrivaths, Jeremy Moore, Michael Po-
zar, and Theresa Vu. 2006. Multilevel Coarse-to-
Fine PCFG Parsing. In Proc. of HLT-NAACL 2006, 
New York, USA, 168-175.  
Eugene Charniak, Sharon Goldwater, and Mark John-
son. 1998. Edge-Based Best-First Chart Parsing. In 
Proc. of the Six Workshop on Very Large Corpora, 
pages: 127-133. 
David Chiang. 2005. A Hierarchical Phrase-Based 
Model for Statistical Machine Translation. In Proc. 
of ACL 2005, Ann Arbor, Michigan, pages: 263-
270. 
David Chiang. 2007. Hierarchical Phrase-based 
Translation. Computational Linguistics. 33(2): 
202-208. 
Michel Galley, Jonathan Graehl, Kevin Knight, Da-
niel Marcu, Steve DeNeefe, Wei Wang, and Igna-
cio Thayer. 2006. Scalable Inference and Training 
of Context-Rich Syntactic Translation Models. In 
Proc. of ACL 2006, Sydney, Australia, pages: 961-
968. 
Michel Galley, Mark Hopkins, Kevin Knight, and 
Daniel Marcu. 2004. What?s in a translation rule? 
In Proc. of HLT-NAACL 2004, Boston, USA, pag-
es: 273-280. 
Liang Huang. 2007. Binarization, Synchronous Bina-
rization, and Target-side binarization.  In Proc. of 
HLT-NAACL 2007 / AMTA workshop on Syntax 
and Structure in Statistical Translation, New York, 
USA, pages: 33-40. 
Tadao Kasami. 1965. An Efficient Recognition and 
Syntax Analysis Algorithm for Context-Free Lan-
guages. Technical Report AFCRL-65-758, Air 
Force Cambridge Research Laboratory, Bedford, 
Massachusetts. 
Philipp Koehn. 2004. Statistical Significance Tests for 
Machine Translation Evaluation. In Proc. of 
EMNLP 2004, Barcelona, Spain , pages: 388?395. 
Daniel Marcu, Wei Wang, Abdessamad Echihabi, and 
Kevin Knight. 2006. SPMT: Statistical machine 
translation with syntactified target language phras-
es. In Proc. of EMNLP 2006, Sydney, Australia, 
pages: 44-52. 
Giorgio Satta and Enoch Peserico. 2005. Some Com-
putational Complexity Results for Synchronous 
Context-Free Grammars. In Proc. of HLT-EMNLP 
2005, Vancouver, pages: 803-810. 
L. Shapiro and A. B. Stephens. 1991. Bootstrap per-
colation, the Sch? oder numbers, and the n-kings 
problem. SIAM Journal on Discrete Mathematics, 
4(2):275-280. 
Xinying Song, Shilin Ding and Chin-Yew Lin. 2008. 
Better Binarization for the CKY Parsing. In Proc. 
of EMNLP 2008, Hawaii, pages: 167-176. 
Yoshimasa Tsuruoka and Junichi Tsujii. 2004. Itera-
tive CKY Parsing for Probabilistic Context-Free 
Grammars. In Proc. of IJCNLP 2004, pages: 52-
60. 
Wei Wang  and  Kevin Knight and Daniel Marcu. 
2007. Binarizing Syntax Trees to Improve Syntax-
Based Machine Translation Accuracy. In Proc. of 
EMNLP-CoNLL 2007, Prague, Czech Republic, 
pages: 746-754. 
D. H. Younger. 1967. Recognition and Parsing of 
Context-Free Languages in Time n3. Information 
and Control, 10(2):189-208. 
Hao Zhang, Liang Huang, Daniel Gildea, and Kevin 
Knight. 2006. Synchronous Binarization for Ma-
chine Translation. In Proc. of HLT-NAACL 2006, 
New York, USA, pages: 256- 263. 
370
Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 720?727,
Prague, Czech Republic, June 2007. c?2007 Association for Computational Linguistics
A Probabilistic Approach to Syntax-based Reordering
for Statistical Machine Translation
Chi-Ho Li, Dongdong Zhang, Mu Li, Ming Zhou
Microsoft Research Asia
Beijing, China
chl, dozhang@microsoft.com
muli, mingzhou@microsoft.com
Minghui Li, Yi Guan
Harbin Institute of Technology
Harbin, China
mhli@insun.hit.edu.cn
guanyi@insun.hit.edu.cn
Abstract
Inspired by previous preprocessing ap-
proaches to SMT, this paper proposes a
novel, probabilistic approach to reordering
which combines the merits of syntax and
phrase-based SMT. Given a source sentence
and its parse tree, our method generates,
by tree operations, an n-best list of re-
ordered inputs, which are then fed to stan-
dard phrase-based decoder to produce the
optimal translation. Experiments show that,
for the NIST MT-05 task of Chinese-to-
English translation, the proposal leads to
BLEU improvement of 1.56%.
1 Introduction
The phrase-based approach has been considered the
default strategy to Statistical Machine Translation
(SMT) in recent years. It is widely known that the
phrase-based approach is powerful in local lexical
choice and word reordering within short distance.
However, long-distance reordering is problematic
in phrase-based SMT. For example, the distance-
based reordering model (Koehn et al, 2003) al-
lows a decoder to translate in non-monotonous or-
der, under the constraint that the distance between
two phrases translated consecutively does not ex-
ceed a limit known as distortion limit. In theory the
distortion limit can be assigned a very large value
so that all possible reorderings are allowed, yet in
practise it is observed that too high a distortion limit
not only harms efficiency but also translation per-
formance (Koehn et al, 2005). In our own exper-
iment setting, the best distortion limit for Chinese-
English translation is 4. However, some ideal trans-
lations exhibit reorderings longer than such distor-
tion limit. Consider the sentence pair in NIST MT-
2005 test set shown in figure 1(a): after translating
the word ?V/mend?, the decoder should ?jump?
across six words and translate the last phrase ?
? ?_/fissures in the relationship?. Therefore,
while short-distance reordering is under the scope
of the distance-based model, long-distance reorder-
ing is simply out of the question.
A terminological remark: In the rest of the paper,
we will use the terms global reordering and local
reordering in place of long-distance reordering and
short-distance reordering respectively. The distinc-
tion between long and short distance reordering is
solely defined by distortion limit.
Syntax1 is certainly a potential solution to global
reordering. For example, for the last two Chinese
phrases in figure 1(a), simply swapping the two chil-
dren of the NP node will produce the correct word
order on the English side. However, there are also
reorderings which do not agree with syntactic anal-
ysis. Figure 1(b) shows how our phrase-based de-
coder2 obtains a good English translation by reorder-
ing two blocks. It should be noted that the second
Chinese block ??e? and its English counterpart
?at the end of? are not constituents at all.
In this paper, our interest is the value of syntax in
reordering, and the major statement is that syntactic
information is useful in handling global reordering
1Here by syntax it is meant linguistic syntax rather than for-
mal syntax.
2The decoder is introduced in section 6.
720
Figure 1: Examples on how syntax (a) helps and (b) harms reordering in Chinese-to-English translation
The lines and nodes on the top half of the figures show the phrase structure of the Chinese sentences, while the links on the bottom
half of the figures show the alignments between Chinese and English phrases. Square brackets indicate the boundaries of blocks
found by our decoder.
and it achieves better MT performance on the ba-
sis of the standard phrase-based model. To prove it,
we developed a hybrid approach which preserves the
strength of phrase-based SMT in local reordering as
well as the strength of syntax in global reordering.
Our method is inspired by previous preprocessing
approaches like (Xia and McCord, 2004), (Collins
et al, 2005), and (Costa-jussa` and Fonollosa, 2006),
which split translation into two stages:
S ? S? ? T (1)
where a sentence of the source language (SL), S,
is first reordered with respect to the word order of
the target language (TL), and then the reordered SL
sentence S? is translated as a TL sentence T by
monotonous translation.
Our first contribution is a new translation model
as represented by formula 2:
S ? n? S? ? n? T ? T? (2)
where an n-best list of S?, instead of only one S?, is
generated. The reason of such change will be given
in section 2. Note also that the translation process
S??T is not monotonous, since the distance-based
model is needed for local reordering. Our second
contribution is our definition of the best translation:
argmax
T
exp(?rlogPr(S?S?)+
?
i
?iFi(S??T ))
where Fi are the features in the standard phrase-
based model and Pr(S ? S?) is our new feature,
viz. the probability of reordering S as S?. The de-
tails of this model are elaborated in sections 3 to 6.
The settings and results of experiments on this new
model are given in section 7.
2 Related Work
There have been various attempts to syntax-
based SMT, such as (Yamada and Knight, 2001)
and (Quirk et al, 2005). We do not adopt these
models since a lot of subtle issues would then be in-
troduced due to the complexity of syntax-based de-
coder, and the impact of syntax on reordering will
be difficult to single out.
There have been many reordering strategies un-
der the phrase-based camp. A notable approach is
lexicalized reordering (Koehn et al, 2005) and (Till-
mann, 2004). It should be noted that this approach
achieves the best result within certain distortion limit
and is therefore not a good model for global reorder-
ing.
There are a few attempts to the preprocessing
approach to reordering. The most notable ones
are (Xia and McCord, 2004) and (Collins et al,
2005), both of which make use of linguistic syntax
in the preprocessing stage. (Collins et al, 2005) an-
alyze German clause structure and propose six types
721
of rules for transforming German parse trees with
respect to English word order. Instead of relying
on manual rules, (Xia and McCord, 2004) propose
a method in learning patterns of rewriting SL sen-
tences. This method parses training data and uses
some heuristics to align SL phrases with TL ones.
From such alignment it can extract rewriting pat-
terns, of which the units are words and POSs. The
learned rewriting rules are then applied to rewrite SL
sentences before monotonous translation.
Despite the encouraging results reported in these
papers, the two attempts share the same shortcoming
that their reordering is deterministic. As pointed out
in (Al-Onaizan and Papineni, 2006), these strategies
make hard decisions in reordering which cannot be
undone during decoding. That is, the choice of re-
ordering is independent from other translation fac-
tors, and once a reordering mistake is made, it can-
not be corrected by the subsequent decoding.
To overcome this weakness, we suggest a method
to ?soften? the hard decisions in preprocessing. The
essence is that our preprocessing module generates
n-best S?s rather than merely one S?. A variety of
reordered SL sentences are fed to the decoder so
that the decoder can consider, to certain extent, the
interaction between reordering and other factors of
translation. The entire process can be depicted by
formula 2, recapitulated as follows:
S ? n? S? ? n? T ? T? .
Apart from their deterministic nature, the two
previous preprocessing approaches have their own
weaknesses. (Collins et al, 2005) count on man-
ual rules and it is suspicious if reordering rules for
other language pairs can be easily made. (Xia and
McCord, 2004) propose a way to learn rewriting
patterns, nevertheless the units of such patterns are
words and their POSs. Although there is no limit to
the length of rewriting patterns, due to data sparse-
ness most patterns being applied would be short
ones. Many instances of global reordering are there-
fore left unhandled.
3 The Acquisition of Reordering
Knowledge
To avoid this problem, we give up using rewriting
patterns and design a form of reordering knowledge
which can be directly applied to parse tree nodes.
Given a node N on the parse tree of an SL sentence,
the required reordering knowledge should enable the
preprocessing module to determine how probable
the children of N are reordered.3 For simplicity, let
us first consider the case of binary nodes only. Let
N1 and N2, which yield phrases p1 and p2 respec-
tively, be the child nodes of N . We want to deter-
mine the order of p1 and p2 with respect to their TL
counterparts, T (p1) and T (p2). The knowledge for
making such a decision can be learned from a word-
aligned parallel corpus. There are two questions in-
volved in obtaining training instances:
? How to define T (pi)?
? How to define the order of T (pi)s?
For the first question, we adopt a similar method
as in (Fox, 2002): given an SL phrase ps =
s1 . . . si . . . sn and a word alignment matrix A, we
can enumerate the set of TL words {ti : ti?A(si)},
and then arrange the words in the order as they ap-
pear in the TL sentence. Let first(t) be the first word
in this sorted set and last(t) be the last word. T (ps)
is defined as the phrase first(t) . . . last(t) in the TL
sentence. Note that T (ps) may contain words not in
the set {ti}.
The question of the order of two TL phrases is not
a trivial one. Since a word alignment matrix usu-
ally contains a lot of noises as well as one-to-many
and many-to-many alignments, two TL phrases may
overlap with each other. For the sake of the quality
of reordering knowledge, if T (p1) and T (p2) over-
lap, then the node N with children N1 and N2 is
not taken as a training instance. Obviously it will
greatly reduce the amount of training input. To rem-
edy data sparseness, less probable alignment points
are removed so as to minimize overlapping phrases,
since, after removing some alignment point, one of
the TL phrases may become shorter and the two
phrases may no longer overlap. The implementation
is similar to the idea of lexical weight in (Koehn et
al., 2003): all points in the alignment matrices of the
entire training corpus are collected to calculate the
probabilistic distribution, P (t|s), of some TL word
3Some readers may prefer the expression the subtree rooted
at node N to node N . The latter term is used in this paper for
simplicity.
722
t given some SL word s. Any pair of overlapping
T (pi)s will be redefined by iteratively removing less
probable word alignments until they no longer over-
lap. If they still overlap after all one/many-to-many
alignments have been removed, then the refinement
will stop and N , which covers pis, is no longer taken
as a training instance.
In sum, given a bilingual training corpus, a parser
for the SL, and a word alignment tool, we can collect
all binary parse tree nodes, each of which may be an
instance of the required reordering knowledge. The
next question is what kind of reordering knowledge
can be formed out of these training instances. Two
forms of reordering knowledge are investigated:
1. Reordering Rules, which have the form
Z : X Y ?
{
X Y Pr(IN-ORDER)
Y X Pr(INVERTED)
where Z is the phrase label of a binary node
and X and Y are the phrase labels of Z?s chil-
dren, and Pr(INVERTED) and Pr(IN-ORDER)
are the probability that X and Y are inverted on
TL side and that not inverted, respectively. The
probability figures are estimated by Maximum
Likelihood Estimation.
2. Maximum Entropy (ME) Model, which does
the binary classification whether a binary
node?s children are inverted or not, based on a
set of features over the SL phrases correspond-
ing to the two children nodes. The features that
we investigated include the leftmost, rightmost,
head, and context words4, and their POSs, of
the SL phrases, as well as the phrase labels of
the SL phrases and their parent.
4 The Application of Reordering
Knowledge
After learning reordering knowledge, the prepro-
cessing module can apply it to the parse tree, tS ,
of an SL sentence S and obtain the n-best list of
S?. Since a ranking of S? is needed, we need some
way to score each S?. Here probability is used as
the scoring metric. In this section it is explained
4The context words of the SL phrases are the word to the left
of the left phrase and the word to the right of the right phrase.
how the n-best reorderings of S and their associated
scores/probabilites are computed.
Let us first look into the scoring of a particular
reordering. Let Pr(p?p?) be the probability of re-
ordering a phrase p into p?. For a phrase q yielded by
a non-binary node, there is only one ?reordering? of
q, viz. q itself, thus Pr(q?q) = 1. For a phrase p
yielded by a binary node N , whose left child N1 has
reorderings pi1 and right child N2 has the reorder-
ings pj2 (1 ? i, j ? n), p? has the form pi1pj2 or pj2pi1.
Therefore, Pr(p?p?) =
{
Pr(IN-ORDER)? Pr(pi1?pi
?
1 )? Pr(pj2?pj
?
2 )
Pr(INVERTED)? Pr(pj2?pj
?
2 )? Pr(pi1?pi
?
1 )
The figures Pr(IN-ORDER) and Pr(INVERTED) are
obtained from the learned reordering knowledge. If
reordering knowledge is represented as rules, then
the required probability is the probability associated
with the rule that can apply to N . If reordering
knowledge is represented as an ME model, then the
required probability is:
P (r|N) = exp(
?
i ?ifi(N, r))?
r? exp(
?
i ?ifi(N, r?))
where r?{IN-ORDER, INVERTED}, and fi?s are fea-
tures used in the ME model.
Let us turn to the computation of the n-best re-
ordering list. Let R(N) be the number of reorder-
ings of the phrase yielded by N , then:
R(N) =
{
2R(N1)R(N2) if N has children N1, N2
1 otherwise
It is easily seen that the number of S?s increases ex-
ponentially. Fortunately, what we need is merely an
n-best list rather than a full list of reorderings. Start-
ing from the leaves of tS , for each node N covering
phrase p, we only keep track of the n p?s that have
the highest reordering probability. Thus R(N) ? n.
There are at most 2n2 reorderings for any node and
only the top-scored n reorderings are recorded. The
n-best reorderings of S, i.e. the n-best reorderings
of the yield of the root node of tS , can be obtained
by this efficient bottom-up method.
5 The Generalization of Reordering
Knowledge
In the last two sections reordering knowledge is
learned from and applied to binary parse tree nodes
723
only. It is not difficult to generalize the theory of
reordering knowledge to nodes of other branching
factors. The case of binary nodes is simple as there
are only two possible reorderings. The case of 3-ary
nodes is a bit more complicated as there are six.5 In
general, an n-ary node has n! possible reorderings
of its children. The maximum entropy model has the
same form as in the binary case, except that there are
more classes of reordering patterns as n increases.
The form of reordering rules, and the calculation of
reordering probability for a particular node, can also
be generalized easily.6 The only problem for the
generalized reordering knowledge is that, as there
are more classes, data sparseness becomes more se-
vere.
6 The Decoder
The last three sections explain how the S?n?S?
part of formula 2 is done. The S??T
part is simply done by our re-implementation
of PHARAOH (Koehn, 2004). Note that non-
monotonous translation is used here since the
distance-based model is needed for local reordering.
For the n?T? T? part, the factors in consideration
include the score of T returned by the decoder, and
the reordering probability Pr(S ? S?). In order
to conform to the log-linear model used in the de-
coder, we integrate the two factors by defining the
total score of T as formula 3:
exp(?r logPr(S?S?) +
?
i
?iFi(S??T )) (3)
The first term corresponds to the contribution of
syntax-based reordering, while the second term that
of the features Fi used in the decoder. All the fea-
ture weights (?s) were trained using our implemen-
tation of Minimum Error Rate Training (Och, 2003).
The final translation T? is the T with the highest total
score.
5Namely, N1N2N3, N1N3N2, N2N1N3, N2N3N1,
N3N1N2, and N3N2N1, if the child nodes in the original order
are N1, N2, and N3.
6For example, the reordering probability of a phrase p =
p1p2p3 generated by a 3-ary node N is
Pr(r)?Pr(pi1)?Pr(pj2)?Pr(pk3)
where r is one of the six reordering patterns for 3-ary nodes.
It is observed in pilot experiments that, for a lot of
long sentences containing several clauses, only one
of the clauses is reordered. That is, our greedy re-
ordering algorithm (c.f. section 4) has a tendency to
focus only on a particular clause of a long sentence.
The problem was remedied by modifying our de-
coder such that it no longer translates a sentence at
once; instead the new decoder does:
1. split an input sentence S into clauses {Ci};
2. obtain the reorderings among {Ci}, {Sj};
3. for each Sj , do
(a) for each clause Ci in Sj , do
i. reorder Ci into n-best C ?is,
ii. translate each C ?i into T (C
?
i),
iii. select T? (C ?i);
(b) concatenate {T? (C ?i)} into Tj ;
4. select T?j .
Step 1 is done by checking the parse tree if there
are any IP or CP nodes7 immediately under the root
node. If yes, then all these IPs, CPs, and the remain-
ing segments are treated as clauses. If no, then the
entire input is treated as one single clause. Step 2
and step 3(a)(i) still follow the algorithm in sec-
tion 4. Step 3(a)(ii) is trivial, but there is a subtle
point about the calculation of language model score:
the language model score of a translated clause is not
independent from other clauses; it should take into
account the last few words of the previous translated
clause. The best translated clause T? (C ?i) is selected
in step 3(a)(iii) by equation 3. In step 4 the best
translation T?j is
argmax
Tj
exp(?rlogPr(S?Sj)+
?
i
score(T (C ?i))).
7 Experiments
7.1 Corpora
Our experiments are about Chinese-to-English
translation. The NIST MT-2005 test data set is used
for evaluation. (Case-sensitive) BLEU-4 (Papineni
et al, 2002) is used as the evaluation metric. The
7 IP stands for inflectional phrase and CP for complementizer
phrase. These two types of phrases are clauses in terms of the
Government and Binding Theory.
724
Branching Factor 2 3 >3
Count 12294 3173 1280
Percentage 73.41 18.95 7.64
Table 1: Distribution of Parse Tree Nodes with Dif-
ferent Branching Factors Note that nodes with only one
child are excluded from the survey as reordering does not apply
to such nodes.
test set and development set of NIST MT-2002 are
merged to form our development set. The training
data for both reordering knowledge and translation
table is the one for NIST MT-2005. The GIGA-
WORD corpus is used for training language model.
The Chinese side of all corpora are segmented into
words by our implementation of (Gao et al, 2003).
7.2 The Preprocessing Module
As mentioned in section 3, the preprocessing mod-
ule for reordering needs a parser of the SL, a word
alignment tool, and a Maximum Entropy training
tool. We use the Stanford parser (Klein and Man-
ning, 2003) with its default Chinese grammar, the
GIZA++ (Och and Ney, 2000) alignment package
with its default settings, and the ME tool developed
by (Zhang, 2004).
Section 5 mentions that our reordering model can
apply to nodes of any branching factor. It is inter-
esting to know how many branching factors should
be included. The distribution of parse tree nodes
as shown in table 1 is based on the result of pars-
ing the Chinese side of NIST MT-2002 test set by
the Stanford parser. It is easily seen that the major-
ity of parse tree nodes are binary ones. Nodes with
more than 3 children seem to be negligible. The 3-
ary nodes occupy a certain proportion of the distri-
bution, and their impact on translation performance
will be shown in our experiments.
7.3 The decoder
The data needed by our Pharaoh-like decoder are
translation table and language model. Our 5-gram
language model is trained by the SRI language mod-
eling toolkit (Stolcke, 2002). The translation table
is obtained as described in (Koehn et al, 2003), i.e.
the alignment tool GIZA++ is run over the training
data in both translation directions, and the two align-
Test Setting BLEU
B1 standard phrase-based SMT 29.22
B2 (B1) + clause splitting 29.13
Table 2: Experiment Baseline
Test Setting BLEU BLEU
2-ary 2,3-ary
1 rule 29.77 30.31
2 ME (phrase label) 29.93 30.49
3 ME (left,right) 30.10 30.53
4 ME ((3)+head) 30.24 30.71
5 ME ((3)+phrase label) 30.12 30.30
6 ME ((4)+context) 30.24 30.76
Table 3: Tests on Various Reordering Models
The 3rd column comprises the BLEU scores obtained by re-
ordering binary nodes only, the 4th column the scores by re-
ordering both binary and 3-ary nodes. The features used in the
ME models are explained in section 3.
ment matrices are integrated by the GROW-DIAG-
FINAL method into one matrix, from which phrase
translation probabilities and lexical weights of both
directions are obtained.
The most important system parameter is, of
course, distortion limit. Pilot experiments using the
standard phrase-based model show that the optimal
distortion limit is 4, which was therefore selected for
all our experiments.
7.4 Experiment Results and Analysis
The baseline of our experiments is the standard
phrase-based model, which achieves, as shown by
table 2, the BLEU score of 29.22. From the same
table we can also see that the clause splitting mech-
anism introduced in section 6 does not significantly
affect translation performance.
Two sets of experiments were run. The first set,
of which the results are shown in table 3, tests the
effect of different forms of reordering knowledge.
In all these tests only the top 10 reorderings of
each clause are generated. The contrast between
tests 1 and 2 shows that ME modeling of reordering
outperforms reordering rules. Tests 3 and 4 show
that phrase labels can achieve as good performance
as the lexical features of mere leftmost and right-
most words. However, when more lexical features
725
Input 0 2005#?R????q?Z??/???=?
Reference Hainan province will continue to increase its investment in the public services and
social services infrastructures in 2005
Baseline Hainan Province in 2005 will continue to increase for the public service and social
infrastructure investment
Translation with
Preprocessing
Hainan Province in 2005 will continue to increase investment in public services
and social infrastructure
Table 4: Translation Example 1
Test Setting BLEU
a length constraint 30.52
b DL=0 30.48
c n=100 30.78
Table 5: Tests on Various Constraints
are added (tests 4 and 6), phrase labels can no longer
compete with lexical features. Surprisingly, test 5
shows that the combination of phrase labels and lex-
ical features is even worse than using either phrase
labels or lexical features only.
Apart from quantitative evaluation, let us con-
sider the translation example of test 6 shown in ta-
ble 4. To generate the correct translation, a phrase-
based decoder should, after translating the word
?? as ?increase?, jump to the last word ?=
?(investment)?. This is obviously out of the capa-
bility of the baseline model, and our approach can
accomplish the desired reordering as expected.
By and large, the experiment results show that no
matter what kind of reordering knowledge is used,
the preprocessing of syntax-based reordering does
greatly improve translation performance, and that
the reordering of 3-ary nodes is crucial.
The second set of experiments test the effect of
some constraints. The basic setting is the same as
that of test 6 in the first experiment set, and reorder-
ing is applied to both binary and 3-ary nodes. The
results are shown in table 5.
In test (a), the constraint is that the module does
not consider any reordering of a node if the yield
of this node contains not more than four words.
The underlying rationale is that reordering within
distortion limit should be left to the distance-based
model during decoding, and syntax-based reorder-
ing should focus on global reordering only. The
result shows that this hypothesis does not hold.
In practice syntax-based reordering also helps lo-
cal reordering. Consider the translation example
of test (a) shown in table 6. Both the baseline
model and our model translate in the same way up
to the word ??w? (which is incorrectly translated
as ?and?). From this point, the proposed preprocess-
ing model correctly jump to the last phrase ??q?
?X/discussed?, while the baseline model fail to do
so for the best translation. It should be noted, how-
ever, that there are only four words between ??w?
and the last phrase, and the desired order of decod-
ing is within the capability of the baseline system.
With the feature of syntax-based global reordering,
a phrase-based decoder performs better even with
respect to local reordering. It is because syntax-
based reordering adds more weight to a hypothesis
that moves words across longer distance, which is
penalized by the distance-based model.
In test (b) distortion limit is set as 0; i.e. reorder-
ing is done merely by syntax-based preprocessing.
The worse result is not surprising since, after all,
preprocessing discards many possibilities and thus
reduce the search space of the decoder. Some local
reordering model is still needed during decoding.
Finally, test (c) shows that translation perfor-
mance does not improve significantly by raising the
number of reorderings. This implies that our ap-
proach is very efficient in that only a small value of
n is capable of capturing the most important global
reordering patterns.
8 Conclusion and Future Work
This paper proposes a novel, probabilistic approach
to reordering which combines the merits of syntax
and phrase-based SMT. On the one hand, global
reordering, which cannot be accomplished by the
726
Input ?$3 ,?)Z?C?wOcu??q??X
Reference Meanwhile , Yushchenko and his assistants discussed issues concerning the estab-
lishment of a new government
Baseline The same time , Yushchenko assistants and a new Government on issues discussed
Translation with
Preprocessing
The same time , Yushchenko assistants and held discussions on the issue of a new
government
Table 6: Translation Example 2
phrase-based model, is enabled by the tree opera-
tions in preprocessing. On the other hand, local re-
ordering is preserved and even strengthened in our
approach. Experiments show that, for the NIST MT-
05 task of Chinese-to-English translation, the pro-
posal leads to BLEU improvement of 1.56%.
Despite the encouraging experiment results, it
is still not very clear how the syntax-based and
distance-based models complement each other in
improving word reordering. In future we need to
investigate their interaction and identify the contri-
bution of each component. Moreover, it is observed
that the parse trees returned by a full parser like
the Stanford parser contain too many nodes which
seem not be involved in desired reorderings. Shal-
low parsers should be tried to see if they improve
the quality of reordering knowledge.
References
Yaser Al-Onaizan, and Kishore Papineni. 2006. Distor-
tion Models for Statistical Machine Translation. Pro-
ceedings for ACL 2006.
Michael Collins, Philipp Koehn, and Ivona Kucerova.
2005. Clause Restructuring for Statistical Machine
Translation. Proceedings for ACL 2005.
M.R. Costa-jussa`, and J.A.R. Fonollosa. 2006. Statis-
tical Machine Reordering. Proceedings for EMNLP
2006.
Heidi Fox. 2002. Phrase Cohesion and Statistical Ma-
chine Translation. Proceedings for EMNLP 2002.
Jianfeng Gao, Mu Li, and Chang-Ning Huang 2003.
Improved Source-Channel Models for Chinese Word
Segmentation. Proceedings for ACL 2003.
Dan Klein and Christopher D. Manning. 2003. Accurate
Unlexicalized Parsing. Proceedings for ACL 2003.
Philipp Koehn, Franz J. Och, and Daniel Marcu. 2003.
Statistical Phrase-based Translation. Proceedings for
HLT-NAACL 2003.
Philipp Koehn. 2004. Pharaoh: a Beam Search De-
coder for Phrase-Based Statistical Machine Transla-
tion Models. Proceedings for AMTA 2004.
Philipp Koehn, Amittai Axelrod, Alexandra Birch
Mayne, Chris Callison-Burch, Miles Osborne, and
David Talbot 2005. Edinburgh System Description
for the 2005 IWSLT Speech Translation Evaluation.
Proceedings for IWSLT 2005.
Franz J. Och. 2003. Minimum Error Rate Training in
Statistical Machine Translation. Proceedings for ACL
2003.
Franz J. Och, and Hermann Ney. 2000. Improved Statis-
tical Alignment Models. Proceedings for ACL 2000.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: a Method for Automatic Eval-
uation of Machine Translation. Proceedings for ACL
2002.
Chris Quirk, Arul Menezes, and Colin Cherry. 2005. De-
pendency Treelet Translation: Syntactically Informed
Phrasal SMT. Proceedings for ACL 2005.
Andreas Stolcke. 2002. SRILM - An Extensible Lan-
guage Modeling Toolkit. Proceedings for the Interna-
tional Conference on Spoken Language Understand-
ing 2002.
Christoph Tillmann. 2004. A Unigram Orientation
Model for Statistical Machine Translation. Proceed-
ings for ACL 2004.
Fei Xia, and Michael McCord 2004. Improving a Statis-
tical MT System with Automatically Learned Rewrite
Patterns. Proceedings for COLING 2004.
Kenji Yamada, and Kevin Knight. 2001. A syntax-
based statistical translation model. Proceedings for
ACL 2001.
Le Zhang. 2004. Maximum Entropy
Modeling Toolkit for Python and C++.
http://homepages.inf.ed.ac.uk/s0450736/maxent toolkit.html.
727
Proceedings of ACL-08: HLT, pages 89?96,
Columbus, Ohio, USA, June 2008. c?2008 Association for Computational Linguistics
Measure Word Generation for English-Chinese SMT Systems 
 
 
Dongdong Zhang1, Mu Li1, Nan Duan2, Chi-Ho Li1, Ming Zhou1 
1Microsoft Research Asia 2Tianjin University 
Beijing, China Tianjin, China 
{dozhang,muli,v-naduan,chl,mingzhou}@microsoft.com 
 
 
 
 
 
 
Abstract 
Measure words in Chinese are used to indi-
cate the count of nouns. Conventional sta-
tistical machine translation (SMT) systems do 
not perform well on measure word generation 
due to data sparseness and the potential long 
distance dependency between measure words 
and their corresponding head words. In this 
paper, we propose a statistical model to gen-
erate appropriate measure words of nouns for 
an English-to-Chinese SMT system. We mod-
el the probability of measure word generation 
by utilizing lexical and syntactic knowledge 
from both source and target sentences. Our 
model works as a post-processing procedure 
over output of statistical machine translation 
systems, and can work with any SMT system. 
Experimental results show our method can 
achieve high precision and recall in measure 
word generation. 
1 Introduction 
In linguistics, measure words (MW) are words or 
morphemes used in combination with numerals or 
demonstrative pronouns to indicate the count of 
nouns1, which are often referred to as head words 
(HW). 
Chinese measure words are grammatical units 
and occur quite often in real text. According to our 
survey on the measure word distribution in the 
Chinese Penn Treebank and the test datasets distri-
buted by Linguistic Data Consortium (LDC) for 
Chinese-to-English machine translation evaluation, 
the average occurrence is 0.505 and 0.319 measure 
                                                 
1 The uncommon cases of verbs are not considered. 
words per sentence respectively. Unlike in Chinese, 
there is no special set of measure words in English. 
Measure words are usually used for mass nouns 
and any semantically appropriate nouns can func-
tion as the measure words. For example, in the 
phrase three bottles of water, the word bottles acts 
as a measure word. Countable nouns are almost 
never modified by measure words2. Numerals and 
indefinite articles are directly followed by counta-
ble nouns to denote the quantity of objects.  
Therefore, in the English-to-Chinese machine 
translation task we need to take additional efforts 
to generate the missing measure words in Chinese. 
For example, when translating the English phrase 
three books into the Chinese phrases ?????, 
where three corresponds to the numeral ??? and 
books corresponds to the noun ???, the Chinese 
measure word ??? should be generated between 
the numeral and the noun.  
In most statistical machine translation (SMT) 
models (Och et al, 2004; Koehn et al, 2003; 
Chiang, 2005), some of measure words can be 
generated without modification or additional 
processing. For example, in above translation, the 
phrase translation table may suggest the word three 
be translated into ???, ????, ????, etc, and 
the word books into ???, ????, ???? (scroll), 
etc. Then the SMT model selects the most likely 
combination ????? as the final translation re-
sult. In this example, a measure word candidate set 
consisting of ??? and ??? can be generated by 
bilingual phrases (or synchronous translation rules), 
and the best measure word ??? from the measure  
                                                 
2 There are some exceptional cases, such as ?100 head of cat-
tle?. But they are very uncommon. 
89
  
 
 
 
 
 
 
 
 
 
 
 
 
word candidate set can be selected by the SMT 
decoder. However, as we will show below, existing 
SMT systems do not deal well with the measure 
word generation in general due to data sparseness 
and long distance dependencies between measure 
words and their corresponding head words.  
Due to the limited size of bilingual corpora, 
many measure words, as well as the collocations 
between a measure and its head word, cannot be 
well covered by the phrase translation table in an 
SMT system. Moreover, Chinese measure words 
often have a long distance dependency to their 
head words which makes language model ineffec-
tive in selecting the correct measure words from 
the measure word candidate set. For example, in 
Figure 1 the distance between the measure word 
??? and its head word ???? (undertaking) is 15. 
In this case, an n-gram language model with n<15 
cannot capture the MW-HW collocation. Table 1 
shows the relative position?s distribution of head 
words around measure words in the Chinese Penn 
Treebank, where a negative position indicates that 
the head word is to the left of the measure word 
and a positive position indicates that the head word 
is to the right of the measure word. Although lots 
of measure words are close to the head words they 
modify, more than sixteen percent of measure 
words are far away from their corresponding head 
words (the absolute distance is more than 5). 
To overcome the disadvantage of measure word 
generation in a general SMT system, this paper 
proposes a dedicated statistical model to generate 
measure words for English-to-Chinese translation. 
We model the probability of measure word gen-
eration by utilizing rich lexical and syntactic 
knowledge from both source and target sentences. 
Three steps are involved in our method to generate 
measure words: Identifying the positions to gener-
ate measure words, collecting the measure word 
candidate set and selecting the best measure word. 
Our method is performed as a post-processing pro-
cedure of the output of SMT systems. The advan-
tage is that it can be easily integrated into any SMT 
system. Experimental results show our method can 
significantly improve the quality of measure word 
generation. We also compared the performance of 
our model based on different contextual informa-
tion, and show that both large-scale monolingual 
data and parallel bilingual data can be helpful to 
generate correct measure words. 
Position Occurrence Position Occurrence
1 39.5% -1 0 
2 15.7% -2 0 
3 4.7% -3 8.7% 
4 1.4% -4 6.8% 
5 2.1% -5 4.3% 
>5 8.8% <-5 8.0% 
Table 1. Position distribution of head words 
2 Our Method 
2.1 Measure word  generation in Chinese 
In Chinese, measure words are obligatory in cer-
tain contexts, and the choice of measure word 
usually depends on the head word?s semantics (e.g., 
shape or material). The set of Chinese measure 
words is a relatively close set and can be classified 
into two categories based on whether they have a 
corresponding English translation. Those not hav-
ing an English counterpart need to be generated 
during translation. For those having English trans-
lations, such as ??? (meter), ??? (ton), we just 
use the translation produced by the SMT system 
itself. According to our survey, about 70.4% of 
measure words in the Chinese Penn Treebank need 
Figure 1.  Example of long distance dependency between MW and its modified HW 
??/??/ 
??/ ?/ 
? ?? 
Pudong 's de-
velopment and 
opening up is a century-spanning 
/?/?
?/ 
for vigorously promoting shanghai 
and constructing a modern econom-
ic , trade , and financial center  undertaking
??/??/ ?/ ?? /??? /??
/ ?/ ??/ ? /??/ ??/ ?/ 
? 
. 
?
90
 to be explicitly generated during the translation 
process. 
In Chinese, there are generally stable linguistic 
collocations between measure words and their head 
words. Once the head word is determined, the col-
located measure word can usually be selected ac-
cordingly. However, there is no easy way to identi-
fy head words in target Chinese sentences since for 
most of the time an SMT output is not a well 
formed sentence due to translation errors. Mistake 
of head word identification may cause low quality 
of measure word generation. In addition, some-
times the head word itself is not enough to deter-
mine the measure word. For example, in Chinese 
sentences ???? 5??? (there are five people 
in his family) and ???? 5???????? (a 
total of five people attended the meeting), where 
??? (people) is the head word collocated with two 
different measure words ??? and ???, we cannot 
determine the measure word just based on the head 
word ???.   
2.2 Framework 
In our framework, a statistical model is used to 
generate measure words. The model is applied to 
SMT system outputs as a post-processing proce-
dure. Given an English source sentence, an SMT 
decoder produces a target Chinese translation, in 
which positions for measure word generation are 
identified. Based on contextual information con-
tained in both input source sentence and SMT sys-
tem?s output translation, a measure word candidate 
set M is constructed. Then a measure word selec-
tion model is used to select the best one from M. 
Finally, the selected measure word is inserted into 
previously determined measure word slot in the 
SMT system?s output, yielding the final translation 
result. 
2.3 Measure word position identification 
To identify where to generate measure words in the 
SMT outputs, all positions after numerals are 
marked at first since measure words often follow 
numerals. For other cases in which measure words 
do not follow numerals (e.g., ??? /? /??? 
(many computers), where ??? is a measure word 
and ???? (computers) is its head word), we just 
mine the set of words which can be followed by 
measure words from training corpus.  Most of 
words in the set are pronouns such as ??? (this), 
??? (that) and ???? (several). In the SMT out-
put, the positions after these words are also identi-
fied as candidate positions to generate measure 
words.  
2.4 Candidate measure word generation 
To avoid high computation cost, the measure word 
candidate set only consists of those measure words 
which can form valid MW-HW collocations with 
their head words. We assume that all the surround-
ing words within a certain window size centered on 
the given position to generate a measure word are 
potential head words, and require that a measure 
word candidate must collocate with at least one of 
the surrounding words. Valid MW-HW colloca-
tions are mined from the training corpus and a sep-
arate lexicon resource.  
There is a possibility that the real head word is 
outside the window of given size. To address this 
problem, we also use a source window centered on 
the position ps, which is aligned to the target meas-
ure word position pt. The link between ps and pt 
can be inferred from SMT decoding result. Thus, 
the chance of capturing the best measure word in-
creases with the aid of words located in the source 
window. For example, given the window size of 10, 
although the target head word ???? (undertaking) 
in Figure 1 is located outside the target window, its 
corresponding source head word undertaking can 
be found in the source window. Based on this 
source head word, the best measure word ??? will 
be included into the candidate measure word set. 
This example shows how bilingual information can 
enrich the measure word candidate set. 
Another special word {NULL} is always in-
cluded in the measure word candidate set. {NULL} 
represents those measure words having a corres-
ponding English translation as mentioned in Sec-
tion 2.1. If {NULL} is selected, it means that we 
need not generate any measure word at the current 
position. Thus, no matter what kinds of measure 
words they are, we can handle the issue of measure 
word generation in a unified framework.  
2.5 Measure word selection model 
After obtaining the measure word candidate set M, 
a measure word selection model is employed to 
select the best one from M. Given the contextual 
information C in both source window and target 
91
 window, we model the measure word selection as 
finding the measure word m* with highest post-
erior probability given C: 
?? = argmax????(?|?)                  (1) 
To leverage the collocation knowledge between 
measure words and head words, we extend (1) by 
introducing a hidden variable h where H represents 
all candidate head words located within the target 
window: 
     ?? = argmax??? ? ?(?, ?|?)???  
           = argmax??? ? ?(?|?)?(?|?, ?)???   (2) 
In (2), ?(?|?) is the head word selection proba-
bility and is empirically estimated according to the 
position distribution of head words in Table 1. 
?(?|?, ?) is the conditional probability of m given 
both h and C. We use maximum entropy model to 
compute ?(?|?, ?): 
            ?(?|?, ?) = exp(? ?? ??(?,?)? )? exp(? ?? ??(??,?)? )????      (3) 
Based on the different features used in the com-
putation of ?(?|?, ?) , we can train two sub-
models ? a monolingual model (Mo-ME) which 
only uses monolingual (Chinese) features and a 
bilingual model (Bi-ME) which integrates bilingual 
features. The advantage of the Mo-ME model is 
that it can employ an unlimited monolingual target 
training corpora, while the Bi-ME model leverages 
rich features including both the source and target 
information and may improve the precision. Com-
pared to the Mo-ME model, the Bi-ME model suf-
fers from small scale of parallel training data. To 
leverage advantages of both models, we use a 
combined model Co-ME, by linearly combing the 
monolingual and bilingual sub-models: 
?? = argmax??????????  + (1 ? ?)??????  
where ? ? [0,1] is a free parameter that can be op-
timized on held-out data and it was set to 0.39 in 
our experiments. 
2.6 Features 
The computation of Formula (3) involves the fea-
tures listed in Table 2 where the Mo-ME model 
only employs target features and the Bi-ME model 
leverages both target features and source features.  
For target features, n-gram language model 
score is defined as the sum of log n-gram probabil-
ities within the target window after the measure 
word is filled into the measure word slot. The 
MW-HW collocation feature is defined to be a 
function f1 to capture the collocation between a 
measure word and a head word. For features of 
surrounding words, the feature function f2 is de-
fined as 1 if a certain word exists at a certain posi-
tion, otherwise 0. For example, f2(?,-2)=1 means 
the second word on the left is ???. f2(?,3)=1 
means the third word on the right is ???. For 
punctuation position feature function f3, the feature 
value is 1 when there is a punctuation following 
the measure word, which indicates the target head 
word may appear to the left of measure word. Oth-
erwise, it is 0. In practice, we can also ignore the 
position part, i.e., a word appears anywhere within 
the window is viewed as the same feature. 
 Target features Source features 
n-gram language model 
score 
MW-HW collocation
MW-HW collocation surrounding words 
surrounding words source head word 
punctuation position POS tags 
Table 2. Features used in our model 
For source language side features, MW-HW col-
location and surrounding words are used in a simi-
lar way as does with target features. The source 
head word feature is defined to be a function f4 to 
indicate whether a word ei is the source head word 
in English according to a parse tree of the source 
sentence. Similar to the definition of lexical fea-
tures, we also use a set of features based on POS 
tags of source language. 
3 Model Training and Application 
3.1 Training 
We parsed English and Chinese sentences to get 
training samples for measure word generation 
model. Based on the source syntax parse tree, for 
each measure word, we identified its head word by 
using a toolkit from (Chiang and Bikel, 2002) 
which can heuristically identify head words for 
sub-trees. For the bilingual corpus, we also per-
form word alignment to get correspondences be-
tween source and target words. Then, the colloca-
tion between measure words and head words and 
their surrounding contextual information are ex-
tracted to train the measure word selection models. 
According to word alignment results, we classify 
92
 measure words into two classes based on whether 
they have non-null translations. We map Chinese 
measure words having non-null translations to a 
unified symbol {NULL} as mentioned in Section 
2.4, indicating that we need not generate these kind 
of measure words since they can be translated from 
English.  
In our work, the Berkeley parser (Petrov and 
Klein, 2007) was employed to extract syntactic 
knowledge from the training corpus. We ran GI-
ZA++ (Och and Ney, 2000) on the training corpus 
in both directions with IBM model 4, and then ap-
plied the refinement rule described in (Koehn et al, 
2003) to obtain a many-to-many word alignment 
for each sentence pair. We used the SRI Language 
Modeling Toolkit (Stolcke, 2002) to train a five-
gram model with modified Kneser-Ney smoothing 
(Chen and Goodman, 1998). The Maximum Entro-
py training toolkit from (Zhang, 2006) was em-
ployed to train the measure word selection model. 
3.2 Measure word generation 
As mentioned in previous sections, we apply our 
measure word generation module into SMT output 
as a post-processing step. Given a translation from 
an SMT system, we first determine the position pt 
at which to generate a Chinese measure word. Cen-
tered on pt, a surrounding word window with spe-
cified size is determined. From translation align-
ments, the corresponding source position ps aligned 
to pt can be referred.  In the same way, a source 
window centered on ps is determined as well. Then, 
contextual information within the windows in the 
source and the target sentence is extracted and fed 
to the measure word selection model. Meanwhile, 
the candidate set is obtained based on words in 
both windows. Finally, each measure word in the 
candidate set is inserted to the position pt, and its 
score is calculated based on the models presented 
in Section 2.5. The measure word with the highest 
probability will be chosen.  
There are two reasons why we perform measure 
word generation for SMT systems as a post-
processing step. One is that in this way our method 
can be easily applied to any SMT system. The oth-
er is that we can leverage both source and target 
information during the measure word generation 
process. We do not integrate our measure word 
generation module into the SMT decoder since 
there is only little target contextual information 
available during SMT decoding. Moreover, as we 
will show in experiment section, a pre-processing 
method does not work well when only source in-
formation is available. 
4 Experiments 
4.1 Data 
In the experiments, the language model is a Chi-
nese 5-gram language model trained with the Chi-
nese part of the LDC parallel corpus and the Xin-
hua part of the Chinese Gigaword corpus with 
about 27 million words. We used an SMT system 
similar to Chiang (2005), in which FBIS corpus is 
used as the bilingual training data. The training 
corpus for Mo-ME model consists of the Chinese 
Peen Treebank and the Chinese part of the LDC 
parallel corpus with about 2 million sentences. The 
Bi-ME model is trained with FBIS corpus, whose 
size is smaller than that used in Mo-ME model 
training. 
We extracted both development and test data set 
from years of NIST Chinese-to-English evaluation 
data by filtering out sentence pairs not containing 
measure words. The development set is extracted 
from NIST evaluation data from 2002 to 2004, and 
the test set consists of sentence pairs from NIST 
evaluation data from 2005 to 2006. There are 759 
testing cases for measure word generation in our 
test data consisting of 2746 sentence pairs. We use 
the English sentences in the data sets as input to 
the SMT decoder, and apply our proposed method 
to generate measure words for the output from the 
decoder. Measure words in Chinese sentences of 
the development and test sets are used as refer-
ences. When there are more than one measure 
words acceptable at some places, we manually 
augment the references with multiple acceptable 
measure words. 
4.2 Baseline 
Our baseline is the SMT output where measure 
words are generated by a Hiero-like SMT decoder 
as discussed in Section 1. Due to noises in the Chi-
nese translations introduced by the SMT system, 
we cannot correctly identify all the positions to 
generate measure words. Therefore, besides preci-
sion we examine recall in our experiments. 
4.3 Evaluation over SMT output 
Table 3 and Table 4 show the precision and recall 
of our measure word generation method. From the 
93
 experimental results, the Mo-ME, Bi-ME and Co-
ME models all outperform the baseline. Compared 
with the baseline, the Mo-ME method takes advan-
tage of a large size monolingual training corpus 
and reduces the data sparseness problem. The ad-
vantage of the Bi-ME model is being able to make 
full use of rich knowledge from both source and 
target sentences. Also as shown in Table 3 and Ta-
ble 4, the Co-ME model always achieve the best 
results when using the same window size since it 
leverages the advantage of both the Mo-ME and 
the Bi-ME models. 
Wsize Baseline Mo-ME Bi-ME Co-ME
6  
 
54.82% 
64.29% 67.15%  67.66% 
8 64.93% 68.50%  69.00% 
10 64.72% 69.40% 69.58%
12 65.46% 69.40% 69.76%
14 65.61% 69.69%  70.03% 
Table 3. Precision over SMT output 
Wsize Baseline Mo-ME Bi-ME Co-ME
6  
 
45.61% 
51.48% 53.69%  54.09% 
8 51.98% 54.75%  55.14% 
10 51.81% 55.44% 55.58%
12 52.38% 55.44% 55.72%
14 52.50% 55.67%  55.93% 
Table 4. Recall over SMT output 
We can see that the Bi-ME model can achieve 
better results than the Mo-ME model in both recall 
and precision metrics although only a small sized 
bilingual corpus is used for Bi-ME model training. 
The reason is that the Mo-ME model cannot cor-
rectly handle the cases where head words are lo-
cated outside the target window. However, due to 
word order differences between English and Chi-
nese, when target head words are outside the target 
window, their corresponding source head words 
might be within the source window. The capacity 
of capturing head words is improved when both 
source and target windows are used, which demon-
strates that bilingual knowledge is useful for meas-
ure word generation. 
We compare the results for each model with dif-
ferent window sizes. Larger window size can lead 
to better results as shown in Table 3 and Table 4 
since more contextual knowledge is used to model 
measure word generation. However, enlarging the 
window size does not bring significant improve-
ments, The major reason is that even a small win-
dow size is already able to cover most of measure 
word collocations, as indicated by the position dis-
tribution of head words in Table 1.  
The quality of the SMT output also affects the 
quality of measure word generation since our me-
thod is performed in a post-processing step over 
the SMT output. Although translation errors de-
grade the measure word generation accuracy, we 
achieve about 15% improvement in precision and a 
10% increase in recall over baseline. We notice 
that the recall is relatively lower. Part of the reason 
is some positions to generate measure words are 
not successfully identified due to translation errors. 
In addition to precision and recall, we also evaluate 
the Bleu score (Papineni et al, 2002) changes be-
fore and after applying our measure word genera-
tion method to the SMT output. For our test data, 
we only consider sentences containing measure 
words for Bleu score evaluation. Our measure 
word generation step leads to a Bleu score im-
provement of 0.32 where the window size is set to 
10, which shows that it can improve the translation 
quality of an English-to-Chinese SMT system. 
4.4 Evaluation over reference data 
To isolate the impact of the translation errors in 
SMT output on the performance of our measure 
word generation model, we conducted another ex-
periment with reference bilingual sentences in 
which measure words in Chinese sentences are 
manually removed. This experiment can show the 
performance upper bound of our method without 
interference from an SMT system. Table 5 shows 
the results. Compared to the results in Table 3, the 
precision improvement in the Mo-ME model is 
larger than that in the Bi-ME model, which shows 
that noisy translation of the SMT system has more 
serious influence on the Mo-ME model than the 
Bi-ME model. This also indicates that source in-
formation without noises is helpful for measure 
word generation. 
Wsize Mo-ME Bi-ME Co-ME 
6 71.63% 74.92% 75.72% 
8 73.80% 75.48% 76.20% 
10 73.80% 74.76% 75.48% 
12 73.80% 75.24% 75.96% 
14 73.56% 75.48% 76.44% 
Table 5. Results over reference data 
94
 4.5 Impacts of features 
In this section, we examine the contribution of 
both target language based features and source 
language based features in our model. Table 6 and 
Table 7 show the precision and recall when using 
different features. The window size is set to 10. In 
the tables, Lm denotes the n-gram language model 
feature, Tmh denotes the feature of collocation be-
tween target head words and the candidate measure 
word, Smh denotes the feature of collocation be-
tween source head words and the candidate meas-
ure word, Hs denotes the feature of source head 
word selection, Punc denotes the feature of target 
punctuation position, Tlex denotes surrounding 
word features in translation, Slex denotes surround-
ing word features in source sentence, and Pos de-
notes Part-Of-Speech feature. 
Feature setting Precision Recall 
Baseline 54.82% 45.61% 
Lm 51.11% 41.24% 
+Tmh 61.43% 49.22% 
+Punc 62.54% 50.08% 
+Tlex 64.80% 51.87% 
Table 6. Feature contribution in Mo-ME model 
Feature setting Precision Recall 
Baseline 54.82% 45.61% 
Lm 51.11% 41.24% 
+Tmh+Smh 64.50% 51.64% 
+Hs 65.32% 52.26% 
+Punc 66.29% 53.10% 
+Pos 66.53% 53.25% 
+Tlex 67.50% 54.02% 
+Slex 69.52% 55.54% 
Table 7. Feature contribution in Bi-ME model 
The experimental results show that all the fea-
tures can bring incremental improvements. The 
method with only Lm feature performs worse than 
the baseline. However, with more features inte-
grated, our method outperforms the baseline, 
which indicates each kind of features we selected 
is useful for measure word generation. According 
to the results, the feature of MW-HW collocation 
has much contribution to reducing the selection 
error of measure words given head words. The 
contribution of Slex feature explains that other sur-
rounding words in source sentence are also helpful 
since head word determination in source language 
might be incorrect due to errors in English parse 
trees. Meanwhile, the contribution from Smh, Hs 
and Slex features demonstrates that bilingual 
knowledge can play an important role for measure 
word generation. Compared with lexicalized fea-
tures, we do not get much benefit from the Pos 
features. 
4.6 Error analysis 
We conducted an error analysis on 100 randomly 
selected sentences from the test data. There are 
four major kinds of errors as listed in Table 8. 
Most errors are caused by failures in finding posi-
tions to generate measure words. The main reason 
for this is some hint information used to identify 
measure word positions is missing in the noisy 
output of SMT systems. Two kinds of errors are 
introduced by incomplete head word and MW-HW 
collocation coverage, which can be solved by en-
larging the size of training corpus. There are also 
head word selection errors due to incorrect syntax 
parsing. 
Error type Ratio 
unseen head word  32.14% 
unseen MW-HW collocation 10.71% 
missing MW position 39.29% 
incorrect HW selection 10.71% 
others 7.14% 
Table 8. Error distribution 
4.7 Comparison with other methods 
In this section we compare our statistical methods 
with the pre-processing method and the rule-based 
methods for measure word generation in a transla-
tion task.  
In pre-processing method, only source language 
information is available. Given a source sentence, 
the corresponding syntax parse tree Ts is first con-
structed with an English parser. Then the pre-
processing method chooses the source head word 
hs based on Ts. The candidate measure word with 
the highest probability collocated with hs is se-
lected as the best result, where the measure word 
candidate set corresponding to each head word is 
mined over a bilingual training corpus in advance. 
We achieved precision 58.62% and recall 49.25%, 
which are worse than the results of our post-
processing based methods. The weakness of the 
pre-processing method is twofold. One problem is 
data sparseness with respect to collocations be-
95
 tween English head words and Chinese measure 
words. The other problem comes from the English 
head word selection error introduced by using 
source parse trees.  
We also compared our method with a well-
known rule-based machine translation system ? 
SYSTRAN3. We translated our test data with SY-
STRAN?s English-to-Chinese translation engine. 
The precision and recall are 63.82% and 51.09% 
respectively, which are also lower than our method.  
5 Related Work  
Most existing rule-based English-to-Chinese MT 
systems have a dedicated module handling meas-
ure word generation. In general a rule-based me-
thod uses manually constructed rule patterns to 
predict measure words. Like most rule based ap-
proaches, this kind of system requires lots of hu-
man efforts of experienced linguists and usually 
cannot easily be adapted to a new domain. The 
most relevant work based on statistical methods to 
our research might be statistical technologies em-
ployed to model issues such as morphology gener-
ation (Minkov et al, 2007). 
6 Conclusion and Future Work 
In this paper we propose a statistical model for 
measure word generation for English-to-Chinese 
SMT systems, in which contextual knowledge 
from both source and target sentences is involved. 
Experimental results show that our method not on-
ly achieves high precision and recall for generating 
measure words, but also improves the quality of 
English-to-Chinese SMT systems. 
In the future, we plan to investigate more fea-
tures and enlarge coverage to improve the quality 
of measure word generation, especially reduce the 
errors found in our experiments. 
Acknowledgements 
Special thanks to David Chiang, Stephan Stiller 
and the anonymous reviewers for their feedback 
and insightful comments. 
References 
Stanley F. Chen and Joshua Goodman. 1998. An Empir-
ical study of smoothing techniques for language 
                                                 
3 http://www.systransoft.com/ 
modeling. Technical Report TR-10-98, Harvard Uni-
versity Center for Research in Computing Technolo-
gy, 1998. 
David Chiang and Daniel M. Bikel. 2002. Recovering 
latent information in treebanks. Proceedings of COL-
ING '02, 2002.  
David Chiang. 2005. A hierarchical phrase-based mod-
el for statistical machine translation. In Proceedings 
of ACL 2005, pages 263-270. 
Philipp Koehn, Franz J. Och, and Daniel Marcu. 2003. 
Statistical phrase-based translation. In Proceedings of 
HLT-NAACL 2003, pages 127-133.  
Einat Minkov, Kristina Toutanova, and Hisami Suzuki. 
2007. Generating complex morphology for machine 
translation. In Proceedings of 45th Annual Meeting 
of the ACL, pages 128-135. 
Franz J. Och and Hermann Ney. 2000. Improved statis-
tical alignment models. In Proceedings of 38th An-
nual Meeting of the ACL, pages 440-447.  
Franz J. Och and Hermann Ney. 2004. The alignment 
template approach to statistical machine translation. 
Computational Linguistics, 30:417-449. 
Kishore Papineni, Salim Roukos, ToddWard, and WeiJ-
ing Zhu. 2002. BLEU: a method for automatic evalu-
ation of machine translation. In Proceedings of 40th 
Annual Meeting of the ACL, pages 311-318. 
Slav Petrov and Dan Klein. 2007. Improved inference 
for unlexicalized parsing. In Proceedings of HLT-
NAACL, 2007. 
Andreas Stolcke. 2002. SRILM - an extensible language 
modeling toolkit. In Proceedings of International 
Conference on Spoken Language Processing, volume 
2, pages 901-904.  
Le Zhang. MaxEnt toolkit. 2006. http://homepages.inf. 
ed.ac.uk/s0450736/maxent_toolkit.html  
96
Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP, pages 585?592,
Suntec, Singapore, 2-7 August 2009. c?2009 ACL and AFNLP
Collaborative Decoding: Partial Hypothesis Re-ranking 
Using Translation Consensus between Decoders 
 
Mu Li1, Nan Duan2, Dongdong Zhang1, Chi-Ho Li1, Ming Zhou1 
        1Microsoft Research Asia                                        2Tianjin University 
              Beijing, China                                                     Tianjin, China 
{muli,v-naduan,dozhang,chl,mingzhou}@microsoft.com 
 
 
Abstract 
This paper presents collaborative decoding 
(co-decoding), a new method to improve ma-
chine translation accuracy by leveraging trans-
lation consensus between multiple machine 
translation decoders. Different from system 
combination and MBR decoding, which post-
process the n-best lists or word lattice of ma-
chine translation decoders, in our method mul-
tiple machine translation decoders collaborate 
by exchanging partial translation results. Us-
ing an iterative decoding approach, n-gram 
agreement statistics between translations of 
multiple decoders are employed to re-rank 
both full and partial hypothesis explored in 
decoding. Experimental results on data sets for 
NIST Chinese-to-English machine translation 
task show that the co-decoding method can 
bring significant improvements to all baseline 
decoders, and the outputs from co-decoding 
can be used to further improve the result of 
system combination. 
1 Introduction 
Recent research has shown substantial improve-
ments can be achieved by utilizing consensus 
statistics obtained from outputs of multiple ma-
chine translation systems. Translation consensus 
can be measured either at sentence level or at 
word level. For example, Minimum Bayes Risk 
(MBR) (Kumar and Byrne, 2004) decoding over 
n-best list tries to find a hypothesis with lowest 
expected loss with respect to all the other transla-
tions, which can be viewed as sentence-level 
consensus-based decoding. Word based methods 
proposed range from straightforward consensus 
voting (Bangalore et al, 2001; Matusov et al, 
2006) to more complicated word-based system 
combination model (Rosti et al, 2007; Sim et al, 
2007). Typically, the resulting systems take out-
puts of individual machine translation systems as 
input, and build a new confusion network for 
second-pass decoding. 
There have been many efforts dedicated to ad-
vance the state-of-the-art performance by com-
bining multiple systems? outputs. Most of the 
work focused on seeking better word alignment 
for consensus-based confusion network decoding 
(Matusov et al, 2006) or word-level system 
combination (He et al, 2008; Ayan et al, 2008). 
In addition to better alignment, Rosti et al 
(2008) introduced an incremental strategy for 
confusion network construction; and Hildebrand 
and Vogel (2008) proposed a hypotheses re-
ranking model for multiple systems? outputs with 
more features including word translation proba-
bility and n-gram agreement statistics. 
A common property of all the work mentioned 
above is that the combination models work on 
the basis of n-best translation lists (full hypo-
theses) of existing machine translation systems. 
However, the n-best list only presents a very 
small portion of the entire search space of a Sta-
tistical Machine Translation (SMT) model while 
a majority of the space, within which there are 
many potentially good translations, is pruned 
away in decoding. In fact, due to the limitations 
of present-day computational resources, a consi-
derable number of promising possibilities have to 
be abandoned at the early stage of the decoding 
process. It is therefore expected that exploring 
additional possibilities beyond n-best hypotheses 
lists for full sentences could bring improvements 
to consensus-based decoding. 
In this paper, we present collaborative decod-
ing (or co-decoding), a new SMT decoding 
scheme to leverage consensus information be-
tween multiple machine translation systems. In 
this scheme, instead of using a post-processing 
step, multiple machine translation decoders col-
laborate during the decoding process, and trans-
lation consensus statistics are taken into account 
to improve ranking not only for full translations, 
but also for partial hypotheses. In this way, we 
585
expect to reduce search errors caused by partial 
hypotheses pruning, maximize the contribution 
of translation consensus, and result in better final 
translations. 
We will discuss the general co-decoding mod-
el, requirements for decoders that enable colla-
borative decoding and describe the updated mod-
el structures. We will present experimental re-
sults on the data sets of NIST Chinese-to-English 
machine translation task, and demonstrate that 
co-decoding can bring significant improvements 
to baseline systems.  We also conduct extensive 
investigations when different settings of co-
decoding are applied, and make comparisons 
with related methods such as word-level system 
combination of hypothesis selection from mul-
tiple n-best lists.  
The rest of the paper is structured as follows. 
Section 2 gives a formal description of the co-
decoding model, the strategy to apply consensus 
information and hypotheses ranking in decoding. 
In Section 3, we make detailed comparison be-
tween co-decoding and related work such as sys-
tem combination and hypotheses selection out of 
multiple systems.  Experimental results and dis-
cussions are presented in Section 4. Section 5 
concludes the paper. 
2 Collaborative Decoding 
2.1 Overview 
Collaborative decoding does not present a full 
SMT model as other SMT decoders do such as 
Pharaoh (Koehn, 2004) or Hiero (Chiang, 2005). 
Instead, it provides a framework that accommo-
dates and coordinates multiple MT decoders. 
Conceptually, collaborative decoding incorpo-
rates the following four constituents:  
1. Co-decoding model. A co-decoding model 
consists of a set of member models, which 
are a set of augmented baseline models. We 
call decoders based on member models 
member decoders, and those based on base-
line models baseline decoders. In our work, 
any Maximum A Posteriori (MAP) SMT 
model with log-linear formulation (Och, 
2002) can be a qualified candidate for a 
baseline model. The requirement for a log-
linear model aims to provide a natural way to 
integrate the new co-decoding features. 
2. Co-decoding features. Member models are 
built by adding additional translation consen-
sus -based co-decoding features to baseline 
models. A baseline model can be viewed as a 
special case of member model with all co-
decoding feature values set to 0. Accordingly, 
a baseline decoder can be viewed as a special 
setting of a member decoder. 
3. Decoder coordinating. In co-decoding, each 
member decoder cannot proceed solely based 
on its own agenda. To share consensus statis-
tics with others, the decoding must be per-
formed in a coordinated way.  
4. Model training. Since we use multiple inter-
related decoders and introduce more features 
in member models, we also need to address 
the parameter estimation issue in the frame-
work of co-decoding. 
In the following sub-sections we first establish a 
general model for co-decoding, and then present 
details of feature design and decoder implemen-
tation, as well as parameter estimation in the co-
decoding framework. We leave the investigation 
of using specific member models to the experi-
ment section. 
2.2 Generic Collaborative Decoding Model 
For a given source sentence f, a member model 
in co-decoding finds the best translation ?? 
among the set of possible candidate translations 
?(?) based on a scoring function ?: 
?? = argmax???(?)?(?) (1) 
In the following, we will use ??  to denote the 
???  member decoder, and also use the notation 
??(?) for the translation hypothesis space of f 
determined by ?? . The ?
??  member model can 
be written as: 
??  ? = ?? (?, ?) + ??(?,??(?))
?,???
 (2) 
where ?? (?, ?) is the score function of the ?
??  
baseline model, and each ??(?,??(?)) is a par-
tial consensus score function with respect to ??  
and is defined over e and ?? ? :  
?? ?,?? ?  = ?? ,?  ??,?(?,?? ? ) 
?
 (3) 
where each ?? ,?(?,?? ? ) is a feature function 
based on a consensus measure between e and 
?? ? , and ??,?  is the corresponding feature 
weight. Feature index l ranges over all consen-
sus-based features in Equation 3. 
2.3 Decoder Coordination 
Before discussing the design and computation of 
translation consensus -based features, we first 
586
describe the multiple decoder coordination issue 
in co-decoding. Note that in Equation 2, though 
the baseline score function ??  ?, ?  can be 
computed inside each decoder, the case of 
??(?,??(?))  is more complicated. Because 
usually it is not feasible to enumerate the entire 
hypothesis space for machine translation, we ap-
proximate ?? ?  with n-best hypotheses by 
convention. Then there is a circular dependency 
between co-decoding features and ??(?) : on 
one hand, searching for n-best approximation of 
??(?) requires using Equation 2 to select top-
ranked hypotheses; while on the other hand, Eq-
uation 2 cannot be computed until every ??(?) 
is available.  
We address this issue by employing a boot-
strapping method, in which the key idea is that 
we can use baseline models? n-best hypotheses 
as seeds, and iteratively refine member models? 
n-best hypotheses with co-decoding. Similar to a 
typical phrase-based decoder (Koehn, 2004), we 
associate each hypothesis with a coverage vector 
c to track translated source words in it. We will 
use ??(?,?) for the set of hypotheses associated 
with c, and we also denote with ??(?) =
 ??(?,?)?  the set of all hypotheses generated 
by member decoder ??  in decoding. The co-
decoding process can be described as follows: 
1. For each member decoder ?? , perform de-
coding with a baseline model, and memorize 
all translation hypotheses generated during 
decoding in ??(?); 
2. Re-group translation hypotheses in ??(?) 
into a set of buckets  ?? ?,?  by the cover-
age vector c associated with each hypothesis; 
3. Use member decoders to re-decode source 
sentence ? with member models. For mem-
ber decoder ?? , consensus-based features of 
any hypotheses associated with coverage 
vector c are computed based on current set-
ting of ?? ?,?  for all s but k. New hypo-
theses generated by ??  in re-decoding are 
cached in ??
? (?); 
4. Update all ??(?) with ??
? (?); 
5. Iterate from step 2 to step 4 until a preset 
iteration limit is reached. 
In the iterative decoding procedure described 
above, hypotheses of different decoders can be 
mutually improved. For example, given two de-
coders ?1  and ?2  with hypotheses sets ?1  and 
?2 , improvements on ?1  enable ?2  to improve 
?2, and in turn ?1 benefits from improved ?2, 
and so forth. 
Step 2 is used to facilitate the computation of 
feature functions ?? ,?(?,?? ? ) , which require 
both e and every hypothesis in ?? ?   should be 
translations of the same set of source words. This 
step seems to be redundant for CKY-style MT 
decoders (Liu et al, 2006; Xiong et al, 2006; 
Chiang, 2005) since the grouping is immediately 
available from decoders because all hypotheses 
spanning the same range of source sentence have 
been stacked together in the same chart cell. But 
to be a general framework, this step is necessary 
for some state-of-the-art phrase-based decoders 
(Koehn, 2007; Och and Ney, 2004) because in 
these decoders, hypotheses with different cover-
age vectors can co-exist in the same bin, or hypo-
theses associated with the same coverage vector 
might appear in different bins.  
Note that a member model does not enlarge 
the theoretical search space of its baseline model, 
the only change is hypothesis scoring. By re-
running a complete decoding process, member 
model can be applied to re-score all hypotheses 
explored by a decoder. Therefore step 3 can be 
viewed as full-scale hypothesis re-ranking be-
cause the re-ranking scope is beyond the limited 
n-best hypotheses currently cached in ?? .  
In the implementation of member decoders, 
there are two major modifications compared to 
their baseline decoders. One is the support for 
co-decoding features, including computation of 
feature values and the use of augmented co-
decoding score function (Equation 2) for hypo-
thesis ranking and pruning. The other is hypothe-
sis grouping based on coverage vector and a me-
chanism to effectively access grouped hypothes-
es in step 2 and step 3. 
2.4 Co-decoding Features 
We now present the consensus-based feature 
functions  ?? ,?(?,?? ? ) introduced in Equation 
3. In this work all the consensus-based features 
have the following formulation: 
?? ,? ?,?? ?  =  ? ?? ?? ??(?, ??)
????? ? 
 (4) 
where e is a translation of f by decoder ?? (? ?
?), ? ?  is a translation in ?? ?  and ? ?? ??  is 
the posterior probability of translation ? ?  deter-
mined by decoder ??  given source sentence f. 
??(?, ??) is a consensus measure defined on e and 
??, by varying which different feature functions 
can be obtained.  
587
Referring to the log-linear model formulation, 
the translation posterior ? ?? ??  can be com-
puted as: 
? ?? ?? =
exp ??? ??  
 exp ??? ???  ??? ??? ? 
 (5) 
where ??(?) is the score function given in Equa-
tion 2, and  ? is a scaling factor following the 
work of Tromble et al (2008) 
To compute the consensus measures, we fur-
ther decompose each ?? ?, ??  into n-gram 
matching statistics between e and ??. Here we do 
not discriminate among different lexical n-grams 
and are only concerned with statistics aggrega-
tion of all n-grams of the same order. For each n-
gram of order n, we introduce a pair of comple-
mentary consensus measure functions ??+ ?, ??  
and ??? ?, ??  described as follows: 
??+ ?, ?
?  is the n-gram agreement measure 
function which counts the number of occurrences 
in ? ?of n-grams in e. So the corresponding fea-
ture value will be the expected number of occur-
rences in ?? ?  of all n-grams in e:  
??+ ?, ?? = ?(??
?+??1 , ??)
 ? ??+1
?=1
 
where ?(?,?)  is a binary indicator function ? 
? ??
?+??1 , ??  is 1 if the n-gram ??
?+??1 occurs in 
? ?  and 0 otherwise. 
??? ?, ?
?  is the n-gram disagreement meas-
ure function which is complementary to 
??+ ?, ?
? : 
??? ?, ?? =  1? ? ??
?+??1 , ??  
 ? ??+1
?=1
 
This feature is designed because ??+ ?, ?
?  
does not penalize long translation with low pre-
cision. Obviously we have the following: 
??+ ?, ?? + ??? ?, ?? =  ? ? ? + 1 
So if the weights of agreement and disagree-
ment features are equal, the disagreement-based 
features will be equivalent to the translation 
length features. Using disagreement measures 
instead of translation length there could be two 
potential advantages: 1) a length feature has been 
included in the baseline model and we do not 
need to add one; 2) we can scale disagreement 
features independently and gain more modeling 
flexibility. 
Similar to a language model score, n-gram 
consensus -based feature values cannot be 
summed up from smaller hypotheses. Instead, it 
must be re-computed when building each new 
hypothesis. 
2.5 Model Training 
We adapt the Minimum Error Rate Training 
(MERT) (Och, 2003) algorithm to estimate pa-
rameters for each member model in co-decoding. 
Let ??  be the feature weight vector for member 
decoder ?? , the training procedure proceeds as 
follows: 
1. Choose initial values for ?1 ,? ,??   
2. Perform co-decoding using all member de-
coders on a development set D with 
?1 ,? ,?? . For each decoder ?? , find a new 
feature weight vector ??
?  which optimizes 
the specified evaluation criterion L on D us-
ing the MERT algorithm based on the n-best 
list ??  generated by ?? : 
??
? = argmax? ? (?|?,??  ,?)) 
where T denotes the translations selected by 
re-ranking the translations in ??  using a 
new feature weight vector ? 
3. Let ?1 = ?1
? ,? ,?? = ??
?  and repeat step 2 
until convergence or a preset iteration limit is 
reached. 
 
Figure 1. Model training for co-decoding 
In step 2, there is no global criterion to optim-
ize the co-decoding parameters across member 
models. Instead, parameters of different member 
models are tuned to maximize the evaluation cri-
teria on each member decoder?s own n-best out-
put.  Figure 1 illustrates the training process of 
co-decoding with 2 member decoders. 
Source sentence
decoder
1
decoder
2
?1
MERT
?2
MERT
co-decoding
ref
1?? 2??
588
2.6 Output Selection 
Since there is more than one model in co-
decoding, we cannot rely on member model?s 
score function to choose one best translation 
from multiple decoders? outputs because the 
model scores are not directly comparable. We 
will examine the following two system combina-
tion -based solutions to this task: 
? Word-level system combination (Rosti et al, 
2007) of member decoders? n-best outputs  
? Hypothesis selection from combined n-best 
lists as proposed in Hildebrand  and Vogel 
(2008) 
3 Experiments 
In this section we present experiments to eva-
luate the co-decoding method. We first describe 
the data sets and baseline systems. 
3.1 Data and Metric 
We conduct our experiments on the test data 
from the NIST 2005 and NIST 2008 Chinese-to-
English machine translation tasks. The NIST 
2003 test data is used for development data to 
estimate model parameters. Statistics of the data 
sets are shown in Table 1. In our experiments all 
the models are optimized with case-insensitive 
NIST version of BLEU score and we report re-
sults using this metric in percentage numbers. 
 
Data set # Sentences # Words 
NIST 2003 (dev) 919 23,782 
NIST 2005 (test) 1,082 29,258 
NIST 2008 (test) 1,357 31,592 
Table 1: Data set statistics 
We use the parallel data available for the 
NIST 2008 constrained track of Chinese-to-
English machine translation task as bilingual 
training data, which contains 5.1M sentence 
pairs, 128M Chinese words and 147M English 
words after pre-processing. GIZA++ is used to 
perform word alignment in both directions with 
default settings, and the intersect-diag-grow me-
thod is used to generate symmetric word align-
ment refinement. 
The language model used for all models (in-
clude decoding models and system combination 
models described in Section 2.6) is a 5-gram 
model trained with the English part of bilingual 
data and xinhua portion of LDC English Giga-
word corpus version 3. 
3.2 Member Decoders 
We use three baseline decoders in the experi-
ments. The first one (SYS1) is re-implementation 
of Hiero, a hierarchical phrase-based decoder. 
Phrasal rules are extracted from all bilingual sen-
tence pairs, while rules with variables are ex-
tracted only from selected data sets including 
LDC2003E14, LDC2003E07, LDC2005T06 and 
LDC2005T10, which contain around 350,000 
sentence pairs, 8.8M Chinese words and 10.3M 
English words. The second one (SYS2) is a BTG 
decoder with lexicalized reordering model based 
on maximum entropy principle as proposed by 
Xiong et al (2006). We use all the bilingual data 
to extract phrases up to length 3. The third one 
(SYS3) is a string-to-dependency tree ?based 
decoder as proposed by Shen et al (2008). For 
rule extraction we use the same setting as in 
SYS1. We parsed the language model training 
data with Berkeley parser, and then trained a de-
pendency language model based on the parsing 
output. All baseline decoders are extended with 
n-gram consensus ?based co-decoding features 
to construct member decoders. By default, the 
beam size of 20 is used for all decoders in the 
experiments. We run two iterations of decoding 
for each member decoder, and hold the value of 
?  in Equation 5 as a constant 0.05, which is 
tuned on the test data of NIST 2004 Chinese-to-
English machine translation task. 
3.3 Translation Results 
We first present the overall results of co-
decoding on both test sets using the settings as 
we described. For member decoders, up to 4-
gram agreement and disagreement features are 
used. We also implemented the word-level sys-
tem combination (Rosti et al, 2007) and the hy-
pothesis selection method (Hildebrand and Vogel, 
2008). 20-best translations from all decoders are 
used in the experiments for these two combina-
tion methods. Parameters for both system com-
bination and hypothesis selection are also tuned 
on NIST 2003 test data. The results are shown in 
Table 2. 
 
 NIST 2005 NIST 2008 
SYS1 38.66/40.08 27.67/29.19 
SYS2 38.04/39.93 27.25/29.14 
SYS3 39.50/40.32 28.75/29.68 
Word-level Comb 40.45/40.85 29.52/30.35 
Hypo Selection 40.09/40.50 29.02/29.71 
Table 2: Co-decoding results on test data 
589
In the Table 2, the results of a member decod-
er and its corresponding baseline decoder are 
grouped together with the later one for the mem-
ber decoders. On both test sets, every member 
decoder performs significantly better than its 
baseline decoder (using the method proposed in 
Koehn (2004) for statistical significance test).  
We apply system combination methods to the 
n-best outputs of both baseline decoders and 
member decoders. We notice that we can achieve 
even better performance by applying system 
combination methods to member decoders? n-
best outputs. However, the improvement margins 
are smaller than those of baseline decoders on 
both test sets. This could be the result of less di-
versified outputs from co-decoding than those 
from baseline decoders. In particular, the results 
for hypothesis selection are only slightly better 
than the best system in co-decoding.  
We also evaluate the performance of system 
combination using different n-best sizes, and the 
results on NIST 2005 data set are shown in Fig-
ure 2, where bl- and co- legends denote combina-
tion results of baseline decoding and co-decoding 
respectively. From the results we can see that 
combination based on co-decoding?s outputs per-
forms consistently better than that based on base-
line decoders? outputs for all n-best sizes we ex-
perimented with. However, we did not observe 
any significant improvements for both combina-
tion schemes when n-best size is larger than 20. 
 
Figure 2. Performance of system combination 
with different sizes of n-best lists 
One interesting observation in Table 2 is that 
the performance gap between baseline decoders 
is narrowed through co-decoding. For example, 
the 1.5 points gap between SYS2 and SYS3 on 
NIST 2008 data set is narrowed to 0.5. Actually 
we find that the TER score between two member 
decoders? outputs are significantly reduced (as 
shown in Table 3), which indicates that the out-
puts become more similar due to the use of con-
sensus information. For example, the TER score 
between SYS2 and SYS3 of the NIST 2008 out-
puts are reduced from 0.4238 to 0.2665.  
 
 NIST 2005 NIST 2008 
SYS1 vs. SYS2 0.3190/0.2274 0.4016/0.2686 
SYS1 vs. SYS3 0.3252/0.1840 0.4176/0.2469 
SYS2 vs. SYS3 0.3498/0.2171 0.4238/0.2665 
Table 3: TER scores between co-decoding  
translation outputs 
In the rest of this section we run a series of 
experiments to investigate the impacts of differ-
ent factors in co-decoding. All the results are 
reported on NIST 2005 test set.  
We start with investigating the performance 
gain due to partial hypothesis re-ranking. Be-
cause Equation 3 is a general model that can be 
applied to both partial hypothesis and n-best (full 
hypothesis) re-scoring, we compare the results of 
both cases. Figure 3 shows the BLEU score 
curves with up to 1000 candidates used for re-
ranking. In Figure 3, the suffix p denotes results 
for partial hypothesis re-ranking, and f for n-best 
re-ranking only. For partial hypothesis re-
ranking, obtaining more top-ranked results re-
quires increasing the beam size, which is not af-
fordable for large numbers in experiments. We 
work around this issue by approximating beam 
sizes larger than 20 by only enlarging the beam 
size for the span covering the entire source sen-
tence. From Figure 3 we can see that all decoders 
can gain improvements before the size of candi-
date set reaches 100. When the size is larger than 
50, co-decoding performs consistently and sig-
nificantly better than the re-ranking results on 
any baseline decoder?s n-best outputs.  
 
Figure 3. Partial hypothesis vs. n-best re-ranking 
results on NIST 2005 test data 
Figure 4 shows the BLEU scores of a two-
system co-decoding as a function of re-decoding 
iterations. From the results we can see that the 
results for both decoders converge after two ite-
rations.  
In Figure 4, iteration 0 denotes decoding with 
baseline model. The setting of iteration 1 can be 
viewed as the case of partial co-decoding, in 
39.5
39.8
40.0
40.3
40.5
40.8
41.0
41.3
10 20 50 100 200
bl-comb
co-comb
bl-hyposel
co-hyposel
38.0
38.5
39.0
39.5
40.0
40.5
41.0
41.5
10 20 50 100 200 500 1000
SYS1f
SYS2f
SYS3f
SYS1p
SYS2p
SYS3p
590
which one decoder uses member model and the 
other keeps using baseline model. The results 
show that member models help each other: al-
though improvements can be made using a single 
member model, best BLEU scores can only be 
achieved when both member models are used as 
shown by the results of iteration 2. The results 
also help justify the independent parameter esti-
mation of member decoders described in Section 
2.5, since optimizing the performance of one de-
coder will eventually bring performance im-
provements to all member decoders. 
 
Figure 4. Results using incremental iterations  
in co-decoding 
Next we examine the impacts of different con-
sensus-based features in co-decoding. Table 4 
shows the comparison results of a two-system 
co-decoding using different settings of n-gram 
agreement and disagreement features. It is clear-
ly shown that both n-gram agreement and disa-
greement types of features are helpful, and using 
them together is the best choice. 
 SYS1 SYS2 
Baseline 38.66 38.04 
+agreement ?disagreement 39.36 39.02 
?agreement +disagreement  39.12 38.67 
+agreement +disagreement 39.68 39.61 
Table 4: Co-decoding with/without n-gram 
agreement and disagreement features 
In Table 5 we show in another dimension the 
impact of consensus-based features by restricting 
the maximum order of n-grams used to compute 
agreement statistics. 
 SYS1 SYS2 
1 38.75 38.27 
2  39.21 39.10 
3 39.48 39.25 
4 39.68 39.61 
5 39.52 39.36 
6 39.58 39.47 
Table 5: Co-decoding with varied n-gram agree-
ment and disagreement features 
From the results we do not observe BLEU im-
provement for ? > 4. One reason could be that 
the data sparsity for high-order n-grams leads to 
over fitting on development data. 
We also empirically investigated the impact of 
scaling factor ? in Equation 5. It is observed in 
Figure 5 that the optimal value is between 0.01 ~ 
0.1 on both development and test data.  
 
Figure 5. Impact of scaling factor ?  
4 Discussion 
Word-level system combination (system combi-
nation hereafter) (Rosti et al, 2007; He et al, 
2008) has been proven to be an effective way to 
improve machine translation quality by using 
outputs from multiple systems. Our method is 
different from system combination in several 
ways. System combination uses unigram consen-
sus only and a standalone decoding model irrele-
vant to single decoders. Our method uses agree-
ment information of n-grams, and consensus fea-
tures are integrated into decoding models. By 
constructing a confusion network, system com-
bination is able to generate new translations dif-
ferent from any one in the input n-best lists, 
while our method does not extend the search 
spaces of baseline decoding models. Member 
decoders only change the scoring and ranking of 
the candidates in the search spaces. Results in 
Table 2 show that these two approaches can be 
used together to obtain further improvements. 
The work on multi-system hypothesis selec-
tion of Hildebrand and Vogel (2008) bears more 
resemblance to our method in that both make use 
of n-gram agreement statistics. They also empiri-
cally show that n-gram agreement is the most 
important factor for improvement apart from 
language models.  
Lattice MBR decoding (Tromble et al, 2008) 
also uses n-gram agreement statistics. Their work 
focuses on exploring larger evidence space by 
using a translation lattice instead of the n-best list. 
They also show the connection between expected 
n-gram change and corpus Log-BLEU loss. 
37.5
38.0
38.5
39.0
39.5
40.0
0 1 2 3 4
SYS1
SYS2
38.0
38.5
39.0
39.5
40.0
0 0.01 0.03 0.05 0.1 0.2 0.5 1
Dev SYS1
Dev SYS2
Test SYS1
Test SYS2
591
5 Conclusion 
Improving machine translation with multiple sys-
tems has been a focus in recent SMT research. In 
this paper, we present a framework of collabora-
tive decoding, in which multiple MT decoders 
are coordinated to search for better translations 
by re-ranking partial hypotheses using aug-
mented log-linear models with translation con-
sensus -based features. An iterative approach is 
proposed to re-rank all hypotheses explored in 
decoding. Experimental results show that with 
collaborative decoding every member decoder 
performs significantly better than its baseline 
decoder. In the future, we will extend our method 
to use lattice or hypergraph to compute consen-
sus statistics instead of n-best lists. 
References  
Necip Fazil Ayan,  Jing Zheng, and Wen Wang. 2008. 
Improving alignments for better confusion net-
works for combining machine translation systems. 
In Proc. Coling, pages 33-40. 
Srinivas Bangalore, German Bordel and Giuseppe 
Riccardi. 2001. Computing consensus translation 
from multiple machine translation systems. In 
Proc. ASRU, pages 351-354. 
David Chiang. 2005. A hierarchical phrase-based 
model for statistical machine translation. In Proc. 
ACL, pages 263-270. 
Xiaodong He, Mei Yang, Jianfeng Gao, Patrick 
Nguyen, and Robert Moore. 2008. Indirect-hmm-
based hypothesis for combining outputs from ma-
chine translation systems.  In Proc. EMNLP, pages 
98-107. 
Almut Silja Hildebrand and Stephan Vogel. 2008. 
Combination of machine translation systems via 
hypothesis selection from combined n-best lists. In 
8th AMTA conference, pages 254-261. 
Philipp Koehn, 2004. Statistical significance tests for 
machine translation evaluation. In Proc. EMNLP. 
Philipp Koehn, 2004. Pharaoh: a beam search decoder 
for phrase-based statistical machine translation 
model. In Proc. 6th AMTA Conference, pages 115-
124. 
Philipp Koehn, Hieu Hoang, Alexandra Brich, Chris 
Callison-Burch, Marcello Federico, Nicola Bertol-
di, Brooke Cowan, Wade Shen, Christine Moran, 
Richard Zens, Chris Dyer, Ondrej Bojar, Alexandra 
Constantin, Evan Herbst. 2007. Moses: open 
source toolkit for statistical machine translation. In 
Proc. ACL, demonstration session. 
Shankar Kumar and William Byrne 2004. Minimum 
Bayes-Risk Decoding for Statistical Machine 
Translation. In HLT-NAACL, pages 169-176. 
Yang Liu, Qun Liu, Shouxun Lin. 2006. Tree-to-
string alignment template for statistical machine 
translation. In Proc. ACL-Coling, pages 609-616. 
Evgeny Matusov, Nicola Ueffi ng, and Hermann Ney. 
2006. Computing consensus translation from mul-
tiple machine translation systems using enchanced 
hypotheses alignment. In Proc. EACL, pages 33-
40. 
Franz Och and Hermann Ney. 2002. Discriminative 
training and maximum entropy models for statis-
tical machine translation. In Proc. ACL, pages 295-
302. 
Franz Och. 2003. Minimum error rate training in sta-
tistical machine translation. In Proc. ACL, pages 
160-167. 
Franz Och and Hermann Ney. 2004. The alignment 
template approach to statistical machine transla-
tion. Computational Linguistics, 30(4), pages 417-
449 
Antti-Veikko Rosti, Necip Fazil Ayan, Bing Xiang, 
Spyros Matsoukas, Richard Schwartz, and Bonnie 
Dorr. 2007. Combining outputs from multiple ma-
chine translation systems. In HLT-NAACL, pages 
228-235 
Antti-Veikko Rosti, Bing Zhang, Spyros Matsoukas, 
and Richard Schwartz. 2008. Incremental hypothe-
sis alignment for building confusion networks with 
application to machine translation system combina-
tion. In Proc. Of the Third ACL Workshop on Sta-
tistical Machine Translation, pages 183-186. 
K.C. Sim, W. Byrne, M. Gales, H. Sahbi, and P. 
Woodland. 2007. Consensus network decoding for 
statistical machine translation system combination. 
In ICASSP. 
Libin Shen, Jinxi Xu, and Ralph Weischedel. 2008. A 
new string-to-dependency machine translation al-
gorithm with a target dependency language model. 
In Proc. HLT-ACL, pages 577-585. 
Roy W. Tromble, Shankar Kumar, Franz Och, and 
Wolfgang Macherey. 2008. Lattice minimum 
bayes-risk decoding for statistical machine transla-
tion. In Proc. EMNLP, pages 620-629. 
Deyi Xiong, Qun Liu and Shouxun Lin. 2006. Maxi-
mum entropy based phrase reordering model for 
statistical machine translation. In Proc. ACL, pages 
521-528. 
 
592
Proceedings of the Third Workshop on Statistical Machine Translation, pages 1?8,
Columbus, Ohio, USA, June 2008. c?2008 Association for Computational Linguistics
An Empirical Study in Source Word Deletion
for Phrase-based Statistical Machine Translation
Chi-Ho Li, Dongdong Zhang, Mu Li, Ming Zhou
Microsoft Research Asia
Beijing, China
chl, dozhang@microsoft.com
muli, mingzhou@microsoft.com
Hailei Zhang
Northeastern University of China
Shenyang, China
hailei.zh@gmail.com
Abstract
The treatment of ?spurious? words of source
language is an important problem but often
ignored in the discussion on phrase-based
SMT. This paper explains why it is impor-
tant and why it is not a trivial problem, and
proposes three models to handle spurious
source words. Experiments show that any
source word deletion model can improve a
phrase-based system by at least 1.6 BLEU
points and the most sophisticated model
improves by nearly 2 BLEU points. This
paper also explores the impact of training
data size and training data domain/genre on
source word deletion.
1 Introduction
It is widely known that translation is by no
means word-to-word conversion. Not only be-
cause sometimes a word in some language trans-
lates as more than one word in another language,
also every language has some ?spurious? words
which do not have any counterpart in other lan-
guages. Consequently, an MT system should be
able to identify the spurious words of the source
language and not translate them, as well as to gen-
erate the spurious words of the target language.
This paper focuses on the first task and studies
how it can be handled in phrase-based SMT.
An immediate reaction to the proposal of inves-
tigating source word deletion (henceforth SWD)
is: Is SWD itself worth our attention? Isn?t it a
trivial task that can be handled easily by existing
techniques? One of the reasons why we need to
pay attention to SWD is its significant improve-
ment to translation performance, which will be
shown by the experiments results in section 4.2.
Another reason is that SWD is not a trivial task.
While some researchers think that the spurious
words of a language are merely function words
or grammatical particles, which can be handled
by some simple heuristics or statistical means,
there are in fact some tricky cases of SWD which
need sophisticated solution. Consider the follow-
ing example in Chinese-to-English translation: in
English we have the subordinate clause ?accord-
ing to NP?, where NP refers to some source of
information. The Chinese equivalent of this
clause can sometimes be ?ACCORDING-TO/??
NP EXPRESS/,+?; that is, in Chinese we could
have a clause rather than a noun phrase following
the preposition ACCORDING-TO/??. There-
fore, when translating Chinese into English, the
content word EXPRESS/,+ should be consid-
ered spurious and not to be translated. Of course,
the verb EXPRESS/,+ is not spurious in other
contexts. It is an example that SWD is not only
about a few function words, and that the solu-
tion to SWD has to take context-sensitive factors
into account. Moreover, the solution needed for
such tricky cases seems to be beyond the scope
of current phrase-based SMT, unless we have a
very large amount of training data which cov-
ers all possible variations of the Chinese pattern
?ACCORDING-TO/?? NP EXPRESS/,+?.
Despite the obvious need for handling spuri-
ous source words, it is surprising that phrase-
based SMT, which is a major approach to SMT,
does not well address the problem. There are
two possible ways for a phrase-based system to
deal with SWD. The first one is to allow a source
1
language phrase to translate to nothing. How-
ever, no existing literature has mentioned such
a possibility and discussed the modifications re-
quired by such an extension. The second way is
to capture SWD within the phrase pairs in trans-
lation table. That is, suppose there is a foreign
phrase F? = (fAfBfC) and an English phrase
E? = (eAeC), where fA is aligned to eA and fC
to eC , then the phrase pair (F? , E?) tacitly deletes
the spurious word fB . Such a SWD mechanism
fails when data sparseness becomes a problem. If
the training data does not have any word sequence
containing fB , then the spurious fB cannot asso-
ciate with other words to form a phrase pair, and
therefore cannot be deleted tacitly in some phrase
pair. Rather, the decoder can only give a phrase
segmentation that treats fB itself as a phrase, and
this phrase cannot translate into nothing, as far
as the SMT training and decoding procedure re-
ported by existing literature are used. In sum, the
current mechanism of phrase-based SMT is not
capable of handling all cases of SWD.
In this paper, we will present, in section 3, three
SWD models and elaborate how to apply each
of them to phrase-based SMT. Experiment set-
tings are described in section 4.1, followed by the
report and analysis of experiment results, using
BLEU as evaluation metric, in section 4.2, which
also discusses the impact of training data size and
training data domain on SWD models. Before
making our conclusions, the effect of SWD on an-
other evaluation metric, viz. METEOR, is exam-
ined in section 5.
2 Literature Review
Research work in SMT seldom treats SWD as
a problem separated from other factors in trans-
lation. However, it can be found in differ-
ent SMT paradigms the mechanism of handling
SWD. As to the pioneering IBM word-based
SMT models (Brown et al, 1990), IBM mod-
els 3, 4 and 5 handle spurious source words by
considering them as corresponding to a particular
EMPTY word token on the English side, and by the
fertility model which allows the English EMPTY
to generate a certain number of foreign words.
As to the hierarchical phrase-based ap-
proach (Chiang, 2007), its hierarchical rules are
more powerful in SWD than the phrase pairs
in conventional phrase-based approach. For
instance, the ?ACCORDING-TO/?? NP EX-
PRESS/,+? example in the last section can be
handled easily by the hierarchical rule
X ?<?? X,+, according to X > .
In general, if the deletion of a source word
depends on some context cues, then the hier-
archical approach is, at least in principle, ca-
pable of handling it correctly. However, it is
still confronted by the same problem as the con-
ventional phrase-based approach regarding those
words whose ?spuriousness? does not depend on
any context.
3 Source Word Deletion Models
This section presents a number of solutions to the
problem of SWD. These solutions share the same
property that a specific empty symbol ? on the tar-
get language side is posited and any source word
is allowed to translate into ?. This symbol is in-
visible in every module of the decoder except the
translation model. That is, ? is not counted when
calculating language model score, word penalty
and any other feature values, and it is omitted in
the final output of the decoder. It is only used to
delete spurious source words and refine transla-
tion model scores accordingly.
It must be noted that in our approach phrases
comprising more than one source word are not al-
lowed to translate into ?. This constraint is based
on our subjective evaluation of alignment matrix,
which indicates that the un-alignment of a con-
tinuous sequence of two or more source words is
far less accurate than the un-alignment of a sin-
gle source word lying within aligned neighbors.
Consequently, in order to treat a source word as
spurious, the decoder must give a phrase segmen-
tation that treats the word itself as a phrase.
Another important modification to the phrase-
based architecture is a new feature added to the
log-linear model. The new feature, ?-penalty, rep-
resents how many source words translate into ?.
The purpose of this feature is the same as that
of the feature of word penalty. As many features
used in the log-linear model have values of log-
arithm of probability, candidate translations with
more words have, in general, lower scores, and
2
Model 1 P (?)
Model 2 P (?|f)
Model 3 PCRF (?|~F (f)
Table 1: Summary of the Three SWD Models
therefore the decoder has a bias towards shorter
translations. Word penalty (in fact, it should be
renamed as word reward) is used to neutralize
this bias. Similarly, the more source words trans-
late into ?, the shorter the translation will be,
and therefore the higher score the translation will
have. The ?-penalty is proposed to neutralize the
bias towards shorter translations.
The core of the solutions is the SWD model,
which calculates P (?|f), the probability distribu-
tion of translating some source word f to ?. Three
SWD models will be elaborated in the following
subsections. They differ from each other by the
conditions of the probability distribution, as sum-
marized in Table 1. Model 1 is a uniform prob-
ability distribution that does not take the source
word f into account. Model 2 is a simple proba-
bility distribution conditioned on the lexical form
of f only. Model 3 is a more complicated distribu-
tion conditioned on a feature vector of f , and the
distribution is estimated by the method of Condi-
tional Random Field.
3.1 Model 1: Uniform Probability
The first model assumes a uniform probability
of translation to ?. This model is inspired by
the HMM-based alignment model (Och and Ney,
2000a), which posits a probability P0 for align-
ment of some source word to the empty word
on the target language side, and weighs all other
alignment probabilities by the factor 1 ? P0. In
the same style, SWD model 1 posits a probability
P (?) for the translation of any source word to ?.
The probabilities of normal phrase pairs should
be weighed accordingly. For a source phrase
containing only one word, its weight is simply
P (??) = 1 ? P (?). As to a source phrase con-
taining more than one word, it implies that every
word in the phrase does not translate into ?, and
therefore the weighing factor P (??) should be mul-
tiplied as many times as the number of words in
the source phrase. In sum, for any phrase pair
< F? , E? >, its probability is
P (E?|F? ) =
{
P (?) ifE? = (?)
P (??)|F? |PT (E?|F? ) otherwise
where PT (E?|F? ) is the probability of the phrase
pair as registered in the translation table, and |F? |
is the length of the phrase F? . The estimation of
P (?) is done by MLE:
P (?) = number of unaligned source word tokensnumber of source word tokens .
3.2 Model 2: EMPTY as Normal Word
Model 1 assumes that every word is as likely to be
spurious as any other word. Definitely this is not
a reasonable assumption, since certain function
words and grammatical particles are more likely
to be spurious than other words. Therefore, in our
second SWD model the probability of translating
a source word f to ? is conditioned on f itself.
This probability, P (?|f), is in the same form as
the probability of a normal phrase pair, P (E?|F? ),
if we consider ? as some special phrase of the tar-
get language and f as a source language phrase
on its own. Thus P (?|f) can be estimated and
recorded in the same way as the probability of
normal phrase pairs. During the phase of phrase
enumeration, in addition to enumerating all nor-
mal phrase pairs, we also enumerate all unaligned
source words f and add phrase pairs of the form
< (f), (?) >. These special phrase pairs, TO-
EMPTY phrase pairs, are fed to the module of
phrase scoring along with the normal phrase pairs.
Both types of phrase pairs are then stored in the
translation table with corresponding phrase trans-
lation probabilities. It can be seen that, since the
probabilities of normal phrase pairs are estimated
in the same procedure as those of TO-EMPTY
phrase pairs, they do not need re-weighing as in
the case of SWD model 1.
3.3 Model 3: Context-sensitive Model
Although model 2 is much more informative than
model 1, it is still unsatisfactory if we consider
the problem of SWD as a problem of tagging.
The decoder can be conceived as if it carries out
a tagging task over the source language sentence:
each source word is tagged either as ?spurious? or
?non-spurious?. Under such a perspective, SWD
3
model 2 is merely a unigram tagging model, and
it uses only one feature template, viz. the lex-
ical form of the source word in hand. Such a
model can by no means encode any contextual
information, and therefore it cannot handle the
?ACCORDING-TO/?? NP EXPRESS/,+? ex-
ample in section 1.
An obvious solution to this limitation is a more
powerful tagging model augmented with context-
sensitive feature templates. Inspired by research
work like (Lafferty et al, 2001) and (Sha and
Pereira, 2003), our SWD model 3 uses first-order
Conditional Random Field (CRF) to tackle the
tagging task.1 The CRF model uses the follow-
ing feature templates:
1. the lexical form and the POS of the foreign
word f itself;
2. the lexical forms and the POSs of f?2, f?1,
f+1, and f+2, where f?2 and f?1 are the two
words to the left of f , and f+1 and f+2 are
the two words to the right of f ;
3. the lexical form and the POS of the head
word of f ;
4. the lexical forms and the POSs of the depen-
dent words of f .
The lexical forms are the major source of infor-
mation whereas the POSs are employed to allevi-
ate data sparseness. The neighboring words are
used to capture local context information. For ex-
ample, in Chinese there is often a comma after
verbs like ?said? or ?stated?, and such a comma
is not translated to any word or punctuation in
English. These spurious commas are therefore
identified by their immediate left neighbors. The
head and dependent words are employed to cap-
ture non-local context information found by some
dependency parser. For the ?ACCORDING-TO/?
? NP EXPRESS/,+? example in section 1,
the Chinese word ACCORDING-TO/?? is the
head word of EXPRESS/,+. The spurious to-
ken of EXPRESS/,+ in this pattern can be dis-
tinguished from the non-spurious tokens through
the feature template of head word.
1Maximum Entropy was also tried in our experiments but
its performance is not as good as CRF.
The training data for the CRF model comprises
the alignment matrices of the bilingual training
data for the MT system. A source word (token)
in the training data is tagged as ?non-spurious? if
it is aligned to some target word(s), otherwise it is
tagged as ?spurious?. The sentences in the train-
ing data are also POS-tagged and parsed by some
dependency parser, so that each word can be as-
signed values for the POS-based feature templates
as well as the feature templates of head word and
dependency words.
The trained CRF model can then be used to
augment the decoder to tackle the SWD problem.
An input source sentence should first be POS-
tagged and parsed for assigning feature values.
The probability for f being spurious, P (?|f), is
then calculated by the trained CRF model as
PCRF (spurious|~F (f)).
The probability for f being non-spurious is sim-
ply 1 ? P (?|f). For a normal phrase pair
< F? , E? > recorded in the translation table,
its phrase translation probability and the lexical
weight should be re-weighed by the probabilities
of non-spuriousness. The weighing factor is
?
fi?F?
(1? P (?|fi)),
since the translation of F? into E? means the de-
coder considers every word in F? as non-spurious.
4 Experiments
4.1 Experiment Settings
A series of experiments were run to compare the
performance of the three SWD models against the
baseline, which is the standard phrase-based ap-
proach to SMT as elaborated in (Koehn et al,
2003). The experiments are about Chinese-to-
English translation. The bilingual training data
is the one for NIST MT-2006. The GIGAWORD
corpus is used for training language model. The
development/test corpora are based on the test
sets for NIST MT-2005/6.
The alignment matrices of the training data are
produced by the GIZA++ (Och and Ney, 2000b)
word alignment package with its default settings.
The subsequent construction of translation table
was done in exactly the same way as explained
4
in (Koehn et al, 2003). For SWD model 2,
the phrase enumeration step is modified as de-
scribed in section 3.2. We used the Stanford
parser (Klein and Manning, 2003) with its default
Chinese grammar for its POS-tagging as well as
finding the head/dependent words of all source
words. The CRF toolkit used for model 3 is
CRF++2. The training data for the CRF model
should be the same as that for translation table
construction. However, since there are too many
instances (every single word in the training data
is an instance) with a huge feature space, no pub-
licly available CRF toolkit can handle the entire
training set of NIST MT-2006.3 Therefore, we
can use at most only about one-third of the NIST
training set (comprising the FBIS, B1, and T10
sections) for CRF training.
The decoder in the experiments is our re-
implementation of HIERO (Chiang, 2007), aug-
mented with a 5-gram language model and a re-
ordering model based on (Zhang et al, 2007).
Note that no hierarchical rule is used with the de-
coder; the phrase pairs used are still those used
in conventional phrase-based SMT. Note also that
the decoder does not translate OOV at all even
in the baseline case, and thus the SWD models
do not improve performance simply by removing
OOVs.
In order to test the effect of training data size on
the performance of the SWD models, three varia-
tions of training data were used:
FBIS Only the FBIS section of the NIST training
set is used as training data (for both transla-
tion table and the CRF model in model 3).
This section constitutes about 10% of the en-
tire NIST training set. The purpose of this
variation is to test the performance of each
model when very small amount of data are
available.
BFT Only the B1, FBIS, and T10 sections of the
NIST training set are used as training data.
These sections are about one-third of the en-
tire NIST training set. The purpose of this
2http://crfpp.sourceforge.net/
3Apart from CRF++, we also tried FLEX-
CRF (http://flexcrfs.sourceforge.net) and MALLET
(http://mallet.cs.umass.edu).
Data baseline model 1 model 2 model 3
FBIS 28.01 29.71 29.48 29.64
BFT 29.82 31.55 31.61 31.75
NIST 29.77 31.39 31.33 31.71
Table 2: BLEU scores in Experiment 1: NIST?05 as
dev and NIST?06 as test
variation is to test each model when medium
size of data are available.4
NIST All the sections of the NIST training set
are used. The purpose of this variation is to
test each model when a large amount of data
are available.
(Case-insensitive) BLEU-4 (Papineni et al,
2002) is used as the evaluation metric. In each
test in our experiments, maximum BLEU training
were run 10 times, and thus there are 10 BLEU
scores for the test set. In the following we will
report the mean scores only.
4.2 Experiment Results and Analysis
Table 2 shows the results of the first experiment,
which uses the NIST MT-2005 test set as develop-
ment data and the NIST MT-2006 test set as test
data. The most obvious observation is that any
SWD model achieves much higher BLEU score
than the baseline, as there is at least 1.6 BLEU
point improvement in each case, and in some case
the improvement of using SWD is nearly 2 BLEU
points. This clearly proves the importance of
SWD in phrase-based SMT.
The difference between the performance of the
various SWD models is much smaller. Yet there
are still some noticeable facts. The first one is
that model 1 gives the best result in the case of
using only FBIS as training data but it fails to
do so when more training data is available. This
phenomenon is not strange since model 2 and
model 3 are conditioned on more information and
therefore they need more training data.
The second observation is about the strength of
SWD model 3, which achieves the best BLEU
score in both the BFT and NIST cases. While
its improvement over models 1 and 2 is marginal
in the case of BFT, its performance in the NIST
4Note also that the BFT data set is the largest training
data that the CRF model in model 3 can handle.
5
case is remarkable. A suspicion to the strength of
model 3 is that in the NIST case both models 1
and 2 use the entire NIST training set for esti-
mating P (?), while model 3 uses only the BFT
sections to train its CRF model. It may be that
the BFT sections are more consistent with the test
data set than the other NIST sections, and there-
fore a SWD model trained on BFT sections only
is better than that trained on the entire NIST. This
conjecture is supported by the fact that in all four
settings the BLEU scores in the NIST case are
lower than those in the BFT case, which suggests
that other NIST sections are noisy. While it is im-
possible to test model 3 with the entire NIST, it is
possible to restrict the data for the estimation of
P (?|f) in model 1 to the BFT sections only and
check if such a restriction helps.5 We estimated
the uniform probability P (?) from only the BFT
sections and used it with the translation table con-
structed from the complete NIST training set. The
BLEU score thus obtained is 31.24, which is even
lower than the score (31.39) of the original case
of using the entire NIST for both translation table
and P (?|f) estimation. In sum, the strength of
model 3 is not simply due to the choice of train-
ing data.
The test set used in Experiment 1 distinguishes
itself from the development data and the training
data by its characteristics of combining text from
different genres. There are three sources of the
NIST MT-2006 test set, viz. ?newswire?, ?news-
group?, and ?broadcast news?, while our devel-
opment data and the NIST training set comprises
only newswire text and text of similar style. It is
an interesting question whether SWD only works
for some genres (say, newswire) but not for other
genres. In fact, it is dubious whether SWD fits the
test set to the same extent as it fits the develop-
ment set. That is, perhaps SWD contributes to the
improvement in Experiment 1 simply by improv-
ing the translation of the development set which is
composed of newswire text only, and SWD may
not benefit the translation of the test data at all.
In order to test this conjecture, we ran Experi-
ment 2, in which the SWD models were still ap-
plied to the development data during training, but
5Unfortunately this way does not work for model 2 as
the estimation of P (?|f) and the construction of translation
table are tied together.
Data model 1 model 2 model 3
FBIS 29.85 29.91 29.95
BFT 31.73 31.84 32.08
NIST 31.70 31.82 32.05
Table 3: BLEU scores in Experiment 2, which is the
same as Experiment 1 but no word is deleted for test
corpus. Note: the baseline scores are the same as the
baselines in Experiment 1 (Table 2).
all SWD models stopped working when translat-
ing the test data with the trained parameters. The
results are shown in Table 3. These results are
very discouraging if we compare each cell in Ta-
ble 3 against the corresponding cell in Table 2: in
all cases SWD seems harmful to the translation of
the test data. It is tempting to accept the conclu-
sion that SWD works for newswire text only.
To scrutinize the problem, we split up the test
data set into two parts, viz. the newswire sec-
tion and the non-newswire section, and ran ex-
periments separately. Table 4 shows the results
of Experiment 3, in which the development data
is still the NIST MT-2005 test set and the test
data is the newswire section of NIST MT-2006
test set. It is confirmed that if test data shares
the same genre as the training/development data,
then SWD does improve translation performance
a lot. It is also observed that more sophisticated
SWD models perform better when provided with
sufficient training data, and that model 3 exhibits
remarkable improvement when it comes to the
NIST case.
Of course, the figures in Table 5, which shows
the results of Experiment 4 where the non-
newswire section of NIST MT-2006 test set is
used as test data, still leave us the doubt that SWD
is useful for a particular genre only. After all, it
is reasonable to assume that a model trained from
data of a particular domain can give good perfor-
mance only to data of the same domain. On the
other hand, the language model is another cause
of the poor performance, as the GIGAWORD cor-
pus is also of the newswire style.
While we cannot prove the value of SWD with
respect to training data of other genres in the
mean time, we could test the effect of using de-
velopment data of other genres. In our last ex-
periment, the first halves of both the newswire
6
apply SWD for test set no SWD for test set
Data model 1 model 2 model 3 model 1 model 2 model 3
FBIS 30.81 30.81 30.68 29.23 29.61 29.46
BFT 33.57 33.74 33.71 31.88 31.87 32.25
NIST 33.65 34.01 34.42 32.14 32.59 32.87
Table 4: BLEU scores in Experiment 3, which is the same as Experiments 1 and 2 but only the newswire section
of NIST?06 test set is used. Note: the baseline scores are the same as the baselines in Experiment 1 (Table 2).
apply SWD for test set no SWD for test set
Data model 1 model 2 model 3 model 1 model 2 model 3
FBIS 29.19 28.86 29.16 30.07 29.67 30.08
BFT 30.62 30.64 30.86 31.66 31.83 32.00
NIST 30.34 30.10 30.46 31.50 31.45 31.66
Table 5: BLEU scores in Experiment 4, which is the same as Experiments 1 and 2 but only the non-newswire
section of NIST?06 test set is used. Note: the baseline scores are the same as the baselines in Experiment 1
(Table 2).
Data baseline model 1 model 2 model 3
FBIS 26.87 27.79 27.51 27.61
BFT 29.11 30.38 30.49 30.41
NIST 29.34 30.63 30.95 31.00
Table 6: BLEU scores in Experiment 5: which is the
same as Experiment 1 but uses half of NIST?06 as de-
velopment set and another half of NIST?06 as test set.
and non-newswire sections of NIST MT-2006 test
set are combined to form the new development
data, and the second halves of the two sections
are combined to form the new test data. The new
development data is therefore consistent with the
new test data. If SWD, or at least a SWD model
from newswire, is harmful to the non-newswire
section, which constitutes about 60% of the de-
velopment/test data, then it will be either that the
parameter training process minimizes the impact
of SWD, or that the SWD model will make the
parameter training process fail to search for good
parameter values. The consequence of either case
is that the baseline setting should produce similar
or even higher BLEU score than the settings that
employ some SWD model. Experiment results, as
shown in Table 6, illustrate that SWD is still very
useful even when both development and test sets
contain texts of different genres from the training
text. It is also observed, however, that the three
SWD models give rise to roughly the same BLEU
scores, indicating that the SWD training data do
not fit the test/development data very well as even
the more sophisticated models are not benefited
from more data.
5 Experiments using METEOR
The results in the last section are all evaluated us-
ing the BLEU metric only. It is dubious whether
SWD is useful regarding recall-oriented metrics
like METEOR (Banerjee and Lavie, 2005), since
SWD removes information in source sentences.
This suspicion is to certain extent confirmed by
our application of METEOR to the translation
outputs of Experiment 1 (c.f. Table 7), which
shows that all SWD models achieve lower ME-
TEOR scores than the baseline. However, SWD is
not entirely harmful to METEOR: if SWD is ap-
plied to parameter tuning only but not for the test
set, (i.e. Experiment 2), even higher METEOR
scores can be obtained. This puzzling observa-
tion may be because the parameters of the de-
coder are optimized with respect to BLEU score,
and SWD benefits parameter tuning by improv-
ing BLEU score. In future experiments, maxi-
mum METEOR training should be used instead
of maximum BLEU training so as to examine if
SWD is really useful for parameter tuning.
7
Experiment 1 Experiment 2
SWD for both dev/test SWD for dev only
Data baseline model 1 model 2 model 3 model 1 model 2 model 3
FBIS 50.07 47.90 49.83 49.34 51.58 51.08 51.17
BFT 52.47 50.55 51.89 52.10 54.72 54.43 54.30
NIST 52.12 49.86 50.97 51.59 54.14 53.82 54.01
Table 7: METEOR scores in Experiments 1 and 2
6 Conclusion and Future Work
In this paper, we have explained why the han-
dling of spurious source words is not a trivial
problem and how important it is. Three solu-
tions, with increasing sophistication, to the prob-
lem of SWD are presented. Experiment results
show that, in our setting of using NIST MT-2006
test set, any SWD model leads to an improvement
of at least 1.6 BLEU points, and SWD model 3,
which makes use of contextual information, can
improve up to nearly 2 BLEU points. If only
the newswire section of the test set is considered,
SWD model 3 is even more superior to the other
two SWD models.
The effect of training data size on SWD has
also been examined, and it is found that more
sophisticated SWD models do not outperform
unless they are provided with sufficient amount
of data. As to the effect of training data do-
main/genre on SWD, it is clear that SWD models
trained on text of certain genre perform the best
when applied to text of the same genre. While
it is infeasible for the time being to test if SWD
works well for non-newswire style of training
data, we managed to illustrate that SWD based on
newswire text still to certain extent benefits the
training and translation of non-newswire text.
In future, two extensions of our system are
needed for further examination of SWD. The first
one is already mentioned in the last section: max-
imum METEOR training should be implemented
in order to fully test the effect of SWD regard-
ing METEOR. The second extension is about the
weighing factor in models 1 and 3. The current
implementation assumes that all source words
in a normal phrase pair need to be weighed by
1? P (?). However, in fact some source words in
a source phrase are tacitly deleted (as explained
in the Introduction). Thus the word alignment in-
formation within phrase pairs need to be recorded
and the weighing of a normal phrase pair should
be done in accordance with such alignment infor-
mation.
References
Brown, P., J. Cocke, S. Della Pietra, V. Della Pietra,
F. Jelinek, J. Lafferty, R. Mercer, and P. Roossin.
1990. A Statistical Approach to Machine Transla-
tion Computational Linguistics, 16(2).
Satanjeev Banerjee and Alon Lavie. 2005. METEOR:
An Automatic Metric for MT Evaluation with Im-
proved Correlation with Human Judgments. Pro-
ceedings of Workshop on Evaluation Measures for
MT and/or Summarization at ACL 2005.
David Chiang. 2007. Hierarchical Phrase-based
Translation. Computational Linguistics, 33(2).
Dan Klein and Christopher D. Manning. 2003. Ac-
curate Unlexicalized Parsing. Proceedings for ACL
2003.
Philipp Koehn, Franz J. Och, and Daniel Marcu. 2003.
Statistical Phrase-based Translation. Proceedings
for HLT-NAACL 2003.
John Lafferty, Andrew McCallum, and Fernando
Pereira 2001. Conditional Random Fields: Prob-
abilistic Models for Segmenting and Labeling Se-
quence Data. Proceedings for 18th International
Conf. on Machine Learning.
Franz J. Och, and Hermann Ney. 2000. A comparison
of alignment models for statistical machine transla-
tion. Proceedings of COLING 2000.
Franz J. Och, and Hermann Ney. 2000. Improved
Statistical Alignment Models. Proceedings for ACL
2000.
Kishore Papineni, Salim Roukos, Todd Ward, and
Wei-Jing Zhu. 2002. BLEU: a Method for Auto-
matic Evaluation of Machine Translation. Proceed-
ings for ACL 2002.
Fei Sha, Fernando Pereira. 2003. Shallow parsing
with conditional random fields. Proceedings of
NAACL 2003.
Dongdong Zhang, Mu Li, Chi-Ho Li and Ming
Zhou. 2007. Phrase Reordering Model Integrat-
ing Syntactic Knowledge for SMT. Proceedings for
EMNLP 2007.
8
Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010), pages 313?321,
Beijing, August 2010
Mixture Model-based Minimum Bayes Risk Decoding using Multiple 
Machine Translation Systems 
Nan Duan1 
School of Computer Science and Technology 
Tianjin University 
v-naduan@microsoft.com 
Mu Li, Dongdong Zhang, Ming Zhou 
Microsoft Research Asia 
muli@microsoft.com  
dozhang@microsoft.com 
mingzhou@microsoft.com 
Abstract 
We present Mixture Model-based Mini-
mum Bayes Risk (MMMBR) decoding, 
an approach that makes use of multiple 
SMT systems to improve translation ac-
curacy. Unlike existing MBR decoding 
methods defined on the basis of single 
SMT systems, an MMMBR decoder re-
ranks translation outputs in the combined 
search space of multiple systems using 
the MBR decision rule and a mixture dis-
tribution of component SMT models for 
translation hypotheses. MMMBR decod-
ing is a general method that is indepen-
dent of specific SMT models and can be 
applied to various commonly used search 
spaces. Experimental results on the NIST 
Chinese-to-English MT evaluation tasks 
show that our approach brings significant 
improvements to single system-based 
MBR decoding and outperforms a state-
of-the-art system combination method. 1 
1 Introduction 
Minimum Bayes Risk (MBR) decoding is be-
coming more and more popular in recent Statis-
tical Machine Translation (SMT) research. This 
approach requires a second-pass decoding pro-
cedure to re-rank translation hypotheses by risk 
scores computed based on model?s distribution. 
Kumar and Byrne (2004) first introduced 
MBR decoding to SMT field and developed it on 
the N-best list translations. Their work has 
shown that MBR decoding performs better than 
Maximum a Posteriori (MAP) decoding for dif-
ferent evaluation criteria. After that, many dedi-
                                                 
1 This work has been done while the author was visiting 
Microsoft Research Asia. 
cated efforts have been made to improve the per-
formances of SMT systems by utilizing MBR-
inspired methods. Tromble et al (2008) pro-
posed a linear approximation to BLEU score 
(log-BLEU) as a new loss function in MBR de-
coding and extended it from N-best lists to lat-
tices, and Kumar et al (2009) presented more 
efficient algorithms for MBR decoding on both 
lattices and hypergraphs to alleviate the high 
computational cost problem in Tromble et al?s 
work. DeNero et al (2009) proposed a fast con-
sensus decoding algorithm for MBR for both 
linear and non-linear similarity measures. 
All work mentioned above share a common 
setting: an MBR decoder is built based on one 
and only one MAP decoder. On the other hand, 
recent research has shown that substantial im-
provements can be achieved by utilizing consen-
sus statistics over multiple SMT systems (Rosti 
et al, 2007; Li et al, 2009a; Li et al, 2009b; 
Liu et al, 2009). It could be desirable to adapt 
MBR decoding to multiple SMT systems as well. 
In this paper, we present Mixture Model-
based Minimum Bayes Risk (MMMBR) decoding, 
an approach that makes use of multiple SMT 
systems to improve translation performance. In 
this work, we can take advantage of a larger 
search space for hypothesis selection, and em-
ploy an improved probability distribution over 
translation hypotheses based on mixture model-
ing, which linearly combines distributions of 
multiple component systems for Bayes risk 
computation. The key contribution of this paper 
is the usage of mixture modeling in MBR, which 
allows multiple SMT models to be involved in 
and makes the computation of n-gram consensus 
statistics to be more accurate. Evaluation results 
have shown that our approach not only brings 
significant improvements to single system-based 
MBR decoding but also outperforms a state-of-
the-art word-level system combination method. 
313
The rest of the paper is organized as follows: 
In Section 2, we first review traditional MBR 
decoding method and summarize various search 
spaces that can be utilized by an MBR decoder. 
Then, we describe how a mixture model can be 
used to combine distributions of multiple SMT 
systems for Bayes risk computation. Lastly, we 
present detailed MMMBR decoding model on 
multiple systems and make comparison with 
single system-based MBR decoding methods. 
Section 3 describes how to optimize different 
types of parameters. Experimental results will be 
shown in Section 4. Section 5 discusses some 
related work and Section 6 concludes the paper. 
2 Mixture Model-based MBR Decoding 
2.1 Minimum Bayes Risk Decoding 
Given a source sentence  , MBR decoding aims 
to find the translation with the least expected 
loss under a probability distribution. The objec-
tive of an MBR decoder can be written as: 
         
     
                 
    
  (1) 
where   denotes a search space for hypothesis 
selection;    denotes an evidence space for 
Bayes risk computation;      denotes a function 
that measures the loss between    and  ;      is 
the underlying distribution based on  . 
Some of existing work on MBR decoding fo-
cused on exploring larger spaces for both    
and   , e.g. from N-best lists to lattices or 
hypergraphs (Tromble et al, 2008; Kumar et al, 
2009). Various loss functions have also been 
investigated by using different evaluation crite-
ria for similarity computation, e.g. Word Error 
Rate, Position-independent Word Error Rate, 
BLEU and log-BLEU (Kumar and Byrne, 2004; 
Tromble et al, 2008). But less attention has 
been paid to distribution     . Currently, many 
SMT systems based on different paradigms can 
yield similar performances but are good at mod-
eling different inputs in the translation task 
(Koehn et al, 2004a; Och et al, 2004; Chiang, 
2007; Mi et al, 2008; Huang, 2008). We expect 
to integrate the advantages of different SMT 
models into MBR decoding for further im-
provements. In particular, we make in-depth in-
vestigation into MBR decoding concentrating on 
the translation distribution      by leveraging a 
mixture model based on multiple SMT systems. 
2.2 Summary of Translation Search Spaces 
There are three major forms of search spaces 
that can be obtained from an MAP decoder as a 
byproduct, depending on the design of the de-
coder: N-best lists, lattices and hypergraphs. 
An N-best list contains the   most probable 
translation hypotheses produced by a decoder. It 
only presents a very small portion of the entire 
search space of an SMT model. 
A hypergraph is a weighted acyclic graph 
which compactly encodes an exponential num-
ber of translation hypotheses. It allows us to 
represent both phrase-based and syntax-based 
systems in a unified framework. Formally, a 
hypergraph  is a pair      , where   is a 
set of hypernodes and   is a set of hyperedges. 
Each hypernode     corresponds to transla-
tion hypotheses with identical decoding states, 
which usually include the span       of the 
words being translated, the grammar symbol   
for that span and the left and right boundary 
words of hypotheses for computing language 
model (LM) scores. Each hyperedge     cor-
responds to a translation rule and connects a 
head node      and a set of tail nodes     . The 
number of tail nodes        is called the arity of 
the hyperedge   and the arity of a hypergraph is 
the maximum arity of its hyperedges. If the arity 
of a hyperedge   is zero,      is then called a 
source node. Each hypergraph has a unique root 
node and each path in a hypergraph induces a 
translation hypothesis. A lattice (Ueffing et al, 
2002) can be viewed as a special hypergraph, in 
which the maximum arity is one. 
2.3 Mixture Model for SMT 
We first describe how to construct a general dis-
tribution for translation hypotheses over multiple 
SMT systems using mixture modeling for usage 
in MBR decoding. 
Mixture modeling is a technique that has been 
applied to many statistical tasks successfully. 
For the SMT task in particular, given   SMT 
systems with their corresponding model distribu-
tions, a mixture model is defined as a probability 
distribution over the combined search space of 
all component systems and computed as a 
weighted sum of component model distributions: 
314
                      
 
   
  (2) 
In Equation 2,            are system weights 
which hold following constraints:        
and    
 
     ,            is the  
th distri-
bution estimated on the search space   based 
on the log-linear formulation: 
           
              
                     
  
where         is the score function of the  
th 
system for translation  ,          is a scaling 
factor that determines the flatness of the distri-
bution    sharp (    ) or smooth (    ). 
Due to the inherent differences in SMT mod-
els, translation hypotheses have different distri-
butions in different systems. A mixture model 
can effectively combine multiple distributions 
with tunable system weights. The distribution of 
a single model used in traditional MBR can be 
seen as a special mixture model, where   is one. 
2.4 Mixture Model for SMT 
Let              denote   machine translation 
systems,   denotes the search space produced 
by system    in MAP decoding procedure. An 
MMMBR decoder aims to seek a translation 
from the combined search space       that 
maximizes the expected gain score based on a 
mixture model         . We write the objec-
tive function of MMMBR decoding as: 
         
    
                
   
  (3) 
For the gain function     , we follow Trom-
ble et al (2008) to use log-BLEU, which is 
scored by the hypothesis length and a linear 
function of n-gram matches as: 
            
            
    
 
     
In this definition,   is a reference translation, 
     is the length of hypothesis   ,   is an n-
gram presented in   ,     
   is the number of 
times that  occurs in   , and       is an indi-
cator function which equals to 1 when   occurs 
in   and 0 otherwise.            are model 
parameters, where   is the maximum order of 
the n-grams involved. 
For the mixture model     , we replace it by 
Equation 2 and rewrite the total gain score for 
hypothesis    in Equation 3: 
                
   
 
                      
    
 
    
 
                  
   
 
    
 
                   
    
 
   
  
 
 
 
 
 
 
(4) 
In Equation 4, the total gain score on the com-
bined search space   can be further decom-
posed into each local search space    with a 
specified distribution           . This is a nice 
property and it allows us to compute the total 
gain score as a weighted sum of local gain 
scores on different search spaces. We expand the 
local gain score for    computed on search space 
   with            using log-BLEU as: 
                   
    
 
       
            
       
 
 
    
           
     
            
          
 
                           
We make two approximations for the situations 
when    : the first is                   
and the second is                      
          In fact, due to the differences in ge-
nerative capabilities of SMT models, training 
data selection and various pruning techniques 
used, search spaces of different systems are al-
ways not identical in practice. For the conveni-
ence of formal analysis, we treat all            
as ideal distributions with assumptions that all 
systems work in similar settings, and translation 
candidates are shared by all systems. 
The method for computing n-gram posterior 
probability          in Equation 5 depends on 
different types of search space  : 
? When   is an N-best list, it can be com-
puted immediately by enumerating all trans-
lation candidates in the N-best list: 
                         
    
  
315
? When   is a hypergraph (or a lattice) that 
encodes exponential number of hypotheses, 
it is often impractical to compute this proba-
bility directly.  In this paper, we use the al-
gorithm presented in Kumar et al (2009) 
which is described in Algorithm 12: 
                   
         
   
          
    
 
             
                         
       
 
             
                 
   
  
           counts the edge   with n-gram 
  that has the highest edge posterior proba-
bility relative to predecessors in the entire 
graph  , and          is the edge posterior 
probability that can be efficiently computed 
with standard inside and outside probabili-
ties      and      as: 
         
 
    
                
      
  
where     is the weight of hyperedge   in 
  ,      is the normalization factor that 
equals to the inside probability of the root 
node in  .  
 
Algorithm 1: Compute n-gram posterior proba-
bilities on hypergraph   (Kumar et al, 2009) 
1: sort hypernodes topologically 
2: compute inside/outside probabilities      and      
for each hypernode      
3: compute edge posterior probability          for 
each hyperedge       
4: for each hyperedge      do  
5:       merge n-grams on      and keep the highest 
probability when n-grams are duplicated 
6:      apply the rule of edge   to n-grams on      and 
propagate     gram prefixes/suffixes to      
7:          for each n-gram   introduced by   do  
8:      if                      then 
9:                                           
                     
10:           else 
11:                                 
12:   end if 
13:  end for   
14: end for 
15: return n-gram posterior probability set             
                                                 
2 We omit the similar algorithm for lattices because of their 
homogenous structures comparing to hypergraphs as we 
discussed in Section 2.2. 
Thus, the total gain score for hypothesis    on 
       can be further expanded as: 
   
 
                   
    
 
   
 
    
 
      
            
          
 
 
 
   
 
    
 
      
            
          
 
  
     
 
    
            
     
 
        
 
  
       
            
      
 
                            
where                   is a mixture n-
gram posterior probability. The most important 
fact derived from Equation 6 is that, the mixture 
of different distributions can be simplified to the 
weighted sum of n-gram posterior probabilities 
on different search spaces.  
We now derive the decision rule of MMMBR 
decoding based on Equation 6 below: 
         
    
    
            
      
 
  (7) 
We also notice that MAP decoding and MBR 
decoding are two different ways of estimating 
the probability        and each of them has 
advantages and disadvantages. It is desirable to 
interpolate them together when choosing the fi-
nal translation outputs. So we include each sys-
tem?s MAP decoding cost as an additional fea-
ture further and modify Equation 7 to: 
         
 ?  
    
            
      
 
 
             
       
 
  
 
 
 
 
(8) 
where       
        is the model cost as-
signed by the MAP decoder    for hypothesis  
 . 
Because the costs of MAP decoding on different 
SMT models are not directly comparable, we 
utilize the MERT algorithm to assign an appro-
priate weight    for each component system.  
Compared to single system-based MBR de-
coding, which obeys the decision rule below:  
         
     
    
            
         
 
   
316
MMMBR decoding has a similar objective func-
tion (Equation 8). The key difference is that, in 
MMMBR decoding, n-gram posterior probabili-
ty      is computed as              based on 
an ensemble of search spaces; meanwhile, in 
single system-based MBR decoding, this quanti-
ty is computed locally on single search space  . 
The procedure of MMMBR decoding on mul-
tiple SMT systems is described in Algorithm 2. 
 
Algorithm 2: MMMBR decoding on multiple 
SMT systems 
1: for each component system    do 
2:     run MAP decoding and generate the correspond-
ing search space   
3:  compute the n-gram posterior probability set 
            for   based on Algorithm 1 
4: end for 
5 compute the mixture n-gram posterior  probability 
                 for each  : 
6: for each unique n-gram   appeared in     do 
7:      for each search space   do 
8                   
9:         end for 
10: end for  
11: for each hyperedge   in     do 
12:     assign      to the edge   for all   contained in   
13: end for 
14: return the best path according to Equation 8 
 
3 A Two-Pass Parameter Optimization 
In Equation 8, there are two types of parameters: 
parameters introduced by the gain function      
and the model cost        , and system weights 
introduced by the mixture model     . Because 
Equation 8 is not a linear function when all pa-
rameters are taken into account, MERT algo-
rithm (Och, 2003) cannot be directly applied to 
optimize them at the same time. Our solution is 
to employ a two-pass training strategy, in which 
we optimize parameters for MBR first and then 
system weights for the mixture model. 
3.1 Parameter Optimization for MBR 
The inputs of an MMMBR decoder can be a 
combination of translation search spaces with 
arbitrary structures. For the sake of a general and 
convenience solution for optimization, we utilize 
the simplest N-best lists with proper sizes as 
approximations to arbitrary search spaces to 
optimize MBR parameters using MERT in the 
first-pass training. System weights can be set 
empirically based on different performances, or 
equally without any bias. Note that although we 
tune MBR parameters on N-best lists, n-gram 
posterior probabilities used for Bayes risk 
computation could still be estimated on 
hypergraphs for non N-best-based search spaces. 
3.2 Parameter Optimization for Mixture 
Model 
After MBR parameters optimized, we begin to 
tune system weights for the mixture model in the 
second-pass training. We rewrite Equation 8 as: 
         
 ?  
   
 
     
    
                                          
          
 
 
                                              
        
 
          
For each   , the aggregated score surrounded 
with braces can be seen as its feature value. Eq-
uation 9 now turns to be a linear function for all 
weights and can be optimized by the MERT. 
4 Experiments 
4.1 Data and Metric 
We conduct experiments on the NIST Chinese-
to-English machine translation tasks. We use the 
newswire portion of the NIST 2006 test set 
(MT06-nw) as the development set for parameter 
optimization, and report results on the NIST 
2008 test set (MT08). Translation performances 
are measured in terms of case-insensitive BLEU 
scores. Statistical significance is computed using 
the bootstrap re-sampling method proposed by 
Koehn (2004b). Table 1 gives data statistics. 
 
Data Set #Sentence #Word 
MT06-nw (dev) 616 17,316 
MT08 (test) 1,357 31,600 
Table 1. Statistics on dev and test data sets 
All bilingual corpora available for the NIST 
2008 constrained track of Chinese-to-English 
machine translation task are used as training data, 
which contain 5.1M sentence pairs, 128M Chi-
nese words and 147M English words after pre-
processing. Word alignments are performed by 
GIZA++ with an intersect-diag-grow refinement.  
317
A 5-gram language model is trained on the 
English side of all bilingual data plus the Xinhua 
portion of LDC English Gigaword Version 3.0. 
4.2 System Description 
We use two baseline systems. The first one 
(SYS1) is a hierarchical phrase-based system 
(Chiang, 2007) based on Synchronous Context 
Free Grammar (SCFG), and the second one 
(SYS2) is a phrasal system (Xiong et al, 2006) 
based on Bracketing Transduction Grammar 
(Wu, 1997) with a lexicalized reordering com-
ponent based on maximum entropy model. 
Phrasal rules shared by both systems are ex-
tracted on all bilingual data, while hierarchical 
rules for SYS1 only are extracted on a selected 
data set, including LDC2003E07, LDC2003E14, 
LDC2005T06, LDC2005T10, LDC2005E83, 
LDC2006E26, LDC2006E34, LDC2006E85 and 
LDC2006E92, which contain about 498,000 sen-
tence pairs. Translation hypergraphs are generat-
ed by each baseline system during the MAP de-
coding phase, and 1000-best lists used for 
MERT algorithm are extracted from hyper-
graphs by the k-best parsing algorithm (Huang 
and Chiang, 2005). We tune scaling factor to 
optimize the performance of HyperGraph-based 
MBR decoding (HGMBR) on MT06-nw for 
each system (0.5 for SYS1 and 0.01 for SYS2). 
4.3 MMMBR Results on Multiple Systems 
We first present the overall results of MMMBR 
decoding on two baseline systems. 
To compare with single system-based MBR 
methods, we re-implement N-best MBR, which 
performs MBR decoding on 1000-best lists with 
the fast consensus decoding algorithm (DeNero 
et al, 2009), and HGMBR, which performs 
MBR decoding on a hypergraph (Kumar et al, 
2009). Both methods use log-BLEU as the loss 
function. We also compare our method with 
IHMM Word-Comb, a state-of-the-art word-level 
system combination approach based on incre-
mental HMM alignment proposed by Li et al 
(2009b). We report results of MMMBR decod-
ing on both N-best lists (N-best MMMBR) and 
hypergraphs (Hypergraph MMMBR) of two 
baseline systems. As MBR decoding can be used 
for any SMT system, we also evaluate MBR-
IHMM Word-Comb, which uses N-best lists 
generated by HGMBR on each baseline systems. 
The default beam size is set to 50 for MAP de-
coding and hypergraph generation. The setting 
of N-best candidates used for (MBR-) IHMM 
Word-Comb is the same as the one used in Li et 
al. (2009b). The maximum order of n-grams in-
volved in MBR model is set to 4. Table 2 shows 
the evaluation results. 
 
 MT06-nw MT08 
 SYS1 SYS2 SYS1 SYS2 
MAP 38.1 37.1 28.5 28.0 
N-best MBR 38.3 37.4 29.0 28.1 
HGMBR 38.3 37.5 29.1 28.3 
IHMM 
Word-Comb 
39.1 29.3 
MBR-IHMM 
Word-Comb 
39.3 29.7 
N-best 
MMMBR 
39.0* 29.4* 
Hypergraph 
MMMBR 
39.4*+ 29.9*+ 
Table 2. MMMBR decoding on multiple sys-
tems (*: significantly better than HGMBR with 
      ; +: significantly better than IHMM 
Word-Comb with       ) 
From Table 2 we can see that, compared to 
MAP decoding, N-best MBR and HGMBR only 
improve the performance in a relative small 
range (+0.1~+0.6 BLEU), while MMMBR de-
coding on multiple systems can yield significant 
improvements on both dev set (+0.9 BLEU on 
N-best MMMBR and +1.3 BLEU on Hyper-
graph MMMBR) and test set (+0.9 BLEU on N-
best MMMBR and +1.4 BLEU on Hypergraph 
MMMBR); compared to IHMM Word-Comb, 
N-best MMMBR can achieve comparable results 
on both dev and test sets, while Hypergraphs 
MMMBR can achieve even better results (+0.3 
BLEU on dev and +0.6 BLEU on test); com-
pared to MBR-IHMM Word-Comb, Hypergraph 
MMMBR can also obtain comparable results 
with tiny improvements (+0.1 BLEU on dev and 
+0.2 BLEU on test). However, MBR-IHMM 
Word-Comb has ability to generate new hypo-
theses, while Hypergraph MMMBR only choos-
es translations from original search spaces. 
We next evaluate performances of MMMBR 
decoding on hypergraphs generated by different 
beam size settings, and compare them to (MBR-) 
318
IHMM Word-Comb with the same candidate 
size and HGMBR with the same beam size. We 
list the results of MAP decoding for comparison. 
The comparative results on MT08 are shown in 
Figure 1, where X-axis is the size used for all 
methods each time, Y-axis is the BLEU score, 
MAP-  and HGMBR-  stand for MAP decoding 
and HGMBR decoding for the  th system. 
 
Figure 1. MMMBR vs. (MBR-) IHMM Word-
Comb and HGMBR with different sizes 
From Figure 1 we can see that, MMMBR de-
coding performs consistently better than both 
(MBR-) IHMM Word-Comb and HGMBR on 
all sizes. The gains achieved are around +0.5 
BLEU compared to IHMM Word-Comb, +0.2 
BLEU compared to MBR-IHMM Word-Comb, 
and +0.8 BLEU compared to HGMBR. Com-
pared to MAP decoding, the best result (30.1) is 
obtained when the size is 100, and the largest 
improvement (+1.4 BLEU) is obtained when the 
size is 50. However, we did not observe signifi-
cant improvement when the size is larger than 50.  
We then setup an experiment to verify that the 
mixture model based on multiple distributions is 
more effective than any individual distributions 
for Bayes risk computation in MBR decoding. 
We use Mix-HGMBR to denote MBR decoding 
performed on single hypergraph of each system 
in the meantime using a mixture model upon 
distributions of two systems for Bayes risk com-
putation. We compare it with HGMBR and 
Hypergraph MMMBR and list results in Table 3. 
 
 MT08 
 SYS1 SYS2 
HGMBR 29.1 28.3 
Mix-HGMBR 29.4 28.9 
Hypergraph MMMBR 29.9 
Table 3. Performance of MBR decoding on dif-
ferent settings of search spaces and distributions 
It can be seen that based on the same search 
space, the performance of Mix-HGMBR is sig-
nificantly better than that of HGMBR (+0.3/+0.6 
BLEU on dev/test). Yet the performance is still 
not as good as Hypergraph, which indicates the 
fact that the mixture model and the combination 
of search spaces are both helpful to MBR decod-
ing, and the best choice is to use them together. 
We also empirically investigate the impacts of 
different system weight settings upon the per-
formances of Hypergraph MMMBR on dev set 
in Figure 2, where X-axis is the weight    for 
SYS1, Y-axis is the BLEU score. The weight    
for SYS2 equals to      as only two systems 
involved. The best evaluation result on dev set is 
achieved when the weight pair is set to 0.7/0.3 
for SYS1/SYS2, which is also very close to the 
one trained automatically by the training strategy 
presented in Section 3.2. Although this training 
strategy can be processed repeatedly, the per-
formance is stable after the 1st round finished. 
 
Figure 2. Impacts of different system weights in 
the mixture model 
4.4 MMMBR Results on Identical Systems 
with Different Translation Models 
Inspired by Macherey and Och (2007), we ar-
range a similar experiment to test MMMBR de-
coding for each baseline system on an ensemble 
of sub-systems built by the following two steps. 
Firstly, we iteratively apply the following 
procedure 3 times: at the  th time, we randomly 
sample 80% sentence pairs from the total bilin-
gual data to train a translation model and use it 
to build a new system based on the same decod-
er, which is denoted as sub-system- . Table 4 
shows the evaluation results of all sub-systems 
on MT08, where MAP decoding (the former 
ones) and corresponding HGMBR (the latter 
ones) are grouped together by a slash. We set al 
beam sizes to 20 for a time-saving purpose. 
27.5
28.0
28.5
29.0
29.5
30.0
30.5
10 20 50 100 150
MAP-1
MAP-2
HGMBR-1
HGMBR-2
IHMM
MBR-IHMM
MMMBR
38.5
38.7
38.9
39.1
39.3
39.5
0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9
MMMBR
319
 MT08 
 SYS1 SYS2 
Baseline 28.4/29.0 27.6/27.8 
sub-system-1 28.1/28.5 26.8/27.3 
sub-system-2 28.3/28.4 27.0/27.1 
sub-system-3 27.7/28.0 27.3/27.6 
Table 4. Performance of sub-systems 
Secondly, starting from each baseline system, 
we gradually add one more sub-system each 
time and perform Hypergraph MMMBR on 
hypergraphs generated by current involved sys-
tems. Table 5 shows the evaluation results. 
 
 MT08 
 SYS1 SYS2 
MAP 28.4 27.6 
HGMBR 29.0 27.8 
Hypergraph MMMBR 
+ sub-system-1 29.1 27.9 
+ sub-system-2 29.1 28.1 
+ sub-system-3 29.3 28.3 
Table 5. Performance of Hypergraph MMMBR 
on multiple sub-systems 
We can see from Table 5 that, compared to 
the results of MAP decoding, MMMBR decod-
ing can achieve significant improvements when 
more than one sub-system are involved; however, 
compared to the results of HGMBR on baseline 
systems, there are few changes of performance 
when the number of sub-systems increases. One 
potential reason is that the translation hypotheses 
between multiple sub-systems under the same 
SMT model hold high degree of correlation, 
which is discussed in Macherey and Och (2007). 
We also evaluate MBR-IHMM Word-Comb 
on N-best lists generated by each baseline sys-
tem with its corresponding three sub-systems. 
Evaluation results are shown in Table 6, where 
Hypergraph MMMBR still outperforms MBR-
IHMM Word-Comb on both baseline systems. 
 
 MT08 
 SYS1 SYS2 
MBR-IHMM Word-Comb 29.1 28.0 
Hypergraph MMMBR 29.3 28.3 
Table 6. Hypergraph MMMBR vs. MBR-IHMM 
Word-Comb with multiple sub-systems 
5 Related Work 
Employing consensus between multiple systems 
to improve machine translation quality has made 
rapid progress in recent years. System combina-
tion methods based on confusion networks (Ros-
ti et al, 2007; Li et al, 2009b) have shown 
state-of-the-art performances in MT benchmarks. 
Different from them, MMMBR decoding me-
thod does not generate new translations. It main-
tains the essential of MBR methods to seek 
translations from existing search spaces. Hypo-
thesis selection method (Hildebrand and Vogel, 
2008) resembles more our method in making use 
of n-gram statistics. Yet their work does not be-
long to the MBR framework and treats all sys-
tems equally. Li et al (2009a) presents a co-
decoding method, in which n-gram agreement 
and disagreement statistics between translations 
of multiple decoders are employed to re-rank 
both full and partial hypotheses during decoding. 
Liu et al (2009) proposes a joint-decoding me-
thod to combine multiple SMT models into one 
decoder and integrate translation hypergraphs 
generated by different models. Both of the last 
two methods work in a white-box way and need 
to implement a more complicated decoder to 
integrate multiple SMT models to work together; 
meanwhile our method can be conveniently used 
as a second-pass decoding procedure, without 
considering any system implementation details. 
6 Conclusions and Future Work 
In this paper, we have presented a novel 
MMMBR decoding approach that makes use of 
a mixture distribution of multiple SMT systems 
to improve translation accuracy. Compared to 
single system-based MBR decoding methods, 
our method can achieve significant improve-
ments on both dev and test sets. What is more, 
MMMBR decoding approach also outperforms a 
state-of-the-art system combination method.  We 
have empirically verified that the success of our 
method comes from both the mixture modeling 
of translation hypotheses and the combined 
search space for translation selection. 
In the future, we will include more SMT sys-
tems with more complicated models into our 
MMMBR decoder and employ more general 
MERT algorithms on hypergraphs and lattices 
(Kumar et al, 2009) for parameter optimization. 
320
References 
Chiang David. 2007. Hierarchical Phrase Based 
Translation. Computational Linguistics, 33(2): 
201-228. 
DeNero John, David Chiang, and Kevin Knight. 2009. 
Fast Consensus Decoding over Translation 
Forests. In Proc. of 47th Meeting of the Associa-
tion for Computational Linguistics, pages 567-575. 
Hildebrand Almut Silja and Stephan Vogel. 2008. 
Combination of Machine Translation Systems 
via Hypothesis Selection from Combined N-
best lists. In Proc. of the Association for Machine 
Translation in the Americas, pages 254-261. 
Huang Liang and David Chiang. 2005. Better k-best 
Parsing. In Proc. of 7th International Conference 
on Parsing Technologies, pages 53-64. 
Huang Liang. 2008. Forest Reranking: Discrimin-
ative Parsing with Non-Local Features. In 
Proc. of 46th Meeting of the Association for Com-
putational Linguistics, pages 586-594. 
Koehn Philipp. 2004a. Phrase-based Model for 
SMT. Computational Linguistics, 28(1): 114-133. 
Koehn Philipp. 2004b. Statistical Significance 
Tests for Machine Translation Evaluation. In 
Proc. of Empirical Methods on Natural Language 
Processing, pages 388-395. 
Kumar Shankar and William Byrne. 2004. Minimum 
Bayes-Risk Decoding for Statistical Machine 
Translation. In Proc. of the North American 
Chapter of the Association for Computational Lin-
guistics, pages 169-176.  
Kumar Shankar, Wolfgang Macherey, Chris Dyer, 
and Franz Och. 2009. Efficient Minimum Error 
Rate Training and Minimum Bayes-Risk De-
coding for Translation Hypergraphs and Lat-
tices. In Proc. of 47th Meeting of the Association 
for Computational Linguistics, pages 163-171. 
Li Mu, Nan Duan, Dongdong Zhang, Chi-Ho Li, and 
Ming Zhou. 2009a. Collaborative Decoding: 
Partial Hypothesis Re-Ranking Using Trans-
lation Consensus between Decoders. In Proc. 
of 47th Meeting of the Association for Computa-
tional Linguistics, pages 585-592. 
Liu Yang, Haitao Mi, Yang Feng, and Qun Liu. 2009. 
Joint Decoding with Multiple Translation 
Models. In Proc. of 47th Meeting of the Associa-
tion for Computational Linguistics, pages 576-584. 
Li Chi-Ho, Xiaodong He, Yupeng Liu, and Ning Xi. 
2009b. Incremental HMM Alignment for MT 
system Combination. In Proc. of 47th Meeting of 
the Association for Computational Linguistics, 
pages 949-957. 
Mi Haitao, Liang Huang, and Qun Liu. 2008. Forest-
Based Translation. In Proc. of 46th Meeting of 
the Association for Computational Linguistics, 
pages 192-199. 
Macherey Wolfgang and Franz Och. 2007. An Em-
pirical Study on Computing Consensus Trans-
lations from multiple Machine Translation 
Systems. In Proc. of Empirical Methods on Natu-
ral Language Processing, pages 986-995. 
Och Franz. 2003. Minimum Error Rate Training 
in Statistical Machine Translation. In Proc. of 
41th Meeting of the Association for Computational 
Linguistics, pages 160-167. 
Och Franz and Hermann Ney. 2004. The Alignment 
template approach to Statistical Machine 
Translation. Computational Linguistics, 30(4): 
417-449. 
Rosti Antti-Veikko, Spyros Matsoukas, and Richard 
Schwartz. 2007. Improved Word-Level System 
Combination for Machine Translation. In Proc. 
of 45th Meeting of the Association for Computa-
tional Linguistics, pages 312-319. 
Roy Tromble, Shankar Kumar, Franz Och, and Wolf-
gang Macherey. 2008. Lattice Minimum Bayes-
Risk Decoding for Statistical Machine Trans-
lation. In Proc. of Empirical Methods on Natural 
Language Processing, pages 620-629. 
Ueffing Nicola, Franz Och, and Hermann Ney. 2002. 
Generation of Word Graphs in Statistical Ma-
chine Translation. In Proc. of Empirical Me-
thods on Natural Language Processing, pages 
156-163. 
Wu Dekai. 1997. Stochastic Inversion Transduc-
tion Grammars and Bilingual Parsing of Pa-
rallel Corpora. Computational Linguistics, 
23(3): 377-404. 
Xiong Deyi, Qun Liu, and Shouxun Lin. 2006. Max-
imum Entropy based Phrase Reordering 
Model for Statistical Machine Translation. In 
Proc. of 44th Meeting of the Association for Com-
putational Linguistics, pages 521-528. 
321
Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010), pages 662?670,
Beijing, August 2010
Adaptive Development Data Selection for Log-linear Model
in Statistical Machine Translation
Mu Li
Microsoft Research Asia
muli@microsoft.com
Yinggong Zhao?
Nanjing University
zhaoyg@nlp.nju.edu.cn
Dongdong Zhang
Microsoft Research Asia
dozhang@microsoft.com
Ming Zhou
Microsoft Research Asia
mingzhou@microsoft.com
Abstract
This paper addresses the problem of dy-
namic model parameter selection for log-
linear model based statistical machine
translation (SMT) systems. In this work,
we propose a principled method for this
task by transforming it to a test data de-
pendent development set selection prob-
lem. We present two algorithms for au-
tomatic development set construction, and
evaluated our method on several NIST
data sets for the Chinese-English trans-
lation task. Experimental results show
that our method can effectively adapt
log-linear model parameters to different
test data, and consistently achieves good
translation performance compared with
conventional methods that use a fixed
model parameter setting across different
data sets.
1 Introduction
In recent years, log-linear model (Och and Ney,
2002) has been a mainstream method to formu-
late statistical models for machine translation. Us-
ing this formulation, various kinds of relevant
properties and data statistics used in the transla-
tion process, either on the monolingual-side or on
the bilingual-side, are encoded and used as real-
valued feature functions, thus it provides an ef-
fective mathematical framework to accommodate
a large variety of SMT formalisms with different
computational linguistic motivations.
?This work was done while the author was visiting Mi-
crosoft Research Asia.
Formally, in a log-linear SMT model, given a
source sentence f , we are to find a translation e?
with largest posterior probability among all possi-
ble translations:
e? = argmax
e
Pr(e|f)
and the posterior probability distribution Pr(e|f)
is directly approximated by a log-linear formula-
tion:
Pr(e|f) = p?(e|f)
= exp(
?M
m=1 ?mhm(e, f))?
e? exp(
?M
m=1 ?mhm(e?, f))
(1)
in which hm?s are feature functions and ? =
(?1, . . . , ?M ) are model parameters (feature
weights).
For a successful practical log-linear SMT
model, it is usually a combined result of the sev-
eral efforts:
? Construction of well-motivated SMT models
? Accurate estimation of feature functions
? Appropriate scaling of log-linear model fea-
tures (feature weight tuning).
In this paper, we focus on the last mentioned
issue ? parameter tuning for log-linear model.
In general, log-linear model parameters are opti-
mized on a held-out development data set. Us-
ing this method, similarly to many machine learn-
ing tasks, the model parameters are solely tuned
based on the development data, and the optimal-
ity of obtained model on unseen test data relies
on the assumption that both development and test
data observe identical probabilistic distribution,
662
which often does not hold for real-world data. The
goal of this paper is to investigate novel meth-
ods for test data dependent model parameter se-
lection. We begin with discussing the principle
of parameter learning for log-linear SMT models,
and explain the rationale of task transformation
from parameter selection to development data se-
lection. We describe two algorithms for automatic
development set construction, and evaluated our
method on several NIST MT evaluation data sets.
Experimental results show that our method can ef-
fectively adapt log-linear model parameters to dif-
ferent test data and achieves consistent good trans-
lation performance compared with conventional
methods that use a group of fixed model param-
eters across different data sets.
2 Model Learning for SMT with
Log-linear Models
Model learning refers to the task to estimate a
group of suitable log-linear model parameters
? = (?1, . . . , ?M ) for use in Equation 1, which is
often formulated as an optimization problem that
finds the parameters maximizing certain goodness
of the translations generated by the learnt model
on a development corpus D. The goodness can be
measured with either the translations? likelihood
or specific machine translation evaluation metrics
such as TER or BLEU.
More specifically, let e? be the most probable
translation of D with respect to model parameters
?, and E(e?,?, D) be a score function indicating
the goodness of translation e?, then a parameter
estimation algorithm will try to find the ? which
satisfies:
?? = argmax
?
E(e?,?, D) (2)
Note when the goodness scoring function E(?)
is specified, the parameter learning criterion in
Equation 2 indicates that the derivation of model
parameters ?? only depends on development data
D, and does not require any knowledge of test
data T . The underlying rationale for this rule is
that if the test data T observes the same distribu-
tion as D, ?? will be optimal for both of them.
On the other side, however, when there are mis-
matches between development and test data, the
translation performance on test data will be sub-
optimal, which is very common for real-world
data. Due to the difference between data sets, gen-
erally there is no such ?? that is optimal for multi-
ple data sets at the same time. Table 1 shows some
empirical evidences when two data sets are mutu-
ally used as development and test data. In this set-
ting, we used a hierarchical phrase based decoder
and 2 years? evaluation data of NIST Chinese-
to-English machine translation task (for the year
2008 only the newswire subset was used because
we want to limit both data sets within the same do-
main to show that data mismatch also exists even
if there is no domain difference), and report re-
sults using BLEU scores. Model parameters were
tuned using the MERT algorithm (Och, 2003) op-
timized for BLEU metric.
Dev data MT05 MT08-nw
MT05 0.402 0.306
MT08-nw 0.372 0.343
Table 1: Translation performance of cross devel-
opment/test on two NIST evaluation data sets.
In our work, we present a solution to this prob-
lem by using test data dependent model parame-
ters for test data translation. As discussed above,
since model parameters are solely determined by
development dataD, selection of log-linear model
parameters is basically equivalent to selecting a
set of development data D.
However, automatic development data selection
in current SMT research remains a relatively open
issue. Manual selection based on human experi-
ence and observation is still a common practice.
3 Adaptive Model Parameter Selection
An important heuristic behind manual develop-
ment data selection is to use the dataset which is
as similar to test set as possible in order to work
around the data mismatch problem to maximal
extent. There are also empirical evidences sup-
porting this heuristics. For instance, it is gener-
ally perceived that data set MT03 is more similar
to MT05, while MT06-nw is closer to MT08-nw.
Table 2 shows experimental results using model
parameters induced from MT03 and MT06-nw as
663
development sets with the same settings as in Ta-
ble 1. As expected, MT06-nw is far more suitable
than MT03 as the development data for MT08-
nw; yet for test set MT05, the situation is just the
opposite.
Dev data MT05 MT08-nw
MT03 0.397 0.306
MT06-nw 0.381 0.337
Table 2: Translation performance on different test
sets of using different development sets.
In this work, this heuristic is further exploited
for automatic development data selection when
there is no prior knowledge of the test data avail-
able. In the following discussion, we assume the
availability of a set of candidate source sentences
together with translation references that are qual-
ified for the log-linear model parameter learning
task. Let DF be the full candidate set, given a test
set T , the task of selecting a set of development
data which can optimize the translation quality on
T can be transformed to searching for a suitable
subset of DF which is most similar to T :
D? = argmax
D?DF
Sim(D,T )
To achieve this goal, we need to address the fol-
lowing key issues:
? How to define and compute Sim(D,T ), the
similarity between different data sets;
? How to extract development data sets from a
full candidate set for unseen test data.
3.1 Dataset Similarity
Computing document similarity is a classical task
in many research areas such as information re-
trieval and document classification. However, typ-
ical methods for computing document similarity
may not be suitable for our purpose. The reasons
are two-fold:
1. The sizes of both development and test data
are small in usual circumstances, and using
similarity measures such as cosine or dice
coefficient based on term vectors will suffer
from severe data sparseness problems. As a
result, the obtained similarity measure will
not be statistically reliable.
2. More importantly, what we care about here
is not the surface string similarity. Instead,
we need a method to measure how similar
two data sets are from the view of a log-linear
SMT model.
Next we start with discussing the similarity
between sentences. Given a source sentence
f , we denote its possible translation space with
H(f). In a log-linear SMT model, every trans-
lation e ? H(f) is essentially a feature vector
h(e) = (h1, . . . , hM ). Accordingly, the similar-
ity between two sentences f1 and f2 should be de-
fined on the feature space of the model in use. Let
V (f) = {h(e) : e ? H(f)} be the set of feature
vectors for all translations inH(f), we have
Sim(f1, f2) = Sim
(
V (f1),V (f2)
)
(3)
Because it is not practical to compute Equation
3 directly by enumerating all translations inH(f1)
and H(f2) due to the huge search space in SMT
tasks, we need to resort to some approximations.
A viable solution to this is that if we can use a
single feature vector h?(f) to represent V (f), then
Equation 3 can be simply computed using existing
vector similarity measures.
One reasonable method to derive h?(f) is to use
a feature vector based on the average principle ?
each dimension of the vector is set to the expec-
tation of its corresponding feature value over all
translations:
h?(f) =
?
e?H(f)
P (e|f)h(e) (4)
An alternative and much simpler way to com-
pute h?(f) is to employ the max principle in which
we just use the feature vector of the best transla-
tion inH(f):
h?(f) = h(e?) (5)
where e? = argmaxe P (e|f).
Note that in both Equation 4 and Equation 5
we make use of e?s posterior probability P (e|f).
664
Since the true distribution is unknown, a pre-
learnt modelM has to be used to assign approxi-
mate probabilities to translations, which indicates
that the obtained similarity depends on a specific
model. As a convention, we use SimM(f1, f2) to
denote the similarity between f1 and f2 based on
M, and callM the reference model of the com-
puted similarity. To avoid unexpected bias caused
by a single reference model, multiple reference
models can be simultaneously used, and the simi-
larity is defined to be the maximum of all model-
dependent similarity values:
Sim(f1, f2) = maxM SimM(f1, f2) (6)
where M belongs to {M1, . . . ,Mn}, which is
the set of reference models under consideration.
To generalize this method to data set level, we
compute the vector h?(S) for a data set S =
(f1, . . . , f|S|) as follows:
h?(S) =
|S|?
i=1
h?(fi) (7)
3.2 Development Sets Pre-construction
In the following, we sketch a method for automat-
ically building a set of development data based on
the full candidate set DF before seeing any test
data.
Theoretically, a subset of DF containing ran-
domly sampled sentences from DF will not meet
our requirement well because it is very probable
that it will observe a distribution similar to DF .
What we expect is that the pre-built development
sets can approximate as many as possible typi-
cal data distributions that can be estimated from
subsets of DF . Our solution is based on the as-
sumption that DF can be depicted by some mix-
ture models, hence we can use classical cluster-
ing methods such as k-means to partition DF into
subsets with different distributions.
Let SF be the set of extracted development data
from DF . The construction of SDF proceeds as
following:
1. Train a log-linear model MF using DF as
development data;
2. Compute a feature vector h?(d)1 for each sen-
tence d ? DF usingMF as reference model;
3. Cluster sentences in DF using h?(d)/|d| as
feature vectors;
4. Add obtained sentence clusters to SDF as
candidate development sets.
In the third step, since the feature vector h?(d)
is defined at sentence level, it is averaged by the
number of words in d so that it is irrelevant to the
length of a sentence. Considering the outputs of
unsupervised data clustering methods are usually
sensitive to initial conditions, we include in SDF
sentence clusters based on different initialization
configurations to remove related random effects.
An initialization configuration for sentence clus-
tering in our work includes starting point for each
cluster and total number of clusters. In fact, the
inclusion of more sentence clusters increases the
diversity of the resulted SDF as well.
At decoding time, when a test set T is pre-
sented, we compute the similarity between T and
each development set D ? SDF , and choose the
one with largest similarity score as the develop-
ment set for T :
D? = argmax
D?SDF
Sim(T,D) (8)
When a single reference model is used to com-
pute Sim(T,D), MF is a natural choice. In the
multi-model setting as shown in Equation 6, mod-
els learnt from the development sets in SDF can
serve this purpose.
Note in this method model learning is not re-
quired for every new test set because the model
parameters for each development set in SDF can
also be pre-learnt and ready to be used for decod-
ing.
3.3 Dynamic Development Set Construction
In the previous method, test data T is only in-
volved in the process of choosing a development
set from a list of candidates but not in process of
development set construction. Next we present a
1Throughout this paper, a development sentence d gener-
ally refers to the source part of it if there is no extra explana-
tion.
665
method for building a development set on demand
based on test data T .
Let DF = (d1, . . . , dn) be the data set con-
taining all candidate sentences for development
data selection. The method is iterative process in
which development data and learnt model are al-
ternatively updated. Detailed steps are illustrated
as follows:
1. Let i = 0, D0 = DF ;
2. Train a modelMi based on Di;
3. For each dk ? DF , compute the similarity
score SimMi(T, dk) between T and dk based
on modelMi;
4. Select top n candidate sentences with highest
similarity scores from DF to form Di+1;
5. Repeat step 2 to step 4 until the similarity be-
tween T and latest selected development data
converges (the increase in similarity measure
is less than a specified threshold compared to
last round) or the specified iteration limit is
reached.
In step 4, Di+1 is greedily extracted from DF ,
and there is no guarantee that SimMi(T,Di+1)
will increase or decrease after a new sentence is
added to Di+1. Thereby the number of selected
sentences n needs to be empirically determined.
If n is too small, neither the selected data nor the
learnt model parameters will be statistically reli-
able; while if n is too large, we may have to in-
clude some sentences that are not suitable for test
data in the development data, and miss the oppor-
tunity to extract the most desirable development
set.
One drawback of this method is the relatively
high computational cost because it requires multi-
ple parameter training passes when any test set is
presented to the system for translation.
4 Experiments
4.1 Data
Experiments were conducted on the data sets
used for NIST Chinese-English machine transla-
tion evaluation tasks. MT03 and MT06 data sets,
which contain 919 and 1,664 sentences respec-
tively, were used for development data in vari-
ous settings. MT04, MT05 and MT08 data sets
were used for test purpose. In some settings, we
also used a test set MT0x, which containing 1,000
sentences randomly sampled from the above 3
data sets. All the translation performance results
were measured in terms of case-insensitive BLEU
scores.
For all experiments, all parallel corpora avail-
able to the constrained track of NIST 2008
Chinese-English MT evaluation task were used
for translation model training, which consist of
around 5.1M bilingual sentence pairs. GIZA++
was used for word alignment in both directions,
which was further refined with the intersec-diag-
grow heuristics.
We used a 5-gram language model which was
trained from the Xinhua portion of English Giga-
word corpus version 3.0 from LDC and the En-
glish part of parallel corpora.
4.2 Machine Translation System
We used an in-house implementation of the hierar-
chical phrase-based decoder as described in Chi-
ang (2005). In addtion to the standard features
used in Chiang (2005), we also used a lexicon fea-
ture indicating how many word paris in the trans-
lation found in a conventional Chinese-English
lexicon. Phrasal rules were extracted from all the
parallel data, but hierarchical rules were only ex-
tracted from the FBIS part of the parallel data
which contains around 128,000 sentence pairs.
For all the development data, feature weights of
the decoder were tuned using the MERT algorithm
(Och, 2003).
4.3 Results of Development Data
Pre-construction
In the following we first present some overall re-
sults using the method of development data pre-
construction, then dive into more detailed settings
of the experiments.
Table 3 shows the results using 3 different data
sets for log-linear model parameter tuning. El-
ements in the first column indicate the data sets
used for parameter tuning, and other columns con-
tain evaluation results on different test sets. In the
666
Tuning set MT04 MT05 MT08 MT0x
MT03 0.399 / 0.392 0.395 / 0.390 0.241 / 0.258 0.319 / 0.322
MT06 0.381 / 0.388 0.382 / 0.391 0.275 / 0.283 0.343 / 0.342
MT03+MT06 0.391 / 0.401 0.392 / 0.397 0.265 / 0.281 0.336 / 0.345
Oracle cluster 0.401 0.398 0.293 0.345
Self-training 0.406 0.402 0.298 0.351
Table 3: Translation performance using different methods and data sets for parameter tuning.
third row of the table, MT03+MT06 means com-
bining the data sets of MT03 and MT06 together
to form a larger tuning set. The first number in
each cell denotes the BLEU score using the tuning
set as standard development setD, and the second
for using the tuning set as a candidate set DF .
For all experiment settings in the table, we
used cosine value between feature vectors to mea-
sure similarity between data sets, and feature vec-
tors were computed according to Equation 5 and
Equation 7 using a reference model which is
trained on the corresponding candidate set DF as
development set.2 We adopted the k-means algo-
rithm for data clustering with the number of clus-
ters iterating from 2 to 5. In each iteration, we ran
4 passes of clustering using different initial values.
Therefore, in total there are 56 sentence clusters
generated in each SDF .3
From the table it can be seen that given
the same set of sentences (MT03, MT06 and
MT03+MT06), when they are used as the can-
didate set DF for the development set pre-
construction method, the translation performance
is generally better than when they are just used as
development sets as a whole. Using MT03 data
set as DF is an exception: there is slight perfor-
mance drop on test sets MT04 and MT05, but it
also helps reduce the performance see-saw prob-
lem on different test sets as shown in Table 1.
Meanwhile, in the other two settings of DF , we
observed significant BLEU score increase on all
test sets but MT0x (on which the performance al-
most kept unchanged). In addition, the fact that
using MT03+MT06 as DF achieves best (or al-
2For example, in all the experiments in the row of MT03
as DF , we use the same reference model trained with MT03
as development set.
3Sometimes some clusters are empty or contain too few
sentences, so the actual number may be smaller.
most best) performance on all test sets implies that
it should be a better choice to include as diverse
data as possible in DF .
We also appended two oracle BLEU numbers
for each test set in Table 3 for reference. One is
denoted with oracle cluster, which is the high-
est possible BLEU that can be achieved on the
test set when the development set must be cho-
sen from the sentence clusters in SMT03+MT06.
The other is labeled as self-training, which is the
BLEU score that can be obtained when the test
data itself is used as development data. This num-
ber can serve as actual performance upper bound
on the test set.
Next we investigated the impact of using dif-
ferent ways to compute feature vectors presented
in Section 3.1. We re-ran some previous exper-
iments on test sets MT04, MT05 and MT08 us-
ing MT03+MT06 as DF . Most settings were kept
unchanged except that the feature vector of each
sentence was computed according to Equation 4.
A 20-best translation list was used to approximate
H(f). The results are shown in Table 4.
Test set average max
MT04 0.397 0.401
MT05 0.393 0.397
MT08 0.286 0.281
Table 4: Translation performance when using av-
eraged feature values for similarity computation.
The numbers in the second column are based
on Equation 4. Numbers based on Equation 5 are
also listed in the third column for comparison. In
all the experiment settings we did not observe con-
sistent or significant advantage when using Equa-
tion 4 over using Equation 5. Since Equation 5
667
is much simpler, it is a good decision to use it in
practice. So did we conduct all following experi-
ments based on Equation 5.
We are also interested in the correlation be-
tween two measures: the similarity between de-
velopment and test data and the actual translation
performance on test data.
First we would like to echo the motivating ex-
periment presented in Section 3. Table 5 shows
the similarity between the data sets used in the ex-
periment withMMT03+MT06 as reference model.
Obviously the results in Table 2 and Table 5 fit
each other very well.
Dev data MT05 MT08-nw
MT03 0.99988 0.99012
MT06-nw 0.99004 0.99728
Table 5: Similarity between NIST data sets.
Figure 1 shows the results of a set of more com-
prehensive experiments on MT05 data set con-
cerning the similarity between development and
test sets.
 0.28
 0.3
 0.32
 0.34
 0.36
 0.38
 0.4
 10  20  30  40  50  60
BL
EU
Rank of development set
Multiple
MT03+MT06
MT06
Figure 1: Correlation between similarity and
BLEU on MT05 data set
In the figure, every data line shows how BLEU
score changes when different pre-built develop-
ment set in SMT03+MT06 is used for model learn-
ing. The data points in each line are sorted by the
rank of similarity between the development set in
use and the MT05 data set. We also compared re-
sults based on 3 reference model settings. In the
first one (multiple), the similarity was computed
using Equation 6, and the reference model set con-
tains all models learnt from the development sets
in SMT03+MT06. The other two settings use refer-
ence models learnt from MT06 and MT03+MT06
data sets respectively.
We can observe from the figure that the corre-
lation between BLEU scores and data set similar-
ity can only be identified on macro scales for all
the three similarity settings. Although using data
similarity may not be able to select the perfect de-
velopment data set from SDF , by picking a devel-
opment set with highest similarity score, we can
usually (almost always) get good enough BLEU
scores in our experiments.
4.4 Results of Development Data Dynamic
Generation
We ran two sets of experiments for the method of
development data dynamic construction.
The first one was designed to investigate how
the size of extracted development data affects the
translation performance. Using MT05 and MT08
as test sets and MT03+MT06 as DF , we ran ex-
periments for the algorithm presented in Section
3.3 with n = 200 to n = 1, 000. In this ex-
periment we did not observe significant enough
changes in BLEU scores ? the difference between
the highest and lowest numbers is generally less
than 0.005.
The second one aimed at examining how BLEU
numbers changes when the extracted development
data were iteratively updated. Figure 2 shows one
set of results on test sets MT05 and MT08 using
MT03+MT06 data set as DF and n set to 400.
 0.365
 0.37
 0.375
 0.38
 0.385
 0.39
 0.395
 0.4
 0.405
 1  2  3  4  5  6  7
 0.265
 0.27
 0.275
 0.28
 0.285
 0.29
 0.295
 0.3
 0.305
M
T0
5 
BL
EU
M
T0
8 
BL
EU
Iteration
MT05
MT08
Figure 2: BLEU score as function of iteration in
dynamic development data extraction.
The similarity usually converged after 2 to 3 it-
668
erations, which is consistent with trend of BLEU
scores on test sets. However, in all our experimen-
tal settings, we did not observe any results signif-
icantly better than using the development set pre-
construction method.
5 Discussions
Some of the previous work related to building
adaptive SMT systems were discussed in the do-
main adaptation context, in which one fundamen-
tal idea is to estimate a more suitable domain-
specific translation model or language model.
When the target domain is already known, adding
a small amount of domain data (both monolingual
and bilingual) to the existing training corpora has
been shown to be very effective in practice. But
model adaptation is required in more scenarios
other than explicitly defined domains. As shown
by the results in Table 2, even for the data from
the same domain, distribution mismatch can also
be a problem.
There are also considerable efforts made to deal
with the unknown distribution of text to be trans-
lated, and the research topics were still focused on
translation and language model adaptation. Typ-
ical methods used in this direction include dy-
namic data selection (Lu? et al, 2007; Zhao et al,
2004; Hildebrand et al, 1995) and data weighting
(Foster and Kuhn, 2007; Matsoukas et al, 2009).
All the mentioned methods use information re-
trieval techniques to identify relevant training data
from the entire training corpora.
Our work presented here also makes no as-
sumption about the distribution of test data, but
it differs from the previous methods significantly
from a log-linear model?s perspective. Adjust-
ing translation and language models based on test
data can be viewed as adaptation of feature val-
ues, while our method is essentially adaptation of
feature weights. This difference makes these two
kinds of methods complementary to each other ?
it is possible to make further improvement by us-
ing both of them in one task.
To our knowledge, there is no dedicated discus-
sion on principled methods to perform develop-
ment data selection in previous research. In Lu?
et al (2007), log-linear model parameters can
also be adjusted at decoding time. But in their
approach, the adjustment was based on heuristic
rules and re-weighted training data distribution.
In addition, compared with training data selection,
the computational cost of development data selec-
tion is much smaller.
From machine learning perspective, both pro-
posed methods can be viewed as certain form
of transductive learning applied to the SMT task
(Ueffing et al, 2007). But our methods do
not rely on surface similarities between training
and training/development sentences, and develop-
ment/test sentences are not used to re-train SMT
sub-models.
6 Conclusions and Future Work
In this paper, we addressed the data mismatch is-
sue between training and decoding time of log-
linear SMT models, and presented principled
methods for dynamically inferring test data de-
pendent model parameters with development set
selection. We describe two algorithms for this
task, development set pre-construction and dy-
namic construction, and evaluated our method
on the NIST data sets for the Chinese-English
translation task. Experimental results show that
our methods are capable of consistently achiev-
ing good translation performance on multiple
test sets with different data distributions without
manual tweaking of log-linear model parameters.
Though theoretically using the dynamic construc-
tion method could bring better results, the pre-
construction method performs comparably well in
our experimental settings. Considering the fact
that the pre-consruction method is computation-
ally cheaper, it should be a better choice in prac-
tice.
In the future, we are interested in two direc-
tions. One is to explore the possibility to perform
data clustering on test set as well and choosing
suitable model parameters for each cluster sepa-
rately. The other involves dynamic SMT model
selection ? for example, some parts of the test
data fit the phrase-based model better while other
parts can be better translated using a syntax-based
model.
669
References
David Chiang. 2005. A hierarchical phrase-based
model for statistical machine translation. In Proc.
of the 43th Annual Meeting of the Association
for Computational Linguistic (ACL). Ann Arbor,
Michigan.
George Foster and Roland Kuhn. 2007. Mixture-
Model Adaptation for SMT. In Proc. of the Second
ACL Workshop on Statistical Machine Translation..
Prague, Czech Republic.
Michel Galley, Mark Hopkins, Kevin Knight and
Daniel Marcu. 2004. What?s in a translation rule?
In Proc. of the Human Language Technology Conf.
(HLT-NAACL). Boston, Massachusetts.
Almut Hildebrand, Matthias Eck, Stephan Vogel, and
Alex Waibel. 1995. Adaptation of the Transla-
tion Model for Statistical Machine translation Based
on Information Retrieval. In Proc. of EAMT. Bu-
dapest, Hungary.
Philipp Koehn, Franz Och and Daniel Marcu. 2003.
Statistical Phrase-Based Translation. In Proc. of the
Human Language Technology Conf. (HLT-NAACL).
Edmonton, Canada.
Yang Liu, Qun Liu, and Shouxun Lin. 2007. Tree-
to-string alignment template for statistical machine
translation. In Proc. of the 45th Annual Meet-
ing of the Association for Computational Linguistic
(ACL). Prague, Czech Republic.
Yajuan Lu?, Jin Huang and Qun Liu. 2007. Improv-
ing Statistical Machine Translation Performance by
Training Data Selection and Optimization. In Proc.
of the Conference on Empirical Methods in Natural
Language Processing. Prague, Czech Republic.
Spyros Matsoukas, Antti-Veikko I. Rosti and Bing
Zhang. 2009. Discriminative Corpus Weight Es-
timation for Machine Translation. In Proc. of the
Conference on Empirical Methods in Natural Lan-
guage Processing. Singapore.
Franz Och and Hermann Ney. 2002. Discriminative
Training and Maximum Entropy Models for Statis-
tical Machine Translation. In Proc. of the 40th An-
nual Meeting of the Association for Computational
Linguistic (ACL). Philadelphia, PA.
Franz Och. 2003. Minimum Error Rate Training in
Statistical Machine Translation. In Proc. of the 41th
Annual Meeting of the Association for Computa-
tional Linguistic (ACL). Sapporo, Japan.
Nicola Ueffing, Gholamreza Haffari and Anoop
Sarkar. 2007. Transductive learning for statistical
machine translation. In Proc. of the Annual Meet-
ing of the Association for Computational Linguis-
tics. Prague, Czech Republic.
Bing Zhao, Matthias Eck, and Stephan Vogel. 2004.
Language Model Adaptation for Statistical Machine
Translation with Structured Query Models. In Proc.
of COLING. Geneva, Switzerland.
670
Coling 2010: Poster Volume, pages 214?222,
Beijing, August 2010
Hybrid Decoding: Decoding with Partial Hypotheses Combination over
Multiple SMT Systems?
Lei Cui?, Dongdong Zhang?, Mu Li?, Ming Zhou?, and Tiejun Zhao?
?School of Computer Science and Technology
Harbin Institute of Technology
{cuilei,tjzhao}@mtlab.hit.edu.cn
?Microsoft Research Asia
{dozhang,muli,mingzhou}@microsoft.com
Abstract
In this paper, we present hybrid decod-
ing ? a novel statistical machine transla-
tion (SMT) decoding paradigm using mul-
tiple SMT systems. In our work, in ad-
dition to component SMT systems, sys-
tem combination method is also employed
in generating partial translation hypothe-
ses throughout the decoding process, in
which smaller hypotheses generated by
each component decoder and hypotheses
combination are used in the following de-
coding steps to generate larger hypothe-
ses. Experimental results on NIST evalu-
ation data sets for Chinese-to-English ma-
chine translation (MT) task show that our
method can not only achieve significant
improvements over individual decoders,
but also bring substantial gains compared
with a state-of-the-art word-level system
combination method.
1 Introduction
In recent years, system combination for SMT has
been known to be quite effective with translation
consensus information built from multiple SMT
systems. The combination approaches can be
classified into two types. One is the combination
with each system?s outputs, which can be seen as
full hypotheses combination. The other is the par-
tial hypotheses (PHS) combination during the de-
coding phase.
A lot of impressive work has been done to im-
prove the performance of the SMT systems by uti-
?This work has been done while the first author was vis-
iting Microsoft Research Asia.
lizing consensus statistics which come from sin-
gle system or multiple systems. For example,
Minimum Bayes Risk (MBR) (Kumar and Byrne,
2004) decoding over n-best list finds a translation
that has lowest expected loss with all the other hy-
potheses, and it shows that improvement over the
Maximum a Posteriori (MAP) decoding. Several
word-based methods (Rosti et al, 2007a; Sim et
al., 2007) have also been proposed. Usually, these
methods take n-best list from different SMT sys-
tems as inputs, and construct a confusion network
for second-pass decoding. There are also a lot of
research work to advance the confusion network
construction by finding better alignment between
the skeleton and the other hypotheses (He et al,
2008; Ayan et al, 2008). Typically, all the ap-
proaches above only use full hypotheses but have
no access to the PHS information.
Moreover, some dedicated efforts have been
tried by manipulating PHS between multiple MT
systems. Collaborative decoding (co-decoding)
(Li et al, 2009) leverages translation consensus
by exchanging partial translation results and re-
ranking both full and partial hypotheses explored
in decoding. However, no new PHS are generated
compared to the individual decoding but only the
ranking is affected. Liu et al (2009) proposes
joint decoding, a method that integrates multiple
translation models in one decoder. Although joint
decoding is able to generate new translations com-
pared to single decoder, it has to use the PHS
existed in one of its component decoder at each
step. Different from their work, we propose a
new perspective which leverages outputs from lo-
cal word-level combination. This will potentially
bring much benefit of performance since word-
214
level combination can produce more promising
PHS.
The word-level system combination method is
employed to generate partial translation hypothe-
ses in our hybrid decoding framework. In this
sense, full hypotheses word-level combination
(FH-Comb) method(Rosti et al, 2007a; Sim et al,
2007; He et al, 2008; Ayan et al, 2008) can be
considered as a special case of hybrid decoding,
where their combinations are only performed on
the largest hypotheses. Similar with FH-Comb,
hybrid decoding also uses word alignment infor-
mation. However, challenge exists in hybrid de-
coding as word alignment needs to be carefully
conducted through the decoding process. Obvi-
ously, document-level word alignment methods
such as GIZA++(Och and Ney, 2000) are quite
time consuming and unpractical to be embedded
into hybrid decoding. We propose a heuristic
method that can conduct word alignment of par-
tial hypotheses based on word alignment informa-
tion of phrase pairs learnt automatically from the
model training process. In this way, more PHS are
generated and the search space is enlarged sub-
stantially, which brings better translation results.
The rest of the paper is organized as follows:
Section 2 gives a formal description of hybrid
decoding, including framework overview, word-
level PHS combination and parameter estimation.
We conduct experiments with different settings
and make comparison between our method and
baseline, as well as a state-of-the-art word-level
system combination method in Section 3. Exper-
imental results discussion is presented in Section
4. Section 5 concludes the paper.
2 Hybrid Decoding
2.1 Overview
Different system combination methods (Li et al,
2009; Liu et al, 2009) offer different frameworks
to coordinate multiple SMT decoders. Hybrid de-
coding provides a new scheme to organize mul-
tiple decoders to work synchronously. As the
decoding algorithms may differ in multiple de-
coders1, hybrid decoding has some difficulty in
1In the SMT area, some decoders use left-right decod-
ing to generate the hypothesis and?Pharaoh?(Koehn et al,
integrating different decoding algorithms. With-
out loss of generality, we assume that bottom-up
CKY-based decoding is adopted in each individ-
ual decoder, which is the same as co-decoding
(Li et al, 2009) and joint decoding (Liu et al,
2009). Hybrid decoding collects n-best PHS of a
source span2 from multiple decoders, then results
from word-level PHS combination of that span are
given back to each decoder, mixed with the origi-
nal PHS. After that, we re-rank the hybrid list and
continue the decoding. In an example with two
decoders, parts of the whole decoding process are
illustrated in Figure 1 and can be summarized as
follows:
3-5 6-6 3-4 5-6
3-6 3-6
1-2 3-6 1-2
1-6 1-6
1-6
Decoder1 Decoder2
Local decoding layer
Local decoding layer
Local decoding layer
Local combination layer
Local combination layer
3-6 3-6
1-6 1-6
Figure 1: Hybrid decoding with two decoders,
where the string ?s-e?means the source span
starts from position s and ends at position e. The
blank rectangles represent the n-best partial trans-
lations of each decoder, and the shaded rectan-
gles illustrate the n-best local combination out-
puts. The ovals denote bottom-up CKY-based de-
coding results.
2003) is one of them, while others adopt bottom-up decoding
which is represented by?Hiero?(Chiang, 2007).
2The word ?span?is used to represent translation unit
in CKY-based decoders, which denotes one or more consec-
utive words in the source sentence.
215
1. Individual decoding. Each individual de-
coder should maintain the n-best PHS of
each span from the bottom. After all the in-
dividual decoders finish translating the same
span, they feed their own partial translations
into a public container which can be used for
word-level PHS combination, then get back
the partial combination outputs for step 3.
2. Local word-level combination. After fed
with PHS from multiple decoders, a confu-
sion network is built and word-level combi-
nation for PHS is conducted. The obtained
new partial translations are given back to
each individual decoder to continue the de-
coding.
3. Mix new PHS with the original ones. The
span in each individual decoder will receive
the corresponding new PHS from the local
combination outputs. The feature space of
the new PHS is not exactly the same with that
of the original ones. It has to be mapped in
some way then the mixed hypotheses are re-
ranked.
In the following sub-sections, we first present
the background of word-level combination for
PHS, then introduce hybrid decoding algorithm in
detail, as well as the feature definition and param-
eter estimation.
2.2 Word-Level Combination for Partial
Hypotheses
Most word-level system combination methods are
based on confusion network decoding. In con-
fusion network construction, one hypothesis has
to be selected as the skeleton which determines
the word order of the combination results. Other
hypotheses are aligned against the skeleton. Ei-
ther votes or some word confidence scores are as-
signed to each word in the network.
Most of the research on confusion network con-
struction focuses on seeking better word align-
ment between the skeleton and the other hypothe-
ses. So far, several word alignment procedures are
used for SMT system combination, which mainly
are GIZA++ alignments (Matusov et al, 2006),
TER alignments (Sim et al, 2007) and IHMM
??||| political ||| 0-0
????||| political and economic ||| 0-0 1-2
??||| economic ||| 0-0
????||| economic interests ||| 0-0 1-1
?? [X1] ||| political and [X1] ||| 0-0 1-2
Figure 2: The example of translation alignment
from phrase-table and rule-table
alignments (He et al, 2008). Similar with general
word-level system combination method, word-
level PHS combination also uses word alignment
information. However, in hybrid decoding, it is
quite time-consuming and impractical to conduct
word alignment like GIZA++ for each span. For-
tunately, unit hypotheses word alignment can be
obtained from the model training process, which
is shown in Figure 2. We devise a heuristic
approach for PHS alignment that leverages the
translation derivations from the sub-phrases. The
derivation information ultimately comes from the
phrase table in phrase-based systems (Koehn et
al., 2003; Xiong et al, 2006) or the rule table in
syntactic-based systems (Chiang, 2007; Liu et al,
2007; Galley et al, 2006).
The derivation is built in a phrase-based sys-
tem as follows. For example, we have two phrase
translations ??? ? ||| our ||| 0-0 1-0?and
????? ||| economic interests ||| 0-0 1-1?,
where string ?m-n?means the mth word in the
source phrase is aligned to the nth word in the tar-
get phrase. When combining the two phrases for
generating ??? ? ?? ???, we obtain
the translation hypothesis as ?our economic in-
terests?and also integrate the alignment fragment
to get ?0-0 1-0 2-1 3-2?. The case is similar in
syntactic-based system for non-terminal substitu-
tion, which we will not discuss further here.
Next, we introduce the skeleton-to-hypothesis
word alignment algorithm in detail. With the
translation derivations, the skeleton-to-hypothesis
(sk2hy) word alignment can be performed based
on the source-to-skeleton (so2sk) and source-to-
hypothesis (so2hy) word alignment as they share
the same source sentence. The basic idea is to
construct the sk2hy word alignment with the min-
imum correspondence subsets (MCS). A MCS is
defined as a triple < SK,HY, SO > where the
216
SK is the subset of skeleton words, HY is the
subset of the hypothesis words, and SO is the
minimum source word set that all target words in
both SK and HY are aligned to. Figure 3 shows
the algorithm for skeleton-to-hypothesis align-
ment. Most of the pseudo-code is self-explained
except for some subroutines, which are listed in
Table 1.
1: procedure SKEHYPALIGN(so2sk, so2hy)
2: repeat
3: Fetch out a source word to SO
4: SO1 = SO2 = SO
5: repeat
6: SO=UNION(SO1, SO2)
7: SK=GETALIGN(SO, so2sk)
8: HY =GETALIGN(SO, so2hy)
9: SO1=GETALIGN(SK, so2sk)
10: SO2=GETALIGN(HY, so2hy)
11: until |SO1| == |SO2| == |SO|
12: simmax = ?infinity
13: for all sk ? SK do
14: for all hy ? HY do
15: sim =SIM(sk, hy)
16: if sim ? simmax then
17: simmax = sim
18: skmax = sk
19: hymax = hy
20: end if
21: end for
22: end for
23: ADDALIGN(skmax, hymax)
24: until all the source words are fetched out
25: end procedure
Figure 3: Algorithm for skeleton-to-hypothesis
alignment
Subroutines Description
UNION(A,B) the union of set A and set B
GETALIGN(S,align) get the words aligned to
S based on align
SIM(w1,w2) similarity between w1 and w2,
we use edit distance here
ADDALIGN(w1,w2) align w1 with w2
Table 1: Description for subroutines
Due to the variety of the word order in n-
best outputs, skeleton selection becomes essen-
tial in confusion network construction. The sim-
plest way is to use the top-1 PHS from any indi-
vidual decoder with the best performance under
some criteria. However, this cannot always lead
to better performance on some evaluation met-
rics (Rosti et al, 2007a). An alternative would
be MBR method with some loss function such as
TER (Snover et al, 2006) or BLEU (Papineni et
al., 2002). We show the experimental results of
two skeleton selection methods for PHS combina-
tion in Section 3.
2.3 Hybrid Decoding Model
For a given source sentence f , any individual de-
coder in hybrid decoding finds the best transla-
tion e? among the possible translation hypotheses
?(f) in terms of a ranking function F :
e? = argmaxe??(f)F(e) (1)
Suppose we have n individual decoders. The
ranking function Fn of the nth decoder can be
written as:
Fn(e) =
m?
i=1
?n,ihn,i(f, e) (2)
where each hn,i(f, e) is a feature function of the
nth decoder, and ?n,i is the corresponding feature
weight. m is the number of features in each de-
coder.
The final result of hybrid decoder is the top-
1 translation from the confusion network, which
is constructed on multiple decoders with the last
layer?s output of CKY-based decoding.
2.4 Hybrid Decoding Algorithm
The hybrid decoder acts as a control unit which
controls the synchronization of multiple individ-
ual decoders. The algorithm is fully demonstrated
in Figure 4. The hybrid decoder pushes the same
span f ji to different decoders and gets back the n-
best PHS (lines 2-6). When the span?s length is
too small, both word alignment and partial com-
bination results are not accurate. We predefine a
fixed threshold ? which is used for determining the
start-up of combination (line 7). When the length
condition holds, the n-best PHS of each individual
217
decoder are stored in container G (lines 8). Con-
fusion network is constructed and new PHS can be
extracted from it and are further mixed and sorted
with the original ones (lines 11-15).
1: procedure HYBRIDDECODING(fn1 , D)
2: for l? 1...n do
3: for all i, j s.t. j ? i = l do
4: G? ?
5: for all d ? D do
6: nbest =DECODING(d, i, j)
7: if j ? i ? ? then
8: ADD(G,nbest)
9: end if
10: end for
11: cn =CONNETBUILD(G)
12: nbest? =GETPARHYP(cn)
13: for all d ? D do
14: MIXSORT(nbestd, nbest?)
15: end for
16: end for
17: end for
18: end procedure
Figure 4: Hybrid decoding algorithm
2.5 Hybrid Decoding Features
Next we present the PHS word-level combination
feature functions for hybrid decoding. Following
(Rosti et al, 2007b), four features are utilized to
model the PHS as:
Word Confidence Feature hwc(e)
The word confidence feature is computed as
hwc(e) =
?n
i=1 ?iciw, where n is the num-
ber of the systems, ?i is the system confi-
dence of system i, and ciw is the word confi-
dence of word w in system i.
Word Penalty Feature hwp(e)
Word penalty feature is the number of words
in the partial hypothesis (PH).
Null Penalty Feature hnp(e)
For null penalty feature, we mean the number
of NULL links along the PH when extracted
from the confusion network.
Language Model Feature hlm(e)
Different from the above three combination
features, which can be obtained during the
confusion network construction or hypothe-
ses extraction, the language model feature
cannot be summed up on the fly. Instead,
it must be re-computed when building each
new PH.
2.6 Feature Space Mapping
The features used in hybrid decoding can be clas-
sified into two categories: features for individual
decoders (FID) and features for PHS word-level
combination (FComb), and they are independent.
When mixing the new PHS with the original ones
of individual decoders, FComb space has to be
mapped to a FID space. However, several features
in FID are not defined in FComb, such as source
to target (S2T) phrase probability, target to source
(T2S) phrase probability, S2T lexical probability,
T2S lexical probability and other model specific
features. A mapping function H needs to be de-
fined as follows:
Ffid = H(Ffcomb) (3)
where Ffcomb denotes the feature vector from
FComb space, while Ffid is the feature vector
from FID space.
An easy mapping function is implemented with
an intuitive motivation: PHS combination results
are better than the ones in individual decoder and
we prefer not to disorder the original search space.
Thus, the undefined feature values of PHS from
FComb space are assigned by corresponding fea-
ture values of the top-1 PH in original decoder.
Experiments show that our method is not only
practical but also quite effective.
2.7 Parameter Estimation
Minimum Error Rate Training (MERT) (Och,
2003) algorithm is adopted to estimate feature
weights for hybrid decoding. As hybrid decoder
makes use of PHS from both individual decoders
and combination results as a whole, we devise
a new feature vector representation. The feature
vectors from FID space and FComb space are sim-
ply concatenated to form a longer vector without
overlapping. The weights are tuned simultane-
ously in order to reach a relatively global optima.
218
3 Experiment
3.1 Data and Metric
We conducted our experiments on the test data
of NIST 2005 and NIST 2006 Chinese-to-English
machine translation tasks. The NIST 2003 test
data is used as the development data to tune the
parameters. Statistics of the data sets are shown in
Table 2. Translation performances are measured
with case-insensitive BLEU4 score (Papineni et
al., 2002). Statistical significance test is per-
formed using the bootstrap re-sampling method
proposed by Koehn (2004).
The bilingual training corpora we used are
listed in Table 3, which contains 498K sentence
pairs, 12.1M Chinese words and 13.8M English
words after pre-processing. Word alignment is
performed by GIZA++ (Och and Ney, 2000) in
both directions with the default setting, and the
intersect-diag-grow method is used to refine the
symmetric word alignment.
Data Set # Sentences
NIST 2003(dev) 919
NIST 2005(test) 1,082
NIST 2006(test) 1,664
Table 2: Statistics of test/dev data sets
LDC ID Description
LDC2003E07 Ch/En Treebank Par Corpus
LDC2003E14 FBIS Multilanguage Texts
LDC2005T06 Ch News Translation Text Part 1
LDC2005T10 Ch/En News Magazine Par Text
LDC2005E83 GALE Y1 Q1 Release - Translations
LDC2006E26 GALE Y1 - En/Ch Par Financial News
LDC2006E34 GALE Y1 Q2 Release - Translations
V2.0
LDC2006E85 GALE Y1 Q3 Release - Translations
LDC2006E92 GALE Y1 Q4 Release - Translations
Table 3: Training corpora for Chinese-English
translation
The language model used for hybrid decoding
and all the baseline systems is a 5-gram model
trained with the Xinhua portion of LDC English
Gigaword Version 3.0 plus the English part of
bilingual training data.
3.2 Implementation
We use two baseline systems. The first one
(SYS1) is re-implementation of Hiero, a hi-
erarchical phrase-based system (Chiang, 2007)
based on Synchronous Context Free Grammar
(SCFG). Phrasal translation rules and hierarchi-
cal translation rules with nonterminals are ex-
tracted from all the bilingual sentence pairs.
The second one (SYS2) is a phrase-based sys-
tem (Xiong et al, 2006) based on Bracketing
Transduction Grammar (Wu, 1997) with a lex-
icalized reordering model (Zhang et al, 2007)
under maximum entropy framework, where the
phrasal translation rules are exactly the same
with that of SYS1. The lexicalized reorder-
ing model is trained using the MaxEnt toolkit
(Zhang, 2006) where the training instances are
extracted from subset of the training corpora,
which contains LDC2003E07, LDC2003E14,
LDC2005T06, LDC2005T10. Both systems use
the bottom-up CKY-based decoding with cube-
pruning (Chiang, 2007) and the beam size is set
to 10 for decoding efficiency.
For hybrid decoder, we set ? to be
sentence.length ? 3, meaning that the PHS of
individual decoders only perform local combi-
nation in the last three layers. The reason why
we adopt this setting is because we find that
starting local combination on short spans hurts
the performance badly on test data. Experimental
results are shown in the next section.
3.3 Translation Results
We present the overall results of hybrid decod-
ing over two baseline systems on both test sets.
We also implement an IHMM-based word-level
system combination method (He et al, 2008) to
make comparison with hybrid decoding system,
and the n-best candidates used for IHMM-based
word-level system combination is set to 10. Pa-
rameters for all the systems are tuned on NIST
2003 test set. The results are shown in Table 4.
In Table 4, we find that the hybrid decoding per-
forms significantly better than SYS1 and SY2 on
both test sets. Besides, compared to IHMM word-
level system combination method, hybrid decod-
ing also brings substantial gains with 0.63% and
0.92% points respectively.
219
NIST 2005 NIST 2006
SYS1 0.3745 0.3346
SYS2 0.3699 0.3296
IHMM Word-Comb 0.3821? 0.3421?
Hybrid 0.3884?+ 0.3513?+
Table 4: Hybrid decoding results on test sets,
*:significantly better than SYS1 and SYS2 with
p<0.01, +:significantly better than IHMM Word-
Comb with p<0.01
We also try different layers for determining
the start-up of local word-level PHS combination.
Figure 5 gives the intuitive BLEU results.
 
0.32
0.33
0.34
0.35
0.36
0.37
0.38
0.39
0.4
Figure 5: Performance of hybrid decoding with
different start-up settings on NIST 2005 test set,
where the ?lastM? means to conduct local word-
level PHS combination in the last M layers from
the perspective of CKY-based decoding.
As shown in Figure 5, the performance drops
drastically if we start to conduct word-level PHS
combination too early. After considering about ef-
ficiency and performance, we determine to do that
in the last three layers.
We then investigate the effects on hybrid de-
coding with different beam sizes, and compare the
trend with two baseline systems and IHMM-based
word-level system combination method as well.
The results are illustrated in Figure 6.
From what we see in Figure 6, the performance
of each system is monotonically increasing as the
beam size becomes larger. Hybrid decoding per-
forms consistently better than IHMM Word-Comb
when the beam size is small, and the largest im-
provement (+0.63% points) is obtained when the
beam size is set to 10. However, as the beam size
 
0.355
0.36
0.365
0.37
0.375
0.38
0.385
0.39
0.395
0.4
10 20 50 100
SYS1
SY S2
IHMM
Hybrid
Figure 6: Performance of hybrid decoding with
different beam sizes on NIST 2005 test set
increases, the performance gap is getting narrow.
One intuitive observation is that hybrid decoding
performs slightly worse than IHMM Word-Comb
when the beam size is set to 100. One possible
reason for this phenomenon is that, the alignment
noise may be introduced to hybrid decoding since
we have to generate monolingual alignments with
many poor translation derivations.
The confusion network for PHS of each system
can be built independently. We would like to eval-
uate the performance of single system hybrid de-
coding. Table 5 gives the results on both Hiero
and BTG decoders.
NIST 2005 NIST 2006
SYS1 SYS2 SYS1 SYS2
baseline 0.3745 0.3699 0.3346 0.3296
self-comb 0.3770 0.3758? 0.3358 0.3355?
Table 5: Hybrid decoding for single system,
*:significantly better than baseline with p<0.05
Table 5 shows that BTG decoder (SYS2) has
more potential for so-called ?self-boosting?.
The self-combination of BTG decoder improves
the performance substantially over the baseline.
However, we did not observe any significant im-
provement for Hiero decoder (SYS1).
Finally, we examine the impacts of skeleton se-
lection for PHS in hybrid decoding. The results in
Table 6 demonstrate that, compared to the top-1
selection method, translation performance can be
improved significantly with MBR-based skeleton
selection method. It strongly suggests that choos-
ing the skeleton with more consistent word order
220
will lead to better translation results.
NIST 2005 NIST 2006
Top-1 0.3817 0.3415
MBR 0.3884? 0.3513?
Table 6: Skeleton selection in hybrid decoding,
*:significantly better than top-1 skeleton selection
method with p<0.01
4 Discussion
System combination methods have been widely
used in SMT to improve the performance. For
example, in (Rosti et al, 2007a), several combi-
nation methods have been proposed to make use
of different kinds of consensus information. In
(He et al, 2008), better word alignment method is
adopted to advance the word-level system combi-
nation. Our method is different from these meth-
ods in the sense that we do not exclusively rely
on the n-best full hypotheses from each individual
decoder, but emphasize the importance of word-
level combination for PHS. Thus, it enlarges the
search space and is more prone to find better trans-
lations. Experimental results have shown the ef-
fectiveness of our method.
The idea of multiple systems collaborative de-
coding (Li et al, 2009) works well on re-ranking
the outputs of each system using n-gram agree-
ment statistics. However, no new translation re-
sults are generated compared to individual decod-
ing. Our method takes advantage of confusion
network to give PHS which cannot be seen before.
Although (Liu et al, 2009) also work on PHS,
we explore the cooperation of multiple systems
from a new perspective. They use translation
derivations from different decoders jointly as a
bridge to connect different models. Different from
their work, we devise a heuristic method to ob-
tain word alignment information from the deriva-
tion of each decoder, which can be embedded
for word-level PHS combination easily and effi-
ciently.
5 Conclusion and Future Work
In this paper, we propose a new SMT decoding
framework named hybrid decoding, in which mul-
tiple decoders work synchronously to conduct lo-
cal decoding and local word-level PHS combina-
tion in turn. We also devise a heuristic method to
obtain word alignment information directly from
the translation derivations, which is both intuitive
and efficient. Experimental results show that with
hybrid decoding the overall performance can be
improved significantly over both the individual
baseline decoder and the state-of-the-art system
combination method.
In the future, we will involve more individual
SMT decoders into hybrid decoding. In addition,
we would like to keep on this work in two direc-
tions. On the one hand, start-up threshold of PHS
combination will be explored in detail to find its
underlying impact on hybrid decoding. On the
other hand, we will try to employ a more theoreti-
cally sound approach to conduct the feature space
mapping from the feature space of confusion net-
work to that of individual decoders.
References
Ayan, Necip Fazil, Jing Zheng, and Wen Wang. 2008.
Improving alignments for better confusion networks
for combining machine translation systems. In Pro-
ceedings of the 22nd International Conference on
Computational Linguistics, pages 33-40
Chiang, David. 2005. A hierarchical phrase-based
model for statistical machine translation. In Pro-
ceedings of the 43rd Annual Meeting of the Associ-
ation for Computational Linguistics, pages 263-270
Chiang, David. 2007. Hierarchical phrase-based
translation. Computational Linguistics, 33(2):
pages 201-228
Galley, Michel, Jonathan Graehl, Kevin Knight, Daniel
Marcu, Steve DeNeefe, Wei Wang, and Ignacio
Thayer. 2006. Scalable inference and training of
context-rich syntactic translation models. In Pro-
ceedings of the 21st International Conference on
Computational Linguistics and 44th Annual Meet-
ing of the Association for Computational Linguis-
tics, pages 961-968
He, Xiaodong, Mei Yang, Jianfeng Gao, Patrick
Nguyen, and Robert Moore. 2008. Indirect-hmm-
based hypothesis for combining outputs from ma-
chine translation systems. In Proceedings of the
2008 Conference on Empirical Methods in Natural
Language Processing, pages 98-107
Koehn, Phillip, Franz J. Och, and Daniel Marcu. 2003.
Statistical phrase-based translation. In Proceed-
221
ings of the 2003 Human Language Technology Con-
ference of the North American Chapter of the Asso-
ciation for Computational Linguistics, pages 48-54
Koehn, Phillip. 2004. Statistical significance tests for
machine translation evaluation. In Proceedings of
the 2004 Conference on Empirical Methods in Nat-
ural Language Processing, pages 388-395
Kumar, Shankar and William Byrne. 2004. Minimum
bayes-risk decoding for statistical machine trans-
lation. In Proceedings of the 2004 Human Lan-
guage Technology Conference of the North Ameri-
can Chapter of the Association for Computational
Linguistics, pages 169-176
Li, Mu, Nan Duan, Dongdong Zhang, Chi-Ho Li, and
Ming Zhou. 2009. Collaborative decoding: par-
tial hypothesis re-ranking using translation consen-
sus between decoders. In Proceedings of the Joint
Conference of the 47th Annual Meeting of the ACL
and the 4th International Joint Conference on Natu-
ral Language Processing of the AFNLP, pages 585-
592
Liu, Yang, Yun Huang, Qun Liu, and Shouxun Lin.
2007. Forest-to-string statistical translation rules.
In Proceedings of the 45th Annual Meeting of the
Association of Computational Linguistics, pages
704-711
Liu, Yang, Haitao Mi, Yang Feng, and Qun Liu. 2009.
Joint decoding with multiple translation models. In
Proceedings of the Joint Conference of the 47th An-
nual Meeting of the ACL and the 4th International
Joint Conference on Natural Language Processing
of the AFNLP, pages 576-584
Matusov, Evgeny, Nicola Ueffing, and Hermann Ney.
2006. Computing consensus translation from mul-
tiple machine translation systems using enhanced
hypotheses alignment. In 11th Conference of the
European Chapter of the Association for Computa-
tional Linguistics, pages 33-40
Och, Franz Josef. and Hermann Ney. 2000. Improved
statistical alignment models. In Proceedings of the
38th Annual Meeting of the Association for Compu-
tational Linguistics, pages 440-447
Och, Franz Josef. 2003. Minimum Error Rate Train-
ing in Statistical Machine Translation. In Proceed-
ings of the 41st Annual Meeting of the Association
for Computational Linguistics, pages 160-167
Papineni, Kishore, Salim Roukos, Todd Ward, and
Wei-Jing Zhu. 2002. BLEU: a method for auto-
matic evaluation of machine translation. In Pro-
ceedings of the 40th Annual Meeting of the Associ-
ation for Computational Linguistics, pages 311-318
Rosti, Antti-Veikko, Necip Fazil Ayan, Bing Xiang,
Spyros Matsoukas, Richard Schwartz, and Bonnie
Dorr. 2007. Combining outputs from multiple ma-
chine translation systems. In Proceedings of the
2007 Human Language Technology Conference of
the North American Chapter of the Association for
Computational Linguistics, pages 228-235
Rosti, Antti-Veikko, Spyros Matsoukas, and Richard
Schwartz. 2007. Improved word-level system com-
bination for machine translation. In Proceedings of
the 45th Annual Meeting of the Association of Com-
putational Linguistics, pages 312-319
Sim, K.C., W. Byrne, M. Gales, H. Sahbi, and P.
Woodland. 2007. Consensus network decoding
for statistical machine translation combinnation. In
32nd IEEE International Conference on Acoustics,
Speech, and Signal Processing
Snover, Matthew, Bonnie Dorr, Richard Schwartz, Lin-
nea Micciula, and John Makhoul. 2006. A study of
translation edit rate with targeted human annota-
tion. In the 7th conference of the Association for
Machine Translation in the Americas, pages 223-
231
Wu, Dekai. 1997. Stochastic inversion transduction
grammars and bilingual parsing of parallel cor-
pora. Computational Linguistics, 23(3): pages 377-
404
Xiong, Deyi, Qun Liu, and Shouxun Lin. 2006.
Maximum entropy based phrase reordering model
for statistical machine translation. In Proceedings
of the 21st International Conference on Computa-
tional Linguistics and 44th Annual Meeting of the
Association for Computational Linguistics, pages
521-528
Zhang, Dongdong, Mu Li, Chi-Ho Li, Ming Zhou.
2007. Phrase Reordering Model Integrating Syn-
tactic Knowledge for SMT. In Proceedings of the
2007 Joint Conference on Empirical Methods in
Natural Language Processing and Computational
Natural Language Learning, pages 533-540
Zhang, Le. 2006. Maximum entropy model-
ing toolkit for python and c++. available at
http://homepages.inf.ed.ac.uk/
lzhang10/maxent_toolkit.html.
222
Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers,
pages 1144?1153, Dublin, Ireland, August 23-29 2014.
A Lexicalized Reordering Model  
for Hierarchical Phrase-based Translation
1
 
 
Hailong Cao1, Dongdong Zhang2, Mu Li2, Ming Zhou2 and Tiejun Zhao1 
1Harbin Institute of Technology, Harbin, P.R. China 
2Microsoft Research Asia, Beijing, P.R. China 
{hailong, tjzhao}@mtlab.hit.edu.cn 
{Dongdong.Zhang, muli, mingzhou}@microsoft.com 
Abstract 
Lexicalized reordering model plays a central role in phrase-based statistical machine translation sys-
tems. The reordering model specifies the orientation for each phrase and calculates its probability con-
ditioned on the phrase. In this paper, we describe the necessity and the challenge of introducing such a 
reordering model for hierarchical phrase-based translation. To deal with the challenge, we propose a 
novel lexicalized reordering model which is built directly on synchronous rules. For each target phrase 
contained in a rule, we calculate its orientation probability conditioned on the rule. We test our model 
on both small and large scale data. On NIST machine translation test sets, our reordering model 
achieved a 0.6-1.2 BLEU point improvements for Chinese-English translation over a strong baseline 
hierarchical phrase-based system. 
1 Introduction 
In statistical machine translation, the problem of reordering source language into the word order of the 
target language remains a central research topic. Statistical phrase-based translation models (Och and 
Ney, 2004; Koehn et al., 2003) are good at local reordering, or the reordering of words within the 
phrase, since the order is specified by phrasal translations. However, phrase-based models remain 
weak at long-distance reordering, or the reordering of the phrases. To improve the reordering of the 
phrases, two types of models have been developed. 
The first one is lexicalized reordering models (Tillman, 2004; Huang et al., 2005; Al-Onaizan and 
Papineni, 2006; Nagata et al., 2006; Xiong et al., 2006; Zens and Ney, 2006; Koehn et al., 2007; Gal-
ley and Manning, 2008; Cherry et al., 2012) which predict reordering by taking advantage of lexical 
information. The model in (Koehn et al., 2007) distinguishes three orientations with respect to the pre-
vious and the next phrase?monotone (M), swap (S) and discontinuous (D). For example, we can ex-
tract a phrase pair ?xiayou ||| the lower reach of? whose orientations with respect to the previous and 
the next phrase are D and S respectively, as shown in Figure 1. Such a model is simple and effective, 
and has become a standard component of phrase-based systems such as MOSES.  
 
 
Figure 1. Phrase orientations for Chinese-English translation. 
 
The other is a hierarchical phrase-based (HPB) translation model (Chiang, 2007) based on synchro-
nous grammar. In the HPB model, a synchronous grammar rule may contain both terminals (words) 
and nonterminals (sub-phrases). The order of terminals and nonterminal are specified by the rule. For 
                                                 
This work is licenced under a Creative Commons Attribution 4.0 International License. Page numbers and proceedings footer 
are added by the organizers. License details: http://creativecommons.org/licenses/by/4.0/ 
1144
example, the translation rule <X xiayou, the lower reach of X > specifies that the translation of sub 
phrase X before ?xiayou? should be put after ?the lower reach of?. 
One problem with the HPB model is that the application of a rule is independent of the actual sub 
phrase. For example, the rule <X xiayou, the lower reach of X > will always swap the translation of X 
and ?xiayou?, no matter what is covered by X. This is an over-generalization problem. Much work has 
been done to solve this issue. For example, Zollmann and Venugopal (2006) annotate non-terminals 
by syntactic categories. He et al. (2008) proposes maximum entropy models which combine rich con-
text information for selecting translation rules during decoding. Huang et al. (2010) automatically in-
duce a set of latent syntactic categories to annotate nonterminals. These works alleviate the over-
generalization problem by considering the content of X. In this paper, we try to solve it from an alter-
native view by modeling whether the phrases covered by X prefer the order specified by the rule. This 
has led us to borrow the lexicalized reordering model from the phrase-based model for the HPB model. 
We propose a novel lexicalized reordering model for hierarchical phrase-based translation and 
achieved a 0.6-1.2 BLEU point improvements for Chinese-English translation over a strong HPB 
baseline system. 
2 Related work 
In this section, we briefly review two types of related work which are a nonterminal-based lexicalized 
reordering models and a path-based lexicalized reordering model. Both of them calculate the orienta-
tion for HPB translation. 
2.1 Nonterminal-based lexicalized reordering models 
Xiao et al. (2011) proposed an orientation model for HPB translation. The orientation probability of a 
derivation is calculated as the product of orientation probabilities of all nonterminals except the root.  
In order to define the relative orders of nonterminals and their adjacent phrase, they expand the align-
ment in a rule to include both terminals and nonterminals. There may be multiple ways to segment a 
rule into phrases; they use the maximum adjacent phrase similar to Galley and Manning (2008). They 
significantly outperformed the HPB system on both Chinese-English and German-English translation.  
Xiao et al. (2011) use the boundary word feature of nonterminals without considering their internal 
structure. For example, in Figure 1, suppose nonterminal X1 is not the root node and the orientation 
probability of X1 will condition on ?zhe, xiayou, this, river?.  
In this paper, we will consider how the words covered by the nonterminal X1 are reordered. Rather 
than using ?xiayou? as a feature to determine the orientation of X1 with respect to the next phrase, we 
think the immediately translated source word ?huanghe? could be more informative through it is not 
on the boundary of X1 , since ?huanghe? is the exact starting point from where we search for the next 
phrase to translate. 
Huck et al. (2013) proposed a very effective phrase orientation model for HPB translation. The 
model is also based on nonterminal. They extracted phrase orientation probabilities from word-aligned 
training data for use with hierarchical phrase inventories, and scored orientations in hierarchical de-
coding.  
2.2 Path-based lexicalized reordering model 
The most recent related work is Nguyen and Vogel (2013). They map a HPB derivation into a discon-
tinuous phrase-based translation path in the following two steps: 
1) Represent each rule as a sequence of phrase pairs and non-terminals.  
2) The rules? sequences are used to find the corresponding phrase-based path of a HPB derivation 
and calculate the phrase-based reordering features. 
 
 
Figure 2. The phrase-based path of the derivation in Figure 1. 
1145
A phrase-based path is the sequence of phrase pairs, whose source sides covers the source sentences 
and whose target sides generated the target sentences from left to right. For example, the phrase-based 
path of the derivation in Figure 1 is shown in Figure 2. 
The phrase-based reordering features for the above phrase-based path are: 
 
>)is  thisshi, zhe|<(log DPnext ,                      >)of reachlower   thexiayou,|<(log DPprevious , 
>)of reachlower   thexiayou,|<(log SPnext , >)river yellow  thehuanghe,|<(log SPprevious . 
 
Nguyen and Vogel (2013) achieved significant improvement over both phrase-based and HPB models 
on three language pairs respectively.  
One problem with the above work is that they did not use rules with unaligned source or target 
phrases. Though this can get faster and better Arabic-English translation, it leads to a 0.49 BLEU point 
loss for Chinese-English translation. 
Another problem with path-based model is: there are many forms of HPB rules which we cannot 
map into a reasonable sequence of phrase pairs and non-terminals. We will show this with an example 
derivation shown in Figure 3. The main difference between Figure 3 and Figure 1 is there is such a 
rule <fangzhi X, prevent X from> that a source phrase ?fangzhi? is aligned with a discontinuous target 
phrase ?prevent?from?. This makes it hard to find the corresponding phrase-based path because we 
do not know what is the right order of ?fangzhi ||| prevent?from? and ?daozei ||| the thieves? in the 
discontinuous phrase-based path. We face the following dilemmas: 
 
? If ?fangzhi ||| prevent?from? goes first, then the discontinuous phrase-based path is as shown in 
Figure 4(a). On such a path, we will consider the orientation of ?the thieves? with respect to 
?breaking in?. This is unreasonable because ?the thieves? and ?breaking in? are not adjacent in the 
target side. It does not satisfy the definition of the phrase-based reordering model which predicts 
the orientation with respect to previous or next adjacent target phrase.  
? If ?daozei ||| the thieves? goes first, then the discontinuous phrase-based path is as shown in Figure 
4(b). This is unreasonable because ?The policeman? and ?the thieves? are not adjacent on the tar-
get side. 
 
 
Figure 3. Example of Chinese-English translation and its derivation. 
 
 
          
(a)                                                                                         (b) 
                  
 Figure 4. Two discontinuous phrase-based path candidates of the HPB derivation. 
 
From the above example, we can see that if a target phrase is aligned to a discontinuous target 
phrase in a HPB rule, then it is hard to find a reasonable path whose target sides can generate the tar-
get sentence from left to right.  
1146
3 Our lexicalized reordering model 
Rather than mapping a HPB derivation into a discontinuous phrase-based path and applying reordering 
model built on phrases, we propose a lexicalized reordering model which is built directly on HPB 
rules. For each target phrase contained in a HPB rule, we calculate its orientation probability condi-
tioned on the rule. For the example derivation in Figure 3, we represent it by the structure shown in the 
following figure: 
 
 
Figure 5.  Our representation of the HPB derivation in Figure 3.  
 
Different from Figure 4(a) and Figure 4(b) which contain a discontinuous phrase ?prevent?from?, we 
represent ?prevent?from? as two individual target phrases: ?prevent? and ?from?. Instead of consid-
ering the orientation of ?prevent?from?, we consider the orientation of ?prevent? and ?from? respec-
tively. For example, we will consider the orientation of ?prevent? with respect the previous phrase 
?the policeman? 
prevent)(previousO
, and the orientation of ?prevent? with respect the next phrase ?the 
thieves? prevent)(nextO . The probabilities of both prevent)(previousO
and prevent)(nextO are conditioned 
on the rule <fangzhi X, prevent X from>. 
In Figure 5, every two neighboring target phrases are adjacent in the original target side. In this way, 
we can borrow the phrase-based reordering model which calculates the orientation with respect to pre-
vious and next adjacent phrase.  
More formally, we represent a HPB rule in the general form of: 
 
??? ?,X...XX,X...XX 2211022110 nnnn ttttssssr  
 
where n is the number of nonterminals. ...n,isi 1? , is the source phrase which is a continuous source 
word sequences. ...n,iti 1? , is the target phrase which is a continuous target word sequences. We use 
?  to represent the alignment of words and nonterminals in the rule. Note that is or it can be empty if 
there are adjacent nonterminals or there is nonterminal on the boundary. The lexicalized reordering 
probability of rule r is defined as the product of each target phrase?s orientation probabilities condi-
tioned on the rule r: 
 
)|)(()|)((
0
r,itOPr,itOP inextnext
n
i
ipreviousprevious?
?
 
 
In the above equation, each probability is conditioned on the whole rule. In this way, we avoid the 
problem of mapping a HPB derivation into a discontinuous phrase-based path. There are two ad-
vantages for our reordering model: 
? It is compatible with HPB rules which contain unaligned phrases. 
? It is compatible with HPB rules in which a source phrase is aligned to a discontinuous target 
phrase. 
Actually, our model is compatible with any kind of HPB rules since it is defined on the general 
form of rule. 
Now we describe how to define )( iprevious tO and )( inext tO in the model. Suppose it  contains ik target 
words and we write 
it as )()1-()2()1( ... ii kikiiii wwwwt ?
. Then we define: 
 
),()()( )1(1-)1()1( iiipreviousiprevious wwOwOtO ?? ,          ),()(( 1)()()( ??? iii kikikinextinext wwOwOtO
 
 
1147
where ),( 1?jj wwO is the orientation of two adjacent target words and is determined as follows: 
If ( )(1)( 1??? jj wlmwrm
 )           MwwO jj ?? ),( 1
; 
Else if (  )(1)( 1 jj wlmwrm ???
 )  SwO jj ?? ),( 1
; 
Else                                                         DwwO jj ?? ),( 1
; 
)(wrm is the position of the right most source word aligned to target word w; )(wlm is the position of 
the left most source word aligned to target word w. 
Above is our lexicalized reordering model which is built upon HPB rules. We complete its descrip-
tion using an example. For the rule <fangzhi X, prevent X from>, n=1, 
0 prevent? ??t  and 
1 from? ??t , the lexicalized reordering probability is: 
 
( (prevent)|<fangzhi X, prevent X from>,0 ) ( (prevent)|<fangzhi X, prevent X from> ,0)
( (from)| f i , r t  fr ,1) ( (from)|<fangzhi X,
?
? ?
previous previous next next
previous previous next next
P O P O
O P O  prevent X from> ,1)
 
 
Note that we calculate the orientation of plain phrase pairs in the same way as for HPB rules. We 
can represent a phrase pair in the form of ??? ?,, 00 tsr , which is a rule that does not contain any 
nonterminal. Then we can apply our above model which is general enough to cover both HPB rules 
and plain phrase pairs. 
4 Training and decoding 
The training of our model is similar to the reordering model of Moses. During the standard phrase pair 
extraction and rule extraction, besides the nonterminal alignment in rules, we also keep the lexical 
alignments and orientations. If a phrase pair or a rule is observed with more than one set of alignment, 
we only keep the most frequent one and only count the orientations corresponding to the most frequent 
alignment.  
Following Moses, we use relative frequency and add 0.5 smoothing technique to estimate the orien-
tation probability based on all samples collected from the training corpus. Generally, given a rule r 
with n target phrases, we estimated the reordering probability for each 
it as follows: 
 
0.5 # ( )( ( )| ) 1.5 #( )
? ???? ?
previous i
previous previous i
O t rP O t r, i r
,         0 5 ( ( ), )( ( ) | ) 1 5 ( )
?? ?
next inext next i
. # O t rP O t r, i . # r
 
 
For each parallel sentences pair, we add a start and an end mark on both sides. They are aligned re-
spectively. 
Our phrase pairs and rules are extracted from word aligned parallel sentences. There are many 
phrase pairs and rules which contain unaligned target or source words. How to deal with them is quite 
important for our reordering model. We will describe how to process them in the following two sub-
sections. 
4.1 The processing of unaligned target words 
Our main principle for processing an unaligned word is to: skip it and use the nearest aligned word. 
For example in Figure 3, the orientation of ?prevent? with respect to the next phrase is determined by: 
 
)  the(prevent,prevent)( OOnext ? 
 
If the target word ?the? is unaligned and ?thieves? is aligned with ?daozei?, we will define: 
 
(prevent) (prevent, the) (prevent, thieves)? ? ?nextO O O M 
 
Similarly, in Figure 1, the orientation of ?the lower reach of? with respect with ?the yellow river? is 
determined by O(of, the). Suppose both ?of? and ?the? are unaligned and there are alignments for 
?reach-xiayou? and ?yellow-huanghe?, we will have: 
1148
 SOO = yellow)(reach, = the)(of,  
 
We believe this orientation is consistent with our intuitions.  
More formally, before we determine the orientation of two adjacent target words ),( qp wwO ?we 
apply the following processing procedure: 
 
While (target word
pw is unaligned) p--; 
While (target word 
qw is unaligned) q++; 
 
If all words in a target phrase 
it  are unaligned, we do not need to consider its orientation since it  
does not trigger any movement along the source words at all. Actually, it will be skipped when we de-
termine the orientation of the previous and next aligned target phrases. (See also the decoding algo-
rithm in Section 4.3) 
4.2 The processing of unaligned source words 
The processing of Section 4.1 can guarantee that the orientation is determined based on two aligned 
target words, namely
pw and qw ,which must be continuous or separated by unaligned target words.  
Now we introduce the processing of unaligned source words. Before we determine the orientation 
of two target words ),( qp wwO ?we apply the following procedure to modify the position index of 
the left most source word aligned to 
pw and qw respectively: 
 
While (the 
th1)-)(( pwlm  source word is unaligned) --)( pwlm ; 
While (the thqwlm )1-)((  source word is unaligned) --)( qwlm ; 
 
For the example shown in the Figure 6, initially we have 1)( 1 ?wrm and 4)( 4 ?wlm . Since the 
source words 
3w  and 2w  are unaligned, our procedure will modify the value of )( 4wlm from 4 to 2. 
Finally, since )(1)( 41 wlmwrm ?? , the orientation of the two phrases marked by rectangular boxes in 
Figure 6 is: 
 
MwwOwwO ?? ),(),( 4132  
 
Again, we believe this result is consistent with our intuition. 
 
 
Figure 6. An example of phrases contain unaligned words 
 
Note that during decoding, both the unaligned source and target words are also processed in the 
same way as in the training step. This makes our lexicalized reordering model consistent. 
4.3 Decoding  
Now we introduce how to integrate our reordering model into the HPB system during the standard 
CYK bottom-up decoding.   
During decoding, if we just apply a plain phrase, we do not need to consider the orientation at once. 
It will be triggered when the phrase is used to compose a larger translation hypothesis together with 
other phrases or rules. 
We need to calculate the reordering features whenever we apply a HPB rule or a glue rule during 
the CYK decoding. Generally, given a rule ??? ?,X...XX,X...XX 2211022110 nnnn ttttssssr  defined in 
section 3, we calculate the reordering probability for the span covered by r with algorithm 1. In the 
algorithm, LL(X) represents the lowest rule which covers the left most word of X; LR(X) is the lowest 
1149
rule which covers the right most word of X; Both LL(X) and LR(X) can be found by traversing the 
derivation tree top to down recursively. LI(r) is the index of the last target phrase of rule r.  
As in the example shown in Figure 3, for the rule r2=<X2 jinru, X2 breaking in>, the orientation of 
X2 and ?breaking in? is: 
 
(breaking in) (from, breaking)? ? ?previousO O O D 
 
The right most target word of X2 is ?from?, the lowest rule covering ?from? is r3=<fangzhi X4, prevent 
X4 from> and the index of the last target phrase of r3 is 1. So the reordering probability is: 
 
)1,0, 31 (D|rP)(D|rPprob nextprevious ??  
 
Note that, for readability, we use the product of probabilities to demonstrate the decoding process. 
Actually in practice, we use a linear model which sums the weighted log probabilities. 
 
prob=1; 
for (int i=1; i<=n; i++) 
{ 
   if (
1?it  is not empty and contains aligned words) 
   { 
            ;0,*
;1,*
;1
))(O|LL(XPprob
)i(O|rPprob
)(tOO
iprevious
next
inext
?
??
? ?
 
    } 
   if (
it  is not empty and contains aligned words) 
   { 
            
);|(*
;)(,*
);(
r,iOPprob
))LR(XLI)(O|LR(XPprob
tOO
previous
iinext
iprevious
?
?
?  
    } 
   else if (i<n)  
   {             
          //
iX  and 1?iX  are continuous 
          //or all words between them is unaligned  
      
1
??
??
??
 ( );
 ( );
the firs  phra
??
?
se of ;
( );
* ,LI( ))?
??
?
* ;,0)
;?
?
?
?
?
?
?
?
? ?
p i
q i
q
previous
next p p
previous q
rule r LR X
rule r LL X
t r
O O t
prob P (O | r r
prob P (O | r
i
 
    } 
} 
 
Algorithm 1. Calculating the reordering probability for a span covered by a rule:
??? ?,X...XX,X...XX 2211022110 nnnn ttttssssr . 
 
As shown in Algorithm 1, the reordering probability depends on the lowest rules which cover the 
left/right most word. Therefore, we keep the lowest rules which cover the left/right most word for each 
partial translation. If two partial translations are same in everything but differ in the lowest rule, we 
need to keep both of them, rather than only keep the one with higher score. This will increase the 
complexity of the searching. 
4.4 Discussion 
Orientation can be determined based on word, phrase and hierarchical phrase (Galley and Manning, 
2008). What we adopt in this paper is word based orientation. It is based on the following considera-
tions: 
? Our baseline is a HPB system, which can capture hierarchical orientation. We use word based ori-
entation with the aim to complement the HPB system. 
? Word based orientation is consistent during training and decoding; phrase based orientation is 
prone to inconsistent between training and decoding.  
Galley and Manning (2008) has pointed out an inconsistency in Moses between training and decod-
ing. Here we would like to note that phrase based orientation depends on phrase segmentation. For 
example, in Figure 1, the orientation of phrase ?this is? with respect to next phrase could be either: 
? D, if we think the next phrase is ?the lower reach of ? which is what Figure 1 shows. 
? or S, if the next phrase is ?the lower reach of the yellow river? which can compose a legal phrase 
pair with ?huanghe xiayou? according to the standard phrase pair extraction algorithm.  
1150
The decision to adopt word-based orientation makes our work similar with Hayashi et al. (2010) who 
proposed a word-based reordering model for HPB system. The difference between our work and 
Hayashi et al. (2010) is: they adopt the reordering model proposed by Tromble and Eisner (2009) for 
the preprocessing approach, while we borrow the idea of lexicalized  reordering models which are 
originally proposed for phrase-based machine translation. 
5 Experiments 
5.1 Experimental settings 
Our baseline system is re-implementation of Hiero, a hierarchical phrase-based system (Chiang, 
2007). Besides the standard features of a HPB model, there are six reordering features in our reorder-
ing model which are M, S and D with respect to the previous and next phrase respectively. They are 
integrated into the log-linear model of the HPB system. The Minimum Error Rate Training (MERT) 
(Och, 2003) algorithm is adopted to tune feature weights for translation systems. 
We test our reordering model on a Chinese-English translation task. The NIST evaluation set MT06 
was used as our development set to tune the feature weights, and the test data are MT04, MT 05 and 
MT08. We first conduct experiments by using the FBIS parallel training corpus, and then further test 
the effect of our method on a large scale parallel training corpus. 
Word alignment is performed by GIZA++ (Och and Ney, 2000) in both directions with the default 
setting. The language model is a 4-gram model trained with the Xinhua portion of LDC English Gi-
gaword Version 3.0 and the English part of the bilingual training data. Translation performances are 
measured with case-insensitive BLEU4 score (Papineni et al., 2002). 
5.2 Experimental results on FBIS corpus 
We first conduct experiments by using the FBIS parallel corpus to train the model of both the baseline 
and our lexicalized reordering model. After pre-processing, the statistics of FBIS corpus is shown in 
table 1. 
 
 #sentences #words 
Chinese 128832 3016570 
English 128832 3922816 
Table 1. The statistics of FBIS corpus 
 
Table 2 summarizes the translation performance. The first row shows the results of baseline HPB 
system, and the second row shows the results when we integrated our lexicalized reordering model 
(LRM). We get 1.2, 0.8 and 0.7 BLEU point improvements over the baseline HPB system on three test 
sets respectively. 
 
 MT04 MT05 MT08 
HPB 33.53 32.97 25.08 
HPB+LRM 34.71 33.77 25.84 
Table 2. Translation performance on the FBIS corpus. 
5.3 Experimental results on large scale  corpus 
To further test the effect of our reordering model, we use a large scale corpus released by LDC. The 
catalog number of them is LDC2003E07, LDC2003E14, LDC2005T06, LDC2005T10, LDC2005E83, 
LDC2006E26, LDC2006E34, LDC2006E85 and LDC2006E92. There are 498K sentence pairs, 12.1M 
Chinese words and 13.8M English words. Table 3 summarizes the translation performance on the 
large scale of corpus.  
 
 MT04 MT05 MT08 
HPB 38.72 37.59 29.03 
HPB+LRM 39.81 38.24 29.63 
Table 3. Translation performance on a large scale parallel corpus. 
1151
Our model is still effective when we train the translation system on large scale data. We get 1.1, 0.7 
and 0.6 BLEU point improvements over the baseline HPB system on three test sets respectively. 
6 Conclusion and future work 
We proposed a novel lexicalized reordering model for hierarchical phrase based machine translation. 
The model is compatible with any kind of HPB rules no matter how complex the alignments are. We 
tested our reordering model on both small and large scale data. On NIST machine translation test sets, 
our reordering model achieved a 0.6-1.2 BLEU point improvements for Chinese-English translation 
over a strong baseline hierarchical phrase-based system. 
In future work, we will further test our model on other language pairs and compare it with other re-
ordering models for HPB translation. 
Acknowledgments 
We thank anonymous reviewers for insightful comments. The work of Hailong Cao is sponsored by 
Microsoft Research Asia Star Track Visiting Young Faculty Program. The work of HIT is also funded 
by the project of National Natural Science Foundation of China (No. 61173073) and International Sci-
ence & Technology Cooperation Program of China (No. 2014DFA11350). 
Reference 
Yaser Al-Onaizan and Kishore Papineni. 2006. Distortion Models for Statistical Machine Translation. In Pro-
ceedings of ACL. 
Colin Cherry, Robert C. Moore and Chris Quirk. 2012. On Hierarchical Re-ordering and Permutation Parsing for 
Phrase-based Decoding. In Proceedings of  NAACL Workshop on SMT. 
David Chiang. 2007. Hierarchical Phrase-based Translation. Computational Linguistics, 33(2):201?228. 
Michel Galley and Christopher D. Manning. 2008. A Simple and Effective Hierarchical Phrase Reordering Mod-
el. In Proceedings of EMNLP. 
Katsuhiko Hayashi, Hajime Tsukada, Katsuhito Sudoh, Kevin Duh and Seiichi Yamamoto. 2010. Hierarchical 
Phrase-based Machine Translation with Word-based Reordering Model. In Proceedings of COLING. 
Zhongjun He, Qun Liu, Shouxun Lin. 2008. Improving Statistical Machine Translation using Lexicalized Rule 
Selection. In Proceedings of COLING. 
Liang Huang, Hao Zhang and Daniel Gildea. 2005. Machine Translation as Lexicalized Parsing with Hooks. In 
Proceedings of IWPT. 
Zhongqiang Huang, Martin ?mejrek, and Bowen Zhou. 2010. Soft Syntactic Constraints for Hierarchical Phrase-
based Translation Using Latent Syntactic Distributions. In Proceedings of EMNLP.  
Matthias Huck, Joern Wuebker, Felix Rietig, and Hermann Ney. 2013. A Phrase Orientation Model for Hierar-
chical Machine Translation. In Proceedings of ACL Workshop on SMT.  
Philipp Koehn, Hieu Hoang, Alexandra Birch Mayne, Christopher Callison-Burch, Marcello Federico, Nicola 
Bertoldi, Brooke Cowan, Wade Shen, Christine Moran, Richard Zens, Chris Dyer, Ondrej Bojar, Alexandra 
Constantin, and Evan Herbst.. 2007. Moses: Open Source Toolkit for Statistical Machine Translation. In ACL 
demonstration session. 
Masaaki Nagata, Kuniko Saito, Kazuhide Yamamoto and Kazuteru Ohashi. 2006. A Clustered Global Phrase 
Reordering Model for Statistical Machine Translation. In Proceedings of ACL. 
Thuylinh Nguyen and Stephan Vogel. 2013. Integrating Phrase-based Reordering Features into Chart-based De-
coder for Machine Translation. In Proceedings of ACL. 
Franz Josef Och and Hermann Ney. 2000. Improved Statistical Alignment Models. In Proceedings of ACL. 
Franz Josef Och and Hermann Ney. 2004. The Alignment Template Approach to Statistical Machine Translation. 
Computational Linguistics, 30(4):417?449. 
Franz Josef Och. 2003. Minimum Error Rate Training in Statistical Machine Translation. In Proceedings of ACL. 
1152
 Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. 2002. BLEU: a Method for Automatic Evalua-
tion of Machine Translation. In Proceedings of ACL. 
Christoph Tillmann. 2004. A Unigram Orientation Model for Statistical Machine Translation. In Proceedings of 
HLT-NAACL. 
Roy Tromble, Jason Eisner. 2009. Learning Linear Ordering Problems for Better Translation. In Proceedings of 
EMNLP. 
Xinyan Xiao, Jinsong Su, Yang Liu, Qun Liu, and Shouxun Lin. 2011. An Orientation Model for Hierarchical 
Phrase-based Translation. In Proceedings of IALP.  
Deyi Xiong, Qun Liu, and Shouxun Lin. 2006. Maximum Entropy Based Phrase Reordering Model for Statisti-
cal Machine Translation. In Proceedings of ACL. 
Richard Zens and Hermann Ney. 2006. Discriminative Reordering Models for Statistical Machine Translation. 
In Proceedings of Workshop on SMT. 
Andreas Zollmann and Ashish Venugopal. 2006. Syntax Augmented Machine Translation via Chart Parsing. In 
Proceedings of NAACL Workshop on SMT. 
1153
Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers,
pages 2227?2236, Dublin, Ireland, August 23-29 2014.
 Soft Dependency Matching                                                                               
for Hierarchical Phrase-based Machine Translation
1
 
Hailong Cao1, Dongdong Zhang2, Ming Zhou2 and Tiejun Zhao1 
1Harbin Institute of Technology, Harbin, P.R. China 
2Microsoft Research Asia, Beijing, P.R. China 
{hailong, tjzhao}@mtlab.hit.edu.cn 
{Dongdong.Zhang, mingzhou}@microsoft.com 
Abstract 
This paper proposes a soft dependency matching model for hierarchical phrase-based (HPB) machine 
translation. When a HPB rule is extracted, we enrich it with dependency knowledge automatically learnt 
from the training data. The dependency knowledge not only encodes the dependency relations between 
the components inside the rule, but also contains the dependency relations between the rule and its con-
text. When a rule is applied to translate a sentence, the dependency knowledge is used to compute the 
syntactic structural consistency of the rule against the dependency tree of the sentence. We characterize 
the structure consistency by three features and integrate them into the standard SMT log-linear model to 
guide the translation process. Our method is evaluated on multiple Chinese-to-English machine transla-
tion test sets. The experimental results show that our soft matching model achieves 0.7-1.4 BLEU points 
improvements over a strong baseline of an in-house implemented HPB translation system. 
1 Introduction 
HPB model (Chiang, 2007) is widely used and has consistently delivered state-of-the-art performance. 
This model extends the phrase-based model (Koehn et al., 2003) by using the formal synchronous 
grammar to well capture the recursiveness of language during translation. In a formal synchronous 
grammar, the syntactic unit could be any sequence of contiguous terminals and non-terminals, which 
may not necessarily satisfy the linguistic constraints. HPB model is powerful to cover any format of 
translation pairs, but it might introduce ungrammatical rules and produce poor quality translations. 
To generate grammatical translations, lots of syntax-based models have been proposed by Galley et 
al. (2004), Liu et al. (2006), Huang et al. (2006), Mi et al. (2008), Shen et al. (2008), Xie et al. (2011), 
Zhang et al. (2008), etc. In these models, the syntactic units should be compatible with the syntactic 
structure of either the source sentence or the target sentence. These approaches can generate more 
grammatical translations by capturing the structural difference between language pairs. However, 
these models need special efforts to capture non-syntactic translation knowledge to improve the trans-
lation performance.  
It is desired to combine the advantages of syntax-based models and the HPB model (Stein et al., 
2010). There has been much work trying to improve HPB model by incorporating syntax information. 
Marton and Resnik (2008) leverage linguistic constituents to constrain the decoding softly. Some work 
go further to augment the non-terminals in HPB rules with syntactic tags which depend on the syntac-
tic structure covered by the non-terminals (Zollmann and Venugopal, 2006; Chiang, 2010; Li et al., 
2012; Huang et al., 2013). For example, given below HPB rules (1-4), the source non-terminal X 
could be refined into NP or PP as shown in rules (5-8) respectively.  
 
(1) <? ? X, borrowed X>                  (2) <? ? X, lent X>  
(3) <X1 ? ? X2, borrowed X2 X1>     (4) <X1 ? ? X2, X1 borrowed X2>  
 
(5) <?? NP, borrowed X>                  (6) <?? NP, lent X> 
(7) <PP ?? NP, borrow X2 X1>          (8) <NP ?? NP, X1 lent X2> 
 
Although augmenting the non-terminals with syntactic tags in these methods achieved better results 
for HPB model, they have limitations that the syntax information on the non-terminals are not discrim-
                                                 
This work is licenced under a Creative Commons Attribution 4.0 International License. Page numbers and proceedings footer 
are added by the organizers. License details: http://creativecommons.org/licenses/by/4.0/ 
 
2227
inative enough due to the limited context covered by the HPB rule. For example, rule (5) and (6) are 
still not discriminative when translating below two sentences (9) and (10). 
 
(9) ????????(I borrowed a book from him)    (10) ????????(I lent a book to him) 
 
where the common phrase ??????? appear in both sentences. Obviously, although rule (5) and 
(6) share same source sides, rule (5) can only be applied to the translation of sentence (9) and rule (6) 
to sentence (10). Otherwise, inappropriate application will lead to wrong translations. Rule (5) and (6) 
are not discriminative due to no consideration of their outside context during the translation.  
Motivated by such observation, we proposed an alternative approach, called soft dependency 
matching model, to incorporate into each HPB rule the source syntactic dependencies connecting the 
contents inside the rule with the context outside the rule. The dependency knowledge associated with 
HPB rules is automatically learnt from bilingual training corpus. They make HPB rules discriminative 
according to global context.  
 
 
Figure 1.  Dependency information associated with two rules. LC and RC mean the source context on 
the left and right of the rule respectively. 
 
Figure 1 shows two rules associated with different dependencies. The first one is applicable to the case 
when some word on the left side depends on the word ??? in the rule, and the second one is applica-
ble to the case when the word ??? in the rule depends on some word on the right side. 
During SMT decoding, first we parse the source sentence to get the dependency tree. When a HPB 
rule is applied to translate the sentence, we calculate structural consistency between the dependency 
knowledge associated with the rule and dependency tree structure of the source sentence. The con-
sistency degree is integrated into the SMT log-linear model as features to encourage syntactic hypoth-
eses and penalize the hypotheses violating syntactic constraints. 
Compared with previous work that incorporate syntax knowledge into HPB model, the advantage of 
our soft dependency matching model is: 
? It not only captures the dependency relations between the components inside the rule, but also 
models the dependency relations between the rule and its context from a global view. 
? Without increasing the amount of rules or the searching space, our model can capture the syntactic 
variation for all of the rules (syntactic or non-syntactic, well-formed or ill-formed). 
? Our model can take advantage of the dependency knowledge on both terminals and non-terminals.  
We evaluate the performance of our soft dependency matching model on Chinese-to-English trans-
lation task. Experimental results show that our method can achieve the improvements of 0.7-1.4 
BLEU points over the baseline HPB model on multiple NIST MT evaluation test sets. 
2 Related Work 
Ever since the invention of phrase-based model, a lot of efforts have been made to incorporate linguis-
tic syntax. Cherry(2008) and Marton and Resnik (2008) leverage linguistic constituent to constrain the 
decoding softly. In their methods, a translation hypothesis gets an extra credit if it respects the parse 
tree but may incur a cost if it violates a constituent boundary. The soft constrain based methods 
achieved promising results on various language pairs. One problem of these methods is that exactly 
matching syntactic constraints cannot always guarantee a good translation, and violating syntactic 
structure does not always induce a poor translation. It could be more reasonable if the credit and penal-
ty is learnt from the parallel training data. In this work, we learn this kind of constrain knowledge di-
rectly from the syntactic structures over the training corpus.  
Xiong et al. (2009) present a method that automatically learns syntactic constraints from training 
data for the ITG based translation (Wu, 1997; Xiong et al., 2006).  They utilize the syntactic con-
straints to estimates the extent to which a span is bracketable. Though the effect was demonstrated on 
the ITG based model, the method is also applicable to the HPB model. The main difference between 
Xiong et al. (2009) and our work is that we try to estimate the structural consistency of each rule 
2228
against the source syntax tree. For rules which are same in the source side but different in the target 
side, our method will distinguish the inconsistency degree for different rules. While, for such rules, 
Xiong et al. (2009) will give a same score which will be used to compete with rules in other spans. 
More recently, Huang et al. (2013) associate each non-terminal with the distribution of tags that is 
used to measure the consistency of syntactic compatibility of the translation rule on source spans. Our 
work is similar to Huang et al. (2013) since we also represent the syntactic variation of translation 
rules in the form of distribution. The main difference is that they annotate non-terminals with head 
POS tags while we use dependency triples (over both terminals and non-terminals) to explicitly repre-
sent both the dependency relations inside the rule, and that between the rule and its context. 
Both above related work and our work need parse the source sentence to get syntactic context be-
fore decoding. There are also some methods incorporating syntax information without the need of 
online parsing the source sentences (Zollmann and Venugopal, 2006; Shen et al, 2009; Chiang, 2010). 
They parse the training data to label the non-terminals with syntactic tags. During the bottom-up de-
coding, the tags are used to model the substitution of non-terminals in a soft way (Shen et al, 2009; 
Chiang, 2010) or in a hard way (Zollmann and Venugopal, 2006).  
Gao et al. (2011) derive soft constraints from the source dependency parsing for the HPB translation. 
They focus on the relative order of each dependent word and its head word after translation, while our 
method models whether the dependency information of a rule matches the context or not.  
Our work utilizes contextual information around translation rules. In this sense, it is similar to He et 
al. (2008) and Liu et al. (2008). The main difference between their work and our work is that they lev-
erage lexical context for rule selection while we focus on the syntactic contextual information. 
3 Hierarchical Phrase based Machine Translation 
Our model proposed in this paper is an extension of the HPB model (Chiang, 2007). Formally, HPB 
model is a weighted synchronous context free grammar. It employs a generalization of the standard 
plain phrase extraction approach in order to acquire the synchronous rules of the grammar directly 
from word-aligned parallel text. Rules have the form of: 
          
where X is a nonterminal,   and   are both strings of terminals and non-terminals from source and tar-
get side respectively, and ? is a one-to-one correspondence between nonterminal occurrences in   and 
 . Associated with each rule is a set of feature functions with the form        . These feature functions 
are combined into a log-linear model. When a rule is applied during SMT decoding, its score is calcu-
lated as: 
?          
 
 
where    is the weight associated with feature function         . The feature weights are typically op-
timized using minimum error rate training algorithm (Och, 2003).  
4 Soft Dependency Matching Model 
In order to incorporate syntactic knowledge to refine both the word ordering and word sense disam-
biguation for HPB model, we propose a soft dependency matching model (SDMM). It extends HPB 
rule into a form which is named as SDMM rule: 
 
              
 
where RDT(rule?s dependency triples) is a set of dependency triples defined on source string   . Each 
element in RDT is a triple representing dependency knowledge in the form: 
 
{m-h-l} 
 
where m and h are the dependent and head respectively, l is the label of the dependency relation type. 
m and h could be any of terminals, non-terminals, LC and RC, where LC denotes the left context and 
RC the right context. 
In the following two sub-sections, we will explain the details of SDMM rule extensions for both 
plain phrases (i.e., there are no non-terminals in both         ) and hierarchical rules (i.e., there are at 
2229
least one non-terminal in both         )  respectively. For simplicity, we ignore the correspondence   
in the representations of both HPB rules and SDMM rules. 
 
 
Figure 2: An illustration of a dependency parse tree for the source side of a word-aligned parallel sen-
tences pair. 
4.1 SDMM Over Plain Phrase Rules 
Figure 2 illustrates a parallel sentence together with word alignments and source dependency parse 
tree, from which we can extract the phrase pairs of HPB rules like: 
 
(11) < ? ? ?, a book >        (12) < ? ? ? ? ?, borrowed a book > 
 
By incorporating syntactic knowledge, we can extend these HPB rules into SDMM rules as shown 
in Figure 3(a) and Figure 3(b) respectively.  
 
Figure 3: An illustration of two phrase pairs annotated with a set of dependency triples. 
 
Formally, the RDT corresponding to phrase pair (11) is {?-LC-dobj}. The RDT corresponding to 
phrase pair (12) is {LC-?-nsubj, LC-?-prep}. 
Now we describe how to build the RDT when a phrase pair is extracted from a sentence pair during 
the training step. First, we initialize RDT to be empty. Then, for each dependency triple ?m-h-l? in the 
parse tree of the source sentence, if either m or h is covered by the source phrase in the rule, we add it 
to RDT. However, if both m and h are covered by the source phrase, we will ignore it because it holds 
less syntactic information beyond HPB rule itself. For example, the dependency triple ?? -? -
nummod? is excluded from RDT for both phrase pair (11) and phrase pair (12). In addition, we do not 
add the dependency triple ?m-h-l? into RDT if both m and h are not contained in source phrase, be-
cause it is not related to phrase pair at all. The dependency triple ??-?-pobj? is such a case for both 
phrase pair (11) and phrase pair (12).  
Finally, we normalize the word in RDT that is not covered by the source phrase with either LC 
(stands for the left context) or RC (stands for the right context) according to its relative position to the 
source phrase. For example, in the RDT for phrase pair (11), we normalize ??-?-dobj? as ??-LC-
dobj? since the word??? is not covered by the source phrase and it is treated as left context.  
Note that for each context word outside the source phrase, we only record whether it is on the left or 
on the right of phrase. We do not further consider its lexical form and its distance to the source phrase. 
For example, in the two dependency triples in Figure 3(b), both the dependent word ??? and ??? are 
normalized into LC. In this way, we can generalize the dependency triples in RDT and alleviate the 
data sparseness problem. In fact, there might be duplicated dependency triples for a phrase pair. In this 
case, we only keep one of them. 
4.2 SDMM over Hierarchical Rules 
Hierarchical rules are usually generated by substituting sub-phrases with non-terminals from plain 
phrase pairs. For example, given the parallel sentence and the two phrase pairs in Section 4.1, we can 
get a hierarchical rule like:  
<? ? X, borrowed X> 
To extend hierarchical rules into SDMM rules, we add dependency information to source terminals 
or non-terminals in RDT. Figure 4 shows an example representing an SDMM rule: 
2230
 
Figure 4: An illustration of a hierarchical rule annotated with a set of dependency triples. 
 
The generation of SDMM rules over hierarchical rules is similar to that of plain phrase rules. The only 
difference lies in processing the non-terminals, whose dependencies are inferred from the words they 
covered. For example, the RDT of the above SDMM rule would be:{LC-?-nsubj, LC-?-prep, X-?-
dobj} 
Similarly, any dependencies over two terminals contained in the source rule are not included in 
RDT, and dependencies inferred from same non-terminals are excluded as well. In addition, depend-
encies between two non-terminals are ignored.   
4.3 SDMM Rule Composing 
A same HPB rule (either plain phrase pair or a hierarchical rule) can be extracted from different bilin-
gual sentences. Therefore, the same HPB rule could be extended into multiple SDMM rules. For ex-
ample, given a parallel sentence pair shown in Figure 5, 
 
 
Figure 5: An example of a dependency tree over the source sentence together with the word-aligned 
target sentence. 
 
we might get a SDMM rule as shown in Figure 6. Compared to the SDMM rule in Figure 4, there is an 
additional dependency triple ?LC-?-tmod? in RDT. 
 
 
Figure 6: An illustration of dependency triples associated to a hierarchical rule. 
 
Intuitively, we can process SDMM rules independently although they share the same information of 
HPB rules. However, this will exacerbate the data sparseness problem and make the computation inef-
ficient due to dramatically increased model size. An alternative way is only to keep the most frequent 
RDT information for the same HPB rules. Though this can get a very concise model, a lot of useful 
syntactic information might be lost. 
We propose a balanced composing method to make a trade-off between knowledge representation 
and computation efficiency of SDMM rules. Suppose there are more than one SDMM rules with dif-
ferent      but the same HPB rule, we compose them by the union and get the new form of RDT as: 
 
    ?    
 
 
 
In addition, we record the frequency of HPB rule as well as that of each dependency triple in RDT 
as: 
             ,                 
 
where              is the number of times that HPB rule           is extracted from the 
training data, and                 is the frequency that    and           co-occur. For ex-
ample, suppose SDMM rules in Figure 4 and Figure 6 occurs 9 and 1 times respectively, we can com-
pose them into the form as shown in Figure 7.  
 
2231
 
Figure 7: Composed form of the dependency annotation of a rule. The integers following the colons 
denote occurring times. 
 
Therefore, the composed SDMM rule will be represented by the original HPB rule <? ? X, bor-
rowed X> together with RDT and its frequency information shown in Table 1. 
 
RDT # 
{ LC-?-tmod, 
   LC-?-nsubj, 
   LC-?-prep, 
    X - ?-dobj } 
1 
10 
10 
10 
Table 1. The RDT and its frequency information of a composed SDMM rule. 
4.4 Consistency of SDMM Rules 
So far we have described how to enrich a rule with RDT in the training step. Now we introduce how to 
use the RDT of each rule to guide the translation process. 
In the decoding, we parse the source sentence to get the dependency parse tree as shown in Figure 8. 
When we apply a rule to get a partial translation for a span, we also extract a set of dependency triples 
based on the parse tree in the exact same way that is used in the training step. We denote this by CDT 
(context dependency triples). Suppose the rule <? ? X, borrowed X> is applied to translate the un-
derlined span in Figure 8, then the CDT for the rule is: {LC-?-nsubj, LC-?- dep, X-?-dobj}. 
 
 
Figure 8: A sentence to be translated and its dependency parse tree. 
 
In order to evaluate whether a SDMM rule is applicable to translate a sentence or not from the syn-
tactic view, we model the structural consistency of SDMM rule against source dependency tree by cal-
culating the matching degree between RDT and CDT. The example in Figure 9 illustrates how we 
compute the matching degree between the SDMM rule in Figure 7 and CDT over the source depend-
ency tree in Figure 8. We estimate the matching degree based on three sets including the relative com-
plement set of CDT in RDT, the intersection set of RDT and CDT, and the relative complement set of 
RDT in CDT. 
 
 
Figure 9: Three different sets of dependency triples to model the structural consistency of syntactic 
matching. 
 
The statistics over above three sets are leveraged to design three features which are incorporated into 
SMT log-linear model to encourage and penalize various syntactic motivated hypotheses. The first 
feature is called as the lost dependency triple feature   . It is calculated based on the set RDT\CDT as: 
 
   ?                   
         
              
2232
 where   is the indicator function whose value is one if and only if the condition is true, otherwise its 
value is zero. The motivation of     is that: if a dependency triple which always co-occur with the HPB 
rule is not observed in CDT, it indicates the current SDMM rule may mismatch with the source sen-
tence and therefore we need to penalize its application. In Figure 9, ?LC-?-prep? is such a dependen-
cy triple. However, for the less frequent dependency triples in RDT such as ?LC-?-tmod? in Figure 8, 
there is no penalty on it although it is not found in CDT.  
The second feature is the unexpected dependency triple feature   , which is computed as :   
   |       | 
 
This feature is the number of dependency triples in CDT that never co-occur with the rule in the train-
ing data. In Figure 9, ?LC-?-dep? is such a case. Intuitively, the higher the value    is, the higher in-
consistency degree is, because it means that many dependency triples in CDT are never observed in 
the training corpus. We should discourage the application of the corresponding SDMM rule.  
The third feature is the matched dependency triple feature  
 
 which is calculated based on 
RDT?CDT. It is directly used to model the structural consistency over all the dependency triples in 
RDT?CDT for the application of HPB rule          . Formally,  
 
 is defined as the sum of log 
probability of each dependency triple in RDT?CDT conditioned on the HPB rule: 
 
   ?        |           
         
 
 
where    |           is the probability of a dependency triple  associated to a HPB rule    
       . We estimate it based on the relative frequency and experimentally use the adding 0.5 
smoothing. 
5 Experiments 
5.1 Experimental Settings 
Our baseline is the re-implementation of the Hiero system (Chiang, 2007). When our soft dependency 
matching model is integrated, the HPB rule is extended into the form of  
              and the score is calculated by: 
 
?           
 
                                              
 
where the additional three features are defined in Section 4.3,   ,    and    are corresponding feature 
weights. 
We test our soft dependency matching model on a Chinese-English translation task. The NIST06 
evaluation data was used as our development set to tune the feature weights, and NIST04, NIST05 and 
NIST08 evaluation data are our test sets. We first conduct experiments by using the FBIS parallel cor-
pus, and then further test the performance of our method on a large scale training corpus. 
Word alignment is performed by GIZA++ (Och and Ney, 2000) in both directions with the default set-
ting. 4-gram language model is trained over the Xinhua portion of LDC English Gigaword Version 3.0 
and the English part of the bilingual training data. Feature weights are tuned with the minimum error 
rate training algorithm (Och, 2003).Translation performance is measured with case-insensitive BLEU4 
score (Papineni et al., 2002). 
All the Chinese sentences in the training set, development set and test set are parsed by an in-house 
developed dependency parser based on shift-reduce algorithm (Zhang and Nivre, 2011). There are 45 
named grammatical relations plus a default relation representing unknown cases. The detailed descrip-
tions about dependency parsing are explained in Chang et al. (2009). 
5.2 Experimental Results on FBIS Corpus 
We first conduct experiments by using the FBIS parallel corpus to train the model of both the baseline 
and the soft dependency matching model. Table 2 shows the statistics of FBIS corpus after the pre-
processing. 
 
2233
  #sentences #words 
Chinese 128,832 3,016,570 
English 128,832 3,922,816 
Table 2. The statistics of FBIS corpus 
 
The evaluation results over FBIS corpus are reported in Table 3. The first row shows the results of 
baseline, the next three rows show the effect of three features respectively and the last row gives the 
result when all features are integrated together. Based on Table 3, we can see that each individual fea-
ture improves the performance. Among all integrated features, the third feature  
 
 is the most effec-
tive one. The best performance is achieved when using all three features, where we get 1.4, 0.9 and 1.2 
BLEU points improvements respectively over the baseline on three test sets. 
 
 NIST04 NIST05 NIST08 
Baseline 33.53 32.97 25.08 
Baseline+fl 34.59 33.44 25.69 
Baseline+fu 34.48 33.59 25.51 
Baseline+fm 34.73 33.74 25.76 
Baseline+fl+fu+fm 34.96 33.91 26.28 
Table 3. Translation performance over BLEU% when models are trained on the FBIS corpus. 
5.3 Experimental Results on Large Scale  Corpus 
To further test the effect of our soft dependency matching model, we use a large scale corpus released 
by LDC. The catalog number of them is LDC2003E07, LDC2003E14, LDC2005T06, LDC2005T10, 
LDC2005E83, LDC2006E26, LDC2006E34, LDC2006E85 and LDC2006E92. There are 498K sen-
tence pairs, 12.1M Chinese words and 13.8M English words. Table 4 summarizes the translation per-
formance on the large scale of corpus. Our model is still effective when we train the translation system 
on large scale data. We get 1.3, 0.7 and 1.0 BLEU point improvements over the baseline on three test 
sets respectively, which shows that our method can consistently improve HPB system over different 
sized training corpus. 
 
 NIST04 NIST05 NIST08 
Baseline 38.72 37.59 29.03 
Baseline+fl+fu+fm 40.00 38.34 30.06 
Table 4. Translation performance over BLEU% when models are trained on a large scale parallel 
 corpus. 
5.4 Decoding Cost 
Incorporating syntax can improve the translation performance, but it might increase the SMT decoding 
complexity. One advantage of our method is that it does not increase the amount of translation rules, 
so the searching space is not enlarged. Table 5 shows the decoding time comparison with the baseline 
when models are trained on the FBIS corpus. The average decoding time per sentence is only in-
creased by about 12% due to the parsing of source sentences and the computation of the features. We 
believe that this is acceptable given the performance gain. 
 
 NIST04 NIST05 NIST08 
Baseline 0.67sec 0.78sec 0.50sec 
Baseline+fl+fu+fm 0.88sec 0.87sec 0.56sec 
Table 5.  The average decoding time per sentence, measured in second/sentence. 
6 Conclusion and Future Work 
We proposed a soft dependency matching model for HPB machine translation. We enrich the HPB 
rule with dependency knowledge learnt from the training data. The dependency knowledge allows our 
model to capture the both the dependency relations inside the rule and the dependency relations be-
tween the rule and its context from a global view. During decoding, the syntax structural consistency 
of rules against source dependency tree is calculated and converted into SMT log-linear model fea-
2234
tures to guide the translation process. The experimental results show that our soft matching model 
achieves significant improvements over a strong baseline of an in-house implemented HPB system. 
In future work, there is much room to improve the performance via our method. First, we can dis-
criminatively learn the contribution of the dependency knowledge of each rule based on the training 
data. Second, we can go beyond the current ?bag of dependency triples? representation by composing 
them hierarchically to capture deep syntactic information. Third, section 2 has discussed the theoreti-
cal difference with related work on adding source syntax into the HPB model, we are interested in 
empirically comparing our method with them and combining it with them to get further improvement. 
Acknowledgments 
We thank anonymous reviewers for insightful comments. The work of Hailong Cao is sponsored by 
Microsoft Research Asia Star Track Visiting Young Faculty Program. The work of HIT is also funded 
by the project of National Natural Science Foundation of China (No. 61173073) and International Sci-
ence & Technology Cooperation Program of China (No. 2014DFA11350). 
Reference 
Pi-Chuan Chang, Huihsin Tseng, Dan Jurafsky, and Christopher D. Manning.  2009. Discriminative Reordering 
with Chinese Grammatical Relations Features. In Proceedings of NAACL Workshop on SSST. 
Colin Cherry. 2008. Cohesive Phrase-based Decoding for Statistical Machine Translation. In Proceedings of 
ACL. 
David Chiang. 2007. Hierarchical Phrase-based Translation. Computational Linguistics, 33(2):201?228. 
Yang Gao, Philipp Koehn, and Alexandra Birch. 2011. Soft Dependency Constraints for Reordering in Hierar-
chical Phrase-Based Translation. In Proceedings of EMNLP.  
Zhongjun He, Qun Liu, Shouxun Lin. 2008. Improving Statistical Machine Translation using Lexicalized Rule 
Selection. In Proceedings of COLING. 
Liang Huang, Kevin Knight, and Aravind Joshi. 2006. Statistical syntax-directed translation with extended-
domain of locality. In Proceedings of AMTA. 
Zhongqiang Huang, Martin ?mejrek, and Bowen Zhou. 2010. Soft Syntactic Constraints for Hierarchical Phrase-
based Translation Using Latent Syntactic Distributions. In Proceedings of EMNLP. 
Zhongqiang Huang, Jacob Devlin, and Rabih Zbib. 2013. Factored Soft Syntactic Contraints for Hierarchical 
Machine Translation. In Proceedings of EMNLP.  
Franz Josef Och and Hermann Ney. 2000. Improved Statistical Alignment Models. In Proceedings of ACL. 
Franz Josef Och. 2003. Minimum Error Rate Training in Statistical Machine Translation. In Proceedings of ACL. 
Philipp Koehn, Franz Josef Och, Daniel Marcu. 2003. Statistical phrase based translation. In Proceedings of 
NAACL. 
Junhui Li, Zhaopeng Tu, Guodong Zhou, and Josef van Genabith. 2012. Using Syntactic Head Information in 
Hierarchical Phrase-based Translation. In Proceedings of WMT. 
Yang Liu, Qun Liu, and Shouxun Lin. 2006. Tree to-string alignment template for statistical machine translation. 
In Proceedings of ACL. 
Yuval Marton and Philip Resnik. 2008. Soft syntactic constraints for hierarchical phrased-based translation. In 
Proceedings of ACL.  
Haitao Mi and Liang Huang. 2008. Forest-based translation rule extraction. In Proceedings of EMNLP.  
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. 2002. BLEU: a Method for Automatic Evalua-
tion of Machine Translation. In Proceedings of ACL. 
 Libin Shen, Jinxi Xu, and Ralph Weischedel. 2008. A new string-to-dependency machine translation algorithm 
with a target dependency language model. In Proceedings of ACL. 
Libin Shen, Jinxi Xu, Bing Zhang, Spyros Matsoukas, and Ralph Weischedel. 2009. Effective use of linguistic 
and contextual information for statistical machine translation. In Proceedings of EMNLP.  
2235
Daniel Stein, Stephan Peitz, David Vilar, and Hermann Ney. 2010. A Cocktail of Deep Syntactic Features for 
Hierarchical Machine Translation. In Conference of the Association for Machine Translation in the Americas. 
Dekai Wu. 1997. Stochastic inversion transduction grammars and bilingual parsing of parallel corpora. Compu-
tational Linguistics, 23(3):377?404. 
Jun Xie, Haitao Mi and Qun Liu. 2011. A Novel Dependency-to-String Model for Statistical Machine Transla-
tion. In Proceedings of EMNLP. 
Deyi Xiong, Qun Liu, and Shouxun Lin. 2006. Maximum Entropy Based Phrase Reordering Model for Statisti-
cal Machine Translation. In Proceedings of ACL. 
Deyi Xiong, Min Zhang, Aiti Aw, Haizhou Li. 2009. A Syntax-Driven Bracketing Model for Phrase-Based 
Translation.  In Proceedings of ACL. 
Andreas Zollmann and Ashish Venugopal. 2006. Syntax Augmented Machine Translation via Chart Parsing. In 
Proceedings of NAACL Workshop on SMT.  
Min Zhang, Hongfei Jiang, Aiti Aw, Haizhou Li, Chew Lim Tan, and Sheng Li. 2008. A tree sequence align-
ment-based tree-to-tree translation model. In Proceedings of ACL. 
Yue Zhang and Joakim Nivre. 2011. Transition-based Dependency Parsing with Rich Non-local Features In Pro-
ceedings of ACL. 
2236
Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1055?1065,
Seattle, Washington, USA, 18-21 October 2013. c?2013 Association for Computational Linguistics
Multi-domain Adaptation for SMT Using Multi-task Learning?
Lei Cui1, Xilun Chen2, Dongdong Zhang3, Shujie Liu3, Mu Li3, and Ming Zhou3
1Harbin Institute of Technology, Harbin, P.R. China
leicui@hit.edu.cn
2Cornell University, Ithaca, NY, U.S.
xlchen@cs.cornell.edu
3Microsoft Research Asia, Beijing, P.R. China
{dozhang,shujliu,muli,mingzhou}@microsoft.com
Abstract
Domain adaptation for SMT usually adapts
models to an individual specific domain.
However, it often lacks some correlation
among different domains where common
knowledge could be shared to improve the
overall translation quality. In this paper, we
propose a novel multi-domain adaptation ap-
proach for SMT using Multi-Task Learning
(MTL), with in-domain models tailored for
each specific domain and a general-domain
model shared by different domains. The pa-
rameters of these models are tuned jointly via
MTL so that they can learn general knowledge
more accurately and exploit domain knowl-
edge better. Our experiments on a large-
scale English-to-Chinese translation task val-
idate that the MTL-based adaptation approach
significantly and consistently improves the
translation quality compared to a non-adapted
baseline. Furthermore, it also outperforms the
individual adaptation of each specific domain.
1 Introduction
Domain adaptation is an active topic in statisti-
cal machine learning and aims to alleviate the do-
main mismatch between training and testing data.
Like many machine learning tasks, Statistical Ma-
chine Translation (SMT) assumes that the data dis-
tributions of training and testing domains are sim-
ilar. However, this assumption does not hold for
real world SMT systems since training data for
SMT models may come from a variety of domains.
The translation quality is often unsatisfactory when
?This work was done while the first and second authors were
visiting Microsoft Research Asia.
translating texts from a specific domain using a gen-
eral model that is trained over a hotchpotch of bilin-
gual corpora. Therefore, domain adaptation is cru-
cial for SMT systems to achieve better performance.
Previous research on domain adaptation for SMT
includes data selection and weighting (Eck et al,
2004; Lu? et al, 2007; Foster et al, 2010; Moore and
Lewis, 2010; Axelrod et al, 2011), mixture mod-
els (Foster and Kuhn, 2007; Koehn and Schroeder,
2007; Sennrich, 2012; Razmara et al, 2012), and
semi-supervised transductive learning (Ueffing et
al., 2007), etc. Most of these methods adapt SMT
models to a specific domain according to testing data
and have achieved good performance. It is natural
that real world SMT systems should adapt the mod-
els to multiple domains because the input may be
heterogeneous, so that the overall translation qual-
ity can be improved. Although we can easily ap-
ply these methods to multiple domains individually,
it is difficult to use the common knowledge across
different domains. To leverage the common knowl-
edge, we need to devise a multi-domain adaptation
approach that jointly adapts the SMT models.
Multi-domain adaptation has been proved quite
effective in sentiment analysis (Dredze and Cram-
mer, 2008) and web ranking (Chapelle et al, 2011),
where the commonalities and differences across
multiple domains are explicitly addressed by Multi-
task Learning (MTL). MTL is an approach that
learns one target problem with other related prob-
lems at the same time, using a shared feature repre-
sentation. The key advantage of MTL is to enable
implicit data sharing and regularization. Therefore,
it often leads to a better model for each task. Anal-
ogously, we expect that the overall translation qual-
ity can be further improved by using an MTL-based
1055
TM1
S1 S2 S3
Multiple In-domain 
Translation Models
& Language Models
One General-domain 
Translation Model
& Language Model
T1
LM1
TM2
LM2
TM3
LM3
Domain-specific 
SMT Systems
In-domain
Training Data T2 T3
T
MTL-based Tuning
Entire
Training Data
TM-G
LM-G
TMN
LMN
TN
SN
...
...
...
...
Figure 1: An example with N pre-defined domains, where T is the entire training corpus. Ti is the in-domain training
data for the i-th domain selected from T using the bilingual cross-entropy based method (Axelrod et al, 2011). The
in-domain TMi and LMi are trained using the in-domain training data Ti. The general-domain models TM-G and LM-
G are trained using the entire training corpus T . Si is the domain-specific SMT system for the i-th domain, leveraging
the in-domain models and the general-domain models as features.
multi-domain adaptation approach.
In this paper, we use MTL to jointly adapt SMT
models to multiple domains. Specifically, we de-
velop multiple SMT systems based on mixture mod-
els, where each system is tailored for one specific
domain with an in-domain Translation Model (TM)
and an in-domain Language Model (LM). Mean-
while, all the systems share a same general-domain
TM and LM. These SMT systems are considered as
several related tasks with a shared feature represen-
tation, which fits well into a unified MTL frame-
work. With the MTL-based joint tuning, general
knowledge can be better learned by the general-
domain models, while domain knowledge can be
better exploited by the in-domain models as well.
By using a distributed stochastic learning approach
(Simianer et al, 2012), we can estimate the fea-
ture weights of multiple SMT systems at the same
time. Furthermore, we modify the algorithm to treat
in-domain and general-domain features separately,
which brings regularization to multiple SMT sys-
tems in an efficient way. Experimental results have
shown that our method can significantly improve the
translation quality on multiple domains over a non-
adapted baseline. Moreover, the MTL-based adap-
tation also outperforms the conventional individual
adaptation approach towards each domain.
The rest of the paper is organized as follows: The
proposed approach is explained in Section 2. Exper-
imental results are presented in Section 3. Section 4
introduces some related work. Section 5 concludes
the paper and suggests future research directions.
2 The Proposed Approach
Figure 1 gives an example with N pre-defined do-
mains to illustrate the main idea. There are three
steps in the training phase. First, in-domain train-
ing data is selected according to the pre-defined do-
mains (Section 2.1). Second, in-domain models and
general-domain models are trained to develop the
domain-specific SMT systems (Section 2.2). Third,
multiple domain-specific SMT systems are tuned
jointly by using an MTL-based approach (Section
2.3).
2.1 In-domain Data Selection
In the first step, in-domain bilingual data is selected
from all the bilingual data to train in-domain TMs.
We use the bilingual cross-entropy based approach
(Axelrod et al, 2011) to obtain the in-domain data:
[HI?src(s)?HG?src(s)]+[HI?tgt(t)?HG?tgt(t)] (1)
1056
where {s,t} is a bilingual sentence pair in the entire
bilingual corpus. HI?xxx(?) and HG?xxx(?) repre-
sent the cross-entropy of a string according to an in-
domain LM and a general-domain LM, respectively.
?xxx? denotes either the source language (src) or the
target language (tgt). HI?src(s)?HG?src(s) is the
cross-entropy difference of string s between the in-
domain and general-domain source-side LMs, and
HI?tgt(t) ? HG?tgt(t) is the cross-entropy differ-
ence of string t between the in-domain and general-
domain target-side LMs. This criterion biases to-
wards sentence pairs that are like the in-domain cor-
pus but unlike the general-domain corpus. There-
fore, the sentence pairs with lower scores (larger dif-
ferences) are presumed to be better.
Now, the question is how to find sufficient mono-
lingual data to train in-domain LMs. A straight-
forward solution is to collect the data from the in-
ternet. There are a large number of monolingual
webpages with domain information from web por-
tal sites1, which can be collected to train in-domain
LMs. In large-scale real world SMT systems, practi-
cal domain adaptation techniques should target more
domains rather than just one due to heterogeneous
input. Therefore, we use a web crawler to collect
monolingual webpages ofN domains from web por-
tal sites, for both the source language and the tar-
get language. The statistics of web-crawled data is
given in Section 3.1. We use the web-crawled mono-
lingual documents to train N in-domain source-side
LMs and N in-domain target-side LMs. Addition-
ally, we also train the source-side and target-side
general-domain LMs with all the web-crawled doc-
uments from different domains. Finally, these in-
domain and general-domain LMs are used to select
in-domain bilingual data for different domains ac-
cording to Formula (1).
2.2 SMT Systems with Mixture Models
In the second step, with the selected in-domain train-
ing data, we develop SMT systems based on mix-
ture models. In particular, we use the mixture model
based approach proposed by Koehn and Schroeder
1Many web portal sites contain domain information
for webpages, such as ?www.yahoo.com? in English and
?www.sina.com.cn? in Chinese and etc. The webpages are of-
ten categorized by human editors into different domains, such
as politics, sports, business, etc.
(2007). Specifically, we have developed N SMT
systems for N domains respectively, where each
system is a typical log-linear model. For each sys-
tem, the best translation candidate f? is given by:
f? = argmax
f
{P (f |e)} (2)
where the translation probability P (f |e) is given by:
P (f |e) ?
?
i
wi ? log ?i(f, e)
=
?
j?I
wj ? log ?j(f, e)
? ?? ?
In-domain
+
?
k?G
wk ? log ?k(f, e)
? ?? ?
General domain
(3)
where ?j(f, e) is the in-domain feature function and
wj is the corresponding feature weight. ?k(f, e) is
the general-domain feature function and wk is the
feature weight. The detailed feature description is
as follows:
In-domain features
? An in-domain TM, including phrase translation
probabilities and lexical weights for both direc-
tions (4 features)
? An in-domain target-side LM (1 feature)
? word count (1 feature)
? phrase count (1 feature)
? NULL penalty (1 feature)
? Number of hierarchical rules used (1 feature)
General-domain features
? A general-domain TM, including phrase trans-
lation probabilities and lexical weights for both
directions (4 features)
? A general-domain target-side LM (1 feature)
The feature description indicates that each SMT
system contains two TMs and two LMs. The in-
domain TMs are trained using the selected bilin-
gual training data according to Formula (1), and the
general-domain TM is trained using the entire bilin-
gual training data. For the LMs, we re-use the target-
side in-domain LMs and general-domain LM trained
1057
for data selection (Section 2.1). Compared with a
normal single-model system, the system with mix-
ture models can balance the contributions from the
general-domain and in-domain knowledge. Hence it
potentially benefits from both.
2.3 MTL-based Tuning
In the third step, the feature weights in multiple
domain-specific SMT systems are estimated. In-
stead of tuning each domain-specific system sepa-
rately, we treat different systems as related tasks and
tune them jointly in an MTL framework. There are
two main reasons for MTL-based tuning:
1. Domain-specific translation tasks share the
same general-domain LM and TM. MTL often
leads to better performance by leveraging com-
monalities among different tasks.
2. By enforcing that the general-domain LM and
TM perform equally across different domains,
MTL provides a kind of regularization to pre-
vent over-fitting.
Formally, the objective function of the proposed
MTL-based approach is described as follows:
min
W
{
N?
i=1
Loss(Ei, e?(Fi,wi))
}
(4)
where N is the number of pre-defined domains.
{Fi,Ei} is the in-domain development dataset for the
i-th domain. Fi denotes the source sentences and Ei
denotes the reference translations. wi is a D-length
feature weight column vector for the i-th domain,
where D is the dimension of the feature space. W is
a N -by-D matrix, representing [w1|w2| . . . |wN ]T .
e?(Fi,wi) are the best translations obtained for Fi
with parameters wi. Loss(?, ?) denotes the loss be-
tween the system?s output and the reference trans-
lations. The basic idea of the objective function is
to minimize the sum of loss functions for all the do-
mains, rather than one domain at a time. Therefore,
by adjusting the in-domain and general-domain fea-
ture weights, the translation quality is expected to be
good across different domains.
To effectively tune SMT systems jointly, we mod-
ify the asynchronous Stochastic Gradient Descend
(SGD) Algorithm (Simianer et al, 2012) to optimize
objective function (4). We follow the pairwise rank-
ing approach with the perceptron algorithm (Shen
and Joshi, 2005) to update feature weights. Let a
translation candidate be denoted by its feature vector
v ? RD, the pairwise preference for training is con-
structed by ranking two candidates according to the
smoothed sentence-level BLEU (Liang et al, 2006).
For a preference pair v[j]=(v(1), v(2)) where v(1) is
preferred, a hinge loss is used:
L(wi) = (??wi, v(1) ? v(2)?)+ (5)
where (x)+ = max(0, x) and ??, ?? denotes the in-
ner product of two vectors. With the perceptron al-
gorithm (Shen and Joshi, 2005), the gradient of the
hinge loss is:
?L(wi) =
{
v(2) ? v(1) if?wi, v(1) ? v(2)? ? 0
0 otherwise
(6)
The training instances for the discriminative
learning in pairwise ranking are made by comparing
the N-best list of the translation candidates scored
by the smoothed sentence-level BLEU (Liang et al,
2006). Following Simianer et al (2012), the N-best
list is divided into three bins: the top 10% (High),
the middle 80% (Middle), and the last 10% (Low).
These bins are used for pairwise ranking where the
translation preference pairs are built between the
candidates in High-Middle, Middle-Low, and High-
Low, but not the candidates within the same bin,
which is shown in Figure 2. The idea is to guar-
antee that the ranker is more discriminative to prefer
the good translations to the bad ones.
High: 10%
Middle: 80%
Low: 10%
N-best list
Figure 2: Training instances for pairwise ranking.
1058
Algorithm 1 Modified Asynchronous SGD
1: Distribute N domain-specific decoders to N ma-
chines
2: Initialize w1,w2, . . . ,wN ? 0
3: for epochs t? 0 . . . T ? 1 do
4: for all domains d ? {1 . . . N}: parallel do
5: ud,t,0,0 = wd
6: S = |Fd|
7: for all i ? {0 . . . S ? 1} do
8: Decode i-th sentence with ud,t,i,0
9: P = No. of pairs built from the N-best list
10: for all pairs v[j], j ? {0 . . . P ? 1} do
11: ud,t,i,j+1 ? ud,t,i,j ? ??L(ud,t,i,j)
12: end for
13: ud,t,i+1,0 ? ud,t,i,P
14: end for
15: end for
16: for all domains d ? {1 . . . N} do
17: wd = ud,t,S,0
18: end for
19: WG ? [wG1 | . . . |w
G
N ]
T
20: for all domains d ? {1 . . . N} do
21: for k ? 1 . . . |wGd | do
22: wGd [k] =
1
N
?N
n=1 W
G[n][k]
23: end for
24: wd ?
[wId
wGd
]
25: end for
26: end for
27: return w1,w2, . . . ,wN
Our modified algorithm is illustrated in Algorithm
1. Each column vector wi is further split into two
parts wIi and w
G
i , representing the In-domain and
General-domain feature weights respectively. In Al-
gorithm 1, we first distribute the domain-specific
SMT decoders to different machines and initialize
the feature weights (line 1-2). Typically, the SGD al-
gorithm runs in several iterations (In this study, we
set the number of epochs T to 20) (line 3). Multi-
ple SMT decoders run in parallel and each decoder
updates its feature weights individually using its in-
domain development data (line 4-15). For each do-
main, the domain-specific decoder translates each
in-domain development sentence and determines the
N-best translations (line 4-8). The preference pairs
are built and used to update the parameters by gra-
dient descent with ? = 0.0001 (line 9-13). Each
domain-specific decoder translates its in-domain de-
velopment data multiple times. After each itera-
tion, feature weights from all decoders are collected
(line 16-19). In contrast to the original algorithm
(Simianer et al, 2012), we only average the general-
domain feature weights wG1 , . . . ,w
G
N , but do not av-
erage the in-domain feature weights (line 20-25).
The reason is we hope to leverage the commonalities
among these systems. Meanwhile, general knowl-
edge is enforced to be conveyed equally across dif-
ferent domains. Finally, the algorithm returns all
the domain-specific feature weights w1,w2, . . . ,wN
that are used for testing (line 27).
After the joint MTL-based tuning, the feature
weights tailored for domain-specific SMT systems
are used to translate the testing data. We collect in-
domain testing data for each domain to evaluate the
domain-specific systems. Although this is not al-
ways the case in real applications where the testing
domain is known, this study mainly focuses on the
effectiveness of the MTL-based tuning approach.
3 Experiments
3.1 Data
We evaluated our MTL-based domain adaptation
approach on a large-scale English-to-Chinese ma-
chine translation task. The training data consisted
of two parts: monolingual data and bilingual data.
The monolingual data was used to train the source-
side and target-side LMs, both of which were used
for data selection in Section 2.1. In addition, the
target-side LMs were re-used in the SMT systems
as features. As mentioned in Section 2.1, we built a
web crawler to collect a large number of webpages
from web portal sites in English and Chinese respec-
tively. In the experiments, we mainly focused on six
popular domains, namely Business, Entertainment,
Health, Science & Technology, Sports, and Politics.
For both English and Chinese webpages, the HTML
tags were removed and the main content was ex-
tracted. The data statistics are shown in Table 1.
The bilingual data we used was mainly mined
from the web using the method proposed by Jiang
et al (2009), with a post-processing step using our
bilingual data cleaning method (Cui et al, 2013).
Therefore, the data quality is pretty good. In addi-
tion, we also used the English-Chinese parallel cor-
pus released by LDC2. In total, the bilingual data
2LDC2003E07, LDC2003E14, LDC2004E12,
LDC2005T06, LDC2005T10, LDC2005E83, LDC2006E26,
1059
Domain
English Chinese
Docs Words Docs Words
Business 21M 10.4B 7.91M 2.73B
Ent. 18.3M 8.29B 4.16M 1.31B
Health 8.7M 4.73B 0.9M 0.42B
Sci&Tech 10.9M 5.33B 5.28M 1.6B
Sports 18.9M 9.58B 2.49M 0.59B
Politics 10.3M 5.56B 1.67M 0.39B
Table 1: Statistics of web-crawled monolingual data, in
numbers of documents and words (main content). ?M?
refers to million and ?B? refers to billion.
contained around 30 million sentence pairs, with
404M words in English and 329M words in Chi-
nese. For each domain, we used the cross-entropy
based method in Section 2.1 to rank the entire bilin-
gual data, and the top 10% sentence pairs from the
ranked bilingual data were selected as the in-domain
data to train the in-domain TM. Moreover, we pre-
pared 2,000 in-domain sentences for development
and 1,000 in-domain sentences for testing in each
domain. The details are shown in Table 2.
Domain
Train Dev Test
En Ch En Ch En Ch
Business 30M 28M 36K 35K 19K 19K
Ent. 25M 22M 21K 18K 13K 12K
Health 23M 20M 33K 33K 21K 22K
Sci&Tech 28M 26M 46K 45K 27K 27K
Sports 19M 16M 18K 14K 10K 9K
Politics 28M 24M 19K 17K 13K 12K
Table 2: Statistics of in-domain training, development
and testing data, in number of words.
3.2 Setup
An in-house hierarchical phrase-based SMT de-
coder was implemented for our experiments. The
CKY decoding algorithm was used and cube prun-
ing was performed with the same default parameter
settings as in Chiang (2007). We used a 100-best
list from the decoder for the pairwise ranking al-
gorithm. Translation models were trained over the
bilingual data that was automatically word-aligned
using GIZA++ (Och and Ney, 2003) in both direc-
tions, and the diag-grow-final heuristic was used to
LDC2006E34, LDC2006E85, LDC2006E92.
refine the symmetric word alignment. The phrase
tables were filtered to retain top-20 translation can-
didates for each source phrase for efficiency. An
in-house language modeling toolkit was used to
train the 4-gram language models with modified
Kneser-Ney smoothing (Kneser and Ney, 1995) over
the web-crawled data. The evaluation metric for
the overall translation quality was case-insensitive
BLEU4 (Papineni et al, 2002). A statistical sig-
nificance test was performed using the bootstrap re-
sampling method (Koehn, 2004).
3.3 Baseline
We have two baselines. The first baseline is a non-
adapted Hiero using our implementation. It con-
tained the general-domain TM and LM, as well as
other standard features. In addition, the fix-discount
method (Foster et al, 2006) for phrase table smooth-
ing was also used. The system was general-domain
oriented and it was tuned by using MERT (Och,
2003) with a combination of six in-domain develop-
ment datasets. The second baseline is Google Online
Translation Service3. We obtained the English-to-
Chinese translations of the testing data from Google
Translation to have a more solid comparison.
Moreover, we also compared our method with the
adapted systems towards each domain individually
(Koehn and Schroeder, 2007). This is to demon-
strate the superiority of our MTL-based tuning ap-
proach across different domains.
3.4 Results
The end-to-end translation performance is shown in
Table 3. We found that the baseline has a similar
performance to Google Translation, with certain do-
mains performed even better (Business, Sci&Tech,
Sports, Politics). This demonstrates that the transla-
tion quality of our baseline is state-of-the-art. More-
over, we can answer three questions according to the
experimental results as follow:
First, is domain mismatch a significant prob-
lem for a real world SMT system? We used the
same system only with general-domain TM and LM,
but tuned towards each domain individually using
in-domain dev data. Table 3 shows that the setting
?[A] G-TM + G-LM? performs much better than
3http://translate.google.com
1060
Business Ent. Health Sci&Tech Sports Politics
[N] Baseline (G-TM + G-LM) 27.19 17.87 25.79 25.34 25.53 23.01
Google Translation 26.01 18.44 27.71 25.07 24.08 22.97
[A] G-TM + G-LM 29.58 19.08 28.80 26.84 30.28 25.64
[A] I-TM + I-LM 28.20 17.25 27.20 25.41 30.12 22.97
[A] (G+I)-TM + G-LM 29.45 19.22 28.93 27.01 31.01 25.40
[A] (G+I)-TM + I-LM 29.60 19.43 28.94 27.05 34.36 25.98
[A] (G+I)-LM + G-TM 29.66 19.50 29.00 27.10 33.60 26.03
[A] (G+I)-LM + I-TM 28.50 17.66 27.58 25.99 30.44 23.30
[A] (G+I)-TM + (G+I)-LM 29.82 19.53 29.03 26.94 33.77 26.09
[A,MTL] (G+I)-TM + (G+I)-LM 30.26 19.94 29.08 27.17 34.11 26.50
Table 3: End-to-end experimental results (BLEU4%) with large-scale training data (p < 0.05). ?[N]? means the system
is non-adapted and tuned using MERT on general-domain dev data. ?[A]? denotes that the system is adapted towards
each domain individually using MERT on in-domain dev data. ?[A,MTL]? indicates that the system was tuned using
our MTL-based approach on in-domain dev data. ?I-TM? and ?G-TM? denote the in-domain and general-domain
translation model. ?I-LM? and ?G-LM? denote the in-domain and general-domain language model. We also obtained
translations of the testing data using Google Translation for comparison.
the non-adapted baseline across all domains with at
least 1.2 BLEU points. In addition, the setting ?[A]
G-TM + G-LM? also outperforms Google Transla-
tion on all domains. Analogous to previous research,
this confirms that the domain mismatch indeed ex-
ists and the parameter estimation using in-domain
dev data is quite useful.
Second, does the mixture models based adap-
tation work for a variety of domains? We experi-
mented with different settings with multiple TMs or
LMs, or both. It is interesting to note that for large-
scale SMT systems, using in-domain models alone
is inferior to using the general models alone. The
setting ?[A] G-TM + G-LM? is better than the set-
ting ?[A] I-TM + I-LM? across different domains.
The reason is the data for general models has already
included the in-domain data and the data coverage is
much larger, thus the probability estimation is more
reliable and the translation quality is much better.
For the LM, the in-domain LM performs better
than the general-domain LM because our mono-
lingual data (Table 1) for each domain is already
sufficient for training an in-domain LM with good
performance. From Table 3, we observed that the
setting ?[A] (G+I)-TM + I-LM? outperforms ?[A]
(G+I)-TM + G-LM?, with the ?Sports? domain be-
ing the most significant. For the TM, the per-
formance of the in-domain TM is inferior to the
general-domain TM. The results show that the set-
ting ?[A] (G+I)-LM + G-TM? is significantly better
than ?[A] (G+I)-LM + I-TM?. The main reason is
the data coverage for in-domain TM is much smaller
than the general model. When each system uses two
TMs and two LMs, it consistently results in better
performance, indicating that mixture models are cru-
cial for domain adaptation in SMT.
Third, can MTL further improve the transla-
tion quality? We used the MTL-based approach to
jointly tune multiple domain-specific systems, lever-
aging the commonalities among different but related
tasks. From Table 3, the MTL-based approach sig-
nificantly improve the translation quality over the
non-adapted baseline, and also outperforms conven-
tional mixture models based methods. In particular,
the ?Sports? domain benefits the most from the in-
domain knowledge, which confirms that domain dis-
crepancy should be addressed and may bring large
improvements on certain domains.
3.5 Discussion
According to our experiments, only averaging over
the out-of-domain feature weights returned robust
and converged results. We do not have theoreti-
cally grounded guarantee. However, we observed
that the BLEU score of our method on DEV data
was slightly lower than that in the baseline system,
which indicates the out-of-domain features are less
over-fitting on the domain-specific DEV data since
1061
SOURSE A point begins with a player serving the ball. This means one player hits
the ball towards the other player. (The serve must be played from behind the
baseline and must
::::
land in the service box . Players get two attempts to make
a good serve.)
REF ? ? ? ? ???? ? ? ? ? ? ? ? ? ???? ? ? ????
??(??????????????????
::::
????? ??? ??
????????????)
[N] Baseline (G-TM + G-LM) ????????????????????????????
(???????????????
:::::::
??? ??? ????????
????????)
[A] (G+I)-TM + (G+I)-LM ???????????????? ????????(???
???????? ??? ?
:::::
????????????????
????)
[A,MTL](G+I)-TM + (G+I)-LM ????????????? ???????????(????
??????????
:::::::
??? ??? ????????????
????)
Table 4: Examples illustrating some different translations, where the Chinese phrases are translated from the English
phrases with the same symbols (e.g., underline, wavy-line, and box). The details are explained in Section 3.5.
we enforced them to play the same role across dif-
ferent domains. It seems that averaging the out-of-
domain feature weights can be considered as a kind
of regularization.
An example sentence from the Sports domain
with translations from different methods is shown
in Table 4. In this sentence, the baseline always
translates ?player? to ???? (game player), which
should be ???? (ball player). And, the base-
line translates ?serve? to ???? (work for), which
should be ???? (put the ball into play). The phrase
?service box? here means ?????, which denotes
the zone where the ball is to be served. However, the
baseline incorrectly splits them into two words, then
translates ?service? to ???? and ?box? to ???.
In contrast, the approaches with adapted models are
able to translate these words very well.
Both our MTL-based approach and the conven-
tional adaptation methods leverage the mixture mod-
els. A natural question is why our MTL-based ap-
proach performs better than the individual adapta-
tion. To answer this question, we looked into the
details of the tuning and decoding procedures in the
MTL-based approach. We observed that the BLEU
score on the development data for each system was
lower than the score when conducting individual
adaptation. Considering that the algorithm enforc-
ing the general features play the same role across
different domains, we suspect that MTL-based ap-
proach introduces a kind of regularization for each
domain-specific system. The regularization prevents
the general features from biasing towards certain do-
mains to the extreme. This property is quite impor-
tant for real world SMT systems. Usually, a sen-
tence is composed of some domain-specific words
and some general words, so it is often improper to
translate every word in the sentence using the in-
domain knowledge. For the example in Table 4,
the individual adaptation method ?[A] (G+I)-TM +
(G+I)-LM? translates ?land? to ???? (zone) im-
properly, because ???? appears more often in the
Sports text than the general-domain text. This shows
that the individual adaptation methods tend to over-
fit the in-domain development data. In contrast, the
MTL-based approach ?[A,MTL](G+I)-TM + (G+I)-
LM? just translates ?land? to ????? (fall on),
which is more appropriate.
4 Related Work
4.1 Domain Adaptation
One direction of domain adaptation explored the
data selection and weighting approach to improve
the performance of SMT on specific domains. Eck
1062
et al (2004) first decoded the testing data with a
general TM, and then used the translation results
to train an adapted LM, which was in turn used to
re-decode the testing data. Lu? et al (2007) tried
to weight the training data according to the similar-
ity with test data using information retrieval mod-
els, while Foster et al (2010) trained a discrimina-
tive model to estimate a weight for each sentence
in the training corpus. Other methods conducted
data selection based on cross-entropy (Moore and
Lewis, 2010), and Axelrod et al (2011) further ex-
tended their cross-entropy based method to the se-
lection of bilingual corpus in the hope that more rel-
evant corpus to the target domain could yield smaller
models with better performance. Other methods
included using semi-supervised transductive learn-
ing techniques to exploit the monolingual in-domain
data (Ueffing et al, 2007).
Adaptation methods also involved the utiliza-
tion of mixture models. Foster and Kuhn (2007)
explored a number of variants of utilizing multi-
ple TMs and LMs by interpolation. Koehn and
Schroeder (2007) used MERT to simultaneously
tune two TMs or LMs. Sennrich (2012) investi-
gated the TM perplexity minimization as a method
to set model weights in mixture modeling. In ad-
dition, inspired by system combination approaches,
Razmara et al (2012) used the ensemble decoding
method to mix multiple translation models, which
outperformed a variety of strong baselines.
Generally, most previous methods merely con-
ducted domain adaption for a single domain, rather
than multiple domains at the same time. One could
also simply build multiple SMT systems that were
adapted to multiple domains, but they were often
separated and not tuned together. So far, there has
been little research into the multi-domain adaptation
problem over mixture models for SMT systems, as
proposed in this paper.
4.2 Multi-task Learning
In machine learning, MTL is an approach to learn
one target problem with other related problems at
the same time. This often leads to a better model for
the main task because it allows the learner to use the
commonality among the tasks. MTL is performed
by learning tasks in parallel while using a shared
representation. Therefore, what is learned for each
task can help other tasks be learned better.
MTL was successfully applied in some Natu-
ral Language Processing (NLP) tasks. For exam-
ple, Blitzer et al (2006) extended the MTL ap-
proach (Ando and Zhang, 2005) to domain adapta-
tion tasks in part-of-speech tagging. Collobert and
Weston (2008) proposed using deep neural networks
to train a set of tasks, including part-of-speech tag-
ging, chunking, named entity recognition, and se-
mantic roles labeling. They reported that jointly
learning these tasks led to superior performance.
MTL was also applied in sentiment analysis (Dredze
and Crammer, 2008) and web ranking (Chapelle
et al, 2011) to address the multi-domain learning
and adaptation. In SMT, Duh et al (2010) pro-
posed using MTL for N-best re-ranking on sparse
feature sets, where each N-best list corresponded to
a distinct task. Simianer et al (2012) proposed dis-
tributed stochastic learning with feature selection in-
spired by MTL. The distributed learning approach
outperformed several other training methods includ-
ing MIRA and SGD.
Inspired by these methods, we used MTL to tune
multiple SMT systems at the same time, where each
system was composed of in-domain and general-
domain models. Through a shared feature represen-
tation, the commonalities among the SMT systems
were better learned by the general models. In ad-
dition, domain-specific translation knowledge was
also better characterized by the in-domain models.
5 Conclusion and Future Work
In this paper, we propose an MTL-based approach to
address multi-domain adaptation for SMT. We first
use the cross-entropy based data selection method
to obtain in-domain bilingual data. After that, in-
domain TMs and LMs are trained for each domain-
specific SMT system. In addition, the general-
domain TM and LM are also trained and shared
across different systems. Finally, MTL is lever-
aged to tune multiple systems jointly. Experimen-
tal results have shown that our approach is quite
promising for the multi-domain adaptation problem,
and it brings significant improvement over both the
non-adapted baselines and the conventional domain
adaptation methods with mixture models.
We assume the domain information for testing
1063
data is known beforehand in this study. However,
this is not always the case for real world SMT sys-
tems. Therefore, to apply our approach in real appli-
cations, the domain information needs to be identi-
fied automatically. In the future, we will pre-define
more popular domains and develop automatic do-
main classifiers. For those domains that are iden-
tified with high confidence, we use the domain-
specific system to translate the texts. For other texts,
we use the general system to translate them. Fur-
thermore, since our approach is a general training
method, we may also combine this approach with
other domain adaptation methods to get more per-
formance improvement.
Acknowledgments
We are especially grateful to Nan Yang, Yajuan
Duan, Hong Sun and Danran Chen for the helpful
discussions. We also thank the anonymous review-
ers for their insightful comments.
References
Rie Kubota Ando and Tong Zhang. 2005. A framework
for learning predictive structures from multiple tasks
and unlabeled data. The Journal of Machine Learning
Research, 6:1817?1853.
Amittai Axelrod, Xiaodong He, and Jianfeng Gao. 2011.
Domain adaptation via pseudo in-domain data selec-
tion. In Proceedings of the 2011 Conference on
Empirical Methods in Natural Language Processing,
pages 355?362, Edinburgh, Scotland, UK., July. As-
sociation for Computational Linguistics.
John Blitzer, Ryan McDonald, and Fernando Pereira.
2006. Domain adaptation with structural correspon-
dence learning. In Proceedings of the 2006 Confer-
ence on Empirical Methods in Natural Language Pro-
cessing, pages 120?128, Sydney, Australia, July. As-
sociation for Computational Linguistics.
Olivier Chapelle, Pannagadatta Shivaswamy, Srinivas
Vadrevu, Kilian Weinberger, Ya Zhang, and Belle
Tseng. 2011. Boosted multi-task learning. Machine
learning, 85(1-2):149?173.
David Chiang. 2007. Hierarchical phrase-based transla-
tion. Computational Linguistics, 33(2):201?228.
Ronan Collobert and Jason Weston. 2008. A unified ar-
chitecture for natural language processing: deep neu-
ral networks with multitask learning. In Proceedings
of the 25th international conference onMachine learn-
ing, pages 160?167. ACM.
Lei Cui, Dongdong Zhang, Shujie Liu, Mu Li, and Ming
Zhou. 2013. Bilingual data cleaning for smt using
graph-based random walk. In Proceedings of the 51st
Annual Meeting of the Association for Computational
Linguistics (Volume 2: Short Papers), pages 340?345,
Sofia, Bulgaria, August. Association for Computa-
tional Linguistics.
Mark Dredze and Koby Crammer. 2008. Online methods
for multi-domain learning and adaptation. In Proceed-
ings of the 2008 Conference on Empirical Methods in
Natural Language Processing, pages 689?697, Hon-
olulu, Hawaii, October. Association for Computational
Linguistics.
Kevin Duh, Katsuhito Sudoh, Hajime Tsukada, Hideki
Isozaki, and Masaaki Nagata. 2010. N-best reranking
by multitask learning. In Proceedings of the Joint Fifth
Workshop on Statistical Machine Translation and Met-
ricsMATR, pages 375?383, Uppsala, Sweden, July.
Association for Computational Linguistics.
Matthias Eck, Stephan Vogel, and Alex Waibel. 2004.
Language model adaptation for statistical machine
translation based on information retrieval. In In Proc.
of LREC.
George Foster and Roland Kuhn. 2007. Mixture-model
adaptation for SMT. In Proceedings of the Second
Workshop on Statistical Machine Translation, pages
128?135, Prague, Czech Republic, June. Association
for Computational Linguistics.
George Foster, Roland Kuhn, and Howard Johnson.
2006. Phrasetable smoothing for statistical machine
translation. In Proceedings of the 2006 Conference on
Empirical Methods in Natural Language Processing,
pages 53?61, Sydney, Australia, July. Association for
Computational Linguistics.
George Foster, Cyril Goutte, and Roland Kuhn. 2010.
Discriminative instance weighting for domain adapta-
tion in statistical machine translation. In Proceedings
of the 2010 Conference on Empirical Methods in Natu-
ral Language Processing, pages 451?459, Cambridge,
MA, October. Association for Computational Linguis-
tics.
Long Jiang, Shiquan Yang, Ming Zhou, Xiaohua Liu, and
Qingsheng Zhu. 2009. Mining bilingual data from the
web with adaptively learnt patterns. In Proceedings
of the Joint Conference of the 47th Annual Meeting
of the ACL and the 4th International Joint Conference
on Natural Language Processing of the AFNLP, pages
870?878, Suntec, Singapore, August. Association for
Computational Linguistics.
Reinhard Kneser and Hermann Ney. 1995. Improved
backing-off for m-gram language modeling. In Acous-
tics, Speech, and Signal Processing, 1995. ICASSP-
95., 1995 International Conference on, volume 1,
pages 181?184. IEEE.
1064
Philipp Koehn and Josh Schroeder. 2007. Experiments
in domain adaptation for statistical machine transla-
tion. In Proceedings of the Second Workshop on Sta-
tistical Machine Translation, pages 224?227, Prague,
Czech Republic, June. Association for Computational
Linguistics.
Philipp Koehn. 2004. Statistical significance tests for
machine translation evaluation. In Dekang Lin and
Dekai Wu, editors, Proceedings of EMNLP 2004,
pages 388?395, Barcelona, Spain, July. Association
for Computational Linguistics.
Percy Liang, Alexandre Bouchard-Co?te?, Dan Klein, and
Ben Taskar. 2006. An end-to-end discriminative ap-
proach to machine translation. In Proceedings of the
21st International Conference on Computational Lin-
guistics and 44th Annual Meeting of the Association
for Computational Linguistics, pages 761?768, Syd-
ney, Australia, July. Association for Computational
Linguistics.
Yajuan Lu?, Jin Huang, and Qun Liu. 2007. Improving
statistical machine translation performance by train-
ing data selection and optimization. In Proceedings
of the 2007 Joint Conference on Empirical Methods
in Natural Language Processing and Computational
Natural Language Learning (EMNLP-CoNLL), pages
343?350, Prague, Czech Republic, June. Association
for Computational Linguistics.
Robert C. Moore and William Lewis. 2010. Intelligent
selection of language model training data. In Pro-
ceedings of the ACL 2010 Conference Short Papers,
pages 220?224, Uppsala, Sweden, July. Association
for Computational Linguistics.
Franz Josef Och and Hermann Ney. 2003. A system-
atic comparison of various statistical alignment mod-
els. Computational Linguistics, 29(1):19?51.
Franz Josef Och. 2003. Minimum error rate training in
statistical machine translation. In Proceedings of the
41st Annual Meeting of the Association for Compu-
tational Linguistics, pages 160?167, Sapporo, Japan,
July. Association for Computational Linguistics.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: a method for automatic eval-
uation of machine translation. In Proceedings of 40th
Annual Meeting of the Association for Computational
Linguistics, pages 311?318, Philadelphia, Pennsylva-
nia, USA, July. Association for Computational Lin-
guistics.
Majid Razmara, George Foster, Baskaran Sankaran, and
Anoop Sarkar. 2012. Mixing multiple translation
models in statistical machine translation. In Proceed-
ings of the 50th Annual Meeting of the Association for
Computational Linguistics (Volume 1: Long Papers),
pages 940?949, Jeju Island, Korea, July. Association
for Computational Linguistics.
Rico Sennrich. 2012. Perplexity minimization for trans-
lation model domain adaptation in statistical machine
translation. In Proceedings of the 13th Conference of
the European Chapter of the Association for Compu-
tational Linguistics, pages 539?549, Avignon, France,
April. Association for Computational Linguistics.
Libin Shen and Aravind K Joshi. 2005. Ranking and
reranking with perceptron. Machine Learning, 60(1-
3):73?96.
Patrick Simianer, Stefan Riezler, and Chris Dyer. 2012.
Joint feature selection in distributed stochastic learn-
ing for large-scale discriminative training in smt. In
Proceedings of the 50th Annual Meeting of the Associ-
ation for Computational Linguistics (Volume 1: Long
Papers), pages 11?21, Jeju Island, Korea, July. Asso-
ciation for Computational Linguistics.
Nicola Ueffing, Gholamreza Haffari, and Anoop Sarkar.
2007. Transductive learning for statistical machine
translation. In Proceedings of the 45th Annual Meet-
ing of the Association of Computational Linguistics,
pages 25?32, Prague, Czech Republic, June. Associa-
tion for Computational Linguistics.
1065
Proceedings of the ACL 2010 Conference Short Papers, pages 6?11,
Uppsala, Sweden, 11-16 July 2010. c?2010 Association for Computational Linguistics
A Joint Rule Selection Model for Hierarchical Phrase-based Translation?
Lei Cui?, Dongdong Zhang?, Mu Li?, Ming Zhou?, and Tiejun Zhao?
?School of Computer Science and Technology
Harbin Institute of Technology, Harbin, China
{cuilei,tjzhao}@mtlab.hit.edu.cn
?Microsoft Research Asia, Beijing, China
{dozhang,muli,mingzhou}@microsoft.com
Abstract
In hierarchical phrase-based SMT sys-
tems, statistical models are integrated to
guide the hierarchical rule selection for
better translation performance. Previous
work mainly focused on the selection of
either the source side of a hierarchical rule
or the target side of a hierarchical rule
rather than considering both of them si-
multaneously. This paper presents a joint
model to predict the selection of hierar-
chical rules. The proposed model is esti-
mated based on four sub-models where the
rich context knowledge from both source
and target sides is leveraged. Our method
can be easily incorporated into the prac-
tical SMT systems with the log-linear
model framework. The experimental re-
sults show that our method can yield sig-
nificant improvements in performance.
1 Introduction
Hierarchical phrase-based model has strong ex-
pression capabilities of translation knowledge. It
can not only maintain the strength of phrase trans-
lation in traditional phrase-based models (Koehn
et al, 2003; Xiong et al, 2006), but also char-
acterize the complicated long distance reordering
similar to syntactic based statistical machine trans-
lation (SMT) models (Yamada and Knight, 2001;
Quirk et al, 2005; Galley et al, 2006; Liu et al,
2006; Marcu et al, 2006; Mi et al, 2008; Shen et
al., 2008).
In hierarchical phrase-based SMT systems, due
to the flexibility of rule matching, a huge number
of hierarchical rules could be automatically learnt
from bilingual training corpus (Chiang, 2005).
SMT decoders are forced to face the challenge of
?This work was finished while the first author visited Mi-
crosoft Research Asia as an intern.
proper rule selection for hypothesis generation, in-
cluding both source-side rule selection and target-
side rule selection where the source-side rule de-
termines what part of source words to be translated
and the target-side rule provides one of the candi-
date translations of the source-side rule. Improper
rule selections may result in poor translations.
There is some related work about the hierarchi-
cal rule selection. In the original work (Chiang,
2005), the target-side rule selection is analogous to
the model in traditional phrase-based SMT system
such as Pharaoh (Koehn et al, 2003). Extending
this work, (He et al, 2008; Liu et al, 2008) in-
tegrate rich context information of non-terminals
to predict the target-side rule selection. Different
from the above work where the probability dis-
tribution of source-side rule selection is uniform,
(Setiawan et al, 2009) proposes to select source-
side rules based on the captured function words
which often play an important role in word re-
ordering. There is also some work considering to
involve more rich contexts to guide the source-side
rule selection. (Marton and Resnik, 2008; Xiong
et al, 2009) explore the source syntactic informa-
tion to reward exact matching structure rules or
punish crossing structure rules.
All the previous work mainly focused on either
source-side rule selection task or target-side rule
selection task rather than both of them together.
The separation of these two tasks, however, weak-
ens the high interrelation between them. In this pa-
per, we propose to integrate both source-side and
target-side rule selection in a unified model. The
intuition is that the joint selection of source-side
and target-side rules is more reliable as it conducts
the search in a larger space than the single selec-
tion task does. It is expected that these two kinds
of selection can help and affect each other, which
may potentially lead to better hierarchical rule se-
lections with a relative global optimum instead of
a local optimum that might be reached in the pre-
6
vious methods. Our proposed joint probability
model is factored into four sub-models that can
be further classified into source-side and target-
side rule selection models or context-based and
context-free selection models. The context-based
models explore rich context features from both
source and target sides, including function words,
part-of-speech (POS) tags, syntactic structure in-
formation and so on. Our model can be easily in-
corporated as an independent feature into the prac-
tical hierarchical phrase-based systems with the
log-linear model framework. The experimental re-
sults indicate our method can improve the system
performance significantly.
2 Hierarchical Rule Selection Model
Following (Chiang, 2005), ??, ?? is used to repre-
sent a synchronous context free grammar (SCFG)
rule extracted from the training corpus, where ?
and ? are the source-side and target-side rule re-
spectively. Let C be the context of ??, ??. For-
mally, our joint probability model of hierarchical
rule selection is described as follows:
P (?, ?|C) = P (?|C)P (?|?,C) (1)
We decompose the joint probability model into
two sub-models based on the Bayes formulation,
where the first sub-model is source-side rule se-
lection model and the second one is the target-side
rule selection model.
For the source-side rule selection model, we fur-
ther compute it by the interpolation of two sub-
models:
?Ps(?) + (1? ?)Ps(?|C) (2)
where Ps(?) is the context-free source model
(CFSM) and Ps(?|C) is the context-based source
model (CBSM), ? is the interpolation weight that
can be optimized over the development data.
CFSM is the probability of source-side rule se-
lection that can be estimated based on maximum
likelihood estimation (MLE) method:
Ps(?) =
?
? Count(??, ??)
Count(?)
(3)
where the numerator is the total count of bilin-
gual rule pairs with the same source-side rule that
are extracted based on the extraction algorithm in
(Chiang, 2005), and the denominator is the total
amount of source-side rule patterns contained in
the monolingual source side of the training corpus.
CFSM is used to capture how likely the source-
side rule is linguistically motivated or has the cor-
responding target-side counterpart.
For CBSM, it can be naturally viewed as a clas-
sification problem where each distinct source-side
rule is a single class. However, considering the
huge number of classes may cause serious data
sparseness problem and thereby degrade the clas-
sification accuracy, we approximate CBSM by a
binary classification problem which can be solved
by the maximum entropy (ME) approach (Berger
et al, 1996) as follows:
Ps(?|C) ? Ps(?|?,C)
=
exp[
?
i ?ihi(?, ?,C)]?
?? exp[
?
i ?ihi(?
? , ?, C)]
(4)
where ? ? {0, 1} is the indicator whether the
source-side rule is applied during decoding, ? = 1
when the source-side rule is applied, otherwise
? = 0; hi is a feature function, ?i is the weight
of hi. CBSM estimates the probability of the
source-side rule being selected according to the
rich context information coming from the surface
strings and sub-phrases that will be reduced to
non-terminals during decoding.
Analogously, we decompose the target-side rule
selection model by the interpolation approach as
well:
?Pt(?) + (1? ?)Pt(?|?,C) (5)
where Pt(?) is the context-free target model
(CFTM) and Pt(?|?,C) is the context-based tar-
get model (CBTM), ? is the interpolation weight
that can be optimized over the development data.
In the similar way, we compute CFTM by the
MLE approach and estimate CBTM by the ME
approach. CFTM computes how likely the target-
side rule is linguistically motivated, while CBTM
predicts how likely the target-side rule is applied
according to the clues from the rich context infor-
mation.
3 Model Training of CBSM and CBTM
3.1 The acquisition of training instances
CBSM and CBTM are trained by ME approach for
the binary classification, where a training instance
consists of a label and the context related to SCFG
rules. The context is divided into source context
7
Figure 1: Example of training instances in CBSM and CBTM.
and target context. CBSM is trained only based
on the source context while CBTM is trained over
both the source and the target context. All the
training instances are automatically constructed
from the bilingual training corpus, which have la-
bels of either positive (i.e., ? = 1) or negative (i.e.,
? = 0). This section explains how the training in-
stances are constructed for the training of CBSM
and CBTM.
Let s and t be the source sentence and target
sentence,W be the word alignment between them,
rs be a source-side rule that pattern-matches a
sub-phrase of s, rt be the target-side rule pattern-
matching a sub-phrase of t and being aligned to rs
based on W , and C(r) be the context features re-
lated to the rule r which will be explained in the
following section.
For the training of CBSM, if the SCFG rule
?rs, rt? can be extracted based on the rule extrac-
tion algorithm in (Chiang, 2005), ?? = 1, C(rs)?
is constructed as a positive instance, otherwise
?? = 0, C(rs)? is constructed as a negative in-
stance. For example in Figure 1(a), the context of
source-side rule ?X1 hezuo? that pattern-matches
the phrase ?youhao hezuo? produces a positive
instance, while the context of ?X1 youhao? that
pattern-matches the source phrase ?de youhao? or
?shuangfang de youhao? will produce a negative
instance as there are no corresponding plausible
target-side rules that can be extracted legally1.
For the training of CBTM, given rs, suppose
there is a SCFG rule set {?rs, rkt ?|1 ? k ? n}
extracted from multiple distinct sentence pairs in
the bilingual training corpus, among which we as-
sume ?rs, rit? is extracted from the sentence pair
?s, t?. Then, we construct ?? = 1, C(rs), C(rit)?
1Because the aligned target words are not contiguous and
?cooperation? is aligned to the word outside the source-side
rule.
as a positive instance, while the elements in {?? =
0, C(rs), C(r
j
t )?|j 6= i ? 1 ? j ? n} are viewed
as negative instances since they fail to be applied
to the translation from s to t. For example in Fig-
ure 1(c), Rule (1) and Rule (2) are two different
SCFG rules extracted from Figure 1(a) and Figure
1(b) respectively, where their source-side rules are
the same. As Rule (1) cannot be applied to Fig-
ure 1(b) for the translation and Rule (2) cannot
be applied to Figure 1(a) for the translation either,
?? = 1, C(ras ), C(r
a
t )? and ?? = 1, C(r
b
s), C(r
b
t )?
are constructed as positive instances while ?? =
0, C(ras ), C(r
b
t )? and ?? = 0, C(r
b
s), C(r
a
t )? are
viewed as negative instances. It is noticed that
this instance construction method may lead to a
large quantity of negative instances and choke the
training procedure. In practice, to limit the size
of the training set, the negative instances con-
structed based on low-frequency target-side rules
are pruned.
3.2 Context-based features for ME training
ME approach has the merit of easily combining
different features to predict the probability of each
class. We incorporate into the ME based model
the following informative context-based features
to train CBSM and CBTM. These features are
carefully designed to reduce the data sparseness
problem and some of them are inspired by pre-
vious work (He et al, 2008; Gimpel and Smith,
2008; Marton and Resnik, 2008; Chiang et al,
2009; Setiawan et al, 2009; Shen et al, 2009;
Xiong et al, 2009):
1. Function word features, which indicate
whether the hierarchical source-side/target-
side rule strings and sub-phrases covered by
non-terminals contain function words that are
often important clues of predicting syntactic
structures.
8
2. POS features, which are POS tags of the
boundary source words covered by non-
terminals.
3. Syntactic features, which are the constituent
constraints of hierarchical source-side rules
exactly matching or crossing syntactic sub-
trees.
4. Rule format features, which are non-
terminal positions and orders in source-
side/target-side rules. This feature interacts
between source and target components since
it shows whether the translation ordering is
affected.
5. Length features, which are the length
of sub-phrases covered by source non-
terminals.
4 Experiments
4.1 Experiment setting
We implement a hierarchical phrase-based system
similar to the Hiero (Chiang, 2005) and evaluate
our method on the Chinese-to-English translation
task. Our bilingual training data comes from FBIS
corpus, which consists of around 160K sentence
pairs where the source data is parsed by the Berke-
ley parser (Petrov and Klein, 2007). The ME train-
ing toolkit, developed by (Zhang, 2006), is used to
train our CBSM and CBTM. The training size of
constructed positive instances for both CBSM and
CBTM is 4.68M, while the training size of con-
structed negative instances is 3.74M and 3.03M re-
spectively. Following (Setiawan et al, 2009), we
identify function words as the 128 most frequent
words in the corpus. The interpolation weights are
set to ? = 0.75 and ? = 0.70. The 5-gram lan-
guage model is trained over the English portion
of FBIS corpus plus Xinhua portion of the Giga-
word corpus. The development data is from NIST
2005 evaluation data and the test data is from
NIST 2006 and NIST 2008 evaluation data. The
evaluation metric is the case-insensitive BLEU4
(Papineni et al, 2002). Statistical significance in
BLEU score differences is tested by paired boot-
strap re-sampling (Koehn, 2004).
4.2 Comparison with related work
Our baseline is the implemented Hiero-like SMT
system where only the standard features are em-
ployed and the performance is state-of-the-art.
We compare our method with the baseline and
some typical approaches listed in Table 1 where
XP+ denotes the approach in (Marton and Resnik,
2008) and TOFW (topological ordering of func-
tion words) stands for the method in (Setiawan et
al., 2009). As (Xiong et al, 2009)?s work is based
on phrasal SMT system with bracketing transduc-
tion grammar rules (Wu, 1997) and (Shen et al,
2009)?s work is based on the string-to-dependency
SMT model, we do not implement these two re-
lated work due to their different models from ours.
We also do not compare with (He et al, 2008)?s
work due to its less practicability of integrating
numerous sub-models.
Methods NIST 2006 NIST 2008
Baseline 0.3025 0.2200
XP+ 0.3061 0.2254
TOFW 0.3089 0.2253
Our method 0.3141 0.2318
Table 1: Comparison results, our method is signif-
icantly better than the baseline, as well as the other
two approaches (p < 0.01)
As shown in Table 1, all the methods outper-
form the baseline because they have extra mod-
els to guide the hierarchical rule selection in some
ways which might lead to better translation. Ap-
parently, our method also performs better than the
other two approaches, indicating that our method
is more effective in the hierarchical rule selection
as both source-side and target-side rules are se-
lected together.
4.3 Effect of sub-models
Due to the space limitation, we analyze the ef-
fect of sub-models upon the system performance,
rather than that of ME features, part of which have
been investigated in previous related work.
Settings NIST 2006 NIST 2008
Baseline 0.3025 0.2200
Baseline+CFSM 0.3092? 0.2266?
Baseline+CBSM 0.3077? 0.2247?
Baseline+CFTM 0.3076? 0.2286?
Baseline+CBTM 0.3060 0.2255?
Baseline+CFSM+CFTM 0.3109? 0.2289?
Baseline+CFSM+CBSM 0.3104? 0.2282?
Baseline+CFTM+CBTM 0.3099? 0.2299?
Baseline+all sub-models 0.3141? 0.2318?
Table 2: Sub-model effect upon the performance,
*: significantly better than baseline (p < 0.01)
As shown in Table 2, when sub-models are inte-
9
grated as independent features, the performance is
improved compared to the baseline, which shows
that each of the sub-models can improve the hier-
archical rule selection. It is noticeable that the per-
formance of the source-side rule selection model
is comparable with that of the target-side rule se-
lection model. Although CFSM and CFTM per-
form only slightly better than the others among
the individual sub-models, the best performance is
achieved when all the sub-models are integrated.
5 Conclusion
Hierarchical rule selection is an important and
complicated task for hierarchical phrase-based
SMT system. We propose a joint probability
model for the hierarchical rule selection and the
experimental results prove the effectiveness of our
approach.
In the future work, we will explore more useful
features and test our method over the large scale
training corpus. A challenge might exist when
running the ME training toolkit over a big size
of training instances from the large scale training
data.
Acknowledgments
We are especially grateful to the anonymous re-
viewers for their insightful comments. We also
thank Hendra Setiawan, Yuval Marton, Chi-Ho Li,
Shujie Liu and Nan Duan for helpful discussions.
References
Adam L. Berger, Vincent J. Della Pietra, and Stephen
A. Della Pietra. 1996. A Maximum Entropy Ap-
proach to Natural Language Processing. Computa-
tional Linguistics, 22(1): pages 39-72.
David Chiang. 2005. A Hierarchical Phrase-Based
Model for Statistical Machine Translation. In Proc.
ACL, pages 263-270.
David Chiang, Kevin Knight, and Wei Wang. 2009.
11,001 New Features for Statistical Machine Trans-
lation. In Proc. HLT-NAACL, pages 218-226.
Michel Galley, Jonathan Graehl, Kevin Knight, Daniel
Marcu, Steve DeNeefe, Wei Wang, and Ignacio
Thayer. 2006. Scalable Inference and Training of
Context-Rich Syntactic Translation Models. In Proc.
ACL-Coling, pages 961-968.
Kevin Gimpel and Noah A. Smith. 2008. Rich Source-
Side Context for Statistical Machine Translation. In
Proc. the Third Workshop on Statistical Machine
Translation, pages 9-17.
Zhongjun He, Qun Liu, and Shouxun Lin. 2008. Im-
proving Statistical Machine Translation using Lexi-
calized Rule Selection. In Proc. Coling, pages 321-
328.
Philipp Koehn. 2004. Statistical Significance Tests for
Machine Translation Evaluation. In Proc. EMNLP.
Philipp Koehn, Franz J. Och, and Daniel Marcu. 2003.
Statistical Phrase-Based Translation. In Proc. HLT-
NAACL, pages 127-133.
Qun Liu, Zhongjun He, Yang Liu, and Shouxun Lin.
2008. Maximum Entropy based Rule Selection
Model for Syntax-based Statistical Machine Trans-
lation. In Proc. EMNLP, pages 89-97.
Yang Liu, Yun Huang, Qun Liu, and Shouxun Lin.
2007. Forest-to-String Statistical Translation Rules.
In Proc. ACL, pages 704-711.
Yang Liu, Qun Liu, and Shouxun Lin. 2006. Tree-to-
String Alignment Template for Statistical Machine
Translation. In Proc. ACL-Coling, pages 609-616.
Daniel Marcu, Wei Wang, Abdessamad Echihabi, and
Kevin Knight. 2006. SPMT: Statistical Ma-
chine Translation with Syntactified Target Language
Phrases. In Proc. EMNLP, pages 44-52.
Yuval Marton and Philip Resnik. 2008. Soft Syntactic
Constraints for Hierarchical Phrased-Based Trans-
lation. In Proc. ACL, pages 1003-1011.
Haitao Mi, Liang Huang, and Qun Liu. 2008. Forest-
Based Translation. In Proc. ACL, pages 192-199.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: a Method for Automatic
Evaluation of Machine Translation. In Proc. ACL,
pages 311-318.
Slav Petrov and Dan Klein. 2007. Improved Inference
for Unlexicalized Parsing. In Proc. HLT-NAACL,
pages 404-411.
Chris Quirk, Arul Menezes, and Colin Cherry. 2005.
Dependency Treelet Translation: Syntactically In-
formed Phrasal SMT. In Proc. ACL, pages 271-279.
Libin Shen, Jinxi Xu, and Ralph Weischedel. 2008. A
New String-to-Dependency Machine Translation Al-
gorithm with a Target Dependency Language Model.
In Proc. ACL, pages 577-585.
Libin Shen, Jinxi Xu, Bing Zhang, Spyros Matsoukas,
and Ralph Weischedel. 2009. Effective Use of Lin-
guistic and Contextual Information for Statistical
Machine Translation. In Proc. EMNLP, pages 72-
80.
Hendra Setiawan, Min Yen Kan, Haizhou Li, and Philip
Resnik. 2009. Topological Ordering of Function
Words in Hierarchical Phrase-based Translation. In
Proc. ACL, pages 324-332.
10
Dekai Wu. 1997. Stochastic Inversion Transduction
Grammars and Bilingual Parsing of Parallel Cor-
pora. Computational Linguistics, 23(3): pages 377-
403.
Deyi Xiong, Qun Liu, and Shouxun Lin. 2006. Maxi-
mum Entropy Based Phrase Reordering Model for
Statistical Machine Translation. In Proc. ACL-
Coling, pages 521-528.
Deyi Xiong, Min Zhang, Aiti Aw, and Haizhou Li.
2009. A Syntax-Driven Bracketing Model for
Phrase-Based Translation. In Proc. ACL, pages
315-323.
Kenji Yamada and Kevin Knight. 2001. A Syntax-
based Statistical Translation Model. In Proc. ACL,
pages 523-530.
Le Zhang. 2006. Maximum entropy mod-
eling toolkit for python and c++. avail-
able at http://homepages.inf.ed.ac.uk/
lzhang10/maxent_toolkit.html.
11
Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 912?920,
Jeju, Republic of Korea, 8-14 July 2012. c?2012 Association for Computational Linguistics
A Ranking-based Approach to Word Reordering
for Statistical Machine Translation?
Nan Yang?, Mu Li?, Dongdong Zhang?, and Nenghai Yu?
?MOE-MS Key Lab of MCC
University of Science and Technology of China
v-nayang@microsoft.com, ynh@ustc.edu.cn
?Microsoft Research Asia
{muli,dozhang}@microsoft.com
Abstract
Long distance word reordering is a major
challenge in statistical machine translation re-
search. Previous work has shown using source
syntactic trees is an effective way to tackle
this problem between two languages with sub-
stantial word order difference. In this work,
we further extend this line of exploration and
propose a novel but simple approach, which
utilizes a ranking model based on word or-
der precedence in the target language to repo-
sition nodes in the syntactic parse tree of a
source sentence. The ranking model is auto-
matically derived from word aligned parallel
data with a syntactic parser for source lan-
guage based on both lexical and syntactical
features. We evaluated our approach on large-
scale Japanese-English and English-Japanese
machine translation tasks, and show that it can
significantly outperform the baseline phrase-
based SMT system.
1 Introduction
Modeling word reordering between source and tar-
get sentences has been a research focus since the
emerging of statistical machine translation. In
phrase-based models (Och, 2002; Koehn et al,
2003), phrase is introduced to serve as the funda-
mental translation element and deal with local re-
ordering, while a distance based distortion model is
used to coarsely depict the exponentially decayed
word movement probabilities in language transla-
tion. Further work in this direction employed lexi-
?This work has been done while the first author was visiting
Microsoft Research Asia.
calized distortion models, including both generative
(Koehn et al, 2005) and discriminative (Zens and
Ney, 2006; Xiong et al, 2006) variants, to achieve
finer-grained estimations, while other work took into
account the hierarchical language structures in trans-
lation (Chiang, 2005; Galley and Manning, 2008).
Long-distance word reordering between language
pairs with substantial word order difference, such as
Japanese with Subject-Object-Verb (SOV) structure
and English with Subject-Verb-Object (SVO) struc-
ture, is generally viewed beyond the scope of the
phrase-based systems discussed above, because of
either distortion limits or lack of discriminative fea-
tures for modeling. The most notable solution to this
problem is adopting syntax-based SMT models, es-
pecially methods making use of source side syntac-
tic parse trees. There are two major categories in this
line of research. One is tree-to-string model (Quirk
et al, 2005; Liu et al, 2006) which directly uses
source parse trees to derive a large set of translation
rules and associated model parameters. The other
is called syntax pre-reordering ? an approach that
re-positions source words to approximate target lan-
guage word order as much as possible based on the
features from source syntactic parse trees. This is
usually done in a preprocessing step, and then fol-
lowed by a standard phrase-based SMT system that
takes the re-ordered source sentence as input to fin-
ish the translation.
In this paper, we continue this line of work and
address the problem of word reordering based on
source syntactic parse trees for SMT. Similar to most
previous work, our approach tries to rearrange the
source tree nodes sharing a common parent to mimic
912
the word order in target language. To this end, we
propose a simple but effective ranking-based ap-
proach to word reordering. The ranking model is
automatically derived from the word aligned parallel
data, viewing the source tree nodes to be reordered
as list items to be ranked. The ranks of tree nodes are
determined by their relative positions in the target
language ? the node in the most front gets the high-
est rank, while the ending word in the target sentence
gets the lowest rank. The ranking model is trained
to directly minimize the mis-ordering of tree nodes,
which differs from the prior work based on maxi-
mum likelihood estimations of reordering patterns
(Li et al, 2007; Genzel, 2010), and does not require
any special tweaking in model training. The ranking
model can not only be used in a pre-reordering based
SMT system, but also be integrated into a phrase-
based decoder serving as additional distortion fea-
tures.
We evaluated our approach on large-scale
Japanese-English and English-Japanese machine
translation tasks, and experimental results show that
our approach can bring significant improvements to
the baseline phrase-based SMT system in both pre-
ordering and integrated decoding settings.
In the rest of the paper, we will first formally
present our ranking-based word reordering model,
then followed by detailed steps of modeling train-
ing and integration into a phrase-based SMT system.
Experimental results are shown in Section 5. Section
6 consists of more discussions on related work, and
Section 7 concludes the paper.
2 Word Reordering as Syntax Tree Node
Ranking
Given a source side parse tree Te, the task of word
reordering is to transform Te to T ?e, so that e
? can
match the word order in target language as much as
possible. In this work, we only focus on reordering
that can be obtained by permuting children of every
tree nodes in Te. We use children to denote direct de-
scendants of tree nodes for constituent trees; while
for dependency trees, children of a node include not
only all direct dependents, but also the head word
itself. Figure 1 gives a simple example showing the
word reordering between English and Japanese. By
rearranging the position of tree nodes in the English
I am trying to play music
?? ??? ?? ???? ????
PRP VBP VBG TO VB NN
NP
VP
VP
NP
S
VP
VP
S
I amtryingtoplaymusic
PRP VBPVBGTOVBNN
NP
VP
VP
NP
S
VP
VP
?? ??? ?? ???? ????
Original 
Tree
Reordered Tree
S
j0 j1 j2 j3 j4
e0 e1 e2 e3 e4 e5
j0 j1 j2 j3 j4
e0 e1 e2 e3 e4 e5
Figure 1: An English-to-Japanese sentence pair. By
permuting tree nodes in the parse tree, the source
sentence is reordered into the target language or-
der. Constituent tree is shown above the source
sentence; arrows below the source sentences show
head-dependent arcs for dependency tree; word
alignment links are lines without arrow between the
source and target sentences.
parse tree, we can obtain the same word order of
Japanese translation. It is true that tree-based re-
ordering cannot cover all word movement operations
in language translation, previous work showed that
this method is still very effective in practice (Xu et
al., 2009, Visweswariah et al, 2010).
Following this principle, the word reordering task
can be broken into sub-tasks, in which we only
need to determine the order of children nodes for
all non-leaf nodes in the source parse tree. For a
tree node t with children {c1, c2, . . . , cn}, we re-
arrange the children to target-language-like order
{cpi(i1), cpi(i2), . . . , cpi(in)}. If we treat the reordered
position pi(i) of child ci as its ?rank?, the reorder-
913
ing problem is naturally translated into a ranking
problem: to reorder, we determine a ?rank? for each
child, then the children are sorted according to their
?ranks?. As it is often impractical to directly assign
a score for each permutation due to huge number of
possible permutations, a widely used method is to
use a real valued function f to assign a value to each
node, which is called a ranking function (Herbrich
et al, 2000). If we can guarantee (f(i)? f(j)) and
(pi(i) ? pi(j)) always has the same sign, we can get
the same permutation as pi because values of f are
only used to sort the children. For example, con-
sider the node rooted at trying in the dependency
tree in Figure 1. Four children form a list {I, am, try-
ing, play} to be ranked. Assuming ranking function
f can assign values {0.94, ?1.83, ?1.50, ?1.20}
for {I, am, trying, play} respectively, we can get a
sorted list {I, play, trying, am}, which is the desired
permutation according to the target.
More formally, for a tree node t with children
{c1, c2, . . . , cn}, our ranking model assigns a rank
f(ci, t) for each child ci, then the children are sorted
according to the rank in a descending order. The
ranking function f has the following form:
f(ci, t) =
?
j
?j(ci, t) ? wj (1)
where the ?j is a feature representing the tree node t
and its child ci, and wj is the corresponding feature
weight.
3 Ranking Model Training
To learn ranking function in Equation (1), we need to
determine the feature set ? and learn weight vector
w from reorder examples. In this section, we first
describe how to extract reordering examples from
parallel corpus; then we show our features for rank-
ing function; finally, we discuss how to train the
model from the extracted examples.
3.1 Reorder Example Acquisition
For a sentence pair (e, f, a) with syntax tree Te on
the source side, we need to determine which re-
ordered tree T ?e? best represents the word order in
target sentence f . For a tree node t in Te, if its chil-
dren align to disjoint target spans, we can simply ar-
range them in the order of their corresponding target
Prob lem w ith latter procedure
??
lies
? ?? ??? ? ?
in ?
? ??
Prob lem w ith latter procedure
??
lies
? ?? ??? ? ?
in ?
? ??
( a)  gold alignment
( b )  auto alignment
Figure 2: Fragment of a sentence pair. (a) shows
gold alignment; (b) shows automatically generated
alignment which contains errors.
spans. Figure 2 shows a fragment of one sentence
pair in our training data. Consider the subtree rooted
at word ?Problem?. With the gold alignment, ?Prob-
lem? is aligned to the 5th target word, and ?with
latter procedure? are aligned to target span [1, 3],
thus we can simply put ?Problem? after ?with latter
procedure?. Recursively applying this process down
the subtree, we get ?latter procedure with Problem?
which perfectly matches the target language.
As pointed out by (Li et al, 2007), in practice,
nodes often have overlapping target spans due to er-
roneous word alignment or different syntactic struc-
tures between source and target sentences. (b) in
Figure 2 shows the automatically generated align-
ment for the sentence pair fragment. The word
?with? is incorrectly aligned to the 6th Japanese
word ?ha?; as a result, ?with latter procedure? now
has target span [1, 6], while ?Problem? aligns to
[5, 5]. Due to this overlapping, it becomes unclear
which permutation of ?Problem? and ?with latter
procedure? is a better match of the target phrase; we
need a better metric to measure word order similar-
ity between reordered source and target sentences.
We choose to find the tree T ?e? with minimal align-
ment crossing-link number (CLN) (Genzel, 2010)
to f as our golden reordered tree.1 Each crossing-
1A simple solution is to exclude all trees with overlapping
target spans from training. But in our experiment, this method
914
link (i1j1, i2j2) is a pair of alignment links crossing
each other. CLN reaches zero if f is monotonically
aligned to e?, and increases as there are more word
reordering between e? and f . For example, in Fig-
ure 1, there are 6 crossing-links in the original tree:
(e1j4, e2j3), (e1j4, e4j2), (e1j4, e5j1), (e2j3, e4j2),
(e2j3, e5j1) and (e4j2, e5j1); thus CLN for the origi-
nal tree is 6. CLN for the reordered tree is 0 as there
are no crossing-links. This metric is easy to com-
pute, and is not affected by unaligned words (Gen-
zel, 2010).
We need to find the reordered tree with minimal
CLN among all reorder candidates. As the number
of candidates is in the magnitude exponential with
respect to the degree of tree Te 2, it is not always
computationally feasible to enumerate through all
candidates. Our solution is as follows.
First, we give two definitions.
? CLN(t): the number of crossing-links
(i1j1, i2j2) whose source words e?i1 and e
?
i2
both fall under sub span of the tree node t.
? CCLN(t): the number of crossing-links
(i1j1, i2j2) whose source words e?i1 and e
?
i2 fall
under sub span of t?s two different children
nodes c1 and c2 respectively.
Apparently CLN of a tree T ? equals to
CLN(root of T ?), and CLN(t) can be recur-
sively expressed as:
CLN(t) = CCLN(t) +
?
child c of t
CLN(c)
Take the original tree in Figure 1 for example. At the
root node trying, CLN(trying) is 6 because there are
six crossing-links under its sub-span: (e1j4, e2j3),
(e1j4, e4j2), (e1j4, e5j1), (e2j3, e4j2), (e2j3, e5j1)
and (e4j2, e5j1). On the other hand, CCLN(trying)
is 5 because (e4j2, e5j1) falls under its child node
play, thus does not count towards CCLN of trying.
From the definition, we can easily see that
CCLN(t) can be determined solely by the order of
t?s direct children, and CLN(t) is only affected by
discarded too many training instances and led to degraded re-
ordering performance.
2In our experiments, there are nodes with more than 10 chil-
dren for English dependency trees.
the reorder in the subtree of t. This observation en-
ables us to divide the task of finding the reordered
tree T ?e? with minimal CLN into independently find-
ing the children permutation of each node with min-
imal CCLN. Unfortunately, the time cost for the sub-
task is stillO(n!) for a node with n children. Instead
of enumerating through all permutations, we only
search the Inversion Transduction Grammar neigh-
borhood of the initial sequence (Tromble, 2009). As
pointed out by (Tromble, 2009), the ITG neighbor-
hood is large enough for reordering task, and can be
searched through efficiently using a CKY decoder.
After finding the best reordered tree T ?e? , we can
extract one reorder example from every node with
more than one child.
3.2 Features
Features for the ranking model are extracted from
source syntax trees. For English-to-Japanese task,
we extract features from Stanford English Depen-
dency Tree (Marneffe et al, 2006), including lexi-
cons, Part-of-Speech tags, dependency labels, punc-
tuations and tree distance between head and depen-
dent. For Japanese-to-English task, we use a chunk-
based Japanese dependency tree (Kudo and Mat-
sumoto, 2002). Different from features for English,
we do not use dependency labels because they are
not available from the Japanese parser. Additionally,
Japanese function words are also included as fea-
tures because they are important grammatical clues.
The detailed feature templates are shown in Table 1.
3.3 Learning Method
There are many well studied methods available to
learn the ranking function from extracted examples.,
ListNet (?) etc. We choose to use RankingSVM
(Herbrich et al, 2000), a pair-wised ranking method,
for its simplicity and good performance.
For every reorder example t with children
{c1, c2, . . . , cn} and their desired permutation
{cpi(i1), cpi(i2), . . . , cpi(in)}, we decompose it into a
set of pair-wised training instances. For any two
children nodes ci and cj with i < j , we extract a
positive instance if pi(i) < pi(j), otherwise we ex-
tract a negative instance. The feature vector for both
positive instance and negative instance is (?ci??cj ),
where ?ci and ?cj are feature vectors for ci and cj
915
E-J
cl cl ? dst cl ? pct
cl ? dst ? pct cl ? lcl cl ? rcl
cl ? lcl ? dst cl ? rcl ? dst cl ? clex
cl ? clex cl ? clex ? dst cl ? clex ? dst
cl ? hlex cl ? hlex cl ? hlex ? dst
cl ? hlex ? dst cl ? clex ? pct cl ? clex ? pct
cl ? hlex ? pct cl ? hlex ? pct
J-E
ctf ctf ? dst ctf ? lct
ctf ? rct ctf ? lct ? dst cl ? rct ? dst
ctf ? clex ctf ? clex ctf ? clex ? dst
ctf ? clex ? dst ctf ? hf ctf ? hf
ctf ? hf ? dst ctf ? hf ? dst ctf ? hlex
ctf ? hlex ctf ? hlex ? dst ctf ? hlex ? dst
Table 1: Feature templates for ranking function. All
templates are implicitly conjuncted with the pos tag
of head node.
c: child to be ranked; h: head node
lc: left sibling of c; rc: right sibling of c
l: dependency label; t: pos tag
lex: top frequency lexicons
f : Japanese function word
dst: tree distance between c and h
pct: punctuation node between c and h
respectively. In this way, ranking function learning
is turned into a simple binary classification problem,
which can be easily solved by a two-class linear sup-
port vector machine.
4 Integration into SMT system
There are two ways to integrate the ranking reorder-
ing model into a phrase-based SMT system: the pre-
reorder method, and the decoding time constraint
method.
For pre-reorder method, ranking reorder model
is applied to reorder source sentences during both
training and decoding. Reordered sentences can go
through the normal pipeline of a phrase-based de-
coder.
The ranking reorder model can also be integrated
into a phrase based decoder. Integrated method takes
the original source sentence e as input, and ranking
model generates a reordered e? as a word order ref-
erence for the decoder. A simple penalty scheme
is utilized to penalize decoder reordering violating
ranking reorder model?s prediction e?. In this paper,
our underlying decoder is a CKY decoder follow-
ing Bracketing Transduction Grammar (Wu, 1997;
Xiong et al, 2006), thus we show how the penalty
is implemented in the BTG decoder as an example.
Similar penalty can be designed for other decoders
without much effort.
Under BTG, three rules are used to derive transla-
tions: one unary terminal rule, one straight rule and
one inverse rule:
A ? e/f
A ? [A1, A2]
A ? ?A1, A2?
We have three penalty triggers when any rules are
applied during decoding:
? Discontinuous penalty fdc: it fires for all rules
when source span of either A, A1 or A2 is
mapped to discontinuous span in e?.
? Wrong straight rule penalty fst: it fires for
straight rule when source spans of A1 and A2
are not mapped to two adjacent spans in e? in
straight order.
? Wrong inverse rule penalty fiv: it fires for in-
verse rule when source spans of A1 and A2 are
not mapped to two adjacent spans in e? in in-
verse order.
The above three penalties are added as additional
features into the log-linear model of the phrase-
based system. Essentially they are soft constraints
to encourage the decoder to choose translations with
word order similar to the prediction of ranking re-
order model.
5 Experiments
To test our ranking reorder model, we carry out ex-
periments on large scale English-To-Japanese, and
Japanese-To-English translation tasks.
5.1 Data
5.1.1 Evaluation Data
We collect 3,500 Japanese sentences and 3,500
English sentences from the web. They come from
916
a wide range of domains, such as technical docu-
ments, web forum data, travel logs etc. They are
manually translated into the other language to pro-
duce 7,000 sentence pairs, which are split into two
parts: 2,000 pairs as development set (dev) and the
other 5,000 pairs as test set (web test).
Beside that, we collect another 999 English sen-
tences from newswire domain which are translated
into Japanese to form an out-of-domain test data set
(news test).
5.1.2 Parallel Corpus
Our parallel corpus is crawled from the web,
containing news articles, technical documents, blog
entries etc. After removing duplicates, we have
about 18 million sentence pairs, which contain about
270 millions of English tokens and 320 millions of
Japanese tokens. We use Giza++ (Och and Ney,
2003) to generate the word alignment for the parallel
corpus.
5.1.3 Monolingual Corpus
Our monolingual Corpus is also crawled from the
web. After removing duplicate sentences, we have a
corpus of over 10 billion tokens for both English and
Japanese. This monolingual corpus is used to train
a 4-gram language model for English and Japanese
respectively.
5.2 Parsers
For English, we train a dependency parser as (Nivre
and Scholz, 2004) on WSJ portion of Penn Tree-
bank, which are converted to dependency trees us-
ing Stanford Parser (Marneffe et al, 2006). We con-
vert the tokens in training data to lower case, and
re-tokenize the sentences using the same tokenizer
from our MT system.
For Japanese parser, we use CABOCHA, a
chunk-based dependency parser (Kudo and Mat-
sumoto, 2002). Some heuristics are used to adapt
CABOCHA generated trees to our word segmenta-
tion.
5.3 Settings
5.3.1 Baseline System
We use a BTG phrase-based system with a Max-
Ent based lexicalized reordering model (Wu, 1997;
Xiong et al, 2006) as our baseline system for
both English-to-Japanese and Japanese-to-English
Experiment. The distortion model is trained on the
same parallel corpus as the phrase table using a
home implemented maximum entropy trainer.
In addition, a pre-reorder system using manual
rules as (Xu et al, 2009) is included for the English-
to-Japanese experiment (ManR-PR). Manual rules
are tuned by a bilingual speaker on the development
set.
5.3.2 Ranking Reordering System
Ranking reordering model is learned from the
same parallel corpus as phrase table. For efficiency
reason, we only use 25% of the corpus to train our
reordering model. LIBLINEAR (Fan et al, 2008) is
used to do the SVM optimization for RankingSVM.
We test it on both pre-reorder setting (Rank-PR)
and integrated setting (Rank-IT).
5.4 End-to-End Result
system dev web test news test
E-J
Baseline 21.45 21.12 14.18
ManR-PR 23.00 22.42 15.61
Rank-PR 22.92 22.51 15.90
Rank-IT 23.14 22.85 15.72
J-E
Baseline 25.39 24.20 14.26
Rank-PR 26.57 25.56 15.42
Rank-IT 26.72 25.87 15.27
Table 2: BLEU(%) score on dev and test data for
both E-J and J-E experiment. All settings signifi-
cantly improve over the baseline at 95% confidence
level. Baseline is the BTG phrase system system;
ManR-PR is pre-reorder with manual rule; Rank-PR
is pre-reorder with ranking reorder model; Rank-IT
is system with integrated ranking reorder model.
From Table 2, we can see our ranking reordering
model significantly improves the performance for
both English-to-Japanese and Japanese-to-English
experiments over the BTG baseline system. It also
out-performs the manual rule set on English-to-
Japanese result, but the difference is not significant.
5.5 Reordering Performance
In order to show whether the improved performance
is really due to improved reordering, we would like
to measure the reorder performance directly.
917
As we do not have access to a golden re-
ordered sentence set, we decide to use the align-
ment crossing-link numbers between aligned sen-
tence pairs as the measure for reorder performance.
We train the ranking model on 25% of our par-
allel corpus, and use the rest 75% as test data
(auto). We sample a small corpus (575 sentence
pairs) and do manual alignment (man-small). We
denote the automatic alignment for these 575 sen-
tences as (auto-small). From Table 3, we can see
setting auto auto-small man-small
None 36.3 35.9 40.1
E-J
Oracle 4.3 4.1 7.4
ManR 13.4 13.6 16.7
Rank 12.1 12.8 17.2
J-E
Oracle 6.9 7.0 9.4
Rank 15.7 15.3 20.5
Table 3: Reorder performance measured by
crossing-link number per sentence. None means the
original sentences without reordering; Oracle means
the best permutation allowed by the source parse
tree; ManR refers to manual reorder rules; Rank
means ranking reordering model.
our ranking reordering model indeed significantly
reduces the crossing-link numbers over the original
sentence pairs. On the other hand, the performance
of the ranking reorder model still fall far short of or-
acle, which is the lowest crossing-link number of all
possible permutations allowed by the parse tree. By
manual analysis, we find that the gap is due to both
errors of the ranking reorder model and errors from
word alignment and parser.
Another thing to note is that the crossing-link
number of manual alignment is higher than auto-
matic alignment. The reason is that our annotators
tend to align function words which might be left un-
aligned by automatic word aligner.
5.6 Effect of Ranking Features
Here we examine the effect of features for ranking
reorder model. We compare their influence on Rank-
ingSVM accuracy, alignment crossing-link number,
end-to-end BLEU score, and the model size. As
Table 4 shows, a major part of reduction of CLN
comes from features such as Part-of-Speech tags,
Features Acc. CLN BLEU Feat.#
E-J
tag+label 88.6 16.4 22.24 26k
+dst 91.5 13.5 22.66 55k
+pct 92.2 13.1 22.73 79k
+lex100 92.9 12.1 22.85 347k
+lex1000 94.0 11.5 22.79 2,410k
+lex2000 95.2 10.7 22.81 3,794k
J-E
tag+fw 85.0 18.6 25.43 31k
+dst 90.3 16.9 25.62 65k
+lex100 91.6 15.7 25.87 293k
+lex1000 92.4 14.8 25.91 2,156k
+lex2000 93.0 14.3 25.84 3,297k
Table 4: Effect of ranking features. Acc. is Rank-
ingSVM accuracy in percentage on the training data;
CLN is the crossing-link number per sentence on
parallel corpus with automatically generated word
alignment; BLEU is the BLEU score in percentage
on web test set on Rank-IT setting (system with in-
tegrated rank reordering model); lexn means n most
frequent lexicons in the training corpus.
dependency labels (for English), function words (for
Japanese), and the distance and punctuations be-
tween child and head. These features also corre-
spond to BLEU score improvement for End-to-End
evaluations. Lexicon features generally continue to
improve the RankingSVM accuracy and reduce CLN
on training data, but they do not bring further im-
provement for SMT systems beyond the top 100
most frequent words. Our explanation is that less
frequent lexicons tend to help local reordering only,
which is already handled by the underlying phrase-
based system.
5.7 Performance on different domains
From Table 2 we can see that pre-reorder method has
higher BLEU score on news test, while integrated
model performs better on web test set which con-
tains informal texts. By error analysis, we find that
the parser commits more errors on informal texts,
and informal texts usually have more flexible trans-
lations. Pre-reorder method makes ?hard? decision
before decoding, thus is more sensitive to parser er-
rors; on the other hand, integrated model is forced
to use a longer distortion limit which leads to more
search errors during decoding time. It is possible to
918
use system combination method to get the best of
both systems, but we leave this to future work.
6 Discussion on Related Work
There have been several studies focusing on compil-
ing hand-crafted syntactic reorder rules. Collins et
al. (2005), Wang et al (2007), Ramanathan et al
(2008), Lee et al (2010) have developed rules for
German-English, Chinese-English, English-Hindi
and English-Japanese respectively. Xu et al (2009)
designed a clever precedence reordering rule set for
translation from English to several SOV languages.
The drawback for hand-crafted rules is that they de-
pend upon expert knowledge to produce and are lim-
ited to their targeted language pairs.
Automatically learning syntactic reordering rules
have also been explored in several work. Li et
al. (2007) and Visweswariah et al (2010) learned
probability of reordering patterns from constituent
trees using either Maximum Entropy or maximum
likelihood estimation. Since reordering patterns
are matched against a tree node together with all
its direct children, data sparseness problem will
arise when tree nodes have many children (Li et
al., 2007); Visweswariah et al (2010) also men-
tioned their method yielded no improvement when
applied to dependency trees in their initial experi-
ments. Genzel (2010) dealt with the data sparseness
problem by using window heuristic, and learned re-
ordering pattern sequence from dependency trees.
Even with the window heuristic, they were unable
to evaluate all candidates due to the huge num-
ber of possible patterns. Different from the pre-
vious approaches, we treat syntax-based reordering
as a ranking problem between different source tree
nodes. Our method does not require the source
nodes to match some specific patterns, but encodes
reordering knowledge in the form of a ranking func-
tion, which naturally handles reordering between
any number of tree nodes; the ranking function is
trained by well-established rank learning method to
minimize the number of mis-ordered tree nodes in
the training data.
Tree-to-string systems (Quirk et al, 2005; Liu et
al., 2006) model syntactic reordering using minimal
or composed translation rules, which may contain
reordering involving tree nodes from multiple tree
levels. Our method can be naturally extended to deal
with such multiple level reordering. For a tree-to-
string rule with multiple tree levels, instead of rank-
ing the direct children of the root node, we rank all
leaf nodes (Most are frontier nodes (Galley et al,
2006)) in the translation rule. We need to redesign
our ranking feature templates to encode the reorder-
ing information in the source part of the translation
rules. We need to remember the source side con-
text of the rules, the model size would still be much
smaller than a full-fledged tree-to-string system be-
cause we do not need to explicitly store the target
variants for each rule.
7 Conclusion and Future Work
In this paper we present a ranking based reorder-
ing method to reorder source language to match the
word order of target language given the source side
parse tree. Reordering is formulated as a task to rank
different nodes in the source side syntax tree accord-
ing to their relative position in the target language.
The ranking model is automatically trained to min-
imize the mis-ordering of tree nodes in the training
data. Large scale experiment shows improvement on
both reordering metric and SMT performance, with
up to 1.73 point BLEU gain in our evaluation test.
In future work, we plan to extend the ranking
model to handle reordering between multiple lev-
els of source trees. We also expect to explore bet-
ter way to integrate ranking reorder model into SMT
system instead of a simple penalty scheme. Along
the research direction of preprocessing the source
language to facilitate translation, we consider to not
only change the order of the source language, but
also inject syntactic structure of the target language
into source language by adding pseudo words into
source sentences.
Acknowledgements
Nan Yang and Nenghai Yu were partially supported
by Fundamental Research Funds for the Central
Universities (No. WK2100230002), National Nat-
ural Science Foundation of China (No. 60933013),
and National Science and Technology Major Project
(No. 2010ZX03004-003).
919
References
David Chiang. 2005. A Hierarchical Phrase-Based
Model for Statistical Machine Translation. In Proc.
ACL, pages 263-270.
Michael Collins, Philipp Koehn and Ivona Kucerova.
2005. Clause restructuring for statistical machine
translation. In Proc. ACL.
R.-E. Fan, K.-W. Chang, C.-J. Hsieh, X.-R. Wang, and
C.-J. Lin. 2008. LIBLINEAR: A library for large lin-
ear classification. In Journal of Machine Learning Re-
search.
Michel Galley, Jonathan Graehl, Kevin Knight, Daniel
Marcu, Steve DeNeefe, Wei Wang, and Ignacio
Thayer. 2006. Scalable Inference and Training of
Context-Rich Syntactic Translation Models. In Proc.
ACL-Coling, pages 961-968.
Michel Galley and Christopher D. Manning. 2008. A
Simple and Effective Hierarchical Phrase Reordering
Model. In Proc. EMNLP, pages 263-270.
Dmitriy Genzel. 2010. Automatically Learning Source-
side Reordering Rules for Large Scale Machine Trans-
lation. In Proc. Coling, pages 376-384.
Ralf Herbrich, Thore Graepel, and Klaus Obermayer
2000. Large Margin Rank Boundaries for Ordinal Re-
gression. In Advances in Large Margin Classifiers,
pages 115-132.
Philipp Koehn, Amittai Axelrod, Alexandra Birch
Mayne, Chris Callison-Burch, Miles Osborne and
David Talbot. 2005. Edinborgh System Description
for the 2005 IWSLT Speech Translation Evaluation. In
International Workshop on Spoken Language Transla-
tion.
Philipp Koehn, Franz J. Och, and Daniel Marcu. 2003.
Statistical Phrase-Based Translation. In Proc. HLT-
NAACL, pages 127-133.
Taku Kudo, Yuji Matsumoto. 2002. Japanese Depen-
dency Analysis using Cascaded Chunking. In Proc.
CoNLL, pages 63-69.
Young-Suk Lee, Bing Zhao and Xiaoqiang Luo. 2010.
Constituent reordering and syntax models for English-
to-Japanese statistical machine translation. In Proc.
Coling.
Chi-Ho Li, Minghui Li, Dongdong Zhang, Mu Li and
Ming Zhou and Yi Guan 2007. A Probabilistic Ap-
proach to Syntax-based Reordering for Statistical Ma-
chine Translation. In Proc. ACL, pages 720-727.
Yang Liu, Qun Liu, and Shouxun Lin. 2006. Tree-
to-String Alignment Template for Statistical Machine
Translation. In Proc. ACL-Coling, pages 609-616.
Marie-Catherine de Marneffe, Bill MacCartney and
Christopher D. Manning. 2006. Generating Typed
Dependency Parses from Phrase Structure Parses. In
LREC 2006
Joakim Nivre and Mario Scholz 2004. Deterministic De-
pendency Parsing for English Text. In Proc. Coling.
Franz J. Och. 2002. Statistical Machine Translation:
From Single Word Models to Alignment Template.
Ph.D.Thesis, RWTH Aachen, Germany
Franz J. Och and Hermann Ney. 2003. A Systematic
Comparison of Various Statistical Alignment Models.
Computational Linguistics, 29(1): pages 19-51.
Chris Quirk, Arul Menezes, and Colin Cherry. 2005. De-
pendency Treelet Translation: Syntactically Informed
Phrasal SMT. In Proc. ACL, pages 271-279.
A. Ramanathan, Pushpak Bhattacharyya, Jayprasad
Hegde, Ritesh M. Shah and Sasikumar M. 2008.
Simple syntactic and morphological processing can
help English-Hindi Statistical Machine Translation.
In Proc. IJCNLP.
Roy Tromble. 2009. Search and Learning for the Lin-
ear Ordering Problem with an Application to Machine
Translation. Ph.D. Thesis.
Karthik Visweswariah, Jiri Navratil, Jeffrey Sorensen,
Vijil Chenthamarakshan and Nandakishore Kamb-
hatla. 2010. Syntax Based Reordering with Automat-
ically Derived Rules for Improved Statistical Machine
Translation. In Proc. Coling, pages 1119-1127.
Chao Wang, Michael Collins, Philipp Koehn. 2007. Chi-
nese syntactic reordering for statistical machine trans-
lation. In Proc. EMNLP-CoNLL.
Dekai Wu. 1997. Stochastic Inversion Transduction
Grammars and Bilingual Parsing of Parallel Corpora.
Computational Linguistics, 23(3): pages 377-403.
Deyi Xiong, Qun Liu, and Shouxun Lin. 2006. Maxi-
mum Entropy Based Phrase Reordering Model for Sta-
tistical Machine Translation. In Proc. ACL-Coling,
pages 521-528.
Peng Xu, Jaeho Kang, Michael Ringgaard, Franz Och.
2009. Using a Dependency Parser to Improve SMT
for Subject-Object-Verb Languages. In Proc. HLT-
NAACL, pages 376-384.
Richard Zens and Hermann Ney. 2006. Discriminative
Reordering Models for Statistical Machine Transla-
tion. In Proc. Workshop on Statistical Machine Trans-
lation, HLT-NAACL, pages 127-133.
920
Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 950?958,
Jeju, Republic of Korea, 8-14 July 2012. c?2012 Association for Computational Linguistics
Hierarchical Chunk-to-String Translation?
Yang Feng? Dongdong Zhang? Mu Li? Ming Zhou? Qun Liu?
? Department of Computer Science ? Microsoft Research Asia
University of Sheffield dozhang@microsoft.com
Sheffield, UK muli@microsoft.com
y.feng@shef.ac.uk mingzhou@microsoft.com
?Key Laboratory of Intelligent Information Processing
Institute of Computing Technology
Chinese Academy of Sciences
liuqun@ict.ac.cn
Abstract
We present a hierarchical chunk-to-string
translation model, which can be seen as a
compromise between the hierarchical phrase-
based model and the tree-to-string model,
to combine the merits of the two models.
With the help of shallow parsing, our model
learns rules consisting of words and chunks
and meanwhile introduce syntax cohesion.
Under the weighed synchronous context-free
grammar defined by these rules, our model
searches for the best translation derivation
and yields target translation simultaneously.
Our experiments show that our model signif-
icantly outperforms the hierarchical phrase-
based model and the tree-to-string model on
English-Chinese Translation tasks.
1 Introduction
The hierarchical phrase-based model (Chiang, 2007)
makes an advance of statistical machine translation
by employing hierarchical phrases, which not only
uses phrases to learn local translations but also uses
hierarchical phrases to capture reorderings of words
and subphrases which can cover a large scope. Be-
sides, this model is formal syntax-based and does
not need to specify the syntactic constituents of
subphrases, so it can directly learn synchronous
context-free grammars (SCFG) from a parallel text
without relying on any linguistic annotations or as-
sumptions, which makes it used conveniently and
widely.
?This work was done when the first author visited Microsoft
Research Asia as an intern.
However, it is often desirable to consider syntac-
tic constituents of subphrases, e.g. the hierarchical
phrase
X ? ?X 1 for X 2 , X 2 de X 1 ?
can be applied to both of the following strings in
Figure 1
?A request for a purchase of shares?
?filed for bankruptcy?,
and get the following translation, respectively
?goumai gufen de shenqing?
?pochan de shenqing?.
In the former, ?A request? is a NP and this rule acts
correctly while in the latter ?filed? is a VP and this
rule gives a wrong reordering. If we specify the first
X on the right-hand side to NP, this kind of errors
can be avoided.
The tree-to-string model (Liu et al, 2006; Huang
et al, 2006) introduces linguistic syntax via source
parse to direct word reordering, especially long-
distance reordering. Furthermore, this model is for-
malised as Tree Substitution Grammars, so it ob-
serves syntactic cohesion. Syntactic cohesion means
that the translation of a string covered by a subtree
in a source parse tends to be continuous. Fox (2002)
shows that translation between English and French
satisfies cohesion in the majority cases. Many pre-
vious works show promising results with an as-
sumption that syntactic cohesion explains almost
all translation movement for some language pairs
(Wu, 1997; Yamada and Knight, 2001; Eisner, 2003;
Graehl and Knight, 2004; Quirk et al, 2005; Cherry,
2008; Feng et al, 2010).
950
But unfortunately, the tree-to-string model re-
quires each node must be strictly matched during
rule matching, which makes it strongly dependent
on the relationship of tree nodes and their roles in
the whole sentence. This will lead to data sparse-
ness and being vulnerable to parse errors.
In this paper, we present a hierarchical chunk-to-
string translation model to combine the merits of the
two models. Instead of parse trees, our model intro-
duces linguistic information in the form of chunks,
so it does not need to care the internal structures and
the roles in the main sentence of chunks. Based on
shallow parsing results, it learns rules consisting of
either words (terminals) or chunks (nonterminals),
where adjacent chunks are packed into one nonter-
minal. It searches for the best derivation through the
SCFG-motivated space defined by these rules and
get target translation simultaneously. In some sense,
our model can be seen as a compromise between
the hierarchical phrase-based model and the tree-to-
string model, specifically
? Compared with the hierarchical phrase-based
model, it integrates linguistic syntax and sat-
isfies syntactic cohesion.
? Compared with the tree-to-string model, it only
needs to perform shallow parsing which intro-
duces less parsing errors. Besides, our model
allows a nonterminal in a rule to cover several
chunks, which can alleviate data sparseness and
the influence of parsing errors.
? we refine our hierarchical chunk-to-string
model into two models: a loose model (Section
2.1) which is more similar to the hierarchical
phrase-based model and a tight model (Section
2.2) which is more similar to the tree-to-string
model.
The experiments show that on the 2008 NIST
English-Chinese MT translation test set, both the
loose model and the tight model outperform the hi-
erarchical phrase-based model and the tree-to-string
model, where the loose model has a better perfor-
mance. While in terms of speed, the tight model
runs faster and its speed ranking is between the tree-
to-string model and the hierarchical phrase-based
model.
NP IN NP IN NP VBD VP
A request for a purchase of shares was made
goumai gufen de shenqing bei dijiao
?? ?? ? ?? ? ??
(a)
NP VBZ VBN IN NP
The bank has filed for bankruptcy
gai yinhang yijing shenqing pochan
? ?? ?? ?? ??
(b)
Figure 1: A running example of two sentences. For each
sentence, the first row gives the chunk sequence.
S
NP
DT
The
NN
bank
VP
VBZ
has
VP
VBN
filed
PP
IN
for
NP
NN
bankruptcy
(a) A parse tree
B-NP I-NP B-VBZ B-VBN B-IN B-NP
The bank has filed for bankruptcy
(b) A chunk sequence got from the parse tree
Figure 2: An example of shallow parsing.
2 Modeling
Shallow parsing (also chunking) is an analysis of
a sentence which identifies the constituents (noun
groups, verbs, verb groups, etc), but neither spec-
ifies their internal structures, nor their roles in the
main sentence. In Figure 1, we give the chunk se-
quence in the first row for each sentence. We treat
shallow parsing as a sequence label task, and a sen-
tence f can have many possible different chunk la-
bel sequences. Therefore, in theory, the conditional
probability of a target translation e conditioned on
the source sentence f is given by taking the chunk
label sequences as a latent variable c:
p(e|f) =
?
c
p(c|f)p(e|f , c) (1)
951
In practice, we only take the best chunk label se-
quence c? got by
c? = argmax
c
p(c|f) (2)
Then we can ignore the conditional probability
p(c?|f) as it holds the same value for each transla-
tion, and get:
p(e|f) = p(c?|f)p(e|f , c?)
= p(e|f , c?) (3)
We formalize our model as a weighted SCFG.
In a SCFG, each rule (usually called production in
SCFGs) has an aligned pair of right-hand sides ?
the source side and the target side, just as follows:
X ? ??, ?,??
where X is a nonterminal, ? and ? are both strings of
terminals and nonterminals, and ? denotes one-to-
one links between nonterminal occurrences in ? and
nonterminal occurrences in ?. A SCFG produces a
derivation by starting with a pair of start symbols
and recursively rewrites every two coindexed non-
terminals with the corresponding components of a
matched rule. A derivation yields a pair of strings
on the right-hand side which are translation of each
other.
In a weighted SCFG, each rule has a weight and
the total weight of a derivation is the production
of the weights of the rules used by the derivation.
A translation may be produced by many different
derivations and we only use the best derivation to
evaluate its probability. With d denoting a deriva-
tion and r denoting a rule, we have
p(e|f) = max
d
p(d,e|f , c?)
= max
d
?
r?d
p(r,e|f , c?) (4)
Following Och and Ney (2002), we frame our model
as a log-linear model:
p(e|f) = exp
?
k ?kHk(d,e, c?,f)
exp
?
d?,e?,k ?kHk(d?,e?, c?,f)
(5)
where Hk(d,e, c?,f) =
?
r
hk(f , c?, r)
So the best translation is given by
e? = argmax
e
?
k
?kHk(d,e, c?,f) (6)
We employ the same set of features for the log-
linear model as the hierarchical phrase-based model
does(Chiang, 2005).
We further refine our hierarchical chunk-to-string
model into two models: a loose model which is more
similar to the hierarchical phrase-based model and
a tight model which is more similar to the tree-to-
string model. The two models differ in the form of
rules and the way of estimating rule probabilities.
While for decoding, we employ the same decoding
algorithm for the two models: given a test sentence,
the decoders first perform shallow parsing to get the
best chunk sequence, then apply a CYK parsing al-
gorithm with beam search.
2.1 A Loose Model
In our model, we employ rules containing non-
terminals to handle long-distance reordering where
boundary words play an important role. So for the
subphrases which cover more than one chunk, we
just maintain boundary chunks: we bundle adjacent
chunks into one nonterminal and denote it as the first
chunk tag immediately followed by ?-? and next fol-
lowed by the last chunk tag. Then, for the string pair
<filed for bankruptcy, shenqing pochan>, we can
get the rule
r1 : X ? ?VBN 1 for NP 2 , VBN 1 NP 2 ?
while for the string pair <A request for a purchase
of shares, goumai gufen de shenqing>, we can get
r2 : X ? ?NP 1 for NP-NP 2 , NP-NP 2 de NP 1 ?.
The rule matching ?A request for a purchase of
shares was? will be
r3 : X ? ?NP-NP 1 VBD 2 , NP-NP 1 VBD 2 ?.
We can see that in contrast to the method of rep-
resenting each chunk separately, this representation
form can alleviate data sparseness and the influence
of parsing errors.
952
?S 1 , S 1 ? ? ?S 2 X 3 , S 2 X 3 ?
? ?X 4 X 3 , X 4 X 3 ?
? ?NP-NP 5 VBD 6 X 3 , NP-NP 5 VBD 6 X 3 ?
? ?NP 7 for NP-NP 8 VBD 6 X 3 , NP-NP 8 de NP 7 VBD 6 X 3 ?
? ?A request for NP-NP 8 VBD 6 X 3 , NP-NP 8 de shenqing VBD 6 X 3 ?
? ?A request for a purchase of shares VBD 6 X 3 , goumai gufen de shenqing VBD 6 X 3 ?
? ?A request for a purchase of shares was X 3 , goumai gufen de shenqing bei X 3 ?
? ?A request for a purchase of shares was made, goumai gufen de shenqing bei dijiao?
(a) The loose model
?NP-VP 1 , NP-VP 1 ? ? ?NP-VBD 2 VP 3 , NP-VBD 2 VP 3 ?
? ?NP-NP 4 VBD 5 VP 3 , NP-NP 4 VBD 5 VP 3 ?
? ?NP 6 for NP-NP 7 VBD 5 VP 3 , NP-NP 7 de NP 6 VBD 5 VP 3 ?
? ?A request for NP-NP 7 VBD 5 VP 3 , NP-NP 7 de shenqing VBD 5 VP 3 ?
? ?A request for a purchase of shares VBD 5 VP 3 , goumai gufen de shenqing VBD 5 VP 3 ?
? ?A request for a purchase of shares was VP 3 , goumai gufen de shenqing bei VP 3 ?
? ?A request for a purchase of shares was made, goumai gufen de shenqing bei dijiao?
(b) The tight model
Figure 3: The derivations of the sentence in Figure 1(a).
In these rules, the left-hand nonterminal symbol X
can not match any nonterminal symbol on the right-
hand side. So we need a set of rules such as
NP ? ?X 1 , X 1 ?
NP-NP ? ?X 1 , X 1 ?
and so on, and set the probabilities of these rules to
1. To simplify the derivation, we discard this kind of
rules and assume that X can match any nonterminal
on the right-hand side.
Only with r2 and r3, we cannot produce any
derivation of the whole sentence in Figure 1 (a). In
this case we need two special glue rules:
r4 : S ? ?S 1 X 2 , S 1 X 2 ?
r5 : S ? ?X 1 , X 1 ?
Together with the following four lexical rules,
r6 : X ? ?a request, shenqing?
r7 : X ? ?a purchase of shares, goumai gufen?
r8 : X ? ?was, bei?
r9 : X ? ?made, dijiao?
Figure 3(a) shows the derivation of the sentence in
Figure 1(a).
2.2 A Tight Model
In the tight model, the right-hand side of each rule
remains the same as the loose model, but the left-
hand side nonterminal is not X but the correspond-
ing chunk labels. If a rule covers more than one
chunk, we just use the first and the last chunk la-
bels to denote the left-hand side nonterminal. The
rule set used in the tight model for the example in
Figure 1(a) corresponding to that in the loose model
becomes:
r2 : NP-NP ? ?NP 1 for NP-NP 2 , NP-NP 2 de NP 1 ?
r3 : NP-VBD ? ?NP-NP 1 VBD 2 , NP-NP 1 VBD 2 ?.
r6 : NP ? ?a request, shenqing?
r7 : NP-NP ? ?a purchase of shares, goumai gufen?
r8 : VBD ? ?was, bei?
r9 : VP ? ?made, dijiao?
During decoding, we first collect rules for each
span. For a span which does not have any matching
rule, if we do not construct default rules for it, there
will be no derivation for the whole sentence, then we
need to construct default rules for this kind of span
by enumerating all possible binary segmentation of
the chunks in this span. For the example in Figure
1(a), there is no rule matching the whole sentence,
953
so we need to construct default rules for it, which
should be
NP-VP ? ?NP-VBD 1 VP 2 , NP-VBD 1 VP 2 ?.
NP-VP ? ?NP-NP 1 VBD-VP 2 , NP-NP 1 VBD-VP 2 ?.
and so on.
Figure 3(b) shows the derivation of the sentence
in Figure 1(a).
3 Shallow Parsing
In a parse tree, a chunk is defined by a leaf node or
an inner node whose children are all leaf nodes (See
Figure 2 (a)). In our model, we identify chunks by
traversing a parse tree in a breadth-first order. Once
a node is recognized as a chunk, we skip its children.
In this way, we can get a sole chunk sequence given
a parse tree. Then we label each word with a label
indicating whether the word starts a chunk (B-) or
continues a chunk (I-). Figure 2(a) gives an example.
In this method, we get the training data for shallow
parsing from Penn Tree Bank.
We take shallow Parsing (chunking) as a sequence
label task and employ Conditional Random Field
(CRF)1 to train a chunker. CRF is a good choice for
label tasks as it can avoid label bias and use more
statistical correlated features. We employ the fea-
tures described in Sha and Pereira (2003) for CRF.
We do not introduce CRF-based chunkier in this pa-
per and more details can be got from Hammersley
and Clifford (1971), Lafferty et al (2001), Taskar et
al. (2002), Sha and Pereira (2003).
4 Rule Extraction
In what follows, we introduce how to get the rule
set. We learn rules from a corpus that first is bi-
directionally word-aligned by the GIZA++ toolkit
(Och and Ney, 2000) and then is refined using a
?final-and? strategy. We generate the rule set in two
steps: first, we extract two sets of phrases, basic
phrases and chunk-based phrases. Basic phrases are
defined using the same heuristic as previous systems
(Koehn et al, 2003; Och and Ney, 2004; Chiang,
2005). A chunk-based phrase is such a basic phrase
that covers one or more chunks on the source side.
1We use the open source toolkit CRF++ got in
http://code.google.com/p/crfpp/ .
We identity chunk-based phrases ?cj2j1 ,f
j2
j1 ,e
i2
i1? as
follows:
1. A chunk-based phrase is a basic phrase;
2. cj1 begins with ?B-?;
3. fj2 is the end word on the source side or cj2+1
does not begins with ?I-?.
Given a sentence pair ?f ,e,??, we extract rules for
the loose model as follows
1. If ?f j2j1 ,e
i2
i1? is a basic phrase, then we can have
a rule
X ? ?f j2j1 ,e
i2
i1?
2. Assume X ? ??, ?? is a rule with ? =
?1f j2j1 ?2 and ? = ?1e
i2
i1?2, and ?f
j2
j1 ,e
i2
i1? is
a chunk-based phrase with a chunk sequence
Yu ? ? ?Yv, then we have the following rule
X ? ??1Yu-Yv k ?2, ?1Yu-Yv k ?2?.
We evaluate the distribution of these rules in the
same way as Chiang (2007).
We extract rules for the tight model as follows
1. If ?f j2j1 ,e
i2
i1? is a chunk-based phrase with a
chunk sequence Ys ? ? ?Yt, then we can have a
rule
Ys-Yt ? ?f j2j1 ,e
i2
i1?
2. Assume Ys-Yt ? ??, ?? is a rule with ? =
?1f j2j1 ?2 and ? = ?1e
i2
i1?2, and ?f
j2
j1 ,e
i2
i1? is
a chunk-based phrase with a chunk sequence
Yu ? ? ?Yv, then we have the following rule
Ys-Yt ? ??1Yu-Yv k ?2, ?1Yu-Yv k ?2?.
We evaluate the distribution of rules in the same way
as Liu et al (2006).
For the loose model, the nonterminals must be co-
hesive, while the whole rule can be noncohesive: if
both ends of a rule are nonterminals, the whole rule
is cohesive, otherwise, it may be noncohesive. In
contrast, for the tight model, both the whole rule and
the nonterminal are cohesive.
Even with the cohesion constraints, our model
still generates a large number of rules, but not all
954
of the rules are useful for translation. So we follow
the method described in Chiang (2007) to filter the
rule set except that we allow two nonterminals to be
adjacent.
5 Related Works
Watanabe et al (2003) presented a chunk-to-string
translation model where the decoder generates a
translation by first translating the words in each
chunk, then reordering the translation of chunks.
Our model distinguishes from their model mainly
in reordering model. Their model reorders chunks
resorting to a distortion model while our model re-
orders chunks according to SCFG rules which retain
the relative positions of chunks.
Nguyen et al (2008) presented a tree-to-string
phrase-based method which is based on SCFGs.
This method generates SCFGs through syntac-
tic transformation including a word-to-phrase tree
transformation model and a phrase reordering model
while our model learns SCFG-based rules from
word-aligned bilingual corpus directly
There are also some works aiming to introduce
linguistic knowledge into the hierarchical phrase-
based model. Marton and Resnik (2008) took the
source parse tree into account and added soft con-
straints to hierarchical phrase-based model. Cherry
(2008) used dependency tree to add syntactic cohe-
sion. These methods work with the original SCFG
defined by hierarchical phrase-based model and use
linguistic knowledge to assist translation. Instead,
our model works under the new defined SCFG with
chunks.
Besides, some other researchers make efforts on
the tree-to-string model by employing exponentially
alternative parses to alleviate the drawback of 1-best
parse. Mi et al (2008) presented forest-based trans-
lation where the decoder translates a packed forest
of exponentially many parses instead of i-best parse.
Liu and Liu (2010) proposed to parse and to trans-
late jointly by taking tree-based translation as pars-
ing. Given a source sentence, this decoder produces
a parse tree on the source side and a translation on
the target side simultaneously. Both the models per-
form in the unit of tree nodes rather than chunks.
6 Experiments
6.1 Data Preparation
Data for shallow parsing We got training data and
test data for shallow parsing from the standard Penn
Tree Bank (PTB) English parsing task by splitting
the sections 02-21 on the Wall Street Journal Portion
(Marcus et al, 1993) into two sets: the last 1000
sentences as the test set and the rest as the training
set. We filtered the features whose frequency was
lower than 3 and substituted ?? and ?? with ? to
keep consistent with translation data. We used L2
algorithm to train CRF.
Data for Translation We used the NIST training
set for Chinese-English translation tasks excluding
the Hong Kong Law and Hong Kong Hansard2 as the
training data, which contains 470K sentence pairs.
For the training data set, we first performed word
alignment in both directions using GIZA++ toolkit
(Och and Ney, 2000) then refined the alignments
using ?final-and?. We trained a 5-gram language
model with modified Kneser-Ney smoothing on the
Xinhua portion of LDC Chinese Gigaword corpus.
For the tree-to-string model, we parsed English sen-
tences using Stanford parser and extracted rules us-
ing the GHKM algorithm (Galley et al, 2004).
We used our in-house English-Chinese data set
as the development set and used the 2008 NIST
English-Chinese MT test set (1859 sentences) as the
test set. Our evaluation metric was BLEU-4 (Pap-
ineni et al, 2002) based on characters (as the tar-
get language is Chinese), which performed case-
insensitive matching of n-grams up to n = 4 and
used the shortest reference for the brevity penalty.
We used the standard minimum error-rate training
(Och, 2003) to tune the feature weights to maximize
the BLEU score on the development set.
6.2 Shallow Parsing
The standard evaluation metrics for shallow parsing
are precision P, recall R, and their harmonic mean
F1 score, given by:
P = number of exactly recognized chunks
number of output chunks
R = number of exactly recognized chunks
number of reference chunks
2The source side and target side are reversed.
955
Word number Chunk number Accuracy %
23861 12258 94.48
Chunk type P % R % F1 % Found
All 91.14 91.35 91.25 12286
One 90.32 90.99 90.65 5236
NP 93.97 94.47 94.22 5523
ADVP 82.53 84.30 83.40 475
VP 93.66 92.04 92.84 284
ADJP 65.68 69.20 67.39 236
WHNP 96.30 95.79 96.04 189
QP 83.06 80.00 81.50 183
Table 1: Shallow parsing result. The collum Found gives
the number of chunks recognized by CRF, the row All
represents all types of chunks, and the row One represents
the chunks that consist of one word.
F1 =
2 ? P ? R
P +R
Besides, we need another metric, accuracy A, to
evaluate the accurate rate of individual labeling de-
cisions of every word as
A = number of exactly labeled words
number of words
For example, given a reference sequence
B-NP I-NP I-NP B-VP I-VP B-VP, CRF out-
puts a sequence O-NP I-NP I-NP B-VP I-VP I-NP,
then P = 33.33%, A = 66.67%.
Table 1 summaries the results of shallow parsing.
For ?? and ?? were substituted with ? , the perfor-
mance was slightly influenced.
The F1 score of all chunks is 91.25% and the F1
score of One and NP, which in number account for
about 90% of chunks, is 90.65% and 94.22% respec-
tively. F score of NP chunking approaches 94.38%
given in Sha and Pereira (2003).
6.3 Performance Comparison
We compared our loose decoder and tight decoder
with our in-house hierarchical phrase-based decoder
(Chiang, 2007) and the tree-to-string decoder (Liu et
al., 2006). We set the same configuration for all the
decoders as follows: stack size = 30, nbest size = 30.
For the hierarchical chunk-based and phrase-based
decoders, we set max rule length to 5. For the tree-
to-string decoder, we set the configuration of rule
System Dev NIST08 Speed
phrase 0.2843 0.3921 1.163
tree 0.2786 0.3817 1.107
tight 0.2914 0.3987 1.208
loose 0.2936 0.4023 1.429
Table 2: Performance comparison. Phrase represents
the hierarchical phrase-based decoder, tree represents the
tree-to-string decoder, tight represents our tight decoder
and loose represents our loose decoder. The speed is re-
ported by seconds per sentence. The speed for the tree-to-
string decoder includes the parsing time (0.23s) and the
speed for the tight and loose models includes the shallow
parsing time, too.
extraction as: the height up to 3 and the number of
leaf nodes up to 5.
We give the results in Table 2. From the results,
we can see that both the loose and tight decoders
outperform the baseline decoders and the improve-
ment is significant using the sign-test of Collins et
al. (2005) (p < 0.01). Specifically, the loose model
has a better performance while the tight model has a
faster speed.
Compared with the hierarchical phrase-based
model, the loose model only imposes syntactic cohe-
sion cohesion to nonterminals while the tight model
imposes syntax cohesion to both rules and nonter-
minals which reduces search space, so it decoders
faster. We can conclude that linguistic syntax can
indeed improve the translation performance; syntac-
tic cohesion for nonterminals can explain linguis-
tic phenomena well; noncohesive rules are useful,
too. The extra time consumption against hierarchi-
cal phrase-based system comes from shallow pars-
ing.
By investigating the translation result, we find that
our decoder does well in rule selection. For exam-
ple, in the hierarchical phrase-based model, this kind
of rules, such as
X ? ?X of X, ??, X ? ?X for X, ??
and so on, where ? stands for the target component,
are used with a loose restriction as long as the ter-
minals are matched, while our models employ more
stringent constraints on these rules by specifying the
syntactic constituent of ?X?. With chunk labels, our
models can make different treatment for different
situations.
956
System Dev NIST08 Speed
cohesive 0.2936 0.4023 1.429
noncohesive 0.2937 0.3964 1.734
Table 3: Influence of cohesion. The row cohesive rep-
resents the loose system where nonterminals satisfy co-
hesion, and the row noncohesive represents the modified
version of the loose system where nonterminals can be
noncohesive.
Compared with the tree-to-string model, the re-
sult indicates that the change of the source-side lin-
guistic syntax from parses to chunks can improve
translation performance. The reasons should be our
model can reduce parse errors and it is enough to use
chunks as the basic unit for machine translation. Al-
though our decoders and tree-to-string decoder all
run in linear-time with beam search, tree-to-string
model runs faster for it searches through a smaller
SCFG-motivated space.
6.4 Influence of Cohesion
We verify the influence of syntax cohesion via the
loose model. The cohesive model imposes syntax
cohesion on nonterminals to ensure the chunk is re-
ordered as a whole. In this experiment, we introduce
a noncohesive model by allowing a nonterminal to
match part of a chunk. For example, in the nonco-
hesive model, it is legal for a rule with the source
side
?NP for NP-NP?
to match
?request for a purchase of shares?
in Figure 1 (a), where ?request? is part of NP. As
well, the rule with the source side
?NP for a NP-NP?
can match
?request for a purchase of shares?.
In this way, we can ensure all the rules used in the
cohesive system can be used in the noncohesive sys-
tem. Besides cohesive rules, the noncohesive system
can use noncohesive rules, too.
We give the results in Table 3. From the results,
we can see that cohesion helps to reduce search
space, so the cohesive system decodes faster. The
noncohesive system decoder slower, as it employs
System Number Dev NIST08 Speed
loose two 0.2936 0.4023 1.429
loose three 0.2978 0.4037 2.056
tight two 0.2914 0.3987 1.208
tight three 0.2954 0.4026 1.780
Table 4: The influence of the number of nonterminals.
The column number lists the number of nonterminals
used at most in a rule.
more rules, but this does not bring any improvement
of translation performance. As other researches said
in their papers, syntax cohesion can explain linguis-
tic phenomena well.
6.5 Influence of the number of nonterminals
We also tried to allow a rule to hold three nonter-
minals at most. We give the result in Table 4. The
result shows that using three nonterminals does not
bring a significant improvement of translation per-
formance but quite more time consumption. So we
only retain two nonterminals at most in a rule.
7 Conclusion
In this paper, we present a hierarchical chunk-
to-string model for statistical machine translation
which can be seen as a compromise of the hierarchi-
cal phrase-based model and the tree-to-string model.
With the help of shallow parsing, our model learns
rules consisting of either words or chunks and com-
presses adjacent chunks in a rule to a nonterminal,
then it searches for the best derivation under the
SCFG defined by these rules. Our model can com-
bine the merits of both the models: employing lin-
guistic syntax to direct decoding, being syntax co-
hesive and robust to parsing errors. We refine the hi-
erarchical chunk-to-string model into two models: a
loose model (more similar to the hierarchical phrase-
based model) and a tight model (more similar to the
tree-to-string model).
Our experiments show that our decoder can im-
prove translation performance significantly over the
hierarchical phrase-based decoder and the tree-to-
string decoder. Besides, the loose model gives a bet-
ter performance while the tight model gives a faster
speed.
957
8 Acknowledgements
We would like to thank Trevor Cohn, Shujie Liu,
Nan Duan, Lei Cui and Mo Yu for their help,
and anonymous reviewers for their valuable com-
ments and suggestions. This work was supported
in part by EPSRC grant EP/I034750/1 and in part
by High Technology R&D Program Project No.
2011AA01A207.
References
Colin Cherry. 2008. Cohesive phrase-based decoding for
statistical machine translation. In Proc. of ACL, pages
72?80.
David Chiang. 2005. A hierarchical phrase-based model
for statistical machine translation. In Proc. of ACL,
pages 263?270.
David Chiang. 2007. Hierarchical phrase-based transla-
tion. Computational Linguistics, 33:201?228.
Michael Collins, Philipp Koehn, and Ivona Kucerova.
2005. Clause restructuring for statistical machine
translation. In Proc. of ACL, pages 531?540.
Jason Eisner. 2003. Learning non-isomorphic tree map-
pings for machine translation. In Proc. of ACL, pages
205?208.
Yang Feng, Haitao Mi, Yang Liu, and Qun Liu. 2010. An
efficient shift-reduce decoding algorithm for phrased-
based machine translation. In Proc. of Coling:Posters,
pages 285?293.
Heidi Fox. 2002. Phrasal cohesion and statistical ma-
chine translation. In Proc. of EMNLP, pages 304?
3111.
Michel Galley, Mark Hopkins, Kevin Knight, and Daniel
Marcu. 2004. What?s in a translation rule? In Proc of
NAACL, pages 273?280.
Jonathan Graehl and Kevin Knight. 2004. Training tree
transducers. In Proc. of HLT-NAACL, pages 105?112.
J Hammersley and P Clifford. 1971. Markov fields on
finite graphs and lattices. In Unpublished manuscript.
Liang Huang, Kevin Knight, and Aravind Joshi. 2006.
Statistical syntax-directed translation with extended
domain of locality. In Proceedings of AMTA.
Philipp Koehn, Franz J. Och, and Daniel Marcu. 2003.
Statistical phrase-based translation. In Proc. of HLT-
NAACL, pages 127?133.
John Lafferty, Andrew McCallum, and Fernando Pereira.
2001. Conditional random fields: Probabilistic models
for segmenting and labeling sequence data. In Proc. of
ICML, pages 282?289.
Yang Liu and Qun Liu. 2010. Joint parsing and transla-
tion. In Proc. of COLING, pages 707?715.
Yang Liu, Qun Liu, and Shouxun Lin. 2006. Tree-to-
string alignment template for statistical machine trans-
lation. In Proc. of COLING-ACL, pages 609?616.
Mitchell P. Marcus, Beatrice Santorini, and Mary Ann
Marcinkiewicz. 1993. Building a large annotated cor-
pus of english: The penn treebank. Computational
Linguistics, 19:313?330.
Yuval Marton and Philip Resnik. 2008. Soft syntactic
constraints for hierarchical phrased-based translation.
In Proc. of ACL, pages 1003?1011.
Haitao Mi, Liang Huang, and Qun Liu. 2008. Forest-
based translation. In Proc. of ACL, pages 192?199.
Thai Phuong Nguyen, Akira Shimazu, Tu Bao Ho,
Minh Le Nguyen, and Vinh Van Nguyen. 2008. A
tree-to-string phrase-based model for statistical ma-
chine translation. In Proc. of CoNLL, pages 143?150.
Franz Josef Och and Hermann Ney. 2000. Improved
statistical alignment models. In Proc. of ACL.
Franz Josef Och and Hermann Ney. 2002. Discrimina-
tive training and maximum entropy models for statis-
tical machine translation. In Proc. of ACL, pages 295?
302.
Frans J. Och and Hermann Ney. 2004. The alignment
template approach to statistical machine translation.
Computational Linguistics, 30:417?449.
Frans J. Och. 2003. Minimum error rate training in sta-
tistical machine translation. In Proc. of ACL, pages
160?167.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: a method for automatic evalu-
ation of machine translation. In Proceedings of ACL,
pages 311?318.
Chris Quirk, Arul Menezes, and Colin Cherry. 2005. De-
pendency treelet translation: Syntactically informed
phrasal smt. In Proceedings of ACL, pages 271?279.
Fei Sha and Fernando Pereira. 2003. Shallow pars-
ing with conditional random fields. In Proc. of HLT-
NAACL, pages 134?141.
Ben Taskar, Pieter Abbeel, and Daphne Koller. 2002.
Discriminative probabilistic models for relational data.
In Eighteenth Conference on Uncertainty in Artificial
Intelligence.
Taro Watanabe, Eiichiro Sumita, and Hiroshi G. Okuno.
2003. Chunk-based statistical translation. In Proc. of
ACL, pages 303?310.
Dekai Wu. 1997. Stochastic inversion transduction
grammars and bilingual parsing of parallel corpora.
Computational Linguistics, 23:377?403.
Kenji Yamada and Kevin Knight. 2001. A syntax-based
statistical translation model. In Proc. of ACL, pages
523?530.
958
Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 291?295,
Jeju, Republic of Korea, 8-14 July 2012. c?2012 Association for Computational Linguistics
Translation Model Size Reduction for
Hierarchical Phrase-based Statistical Machine Translation
Seung-Wook Lee? Dongdong Zhang? Mu Li? Ming Zhou? Hae-Chang Rim?
? Dept. of Computer & Radio Comms. Engineering, Korea University, Seoul, South Korea
{swlee,rim}@nlp.korea.ac.kr
? Microsoft Research Asia, Beijing, China
{dozhang,muli,mingzhou}@microsoft.com
Abstract
In this paper, we propose a novel method of
reducing the size of translation model for hier-
archical phrase-based machine translation sys-
tems. Previous approaches try to prune in-
frequent entries or unreliable entries based on
statistics, but cause a problem of reducing the
translation coverage. On the contrary, the pro-
posed method try to prune only ineffective
entries based on the estimation of the infor-
mation redundancy encoded in phrase pairs
and hierarchical rules, and thus preserve the
search space of SMT decoders as much as
possible. Experimental results on Chinese-to-
English machine translation tasks show that
our method is able to reduce almost the half
size of the translation model with very tiny
degradation of translation performance.
1 Introduction
Statistical Machine Translation (SMT) has gained
considerable attention during last decades. From a
bilingual corpus, all translation knowledge can be
acquired automatically in SMT framework. Phrase-
based model (Koehn et al, 2003) and hierarchical
phrase-based model (Chiang, 2005; Chiang, 2007)
show state-of-the-art performance in various lan-
guage pairs. This achievement is mainly benefit
from huge size of translational knowledge extracted
from sufficient parallel corpus. However, the errors
of automatic word alignment and non-parallelized
bilingual sentence pairs sometimes have caused the
unreliable and unnecessary translation rule acquisi-
tion. According to Bloodgood and Callison-Burch
(2010) and our own preliminary experiments, the
size of phrase table and hierarchical rule table con-
sistently increases linearly with the growth of train-
ing size, while the translation performance tends to
gain minor improvement after a certain point. Con-
sequently, the model size reduction is necessary and
meaningful for SMT systems if it can be performed
without significant performance degradation. The
smaller the model size is, the faster the SMT de-
coding speed is, because there are fewer hypotheses
to be investigated during decoding. Especially, in a
limited environment, such as mobile device, and for
a time-urgent task, such as speech-to-speech transla-
tion, the compact size of translation rules is required.
In this case, the model reduction would be the one
of the main techniques we have to consider.
Previous methods of reducing the size of SMT
model try to identify infrequent entries (Zollmann
et al, 2008; Huang and Xiang, 2010). Several sta-
tistical significance testing methods are also exam-
ined to detect unreliable noisy entries (Tomeh et al,
2009; Johnson et al, 2007; Yang and Zheng, 2009).
These methods could harm the translation perfor-
mance due to their side effect of algorithms; simi-
lar multiple entries can be pruned at the same time
deteriorating potential coverage of translation. The
proposed method, on the other hand, tries to mea-
sure the redundancy of phrase pairs and hierarchi-
cal rules. In this work, redundancy of an entry is
defined as its translational ineffectiveness, and esti-
mated by comparing scores of entries and scores of
their substituents. Suppose that the source phrase
s1s2 is always translated into t1t2 with phrase en-
try <s1s2?t1t2> where si and ti are correspond-
291
ing translations. Similarly, source phrases s1 and
s2 are always translated into t1 and t2, with phrase
entries, <s1?t1> and <s2?t2>, respectively. In
this case, it is intuitive that <s1s2?t1t2> could be
unnecessary and redundant since its substituent al-
ways produces the same result. This paper presents
statistical analysis of this redundancy measurement.
The redundancy-based reduction can be performed
to prune the phrase table, the hierarchical rule table,
and both. Since the similar translation knowledge
is accumulated at both of tables during the train-
ing stage, our reduction method performs effectively
and safely. Unlike previous studies solely focus on
either phrase table or hierarchical rule table, this
work is the first attempt to reduce phrases and hi-
erarchical rules simultaneously.
2 Proposed Model
Given an original translation model, TM , our goal
is to find the optimally reduced translation model,
TM?, which minimizes the degradation of trans-
lation performance. To measure the performance
degradation, we introduce a new metric named con-
sistency:
C(TM,TM?) =
BLEU(D(s;TM),D(s;TM?)) (1)
where the function D produces the target sentence
of the source sentence s, given the translation model
TM . Consistency measures the similarity between
the two groups of decoded target sentences produced
by two different translation models. There are num-
ber of similarity metrics such as Dices coefficient
(Kondrak et al, 2003), and Jaccard similarity coef-
ficient. Instead, we use BLEU scores (Papineni et
al., 2002) since it is one of the primary metrics for
machine translation evaluation. Note that our con-
sistency does not require the reference set while the
original BLEU does. This means that only (abun-
dant) source-side monolingual corpus is needed to
predict performance degradation. Now, our goal can
be rewritten with this metric; among all the possible
reduced models, we want to find the set which can
maximize the consistency:
TM? = argmax
TM ??TM
C(TM,TM ?) (2)
In minimum error rate training (MERT) stages,
a development set, which consists of bilingual sen-
tences, is used to find out the best weights of fea-
tures (Och, 2003). One characteristic of our method
is that it isolates feature weights of the transla-
tion model from SMT log-linear model, trying to
minimize the impact of search path during decod-
ing. The reduction procedure consists of three
stages: translation scoring, redundancy estimation,
and redundancy-based reduction.
Our reduction method starts with measuring the
translation scores of the individual phrase and the
hierarchical rule. Similar to the decoder, the scoring
scheme is based on the log-linear framework:
PS(p) =
?
i
?ihi(p) (3)
where h is a feature function and ? is its weight.
As the conventional hierarchical phrase-based SMT
model, our features are composed of P (e|f ), P (f |e),
Plex(e|f ), Plex(f |e), and the number of phrases,
where e and f denote a source phrase and a target
phrase, respectively. Plex is the lexicalized proba-
bility. In a similar manner, the translation scores of
hierarchical rules are calculated as follows:
HS(r) =
?
i
?ihi(r) (4)
The features are as same as those that are used for
phrase scoring, except the last feature. Instead of the
phrase number penalty, the hierarchical rule num-
ber penalty is used. The weight for each feature is
shared from the results of MERT. With this scoring
scheme, our model is able to measure how important
the individual entry is during decoding.
Once translation scores for all entries are es-
timated, our method retrieves substituent candi-
dates with their combination scores. The combina-
tion score is calculated by accumulating translation
scores of every member as follows:
CS(p1...n) =
n
?
i=1
PS(pi) (5)
This scoring scheme follows the same manner
what the conventional decoder does, finding the best
phrase combination during translation. By compar-
ing the original translation score with combination
292
scores of its substituents, the redundancy scores are
estimated, as follows:
Red(p) = min
p1...n?Sub(p)
PS(p)?CS(p1...n) (6)
where Sub is the function that retrieves all possi-
ble substituents (the combinations of sub-phrases,
and/or sub-rules that exactly produce the same tar-
get phrase, given the source phrase p). If the com-
bination score of the best substituent is same as the
translation score of p, the redundancy score becomes
zero. In this case, the decoder always produces the
same translation results without p. When the redun-
dancy score is negative, the best substituent is more
likely to be chosen instead of p. This implies that
there is no risk to prune p; the search space is not
changed, and the search path is not changed as well.
Our method can be varied according to the desig-
nation of Sub function. If both of the phrase table
and the hierarchical rule table are allowed, cross re-
duction can be possible; the phrase table is reduced
based on the hierarchical rule table and vice versa.
With extensions of combination scoring and redun-
dancy scoring schemes like following equations, our
model is able to perform cross reduction.
CS(p1...n, h1...m) =
n
?
i=1
PS(pi) +
m
?
i=1
HS(hi) (7)
Red(p) = min
<p1...n,h1...m>?Sub(p)
PS(p)? CS(p1...n, h1...m) (8)
The proposed method has some restrictions for
reduction. First of all, it does not try to prune the
phrase that has no substituents, such as unigram
phrases; the phrase whose source part is composed
of a single word. This restriction guarantees that
the translational coverage of the reduced model is
as high as those of the original translation model.
In addition, our model does not prune the phrases
and the hierarchical rules that have reordering within
it to prevent information loss of reordering. For
instance, if we prune phrase, <s1s2s3?t3t1t2>,
phrases, <s1s2?t1t2> and <s3?t3> are not able
to produce the same target words without appropri-
ate reordering.
Once the redundancy scores for all entries have
been estimated, the next step is to select the best
N entries to prune to satisfy a desired model size.
We can simply prune the first N from the list of en-
tries sorted by increasing order of redundancy score.
However, this method may not result in the opti-
mal reduction, since each redundancy scores are es-
timated based on the assumption of the existence of
all the other entries. In other words, there are depen-
dency relationships among entries. We examine two
methods to deal with this problem. The first is to
ignore dependency, which is the more efficient man-
ner. The other is to prune independent entries first.
After all independent entries are pruned, the depen-
dent entries are started to be pruned. We present the
effectiveness of each method in the next section.
Since our goal is to reduce the size of all transla-
tion models, the reduction is needed to be performed
for both the phrase table and the hierarchical rule
table simultaneously, namely joint reduction. Sim-
ilar to phrase reduction and hierarchical rule reduc-
tion, it selects the best N entries of the mixture of
phrase and hierarchical rules. This method results
in safer pruning; once a phrase is determined to be
pruned, the hierarchical rules, which are related to
this phrase, are likely to be kept, and vice versa.
3 Experiment
We investigate the effectiveness of our reduction
method by conducting Chinese-to-English transla-
tion task. The training data, as same as Cui et
al. (2010), consists of about 500K parallel sentence
pairs which is a mixture of several datasets pub-
lished by LDC. NIST 2003 set is used as a devel-
opment set. NIST 2004, 2005, 2006, and 2008 sets
are used for evaluation purpose. For word align-
ment, we use GIZA++1, an implementation of IBM
models (Brown et al, 1993). We have implemented
a hierarchical phrase-based SMT model similar to
Chiang (2005). The trigram target language model
is trained from the Xinhua portion of English Gi-
gaword corpus (Graff and Cieri, 2003). Sampled
10,000 sentences from Chinese Gigaword corpus
(Graff, 2007) was used for source-side development
dataset to measure consistency. Our main met-
ric for translation performance evaluation is case-
1http://www.statmt.org/moses/giza/GIZA++.html
293
 0.60
 0.70
 0.80
 0.90
 1.00
Co
ns
ist
en
cy
Freq-Cutoff
NoDep
Dep
CrossNoDep
CrossDep
0.286
0.290
0.294
0.298
0% 10% 20% 30% 40% 50% 60%
BL
EU
Phrase Reduction Ratio
0% 10% 20% 30% 40% 50% 60%
Hierarchical Rule Reduction Ratio
0% 10% 20% 30% 40% 50% 60%
Joint Reduction Ratio
Figure 1: Performance comparison. BLEU scores and consistency scores are averaged over four evaluation sets.
insensitive BLEU-4 scores (Papineni et al, 2002).
As a baseline system, we chose the frequency-
based cutoff method, which is one of the most
widely used filtering methods. As shown in Fig-
ure 1, almost half of the phrases and hierarchical
rules are pruned when cutoff=2, while the BLEU
score is also deteriorated significantly. We intro-
duced two methods for selecting the N pruning
entries considering dependency relationships. The
non-dependency method does not consider depen-
dency relationships, while the dependency method
prunes independent entries first. Each method can be
combined with cross reduction. The performance is
measured in three different reduction tasks: phrase
reduction, hierarchical rule reduction, and joint re-
duction. As the reduction ratio becomes higher,
the model size, i.e., the number of entries, is re-
duced while BLEU scores and coverage are de-
creased. The results show that the translation per-
formance is highly co-related with the consistency.
The co-relation scores measured between them on
the phrase reduction and the hierarchical rule reduc-
tion tasks are 0.99 and 0.95, respectively, which in-
dicates very strong positive relationship.
For the phrase reduction task, the dependency
method outperforms the non-dependency method in
terms of BLEU score. When the cross reduction
technique was used for the phrase reduction task,
BLEU score is not deteriorated even when more than
half of phrase entries are pruned. This result implies
that there is much redundant information stored in
the hierarchical rule table. On the other hand, for the
hierarchical rule reduction task, the non-dependency
method shows the better performance. The depen-
dency method sometimes performs worse than the
baseline method. We expect that this is caused by
the unreliable estimation of dependency among hi-
erarchical rules since the most of them are automat-
ically generated from the phrases. The excessive de-
pendency of these rules would cause overestimation
of hierarchical rule redundancy score.
4 Conclusion
We present a novel method of reducing the size of
translation model for SMT. The contributions of the
proposed method are as follows: 1) our method is
the first attempt to reduce the phrase table and the hi-
erarchical rule table simultaneously. 2) our method
is a safe reduction method since it considers the re-
dundancy, which is the practical ineffectiveness of
individual entry. 3) our method shows that almost
the half size of the translation model can be reduced
without significant performance degradation. It may
be appropriate for the applications running on lim-
ited environment, e.g., mobile devices.
294
Acknowledgement
The first author performed this research during an
internship at Microsoft Research Asia. This research
was supported by the MKE(The Ministry of Knowl-
edge Economy), Korea and Microsoft Research, un-
der IT/SW Creative research program supervised by
the NIPA(National IT Industry Promotion Agency).
(NIPA-2010-C1810-1002-0025)
References
Michael Bloodgood and Chris Callison-Burch. 2010.
Bucking the Trend: Large-Scale Cost-Focused Active
Learning for Statistical Machine Translation. In Pro-
ceedings of the 48th Annual Meeting of the Association
for Computational Linguistics, pages 854?864.
Peter F. Brown, Vincent J. Della Pietra, Stephen A. Della
Pietra, and Robert L. Mercer. 1993. The Mathematics
of Statistical Machine Translation: Parameter Estima-
tion. Computational Linguistics, 19:263?311, June.
David Chiang. 2005. A Hierarchical Phrase-based
Model for Statistical Machine Translation. In Pro-
ceedings of the 43th Annual Meeting on Association
for Computational Linguistics, pages 263?270.
David Chiang. 2007. Hierarchical Phrase-based Transla-
tion. Computational Linguistics, 33:201?228, June.
Lei Cui, Dongdong Zhang, Mu Li, Ming Zhou, and
Tiejun Zhao. 2010. Hybrid Decoding: Decoding with
Partial Hypotheses Combination Over Multiple SMT
Systems. In Proceedings of the 23rd International
Conference on Computational Linguistics: Posters,
COLING ?10, pages 214?222, Stroudsburg, PA, USA.
Association for Computational Linguistics.
David Graff and Christopher Cieri. 2003. English Giga-
word. In Linguistic Data Consortium, Philadelphia.
David Graff. 2007. Chinese Gigaword Third Edition. In
Linguistic Data Consortium, Philadelphia.
Fei Huang and Bing Xiang. 2010. Feature-Rich Discrim-
inative Phrase Rescoring for SMT. In Proceedings of
the 23rd International Conference on Computational
Linguistics, COLING ?10, pages 492?500, Strouds-
burg, PA, USA. Association for Computational Lin-
guistics.
Howard Johnson, Joel Martin, George Foster, and Roland
Kuhn. 2007. Improving Translation Quality by Dis-
carding Most of the Phrasetable. In Proceedings of the
2007 Joint Conference on Empirical Methods in Nat-
ural Language Processing and Computational Natu-
ral Language Learning (EMNLP-CoNLL), pages 967?
975.
Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical Phrase-based Translation. In Pro-
ceedings of the 2003 Conference of the North Ameri-
can Chapter of the Association for Computational Lin-
guistics on Human Language Technology - Volume 1,
NAACL ?03, pages 48?54, Stroudsburg, PA, USA. As-
sociation for Computational Linguistics.
Grzegorz Kondrak, Daniel Marcu, and Kevin Knight.
2003. Cognates can Improve Statistical Translation
Models. In Proceedings of the 2003 Conference of the
North American Chapter of the Association for Com-
putational Linguistics on Human Language Technol-
ogy: companion volume of the Proceedings of HLT-
NAACL 2003?short papers - Volume 2, NAACL-Short
?03, pages 46?48, Stroudsburg, PA, USA. Association
for Computational Linguistics.
Franz Josef Och. 2003. Minimum Error Rate Training
in Statistical Machine Translation. In Proceedings of
the 41st Annual Meeting on Association for Compu-
tational Linguistics - Volume 1, ACL ?03, pages 160?
167, Stroudsburg, PA, USA. Association for Compu-
tational Linguistics.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: a Method for Automatic
Evaluation of Machine Translation. In Proceedings of
the 40th Annual Meeting on Association for Computa-
tional Linguistics, ACL ?02, pages 311?318, Morris-
town, NJ, USA. Association for Computational Lin-
guistics.
Nadi Tomeh, Nicola Cancedda, and Marc Dymetman.
2009. Complexity-based Phrase-Table Filtering for
Statistical Machine Translation.
Mei Yang and Jing Zheng. 2009. Toward Smaller, Faster,
and Better Hierarchical Phrase-based SMT. In Pro-
ceedings of the ACL-IJCNLP 2009 Conference Short
Papers, ACLShort ?09, pages 237?240, Stroudsburg,
PA, USA. Association for Computational Linguistics.
Andreas Zollmann, Ashish Venugopal, Franz Och, and
Jay Ponte. 2008. A Systematic Comparison of Phrase-
based, Hierarchical and Syntax-Augmented Statistical
MT. In Proceedings of the 22nd International Con-
ference on Computational Linguistics (Coling 2008),
pages 1145?1152, troudsburg, PA, USA. Association
for Computational Linguistics.
295
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 752?760,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
Punctuation Prediction with Transition-based Parsing 
Dongdong Zhang1, Shuangzhi Wu2, Nan Yang3, Mu Li1 
 
1Microsoft Research Asia, Beijing, China 
2Harbin Institute of Technology, Harbin, China 
3University of Science and Technology of China, Hefei, China 
{dozhang,v-shuawu,v-nayang,muli}@microsoft.com 
 
Abstract 
Punctuations are not available in automatic 
speech recognition outputs, which could cre-
ate barriers to many subsequent text pro-
cessing tasks. This paper proposes a novel 
method to predict punctuation symbols for the 
stream of words in transcribed speech texts. 
Our method jointly performs parsing and 
punctuation prediction by integrating a rich set 
of syntactic features when processing words 
from left to right. It can exploit a global view 
to capture long-range dependencies for punc-
tuation prediction with linear complexity. The 
experimental results on the test data sets of 
IWSLT and TDT4 show that our method can 
achieve high-level performance in punctuation 
prediction over the stream of words in tran-
scribed speech text. 
1 Introduction 
Standard automatic speech recognizers output un-
structured streams of words. They neither perform 
a proper segmentation of the output into sentences, 
nor predict punctuation symbols. The unavailable 
punctuations and sentence boundaries in tran-
scribed speech texts create barriers to many sub-
sequent processing tasks, such as summarization, 
information extraction, question answering and 
machine translation. Thus, the segmentation of 
long texts is necessary in many real applications. 
For example, in speech-to-speech translation, 
continuously transcribed speech texts need to be 
segmented before being fed into subsequent ma-
chine translation systems (Takezawa et al, 1998; 
Nakamura, 2009). This is because current ma-
chine translation (MT) systems perform the trans-
lation at the sentence level, where various models 
used in MT are trained over segmented sentences 
and many algorithms inside MT have an exponen-
tial complexity with regard to the length of inputs. 
The punctuation prediction problem has at-
tracted research interest in both the speech pro-
cessing community and the natural language pro-
cessing community. Most previous work primar-
ily exploits local features in their statistical mod-
els such as lexicons, prosodic cues and hidden 
event language model (HELM) (Liu et al, 2005; 
Matusov et al, 2006; Huang and Zweig, 2002; 
Stolcke and Shriberg, 1996). The word-level mod-
els integrating local features have narrow views 
about the input and could not achieve satisfied 
performance due to the limited context infor-
mation access (Favre et al, 2008). Naturally, 
global contexts are required to model the punctu-
ation prediction, especially for long-range de-
pendencies. For instance, in English question sen-
tences, the ending question mark is long-range de-
pendent on the initial phrases (Lu and Ng, 2010), 
such as ?could you? in Figure 1. There has been 
some work trying to incorporate syntactic features 
to broaden the view of hypotheses in the punctua-
tion prediction models (Roark et al, 2006; Favre 
et al, 2008). In their methods, the punctuation 
prediction is treated as a separated post-procedure 
of parsing, which may suffer from the problem of 
error propagation. In addition, these approaches 
are not able to incrementally process inputs and 
are not efficient for very long inputs, especially in 
the cases of long transcribed speech texts from 
presentations where the number of streaming 
words could be larger than hundreds or thousands. 
In this paper, we propose jointly performing   
punctuation prediction and transition-based de-
pendency parsing over transcribed speech text. 
When the transition-based parsing consumes the 
stream of words left to right with the shift-reduce 
decoding algorithm, punctuation symbols are pre-
dicted for each word based on the contexts of the 
parsing tree. Two models are proposed to cause 
the punctuation prediction to interact with the 
transition actions in parsing. One is to conduct 
transition actions of parsing followed by punctua-
tion predictions in a cascaded way. The other is to 
associate the conventional transition actions of 
parsing with punctuation perditions, so that pre-
dicted punctuations are directly inferred from the 
752
 
(a). The transcribed speech text without punctuations 
 
  
 
 
 
 
(b). Transition-based parsing trees and predicted punctuations over transcribed text 
 
 
(c). Two segmentations are formed when inserting the predicted punctuation symbols into the transcribed text 
Figure 1. An example of punctuation prediction. 
parsing tree. Our models have linear complexity 
and are capable of handling streams of words with 
any length. In addition, the computation of models 
use a rich set of syntactic features, which can im-
prove the complicated punctuation predictions 
from a global view, especially for the long range 
dependencies.  
Figure 1 shows an example of how parsing 
helps punctuation prediction over the transcribed 
speech text. As illustrated in Figure 1(b), two 
commas are predicted when their preceding words 
act as the adverbial modifiers (advmod) during 
parsing. The period after the word ?menu? is pre-
dicted when the parsing of an adverbial clause 
modifier (advcl) is completed. The question mark 
at the end of the input is determined when a direct 
object modifier (dobj) is identified, together with 
the long range clue that the auxiliary word occurs 
before the nominal subject (nsubj). Eventually, 
two segmentations are formed according to the 
punctuation prediction results, shown in Figure 
1(c).  
The training data used for our models is adapted 
from Treebank data by excluding all punctuations 
but keeping the punctuation contexts, so that it can 
simulate the unavailable annotated transcribed 
speech texts. In decoding, beam search is used to 
get optimal punctuation prediction results. We 
conduct experiments on both IWSLT data and 
TDT4 test data sets. The experimental results 
show that our method can achieve higher perfor-
mance than the CRF-based baseline method. 
The paper is structured as follows: Section 2 
conducts a survey of related work. The transition-
based dependency parsing is introduced in Section 
3. We explain our approach to predicting punctu-
ations for transcribed speech texts in Section 4. 
Section 5 gives the results of our experiment. The 
conclusion and future work are given in Section 6. 
2 Related Work 
Sentence boundary detection and punctuation pre-
diction have been extensively studied in the 
speech processing field and have attracted re-
search interest in the natural language processing 
field as well. Most previous work exploits local 
features for the task. Kim and Woodland (2001), 
Huang and Zweig (2002), Christensen et al 
(2001), and Liu et al (2005) integrate both pro-
sodic features (pitch, pause duration, etc.) and lex-
ical features (words, n-grams, etc.) to predict 
punctuation symbols during speech recognition, 
where Huang and Zweig (2002) uses a maximum 
entropy model, Christensen et al (2001) focus on 
finite state and multi-layer perceptron methods, 
and Liu et al (2005) uses conditional random 
fields. However, in some scenarios the prosodic 
cues are not available due to inaccessible original 
raw speech waveforms. Matusov et al (2006) in-
tegrate segmentation features into the log-linear 
model in the statistical machine translation (SMT) 
framework to improve the translation perfor-
mance when translating transcribed speech texts. 
Lu and Ng (2010) uses dynamic conditional ran-
dom fields to perform both sentence boundary and 
sentence type prediction. They achieved promis-
ing results on both English and Chinese tran-
scribed speech texts. The above work only ex-
anyway you may find your favorite if you go through the menu so could you tell me your choice 
                   anyway you may find your favorite if you go  through the menu so could you tell me your choice 
, N N N N N N N N N N . , N N N N N ? 
anyway, you may find your favorite if you go through the menu. so, could you tell me your choice? 
nsubj nsubj poss 
aux
mark pobj 
iobj 
advmod 
advcl 
nsubj dobj 
det poss aux prep 
advmod dobj 
753
ploits local features, so they were limited to cap-
turing long range dependencies for punctuation 
prediction. 
It is natural to incorporate global knowledge, 
such as syntactic information, to improve punctu-
ation prediction performance. Roark et al (2006) 
use a rich set of non-local features including par-
ser scores to re-rank full segmentations. Favre et 
al. (2008) integrate syntactic information from a 
PCFG parser into a log-linear and combine it with 
local features for sentence segmentation. The 
punctuation prediction in these works is per-
formed as a post-procedure step of parsing, where 
a parse tree needs to be built in advance. As their 
parsing over the stream of words in transcribed 
speech text is exponentially complex, their ap-
proaches are only feasible for short input pro-
cessing. Unlike these works, we incorporate punc-
tuation prediction into the parsing which process 
left to right input without length limitations. 
Numerous dependency parsing algorithms 
have been proposed in the natural language pro-
cessing community, including transition-based 
and graph-based dependency parsing. Compared 
to graph-based parsing, transition-based parsing 
can offer linear time complexity and easily lever-
age non-local features in the models (Yamada and 
Matsumoto, 2003; Nivre et al, 2006b; Zhang and 
Clark, 2008; Huang and Sagae, 2010). Starting 
with the work from (Zhang and Nivre, 2011), in 
this paper we extend transition-based dependency 
parsing from the sentence-level to the stream of 
words and integrate the parsing with punctuation 
prediction.  
Joint POS tagging and transition-based de-
pendency parsing are studied in (Hatori et al, 
2011; Bohnet and Nivre, 2012). The improve-
ments are reported with the joint model compared 
to the pipeline model for Chinese and other richly 
inflected languages, which shows that it also 
makes sense to jointly perform punctuation pre-
diction and parsing, although these two tasks of 
POS tagging and punctuation prediction are dif-
ferent in two ways: 1). The former usually works 
on a well-formed single sentence while the latter 
needs to process multiple sentences that are very 
lengthy. 2). POS tags are must-have features to 
parsing while punctuations are not. The parsing 
quality in the former is more sensitive to the per-
formance of the entire task than in the latter. 
3 Transition-based dependency parsing 
In a typical transition-based dependency parsing 
process, the shift-reduce decoding algorithm is 
applied and a queue and stack are maintained 
(Zhang and Nivre, 2011). The queue stores the 
stream of transcribed speech words, the front of 
which is indexed as the current word. The stack 
stores the unfinished words which may be linked 
with the current word or a future word in the 
queue. When words in the queue are consumed 
from left to right, a set of transition actions is ap-
plied to build a parse tree. There are four kinds of 
transition actions conducted in the parsing process 
(Zhang and Nivre, 2011), as described in Table 1.  
 
Action Description 
Shift Fetches the current word from the 
queue and pushes it to the stack 
Reduce Pops the stack 
LeftArc Adds a dependency link from the cur-
rent word to the stack top, and  pops the 
stack 
RightArc Adds a dependency link from the stack 
top to the current word, takes away the 
current word from the queue and 
pushes it to the stack 
Table 1. Action types in transition-based parsing 
The choice of each transition action during the 
parsing is scored by a linear model that can be 
trained over a rich set of non-local features ex-
tracted from the contexts of the stack, the queue 
and the set of dependency labels. As described in 
(Zhang and Nivre, 2011), the feature templates 
could be defined over the lexicons, POS-tags and 
the combinations with syntactic information. 
In parsing, beam search is performed to search 
the optimal sequence of transition actions, from 
which a parse tree is formed (Zhang and Clark, 
2008). As each word must be pushed to the stack 
once and popped off once, the number of actions 
needed to parse a sentence is always 2n, where n 
is the length of the sentence. Thus, transition-
based parsing has a linear complexity with the 
length of input and naturally it can be extended to 
process the stream of words. 
4 Our method 
4.1 Model 
In the task of punctuation prediction, we are given 
a stream of words from an automatic transcription 
of speech text, denoted by ?1
?: = ?1, ?2, ? , ?? . 
We are asked to output a sequence of punctuation 
symbols ?1
?:= ?1, ?2, ? , ??  where ??  is attached 
to ?? to form a sentence like Figure 1(c). If there 
are no ambiguities, ?1
?  is also abbreviated as ?, 
754
similarly for ?1
? as ?. We model the search of the 
best sequence of predicted punctuation symbols 
?? as: 
 
             ?? = argmaxS?(?1
?|?1
?)                     (1) 
 
We introduce the transition-based parsing tree 
? to guide the punctuation prediction in Model (2), 
where parsing trees are constructed over the tran-
scribed text while containing no punctuations. 
  
?? = argmax?? ?(?|?1
?) ? ?(?1
?|?, ?1
?)?     (2) 
 
Rather than enumerate all possible parsing trees, 
we jointly optimize the punctuation prediction 
model and the transition-based parsing model 
with the form:  
 
(??, ??) = argmax(?,?)?(?|?1
?) ?
                                                ?(?1
?|?, ?1
?)           (3) 
 
Let ?1
? be the constructed partial tree when ?1
?  
is consumed from the queue. We decompose the 
Model (3) into:  
 
(??, ??) =
argmax(?,?)? ?(?1
?|?1
??1, ?1
?) ? ?(??|?1
?, ?1
?)??=1                                              
(4) 
 
It is noted that a partial parsing tree uniquely 
corresponds to a sequence of transition actions, 
and vice versa. Suppose ?1
? corresponds to the ac-
tion sequence ?1
?  and let ?? denote the last action 
in ?1
? . As the current word ??  can only be con-
sumed from the queue by either Shift or RightArc 
according to Table 1, we have ?? ?
{?????, ????????} . Thus, we synchronize the 
punctuation prediction with the application of 
Shift and RightArc during the parsing, which is ex-
plained by Model (5).  
 
(??, ??) = argmax(?,?)? ?(?1
? , ?1
? |?1
??1, ?1
?)
?
?=1
? ?(??|?? , ?1
? , ?1
?) 
                                                                      (5) 
 
The model is further refined by reducing the 
computation scope. When a full-stop punctuation 
is determined (i.e., a segmentation is formed), we 
discard the previous contexts and restart a new 
                                                          
1 Specially, ?? is equal to 1 if there are no previous full-stop 
punctuations. 
procedure for both parsing and punctuation pre-
diction over the rest of words in the stream. In this 
way we are theoretically able to handle the unlim-
ited stream of words without needing to always 
keep the entire context history of streaming words. 
Let ?? be the position index of last full-stop punc-
tuation1 before ?, ???
?  and ???
? the partial tree and 
corresponding action sequence over the words 
???
? , Model (5) can be rewritten by: 
 
(??, ??) =
argmax(?,?)? ?(???
? , ???
? |???
??1, ???
? ) ???=1
                                ?(??|?? , ???
? , ???
? )                     (6) 
 
With different computation of Model (6), we 
induce two joint models for punctuation predic-
tion: the cascaded punctuation prediction model 
and the unified punctuation prediction model.  
4.2 Cascaded punctuation prediction model 
(CPP) 
In Model (6), the computation of two sub-models 
is independent. The first sub-model is computed 
based on the context of words and partial trees 
without any punctuation knowledge, while the 
computation of the second sub-model is condi-
tional on the context from the partially built pars-
ing tree ???
?  and the transition action. As the words 
in the stream are consumed, each computation of 
transition actions is followed by a computation of 
punctuation prediction. Thus, the two sub-models 
are computed in a cascaded way, until the optimal 
parsing tree and optimal punctuation symbols are 
generated. We call this model the cascaded punc-
tuation prediction model (CPP). 
4.3 Unified punctuation prediction model 
(UPP) 
In Model (6), if the punctuation symbols can be 
deterministically inferred from the partial tree, 
?(??|??, ???
? , ???
? ) can be omitted because it is al-
ways 1. Similar to the idea of joint POS tagging 
and parsing (Hatori et al, 2011; Bohnet and Nivre, 
2012), we propose attaching the punctuation pre-
diction onto the parsing tree by embedding ?? into 
?? . Thus, we extend the conventional transition 
actions illustrated in Table 1 to a new set of tran-
sition actions for the parsing, denoted by ???: 
 
755
??? = {???????, ??????} ? {?????(?)|? ? ?}
? {????????(?)|? ? ?} 
 
where Q is the set of punctuation symbols to be 
predicted, ? is a punctuation symbol belonging to 
Q, Shift(s) is an action that attaches s to the current 
word on the basis of original Shift action in pars-
ing, RightArc(s) attaches ? to the current word on 
the basis of original RightArc action. 
With the redefined transition action set ???, the 
computation of Model (6) is reformulated as:  
  
(??, ??) =
argmax(?,?)? ? (???
? , ???
??
?
|???
??1, ?????
??1
, ???
? )??=1        (7) 
 
Here, the computation of parsing tree and punc-
tuation prediction is unified into one model where 
the sequence of transition action outputs uniquely 
determines the punctuations attached to the words. 
We refer to it as the unified punctuation predic-
tion model (UPP). 
 
 
 
 
 
 
 
 
(a). Parsing tree and attached punctuation symbols 
 
Shift(,), Shift(N), Shift(N), LeftArc, LeftArc, LeftArc, 
Shift(N), RightArc(?), Reduce, Reduce 
 
(b). The corresponding sequence of transition actions 
Figure 2. An example of punctuation prediction 
using the UPP model, where N is a null type punc-
tuation symbol denoting no need to attach any 
punctuation to the word. 
Figure 2 illustrates an example how the UPP 
model works. Given an input ?so could you tell 
me?, the optimal sequence of transition actions in 
Figure 2(b) is calculated based on the UPP model 
to produce the parsing tree in Figure 2(a). Accord-
ing to the sequence of actions, we can determine 
the sequence of predicted punctuation symbols 
like ?,NNN?? that have been attached to the words 
shown in Figure 2(a). The final segmentation with 
the predicted punctuation insertion could be ?so, 
could you tell me??. 
4.4 Model training and decoding 
In practice, the sub-models in Model (6) and (7) 
with the form of ?(?|?) is computed with a linear 
model ?????(?, ?) as 
 
?????(?, ?) = ?(?, ?) ? ? 
 
where ?(?, ?)  is the feature vector extracted 
from the output ? and the context ?, and ? is the 
weight vector. For the features of the models, we 
incorporate the bag of words and POS tags as well 
as tree-based features shown in Table 2, which are 
the same as those defined in (Zhang and Nivre, 
2011).  
 
(a) ws; w0; w1; w2; ps; p0; p1; p2; wsps; w0p0; w1p1; 
w2p2; wspsw0p0; wspsw0; wspsp0; wsw0p0; 
psw0p0; wsw0; psp0; p0p1; psp0p1; p0p1p2; 
(b) pshpsp0; pspslp0; pspsrp0; psp0p0l; wsd; psd; w0d; 
p0d; wsw0d; psp0d; wsvl; psvl; wsvr; psvr; w0vl; 
p0vl; wsh; psh; ts; w0l; p0l; t0l; w0r; p0r; t0r; w1l; 
p1l; t1l; wsh2; psh2; tsh; wsl2; psl2; tsl2; wsr2; psr2; 
tsr2; w0l2; p0l2; t0l2; pspslpsl2; pspsrpsr2; pspshpsh2; 
p0p0lp0l2; wsTl; psTl; wsTr; psTr; w0Tl; p0Tl; 
Table 2. (a) Features of the bag of words and POS 
tags. (b). Tree-based features. w?word; p?POS 
tag; d?distance between ws and w0; v?number of 
modifiers; t?dependency label; T?set of depend-
ency labels; s, 0, 1 and 2 index the stack top and 
three front items in the queue respectively; h?head; 
l?left/leftmost; r?right/rightmost; h2?head of a 
head; l2?second leftmost; r2?second rightmost. 
The training data for both the CPP and UPP 
models need to contain parsing trees and punctu-
ation information. Due to the absence of annota-
tion over transcribed speech data, we adapt the 
Treebank data for the purpose of model training. 
To do this, we remove all types of syntactic infor-
mation related to punctuation symbols from the 
raw Treebank data, but record what punctuation 
symbols are attached to the words. We normalize 
various punctuation symbols into two types: Mid-
dle-paused punctuation (M) and Full-stop punctu-
ation (F). Plus null type (N), there are three kinds 
of punctuation symbols attached to the words. Ta-
ble 3 illustrates the normalizations of punctuation 
symbols. In the experiments, we did not further 
distinguish the type among full-stop punctuation 
because the question mark and the exclamation 
mark have very low frequency in Treebank data. 
so could you tell me 
, N N N ? 
nsubj iobj 
aux
advmod 
756
But our CPP and UPP models are both independ-
ent regarding the number of punctuation types to 
be predicted. 
 
Punctuations Normalization 
Period, question mark, 
exclamation mark 
Full-stop punctuation 
(F) 
Comma, Colon, semi-
colon 
Middle-paused punctu-
ation (M) 
Multiple Punctuations 
(e.g., !!!!?) 
Full-stop punctuation 
(F) 
Quotations, brackets, 
etc. 
Null (N) 
Table 3. Punctuation normalization in training 
data 
As the feature templates are the same for the 
model training of both CPP and UPP, the training 
instances of CPP and UPP have the same contexts 
but with different outputs. Similar to work in 
(Zhang and Clark, 2008; Zhang and Nivre, 2011), 
we train CPP and UPP by generalized perceptron 
(Collins, 2002).  
In decoding, beam search is performed to get 
the optimal sequence of transition actions in CPP 
and UPP, and the optimal punctuation symbols in 
CPP. To ensure each segment decided by a full-
stop punctuation corresponds to a single parsing 
tree, two constraints are applied in decoding for 
the pruning of deficient search paths. 
(1) Proceeding-constraint: If the partial pars-
ing result is not a single tree, the full-stop 
punctuation prediction in CPP cannot be 
performed. In UPP, if Shift(F) or 
RightArc(F) fail to result in a single parsing 
tree, they cannot be performed as well. 
(2) Succeeding-constraint: If the full-stop 
punctuation is predicted in CPP, or Shift(F) 
and RightArc(F) are performed in UPP, the 
following transition actions must be a se-
quence of Reduce actions until the stack 
becomes empty. 
5 Experiments 
5.1 Experimental setup 
Our training data of transition-based dependency 
trees are converted from phrasal structure trees in 
English Web Treebank (LDC2012T13) and the 
English portion of OntoNotes 4.0 (LDC2011T03) 
by the Stanford Conversion toolkit (Marneffe et 
al., 2006). It contains around 1.5M words in total 
and consist of various genres including weblogs, 
web texts, newsgroups, email, reviews, question-
answer sessions, newswires, broadcast news and 
broadcast conversations. To simulate the tran-
scribed speech text, all words in dependency trees 
are lowercased and punctuations are excluded be-
fore model training. In addition, every ten depend-
ency trees are concatenated sequentially to simu-
late a parsing result of a stream of words in the 
model training. 
There are two test data sets used in our experi-
ments. One is the English corpus of the IWSLT09 
evaluation campaign (Paul, 2009) that is the con-
versional speech text. The other is a subset of the 
TDT4 English data (LDC2005T16) which con-
sists of 200 hours of closed-captioned broadcast 
news.  
In the decoding, the beam size of both the tran-
sition-based parsing and punctuation prediction is 
set to 5. The part-of-speech tagger is our re-imple-
mentation of the work in (Collins, 2002).  
The evaluation metrics of our experiments are 
precision (prec.), recall (rec.) and F1-measure 
(F1). 
For the comparison, we also implement a base-
line method based on the CRF model. It incorpo-
rates the features of bag of words and POS tags 
shown in Table 2(a), which are commonly used in 
previous related work.  
5.2 Experimental results 
We test the performance of our method on both 
the correctly recognized texts and automatically 
recognized texts. The former data is used to eval-
uate the capability of punctuation prediction of 
our algorithm regardless of the noises from speech 
data, as our model training data come from formal 
text instead of transcribed speech data. The usage 
of the latter test data set aims to evaluate the ef-
fectiveness of our method in real applications 
where lots of substantial recognition errors could 
be contained. In addition, we also evaluate the 
quality of our transition-based parsing, as its per-
formance could have a big influence on the quality 
of punctuation prediction. 
5.2.1 Performance on correctly recognized 
text 
The evaluation of our method on correctly recog-
nized text uses 10% of IWSLT09 training set, 
which consists of 19,972 sentences from BTEC 
(Basic Travel Expression Corpus) and 10,061 sen-
tences from CT (Challenge Task). The average in-
put length is about 10 words and each input con-
tains 1.3 sentences on average. The evaluation re-
sults are presented in Table 4.  
757
  Measure   Middle-
Paused 
Full-stop Mixed 
Baseline 
(CRF) 
prec. 33.2% 81.5% 78.8% 
rec. 25.9% 83.8% 80.7% 
F1 29.1% 82.6% 79.8% 
 
CPP 
prec. 51% 89% 89.6% 
rec. 50.3% 93.1% 92.7% 
F1 50.6% 91% 91.1% 
 
UPP 
 
prec. 52.6% 93.2% 92% 
rec. 59.7% 91.3% 92.3% 
F1 55.9% 92.2% 92.2% 
Table 4. Punctuation prediction performance on 
correctly recognized text 
   We achieved good performance on full-stop 
punctuation compared to the baseline, which 
shows our method can efficiently process sen-
tence segmentation because each segment is de-
cided by the structure of a single parsing tree. In 
addition, the global syntactic knowledge used in 
our work help capture long range dependencies of 
punctuations. The performance of middle-paused 
punctuation prediction is fairly low between all 
methods, which shows predicting middle-paused 
punctuations is a difficult task. This is because the 
usage of middle-paused punctuations is very flex-
ible, especially in conversional data. The last col-
umn in Table 4 presents the performance of the 
pure segmentation task where the middle-paused 
and full-stop punctuations are mixed and not dis-
tinguished. The performance of our method is 
much higher than that of the baseline, which 
shows our method is good at segmentation. We 
also note that UPP yields slightly better perfor-
mance than CPP on full-stop and mixed punctua-
tion prediction, and much better performance on 
middle-paused punctuation prediction. This could 
be because the interaction of parsing and punctu-
ation prediction is closer together in UPP than in 
CPP. 
5.2.2 Performance on automatically recog-
nized text 
Table 5 shows the experimental results of punctu-
ation prediction on automatically recognized text 
from TDT4 data that is recognized using SRI?s 
English broadcast news ASR system where the 
word error rate is estimated to be 18%. As the an-
notation of middle-paused punctuations in TDT4 
is not available, we can only evaluate the perfor-
mance of full-stop punctuation prediction (i.e., de-
tecting sentence boundaries). Thus, we merge 
every three sentences into one single input before 
performing full-stop prediction. The average input 
length is about 43 words. 
 
 Measure   Full-stop 
Baseline 
(CRF) 
prec. 37.7% 
rec. 60.7% 
F1 46.5% 
 
CPP 
prec. 63% 
rec. 58.6% 
F1 60.2% 
 
UPP 
 
prec. 73.9% 
rec. 51.6% 
F1 60.7% 
Table 5. Punctuation prediction performance on 
automatically recognized text 
Generally, the performance shown in Table 5 is 
not as high as that in Table 4. This is because the 
speech recognition error from ASR systems de-
grades the capability of model prediction. Another 
reason might be that the domain and style of our 
training data mismatch those of TDT4 data. The 
baseline gets a little higher recall than our method, 
which shows the baseline method tends to make 
aggressive segmentation decisions. However, 
both precision and F1 score of our method are 
much higher than the baseline. CPP has higher re-
call than UPP, but with lower precision and F1 
score. This is in line with Table 4, which consist-
ently illustrates CPP can get higher recall on full-
stop punctuation prediction for both correctly rec-
ognized and automatically recognized texts.  
5.2.3 Performance of transition-based pars-
ing 
Performance of parsing affects the quality of 
punctuation prediction in our work. In this section, 
we separately evaluate the performance of our 
transition-based parser over various domains in-
cluding the Wall Street Journal (WSJ), weblogs, 
newsgroups, answers, email messages and re-
views. We divided annotated Treebank data into 
three data sets: 90% for model training, 5% for the 
development set and 5% for the test set. The accu-
racy of our POS-tagger achieves 96.71%. The 
beam size in the decoding of both our POS-tag-
ging and parsing is set to 5. Table 6 presents the 
results of our experiments on the measures of 
UAS and LAS, where the overall accuracy is ob-
tained from a general model which is trained over 
the combination of the training data from all do-
mains.  
758
We first evaluate the performance of our transi-
tion-based parsing over texts containing punctua-
tions (TCP). The evaluation results show that our 
transition-based parser achieves state-of-the-art 
performance levels, referring to the best depend-
ency parsing results reported in the shared task of 
SANCL 2012 workshop2, although they cannot be 
compared directly due to the different training 
data and test data sets used in the experiments. 
Secondly, we evaluate our parsing model in CPP 
over the texts without punctuations (TOP). Sur-
prisingly, the performance over TOP is better than 
that over TCP. The reason could be that we 
cleaned out data noises caused by punctuations 
when preparing TOP data. These results illustrate 
that the performance of transition-based parsing in 
our method does not degrade after being inte-
grated with punctuation prediction. As a by-prod-
uct of the punctuation prediction task, the outputs 
of parsing trees can benefit the subsequent text 
processing tasks. 
 
 Data sets UAS LAS 
 
 
Texts con-
taining punc-
tuations 
(TCP) 
 
WSJ 92.6% 90.3% 
Weblogs 90.7% 88.2% 
Answers 89.4% 85.7% 
Newsgroups 90.1% 87.6% 
Reviews 90.9% 88.4% 
Email Messages 89.6% 87.1% 
Overall 90.5% 88% 
 
 
Texts with-
out punctua-
tions (TOP) 
WSJ 92.6% 91.1% 
Weblogs 92.5% 91.1% 
Answers 95% 94% 
Newsgroups 92.6% 91.2% 
Reviews 92.6% 91.2% 
Email Messages 92.9% 91.7% 
Overall 92.6% 91.2% 
Table 6. The performance of our transition-based 
parser on written texts. UAS=unlabeled attach-
ment score; LAS=labeled attachment score 
6 Conclusion and Future Work  
In this paper, we proposed a novel method for 
punctuation prediction of transcribed speech texts. 
Our approach jointly performs parsing and punc-
tuation prediction by integrating a rich set of syn-
tactic features. It can not only yield parse trees, but 
also determine sentence boundaries and predict 
punctuation symbols from a global view of the in-
                                                          
2 https://sites.google.com/site/sancl2012/home/shared-
task/results 
puts. The proposed algorithm has linear complex-
ity in the size of input, which can efficiently pro-
cess the stream of words from a purely text pro-
cessing perspective without the dependences on 
either the ASR systems or subsequent tasks. The 
experimental results show that our approach out-
performs the CRF-based method on both the cor-
rectly recognized and automatically recognized 
texts. In addition, the performance of the parsing 
over the stream of transcribed words is state-of-
the-art, which can benefit many subsequent text 
processing tasks. 
    In future work, we will try our method on other 
languages such as Chinese and Japanese, where 
Treebank data is available. We would also like to 
test the MT performance over transcribed speech 
texts with punctuation symbols inserted based on 
our method proposed in this paper.  
References 
B. Bohnet and J. Nivre. 2012. A transition-based sys-
tem for joint part-of-speech tagging and labeled 
non-projective dependency parsing. In Proc. 
EMNLP-CoNLL 2012. 
H. Christensen, Y. Gotoh, and S. Renals. 2001. Punc-
tuation annotation using statistical prosody models. 
In Proc. of ISCA Workshop on Prosody in Speech 
Recognition and Understanding. 
M. Collins. 2002. Discriminative training methods for 
hidden Markov models: Theory and experiments 
with perceptron algorithms. In Proc. EMNLP?02, 
pages 1-8. 
B. Favre, R. Grishman, D. Hillard, H. Ji, D. Hakkani-
Tur, and M. Ostendorf. 2008. Punctuating speech 
for information extraction. In Proc. of ICASSP?08. 
B. Favre, D. HakkaniTur, S. Petrov and D. Klein. 2008. 
Efficient sentence segmentation using syntactic fea-
tures. In Spoken Language Technologies (SLT). 
A. Gravano, M. Jansche, and M. Bacchiani. 2009. Re-
storing punctuation and capitalization in transcribed 
speech. In Proc. of ICASSP?09. 
J. Hatori, T. Matsuzaki, Y. Miyao and J. Tsujii. 2011. 
Incremental joint POS tagging and dependency 
parsing in Chinese. In Proc. Of IJCNLP?11. 
J. Huang and G. Zweig. 2002. Maximum entropy 
model for punctuation annotation from speech. In 
Proc. Of ICSLP?02. 
759
J.H. Kim and P.C. Woodland. 2001. The use of pros-
ody in a combined system for punctuation genera-
tion and speech recognition. In Proc. of Eu-
roSpeech?01. 
Y. Liu, A. Stolcke, E. Shriberg, and M. Harper. 2005. 
Using conditional random fields for sentence 
boundary detection in speech. In Proc. of ACL?05. 
W. Lu and H.T. Ng. 2010. Better Punctuation Predic-
tion with Dynamic Conditional Random Fields. In 
Proc. Of EMNLP?10. Pages 177-186. 
M. Marneffe, B. MacCartney, C.D. Maning. 2006. 
Generating Typed Dependency Parses from Phrase 
Structure Parses. In Proc. LREC?06. 
E. Matusov, A. Mauser, and H. Ney. 2006. Automatic 
sentence segmentation and punctuation prediction 
for spoken language translation. In Proc. of 
IWSLT?06. 
S. Nakamura. 2009. Overcoming the language barrier 
with speech translation technology. In Science & 
Technology Trends - Quarterly Review. No. 31. 
April 2009. 
J. Nivre. 2003. An efficient algorithm for projective de-
pendency parsing. In Proceedings of IWPT, pages 
149?160, Nancy, France. 
J. Nivre and M. Scholz. 2004. Deterministic depend-
ency parsing of English text. In Proc. COLING?04. 
M. Paul. 2009. Overview of the IWSLT 2009 Evalua-
tion Campaign. In Proceedings of IWSLT?09. 
B. Roark, Y. Liu, M. Harper, R. Stewart, M. Lease, M. 
Snover, I. Shafran, B. Dorr, J. Hale, A. Krasnyan-
skaya, and L. Yung. 2006. Reranking for sentence 
boundary detection in conversational speech. In 
Proc. ICASSP, 2006. 
A. Stolcke and E. Shriberg, ?Automatic linguistic seg-
mentation of conversational speech,? Proc. ICSLP, 
vol. 2, 1996. 
A. Stolcke, E. Shriberg, R. Bates, M. Ostendorf, D. 
Hakkani, M. Plauche, G. Tur, and Y. Lu. 1998. Au-
tomatic detection of sentence boundaries and disflu-
encies based on recognized words. In Proc. of 
ICSLP? 98. 
Takezawa, T. Morimoto, T. Sagisaka, Y. Campbell, N. 
Iida, H. Sugaya, F. Yokoo, A. Yamamoto, Seiichi. 
1998. A Japanese-to-English speech translation sys-
tem: ATR-MATRIX.  In Proc. ICSLP?98. 
Y. Zhang and J. Nivre. 2011. Transition-based De-
pendency Parsing with Rich Non-local Features. In 
Proc. of ACL?11, pages 188-193. 
Y. Zhang and S. Clark. A Tale of Two Parsers: inves-
tigating and combing graph-based and transition-
based dependency parsing using beam-search. 2008. 
In Proc. of EMNLP?08, pages 562-571. 
760
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 340?345,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
Bilingual Data Cleaning for SMT using Graph-based Random Walk?
Lei Cui?, Dongdong Zhang?, Shujie Liu?, Mu Li?, and Ming Zhou?
?School of Computer Science and Technology
Harbin Institute of Technology, Harbin, China
leicui@hit.edu.cn
?Microsoft Research Asia, Beijing, China
{dozhang,shujliu,muli,mingzhou}@microsoft.com
Abstract
The quality of bilingual data is a key factor
in Statistical Machine Translation (SMT).
Low-quality bilingual data tends to pro-
duce incorrect translation knowledge and
also degrades translation modeling per-
formance. Previous work often used su-
pervised learning methods to filter low-
quality data, but a fair amount of human
labeled examples are needed which are
not easy to obtain. To reduce the re-
liance on labeled examples, we propose
an unsupervised method to clean bilin-
gual data. The method leverages the mu-
tual reinforcement between the sentence
pairs and the extracted phrase pairs, based
on the observation that better sentence
pairs often lead to better phrase extraction
and vice versa. End-to-end experiments
show that the proposed method substan-
tially improves the performance in large-
scale Chinese-to-English translation tasks.
1 Introduction
Statistical machine translation (SMT) depends on
the amount of bilingual data and its quality. In
real-world SMT systems, bilingual data is often
mined from the web where low-quality data is in-
evitable. The low-quality bilingual data degrades
the quality of word alignment and leads to the in-
correct phrase pairs, which will hurt the transla-
tion performance of phrase-based SMT systems
(Koehn et al, 2003; Och and Ney, 2004). There-
fore, it is very important to exploit data quality in-
formation to improve the translation modeling.
Previous work on bilingual data cleaning often
involves some supervised learning methods. Sev-
eral bilingual data mining systems (Resnik and
?This work has been done while the first author was visit-
ing Microsoft Research Asia.
Smith, 2003; Shi et al, 2006; Munteanu and
Marcu, 2005; Jiang et al, 2009) have a post-
processing step for data cleaning. Maximum en-
tropy or SVM based classifiers are built to filter
some non-parallel data or partial-parallel data. Al-
though these methods can filter some low-quality
bilingual data, they need sufficient human labeled
training instances to build the model, which may
not be easy to acquire.
To this end, we propose an unsupervised ap-
proach to clean the bilingual data. It is intuitive
that high-quality parallel data tends to produce
better phrase pairs than low-quality data. Mean-
while, it is also observed that the phrase pairs that
appear frequently in the bilingual corpus are more
reliable than less frequent ones because they are
more reusable, hence most good sentence pairs are
prone to contain more frequent phrase pairs (Fos-
ter et al, 2006; Wuebker et al, 2010). This kind of
mutual reinforcement fits well into the framework
of graph-based random walk. When a phrase pair
p is extracted from a sentence pair s, s is consid-
ered casting a vote for p. The higher the number
of votes a phrase pair has, the more reliable of the
phrase pair. Similarly, the quality of the sentence
pair s is determined by the number of votes casted
by the extracted phrase pairs from s.
In this paper, a PageRank-style random walk al-
gorithm (Brin and Page, 1998; Mihalcea and Ta-
rau, 2004; Wan et al, 2007) is conducted to itera-
tively compute the importance score of each sen-
tence pair that indicates its quality: the higher the
better. Unlike other data filtering methods, our
proposed method utilizes the importance scores
of sentence pairs as fractional counts to calculate
the phrase translation probabilities based on Maxi-
mum Likelihood Estimation (MLE), thereby none
of the bilingual data is filtered out. Experimen-
tal results show that our proposed approach sub-
stantially improves the performance in large-scale
Chinese-to-English translation tasks.
340
2 The Proposed Approach
2.1 Graph-based random walk
Graph-based random walk is a general algorithm
to approximate the importance of a vertex within
the graph in a global view. In our method, the ver-
tices denote the sentence pairs and phrase pairs.
The importance of each vertex is propagated to
other vertices along the edges. Depending on dif-
ferent scenarios, the graph can take directed or
undirected, weighted or un-weighted forms. Start-
ing from the initial scores assigned in the graph,
the algorithm is applied to recursively compute the
importance scores of vertices until it converges, or
the difference between two consecutive iterations
falls below a pre-defined threshold.
2.2 Graph construction
Given the sentence pairs that are word-aligned
automatically, an undirected, weighted bipartite
graph is constructed which maps the sentence
pairs and the extracted phrase pairs to the ver-
tices. An edge between a sentence pair vertex and
a phrase pair vertex is added if the phrase pair can
be extracted from the sentence pair. Mutual re-
inforcement scores are defined on edges, through
which the importance scores are propagated be-
tween vertices. Figure 1 illustrates the graph struc-
ture. Formally, the bipartite graph is defined as:
G = (V,E)
where V = S ? P is the vertex set, S = {si|1 ?
i ? n} is the set of all sentence pairs. P =
{pj |1 ? j ? m} is the set of all phrase pairs
which are extracted from S based on the word
alignment. E is the edge set in which the edges
are between S and P , thereby E = {?si, pj?|si ?
S, pj ? P, ?(si, pj) = 1}.
?(si, pj) =
{
1 if pj can be extracted from si
0 otherwise
2.3 Graph parameters
For sentence-phrase mutual reinforcement, a non-
negative score r(si, pj) is defined using the stan-
dard TF-IDF formula:
r(si, pj) =
{ PF (si,pj)?IPF (pj)?
p??{p|?(si,p)=1} PF (si,p
?)?IPF (p?) if ?(si, pj) = 1
0 otherwise
Sentence Pair Vertices
Phrase Pair Vertices
s1
s2
s3
p1
p3
p4
p5
p6
p2
Figure 1: The circular nodes stand for S and
square nodes stand for P . The lines capture the
sentence-phrase mutual reinforcement.
where PF (si, pj) is the phrase pair frequency in
a sentence pair and IPF (pj) is the inverse phrase
pair frequency of pj in the whole bilingual corpus.
r(si, pj) is abbreviated as rij .
Inspired by (Brin and Page, 1998; Mihalcea
and Tarau, 2004; Wan et al, 2007), we com-
pute the importance scores of sentence pairs and
phrase pairs using a PageRank-style algorithm.
The weights rij are leveraged to reflect the rela-
tionships between two types of vertices. Let u(si)
and v(pj) denote the scores of a sentence pair ver-
tex and a phrase pair vertex. They are computed
iteratively by:
u(si) = (1?d)+d?
?
j?N(si)
rij?
k?M(pj) rkj
v(pj)
v(pj) = (1?d) +d?
?
j?M(pj)
rij?
k?N(si) rik
u(si)
where d is empirically set to the default value 0.85
that is same as the original PageRank, N(si) =
{j|?si, pj? ? E}, M(pj) = {i|?si, pj? ? E}.
The detailed process is illustrated in Algorithm 1.
Algorithm 1 iteratively updates the scores of sen-
tence pairs and phrase pairs (lines 10-26). The
computation ends when difference between two
consecutive iterations is lower than a pre-defined
threshold ? (10?12 in this study).
2.4 Parallelization
When the random walk runs on some large bilin-
gual corpora, even filtering phrase pairs that ap-
pear only once would still require several days of
CPU time for a number of iterations. To over-
come this problem, we use a distributed algorithm
341
Algorithm 1 Modified Random Walk
1: for all i ? {0 . . . |S| ? 1} do
2: u(si)(0) ? 1
3: end for
4: for all j ? {0 . . . |P | ? 1} do
5: v(pj)(0) ? 1
6: end for
7: ? ? Infinity
8: ? threshold
9: n? 1
10: while ? >  do
11: for all i ? {0 . . . |S| ? 1} do
12: F (si)? 0
13: for all j ? N(si) do
14: F (si)? F (si) + rij?
k?M(pj) rkj
? v(pj)(n?1)
15: end for
16: u(si)(n) ? (1? d) + d ? F (si)
17: end for
18: for all j ? {0 . . . |P | ? 1} do
19: G(pj)? 0
20: for all i ?M(pj) do
21: G(pj)? G(pj) + rij?
k?N(si) rik
? u(si)(n?1)
22: end for
23: v(pj)(n) ? (1? d) + d ?G(pj)
24: end for
25: ? ? max(4u(si)||S|?1i=1 ,4v(pj)||P |?1j=1 )26: n? n+ 1
27: end while
28: return u(si)(n)||S|?1i=0
based on the iterative computation in the Sec-
tion 2.3. Before the iterative computation starts,
the sum of the outlink weights for each vertex
is computed first. The edges are randomly par-
titioned into sets of roughly equal size. Each
edge ?si, pj? can generate two key-value pairs
in the format ?si, rij? and ?pj , rij?. The pairs
with the same key are summed locally and ac-
cumulated across different machines. Then, in
each iteration, the score of each vertex is up-
dated according to the sum of the normalized
inlink weights. The key-value pairs are gener-
ated in the format ?si, rij?
k?M(pj) rkj
? v(pj)? and
?pj , rij?
k?N(si) rik
? u(si)?. These key-value pairs
are also randomly partitioned and summed across
different machines. Since long sentence pairs usu-
ally extract more phrase pairs, we need to normal-
ize the importance scores based on the sentence
length. The algorithm fits well into the MapRe-
duce programming model (Dean and Ghemawat,
2008) and we use it as our implementation.
2.5 Integration into translation modeling
After sufficient number of iterations, the impor-
tance scores of sentence pairs (i.e., u(si)) are ob-
tained. Instead of simple filtering, we use the
scores of sentence pairs as the fractional counts to
re-estimate the translation probabilities of phrase
pairs. Given a phrase pair p = ?f? , e??, A(f?) and
B(e?) indicate the sets of sentences that f? and e?
appear. Then the translation probability is defined
as:
PCW(f? |e?) =
?
i?A(f?)?B(e?) u(si)? ci(f? , e?)?
j?B(e?) u(sj)? cj(e?)
where ci(?) denotes the count of the phrase or
phrase pair in si. PCW(f? |e?) and PCW(e?|f?) are
named as Corpus Weighting (CW) based transla-
tion probability, which are integrated into the log-
linear model in addition to the conventional phrase
translation probabilities (Koehn et al, 2003).
3 Experiments
3.1 Setup
We evaluated our bilingual data cleaning ap-
proach on large-scale Chinese-to-English machine
translation tasks. The bilingual data we used
was mainly mined from the web (Jiang et al,
2009)1, as well as the United Nations parallel cor-
pus released by LDC and the parallel corpus re-
leased by China Workshop on Machine Transla-
tion (CWMT), which contain around 30 million
sentence pairs in total after removing duplicated
ones. The development data and testing data is
shown in Table 1.
Data Set #Sentences Source
NIST 2003 (dev) 919 open test
NIST 2005 (test) 1,082 open test
NIST 2006 (test) 1,664 open test
NIST 2008 (test) 1,357 open test
CWMT 2008 (test) 1,006 open test
In-house dataset 1 (test) 1,002 web data
In-house dataset 2 (test) 5,000 web data
In-house dataset 3 (test) 2,999 web data
Table 1: Development and testing data used in the
experiments.
A phrase-based decoder was implemented
based on inversion transduction grammar (Wu,
1997). The performance of this decoder is simi-
lar to the state-of-the-art phrase-based decoder in
Moses, but the implementation is more straight-
forward. We use the following feature functions
in the log-linear model:
1Although supervised data cleaning has been done in the
post-processing, the corpus still contains a fair amount of
noisy data based on our random sampling.
342
dev NIST 2005 NIST 2006 NIST 2008 CWMT 2008 IH 1 IH 2 IH 3
baseline 41.24 37.34 35.20 29.38 31.14 24.29 22.61 24.19
(Wuebker et al, 2010) 41.20 37.48 35.30 29.33 31.10 24.33 22.52 24.18
-0.25M 41.28 37.62 35.31 29.70 31.40 24.52 22.69 24.64
-0.5M 41.45 37.71 35.52 29.76 31.77 24.64 22.68 24.69
-1M 41.28 37.41 35.28 29.65 31.73 24.23 23.06 24.20
+CW 41.75 38.08 35.84 30.03 31.82 25.23 23.18 24.80
Table 2: BLEU(%) of Chinese-to-English translation tasks on multiple testing datasets (p < 0.05), where
?-numberM? denotes we simply filter number million low scored sentence pairs from the bilingual data
and use others to extract the phrase table. ?CW? means the corpus weighting feature, which incorporates
sentence scores from random walk as fractional counts to re-estimate the phrase translation probabilities.
? phrase translation probabilities and lexical
weights in both directions (4 features);
? 5-gram language model with Kneser-Ney
smoothing (1 feature);
? lexicalized reordering model (1 feature);
? phrase count and word count (2 features).
The translation model was trained over the
word-aligned bilingual corpus conducted by
GIZA++ (Och and Ney, 2003) in both directions,
and the diag-grow-final heuristic was used to re-
fine the symmetric word alignment. The language
model was trained on the LDC English Gigaword
Version 4.0 plus the English part of the bilingual
corpus. The lexicalized reordering model (Xiong
et al, 2006) was trained over the 40% randomly
sampled sentence pairs from our parallel data.
Case-insensitive BLEU4 (Papineni et al, 2002)
was used as the evaluation metric. The parame-
ters of the log-linear model are tuned by optimiz-
ing BLEU on the development data using MERT
(Och, 2003). Statistical significance test was per-
formed using the bootstrap re-sampling method
proposed by Koehn (2004).
3.2 Baseline
The experimental results are shown in Table 2. In
the baseline system, the phrase pairs that appear
only once in the bilingual data are simply dis-
carded because most of them are noisy. In ad-
dition, the fix-discount method in (Foster et al,
2006) for phrase table smoothing is also used.
This implementation makes the baseline system
perform much better and the model size is much
smaller. In fact, the basic idea of our ?one count?
cutoff is very similar to the idea of ?leaving-one-
out? in (Wuebker et al, 2010). The results show
?? ?? ? ? ??
uncharted waters
?? ?? ? ? ??
unexplored new areas
weijing tansuo de xin lingyu
Figure 2: The left one is the non-literal translation
in our bilingual corpus. The right one is the literal
translation made by human for comparison.
that the ?leaving-one-out? method performs al-
most the same as our baseline, thereby cannot
bring other benefits to the system.
3.3 Results
We evaluate the proposed bilingual data clean-
ing method by incorporating sentence scores into
translation modeling. In addition, we also com-
pare with several settings that filtering low-quality
sentence pairs from the bilingual data based on
the importance scores. The last N = { 0.25M,
0.5M, 1M } sentence pairs are filtered before the
modeling process. Although the simple bilin-
gual data filtering can improve the performance on
some datasets, it is difficult to determine the bor-
der line and translation performance is fluctuated.
One main reason is in the proposed random walk
approach, the bilingual sentence pairs with non-
literal translations may get lower scores because
they appear less frequently compared with those
literal translations. Crudely filtering out these data
may degrade the translation performance. For ex-
ample, we have a sentence pair in the bilingual
corpus shown in the left part of Figure 2. Although
the translation is correct in this situation, translat-
ing the Chinese word ?lingyu? to ?waters? appears
very few times since the common translations are
?areas? or ?fields?. However, simply filtering out
this kind of sentence pairs may lead to some loss
of native English expressions, thereby the trans-
343
lation performance is unstable since both non-
parallel sentence pairs and non-literal but parallel
sentence pairs are filtered. Therefore, we use the
importance score of each sentence pair to estimate
the phrase translation probabilities. It consistently
brings substantial improvements compared to the
baseline, which demonstrates graph-based random
walk indeed improves the translation modeling
performance for our SMT system.
3.4 Discussion
In (Goutte et al, 2012), they evaluated phrase-
based SMT systems trained on parallel data with
different proportions of synthetic noisy data. They
suggested that when collecting larger, noisy par-
allel data for training phrase-based SMT, clean-
ing up by trying to detect and remove incor-
rect alignments can actually degrade performance.
Our experimental results confirm their findings
on some datasets. Based on our method, some-
times filtering noisy data leads to unexpected re-
sults. The reason is two-fold: on the one hand,
the non-literal parallel data makes false positive in
noisy data detection; on the other hand, large-scale
SMT systems is relatively robust and tolerant to
noisy data, especially when we remove frequency-
1 phrase pairs. Therefore, we propose to integrate
the importance scores when re-estimating phrase
pair probabilities in this paper. The importance
scores can be considered as a kind of contribution
constraint, thereby high-quality parallel data con-
tributes more while noisy parallel data contributes
less.
4 Conclusion and Future Work
In this paper, we develop an effective approach
to clean the bilingual data using graph-based ran-
dom walk. Significant improvements on several
datasets are achieved in our experiments. For
future work, we will extend our method to ex-
plore the relationships of sentence-to-sentence and
phrase-to-phrase, which is beyond the existing
sentence-to-phrase mutual reinforcement.
Acknowledgments
We are especially grateful to Yajuan Duan, Hong
Sun, Nan Yang and Xilun Chen for the helpful dis-
cussions. We also thank the anonymous reviewers
for their insightful comments.
References
Sergey Brin and Lawrence Page. 1998. The anatomy
of a large-scale hypertextual web search engine.
Computer networks and ISDN systems, 30(1):107?
117.
Jeffrey Dean and Sanjay Ghemawat. 2008. Mapre-
duce: simplified data processing on large clusters.
Communications of the ACM, 51(1):107?113.
George Foster, Roland Kuhn, and Howard Johnson.
2006. Phrasetable smoothing for statistical machine
translation. In Proceedings of the 2006 Conference
on Empirical Methods in Natural Language Pro-
cessing, pages 53?61, Sydney, Australia, July. As-
sociation for Computational Linguistics.
Cyril Goutte, Marine Carpuat, and George Foster.
2012. The impact of sentence alignment errors on
phrase-based machine translation performance. In
Proceedings of AMTA 2012, San Diego, California,
October. Association for Machine Translation in the
Americas.
Long Jiang, Shiquan Yang, Ming Zhou, Xiaohua Liu,
and Qingsheng Zhu. 2009. Mining bilingual data
from the web with adaptively learnt patterns. In Pro-
ceedings of the Joint Conference of the 47th Annual
Meeting of the ACL and the 4th International Joint
Conference on Natural Language Processing of the
AFNLP, pages 870?878, Suntec, Singapore, August.
Association for Computational Linguistics.
Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical phrase-based translation. In Pro-
ceedings of HLT-NAACL 2003 Main Papers, pages
48?54, Edmonton, May-June. Association for Com-
putational Linguistics.
Philipp Koehn. 2004. Statistical significance tests for
machine translation evaluation. In Dekang Lin and
Dekai Wu, editors, Proceedings of EMNLP 2004,
pages 388?395, Barcelona, Spain, July. Association
for Computational Linguistics.
Rada Mihalcea and Paul Tarau. 2004. Textrank:
Bringing order into texts. In Dekang Lin and Dekai
Wu, editors, Proceedings of EMNLP 2004, pages
404?411, Barcelona, Spain, July. Association for
Computational Linguistics.
Dragos Stefan Munteanu and Daniel Marcu. 2005. Im-
proving machine translation performance by exploit-
ing non-parallel corpora. Computational Linguis-
tics, 31(4):477?504.
Franz Josef Och and Hermann Ney. 2003. A sys-
tematic comparison of various statistical alignment
models. Computational Linguistics, 29(1):19?51.
Franz Josef Och and Hermann Ney. 2004. The align-
ment template approach to statistical machine trans-
lation. Computational Linguistics, 30(4):417?449.
344
Franz Josef Och. 2003. Minimum error rate train-
ing in statistical machine translation. In Proceed-
ings of the 41st Annual Meeting of the Association
for Computational Linguistics, pages 160?167, Sap-
poro, Japan, July. Association for Computational
Linguistics.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: a method for automatic
evaluation of machine translation. In Proceedings
of 40th Annual Meeting of the Association for Com-
putational Linguistics, pages 311?318, Philadelphia,
Pennsylvania, USA, July. Association for Computa-
tional Linguistics.
Philip Resnik and Noah A Smith. 2003. The web
as a parallel corpus. Computational Linguistics,
29(3):349?380.
Lei Shi, Cheng Niu, Ming Zhou, and Jianfeng Gao.
2006. A dom tree alignment model for mining paral-
lel data from the web. In Proceedings of the 21st In-
ternational Conference on Computational Linguis-
tics and 44th Annual Meeting of the Association for
Computational Linguistics, pages 489?496, Sydney,
Australia, July. Association for Computational Lin-
guistics.
Xiaojun Wan, Jianwu Yang, and Jianguo Xiao. 2007.
Towards an iterative reinforcement approach for si-
multaneous document summarization and keyword
extraction. In Proceedings of the 45th Annual Meet-
ing of the Association of Computational Linguistics,
pages 552?559, Prague, Czech Republic, June. As-
sociation for Computational Linguistics.
Dekai Wu. 1997. Stochastic inversion transduction
grammars and bilingual parsing of parallel corpora.
Computational Linguistics, 23(3):377?403.
Joern Wuebker, Arne Mauser, and Hermann Ney.
2010. Training phrase translation models with
leaving-one-out. In Proceedings of the 48th Annual
Meeting of the Association for Computational Lin-
guistics, pages 475?484, Uppsala, Sweden, July. As-
sociation for Computational Linguistics.
Deyi Xiong, Qun Liu, and Shouxun Lin. 2006. Max-
imum entropy based phrase reordering model for
statistical machine translation. In Proceedings of
the 21st International Conference on Computational
Linguistics and 44th Annual Meeting of the Associa-
tion for Computational Linguistics, pages 521?528,
Sydney, Australia, July. Association for Computa-
tional Linguistics.
345
Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 133?143,
Baltimore, Maryland, USA, June 23-25 2014.
c?2014 Association for Computational Linguistics
Learning Topic Representation for SMT with Neural Networks
?
Lei Cui
1
, Dongdong Zhang
2
, Shujie Liu
2
, Qiming Chen
3
, Mu Li
2
, Ming Zhou
2
, and Muyun Yang
1
1
School of Computer Science and Technology, Harbin Institute of Technology, Harbin, P.R. China
leicui@hit.edu.cn, ymy@mtlab.hit.edu.cn
2
Microsoft Research, Beijing, P.R. China
{dozhang,shujliu,muli,mingzhou}@microsoft.com
3
Shanghai Jiao Tong University, Shanghai, P.R. China
simoncqm@gmail.com
Abstract
Statistical Machine Translation (SMT)
usually utilizes contextual information
to disambiguate translation candidates.
However, it is often limited to contexts
within sentence boundaries, hence broader
topical information cannot be leveraged.
In this paper, we propose a novel approach
to learning topic representation for paral-
lel data using a neural network architec-
ture, where abundant topical contexts are
embedded via topic relevant monolingual
data. By associating each translation rule
with the topic representation, topic rele-
vant rules are selected according to the dis-
tributional similarity with the source text
during SMT decoding. Experimental re-
sults show that our method significantly
improves translation accuracy in the NIST
Chinese-to-English translation task com-
pared to a state-of-the-art baseline.
1 Introduction
Making translation decisions is a difficult task in
many Statistical Machine Translation (SMT) sys-
tems. Current translation modeling approaches
usually use context dependent information to dis-
ambiguate translation candidates. For exam-
ple, translation sense disambiguation approaches
(Carpuat and Wu, 2005; Carpuat and Wu,
2007) are proposed for phrase-based SMT sys-
tems. Meanwhile, for hierarchical phrase-based
or syntax-based SMT systems, there is also much
work involving rich contexts to guide rule selec-
tion (He et al, 2008; Liu et al, 2008; Marton
and Resnik, 2008; Xiong et al, 2009). Although
these methods are effective and proven successful
in many SMT systems, they only leverage within-
?
This work was done while the first and fourth authors
were visiting Microsoft Research.
sentence contexts which are insufficient in explor-
ing broader information. For example, the word
driver often means ?the operator of a motor ve-
hicle? in common texts. But in the sentence ?Fi-
nally, we write the user response to the buffer, i.e.,
pass it to our driver?, we understand that driver
means ?computer program?. In this case, people
understand the meaning because of the IT topical
context which goes beyond sentence-level analy-
sis and requires more relevant knowledge. There-
fore, it is important to leverage topic information
to learn smarter translation models and achieve
better translation performance.
Topic modeling is a useful mechanism for dis-
covering and characterizing various semantic con-
cepts embedded in a collection of documents. At-
tempts on topic-based translation modeling in-
clude topic-specific lexicon translation models
(Zhao and Xing, 2006; Zhao and Xing, 2007),
topic similarity models for synchronous rules
(Xiao et al, 2012), and document-level translation
with topic coherence (Xiong and Zhang, 2013). In
addition, topic-based approaches have been used
in domain adaptation for SMT (Tam et al, 2007;
Su et al, 2012), where they view different topics
as different domains. One typical property of these
approaches in common is that they only utilize
parallel data where document boundaries are ex-
plicitly given. In this way, the topic of a sentence
can be inferred with document-level information
using off-the-shelf topic modeling toolkits such
as Latent Dirichlet Allocation (LDA) (Blei et al,
2003) or Hidden Topic Markov Model (HTMM)
(Gruber et al, 2007). Most of them also assume
that the input must be in document level. However,
this situation does not always happen since there is
considerable amount of parallel data which does
not have document boundaries. In addition, con-
temporary SMT systems often works on sentence
level rather than document level due to the effi-
ciency. Although we can easily apply LDA at the
133
sentence level, it is quite difficult to infer the topic
accurately with only a few words in the sentence.
This makes previous approaches inefficient when
applied them in real-world commercial SMT sys-
tems. Therefore, we need to devise a systematical
approach to enriching the sentence and inferring
its topic more accurately.
In this paper, we propose a novel approach to
learning topic representations for sentences. Since
the information within the sentence is insufficient
for topic modeling, we first enrich sentence con-
texts via Information Retrieval (IR) methods using
content words in the sentence as queries, so that
topic-related monolingual documents can be col-
lected. These topic-related documents are utilized
to learn a specific topic representation for each
sentence using a neural network based approach.
Neural network is an effective technique for learn-
ing different levels of data representations. The
levels inferred from neural network correspond to
distinct levels of concepts, where high-level rep-
resentations are obtained from low-level bag-of-
words input. It is able to detect correlations among
any subset of input features through non-linear
transformations, which demonstrates the superior-
ity of eliminating the effect of noisy words which
are irrelevant to the topic. Our problem fits well
into the neural network framework and we expect
that it can further improve inferring the topic rep-
resentations for sentences.
To incorporate topic representations as trans-
lation knowledge into SMT, our neural network
based approach directly optimizes similarities be-
tween the source language and target language in a
compact topic space. This underlying topic space
is learned from sentence-level parallel data in or-
der to share topic information across the source
and target languages as much as possible. Addi-
tionally, our model can be discriminatively trained
with a large number of training instances, without
expensive sampling methods such as in LDA or
HTMM, thus it is more practicable and scalable.
Finally, we associate the learned representation to
each bilingual translation rule. Topic-related rules
are selected according to distributional similarity
with the source text, which helps hypotheses gen-
eration in SMT decoding. We integrate topic simi-
larity features in the log-linear model and evaluate
the performance on the NIST Chinese-to-English
translation task. Experimental results demonstrate
that our model significantly improves translation
accuracy over a state-of-the-art baseline.
2 Background: Deep Learning
Deep learning is an active topic in recent years
which has triumphed in many machine learning
research areas. This technique began raising pub-
lic awareness in the mid-2000s after researchers
showed how a multi-layer feed-forward neural
network can be effectively trained. The train-
ing procedure often involves two phases: a layer-
wise unsupervised pre-training phase and a su-
pervised fine-tuning phase. For pre-training, Re-
stricted Boltzmann Machine (RBM) (Hinton et
al., 2006), auto-encoding (Bengio et al, 2006)
and sparse coding (Lee et al, 2006) are most fre-
quently used. Unsupervised pre-training trains the
network one layer at a time and helps guide the pa-
rameters of the layer towards better regions in pa-
rameter space (Bengio, 2009). Followed by fine-
tuning in this parameter region, deep learning is
able to achieve state-of-the-art performance in var-
ious research areas, including breakthrough results
on the ImageNet dataset for objective recognition
(Krizhevsky et al, 2012), significant error reduc-
tion in speech recognition (Dahl et al, 2012), etc.
Deep learning has also been successfully ap-
plied in a variety of NLP tasks such as part-of-
speech tagging, chunking, named entity recog-
nition, semantic role labeling (Collobert et al,
2011), parsing (Socher et al, 2011a), sentiment
analysis (Socher et al, 2011b), etc. Most NLP
research converts a high-dimensional and sparse
binary representation into a low-dimensional and
real-valued representation. This low-dimensional
representation is usually learned from huge
amount of monolingual texts in the pre-training
phase, and then fine-tuned towards task-specific
criterion. Inspired by previous successful re-
search, we first learn sentence representations us-
ing topic-related monolingual texts in the pre-
training phase, and then optimize the bilingual
similarity by leveraging sentence-level parallel
data in the fine-tuning phase.
3 Topic Similarity Model with Neural
Network
In this section, we explain our neural network
based topic similarity model in detail, as well as
how to incorporate the topic similarity features
into SMT decoding procedure. Figure 1 sketches
the high-level overview which illustrates how to
134
?? = ?(?) ?? = ?(?) 
cos(?? , ??) 
???(?, ?) 
 ?  ? 
 English document collection 
 ??  ?? 
Parallel sentence  
IR IR 
? ? 
 Chinese document collection 
Neural Network Training 
Data Preprocessing 
Figure 1: Overview of neural network based topic
similarity model.
learn topic representations using sentence-level
parallel data. Given a parallel sentence pair ?f, e?,
the first step is to treat f and e as queries, and
use IR methods to retrieve relevant documents to
enrich contextual information for them. Specifi-
cally, the ranking model we used is a Vector Space
Model (VSM), where the query and document are
converted into tf-idf weighted vectors. The most
relevant N documents d
f
and d
e
are retrieved and
converted to a high-dimensional, bag-of-words in-
put f and e for the representation learning
1
.
There are two phases in our neural network
training process: pre-training and fine-tuning. In
the pre-training phase (Section 3.1), we build two
neural networks with the same structure but differ-
ent parameters to learn a low-dimensional repre-
sentation for sentences in two different languages.
Then, in the fine-tuning phase (Section 3.2), our
model directly optimizes the similarity of two low-
dimensional representations, so that it highly cor-
relates to SMT decoding. Finally, the learned rep-
resentation is used to calculate similarities which
are integrated as features in SMT decoding proce-
dure (Section 3.3).
3.1 Pre-training using denoising
auto-encoder
In the pre-training phase, we leverage neural
network structures to transform high-dimensional
sparse vectors to low-dimensional dense vectors.
The topic similarity is calculated on top of the
learned dense vectors. This dense representation
should preserve the information from the bag-of-
1
We use f and e to denote the n-of-V vector converted
from the retrieved documents.
words input, meanwhile alleviate data sparse prob-
lem. Therefore, we use a specially designed mech-
anism called auto-encoder to solve this problem.
Auto-encoder (Bengio et al, 2006) is one of the
basic building blocks of deep learning. Assum-
ing that the input is a n-of-V binary vector x rep-
resenting the bag-of-words (V is the vocabulary
size), an auto-encoder consists of an encoding pro-
cess g(x) and a decoding process h(g(x)). The
objective of the auto-encoder is to minimize the
reconstruction error L(h(g(x)), x). Our goal is to
learn a low-dimensional vector which can preserve
information from the original n-of-V vector.
One problem with auto-encoder is that it treats
all words in the same way, making no distinguish-
ment between function words and content words.
The representation learned by auto-encoders tends
to be influenced by the function words, thereby it
is not robust. To alleviate this problem, Vincent et
al. (2008) proposed the Denoising Auto-Encoder
(DAE), which aims to reconstruct a clean, ?re-
paired? input from a corrupted, partially destroyed
vector. This is done by corrupting the initial in-
put x to get a partially destroyed version
?
x. DAE
is capable of capturing the global structure of the
input while ignoring the noise. In our task, for
each sentence, we treat the retrieved N relevant
documents as a single large document and convert
it to a bag-of-words vector x in Figure 2. With
DAE, the input x is manually corrupted by apply-
ing masking noise (randomly mask 1 to 0) and get-
ting
?
x. Denoising training is considered as ?filling
in the blanks? (Vincent et al, 2010), which means
the masking components can be recovered from
the non-corrupted components. For example, in
IT related texts, if the word driver is masked, it
should be predicted through hidden units in neural
networks by active signals such as ?buffer?, ?user
response?, etc.
In our case, the encoding process transforms
the corrupted input
?
x into g(
?
x) with two layers:
a linear layer connected with a non-linear layer.
Assuming that the dimension of the g(
?
x) is L,
the linear layer forms a L ? V matrix W which
projects the n-of-V vector to a L-dimensional hid-
den layer. After the bag-of-words input has been
transformed, they are fed into a subsequent layer
to model the highly non-linear relations among
words:
z = f(W
?
x + b) (1)
where z is the output of the non-linear layer, b is a
135
? ?? 
?(??) 
(?? ??) 
?(?? ?? ,?) 
Figure 2: Denoising auto-encoder with a bag-of-
words input.
L-length bias vector. f(?) is a non-linear function,
where common choices include sigmoid function,
hyperbolic function, ?hard? hyperbolic function,
rectifier function, etc. In this work, we use the
rectifier function as our non-linear function due to
its efficiency and better performance (Glorot et al,
2011):
rec(x) =
{
x if x > 0
0 otherwise
(2)
The decoding process consists of a linear layer
and a non-linear layer with similar network struc-
tures, but different parameters. It transforms the
L-dimensional vector g(
?
x) to a V -dimensional
vector h(g(
?
x)). To minimize reconstruction error
with respect to
?
x, we define the loss function as
the L2-norm of the difference between the uncor-
rupted input and reconstructed input:
L(h(g(
?
x)), x) = ?h(g(
?
x))? x?
2
(3)
Multi-layer neural networks are trained with the
standard back-propagation algorithm (Rumelhart
et al, 1988). The gradient of the loss function
is calculated and back-propagated to the previous
layer to update its parameters. Training neural net-
works involves many factors such as the learning
rate and the length of hidden layers. We will dis-
cuss the optimization of these parameters in Sec-
tion 4.
3.2 Fine-tuning with parallel data
In the fine-tuning phase, we stack another layer on
top of the two low-dimensional vectors to maxi-
mize the similarity between source and target lan-
guages. The similarity scores are integrated into
the standard log-linear model for making transla-
tion decisions. Since the vectors from DAE are
trained using information from monolingual train-
ing data independently, these vectors may be in-
adequate to measure bilingual topic similarity due
to their different topic spaces. Therefore, in this
stage, parallel sentence pairs are used to help con-
necting the vectors from different languages be-
cause they express the same topic. In fact, the ob-
jective of fine-tuning is to discover a latent topic
space which is shared by both languages as much
as possible. This shared topic space is particularly
useful when the SMT decoder tries to match the
source texts and translation candidates in the tar-
get language.
Given a parallel sentence pair ?f, e?, the DAE
learns representations for f and e respectively, as
z
f
= g(f) and z
e
= g(e) in Figure 1. We then take
two vectors as the input to calculate their similar-
ity. Consequently, the whole neural network can
be fine-tuned towards the supervised criteria with
the help of parallel data. The similarity score of
the representation pair ?z
f
, z
e
? is defined as the co-
sine similarity of the two vectors:
sim(f, e) = cos(z
f
, z
e
)
=
z
f
? z
e
?z
f
??z
e
?
(4)
Since a parallel sentence pair should have the
same topic, our goal is to maximize the similar-
ity score between the source sentence and target
sentence. Inspired by the contrastive estimation
method (Smith and Eisner, 2005), for each paral-
lel sentence pair ?f, e? as a positive instance, we
select another sentence pair ?f
?
, e
?
? from the train-
ing data and treat ?f, e
?
? as a negative instance. To
make the similarity of the positive instance larger
than the negative instance by some margin ?, we
utilize the following pairwise ranking loss:
L(f, e) = max{0, ? ? sim(f, e) + sim(f, e
?
)}
(5)
where ? =
1
2
? sim(f, f
?
). The rationale behind
this criterion is, the smaller sim(f, f
?
) is, the more
we should penalize negative instances.
To effectively train the model in this task, neg-
ative instances must be selected carefully. Since
different sentences may have very similar topic
distributions, we select negative instances that are
dissimilar with the positive instances based on the
following criteria:
1. For each positive instance ?f, e?, we select e
?
which contains at least 30% different content
words from e.
136
2. If we cannot find such e
?
, remove ?f, e? from
the training instances for network learning.
The model minimizes the pairwise ranking loss
across all training instances:
L =
?
?f,e?
L(f, e) (6)
We used standard back-propagation algorithm
to further fine-tune the neural network parameters
W and b in Equation (1). The learned neural net-
works are used to obtain sentence topic representa-
tions, which will be further leveraged to infer topic
representations of bilingual translation rules.
3.3 Integration into SMT decoding
We incorporate the learned topic similarity scores
into the standard log-linear framework for SMT.
When a synchronous rule ??, ?? is extracted from
a sentence pair ?f, e?, a triple instance I =
(??, ??, ?f, e?, c) is collected for inferring the
topic representation of ??, ??, where c is the count
of rule occurrence. Following (Chiang, 2007), we
give a count of one for each phrase pair occurrence
and a fractional count for each hierarchical phrase
pair. The topic representation of ??, ?? is then cal-
culated as the weighted average:
z
?
=
?
(??,??,?f,e?,c)?T
{c? z
f
}
?
(??,??,?f,e?,c)?T
{c}
(7)
z
?
=
?
(??,??,?f,e?,c)?T
{c? z
e
}
?
(??,??,?f,e?,c)?T
{c}
(8)
where T denotes all instances for the rule ??, ??,
z
?
and z
?
are the source-side and target-side topic
vectors respectively.
By measuring the similarity between the source
texts and bilingual translation rules, the SMT de-
coder is able to encourage topic relevant transla-
tion candidates and penalize topic irrelevant candi-
dates. Therefore, it helps to train a smarter transla-
tion model with the embedded topic information.
Given a source sentence s to be translated, we de-
fine the similarity as follows:
Sim(z
s
, z
?
) = cos(z
s
, z
?
) (9)
Sim(z
s
, z
?
) = cos(z
s
, z
?
) (10)
where z
s
is the topic representation of s. The
similarity calculated against z
?
or z
?
denotes the
source-to-source or the source-to-target similarity.
We also consider the topic sensitivity estimation
since general rules have flatter distributions while
topic-specific rules have sharper distributions. A
standard entropy metric is used to measure the sen-
sitivity of the source-side of ??, ?? as:
Sen(?) = ?
|z
?
|
?
i=1
z
?i
? log z
?i
(11)
where z
?i
is a component in the vector z
?
. The
target-side sensitivity Sen(?) can be calculated in
a similar way. The larger the sensitivity is, the
more topic-specific the rule manifests.
In addition to traditional SMT features, we add
new topic-related features into the standard log-
linear framework. For the SMT system, the best
translation candidate e? is given by:
e? = argmax
e
P (e|f) (12)
where the translation probability is given by:
P (e|f) ?
?
i
w
i
? log ?
i
(f, e)
=
?
j
w
j
? log ?
j
(f, e)
? ?? ?
Standard
+
?
k
w
k
? log ?
k
(f, e)
? ?? ?
Topic related
(13)
where ?
j
(f, e) is the standard feature function and
w
j
is the corresponding feature weight. ?
k
(f, e)
is the topic-related feature function and w
k
is the
feature weight. The detailed feature description is
as follows:
Standard features: Translation model, includ-
ing translation probabilities and lexical weights
for both directions (4 features), 5-gram language
model (1 feature), word count (1 feature), phrase
count (1 feature), NULL penalty (1 feature), num-
ber of hierarchical rules used (1 feature).
Topic-related features: rule similarity scores
(2 features), rule sensitivity scores (2 features).
4 Experiments
4.1 Setup
We evaluate the performance of our neural net-
work based topic similarity model on a Chinese-
to-English machine translation task. In neural net-
work training, a large number of monolingual doc-
uments are collected in both source and target lan-
guages. The documents are mainly from two do-
mains: news and weblog. We use Chinese and
137
English Gigaword corpus (Version 5) which are
mainly from news domain. In addition, we also
collect weblog documents with a variety of top-
ics from the web. The total data statistics are
presented in Table 1. These documents are built
in the format of inverted index using Lucene
2
,
which can be efficiently retrieved by the paral-
lel sentence pairs. The most relevant N docu-
ments are collected, where we experiment with
N = {1, 5, 10, 20, 50}.
Domain
Chinese English
Docs Words Docs Words
News 5.7M 5.4B 9.9M 25.6B
Weblog 2.1M 8B 1.2M 2.9B
Total 7.8M 13.4B 11.1M 28.5B
Table 1: Statistics of monolingual data, in num-
bers of documents and words (main content). ?M?
refers to million and ?B? refers to billion.
We implement a distributed framework to speed
up the training process of neural networks. The
network is learned with mini-batch asynchronous
gradient descent with the adaptive learning rate
procedure called AdaGrad (Duchi et al, 2011).
We use 32 model replicas in each iteration during
the training. The model parameters are averaged
after each iteration and sent to each replica for the
next iteration. The vocabulary size for the input
layer is 100,000, and we choose different lengths
for the hidden layer as L = {100, 300, 600, 1000}
in the experiments. In the pre-training phase, all
parallel data is fed into two neural networks re-
spectively for DAE training, where network pa-
rameters W and b are randomly initialized. In
the fine-tuning phase, for each parallel sentence
pair, we randomly select other ten sentence pairs
which satisfy the criterion as negative instances.
These training instances are leveraged to optimize
the similarity of two vectors.
In SMT training, an in-house hierarchical
phrase-based SMT decoder is implemented for our
experiments. The CKY decoding algorithm is
used and cube pruning is performed with the same
default parameter settings as in Chiang (2007).
The parallel data we use is released by LDC
3
. In
total, the datasets contain nearly 1.1 million sen-
tence pairs. Translation models are trained over
the parallel data that is automatically word-aligned
2
http://lucene.apache.org/
3
LDC2003E14, LDC2002E18, LDC2003E07,
LDC2005T06, LDC2005T10, LDC2005E83, LDC2006E34,
LDC2006E85, LDC2006E92, LDC2006E26, LDC2007T09
using GIZA++ in both directions, and the diag-
grow-final heuristic is used to refine symmetric
word alignment. An in-house language modeling
toolkit is used to train the 5-gram language model
with modified Kneser-Ney smoothing (Kneser and
Ney, 1995). The English monolingual data used
for language modeling is the same as in Table
1. The NIST 2003 dataset is the development
data. The testing data consists of NIST 2004,
2005, 2006 and 2008 datasets. The evaluation
metric for the overall translation quality is case-
insensitive BLEU4 (Papineni et al, 2002). The
reported BLEU scores are averaged over 5 times
of running MERT (Och, 2003). A statistical sig-
nificance test is performed using the bootstrap re-
sampling method (Koehn, 2004).
4.2 Baseline
The baseline is a re-implementation of the Hiero
system (Chiang, 2007). The phrase pairs that ap-
pear only once in the parallel data are discarded
because most of them are noisy. We also use
the fix-discount method in Foster et al (2006)
for phrase table smoothing. This implementation
makes the system perform much better and the
translation model size is much smaller.
We compare our method with the LDA-based
approach proposed by Xiao et al (2012). In (Xiao
et al, 2012), the topic of each sentence pair is ex-
actly the same as the document it belongs to. Since
some of our parallel data does not have document-
level information, we rely on the IR method to
retrieve the most relevant document and simulate
this approach. The PLDA toolkit (Liu et al, 2011)
is used to infer topic distributions, which takes
34.5 hours to finish.
4.3 Effect of retrieved documents and length
of hidden layers
We illustrate the relationship among translation
accuracy (BLEU), the number of retrieved docu-
ments (N ) and the length of hidden layers (L) on
different testing datasets. The results are shown in
Figure 3. The best translation accuracy is achieved
when N=10 for most settings. This confirms that
enriching the source text with topic-related doc-
uments is very useful in determining topic repre-
sentations, thereby help to guide the synchronous
rule selection. However, we find that as N be-
comes larger in the experiments, e.g. N=50, the
translation accuracy drops drastically. As more
documents are retrieved, less relevant information
138
0 5 10 20 5042
42.2
42.4
42.6
42.8
43
Number of Retrieved Documents (N)
BLE
U
NIST 2004
 
 L=100L=300L=600L=1000
0 5 10 20 5041
41.2
41.4
41.6
41.8
42
Number of Retrieved Documents (N)
BLE
U
NIST 2005
 
 L=100L=300L=600L=1000
0 5 10 20 5037.8
38
38.2
38.4
38.6
38.8
39
39.2
Number of Retrieved Documents (N)
BLE
U
NIST 2006
 
 L=100L=300L=600L=1000
0 5 10 20 5031
31.2
31.4
31.6
31.8
32
Number of Retrieved Documents (N)
BLE
U
NIST 2008
 
 L=100L=300L=600L=1000
Figure 3: End-to-end translation results (BLEU%) using all standard and topic-related features, with
different settings on the number of retrieved documents N and the length of hidden layers L.
is also used to train the neural networks. Irrel-
evant documents bring so many unrelated topic
words hence degrade neural network learning per-
formance.
Another important factor is the length of hid-
den layers L in the network. In deep learning, this
parameter is often empirically tuned with human
efforts. As shown in Figure 3, the translation accu-
racy is better when L is relatively small. Actually,
there is no obvious distinction of the performance
when L is less than 600. However, when L equals
1,000, the translation accuracy is inferior to other
settings. The main reason is that parameters in
the neural networks are too many to be effectively
trained. As we know when L=1000, there are a
total of 100, 000? 1, 000 parameters between the
linear and non-linear layers in the network. Lim-
ited training data prevents the model from getting
close to the global optimum. Therefore, the model
is likely to fall in local optima and lead to unac-
ceptable representations.
4.4 Effect of topic related features
We evaluate the performance of adding new topic-
related features to the log-linear model and com-
pare the translation accuracy with the method in
(Xiao et al, 2012). To make different methods
comparable, we set the dimension of topic rep-
resentation as 100 for all settings. This takes 10
hours in pre-training phase and 22 hours in fine-
tuning phase. Table 2 shows how the accuracy is
improved with more features added. The results
confirm that topic information is indispensable for
SMT since both (Xiao et al, 2012) and our neural
network based method significantly outperforms
the baseline system. Our method improves 0.86
BLEU points at most and 0.76 BLEU points on
average over the baseline. We observe that source-
side similarity is more effective than target-side
similarity, but their contributions are cumulative.
This proves that bilingually induced topic repre-
sentation with neural network helps the SMT sys-
tem disambiguate translation candidates. Further-
more, rule sensitivity features improve SMT per-
formance compared with only using similarity fea-
tures. Because topic-specific rules usually have a
larger sensitivity score, they can beat general rules
when they obtain the same similarity score against
the input sentence. Finally, when all new fea-
tures are integrated, the performance is the best,
preforming substantially better than (Xiao et al,
2012) with 0.39 BLEU points on average.
It is worth mentioning that the performance
of (Xiao et al, 2012) is similar to the settings
with N=1 and L=100 in Figure 3. This is not
simply coincidence since we can interpret their
approach as a special case in our neural net-
work method: when a parallel sentence pair has
139
Settings NIST 2004 NIST 2005 NIST 2006 NIST 2008 Average
Baseline 42.25 41.21 38.05 31.16 38.17
(Xiao et al, 2012) 42.58 41.61 38.39 31.58 38.54
Sim(Src) 42.51 41.55 38.53 31.57 38.54
Sim(Trg) 42.43 41.48 38.4 31.49 38.45
Sim(Src+Trg) 42.7 41.66 38.66 31.66 38.67
Sim(Src+Trg)+Sen(Src) 42.77 41.81 38.85 31.73 38.79
Sim(Src+Trg)+Sen(Trg) 42.85 41.79 38.76 31.7 38.78
Sim(Src+Trg)+Sen(Src+Trg) 42.95 41.97 38.91 31.88 38.93
Table 2: Effectiveness of different features in BLEU% (p < 0.05), with N=10 and L=100. ?Sim?
denotes the rule similarity feature and ?Sen? denotes rule sensitivity feature. ?Src? and ?Trg? means
utilizing source-side/target-side rule topic vectors to calculate similarity or sensitivity, respectively. The
?Average? setting is the averaged result of four datasets.
document-level information, that document will
be retrieved for training; otherwise, the most rel-
evant document will be retrieved from the mono-
lingual data. Therefore, our method can be viewed
as a more general framework than previous LDA-
based approaches.
4.5 Discussion
In this section, we give a case study to explain
why our method works. An example of transla-
tion rule disambiguation for a sentence from the
NIST 2005 dataset is shown in Figure 4. We find
that the topic of this sentence is about ?rescue af-
ter a natural disaster?. Under this topic, the Chi-
nese rule ??? X? should be translated to ?de-
liver X? or ?distribute X?. However, the baseline
system prefers ?send X? rather than those two can-
didates. Although the translation probability of
?send X? is much higher, it is inappropriate in this
context since it is usually used in IT texts. For
example, ?????, send emails?, ?????,
send messages? and ?????, send data?. In
contrast, with our neural network based approach,
the learned topic distributions of ?deliver X? or
?distribute X? are more similar with the input sen-
tence than ?send X?, which is shown in Figure 4.
The similarity scores indicate that ?deliver X? and
?distribute X? are more appropriate to translate the
sentence. Therefore, adding topic-related features
is able to keep the topic consistency and substan-
tially improve the translation accuracy.
5 Related Work
Topic modeling was first leveraged to improve
SMT performance in (Zhao and Xing, 2006; Zhao
and Xing, 2007). They proposed a bilingual
topical admixture approach for word alignment
and assumed that each word-pair follows a topic-
specific model. They reported extensive empir-
ical analysis and improved word alignment ac-
curacy as well as translation quality. Follow-
ing this work, (Xiao et al, 2012) extended topic-
specific lexicon translation models to hierarchical
phrase-based translation models, where the topic
information of synchronous rules was directly in-
ferred with the help of document-level informa-
tion. Experiments show that their approach not
only achieved better translation performance but
also provided a faster decoding speed compared
with previous lexicon-based LDA methods.
Another direction of approaches leveraged topic
modeling techniques for domain adaptation. Tam
et al (2007) used bilingual LSA to learn latent
topic distributions across different languages and
enforce one-to-one topic correspondence during
model training. They incorporated the bilingual
topic information into language model adaptation
and lexicon translation model adaptation, achiev-
ing significant improvements in the large-scale
evaluation. (Su et al, 2012) investigated the rela-
tionship between out-of-domain bilingual data and
in-domain monolingual data via topic mapping
using HTMM methods. They estimated phrase-
topic distributions in translation model adaptation
and generated better translation quality. Recently,
Chen et al (2013) proposed using vector space
model for adaptation where genre resemblance is
leveraged to improve translation accuracy. We
also investigated multi-domain adaptation where
explicit topic information is used to train domain
specific models (Cui et al, 2013).
Generally, most previous research has leveraged
conventional topic modeling techniques such as
LDA or HTMM. In our work, a novel neural net-
work based approach is proposed to infer topic
representations for parallel data. The advantage of
140
S
rc
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
R
ef
(1
)
th
e
un
it
ed
na
ti
on
s
ch
il
dr
en
?s
fu
nd
ha
s
al
so
be
gu
n
de
li
ve
ri
ng
ba
si
c
m
ed
ic
al
ki
ts
(2
)
th
e
un
ic
ef
ha
s
al
so
st
ar
te
d
to
di
st
ri
bu
te
ba
si
c
m
ed
ic
al
ki
ts
(3
)
th
e
un
it
ed
na
ti
on
s
ch
il
dr
en
?s
fu
nd
ha
s
al
so
be
gu
n
di
st
ri
bu
ti
ng
ba
si
c
m
ed
ic
al
ki
ts
(4
)
th
e
un
it
ed
na
ti
on
s
ch
il
dr
en
?s
fu
nd
ha
s
be
gu
n
de
li
ve
ri
ng
ba
si
c
m
ed
ic
al
ki
ts
B
as
el
in
e
th
e
un
it
ed
na
ti
on
s
ch
il
dr
en
?s
fu
nd
be
ga
n
to
se
nd
ba
si
c
m
ed
ic
al
ki
ts
O
ur
s
th
e
un
it
ed
na
ti
on
s
ch
il
dr
en
?s
fu
nd
ha
s
be
gu
n
to
di
st
ri
bu
te
ba
si
c
m
ed
ic
al
ki
ts
T
ab
le
4:
A
ck
n
ow
le
d
gm
en
ts
T
he
ac
kn
ow
le
dg
m
en
ts
sh
ou
ld
go
im
m
ed
ia
te
ly
be
-
fo
re
th
e
re
fe
re
nc
es
.
D
o
no
t
nu
m
be
r
th
e
ac
kn
ow
l-
ed
gm
en
ts
se
ct
io
n.
D
o
no
t
in
cl
ud
e
th
is
se
ct
io
n
w
he
n
su
bm
it
ti
ng
yo
ur
pa
pe
r
fo
r
re
vi
ew
.
R
ef
er
en
ce
s
Y
os
hu
a
B
en
gi
o,
P
as
ca
l
L
am
bl
in
,
D
an
P
op
ov
ic
i,
an
d
H
ug
o
L
ar
oc
he
ll
e.
20
06
.
G
re
ed
y
la
ye
r-
w
is
e
tr
ai
n-
in
g
of
de
ep
ne
tw
or
ks
.
In
B
.
S
ch
o?l
ko
pf
,
J.
P
la
tt
,
an
d
T
.
H
of
fm
an
,
ed
it
or
s,
A
dv
an
ce
s
in
N
eu
ra
l
In
fo
rm
a-
ti
on
P
ro
ce
ss
in
g
Sy
st
em
s
19
,
pa
ge
s
15
3?
16
0.
M
IT
P
re
ss
,C
am
br
id
ge
,M
A
.
Y
os
hu
a
B
en
gi
o.
20
09
.
L
ea
rn
in
g
de
ep
ar
ch
it
ec
tu
re
s
fo
r
ai
.
Fo
un
d.
Tr
en
ds
M
ac
h.
L
ea
rn
.,
2(
1)
:1
?1
27
,
Ja
n-
ua
ry
.
D
av
id
M
.
B
le
i,
A
nd
re
w
Y
.
N
g,
an
d
M
ic
ha
el
I.
Jo
rd
an
.
20
03
.
L
at
en
t
di
ri
ch
le
t
al
lo
ca
ti
on
.
J.
M
ac
h.
L
ea
rn
.
R
es
.,
3:
99
3?
10
22
,M
ar
ch
.
M
ar
in
e
C
ar
pu
at
an
d
D
ek
ai
W
u.
20
07
.
C
on
te
xt
-
de
pe
nd
en
t
ph
ra
sa
l
tr
an
sl
at
io
n
le
xi
co
ns
fo
r
st
at
is
ti
ca
l
m
ac
hi
ne
tr
an
sl
at
io
n.
P
ro
ce
ed
in
gs
of
M
ac
hi
ne
Tr
an
s-
la
ti
on
Su
m
m
it
X
I,
pa
ge
s
73
?8
0.
D
av
id
C
hi
an
g.
20
07
.
H
ie
ra
rc
hi
ca
l
ph
ra
se
-b
as
ed
tr
an
s-
la
ti
on
.
C
om
pu
ta
ti
on
al
L
in
gu
is
ti
cs
,3
3(
2)
:2
01
?2
28
.
R
on
an
C
ol
lo
be
rt
,J
as
on
W
es
to
n,
L
e?o
n
B
ot
to
u,
M
ic
ha
el
K
ar
le
n,
K
or
ay
K
av
uk
cu
og
lu
,
an
d
P
av
el
K
uk
sa
.
20
11
.
N
at
ur
al
la
ng
ua
ge
pr
oc
es
si
ng
(a
lm
os
t)
fr
om
sc
ra
tc
h.
J.
M
ac
h.
L
ea
rn
.
R
es
.,
12
:2
49
3?
25
37
,
N
ov
em
be
r.
G
.
E
.
D
ah
l,
D
on
g
Y
u,
L
i
D
en
g,
an
d
A
.
A
ce
ro
.
20
12
.
C
on
te
xt
-d
ep
en
de
nt
pr
e-
tr
ai
ne
d
de
ep
ne
ur
al
ne
tw
or
ks
fo
r
la
rg
e-
vo
ca
bu
la
ry
sp
ee
ch
re
co
gn
it
io
n.
Tr
an
s.
A
u-
di
o,
Sp
ee
ch
an
d
L
an
g.
P
ro
c.
,2
0(
1)
:3
0?
42
,J
an
ua
ry
.
Jo
hn
D
uc
hi
,
E
la
d
H
az
an
,
an
d
Y
or
am
S
in
ge
r.
20
11
.
A
da
pt
iv
e
su
bg
ra
di
en
t
m
et
ho
ds
fo
r
on
li
ne
le
ar
ni
ng
an
d
st
oc
ha
st
ic
op
ti
m
iz
at
io
n.
J.
M
ac
h.
L
ea
rn
.
R
es
.,
12
:2
12
1?
21
59
,J
ul
y.
G
eo
rg
e
F
os
te
r,
R
ol
an
d
K
uh
n,
an
d
H
ow
ar
d
Jo
hn
so
n.
20
06
.
P
hr
as
et
ab
le
sm
oo
th
in
g
fo
r
st
at
is
ti
ca
l
m
ac
hi
ne
tr
an
sl
at
io
n.
In
P
ro
ce
ed
in
gs
of
th
e
20
06
C
on
fe
re
nc
e
on
E
m
pi
ri
ca
l
M
et
ho
ds
in
N
at
ur
al
L
an
gu
ag
e
P
ro
-
ce
ss
in
g,
pa
ge
s
53
?6
1,
S
yd
ne
y,
A
us
tr
al
ia
,
Ju
ly
.
A
s-
so
ci
at
io
n
fo
r
C
om
pu
ta
ti
on
al
L
in
gu
is
ti
cs
.
A
m
it
G
ru
be
r,
M
ic
ha
lR
os
en
-z
vi
,a
nd
Y
ai
r
W
ei
ss
.
20
07
.
H
id
de
n
to
pi
c
m
ar
ko
v
m
od
el
s.
In
In
P
ro
ce
ed
in
gs
of
A
rt
ifi
ci
al
In
te
ll
ig
en
ce
an
d
St
at
is
ti
cs
.
Z
ho
ng
ju
n
H
e,
Q
un
L
iu
,
an
d
S
ho
ux
un
L
in
.
20
08
.
Im
-
pr
ov
in
g
st
at
is
ti
ca
lm
ac
hi
ne
tr
an
sl
at
io
n
us
in
g
le
xi
ca
l-
iz
ed
ru
le
se
le
ct
io
n.
In
P
ro
ce
ed
in
gs
of
th
e
22
nd
In
-
te
rn
at
io
na
l
C
on
fe
re
nc
e
on
C
om
pu
ta
ti
on
al
L
in
gu
is
-
ti
cs
(C
ol
in
g
20
08
),
pa
ge
s
32
1?
32
8,
M
an
ch
es
te
r,
U
K
,
A
ug
us
t.
C
ol
in
g
20
08
O
rg
an
iz
in
g
C
om
m
it
te
e.
G
eo
ff
re
y
E
.
H
in
to
n,
S
im
on
O
si
nd
er
o,
an
d
Y
ee
-W
hy
e
T
eh
.
20
06
.
A
fa
st
le
ar
ni
ng
al
go
ri
th
m
fo
r
de
ep
be
li
ef
ne
ts
.
N
eu
ra
l
C
om
pu
t.
,1
8(
7)
:1
52
7?
15
54
,J
ul
y.
R
ei
nh
ar
d
K
ne
se
r
an
d
H
er
m
an
n
N
ey
.
19
95
.
Im
-
pr
ov
ed
ba
ck
in
g-
of
f
fo
r
m
-g
ra
m
la
ng
ua
ge
m
od
el
in
g.
In
A
co
us
ti
cs
,
Sp
ee
ch
,
an
d
Si
gn
al
P
ro
ce
ss
in
g,
19
95
.
IC
A
SS
P
-9
5.
,1
99
5
In
te
rn
at
io
na
lC
on
fe
re
nc
e
on
,v
ol
-
um
e
1,
pa
ge
s
18
1?
18
4.
IE
E
E
.
P
hi
li
pp
K
oe
hn
.
20
04
.
S
ta
ti
st
ic
al
si
gn
ifi
ca
nc
e
te
st
s
fo
r
m
ac
hi
ne
tr
an
sl
at
io
n
ev
al
ua
ti
on
.
In
D
ek
an
g
L
in
an
d
D
ek
ai
W
u,
ed
it
or
s,
P
ro
ce
ed
in
gs
of
E
M
N
L
P
20
04
,
pa
ge
s
38
8?
39
5,
B
ar
ce
lo
na
,
S
pa
in
,
Ju
ly
.
A
ss
oc
ia
ti
on
fo
r
C
om
pu
ta
ti
on
al
L
in
gu
is
ti
cs
.
A
le
x
K
ri
zh
ev
sk
y,
Il
ya
S
ut
sk
ev
er
,
an
d
G
eo
ff
H
in
to
n.
20
12
.
Im
ag
en
et
cl
as
si
fi
ca
ti
on
w
it
h
de
ep
co
nv
ol
u-
ti
on
al
ne
ur
al
ne
tw
or
ks
.
In
P.
B
ar
tl
et
t,
F.
C
.N
.P
er
ei
ra
,
C
.J
.C
.
B
ur
ge
s,
L
.
B
ot
to
u,
an
d
K
.Q
.
W
ei
nb
er
ge
r,
ed
-
it
or
s,
A
dv
an
ce
s
in
N
eu
ra
l
In
fo
rm
at
io
n
P
ro
ce
ss
in
g
Sy
st
em
s
25
,p
ag
es
11
06
?1
11
4.
H
on
gl
ak
L
ee
,
A
le
xi
s
B
at
tl
e,
R
aj
at
R
ai
na
,
an
d
A
n-
dr
ew
Y
.
N
g.
20
06
.
E
ffi
ci
en
t
sp
ar
se
co
di
ng
al
go
-
ri
th
m
s.
In
B
.
S
ch
o?l
ko
pf
,
J.
P
la
tt
,
an
d
T
.
H
of
fm
an
,
ed
it
or
s,
A
dv
an
ce
s
in
N
eu
ra
l
In
fo
rm
at
io
n
P
ro
ce
ss
in
g
Sy
st
em
s
19
,p
ag
es
80
1?
80
8.
M
IT
P
re
ss
,C
am
br
id
ge
,
M
A
.
Q
un
L
iu
,
Z
ho
ng
ju
n
H
e,
Y
an
g
L
iu
,
an
d
S
ho
ux
un
L
in
.
20
08
.
M
ax
im
um
en
tr
op
y
ba
se
d
ru
le
se
le
ct
io
n
m
od
el
fo
r
sy
nt
ax
-b
as
ed
st
at
is
ti
ca
l
m
ac
hi
ne
tr
an
sl
at
io
n.
In
P
ro
ce
ed
in
gs
of
th
e
20
08
C
on
fe
re
nc
e
on
E
m
pi
ri
ca
l
M
et
ho
ds
in
N
at
ur
al
L
an
gu
ag
e
P
ro
ce
ss
in
g,
pa
ge
s
89
?9
7,
H
on
ol
ul
u,
H
aw
ai
i,
O
ct
ob
er
.
A
ss
oc
ia
ti
on
fo
r
C
om
pu
ta
ti
on
al
L
in
gu
is
ti
cs
.
Z
hi
yu
an
L
iu
,
Y
uz
ho
u
Z
ha
ng
,
E
dw
ar
d
Y
.
C
ha
ng
,
an
d
M
ao
so
ng
S
un
.
20
11
.
P
ld
a+
:
P
ar
al
le
l
la
te
nt
di
ri
ch
le
t
al
lo
ca
ti
on
w
it
h
da
ta
pl
ac
em
en
ta
nd
pi
pe
li
ne
pr
oc
es
s-
in
g.
A
C
M
Tr
an
sa
ct
io
ns
on
In
te
ll
ig
en
t
Sy
st
em
s
an
d
Te
ch
no
lo
gy
,
sp
ec
ia
l
is
su
e
on
L
ar
ge
Sc
al
e
M
ac
hi
ne
L
ea
rn
in
g.
S
of
tw
ar
e
av
ai
la
bl
e
at
h
t
t
p
:
/
/
c
o
d
e
.
g
o
o
g
l
e
.
c
o
m
/
p
/
p
l
d
a
.
0
20
40
60
80
100
00.020.040.060.080.1???
???
???
???
???
???
0
20
40
60
80
100
00.020.040.060.080.1
<?? 
X , de
liver 
X>
0
20
40
60
80
100
00.020.040.060.080.1
<?? 
X , di
stribu
te X>
0
20
40
60
80
100
00.020.040.060.080.1
<?? 
X , se
nd X>
S
et
ti
n
gs
N
IS
T
20
04
N
IS
T
20
05
N
IS
T
20
06
N
IS
T
20
08
A
ve
ra
ge
B
as
el
in
e
42
.2
5
41
.2
1
38
.0
5
31
.1
6
38
.1
7
(X
ia
o
et
al
.,
20
12
)
42
.5
8
41
.6
1
38
.3
9
31
.5
8
38
.5
4
S
im
(S
rc
)
42
.5
1
41
.5
5
38
.5
3
31
.5
7
38
.5
4
S
im
(T
rg
)
42
.4
3
41
.4
8
38
.4
31
.4
9
38
.4
5
S
im
(S
rc
+
T
rg
)
42
.7
41
.6
6
38
.6
6
31
.6
6
38
.6
7
S
im
(S
rc
+
T
rg
)+
S
en
(S
rc
)
42
.7
7
41
.8
1
38
.8
5
31
.7
3
38
.7
9
S
im
(S
rc
+
T
rg
)+
S
en
(T
rg
)
42
.8
5
41
.7
9
38
.7
6
31
.7
38
.7
8
S
im
(S
rc
+
T
rg
)+
S
en
(S
rc
+
T
rg
)
42
.9
5
41
.9
7
38
.9
1
31
.8
8
38
.9
3
T
ab
le
2:
E
ff
ec
ti
ve
ne
ss
of
di
ff
er
en
t
fe
at
ur
es
in
B
L
E
U
%
(p
<
0.
05
),
w
it
h
N
=
10
an
d
L
=
10
0.
?S
im
?
de
no
te
s
th
e
ru
le
si
m
il
ar
it
y
fe
at
ur
e
an
d
?S
en
?
de
no
te
s
ru
le
se
ns
it
iv
it
y
fe
at
ur
e.
?S
rc
?
an
d
?T
rg
?
m
ea
ns
ut
il
iz
in
g
so
ur
ce
-s
id
e/
ta
rg
et
-s
id
e
ru
le
to
pi
c
ve
ct
or
s
to
ca
lc
ul
at
e
si
m
il
ar
it
y
or
se
ns
it
iv
it
y,
re
sp
ec
ti
ve
ly
.
T
he
?A
ve
ra
ge
?
se
tt
in
g
is
th
e
av
er
ag
ed
re
su
lt
s
of
fo
ur
da
ta
se
ts
.
pa
re
d
w
it
h
on
ly
us
in
g
si
m
il
ar
it
y
fe
at
ur
es
.
B
ec
au
se
to
pi
c-
sp
ec
ifi
c
ru
le
s
us
ua
ll
y
ha
ve
a
la
rg
er
se
ns
it
iv
-
it
y
sc
or
e,
th
ey
ca
n
be
at
ge
ne
ra
l
ru
le
s
w
he
n
th
ey
ob
ta
in
th
e
sa
m
e
si
m
il
ar
it
y
sc
or
e
ag
ai
ns
t
th
e
in
pu
t
se
nt
en
ce
.
F
in
al
ly
,
w
he
n
al
l
ne
w
fe
at
ur
es
ar
e
in
-
te
gr
at
ed
,
th
e
pe
rf
or
m
an
ce
is
th
e
be
st
,
pr
ef
or
m
in
g
su
bs
ta
nt
ia
ll
y
be
tt
er
th
an
(X
ia
o
et
al
.,
20
12
)
w
it
h
0.
39
B
L
E
U
po
in
ts
on
av
er
ag
e.
O
ne
in
te
re
st
in
g
ob
se
rv
at
io
n
is
,t
he
pe
rf
or
m
an
ce
of
(X
ia
o
et
al
.,
20
12
)
is
qu
it
e
si
m
il
ar
to
th
e
se
t-
ti
ng
s
w
it
h
N
=
1
an
d
L
=
10
0
in
F
ig
ur
e
3.
T
hi
s
is
no
t
si
m
pl
y
co
in
ci
de
nc
e
si
nc
e
w
e
ca
n
in
te
rp
re
t
th
ei
r
ap
pr
oa
ch
as
a
sp
ec
ia
l
ca
se
in
ou
r
ne
ur
al
ne
t-
w
or
k
m
et
ho
d.
W
he
n
a
pa
ra
ll
el
se
nt
en
ce
pa
ir
ha
s
do
cu
m
en
t-
le
ve
l
in
fo
rm
at
io
n,
th
at
do
cu
m
en
t
w
il
l
be
re
tr
ie
ve
d
fo
r
tr
ai
ni
ng
.
O
th
er
w
is
e,
th
e
m
os
ts
im
-
il
ar
do
cu
m
en
t
w
il
l
be
ob
ta
in
ed
fr
om
th
e
m
on
ol
in
-
gu
al
da
ta
.
O
ur
m
et
ho
d
ca
n
be
vi
ew
ed
as
a
m
or
e
ge
ne
ra
l
fr
am
ew
or
k
th
an
pr
ev
io
us
L
D
A
-b
as
ed
ap
-
pr
oa
ch
es
.
4.
5
D
is
cu
ss
io
n
In
ou
r
ex
pe
ri
m
en
ts
,
In
pr
ev
io
us
L
D
A
-b
as
ed
m
et
ho
d,
if
a
do
cu
m
en
t
D
oc
co
nt
ai
ns
M
se
nt
en
ce
s,
al
l
M
se
nt
en
ce
s
w
il
l
sh
ar
e
th
e
sa
m
e
to
pi
c
di
st
ri
bu
ti
on
of
D
oc
.
A
l-
th
ou
gh
di
ff
er
en
t
se
nt
en
ce
s
m
ay
ex
pr
es
s
sl
ig
ht
ly
di
ff
er
en
t
im
pl
ic
at
io
ns
an
d
th
e
to
pi
c
w
il
l
ch
an
ge
,
th
e
co
nv
en
ti
on
al
L
D
A
-b
as
ed
ap
pr
oa
ch
do
es
no
t
ta
ke
th
e
to
pi
c
tr
an
si
ti
on
in
to
co
ns
id
er
at
io
n.
In
co
n-
tr
as
t,
ou
r
ap
pr
oa
ch
di
re
ct
ly
le
ar
ns
th
e
to
pi
c
re
p-
re
se
nt
at
io
n
w
it
h
an
ab
un
da
nc
y
of
re
la
te
d
do
cu
-
m
en
ts
.
In
ad
di
ti
on
al
to
th
e
or
ig
in
al
do
cu
m
en
tf
ro
m
w
hi
ch
th
e
se
nt
en
ce
is
ex
tr
ac
te
d,
th
e
IR
m
et
ho
d
al
so
re
tr
ie
ve
s
ot
he
r
re
le
va
nt
do
cu
m
en
ts
w
hi
ch
pr
o-
vi
de
co
m
pl
em
en
ta
ry
to
pi
c
in
fo
rm
at
io
n.
T
he
re
fo
re
,
th
e
to
pi
c
re
pr
es
en
ta
ti
on
s
le
ar
ne
d
ar
e
m
or
e
fi
ne
-
gr
ai
ne
d
an
d
th
us
m
or
e
ac
cu
ra
te
.
R
u
le
s
P
(?
|?
)
S
im
(z
s
,z
?
)
??
?
X
,d
el
iv
er
X
?
0.
02
37
0.
84
69
??
?
X
,d
is
tr
ib
ut
e
X
?
0.
05
46
0.
82
68
??
?
X
,s
en
d
X
?
0.
24
64
0.
61
19
T
ab
le
3:
D
ev
el
op
m
en
ta
nd
te
st
in
g
da
ta
us
ed
in
th
e
ex
pe
ri
m
en
ts
.
5
R
el
at
ed
W
or
k
T
op
ic
m
od
el
in
g
w
as
fi
rs
t
le
ve
ra
ge
d
to
im
pr
ov
e
S
M
T
pe
rf
or
m
an
ce
in
(Z
ha
o
an
d
X
in
g,
20
06
;Z
ha
o
an
d
X
in
g,
20
07
).
T
he
y
pr
op
os
ed
a
bi
li
ng
ua
l
to
pi
ca
l
ad
m
ix
tu
re
ap
pr
oa
ch
fo
r
w
or
d
al
ig
nm
en
t
an
d
as
su
m
ed
th
at
ea
ch
w
or
d-
pa
ir
fo
ll
ow
s
a
to
pi
c-
sp
ec
ifi
c
m
od
el
.
T
he
y
re
po
rt
ed
ex
te
ns
iv
e
em
pi
r-
ic
al
an
al
ys
is
an
d
im
pr
ov
ed
w
or
d
al
ig
nm
en
t
ac
-
cu
ra
cy
as
w
el
l
as
tr
an
sl
at
io
n
qu
al
it
y.
F
ol
lo
w
-
in
g
th
is
w
or
k,
(X
ia
o
et
al
.,
20
12
)
ex
te
nd
ed
to
pi
c-
sp
ec
ifi
c
le
xi
co
n
tr
an
sl
at
io
n
m
od
el
s
to
hi
er
ar
ch
ic
al
ph
ra
se
-b
as
ed
tr
an
sl
at
io
n
m
od
el
s,
w
he
re
th
e
to
pi
c
in
fo
rm
at
io
n
of
sy
nc
hr
on
ou
s
ru
le
s
w
as
di
re
ct
ly
in
-
fe
rr
ed
w
it
h
th
e
he
lp
of
do
cu
m
en
t-
le
ve
l
in
fo
rm
a-
ti
on
.
E
xp
er
im
en
ts
sh
ow
th
at
th
ei
r
ap
pr
oa
ch
no
t
on
ly
ac
hi
ev
ed
be
tt
er
tr
an
sl
at
io
n
pe
rf
or
m
an
ce
bu
t
al
so
pr
ov
id
ed
a
fa
st
er
de
co
di
ng
sp
ee
d
co
m
pa
re
d
w
it
h
pr
ev
io
us
le
xi
co
n-
ba
se
d
m
et
ho
ds
.
A
no
th
er
di
re
ct
io
n
of
ap
pr
oa
ch
es
le
ve
ra
ge
d
to
pi
c
m
od
el
in
g
te
ch
ni
qu
es
fo
r
do
m
ai
n
ad
ap
ta
ti
on
.
T
am
et
al
.
(2
00
7)
us
ed
bi
li
ng
ua
l
L
S
A
to
le
ar
n
la
te
nt
to
pi
c
di
st
ri
bu
ti
on
s
ac
ro
ss
di
ff
er
en
t
la
ng
ua
ge
s
an
d
en
fo
rc
e
on
e-
to
-o
ne
to
pi
c
co
rr
es
po
nd
en
ce
du
ri
ng
m
od
el
tr
ai
ni
ng
.
T
he
y
in
co
rp
or
at
ed
th
e
bi
li
ng
ua
l
to
pi
c
in
fo
rm
at
io
n
in
to
la
ng
ua
ge
m
od
el
ad
ap
ta
ti
on
an
d
le
xi
co
n
tr
an
sl
at
io
n
m
od
el
ad
ap
ta
ti
on
,
ac
hi
ev
-
in
g
si
gn
ifi
ca
nt
im
pr
ov
em
en
ts
in
th
e
la
rg
e-
sc
al
e
ev
al
ua
ti
on
.
(S
u
et
al
.,
20
12
)
in
ve
st
ig
at
ed
th
e
re
la
-
ti
on
sh
ip
be
tw
ee
n
ou
t-
of
-d
om
ai
n
bi
li
ng
ua
ld
at
a
an
d
in
-d
om
ai
n
m
on
ol
in
gu
al
da
ta
vi
a
to
pi
c
m
ap
pi
ng
us
-
Figure 4: An exampl from the NIST 2005 dataset. We ill strate the normalized topic repres ntations of
the source sentence and three ambiguous synchronous rules. Details are explained in Section 4.5.
our method is that it is applicable to both sentence-
level and doc ment-level SMT, since we do not
place any restricti ns on the input. In addition, our
method directly maximizes the similarity between
parallel sentence pairs, which is ideal for SMT de-
coding. Compared to document-level topic mod-
eling which uses the topic of a document for all
sentences within the document (Xiao et al, 2012),
our contributions are:
? We proposed a more general approach to
leveraging topic information for SMT by us-
ing IR methods to get a collection of related
documents, regardless of whether or not doc-
ument boundaries are explicitly given.
? We used neural networks to learn topic repre-
sentations more accurately, with more practi-
cable and scalable modeling techniques.
? We directly optimized bilingual topic simi-
larity in the deep learning framework with
the help of sentence-level parallel data, so
that the learned representation could be easily
used in SMT decoding procedure.
6 Conclusion and Future Work
In this paper, we propose a neural network based
approach to learning bilingual topic representa-
tion for SMT. We enrich contexts of parallel sen-
tence pairs with topic related monolingual data
and obtain a set of documents to represent sen-
tences. These documents are converted to a bag-
of-words input and fed into neural networks. The
learned low-dimensional vector is used to obtain
the topic representations of synchronous rules. In
SMT decoding, appropriate rules a e selected to
best match source texts according to their similar-
ity in the topic space. Experimental results show
that our approach is promising for SMT systems to
learn a better translation model. It is a significant
improvement over the state-of-the-art Hiero sys-
tem, as well as a conventional LDA-based method.
In the future research, we will extend our neural
network methods to address document-level trans-
lation, where topic transition between sentences is
a crucial problem to be solved. Since the transla-
tion of the current sentence is usually influenced
by the topic of previous sentences, we plan to
leverage recurrent neural networks to model this
phenomenon, where the history translation infor-
mation is naturally combined in the model.
Acknowledgments
We are grateful to the anonymous reviewers for
their insightful comments. We also thank Fei
Huang (BBN), Nan Yang, Yajuan Duan, Hong Sun
and Duyu Tang for the helpful discussions. This
work is supported by the National Natural Science
Foundation of China (Granted No. 61272384)
141
References
Yoshua Bengio, Pascal Lamblin, Dan Popovici, and
Hugo Larochelle. 2006. Greedy layer-wise train-
ing of deep networks. In B. Sch?olkopf, J. Platt, and
T. Hoffman, editors, Advances in Neural Informa-
tion Processing Systems 19, pages 153?160. MIT
Press, Cambridge, MA.
Yoshua Bengio. 2009. Learning deep architectures for
ai. Found. Trends Mach. Learn., 2(1):1?127, Jan-
uary.
David M. Blei, Andrew Y. Ng, and Michael I. Jordan.
2003. Latent dirichlet alocation. J. Mach. Learn.
Res., 3:993?1022, March.
Marine Carpuat and Dekai Wu. 2005. Word sense dis-
ambiguation vs. statistical machine translation. In
Proceedings of the 43rd Annual Meeting of the As-
sociation for Computational Linguistics (ACL?05),
pages 387?394, Ann Arbor, Michigan, June. Asso-
ciation for Computational Linguistics.
Marine Carpuat and Dekai Wu. 2007. Context-
dependent phrasal translation lexicons for statistical
machine translation. Proceedings of Machine Trans-
lation Summit XI, pages 73?80.
Boxing Chen, Roland Kuhn, and George Foster. 2013.
Vector space model for adaptation in statistical ma-
chine translation. In Proceedings of the 51st An-
nual Meeting of the Association for Computational
Linguistics (Volume 1: Long Papers), pages 1285?
1293, Sofia, Bulgaria, August. Association for Com-
putational Linguistics.
David Chiang. 2007. Hierarchical phrase-based trans-
lation. Computational Linguistics, 33(2):201?228.
Ronan Collobert, Jason Weston, L?eon Bottou, Michael
Karlen, Koray Kavukcuoglu, and Pavel Kuksa.
2011. Natural language processing (almost) from
scratch. J. Mach. Learn. Res., 12:2493?2537,
November.
Lei Cui, Xilun Chen, Dongdong Zhang, Shujie Liu,
Mu Li, and Ming Zhou. 2013. Multi-domain adap-
tation for SMT using multi-task learning. In Pro-
ceedings of the 2013 Conference on Empirical Meth-
ods in Natural Language Processing, pages 1055?
1065, Seattle, Washington, USA, October. Associa-
tion for Computational Linguistics.
George E. Dahl, Dong Yu, Li Deng, and Alex Acero.
2012. Context-dependent pre-trained deep neural
networks for large-vocabulary speech recognition.
IEEE Transactions on Audio, Speech and Language
Processing, 20(1):30?42, January.
John Duchi, Elad Hazan, and Yoram Singer. 2011.
Adaptive subgradient methods for online learning
and stochastic optimization. J. Mach. Learn. Res.,
12:2121?2159, July.
George Foster, Roland Kuhn, and Howard Johnson.
2006. Phrasetable smoothing for statistical machine
translation. In Proceedings of the 2006 Conference
on Empirical Methods in Natural Language Pro-
cessing, pages 53?61, Sydney, Australia, July. As-
sociation for Computational Linguistics.
Xavier Glorot, Antoine Bordes, and Yoshua Bengio.
2011. Deep sparse rectifier networks. In Proceed-
ings of the 14th International Conference on Arti-
ficial Intelligence and Statistics. JMLR W&CP Vol-
ume, volume 15, pages 315?323.
Amit Gruber, Michal Rosen-zvi, and Yair Weiss. 2007.
Hidden topic markov models. In In Proceedings of
Artificial Intelligence and Statistics.
Zhongjun He, Qun Liu, and Shouxun Lin. 2008. Im-
proving statistical machine translation using lexical-
ized rule selection. In Proceedings of the 22nd In-
ternational Conference on Computational Linguis-
tics (Coling 2008), pages 321?328, Manchester, UK,
August. Coling 2008 Organizing Committee.
Geoffrey E. Hinton, Simon Osindero, and Yee-Whye
Teh. 2006. A fast learning algorithm for deep belief
nets. Neural Comput., 18(7):1527?1554, July.
Reinhard Kneser and Hermann Ney. 1995. Im-
proved backing-off for m-gram language modeling.
In Acoustics, Speech, and Signal Processing, 1995.
ICASSP-95., 1995 International Conference on, vol-
ume 1, pages 181?184. IEEE.
Philipp Koehn. 2004. Statistical significance tests for
machine translation evaluation. In Dekang Lin and
Dekai Wu, editors, Proceedings of EMNLP 2004,
pages 388?395, Barcelona, Spain, July. Association
for Computational Linguistics.
Alex Krizhevsky, Ilya Sutskever, and Geoff Hinton.
2012. Imagenet classification with deep convolu-
tional neural networks. In P. Bartlett, F.C.N. Pereira,
C.J.C. Burges, L. Bottou, and K.Q. Weinberger, ed-
itors, Advances in Neural Information Processing
Systems 25, pages 1106?1114.
Honglak Lee, Alexis Battle, Rajat Raina, and An-
drew Y. Ng. 2006. Efficient sparse coding algo-
rithms. In B. Sch?olkopf, J. Platt, and T. Hoffman,
editors, Advances in Neural Information Processing
Systems 19, pages 801?808. MIT Press, Cambridge,
MA.
Qun Liu, Zhongjun He, Yang Liu, and Shouxun Lin.
2008. Maximum entropy based rule selection model
for syntax-based statistical machine translation. In
Proceedings of the 2008 Conference on Empirical
Methods in Natural Language Processing, pages
89?97, Honolulu, Hawaii, October. Association for
Computational Linguistics.
Zhiyuan Liu, Yuzhou Zhang, Edward Y. Chang, and
Maosong Sun. 2011. Plda+: Parallel latent dirichlet
allocation with data placement and pipeline process-
ing. ACM Transactions on Intelligent Systems and
142
Technology, special issue on Large Scale Machine
Learning. Software available at http://code.
google.com/p/plda.
Yuval Marton and Philip Resnik. 2008. Soft syntactic
constraints for hierarchical phrased-based transla-
tion. In Proceedings of ACL-08: HLT, pages 1003?
1011, Columbus, Ohio, June. Association for Com-
putational Linguistics.
Franz Josef Och. 2003. Minimum error rate train-
ing in statistical machine translation. In Proceed-
ings of the 41st Annual Meeting of the Association
for Computational Linguistics, pages 160?167, Sap-
poro, Japan, July. Association for Computational
Linguistics.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: a method for automatic
evaluation of machine translation. In Proceedings
of 40th Annual Meeting of the Association for Com-
putational Linguistics, pages 311?318, Philadelphia,
Pennsylvania, USA, July. Association for Computa-
tional Linguistics.
David E. Rumelhart, Geoffrey E. Hinton, and Ronald J.
Williams. 1988. Neurocomputing: Foundations
of research. chapter Learning Representations
by Back-propagating Errors, pages 696?699. MIT
Press, Cambridge, MA, USA.
Noah A. Smith and Jason Eisner. 2005. Contrastive
estimation: Training log-linear models on unlabeled
data. In Proceedings of the 43rd Annual Meet-
ing of the Association for Computational Linguis-
tics (ACL?05), pages 354?362, Ann Arbor, Michi-
gan, June. Association for Computational Linguis-
tics.
Richard Socher, Cliff C. Lin, Andrew Y. Ng, and
Christopher D. Manning. 2011a. Parsing Natural
Scenes and Natural Language with Recursive Neural
Networks. In Proceedings of the 26th International
Conference on Machine Learning (ICML).
Richard Socher, Jeffrey Pennington, Eric H. Huang,
Andrew Y. Ng, and Christopher D. Manning. 2011b.
Semi-supervised recursive autoencoders for predict-
ing sentiment distributions. In Proceedings of the
2011 Conference on Empirical Methods in Natural
Language Processing, pages 151?161, Edinburgh,
Scotland, UK., July. Association for Computational
Linguistics.
Jinsong Su, Hua Wu, Haifeng Wang, Yidong Chen,
Xiaodong Shi, Huailin Dong, and Qun Liu. 2012.
Translation model adaptation for statistical machine
translation with monolingual topic information. In
Proceedings of the 50th Annual Meeting of the As-
sociation for Computational Linguistics (Volume 1:
Long Papers), pages 459?468, Jeju Island, Korea,
July. Association for Computational Linguistics.
Yik-Cheung Tam, Ian Lane, and Tanja Schultz. 2007.
Bilingual lsa-based adaptation for statistical ma-
chine translation. Machine Translation, 21(4):187?
207, December.
Pascal Vincent, Hugo Larochelle, Yoshua Bengio, and
Pierre-Antoine Manzagol. 2008. Extracting and
composing robust features with denoising autoen-
coders. In Proceedings of the 25th International
Conference on Machine Learning, ICML ?08, pages
1096?1103, New York, NY, USA. ACM.
Pascal Vincent, Hugo Larochelle, Isabelle Lajoie,
Yoshua Bengio, and Pierre-Antoine Manzagol.
2010. Stacked denoising autoencoders: Learning
useful representations in a deep network with a local
denoising criterion. J. Mach. Learn. Res., 11:3371?
3408, December.
Xinyan Xiao, Deyi Xiong, Min Zhang, Qun Liu, and
Shouxun Lin. 2012. A topic similarity model for
hierarchical phrase-based translation. In Proceed-
ings of the 50th Annual Meeting of the Association
for Computational Linguistics (Volume 1: Long Pa-
pers), pages 750?758, Jeju Island, Korea, July. As-
sociation for Computational Linguistics.
Deyi Xiong and Min Zhang. 2013. A topic-based co-
herence model for statistical machine translation. In
AAAI.
Deyi Xiong, Min Zhang, Aiti Aw, and Haizhou Li.
2009. A syntax-driven bracketing model for phrase-
based translation. In Proceedings of the Joint Con-
ference of the 47th Annual Meeting of the ACL and
the 4th International Joint Conference on Natural
Language Processing of the AFNLP, pages 315?
323, Suntec, Singapore, August. Association for
Computational Linguistics.
Bing Zhao and Eric P. Xing. 2006. Bitam: Bilingual
topic admixture models for word alignment. In Pro-
ceedings of the COLING/ACL 2006 Main Confer-
ence Poster Sessions, pages 969?976, Sydney, Aus-
tralia, July. Association for Computational Linguis-
tics.
Bing Zhao and Eric P. Xing. 2007. Hm-bitam: Bilin-
gual topic exploration, word alignment, and trans-
lation. In J.C. Platt, D. Koller, Y. Singer, and
S. Roweis, editors, Advances in Neural Informa-
tion Processing Systems 20, pages 1689?1696. MIT
Press, Cambridge, MA.
143
