Proceedings of the Human Language Technology Conference of the North American Chapter of the ACL, pages 407?414,
New York, June 2006. c?2006 Association for Computational Linguistics
Language Model Information Retrieval with Document Expansion
Tao Tao, Xuanhui Wang, Qiaozhu Mei, ChengXiang Zhai
Department of Computer Science
University of Illinois at Urbana Champaign
Abstract
Language model information retrieval de-
pends on accurate estimation of document
models. In this paper, we propose a docu-
ment expansion technique to deal with the
problem of insufficient sampling of docu-
ments. We construct a probabilistic neigh-
borhood for each document, and expand
the document with its neighborhood infor-
mation. The expanded document provides
a more accurate estimation of the docu-
ment model, thus improves retrieval ac-
curacy. Moreover, since document expan-
sion and pseudo feedback exploit different
corpus structures, they can be combined to
further improve performance. The experi-
ment results on several different data sets
demonstrate the effectiveness of the pro-
posed document expansion method.
1 Introduction
Information retrieval with statistical language mod-
els (Lafferty and Zhai, 2003) has recently attracted
much more attention because of its solid theoreti-
cal background as well as its good empirical per-
formance. In this approach, queries and documents
are assumed to be sampled from hidden generative
models, and the similarity between a document and
a query is then calculated through the similarity be-
tween their underlying models.
Clearly, good retrieval performance relies on the
accurate estimation of the query and document mod-
els. Indeed, smoothing of document models has
been proved to be very critical (Chen and Good-
man, 1998; Kneser and Ney, 1995; Zhai and Laf-
ferty, 2001b). The need for smoothing originated
from the zero count problem: when a term does not
occur in a document, the maximum likelihood esti-
mator would give it a zero probability. This is un-
reasonable because the zero count is often due to in-
sufficient sampling, and a larger sample of the data
would likely contain the term. Smoothing is pro-
posed to address the problem.
While most smoothing methods utilize the global
collection information with a simple interpolation
(Ponte and Croft, 1998; Miller et al, 1999; Hiemstra
and Kraaij, 1998; Zhai and Lafferty, 2001b), sev-
eral recent studies (Liu and Croft, 2004; Kurland and
Lee, 2004) have shown that local corpus structures
can be exploited to improve retrieval performance.
In this paper, we further study the use of local cor-
pus structures for document model estimation and
propose to use document expansion to better exploit
local corpus structures for estimating document lan-
guage models.
According to statistical principles, the accuracy of
a statistical estimator is largely determined by the
sampling size of the observed data; a small data
set generally would result in large variances, thus
can not be trusted completely. Unfortunately, in re-
trieval, we often have to estimate a model based on a
single document. Since a document is a small sam-
ple, our estimate is unlikely to be very accurate.
A natural improvement is to enlarge the data sam-
ple, ideally in a document-specific way. Ideally, the
enlarged data sample should come from the same
original generative model. In reality, however, since
407
the underlying model is unknown to us, we would
not really be able to obtain such extra data. The
essence of this paper is to use document expansion
to obtain high quality extra data to enlarge the sam-
ple of a document so as to improve the accuracy
of the estimated document language model. Docu-
ment expansion was previously explored in (Sing-
hal and Pereira, 1999) in the context of the vec-
tor space retrieval model, mainly involving selecting
more terms from similar documents. Our work dif-
fers from this previous work in that we study doc-
ument expansion in the language modeling frame-
work and implement the idea quite differently.
Our main idea is to augment a document prob-
abilistically with potentially all other documents in
the collection that are similar to the document. The
probability associated with each neighbor document
reflects how likely the neighbor document is from
the underlying distribution of the original document,
thus we have a ?probabilistic neighborhood?, which
can serve as ?extra data? for the document for es-
timating the underlying language model. From the
viewpoint of smoothing, our method extends the ex-
isting work on using clusters for smoothing (Liu and
Croft, 2004) to allow each document to have its own
cluster for smoothing.
We evaluated our method using six representative
retrieval test sets. The experiment results show that
document expansion smoothing consistently outper-
forms the baseline smoothing methods in all the data
sets. It also outperforms a state-of-the-art cluster-
ing smoothing method. Analysis shows that the
improvement tends to be more significant for short
documents, indicating that the improvement indeed
comes from the improved estimation of the docu-
ment language model, since a short document pre-
sumably would benefit more from the neighborhood
smoothing. Moreover, since document expansion
and pseudo feedback exploit different corpus struc-
tures, they can be combined to further improve per-
formance. As document expansion can be done in
the indexing stage, it is scalable to large collections.
2 Document Expansion Retrieval Model
2.1 The KL-divergence retrieval model
We first briefly review the KL-divergence retrieval
model, on which we will develop the document
expansion technique. The KL-divergence model
is a representative state-of-the-art language model-
ing approach for retrieval. It covers the basic lan-
guage modeling approach (i.e., the query likelihood
method) as a special case and can support feedback
more naturally.
In this approach, a query and a document are as-
sumed to be generated from a unigram query lan-
guage model ?Q and a unigram document languagemodel ?D, respectively. Given a query and a docu-ment, we would first compute an estimate of the cor-
responding query model (??Q) and document model
(??D), and then score the document w.r.t. the querybased on the KL-divergence of the two models (Laf-
ferty and Zhai, 2001):
D(??Q || ??d) =
?
w?V
p(w|??Q) ? log
p(w|??Q)
p(w|??d)
.
where V is the set of all the words in our vocabulary.
The documents can then be ranked according to the
ascending order of the KL-divergence values.
Clearly, the two fundamental problems in such a
model are to estimate the query model and the doc-
ument model, and the accuracy of our estimation of
these models would affect the retrieval performance
significantly. The estimation of the query model
can often be improved by exploiting the local cor-
pus structure in a way similar to pseudo-relevance
feedback (Lafferty and Zhai, 2001; Lavrenko and
Croft, 2001; Zhai and Lafferty, 2001a). The esti-
mation of the document model is most often done
through smoothing with the global collection lan-
guage model (Zhai and Lafferty, 2001b), though re-
cently there has been some work on using clusters
for smoothing (Liu and Croft, 2004). Our work is
mainly to extend the previous work on document
smoothing and improve the accuracy of estimation
by better exploiting the local corpus structure. We
now discuss all these in detail.
2.2 Smoothing of document models
Given a document d, the simplest way to estimate
the document language model is to treat the docu-
ment as a sample from the underlying multinomial
word distribution and use the maximum likelihood
estimator: P (w|??d) = c(w,d)|d| , where c(w, d) isthe count of word w in document d, and |d| is the
408
length of d. However, as discussed in virtually all
the existing work on using language models for re-
trieval, such an estimate is problematic and inaccu-
rate; indeed, it would assign zero probability to any
word not present in document d, causing problems
in scoring a document with query likelihood or KL-
divergence (Zhai and Lafferty, 2001b). Intuitively,
such an estimate is inaccurate because the document
is a small sample.
To solve this problem, many different smoothing
techniques have been proposed and studied, usually
involving some kind of interpolation of the maxi-
mum likelihood estimate and a global collection lan-
guage model (Hiemstra and Kraaij, 1998; Miller et
al., 1999; Zhai and Lafferty, 2001b). For exam-
ple, Jelinek-Mercer(JM) and Dirichlet are two com-
monly used smoothing methods (Zhai and Lafferty,
2001b). JM smoothing uses a fixed parameter ? to
control the interpolation:
P (w|??d) = ?
c(w, d)
|d| + (1 ? ?)P (w|?C),
while the Dirichlet smoothing uses a document-
dependent coefficient (parameterized with ?) to con-
trol the interpolation:
P (w|??d) =
c(w, d) + ?P (w|?C)
|d| + ? .
Here P (w|?C) is the probability of word w given bythe collection language model ?C , which is usuallyestimated using the whole collection of documents
C , e.g., P (w|?C) =
P
d?C c(d,w)
P
d?C |d|
.
2.3 Cluster-based document model (CBDM)
Recently, the cluster structure of the corpus has been
exploited to improve language models for retrieval
(Kurland and Lee, 2004; Liu and Croft, 2004). In
particular, the cluster-based language model pro-
posed in (Liu and Croft, 2004) uses clustering infor-
mation to further smooth a document model. It di-
vides all documents into K different clusters (K =
1000 in their experiments). Both cluster informa-
tion and collection information are used to improve
the estimate of the document model:
P (w|??d) = ?
c(w, d)
|d| + (1 ? ?)
?[?P (w|?Ld) + (1 ? ?)P (w|?C )],
where ?Ld stands for document d?s cluster modeland ? and ? are smoothing parameters. In this
clustering-based smoothing method, we first smooth
a cluster model with the collection model using
Dirichlet smoothing, and then use smoothed cluster
model as a new reference model to further smooth
the document model using JM smoothing; empirical
results show that the added cluster information in-
deed enhances retrieval performance (Liu and Croft,
2004).
2.4 Document expansion
From the viewpoint of data augmentation, the
clustering-based language model can be regarded as
?expanding? a document with more data from the
cluster that contains the document. This is intu-
itively better than simply expanding every document
with the same collection language model as in the
case of JM or Dirichlet smoothing. Looking at it
from this perspective, we see that, as the ?extra data?
for smoothing a document model, the cluster con-
taining the document is often not optimal. Indeed,
the purpose of clustering is to group similar doc-
uments together, hence a cluster model represents
well the overall property of all the documents in the
cluster. However, such an average model is often not
accurate for smoothing each individual document.
We illustrate this problem in Figure 1(a), where we
show two documents d and a in cluster D. Clearly
the generative model of cluster D is more suitable
for smoothing document a than document d. In gen-
eral, the cluster model is more suitable for smooth-
ing documents close to the centroid, such as a, but is
inaccurate for smoothing a document at the bound-
ary, such as d.
To achieve optimal smoothing, each document
should ideally have its own cluster centered on the
document, as shown in Figure 1(b). This is pre-
cisely what we propose ? expanding each document
with a probabilistic neighborhood around the doc-
ument and estimate the document model based on
such a virtual, expanded document. We can then ap-
ply any simple interpolation-based method (e.g., JM
or Dirichlet) to such a ?virtual document? and treat
the word counts given by this ?virtual document? as
if they were the original word counts.
The use of neighborhood information is worth
more discussion. First of all, neighborhood is not a
409
cluster D
d d
d?s neighbors
(a) (b)
a
Figure 1: Clusters, neighborhood, and document ex-
pansion
clearly defined concept. In the narrow sense, only
a few documents close to the original one should
be included in the neighborhood, while in the wide
sense, the whole collection can be potentially in-
cluded. It is thus a challenge to define the neighbor-
hood concept reasonably. Secondly, the assumption
that neighbor documents are sampled from the same
generative model as the original document is not
completely valid. We probably do not want to trust
them so much as the original one. We solve these
two problems by associating a confidence value with
every document in the collection, which reflects our
belief that the document is sampled from the same
underlying model as the original document. When a
document is close to the original one, we have high
confidence, but when it is farther apart, our confi-
dence would fade away. In this way, we construct
a probabilistic neighborhood which can potentially
include all the documents with different confidence
values. We call a language model based on such a
neighborhood document expansion language model
(DELM).
Technically, we are looking for a new enlarged
document d? for each document d in a text collec-
tion, such that the new document d? can be used
to estimate the hidden generative model of d more
accurately. Since a good d? should presumably be
based on both the original document d and its neigh-
borhood N(d), we define a function ?:
d? = ?(d,N(d)). (1)
The precise definition of the neighborhood con-
cept N(d) relies on the distance or similarity be-
tween each pair of documents. Here, we simply
choose the commonly used cosine similarity, though
other choices may also be possible. Given any two
document models X and Y , the cosine similarity is
d
Figure 2: Normal distribution of confidence values.
defined as:
sim(X,Y ) =
?
i xi ? yi
?
?
i(xi)2 ?
?
i(yi)2
.
To model the uncertainty of neighborhood, we as-
sign a confidence value ?d(b) to every document b inthe collection to indicate how strongly we believe b
is sampled from d?s hidden model. In general, ?d(b)can be set based on the similarity of b and d ? the
more similar b and d are, the larger ?d(b) wouldbe. With these confidence values, we construct a
probabilistic neighborhood with every document in
it, each with a different weight. The whole problem
is thus reduced to how to define ?d(b) exactly.Intuitively, an exponential decay curve can help
regularize the influence from remote documents. We
therefore want ?d(b) to satisfy a normal distributioncentered around d. Figure 2 illustrates the shape
of this distribution. The black dots are neighbor-
hood documents centered around d. Their proba-
bility values are determined by their distances to the
center. We fortunately observe that the cosine sim-
ilarities, which we use to decide the neighborhood,
are roughly of this decay shape. We thus use them
directly without further transformation because that
would introduce unnecessary parameters. We set
?d(b) by normalizing the cosine similarity scores :
?d(b) =
sim(d, b)
?
b??C?{d} sim(d, b?)
.
Function ? serves to balance the confidence be-
tween d and its neighborhood N(d) in the model es-
timation step. Intuitively, a shorter document is less
sufficient, hence needs more help from its neighbor-
hood. Conversely, a longer one can rely more on
itself. We use a parameter ? to control this balance.
Thus finally, we obtain a pseudo document d? with
410
the following pseudo term count:
c(w, d?) = ?c(w, d) + (1 ? ?)
?
?
b?C?{d}
(?d(b) ? c(w, b)),
We hypothesize that, in general, ?d can be estimatedmore accurately from d? rather than d itself because
d? contains more complete information about ?d.This hypothesis can be tested by by comparing the
retrieval results of applying any smoothing method
to d with those of applying the same method to d?.
In our experiments, we will test this hypothesis with
both JM smoothing and Dirichlet smoothing.
Note that the proposed document expansion tech-
nique is quite general. Indeed, since it transforms
the original document to a potentially better ?ex-
panded document?, it can presumably be used to-
gether with any retrieval method, including the vec-
tor space model. In this paper, we focus on evalu-
ating this technique with the language modeling ap-
proach.
Because of the decay shape of the neighborhood
and for the sake of efficiency, we do not have to ac-
tually use all documents in C?{d}. Instead, we can
safely cut off the documents on the tail, and only use
the top M closest neighbors for each document. We
show in the experiment section that the performance
is not sensitive to the choice of M when M is suf-
ficiently large (for example 100). Also, since doc-
ument expansion can be done completely offline, it
can scale up to large collections.
3 Experiments
We evaluate the proposed method over six repre-
sentative TREC data sets (Voorhees and Harman,
2001): AP (Associated Press news 1988-90), LA
(LA Times), WSJ (Wall Street Journal 1987-92),
SJMN (San Jose Mercury News 1991), DOE (De-
partment of Energy), and TREC8 (the ad hoc data
used in TREC8). Table 1 shows the statistics of these
data.
We choose the first four TREC data sets for per-
formance comparison with (Liu and Croft, 2004).
To ensure that the comparison is meaningful, we use
identical sources (after all preprocessing). In addi-
tion, we use the large data set TREC8 to show that
our algorithm can scale up, and use DOE because its
#document queries #total qrel
AP 242918 51-150 21819
LA 131896 301-400 2350
WSJ 173252 51-100 and 151-200 10141
SJMN 90257 51-150 4881
TREC8 528155 401-450 4728
DOE 226087 DOE queries 2047
Table 1: Experiment data sets
documents are usually short, and our previous expe-
rience shows that it is a relatively difficult data set.
3.1 Neighborhood document expansion
Our model boils down to a standard query likelihood
model when no neighborhood document is used. We
therefore use two most commonly used smoothing
methods, JM and Dirichlet , as our baselines. The re-
sults are shown in Table 2, where we report both the
mean average precision (MAP) and precision at 10
documents. JM and Dirichlet indicate the standard
language models with JM smoothing and Dirichlet
smoothing respectively, and the other two are the
ones combined with our document expansion. For
both baselines, we tune the parameters (? for JM,
and ? for Dirichlet) to be optimal. We then use the
same values of ? or ? without further tuning for the
document expansion runs, which means that the pa-
rameters may not necessarily optimal for the docu-
ment expansion runs. Despite this disadvantage, we
see that the document expansion runs significantly
outperform their corresponding baselines, with more
than 15% relative improvement on AP. The parame-
ters M and ? were set to 100 and 0.5, respectively.
To understand the improvement in more detail, we
show the precision values at different levels of recall
for the AP data in Table 3. Here we see that our
method significantly outperforms the baseline at ev-
ery precision point.
In our model, we introduce two additional param-
eters: M and ?. We first examine M here, and then
study ? in Section 3.3. Figure 3 shows the perfor-
mance trend with respect to the values of M . The
x-axis is the values of M , and the y-axis is the non-
interpolated precision averaging over all 50 queries.
We draw two conclusions from this plot: (1) Neigh-
borhood information improves retrieval accuracy;
adding more documents leads to better retrieval re-
sults. (2) The performance becomes insensitive to
411
Data JM DELM+JM (impr. %) Dirichlet DELM + Diri.(impr. %)
AP AvgPrec 0.2058 0.2405 (16.8%***) 0.2168 0.2505 (15.5%***)
P@10 0.3990 0.4444 (11.4%***) 0.4323 0.4515 (4.4%**)
DOE AvgPrec 0.1759 0.1904 (8.3%***) 0.1804 0.1898 (5.2%**)
P@10 0.2629 0.2943 (11.9%*) 0.2600 0.2800 (7.7%*)
TREC8 AvgPrec 0.2392 0.2539 (6.01%**) 0.2567 0.2671 (4.05%*)
P@10 0.4300 0.4460 (3.7%) 0.4500 0.4740 (5.3%*)
Table 2: Comparisons with baselines. *,**,*** indicate that we accept the improvement hypothesis by
Wilcoxon test at significance level 0.1, 0.05, 0.01 respectively.
AP, TREC queries 51-150
Dirichlet DELM+Diri Improvement(%)
Rel. 21819 21819
Rel.Retr. 10126 10917 7.81% ***
Prec.
0.0 0.6404 0.6605 3.14% *
0.1 0.4333 0.4785 10.4% ***
0.2 0.3461 0.3983 15.1% ***
0.3 0.2960 0.3496 18.1% ***
0.4 0.2436 0.2962 21.6% ***
0.5 0.2060 0.2418 17.4% ***
0.6 0.1681 0.1975 17.5% ***
0.7 0.1290 0.1580 22.5% ***
0.8 0.0862 0.1095 27.0% **
0.9 0.0475 0.0695 46.3% **
1.0 0.0220 0.0257 16.8%
ave. 0.2168 0.2505 15.5% ***
Table 3: PR curve on AP data. *,**,*** indicate that
we accept the improvement hypothesis by Wilcoxon
test at significant level 0.1, 0.05, 0.01 respectively.
M when M is sufficiently large, namely 100. The
reason is twofold: First, since the neighborhood is
centered around the original document, when M is
large, the expansion may be evenly magnified on all
term dimensions. Second, the exponentially decay-
ing confidence values reduce the influence of remote
documents.
3.2 Comparison with CBDM
In this section, we compare the CBDM method us-
ing the model performing the best in (Liu and Croft,
2004)1. Furthermore, we also set Dirichlet prior pa-
rameter ? = 1000, as mentioned in (Liu and Croft,
2004), to rule out any potential influence of Dirichlet
smoothing.
Table 4 shows that our model outperforms CBDM
in MAP values on four data sets; the improvement
1We use the exact same data, queries, stemming and all
other preprocessing techniques. The baseline results in (Liu and
Croft, 2004) are confirmed.
0.17
0.18
0.19
0.2
0.21
0.22
0.23
0.24
0.25
0.26
0.27
0 100 200 300 400 500 600 700 800
a
ve
ra
ge
 p
re
ce
sio
n
M : the number of  neighborhood documents
AP
DOE
TREC8
Figure 3: Performance change with respect to M
CBDM DELM+Diri. improvement(%)
AP 0.2326 0.2505 7.7%
LA 0.2590 0.2655 2.5%
WSJ 0.3006 0.3113 3.6%
SJMN 0.2171 0.2266 4.3%
Table 4: Comparisons with CBDM.
presumably comes from a more principled way of
exploiting corpus structures. Given that clustering
can at least capture the local structure to some ex-
tent, it should not be very surprising that the im-
provement of document expansion over CBDM is
much less than that over the baselines.
Note that we cannot fulfill Wilcoxon test because
of the lack of the individual query results of CBDM.
3.3 Impact on short documents
Document expansion is to solve the insufficient sam-
pling problem. Intuitively, a short document is less
sufficient than a longer one, hence would need more
?help? from its neighborhood. We design experi-
ments to test this hypothesis.
Specifically, we randomly shrink each document
in AP88-89 to a certain percentage of its original
length. For example, a shrinkage factor of 30%
means each term has 30% chance to stay, or 70%
chance to be filtered out. In this way, we reduce the
original data set to a new one with the same number
412
average doc length 30% 50% 70% 100%
baseline 0.1273 0.1672 0.1916 0.2168
document expansion 0.1794 0.2137 0.2307 0.2505
optimal ? 0.2 0.3 0.3 0.4
improvement(%) 41% 28% 20% 16%
Table 5: Impact on short documents (in MAP)
0.14
0.16
0.18
0.2
0.22
0.24
0.26
0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9
a
ve
ra
ge
 p
re
cis
io
n
alpha
30%
50%
70%
100%
Figure 4: Performance change with respect to ?
of documents but a shorter average document length.
Table 5 shows the experiment results over docu-
ment sets with different average document lengths.
The results indeed support our hypothesis that doc-
ument expansion does help short documents more
than longer ones. While we can manage to improve
41% on a 30%-length corpus, the same model only
gets 16% improvement on the full length corpus.
To understand how ? affects the performance we
plot the sensitivity curves in Figure 4. The curves all
look similar, but the optimal points slightly migrate
when the average document length becomes shorter.
A 100% corpus gets optimal at ? = 0.4, but 30%
corpus has to use ? = 0.2 to obtain its optimum.
(All optimal ? values are presented in the fourth row
of Table 5.)
3.4 Further improvement with pseudo
feedback
Query expansion has been proved to be an effec-
tive way of utilizing corpus information to improve
the query representation (Rocchio, 1971; Zhai and
Lafferty, 2001a). It is thus interesting to examine
whether our model can be combined with query ex-
pansion to further improve the retrieval accuracy.
We use the model-based feedback proposed in (Zhai
and Lafferty, 2001a) and take top 5 returned docu-
ments for feedback. There are two parameters in the
model-based pseudo feedback process: the noisy pa-
DELM pseudo DELM+pseudo Impr.(%)
AP 0.2505 0.2643 0.2726 3.14%*
LA 0.2655 0.2769 0.2901 4.77%
TREC8 0.2671 0.2716 0.2809 3.42%**
DOE 0.1898 0.1918 0.2046 6.67%***
Table 6: Combination with pseudo feed-
back.*,**,*** indicate that we accept the improve-
ment hypothesis by Wilcoxon test at significant
level 0.1, 0.05, 0.01 respectively.
pseu. inter. combined (%) z-score
AP 0.2643 0.2450 0.2660 (0.64%) -0.2888
LA 0.2769 0.2662 0.2636 (-0.48%) -1.0570
TREC8 0.2716 0.2702 0.2739 (0.84%) -1.6938
Table 7: Performance of the interpolation algorithm
combined with the pseudo feedback.
rameter ? and the interpolation parameter ?2. We fix
? = 0.9 and tune ? to optimal, and use them directly
in the feedback process combined with our models.
(It again means that ? is probably not optimal in our
results.) The combination is conducted in the fol-
lowing way: (1) Retrieve documents by our DELM
method; (2) Choose top 5 document to do the model-
based feedback; (3) Use the expanded query model
to retrieve documents again with DELM method.
Table 6 shows the experiment results (MAP); in-
deed, by combining DELM with pseudo feedback,
we can obtain significant further improvement of
performance.
As another baseline, we also tested the algorithm
proposed in (Kurland and Lee, 2004). Since the al-
gorithm overlaps with pseudo feedback process, it is
not easy to further combine them. We implement its
best-performing algorithm, ?interpolation? (labeled
as inter. ), and show the results in Table 7. Here,
we use the same three data sets as used in (Kurland
and Lee, 2004). We tune the feedback parameters to
optimal in each experiment. The second last column
in Table 7 shows the performance of combination of
the ?interpolation? model with the pseudo feedback
and its improvement percentage. The last column is
the z-scores of Wilcoxon test. The negative z-scores
indicate that none of the improvement is significant.
2 (Zhai and Lafferty, 2001a) uses different notations. We
change them because ? has already been used in our own
model.
413
4 Conclusions
In this paper, we proposed a novel document expan-
sion method to enrich the document sample through
exploiting the local corpus structure. Unlike pre-
vious cluster-based models, we smooth each doc-
ument using a probabilistic neighborhood centered
around the document itself.
Experiment results show that (1) The proposed
document expansion method outperforms both the
?no expansion? baselines and the cluster-based mod-
els. (2) Our model is relatively insensitive to the set-
ting of parameter M as long as it is sufficiently large,
while the parameter ? should be set according to the
document length; short documents need a smaller
? to obtain more help from its neighborhood. (3)
Document expansion can be combined with pseudo
feedback to further improve performance. Since any
retrieval model can be presumably applied on top of
the expanded documents, we believe that the pro-
posed technique can be potentially useful for any re-
trieval model.
5 Acknowledgments
This work is in part supported by the National Sci-
ence Foundation under award number IIS-0347933.
We thank Xiaoyong Liu for kindly providing us sev-
eral processed data sets for our performance com-
parison. We thank Jing Jiang and Azadeh Shakery
for helping improve the paper writing, and thank the
anonymous reviewers for their useful comments.
References
S. F. Chen and J. Goodman. 1998. An empirical study of
smoothing techniques for language modeling. Techni-cal Report TR-10-98, Harvard University.
D. Hiemstra and W. Kraaij. 1998. Twenty-one at trec-7:
Ad-hoc and cross-language track. In Proc. of Seventh
Text REtrieval Conference (TREC-7).
R. Kneser and H. Ney. 1995. Improved smoothing for m-
gram languagemodeling. In Proceedings of the Inter-
national Conference on Acoustics, Speech and Signal
Processing.
Oren Kurland and Lillian Lee. 2004. Corpus structure,language models, and ad hoc information retrieval. In
SIGIR ?04: Proceedings of the 27th annual interna-
tional conference on Research and development in in-
formation retrieval, pages 194?201. ACM Press.
John Lafferty and Chengxiang Zhai. 2001. Document
language models, query models, and risk minimiza-tion for information retrieval. In Proceedings of SI-
GIR?2001, pages 111?119, Sept.
John Lafferty and ChengXiang Zhai. 2003. Probabilistic
relevance models based on document and query gen-eration.
Victor Lavrenko and Bruce Croft. 2001. Relevance-based language models. In Proceedings of SI-
GIR?2001, Sept.
Xiaoyong Liu and W. Bruce Croft. 2004. Cluster-basedretrieval using language models. In SIGIR ?04: Pro-
ceedings of the 27th annual international conference
on Research and development in information retrieval,pages 186?193. ACM Press.
D. H. Miller, T. Leek, and R. Schwartz. 1999. A hid-den markov model information retrieval system. In
Proceedings of the 1999 ACM SIGIR Conference on
Research and Development in Information Retrieval,pages 214?221.
J. Ponte and W. B. Croft. 1998. A language modeling
approach to information retrieval. In Proceedings of
the ACM SIGIR, pages 275?281.
J. Rocchio. 1971. Relevance feedback in information re-trieval. In The SMART Retrieval System: Experiments
in Automatic Document Processing, pages 313?323.Prentice-Hall Inc.
Amit Singhal and Fernando Pereira. 1999. Documentexpansion for speech retrieval. In SIGIR ?99: Pro-
ceedings of the 22nd annual international ACM SIGIR
conference on Research and development in informa-
tion retrieval, pages 34?41. ACM Press.
E. Voorhees and D. Harman, editors. 2001. Proceedings
of Text REtrieval Conference (TREC1-9). NIST Spe-cial Publications. http://trec.nist.gov/pubs.html.
Chengxiang Zhai and John Lafferty. 2001a. Model-based feedback in the KL-divergence retrieval model.
In Tenth International Conference on Information and
Knowledge Management (CIKM 2001), pages 403?410.
Chengxiang Zhai and John Lafferty. 2001b. A study
of smoothing methods for language models applied toad hoc information retrieval. In Proceedings of SI-
GIR?2001, pages 334?342, Sept.
414
Proceedings of ACL-08: HLT, pages 816?824,
Columbus, Ohio, USA, June 2008. c?2008 Association for Computational Linguistics
Generating Impact-Based Summaries for Scientific Literature
Qiaozhu Mei
University of Illinois at Urbana-
Champaign
qmei2@uiuc.edu
ChengXiang Zhai
University of Illinois at Urbana-
Champaign
czhai@cs.uiuc.edu
Abstract
In this paper, we present a study of a novel
summarization problem, i.e., summarizing the
impact of a scientific publication. Given a pa-
per and its citation context, we study how to
extract sentences that can represent the most
influential content of the paper. We propose
language modeling methods for solving this
problem, and study how to incorporate fea-
tures such as authority and proximity to ac-
curately estimate the impact language model.
Experiment results on a SIGIR publication
collection show that the proposed methods
are effective for generating impact-based sum-
maries.
1 Introduction
The volume of scientific literature has been growing
rapidly. From recent statistics, each year 400,000
new citations are added to MEDLINE, the major
biomedical literature database 1. This fast growth
of literature makes it difficult for researchers, espe-
cially beginning researchers, to keep track of the re-
search trends and find high impact papers on unfa-
miliar topics.
Impact factors (Kaplan and Nelson, 2000) are
useful, but they are just numerical values, so they
cannot tell researchers which aspects of a paper are
influential. On the other hand, a regular content-
based summary (e.g., the abstract or conclusion sec-
tion of a paper or an automatically generated topical
summary (Giles et al, 1998)) can help a user know
1http://www.nlm.nih.gov/bsd/history/tsld024.htm
about the main content of a paper, but not necessar-
ily the most influential content of the paper. Indeed,
the abstract of a paper mostly reflects the expected
impact of the paper as perceived by the author(s),
which could significantly deviate from the actual im-
pact of the paper in the research community. More-
over, the impact of a paper changes over time due to
the evolution and progress of research in a field. For
example, an algorithm published a decade ago may
be no longer the state of the art, but the problem def-
inition in the same paper can be still well accepted.
Although much work has been done on text sum-
marization (See Section 6 for a detailed survey), to
the best of our knowledge, the problem of impact
summarization has not been studied before. In this
paper, we study this novel summarization problem
and propose language modeling-based approaches
to solving the problem. By definition, the impact
of a paper has to be judged based on the consent of
research community, especially by people who cited
it. Thus in order to generate an impact-based sum-
mary, we must use not only the original content, but
also the descriptions of that paper provided in papers
which cited it, making it a challenging task and dif-
ferent from a regular summarization setup such as
news summarization. Indeed, unlike a regular sum-
marization system which identifies and interprets the
topic of a document, an impact summarization sys-
tem should identify and interpret the impact of a pa-
per.
We define the impact summarization problem in
the framework of extraction-based text summariza-
tion (Luhn, 1958; McKeown and Radev, 1995), and
cast the problem as an impact sentence retrieval
816
problem. We propose language models to exploit
both the citation context and original content of a
paper to generate an impact-based summary. We
study how to incorporate features such as author-
ity and proximity into the estimation of language
models. We propose and evaluate several different
strategies for estimating the impact language model,
which is key to impact summarization. No exist-
ing test collection is available for evaluating impact
summarization. We construct a test collection us-
ing 28 years of ACM SIGIR papers (1978 - 2005)
to evaluate the proposed methods. Experiment re-
sults on this collection show that the proposed ap-
proaches are effective for generating impact-based
summaries. The results also show that using both the
original document content and the citation contexts
is important and incorporating citation authority and
proximity is beneficial.
An impact-based summary is not only useful for
facilitating the exploration of literature, but also
helpful for suggesting query terms for literature
retrieval, understanding the evolution of research
trends, and identifying the interactions of different
research fields. The proposed methods are also ap-
plicable to summarizing the impact of documents in
other domains where citation context exists, such as
emails and weblogs.
The rest of the paper is organized as follows. In
Section 2 and 3, we define the impact-based summa-
rization problem and propose the general language
modeling approach. In Section 4, we present differ-
ent strategies and features for estimating an impact
language model, a key challenge in impact summa-
rization. We discuss our experiments and results in
Section 5. Finally, the related work and conclusions
are discussed in Section 6 and Section 7.
2 Impact Summarization
Following the existing work on topical summariza-
tion of scientific literature (Paice, 1981; Paice and
Jones, 1993), we define an impact-based summary
of a paper as a set of sentences extracted from
a paper that can reflect the impact of the paper,
where ?impact? is roughly defined as the influence
of the paper on research of similar or related top-
ics as reflected in the citations of the paper. Such
an extraction-based definition of summarization has
also been quite common in most existing general
summarization work (Radev et al, 2002).
By definition, in order to generate an impact sum-
mary of a paper, we must look at how other papers
cite the paper, use this information to infer the im-
pact of the paper, and select sentences from the orig-
inal paper that can reflect the inferred impact. Note
that we do not directly use the sentences from the ci-
tation context to form a summary. This is because in
citations, the discussion of the paper cited is usually
mixed with the content of the paper citing it, and
sometimes also with discussion about other papers
cited (Siddharthan and Teufel, 2007).
Formally, let d = (s0, s1, ..., sn) be a paper tobe summarized, where si is a sentence. We referto a sentence (in another paper) in which there is
an explicit citation of d as a citing sentence of d.
When a paper is cited, it is often discussed consec-
utively in more than one sentence near the citation,
thus intuitively we would like to consider a window
of sentences centered at a citing sentence; the win-
dow size would be a parameter to set. We call such
a window of sentences a citation context, and use C
to denote the union of all the citation contexts of d
in a collection of research papers. Thus C itself is
a set (more precisely bag) of sentences. The task
of impact-based summarization is thus to 1) con-
struct a representation of the impact of d, I , based
on d and C; 2) design a scoring function Score(.)
to rank sentences in d based on how well a sentence
reflects I . A user-defined number of top-ranked sen-
tences can then be selected as the impact summary
for d.
The formulation above immediately suggests that
we can cast the impact summarization problem as
a retrieval problem where each candidate sentence
in d is regarded as a ?document,? the impact of the
paper (i.e., I) as a ?query,? and our goal is to ?re-
trieve? sentences that can reflect the impact of the
paper as indicated by the citation context. Looking
at the problem in this way, we see that there are two
main challenges in impact summarization: first, we
must be able to infer the impact based on both the
citation contexts and the original document; second,
we should measure how well a sentence reflects this
inferred impact. To solve these challenges, in the
next section, we propose to model impact with un-
igram language models and score sentences using
817
Kullback-Leibler divergence. We further propose
methods for estimating the impact language model
based on several features including the authority of
citations, and the citation proximity.
3 Language Models for Impact
Summarization
3.1 Impact language models
From the retrieval perspective, our collection is the
paper to be summarized, and each sentence is a
?document? to be retrieved. However, unlike in the
case of ad hoc retrieval, we do not really have a
query describing the impact of the paper; instead,
we have a lot of citation contexts that can be used
to infer information about the query. Thus the main
challenge in impact summarization is to effectively
construct a ?virtual impact query? based on the cita-
tion contexts.
What should such a virtual impact query look
like? Intuitively, it should model the impact-
reflecting content of the paper. We thus propose to
represent such a virtual impact query with a unigram
language model. Such a model is expected to assign
high probabilities to those words that can describe
the impact of paper d, just as we expect a query
language model in ad hoc retrieval to assign high
probabilities to words that tend to occur in relevant
documents (Ponte and Croft, 1998). We call such a
language model the impact language model of paper
d (denoted as ?I ); it can be estimated based on both
d and its citation context C as will be discussed in
Section 4.
3.2 KL-divergence scoring
With the impact language model in place, we
can then adopt many existing probabilistic retrieval
models such as the classical probabilistic retrieval
models (Robertson and Sparck Jones, 1976) and the
Kullback-Leibler (KL) divergence retrieval model
(Lafferty and Zhai, 2001; Zhai and Lafferty, 2001a),
to solve the problem of impact summarization by
scoring sentences based on the estimated impact lan-
guage model. In our study, we choose to use the KL-
divergence scoring method to score sentences as this
method has performed well for regular ad hoc re-
trieval tasks (Zhai and Lafferty, 2001a) and has an
information theoretic interpretation.
To apply the KL-divergence scoring method, we
assume that a candidate sentence s is generated from
a sentence language model ?s. Given s in d and thecitation context C , we would first estimate ?s basedon s and estimate ?I based on C , and then score swith the negative KL divergence of ?s and ?I . Thatis,
Score(s) = ?D(?I ||?s)
=
?
w?V
p(w|?I) log p(w|?s)?
?
w?V
p(w|?I) log p(w|?I)
where V is the set of words in our vocabulary and w
denotes a word.
From the information theoretic perspective, the
KL-divergence of ?s and ?I can be interpretedas measuring the average number of bits wasted
in compressing messages generated according to
?I (i.e., impact descriptions) with coding non-optimally designed based on ?s. If ?s and ?I arevery close, the KL-divergence would be small and
Score(s) would be high, which intuitively makes
sense. Note that the second term (entropy of ?I ) isindependent of s, so it can be ignored for ranking s.
We see that according to the KL-divergence scor-
ing method, our main tasks are to estimate ?s and
?I . Since s can be regarded as a short document, wecan use any standard method to estimate ?s. In thiswork, we use Dirichlet prior smoothing (Zhai and
Lafferty, 2001b) to estimate ?s as follows:
p(w|?s) =
c(w, s) + ?s ? P (w|D)
|s| + ?s
(1)
where |s| is the length of s, c(w, s) is the count of
word w in s, p(w|D) is a background model esti-
mated using c(w,D)P
w??V c(w?,D)
(D can be the set of all
the papers available to us) and ?s is a smoothing pa-rameter to be empirically set. Note that as the length
of a sentence is very short, smoothing is critical for
addressing the data sparseness problem.
The remaining challenge is to estimate ?I accu-rately based on d and its citation contexts.
4 Estimation of Impact Language Models
Intuitively, the impact of a paper is mostly reflected
in the citation context. Thus the estimation of the
impact language model should be primarily based
on the citation context C . However, we would like
818
our impact model to be able to help us select impact-
reflecting sentences from d, thus it is important for
the impact model to explain well the paper content
in general. To achieve this balance, we treat the ci-
tation context C as prior information and the current
document d as the observed data, and use Bayesian
estimation to estimate the impact language model.
Specifically, let p(w|C) be a citation context lan-
guage model estimated based on the citation con-
text C . We define Dirichlet prior with parameters
{?Cp(w|C)}w?V for the impact model, where ?Cencodes our confidence on this prior and effectively
serves as a weighting parameter for balancing the
contribution of C and d for estimating the impact
model. Given the observed document d, the poste-
rior mean estimate of the impact model would be
(MacKay and Peto, 1995; Zhai and Lafferty, 2001b)
P (w|?I) =
c(w, d) + ?cp(w|C)
|d| + ?c
(2)
?c can be interpreted as the equivalent sample size ofour prior. Thus setting ?c = |d| means that we putequal weights on the citation context and the doc-
ument itself. ?c = 0 yields p(w|?I) = p(w|d),which is to say that the impact is entirely captured
by the paper itself, and our impact summarization
problem would then become the standard single doc-
ument (topical) summarization. Intuitively though,
we would want to set ?c to a relatively large num-ber to exploit the citation context in our estimation,
which is confirmed in our experiments.
An alternative way is to simply interpolate p(w|d)
and p(w|C) with a constant coefficient:
p(w|?I) = (1 ? ?)p(w|d) + ?p(w|C) (3)
We will compare the two strategies in Section 5.
How do we estimate p(w|C)? Intuitively, words
occurring in C frequently should have high proba-
bilities. A simple way is to pool together all the sen-
tences in C and use the maximum likelihood estima-
tor,
p(w|C) =
?
s?C c(w, s)
?
w??V
?
s??C c(w?, s?)
(4)
where c(w, s) is the count of w in s.
One deficiency of this simple estimate is that we
treat all the (extended) citation sentences equally.
However, there are at least two reasons why we want
to assign unequal weights to different citation sen-
tences: (1) A sentence closer to the citation label
should contribute more than one far away. (2) A sen-
tence occurring in a highly authorative paper should
contribute more than that in a less authorative paper.
To capture these two heuristics, we define a weight
coefficient ?s for a sentence s in C as follows:
?s = pg(s)pr(s)
where pg(s) is an authority score of the paper con-
taining s and pr(s) is a proximity score that rewards
a sentence close to the citation label.
For example, pg(s) can be the PageRank value
(Brin and Page, 1998) of the document with s, which
measures the authority of the document based on a
citation graph, and is computed as follows: We con-
struct a directed graph from the collection of scien-
tific literature with each paper as a vertex and each
citation as a directed edge pointing from the citing
paper to the cited paper. We can then use the stan-
dard PageRank algorithm (Brin and Page, 1998) to
compute a PageRank value for each document. We
used this approach in our experiments.
We define pr(s) as pr(s) = 1?k , where k is thedistance (counted in terms of the number of sen-
tences) between sentence s and the center sentence
of the window containing s; by ?center sentence?,
we mean the citing sentence containing the citation
label. Thus the sentence with the citation label will
have a proximity of 1 (because k = 0), while the
sentences away from the citation label will have a
decaying weight controlled by parameter ?.
With ?s, we can then use the following?weighted? maximum likelihood estimate for the
impact language model:
p(w|C) =
?
s?C ?sc(w, s)
?
w??V
?
s??C ?s?c(w?, s?)
(5)
As we will show in Section 5, this weighted
maximum likelihood estimate performs better than
the simple maximum likelihood estimate, and both
pg(s) and pr(s) are useful.
819
5 Experiments and Results
5.1 Experiment Design
5.1.1 Test set construction
Because no existing test set is available for evalu-
ating impact summarization, we opt to create a test
set based on 28 years of ACM SIGIR papers (1978
- 2005) available through the ACM Digital Library2
and the SIGIR membership. Leveraging the explicit
citation information provided by ACM Digital Li-
brary, for each of the 1303 papers, we recorded all
other papers that cited the paper and extracted the
citation context from these citing papers. Each ci-
tation context contains 5 sentences with 2 sentences
before and after the citing sentence.
Since a low-impact paper would not be useful for
evaluating impact summarization, we took all the
14 papers from the SIGIR collection that have no
less than 20 citations by papers in the same col-
lection as candidate papers for evaluation. An ex-
pert in Information Retrieval field read each paper
and its citation context, and manually created an
impact-based summary by selecting all the ?impact-
capturing? sentences from the paper. Specifically,
the expert first attempted to understand the most in-
fluential content of a paper by reading the citation
contexts. The expert then read each sentence of
the paper and made a decision whether the sentence
covers some ?influential content? as indicated in the
citation contexts. The sentences that were decided
as covering some influential content were then col-
lected as the gold standard impact summary for the
paper.
We assume that the title of a paper will always
be included in the summary, so we excluded the ti-
tle both when constructing the gold standard and
when generating a summary. The gold standard
summaries have a minimum length of 5 sentences
and a maximum length of 18 sentences; the me-
dian length is 9 sentences. These 14 impact-based
summaries are used as gold standards for our exper-
iments, based on which all summaries generated by
the system are evaluated. This data set is available at
http://timan.cs.uiuc.edu/data/impact.html. We must
admit that using only 14 papers and only one expert
for evaluation is a limitation of our work. However,
2http://www.acm.org/dl
going beyond the 14 papers would risk reducing the
reliability of impact judgment due to the sparseness
of citations. How to develop a better test collection
is an important future direction.
5.1.2 Evaluation Metrics
Following the current practice in evaluating sum-
marization, particularly DUC3, we use the ROUGE
evaluation package (Lin and Hovy, 2003). Among
ROUGE metrics, ROUGE-N (models n-gram co-
occurrence, N = 1, 2) and ROUGE-L (models
longest common sequence) generally perform well
in evaluating both single-document summarization
and multi-document summarization (Lin and Hovy,
2003). Since they are general evaluation measures
for summarization, they are also applicable to eval-
uating the MEAD-Doc+Cite baseline method to be
described below. Thus although we evaluated our
methods with all the metrics provided by ROUGE,
we only report ROUGE-1 and ROUGE-L in this pa-
per (other metrics give very similar results).
5.1.3 Baseline methods
Since impact summarization has not been previ-
ously studied, there is no natural baseline method to
compare with. We thus adapt some state-of-the-art
conventional summarization methods implemented
in the MEAD toolkit (Radev et al, 2003)4 to obtain
three baseline methods: (1) LEAD: It simply ex-
tracts sentences from the beginning of a paper, i.e.,
sentences in the abstract or beginning of the intro-
duction section; we include LEAD to see if such
?leading sentences? reflect the impact of a paper as
authors presumably would expect to summarize a
paper?s contributions in the abstract. (2) MEAD-
Doc: It uses the single-document summarizer in
MEAD to generate a summary based solely on the
original paper; comparison with this baseline can
tell us how much better we can do than a conven-
tional topic-based summarizer that does not consider
the citation context. (3) MEAD-Doc+Cite: Here
we concatenate all the citation contexts in a paper to
form a ?citation document? and then use the MEAD
multidocument summarizer to generate a summary
from the original paper plus all its citation docu-
ments; this baseline represents a reasonable way
3http://duc.nist.gov/
4?http://www.summarization.com/mead/?
820
Sum. Length Metric Random LEAD MEAD-Doc MEAD-Doc+Cite KL-Divergence
3 ROUGE-1 0.163 0.167 0.301* 0.248 0.323
3 ROUGE-L 0.144 0.158 0.265 0.217 0.299
5 ROUGE-1 0.230 0.301 0.401 0.333 0.467
5 ROUGE-L 0.214 0.292 0.362 0.298 0.444
10 ROUGE-1 0.430 0.514 0.575 0.472 0.649
10 ROUGE-L 0.396 0.494 0.535 0.428 0.622
15 ROUGE-1 0.538 0.610 0.685 0.552 0.730
15 ROUGE-L 0.499 0.586 0.650 0.503 0.705
Table 1: Performance Comparison of Summarizers
of applying an existing summarization method to
generate an impact-based summary. Note that this
method may extract sentences in the citation con-
texts but not in the original paper.
5.2 Basic Results
We first show some basic results of impact sum-
marization in Table 1. They are generated us-
ing constant coefficient interpolation for the impact
language model (i.e., Equation 3) with ? = 0.8,
weighted maximum likelihood estimate for the ci-
tation context model (i.e., Equation 5) with ? = 3,
and ?s = 1, 000 for candidate sentence smoothing(Equation 1). These results are not necessarily opti-
mal as will be seen when we examine parameter and
method variations.
From Table 1, we see clearly that our method
consistently outperforms all the baselines. Among
the baselines, MEAD-Doc is consistently better than
both LEAD and MEAD-Doc+Cite. While MEAD-
Doc?s outperforming LEAD is not surprising, it is
a bit surprising that MEAD-Doc also outperforms
MEAD-Doc+Cite as the latter uses both the cita-
tion context and the original document. One possi-
ble explanation may be that MEAD is not designed
for impact summarization and it has been trapped
by the distracting content in the citation context 5.
Indeed, this can also explain why MEAD-Doc+Cite
tends to perform worse than LEAD by ROUGE-L
since if MEAD-Doc+Cite picks up sentences from
the citation context rather than the original papers,
it would not match as well with the gold standard
as LEAD which selects sentences from the origi-
5One anonymous reviewer suggested an interesting im-
provement to the MEAD-Doc+Cite baseline, in which we
would first extract sentences from the citation context and then
for each extracted sentence find a similar one in the original pa-
per. Unfortunately, we did not have time to test this approach
before the deadline for the camera-ready version of this paper.
nal papers. These results thus show that conven-
tional summarization techniques are inadequate for
impact summarization, and the proposed language
modeling methods are more effective for generating
impact-based summaries.
In Table 2, we show a sample impact-based sum-
mary and the corresponding MEAD-Doc regular
summary. We see that the regular summary tends
to have general sentences about the problem, back-
ground and techniques, not very informative in con-
veying specific contributions of the paper. None of
these sentences was selected by the human expert. In
contrast, the sentences in the impact summary cover
several details of the impact of the paper (i.e., spe-
cific smoothing methods especially Dirichlet prior,
sensitivity of performance to smoothing, and dual
role of smoothing), and sentences 4 and 6 are also
among the 8 sentences picked by the human expert.
Interestingly, neither sentence is in the abstract of
the original paper, suggesting a deviation of the ac-
tual impact of a paper and that perceived by the au-
thor(s).
5.3 Component analysis
We now turn to examine the effectiveness of each
component in the proposed methods and different
strategies for estimating ?I .
Effectiveness of interpolation: We hypothesized
that we need to use both the original document and
the citation context to estimate ?I . To test this hy-pothesis, we compare the results of using only d,
only the citation context, and interpolation of them
in Table 3. We show two different strategies of inter-
polation (i.e., constant coefficient with ? = 0.8 and
Dirichlet with ?c = 20, 000) as described in Sec-tion 4.
From Table 3, we see that both strategies of in-
terpolation indeed outperform using either the origi-
821
Impact-based summary:
1. Figure 5: Interpolation versus backoff for Jelinek-Mercer (top), Dirichlet smoothing (middle), and absolute discounting (bottom).
2. Second, one can de-couple the two different roles of smoothing by adopting a two stage smoothing strategy in which Dirichlet smoothing is
first applied to implement the estimation role and Jelinek-Mercer smoothing is then applied to implement the role of query modeling
3. We find that the backoff performance is more sensitive to the smoothing parameter than that of interpolation, especially in Jelinek-Mercer
and Dirichlet prior.
4. We then examined three popular interpolation-based smoothing methods (Jelinek-Mercer method, Dirichlet priors, and absolute discounting),
as well as their backoff versions, and evaluated them using several large and small TREC retrieval testing collections.
summary 5. By rewriting the query-likelihood retrieval model using a smoothed document language model, we derived a general retrieval
formula where the smoothing of the document language model can be interpreted in terms of several heuristics used intraditional models,
including TF-IDF weighting and document length normalization.
6. We find that the retrieval performance is generally sensitive to the smoothing parameters, suggesting that an understanding and appropriate
setting of smoothing parameters is very important in the language modeling approach.
Regular summary (generated using MEAD-Doc):
1. Language modeling approaches to information retrieval are attractive and promising because they connect the problem of retrieval with that
of language model estimation, which has been studied extensively in other application areas such as speech recognition.
2. The basic idea of these approaches is to estimate a language model for each document, and then rank documents by the likelihood of the
query according to the estimated language model.
3. On the one hand, theoretical studies of an underlying model have been developed; this direction is, for example, represented by the various
kinds of logic models and probabilistic models (e.g., [14, 3, 15, 22]).
4. After applying the Bayes? formula and dropping a document-independent constant (since we are only interested in ranking documents), we
have p(d|q) ? (q|d)p(d).
5. As discussed in [1], the righthand side of the above equation has an interesting interpretation, where, p(d) is our prior belief that d is relevant
to any query and p(q|d) is the query likelihood given the document, which captures how well the document ?fits? the particular query q.
6. The probability of an unseen word is typically taken as being proportional to the general frequency of the word, e.g., as computed using the
document collection.
Table 2: Impact-based summary vs. regular summary for the paper ?A study of smoothing methods for language
models applied to ad hoc information retrieval?.
nal document model (p(w|d)) or the citation context
model (p(w|C)) alone, which confirms that both the
original paper and the citation context are important
for estimating ?I . We also see that using the citationcontext alone is better than using the original paper
alone, which is expected. Between the two strate-
gies, Dirichlet dynamic coefficient is slightly better
than constant coefficient (CC), after optimizing the
interpolation parameter for both strategy.
Interpolation
Measure P (w|d) P (w|C) ConstCoef Dirichlet
ROUGE-1 0.529 0.635 0.643 0.647
ROUGE-L 0.501 0.607 0.619 0.623
Table 3: Effectiveness of interpolation
Citation authority and proximity: These heuris-
tics are very interesting to study as they are unique
to impact summarization and not well studied in the
existing summarization work.
pg(s) pr(s)=1/?k
pr(s) off ? = 2 ? = 3 ? = 4
Off 0.685 0.711 0.714 0.700
On 0.708 0.712 0.706 0.703
Table 4: Authority (pg(s)) and proximity (pr(s))
In Table 4, we show the ROUGE-L values for var-
ious combinations of these two heuristics (summary
length is 15). We turn off either pg(s) or pr(s) by
setting it to a constant; when both are turned off, we
have the unweighted MLE of p(w|C) (Equation 4).
Clearly, using weighted MLE with any of the two
heuristics is better than the unweighted MLE, indi-
cating that both heuristics are effective. However,
combining the two heuristics does not always im-
prove over using a single one. Since intuitively these
two heuristics are orthogonal, this may suggest that
our way of combining the two scores (i.e., taking a
product of them) may not be optimal; further study
is needed to better understand this. The ROUGE-1
results are similar.
Tuning of other parameters: There are three other
parameters which need to be tuned: (1) ?s for can-didate sentence smoothing (Equation 1); (2) ?c inDirichlet interpolation for impact model estimation
(Equation 2); and (3) ? in constant coefficient inter-
polation (Equation 3). We have examined the sen-
sitivity of performance to these parameters. In gen-
eral, for a wide range of values of these parameters,
the performance is relatively stable and near opti-
mal. Specifically, the performance is near optimal as
822
long as ?s and ?c are sufficiently large (?s ? 1000,
?c ? 20, 000), and the interpolation parameter ? isbetween 0.4 and 0.9.
6 Related Work
General text summarization, including single docu-
ment summarization (Luhn, 1958; Goldstein et al,
1999) and multi-document summarization (Kraaij et
al., 2001; Radev et al, 2003) has been well stud-
ied; our work is under the framework of extractive
summarization (Luhn, 1958; McKeown and Radev,
1995; Goldstein et al, 1999; Kraaij et al, 2001),
but our problem formulation differs from any exist-
ing formulation of the summarization problem. It
differs from regular single-document summarization
because we utilize extra information (i.e. citation
contexts) to summarize the impact of a paper. It also
differs from regular multi-document summarization
because the roles of original documents and cita-
tion contexts are not equivalent. Specifically, cita-
tion contexts serve as an indicator of the impact of
the paper, but the summary is generated by extract-
ing the sentences from the original paper.
Technical paper summarization has also been
studied (Paice, 1981; Paice and Jones, 1993; Sag-
gion and Lapalme, 2002; Teufel and Moens, 2002),
but the previous work did not explore citation con-
text to emphasize the impact of papers.
Citation context has been explored in several
studies (Nakov et al, 2004; Ritchie et al, 2006;
Schwartz et al, 2007; Siddharthan and Teufel,
2007). However, none of the previous studies has
used citation context in the same way as we did,
though the potential of directly using citation sen-
tences (called citances) to summarize a paper was
pointed out in (Nakov et al, 2004).
Recently, people have explored various types of
auxiliary knowledge such as hyperlinks (Delort et
al., 2003) and clickthrough data (Sun et al, 2005), to
summarize a webpage; such work is related to ours
as anchor text is similar to citation context, but it is
based on a standard formulation of multi-document
summarization and would contain only sentences
from anchor text.
Our work is also related to work on using lan-
guage models for retrieval (Ponte and Croft, 1998;
Zhai and Lafferty, 2001b; Lafferty and Zhai, 2001)
and summarization (Kraaij et al, 2001). However,
we do not have an explicit query and constructing
the impact model is a novel exploration. We also
proposed new language models to capture the im-
pact.
7 Conclusions
We have defined and studied the novel problem of
summarizing the impact of a research paper. We cast
the problem as an impact sentence retrieval problem,
and proposed new language models to model the im-
pact of a paper based on both the original content
of the paper and its citation contexts in a literature
collection with consideration of citation autority and
proximity.
To evaluate impact summarization, we created a
test set based on ACM SIGIR papers. Experiment
results on this test set show that the proposed im-
pact summarization methods are effective and out-
perform several baselines that represent the existing
summarization methods.
An important future work is to construct larger
test sets (e.g., of biomedical literature) to facilitate
evaluation of impact summarization. Our formula-
tion of the impact summarization problem can be
further improved by going beyond sentence retrieval
and considering factors such as redundancy and co-
herency to better organize an impact summary. Fi-
nally, automatically generating impact-based sum-
maries can not only help users access and digest
influential research publications, but also facilitate
other literature mining tasks such as milestone min-
ing and research trend monitoring. It would be in-
teresting to explore all these applications.
Acknowledgments
We are grateful to the anonymous reviewers for their
constructive comments. This work is in part sup-
ported by a Yahoo! Graduate Fellowship and NSF
grants under award numbers 0713571, 0347933, and
0428472.
References
Sergey Brin and Lawrence Page. 1998. The anatomy
of a large-scale hypertextual web search engine. In
Proceedings of the Seventh International Conference
on World Wide Web, pages 107?117.
823
J.-Y. Delort, B. Bouchon-Meunier, and M. Rifqi. 2003.
Enhanced web document summarization using hyper-
links. In Proceedings of the Fourteenth ACM Confer-
ence on Hypertext and Hypermedia, pages 208?215.
C. Lee Giles, Kurt D. Bollacker, and Steve Lawrence.
1998. Citeseer: an automatic citation indexing sys-
tem. In Proceedings of the Third ACM Conference on
Digital Libraries, pages 89?98.
Jade Goldstein, Mark Kantrowitz, Vibhu Mittal, and
Jaime Carbonell. 1999. Summarizing text documents:
sentence selection and evaluation metrics. In Proceed-
ings of ACM SIGIR 99, pages 121?128.
Nancy R. Kaplan and Michael L. Nelson. 2000. Deter-
mining the publication impact of a digital library. J.
Am. Soc. Inf. Sci., 51(4):324?339.
W. Kraaij, M. Spitters, and M. van der Heijden. 2001.
Combining a mixture language model and naive bayes
for multi-document summarisation. In Proceedings of
the DUC2001 workshop.
John Lafferty and Chengxiang Zhai. 2001. Document
language models, query models, and risk minimiza-
tion for information retrieval. In Proceedings of ACM
SIGIR 2001, pages 111?119.
Chin-Yew Lin and Eduard Hovy. 2003. Automatic evalu-
ation of summaries using n-gram co-occurrence statis-
tics. In Proceedings of the 2003 Conference of the
North American Chapter of the Association for Com-
putational Linguistics on Human Language Technol-
ogy, pages 71?78.
H. P. Luhn. 1958. The automatic creation of literature
abstracts. IBM Journal of Research and Development,
2(2):159?165.
D. MacKay and L. Peto. 1995. A hierarchical Dirich-
let language model. Natural Language Engineering,
1(3):289?307.
Kathleen McKeown and Dragomir R. Radev. 1995. Gen-
erating summaries of multiple news articles. In Pro-
ceedings of the 18th Annual International ACM SIGIR
Conference on Research and Development in Informa-
tion Retrieval, pages 74?82.
P. Nakov, A. Schwartz, and M. Hearst. 2004. Citances:
Citation sentences for semantic analysis of bioscience
text. In Proceedings of ACM SIGIR?04 Workshop on
Search and Discovery in Bioinformatics.
Chris D. Paice and Paul A. Jones. 1993. The identifi-
cation of important concepts in highly structured tech-
nical papers. In Proceedings of the 16th Annual In-
ternational ACM SIGIR Conference on Research and
Development in Information Retrieval, pages 69?78.
C. D. Paice. 1981. The automatic generation of literature
abstracts: an approach based on the identification of
self-indicating phrases. In Proceedings of the 3rd An-
nual ACM Conference on Research and Development
in Information Retrieval, pages 172?191.
Jay M. Ponte and W. Bruce Croft. 1998. A language
modeling approach to information retrieval. In Pro-
ceedings of the 21st Annual International ACM SIGIR
Conference on Research and Development in Informa-
tion Retrieval, pages 275?281.
Dragomir R. Radev, Eduard Hovy, and Kathleen McKe-
own. 2002. Introduction to the special issue on sum-
marization. Comput. Linguist., 28(4):399?408.
Dragomir R. Radev, Simone Teufel, Horacio Saggion,
Wai Lam, John Blitzer, Hong Qi, Arda Celebi, Danyu
Liu, and Elliott Drabek. 2003. Evaluation challenges
in large-scale document summarization: the mead
project. In Proceedings of the 41st Annual Meeting
on Association for Computational Linguistics, pages
375?382.
A. Ritchie, S. Teufel, and S. Robertson. 2006. Creating
a test collection for citation-based ir experiments. In
Proceedings of the HLT-NAACL 2006, pages 391?398.
S. Robertson and K. Sparck Jones. 1976. Relevance
weighting of search terms. Journal of the American
Society for Information Science, 27:129?146.
Hpracop Saggion and Guy Lapalme. 2002. Generating
indicative-informative summaries with sumUM. Com-
putational Linguistics, 28(4):497?526.
A. S. Schwartz, A. Divoli, and M. A. Hearst. 2007. Mul-
tiple alignment of citation sentences with conditional
random fields and posterior decoding. In Proceedings
of the 2007 EMNLP-CoNLL, pages 847?857.
A. Siddharthan and S. Teufel. 2007. Whose idea was
this, and why does it matter? attributing scientific
work to citations. In Proceedings of NAACL/HLT-07,
pages 316?323.
Jian-Tao Sun, Dou Shen, Hua-Jun Zeng, Qiang Yang,
Yuchang Lu, and Zheng Chen. 2005. Web-page sum-
marization using clickthrough data. In Proceedings
of the 28th Annual International ACM SIGIR Confer-
ence on Research and Development in Information Re-
trieval, pages 194?201.
Simone Teufel and Marc Moens. 2002. Summariz-
ing scientific articles: experiments with relevance and
rhetorical status. Comput. Linguist., 28(4):409?445.
ChengXiang Zhai and John Lafferty. 2001a. Model-
based feedback in the language modeling approach
to information retrieval. In Proceedings of the Tenth
International Conference on Information and Knowl-
edge Management (CIKM 2001), pages 403?410.
Chengxiang Zhai and John Lafferty. 2001b. A study
of smoothing methods for language models applied to
ad hoc information retrieval. In Proceedings of the
24th Annual International ACM SIGIR Conference on
Research and Development in Information Retrieval,
pages 334?342.
824
 From Text to Exhibitions: A New Approach for E-Learning on Language and 
Literature based on Text Mining 
Qiaozhu Mei 
Department of Electrical Engineering and 
Computer Science 
Vanderbilt University  
Box 1679 Station B  
Nashville, TN 37235 USA 
qiaozhu.mei@vanderbilt.edu 
Junfeng Hu 
Department of Computer Science  
Institute of Computational Linguistics 
Peking University 
100871, Beijing, China 
hujf@pku.edu.cn 
 
Abstract 
Unlike many well established approaches for 
E-Learning on science fields, there isn?t a 
commonly accepted approach of E-Learning 
on humanities fields, especially language and 
literature. Because the knowledge on language 
and literature depends too much on texts, 
advanced text processing has become a 
bottleneck for E-Learning on these domains. 
In traditional learning frameworks learners 
would easily get boring with mass pure texts. 
This article introduces a new approach for E-
Learning on language and literature, by 
intelligently extracting real or virtual objects 
from texts and integrating them as exhibitions 
in a digital museum system. This article also 
discussed how to generate exhibitions from 
texts with computational linguistics methods 
as well as how this E-Learning framework 
pushes the research of computational 
linguistics. The discussion of E-Learning by 
Digital Museum is based on the design of 
Digital Museum of Chinese Ancient Poetry, 
by Peking University. 
1 Introduction 
Computer based Education has become a very 
hot and productive topic in recent years. However, 
most of the existing methodology and models are 
based on science domain. This is because the 
teaching and learning on science domain relies 
much on the ability of reasoning and computation, 
which directly utilizes the advantage of computer. 
The most important carriers of Knowledge on 
humanities domain, especially literature and 
language are textual materials. Therefore, unlike E-
Learning on science and technical fields, a more 
intelligent way of using computer to deal with texts 
is required. Traditional E-Learning models on 
language and literature rely too much on pure text. 
Relevant frameworks include Digital-Archives, 
Digital-Libraries and Digital publications. Most of 
them are just ?gathering mass text materials and 
providing them online?, thus the interface between 
system and learners is onefold, non-interactive and 
lack of guidance. Learners easily get missed in 
excessive bald texts without a ?docent [2]? to 
advise them how to select a well organized 
knowledge structure and a learning pathway. 
Searching and retrieving modules are provided in 
those models to various extents, which provide a 
knowledge retriever. However, it still cannot go 
beyond texts. 
Recently, Digital Museum systems are believed 
to be able to provide a vivid interface which carries 
educational uses to participants. Teaching and 
learning becomes much easier from the special 
circumstance of learning in the presence of real 
objects, which inspires curiosity and creative 
thinking, and gives museums the potential to 
develop distinctive and meaningful educational 
experiences [5].  
There are many good examples that approach E-
learning on humanities fields with a system similar 
to a Digital Museum. The National Palace Museum 
system in Taiwan offers 14 courses on the cultural 
relics of China [3]. Digital Museums on more than 
10 major fields in nature and culture have been 
designed along with Taiwan?s nation wide Digital 
Museum plan. Lo, Feng-ju et? al have designed a 
digital museum of Chinese Ancient Literature, 
which provides some sub-exhibitions of poetry and 
fictions in formats of photocopy of the actual paper 
edition of ancient texts.[7] These works have been 
well exploring the primitive application of Digital 
Museum in E-Learning on Humanities Fields. 
To satisfy the needs of E-Learning on Language 
and Literature fields, a modern digital museum 
should have some specific features. It should 
provide a mechanism to process texts, which 
would be able to integrate some computational 
linguistics methods. It should also provide a way to 
organize knowledge beyond the texts, and be able 
to provide guidance for learning. This can be 
achieved by generating objects out from texts and 
organizing them into interactive exhibitions that 
can be personalized. Moreover, the digital museum 
framework should be reusable to different scope of 
background knowledge. Such a modern digital 
museum associating text processing mechanism is 
believed to be a sound approach of E-Learning on 
Language and Literature.  
This article discussed this approach on the 
Digital Museum framework design, how it is 
associated with Computational Linguistics, and 
how to integrate knowledge to maximize the E-
Learning efficiency. These discussions will be 
based on an example of the Digital Museum of 
Chinese Ancient Poetry Art, by Peking University 
2003. [10] The following section will discuss the 
general framework design of digital museum. We 
will discuss text processing work behind the 
Digital Museum in Section 3, and Knowledge 
Processing and integration in Section 4. Some 
more discussion and future work will be provided 
at the conclusion section.  
2 The Digital Museum Framework 
Instead of digital library and traditional digital 
museum systems, which provide single function of 
exhibition, a modern digital museum provides 
multidimensional functions. Generally, a modern 
digital museum has three key functions, exhibition, 
education and research. In our design of Digital 
Museum for Language and Literature, the three 
dimansion would be: interacting theme based 
exhibitions from texts, E-Learning modules on 
language and literature, and related research on 
Computational Linguistics. 
2.1 Digital Museum and E-Learning on 
Language and Literature 
Digital Museum systems have gone beyond 
exhibitions of digital collections. Instead, they 
would increasingly emphasize educational uses 
rather than traditional exhibitions. It provides users 
with educational and well-motivated exhibitions 
[13]. UK-wide Digital Museum linked exhibitions 
connected by subject and theme with an integrated 
learning environment [6]. By 2000, the National 
Science Plan of Digital Museums of Taiwan has 
defined a specific and integrated program on how 
to utilize scientific technology, especially 
information technology, and how to digitalize the 
archives in both cultural and natural fields, with 
significant humanistic meaning. It has conducted 
further discussions on how to apply these kinds of 
digital projects and productions to education, 
research and industrialization, for the sake of 
conserving culture, promoting education, inspiring 
research and increment of industrialization. [3]. 
Knowledge on a learning topic should be 
organized  as an exhibition theme, which is 
represented by a series of real or virtual objects 
and detailed descriptions. Exhibitions of various 
themes are linked together corresponding to the 
relativity of their themes. Learners can participate 
in the Digital Museum by choosing a pathway of 
linked exhibitions with a typical topic. Special 
modules will also be provided for participants to 
interacting with the system, which will be 
discussed in section 4.  
2.2 General Architecture Design of a Digital 
Museum 
The life cycle of a modern digital museum looks 
like a fountain model  [11]. There are feedbacks 
from each design phase to previous phases. There 
are several milestones in the life cycle, each of 
which acts as a knowledge container and a 
foundation of knowledge processing on upper 
levels. [14]. These knowledge containers are as 
follows: 
Milestones Functionality 
 
Information Origin Pool: 
(Primitive Corpus) 
The mass storage of large-scale 
information from preliminary 
digitalization work. 
 
Refined Knowledge Bases 
(Refined Corpus) 
Database storage of useful and 
relevant knowledge from 
knowledge refining.  
 
Metadata for Exhibitions 
Metadata describing ontology, 
with all detailed metadata for 
knowledge flows, items and 
relations 
 
Integrated Exhibition Base 
 
Database for Exhibiting items, 
individual or integrated, for regular 
accessing by system.  
 
Reusable Tool Base for 
Functional Modules 
Tool pool for reusable module 
functions, individual or integrated 
components for various use. 
 
Multi-functional  Interface 
Web-based interface for 
exhibitions, education and 
research.  
Table 1: Milestones within the Digital Museum 
Architecture 
 
Based on these milestones, the general 
architecture of a Digital Museum on Language and 
Literature can be represented in the following 
figure:
 
 
Figure1: General Architecture of a Digital 
Museum based on language processing 
2.3 Example: Introduction to the Digital 
Museum of Chinese Ancient Poetry 
The Digital Museum of Chinese Ancient Poetry 
Art [10] is a research model by Peking University, 
Beijing, combining E-Learning, computer assisted 
research on Chinese Ancient Poetry and 
computational linguistics. A prototype of this 
Digital Museum was designed in order to meet the 
needs of exhibition, education and research on the 
art of Chinese Ancient Poetry. The analysis, design 
and implementation of this project were on a 
highly abstract level. 
2.3.1 Corpus, Design and Prototype System 
The information origin pool and the refined 
knowledge base of this project were also the 
corpus for related computational linguistics 
research. It involves Chinese Ancient Poetry across 
2,000 years, approximately 100,000 items [10]. 
Other advanced knowledge bases such as Author 
Information base, Image and media base, Location 
information base and Word lists were constructed.  
In the design of this Digital Museum system, 
knowledge mining was divided into two types, 
item entity information mining and relational 
information mining. Item entity information was 
detailed to exhibiting items, characters, images, 
media, locations and words. Relational information 
reflected all aspects of relations among items. 
Metadata for each category of instances was 
defined in the design phase. Particularly, a group 
of items with relating meaning was structured as a 
virtual item class, which was also treated as a 
specific item.  
In the prototype system, items of poetry, 
character, location and others were exhibited along 
with all related formats of knowledge. Users can 
leap from one item to its related items, and learn 
them in the context where they originally belongs. 
Sample exhibitions on specific themes, such as 
clothing, plants, food and spring were also 
designed. 
2.3.2 E-Learning and Related research from 
this Digital Museum 
In the dimension of learning, Digital Museum of 
Chinese Ancient Poetry explored the study of E-
Leaning system for the language and literature 
features of Chinese Ancient Poetry. It enabled a 
way to learn a poem in its background environment, 
with reference to its related poetry and other 
related objects in multiple formats. The system 
also presented statistical research results of the 
corpus to users, such as the words usages of 
authors, the cooccurrence of words, the likelihood 
of the hidden meanings of words, which help users 
to be well-informed and easier to understand in 
learning a poem or a word.  
In the dimension of research, the digital museum 
is closely related to specific research topics on 
computational linguistics, especially statistical  
natural language processing. We refined unknown 
words from the corpus though statistic methods 
and explored to cluster them into concepts. In this 
way, we studied the hidden meanings of words and 
poetry in context and studied the relation discovery 
among poems. We also conducted some research 
of knowledge mining and discovering from corpus, 
which can also inspire extended researches like 
Computer Assisted archaeology on Chinese 
Ancient Poetry. 
3 Language Processing behind the Digital 
Museum Framework 
Knowledge of humanities areas, especially 
language and literature, is commonly carried by 
texts. Therefore, the language processing, 
specifically the text processing will be vital for 
transforming pure texts and domain knowledge 
into abstracted exhibitions. Actually, most digital 
museums today haven't made good use of 
computational linguistics techniques. Most of them 
remain on organizing exhibitions manually and 
providing them online. Those exhibitions are 
relatively isolated from each other.  
However, there are remarkable relations among 
text units and real objects and topics, which are 
hidden in the texts. For example, the word 
?willow? seems having nothing to do with ?getting 
apart? by the semantic definitions, but in the 
context, ?breaking a willow branch? does indicate 
?send-off friends?, or ?seeing a friend leaving? in 
Chinese Ancient Poetry.  
These meaningful entities and relations can be 
learned from the statistical analysis of large scale 
poetry texts. The use of computational linguistics 
methods here is crucial, which distinguishes it with 
traditional Digital Museum models. Statistical 
natural language processing over large scale corpus 
is the most significant approach we have adopted 
in this research.  
3.1 Construction of Corpuses and Integrated 
Knowledge bases 
The first phase of language processing is to build 
corpora and knowledge bases. Primitive corpora 
are constructed by archive digitalization. Refined 
corpora are constructed by applying language 
processors on the primitive corpus. We can use 
Digital Museum of Chinese Ancient Poetry for 
example.  
For the Digital Museum of Chinese Ancient 
Poetry Art,  the primitive corpora include texts of 
poems over 1, 200, 000 lines, descriptions of 4000 
authors, a name dictionary and a location 
dictionary. The refined corpora include a words 
dictionary which is thoroughly discovered from the 
texts, a concept base constructed by supervised 
word clustering and a storage of words 
cooccurances. Other knowledge bases include 
images, music, medias(reading), relics, events, and 
a series of expertise knowledge on Chinese 
Ancoent Poetry.  
The general ontology of domain knowledge was 
carefully studied. Important entities and relations 
from texts and related domains were determined. 
Consequently, we carefully designed the metadata 
and chose a database system to maintain the 
knowledge base. This knowledge base should be 
expandable so that  it can contain texts, entities 
from related domains, and relations.  
The last step of this phase is to design an 
referencing mechanism to query and get the 
answer. The outcome of this phase is an integrated 
knowledge base, the textual part of which is the 
corpus for mining and knowledge discovery. 
3.2 Text Mining: Extracting Objects from 
Texts 
As soon as the corpora and knowledge bases are 
constructed, higher level methods of natural 
language processing are applied to mine in the 
corpus. The goal is to find objects abstracted from 
texts, which are organized by individual topics. 
Statistical natural language processing plays a very 
important role in this procedure, which can be 
described in the following three levels.  
3.2.1 Extracting Direct Relevant Objects from 
Texts. 
Textual knowledge is not ?dead? in the fields of 
language and literature. It is interacting with 
knowledge in other forms, by other carrier or on 
other abstract level.  Taking Chinese ancient poetry 
for example, a poem is associated to its author, its 
era and its writing background. The textual body of 
a poem also refers to certain persons, events, 
locations, plants, scenes, feelings and other entities, 
either real or virtual. In addition, there are various 
sources of objects relevant to the poem, such as 
paintings, calligraphy works, music and cultural 
relics, etc. All these entities above are so important 
to the synopsis of the poem that it is an advisable 
way to learn the poem with the appearance of these 
objects. Furthermore, relying on these directly 
relevant objects makes teaching and learning much 
more open and exciting than barely focusing on 
texts.  
In the early phase of Digital Museum design, an 
integrated exhibition base is built, in which directly 
relevant entities of the texts are refined, stored in 
relational or XML databases and associated with 
the body of texts. 
3.2.2 Discovering Hidden Entities and Relations 
Associated with Language Units. 
As the Computer assisted research develops on 
these fields, we can work on the hidden knowledge 
of texts by means of text mining and retrieval.  As 
language technology evolves, a computational age 
of language has arrived [1].  We can conduct 
computer assisted analytical research on language, 
with both linguistic and statistical approaches. In 
the research on the language of Chinese ancient 
poetry, we studied the statistical concurrences and 
meaningful units in the texts, extracted words from 
collocations and clustered words into meaningful 
concepts. In further research, we explored ways to 
study the hidden meanings of the words and 
collocations, especially those related to emotions 
of human. Consequently, expected to learn 
emotional characteristic of a poem, associating 
words, concepts and other units it refers with the 
similar characteristic.  
On the other hand, language and texts are the 
most important carriers of cultural fragments. 
Many interesting knowledge patterns are hidden in 
the texts.  There is a considerable proportion of 
Chinese ancient history and culture buried in the 
texts of Chinese ancient poetry, which evolutes 
along more than 2,000 years and involves locations 
all over China. By language techniques, fragments 
of culture can be mined from the texts, refined and 
stored, and finally integrated into interacting 
virtual scenes.   
By this we can discover hidden entities and 
relations associated with text and expand it to 
analytical meaningful segments.  
3.2.3 Expanding Indirect Relations. 
In our framework, knowledge entities are not 
living alone but interacting. Both textual entities 
and other objects are associated to its relevant 
entity set. There are two kinds of relations 
identifying that two entities are interacting, direct 
relation, which have already been discussed above, 
and indirect relation.  For instance, a poem refers 
to various knowledge objects, thus poems referring 
to the same objects are indirectly interacting with 
each other. These poems are involved in their 
relevant entity set, with ?identical reference? as an 
indirect relation.  In a more intelligent level, poems 
with the similar hidden meanings or relevant 
emotions are arranged together as a set. This set 
can be associated with a topic, a subject, a scene or 
a specific semantic cluster.  
In these three approaches to expand textual 
knowledge into relevant objects, a former purely 
textual entity has been developed as involving in 
the surrounding of various relevant objects, real or 
virtual. Thus we complete the procedure of 
extracting objects for exhibitions from texts. An 
example from poems to objects is as follows: 
 Figure2: Expanding Objects Set from a Poem 
Text. 
 
3.3 Theme Driven Knowledge Discovery 
From the statistical analysis on character 
concurrences, we applied various methods to 
discover unknown words from the texts. Chinese 
language is different from other language because 
there isn?t natural interval from a word to another. 
We consider all words to be unknown in the 
beginning and generate a word dictionary from the 
filtering by mutual information value, m-test and 
other statistical methods.  
Upon the word dictionary, we conducted words 
clustering by the distance of words concurrence 
vectors. This procedure has abstracted concepts 
from words. After supervised filtering, these 
concepts will indicate some hidden semantic 
meanings.  
The consecutive knowledge discovery work will 
be theme driven. First, a theme, or a learning topic 
is decided, some features and key concepts of this 
theme will be decided with the expert knowledge. 
Using statistical methods, we can find the concepts 
and words which are semantically similar or in 
some way related to this theme. Then, directly and 
indirectly related objects (discussed in section 3.2) 
will be associated with the topic. Then, reluctant 
units are eliminated. We will filter the most 
significant entities and relations, which can be 
represented by combinations of both concepts and 
words, and organize them around the theme. In this 
way, we can put the topic/theme back to its ancient 
living environment.  
Further works includes rebuilding ancient 
scenarios where the topic belongs, and mining for 
relations among topics.  
4 Knowledge Processing and Integration of 
the Digital Museum  
Knowledge processing plays a very significant 
role in the Digital Museum framework. It is 
involved as a clue throughout the life cycle of the 
digital museum. The entire design and 
implementing of the digital museum is focusing on 
language processing, knowledge discovery and 
exhibition integrating. The knowledge processing 
procedures can be represented in the following 
figure: 
 Figure3: Knowledge Processing in this digital 
museum. 
 
4.1 Knowledge Processing Hierarchy  
An intelligent platform of knowledge deals with 
knowledge in five primary hierarchies, namely, 
knowledge citation, knowledge application, 
knowledge transmitting, knowledge learning and 
knowledge developing [8]. This division of 
knowledge hierarchies remarkably adapts the 
needs of an E-Learning program. In the study of 
this article, we make a little modification to this 
division and applied it to the Digital Museum 
system as follows:  
Knowledge Citation 
Knowledge Applying 
Knowledge Learning 
Learning and Teaching  
Knowledge Mining 
Knowledge Representing 
Knowledge Representing to Users  
Information Interacting 
Knowledge Developing 
Table 1: A knowledge processing hierachy in the 
Digital Museum 
 
Poem 
Persons Locations 
Relics 
Events 
Other 
Words Concepts 
Emotions 
Cultural 
 
Fragments 
Scenes 
Relevant Entity Sets? 
Poems, Topics, Scenes, 
Texts, Concepts, Themes, 
Words, Other entities? 
Texts 
Images Medias 
Virtual 
Realities 
 
Actually, this division is somewhat relative and 
not absolute. For instance, in some activities 
defined as knowledge representation and 
knowledge developing, we may also need to do 
knowledge citation and applying. However, this 
division of knowledge hierarchy would help to 
define the functions of Knowledge Platform and 
content the needs for knowledge by systems and 
users. [8] 
The Digital Museum presents multidimensions 
according to the three functions of exhibition, 
education and research. The processing targets, 
procedures and emphases on Knowledge vary 
among dimensions.  
In the dimension of exhibition, system focuses 
on Knowledge citation and Knowledge 
representing in the hierarchy above.  
In the dimension of e-learning, system focuses 
on the hierarchy of Knowledge applying, learning 
and teaching, Knowledge Representing and 
information interaction.  
In the dimension of computational linguistics 
research, system emphasizes the hierarchy of 
Knowledge Mining and Knowledge developing.  
4.2 Two Types of Integration for Knowledge 
Objects 
After discussing the generating of objects from 
the texts, we would be interested in how to 
integrate them for E-Learning.  
Relating and interacting objects are extracted 
from texts and stored in the exhibition base. The 
next phase is to arrange exhibitions by selecting, 
dividing and integrating these objects, and 
construct the digital museum interface.  
There are two key forms of objects integration, 
tutored and theme-oriented exhibitions and virtual 
scenarios.  
In the first form, tutored theme-oriented 
exhibition, objects relevant to a specific subject or 
theme are integrated and represented in multi-
modals. This interface design provides a dynamic 
exhibition module by grouping texts and their 
relevant objects in various formats together, 
providing docent knowledge for this topic and 
links to relevant topic exhibitions. Learners 
participate in one exhibition and go through links 
fitting to their needs or under instructions, thus 
personalized learning paths are formed.   
There are two tips in tutored theme-oriented 
exhibitions. One is ?multi-modal?. Personalized 
exhibitions in our framework enable learning 
through multi channels, in forms of texts, image, 
music and virtual reality, etc. Also taking Chinese 
ancient poetry for example, we first discover the 
relevant scenes and hidden emotions of a poem, 
select objects referring to similar scenes and 
emotions, provide them as background materials 
and then integrate them with the poem.  A more 
detailed instance is the Auto-matching poems and 
paintings. The other is ?interactive?. In our 
framework, a learner can add his remarks or 
discuss in every exhibition topic. These remarks 
are processed and stored as new relevant objects to 
this topic. Users can also provide materials or 
background information to an object or a topic, and 
can provide their own exhibition plans of new 
organizations of objects. The system studies the 
feedbacks and provides users with personalized 
participation paths.  
The second integration form is scenarios. 
Knowledge objects were recorded in texts from 
their original living environments. By collecting 
and extracting relevant objects from texts and 
analytical researching on their relevant 
environmental elements such as emotions, we are 
able to put a textual object back to a scene 
representing its original living environment by 
rebuilding these origin scenes. Teaching and 
learning are made easier and more exciting with 
participating in the original scenes that a topic 
really lived. With the technology of multimedia 
and virtual reality, we are able to integrate objects 
and environmental elements surrounding a specific 
topic and rebuild a virtual scene, which is 
represented in our framework as multimedia 
demonstration, tests and games.  
These two key integrating patterns organize 
various formats of objects and represent these 
integrated exhibitions to users in an interactive and 
personalized way. It maximizes the educational use 
of a digital museum on language and literature 
fields.  
 Figure3:Integrating exhibits in the Digital 
Museum on Chinese Ancient Poetry. 
5 Conclusion 
Computer-based education on language and 
literature has both its advantage and difficulty. On 
one hand it provides learners with abundant 
relating materials, on the other hand it?s tedious 
and difficult for learners to acquire knowledge in 
the sea of information. The approach of extracting 
objects from texts, and integrating them to build an 
interactive and vivid exhibitions enables learners 
both to explore in broad scope of knowledge and to 
enjoy exciting and comprehensible learning. 
Computer techniques are adopted in the framework 
of Digital Museums to maximize its educational 
use. How to make use of the methods from 
computational linguistics, especially statistical 
methods is the bottleneck or the key to success of 
this e-learning approach. On the other hand, the 
needs of e-learning and the abstracting of digital 
exhibitions from texts have very positive effect on 
pushing the research of computational linguistics. 
Significant techniques include unknown word 
discovery, clustering and other issues in text 
mining. Besides the conituous work on text mining, 
future research will focus on how to personalize 
the learning paths of learners, and enable in-time 
processing of user feedbacks. Investigations and 
evaluations will be made both on the e-learning 
system and the efficiency of text mining 
techniques over typical kinds of texts, like Chinese 
ancient poetry. 
6 Acknowledgements 
The authors would thank people in Institute of 
Computational Linguistics, Peking University, who 
gave great help for this research. We will 
especially thanks Miz. Feng-ju Lo, who has given 
us great help ever since the research starts. 
References  
1. Martin A. Nowak, Natalia L. Komarova, Partha 
Niyogi, Computational and Evolutionary 
Aspects of Language, Nature, VOL417, 6 June 
2002 
2. W.Rayward, M. Twidale, From Docent to 
Cyberdocent: education and Guidance in the 
Virtual Museum, Archives and Museum 
Informatics 13, 1999, p23-p53.  
3. Ching-Chun Hsieh, Ying-Chun Hsieh et al 
?Samples of Digital Archive in Taiwan National 
Digital Archive Program?, 2003 
4. Shun-tzu Tsai, Chun-ko Hsieh, Diversity and 
Aesthetic Appeal for a Virtual Reality World of 
Chinese Art, proceeding of the Seventh 
International Conference on Virtual System and 
Multimedia,  2001 
5. ?The Learning Power of Museums?A Vision 
for Museum Education? Published by 
Department for Culture, Media and Sport, 
United Kingdom, 2000  
6. Louise Smith, ?Building the Digital Museum: A 
National Resource for the Learning Age.? joint 
report of The National Museums Directors? 
Conference, Resource and mda, UK, 10 August 
2000 
7. Feng-ju Lo, et al Ancient Literature Museum: 
Design of an E-learning System for non-
Chinese Major, the 4th International Workshop 
on Computer, Multimedia and Education of 
Language, Taiwan, 2000 
8. Chuanzhong Li, Jingzhong Zhang, ?Idea of 
Intelligent Knowledge Platform and a 
Rudimental Prototype?, Research and 
Development on the World Science & 
Technology, Volume 23 Issue 6, 2001 
9. Junfeng Hu, Shiwen Yu, Word meaning 
Similarity analysis in Chinese Ancient Poetry, 
ICL Technical Report, Peking University, 2001 
10.Qiaozhu Mei, ?A Digital Museum of Ancient 
Chinese Poetry Art: It?s Design, Realization and 
Related Researches on Computational 
Linguistics?, Thesis for Bachelor?s Degree in 
Peking University, 2003.6 
11.Krish Pillai, ?The Fountain Model and Its 
Impact on Project Schedule?, ACM SIGSOFT 
Software Engineering Notes, Volume 21 Issue 
2, March 1996 
12.Nikos Kladias, Tassos Pantazidis, Manolis 
Avagianos, A Virtual Reality Learning 
Environment Providing Access to Digital 
Museums, 1998 MultiMedia Modeling October, 
1998, p193 
13. Jen-Shin Hong, Bai-Hsuen Chen, Jieh Hsiang, 
Tien-Yu Hsu, ?Content Management for Digital 
Museum Exhibitions?, Proceeding of JCDL 
2001, pp.450, June 24-28, 2001 
14.Qiaozhu Mei, A Knowledge Processing 
Oriented Life Cycle Study from a Digital 
Museum System., The 42nd ACM Southeast 
Conference, Huntsville, 2004 
 
 
 
 
 
 
Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 1077?1087,
MIT, Massachusetts, USA, 9-11 October 2010. c?2010 Association for Computational Linguistics
Context Comparison of Bursty Events in Web Search and Online Media
Yunliang Jiang
University of Illinois
Urbana, IL, 61801
jiang8@illinois.edu
Cindy Xide Lin
University of Illinois
Urbana, IL, 61801
xidelin2@illinois.edu
Qiaozhu Mei
University of Michigan
Ann Arbor, MI, 48109
qmei@umich.edu
Abstract
In this paper, we conducted a systematic com-
parative analysis of language in different con-
texts of bursty topics, including web search,
news media, blogging, and social bookmark-
ing. We analyze (1) the content similarity and
predictability between contexts, (2) the cov-
erage of search content by each context, and
(3) the intrinsic coherence of information in
each context. Our experiments show that so-
cial bookmarking is a better predictor to the
bursty search queries, but news media and so-
cial blogging media have a much more com-
pelling coverage. This comparison provides
insights on how the search behaviors and so-
cial information sharing behaviors of users are
correlated to the professional news media in
the context of bursty events.
1 Introduction
Search is easy. Every day people are repeating the
queries they have used before, trying to access the
same web pages. A smart search engine tracks the
preference and returns it next time when it sees the
same query. When I search for ?msr? I always try to
access Microsoft research; and even if I misspelled
it, a smart search engine could suggest a correct
query based on my query history, the current ses-
sion of queries, and/or the queries that other people
have been using.
Search is hard. I search for ?social computing?
because there was such a new program in NSF; but
the search engine might have not yet noticed that.
People use ?msg? to access monosodium glutamate
in most of the cases, but tonight there is a big game
in Madison square garden. H1N1 suddenly became
a hot topic, followed by a burst of the rumor that
it was a hoax, and then the vaccine. The informa-
tion need of users changed dramatically during such
a period. When a new event happens, the burst of
new contents and new interests make it hard to pre-
dict what people would search and to suggest what
queries they should use.
Web search is easy when the information need of
the users is stable and when we have enough histor-
ical clicks. It becomes much more difficult when a
new information need knocks the door or when there
is a sudden change of the information need. Such a
shift of the information need is usually caused by a
burst of new events or new interests.
When we are lack of enough historical observa-
tions, why don?t we seek help from other sources?
A bursting event will not only influence what we
search, but hopefully also affect what we read, what
we write, and what we tag. Indeed, there is al-
ready considerable effort in seeking help from these
sources, by the integration of news and blogs into
search results or the use of social bookmarks to
enhance search. These conclusions, however, are
mostly drawn in a general context (e.g., with gen-
eral search queries). To what extent are they use-
ful when dealing with busty events? How is the
bursting content in web search, news media, social
media, and social bookmarks correlating and dif-
ferent from each other? Prior to the development
of desirable applications (e.g. enhancing search re-
sults, query suggestion, keyword bidding on adver-
tisement, etc) by integrating the information from all
these sources, it is appealing to have an investigation
of feasibility.
In this work, we conduct a systematic compara-
tive study of what we search, what we read, what
1077
we write, and what we tag in the scenarios of bursty
events. Specifically, we analyze the language used
in different contexts of bursty events, including two
different query log contexts, two news media con-
texts, two blog contexts, and an additional con-
text of social bookmarks. A variety of experiments
have been conducted, including the content similar-
ity and cross-entropy between sources, the coverage
of search queries in online media, and an in-depth
semantic comparison of sources based on language
networks.
In the rest of this paper, a summary of related
work is briefly described in Section 2. We then
present the experiments setup in Section 3, The re-
sults of the experiments is presented in Section 4. Fi-
nally, our major findings from the comparative anal-
ysis are drawn in Section 5.
2 Related Work
Recently, a rich body of work has focused on how
to find the bursting patterns from time-series data
using various approaches such as time-graph analy-
sis (Kleinberg, 2003; Kuman et al, 2003), context-
based analysis (Gabrilovich et al, 2004), moving-
average analysis (Vlachos et al, 2004), and fre-
quency analysis (Gruhl et al, 2005), etc. These
methods are all related to the preprocessing step of
our analysis: detecting bursty queries from the query
log effectively.
The comparison of two web sources at a time is
widely studied recently. (Sood et al, 2007) dis-
cussed how to leverage the relation between social
tags and web blogs. (Lloyd et al, 2006; Gamon et
al., 2008; Cointet et al, 2008) investigated the rela-
tions between news and blogs. Also some work has
aimed to utilize one external web source to help web
search. For example, (Diaz, 2009) integrated the
news results into general search. (Bao et al, 2007;
Heymann et al, 2008; Krause et al, 2008; Bischoff
et al, 2008) focused on improving search by the so-
cial tags. Compared with the above, our comparison
analysis tries to explore the interactions among mul-
tiple web sources including the search logs.
Similar to our work, some recent work (Adar et
al., 2007; Sun et al, 2008) has addressed the com-
parison among multiple web sources. For exam-
ple, (Adar et al, 2007) did a comprehensive corre-
lation study among queries, blogs, news and TV re-
sults. However, different from the content-free anal-
ysis above, our work compares the sources based on
the content.
Our work can lead to many useful search applica-
tions, such as query suggestion which takes as in-
put a specific query and returns as output one or
several suggested queries. The approaches include
query term cooccurrence (Jones et al, 2006), query
sessions (Radlinski and Joachims, 2005), and click-
through (Mei et al, 2008), respectively.
3 Analysis Setup
Tasks of web information retrieval such as web
search generally perform very well on frequent
and navigational queries (Broder, 2002) such like
?chicago? or ?yahoo movies.? A considerable chal-
lenge in web search remains in how to handle infor-
mational queries, especially queries that reflect new
information need and suddenly changed information
need of users. Many such scenarios are caused by
the emergence of bursty events (e.g., ?van gogh? be-
came a hot query in May 2006 since a Van Goghs
portrait was sold for 40.3 million in New York dur-
ing that time). The focus of this paper is to analyze
how other online media sources react to those bursty
events and how those reactions compare to the re-
action in web search. This analysis thus serves as
an primitive investigation of the feasibility of lever-
aging other sources to enhance the search of bursty
topics.
Therefore, we focus on the ?event-related? topics
which present as bursty queries submitted to a search
engine. These queries not only reflect the suddenly
changed information need of users, but also trigger
the correlated reactions in other online sources, such
as news media, blog media, social bookmarks, etc.
We begin with the extraction of bursty topics from
the query log.
3.1 Bursty Topic Extraction
Search engine logs (or query logs) store the history
of users? search behaviors, which reflect users? in-
terests and information need. The query log of a
commercial search engine consists of a huge amount
of search records, each of which typically contains
the following information: the query submitted by
1078
a user, the time at which the query was submitted,
and/or the URL which the user clicked on after the
query was submitted, etc. It is common practice
to segment query log into search sessions, each of
which represents one user?s searching activities in a
short period of time.
We explore a sample of the log of the Microsoft
Live search engine1, which contains 14.9M search
records over 1 month (May 2006).
3.1.1 Find bursty queries from query log
How to extract the queries that represent bursty
events? We believe that bursty queries present the
pattern that its day-by-day search volume shows a
significant spike ? that is, the frequency that the user
submit this query should suddenly increase at one
specific time and drop down after a while. This as-
sumption is consistent with existing work of finding
bursty patterns in emails, scientific literature (Klein-
berg, 2003), and blogs (Gruhl et al, 2005).
Following (Gruhl et al, 2005), we utilize a simple
but effective method to collect bursty topics in the
query log data as follows:
? We choose bigrams as the basic presentation of
bursty topics since bigrams present the information
need of users more clearly and completely than un-
igrams and also have a larger coverage in the query
log comparing to n-grams (n ? 3).
? We only consider the bigram queries which ap-
pear more frequently than a threshold s per month.
This is reasonable since a bursty event usually
causes a large volume of search activities.
? Let fmax(q) be the maximum search volume of
a query q in one day (i.e., day d). Let f??5(q) be the
upper bound of the daily search volume of q out-
side a time window of 5 days centered at day d. If
fmax(q) is ?significantly higher? than f??5(q) (i.e.,
rm = f?max(q)/f?5(q) > m), we consider q as a
query with a spike pattern (m is an empirical thresh-
old).
? The ratio above may be vulnerable to the query
that has more than one spike. To solve this, we de-
fine f??5(q) as the average of daily search volume
of q outside the same time window. This gives us
an alternative ratio ra = fmax(q)/f??5(q). We fur-
ther balance these two ratios by ranking the bursty
1Now known as Bing: www.bing.com
queries using
score(q) = ? ? rm(q) + (1? ?) ? ra(q) (1)
By setting s = 20, m = 2.5, ? = 0.8 (based on
several tests), we select the top 130 bigram queries
which form the pool of bursty topics for our anal-
ysis. Table 1 shows some of these topics, covering
multiple domains: politics, science, art, sports, en-
tertainment, etc.
ID Topic ID Topic
1 kentucky election 66 orlando hernandez
2 indiana election 75 daniel biechele
8 van goph 81 hurricane forecast
24 north korea 92 93 memorial
34 pacific quake 113 holloway case
52 florida fires 128 stephen colbert
63 hunger strike 130 bear attack
Table 1: Examples of News Topics
3.2 Context extraction from multiple sources
Once we select the pool of bursty topics, we gather
the contexts of each topic from multiple sources:
query log, news media, blog media, and social book-
marks. We assume that the language in these con-
texts will reflect the reactions of the bursty events in
corresponding online media.
3.2.1 Super query context
The most straightforward context of bursty events
in web search is the query string, which directly
reflects the users? interests and perspectives in the
topic. We therefore define the first type of context of
a bursty topic in query log as the set of surrounding
terms of that bursty bigram in the (longer) queries.
For example, the word aftermath in the query ?haiti
earthquake aftermath? is a term in the context of the
bursty topic haiti earthquake.
Formally, we define a Super Query of a bursty
topic t, sq(t), as the query which contains the bi-
gram query t lexically as a substring. For each
bursty topic t, we scan the whole query log Q and
retrieve all the super queries of t to form the context
which is represented by SQ(t).
SQ(t) = {q|q ? Q and q = sq(t)}
1079
SQ(t) is defined as the super query context of t.
For example, the super query context of ?kentucky
election? contains terms such as ?2006,? ?results,?
?christian county,? etc. These terms indicate what
aspects the users are most interested in Kentucky
Election during May 2006.
The super query context is widely explored by
search engines to provide query expansion and
query completion (Jones et al, 2006).
3.2.2 Query session context
Another interesting context of a bursty topic in
query log is the sequence of queries that a user
searches after he submitted the bursty query q. This
context usually reflects how a user reformulates the
representation of his information need and implicitly
clarifies his interests in the topic.
We define a Query Session containing a bursty
topic t, qs(t), as the queries which are issued by the
same user after he issued t, within 30 minutes. For
each bursty topic t, we collect all the qs(t) to form
the query session context of t, QS(t):
QS(t) = {q|q ? Q and q ? qs(t)}
In web search, the query session context is usu-
ally utilized to provide query suggestion and query
reformulation (Radlinski and Joachims, 2005).
3.2.3 News contexts
News articles written by critics and journalists re-
flect the reactions and perspectives of such profes-
sional group of people to a bursty event. We col-
lect news articles about these 130 bursty topics from
Google News2, by finding the most relevant news
articles which (1) match the bursty topic t, (2) were
published in May, 2006, and (3) were published by
any of the five major news medias: CNN, NBC,
ABC, New York Times and Washington Post.
We then retrieve the title and body of each news
article. This provides us two contexts of each bursty
topic t: the set of relevant news titles, NT (t), and
the set of relevant news bodies, NB(t).
3.2.4 Blog contexts
Compared with news articles, blog articles are
written by common users in the online communi-
ties, which are supposed to reflect the reactions and
2http://news.google.com/
opinions of the public to the bursty events. We col-
lect blog articles about these 130 topics from Google
Blog3, by finding the most relevant blog articles
which (1) match the bursty topic t, (2) were pub-
lished in May, 2006 (3) were published in the most
popular blog community, Blogspot4. We then re-
trieve the title and body of each relevant blog post re-
spectively. This provides another two contexts: the
set of relevant blog titles, BT (t), and the set of rel-
evant blog bodies, BB(t).
3.2.5 Social bookmarking context
Social bookmarks form a new source of social
media that allows the users to tag the webpages they
are interested in and share their tags with others. The
tags are supposed to reflect how the users describe
the content of the pages and their perspectives of the
content in a concise way.
We use a sample of Delicious5 bookmarks in May,
2006, which contains around 1.37M unique URLs.
We observe that the bursty bigram queries are also
frequently used as tags in Delicious. We thus con-
struct another context of bursty events by collecting
all the tags that are used to tag the same URLs as the
bursty topic.
Formally, we define DT (t) as the context of so-
cial tags of a topic t,
DT (t) = {tag|?url, s.t. tag, t ? B(url)},
where url is a URL and B(url) stands for the set of
all bookmarks of url.
3.3 Context Statistics
Now we have constructed the set of 130 bursty
topics and 7 corresponding contexts from various
sources. We believe that these contexts well repre-
sent the various types of online media and sources.
For each context, we then clean the data by re-
moving stopwords and the bursty topic keywords
themselves. We then represent it as either the set
of unigrams or bigrams from this context. Table 2
shows the basic statistics of each context:
From Table 2 we observe the following facts:
? The query session context covers more terms
(both unigrams and bigrams) than the super query
3http://blogsearch.google.com/
4http://www.blogspot.com/
5http://delicious.com/
1080
N T S M S A U M U A B M B
SQ 130 76k 5.3k 32.7 390 24.3 235
QS 126 108k 5.8k 224 1.5k 150 1062
NT 118 4.7k 411 105 627 102 722
NB 118 4.7k 411 4.7k 22k 22k 257k
BT 128 5.8k 99 184 459 169 451
BB 128 5.8k 99 4.1k 15k 12k 69k
DT 71 2.3k 475 137 2.0k N/A N/A
N: The number of topics covered
T S: The total number of records/documents
M S: The max number of records/documents per topic
A U: The avg number of unique unigrams per topic
M U: The max number of unique unigrams
A B: The avg number of unique bigrams
M B: The max number of unique bigrams
Table 2: Basic statistics of collections
context. In both contexts, the average number of
unique bigrams is smaller than unigrams. This is
because queries in search are usually very short. Af-
ter removing stopwords and topic keywords, quite a
few queries have no bigram in these contexts.
? News articles and blog articles cover most of the
bursty topics and contain a rich set of unigrams and
bigrams in the corresponding contexts.
? The Delicious context only covers less than 60%
of bursty topics. We couldn?t extract bigrams from
bookmarks since delicious provides a ?bag-of-tags?
interface.
In Section 4, we present a comprehensive analy-
sis of these different contexts of bursty topics, with
three different types of comparison.
4 Experiment
In this section, we present a comprehensive compar-
ative analysis of the different contexts, which repre-
sent the reactions to the bursty topics in correspond-
ing sources.
4.1 Similarity & Predictability analysis
Our first task is to compare the content similarity
of these sources. This will help us to understand
how well the language usage in one context can be
leveraged to predict the language usage in another
context. This is especially useful to predict the con-
tent in web search. By representing each context
of a bursty topic as a vector space model of uni-
grams/bigrams, we first compute and compare the
average cosine similarity between contexts. We only
include contexts with more than 5 unigram/bigrams
into this comparison. The results are shown in Table
A and Table B, respectively. Each table is followed
by a heat map to visualize the pattern.
To investigate how well one source can predict
the content of another, we also represent each con-
text of a bursty topic as a unigram/bigram language
model and compute the Cross Entropy (Kullback
and Leibler, 1951) between every pairs of contexts.
Cross Entropy measures how certain one probabil-
ity distribution predicts another. We calculate such
measure based on the following definition:
HCE(m||n) = H(m) +DKL(m||n)
We smooth the unigram language models using
Laplace smoothing (Field, 1988) and the bigram lan-
guage models using Katz back-off model (Katz,
1987).
The results are shown in Table C and Table D,
followed by the corresponding heat maps. For each
value HCE(m||n) in the table cell, m stands for the
context in the row and n stands for the context in
the column. Please note that in Figure 3, 4, a larger
HCE value corresponds to a lighter cell.
4.1.1 Results
From the results shown in Table A-D, or in Fig-
ure 1- 4 more visually, some interesting phenomena
can be observed:
? Compared with other contexts, query session
is much more similar to the super query. This
makes sense because many super queries would be
included in the query session.
? Compared with news and blog, the delicious
context is closer to the query log context. In fact, de-
licious is reasonably close to all the other contexts.
This means social tags could be an effective source
to enhance bursty topics in web search in terms of
query suggestion. However, as Table 2 shows, only
less than 60% of topics can be covered by delicious
tag. We have to explore other sources to make a
comprehensive prediction.
? In the news and blog contexts, the title contexts
are more similar to the query contexts than the body
contexts. This may be because titles usually con-
cisely describe the topic while bodies contain much
more details and irrelevant contents.
1081
Context SQ QS NT NB BT BB DT
SQ 1.0 0.405 0.122 0.072 0.119 0.061 0.188
QS 0.405 1.0 0.049 0.062 0.066 0.054 0.112
NT 0.122 0.049 1.0 0.257 0.186 0.152 0.120
NB 0.072 0.062 0.257 1.0 0.191 0.362 0.114
BT 0.119 0.066 0.186 0.191 1.0 0.242 0.141
BB 0.061 0.054 0.152 0.362 0.242 1.0 0.107
DT 0.188 0.112 0.120 0.114 0.141 0.107 1.0
Table A: Cosine similarity for unigram vectors
 
Figure 1: Heat map of table A
Source SQ QS NT NB BT BB
SQ 1.0 0.290 0.028 0.024 0.047 0.027
QS 0.290 1.0 0.004 0.010 0.011 0.009
NT 0.028 0.004 1.0 0.041 0.026 0.011
NB 0.024 0.010 0.041 1.0 0.023 0.040
BT 0.047 0.011 0.026 0.023 1.0 0.044
BB 0.027 0.009 0.011 0.040 0.044 1.0
We do not build bigram vector for DT
Table B: Cosine similarity for bigram vectors
 
Figure 2: Heat map of table B
Source SQ QS NT NB BT BB DT
SQ 1.698 4.911 7.538 8.901 7.948 9.050 7.498
QS 7.569 3.842 9.487 11.130 9.997 11.546 8.972
NT 8.957 10.868 3.718 7.946 9.006 9.605 8.825
NB 11.217 12.897 11.317 7.241 12.282 11.582 11.739
BT 9.277 11.084 9.085 10.295 4.637 9.365 9.180
BB 11.053 12.842 11.593 11.742 12.001 7.232 11.525
DT 8.457 9.794 8.521 9.511 8.831 9.473 2.990
Table C: Cross entropy for unigram distribution
 
Figure 3: Heat map of table C
Source SQ QS NT NB BT BB
SQ 1.891 2.685 4.290 4.540 4.319 4.607
QS 6.800 3.430 8.144 9.049 8.528 9.304
NT 5.444 5.499 3.652 4.733 5.106 5.218
NB 11.572 11.797 11.254 8.731 11.544 11.073
BT 5.664 5.674 5.503 5.495 4.597 5.301
BB 10.745 10.796 10.517 10.455 10.526 8.518
We do not build bigram distribution for DT
Table D: Cross Entropy for bigram distribution
 
Figure 4: Heat map of table D
1082
HCE(SQ||n)
n: NT BT NB BB
Uni: 7.538 7.948 8.901 9.050
Bi: 4.290 4.319 4.540 4.607
HCE(m||SQ)
m: NT BT NB BB
Uni: 8.927 9.277 11.217 11.053
Bi: 5.445 5.664 11.572 10.745
Table 3: Cross-entropy among three sources
? News would be a better predictor of the query
than blog in general. This is interesting, which indi-
cates that many search activities may be initialized
by reading the news.
? News and blogs are much more similar to each
other than query logs. We hypothesize that this re-
sult reflects the behavior how people write blogs
about bursty events ? typically they may have read
several news articles before writing their own blog.
In the blog, they may directly quote or retell a part
of the news article and then add their opinion.
? Table 3 reveals the generation relations among
three sources: query, news and blog. From the
upper table, we can observe that queries are more
likely to be generated by news articles, rather than
blog articles. From the lower table, we can observe
that queries are more likely to generate blog arti-
cles(body), rather than news articles(body). This re-
sult is quite interesting, which indicates the users?
actual behaviors: when a bursty event happens, users
would search them from web after they read it from
some news articles. And users would write their
own blogs to discuss the event after they retrieve and
digest information from the web.
? From Table 3 we also find that queries are more
likely to generate news title, rather than blog title. It
is natural since blogs are written by kinds of people.
The content especially the title part contains more
uncertainty.
4.1.2 Case study
We then conduct the analysis to the level of indi-
vidual topics. Table 4 shows the correlation of each
pair of contexts, computed based on the similarity
between topics in SQ and corresponding topics in
these contexts. We can observe that News and Blog
are correlated with each other tightly. If one is a
good predictor of bursty queries, the other one also
tends to be.
QS NT NB BT BB DT
QS 0.46 0.59 0.58 0.75 0.46
NT 0.73 0.79 0.59 0.61
NB 0.71 0.68 0.61
BT 0.78 0.59
BB 0.48
Table 4: Correlations of the similarity with SQ
For some topics like ?stephen colbert,? and ?three
gorges,? both News and Blog are quite similar to
the queries, which implies some intrinsic properties
(coherence) of these topics: users would refer to the
same content when using the topic terms in different
sources.
We also find that a few topics like ?hot
dogs,? ?bear attack,? for which the similarity of
(SQ,News) and (SQ,Blog) are both low. It is
probably because these topics are too diverse and
carries a lot of ambiguity.
Although in most cases they are correlated, some-
times News and Blog show different trends in the
similarity to the queries. For example, News is
quite similar to the queries on the topics such as
?holloway case? and ?jazz fest? while Blog is dis-
similar. For these unfamiliar topics, users possi-
bly search the web ?after? they read the news arti-
cles and express their diverse opinions in the blog.
In contrast, on the topics like ?insurance rate? or
?consolidation loans,? Blog is similar to the queries
while News is not. For these daily-life-related
queries, users would express the similar opinions
when they search or write blogs, while news articles
typically report such ?professional? viewpoints.
4.2 Coverage analysis
Are social bookmarks the best source to predict
bursty content in search? It looks so from the sim-
ilarity comparison, if they have a good coverage of
search contents. In this experiment, we analyze the
coverage of query contexts in other contexts in a sys-
tematic way. If the majority of terms in the super
query context would be covered by a small propor-
tion of top words from another source, this source
has the potential.
1083
4.2.1 Unigram coverage
We first analyze the coverage of unigrams from
the super query context in four other contexts: QS,
DT , News (the combination of NT and NB) and
Blog (the combination of BT and BB) to compare
with SQ. For each source, we rank the unigrams by
frequency. Figure 5(a) shows the average trend of
SQ-unigram coverage in different sources. The x-
coordinate refers to the ratio of top unigrams in one
source to the number of unigrams in SQ. For ex-
ample, if SQ contains n unigrams, the ratio 2 stands
for the top 2n unigrams in the other source. The y-
coordinate refers to the coverage rate of SQ. We can
observe that:
? Query Session naturally covers most of the su-
per query terms (over 70%).
? Though delicious tags are more similar to
queries than news and blog, as well as a relatively
higher coverage rate than the other two while size
ratio is small, the overall coverage rate is quite low:
only 21.28%. Note that this is contradict to existing
comparative studies between social bookmarks and
search logs (Bischoff et al, 2008). Clearly, when
considering bursty queries, the coverage and effec-
tiveness of social bookmarks is much lower than
considering all queries. Handling bursty queries is
much more difficult; only using social bookmarks to
predict queries is not a good choice. Other useful
sources should be enrolled.
? As the growth of the size ratio, the coverage
rate of news and blogs are both gradually increased.
When stable, both of them arrive at a relatively high
level (news: 66.36%, blog: 63.80%), which means
news and blogs have a higher potential to predict the
bursty topics in search. Moreover, in most cases,
news is still prior to blog ? not only the overall rate,
but also the size ratio comparison while the coverage
rate reaches 50% (news:109 < blog:183).
4.2.2 Bigram Coverage
Also we analyze the bigram coverage. This time
we only have 3 sources (no DT ). We rank the bi-
grams by the pointwise mutual information instead
of frequency, since not all the bigrams are ?real? col-
locations. Figure 5(b) shows the results.
Different from the unigram coverage, except that
the query session can naturally keep a high coverage
rate (66.07%), both news and blog cover poorly. For
this issue, we should re-consider the behavior that
users search and write articles. News or blog arti-
cles consist of completed sentences and paragraphs
which would contain plenty of meaningful bigrams.
However search queries consist of keywords ? rel-
atively discrete and regardless of order. Therefore,
except some proper nouns such as person?s name, a
lot of bigrams in the query log are formed in an ad-
hoc way. Since the different expressions of search
and writing, detecting unigrams is more informa-
tional than bigrams.
4.3 Coherence analysis
The above two experiments discuss the inter-
relations among different contexts. In this section
we will discuss the inner-relation within each par-
ticular context ? when it comes to a particular bursty
topic, how coherent is the information in each con-
text? Does the discussion keep consistent, or slip
into ambiguity?
We represent all the terms forming each context of
a bursty topic as a weighted graph: G = (V,E,W ),
where each v ? V stands for each term, wv stands
for the weight of vertex v in G, and each e ? E
stands for the semantic closeness between a pair of
terms (u, v) measured by sim(u, v). We define the
density of such a semantic graph as follows:
Den(G) = ?u,v?V,u 6=vsim(u, v)wuwv?u,v?V,u 6=vwuwv (2)
If sim(u, v) values the semantic similarity between
u and v, a high value of Den(G) implies that the
whole context is semantically consistent. Otherwise,
it may be diverse or ambiguous.
We build the graph of each context based on
WordNet6. For a pair of words, WordNet provides a
series of measures of the semantic similarity (Peder-
sen et al, 2004). We use the Path Distance Similar-
ity (path for short) and Lin Similarity (lin for short)
to measure sim(u, v). Both measures range in [0, 1].
For the convenience of computation, we choose
the top 1100 unigrams ranked by term frequency in
each source (if any) to represent the whole context
on one specific topic.
6http://wordnet.princeton.edu/
1084
(a) Unigram (b) Bigram
Figure 5: Coverage results
4.3.1 Overall
Table 5 shows the average overall density of each
sources over all the topics. From the table we can
Source path lin
SQ 0.098 0.128
QS 0.071 0.082
NT 0.103 0.129
NB 0.109 0.139
BT 0.099 0.109
BB 0.116 0.147
DT 0.102 0.127
Table 5: Overall Density
observe that QS has the lowest density in both of
the measures. It is because the queries in one user
session can easily shift to other (irrelevant) topics
even in a short time.
Another interesting phenomenon comes out that
for either news or blog, the body is denser than the
title, even if the body context contains much more
terms. It can be explained by the roles of the title
and the body in one article: the title contains a series
of words which briefly summarize a topic while the
body part would describe and discuss the title in de-
tails. When it maps to the semantic word network,
the title tends to contain the vertices scattered in the
graph, while the context of the body part would add
more semantically related vertices around the origi-
nal vertices to strength the relations. Thus, the body
part has a higher density than the title part.
4.3.2 The trend analysis
Figure 6 shows the tendency of the density in each
source. The x-coordinate refers to the TopN uni-
grams ranked by the term frequency in each source.
From Figure 6 we can find that in most cases, the av-
erage density will gradually decrease while less im-
portant terms are added, which implies that the most
important terms are denser, and other terms would
disperse the topic.
To better evaluate this tendency of each source,
Table 6 shows the change rate of the highest density
to the overall density measured by lin. We can easily
find the following facts:
? The highest density is achieved when a small
proportion of top terms are counted (6 sources for
Top5 and one for Top20), which also supports our
hypothesis: the more important, the more coherent.
? BB?s density drops the fastest of all (15.1%),
following by DT (10.6%). It may be because both
blog and delicious tag are generated by many users.
And the diversity of the users leads to different
prospectives, which dilutes the context significantly.
? Both NT and NB drop quite slowly (5.8%,
6.8%), which means the professional journalists
would have the relatively similar prospectives on the
same topic. Thus the topic does not disperse too
much. BT also keeps a high stability.
? Compared with news, blog is easier to disperse,
which can be reflected by the density comparison
between NT and BT . Although the density of BB
is still higher than NT , we should notice that these
two sources are not completely covered ? about 3/4
unigrams in these two contexts are not included in
1085
(a) path (b) lin
Figure 6: Trend of Density
the semantic networks. The curves clearly shows
BB dropped faster than NB. One can expect that
NB becomes denser than BB if all the unigrams in
both sources are included in the network.
Source Highest Den. Overall Den. Change
SQ 0.140(Top5) 0.128 -8.6%
QS 0.091(Top5) 0.082 -9.9%
NT 0.137(Top5) 0.129 -5.8%
NB 0.148(Top5) 0.139 -6.8%
BT 0.114(Top20) 0.109 -4.4%
BB 0.172(Top5) 0.147 -15.1%
DT 0.142(Top5) 0.127 -10.6%
Table 6: Tendency analysis of Density (Lin)
4.3.3 Case Study
From these 130 news topics, some of them shows
a special tendency of coherence. For example,
when more words are included, the density of the
topic?three gorge? drops rapidly in most of the
sources. The topic ?florida fires? has the same
trend. These topics are typically ?focus? topics,
which means users clearly pursue the unique event
while they use these terms. Thus, the density in top
unigrams is very high. It drops rapidly since users?
personal interests and opinions toward to this event
will be enrolled gradually.
In contrast, some topics like ?heather mills,?, ?in-
surance rate? express differently: their densities
gradually increase with the growth of the terms. By
observing these topics we find they are usually di-
verse topics (e.g: famous person name or entity
name), which may lead to diverse search intentions
of users. So the density of top unigrams is low and
gradually increased since one main aspect is proba-
bly strengthened.
5 Conclusion and Future work
In this paper, we have studied and compared how
the web content reacts to bursty events in multi-
ple contexts of web search and online media. Af-
ter a series of comprehensive experiments including
content similarity and predictability, the coverage
of search content, and semantic diversity, we found
that social bookmarks are not enough to predict the
queries because of a low coverage. Other sources
like news and blogs need to be added. Furthermore,
news can be seen as a consistent source which would
not only trigger the discussion of bursty events in
blogs but also in search queries.
When the target is to diversify the search results
and query suggestions, blogs and social bookmarks
are potentially useful accessory sources because of
the high diversity of content.
Our work serves as a feasibility investigation of
query suggestion for bursty events. Future work
would address on how to systematically predict and
recommend the bursty queries using online media,
as well as a reasonable evaluation metrics upon it.
Acknowledgments
We thank Prof. Kevin Chang for his support in data
and useful discussion. We thank the three anony-
mous reviewers for their useful comments. This
work is in part supported by the National Science
Foundation under award number IIS-0968489.
1086
References
Jon Kleinberg 2003. Bursty and Hierarchical Structure
in Streams Data Mining and Knowledge Discovery,
Vol 7(4):373-397
Daniel Gruhl, R. Guha, Ravi Kumar, Jasmine Novak and
Andrew Tomkins 2005. The Predictive Power of On-
line Chatter KDD ?05: Proceedings of the eleventh
ACM SIGKDD international conference on Knowl-
edge discovery in data mining, 78-87.
Ravi Kumar, Jasmine Novak, Prabhakar Raghavan and
Andrew Tomkins 2003. On the Bursty Evolution of
Blogspace WWW ?03: Proc. of the 12th International
World Wide Web Conference, 568-576.
Michail Vlachos and Christopher Meek and Zografoula
Vagena and Dimitrios Gunopulos 2004. Identifying
similarities, periodicities and bursts for online search
queries SIGMOD ?04: Proceedings of the 2004 ACM
SIGMOD international conference on Management of
data, 131-142.
Evgeniy Gabrilovich, Susan Dumais and Eric Horvitz
2004. Newsjunkie: providing personalized newsfeeds
via analysis of information novelty WWW ?04: Pro-
ceedings of the 13th international conference on World
Wide Web, 482-490.
Eytan Adar, Daniel S. Weld, and Brian N. Bershad and
Steven S. Gribble 2007. Why we search: visualizing
and predicting user behavior WWW ?07: Proceedings
of the 16th international conference on World Wide
Web, 161-170.
Aixin Sun, Meishan Hu and Ee-Peng Lim 2008. Search-
ing blogs and news: a study on popular queries SI-
GIR ?08: Proceedings of the 31st annual international
ACM SIGIR conference on Research and development
in information retrieval, 729-730.
JeanPhilippe Cointet, Emmanuel Faure and Camille Roth
2008. Intertemporal topic correlations in online media
: A Comparative Study on Weblogs and News Web-
sites ICWSM ?08: International Coference on We-
blogs and Social Media
Levon Lloyd, Prachi Kaulgud and Steven Skiena 2006.
Newspapers vs. Blogs: Who Gets the Scoop? AAAI
Spring Symposium on Computational Approaches to
Analyzing Weblogs
Michael Gamon, Sumit Basu, Dmitriy Belenko, Danyel
Fisher, Matthew Hurst, and Arnd Christian Konig
2008. BLEWS: Using Blogs to Provide Context for
News Articles ICWSM ?08: International Coference
on Weblogs and Social Media
Sanjay Sood, Sara Owsley, Kristian Hammond and Larry
Birnbaum 2007. TagAssist: Automatic Tag Sugges-
tion for Blog Posts ICWSM ?07: International Cofer-
ence on Weblogs and Social Media
Fernando Diaz 2009. Integration of news content into
web results WSDM ?09: Proceedings of the Second
ACM International Conference on Web Search and
Data Mining, 182-191.
Beate Krause, Andreas Hotho and Gerd Stumme 2008.
A Comparison of Social Bookmarking with Tradi-
tional Search Advances in Information Retrieval, Vol
4956/2008:101-113.
Paul Heymann, Georgia Koutrika and Hector Garcia-
Molina 2008. Can social bookmarking improve web
search? WSDM ?08: Proceedings of the international
conference on Web search and web data mining, 195-
206.
Shenghua Bao, Guirong Xue, Xiaoyuan Wu, Yong Yu,
Ben Fei and Zhong Su 2007. Optimizing web search
using social annotations WWW ?07: Proceedings of
the 16th international conference on World Wide Web,
501-510.
Kerstin Bischoff, Claudiu S. Firan, Wolfgang Nejdl and
Raluca Paiu 2008. Can all tags be used for search?
CIKM ?08: Proceeding of the 17th ACM conference
on Information and knowledge management, 193-202.
Rosie Jones, Benjamin Rey and Omid Madani 2006.
Generating query substitutions Proceedings of the
15th international conference on World Wide Web,,
387-396.
Filip Radlinski and Thorsten Joachims 2005. Query
chains: learning to rank from implicit feedback Pro-
ceedings of the 11th ACM SIGKDD international con-
ference on Knowledge discovery in data mining, 239-
248.
Qiaozhu Mei, Dengyong Zhou and Kenneth Church
2008. Query suggestion using hitting time CIKM ?08:
Proceeding of the 17th ACM conference on Informa-
tion and knowledge management, 469-478
Andrei Broder 2002. A Taxonomy of Web Search SIGIR
Forum, Vol 36(2):3-10.
Solomon Kullback and Richard Leibler 1951. On In-
formation and Sufficience Annals of Mathematical
Statistics, Vol 22(1):79-86.
David A. Field 1988. Laplacian Smoothing and Delau-
nay Triangulations Communications in Applied Nu-
merical Methods, Vol 4:709-712.
Stephen M. Katz 1987 Estimation of probabilities from
sparse data for the language model component of a
speech recogniser IEEE Transactions on Acoustics,
Speech, and Signal Processing, 35(3), 400-401.
Ted Pedersen, Siddharth Patwardhan and Jason Miche-
lizzi 2004. WordNet::Similarity: measuring the relat-
edness of concepts PHLT-NAACL ?04: Demonstration
Papers at HLT-NAACL 2004 on XX, 38-41.
1087
Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 1589?1599,
Edinburgh, Scotland, UK, July 27?31, 2011. c?2011 Association for Computational Linguistics
Rumor has it: Identifying Misinformation in Microblogs
Vahed Qazvinian Emily Rosengren Dragomir R. Radev Qiaozhu Mei
University of Michigan
Ann Arbor, MI
{vahed,emirose,radev,qmei}@umich.edu
Abstract
A rumor is commonly defined as a state-
ment whose true value is unverifiable. Ru-
mors may spread misinformation (false infor-
mation) or disinformation (deliberately false
information) on a network of people. Identi-
fying rumors is crucial in online social media
where large amounts of information are easily
spread across a large network by sources with
unverified authority. In this paper, we address
the problem of rumor detection in microblogs
and explore the effectiveness of 3 categories of
features: content-based, network-based, and
microblog-specific memes for correctly iden-
tifying rumors. Moreover, we show how these
features are also effective in identifying disin-
formers, users who endorse a rumor and fur-
ther help it to spread. We perform our exper-
iments on more than 10,000 manually anno-
tated tweets collected from Twitter and show
how our retrieval model achieves more than
0.95 in Mean Average Precision (MAP). Fi-
nally, we believe that our dataset is the first
large-scale dataset on rumor detection. It can
open new dimensions in analyzing online mis-
information and other aspects of microblog
conversations.
1 Introduction
A rumor is an unverified and instrumentally relevant
statement of information spread among people (Di-
Fonzo and Bordia, 2007). Social psychologists ar-
gue that rumors arise in contexts of ambiguity, when
the meaning of a situation is not readily apparent,
or potential threat, when people feel an acute need
for security. For instance rumors about ?office ren-
ovation in a company? is an example of an ambigu-
ous context, and the rumor that ?underarm deodor-
ants cause breast cancer? is an example of a context
in which one?s well-being is at risk (DiFonzo et al,
1994).
The rapid growth of online social media has made
it possible for rumors to spread more quickly. On-
line social media enable unreliable sources to spread
large amounts of unverified information among peo-
ple (Herman and Chomsky, 2002). Therefore, it is
crucial to design systems that automatically detect
misinformation and disinformation (the former of-
ten seen as simply false and the latter as deliberately
false information).
Our definition of a rumor is established based on
social psychology, where a rumor is defined as a
statement whose truth-value is unverifiable or delib-
erately false. In-depth rumor analysis such as deter-
mining the intent and impact behind the spread of
a rumor is a very challenging task and is not possi-
ble without first retrieving the complete set of social
conversations (e.g., tweets) that are actually about
the rumor. In our work, we take this first step to
retrieve a complete set of tweets that discuss a spe-
cific rumor. In our approach, we address two basic
problems. The first problem concerns retrieving on-
line microblogs that are rumor-related. In the second
problem, we try to identify tweets in which the ru-
mor is endorsed (the posters show that they believe
the rumor).
2 Related Work
We review related work on 3 main areas: Analyzing
rumors, mining microblogs, and sentiment analysis
and subjectivity detection.
2.1 Rumor Identification and Analysis
Though understanding rumors has been the sub-
ject of research in psychology for some time (All-
port and Lepkin, 1945), (Allport and Postman,
1947), (DiFonzo and Bordia, 2007), research has
1589
only recently begun to investigate how rumors are
manifested and spread differently online. Mi-
croblogging services, like Twitter, allow small
pieces of information to spread quickly to large au-
diences, allowing rumors to be created and spread in
new ways (Ratkiewicz et al, 2010).
Related research has used different methods to
study the spread of memes and false information
on the web. Leskovec et al use the evolution
of quotes reproduced online to identify memes and
track their spread overtime (Leskovec et al, 2009).
Ratkiewicz et al (Ratkiewicz et al, 2010) created
the ?Truthy? system, identifying misleading politi-
cal memes on Twitter using tweet features, includ-
ing hashtags, links, and mentions. Other projects
focus on highlighting disputed claims on the Inter-
net using pattern matching techniques (Ennals et al,
2010). Though our project builds on previous work,
our work differs in its general focus on identifying
rumors from a corpus of relevant phrases and our at-
tempts to further discriminate between phrases that
confirm, refute, question, and simply talk about ru-
mors of interest.
Mendoza et al explore Twitter data to analyze the
behavior of Twitter users under the emergency situ-
ation of 2010 earthquake in Chile (Mendoza et al,
). They analyze the re-tweet network topology and
find that the patterns of propagation in rumors dif-
fer from news because rumors tend to be questioned
more than news by the Twitter community.
2.2 Sentiment Analysis
The automated detection of rumors is similar to tra-
ditional NLP sentiment analysis tasks. Previous
work has used machine learning techniques to iden-
tify positive and negative movie reviews (Pang et
al., 2002). Hassan et al use a supervised Markov
model, part of speech, and dependency patterns to
identify attitudinal polarities in threads posted to
Usenet discussion posts (Hassan et al, 2010). Oth-
ers have designated sentiment scores for news sto-
ries and blog posts based on algorithmically gener-
ated lexicons of positive and negative words (God-
bole et al, 2007). Pang and Lee provide a detailed
overview of current techniques and practices in sen-
timent analysis and opinion mining (Pang and Lee,
2008; Pang and Lee, 2004).
Though rumor classification is closely related to
opinion mining and sentiment analysis, it presents
a different class of problem because we are con-
cerned not just with the opinion of the person post-
ing a tweet, but with whether the statements they
post appear controversial. The automatic identifica-
tion of rumors from a corpus is most closely related
to the identification of memes done in (Leskovec et
al., 2009), but presents new challenges since we seek
to highlight a certain type of recurring phrases. Our
work presents one of the first attempts at automatic
rumor analysis.
2.3 Mining Twitter Data
With its nearly constant update of new posts and
public API, Twitter can be a useful source for
collecting data to be used in exploring a num-
ber of problems related to natural language pro-
cessing and information diffusion (Bifet and Frank,
2010). Pak and Paroubek demonstrated experimen-
tally that despite frequent occurrences of irregular
speech patterns in tweets, Twitter can provide a use-
ful corpus for sentiment analysis (Pak and Paroubek,
2010). The diversity of Twitter users make this
corpus especially valuable. Ratkiewicz et alalso
use Twitter to detect and track misleading political
memes (Ratkiewicz et al, 2010).
Along with many advantages, using Twitter as a
corpus for sentiment analysis does present unusual
challenges. Because posts are limited to 140 charac-
ters, tweets often contain information in an unusu-
ally compressed form and, as a result, grammar used
may be unconventional. Instances of sarcasm and
humor are also prevalent (Bifet and Frank, 2010).
The procedures we used for the collection and anal-
ysis of tweets are similar to those described in previ-
ous work. However, our goal of developing compu-
tational methods to identify rumors being transmit-
ted through tweets differentiates our project.
3 Problem Definition
Assume we have a set of tweets that are about the
same topic that has some controversial aspects. Our
objective in this work is two-fold: (1) Extract tweets
that are about the controversial aspects of the story
and spread misinformation (Rumor retrieval). (2)
Identify users who believe that misinformation ver-
sus users who refute or question the rumor (Belief
1590
Name Rumor Regular Expression Query Status #tweets
obama Is Barack Obama muslim? Obama & (muslim|islam) false 4975
airfrance Air France mid-air crash photos? (air.france|air france) & (photo|pic|pix) false 505
cellphone Cell phone numbers going public? (cell|cellphone|cell phone) mostly false 215
michelle Michelle Obama hired too many staff? staff & (michelle obama|first lady|1st lady) partly true 299
palin Sarah Palin getting divorced? palin & divorce false 4423
Table 1: List of rumor examples and their corresponding queries used to collect data from Twitter
classification).
The following two tweets are two instances of the
tweets written about president Obama and the Mus-
lim world. The first tweet below is about president
Obama and Muslim world, where the second tweet
spread misinformation that president Obama is Mus-
lim.
(non-rumor) ?As Obama bows to Muslim leaders
Americans are less safe not only at home but also
overseas. Note: The terror alert in Europe... ?
(rumor) ?RT @johnnyA99 Ann Coulter Tells Larry
King Why People Think Obama Is A Muslim
http://bit.ly/9rs6pa #Hussein via @NewsBusters
#tcot ..?
The goal of the retrieval task is to discriminate
between such tweets. In the second task, we use
the tweets that are flagged as rumorous, and identify
users that endorse (believe) the rumor versus users
who deny or question it. The following three tweets
are about the same story. The first user is a believer
and the second and third are not.
(confirm) ?RT @moronwatch: Obama?s a Muslim. Or
if he?s not, he sure looks like one #whyimvotingre-
publican.?
(deny) ?Barack Obama is a Christian man who had
a Christian wedding with 2 kids baptised in Jesus
name. Tea Party clowns call that muslim #p2 #gop?
(doubtful) ?President Barack Obama?s Religion:
Christian, Muslim, or Agnostic? - The News
of Today (Google): Share With Friend...
http://bit.ly/bk42ZQ?
The first task is substantially more challenging
than a standard IR task because of the requirement of
both high precision (every result should be actually
discussing the rumor) and high recall (the set should
be complete). To do this, we submit a handcrafted
regexp (extracted from about.com) to Twitter and re-
trieve a large primitive set of tweets that is supposed
to have a high recall. This set however, contains a lot
of false positives, tweets that match the regexp but
are not about the rumor (e.g., ?Obama meets muslim
leaders?). Moreover, a rumor is usually stated using
various instances (e.g., ?Barack HUSSEIN Obama?
versus ?Obama is muslim?). Our goal is then to de-
sign a learning framework that filters all such false
positives and retrieves various instances of the same
rumor
Although our second task, belief classification,
can be viewed as an opinion mining task, it is sub-
stantially different from opinion mining in nature.
The difference from a standard opinion mining task
is that here we are looking for attitudes about a sub-
tle statement (e.g., ?Palin is getting divorce?) instead
of the overall sentiment of the text or the opinion
towards an explicit object or person (e.g., ?Sarah
Palin?).
4 Data
As September 2010, Twitter reports that its users
publish nearly 95 million tweets per day1. This
makes Twitter an excellent case to analyze misin-
formation in social media.
Our goal in this work was to collect and annotate
a large dataset that includes all the tweets that are
written about a rumor in a certain period of time. To
collect such a complete and self-contained dataset
about a rumor, we used the Twitter search API, and
retrieved all the tweets that matched a given regular
expression. This API is the only API that returns re-
sults from the entire public Twitter stream and not
a small randomly selected sample. To overcome the
rate limit enforced by Twitter, we collected match-
ing tweets once per hour, and remove any duplicates.
To use the search API, we carefully designed reg-
ular expression queries to be broad enough to match
1http://twitter.com/about
1591
all the tweets that are about a rumor. Each query
represents a popular rumor that is listed as ?false?
or only ?partly true? on About.com?s Urban Leg-
ends reference site2 between 2009 and 2010. Table 1
lists the rumor examples that we used to collect our
dataset alng with their corresponding regular ex-
pression queries and the number of tweets collected.
4.1 Annotation
We asked two annotators to go over all the tweets
in the dataset and mark each tweet with a ?1? if it
is about any of the rumors from Table 1, and with
a ?0? otherwise. This annotation scheme will be
used in our first task to detect false positives, tweets
that match the broad regular expressions and are re-
trieved, but are not about the rumor. For instance,
both of the following tweets match the regular ex-
pression for the palin example, but only the sec-
ond one is rumorous.
(0) ?McCain Divorces Palin over her ?untruths and out
right lies? in the book written for her. McCain?s
team says Palin is a petty liar and phony?
(1) ?Sarah and Todd Palin to divorce, according to local
Alaska paper. http://ow.ly/iNxF?
We also asked the annotators to mark each pre-
viously annotated rumorous tweet with ?11? if the
tweet poster endorses the rumor and with ?12? if the
user refutes the rumor, questions its credibility, or is
neutral.
(12) ?Sarah Palin Divorce Rumor Debunked on Face-
book http://ff.im/62Evd?
(11) ?Todd and Sarah Palin to divorce
http://bit.ly/15StNc?
Our annotation of more than 10,400 tweets shows
that %35 of all the instances that matched the regu-
lar expressions are false positives, tweets that are not
rumor-related but match the initial queries. More-
over, among tweets that are about particular ru-
mors, nearly %43 show the poster believe the rumor,
demonstrating the importance of identifying misin-
formation and those who are misinformed. Table 2
shows the basic statistics extracted from the annota-
tions for each story.
2http://urbanlegends.about.com
Rumor non-rumor (0) believe (11) deny/ (12) total
doubtful/neutral
obama 3,036 926 1,013 4975
airfrance 306 71 128 505
cellphone 132 74 9 215
michelle 83 191 25 299
palin 86 1,709 2,628 4,423
total 3,643 2,971 3,803 10,417
Table 2: Number of instances in each class from the an-
notated data
task ?
rumor retrieval 0.954
belief classification 0.853
Table 3: Inter-judge agreement in two annotation tasks in
terms of ?-statistic
4.2 Inter-Judge Agreement
To calculate the annotation accuracy, we annotated
500 instances twice. These annotations were com-
pared with each other, and the Kappa coefficient (?)
was calculated. The ? statistic is formulated as
? = Pr(a)? Pr(e)1? Pr(e)
where Pr(a) is the relative observed agreement
among raters, and Pr(e) is the probability that anno-
tators agree by chance if each annotator is randomly
assigning categories (Krippendorff, 1980; Carletta,
1996). Table 3 shows that annotators can reach
a high agreement in both extracting rumors (? =
0.95) and identifying believers (? = 0.85).
5 Approach
In this section, we describe a general framework,
which given a tweet, predicts (1) whether it is a
rumor-related statement, and if so (2) whether the
user believes the rumor or not. We describe 3 sets of
features, and explain why these are intuitive to use
for identification of rumors.
We process the tweets as they appear in the user
timeline, and do not perform any pre-processing.
Specially, we think that capitalization might be an
important property. So, we do not lower-case the
tweet texts either.
Our approach is based on building different Bayes
classifiers as high level features and then learning
a linear function of these classifiers for retrieval in
the first task and classification in the second. Each
1592
Bayes classifier, which corresponds to a feature fi,
calculates the likelihood ratio for a given tweet t, as
shown in Equation 1.
P (?+i |t)
P (??i |t)
= P (?
+
i )
P (??i )
P (t|?+i )
P (t|??i )
(1)
Here ?+i and ??i are two probabilistic models built
based on feature fi using a set of positive (+) and
negative (?) training data. The likelihood ratio ex-
presses how many times more likely the tweet t is
under the positive model than the negative model
with respect to fi.
For computational reasons and to avoid dealing
with very small numbers we use the log of the like-
lihood ratio to build each classifier.
LLi = log
P (?+i |t)
P (??i |t)
= log P (?
+
i )
P (??i )
+ log P (t|?
+
i )
P (t|??i )
(2)
The first term P (?+i )P (??i ) can be easily calculated us-ing the maximum likelihood estimates of the prob-
abilities (i.e., the estimate of each probability is the
corresponding relative frequency). The second term
is calculated using various features that we explain
below.
5.1 Content-based Features
The first set of features are extracted from the text of
the tweets. We propose 4 content based features. We
follow (Hassan et al, 2010) and present the tweet
with 2 different patterns:
? Lexical patterns: All the words and segments
in the tweet are represented as they appear and
are tokenized using the space character.
? Part-of-speech patterns: All words are replaced
with their part-of-speech tags. To find the part-
of-speech of a hashtag we treat it as a word
(since they could have semantic roles in the
sentence), by omitting the tag sign, and then
precede the tag with the label TAG/. We also
introduce a new tag, URL, for URLs that appear
in a tweet.
From each tweet we extract 4 (2 ? 2) features,
corresponding to unigrams and bigrams of each rep-
resentation. Each feature is the log-likelihood ra-
tio calculated using Equation 2. More formally,
we represent each tweet t, of length n, lexically
as (w1w2 ? ? ?wn) and with part-of-speech tags as
(p1p2 ? ? ? pn). After building the positive and nega-
tive models (?+, ??) for each feature using the train-
ing data, we calculate the likelihood ratio as defined
in Equation 2 where
P (t|?+)
P (t|??) =
n?
j=1
log P (wj |?
+)
P (wj |??)
(3)
for unigram-lexical features (TXT1) and
P (t|?+)
P (t|??) =
n?1?
j=1
log P (wjwj+1|?
+)
P (wjwj+1|??)
(4)
for bigram-based lexical features (TXT2). Simi-
larly, we define the unigram and bigram-based part-
of-speech features (POS1 and POS2) as the log-
likelihood ratio with respect to the positive and neg-
ative part-of-speech models.
5.2 Network-based Features
The features that we have proposed so far are all
based on the content of individual tweets. In the
second set of features we focus on user behavior on
Twitter. We observe 4 types of network-based prop-
erties, and build 2 features that capture them.
Twitter enables users to re-tweet messages from
other people. This interaction is usually easy to de-
tect because the re-tweeted messages generally start
with the specific pattern: ?RT @user?. We use this
property to infer about the re-tweeted message.
Let?s suppose a user ui re-tweets a message t from
the user uj (ui: ?RT @uj t?). Intuitively, t is more
likely to be a rumor if (1) uj has a history of posting
or re-tweeting rumors, or (2) ui has posted or re-
tweeted rumors in the past.
Given a set of training instances, we build a pos-
itive (?+) and a negative (??) user models. The
first model is a probability distribution over all users
that have posted a positive instance or have been re-
tweeted in a positive instance. Similarly, the sec-
ond model is a probability distribution over users
1593
that have posted (or been re-tweeted in) a negative
instance. After building the models, for a given
tweet we calculate two log-likelihood ratios as two
network-based features.
The first feature is the log-likelihood ratio that ui
is under a positive user model (USR1) and the sec-
ond feature is the log-likelihood ratio that the tweet
is re-tweeted from a user (uj) who is under a positive
user model than a negative user model (USR2).
The distinction between the posting user and the
re-tweeted user is important, since some times the
users modify the re-tweeted message in a way that
changes its meaning and intent. In the following ex-
ample, the original user is quoting president Obama.
The second user is re-tweeting the first user, but has
added more content to the tweet and made it sound
rumorous.
original message (non-rumor) ?Obama says he?s do-
ing ?Christ?s work?.?
re-tweeted (rumor) ?Obama says he?s doing ?Christ?s
work.? Oh my God, CHRIST IS A MUSLIM.?
5.3 Twitter Specific Memes
Our final set of features are extracted from memes
that are specific to Twitter: hashtags and URLs.
Previous work has shown the usefulness of these
memes (Ratkiewicz et al, 2010).
5.3.1 Hashtags
One emergent phenomenon in the Twitter ecosys-
tem is the use of hashtags: words or phrases prefixed
with a hash symbol (#). These hashtags are created
by users, and are widely used for a few days, then
disappear when the topic is outdated (Huang et al,
2010).
In our approach, we investigate whether hashtags
used in rumor-related tweets are different from other
tweets. Moreover, we examine whether people who
believe and spread rumors use hashtags that are dif-
ferent from those seen in tweets that deny or ques-
tion a rumor.
Given a set of training tweets of positive and neg-
ative examples, we build two statistical models (?+,
??), each showing the usage probability distribution
of various hashtags. For a given tweet, t, with a set
of m hashtags (#h1 ? ? ?#hm), we calculate the log-
likelihood ratio using Equation 2 where
Feature LL-ratio model
Content
TXT1 content unigram content unigram
TXT2 content bigram content unigram
POS1 content pos content pos unigram
POS2 content pos content pos bigram
Twitter
URL1 content unigram target URL unigram
URL2 content bigram target URL bigram
TAG hashtag hashtag
Network USR1 tweeting user all users in the dataUSR2 re-tweeted user all users in the data
Table 4: List of features used in our optimization frame-
work. Each feature is a log-likelihood ratio calculated
against a a positive (+) and negative (?) training models.
P (t|?+)
P (t|??) =
m?
j=1
log P (#hj |?
+)
P (#hj |??)
(5)
5.3.2 URLs
Previous work has discussed the role of URLs
in information diffusion on Twitter (Honeycutt and
Herring, 2009). Twitter users share URLs in their
tweets to refer to external sources or overcome the
length limit forced by Twitter. Intuitively, if a tweet
is a positive instance, then it is likely to be similar to
the content of URLs shared by other positive tweets.
Using the same reasoning, if a tweet is a negative
instance, then it should be more similar to the web
pages shared by other negative instances.
Given a set of training tweets, we fetch all the
URLs in these tweets and build ?+ and ?? once for
unigrams and once for bigrams. These models are
merely built on the content of the URLs and ignore
the tweet content. Similar to previous features, we
calculate the log-likelihood ratio of the content of
each tweet with respect to ?+ and ?? for unigrams
(URL1) and bigrams URL2).
Table 4 summarizes the set of features used in our
proposed framework, where each feature is a log-
likelihood ratio calculated against a positive (+) and
negative (?) training models. To build these lan-
guage models, we use the CMU Language Modeling
toolkit (Clarkson and Rosenfeld, 1997).
5.4 Optimization
We build an L1-regularized log-linear model (An-
drew and Gao, 2007) on various features discussed
before to predict each tweet. Suppose, a procedure
generates a set of candidates for an input x. Also,
1594
let?s suppose ? : X ? Y ? RD is a function that
maps each (x, y) to a vector of feature values. Here,
the feature vector is the vector of coefficients corre-
sponding to different network, content, and twitter-
based properties, and the parameter vector ? ? RD
(D ? 9 in our experiments) assigns a real-valued
weight to each feature. This estimator chooses ? to
minimize the sum of least squares and a regulariza-
tion term R.
?? = argmin
?
{12
?
i
||??, xi? ? yi||22 +R(?)} (6)
where the regularizer term R(?) is the weighted L1
norm of the parameters.
R(?) = ?
?
j
|?j | (7)
Here, ? is a parameter that controls the amount of
regularization (set to 0.1 in our experiments).
Gao et. al (Gao et al, 2007) argue that op-
timizing L1-regularized objective function is chal-
lenging since its gradient is discontinuous whenever
some parameters equal zero. In this work, we use
the orthant-wise limited-memory quasi-Newton al-
gorithm (OWL-QN), which is a modification of L-
BFGS that allows it to effectively handle the dis-
continuity of the gradient (Andrew and Gao, 2007).
OWL-QN is based on the fact that when restricted
to a single orthant, the L1 regularizer is differen-
tiable, and is in fact a linear function of ?. Thus,
as long as each coordinate of any two consecutive
search points does not pass through zero R(?) does
not contribute at all to the curvature of the function
on the segment joining them. Therefore, we can use
L-BFGS to approximate the Hessian of L(?) alone
and use it to build an approximation to the full reg-
ularized objective that is valid on a given orthant.
This algorithm works quite well in practice, and typ-
ically reaches convergence in even fewer iterations
than standard L-BFGS (Gao et al, 2007).
6 Experiments
We design 2 sets of experiments to evaluate our ap-
proach. In the first experiment we assess the effec-
tiveness of the proposed method when employed in
an Information Retrieval (IR) framework for rumor
retrieval and in the second experiment we employ
various features to detect users? beliefs in rumors.
6.1 Rumor Retrieval
In this experiment, we view different stories as
queries, and build a relevance set for each query.
Each relevance set is an annotation of the entire
10,417 tweets, where each tweet is marked as rel-
evant if it matches the regular expression query and
is marked as a rumor-related tweet by the annotators.
For instance, according to Table 2 the cellphone
dataset has only 83 relevant documents out of the
entire 10,417 documents.
For each query we use 5-fold cross-validation,
and predict the relevance of tweets as a function of
their features. We use these predictions and rank
all the tweets with respect to the query. To evalu-
ate the performance of our ranking model for a sin-
gle query (Q) with the set of relevant documents
{d1, ? ? ? , dm}, we calculate Average Precision as
AP (Q) = 1m
m?
k=1
Precision(Rk) (8)
where Rk is the set of ranked retrieval results from
the top result to the kth relevant document, dk (Man-
ning et al, 2008).
6.1.1 Baselines
We compare our proposed ranking model with a
number of other retrieval models. The first two sim-
ple baselines that indicate a difficulty lower-bound
for the problem are Random and Uniform meth-
ods. In the Random baseline, documents are ranked
based on a random number assignment to them. In
the Uniform model, we use a 5-fold cross validation,
and in each fold the label of the test documents is de-
termined by the majority vote from the training set.
The main baseline that we use in this work, is the
regular expression that was submitted to Twitter to
collect data (regexp). Using the same regular ex-
pression to mark the relevance of the documents will
cause a recall value of 1.00 (since it will retrieve all
the relevant documents), but will also retrieve false
positives, tweets that match the regular expression
but are not rumor-related. We would like to inves-
tigate whether using training data will help us de-
crease the rate of false positives in retrieval.
Finally, using the Lemur Toolkit software3, we
employ a KL divergence retrieval model with
3http://www.lemurproject.org/
1595
Dirichlet smoothing (KL). In this model, documents
are ranked according to the negation of the diver-
gence of query and document language models.
More formally, given the query language model ?Q,
and the document language model ?D, the docu-
ments are ranked by ?D(?Q||?D), where D is the
KL-divergence between the two models.
D(?Q||?D) =
?
w
p(w|?Q) log
p(w|?Q)
p(w|?D)
(9)
To estimate p(w|?D), we use Bayesian smoothing
with Dirichlet priors (Berger, 1985).
ps(w|?D) =
C(w,D) + ?.p(w|?S)
?+
?
w C(w,D)
(10)
where, ? is a parameter, C is the count function, and
thetaS is the collection language model. Higher val-
ues of ? put more emphasis on the collection model.
Here, we try two variants of the model, one using
the default parameter value in Lemur (? = 2000),
and one in which ? is tuned based on the the data
(? = 10). Using the test data to tune the parameter
value, ?, will help us find an upper-bound estimate
of the effectiveness of this method.
Table 5 shows the Mean Average Precision
(MAP) and F?=1 for each method in the rumor re-
trieval task. This table shows that a method that
employs training data to re-rank documents with
respect to rumors makes significant improvements
over the baselines and outperforms other strong re-
trieval systems.
6.1.2 Feature Analysis
To investigate the effectiveness of using indi-
vidual features in retrieving rumors, we perform
5-fold cross validations for each query, using
different feature sets each time. Figure 1 shows
the average precision and recall for our pro-
posed optimization system when content-based
(TXT1+TXT2+POS1+POS2), network-based
(USR1+USR2), and twitter specific memes
(TAG+URL1+URL2) are employed individually.
Figure 1 shows that features that are calculated us-
ing the content language models are very effective in
achieving high precision and recall. Twitter specific
features, especially hashtags, can result in high pre-
cisions but lead to a low recall value because many
Figure 1: Average precision and recall of the proposed
method employing each set of features: content-based,
network-based, and twitter specific.
tweets do not share hashtags or are not written based
on the contents of external URLs.
Finally, we find that user history can be a good
indicator of rumors. However, we believe that this
feature could be more helpful with a complete user
set and a more comprehensive history of their activ-
ities.
6.1.3 Domain Training Data
As our last experiment with rumor retrieval we in-
vestigate how much new labeled data from an emer-
gent rumor is required to effectively retrieve in-
stances of that particular rumor. This experiment
helps us understand how our proposed framework
could be generalized to other stories.
To do this experiment, we use the obama story,
which is a large dataset with a significant number of
false positive instances. We extract 400 randomly
selected tweets from this dataset and keep them for
testing. We also build an initial training dataset of
the other 4 rumors, and label them as not relevant.
We assess the performance of the retrieval model as
we gradually add the rest of the obama tweets. Fig-
ure 2 shows both Average Precision and labeling ac-
curacy versus the size of the labeled data used from
the obama dataset. This plot shows that both mea-
sures exhibit a fast growth and reach 80% when the
number of labeled data reaches 2000.
6.2 Belief Classification
In previous experiments we showed that maximiz-
ing a linear function of log-likelihood ratios is an
effective method in retrieving rumors. Here, we in-
1596
Method MAP 95% C.I. F?=1 95% C.I.
Random 0.129 [-0.065, 0.323] 0.164 [-0.051, 0.379]
Uniform 0.129 [-0.066, 0.324] 0.198 [-0.080, 0.476]
regexp 0.587 [0.305, 0.869] 0.702 [0.479, 0.925]
KL (? = 2000) 0.678 [0.458, 0.898] 0.538 [0.248, 0.828]
KL (? = 10) 0.803 [0.641, 0.965] 0.681 [0.614, 0.748]
LL (all 9 features) 0.965 [0.936, 0.994] 0.897 [0.828, 0.966]
Table 5: Mean Average Precision (MAP) and F?=1 of each method in the rumor retrieval task. (C.I.: Confidence
Interval)
Method Accuracy Precision Recall F?=1 Win/Loss Ratio
random 0.501 0.441 0.513 0.474 1.004
uniform 0.439 0.439 1.000 0.610 0.781
TXT 0.934 0.925 0.924 0.924 14.087
POS 0.742 0.706 0.706 0.706 2.873
content (TXT+POS) 0.941 0.934 0.930 0.932 15.892
network (USR) 0.848 0.873 0.765 0.815 5.583
TAG 0.589 0.734 0.099 0.175 1.434
URL 0.664 0.630 0.570 0.598 1.978
twitter (TAG+URL) 0.683 0.658 0.579 0.616 2.155
all 0.935 0.944 0.906 0.925 14.395
Table 6: Accuracy, precision, recall, F?=1, and win/loss ratio of belief classification using different features.
Figure 2: Average Precision and Accuracy learning curve
for the proposed method employing all 9 features.
vestigate whether this method, and in particular, the
proposed features are useful in detecting users? be-
liefs in a rumor that they post about. Unlike re-
trieval, detecting whether a user endorses a rumor or
refutes it may be possible using similar methods re-
gardless of the rumor. Intuitively, linguistic features
such as negation (e.g., ?obama is not a muslim?), or
capitalization (e.g., ?barack HUSSEIN obama ...?),
user history (e.g., liberal tweeter vs. conservative
tweeter), hashtags (e.g., #tcot vs. #tdot), and URLs
(e.g., links to fake airfrance crash photos) should
help to identify endorsements.
We perform this experiment by making a pool
of all the tweets that are marked as ?rumorous? in
the annotation task. Table 2 shows that there are
6,774 such tweets, from which 2,971 show belief
and 3,803 tweets show that the user is doubtful, de-
nies, or questions it.
Using various feature settings, we perform 5-fold
cross-validation on these 6,774 rumorous tweets.
Table 6 shows the results of this experiment in terms
of F-score, classification accuracy, and win/loss ra-
tio, the ratio of correct classification to an incorrect
1597
classification.
7 Conclusion
In this paper we tackle the fairly unaddressed prob-
lem of identifying misinformation and disinform-
ers in Microblogs. Our contributions in this pa-
per are two-fold: (1) We propose a general frame-
work that employs statistical models and maximizes
a linear function of log-likelihood ratios to retrieve
rumorous tweets that match a more general query.
(2) We show the effectiveness of the proposed fea-
ture in capturing tweets that show user endorsement.
This will help us identify disinformers or users that
spread false information in online social media.
Our work has resulted in a manually annotated
dataset of 10,000 tweets from 5 different controver-
sial topics. To the knowledge of authors this is the
first large-scale publicly available rumor dataset, and
can open many new dimensions in studying the ef-
fects of misinformation or other aspects of informa-
tion diffusion in online social media.
In this paper we effectively retrieve instances of
rumors that are already identified and evaluated by
an external source such as About.com?s Urban Leg-
ends reference. Identifying new emergent rumors
directly from the Twitter data is a more challenging
task. As our future work, we aim to build a sys-
tem that employs our findings in this paper and the
emergent patterns in the re-tweet network topology
to identify whether a new trending topic is a rumor
or not.
8 Acknowledgments
The authors would like to thank Paul Resnick,
Rahul Sami, and Brendan Nyhan for helpful discus-
sions. This work is supported by the National Sci-
ence Foundation grant ?SoCS: Assessing Informa-
tion Credibility Without Authoritative Sources? as
IIS-0968489. Any opinions, findings, and conclu-
sions or recommendations expressed in this paper
are those of the authors and do not necessarily re-
flect the views of the supporters.
References
Floyd H. Allport and Milton Lepkin. 1945. Wartime ru-
mors of waste and special privilege: why some people
believe them. Journal of Abnormal and Social Psy-
chology, 40(1):3 ? 36.
Gordon Allport and Leo Postman. 1947. The psychology
of rumor. Holt, Rinehart, and Winston, New York.
Galen Andrew and Jianfeng Gao. 2007. Scalable train-
ing of l1-regularized log-linear models. In ICML ?07,
pages 33?40.
James Berger. 1985. Statistical decision theory and
Bayesian Analysis (2nd ed.). New York: Springer-
Verlag.
Albert Bifet and Eibe Frank. 2010. Sentiment knowl-
edge discovery in twitter streaming data. In Bernhard
Pfahringer, Geoff Holmes, and Achim Hoffmann, edi-
tors, Discovery Science, volume 6332 of Lecture Notes
in Computer Science, pages 1?15. Springer Berlin /
Heidelberg.
Jean Carletta. 1996. Assessing agreement on classifi-
cation tasks: the kappa statistic. Comput. Linguist.,
22(2):249?254.
Philip Clarkson and Roni Rosenfeld. 1997. Statistical
language modeling using the cmu-cambridge toolkit.
Proceedings ESCA Eurospeech, 47:45?148.
Nicholas DiFonzo and Prashant Bordia. 2007. Rumor,
gossip, and urban legend. Diogenes, 54:19?35, Febru-
ary.
Nicholas DiFonzo, P. Prashant Bordia, and Ralph L. Ros-
now. 1994. Reining in rumors. Organizational Dy-
namics, 23(1):47?62.
Rob Ennals, Dan Byler, John Mark Agosta, and Barbara
Rosario. 2010. What is disputed on the web? In Pro-
ceedings of the 4th workshop on Information Credibil-
ity, WICOW ?10, pages 67?74.
Jianfeng Gao, Galen Andrew, Mark Johnson, and
Kristina Toutanova. 2007. A comparative study of pa-
rameter estimation methods for statistical natural lan-
guage processing. In ACL ?07.
Namrata Godbole, Manjunath Srinivasaiah, and Steven
Skiena. 2007. Large-scale sentiment analysis for
news and blogs. In Proceedings of the International
Conference on Weblogs and Social Media (ICWSM),
Boulder, CO, USA.
Ahmed Hassan, Vahed Qazvinian, and Dragomir Radev.
2010. What?s with the attitude? identifying sentences
with attitude in online discussions. In Proceedings of
the 2010 Conference on Empirical Methods in Natural
Language Processing, pages 1245?1255, Cambridge,
MA, October. Association for Computational Linguis-
tics.
Edward S Herman and Noam Chomsky. 2002. Manu-
facturing Consent: The Political Economy of the Mass
Media. Pantheon.
Courtenay Honeycutt and Susan C. Herring. 2009. Be-
yond microblogging: Conversation and collaboration
1598
via twitter. Hawaii International Conference on Sys-
tem Sciences, 0:1?10.
Jeff Huang, Katherine M. Thornton, and Efthimis N.
Efthimiadis. 2010. Conversational tagging in twitter.
In Proceedings of the 21st ACM conference on Hyper-
text and hypermedia, HT ?10, pages 173?178.
Klaus Krippendorff. 1980. Content Analysis: An Intro-
duction to its Methodology. Beverly Hills: Sage Pub-
lications.
Jure Leskovec, Lars Backstrom, and Jon Kleinberg.
2009. Meme-tracking and the dynamics of the news
cycle. In KDD ?09: Proceedings of the 15th ACM
SIGKDD international conference on Knowledge dis-
covery and data mining, pages 497?506.
Christopher D. Manning, Prabhakar Raghavan, and Hin-
rich Schu?tze. 2008. Introduction to Information Re-
trieval. Cambridge University Press.
Marcelo Mendoza, Barbara Poblete, and Carlos Castillo.
Twitter under crisis: Can we trust what we rt?
Alexander Pak and Patrick Paroubek. 2010. Twit-
ter as a corpus for sentiment analysis and opinion
mining. In Nicoletta Calzolari (Conference Chair),
Khalid Choukri, Bente Maegaard, Joseph Mariani,
Jan Odijk, Stelios Piperidis, Mike Rosner, and Daniel
Tapias, editors, Proceedings of the Seventh conference
on International Language Resources and Evaluation
(LREC?10), Valletta, Malta, may. European Language
Resources Association (ELRA).
Bo Pang and Lillian Lee. 2004. A sentimental educa-
tion: sentiment analysis using subjectivity summariza-
tion based on minimum cuts. In ACL?04, Morristown,
NJ, USA.
Bo Pang and Lillian Lee. 2008. Opinion mining and
sentiment analysis. Foundations and Trends in Infor-
mation Retrieval, 2:1?135.
Bo Pang, Lillian Lee, and Shivakumar Vaithyanathan.
2002. Thumbs up?: sentiment classification using ma-
chine learning techniques. In Proceedings of confer-
ence on Empirical methods in natural language pro-
cessing, EMNLP?02, pages 79?86.
Jacob Ratkiewicz, Michael Conover, Mark Meiss, Bruno
Gonc?alves, Snehal Patil, Alessandro Flammini, and
Filippo Menczer. 2010. Detecting and tracking
the spread of astroturf memes in microblog streams.
CoRR, abs/1011.3768.
1599
Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 1128?1137,
Uppsala, Sweden, 11-16 July 2010. c?2010 Association for Computational Linguistics
Cross-Lingual Latent Topic Extraction
Duo Zhang
University of Illinois at
Urbana-Champaign
dzhang22@cs.uiuc.edu
Qiaozhu Mei
University of Michigan
qmei@umich.edu
ChengXiang Zhai
University of Illinois at
Urbana-Champaign
czhai@cs.uiuc.edu
Abstract
Probabilistic latent topic models have re-
cently enjoyed much success in extracting
and analyzing latent topics in text in an un-
supervised way. One common deficiency
of existing topic models, though, is that
they would not work well for extracting
cross-lingual latent topics simply because
words in different languages generally do
not co-occur with each other. In this paper,
we propose a way to incorporate a bilin-
gual dictionary into a probabilistic topic
model so that we can apply topic models to
extract shared latent topics in text data of
different languages. Specifically, we pro-
pose a new topic model called Probabilis-
tic Cross-Lingual Latent Semantic Anal-
ysis (PCLSA) which extends the Proba-
bilistic Latent Semantic Analysis (PLSA)
model by regularizing its likelihood func-
tion with soft constraints defined based on
a bilingual dictionary. Both qualitative and
quantitative experimental results show that
the PCLSA model can effectively extract
cross-lingual latent topics from multilin-
gual text data.
1 Introduction
As a robust unsupervised way to perform shallow
latent semantic analysis of topics in text, prob-
abilistic topic models (Hofmann, 1999a; Blei et
al., 2003b) have recently attracted much atten-
tion. The common idea behind these models is the
following. A topic is represented by a multino-
mial word distribution so that words characteriz-
ing a topic generally have higher probabilities than
other words. We can then hypothesize the exis-
tence of multiple topics in text and define a gener-
ative model based on the hypothesized topics. By
fitting the model to text data, we can obtain an es-
timate of all the word distributions corresponding
to the latent topics as well as the topic distributions
in text. Intuitively, the learned word distributions
capture clusters of words that co-occur with each
other probabilistically.
Although many topic models have been pro-
posed and shown to be useful (see Section 2 for
more detailed discussion of related work), most
of them share a common deficiency: they are de-
signed to work only for mono-lingual text data and
would not work well for extracting cross-lingual
latent topics, i.e. topics shared in text data in
two different natural languages. The deficiency
comes from the fact that all these models rely on
co-occurrences of words forming a topical cluster,
but words in different language generally do not
co-occur with each other. Thus with the existing
models, we can only extract topics from text in
each language, but cannot extract common topics
shared in multiple languages.
In this paper, we propose a novel topic model,
called Probabilistic Cross-Lingual Latent Seman-
tic Analysis (PCLSA) model, which can be used to
mine shared latent topics from unaligned text data
in different languages. PCLSA extends the Proba-
bilistic Latent Semantic Analysis (PLSA) model
by regularizing its likelihood function with soft
constraints defined based on a bilingual dictio-
nary. The dictionary-based constraints are key to
bridge the gap of different languages and would
force the captured co-occurrences of words in
each language by PCLSA to be ?synchronized?
so that related words in the two languages would
have similar probabilities. PCLSA can be esti-
mated efficiently using the General Expectation-
Maximization (GEM) algorithm. As a topic ex-
traction algorithm, PCLSA would take a pair of
unaligned document sets in different languages
and a bilingual dictionary as input, and output a
set of aligned word distributions in both languages
that can characterize the shared topics in the two
languages. In addition, it also outputs a topic cov-
1128
erage distribution for each language to indicate the
relative coverage of different shared topics in each
language.
To the best of our knowledge, no previous work
has attempted to solve this topic extraction prob-
lem and generate the same output. The closest
existing work to ours is the MuTo model pro-
posed in (Boyd-Graber and Blei, 2009) and the
JointLDA model published recently in (Jagarala-
mudi and Daume? III, 2010). Both used a bilingual
dictionary to bridge the language gap in a topic
model. However, the goals of their work are dif-
ferent from ours in that their models mainly focus
on mining cross-lingual topics of matching word
pairs and discovering the correspondence at the
vocabulary level. Therefore, the topics extracted
using their model cannot indicate how a common
topic is covered differently in the two languages,
because the words in each word pair share the
same probability in a common topic. Our work fo-
cuses on discovering correspondence at the topic
level. In our model, since we only add a soft con-
straint on word pairs in the dictionary, their prob-
abilities in common topics are generally different,
naturally capturing which shows the different vari-
ations of a common topic in different languages.
We use a cross-lingual news data set and a re-
view data set to evaluate PCLSA. We also propose
a ?cross-collection? likelihood measure to quanti-
tatively evaluate the quality of mined topics. Ex-
perimental results show that the PCLSA model
can effectively extract cross-lingual latent topics
from multilingual text data, and it outperforms a
baseline approach using the standard PLSA on text
data in each language.
2 Related Work
Many topic models have been proposed, and the
two basic models are the Probabilistic Latent Se-
mantic Analysis (PLSA) model (Hofmann, 1999a)
and the Latent Dirichlet Allocation (LDA) model
(Blei et al, 2003b). They and their extensions
have been successfully applied to many prob-
lems, including hierarchical topic extraction (Hof-
mann, 1999b; Blei et al, 2003a; Li and McCal-
lum, 2006), author-topic modeling (Steyvers et al,
2004), contextual topic analysis (Mei and Zhai,
2006), dynamic and correlated topic models (Blei
and Lafferty, 2005; Blei and Lafferty, 2006), and
opinion analysis (Mei et al, 2007; Branavan et al,
2008). Our work is an extension of PLSA by in-
corporating the knowledge of a bilingual dictio-
nary as soft constraints. Such an extension is sim-
ilar to the extension of PLSA for incorporating so-
cial network analysis (Mei et al, 2008a) but our
constraint is different.
Some previous work on multilingual topic mod-
els assume documents in multiple languages are
aligned either at the document level, sentence level
or by time stamps (Mimno et al, 2009; Zhao and
Xing, 2006; Kim and Khudanpur, 2004; Ni et al,
2009; Wang et al, 2007). However, in many ap-
plications, we need to mine topics from unaligned
text corpus. For example, mining topics from
search results in different languages can facilitate
summarization of multilingual search results.
Besides all the multilingual topic modeling
work discussed above, comparable corpora have
also been studied extensively (e.g. (Fung, 1995;
Franz et al, 1998; Masuichi et al, 2000; Sadat
et al, 2003; Gliozzo and Strapparava, 2006)), but
most previous work aims at acquiring word trans-
lation knowledge or cross-lingual text categoriza-
tion from comparable corpora. Our work differs
from this line of previous work in that our goal is
to discover shared latent topics from multi-lingual
text data that are weakly comparable (e.g. the data
does not have to be aligned by time).
3 Problem Formulation
In general, the problem of cross-lingual topic ex-
traction can be defined as to extract a set of com-
mon cross-lingual latent topics covered in text col-
lections in different natural languages. A cross-
lingual latent topic will be represented as a multi-
nomial word distribution over the words in all
the languages, i.e. a multilingual word distri-
bution. For example, given two collections of
news articles in English and Chinese, respectively,
we would like to extract common topics simul-
taneously from the two collections. A discov-
ered common topic, such as the terrorist attack
on September 11, 2001, would be characterized
by a word distribution that would assign relatively
high probabilities to words related to this event in
both English and Chinese (e.g. ?terror?, ?attack?,
?afghanistan?, ?taliban?, and their translations in
Chinese).
As a computational problem, our input is a
multi-lingual text corpus, and output is a set of
cross-lingual latent topics. We now define this
problem more formally.
1129
Definition 1 (Multi-Lingual Corpus) A multi-
lingual corpus C is a set of text collections
{C1, C2, . . . , Cs}, where Ci = {di1, di2, . . . , diMi}
is a collection of documents in language Li with
vocabulary Vi = {wi1, wi2, . . . , wiNi}. Here, Mi is
the total number of documents in Ci, Ni is the to-
tal number of words in Vi, and dij is a document in
collection Ci.
Following the common assumption of bag-of-
words representation, we represent document dij
with a bag of words {wij1 , w
i
j2 , . . . , w
i
jd}, and use
c(wik, dij) to denote the count of word wik in docu-
ment dij .
Definition 2 (Cross-Lingual Topic): A cross-
lingual topic ? is a semantically coherent multi-
nomial distribution over all the words in the vo-
cabularies of languages L1, ..., Ls. That is, p(w|?)
would give the probability of a word w which can
be in any of the s languages under consideration. ?
is semantically coherent if it assigns high probabil-
ities to words that are semantically related either in
the same language or across different languages.
Clearly, we have
?s
i=1
?
w?Vi p(w|?) = 1 for any
cross-lingual topic ?.
Definition 3 (Cross-Lingual Topic Extrac-
tion) Given a multi-lingual corpus C, the task of
cross-lingual topic extraction is to model and ex-
tract k major cross-lingual topics {?1, ?2, . . . , ?k}
from C, where ?i is a cross-lingual topic, and k is
a user specified parameter.
The extracted cross-lingual topics can be di-
rectly used as a summary of the common con-
tent of the multi-lingual data set. Note that once
a cross-lingual topic is extracted, we can eas-
ily obtain its representation in each language Li
by ?splitting? the cross-lingual topic into multi-
ple word distributions in different languages. For-
mally, the word distribution of a cross-lingual
topic ? in language Li is given by pi(wi|?) =
p(wi|?)
?
w?Vi
p(w|?) .
These aligned language-specific word distribu-
tions can directly review the variations of topics
in different languages. They can also be used to
analyze the difference of the coverage of the same
topic in different languages. Moreover, they are
also useful for retrieving relevant articles or pas-
sages in each language and aligning them to the
same common topic, thus essentially also allow-
ing us to integrate and align articles in multiple
languages.
4 Probabilistic Cross-Lingual Latent
Semantic Analysis
In this section, we present our probabilistic cross-
lingual latent semantic analysis (PCLSA) model
and discuss how it can be used to extract cross-
lingual topics from multi-lingual text data.
The main reason why existing topic models
can?t be used for cross-lingual topic extraction is
because they cannot cross the language barrier.
Intuitively, in order to cross the language barrier
and extract a common topic shared in articles in
different languages, we must rely on some kind
of linguistic knowledge. Our PCLSA model as-
sumes the availability of bi-lingual dictionaries for
at least some language pairs, which are generally
available for major language pairs. Specifically,
for text data in languages L1, ..., Ls, if we rep-
resent each language as a node in a graph and
connect those language pairs for which we have a
bilingual dictionary, the minimum requirement is
that the whole graph is connected. Thus, as a min-
imum, we will need s? 1 distinct bilingual dictio-
naries. This is so that we can potentially cross all
the language barriers.
Our key idea is to ?synchronize? the extraction
of monolingual ?component topics? of a cross-
lingual topic from individual languages by forcing
a cross-lingual topic word distribution to assign
similar probabilities to words that are potential
translations according to a Li-Lj bilingual dictio-
nary. We achieve this by adding such preferences
formally to the likelihood function of a probabilis-
tic topic model as ?soft constraints? so that when
we estimate the model, we would try to not only
fit the text data well (which is necessary to extract
coherent component topics from each language),
but also satisfy our specified preferences (which
would ensure the extracted component topics in
different languages are semantically related). Be-
low we present how we implement this idea in
more detail.
A bilingual dictionary for languages Li and Lj
generally would give us a many-to-many map-
ping between the vocabularies of the two lan-
guages. With such a mapping, we can construct
a bipartite graph Gij = (Vij , Eij) between the
two languages where if one word can be poten-
tially translated into another word, the two words
would be connected with an edge. An edge can
be weighted based on the probability of the cor-
responding translation. An example graph for
1130
Chinese-English dictionary is shown in Figure 1.
Figure 1: A Dictionary based Word Graph
With multiple bilingual dictionaries, we can
merge the graphs to generate a multi-partite graph
G = (V,E). Based on this graph, the PCLSA
model extends the standard PLSA by adding a
constraint to the likelihood function to ?smooth?
the word distributions of topics in PLSA on the
multi-partite graph so that we would encourage the
words that are connected in the graph (i.e. pos-
sible translations of each other) to be given simi-
lar probabilities by every cross-lingual topic. Thus
when a cross-lingual topic picks up words that co-
occur in mono-lingual text, it would prefer pick-
ing up word pairs whose translations in other lan-
guages also co-occur with each other, giving us a
coherent multilingual word distribution that char-
acterizes well the content of text in different lan-
guages.
Specifically, let ? = {?j} (j = 1, ..., k) be a set
of k cross-lingual topic models to be discovered
from a multilingual text data set with s languages
such that p(w|?i) is the probability of word w ac-
cording to the topic model ?i.
If we are to use the regular PLSA to model our
data, we would have the following log-likelihood
and we usually use a maximum likelihood estima-
tor to estimate parameters and discover topics.
L(C) =
s
?
i=1
?
d?Ci
?
w
c(w, d) log
k
?
j=1
p(?j |d)p(w|?j)
Our main extension is to add to L(C) a cross-
lingual constraint term R(C) to incorporate the
knowledge of bilingual dictionaries. R(C) is de-
fined as
R(C) = 12
?
?u,v??E
w(u, v)
k
?
j=1
(p(wu|?j)Deg(u) ?
p(wv|?j)
Deg(v) )
2
where w(u, v) is the weight on the edge between
u and v in the multi-partite graph G = (V,E),
which in our experiments is set to 1, and Deg(u)
is the degree of word u, i.e. the sum of the weights
of all the edges ending with u.
Intuitively, R(C) measures the difference be-
tween p(wu|?j) and p(wv|?j) for each pair (u, v)
in a bilingual dictionary; the more they differ, the
larger R(C) would be. So it can be regarded as
a ?loss function? to help us assess how well the
?component word distributions? in multiple lan-
guages are correlated semantically. Clearly, we
would like the extracted topics to have a small
R(C). We choose this specific form of loss func-
tion because it would make it convenient to solve
the optimization problem of maximizing the cor-
responding regularized maximum likelihood (Mei
et al, 2008b). The normalization with Deg(u)
and Deg(v) can be regarded as a way to compen-
sate for the potential ambiguity of u and v in their
translations.
Putting L(C) and R(C) together, we would
like to maximize the following objective function
which is a regularized log-likelihood:
O(C, G) = (1 ? ?)L(C)? ?R(C) (1)
where ? ? (0, 1) is a parameter to balance the
likelihood and the regularizer. When ? = 0, we
recover the standard PLSA.
Specifically, we will search for a set of values
for all our parameters that can maximize the ob-
jective function defined above. Our parameters
include all the cross-lingual topics and the cov-
erage distributions of the topics in all documents,
which we denote by ? = {p(w|?j), p(?j |d)}d,w,j
where j = 1, ..., k, w varies over the entire vo-
cabularies of all the languages , d varies over
all the documents in our collection. This opti-
mization problem can be solved using a General-
ized Expectation-Maximization (GEM) algorithm
as described in (Mei et al, 2008a).
Specifically, in the E-step of the algorithm, the
distribution of hidden variables is computed using
Eq. 2.
z(w, d, j) = p(?j |d)p(w|?j)?
j? p(?j? |d)p(w|?j?)
(2)
Then in the M-step, we need to maximize the
complete data likelihood Q(?;?n):
Q(?;?n) = (1? ?)L?(C)? ?R(C)
1131
where
L?(C) =
?
d
?
w
c(w, d)
?
j
z(w, d, j) log p(?j |d)p(w|?j), (3)
with the constraints that
?
j p(?j |d) = 1 and
?
w p(w|?j) = 1.
There is a closed form solution if we only want
to maximize the L?(C) part:
p(n+1)(?j |d) =
?
w c(w, d)z(w, d, j)
?
w
?
j? c(w, d)z(w, d, j?)
p(n+1)(w|?j) =
?
d c(w, d)z(w, d, j)
?
d
??
w c(w?, d)z(w?, d, j)
(4)
However, there is no closed form solution in the
M-step for the whole objective function. Fortu-
nately, according to GEM we do not need to find
the local maximum of Q(?;?n) in every M-step,
and we only need to find a new value ?n+1 to im-
prove the complete data likelihood, i.e. to make
sure Q(?n+1; ?n) ? Q(?n; ?n). So our method
is to first maximize the L?(C) part using Eq. 4 and
then use Eq. 5 to gradually increase the R(C) part.
p(t+1)(wu|?j) = (1? ?)p(t)(wu|?j) (5)
+ ?
?
?u,v??E
w(u, v)
Deg(v)
p(t)(wv|?j)
Here, parameter ? is the length of each smooth-
ing step. Obviously, after each smoothing step,
the sum of the probabilities of all the words in one
topic is still equal to 1. We smooth the parameters
until we cannot get a better parameter set ?n+1.
Then, we continue to the next E-step. If there is
no ?n+1 s.t. Q(?n+1; ?n) ? Q(?n; ?n), then
we consider ?n to be the local maximum point of
the objective function Eq. 1.
5 Experiment Design
5.1 Data Set
The data set we used in our experiment is collected
from news articles of Xinhua English and Chi-
nese newswires. The whole data set is quite big,
containing around 40,000 articles in Chinese and
35,000 articles in English. For different purpose of
our experiments, we randomly selected different
number of documents from the whole corpus, and
we will describe the concrete statistics in each ex-
periment. To process the Chinese corpus, we use
a simple segmenter1 to split the data into Chinese
phrases. Both Chinese and English stopwords are
removed from our data.
The dictionary file we used for our PCLSA
model is from mandarintools.com2. For each Chi-
nese phrase, if it has several English meanings, we
add an edge between it and each of its English
translation. If one English translation is an En-
glish phrase, we add an edge between the Chinese
phrase and each English word in the phrase.
5.2 Baseline Method
As a baseline method, we can apply the standard
PLSA (Hofmann, 1999a) directly to the multi-
lingual corpus. Since PLSA takes advantage of
the word co-occurrences in the document level to
find semantic topics, directly using it for a multi-
lingual corpus will result in finding topics mainly
reflecting a single language (because words in dif-
ferent languages would not co-occur in the same
document in general). That is, the discovered top-
ics are mostly monolingual. These monolingual
topics can then be aligned based on a bilingual dic-
tionary to suggest a possible cross-lingual topic.
6 Experimental Results
6.1 Qualitative Comparison
To qualitatively compare PCLSAwith the baseline
method, we compare the word distributions of top-
ics extracted by them. The data set we used in this
experiment is selected from the Xinhua News data
during the period from Jun. 8th, 2001 to Jun. 15th,
2001. There are totally 1799 English articles and
1485 Chinese articles in the data set. The num-
ber of topics to be extracted is set to 10 for both
methods.
Table 1 shows the experimental results. To
make it easier to understand, we add an English
translation to each Chinese phrase in our results.
The first ten rows show sample topics of the mod-
eling results of traditional PLSA model. We can
see that it only contains mono-language topics,
i.e. the topics are either in Chinese or in En-
glish. The next ten rows are the results from
our PCLSA model. Compared with the base-
line method, PCLSA can not only find coherent
topics from the cross-lingual corpus, but it can
also show the content about one topic from both
two language corpora. For example, in ?Topic 2?
1http://www.mandarintools.com/segmenter.html
2http://www.mandarintools.com/cedict.html
1132
Table 2: Synthetic Data Set from Xinhua News
English Shrine Olympic Championship
90 101 70
Chinese CPC Anniversary Afghan War Championship
95 206 72
which is about ?Israel? and ?Palestinian?, the Chi-
nese corpus mentions a lot about ?Arafat? who is
the leader of ?Palestinian?, while the English cor-
pus discusses more on topics such as ?cease fire?
and ?women?. Similarly, in ?Topic 9?, the topic
is related to Philippine, the Chinese corpus men-
tions some environmental situation in Philippine,
while the English corpus mentions a lot about
?Abu Sayyaf?.
6.2 Discovering Common Topics
To demonstrate the ability of PCLSA for finding
common topics in cross-lingual corpus, we use
some event names, e.g. ?Shrine? and ?Olympic?,
as queries and randomly select a certain number of
documents from the whole corpus, which are re-
lated to the queries. The number of documents for
each query in the synthetic data set is shown in Ta-
ble 2. In either the English corpus or the Chinese
corpus, we select a smaller number of documents
about topic ?Championship? combined with the
other two topics in the same corpus. In this way,
when we want to extract two topics from either En-
glish or Chinese corpus, the ?Championship? topic
may not be easy to extract, because the other two
topics have more documents in the corpus. How-
ever, when we use PCLSA to extract four topics
from the two corpora together, we expect that the
topic ?Championship? will be found, because now
the sum of English and Chinese documents related
to ?Championship? is larger than other topics. The
experimental result is shown in Table 3. The first
two columns are the two topics extracted from En-
gish corpus, the third and the forth columns are
two topics from Chinese corpus, and the other four
columns are the results from cross-lingual cor-
pus. We can see that in either the Chinese sub-
collection or the English sub-collection, the topic
?Championship? is not extracted as a significant
topic. But, as expected, the topic ?Championship?
is extracted from the cross-lingual corpus, while
the topic ?Olympic? and topic ?Shrine? are merged
together. This demonstrate that PCLSA is capable
of extracting common topics from a cross-lingual
corpus.
6.3 Quantitative Evaluation
We also quantitatively evaluate how well our
PCLSA model can discover common topics
among corpus in different languages. We pro-
pose a ?cross-collection? likelihood measure for
this purpose. The basic idea is: suppose we got
k cross-lingual topics from the whole corpus, then
for each topic, we split the topic into two sepa-
rate set of topics, English topics and Chinese top-
ics, using the splitting formula described before,
i.e. pi(wi|?) = p(w
i|?)
?
w?Vi
p(w|?) . Then, we use the
word distribution of the Chinese topics (translating
the words into English) to fit the English Corpus
and use the word distribution of the English top-
ics (translating the words into Chinese) to fit the
Chinese Corpus. If the topics mined are common
topics in the whole corpus, then such a ?cross-
collection? likelihood should be larger than those
topics which are not commonly shared by the En-
glish and the Chinese corpus. To calculate the
likelihood of fitness, we use the folding-in method
proposed in (Hofmann, 2001). To translate topics
from one language to another, e.g. Chinese to En-
glish, we look up the bilingual dictionary and do
word-to-word translation. If one Chinese word has
several English translations, we simply distribute
its probability mass equally to each English trans-
lation.
For comparison, we use the standard PLSA
model as the baseline. Basically, suppose PLSA
mined k semantic topics in the Chinese corpus and
k semantic topics in the English corpus. Then, we
also use the ?cross-collection? likelihood measure
to see how well those k semantic Chinese topics fit
the English corpus and those k semantic English
topics fit the Chinese corpus.
We totally collect three data sets to compare the
performance. For the first data set, (English 1,
Chinese 1), both the Chinese and English corpus
are chosen from the Xinhua News Data during
the period from 2001.06.08 to 2001.06.15, which
has 1799 English articles and 1485 Chinese ar-
ticles. For the second data set, (English 2, Chi-
nese 2), the Chinese corpus Chinese 2 is the same
as Chinese 1, but the English corpus is chosen
from 2001.06.14 to 2001.06.19 which has 1547
documents. For the third data set, (English 3, Chi-
nese 3), the Chinese corpus is the same as in data
set one, but the English corpus is chosen from
2001.10.02 to 2001.10.07 which contains 1530
documents. In other words, in the first data set,
1133
Table 1: Qualitative Evaluation
Topic 0 Topic 1 Topic 2 Topic 3 Topic 4 Topic 5 Topic 6 Topic 7 Topic 8 Topic 9
j(party) +"(crime) ?C(athlete) ?(palestine) \*(collaboration) s?(education) israel bt dollar china
??j(communist) @(agriculture) 	(champion) ????(palestine) ?0(shanghai) E(ball) palestinian beat percent cooperate
??(revolution) @?(travel) ?)?(championship) 1??(israel) ?(relation) ??(league) eu final million shanghai
j?(party member) Qs(heathendom) ?(base) *?(cease fire) ?)(bilateral) E(soccer) police championship index develop
??(central) ??(public security) ??E(badminton) ?\)(UN) ?4(trade) I?(minute) report play stock beije
?B(ism) w(name) ?(sports) ??(mid east) :(president) ??(team member) secure champion point particulate
?\(cadre) ?(case) ??(final) ??(lebanon) )(country) s(teacher) kill win share matter
??(chairman mao) ?(law enforcement) E(women) j??(macedon) ?P(friendly) ?B?(school) europe olympic close sco
??(chinese communist) =(city) 6?(chess) ?B(conflict) ??(meet) E?(team) egypt game 0 invest
s(leader) ?(penalize) H?(fitness) ??(talk) [?(russia)  (grade A) treaty cup billion project
?)(bilateral) ??(league) israel cooperate ?C(athlete) party eu invest 0 ??(absorb)
\*(collaboration) w(name) 1??(israel) sco particulate j(party) khatami =?(investment) dollar ?
??(talk) E(ball) bt develop 	 communist ireland 7?(billion) percent ?Y?e(abu)
?P(friendly) ??(shenhua) palestinian country athlete revolution ?}(ireland) s?(education) index ?
?(palestine) ??(host) ceasefire president champion ?B(-ism) elect ??(environ. protect.) million (?(particle)country A ?n(arafat) apec ii ?(antiwar) vote ??(money) stock philippine
?\)(UN) ball women shanghai 6?(chess) 3?(comrade) presidential ?B?(school) billion abu
s|(leader) ?y(jinde) jerusalem africa competition ??(revolution) cpc market point ?(base)bilateral ?(season) mideast meet contestant j?(party) iran s(teacher) 7(billion) ?state E?(player) lebanon T?(zemin jiang) v(gymnastics) ideology referendum business share ?(object)
Table 3: Effectiveness of Extracting Common Topics
English 1 English 2 Chinese 1 Chinese 2 Cross 1 Cross 2 Cross 3 Cross 4
japan olympic ??j(CPC) ??F(afghan) koizumi ??(taliban) swim ?|(worker)
shrine ioc ?(championship) ?(taliban) yasukuni /(military) ?(championship) party
visit beije -(world) ??(taliban) ioc city ?y(free style) ??(three)
koizumi game ?.(thought) /(military) japan refugee !y(diving) j.?(marx)
yasukuni july ?X(theory) K?(attack) olympic side ?)?(championship) communist
war bid j.?(marx) ?(US army) beije ?(US army) ???(semi final) marx
august swim ?y(swim) [(laden) shrine q(bomb) competition theory
asia vote ?)?(championship) \?(army) visit 	Y(kabul) ?y(swim) Oj(found party)
criminal championship j(party) q(bomb) ???(olympic) 8?(attack) ?9(record) ??j(CPC)
ii committee Oj(found party) 	Y(kabul) ???.(olympic) 
?(refugee) [??(xuejuan luo) revolution
the English corpus and Chinese corpus are com-
parable with each other, because they cover simi-
lar events during the same period. In the second
data set, the English and Chinese corpora share
some common topics during the overlap period.
The third data is the most tough one since the two
corpora are from different periods. The purpose of
using these three different data sets for evaluation
is to test how well PCLSA can mine common top-
ics from either a data set where the English corpus
and the Chinese corpus are comparable or a data
set where the English corpus and the Chinese cor-
pus rarely share common topics.
The experimental results are shown in Table 4.
Each row shows the ?cross-collection? likelihood
of using the ?cross-collection? topics to fit the data
set named in the first column. For example, in
the first row, the values are the ?cross-collection?
likelihood of using Chinese topics found by differ-
ent methods from the first data set to fit English 1.
The last collum shows howmuch improvement we
got from PCLSA compared with PLSA. From the
results, we can see that in all the data sets, our
PCLSA has higher ?cross-collection? likelihood
value, which means it can find better common top-
ics compared to the baseline method. Notice that
the Chinese corpora are the same in all three data
sets. The results show that both PCLSA and PLSA
get lower ?cross-collection? likelihood for fitting
the Chinese corpora when the data set becomes
?tougher?, i.e. less topic overlapping, but the im-
Table 4: Quantitative Evaluation of Common
Topic Finding (?cross-collection? log-likelihood)
PCLSA PLSA Rel. Imprv.
English 1 -2.86294E+06 -3.03176E+06 5.6%
Chinese 1 -4.69989E+06 -4.85369E+06 3.2%
English 2 -2.48174E+06 -2.60805E+06 4.8%
Chinese 2 -4.73218E+06 -4.88906E+06 3.2%
English 3 -2.44714E+06 -2.60540E+06 6.1%
Chinese 3 -4.79639E+06 -4.94273E+06 3.0%
provement of PCLSA over PLSA does not drop
much. On the other hand, the improvement of
PCLSA over PLSA on the three English corpora
does not show any correlation with the difficulty
of the data set.
6.4 Extracting from Multi-Language Corpus
In the previous experiments, we have shown the
capability and effectiveness of the PCLSA model
in latent topic extraction from two language cor-
pora. In fact, the proposed model is general and
capable of extracting latent topics from multi-
language corpus. For example, if we have dic-
tionaries among multiple languages, we can con-
struct a multi-partite graph based on the corre-
spondence between those vocabularies, and then
smooth the PCLSA model with this graph.
To show the effectiveness of PCLSA in min-
ing multiple language corpus, we first construct a
simulated data set based on 1115 reviews of three
brands of laptops, namely IBM (303), Apple(468)
and DELL(344). To simulate a three language cor-
1134
Table 5: Effectiveness of Latent Topic Extraction from Multi-Language Corpus
Topic 0 Topic 1 Topic 2 Topic 3 Topic 4 Topic 5 Topic 6 Topic 7
cd(apple) battery(dell) mouse(dell) print(apple) port(ibm) laptop(ibm) os(apple) port(dell)port(apple) drive(dell) button(dell) resolution(dell) card(ibm) t20(ibm) run(apple) 2(dell)drive(apple) 8200(dell) touchpad(dell) burn(apple) modem(ibm) thinkpad(ibm) 1(apple) usb(dell)airport(apple) inspiron(dell) pad(dell) normal(dell) display(ibm) battery(ibm) ram(apple) 1(dell)firewire(apple) system(dell) keyboard(dell) image(dell) built(ibm) notebook(ibm) mac(apple) 0(dell)dvd(apple) hour(dell) point(dell) digital(apple) swap(ibm) ibm(ibm) battery(apple) slot(dell)usb(apple) sound(dell) stick(dell) organize(apple) easy(ibm) 3(ibm) hour(apple) firewire(dell)rw(apple) dell(dell) rest(dell) cds(apple) connector(ibm) feel(ibm) 12(apple) display(dell)card(apple) service(dell) touch(dell) latch(apple) feature(ibm) hour(ibm) operate(apple) standard(dell)mouse(apple) life(dell) erase(dell) advertise(dell) cd(ibm) high(ibm) word(apple) fast(dell)
osx(apple) applework(apple) port(dell) battery(dell) lightest(ibm) uxga(dell) light(ibm) battery(apple)memory(dell) file(apple) port(apple) battery(ibm) quality(dell) ultrasharp(dell) ultrabay(ibm) point(dell)special(dell) bounce(apple) port(ibm) battery(apple) year(ibm) display(dell) connector(ibm) touchpad(dell)crucial(dell) quit(apple) firewire(apple) geforce4(dell) hassle(ibm) organize(apple) dvd(ibm) button(dell)memory(apple) word(apple) imac(apple) 100mhz(apple) bania(dell) learn(apple) nice(ibm) hour(apple)memory(ibm) file(ibm) firewire(dell) 440(dell) 800mhz(apple) logo(apple) modem(ibm) battery(ibm)netscape(apple) file(dell) firewire(ibm) bus(apple) trackpad(apple) postscript(apple) connector(dell) battery(dell)reseller(apple) microsoft(apple) jack(apple) 8200(dell) cover(ibm) ll(apple) light(apple) fan(dell)10(dell) ms(apple) playback(dell) 8100(dell) workmanship(dell) sxga(dell) light(dell) erase(dell)special(apple) excel(apple) jack(dell) chipset(dell) section(apple) warm(apple) floppy(ibm) point(apple)
2000(ibm) ram(apple) port(dell) itune(apple) uxga(dell) port(apple) pentium(dell) drive(ibm)window(ibm) ram(ibm) port(apple) applework(apple) screen(dell) port(ibm) processor(dell) drive(dell)2000(apple) ram(dell) port(ibm) imovie(apple) screen(ibm) port(dell) p4(dell) drive(apple)2000(dell) screen(apple) 2(dell) import(apple) screen(apple) usb(apple) power(dell) hard(ibm)window(apple) 1(apple) 2(apple) battery(apple) ultrasharp(dell) plug(apple) pentium(apple) osx(apple)window(dell) screen(ibm) 2(ibm) iphoto(apple) 1600x1200(dell) cord(apple) pentium(ibm) hard(dell)portege(ibm) screen(dell) speak(dell) battery(ibm) display(dell) usb(ibm) keyboard(dell) hard(apple)option(ibm) 1(ibm) toshiba(dell) battery(dell) display(apple) usb(dell) processor(ibm) card(ibm)hassle(ibm) 1(dell) speak(ibm) hour(apple) display(ibm) firewire(apple) processor(apple) dvd(ibm)device(ibm) maco(apple) toshiba(ibm) hour(ibm) view(dell) plug(ibm) power(apple) card(dell)
pus, we use an ?IBM? word, an ?Apple? word, and
a ?Dell? word to replace an English word in their
corpus. For example, we use ?IBM10?, ?Apple10?,
?Dell10? to replace the word ?CD? whenever it ap-
pears in an IBM?s, Apple?s, or Dell?s review. Af-
ter the replacement, the reviews about IBM, Ap-
ple, and Dell will not share vocabularies with each
other. On the other hand, for any three created
words which represent the same English word, we
add three edges among them, and therefore we
get a simulated dictionary graph for our PCLSA
model.
The experimental result is shown in Table 5, in
which we try to extract 8 topics from the cross-
lingual corpus. The first ten rows show the re-
sult of our PCLSA model, in which we set a very
small value to the weight parameter ? for the reg-
ularizer part. This can be used as an approxima-
tion of the result from the traditional PLSA model
on this three language corpus. We can see that
the extracted topics are mainly written in mono-
language. As we set the value of parameter ?
larger, the extracted topics become multi-lingual,
which is shown in the next ten rows. From this
result, we can see the difference between the re-
views of different brands about the similar topic.
In addition, if we set the ? even larger, we will
get topics that are mostly made of the same words
from the three different brands, which means the
extracted topics are very smooth on the dictionary
graph now.
7 Conclusion
In this paper, we study the problem of cross-
lingual latent topic extraction where the task is to
extract a set of common latent topics from multi-
lingual text data. We propose a novel probabilistic
topic model (i.e. the Probabilistic Cross-Lingual
Latent Semantic Analysis (PCLSA) model) that
can incorporate translation knowledge in bilingual
dictionaries as a regularizer to constrain the pa-
rameter estimation so that the learned topic models
would be synchronized in multiple languages. We
evaluated the model using several data sets. The
experimental results show that PCLSA is effec-
tive in extracting common latent topics from mul-
tilingual text data, and it outperforms the baseline
method which uses the standard PLSA to fit each
monolingual text data set.
Our work opens up some interesting future re-
search directions to further explore. First, in
this paper, we have only experimented with uni-
form weighting of edge in the bilingual graph.
It should be very interesting to explore how to
assign weights to the edges and study whether
weighted graphs can further improve performance.
Second, it would also be interesting to further
extend PCLSA to accommodate discovering top-
ics in each language that aren?t well-aligned with
other languages.
8 Acknowledgments
We sincerely thank the anonymous reviewers for
their comprehensive and constructive comments.
The work was supported in part by NASA grant
1135
NNX08AC35A, by the National Science Foun-
dation under Grant Numbers IIS-0713581, IIS-
0713571, and CNS-0834709, and by a Sloan Re-
search Fellowship.
References
David Blei and John Lafferty. 2005. Correlated topic
models. In NIPS ?05: Advances in Neural Informa-
tion Processing Systems 18.
David M. Blei and John D. Lafferty. 2006. Dynamic
topic models. In Proceedings of the 23rd interna-
tional conference on Machine learning, pages 113?
120.
D. Blei, T. Griffiths, M. Jordan, and J. Tenenbaum.
2003a. Hierarchical topic models and the nested
chinese restaurant process. In Neural Information
Processing Systems (NIPS) 16.
D. Blei, A. Ng, and M. Jordan. 2003b. Latent Dirichlet
allocation. Journal of Machine Learning Research,
3:993?1022.
J. Boyd-Graber and D. Blei. 2009. Multilingual topic
models for unaligned text. In Uncertainty in Artifi-
cial Intelligence.
S. R. K. Branavan, Harr Chen, Jacob Eisenstein, and
Regina Barzilay. 2008. Learning document-level
semantic properties from free-text annotations. In
Proceedings of ACL 2008.
Martin Franz, J. Scott McCarley, and Salim Roukos.
1998. Ad hoc and multilingual information retrieval
at IBM. In Text REtrieval Conference, pages 104?
115.
Pascale Fung. 1995. A pattern matching method
for finding noun and proper noun translations from
noisy parallel corpora. In Proceedings of ACL 1995,
pages 236?243.
Alfio Gliozzo and Carlo Strapparava. 2006. Exploit-
ing comparable corpora and bilingual dictionaries
for cross-language text categorization. In ACL-44:
Proceedings of the 21st International Conference
on Computational Linguistics and the 44th annual
meeting of the Association for Computational Lin-
guistics, pages 553?560, Morristown, NJ, USA. As-
sociation for Computational Linguistics.
T. Hofmann. 1999a. Probabilistic latent semantic anal-
ysis. In Proceedings of UAI 1999, pages 289?296.
Thomas Hofmann. 1999b. The cluster-abstraction
model: Unsupervised learning of topic hierarchies
from text data. In IJCAI? 99, pages 682?687.
Thomas Hofmann. 2001. Unsupervised learning by
probabilistic latent semantic analysis. Mach. Learn.,
42(1-2):177?196.
Jagadeesh Jagaralamudi and Hal Daume? III. 2010. Ex-
tracting multilingual topics from unaligned corpora.
In Proceedings of the European Conference on In-
formation Retrieval (ECIR), Milton Keynes, United
Kingdom.
Woosung Kim and Sanjeev Khudanpur. 2004. Lex-
ical triggers and latent semantic analysis for cross-
lingual language model adaptation. ACM Trans-
actions on Asian Language Information Processing
(TALIP), 3(2):94?112.
Wei Li and Andrew McCallum. 2006. Pachinko allo-
cation: Dag-structured mixture models of topic cor-
relations. In ICML ?06: Proceedings of the 23rd in-
ternational conference on Machine learning, pages
577?584.
H. Masuichi, R. Flournoy, S. Kaufmann, and S. Peters.
2000. A bootstrapping method for extracting bilin-
gual text pairs. In Proc. 18th COLINC, pages 1066?
1070.
Qiaozhu Mei and ChengXiang Zhai. 2006. A mixture
model for contextual text mining. In Proceedings of
KDD ?06, pages 649?655.
Qiaozhu Mei, Xu Ling, Matthew Wondra, Hang Su,
and ChengXiang Zhai. 2007. Topic sentiment mix-
ture: Modeling facets and opinions in weblogs. In
Proceedings of WWW ?07.
Qiaozhu Mei, Deng Cai, Duo Zhang, and ChengXiang
Zhai. 2008a. Topic modeling with network regular-
ization. In WWW, pages 101?110.
Qiaozhu Mei, Duo Zhang, and ChengXiang Zhai.
2008b. A general optimization framework for
smoothing language models on graph structures. In
SIGIR ?08: Proceedings of the 31st annual interna-
tional ACM SIGIR conference on Research and de-
velopment in information retrieval, pages 611?618,
New York, NY, USA. ACM.
David Mimno, Hanna M. Wallach, Jason Naradowsky,
David A. Smith, and Andrew Mccallum. 2009.
Polylingual topic models. In Proceedings of the
2009 Conference on Empirical Methods in Natural
Language Processing, pages 880?889, Singapore,
August. Association for Computational Linguistics.
Xiaochuan Ni, Jian-Tao Sun, Jian Hu, and Zheng Chen.
2009. Mining multilingual topics from wikipedia.
In WWW ?09: Proceedings of the 18th international
conference on World wide web, pages 1155?1156,
New York, NY, USA. ACM.
F. Sadat, M. Yoshikawa, and S. Uemura. 2003. Bilin-
gual terminology acquisition from comparable cor-
pora and phrasal translation to cross-language infor-
mation retrieval. In ACL ?03: Proceedings of the
41st Annual Meeting on Association for Computa-
tional Linguistics, pages 141?144.
1136
Mark Steyvers, Padhraic Smyth, Michal Rosen-Zvi,
and Thomas Griffiths. 2004. Probabilistic author-
topic models for information discovery. In Proceed-
ings of KDD?04, pages 306?315.
Xuanhui Wang, ChengXiang Zhai, Xiao Hu, and
Richard Sproat. 2007. Mining correlated bursty
topic patterns from coordinated text streams. In
KDD ?07: Proceedings of the 13th ACM SIGKDD
international conference on Knowledge discovery
and data mining, pages 784?793, New York, NY,
USA. ACM.
Bing Zhao and Eric P. Xing. 2006. Bitam: Bilingual
topic admixture models for word alignment. In In
Proceedings of the 44th Annual Meeting of the As-
sociation for Computational Linguistics.
1137
Proceedings of the TextGraphs-6 Workshop, pages 42?50,
Portland, Oregon, USA, 19-24 June 2011. c?2011 Association for Computational Linguistics
Simultaneous Similarity Learning and Feature-Weight Learning for
Document Clustering
Pradeep Muthukrishnan
Dept of CSE,
University of Michigan
mpradeep@umich.edu
Dragomir Radev
School of Information,
Dept of CSE,
University of Michigan
radev@umich.edu
Qiaozhu Mei
School of Information,
Dept of CSE,
University of Michigan
qmei@umich.edu
Abstract
A key problem in document classification and
clustering is learning the similarity between
documents. Traditional approaches include
estimating similarity between feature vectors
of documents where the vectors are computed
using TF-IDF in the bag-of-words model.
However, these approaches do not work well
when either similar documents do not use the
same vocabulary or the feature vectors are not
estimated correctly.
In this paper, we represent documents and
keywords using multiple layers of connected
graphs. We pose the problem of simultane-
ously learning similarity between documents
and keyword weights as an edge-weight regu-
larization problem over the different layers of
graphs. Unlike most feature weight learning
algorithms, we propose an unsupervised algo-
rithm in the proposed framework to simulta-
neously optimize similarity and the keyword
weights. We extrinsically evaluate the perfor-
mance of the proposed similarity measure on
two different tasks, clustering and classifica-
tion. The proposed similarity measure out-
performs the similarity measure proposed by
(Muthukrishnan et al, 2010), a state-of-the-
art classification algorithm (Zhou and Burges,
2007) and three different baselines on a vari-
ety of standard, large data sets.
1 Introduction
The recent upsurge in the amount of text available
due to the widespread growth of the Internet has led
to the need for large scale, efficient Machine Learn-
ing (ML), Information Retrieval (IR) tools for text
mining. At the heart of many of the ML, IR algo-
rithms is the need for a good similarity measure be-
tween documents. For example, a better similarity
measure almost always leads to better performance
in tasks like document classification, clustering, etc.
Traditional approaches represent documents with
many keywords using a simple feature vector de-
scription. Then, similarity between two documents
is estimated using the dot product between their
corresponding vectors. However, such similarity
measures do not use all the keywords together and
hence, suffer from problems due to sparsity. There
are two major issues in computing similarity be-
tween documents
? Similar documents may not use the same vo-
cabulary.
? Estimating feature weights or weight of a key-
word to the document it is contained in.
For example, consider two publications, X and
Y , in the field of Machine Learning. Let X be a
paper on clustering while Y is on classification. Al-
though the two publications use very different vo-
cabulary, they are semantically similar. Keyword
weights are mostly estimated using the frequency of
the keyword in the document. For example, TF-IDF
based scoring is the most commonly used approach
to compute keyword weights to compute similarity
between documents. However, suppose publications
X and Y mention the keyword ??Machine Learn-
ing?? only once. Although, they are mentioned only
once in the text of the document, for the purposes
of computing semantic similarity between the docu-
42
ments, it would be beneficial to give it a high key-
word weight.
A commonly used approach to estimate seman-
tic similarity between documents is to use an ex-
ternal knowledge source like WordNet (Pedersen
et al, 2004). However, these approaches are do-
main dependent and language dependent. If docu-
ment similarity can not be estimated accurately us-
ing just the text, there have been approaches incor-
porating multiple sources of similarity like link sim-
ilarity, authorship similarity between publications
(Bach et al, 2004; Cortes et al, 2009). (Muthukr-
ishnan et al, 2010) also uses multiple sources of
similarity. In addition to improving similarity es-
timates between documents, it also improves sim-
ilarity estimates between keywords. Co-clustering
(Dhillon et al, 2003) based approaches are used
to alleviate problems due to the sparsity and high-
dimensionality of the data. In co-clustering, the key-
words and the documents are simultaneously clus-
tered by exploiting the duality between them. How-
ever, the approach relies solely on the keyword dis-
tributions to cluster the documents and vice-versa.
It does not take into account the inherent similar-
ity between the keywords (documents) when cluster-
ing the documents (keywords). Also, co-clustering
takes as input the weight of all keywords to corre-
sponding documents.
2 Motivation
First, we explain how similarity learning and fea-
ture weight learning can mutually benefit from each
other using an example. For example, consider the
following three publications in the field of Machine
Translation, (Brown et al, 1990; Gale and Church,
1991; Marcu and Wong, 2002)
Clearly, all the papers belong to the field of Ma-
chine Translation but (Gale and Church, 1991) con-
tains the phrase ??Machine Translation?? only once
in the entire text. However, we can learn to attribute
some similarity between (Brown et al, 1990) and
the second publication using the text in (Marcu and
Wong, 2002). The keywords ??Bilingual Corpora??
and ??Machine Translation?? co-occur in the text in
(Marcu andWong, 2002) which makes the keywords
themselves similar. Now we can attribute some sim-
ilarity between the (Brown et al, 1990) and (Marcu
andWong, 2002) publication since they contain sim-
ilar keywords. This shows how similarity learning
can benefit from important keywords.
Now, assume that ??Machine Translation?? is an
important keyword (high keyword weight) for the
first and third publication while ??Bilingual Cor-
pora?? is an important keyword for the second pub-
lication. We explained how to infer similarity be-
tween the first and second publication using the third
publication as a bridge. Using the newly learned
similarity measure, we can infer that ??Bilingual
Corpora?? is an important keyword for the sec-
ond publication since a similar keyword (??Machine
Translation??) is an important keyword for similar
publications.
Let documents Di and Dj contain keywords Kik
and Kjl. Then intuitively, the similarity between
two documents should be jointly proportional to
? The similarity between keywords Kik and Kjl
? The weights of Kik to Di and Kjl to Dj .
Similarly the weight of a keyword Kik to docu-
ment Di should be jointly proportional to
? The similarity between documents Di and Dj .
? The similarity between keyphrases Kik and
Kjl and weight of Kjl to Dj .
The major contributions of the paper are given be-
low,
? A rich representation model for representing
documents with associated keywords for effi-
ciently estimating document similarity..
? A regularization framework for joint feature
weight (keyword weight) learning and similar-
ity learning.
? An unsupervised algorithm in the proposed
framework to efficiently learn similarity be-
tween documents and the weights of keywords
for each document in a set of documents.
In the next two sections, we formalize and ex-
ploit this observation to jointly optimize similarity
between documents and weight of keywords to doc-
uments in a principled way.
43
3 Problem Formulation
We assume that a set of keywords have been ex-
tracted for the set of documents to be analyzed. The
setup is very general: Documents are represented
by the set of candidate keywords. In addition to
that, we have crude initial similarities estimated
between documents and also between keywords
and the weights of keywords to documents. The
similarities and keyword weights are represented
using two layers of graphs. We formally define the
necessary concepts,
Definition 1: Documents and corresponding
keywords
We have a set of N documents D =
{d1, d2, . . . , dN}. Each document, di has a set
of mi keywords Ki = {ki1, ki2, . . . , kimi}
Definition 2: Document Similarity Graph
The document similarity graph, G1 = (V1, E1),
consists of the set of documents as nodes and the
edge weights represent the initial similarity between
the documents.
Definition 3: Keyword Similarity Graph
The keyword similarity graph, G2 = (V2, E2), con-
sists of the set of keywords as nodes and the edge
weights represent the initial similarity between the
keywords.
The document similarity graph and the keyword
similarity graph can be considered as two layers of
graphs which are connected by the function defined
below
Definition 4: keyword Weights (KW)
There exists an edge between di and kij for 1 ? j ?
mi. Let Z represent the keyword weighting func-
tion, i.e, Zdi,kij represents the weight of keyword
kij t document di.
4 Regularization Framework
?(w,Z) = ?0 ? ISC(w,w?) + ?1 ? IKC(Z,Z?)
+?2 ?KS(w,Z) + ?3 ? SK(Z,w) (1)
where ?0 + ?1 + ?2 + ?3 = 1.
ISC refers to Initial Similarity Criterion and IKC
refers to Initial Keyword weight Criterion. They are
defined as follows
ISC(w,w?) =
?
u,v?G1
(wu,v ? w?u,v)
2 (2)
IKC(Z,Z?) =
?
u?G1,v?G2
(Zu,v ? Z?u,v)
2 (3)
KS refers toKeyword based Similarity and SK refers
to Similarity induced Keyword weight. They are de-
fined as follows
KS(w,Z) =
?
u1,v1?G1
?
u2,v2?G2
Zu1,u2Zv1,v2
(wu1,v1 ? wu2,v2)
2 (4)
and
SK(w,Z) =
?
u1,v1?G1
?
u2,v2?G2
wu1,v1wu2,v2
(Zu1,u2 ? Zv1,v2)
2 (5)
Then the task is to minimize the objective function.
The objective function consists of four parts. The
first and second parts are initial similarity criterion
and initial keyword criterion. They ensure that the
optimized edge weights are close to the initial edge
weights. The weights ?0 and ?1 ensure that the op-
timized weights are close to the initial weights, in
other words, they represent the confidence level in
initial weights.
The significance of the third and the fourth parts
of the objective function are best explained by a sim-
ple example. Consider two graphs, G1 and G2. Let
G1 be the graph containing publications as nodes
and edge weights representing initial similarity val-
ues. Let G2 be the graph corresponding to keywords
and edge weights represent similarity between key-
words. There is an edge from a node u1 in G1 to a
node v1 in G2 if the publication corresponding to u1
contains the keyword corresponding to v1.
According to this example, minimizing the key-
word weight induced similarity part corresponds to
updating similarity values between keywords in pro-
portion to weights of the keywords to the respective
documents they are contained in and the similarity
between the documents. keyword weight induced
similarity part also helps updating similarity values
44
between documents in proportion to weights of key-
words they contain and the similarity between the
contained keywords.
Minimizing the similarity induced keyword part
corresponds to updating keyword weights in propor-
tion to the following
? Similarity between v1 and other keywords v2 ?
G2
? Keyword weight of v2 to documents u2 ? G1
? Similarity between u1 and u2
Therefore, even if frequency of a keyword such
as ??Machine Translation?? in a publication is not
high, it can achieve a high keyword weight if it con-
tains many other similar keywords such as ??Bilin-
gual Corpora?? and ??Word alignment??.
5 Efficient Algorithm
We seek to minimize the objective function using
Alternating Optimization (AO) (Bezdek and Hath-
away, 2002), an approximate optimization method.
Alternating optimization is an iterative procedure for
minimizing (or maximizing) the function f(x) =
f(X1, X2, . . . , Xt) jointly over all variables by al-
ternating restricted minimizations over the individ-
ual subsets of variables X1, . . . , Xt.
In this optimization method, we partition the set
of variables into a set of mutually exclusive, exhaus-
tive subsets. We iteratively perform minimizations
over each subset of variables while maintaining the
other subsets of variables fixed. Formally, let the set
of real-valued variables be X = {X1, X2, . . . , XN}
be partitioned into m subsets, {Y1, Y2, . . . , Ym}.
Also, let si = |Yi|. Then we begin with the ini-
tial set of values {Y 01 , Y20, . . . , Ym0} and make re-
stricted minimizations of the following form,
min
Yi?Rsi
{f(Y1r+1, . . . , Yi?1r+1, Yi, Yi+1r, . . . , Ymr)}
(6)
where i = 1, 2, . . . ,m. The underline notation Yj
indicates that the subset of variables Yj are fixed
with respect to Yi. In the context of our prob-
lem, we update each edge weight while maintaining
other edge weights to be a constant. Then the prob-
lem boils down to the minimization problem over a
single edge weight. For example, let us solve the
minimization problem for edge weight correspond-
ing to (ui, vj) where ui, vj ? G1 (The case where
ui, vj ? G2 is analogous). Clearly the objective
function is a convex function in w(ui, vj). The par-
tial derivative of the objective function with respect
to the edge weight is given below,
??(w,Z)
?wui,vj
= 2?0(wui,vj ? w?ui,vj )
+2?2 ?
?
u2,v2?G2
(wui,vj ? wu2,v2)Zu1,u2Zvj ,v2
+?3 ?
?
u2,v2?G2
(Zui,u2 ? Zvj ,v2)
2wui,vjwu2,v2
. (7)
To minimize the above function, we set the partial
derivative to zero which gives us the following ex-
pression,
wuj ,vk =
1
C1
(?0w?ui,vj +
?2
?
u2,v2?G2
Zui,u2 wu2,v2 Zvj ,v2)(8)
where
C1 = ?0 + ?2
?
u2,v2?G2
Zui,u2 Zvj ,v2
+?3
2
?
u2,v2?G2
(Zui,u2 ? Zvj ,v2)
2wu2,v2
Similarly, we can derive the update equation for
keyword weights, Zui,uj as below,
Zui,uj =
1
C2
(?1Z?ui,uj +
?3
?
v1?G1
?
v2?G2
wui,v1 wuj ,v2 Zv1,v2)
(9)
where,
C2 = ?1 + ?3
?
v1?G1
?
v2?G2
wui,v1 wuj ,v2
+?2
2
?
v1?G1
?
v2?G2
(wui,v1 ? wuj ,v2)
2Zv1,v2
45
The similarity score between two nodes is propor-
tional to the similarity between nodes in the other
layer. For example, the similarity between two doc-
uments (keywords) is proportional to the similarity
between the keywords the documents they contain
(the documents they are contained in). C plays the
role of a normalization constant. Therefore, for sim-
ilarity between two nodes to be high, it is required
that they not only contain a lot of similar nodes in
the other graph but the similar nodes need to be im-
portant to the two target nodes.
Similarly, a particular keyword will have a high
weight to a document if similar keywords have high
weights to similar documents. Also, it is neces-
sary that the similarity between the keywords and
the documents are high.
It can be shown that equations 8 and 9 converge
q? linearly since the minimization problem is con-
vex in each of the variables individually and hence
has a global and unique minimizer (Bezdek and
Hathaway, 2002).
5.1 Layered Random Walk Interpretation
The above algorithm has a very nice intuitive inter-
preation in terms of random walks over the two dif-
ferent graphs. Assume the initial weights are transi-
tion probability values after the graphs are normal-
ized so that each row of the adjacency matrices sums
to 1. Then the similarity between two nodes u and v
in the same graph is computed as sum of two parts.
The first part is ?0 times the initial similarity. This
is necessary so that the optimized similarity values
are not too far away from the initial similarity val-
ues. The second part corresponds to the probability
of a random walk of length 3 starting at u and reach-
ing v through two intermediate nodes from the other
graph.
The semantics of the random walk depends on
whether u, v are documents or keywords. For exam-
ple, if the two nodes are documents, then the simi-
larity between two documents d1 and d2 is the prob-
ability of random walk starting at document d1 and
then moving to a keyword k1 and then moving to
keyword k2 and then to document d2. Note that key-
words k1 and k2 can be the same keyword which
accounts for similarity between documents because
they contain the same keyword.
Also, note that second and higher order depen-
dencies are also taken into account by this algo-
rithm. That is, two papers may become similar be-
cause they contain two keywords which are con-
nected by a path in the keyword graph, whose length
is greater than 1. This is due to the iterative nature
of the algorithm. For example, keywords ??Machine
Translation?? and ??Bilingual corpora?? occur often
together and hence any co-occurrence based simi-
larity measure will assign a high initial similarity
value. Hence two publications which contain these
words will be assigned a non-zero similarity value
after a single iteration. Also, ??Bilingual corpora??
and ??SMT?? (abbreviation for Statistical Machine
Translation) can have a high initial similarity value
which enables assiging a high similarity value be-
tween ??Machine Translation?? and ??SMT??. This
leads to a chain effect as the number of iterations in-
creases which helps assign non-zero similarity val-
ues between semantically similar documents even if
they do not contain common keywords.
6 Experiments
It is very hard to evaluate similarity measures in iso-
lation. Thus, most of the algorithms to compute sim-
ilarity scores are evaluated extrinsically, i.e, the sim-
ilarity scores are used for an external task like clus-
tering or classification and the performance in the
external task is used as the performance measure for
the similarity scores. This also helps demonstrate
the different applications of the computed similar-
ity measure. Thus, we perform a variety of differ-
ent experiments on standard data sets to illustrate
the improved performance of the proposed similar-
ity measure. There are three natural variants of the
algorithm,
? Unified: We compare against the edge-weight
regularization algorithm proposed in (Muthukr-
ishnan et al, 2010). The algorithm has the
same representation as our algorithm but the
optimization is strictly defined over the edge
weights in the two layers of the graph, wij?s
and not on the keyword weights. Therefore,
Zij are maintained constant throughout the al-
gorithm.
? Unified-binary: In this variant, we initialize the
keyword weights to 1, i.e, Zij = 1 whenever
document i contains the keyword j.
46
ACL-ID Paper Title Research Topic
W05-0812 Improved HMM Alignment Models for Languages With Scarce
Resources
Machine Translation
P07-1111 A Re-Examination of Machine Learning Approaches for Sentence-
Level MT Evaluation
Machine Translation
P03-1054 Accurate Unlexicalized Parsing Dependency Parsing
P07-1050 K-Best Spanning Tree Parsing Dependency Parsing
P88-1020 Planning Coherent Multi-Sentential Text Summarization
Table 1: Details of a few sample papers classified according to research topic
? Unified-TFIDF: We initialize the keyword
weights to the TFIDF scores, Zij is set to the
TFIDF score of keyword j for document i.
Experiment Set I: We compare our similarity mea-
sure against other similarity measures in the context
of classification. We also compare against a state
of the art classification algorithm which uses differ-
ent similarity measures due to different feature types
without integrating them into one single similarity
measure. Specifically, we compare our algorithm
against three other similarity baselines in the context
of classification which are listed below.
? Content Similarity: Similarity is computed us-
ing just the feature vector representation using
just the text. We use cosine similarity after pre-
processing each document into a tf.idf vector
for the AAN data set. For all other data sets,
we use the cosine similarity on the binary fea-
ture vector representation that is available.
? Link Similarity: Similarity is computed using
only the links (citations, in the case of publica-
tions). To compute link similarity, we use the
node similarity algorithm proposed by (Harel
and Koren, 2001) using a random walk of
length 3 on the link graph.
? Linear combination: The content similarity
(CS) and link similarity (LS) between docu-
ments x and y are combined in a linear fashion
as ?CS(x, y)+(1??)LS(x, y). We tried dif-
ferent values of ? and report only the best accu-
racy that can be achieved using linear combina-
tion of similarity measures. Note that this is an
upper bound on the accuracy of Multiple Ker-
nel Learning with the restriction of the combi-
nation being affine.
We also compare our algorithm against the follow-
ing algorithms SC-MV: We compare our algorithm
against the spectral classification algorithm for data
with multiple views (Zhou and Burges, 2007). The
algorithm tries to classify data when multiple views
of the data are available. The multiple views are rep-
resented using multiple homogeneous graphs with a
common vertex set. In each graph, the edge weights
represent similarity between the nodes computed us-
ing a single feature type. For our experiments, we
used the link similarity graph and the content simi-
larity graph as described above as the two views of
the same data
We use a semi-supervised graph classification al-
gorithm (Zhu et al, 2003) to perform the classifica-
tion.
Experiment Set II: We illustrate the improved
performance of our similarity measure in the con-
text of clustering. We compare our similarity mea-
sure against the three similarity baselines mentioned
above. We use a spectral graph clustering algorithm
proposed in (Dhillon et al, 2007) to perform the
clustering.
We performed our experiments on three different
data sets. The three data sets are explained below.
? AAN Data: The ACL Anthology is a collec-
tion of papers from the Computational Lin-
guistics journal as well as proceedings from
ACL conferences and workshops and includes
15, 160 papers. To build the ACL Anthology
Network (AAN), (Radev et al, 2009) manu-
ally performed some preprocessing tasks in-
cluding parsing references and building the net-
work metadata, the citation, and the author col-
laboration networks. The full AAN includes
the raw text of all the papers in addition to full
citation and collaboration networks.
We chose a subset of papers in 3 topics (Ma-
47
 0.45
 0.5
 0.55
 0.6
 0.65
 0.7
 0.75
 0.8
 0.85
 0.9
 0  10  20  30  40  50  60  70
ContentLinkLinearSC-MVUnifiedUnified-binaryUnified-TFIDF
(a) AAN
 0.45
 0.5
 0.55
 0.6
 0.65
 0.7
 0.75
 0.8
 10  15  20  25  30  35  40
ContentLinkLinearSC-MVUnifiedUnified-binaryUnified-TFIDF
(b) Cornell
 0.5
 0.55
 0.6
 0.65
 0.7
 0.75
 0.8
 0.85
 10  15  20  25  30  35  40
ContentLinkLinearSC-MVUnifiedUnified-binaryUnified-TFIDF
(c) Texas
 0.5
 0.55
 0.6
 0.65
 0.7
 0.75
 0.8
 0.85
 10  15  20  25  30  35  40  45  50
ContentLinkLinearSC-MVUnifiedUnified-binaryUnified-TFIDF
(d) Washington
 0.5
 0.55
 0.6
 0.65
 0.7
 0.75
 0.8
 0.85
 10  15  20  25  30  35  40  45
ContentLinkLinearSC-MVUnifiedUnified-binaryUnified-TFIDF
(e) Wisconsin
 0.5
 0.55
 0.6
 0.65
 0.7
 0.75
 0.8
 0.85
 0.9
 0.95
 1
 50  100  150  200  250  300  350  400  450  500
ContentLinkLinearSC-MVUnifiedUnified-binaryUnified-TFIDF
(f) Cora
Figure 1: Classification Accuracy on the different data sets. The number of points labeled is plotted along
the x-axis and the y-axis shows the classification accuracy on the unlabeled data.
chine Translation, Dependency Parsing, Sum-
marization) from the ACL anthology. These
topics are three main research areas in Natural
Language Processing (NLP). Specifically, we
collected all papers which were cited by pa-
pers whose titles contain any of the following
phrases, ??Dependency Parsing??, ??Machine
Translation??, ??Summarization??. From this
list, we removed all the papers which contained
any of the above phrases in their title because
48
this would make the clustering task easy. The
pruned list contains 1190 papers. We manually
classified each paper into four classes (Depen-
dency Parsing, Machine Translation, Summa-
rization, Other) by considering the full text of
the paper. The manually cleaned data set con-
sists of 275Machine Translation papers, 73 De-
pendency Parsing papers and 32 Summariza-
tion papers. Table 1 lists a few sample papers
from each class.
WebKB(Sen et al, 2008): The data set con-
sists of a subset of the original WebKB data set.
The corpus consists of 877 web pages collected
from four different universities. Each web page
is represented by a 0/1-valued word vector with
1703 unique words after stemming and remov-
ing stopwords. All words with document fre-
quency less than 10 were removed.
Cora(Sen et al, 2008): The Cora dataset con-
sists of 2708 scientific publications classified
into one of seven classes. The citation network
consists of 5429 links. Each publication in the
dataset is described by a 0/1-valued word vec-
tor indicating the absence/presence of the cor-
responding word from the dictionary. The dic-
tionary consists of 1433 unique words.
For all the data sets, we constructed two graphs,
the kewyord feature graph and the link similarity
graph. The keyword feature layer graph, Gf =
(Vf , Ef , wf ) is a weighted graph where Vf is the
set of all features. The edge weight between key-
words fi and fj represents the similarity between
the features. The edge weights are initialized to the
cosine similarity between their corresponding doc-
ument vectors. The link similarity graph, Go =
(Vo, Eo, wo) is another weighted graph where Vo
is the set of objects. The edge weight represents
the similarity between the documents and is initial-
ized to the similarity between the documents due to
the link structure. The link similarity between two
documents is computed using the similarity mea-
sure proposed by (Harel and Koren, 2001) on the
citation graph. We also performed experiments by
initializing the similarity between documents to the
keyword similarity. Although, our algorithm still
outperforms other algorithms and the baselines (not
shown due to space restrictions), the accuracy using
citation similarity is higher.
7 Results and Discussion
Figure 1 shows the accuracy of the classification ob-
tained using different similarity measures. It can be
seen that the proposed algorithm (both the variants)
performs much better than other similarity measures
by a large margin. The algorithm performs much
better when more information is provided in the
form of TF-IDF scores. We attribute this to the
rich representation of the data. In our algorithm, the
data is represented as a set of heterogeneous graphs
(layers) which are connected together instead of the
normal feature vector representation. Thus, we can
leverage on the similarity between the keywords and
the objects (documents) to iteratively improve sim-
ilarity in both layers. Whereas, in the case of the
algorithm in (Zhou and Burges, 2007) all the graphs
are isolated homogeneous graphs. Hence there is no
information transfer across the different graphs.
For the clustering task, we use Normalized Mu-
tual Information (NMI) (Strehl and Ghosh, 2002)
between the ground truth clusters and the outputted
clustering as the measure of clustering accuracy.
Table 2 shows the Normalized Mutual Informa-
tion scores obtained by the different similarity mea-
sures on the different data sets.
8 Conclusion
In this paper, we have proposed a novel approach
to compute similarity between documents and key-
words iteratively. We formalized the problem of
similarity estimation as an optimization problem in-
duced by a regularization framework over edges in
multiple graphs. We propose an efficient, iterative
algorithm based on Alternating Optimization (AO)
which has a neat, intuitive interpretation in terms
of random walks over multiple graphs. We demon-
strated the improved performance of the proposed
algorithm over many different baselines and a state-
of-the-art classifcation algorithm and a similarity
measure which uses the same information as given
to our algorithm.
49
Similarity Measure AAN Texas Wisconsin Washington Cornell Cora
Content Similarity (Cosine) 0.66 0.34 0.42 0.59 0.63 0.48
Link Similarity 0.45 0.49 0.39 0.52 0.56 0.52
Linear Combination 0.69 0.54 0.46 0.54 0.68 0.54
Unified Similarity 0.78 0.69 0.54 0.66 0.72 0.64
Unified Similarity-Binary 0.80 0.68 0.56 0.69 0.74 0.66
Unified Similarity-TFIDF 0.84 0.70 0.60 0.72 0.78 0.70
Table 2: Normalized Mutual Information scores of the different similarity measures on the different data
sets
References
Francis R. Bach, Gert R. G. Lanckriet, and Michael I.
Jordan. 2004. Multiple kernel learning, conic duality,
and the smo algorithm. In Proceedings of the twenty-
first international conference on Machine learning,
ICML ?04, pages 6?, New York, NY, USA. ACM.
James Bezdek and Richard Hathaway. 2002. Some notes
on alternating optimization. In Nikhil Pal and Michio
Sugeno, editors, Advances in Soft Computing AFSS
2002, volume 2275 of Lecture Notes in Computer Sci-
ence, pages 187?195. Springer Berlin.
Peter F. Brown, John Cocke, Stephen A. Della Pietra,
Vincent J. Della Pietra, Fredrick Jelinek, John D. Laf-
ferty, Robert L. Mercer, and Paul S. Roossin. 1990. A
statistical approach to machine translation. Computa-
tional Linguistics.
Corinna Cortes, Mehryar. Mohri, and Afshin Ros-
tamizadeh. 2009. Learning non-linear combinations
of kernels. In In NIPS.
Inderjit S. Dhillon, Subramanyam Mallela, and Dhar-
mendra S. Modha. 2003. Information-theoretic co-
clustering. In Proceedings of the ninth ACM SIGKDD
international conference on Knowledge discovery and
data mining, KDD ?03, pages 89?98, New York, NY,
USA. ACM.
Inderjit S. Dhillon, Yuqiang Guan, and Brian Kulis.
2007. Weighted graph cuts without eigenvectors
a multilevel approach. IEEE Transactions on Pat-
tern Analysis and Machine Intelligence, 29(11):1944?
1957, November.
William A. Gale and Kenneth Ward Church. 1991. A
program for aligning sentences in bilingual corpora.
In In Proceedings of ACL.
David Harel and Yehuda Koren. 2001. On clustering us-
ing random walks. In Foundations of Software Tech-
nology and Theoretical Computer Science 2245, pages
18?41. Springer-Verlag.
Daniel Marcu andWilliamWong. 2002. A phrase-based,
joint probability model for statistical machine transla-
tion. In In Proceedings of EMNLP.
Pradeep Muthukrishnan, Dragomir Radev, and Qiaozhu
Mei. 2010. Edge weight regularization over multiple
graphs for similarity learning. In In ICDM.
Ted Pedersen, Siddharth Patwardhan, and Jason Miche-
lizzi. 2004. Wordnet::similarity: measuring the
relatedness of concepts. In Demonstration Papers
at HLT-NAACL 2004, HLT-NAACL?Demonstrations
?04, pages 38?41, Stroudsburg, PA, USA. Association
for Computational Linguistics.
Dragomir R. Radev, Pradeep Muthukrishnan, and Vahed
Qazvinian. 2009. The ACL Anthology Network cor-
pus. In In Proceedings of the ACL Workshop on Nat-
ural Language Processing and Information Retrieval
for Digital Libraries.
Prithviraj Sen, Galileo Mark Namata, Mustafa Bilgic,
Lise Getoor, Brian Gallagher, and Tina Eliassi-Rad.
2008. Collective classification in network data. AI
Magazine, 29(3):93?106.
Alexander Strehl and Joydeep Ghosh. 2002. Cluster en-
sembles: a knowledge reuse framework for combining
partitionings. In Eighteenth national conference on
Artificial intelligence, pages 93?98, Menlo Park, CA,
USA. American Association for Artificial Intelligence.
Dengyong Zhou and Christopher J. C. Burges. 2007.
Spectral clustering and transductive learning with mul-
tiple views. In ICML ?07, pages 1159?1166, New
York, NY, USA.
Xiaojin Zhu, Zoubin Ghahramani, and John Lafferty.
2003. Semi-supervised learning using gaussian fields
and harmonic functions. In ICML 2003, pages 912?
919.
50
