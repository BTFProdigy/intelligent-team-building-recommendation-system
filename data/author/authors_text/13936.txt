Coling 2010: Poster Volume, pages 1095?1103,
Beijing, August 2010
?Expresses-an-opinion-about?: using corpus statistics in an information
extraction approach to opinion mining
Asad B. Sayeed, Hieu C. Nguyen,
and Timothy J. Meyer
Department of Computer Science
University of Maryland, College Park
asayeed@cs.umd.edu,
hcnguyen88@gmail.com,
tmeyer1@umd.edu
Amy Weinberg
Institute for Advanced Computer Studies
Department of Linguistics
University of Maryland, College Park
weinberg@umiacs.umd.edu
Abstract
We present a technique for identifying the
sources and targets of opinions without
actually identifying the opinions them-
selves. We are able to use an informa-
tion extraction approach that treats opin-
ion mining as relation mining; we iden-
tify instances of a binary ?expresses-an-
opinion-about? relation. We find that
we can classify source-target pairs as be-
longing to the relation at a performance
level significantly higher than two relevant
baselines.
This technique is particularly suited to
emerging approaches in corpus-based so-
cial science which focus on aggregating
interactions between sources to determine
their effects on socio-economically sig-
nificant targets. Our application is the
analysis of information technology (IT)
innovations. This is an example of a
more general problem where opinion is
expressed using either sub- or supersets
of expressive words found in newswire.
We present an annotation scheme and an
SVM-based technique that uses the lo-
cal context as well as the corpus-wide
frequency of a source-target pair as data
to determine membership in ?expresses-
an-opinion-about?. While the presence
of conventional subjectivity keywords ap-
pears significant in the success of this
technique, we are able to find the most
domain-relevant keywords without sacri-
ficing recall.
1 Introduction
Two problems in sentiment analysis consist of
source attribution and target discovery?who has
an opinion, and about what? These problems are
usually presented in terms of techniques that re-
late them to the actual opinion expressed. We have
a social science application in which the identifi-
cation of sources and targets over a large volume
of text is more important than identifying the ac-
tual opinions particularly in experimenting with
social science models of opinion trends. Con-
sequently, we are able to use lightweight tech-
niques to identify sources and targets without us-
ing resource-intensive techniques to identify opin-
ionated phrases.
Our application for this work is the discovery
of networks of influence among opinion leaders
in the IT field. We are interested in answering
questions about who the leaders in the field are
and how their opinion matches the social and eco-
nomic success of IT innovation. Consequently,
it became necessary for us to construct a system
(figure 1) that finds the expressions in text that re-
fer to an opinion leader?s activities in promoting
or deprecating a technology.
In this paper, we demonstrate an information
extraction (Mooney and Bunescu, 2005) approach
based in relation mining (Girju et al, 2007) that
is effective for this purpose. We describe a tech-
nique by which corpus statistics allow us to clas-
sify pairs of entities and sentiment analysis targets
as instances of an ?expresses-an-opinion-about?
relation in documents in the IT business press.
This genre has the characteristic that many enti-
ties and targets are represented within individual
sentences and paragraphs. Features based on the
1095
Figure 1: Opinion relation classification system.
frequency counts of query results allow us to train
classifiers that allow us to extract ?expresses-an-
opinion-about? instances, using a very simple an-
notation strategy to acquire training examples.
In the IT business press, the opinionated lan-
guage is different from the newswire text for
which many extant sentiment tools were devel-
oped. We use an existing sentiment lexicon along-
side other non-sentiment-specific measures that
adapt resources from newswire-developed senti-
ment analysis projects without imposing the full
complexity of those techniques.
1.1 Corpus-based social science
The ?expresses-an-opinion-about? relation is a bi-
nary relation between opinion sources and tar-
gets. Sources include both people?typically
known experts, corporate representatives, and
other businesspeople?as well as organizations
such as corporations and government bodies. The
targets are the innovation terms. Therefore, the
use of named-entity recognition in this project
only focuses on persons and organizations, as the
targets are a fixed list.
1.2 Reifying opinion in an application
context
A hypothesis implicit in our social science task
is that opinion leaders create trends in IT innova-
tion adoption partly by the text that their activi-
ties generate in the IT business press. This text
has an effect on readers, and these readers act in
such a way that in turn may generate more or less
prominence for a given innovation?and may also
generate further text.
Some of these text-generating activities include
expressions of private states in an opinion source
(e.g., ?I believe that Web 2.0 is the future?). These
kinds of expressions suggest a particular ontol-
ogy of opinion analysis involving discourse re-
lations across various types of clauses (Wilson
and Wiebe, 2005; Wilson et al, 2005a). How-
ever, if we are to track the relative adoption of
IT innovations, we must take into account the
effect of the text on the reader?s opinion about
these innovations?there are expressions other
than those of private states that have an effect on
the reader. These can be considered to be ?opin-
ionated acts1.?
Opinionated acts can include things like pur-
chasing and adoption decisions by organizations.
For example:
And like other top suppliers to Wal-
Mart Stores Inc., BP has been in-
volved in a mandate to affix radio
frequency identification tags with em-
bedded electronic product codes to its
crates and pallets. (ComputerWorld,
January 2005)
In this case, both Wal-Mart and BP have expressed
implicit approval for radio frequency identifica-
tion by adopting it. This may affect the reader?s
own likelihood of support or adoption of the tech-
nology. In this context, we do not directly con-
sider the subjectivity of the opinion source, even
though that may be present.
Opinionated acts include things like implica-
tions of technology use, not just adoption. We
thus define opinion expressions as follows: any
expression involving some actor that is likely to
affect a reader?s own potential to adopt, reject, or
speak positively or negatively of a target. This
would include ?conventional? expressions of pri-
vate states as well as opinionated acts.
Our definition of ?expresses-an-opinion-about?
follows immediately. SourceA expresses an opin-
ion about target B if an interested third party C?s
actions towards B may be affected by A?s textu-
ally recorded actions, in a context where actions
1Somasundaran and Wiebe (2009) mention a related cate-
gory of ?pragmatic opinions? that involve world knowledge.
1096
have positive or negative weight (e.g. purchasing,
promotion, etc.).
1.3 Domain-specific sentiment detection
We construct a system that uses named-entity
recognition and supervised machine learning via
SVMs to automatically discover instances of
?expresses-an-opinion-about? as a binary relation
at reasonably high accuracy and precision.
The advantage of our approach is that, outside
of HMM-based named-entity detection (BBN?s
IdentiFinder), we evade the need for resource-
intensive techniques such as sophsticated gram-
matical models, sequence models, and semantic
role labelling (Choi et al, 2006; Kim and Hovy,
2006) by removing the focus on the actual opinion
expressed. Then we can use a simple supervised
discriminative technique with a joint model of lo-
cal term frequency information and corpus-wide
co-occurrence distributions in order to discover
the raw data for opinion trend modelling. The
most complex instrument we use from sentiment
analysis research on conventional newswire is a
sentiment keyword lexicon (Wilson et al, 2005b);
furthermore, our techniques allow us to distin-
guish sentiment keywords that indicate opinion in
this domain from keywords that actually indicate
that there is no opinion relation between source
and target.
While we show that this lightweight technique
works well at a paragraph level, it can also be used
in conjunction with more resource-intensive tech-
niques used to find ?conventional? opinion ex-
pressions. Also, the use of topic aspects (Soma-
sundaran and Wiebe, 2009) in conjunction with
target names has been associated with an improve-
ment in recall. However, our technique still per-
forms well above the baseline without these im-
provements.
2 Methodology
2.1 Article preparation
We have a list of IT innovations on which our
opinion leader research effort is most closely fo-
cused. This list contains common names that re-
fer to these technologies as well as some alternate
names and abbreviations. We selected articles at
random from the ComputerWorld IT journal that
contained mentions of members of the given list.
These direct mentions were tagged in the docu-
ment as XML entities.
Each article was processed by BBN?s Identi-
Finder 3.3 (Bikel et al, 1999), a named entity
recognition (NER) system that tags named men-
tions of person and organization entities2.
The articles were then divided into paragraphs.
For each paragraph, we generated candidate rela-
tions from the entities and innovations mentioned
therein. To generate candidates, we paired every
entity in the paragraph with every innovation. Re-
dundant pairs are sometimes generated when an
entity is mentioned in multiple ways in the para-
graph. We eliminated most of these by removing
entities whose mentions were substrings of other
mentions. For example, ?Microsoft? and ?Mi-
crosoft Corp.? are sometimes found in the same
paragraph; we eliminate ?Microsoft.?
2.2 Annotation
We processed 20 documents containing 157 rela-
tions in the manner described in the previous sec-
tion. Then two domain experts (chosen from the
authors) annotated every candidate pair in every
document according to the following scheme (il-
lustrated in figure 2):
? If the paragraph associated with the candi-
date pair describes a valid source-target rela-
tion, the experts annotated it with Y.
? If the paragraph does not actually contain
that source-target relation, the experts anno-
tated it with N.
? If either the source or the target is misidenti-
fied (e.g., errors in named entity recognition),
the experts annotated it with X.
The Cohen?s ? score was 0.6 for two annotators.
While this appears to be only moderate agree-
ment, we are still able to achieve good perfor-
mance in our experiments with this value.
2In a separate research effort, we found that IdentiFinder
has a high error rate on IT business press documents, so we
built a system to reduce the error post hoc. We ran this sys-
tem over the IdentiFinder annotations.
1097
Davis says she has especially enjoyed work-
ing with the PowerPad?s bluetooth interfaces to
phones and printers. ?It?s nice getting into new
wireless technology,? she says. The bluetooth
capability will allow couriers to transmit data
without docking their devices in their trucks.
Source Target Class
Davis bluetooth Y/N/X
PowerPad bluetooth Y/N/X
Figure 2: Example paragraph annotation exercise.
We then selected 75 different documents for
each annotator and processed and annotated them
as above. At this point we have the instances and
the classes to which they belong. We labelled 466
instances of Y, 325 instances of N, and 280 in-
stances of X, for a total of 1071 relations.
2.3 Feature vector generation
We have four classes of features for every rela-
tion instance. Each type of feature consists of
counts extracted from an index of 77,227 Comput-
erWorld articles from January 1988 to June 2008
generated by the University of Massachusetts
search engine Indri (Metzler and Croft, 2004).
Each vector is normalized to the unit vector. The
index is not stemmed for performance reasons.
The first type of feature consists of simple doc-
ument frequency statistics for source-target pairs
throughout the corpus. The second type consists
of document frequency counts of source-target
pairs when they are in particularly close proxim-
ity to one another. The third type consists of docu-
ment frequency counts of source target pairs prox-
imate to keywords that reflect subjectivity. The
fourth and final type consist of TFIDF scores of
vocabulary items in the paragraph containing the
putative opinion-holding relation (unigram con-
text features). We use the first three features types
to represent the likelihood in the ?world? that the
source has an opinion about the target and the last
feature type to represent the likelihood of the spe-
cific paragraph containing an opinion that reflects
the source-target relation.
We have a total of 7450 features. Each vec-
tor is represented as a sparse array. 806 features
represent queries on the Indri index. For all the
features, we therefore have 863,226 index queries.
We perform the queries in parallel on 25 proces-
sors to generate the full feature array, which takes
approximately an hour on processors running at
8Ghz. We eliminate all values that are smaller in
magnitude than 0.000001 after unit vector normal-
ization.
2.3.1 Frequency statistics
There are two simple frequency statistics fea-
tures generated from Indri queries. The first is
the raw frequency counts of within-document co-
occurrences of the source and target in the rela-
tion. The second is the mean co-occurrence fre-
quency of the source and target per Computer-
World document.
2.3.2 Proximity counts
For every relation, we query Indri to check how
often the source and the target appear in the same
document in the ComputerWorld corpus within
four word ranges: 5, 25, 100, and 500. That is
to say, if a source and a target appear within five
words of one another, this is included in the five-
word proximity feature. This generates four fea-
tures per relation.
2.3.3 Subjectivity keyword proximity counts
We augment the proximity counts feature with
a third requirement: that the source and target ap-
pear within one of the ranges with a ?subjectivity
keyword.? The keywords are taken from Univer-
sity of Pittsburgh subjectivity lexicon; the utility
of this lexicon is supported in recent work (Soma-
sundaran and Wiebe, 2009).
For performance reasons, we did not use all of
the entries in the subjectivity lexicon. Instead,
we used a TFIDF-based measure to rank the key-
words by their prevalence in the ComputerWorld
corpus where the term frequency is defined over
the entire corpus. Then we selected 200 keywords
with the highest score.
For each keyword, we use the same proximity
ranges (5, 25, 100, and 500) in queries to Indri
where we obtain counts of each keyword-source-
target triple for each range. There are threfore 800
subjectivity keyword features.
1098
Positive class Negative class System Prec / Rec / F Accuracy
Y N Random baseline 0.60 / 0.53 / 0.56 0.52
Y N Maj.-class (Y) baseline 0.59 / 1.00 / 0.74 0.59
Y N Linear kernel 0.70 / 0.73 / 0.72 0.66
Y N RBF kernel 0.72 / 0.76 / 0.75 0.69
Y N/X Random baseline 0.44 / 0.50 / 0.47 0.50
Y N/X RBF kernel 0.65 / 0.55 / 0.59 0.67
Table 1: Results with all features against majority class and random baselines. All values are mean
averages under 10-fold cross validation.
2.3.4 Word context (unigram) features
For each relation, we take term frequency
counts of the paragraph to which the relation be-
longs. We multiply them by the IDF of the term
across the ComputerWorld corpus. This yields
6644 features over all paragraphs.
2.4 Machine learning
On these feature vectors, we trained SVM models
using Joachims? (1999) svmlight tool. We use a
radial basis function kernel with an error cost pa-
rameter of 100 and a ? of 0.25. We also use a lin-
ear kernel with an error cost parameter of 100 be-
cause it is straightforwardly possible with a linear
kernel to extract the top features from the model
generated by svmlight.
3 Experiments
We conducted most of our experiments with only
the Y and N classes, discarding all X; this re-
stricted most of our results to those assuming cor-
rect named entity recognition. Y was the posi-
tive class for training the svmlight models, and
N was the negative class. We also performed ex-
periments with N and X together being the nega-
tive class; this represents the condition that we are
seeking ?expresses-an-opinion-about? even with a
higher named-entity error rate.
We use two baselines. One is a random base-
line with uniform probability for the positive and
negative classes. The other is a majority-class as-
signer (Y is the majority class).
The best system for the Y vs. N experiment was
subjected to feature ablation. We first systemati-
cally removed each of the four feature types indi-
vidually. The feature type whose removal had the
largest effect on performance was removed per-
manently, and the rest of the features were tested
without it. This was done once more, at which
point only one feature type was present in the
models tested.
3.1 Evaluation
All evaluation was performed under 10-fold cross
validation, and we report the mean average of all
performance metrics (precision, recall, harmonic
mean F-measure, and accuracy) across folds.
We define these measures in the standard infor-
mation retrieval form. If tp represents true pos-
itives, tn true negatives, fp false positives, and
fn false negatives, then precision is tp/(tp+fp),
recall tp/(tp + fn), F-measure (harmonic mean)
is 2(prec ? rec)/(prec + rec), and accuracy is
(tp+ tn)/(tp+ fp+ fn+ tn).
4 Results and discussion
The results of the experiments with all features are
listed in table 1.
4.1 ?Perfect? named entity recognition
We achieve best results in the Y versus N case us-
ing the radial basis function kernel. We find im-
provement in F-measure and accuracy at 19% and
17% respectively. Simply assigning the majority
class to all test examples yields a very high re-
call, by definition, but poor precision and accu-
racy; hence its relatively high F-measure does not
reflect high applicability to further processing, as
the false positives would amplify errors in our so-
cial science application.
The linear kernel has results that are below the
RBF kernel for all measures, but are relatively
close to the RBF results.
1099
Subjectivity Proximity Frequency Unigram Prec / Rec / F Accuracy
X X X X 0.72 / 0.76 / 0.75 0.69
X X X 0.67 / 0.89 / 0.76 0.67
X X X 0.71 / 0.77 / 0.73 0.68
X X X 0.70 / 0.78 / 0.74 0.67
X X X 0.69 / 0.77 / 0.73 0.67
X X 0.63 / 0.91 / 0.75 0.64
X X 0.66 / 0.89 / 0.76 0.67
X X 0.65 / 0.90 / 0.76 0.66
X 0.61 / 0.92 / 0.73 0.60
X 0.61 / 0.94 / 0.74 0.60
Table 2: Feature ablation results for RBF kernel on Y vs. N case. The first line is the RBF result with
all features from table 1.
4.2 Introducing erroneous named entities
The case of Y versus N and X together unsurpris-
ingly performed worse than the case where named
entity errors were eliminated. However, relative to
its own random baseline, it performed well, with
a 12% and 17% improvement in F-measure and
accuracy using the RBF kernel. This suggests that
the errors do not introduce enough noise into the
system to produce a large decline in performance.
As X instances are about 26% of the total and
we see a considerable drop in recall, we can say
that some of the X instances are likely to be similar
to valid Y ones; indeed, examination of the named
entity recognizer?s errors suggests that some in-
correct organizations (e.g. product names) occur
in contexts where valid organizations occur. How-
ever, precision and accuracy have not fallen nearly
as far, so that the quality of the output for further
processing is not hurt in proportion to the intro-
duction of X class noise.
4.3 Feature ablation
Table 2 contains the result of our feature abla-
tion experiments. Overall, the removal of features
causes the SVM models to behave increasingly
like a majority class assigner. As we mentioned
earlier, higher recall at the expense of precision
and accuracy is not an optimal outcome for us
even if the F-measure is preserved. In our results,
the F-measure values are remarkably stable.
In the first round of feature removal, the sub-
jectivity keyword features have the biggest ef-
fect with the largest drop in precision and the
largest increase in recall; high-TFIDF words from
a general-purpose subjectivity lexicon allow the
model to assign more items to the negative class.
The next round of feature removal shows
that the proximity features have the next largest
amount of influence on the classifier, as precision
drops by 4%. The proximity features are very sim-
ilar to the subjectivity features in that they too in-
volve queries over windows of limited word sizes;
the subjectivity keyword features only differ in
that a subjectivity keyword must be within the
window as well. That the proximity features are
not more important than the subjectivity features,
implies that the subjectivity keywords matter to
the classifier, even though they are not specific to
the IT domain. However, the proximity of sources
and targets also matters, even in the absence of the
subjectivity keywords.
Finally, we are left with the frequency features
and the unigram context features. Either set of
features supports a level of performance greater
than the random baseline in table 1. However,
the unigram features allow for slightly better re-
call than the frequency features without loss of
precision, but this may not be very surprising, as
there are many more unigram features than fre-
quency features. More importantly, however, ei-
ther of these feature types is sufficient to prevent
the classifier from assigning the majority class all
of the time, although they come close.
1100
Feature type Range Keyword
Subjectivity 500 agreement
Subjectivity 500 critical
Subjectivity 500 want
Subjectivity 100 will
Subjectivity 100 able
Subjectivity 500 worth
Subjectivity 500 benefit
Subjectivity 100 trying
Subjectivity 500 large
Subjectivity 500 competitive
Table 3: The 10 most positive features via a linear
kernel in descending order.
Feature type Range Keyword
Subjectivity 500 low
Subjectivity 500 ensure
Subjectivity 25 want
Subjectivity 100 vice
Subjectivity 500 slow
Subjectivity 100 large
Subjectivity 500 ready
Subjectivity 100 actually
Subjectivity 100 ready
Subjectivity 100 against
Table 4: The 10 most negative features via a linear
kernel in descending order.
4.4 Most discriminative features
The models generated by svmlight under a lin-
ear kernel allow for the extraction of feature
weights by a script written by svmlight?s creator.
We divided the instances into a single 70%/30%
train/test split and trained a classifier with a linear
kernel and an error cost parameter of 100, with re-
sults similar to those reported under 10-fold cross-
validation in table 1. We used all features.
Then we were able to extract the 10 most pos-
itive (table 3) and 10 most negative (table 4) fea-
tures from the model.
Interestingly, all of these are subjectivity key-
word features, even the negatively weighted fea-
tures. The top positive features are often evocative
of business language, such as ?agreement?, ?crit-
ical?, and ?competitive?. Most of them emerge
from queries at the 500-word range, suggesting
that their presence in the document itself is evi-
dence that a source is expressing an opinion about
a target. That most of them are subjectivity fea-
tures is reflected in the feature ablation results in
the previous section.
It is less clear why ?ensure? and ?against?
should be evidence that a source-target pair is not
an instance of ?expresses-an-opinion-about?. On
the other hand, words like ?ready? (which appears
twice) and ?actually? can conceivably reflect sit-
uations in the IT domain that are not matters of
opinion. In either case, this demonstrates one of
the advantages of our technique, as these are fea-
tures that actively assist in classifying some rela-
tion instances as not expressing sentiment. For ex-
ample, contrary to what we would expect, ?want?
in a 25-word window with a source and a tar-
get is actually evidence against an ?expresses-an-
opinion-about? relation in text about IT innova-
tions (ComputerWorld, July 2007):
But Klein, who is director of infor-
mation services and technology, didn?t
want IT to become the blog police.
In this example, Klein is expressing a desire,
but not about the innovation (blogs) in question.
5 Conclusions and future work
5.1 Summary
We constructed and evaluated a system that de-
tects at paragraph level whether entities relevant
to the IT domain have expressed an opinion about
a list of IT innovations of interest to a larger social
science research program. To that end, we used
a combination of co-occurrence statistics gleaned
from a document indexing tool and TFIDF val-
ues from the local term context. Under these
novel conditions, we successfully exceeded sim-
ple baselines by large margins.
Despite only moderate annotator agreement, we
were able to produce results coherent enough to
successfully train classifiers and conduct experi-
ments.
Our feature ablation study suggests that all of
the feature types played a role in improving the
performance of the system over the random and
1101
majority-class baselines. However, the subjec-
tivity keyword features from an existing lexicon
played the largest role, followed by the proxim-
ity and unigram features. Subjectivity keyword
features dominated the ranks of feature weights
under a linear kernel, and the features most pre-
dictive of membership in ?expresses-an-opinion-
about? are words with semantic significance in the
context of the IT business press.
5.2 Application to other domains
We used somewhat na??ve statistics in a simple
machine learning system in order to implement a
form of opinion mining for a particular domain.
The most direct linguistic guidance we provided
our system were the query ranges and the sub-
jectivity lexicon. The generality of this approach
yields the advantage that it can be applied to other
domains where there are ways of expressing senti-
ment unique to those domains outside of newswire
text and product reviews.
5.3 Improving the features
Our use of an existing sentiment lexicon opens the
door in future work for the use of techniques to
bootstrap a larger sentiment lexicon that empha-
sizes domain-specific language in the expression
of opinion, including opinionated acts. In fact,
our results suggest that terminology in the exist-
ing lexicon that is most prominently weighted in
our classifier also tends to be domain-relevant. In
a further iteration, we might also improve perfor-
mance by using terms outside the lexicon that tend
to co-occur with terms from the lexicon.
5.4 Data generation
Our annotation exercise was a very simple one in-
volving a short reading exercise and the selection
of one of three choices per relation instance. This
type of exercise is ideally suited to the ?crowd-
sourcing? technique of paying many individuals
small amounts of money to perform these simple
annotations over the Internet. Previous research
(Snow et al, 2008) suggests that we can generate
very large datasets very quickly in this way; this
is a requirement for expanding to other domains.
5.5 Scalability
In order to classify on the order of 1000 instances,
it took nearly a million queries to the Indri index,
which took a little over an hour to do in parallel
on 25 processors by calling the Indri query engine
afresh at each query. While each query is nec-
essary to generate each feature value, there are a
number of optimizations we could implement to
accelerate the process. Various types of dynamic
programming and caching could be used to han-
dle related queries. One way of scaling up to
larger datasets would be to use the MapReduce
and cloud computing paradigms on which text
processing tools have already been implemented
(Moreira et al, 2007).
The application for this research is a social sci-
ence exercise in exploring trends in IT adoption
by analysing the IT business press. In the end, the
perfect discovery of all instances of ?expresses-
an-opinion-about? is not as important as finding
enough reliable data over a large number of docu-
ments. This work brings us several steps closer in
finding the right combination of features in order
to acquire trend-representative data.
Acknowledgements
This paper is based upon work supported by the
National Science Foundation under Grant IIS-
0729459.
References
Bikel, Daniel M., Richard Schwartz, and Ralph M.
Weischedel. 1999. An algorithm that learns what?s
in a name. Mach. Learn., 34(1-3).
Choi, Yejin, Eric Breck, and Claire Cardie. 2006.
Joint extraction of entities and relations for opinion
recognition. In Proceedings of the Conference on
Empirical Methods in Natural Language Process-
ing (EMNLP).
Girju, Roxana, Preslav Nakov, Vivi Nastase, Stan Sz-
pakowicz, Peter Turney, and Deniz Yuret. 2007.
Semeval-2007 task 04: classification of semantic re-
lations between nominals. In SemEval ?07: Pro-
ceedings of the 4th International Workshop on Se-
mantic Evaluations, pages 13?18, Morristown, NJ,
USA. Association for Computational Linguistics.
Joachims, T. 1999. Making large-scale SVM learn-
ing practical. In Scho?lkopf, B., C. Burges, and
1102
A. Smola, editors, Advances in Kernel Methods -
Support Vector Learning, chapter 11, pages 169?
184. MIT Press, Cambridge, MA.
Kim, Soo-Min and Eduard Hovy. 2006. Extracting
opinions, opinion holders, and topics expressed in
online news media text. In SST ?06: Proceedings of
the Workshop on Sentiment and Subjectivity in Text,
pages 1?8, Morristown, NJ, USA. Association for
Computational Linguistics.
Metzler, Donald and W. Bruce Croft. 2004. Combin-
ing the language model and inference network ap-
proaches to retrieval. Information Processing and
Management, 40(5):735 ? 750.
Mooney, Raymond J. and Razvan Bunescu. 2005.
Mining knowledge from text using information ex-
traction. SIGKDD Explor. Newsl., 7(1):3?10.
Moreira, Jose? E., Maged M. Michael, Dilma Da Silva,
Doron Shiloach, Parijat Dube, and Li Zhang. 2007.
Scalability of the nutch search engine. In Smith,
Burton J., editor, ICS, pages 3?12. ACM.
Rogers, Everett M. 2003. Diffusion of Innovations,
5th Edition. Free Press.
Snow, Rion, Brendan O?Connor, Daniel Jurafsky, and
Andrew Y. Ng. 2008. Cheap and fast?but is it
good?: evaluating non-expert annotations for natu-
ral language tasks. In EMNLP 2008, Morristown,
NJ, USA.
Somasundaran, Swapna and Janyce Wiebe. 2009.
Recognizing stances in online debates. In ACL-
IJCNLP ?09: Proceedings of the Joint Conference of
the 47th Annual Meeting of the ACL and the 4th In-
ternational Joint Conference on Natural Language
Processing of the AFNLP: Volume 1. Association
for Computational Linguistics.
Wilson, Theresa and Janyce Wiebe. 2005. Annotating
attributions and private states. In ACL 2005 Work-
shop: Frontiers in Corpus Annotation II: Pie in the
Sky, pages 53?60.
Wilson, Theresa, Paul Hoffmann, Swapna Somasun-
daran, Jason Kessler, Janyce Wiebe, Yejin Choi,
Claire Cardie, Ellen Riloff, and Siddharth Patward-
han. 2005a. OpinionFinder: A system for subjec-
tivity analysis. In HLT/EMNLP.
Wilson, Theresa, Janyce Wiebe, and Paul Hoffmann.
2005b. Recognizing contextual polarity in phrase-
level sentiment analysis. In HLT/EMNLP.
1103
Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL, pages 345?348,
Los Angeles, California, June 2010. c?2010 Association for Computational Linguistics
Crowdsourcing the evaluation of a domain-adapted named entity
recognition system
Asad B. Sayeed, Timothy J. Meyer,
Hieu C. Nguyen, Olivia Buzek
Department of Computer Science
University of Maryland
College Park, MD 20742
asayeed@cs.umd.edu,
tmeyer1@umd.edu,
{hcnguyen88,olivia.buzek}
@gmail.com
Amy Weinberg
Department of Linguistics
University of Maryland
College Park, MD 20742
weinberg@umiacs.umd.edu
Abstract
Named entity recognition systems sometimes
have difficulty when applied to data from do-
mains that do not closely match the training
data. We first use a simple rule-based tech-
nique for domain adaptation. Data for robust
validation of the technique is then generated,
and we use crowdsourcing techniques to show
that this strategy produces reliable results even
on data not seen by the rule designers. We
show that it is possible to extract large im-
provements on the target data rapidly at low
cost using these techniques.
1 Introduction
1.1 Named entities and errors
In this work, we use crowdsourcing to generate eval-
uation data to validate simple techniques designed to
adapt a widely-used high-performing named entity
recognition system to new domains. Specifically, we
achieve a roughly 10% improvement in precision on
text from the information technology (IT) business
press via post hoc rule-based error reduction. We
first tested the system on a small set of data that we
annotated ourselves. Then we collected data from
Amazon Mechanical Turk in order to demonstrate
that the gain is stable. To our knowledge, there is no
previous work on crowdsourcing as a rapid means
of evaluating error mitigation in named entity rec-
ognizer development.
Named entity recognition (NER) is a well-known
problem in NLP which feeds into many other re-
lated tasks such as information retrieval (IR) and
machine translation (MT) and more recently social
network discovery and opinion mining. Generally,
errors in the underlying NER technology correlate
with a steep price in performance in the NLP sys-
tems further along a processing pipeline, as incor-
rect entities propagate into incorrect translations or
erroneous graphs of social networks.
Not all errors carry the same price. In some ap-
plications, omitting a named entity has the conse-
quence of reducing the availability of training data,
but including an incorrectly identified piece of text
as as a named entity has the consequence of pro-
ducing misleading results. Our application would
be opinion mining; an omitted entity may prevent
the system from attributing an opinion to a source,
but an incorrect entity reveals non-existent opinion
sources.
Machine learning is currently used extensively in
building NER systems. One such system is BBN?s
Identifinder (Bikel et al, 1999). The IdentiFinder al-
gorithm, based on Hidden Markov Models, has been
shown to achieve F-measure scores above 90% when
the training and testing data happen to be derived
from Wall Street Journal text produced in the 1990s.
We use IdentiFinder 3.3 as a starting point for per-
formance improvement in this paper.
The use of machine learning in existing systems
requires us to produce new and costly training data
if we want to adapt these systems directly to other
domains. Our post hoc error reduction strategy is
therefore profoundly different: it relieves us of the
burden of generating complete training examples.
The data we generate are strictly corrections of the
existing system?s output. Our thus cheaper evalua-
tion is therefore primarily on improvements to pre-
345
cision, while minimizing damage to recall, unlike
an evaluation based on retraining with new, fully-
annotated text.
1.2 Crowdsourcing
Crowdsourcing is the use of the mass collabora-
tion of Internet passers-by for large enterprises on
the World Wide Web such as Wikipedia and survey
companies. However, a generalized way to mon-
etize the many small tasks that make up a larger
task is relatively new. Crowdsourcing platforms
like Amazon Mechanical Turk have allowed some
NLP researchers to acquire data for small amounts
of money from large, unspecified groups of Internet
users (Snow et al, 2008; Callison-Burch, 2009).
The use of crowdsourcing for an NLP annotation
task required careful definition of the specifics of
the task. The individuals who perform these tasks
have no specific training, and they are trying to get
through as many tasks as they can, so each task must
be specified very simply and clearly.
Part of our work was to define a named entity
error detection task simply enough that the results
would be consistent across anonymous annotators.
2 Methodology
2.1 Process overview
The overall process for running this experiment was
as follows (figure 1).
Figure 1: Diagram of data pipeline.
First, we performed an initial performance assess-
ment of IdentiFinder on our domain. We selected
200 articles from an IT trade journal. IdentiFinder
was used to tag persons and organizations in these
documents. Domain experts (in this case, the au-
thors of this paper) analyzed the entity tags pro-
duced by the NER system and annotated the erro-
neous tags. We built an error reduction system based
on our error analysis. We then ran the IdentiFinder
output through the error reduction system and eval-
uated its performance against our annotations.
Next, we constructed an Amazon Mechanical
Turk-based interface for na??ve web users or ?Turk-
ers? to annotate the IdentiFinder entities for errors.
We measured the interannotator agreement between
the Turkers and the domain experts, and we evalu-
ated the IdentiFinder output and the repaired output
against the expert-generated and Turker gold stan-
dards.
We selected a new batch of 800 articles and ran
IdentiFinder and the filters on them, and we again
ran our Mechanical Turk application on the Iden-
tiFinder output. We measured the performance of
IdentiFinder and filtered output against the Turker
annotations.
2.2 Performance evaluation
Performance is evaluated in terms of standard pre-
cision and recall of entities. If the system output
contains a person or organization labelled correctly
as such, it considers this to be a hit. If it contains a
person or organization that is mislabelled or other-
wise incorrect in the gold standard annotation, it is
a miss. We compute the F-measure as the harmonic
mean of precision and recall.
As the IdentiFinder output is the baseline, and we
ignore missed entities, by definition the baseline re-
call is 100%.
3 Experiments and results
Here we delve into further detail about the tech-
niques we used and the results that they yielded. The
results are summarized in table 1.
3.1 Baseline performance assessment
We randomly selected 200 documents from Infor-
mationWeek, a major weekly magazine in the IT
business press. Running them through IdentiFinder
produces NIST ACE-standard XML entity markup.
We focused on the ENAMEX tags of person and or-
ganization type that IdentiFinder produces.
After we annotated the ENAMEX tags for errors,
we found that closer inspection of the errors in the
IdentiFinder output allowed us to classify the major-
ity of them into three major categories:
346
Annotator Collection System Precision Recall F-measure
Authors 200 document IdentiFinder only 0.74 1 0.85
Authors 200 document Filtered 0.86 0.98 0.92
MTurk 200 document IdentiFinder only 0.69 1 0.82
MTurk 200 document Filtered 0.79 0.97 0.87
MTurk 800 document IdentiFinder only 0.67 1 0.80
MTurk 800 document Filtered 0.77 0.95 0.85
Table 1: Results of evaluation of different document sets against ground truth source by annotation technique.
? IdentiFinder tags words that are simply not
named entities.
? IdentiFinder assigns the wrong category (per-
son or organization) to an entity.
? IdentiFinder includes extraneous words in an
otherwise correct entity.
The second and third types of error are particu-
larly challenging. An example of the second type is
the following:
Yahoo is a reasonably strong competitor
to Google. It gets about half as much on-
line revenue and search traffic as Google,
. . .
Google is marked twice incorrectly as being a person
rather than an organization.
Finally, here is an example of the third error type:
A San Diego bartender reported that Bill
Gates danced the night away in his bar on
Nov. 11.
IdentiFinder incorrectly marks ?danced? as part of a
person tag.
We were able to find the precision of IdentiFinder
against our annotations: 0.74. This is poorer than the
reported performance of IdentiFinder on Wall Street
Journal text (Bikel et al, 1999).
3.2 Domain-specific error reduction
We wrote a series of rule-based filters to remove
instances of the error types?of which there were
many subtypes?described in the previous sec-
tion. For instance, the third example above was
eliminated via the use of a part-of-speech tagger;
?danced? was labelled as a verb, and entities with
tagged verbs were removed. In the second case,
the mislabelling of Google as a person rather than
an organization is identified by looking at Identi-
Finder?s majority labelling of Google throughout the
corpus?as an organization. Simple rules about cap-
italization allow instances like the first example to
be identified as errors.
This step increases the precision of the system
output to 86%, while only sacrificing a tiny amount
of recall. We see that this 10% increase is main-
tained even on the Mechanical Turk-generated an-
notations.
3.3 Mechanical Turk tasks
The basic unit of Mechanical Turk is the Human In-
telligence Task (HIT). Turkers select HITs presented
as web pages and perform the described task. Data-
collectors create HITs and pay Amazon to disburse
small amounts of money to Turkers who complete
them.
We designed our Mechanical Turk process so that
every HIT we create corresponds to an IdentiFinder-
marked document. Within its corresponding HIT,
each document is broken up into paragraphs. Fol-
lowing every paragraph is a table whose rows con-
sist of every person/organization ENAMEX discov-
ered by IdentiFinder and whose columns consist of
one of the four categories: ?Person,? ?Organization,?
?Neither,? and ?Don?t Know.? Then for each entity,
the user selects exactly one of the four options.
Each HIT is assigned to three different Turkers.
Every entity in that HIT is assigned a person or or-
ganization ENAMEX tag if two of the three Turkers
agreed it was one of those (majority vote); other-
wise, it is marked as an invalid entity.
We calculated the agreement between our annota-
tions and those developed from the Turker majority
347
vote scheme. This yields a Cohen?s ? of 0.68. We
considered this to be substantial agreement.
After processing the same 200 document set from
our own annotation, we found that the precision
of IdentiFinder was 69%, but after error reduction,
it increased to 79% with only a miniscule loss of
known valid entities (recall).
We then took another 800 documents from Infor-
mationWeek and ran them through IdentiFinder. We
did not annotate these documents ourselves, but in-
stead turned them over to Turkers. IdentiFinder out-
put alone has a 67% precision, but after error reduc-
tion, it rises to 77%, and recall is still minimally af-
fected.
4 Discussion
4.1 Benefits
It appears that high-performing NER systems ex-
hibit rather severe domain adaption problems. The
performance of IdentiFinder is quite low on the IT
business press. However, a simple rule-based sys-
tem was able to gain 10% improvement in precision
with little recall sacrificed. This is a particularly im-
portant improvement in applications with low toler-
ance for erroneous entities.
However, rule-based systems built by experts are
known to be vulnerable to new data unseen by the
experts. In order to apply this domain-specific error
reduction reliably, it has to be tested on data gathered
elsewhere. We used crowdsourced data to show that
the rule-based system was robust when confronted
with data that the designers did not see.
One danger in crowdsourcing is a potential lack
of commitment on the part of the annotators, as they
attempt to get through tasks as quickly as possible.
It turns out that in an NER context, we can design a
crowdsourced task that yields relatively reliable re-
sults across data sets by ensuring that for every data
point, there were multiple annotators making only
simple decisions about entity classification.
This method also provides us with a source of eas-
ily acquired supervised training data for testing more
advanced techniques, if required.
4.2 Costs
It took not more than an estimated two person weeks
to complete this work. This includes doing the
expert annotations, designing the Mechanical Turk
tasks, and building the domain-specific error reduc-
tion rules.
For each HIT, each annotator was paid 0.05 USD.
For three annotators for 1000 documents, that is
150.00 USD (plus additional small Amazon sur-
charges and any taxes that apply).
5 Conclusions and Future Work
This work was done on a single publication in a sin-
gle domain. One future experiment would be to see
whether these results are reliable across other pub-
lications in the domain. Another set of experiments
would be to determine the optimum number of an-
notators; we assumed three, but cross-domain results
may be more stable with more annotators.
Retraining an NER system for a particular domain
can be expensive if new annotations must be gen-
erated from scratch. While there is work on using
advanced machine learning techniques for domain
transfer (Guo et al, 2009), simply repairing the the
errors post hoc via a rule-based system can have a
low cost for high gains. This work shows a case
where the results are reliable and the verification
simple, in a context where reducing false positives
is a high priority.
Acknowledgements
This paper is based upon work supported by the Na-
tional Science Foundation under Grant IIS-0729459.
This research was also supported in part by NSF
award IIS-0838801.
References
Daniel M. Bikel, Richard Schwartz, and Ralph M.
Weischedel. 1999. An algorithm that learns what?s
in a name. Mach. Learn., 34(1-3).
Chris Callison-Burch. 2009. Fast, cheap, and creative:
Evaluating translation quality using Amazon?s Me-
chanical Turk. In EMNLP 2009, Singapore, August.
Honglei Guo, Huijia Zhu, Zhili Guo, Xiaoxun Zhang,
Xian Wu, and Zhong Su. 2009. Domain adapta-
tion with latent semantic association for named entity
recognition. In NAACL 2009, Morristown, NJ, USA.
Rion Snow, Brendan O?Connor, Daniel Jurafsky, and An-
drew Y. Ng. 2008. Cheap and fast?but is it good?:
evaluating non-expert annotations for natural language
tasks. In EMNLP 2008, Morristown, NJ, USA.
348
Proceedings of the 5th ACL-HLT Workshop on Language Technology for Cultural Heritage, Social Sciences, and Humanities, pages 69?77,
Portland, OR, USA, 24 June 2011. c?2011 Association for Computational Linguistics
Crowdsourcing syntactic relatedness judgements for opinion mining in the
study of information technology adoption
Asad B. Sayeed, Bryan Rusk, Martin Petrov,
Hieu C. Nguyen, Timothy J. Meyer
Department of Computer Science
University of Maryland
College Park, MD 20742 USA
asayeed@cs.umd.edu,brusk@umd.edu,
martin@martinpetrov.com,
{hcnguyen88,tmeyer88}@gmail.com
Amy Weinberg
Center for the Advanced
Study of Language
and Department of Linguistics
University of Maryland
College Park, MD 20742 USA
aweinberg@casl.umd.edu
Abstract
We present an end-to-end pipeline including
a user interface for the production of word-
level annotations for an opinion-mining task
in the information technology (IT) domain.
Our pre-annotation pipeline selects candidate
sentences for annotation using results from a
small amount of trained annotation to bias the
random selection over a large corpus. Our
user interface reduces the need for the user to
understand the ?meaning? of opinion in our
domain context, which is related to commu-
nity reaction. It acts as a preliminary buffer
against low-quality annotators. Finally, our
post-annotation pipeline aggregates responses
and applies a more aggressive quality filter.
We present positive results using two differ-
ent evaluation philosophies and discuss how
our design decisions enabled the collection of
high-quality annotations under subjective and
fine-grained conditions.
1 Introduction
Crowdsourcing permits us to use a bank of anony-
mous workers with unknown skill levels to perform
complex tasks given a simple breakdown of these
tasks with user interface design that hides the full
task complexity. Use of these techniques is growing
in the areas of computational linguistics and infor-
mation retrieval, particularly since these fields now
rely on the collection of large datasets for use in ma-
chine learning. Considering the variety of applica-
tions, a variety of datasets is needed, but trained,
known workers are an expense in principle that must
be furnished for each one. Consequently, crowd-
sourcing offers a way to collect this data cheaply and
quickly (Snow et al, 2008; Sayeed et al, 2010a).
We applied crowdsourcing to perform the fine-
grained annotation of a domain-specific corpus. Our
user interface design and our annotator quality con-
trol process allows these anonymous workers to per-
form a highly subjective task in a manner that cor-
relates their collective understanding of the task to
our own expert judgements about it. The path to
success provides some illustration of the pitfalls in-
herent in opinion annotation. Our task is: domain
and application-specific sentiment classification at
the sub-sentence level?at the word level.
1.1 Opinions
For our purposes, we define opinion mining (some-
times known as sentiment analysis) to be the re-
trieval of a triple {source, target, opinion} (Sayeed
et al, 2010b; Pang and Lee, 2008; Kim and Hovy,
2006) in which the source is the entity that origi-
nated the opinionated language, the target is a men-
tion of the entity or concept that is the opinion?s
topic, and the opinion is a value (possibly a struc-
ture) that reflects some kind of emotional orientation
expressed by the source towards the target.
In much of the recent literature on automatic
opinion mining, opinion is at best a gradient be-
tween positive and negative or a binary classifica-
tion thereof; further complexity affects the reliability
of machine-learning techniques (Koppel and Schler,
2006).
We call opinion mining ?fine-grained? when we
are attempting to retrieve potentially many different
69
{source, target, opinion} triples per document. This
is particularly challenging when there are multiple
triples even at a sentence level.
1.2 Corpus-based social science
Our work is part of a larger collaboration with social
scientists to study the diffusion of information tech-
nology (IT) innovations through society by identify-
ing opinion leaders and IT-relevant opinionated lan-
guage (Rogers, 2003). A key hypothesis is that the
language used by opinion leaders causes groups of
others to encourage the spread of the given IT con-
cept in the market.
Since the goal of our exercise is to ascertain the
correlation between the source?s behaviour and that
of others, then it may be more appropriate to look
at opinion analysis with the view that what we are
attempting to discover are the views of an aggregate
reader who may otherwise have an interest in the IT
concept in question. We thus define an expression of
opinion in the following manner:
A expresses opinion about B if an in-
terested third party C?s actions towards B
may be affected by A?s textually recorded
actions, in a context where actions have
positive or negative weight.
This perspective runs counter to a widespread view
(Ruppenhofer et al, 2008) which has assumed a
treatment of opinionated language as an observation
of a latent ?private state? held by the source. This
definition reflects the relationship of sentiment and
opinion with the study of social impact and market
prediction. We return to the question of how to de-
fine opinion in section 6.2.
1.3 Crowdsourcing in sentiment analysis
Paid crowdsourcing is a relatively new trend in com-
putational linguistics. Work exists at the paragraph
and document level, and it exists for the Twitter and
blog genres (Hsueh et al, 2009).
A key problem in crowdsourcing sentiment analy-
sis is the matter of quality control. A crowdsourced
opinion mining task is an attempt to use untrained
annotators over a task that is inherently very subjec-
tive. It is doubly difficult for specialized domains,
since crowdsourcing platforms have no way of di-
rectly recruiting domain experts.
Hsueh et al (2009) present results in quality con-
trol over snippets of political blog posts in a task
classifying them by sentiment and political align-
ment. They find that they can use a measurement of
annotator noise to eliminate low-quality annotations
at this coarse level by reweighting snippet ambigu-
ity scores with noise scores. We demonstrate that we
can use a similar annotator quality measure alone to
eliminate low-quality annotations on a much finer-
grained task.
1.4 Syntactic relatedness
We have a downstream application for this annota-
tion task which involves acquiring patterns in the
distribution of opinion-bearing words and targets us-
ing machine learning (ML) techniques. In partic-
ular, we want to acquire the syntactic relationships
between opinion-bearing words and within-sentence
targets. Supervised ML techniques require gold
standard data annotated in advance.
The Multi-Perspective Question-Answering
(MPQA) newswire corpus (Wilson and Wiebe,
2005) and the J. D. Power & Associates (JDPA)
automotive review blog post (Kessler et al, 2010)
corpus are appropriate because both contain sub-
sentence annotations of sentiment-bearing language
as text spans. In some cases, they also include links
to within-sentence targets. This is an example of an
MPQA annotation:
That was the moment at which the fabric
of compassion tore, and worlds cracked
apart; when the contrast and conflict of
civilisational values became so great as
to remove any sense of common ground -
even on which to do battle.
The italicized portion is intended to reflect a negative
sentiment about the bolded portion. However, while
it is the case that the whole italicized phrase repre-
sents a negative sentiment, ?remove? appears to rep-
resent far more of the negativity than ?common? and
?ground?. While there are techniques that depend
on access to entire phrases, our project is to identify
sentiment spans at the length of a single word.
2 Data source
Our corpus for this task is a collection of arti-
cles from the IT professional magazine, Information
70
Week, from the years 1991 to 2008. This consists
of 33K articles of varying lengths including news
bulletins, full-length magazine features, and opin-
ion columns. We obtained the articles via an institu-
tional subscription, and reformatted them in XML1.
Certain IT concepts are particularly significant in
the context of the social science application. Our tar-
get list consists of 59 IT innovations and concepts.
The list includes plurals, common variations, and
abbreviations. Examples of IT concepts include ?en-
terprise resource planning? and ?customer relation-
ship management?. To avoid introducing confound-
ing factors into our results, we only include explicit
mentions and omit pronominal coreference.
3 User interface
Our user interface (figure 1) uses a drag-and-drop
process through which workers make decisions
about whether particular highlighted words within
a given sentence reflect an opinion about a particu-
lar mentioned IT concept or innovation. The user
is presented with a sentence from the corpus sur-
rounded by some before and after context. Under-
neath the text are four boxes: ?No effect on opin-
ion? (none), ?Affects opinion positively? (postive),
?Affects opinion negatively? (negative), and ?Can?t
tell? (ambiguous).
The worker must drag each highlighted word in
the sentence into one of the boxes, as appropriate. If
the worker cannot determine the appropriate box for
a particular word, she is expected to drag this to the
ambiguous box. The worker is presented with de-
tailed instructions which also remind her that most
of words in the sentence are not actually likely to be
involved in the expression of an opinion about the
relevant IT concept2. The worker is not permitted
to submit the task without dragging all of the high-
lighted words to one of the boxes. When a word
is dragged to a box, the word in context changes
colour; the worker can change her mind by clicking
an X next to the word in the box.
1We will likely be able to provide a sample of sentence data
annotated by our process as a resource once we work out docu-
mentation and distribution issues.
2We discovered when testing the interface that workers can
feel obliged to find a opinion about the selected IT concept. We
reduced it by explicitly reminding them that most words do not
express a relevant opinion and by placing the none box first.
We used CrowdFlower to manage the task with
Amazon Mechanical Turk as its distribution chan-
nel. We set CrowdFlower to present three sentences
at a time to users. Only users with USA-based IP
addresses were permitted to perform the final task.
4 Procedure
In this section, we discuss the data processing
pipeline (figure 3) through which we select candi-
dates for annotations and the crowdsourcing inter-
face we present to the end user for classifying indi-
vidual words into categories that reflect the effect of
the word on the worker.
4.1 Data preparation
4.1.1 Initial annotation
Two social science undergraduate students were
hired to do annotations on Information Week with
the original intention of doing all the annotations
this way. There was a training period where they an-
notated about 60 documents in sets of 20 in iterative
consultation with one of the authors. Then they were
given 142 documents to annotate simultaneously in
order to assess their agreement after training.
Annotation was performed in Atlas.ti, an anno-
tation tool popular with social science researchers.
It was chosen for its familiarity to the social sci-
entists involved in our project and because of their
stated preference for using tools that would allow
them to share annotations with colleagues. Atlas.ti
has limitations, including the inability to create hier-
archical annotations. We overcame these limitations
using a special notation to connect related annota-
tions. An annotator highlights a sentence that she
believes contains an opinion about a mentioned tar-
get on one of the lists. She then highlights the men-
tion of the target and, furthermore, highlights the in-
dividual words that express the opinion about the tar-
get, using the notation to connect related highlights.
4.1.2 Candidate selection
While the use of trained annotators did not pro-
duce reliable results (section 6.2) in acceptable time
frames, we decided to use the annotations in a pro-
cess for selecting candidate sentences for crowd-
sourcing. All 219 sentences that the annotators se-
lected as having opinions about within-sentence IT
71
Figure 1: A work unit presented in grayscale. ?E-business? is the IT concept and would be highlighted in blue. The
words in question are highlighted in gray background and turn red after they are dragged to the boxes.
concepts were concatenated into a single string and
converted into a TFIDF unit vector.
We then selected all the sentences that contain
IT concept mentions from the entire Information
Week corpus using an OpenNLP 1.4.3 model as
our sentence-splitter. This produced approximately
77K sentences. Every sentence was converted into a
TFIDF unit vector, and we took the cosine similar-
ity of each sentence with the TFIDF vector. We then
ranked the sentences by cosine similarity.
4.1.3 Selecting highlighted words
We ran every sentence through the Stanford
part-of-speech tagger. Words that belonged to
open classes such as adjectives and verbs were se-
lected along with certain closed-class words such as
modals and negation words. These candidate words
were highlighted in the worker interface.
We did not want to force workers to classify every
single word in a sentence, because this would be too
tedious. So we instead randomly grouped the high-
lighted words into non-overlapping sets of six. (Re-
mainders less than five were dropped from the task.)
We call these combinations of sentence, six words,
and target IT concept a ?highlight group? (figure 2).
Each highlight group represents a task unit which
we present to the worker in our crowdsourcing ap-
plication. We generated 1000 highlight groups from
The amount of industry attention paid to this
new class of integration software speaks volumes
about the need to extend the reach of ERP systems.
The amount of industry attention paid to this
new class of integration software speaks volumes
about the need to extend the reach of ERP systems.
Figure 2: Two highlight groups consisting of the
same sentence and concept (ERP) but different non-
overlapping sets of candidate words.
the top-ranked sentences.
4.2 Crowdsourced annotation
4.2.1 Training gold
We used CrowdFlower partly because of its au-
tomated quality control process. The bedrock of
this process is the annotation of a small amount of
gold standard data by the task designers. Crowd-
Flower randomly selects gold-annotated tasks and
presents them to workers amidst other unannotated
tasks. Workers are evaluated by the percentage of
gold-annotated tasks they perform correctly. The re-
sult of a worker performing a task unit is called a
?judgement.?
Workers are initially presented their gold-
annotated tasks without knowing that they are an-
swering a test question. If they get the question
wrong, CrowdFlower presents the correct answer to
72
them along with a reason why their answer was an
error. They are permitted to write back to the task
designer if they disagree with the gold judgement.
This process functions in a manner analogous to
the training of a machine-learning system. Further-
more, it permits CrowdFlower to exclude or reject
low-quality results. Judgements from a worker who
slips below 65% correctness are rated as untrustwor-
thy and not included in the CrowdFlower?s results.
We created training gold in the manner recom-
mended by CrowdFlower. We randomly selected
50 highlight groups from the 1000 mentioned in the
previous section. We ran these examples through
CrowdFlower using the interface we discuss in the
next section. Then we used the CrowdFlower gold
editor to select 30 highlight groups that contained
clear classification decisions where it appeared that
the workers were in relative consensus and where we
agreed with their decision. Of these, we designated
only the clearest-cut classifications as gold, leav-
ing more ambiguous-seeming ones up to the users.
For example, in the second highlight group in 2, we
would designate software and systems as none and
extend as positive in the training gold and the re-
mainder as up to the workers. That would be a ?min-
imum effort? to indicate that the worker understands
the task the way we do.
Unfortunately, CrowdFlower has some limita-
tions in the way it processes the responses to gold?
it is not possible to define a minimum effort pre-
cisely. CrowdFlower?s setting either allow us to pass
workers based on getting at least one item in each
class correct or by placing all items in their correct
classes. The latter is too strict a criterion for an in-
herently subjective task. So we accepted the former.
We instead applied our minimum effort criterion in
some of our experiments as described in section 4.3.
4.2.2 Full run
We randomly selected another 200 highlight
groups and posted them at 12 US cents for each set
of three highlight groups, with at least three Me-
chanical Turk workers seeing each highlight group.
The 30 training gold highlight groups were posted
along with them. Including CrowdFlower and Ama-
zon fees, the total cost was approximately 60 USD.
We permitted only USA-based workers to access the
task. Once initiated, the entire task took approxi-
Figure 3: Schematic view of pipeline.
mately 24 hours to complete.
4.3 Post-processing
4.3.1 Aggregation
Each individual worker?s ambiguous annotations
are converted to none annotations, as the ambigu-
ous box is intended as an outlet for a worker?s un-
certainty, but we choose to interpret anything that
a worker considers too uncertain to be classified
as positive or negative as something that is not
strongly opinionated under our definitions.
Aggregation is performed by majority vote of the
annotators on each word in each highlight group. If
no classification obtains more than 50% for a given
word, the word is dropped as too ambiguous to be
accepted either way as a result. This aggregation
has the effect of smoothing out individual annotator
differences.
4.3.2 Extended quality control
While CrowdFlower provides a first-pass quality
control system for selecting annotators who are do-
ing the task in good faith and with some understand-
ing of the instructions, we wanted particularly to
select annotators who would be more likely to be
consistent on the most obvious cases without overly
constraining them. Even with the same general idea
of our intentions, some amount of variation among
the annotators is unavoidable; how do we then reject
annotations from those workers who pass Crowd-
Flower?s liberal criteria but still do not have an idea
of annotation close enough to ours?
73
Our solution was to score the annotators post hoc
by their accuracy on our minimum-effort training
gold data. Then we progressively dropped the worst
n annotators starting from n = 0 and measured the
quality of the aggregated annotations as per the fol-
lowing section.
5 Results
This task can be interpreted in two different ways:
as an annotation task and as a retrieval system. An-
notator reliability is an issue insofar as it is impor-
tant that the annotations themselves conform to a
predetermined standard. However, for the machine
learning task that is downstream in our processing
pipeline, obtaining a consistent pattern is more im-
portant than conformance to an explicit definition.
We can thus interpret the results as being the out-
put of a system whose computational hardware hap-
pens to be a crowd of humans rather than silicon,
considering that the time of the ?run? is compara-
ble to many automated systems; Amazon Mechani-
cal Turk?s slogan is ?artificial artificial intelligence?
for a reason.
Nevertheless, we evaluated our procedure under
both interpretations by comparing against our own
annotations in order to assess the quality of our col-
lection, aggregation, and filtering process:
1. As an annotation task: we use Cohen?s ?
between the aggregated and filtered data vs.
our annotations in the belief that higher above-
chance agreement would imply that the aggre-
gate annotation reflected collective understand-
ing of our definition of sentiment. Consider-
ing the inherently subjective nature of this task
and the interdependencies inherent in within-
sentence judgements, Cohen?s ? is not a defini-
tive proof of success or failure.
2. As a retrieval task: Relative to our own an-
notations, we use the standard information re-
trieval measures of precision, recall, and F-
measure (harmonic mean) as well as accuracy.
We merge positive and negative annotations
into a single opinion-bearing class and measure
whether we can retrieve opinion-bearing words
while minimizing words that are, in context,
not opinion-bearing relative to the given target.
(We do not merge the classes for agreement-
based evaluation as there was not much over-
lap between positive and negative classifica-
tions.) The particular relative difference be-
tween precision and recall will suggest whether
the workers had a consistent collective under-
standing of the task.
It should be noted that the MPQA and the JDPA do
not report Cohen?s ? for subjective text spans partly
for the reason we suggest above: the difficulty of as-
sessing objective agreement on a task in which sub-
jectivity is inherent and desirable. There is also a
large class imbalance problem. Both these efforts
substitute retrieval-based measures into their assess-
ment of agreement.
We annotated a randomly-selected 30 of the 200
highlight groups on our own. Those 30 had 169
annotated words of which 117 were annotated as
none, 35 as positive, and 17 as negative. The re-
sults of our process are summarized in table 1.
In the 30 highlight groups, there were 155 total
words for which a majority consensus (>50%) was
reached. 48 words were determined by us in our
own annotation to have opinion weight (positive or
negative). There are only 22 annotators who passed
CrowdFlower?s quality control.
The stringent filter on workers based on their ac-
curacy on our minimum-effort gold annotations has
a remarkable effect on the results. As we exclude
workers, the F-measure and the Cohen?s ? appear
to rise, up to a point. By definition, each exclu-
sion raises the threshold score for acceptance. As
we cross the 80% threshold, the performance of the
system drops noticeably, as the smoothing effect of
voting is lost. Opinion-bearing words also reduce
in number as the threshold rises as some highlight
groups simply have no one voting for them. We
achieve our best result in terms of Cohen?s ? on
dropping the 7 lowest workers. We achieve our high-
est precision and accuracy after dropping the 10 low-
est workers.
Between the 7th and 10th underperforming an-
notator, we find that precision starts to exceed re-
call, possibly due to the loss of retrievable words as
some highlight groups lose all their annotators. Lost
words can be recovered in another round of annota-
tion.
74
Workers excluded No. of words lost (of 48) Prec/Rec/F Acc Cohen?s ? Score threshold
(prior polarity) N/A 0.87 / 0.38 / 0.53 0.79 -0.26 N/A
0 0 0.64 / 0.71 / 0.67 0.79 0.48 0.333
1 0 0.64 / 0.71 / 0.67 0.79 0.48 0.476
3 0 0.66 / 0.73 / 0.69 0.80 0.51 0.560
5 0 0.69 / 0.73 / 0.71 0.81 0.53 0.674
7 2 0.81 / 0.76 / 0.79 0.86 0.65 0.714
10 9 0.85 / 0.74 / 0.79 0.88 0.54 0.776
12 11 0.68 / 0.68 / 0.68 0.82 0.20 0.820
Table 1: Results by number of workers excluded from the task. The prior polarity baseline comes from a lexicon by
Wilson et al (2005) that is not specific to the IT domain.
6 Discussion
We have been able to show that crowdsourcing a
very fine-grained, domain-specific sentiment analy-
sis task with a nonstandard, application-specific def-
inition of sentiment is possible with careful user in-
terface design and mutliple layers of quality control.
Our techniques succeed on two different interpreta-
tions of the evaluation measure, and we can reclaim
any lost words by re-running the task. We used an
elaborate processing pipeline before and after anno-
tation in order to accomplish this. In this section, we
discuss some aspects of the pipeline that led to the
success of this technique.
6.1 Quality
There are three major aspects of our procedure that
directly affect the quality of our results: the first-
pass quality control in CrowdFlower, the majority-
vote aggregation, and the stringent post hoc filtering
of workers. These interact in particular ways.
The first-pass quality control interacts with the
stringent filter in that even if it were possible to
have run the stringent filter on CrowdFlower itself,
it would probably not have been a good idea. Al-
though we intended the stringent filter to be a min-
imum effort, it would have rejected workers too
quickly. It is technically possible to implement the
stringent filtering directly without the CrowdFlower
built-in control, but that would have entailed spend-
ing an unpredictable amount more money paying for
additional unwanted annotations from workers.
Furthermore, the majority-vote aggregation re-
quires that there not be too few annotators; our re-
sults show that filtering the workers too aggressively
harms the aggregation?s smoothing effect. The les-
son we take from this is that it can be beneficial to
accept some amount of ?bad? with the ?good? in im-
plementing a very subjective crowdsourcing task.
6.2 Design decisions
Our successful technique for identifying opinionated
words was developed after multiple iterations using
other approaches which did not succeed in them-
selves but produced outputs that were amenable to
refinement, and so these techniques became part of
a larger pipeline. However, the reasons why they did
not succeed on their own are illustrative of some of
the challenges in both fine-grained domain-specific
opinion annotation and in annotation via crowd-
sourcing under highly subjective conditions.
6.2.1 Direct annotation
We originally intended to stop with the trained an-
notation we described in 4.1.1, but collecting opin-
ionated sentences in this corpus turned out to be very
slow. Despite repeated training rounds, the annota-
tors had a tendency to miss a large number of sen-
tences that the authors found to be relevant. On dis-
cussion with the annotators, it turned out that the
variable length of the articles made it easy to miss
relevant sentences, particularly in the long feature
articles likely to contain opinionated language?a
kind of ?needle-in-a-haystack? problem.
Even worse, however, the annotators were vari-
ably conservative about what constituted an opinion.
One annotator produced far fewer annotations than
the other one?but the majority of her annotations
were also annotated by the other one. Discussion
with the annotators revealed that one of them simply
had a tighter definition of what constituted an opin-
ion. Attempts to define opinion explicitly for them
still led to a situations in which one was far more
conservative than the other.
75
6.2.2 Cascaded crowdsourcing technique
Insofar as we were looking for training data for
use in downstream machine learning techniques,
getting uniform sentence-by-sentence coverage of
the corpus was not necessary. There are 77K sen-
tences in this corpus which mention the relevant IT
concepts; even if only a fraction of them mention the
IT concepts with opinionated language, we would
still have a potentially rich source of training data.
Nevertheless the direct annotation with trained
annotators provided data for selecting candidate sen-
tences for a more rapid annotation. We used the
process in section 4.1.2 and chose the top-ranked
sentences. Then we constructed a task design that
divided the annotation into two phases. In the first
phase, for each candidate sentence, we ask the anno-
tator whether or not the sentence contains opinion-
ated language about the mentioned IT concept. (We
permit ?unsure? answers.)
In the second phase, for each candidate sentence
for which a majority vote of annotators decided that
the sentence contained a relevant opinion, we run
a second task asking whether particular words (se-
lected as per section 4.1.3) were words directly in-
volved in the expression of the opinion.
We tested this process with the 90 top-ranked
sentences. Four individuals in our laboratory an-
swered the ?yes/no/unsure? question of the first
phase. However, when we took their pairwise Co-
hen?s ? score, no two got more than approximately
0.4. We also took majority votes of each subset of
three annotators and found the Cohen?s ? between
them and the fourth. The highest score was 0.7, but
the score was not stable, and we could not trust the
results enough to move onto the second phase.
We also ran this first phase through Amazon Me-
chanical Turk. It turned out that it was far too easy
to cheat on this yes/no question, and some workers
simply answered ?yes? or ?no? all the time. Agree-
ment scores of a Turker majority vote vs. one of the
authors turned out to yield a Cohen?s ? of 0.05?
completely unacceptable.
Discussion with the in-laboratory annotators sug-
gested the roots of the problem: it was the same
problem as with the direct Atlas.ti annotation we re-
ported in the previous section. It was very difficult
for them to agree on what it meant for a sentence to
contain an opinion expressed about a particular con-
cept. Opinions about the nature of opinion ranged
from very ?conservative? to very ?liberal.? Even
explicit definition with examples led annotators to
reach very different conclusions. Furthermore, the
longer the annotators thought about it, the more con-
fused and uncertain they were about the criterion.
What is an opinion can itself be a matter of opin-
ion. It became clear that without very tight review
of annotation and careful task design, asking users
an explicit yes/no question about whether a particu-
lar concept has a particular opinion mentioned in a
particular sentence has the potential to induce over-
thinking by annotators, despite our variations on the
task. The difficulty may also lead to a tendency to
cheat. Crowdsourcing allows us to make use of non-
expert labour on difficult tasks if we can break the
tasks down into simple questions and aggregate non-
expert responses, but we needed a somewhat more
complex task design in order to eliminate the diffi-
culty of the task and the tendency to cheat.
7 Future work
Foremost among the avenues for future work is ex-
perimentation with other vote aggregration and post
hoc filtering schemes. For example, one type of ex-
periment could be the reweighting of votes by an-
notator quality rather than the wholesale dropping
of annotators. Another could involve the use of
general-purpose sentiment analysis lexica to bias the
vote aggregation in the manner of work in sentiment
domain transfer (Tan et al, 2007).
This work also points to the potential for crowd-
sourcing in computational linguistics applications
beyond opinion mining. Our task is a sentiment-
specific instance of a large class of syntactic relat-
edness problems that may suitable for crowdsourc-
ing. One practical application would be in obtaining
training data for coreference detection. Another one
may be in the establishment of empirical support for
theories about syntactic structure.
Acknowledgements
This paper is based on work supported by the Na-
tional Science Foundation under grant IIS-0729459.
76
References
Pei-Yun Hsueh, Prem Melville, and Vikas Sindhwani.
2009. Data quality from crowdsourcing: a study of
annotation selection criteria. In Proceedings of the
NAACL HLT 2009 Workshop on Active Learning for
Natural Language Processing, HLT ?09, Stroudsburg,
PA, USA. Association for Computational Linguistics.
Jason S. Kessler, Miriam Eckert, Lyndsay Clark, and
Nicolas Nicolov. 2010. The 2010 ICWSM JDPA sent-
ment corpus for the automotive domain. In 4th Int?l
AAAI Conference on Weblogs and Social Media Data
Workshop Challenge (ICWSM-DWC 2010).
Soo-Min Kim and Eduard Hovy. 2006. Extracting opin-
ions, opinion holders, and topics expressed in online
news media text. In SST ?06: Proceedings of the Work-
shop on Sentiment and Subjectivity in Text, pages 1?8,
Morristown, NJ, USA. Association for Computational
Linguistics.
Moshe Koppel and Jonathan Schler. 2006. The im-
portance of neutral examples for learning sentiment.
Computational Intelligence, 22(2).
Bo Pang and Lillian Lee. 2008. Opinion mining and
sentiment analysis. Found. Trends Inf. Retr., 2(1-2).
Everett M. Rogers. 2003. Diffusion of Innovations, 5th
Edition. Free Press.
Josef Ruppenhofer, Swapna Somasundaran, and Janyce
Wiebe. 2008. Finding the sources and targets of
subjective expressions. In Nicoletta Calzolari, Khalid
Choukri, Bente Maegaard, Joseph Mariani, Jan Odjik,
Stelios Piperidis, and Daniel Tapias, editors, Proceed-
ings of the Sixth International Language Resources
and Evaluation (LREC?08), Marrakech, Morocco. Eu-
ropean Language Resources Association (ELRA).
Asad B. Sayeed, Timothy J. Meyer, Hieu C. Nguyen,
Olivia Buzek, and Amy Weinberg. 2010a. Crowd-
sourcing the evaluation of a domain-adapted named
entity recognition system. In Human Language Tech-
nologies: The 2010 Annual Conference of the North
American Chapter of the Association for Computa-
tional Linguistics. Association for Computational Lin-
guistics.
Asad B. Sayeed, Hieu C. Nguyen, Timothy J. Meyer, and
Amy Weinberg. 2010b. Expresses-an-opinion-about:
using corpus statistics in an information extraction ap-
proach to opinion mining. In Proceedings of the 23rd
International Conference on Computational Linguis-
tics, COLING ?10.
Rion Snow, Brendan O?Connor, Daniel Jurafsky, and An-
drew Y. Ng. 2008. Cheap and fast?but is it good?:
evaluating non-expert annotations for natural language
tasks. In EMNLP 2008.
Songbo Tan, Gaowei Wu, Huifeng Tang, and Xueqi
Cheng. 2007. A novel scheme for domain-transfer
problem in the context of sentiment analysis. In Pro-
ceedings of the sixteenth ACM conference on Con-
ference on information and knowledge management,
CIKM ?07, New York, NY, USA.
Theresa Wilson and Janyce Wiebe. 2005. Annotating
attributions and private states. In CorpusAnno ?05:
Proceedings of the Workshop on Frontiers in Corpus
Annotations II, Morristown, NJ, USA. Association for
Computational Linguistics.
Theresa Wilson, Janyce Wiebe, and Paul Hoffmann.
2005. Recognizing contextual polarity in phrase-level
sentiment analysis. In HLT/EMNLP.
77
