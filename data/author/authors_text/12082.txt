Proceedings of the Workshop on BioNLP, pages 185?192,
Boulder, Colorado, June 2009. c?2009 Association for Computational Linguistics
TEXT2TABLE:  
Medical Text Summarization System based on Named Entity 
Recognition and Modality Identification 
 
 
Eiji ARAMAKI Yasuhide MIURA Masatsugu TONOIKE 
The university of Tokyo Fuji Xerox Fuji Xerox 
eiji.aramaki@gmail.com Yasuhide.Miura@fujixerox.co.jp masatsugu.tonoike@fujixerox.co.jp 
 
Tomoko OHKUMA 
 
Hiroshi MASHUICHI 
 
Kazuhiko OHE 
Fuji Xerox Fuji Xerox The university of Tokyo Hospital 
ohkuma.tomoko@fujixerox.co.jp hiroshi.masuichi@fujixerox.co.jp kohe@hcc.h.u-tokyo.ac.jp 
 
 
 
Abstract 
With the rapidly growing use of electronic 
health records, the possibility of large-scale 
clinical information extraction has drawn 
much attention. It is not, however, easy to ex-
tract information because these reports are 
written in natural language. To address this 
problem, this paper presents a system that 
converts a medical text into a table structure. 
This system?s core technologies are (1) medi-
cal event recognition modules and (2) a nega-
tive event identification module that judges 
whether an event actually occurred or not. 
Regarding the latter module, this paper also 
proposes an SVM-based classifier using syn-
tactic information. Experimental results dem-
onstrate empirically that syntactic information 
can contribute to the method?s accuracy. 
1 Introduction 
The use of electronic texts in hospitals is increas-
ing rapidly everywhere. This study specifically 
examines discharge summaries, which are reports 
generated by medical personnel at the end of a pa-
tient?s hospital stay. They include massive clinical 
information about a patient?s health, such as the 
frequency of drug usage, related side-effects, and 
correlation between a disease and a patient?s ac-
tions (e.g., smoking, drinking), which enables un-
precedented large-scale research, engendering 
promising findings. 
N
A
(1
(2
(3
                                                          
evertheless, it is not easy to extract clinical in-
formation from the reports because these reports 
are written in natural language. An example of a 
discharge summary is presented in Table 1. The 
table shows records that are full of medical jargon, 
acronyms, shorthand notation, misspellings, and 
sentence fragments (Tawanda et al, 2006). 
To address this problem, this paper presents a 
proposal of a system that extracts medical events 
and date times from a text. It then converts them 
into a table structure. We designate this system 
TEXT2TABLE, which is available from a web 
site 1 . The extraction method, which achieves a 
high accuracy extraction, is based on Conditional 
Random Fields (CRFs) (Lafferty et al, 2001). 
nother problem is posed by events that do not 
actually occur, i.e., future scheduled events, events 
that are merely intended to take place, or hypo-
thetical events. As described herein, we call such 
non-actual events negative events. Negative 
events are frequently mentioned in medical re-
cords; actually, in our corpus, 12% of medical 
events are negative. Several examples of negative 
events (in italic letters) are presented below: 
 
) no headache 
) keep appointment of radiotherapy 
) .. will have intravenous fluids 
1 http://lab0.com/  
185
(4
(4'
(5
th
(6
ac
A
B
T
T
A
) .. came for radiotherapy 
) .. came for headache 
) Every week radiation therapy and chemical 
erapy are scheduled 
) Please call Dr. Smith with worsening head-
he or back pain, or any other concern. 
 
Negative events have two characteristics. First, 
various words and phrases indicate that an event is 
negative. For this study, such a word or phrase that 
makes an event negative is called a negative trig-
ger. For instance, a negation word ?no? is a nega-
tive trigger in (1). A noun ?appointment? in (2) is a 
negative trigger. Similarly, the auxiliary ?will? in 
(3) signals negation. More complex phenomena are 
presented in (4) and (4'). For instance, ?radiother-
apy? in (4) is a negative event because the therapy 
will be held in the future. In contrast, ?headache? 
in (4') is not negative because a patient actually has 
a ?headache?. These indicate that a simple rule-
based approach (such as a list of triggers) can only 
imply classification of whether an event is negative 
or not, and that information of the event category 
(e.g., a therapy or symptom) is required. 
nother characteristic is a long scope of a nega-
tive trigger. Although negative triggers are near the 
descriptive words of events in (1)?(4), there could 
alternatively be a great distance of separation, as 
portrayed in (5) and (6). In (5), a noun coordina-
tion separates a negative trigger from the event. In 
(6), the trigger ?please? renders all events in that 
sentence negative. These indicate that neighboring 
words are insufficient to determine whether an 
event is negative or not. To deal with (5), syntactic 
information is helpful because the trigger and the 
event are neighboring in the dependency structure, 
as portrayed in Fig. 2. To deal with (6), bag-of-
word (BOW) information is desired. 
ecause of the observation described above, this 
paper presents a proposal of a classifier: whether 
an event is negative or not. The proposed classifier 
uses various information, the event category, 
neighboring words, BOW, and dependent phrases. 
he point of this paper is two-fold: (1) We pro-
pose a new type of text-summarizing system 
(TEXT2TABLE) that requires a technique for a 
negative event identification. (2) We investigate 
what kind of information is helpful for negative 
event identification. 
he experiment results revealed that, in spite of 
the risk of parsing error, syntactic information can 
contribute to performance, demonstrating the fea-
sibility of the proposed approach. 
lthough experiments described in this paper are 
related to Japanese medical reports, the proposed 
method does not depend on specific languages or 
domains. 
 
Table 1: A Health Record Sample. 
BRIEF RESUME OF HOSPITAL COURSE : 57 yo with 
NSCLCa with back pain and headache . Trans-
ferred from neurosurgery for additional mgmt 
with palliative XRT to head . Pt initially 
presented with cough and hemoptysis to his 
primary MD . On CXR he was found to have a 
upper left lobe mass . He subsequently un-
derwent bronchoscopy and bx revealed non-
small cell adeno CA. STaging revealed multi-
ple bony mets including skull, spine with 
MRI revealing mild compression of vertebral 
bodies at T9, T11, T12 . T9 with encroach-
ment of spinal cord underwent urgent XRT 
with no response so he was referred to neu-
rosurgery for intervention . MRI-rt. fron-
tal, left temporal, rt cerebellar 
hemorrhagic enhancing lesions- most likely 
extensive intracranial mets? T-spine surgery 
considered second priority and plan to radi-
ate cranially immediately with steroid and 
anticonvulsant . He underwent simulation on 
3/28 to whole brain and T3-T7 fields with 
plan for rx to both sites over 2.5 weeks. 
Over the past 2 weeks he has noted frontal 
and occipital HA with left eyelid swelling, 
ptosis, and denies CP, SOB, no sig. BM in 
past 5 days, small amt of stool after sup-
pository. Neuro?He was Dilantin loaded and a 
level should be checked on 3/31 . He is to 
continue Decadron . Onc?He is to receive XRT 
on 3/31 and daily during that week . Pain 
control?Currently under control with MS con-
tin and MSIR prn. regimen . Follow HA, LBP. 
ENDO?Glucose control monitored while on de-
cadron with SSRI coverage . Will check 
HgbA1C prior to discharge . GI?Aggressive 
bowel regimen to continue at home . Pt is 
Full Code . ADDITIONAL COMMENTS: Please call 
Dr. Xellcaugh with worsening headache or 
back pain, or any other concern . Keep ap-
pointment as scheduled with XRT . Please 
check fingerstick once a day, and record, 
call MD if greater than 200 .  
 
186
 
Figure 1: Visualization result (Left), magnified (Right). 
 
 
Figure 2: Negative Triggers and Events on a Depend-
ency Structure. 
 
Table 2: Corpora and Modalities 
CORPUS MODALITY 
ACE asserted, or other 
TIMEML must, may, should, would, or 
could 
Prasad et al, 
2006 
assertion, belief, facts or eventu-
alities 
Saur? et al, 2007 certain, probable, possible, or 
other 
Inui et al, 2008 affirm, infer, doubt, hear, intend, 
ask, recommend, hypothesize, or 
other 
THIS STUDY S/O, necessity, hope, possible, 
recommend, intend  
 
Table 3: Markup Scheme (Tags and Definitions) 
Tag Definition (Examples) 
R Remedy, Medical operation 
(e.g. radiotherapy) 
T Medical test, Medical examination 
(e.g., CT, MRI) 
D Deasese, Symptom 
(e.g., Endometrial cancer, headache) 
M Medication, administration of a drug 
(e.g., Levofloxacin, Flexeril) 
A patient action 
(e.g., admitted to a hospital) 
V Other verb 
(e.g., cancer spread to ...)  
 
2 Related Works 
2.1 Previous Markup Schemes 
In the NLP field, fact identification has not been 
studied well to date. Nevertheless, similar analyses 
can be found in studies of sentence modality. 
The Automatic Content Extraction (ACE)2 in-
formation extraction program deals with event ex-
traction, by which each event is annotated with 
temporal and modal markers. 
A
S  
A
T
                                                          
 similar effort is made in the TimeML project 
(Pustejovsky et al, 2003). This project specifically 
examines temporal expressions, but several modal 
expressions are also covered. 
Prasad et al (2006) propose four factuality clas-
sifications (certain, probable...etc.) for the Penn 
Discourse TreeBank (PDTB) 3. 
aur? et al (2007) propose three modal categories
for text entailment tasks. 
mong various markup schemes, the most recent 
one is Experience Mining (Inui et al, 2008), which 
collects personal experiences from the web. They 
also distinguish whether an experience is an actual 
one or not, which is a similar problem to that con-
fronting us. 
able 2 portrays a markup scheme adopted by 
each project. Our purpose is similar to that of Ex-
perience Mining. Consequently, we fundamentally 
adopt its markup scheme. However, we modify the 
label to suit medical mannerisms. For example, 
?doubt? is modified into ?(S/O) suspicion of?. Rare 
modalities such as ?hear? are removed. 
 
2.2 Previous Algorithms 
Negation is a traditional topic in medical fields. 
Therefore, we can find many previous studies of 
the topic in the relevant literature. 
An algorithm, NegEx4 was proposed by Chap-
man et al (Chapman et al, 2001a; Chapman et al, 
2001b). It outputs an inference of whether a term is 
positive or negative. The original algorithm is 
based on a list of negation expressions. Goldin et al 
(2003) incorporate machine learning techniques 
(Na?ve Bayes and decision trees) into the algorithm. 
The extended version (ConText) was also proposed 
(Chapman et al, 2007). 
Elkin et al (2005) use a list of negation words 
and a list of negation scope-ending words to iden-
2 http://projects.ldc.upenn.edu/ace/ 
3 http://www.seas.upenn.edu/~pdtb/ 
4 http://www.dbmi.pitt.edu/chapman/NegEx.html 
187
tify negated statements and their scope. Their tech-
nique was used in The MAYO Clinic Vocabulary 
Server (MCVS)5, which encodes clinical expres-
sions into medical ontology (SNOMED-CT) and 
identifies whether the event is positive or negative. 
M
H
T
A
                                                          
utalik et al (2001) earlier developed Negfinder 
to recognize negated patterns in medical texts. 
Their system uses regular expressions to identify 
words indicating negation. Then it passes them as 
special tokens to the parser, which makes use of 
the single-token look-ahead strategy. 
uang and Lowe (2007) implemented a hybrid 
approach to automated negation detection. They 
combined regular expression matching with 
grammatical parsing: negations are classified based 
on syntactic categories. In fact, they are located in 
parse trees. Their hybrid approach can identify ne-
gated concepts in radiology reports even when they 
are located distantly from the negative term. 
he Medical Language Extraction and Encoding 
(MedLEE) system was developed as a general 
natural language processor to encode clinical doc-
uments in a structured form (Friedman et al, 
1994). Negated concepts and certainty modifiers 
are also encoded within the system. 
Veronika et al (2008) published a negation 
scope corpus6 in which both negation and uncer-
tainty are addressed. 
lthough their motivations are identical to ours, 
two important differences are apparent. (1) Previ-
ous (except for Veronika et al, 2008) methods deal 
with the two-way problem (positive or negative), 
whereas the analyses proposed herein tackle more 
fine-grained modalities. (2) Previous studies (ex-
cept for Huang et al, 2007) are based on BOW 
approaches, whereas we use syntactic information. 
3 Medical Text Summarization System: 
TEXT2TABLE 
Because the core problem of this paper is to iden-
tify negative events, this section briefly presents a 
description of the entire system, which consists of 
four steps. The detailed algorithm of negative iden-
tification is explained in Section 4. 
STEP 1: Event Identification 
First, we define the event discussed in this paper. 
We deal with events of six types, as presented in 
5 http://mayoclinproc.highwire.org/content/81/6/741.figures-
only 
6 www.inf.u-szeged.hu/rgai/bioscope 
Table 3. Two of the four are Verb Phrases (base 
VPs); the others are noun phrases (base-NPs). Be-
cause this task is similar to Named Entity Recogni-
tion (NER), we use the state-of-the art NER 
method, which is based on the IOB2 representation 
and Conditional Random Fields (CRFs). In learn-
ing, we use standard features, as shown in Table 4. 
 
Table 4: Features for Event Identification 
Lexicon 
and 
Stem 
Current target word (and its stem) and its 
surrounding words (and stem). The win-
dow size is five words (-2, -1, 0, 1, 2). 
POS Part of speech of current target word and 
its surrounding words (-2, -1, 0, 1, 2). The 
part of speech is analyzed using a POS 
tagger7. 
DIC A fragment for the target word appears in 
the medical dictionary (Ito et al, 2003).  
 
STEP 2: Normalization 
As described in Section 1, a term in a record is 
sometimes an acronym: shorthand notation. Such 
abbreviations are converted into standard notation 
through (1) date time normalization or (2) event 
normalization. 
(1) Date Time Normalization 
As for date time expressions, relative date expres-
sions are converted into YYYY/MM/DD as fol-
lows. 
  On Dec Last year ? 2007/12/XX 
  10 Dec 2008        ? 2008/12/10 
These conversions are based on heuristic rules. 
(2) Event Normalization 
Medical terms are converted into standard notation 
(dictionary entry terms) using orthographic disam-
biguation (Aramaki et al, 2008). 
STEP 3: TIME?EVENT Relation Identification 
Then, each event is tied with a date time. The cur-
rent system relies on a simple rule (i.e., an event is 
tied with the latest date time). 
STEP 4: Negative Identification 
The proposed SVM classifier distinguishes nega-
tive events from other events. The detailed algo-
rithm is described in the next section. 
4 Modality Identification Algorithm 
First, we define the negative. We classify modality 
events into eight types (Table 5). These classifica-
tions are motivated by those used in previous stud-
                                                          
7 http://chasen-legacy.sourceforge.jp/ 
188
ies (Inui et al, 2008). However, we simplify their 
scheme because several categories are rare in this 
domain. 
T
U
hese classes are not exclusive. For that reason, 
they sometimes lead to multiple class events. For 
example, given ?No chemotherapy is planned?, an 
event ?chemotherapy? belongs to two classes, 
which are ?NEGATION? and ?FUTURE?. 
Training Phase 
sing a corpus with modality annotation, we train 
a SVM classifier for each category. The training 
features come from four parts: 
(1) Current phrases: words included in a current 
event. We also regard their STEMs, POSs, and the 
current event category as features. 
(2) Surrounding phrases: words included in the 
current event phrase and its surrounding two 
phrases (p1, p2, n1, n2, as depicted in Fig. 3). The 
unit of the phrase is base-NP/VP, which is pro-
duced by the Japanese parser (Kurohashi et al, 
1994). Its window size is two in the neighboring 
phrase (p1, p2, c, n1, n2). We also deal with their 
STEMs and POSs. 
(3) Dependent phrases: words included in the 
parent phrase of the current phrase (d1 in Fig. 3), 
and grandparent phrases (d2 in Fig. 3). We also 
deal with their STEMs and POSs. 
(4) Previous Event: words (with STEMs and 
POSs) included in the previous (left side) events. 
Additionally, we deal with the previous event cate-
gory and the modality class. 
(5) Bag-of-words: all words (with STEMs and 
POSs) in the sentence. 
 
TEST Phrase 
During the test, each SVM classifier runs. 
Although this task is multiclass labeling, several 
class combinations are unnatural, such as 
FUTURE and S/O. We list up possible label com-
binations (that have at least one occurrence in the 
corpora); if such a combination appears in a text, 
we adapt a high confidence label (using a marginal 
distance). 
 
5 Experiments 
We investigate what kind of information contrib-
utes to the performance in various machine learn-
ing algorithms. 
 
Table 5: Classification of Modalities 
NEGATION An event with negation words 
such as ?not? or ?no?. 
FUTURE An event that is scheduled for 
execution in the future. 
PURPOSE An event that is planed by a doc-
tor, but its time schedule is am-
biguous (just a hope/intention).  
S/O An event (usually a disease) that 
is suspected. For example, given 
?suspected microscopic tumor in 
...?, ?microscopic tumor'' is an 
S/O event.? 
NECESSITY An event (usually a remedy or 
medical test) that is required. 
INTEND An event that is hoped for by a 
patient.  
Note that if the event is hoped by 
a doctor, we regard is a 
PURPOSE or FUTURE. For ex-
ample, given ?He hoped for 
chemical therapy?, ?chemical 
therapy? is INTEND. 
POSSIBLE An event (usually remedy) that is 
possible under the current situa-
tion. 
RECOMMEND An event (usually remedy) that is 
recommended by other doctor(s). 
 
 
5.1 Corpus and Setting 
We collected 435 Japanese discharge summaries in 
which events and the modality are annotated. For 
training, we used the CRF toolkit8 with standard 
parameters. In this experiment setting, the input is 
an event with its contexts. The output is an event 
modality class (positive of negative in two-way) 
(or more detailed modality class in nine-way). 
T
 
                                                          
he core problem addressed in this paper is mo-
dality classification. Therefore, this task setting 
assumes that all events are identified correctly. 
Table 6 presents the event identification accuracy. 
Except for the rare class V (the other verb), we got 
more than 80% F-scores. It is true that the accu-
racy is not perfect. Nevertheless, most of the re-
maining problems in this step will be solved using 
a larger corpus. 
5.2 Comparable Methods 
We conducted experiments in the 10-fold cross 
validation manner. We investigated the perform-
8 http://crfpp.sourceforge.net/ 
189
ance in various feature combinations and the fol-
lowing machine learning methods. 
 
 
Figure 3: Features 
 
Table 6: Event Identification Result. Tag precision re-
call F-score.  
 # P R F 
A (ACTION) 1,556 94.63 91.04 92.80 
V (VERB) 1,047 84.64 74.89 79.47 
D (DISEASE) 3,601 85.56 80.24 82.82 
M (MEDICINE) 1,045 86.99 81.34 84.07 
R (REMEDY) 1,699 84.50 76.36 80.22 
T (TEST) 2,077 84.74 76.68 80.51 
ALL 11,025 84.74 76.68 80.51  
 
Table 7: Various Machine Learning Method 
SVM Support Vector Machine (Vapnik, 
1999). We used TinySVM9 with a 
polynomial kernel (degree=2). 
AP Averaged Perceptron (Collins, 2002) 
PA1 Passive Aggressive I (Crammer et 
al., 2006)* 
PA2 Passive Aggressive II (Crammer et 
al., 2006)* 
CW Confidence Weighted (Dredze et al, 
2008)* 
* The online learning library10 is used for AP PA1,2 
CW . 
 
5.3 Evaluation Metrics 
We adopt evaluation of two types: 
(1) Two-way: positive or negative: 
(2) Nine-way: positive or one of eight modality 
categories. 
Recall and F-measure are investigated in both for 
evaluation precision. 
 
5.4 Results 
The results are shown in Table 8 (Two-Way) and 
in Table 9 (Nine-Way). 
Current Event Category 
The results in ID0?ID1 indicate that the current 
event category (CAT) is useful. However, events 
are sometimes misestimated in real settings. We 
                                                          
In
R
A
A
H
9 http://chasen.org/ taku/software/TinySVM/ 
10 http://code.google.com/p/oll 
must check more practical performance in the fu-
ture. 
Bag-of-words (BOW) Information 
Results in ID1?ID2 indicate that BOW is impor-
tant. 
Surrounding Phrase Contribution 
The results appearing in ID2?ID9 represent the 
contribution of each feature position. From ID3, 
ID4, and ID7 results, next phrases (n1, n2) and 
parent phrases (d1) were able to boost the accuracy. 
Despite the risk of parsing errors, parent phrases 
(d1) are helpful, which is an insight of this study. 
 contrast, we can say that the following features 
had little contribution: previous phrases (p1, p2 
from ID5 and ID6), grandparent phrases (d2 from 
ID8), and previous events (e from ID9). 
egarding p1 and p2, these modalities are rarely 
expressed in the previous parts in Japanese. 
s for d2, the grandparent phrases might be too 
removed from the target events. 
s for e, because texts in health records are frag-
mented, each event might have little relation. 
owever, the above features are also helpful in 
cases with a stronger learning algorithm. 
In fact, among ID10?ID14, the SVM-based 
classifier achieved the best accuracy with all fea-
tures (ID14). 
 
Table 8: Two-way Results 
 
? indicates the used feature. c are features from the cur-
rent phrase. p1, p2, n1, n2 are features from surrounding 
phrases. e are features from a previous event. BOW is a 
bag-of-words using features from an entire sentence. 
CAT is the category of the current event. 
 
190
Learning Methods 
Regarding the learning algorithms, all online learn-
ing methods (ID7 and ID15?17) showed lower ac-
curacies than SVM (ID11), indicating that this task 
requires heavy learning. 
 
Nine-way Results 
Table 9 presents the accuracies of each class. Fun-
damentally, we can obtain high performance in the 
frequent classes (such as NEGATION, PURPOSE, 
and S/O). In contrast, the classifier suffers from 
low frequent classes (such as FUTURE). How to 
handle such examples is a subject of future study. 
 
Table 9: Two-way Results 
 # Preci-
sion 
Re-
call 
F-
measure 
NEGATION 441 84.19 77.36 80.63 
PURPOSE 346 91.35 63.87 75.17 
S/O 242 90.74 72.39 80.53 
FUTURE 97 23.31 55.96 32.91 
POSSIBLE 36 83.33 40.55 54.55 
INTEND 32 76.66 29.35 42.44 
RECOMMEND 21 95.71 38.57 54.98 
NECESSITY 4 100 0 0  
 
4.5 Future Works 
In this section, we will discuss several remaining 
problems. First, as described, the classifier suffers 
from low frequent modality classes. To give more 
examples for such classes is an important problem. 
Our final goal is to realize precise information ex-
traction from health records. Our IE systems are 
already available at the web site (http://lab0.com). 
Comprehensive evaluation of those systems is re-
quired. 
6 Conclusions 
This paper presented a classifier that identified 
whether an event has actually occurred or not. The 
proposed SVM-based classifier uses both BOW 
information and dependency parsing results. The 
experimental results demonstrated 85.8 F-
measure% accuracy and revealed that syntactic 
information can contribute to the method?s accu-
racy. In the future, a method of handling low-
frequency events is strongly desired. 
 
 
Acknowledgments 
Part of this research is supported by Grant-in-Aid 
for Scientific Research (A) of Japan Society for the 
Promotion of Science Project Number:?20680006  
F.Y.2008-20011 and the Research Collaboration 
Project with Fuji Xerox  Co. Ltd. 
References 
Wendy Chapman, Will Bridewell, Paul Hanbury, Greg-
ory F. Cooper, and Bruce Buchanan. 2001a. Evalua-
tion of negation phrases in narrative clinical reports. 
In Proceedings of AMIA Symp, pages 105-109. 
Wendy Chapman, Will Bridewell, Paul Hanbury, Greg-
ory F. Cooper, and Bruce Buchanan. 2001b. A sim-
ple algorithm for identifying negated findings and 
diseases in discharge summaries. Journal of Bio-
medical Informatics, 5:301-310. 
Wendy Chapman, John Dowling and David Chu. 2007. 
ConText: An algorithm for identifying contextual 
features from clinical text. Biological, translational, 
and clinical language processing (BioNLP2007), pp. 
81?88. 
Eiji Aramaki, Takeshi Imai, Kengo Miyo, and Kazuhiko 
Ohe: Orthographic Disambiguation Incorporating 
Transliterated Probability International Joint Confer-
ence on Natural Language Processing (IJCNLP2008), 
pp.48-55, 2008. 
Peter L. Elkin, Steven H. Brown, Brent A. Bauer, Casey 
S. Husser, William Carruth, Larry R. Bergstrom, and 
Dietlind L. Wahner Roedler. A controlled trial of au-
tomated classification of negation from clinical notes. 
BMC Medical Informatics and Decision Making 
5:13. 
C. Friedman, P.O. Alderson, J.H. Austin, J.J. Cimino, 
and S.B. Johnson. 1994. A general natural language 
text processor for clinical radiology. Journal of the 
American Medical Informatics Association, 
1(2):161-174. 
L. Gillick and S.J. Cox. 1989. Some statistical issues in 
the comparison of speech recognition algorithms. In 
Proceedings of IEEE International Conference on 
Acoustics, Speech, and Signal Processing, pages 532-
535. 
Ilya M. Goldin and Wendy Chapman. 2003. Learning to 
detect negation with not in medical texts. In Work-
shop at the 26th ACM SIGIR Conference. 
Yang Huang and Henry J. Lowe. 2007. A novel hybrid 
approach to automated negation detection in clinical 
radiology reports. Journal of the American Medical 
Informatics Association, 14(3):304-311. 
191
Kentaro Inui, Shuya Abe, Hiraku Morita, Megumi Egu-
chi, Asuka Sumida, Chitose Sao, Kazuo Hara, Koji 
Murakami, and Suguru Matsuyoshi. 2008. Experi-
ence mining: Building a large-scale database of per-
sonal experiences and opinions from web documents. 
In Proceedings of the 2008 IEEE/WIC/ACM Interna-
tional Conference on Web Intelligence, pages 314-
321. 
M. Ito, H. Imura, and H. Takahisa. 2003. Igaku- Shoin?s 
Medical Dictionary. Igakusyoin. 
Sadao Kurohashi and Makoto Nagao. 1994. A syntactic 
analysis method of long Japanese sentences based on 
the detection of conjunctive structures. Computa-
tional Linguistics, 20(4). 
Pradeep G. Mutalik, Aniruddha Deshpande, and Pra-
kash M. Nadkarni. 2001. Use of general purpose ne-
gation detection to augment concept indexing of 
medical documents: A quantitative study using the 
umls. Journal of the American Medical Informatics 
Association, 8(6):598-609. 
J. Lafferty, A. McCallum, and F. Pereira: Conditional 
random fields: Probabilistic models for segmenting 
and labeling sequence data, In Proceedings of the In-
ternational Conference on Machine Learning 
(ICML2001), pp.282-289, 2001. 
R. Prasad, N. Dinesh, A. Lee, A. Joshi and B. Webber: 
Annotating Attribution in the Penn Discourse Tree-
Bank, In Proceedings of the International Conference 
on Computational Linguistics and the Annual Con-
ference of the Association for Computational Lin-
guistics (COLING/ACL2006) Workshop on 
Sentiment and Subjectivity in Text, pp.31-38 (2006). 
R. Saur?, and J. Pustejovsky: Determining Modality and 
Factuality for Text Entailment, Proceedings of 
ICSC2007, pp. 509-516 (2007). 
Gaizauskas, A. Setzer, G. Katz, and D.R. Radev. 2003. 
New Directions in Question Answering: Timeml: 
Robust specification of event and temporal expres-
sions in text. AAAI Press. 
SNOMED-CT. 2002. SNOMED Clinical Terms Guide. 
College of American Pathologists.  
Sibanda Tawanda, Tian He, Peter Szolovits, and Uzuner 
Ozlem. 2006. Syntactically informed semantic cate-
gory recognizer for discharge summaries. In Proceed-
ings of the Fall Symposium of the American Medical 
Informatics Association (AMIA 2006), pages 11-15. 
Sibanda Tawanda and Uzuner Ozlem. 2006. Role of 
local context in automatic deidentification of un- 
grammatical, fragmented text. In Proceedings of the 
Human Language Technology conference and the 
North American chapter of the Association for Com-
putational Linguistics (HLT-NAACL2006), pages 
65-73. 
Veronika Vincze, Gyorgy Szarvas, Richard Farkas, Gy-
orgy Mora, and Janos Csirik. 2008. The bioscope 
corpus: biomedical texts annotated for uncertainty, 
negation and their scopes. BMC Bioinformatics, 
9(11). 
 
192
Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014), pages 628?632,
Dublin, Ireland, August 23-24, 2014.
TeamX: A Sentiment Analyzer with Enhanced Lexicon Mapping and
Weighting Scheme for Unbalanced Data
Yasuhide Miura
Fuji Xerox Co., Ltd. / Japan
yasuhide.miura@fujixerox.co.jp
Shigeyuki Sakaki
Fuji Xerox Co., Ltd. / Japan
sakaki.shigeyuki@fujixerox.co.jp
Keigo Hattori
Fuji Xerox Co., Ltd. / Japan
keigo.hattori@fujixerox.co.jp
Tomoko Ohkuma
Fuji Xerox Co., Ltd. / Japan
ohkuma.tomoko@fujixerox.co.jp
Abstract
This paper describes the system that has
been used by TeamX in SemEval-2014
Task 9 Subtask B. The system is a senti-
ment analyzer based on a supervised text
categorization approach designed with fol-
lowing two concepts. Firstly, since lex-
icon features were shown to be effective
in SemEval-2013 Task 2, various lexicons
and pre-processors for them are introduced
to enhance lexical information. Secondly,
since a distribution of sentiment on tweets
is known to be unbalanced, an weighting
scheme is introduced to bias an output of a
machine learner. For the test run, the sys-
tem was tuned towards Twitter texts and
successfully achieved high scoring results
on Twitter data, average F
1
70.96 on Twit-
ter2014 and average F
1
56.50 on Twit-
ter2014Sarcasm.
1 Introduction
The growth of social media has brought a ris-
ing interest to make natural language technologies
that work with informal texts. Sentiment anal-
ysis is one such technology, and several work-
shops such as SemEval-2013 Task 2 (Nakov et
al., 2013), CLEF 2013 RepLab 2013 (Amig?o
et al., 2013), and TASS 2013 (Villena-Rom?an
and Garc??a-Morera, 2013) have recently targeted
tweets or cell phone messages as analysis text.
This paper describes a system that has submit-
ted a sentiment analysis result to Subtask B of
SemEval-2014 Task9 (Rosenthal et al., 2014).
SemEval-2014 Task9 is a rerun of SemEval-2013
Task 2 with different test data, and Subtask B is a
task of message polarity classification.
This work is licenced under a Creative Commons Attribution
4.0 International License. Page numbers and proceedings
footer are added by the organizers. License details: http:
//creativecommons.org/licenses/by/4.0/
The system we prepared is a sentiment ana-
lyzer based on a supervised text categorization
approach. Various features and their extraction
methods are integrated in the system following the
works presented in SemEval-2013 Task 2. Addi-
tionally to these features, we assembled following
notable functionalities to the system:
1. Processes to enhance word-to-lemma map-
ping.
(a) A spelling corrector to normalize out-of-
vocabulary words.
(b) Two Part-of-Speech (POS) taggers to
realize word-to-lemma mapping in two
perspectives.
(c) A word sense disambiguator to obtain
word senses and their confidence scores.
2. An weighting scheme to bias an output of a
machine learner.
Functionalities 1a to 1c are introduced to enhance
information based on lexical knowledge, since
features based on lexicons are shown to be ef-
fective in SemEval-2013 Task 2 (Mohammad et
al., 2013). Functionality 2 is introduced to make
the system adjustable to polarity unbalancedness
known to exists in Twitter data (Nakov et al.,
2013).
The accompanying sections of this papers are
organized as follows. Section 2 describes re-
sources such as labeled texts and lexicons used in
our system. Section 3 explains the details of the
system. Section 4 discusses the submission test
run and some extra test runs that we performed
after the test data release. Finally, section 5 con-
cludes the paper.
2 Resources
2.1 Sentiment Labeled Data
The system is a constrained system, therefore only
the sentiment labeled data distributed by the task
628
Type #Used #Full %
Twitter(train) 6949 9684 71.8
Twitter(dev) 1066 1654 64.4
Twitter(dev-test) 3269 3813 85.7
SMS(dev-test) 2094 2094 100
Table 1: The numbers of messages for each type.
?train?, ?dev?, and ?dev-test? denote training, devel-
opment, and development-test respectively. #Used
is the number of messages that we were able to
obtain, and #Full is the maximum number of mes-
sages that were provided.
Criterion Lexicon
General Inquirer
FORMAL MPQA Subjectivity Lexicon
SentiWordNet
AFINN-111
INFORMAL
Bing Liu?s Opinion Lexicon
NRC Hashtag Sentiment Lexicon
Sentiment140 Lexicon
Table 2: The seven sentiment lexicons and their
criteria.
organizers were used. However, due to accessibil-
ity changes in tweets, a subset of the training, the
development, and the development-test data were
used. Table 1 shows the numbers of messages for
each type.
2.2 Sentiment Lexicons
The system includes seven sentiment lexicons
namely: AFINN-111 (Nielsen, 2011), Bing Liu?s
Opinion Lexicon
1
, General Inquirer (Stone et al.,
1966), MPQA Subjectivity Lexicon (Wilson et al.,
2005), NRCHashtag Sentiment Lexicon (Moham-
mad et al., 2013), Sentiment140 Lexicon (Moham-
mad et al., 2013), and SentiWordNet (Baccianella
et al., 2010). We categorized these seven lexi-
cons to two criteria: ?FORMAL? and ?INFOR-
MAL?. Lexicons that include lemmas of erroneous
words (e.g. misspelled words) were categorized
to ?INFORMAL?. Table 2 illustrates the criteria of
the seven lexicons. These criteria are used in the
process of word-to-lemma mapping processes and
will be explained in Section 3.1.3.
3 System Details
The system is a modularized system consisting
of a variety of pre-processors, feature extractors,
1
http://www.cs.uic.edu/
?
liub/FBS/
sentiment-analysis.html
Text?Normalizer
Stanford?POS?Tagger
Word?Sense?Disambiguator Negation?Detector
word?senses FORMAL?lexicons
Pre?processors
Feature?Extractors
Input Spelling?Corrector
CMU?ARK?POS?Tagger
Negation?Detector
word?ngramscharacter?ngrams clusters
Machine?Learner Prediction?Adjuster
Output
INFORMAL?lexicons
Figure 1: An overview of the system?
and a machine learner. Figure 1 illustrates the
overview of the system.
3.1 Pre-processors
3.1.1 Text Normalizer
The text normalizer performs following three rule-
based normalization of an input text:
? Unicode normalization in form NFKC
2
.
? All upper case letters are converted to lower
case ones (ex. ?GooD? to ?good?).
? URLs are exchanged with string ?URL?s (ex.
?http://example.org? to ?URL?).
3.1.2 Spelling Corrector
A spelling corrector is included in the system to
normalize misspellings. We used Jazzy
3
, an open
source spell checker with US English dictionaries
provided along with Jazzy. Jazzy combines Dou-
bleMetaphone phonetic matching algorithm and a
near-miss match algorithm based on Levenshtein
distance to correct a misspelled word.
3.1.3 POS Taggers
The system includes two POS taggers to realize
word-to-lemma mapping in two perspectives.
Stanford POS Tagger Stanford Log-linear Part-
of-Speech Tagger (Toutanova et al., 2003) is
one POS tagger which is used to map words
2
http://www.unicode.org/reports/tr15/
3
http://jazzy.sourceforge.net/
629
to lemmas of ?FORMAL? criterion lexicons,
and to extract word sense features. A finite-
state transducer based lemmatizer (Minnen et
al., 2001) included in the POS tagger is used
to obtain lemmas of tokenized words.
CMU ARK POS Tagger A POS tagger for
tweets by CMU ARK group (Owoputi et al.,
2013) is another POS tagger used to map
words to lemmas of ?INFORMAL? criterion
lexicons, and to extract ngram features and a
cluster feature.
3.1.4 Word Sense Disambiguator
Aword sense disambiguator is included in the sys-
tem to determine a sense of a word. We used
UKB
4
which implements graph-based word sense
disambiguation based on Personalized PageRank
algorithm (Agirre and Soroa, 2009) on a lexical
knowledge base. As a lexical knowledge base,
WordNet 3.0 (Fellbaum, 1998) included in the
UKB package is used.
3.1.5 Negation Detector
The system includes a simple rule-based negation
detector. The detector is an implementation of the
algorithm on Christopher Potts? Sentiment Sym-
posium Tutorial
5
. The algorithm is a simple algo-
rithm that appends a negation suffix to words that
appear within a negation scope surrounded by a
negation key (ex. ?no?) and a certain punctuation
(ex. ?:?).
3.2 Features
The followings are the features used in the system.
word ngrams Contiguous 1, 2, 3, and 4 grams
of words, and non-contiguous 3 and 4 grams
of words are extracted from a given words.
Non-contiguous ngram are ngrams where one
of words are replaced with a wild card word
?*?. Example of contiguous 3 grams is
?by the way?, and the corresponding noncon-
tiguous variation is ?by * way?.
character ngrams Contiguous 3, 4, and 5 grams
of characters with in a word are extracted
from given words.
lexicons Words are mapped to seven lexicons of
section 2.2. For two sentiment labels (pos-
itive and negative) in each lexicon, follow-
ing four values are extracted: total matched
4
http://ixa2.si.ehu.es/ukb/
5
http://sentiment.christopherpotts.
net/lingstruc.html#negation
I?liked an?example.org?video http://example.org
Sense ID Score01824736?v 0.44231301777210?v 0.35567901776952?v 0.148101?
Sense?ID Score06277280?n 0.68865506277803?n 0.16334304534127?n 0.103199?
text
WSD?result
Feature Weight01824736?v 0.442313
Features 01777210?v 0.35567901776952?v 0.14810106277280?n 0.688655?
Figure 2: An example of word senses feature?
word count, total score, maximal score, and
last word score
6
. For lexicons without senti-
ment scores, score 1.0 is used for all entries.
Note that different POS taggers are used in
word-to-lemma mapping as described in Sec-
tion 3.1.3.
clusters Words are mapped to Twitter Word Clus-
ters of CMU ARK group
7
. The largest clus-
tering result consisting of 1000 clusters from
approximately 56 million tweets is used as
clusters.
word senses A result of the word sense disam-
biguator is extracted as weighted features ac-
cording to their scores. Figure 2 shows an
example of this feature.
The ngram features are introduced as basic bag-
of-words features in a supervised text categoriza-
tion approach. Lexicon features are designed to
strengthen the lexical features of Mohammad et
al. (2013) which have been shown to be effective
in the last year?s task. Cluster features are im-
plemented as an improvement for an supervised
NLP system following the work of Turian et al.
(2010). Word sense features are utilized to help
subjectivity analysis and contextual polarity anal-
ysis (Akkaya et al., 2009).
3.3 Machine Learner
Logistic Regression is utilized as an algorithm of
a supervised machine learning method. As an
implementation of Logistic Regression, LIBLIN-
EAR (Fan et al., 2008) is used. A Logistic Regres-
sion is trained using the features of Section 3.2
with the three polarities (positive, negative, and
neutral) as labels.
6
The total number of lexical features is 7? 2? 4 = 56.
7
http://www.ark.cs.cmu.edu/TweetNLP/
630
Parameters Sources
Parameter Selection Source
C w
pos
w
neg
LiveJournal SMS Twitter Twitter Twitter2014
2014 2013 2013 2014 Sarcasm
Twitter(train)+Twitter(dev) 0.07 1.7 2.6 71.23 62.33 71.28 70.40 53.32
Twitter(dev-test)* 0.03 2.4 3.3 69.44 57.36 72.12 70.96 56.50
SMS(dev-test) 0.80 1.1 1.2 72.99 68.92 65.65 66.66 48.24
SMS(dev-test)+Twitter(dev-test) 0.07 1.9 2.0 72.54 65.44 70.41 69.80 51.09
Table 3: The scores for each source in the test runs. The run with asterisk (*) denotes the submission
run. The values in the ?Sources? columns represent scores in SemEval-2014 Task 9 metric (the average
of positive F
1
and negative F
1
).
3.4 Prediction Adjuster
Since the labels in the tweets data are unbalanced
(Nakov et al., 2013), we prepared a prediction ad-
juster for Logistic Regression output. For each po-
larity l, an weighting factorw
l
that adjusts a proba-
bility output Pr(l) is introduced. An updated pre-
diction label is decided by selecting an l that max-
imizes score(l) which can be expressed as equa-
tion 1.
arg max
l?{pos,neg,neu}
score(l) = w
l
Pr(l) (1)
The approach we took in this prediction adjuster
is a simple approach to bias an output of Logistic
Regression, but may not be a typical approach to
handle unbalanced data. For instance, LIBLIN-
EAR includes the weighting option ?-wi? which
enables a use of different cost parameter C for dif-
ferent classes. One advantage of our approach is
that the change in w
l
does not require a training of
Logistic Regression. Various values of w
l
can be
tested with very low computational cost, which is
helpful in a situation like SemEval tasks where the
time for development is limited.
4 Test Runs
4.1 Submission Test Run
The system was trained using the 8,015 tweets in-
cluded in Twitter(train) and Twitter(dev) described
in Section 2.1. Three parameters: cost parameter
C of Logistic Regression, weight w
pos
of the pre-
diction adjuster, and weight w
neg
of the predic-
tion adjuster, were considered in the submission
test run. For the w
neu
of the prediction adjuster, a
fixed value of 1.0 was used.
Prior to the submission test run, the following
two steps were performed to select a parameter
combination for the submission run.
Step 1 The system with all combinations of C in
range of {0.01 to 0.09 by step 0.01, 0.1 to 0.9
by step 0.1, 1 to 10 by step 1}, w
pos
in range
of {1.0 to 5.0 by step 0.1}, and w
neg
in range
of {1.0 to 5.0 by step 0.1} were prepared
8
.
Step 2 The performances of the system for all
these parameter combinations were calcu-
lated using Twitter(dev-test) described in
Section 2.1.
As a result, the parameter combination C = 0.03,
w
pos
= 2.4, and w
neg
= 3.3 which performed
best in Twitter(dev-test) was selected as a parame-
ter combination for the submission run.
Finally, the system with the selected parameters
was applied to the test set of SemEval-2014 Task
9. ?Twitter(dev-test)? in Table 3 shows the val-
ues of this submission run. The system achieved
high performances on Twitter data: 72.12, 70.96,
and 56.50 on Twitter2013, Twitter2014, and Twit-
ter2014Sarcasm respectively.
4.2 Post-Submission Test Runs
The system performed quite well on Twitter
data but not so well on other data on the sub-
mission run. After the release of the gold
data of the 2014 test tun, we conducted sev-
eral test runs using different parameter combina-
tions. ?Twitter(train)+Twitter(dev)?, ?SMS(dev-
test)?, and ?SMS(dev-test)+Twitter(dev-test)? are
the results of test runs with different data sources
used for the parameter selection process. In ?Twit-
ter(train)+Twitter(dev)?, the parameter combina-
tion that maximizes a micro-average score of 5-
fold cross validation was chosen since the training
data and the parameter selection are equivalent.
The parameter combination selected with ?Twit-
ter(train)+Twitter(dev)? showed similar result as
the submission run, which is high performances
on Twitter data. In the case of ?SMS(dev-test)?, the
system performed well on ?LiveJournal2014? and
?SMS(dev-test)? namely 72.99 and 68.92. How-
8
The total number of parameter combination is 29?51?
51 = 75429.
631
ever, in this parameter combination the scores on
Twitter data were clearly lower than the submis-
sion run. Finally, ?SMS(dev-test)+Twitter(dev-
test)? resulted to a mid performing result, where
scores for each source marked in-between values
of ?Twitter(dev-test)? and ?SMS(dev-test)?.
5 Conclusion
We proposed a system that is designed to enhance
information based on lexical knowledge and to
be adjustable to unbalanced training data. With
parameters tuned towards Twitter data, the sys-
tem successfully achieved high scoring results on
Twitter data, average F
1
70.96 on Twitter2014 and
average F
1
56.50 on Twitter2014Sarcasm.
Additional test runs with different parameter
combination showed that the system can be tuned
to perform well on non-Twitter data such as blogs
or short messages. However, the limitation of our
approach to directly weight a machine learner?s
output was shown, since we could not find a
general purpose parameter combination that can
achieve high scores on any types of data.
Acknowledgements
We would like to thank the anonymous reviewers
for their valuable comments to improve this paper.
References
Eneko Agirre and Aitor Soroa. 2009. Personalizing
PageRank for word sense disambiguation. In Pro-
ceedings of EACL 2009, pages 33?41.
Cem Akkaya, Janyce Wiebe, and Rada Mihalcea.
2009. Subjectivity word sense disambiguation. In
Proceedings of EMNLP 2009, pages 190?199.
Enrique Amig?o, Jorge Carrillo de Albornoz, Irina
Chugur, Adolfo Corujo, Julio Gonzalo, Tamara
Mart??n, Edgar Meij, Maarten de Rijke, and Damiano
Spina. 2013. Overview of RepLab 2013: Evaluat-
ing online reputation monitoring systems. In CLEF
2013 Evaluation Labs and Workshop, Online Work-
ing Notes.
Stefano Baccianella, Andrea Esuli, and Fabrizio Sebas-
tiani. 2010. SentiWordNet 3.0: An enhanced lexical
resource for sentiment analysis and opinion mining.
In Proceedings of LREC 2010, pages 2200?2204.
Rong-En Fan, Kai-Wei Chang, Cho-Jui Hsieh, Xiang-
Rui Wang, and Chih-Jen Lin. 2008. LIBLINEAR:
A library for large linear classification. In Journal of
Machine Learning Research, volume 9, pages 1871?
1874.
Christiane Fellbaum, editor. 1998. WordNet: An Elec-
tronic Lexical Database. MIT Press.
Guido Minnen, John Carroll, and Darren Pearce. 2001.
Applied morphological processing of English. Nat-
ural Language Engineering, 7(3):207?223.
Saif Mohammad, Svetlana Kiritchenko, and Xiaodan
Zhu. 2013. NRC-Canada: Building the state-of-the-
art in sentiment analysis of tweets. In Proceedings
of the seventh international workshop on Semantic
Evaluation Exercises (SemEval-2013), pages 321?
327.
Preslav Nakov, Sara Rosenthal, Zornitsa Kozareva,
Veselin Stoyanov, Alan Ritter, and Theresa Wilson.
2013. SemEval-2013 task 2: Sentiment analysis
in Twitter. In Proceedings of the seventh interna-
tional workshop on Semantic Evaluation Exercises
(SemEval-2013), pages 312?320.
Finn
?
Arup Nielsen. 2011. A new ANEW: Evalu-
ation of a word list for sentiment analysis in mi-
croblogs. In Proceedings of the ESWC2011 Work-
shop on ?Making Sense of Microposts?: Big things
come in small packages, pages 93?98.
Olutobi Owoputi, Brendan O?Connor, Chris Dyer,
Kevin Gimpel, Nathan Schneider, and Noah A.
Smith. 2013. Improved part-of-speech tagging for
online conversational text with word clusters. In
Proceedings of NAACL 2013, pages 380?390.
Sara Rosenthal, Alan Ritter, Preslav Nakov, and
Veselin Stoyanov. 2014. SemEval-2014 task 9:
Sentiment analysis in Twitter. In Proceedings of the
eighth international workshop on Semantic Evalua-
tion Exercises (SemEval-2014).
Philip Stone, Dexter Dunphy, Marshall Smith, and
Daniel Ogilvie. 1966. General Inquirer: A Com-
puter Approach to Content Analysis. MIT Press.
Kristina Toutanova, Dan Klein, Christopher Manning,
and Yoram Singer. 2003. Feature-rich part-of-
speech tagging with a cyclic dependency network.
In Proceedings of HLT-NAACL 2003, pages 252?
259.
Joseph Turian, Lev Ratinov, and Yoshua Bengio. 2010.
Word representations: A simple and general method
for semi-supervised learning. In Proceedings of
ACL 2010, pages 384?394.
Julio Villena-Rom?an and Janine Garc??a-Morera. 2013.
TASS 2013 - Workshop on sentiment analysis at SE-
PLN 2013: An overview. In Proceedings of the
TASS workshop at SEPLN 2013.
Theresa Wilson, Janyce Wiebe, and Paul Hoffmann.
2005. Recognizing contextual polarity in phrase-
level sentiment analysis. In Proceedings of HLT-
EMNLP 2005, pages 347?354.
632
Proceedings of the Second Workshop on NLP Challenges in the Information Explosion Era (NLPIX 2010), pages 75?83,
Beijing, August 2010
Adverse?Effect Relations Extraction from  
Massive Clinical Records 
Yasuhide Miura a, Eiji Aramaki b, Tomoko Ohkuma a, Masatsugu Tonoike a,  
Daigo Sugihara a, Hiroshi Masuichi a and Kazuhiko Ohe c 
     a  Fuji Xerox Co., Ltd.
       b Center for Knowledge Structuring, University of Tokyo
         c University of Tokyo Hospital
yasuhide.miura@fujixerox.co.jp, eiji.aramaki@gmail.com, 
{ohkuma.tomoko,masatsugu.tonoike,daigo.sugihara, 
hiroshi.masuichi}@fujixerox.co.jp,  
kohe@hcc.h.u-tokyo.ac.jp 
 
Abstract 
The rapid spread of electronic health 
records raised an interest to large-scale 
information extraction from clinical 
texts. Considering such a background, 
we are developing a method that can 
extract adverse drug event and effect 
(adverse?effect) relations from massive 
clinical records. Adverse?effect rela-
tions share some features with relations 
proposed in previous relation extrac-
tion studies, but they also have unique 
characteristics. Adverse?effect rela-
tions are usually uncertain. Not even 
medical experts can usually determine 
whether a symptom that arises after a 
medication represents an adverse?
effect relation or not. We propose a 
method to extract adverse?effect rela-
tions using a machine-learning tech-
nique with dependency features. We 
performed experiments to extract ad-
verse?effect relations from 2,577 clini-
cal texts, and obtained F1-score of 
37.54 with an optimal parameters and 
F1-score of 34.90 with automatically 
tuned parameters. The results also 
show that dependency features increase 
the extraction F1-score by 3.59.  
1 Introduction  
The widespread use of electronic health rec-
ords (EHR) made clinical texts to be stored as 
computer processable data. EHRs contain im-
portant information about patients? health. 
However, extracting clinical information from 
EHRs is not easy because they are likely to be 
written in a natural language. 
We are working on a task to extract adverse 
drug event and effect relations from clinical 
records. Usually, the association between a 
drug and its adverse?effect relation is investi-
gated using numerous human resources, cost-
ing much time and money. The motivation of 
our task comes from this situation. An example 
of the task is presented in Figure 1. We defined 
an adverse?effect relation as a relation that 
holds between a drug entity and a symptom 
entity. The sentence illustrates the occurrence 
of the adverse?effect hepatic disorder by the 
Singulair medication.  
 
Figure 1. Example of an adverse?effect relation. 
A hepatic disorder found was suspected drug-induced and the Singulair was stopped.
adverse?effect relation
symptom drug
75
A salient characteristic of adverse?effect re-
lations is that they are usually uncertain. The 
sentence in the example states that the hepatic 
disorder is suspected drug-induced, which 
means the hepatic disorder is likely to present 
an adverse?effect relation. Figure 2 presents an 
example in which an adverse?effect relation is 
suspected, but words to indicate the suspicion 
are not stated. The two effects of the drug??the 
recovery of HbA1c and the appearance of the 
edema??are expressed merely as observation 
results in this sentence. The recovery of 
HbA1c is an expected effect of the drug and 
the appearance of the edema probably repre-
sents an adverse?effect case. The uncertain 
nature of adverse?effect relations often engen-
ders the statement of an adverse?effect rela-
tion as an observed fact. A sentence includ-
ing an adverse?effect relation occasionally be-
comes long to list all observations that ap-
peared after administration of a medication. 
Whether an interpretation that expresses an 
adverse?effect relation, such as drug-induced 
or suspected to be an adverse?effect, exists in a 
clinical record or not depends on a person who 
writes it. However, an adverse?effect relation 
is associated with an undesired effect of a 
medication. Its appearance would engender an 
extra action (e.g. stopped in the first example) 
or lead to an extra indication (e.g. but ? ap-
peared in the second example). Proper han-
dling of this extra information is likely to boost 
the extraction accuracy. 
The challenge of this study is to capture re-
lations with various certainties. To establish 
this goal, we used a dependency structure for 
the adverse?effect relation extraction method. 
Adverse?effect statements are assumed to 
share a dependency structure to a certain 
degree. For example, if we obtain the depend-
ency structures as shown in Figure 3, then we 
can easily determine that the structures are 
similar. Of course, obtaining such perfect pars-
ing results is not always possible. A statistical 
syntactic parser is known to perform badly if a 
text to be parsed belongs to a domain which 
differs from a domain on which the parser is 
trained (Gildea, 2001). A statistical parser will 
likely output incomplete results in these texts 
and will likely have a negative effect on rela-
tion extraction methods which depend on it. 
The specified research topic of this study is to 
investigate whether incomplete dependency 
structures are effective and how they behave in 
the extraction of uncertain relations.  
Figure 2. The example of an adverse-effect relation where the suspicion is not stated. 
Figure 3. The example of a similarity within dependency structures. 
ACTOS 30 recovered HbA1c to 6.5% but an edema appeared after the medication.
A suspected drug-induced hepatic disorder found and the Singulair was stopped.
conjunct
nominal subject nominal subject
nominal subject nominal subject
conjunct
was
ACTOS 30 recovered HbA1c to 6.5% but an edema appeared after the medication.
adverse-effect relation
drug symptom
76
2 Related Works 
Various studies have been done to extract se-
mantic information from texts. SemEval-2007 
Task:04 (Girju et al, 2007) is a task to extract 
semantic relations between nominals. The task 
includes ?Cause?Effect? relation extraction, 
which shares some similarity with a task that 
will be presented herein. Saeger et al (2008) 
presented a method to extract potential trou-
bles or obstacles related to the use of a given 
object. This relation can be interpreted as a 
more general relation of the adverse?effect 
relation. The protein?protein interaction (PPI) 
annotation extraction task of BioCreative II 
(Krallinger et al, 2008) is a task to extract PPI 
from PubMed abstracts. BioNLP?09 Shared 
Task on Event Extraction (Kim et al, 2009) is 
a task to extract bio-molecular events (bio-
events) from the GENIA event corpus.  
Similar characteristics to those of the ad-
verse?effect relation are described in previous 
reports in the bio-medical domain. Friedman et 
al. (1994) describes the certainty in findings of 
clinical radiology. Certainty is also known in 
scientific papers of biomedical domains as 
speculation (Light et al, 2004). Vincze et al 
(2008) are producing a freely available corpus 
including annotations of uncertainty along with 
its scope. 
Dependency structure feature which we uti-
lized to extract adverse?effect relations are 
widely used in relation extraction tasks. We 
present previous works which used syntac-
tic/dependency information as a feature of a 
statistical method. Beamer et al (2007), Giuli-
ano et al (2007), and Hendrickx et al (2007) 
all used syntactic information with machine 
learning techniques in SemEval-2007 Task:04 
and achieved good performance. Riedel et al 
(2009) used dependency path features with a 
statistical relational learning method in Bi-
oNLP?09 Shared Task on Event Extraction and 
achieved the best performance in the event en-
richment subtask. Miyao et al (2008) com-
pared syntactic information of various statisti-
cal parsers on PPI. 
3 Corpus  
We produced an annotated corpus of adverse?
effect relations to develop and test an adverse?
effect relation extraction method. This section 
presents a description of details of the corpus. 
3.1 Texts Comprising the Corpus 
We used a discharge summary among various 
documents in a hospital as the source data of 
the task. The discharge summary is a docu-
ment created by a doctor or another medical 
expert at the conclusion of a hospital stay. 
Medications performed during a stay are writ-
ten in discharge summaries. If adverse?effect 
relations were observed during the stay, they 
are likely to be expressed in free text. Texts 
written in discharge summaries tend to be writ-
ten more roughly than texts in newspaper arti-
cles or scientific papers. For example, the 
amounts of medications are often written in a 
name-value list as shown below: 
?When admitted to the hospital, Artist 6 mg1x, 
Diovan 70 mg1x, Norvasac 5 mg1x and BP 
was 145/83, but after dialysis, BP showed a 
decreasing tendency and in 5/14 Norvasac was 
reduced to 2.5 mg1x.? 
3.2 Why Adverse?Effect Relation Extrac-
tion from Discharge Summaries is 
Important 
In many countries, adverse?effects are investi-
gated through multiple phases of clinical trials, 
but unexpected adverse?effects occur in actual 
medications. One reason why this occurs is 
that drugs are often used in combination with 
others in actual medications. Clinical trials 
usually target single drug use. For that reason, 
the combinatory uses of drugs occasionally 
engender unknown effects. This situation natu-
rally motivates automatic adverse?effect rela-
tion extraction from actual patient records.  
  
77
 3.3 Corpus Size 
We collected 3,012 discharge summaries1 writ-
ten in Japanese from all departments of a hos-
pital. To reduce a cost to survey the occurrence 
of adverse?effects in the summaries, we first 
split the summaries into two sets: SET-A, 
which contains keywords related to adverse?
effects and SET-B, which do not contain the 
keywords. The keywords we used were ?stop, 
change, adverse effect?, and they were chosen 
based on a heuristic. The keyword filtering 
resulted to SET-A with 435 summaries and 
SET-B with 2,577 summaries. Regarding SET-
A, we randomly sampled 275 summaries and 
four annotators annotated adverse?effect in-
formation to these summaries to create the ad-
verse?effect relation corpus. For SET-B, the 
four annotators checked the small portion of 
the summaries. Cases of ambiguity were re-
solved through discussion, and even suspicious 
adverse?effect relations were annotated in the 
corpus as positive data. The overview of the 
summary selection is presented in Figure 4.  
                                                 
1 All private information was removed from them. 
The definition of private information was referred 
from the HIPAA guidelines. 
3.4 Quantities of Adverse?Effects in Clin-
ical Texts 
55.6% (=158/275) of the summaries in SET-A 
contained adverse?effects. 11.3% (=6/53) of 
the summaries in SET-B contained adverse?
effects. Since the ratio of SET-A:SET-B is 
14.4:85.6, we estimated that about 17.7%  
(=0.556?0.144+0.113?0.856) of the summar-
ies contain adverse?effects. Even considering 
that a summary may only include suspected 
adverse?effects, we think that discharge sum-
maries are a valuable resource to explore ad-
verse?effects. 
3.5 Annotated Information 
We annotated information of two kinds to the 
corpus: term information and relation infor-
mation. 
(1) Term Annotation  
Term annotation includes two tags: a tag to 
express a drug and a tag to express a drug ef-
fect. Table 1 presents the definition. In the 
corpus, 2,739 drugs and 12,391 effects were 
annotated. 
(2) Relation Annotation  
Adverse?effect relations are annotated as the 
?relation? attribute of the term tags. We repre-
sent the effect of a drug as a relation between a 
drug tag and a symptom tag. Table 2 presents 
Table 2. Annotation examples. 
Figure 4. The overview of the summary 
selection. 
Table 1. Markup scheme. 
The expression of a disease or 
symptom: e.g. endometrial cancer, 
headache. This tag covers not only a 
noun phrase but also a verb phrase 
such as ?<symptom>feels a pain in 
front of the head</symptom>?.
symptom
The expression of an administrated 
drug: e.g. Levofloxacin, Flexeril. 
drug
Definition and Examplestag
<drug relation=?1?>ACTOS(30)</drug> brought 
both <symptom relation=?1?>headache<symptom> 
and <symptom relation=?1?>insomnia</symptom>.
<drug relation=?1?>Ridora</drug> resumed 
because it is associated with an <symptom 
relation=?1?>eczematous rash</symptom>.
* If a drug has two or more adverse-effects, 
symptoms take a same relation ID.
3,012 
discharge 
summaries
435
summaries
w/ keywords
2,577
summaries
w/o keywords
275
summaries
53
summaries
153
summaries
w/ adverse?
effects
122
summaries
w/o adverse?
effects
6
summaries
w/ adverse?
effects
47
summaries
w/o adverse?
effects
YES NO
Contain keywords?
Random samplingRandom sampling
Contain adverse?
effects?
Contain adverse?
effects?
YES YESNO NO
SET-A (annotated corpus) SET-B
78
several examples, wherein ?relation=1? de-
notes the ID of a adverse?effect relation. In the 
corpus, 236 relations were annotated.  
4 Extraction Method 
We present a simple adverse?effect relation 
extraction method. We extract drug?symptom 
pairs from the corpus and discriminate them 
using a machine-learning technique. Features 
based on morphological analysis and depend-
ency analysis are used in discrimination. This 
approach is similar to the PPI extraction ap-
proach of Miyao et al (2008), in which we 
binary classify pairs whether they are in ad-
verse?effect relations or not. A pattern-based 
semi-supervised approach like Saeger et al 
(2008), or more generally Espresso (Pantel and 
Pennacchiotti, 2006), can also be taken, but we 
chose a pair classification approach to avoid 
the effect of seed patterns. To capture a view 
of an adverseness of a drug, a statistic of ad-
verse?effect relations is important. We do not 
want to favor certain patterns and chose a pair 
classification approach to equally treat every 
relation. Extraction steps of our method are as 
presented below. 
STEP 1: Pair Extraction   
All combinations of drug?symptom pairs that 
appear in a same sentence are extracted. Pairs 
<drug relation=?1?>Lasix</drug> for 
<symptom>hyperpiesia</symptom> has 
been suspended due to the appearance of 
a <symptom relation=?1?>headache
</symptom>.
headacheLasixpositive
hyperpiesiaLasixnegative
symptomdruglabel
ID Feature Definition and Examples
1 Character Distance The number of characters between members of a pair.
2 Morpheme Distance The number of morpheme between members of a pair.
3 Pair Order Order in which a drug and a symptom appear in a text; 
?drug?symptom? or ?symptom?drug?.
4 Symptom Type The type of symptom: ?disease name?, ?medical test name?, 
or ?medical test value?. 
5 Morpheme Chain Base?forms of morphemes that appear between a pair.
6 Dependency Chain Base?forms of morphemes included in the minimal 
dependency path of a pair.
7 Case Frame Chain Verb, case frame, and object triples that appear between a 
pair: e.g. ?examine? ??de?(case particle) ? ?inhalation?, 
?begin? ??wo?(case particle) ??medication?.
8 Case Frame 
Dependency Chain
Verb, case frame, and object triples included in the minimal 
dependency path of a pair.
Figure 6. Dependency chain example. 
 
Figure 5. Pair extraction example. 
hyperpiesia no-PP
for no-PP
Lasix wo-PP
headache no-PP 
appear niyori-PP
suspend ta-AUX
Lasix, wo-PP, headache, no-PP, 
appear, niyori-PP, suspend, ta-AUX
minimal path
Table 3. Features used in adverse-effect extraction. 
79
with the same relation ID become positive 
samples; pairs with different relation IDs be-
come negative samples. Figure 5 shows exam-
ples of positive and negative samples.  
STEP 2: Feature Extraction  
Features presented in Table 3 are extracted. 
The text in the corpus is in Japanese. Some 
features assume widely known characteristics 
of Japanese. For example, the dependency fea-
ture allows a phrase to depend on only one 
phrase that appears after a dependent phrase. 
Figure 6 portrays an example of a dependency 
chain feature. In the example, most terms were 
translated into English, excluding postpositions 
(PP) and auxiliaries (AUX), which are ex-
pressed in italic. To reduce the negative effect 
of feature sparsity, features which appeared in 
more than three summaries are used for fea-
tures with respective IDs 5?8. 
STEP 3: Machine Learning  
The support vector machine (SVM) (Vapnik, 
1995) is trained using positive/negative labels 
and features extracted in prior steps. In testing,                                          
an unlabeled pair is given a positive or nega-
tive label with the trained SVM.  
5 Experiment 
We performed two experiments to evaluate the 
extraction method. 
5.1 Experiment 1 
Experiment 1 aimed to observe the effects of 
the presented features. Five combinations of 
the features were evaluated with a five-fold 
cross validation assuming that an optimal pa-
rameter combination was obtained. The exper-
iment conditions are described below: 
A. Data  
7,690 drug?symptom pairs were extracted 
from the corpus.  Manually annotated infor-
mation was used to identify drugs and symp-
toms. Within 7,690 pairs, 149 pairs failed to 
extract the dependency chain feature. We re-
moved these 149 pairs and used the remaining 
7,541 pairs in the experiment. The 7,541 pairs 
consisted of 367 positive samples and 7,174 
negative samples.  
B. Feature Combinations  
We tested the five combinations of features in 
the experiment. Manually annotated infor-
mation was used for the symptom type feature. 
Features related to morphemes are obtained by 
processing sentences with a Japanese mor-
phology analyzer (JUMAN2 ver. 6.0). Features 
related to dependency and case are obtained by 
processing sentences using a Japanese depend-
ency parser (KNP ver. 3.0; Kurohashi and Na-
gao, 1994).  
C. Evaluations  
We evaluated the extraction method with all 
combinations of SVM parameters in certain 
                                                 
2 http://www-lab25.kuee.kyoto-u.ac.jp/nl-
resource/juman-e.html 
E
D
C
B
A
ID
35.45
35.01
34.39
33.30
26.72
Precision
41.05
40.67
43.06
42.43
46.21
Recall
log(c)=1.0, log(g)=-5.0, p=0.10
log(c)=1.0, log(g)=-5.0, p=0.10
log(c)=1.0, log(g)=-5.0, p=0.10
log(c)=1.0, log(g)=-5.0, p=0.10
log(c)=3.0, log(g)=-5.0, p=0.10
Parameters
37.181,2,3,4,5,6,7,8
36.781,2,3,4,5,6,8
37.541,2,3,4,5,6,7
36.641,2,3,4,5,6
33.051,2,3,4,5
F1-scoreFeature 
Combination
Table 4. Best F1-scores and their parameters. 
Figure 7. Precision?recall distribution. 
80
ranges. We used LIBSVM3 ver. 2.89 as an im-
plementation of SVM. The radial basis func-
tion (RBF) was used as the kernel function of 
SVM. The probability estimates option of 
LIBSVM was used to obtain the confidence 
value of discrimination.  
The gamma parameter of the RBF kernel 
was chosen from the range of [2-20, 20]. The C 
parameter of SVM was chosen from the range 
of [2-10, 210]. The SVM was trained and tested 
on 441 combinations of gamma and C. In test-
ing, the probability threshold parameter p be-
tween [0.05, 0.95] was also chosen, and the F1-
scores of all combination of gamma, C, and p 
were calculated with five-fold cross validation. 
The best F1-scores and their parameter values 
for each combination of features (optimal F1-
scores in this setting) are portrayed in Table 4. 
The precision?recall distribution of F1-scores 
with feature combination C is presented in 
Figure 7.  
5.2 Experiment 2 
Experiment 2 aimed to observe the perfor-
mance of our extraction method when SVM 
parameters were automatically tuned. In this 
experiment, we performed two cross valida-
tions: a cross validation to tune SVM parame-
ters and another cross validation to evaluate 
the extraction method. The experiment condi-
tions are described below:  
A. Data 
The same data as Experiment 1 were used. 
B. Feature Combination  
Feature combination C, which performed best 
in Experiment 1, was used.  
C. Evaluation  
                                                 
3 http://www.csie.ntu.edu.tw/~cjlin/libsvm 
Two five-fold cross validations were per-
formed. The first cross validation divided the 
data to 5 sets (A, B, C, D, and E) each consist-
ing of development set and test set with the 
ratio of 4:1.  The second cross validation train 
and test all combination of SVM parameters (C, 
gamma, and p) in certain ranges and decide the 
optimal parameter combination(s) for  the de-
velopment sets of A, B, C, D, and E. The se-
cond cross validation denotes the execution of 
Experiment 1 for each development set.  For 
each optimal parameter combination of A, B, 
C, D, and E, the corresponding development 
set was trained and the trained model was test-
ed on the corresponding test set. The average 
F1-score on five test sets marked 34.90, which 
is 2.64 lower than the F1-score of Experiment 1 
with the same feature combination. 
6 Discussion 
The result of the experiment reveals the effec-
tiveness of the dependency chain feature and 
the case-frame chain feature. This section pre-
sents a description of the effects of several fea-
tures in detail. The section also mentions re-
maining problems in our extraction method.  
6.1 Effects of the Dependency Chain Fea-
ture and Case-frame Features  
A. Dependency Chain Feature  
The dependency chain features improved the 
F1-score by 3.59 (the F1-score difference be-
tween feature combination A and B). This in-
crease was obtained using 260 improved pairs 
and 127 deproved pairs. Improved pairs con-
Figure 8. Relation between the number of 
pairs and the morpheme distance. 
Figure 9. Number of dependency errors 
in the improved pairs sentences. 
25
93
23
sentence
with no error
sentence
with 1?3
errors
sentence
with 4 or
more errors
0
10
20
30
40
50
distance less 
than 40
distance larger than
or equal to 40
fre
qu
en
cy
improved 
deproved
81
tribute to the increase of a F1-score. Deproved 
pairs have the opposite effect. 
We observed that improved pairs tend to 
have longer morpheme distance compared to 
deproved pairs. Figure 8 shows the relation 
between the number of pairs and the mor-
pheme distance of improved pairs and de-
proved pairs. The ratio between the improved 
pairs and the deproved pairs is 11:1 when the 
distance is greater than 40.  In contrast, the 
ratio is 2:1 when the distance is smaller than 
40. This observation suggests that adverse?
effect relations share dependency structures to 
a certain degree.  
We also observed that in improved pairs, 
dependency errors tended to be low. Figure 9 
presents the manually counted number of de-
pendency errors in the 141 sentences in which 
the 260 improved pairs exist: 65.96 % of the 
sentences included 1?3 errors. The result sug-
gests that the dependency structure is effective 
even if it includes small errors.  
B. Case-frame Features  
The effect of the case-frame dependency chain 
feature differed with the effect of the depend-
ency chain feature. The case-frame chain fea-
ture improved the F1-score by 0.90 (the F1-
score difference between feature combination 
B and C), but the case-frame dependency chain 
feature decreased the F1-score by 0.36 (the F1-
score difference between feature combination 
C and E). One reason for the negative effect of 
the case-frame dependency feature might be 
feature sparsity, but no clear evidence of it has 
been found.  
6.2 Remaining Problems 
A. Imbalanced Data  
The adverse?effect relation pairs we used in 
the experiment were not balanced. Low values 
of optimal probability threshold parameter p 
suggest the degree of imbalance. We are con-
sidering introduction of some kind of method-
ology to reduce negative samples or to use a 
machine learning method that can accommo-
date imbalanced data well.  
B. Use of Medical Resources  
The extraction method we propose uses no 
medical resources. Girju et al (2007) indicate 
the effect of WordNet senses in the classifica-
tion of a semantic relation between nominals. 
Krallinger et al (2008) report that top scoring 
teams in the interaction pair subtask used so-
phisticated interactor protein normalization 
strategies. If medical terms in texts can be 
mapped to a medical terminology or ontology, 
it would likely improve the extraction accuracy.  
C. Fully Automated Extraction 
In the experiments, we used the manually 
annotated information to extract pairs and fea-
tures. This setting is, of course, not real if we 
consider a situation to extract adverse?effect 
relations from massive clinical records, but we 
chose it to focus on the relation extraction 
problem. We performed an event recognition 
experiment (Aramaki et al, 2009) and 
achieved F1-score of about 80. We assume that 
drug expressions and symptom expressions to 
be automatically recognized in a similar accu-
racy.  
We are planning to perform a fully automat-
ed adverse?effect relations extraction from a 
larger set of clinical texts to see the perfor-
mance of our method on a raw corpus. The 
extraction F1-score will likely to decrease, but 
we intend to observe the other aspect of the 
extraction, like the overall tendency of extract-
ed relations.  
7 Conclusion 
We presented a method to extract adverse?
effect relations from texts. One important 
characteristic of adverse?effect relations is that 
they are uncertain in most cases. We per-
formed experiments to extract adverse?effect 
relations from 2,577 clinical texts, and ob-
tained F1-score of 37.54 with optimal SVM 
parameters and F1-score of 34.90 with auto-
matically tuned SVM parameters. Results also 
show that dependency features increase the 
extraction F1-score by 3.59. We observed that 
an increased F1-score was obtained using the 
improvement of adverse?effects with long 
morpheme distance, which suggests that ad-
verse?effect relations share dependency struc-
tures to a certain degree. We also observed that 
the increase of the F1-score was obtained with 
dependency structures that include small errors, 
which suggests that the dependency structure 
is effective even if it includes small errors. 
  
82
References 
Aramaki, Eiji, Yasuhide Miura, Masatsugu Tonoike, 
Tomoko Ohkuma, Hiroshi Masuichi, and 
Kazuhiko Ohe. 2009. TEXT2TABLE: Medical 
Text Summarization System Based on Named 
Entity Recognition and Modality Identification. 
In Proceedings of the BioNLP 2009 Workshop, 
pages 185-192. 
Beamer, Brandon, Suma Bhat, Brant Chee, Andrew 
Fister, Alla Rozovskaya, and Roxana Girju. 
2007. UIUC: A Knowledge-rich Approach to 
Identifying Semantic Relations between Nomi-
nals. In Proceedings of Fourth International 
Workshop on Semantic Evaluations, pages 386-
389. 
Friedman, Carol, Philip O. Alderson, John H. M. 
Austin, James J. Cimino, and Stephen B. John-
son. 1994. A General Natural-language Text 
Processor for Clinical Radiology. Journal of the 
American Medical Informatics Association, 1(2), 
pages 161-174. 
Gildea, Daniel. 2001. Corpus Variation and Parser 
Performance. In Proceedings of the 2001 Con-
ference on Empirical Methods in Natural Lan-
guage Processing, pages 1-9. 
Girju, Roxana, Preslav Nakov, Vivi Nastase,  Stan 
Szpakowicz, Peter Turney, and Deniz Yuret.  
2007. SemEval-2007 task 04: Classification of 
Semantic Relations between Nominals. In Pro-
ceedings of Fourth International Workshop on 
Semantic Evaluations, pages 13-18. 
Giuliano, Claudio, Alberto Lavelli, Daniele Pighin, 
and Lorenza Romano. 2007. FBK-IRST: Kernel 
Methods for Semantic Relation Extraction. In 
Proceedings of the 4th International Workshop 
on Semantic Evaluations, pages 141-144.  
Hendrickx , Iris, Roser Morante, Caroline Sporleder, 
and Antal van den Bosch. 2007. ILK: Machine 
learning of semantic relations with shallow fea-
tures and almost no data. In Proceedings of the 
4th International Workshop on Semantic Evalua-
tions, 187-190. 
Kim, Jin-Dong, Tomoko Ohta, Sampo Pyysalo, 
Yoshinobu Kano, and Jun?ichi Tsujii. 2009. 
Overview of  BioNLP?09 Shared Task on Event 
Extraction. In Proceedings of the BioNLP 2009 
Workshop Companion Volume for Shared Task, 
pages 1-9. 
Krallinger, Martin, Florian Leitner, Carlos  
Rodriguez-Penagos, and Alfonso Valencia. 2008.  
Overview of the protein-protein interaction an-
notation extraction task of BioCreative II. Ge-
nome Biology 2008, 9(Suppl 2):S4. 
Kurohashi, Sadao and Makoto Nagao. 1994. KN 
Parser : Japanese Dependency/Case Structure 
Analyzer. In Proceedings of The International 
Workshop on Sharable Natural Language Re-
sources, pages 22-28. Software available at 
http://www-lab25.kuee.kyoto-u.ac.jp/nl-
resource/knp-e.html. 
Light, Marc, Xin Ying Qiu, and Padmini Srinivasan. 
2004. The Language of Bioscience: Facts, Spec-
ulations, and Statements in Between. In Pro-
ceedings of HLT/NAACL 2004 Workshop: Bi-
oLINK 2004, Linking Biological Literature, On-
tologies and Databases, pages 17-24. 
Miyao, Yusuke, Rune S?tre, Kenji Sagae, Takuya 
Matsuzaki, and Jun'ichi Tsujii. 2008. Task-
oriented Evaluation of Syntactic Parsers and 
Their Representations. In Proceedings of the 
46th Annual Meeting of the Association for 
Computational Linguistics: Human Language 
Technologies, pages 46-54. 
Pantel, Patrick and Marco Pennacchiotti. 2006. Es-
presso: Leveraging Generic Patterns for Auto-
matically Harvesting Semantic Relations. In 
Proceedings of the 21st International Confer-
ence on Computational Linguistics and 44th An-
nual Meeting of the Association for Computa-
tional Linguistics, pages 113-120. 
Riedel, Sebastian, Hong-Woo Chun, Toshihisa 
Takagi, and Jun'ichi Tsujii. 2009. A Markov 
Logic Approach to Bio-Molecular Event Extrac-
tion. In Proceedings of the BioNLP 2009 Work-
shop Companion Volume for Shared Task, pages 
41-49. 
Saeger, Stijn De, Kentaro Torisawa, and Jun?ichi 
Kazama. 2008. Looking for Trouble. In Proceed-
ings of the 22nd International Conference on 
Computational Linguistics, pages 185-192. 
Vapnik, Vladimir N.. 1995. The Nature of Statisti-
cal Learning Theory. Springer-Verlag New York, 
Inc.. 
Vincze, Veronika, Gy?rgy Szarvas, Rich?rd Farkas, 
Gy?rgy M?ra, and J?nos Csirik. 2008.  The Bio-
Scope corpus: biomedical texts annotated for un-
certainty, negation and their scopes. BMC Bioin-
formatics 2008, 9(Suppl 11):S9.  
 
83
Proceedings of the 25th International Conference on Computational Linguistics, pages 54?61,
Dublin, Ireland, August 23-29 2014.
Twitter User Gender Inference Using Combined Analysis                     
of Text and Image Processing 
 
Shigeyuki Sakaki, Yasuhide Miura, Xiaojun Ma, Keigo Hattori, and Tomoko Ohkuma  
Fuji Xerox Co., Ltd. / Japan 
6-1, Minatomirai, Nishi-ku, Yokohama-shi, Kanagawa 
{sakaki.shigeyuki, yasuhide.miura, xiaojun.ma, keigo.hattori, 
ohkuma.tomoko}@fujixerox.co.jp 
Abstract 
Profile inference of SNS users is valuable for marketing, target advertisement, and opinion polls. Sev-
eral studies examining profile inference have been reported to date. Although information of various 
types is included in SNS, most such studies only use text information. It is expected that incorporating 
information of other types into text classifiers can provide more accurate profile inference. As de-
scribed in this paper, we propose combined method of text processing and image processing to im-
prove gender inference accuracy. By applying the simple formula to combine two results derived from 
a text processor and an image processor, significantly increased accuracy was confirmed. 
1 Introduction 
Recently, several researches on profile inference of Social Networking Services (SNS) user conducted 
by analyzing postings have been reported (Rao and Yarowsky, 2010; Han et al., 2013; Makazhanov et 
al., 2013). User profile information such as gender, age, residential area, and political preference have 
attracted attention because they are helpful for marketing, target advertisement, TV viewer rate calcu-
lations, and opinion polls. The major approach to this subject is building a machine learning classifier 
trained by text in postings. However, images posted by a user are rarely used in profile inference. Im-
ages in postings also include features of user profiles. For example, if a user posts many dessert imag-
es, then the user might be female. Therefore, we assumed that highly accurate profile inference will be 
available by analyzing image information and text information simultaneously. 
As described in this paper, we implement gender inference of Japanese Twitter user using text in-
formation and image information. We propose a combined method consisting of text processing and 
image processing, which accepts tweets as input data and outputs a gender probability score. The 
combined method comprises of two steps: step 1) two gender probability scores are inferred respec-
tively by a text processor and an image processor; step 2) the combined score is calculated by merging 
two gender scores with an appropriate ratio. This report is the first describing an attempt to apply the 
combined method of text processing and image processing to profile inference of an SNS user. 
This paper is presented as seven sections: section 2 presents a description of prior work; section 3 
presents a description of the annotation data prepared for this study; section 4 introduces the proposed 
method; section 5 explains preliminary experiments for optimizing the combined method parameter; 
section 6 presents the experimentally obtained result; section 7 summarizes the paper and discusses 
future work. 
2 Prior Work 
Many reports have described studies examining gender inference. The conventional approach to 
this theme is building a machine learning classifier such as Support Vector Machine (SVM) trained by 
text features (Burger et al., 2011; Liu et al., 2012). Most of these studies specifically examine im-
provement of the machine classification methodology rather than expanding features or combining 
features. Different from these studies, Liu et al. (2013) implemented gender inference with incorpora-
tion of a user name into the classifier based on text information. However, the expansion of features 
This work is licensed under a Creative Commons Attribution 4.0 International License. Page numbers and pro-
ceedings footer are added by the organisers. License details: http://creativecommons.org/licenses 
/by/4.0/ 
 
54
 remains in the text field. 
A few reports in the literature describe studies of systems that infer the SNS user gender with in-
formation aside from the text. Ikeda et al. (2013) leverages the accuracy of profile inference based on a 
text feature classifier by combining user cluster information. According to their study, the accuracy of 
classification that deals only with the user cluster is lower than that of the text classifier. The classifier 
using both text and cluster information of a user outperforms their text classifier. This research shows 
that information aside from the text is useful to leverage the performance of profile inference based on 
text and text information is necessary to achieve high accuracy. However, we introduce image infor-
mation that is not used by Ikeda et al (2013). 
Along with text information and cluster information, images are popular informative elements that 
are included in SNS postings. An image includes enough information to infer what is printed in itself, 
and researches to automatically annotate an image with semantic labels are already known (Zhang et 
al., 2012). Automatic image annotation is a machine learning technique that involves a process by 
which a computer system automatically assigns semantic labels to a digital image.  These studies suc-
ceeded in inferences of various objects, such as person, dog, bicycle, chair etc. We supposed that such 
objects in images posted by a user should be useful clues as to a profile inference of a twitter user. As 
a matter of fact, gender inference by image information is reported by Ma et al. (2014), which imple-
mented gender inference by processing images in tweets. Their study, which ignored text information, 
exhibited accuracy of less than 70%. It was much lower than most gender inference work using text 
feature. 
From results of these studies, we concluded that gender inference by text and image information 
invites further study. 
3 Proposed Method 
Our proposed method for combining text processing and image processing is presented in Figure 1. 
First, data of 200 tweets of a user are separated into text data and image data. Each of separated data is 
analyzed using a dedicated processor, a text processor, and an image processor. Both of the processors 
 
 
Figure 1. Combined method constitution. 
text
Gender classifier
G nd probability
c r  of a user
Image
Combined method
Combined gender probability score of a user
Text Processor Image Processor
Gender probability score of a user
Object Classifiers 
Object probability scores
Consolidation of scores
Food-male 
classifiers
Food-female 
classifiers
Pet-female 
classifiers
Toy male 
classifiers
Food-male
probability
score of an image
Food-female
probability
score of an image
Toy-male
probability
score of an image
Pet-female
probability
score of an image
200 tweets
posted by a user
55
 output a user?s gender probability score, the upper/lower ends of which respectively correspond to 
male and female labels. At the end of this process, the combined gender probability score is calculated 
using two probability scores. In this section, details of the two processors and the method of combin-
ing their two results are described. 
3.1 Text Processing 
The text processor is constructed from a text classifier, which accepts text data in tweets and outputs 
the gender probability score of a user. We defined the gender classifier in the text processor as an 
SVM binary classification of a male and female. The SVM classifier is trained based on unigram Bag-
of-words with a linear kernel. The cost parameter C is set to 1.0. Then LIBSVM (Chang and Lin, 
2001) is used as an implementation of SVM. Because words are not divided by spaces in a Japanese 
sentence, Kuromoji (Atilika, 2011), a morphological analysis program for Japanese, is used to obtain 
unigrams. 
To combine two results from the text processor and the image processor, it is necessary to calculate 
each result as a probability value. To retrieve probability scores, we used logistic regression. Logistic 
function converts a distance from a hyper plane to probability scores of 0.0?1.0. The text classifier is a 
male and female binary classification. Therefore, the upper and lower ends of the probability score 
respectively correspond to male and female data. If a score is close to 0.0, then the user has high prob-
ability of being male. If it is close to 1.0, then a user is probably female. 
3.2 Image Processing 
We first tried to infer a Twitter user gender directly by a two-class classifier trained by image feature 
vector calculated by all images posted by a user. However, with some preliminary experiments, we 
found that this approach does not work well, since the large variation of objects made the classifica-
tion difficult with single classifier setting. We, therefore, used the image processing method described 
by Ma et al. (2014) which uses automatic image annotation classifiers (Zhang et al., 2012) to model 
human recognition of different gender tendency in images. The method consists of two steps: step 1) 
annotating images by an image annotation technique at the image level; step 2) consolidating gender 
scores according to annotation results at the user level. 
In the first step, the image labels are defined as the combination of the following two information: 
the gender tendency in images of a user and the objects that images express. Ma et al. (2014) defined 
10 categories of objects in SNS images based on observation on a real dataset. The defined labels are 
cartoon/illustration, famous person, food, goods, memo/leaflet, outdoor/nature, person, pet, screen-
shot/capture, and other. They also indicated that gender tendency in images are coherent with user 
gender, and set three gender labels, male, female, and unknown, for each object label. As a result, 30 
labels constructed from object label and gender label (e.g. ?male-person?) are used in this paper, 
which is described in section 4.2. Then a bag-of-features (BOF) model (Tsai, 2012; Chatfield et al., 
2011) is applied to accomplish the image annotation task. We used local descriptors of SIFT (Lowe, 
1999) and image features are encoded with a k-mean generated codebook with size of 2000. We ap-
plied LLC (Wang et al, 2010) and SPM (Lazebnik et al, 2006) to generating the final presentation of 
image features. Then, the 30 SVM classifiers are trained based on the features of training images: each 
classifier is trained per image label among one-versus-rest strategy. The SVM classifier annotates im-
ages of a user by computing scores, and logistic function is applied to the outputs of the image classi-
fiers in order to obtain probability scores. Each of 30 probability scores shows how an image is close 
to the decision boundary of a particular label. 
In the second step, we integrated the 30 scores of labels assigned to images to yield comprehensive 
scores which imply a user?s gender. Two methods are suggested for the second step. One is computing 
the average of all scores output from each classifier for each of the categories of male and female for a 
user. The other is computing the mean value of only the highest scores of every image for each of the 
categories of male and female for a user. 
3.3 Combined method of Text Processing and Image Processing 
To combine two results derived from text processing and image processing, we used the function be-
low. Scoretext and Scoreimage respectively represent gender probability scores derived from the text pro-
56
 cessor and the image processor. In the function, ? is set as a ratio of the text score and an image score 
to combine two scores appropriately. We introduced ? as a reliability ratio parameter of the scores by 
the text processor and the scores by the image processor. 
 
? ??? ????? 1imagetextcombined ScoreScoreScore  
4 Data 
We prepared user annotation data and image annotation data that we used as training data and evalua-
tion data. User annotation data are input data for the text processor, whereas image annotation data are 
for the image processor. As it is required to prepare a huge number of annotated data as a training cor-
pus, the data is annotated by Yahoo Crowd Sourcing (Yahoo! Japan, 2013). Yahoo Crowd Sourcing is 
a Japanese crowd sourcing service similar to Amazon Mechanical Turk (Amazon, 2005). Therefore 
the annotation process aims to obtain annotation based on human recognition rather than to explore 
truth about users and images of twitter. 
4.1 User Annotation Data 
We first collected Japanese Twitter users according to their streaming tweets. We ignored heavy users 
and Twitter bots. A random sampling of tweets revealed that tweets from heavy users include much 
information that is not useful for profile inference such as short representations of their actions (e.g. 
?Going to bed now? and ?Just waking up?). A Twitter bot is also classed as an uninformative user be-
cause it is a program that automatically generates tweets. During data collection, we filtered out those 
users by setting conditions shown below in Table 1. Finally, we obtained 3976 Twitter users. We 
gathered tweets on each user up to 200. By executing the processes above, we obtained tweet data of 
each user corresponding to the user?s own 200 tweets. 
To obtain gender annotation for this large dataset, we used Yahoo! Crowd Sourcing. As shown in 
Figure 2(a), we set task for every Twitter user: please infer the gender of the user who posted the 
tweets in the URL below. In this task, after reading 200 tweets of a user, the gender label of male or 
female was asked of every Twitter user. To guarantee quality reliability, annotation tasks for one Twit-
ter user were duplicated 10 times by different workers; then a majority vote of 10 annotations was cal-
culated to obtain a gold label. 
As a result of the crowd sourcing tasks, 1733 users were reported as male; 2067 users were reported 
as female. There were 176 users whose votes were split equally between male and female. We re-
Table 1. Filtering conditions used to disqualify heavy users and Twitter bots 
User Types Definition for N Criteria 
Twitter bots Number of tweets posted from Twitter clients on PC/mobile 
by a user 
N<150 
Heavy 
Number of Friends or followers of a user N>200 
Number of Tweets posted in a day by a user N>10 
 
 
(a) User annotation task                                                                                  (b) Image annotation task 
Figure 2. Annotation tasks in crowd sourcing. 
http://www.abc.com/defg/index.html
An w r: ?Male
? Female
Qu stion :
Please infer the gender of the user
 p sted the tweets in the URL below.
?
Question 2:
Please choose the word most 
suitable to express the objects 
included in the image
Answer: ? Male
? Female
?Unknown
Question 1:
Please guess the gender of the 
user who uploaded the image
Answer: ? Cartoon/Illustration
? Food
? Memo/Leaflet
? Person
? Screenshot/Capture
? Famous person
? Goods
? Outdoor/Nature
? Pet
? Others
?
?
57
 moved balanced users from the data. The male and female populations of annotation assumed users 
are 45.6% and 54.4% respectively. This gender proportion tendency is consistent with those reported 
from an earlier study showing that Twitter participants are 55% female (Heli and Piskorski, 2009; 
Burger et al., 2011). Finally, we obtained gender annotation data of 3800 users. We divided these data 
equally between training data and evaluation data: 1900 users for training data and 1900 users for 
evaluation data. 
4.2 Image Annotation Data 
We first made a user list including 1523 users. After checking tweets from these users, we extracted 
9996 images. Image annotation processes were also executed by Yahoo Crowd Sourcing. 
Our image annotation process refers to rules proposed by Ma et al. (2014). As shown in Figure 2(b), 
a worker is requested to provide responses of two kinds for every image: Q1. Please guess the gender 
of the user who uploaded the image; Q2. Please choose the word most suitable to express the objects 
included in the image. The possible responses for Q1 were male, female, and unknown. Those for Q2 
were cartoon/illustration, famous person, food, goods, memo/leaflet, outdoor/nature, person, pet, 
screenshot/capture, and other. It is sometimes difficult to infer a gender of a user solely based on one 
image. Therefore, unknown is set for Q1. From those responses we obtained multiple labels for every 
image, such as ?male-person?. To avoid influence by poor-quality workers, each image was presented 
to 10 different workers. A summation of 10 annotations was executed to obtain gold label data. 
5 Preliminary Experiments 
5.1 Image Processing 
We compared two consolidation methods, computing the average of all scores and computing the av-
erage of the highest scores for 30 object scores. We applied the two method to the training data of the 
user annotation data, and tested them on the evaluation data. Results show that the accuracy of former 
method is 60.11. That of the latter is 65.42. The reason the latter method is superior to the former one 
is probably attributable to noise reduction effects of ignoring low scores. 
5.2 Combined method of Text Processing and Image Processing 
To estimate the optimal value of ?, we conducted a preliminary experiment of the combined method 
with training data. We first prepared text and image probability scores. The text score is obtained by 
executing five-fold cross validation of the text processor for training data. We used the probability 
score derived in section 5.1 as the image score. The accuracies were, respectively, 86.23 and 65.42. 
Next, the combined formula was applied to these probability scores with moving ? from 0 to 1. Figure 
3 shows the correlation between accuracy and ?. To obtain the ? value of the peak, we executed poly-
nomial fitting to a part of the correlation curve where ? is 0.1?0.4. By differentiating this function, we 
calculated the ? value of the peak as equal to 0.244 indicated by the arrow in Figure 3. The accuracy 
reaches 86.73% at the peak, which is 0.50 pt higher than that of the text processor. 
 
Figure 3. Correlation between accuracy and ? in training data. 
(Fitting curve function is 0.9519?3-0.9129?2+0.2756?+0.8409) 
0.85
0.855
0.86
0.865
0.87
0.875
0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1
A
c
c
u
r
a
c
y
 
[
%
]
?
58
 6 Experimental Results 
6.1 Comparing the Accuracies between Three Methods 
We executed an evaluation experiment assessing the three methods: text processing, image processing 
with a selected consolidation method, and the combined method with optimized ? (0.235). Each 
method is applied to evaluation data including 1900 gender-annotated data. Table 2 presents precision, 
recall, F-measure, and accuracy obtained through the evaluation experiments. The text processing ac-
curacy achieves 84.63%, and image processing accuracy is 64.21%. The combined method achieves 
85.11% accuracy, which is 0.48 pt higher than the text processing accuracy.1 We also confirmed that 
both the male and female F-measures become higher than text processing. We concluded that signifi-
cantly increased accuracy obtained using the method combining text processing and image processing. 
 
6.2 Discussion 
We expected the optimal value of ? to be large, since the accuracy of the text processor is explicitly 
higher than that of the image processor. However, the actual optimal ? resulted to the rather small val-
ue, 0.244. This small ? is thought to be caused by a characteristic of the image processor?s gender 
scores. Figure 4 (a) and (b) show the distributions of the gender scores derived by the text processor 
and the image processor. The horizontal axis corresponds to a gender score of a user, ranging from 0, 
highly probable female, to 1, highly probable male. The two distributions are clearly different from 
Table 2. Results obtained using text processing, image processing and combined method. 
 (P, precision; R, recall; F, F-measure; Acc., Accuracy) 
 Male Female Acc. 
P R F P R F  
Text processing 84.65 82.39 83.50 84.62 86.64 83.50 84.63 
Image processing 64.68 66.56 65.60 72.10 62.11 66.74 64.21 
Combined method (? = 0.244) 84.57 83.72 84.16 85.49 86.34 85.91 85.11 
 
  
(a) The distribution of the text processing scores          (b) The distribution of the image processing scores 
Figure 4. The distributions of the probability scores.  
 
 
Figure 5. The distribution of the ?male-person? score of training data of user annotation data. 
0
100
200
300
400
500
600
700
800
900
1000
0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0 
N
u
m
b
e
r
 o
f
 u
s
e
r
s
Probability score
0
100
200
300
400
500
600
700
800
900
1000
1 2 3 4 5 6 0 7 0.8 0.9 1.0 
N
u
m
b
e
r
 o
f
 u
s
e
r
s
Pr bability score
0
1000
2000
3000
4000
5000
00
7000
0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0 
N
u
m
b
e
r
 o
f
 l
a
b
e
l
s
Probability score
1Significance improvement with paired t-test (p=0.09<0.1). 
 
 
59
 each other: the variance of the image scores is much smaller than that of the text. From this character-
istic, the image scores were needed to be amplified in order to reflect them in the final result. In terms 
of ?, this amplification corresponds to a small value. 
The reason why the variance of the image scores became small is in its calculation process. In the 
image processor, the gender score of a user is calculated as the mean of the highest object scores ex-
tracted from each image. Figure 5 shows a distribution of ?male-person? label scores. Though a distri-
bution of each object probability scores centres not at 0.5, highest score selections and the averaging 
of them leads to a mid-range value, in this case 0.5. 
Our intuition behind the introduction of ? was to provide a reliability ratio parameter of the text 
processor and the image processor. But as a matter of fact, this parameter also worked to calibrate the 
scale difference between the two probability scores. From this observation, a function that includes a 
reliability parameter and a calibration parameter separately can be considered as an alternative to the 
proposed function. Using this kind of function will provide further insights about combining a text 
processing and image processing. 
7 Conclusion 
As described herein, we assembled two results retrieved by text and image processors respectively to 
enhance the Twitter user gender inference. Even though the gender inference accuracy already reached 
84.63 solely by the text classifier, we succeeded in improving efficiency further by 0.48 pt. Because 
the image processing in our method is completely independent from the text processing, this combined 
method is applicable to the other gender prediction methods, just like those of Burger and Liu (Burger, 
2011; Liu, 2013). Reported studies about SNS user profile inference targeted basic attributes such as 
gender, age, career, residential area, etc. More worthwhile attributes for marketing that directly indi-
cate user characteristics are desired to predict, for example, hobbies and lifestyles. Images in tweets 
are expected to include clues about these profiles aside from gender. As a subject for future work, we 
will apply our combined method to various profile attributes. 
As the combined method in this paper is simple linear consolidation and ignores a capability of ana-
lyzing both text and image information at the same time, exploring more suitable combined method is 
needed. The simplest way to analyze both text and image information simultaneously is early fusion 
that first creates the large multi-model feature vector constructed by both text and image features and 
then trains a classifier. Meta classifier which infers final class from the outputs of two modalities is 
also considerable method for this subject. Applying more sophisticated combined methods is another 
subject for future work. 
References 
Amazon. 2005. Amazon Mechanical Turk (2005), Available: http://www.mturk/welcom 
Atilika. 2011, Kuromoji. Available: http://www.atilika.org 
John D. Burger, John Henderson, Gerorge Kim, Guido Zarrella. 2011. Discriminating Gender on 
Twitter, In Proc. of the Conference on Empirical Methods in natural Language Processing 
Ken Chatfield, Victor Lempitsky, Andrea Vedaldi, Andrew Zisserman. 2011. The devil is in the de-
tails: an evaluation of recent feature encoding methods, In Proc. of British Machine Vision Confer-
ence 2011 
Chih-Chung Chang, Chih-Jen Lin, 2001. LIBSVM: a Library for Support Vector Machines. Available: 
http://www.csie.ntu.edu.tw/~cjlin/libsvm 
Bo Han, Paul Cook, and Timothy Baldwin. 2013. A Stacking-based Approach to Twitter User Geolo-
cation Prediction, In Proc. of the 51st Annual meeting of Association for Computational Linguistics, 
pages 7-12 
Bill Heli, Mikolaj Jan Piskorski. 2009. New Twitter Research: Men Follow Men and Nobody Tweets, 
Harvard Business Review, June 1. 
60
 Kazushi Ikeda, Gen Hattori, Chihiro Ono, Hideki Asoh, Teruo Higashino. 2013. Twitter User Profil-
ing Based on Text and Community Mining for Market Analysis, Knowledge Based Systems 51, 
pages 35-47. 
Svetlana Lazebnik, Cordelia Schmid, Jean Ponce, 2006. Beyond bags of features: Spatial Pyramid 
Matching for Recognizing Natural Scene Categories, In Proc. of Computer Vision and Pattern 
Recognition 2006, page 2169-2178 
Wendy Liu, Faiyaz Al Zamal, Derek Ruths. 2012. Using Social Media to Infer Gender Composition of 
Commuter Populations, In Proc. of the International Association for the Advancement of Artificial 
Intelligence Conference on Weblogs and Social 
Wendy Liu, Derek Ruths. 2013. What?s in a Name? Using First Names as Features for Gender Infer-
ence in Twitter, In Symposium on Analyzing Microtext 
David G. Lowe. 1999. Object recognition from local scale-invariant features, In Proc. of the Interna-
tional Conference on Computer Vision, pages 1150-1157 
Matt Lynley. 2012. Statistics That Reveal Instagram?s Mind-Blowing Success, Available: 
http://www.businessinsider.com/statistics-that-reveal-instagrams-mind-blowing-success-2012-4 
Xiaojun Ma, Yukihiro Tsuboshita, Noriji Kato. 2014. Gender Estimation for SNS User Profiling Au-
tomatic Image Annotation, In Proc. of the 1st International Workshop on Cross-media Analysis for 
Social Multimedia 
Aibek Makazhanov, Davood Refiei. 2013. Predicting Political Preference of Twitter Users, In Proc. of 
the 2013 IEEE/ACM International Conference on Advances in Social Network and Mining, pages 
298-305 
Alan Mislove, Sune Lehmann, Yong-Yeol Ahn, Jukka-Pekka onnela, J. Hiels Rosenquist. 2011. Un-
dersanding the Demographics of Twitter Users, In Proc. of 5th International AAAI Conference on 
Weblogs and Social Media, pages 554-557 
Delip Rao and David Yarowsky. 2010. Detecting Latent User Properties in Social Media, In Proc. of 
the Neural Information Processing Systems Foundation workshop on Machine Learning for Social 
Networks 
Chih-Fong Tsai. 2012. Bag-of-Words Representation in Image Annotation: A Review, International 
Scholarly Research Notices Artificial Intelligence, Volume 2012, Article ID 376804, 19 pages 
Jinjun Wang, Jinchao Yang, Kai Yu, Fengjun Lv, Thomas Huang, Yihong Gong. 2010. Locality-
constrained linear coding for image classification, In Proc. of  Computer Vision and Pattern Recog-
nition 2010, page 626 
Yahoo! Japan. 2013. Yahoo Crowd Sourcing. Available: http://crowdsourcing.yahoo.co.jp/ 
Dengsheng Zhang, Md Monirul Islam, Guojun Lu. 2012. A review on automatic image annotation, 
Pattern Recognition 45, pages 346-362 
61
