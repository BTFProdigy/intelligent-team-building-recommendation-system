95
96
97
98
Proceedings of the 5th International Workshop on Semantic Evaluation, ACL 2010, pages 436?439,
Uppsala, Sweden, 15-16 July 2010. c?2010 Association for Computational Linguistics
Twitter Based System: Using Twitter for Disambiguating
Sentiment Ambiguous Adjectives
Alexander Pak, Patrick Paroubek
Universite? de Paris-Sud,
Laboratoire LIMSI-CNRS, Ba?timent 508,
F-91405 Orsay Cedex, France
alexpak@limsi.fr, pap@limsi.fr
Abstract
In this paper, we describe our system
which participated in the SemEval 2010
task of disambiguating sentiment ambigu-
ous adjectives for Chinese. Our system
uses text messages from Twitter, a popu-
lar microblogging platform, for building a
dataset of emotional texts. Using the built
dataset, the system classifies the meaning
of adjectives into positive or negative sen-
timent polarity according to the given con-
text. Our approach is fully automatic. It
does not require any additional hand-built
language resources and it is language in-
dependent.
1 Introduction
The dataset of the SemEval task (Wu and Jin,
2010) consists of short texts in Chinese contain-
ing target adjectives whose sentiments need to be
disambiguated in the given contexts. Those adjec-
tives are: ? big, ? small, ? many, ? few, ?
high, ? low,? thick, ? thin, ? deep, shallow,
? heavy, light,?? huge,?? grave.
Disambiguating sentiment ambiguous adjec-
tives is a challenging task for NLP. Previous stud-
ies were mostly focused on word sense disam-
biguation rather than sentiment disambiguation.
Although both problems look similar, the latter is
more challenging in our opinion because impreg-
nated with more subjectivity. In order to solve the
task, one has to deal not only with the semantics
of the context, but also with the psychological as-
pects of human perception of emotions from the
written text.
In our approach, we use Twitter1 microblogging
platform to retrieve emotional messages and form
two sets of texts: messages with positive emotions
and those with negative ones (Pak and Paroubek,
1http://twitter.com
2010). We use emoticons2 as indicators of an emo-
tion (Read, 2005) to automatically classify texts
into positive or negative sets. The reason we use
Twitter is because it allows us to collect the data
with minimal supervision efforts. It provides an
API3 which makes the data retrieval process much
more easier then Web based search or other re-
sources.
After the dataset of emotional texts has been
obtained, we build a classifier based on n-grams
Na??ve Bayes approach. We tested two approaches
to build a sentiment classifier:
1. In the first one, we collected Chinese texts
from Twitter and used them to train a classi-
fier to annotate the test dataset.
2. In the second one, we used machine trans-
lator to translate the dataset from Chinese to
English and annotated it using collected En-
glish texts from Twitter as the training data.
We have made the second approach because we
were able to collect much more of English texts
from Twitter than Chinese ones and we wanted
to test the impact of machine translation on the
performance of our classifier. We have exper-
imented with Google Translate and Yahoo Ba-
belfish4. Google Translate yielded better results.
2 Related work
In (Yang et al, 2007), the authors use web-blogs
to construct a corpora for sentiment analysis and
use emotion icons assigned to blog posts as indica-
tors of users? mood. The authors applied SVM and
CRF learners to classify sentiments at the sentence
level and then investigated several strategies to de-
termine the overall sentiment of the document. As
2An emoticon is a textual representation of an author?s
emotion often used in Internet blogs and textual chats
3http://dev.twitter.com/doc/get/search
4http://babelfish.yahoo.com/
436
the result, the winning strategy is defined by con-
sidering the sentiment of the last sentence of the
document as the sentiment at the document level.
J. Read in (Read, 2005) used emoticons such as
?:-)? and ?:-(? to form a training set for the sen-
timent classification. For this purpose, the author
collected texts containing emoticons from Usenet
newsgroups. The dataset was divided into ?pos-
itive? (texts with happy emoticons) and ?nega-
tive? (texts with sad or angry emoticons) samples.
Emoticons-trained classifiers: SVM and Na??ve
Bayes, were able to obtain up to 70% accuracy on
the test set.
In (Go et al, 2009), authors used Twitter to
collect training data and then to perform a senti-
ment search. The approach is similar to the one
in (Read, 2005). The authors construct corpora
by using emoticons to obtain ?positive? and ?neg-
ative? samples, and then use various classifiers.
The best result was obtained by the Na??ve Bayes
classifier with a mutual information measure for
feature selection. The authors were able to obtain
up to 84% of accuracy on their test set. However,
the method showed a bad performance with three
classes (?negative?, ?positive? and ?neutral?).
In our system, we use a similar idea as in (Go
et al, 2009), however, we improve it by using a
combination of unigrams, bigrams and trigrams (
(Go et al, 2009) used only unigrams). We also
handle negations by attaching a negation particle
to adjacent words when forming ngrams.
3 Our method
3.1 Corpus collection
Using Twitter API we collected a corpus of text
posts and formed a dataset of two classes: positive
sentiments and negative sentiments. We queried
Twitter for two types of emoticons considering
eastern and western types of emoticons5:
? Happy emoticons: :-), :), ? ?, ?o?, etc.
? Sad emoticons: :-(, :(, T T, ; ;, etc.
We were able to obtain 10,000 Twitter posts in
Chinese, and 300,000 posts in English evenly split
between negative and positive classes.
The collected texts were processed as follows to
obtain a set of n-grams:
1. Filtering ? we remove URL links (e.g.
http://example.com), Twitter user names (e.g.
5http://en.wikipedia.org/wiki/Emoticon#Asian style
@alex ? with symbol @ indicating a
user name), Twitter special words (such as
?RT?6), and emoticons.
2. Tokenization ? we segment text by split-
ting it by spaces and punctuation marks, and
form a bag of words. For English, we kept
short forms as a single word: ?don?t?, ?I?ll?,
?she?d?.
3. Stopwords removal ? in English, texts we re-
moved articles (?a?, ?an?, ?the?) from the bag
of words.
4. N-grams construction ? we make a set of n-
grams out of consecutive words.
A negation particle is attached to a word which
precedes it and follows it. For example, a sen-
tence ?I do not like fish? will form three bigrams:
?I do+not?, ?do+not like?, ?not+like fish?. Such
a procedure improves the accuracy of the classi-
fication since the negation plays a special role in
opinion and sentiment expression (Wilson et al,
2005). In English, we used negative particles ?no?
and ?not?. In Chinese, we used the following par-
ticles:
1. ? ? is not + noun
2. ? ? does not + verb, will not + verb
3. ? (?) ? do not (imperative)
4. ? (??) ? does not have
3.2 Classifier
We build a sentiment classifier using the multi-
nomial Na??ve Bayes classifier which is based on
Bayes? theorem.
P (s|M) =
P (s) ? P (M |s)
P (M)
(1)
where s is a sentiment, M is a text. We assume
that a target adjective has the same sentiment po-
larity as the whole text, because in general the
lengths of the given texts are small.
Since we have sets of equal number of positive
and negative messages, we simplify the equation:
P (s|M) =
P (M |s)
P (M)
(2)
6An abbreviation for retweet, which means citation or re-
posting of a message
437
P (s|M) ? P (M |s) (3)
We train Bayes classifiers which use a presence
of an n-grams as a binary feature. We have ex-
perimented with unigrams, bigrams, and trigrams.
Pang et al (Pang et al, 2002) reported that uni-
grams outperform bigrams when doing sentiment
classification of movie reviews, but Dave et al
(Dave et al, 2003) have obtained contrary re-
sults: bigrams and trigrams worked better for the
product-review polarity classification. We tried to
determine the best settings for our microblogging
data. On the one hand high-order n-grams, such
as trigrams, should capture patterns of sentiments
expressions better. On the other hand, unigrams
should provide a good coverage of the data. There-
fore we combine three classifiers that are based
on different n-gram orders (unigrams, bigrams and
trigrams). We make an assumption of conditional
independence of n-gram for the calculation sim-
plicity:
P (s|M) ? P (G1|s) ? P (G2|s) ? P (G3|s) (4)
where G1 is a set of unigrams representing the
message, G2 is a set of bigrams, and G3 is a set of
trigrams. We assume that n-grams are condition-
ally independent:
P (Gn|s) =
?
g?Gn
P (g|s) (5)
Where Gn is a set of n-grams of order n.
P (s|M) ?
?
g?G1
P (g|s)?
?
g?G2
P (g|s)?
?
g?G3
P (g|s)
(6)
Finally, we calculate a log-likelihood of each sen-
timent:
L(s|M) =
?
g?G1
log(P (g|s)) +
?
g?G2
log(P (g|s))
+
?
g?G3
log(P (g|s))
(7)
In order to improve the accuracy, we changed
the size of the context window, i.e. the number of
words before and after the target adjective used for
classification.
4 Experiments and Results
In our experiments, we used two datasets: a trial
dataset containing 100 sentences in Chinese and
0 5 10 15 20 25 30 35 40 45
0.45
0.47
0.49
0.51
0.53
0.55
0.57
0.59
0.61
0.63
0.65
google yahoo
window size
m
ic
ro
 
a
cc
u
ra
cy
Figure 1: Micro accuracy when using Google
Translate and Yahoo Babelfish
0 5 10 15 20 25 30 35 40 45
0.4
0.45
0.5
0.55
0.6
0.65
google yahoo
window size
m
a
cr
o
 
a
cc
u
ra
cy
Figure 2: Macro accuracy when using Google
Translate and Yahoo Babelfish
a test dataset with 2917 sentences. Both datasets
were provided by the task organizers. Micro and
macro accuracy were chosen as the evaluation
metrics.
First, we compared the performance of our
method when using Google Translate and Yahoo
Babelfish for translating the trial dataset. The re-
sults for micro and macro accuracy are shown in
Graphs 1 and 2 respectively. The x-axis repre-
sents a context window-size, equal to a number of
words on both sides of the target adjective. The y-
axis shows accuracy values. From the graphs we
see that Google Translate provides better results,
therefore it was chosen when annotating the test
dataset.
Next, we studied the impact of the context win-
dow size on micro and macro accuracy. The
impact of the size of the context window on
the accuracy of the classifier trained on Chinese
texts is depicted in Graph 3 and for the classifier
trained on English texts with translated test dataset
438
0 5 10 15 20 25 30 35 40 45
0.45
0.47
0.49
0.51
0.53
0.55
0.57
0.59
0.61
0.63
0.65
Micro Macro
window size
a
cc
u
ra
cy
Figure 3: Micro and macro accuracy for the first
approach (training on Chinese texts)
0 5 10 15 20 25 30 35 40 45
0.45
0.47
0.49
0.51
0.53
0.55
0.57
0.59
0.61
0.63
0.65
Micro Macro
window size
a
cc
u
ra
cy
Figure 4: Micro and macro accuracy for the sec-
ond approach (training on English texts which
have been machine translated)
in Graph 4.
The second approach achieves better results.
We were able to obtain 64% of macro and 61% of
micro accuracy when using the second approach
but only 63% of macro and 61% of micro accu-
racy when using the first approach.
Another observation from the graphs is that
Chinese requires a smaller size of a context win-
dow to obtain the best performance. For the first
approach, a window size of 8 words gave the best
macro accuracy. For the second approach, we ob-
tained the highest accuracy with a window size of
22 words.
5 Conclusion
In this paper, we have described our system for
disambiguating sentiments of adjectives in Chi-
nese texts. Our Na??ve Bayes approach uses infor-
mation automatically extracted from Twitter mi-
croblogs using emoticons. The techniques used in
our approach can be applied to any other language.
Our system is fully automate and does not utilize
any hand-built lexicon. We were able to achieve
up to 64% of macro and 61% of micro accuracy at
the SemEval 2010 task
For the future work, we would like to collect
more Chinese texts from Twitter or similar mi-
croblogging platforms. We think that increasing
the training dataset will improve much the accu-
racy of the sentiment disambiguation.
References
Kushal Dave, Steve Lawrence, and David M. Pen-
nock. 2003. Mining the peanut gallery: opinion
extraction and semantic classification of product re-
views. In WWW ?03: Proceedings of the 12th in-
ternational conference on World Wide Web, pages
519?528, New York, NY, USA. ACM.
Alec Go, Lei Huang, and Richa Bhayani. 2009. Twit-
ter sentiment analysis. Final Projects from CS224N
for Spring 2008/2009 at The Stanford Natural Lan-
guage Processing Group.
Alexander Pak and Patrick Paroubek. 2010. Twitter as
a corpus for sentiment analysis and opinion mining.
In Proceedings of LREC 2010.
Bo Pang, Lillian Lee, and Shivakumar Vaithyanathan.
2002. Thumbs up? sentiment classification using
machine learning techniques. In Proceedings of the
Conference on Empirical Methods in Natural Lan-
guage Processing (EMNLP), pages 79?86.
Jonathon Read. 2005. Using emoticons to reduce de-
pendency in machine learning techniques for senti-
ment classification. In Proceedings of the ACL Stu-
dent Research Workshop, pages 43?48.
Theresa Wilson, Janyce Wiebe, and Paul Hoffmann.
2005. Recognizing contextual polarity in phrase-
level sentiment analysis. In HLT ?05: Proceedings
of the conference on Human Language Technology
and Empirical Methods in Natural Language Pro-
cessing, pages 347?354, Morristown, NJ, USA. As-
sociation for Computational Linguistics.
Yunfang Wu and Peng Jin. 2010. Semeval-2010
task 18: Disambiguating sentiment ambiguous ad-
jectives. In SemEval 2010: Proceedings of Interna-
tional Workshop of Semantic Evaluations.
Changhua Yang, Kevin Hsin-Yih Lin, and Hsin-
Hsi Chen. 2007. Emotion classification using
web blog corpora. In WI ?07: Proceedings of
the IEEE/WIC/ACM International Conference on
Web Intelligence, pages 275?278, Washington, DC,
USA. IEEE Computer Society.
439
Introduction
The aim of this workshop is to identify and to synthesize current needs for language-technology
evaluation. The first part of the workshop will focus on one of the most challenging current issues
in language engineering: the evaluation of dialogue systems and models. The second part will
extend the discussion to address the problem of evaluation in language engineering more broadly
and on more theoretical grounds.
The space of possible dialogues is enormous, even for limited domains like travel information
servers. The generalization of evaluation methodologies across different application domains
and languages is an open problem. Review of published evaluations of dialogue models and
systems suggests that usability techniques are the standard method. Dialogue-based system are
often evaluated in terms of standard, objective usability metrics, such as task-completion time and
number of user actions. In the past, researchers have proposed and debated theory-based methods
for modifying and testing the underlying dialogue model, but the most widely used method of
evaluation is usability testing, although more precise and empirical methods for evaluating the
effectiveness of dialogue models have been proposed. For task-based interaction, typical measures
of effectiveness are time-to-completion and task outcome, but the evaluation should focus on user
satisfaction rather than on arbitrary effectiveness measurements. Indeed, the problems faced in
current approaches to measurement of effectiveness dialogue models and systems include:
1. Direct measures are unhelpful because efficient performance on the nominal task may not
represent the most effective interaction
2. Indirect measures usually rely on judgment and are vulnerable to weak relationships between
the inputs and outputs
3. Subjective measures are unreliable and domain-specific
Representative questions to be addressed include but are not limited to:
1. How do we deal with the combinatorial explosion of dialogue states?
2. How can satisfaction be measured with respect to underlying dialogue models?
3. Are there useful direct measures of dialogue properties that do not depend on task efficiency?
4. What is the role of agent-based simulation in evaluation of dialogue models?
Of course, the problems faced in evaluating dialogue and system models are found in other
domains of language engineering, even for non-interactive processes such as part-of-speech tagging,
parsing, semantic disambiguation, information extration, speech transcription, and audio document
indexing. So the issue of evaluation can be viewed at a more generic level, raising fundamental,
theoretical questions such as:
1. What are the interest and benefits of evaluation for language engineering?
2. Do we really need these specific methodologies, since a form of evaluation should always be
present in any scientific investigation?
3. If evaluation is needed in language engineering, is it the case for all domains?
4. What form should it take? Technology evaluation (task-oriented in laboratory environment)
or field/user Evaluation (complete systems in real-life conditions)?
5. We have seen before that the the evaluation of dialogue models is still unsolved, but for
domains where metrics already exists, are they satisfactory and sufficient? How can we take
into account or abstract from the subjective factor introduced by human operators in the
process?
6. Do similarity measures and standards offer appropriate answers to this problem? Most
of the efforts focus on evaluating process, but what about the issue of language resources
evaluation?
In the second part of the workshop we wish to address the problem of evaluation both from a broader
perspective, including novel applications domain for evaluation, new metrics for known tasks and
resource evaluation, as well as look at the problem from a more theoretical point of view, including
the isssue of formal theory of evaluation and infrastructural needs of language engineering. David
Novick & Patrick Paroubek.
The PEACE SLDS understanding evaluation paradigm of the French
MEDIA campaign
Laurence Devillers, He?le`ne Maynard, Patrick Paroubek, Sophie Rosset
LIMSI-CNRS
Bt 508 University of Paris XI - BP 133 F-91403 ORSAY Cedex, France
fdevil,hbm,pap,rossetg@limsi.fr
Abstract
This paper presents a paradigm for
evaluating the context-sensitive under-
standing capability of any spoken lan-
guage dialog system: PEACE (French
acronym for Paradigme d?Evaluation
Automatique de la Compre?hension hors
et En-contexte). This paradigm will be
the basis of the French Technolangue
MEDIA project, in which dialog sys-
tems from various academic and indus-
trial sites will be tested in an evaluation
campaign coordinated by ELRA/ELDA
(over the next two years). Despite pre-
vious efforts such as EAGLES, DISC,
AUPELF ARCB2 or the ongoing Ameri-
can DARPA COMMUNICATOR project,
the spoken dialog community still lacks
common reference tasks and widely
agreed upon methods for comparing
and diagnosing systems and techniques.
Automatic solutions are nowadays be-
ing sought both to make possible the
comparison of different approaches by
means of reliable indicators with generic
evaluation methodologies and also to re-
duce system development costs. How-
ever achieving independence from both
the dialog system and the task per-
formed seems to be more and more a
utopia. Most of the evaluations have
up to now either tackled the system as
a whole, or based the measurements
on dialog-context-free information. The
PEACE proposal aims at bypassing some
of these shortcomings by extracting,
from real dialog corpora, test sets that
synthesize contextual information.
1 Introduction
Generally speaking common reference
tasks (Whittaker et al, 2002) and methods
to compare and diagnose spoken language dialog
systems (SLDS) and spoken dialog techniques
are lacking despite previous efforts futher dis-
cussed in the next section such as EAGLES,
DISC, AUPELF ARCB2 or the ongoing American
project DARPA COMMUNICATOR. Without
an objective assessment of dialog systems, it is
difficult to reuse previous work and to advance
theories. The assessment of a dialog system is
complex in part to the high integration factor
and tight coupling between the various modules
present in any SLDS, for which unfortunately
today, no common accepted reference architecture
exists. Nevertheless, a major problem remains the
dynamic nature of dialog. Consequently to these
shortcomings, researchers are often unable to
provide principled design and system capabilities
for technology transfer. In other research areas,
such as speech recognition and information re-
trieval, common reference tasks have been highly
effective in sharing research costs and efforts. A
similar development is highly needed in the dialog
community.
In this contribution which addresses only a part
of the SLDS evaluation problem, a paradigm for
evaluating the context-sensitive understanding ca-
pability of any spoken language dialog system is
proposed. PEACE (Devillers et al, 2002a) de-
scribed in section 3, is based on test sets extracted
from real corpora, and has three main aspects: it
is generic, contextual and it offers diagnostic ca-
pabilities. Here genericity is envisaged in a con-
text of information dialogs access. The diagnos-
tic aspect is important in order to determine the
different qualities of the systems under test. The
contextual aspect of evaluation is a crucial point
since dialog is dynamic by nature. We propose
to simulate/synthesize the contextual information.
The PEACE paradigm will be tested in the French
Technolangue MEDIA project and will serve as
basis in the comparison and diagnostic evaluation
of systems presented by various academic and in-
dustrial sites (section 4). ELRA/ELDA is the co-
ordinator of the larger scope evaluation campaign
EVALDA, which includes the MEDIA campaign
that began in January 2003.
2 Overview of SLDS evaluation
Without an attempt to be exhaustive, we overview
some recent efforts for evaluation of SLDS.
The objective of the European DISC project was
to write the best-practice guidelines for SLDS de-
velopment and evaluation of its time. DISC has
collected a systematic list of bottom-up evalua-
tion criteria, each corresponding to a partially or-
dered list of properties likely to be encountered
in any SLDS. This properties are positioned on a
grid defining an SLDS abstract architecture and re-
late to various phases of the generic DISC SLDS
development life-cycle (Dybkj?r and al., 1998).
They are complemented by a standard evaluation
pattern made of 10 generic questions (e.g. ?Which
symptoms need to be observed?? ) which has been
instantiated for all the evaluation criteria. If the
DISC results are quite extensive and presented in
an homogeneous way, they do not provide a di-
rect answer to the question of SLDS evaluation.
Its contribution lies more at the specification level.
Although the approach and the goals of the Euro-
pean EAGLES project were different, one could
forward the same remark about the results of the
speech evaluation work group (D. Gibbon, 1997).
In (Fraser, 1998), one find a set of evaluation cri-
teria for voice oriented products and services, or-
ganized in four broad categories.: 1) voice com-
mand, 2) document generation, 3) phone services
4) other.
To the best of our knowledge, the MADCOW
(Multi Site Data COllection Working group) co-
ordination group set up in the USA by ARPA in
the context of the ATIS (Air Travel Information
Services) task to collect corpora, was the first to
propose a common infrastructure for SLDS auto-
matic evaluation (MADCOW, 1992), which also
addressed the problem of language understand-
ing evaluation, based on system answer compar-
ison. Unfortunately no direct diagnostic informa-
tion can be produced, since understanding is ap-
preciated by gauging the distance from the answer
to a pair of minimal and a maximal reference an-
swers. In ATIS, the protocol was only been ap-
plied to context free sentences. Up to now it has
been one of the most used by the community since
it is relatively objective and generic because it re-
lies on counts of explicit information and allows
for a certain variation in the answers. On the other
hand, the method displays a bias toward silence
and does not give the means to appreciate error
severity.
In ARISE (Automatic Railway Information Sys-
tems for Europe) (Lamel, 1998), a corpus of
roughly 10,000 calls has been used in conjunc-
tion with user debriefing questionnaire analysis to
diagnose different versions of a phone informa-
tion server. The hand-tagging objective measures
of the corpus include understanding error counts
(glass box methodology). Although it provides
fine grained diagnostic information, this procedure
cannot be easily generalized since it requires hand-
annotated corpus and access to the internal repre-
sentation of the system.
Two metrics have been developped at MIT
(Glass et al, 2000): the Query Density (QD)
and the Concept Efficiency (CE), which measure
respectively over the course of a dialogue: the
mean number of new concepts introduced per user
query, and the number of turns necessary for each
concept to be understood by the system. Con-
cepts are generated automatically for each utter-
ance with a parsable orthographic transcription as
a series of keyword-value pairs. The higher the
QD, the more effectively a user is able to commu-
nicate information to the system. The CE is an in-
dicator of recognition or understanding errors; the
higher it is, the fewer times a user needs to repeat
himself. These metrics were evaluated on single
systems (JUPITER and and MERCURY); to com-
pare different systems of the same type, one would
need a common ontology. In (Glass et al, 2000),
the authors believe that CE should be related to
user frustation, but to show it they would need to
use the PARADISE framework.
PARADISE (Walker et al, 1998) can be seen
as a sort of meta-paradigm which correlates ob-
jective and subjective measurements. Its ground-
ing hypothesis states that the goal of any SLDS is
to achieve user-satisfaction, which in turn can be
predicted through task success and various interac-
tion costs. With the help of the kappa coefficient
(Carletta, 1996) proposes to represent the dialog
success independently from the task intrinsic com-
plexity, thus opening the way to task generic com-
parative evaluation. PARADISE has been tested
in the COMMUNICATOR project (Walker et al,
2001) with 9 systems working on the same task
over different databases. With four basic measures
(e.g. task completion) the protocol has been able
to predict 37% of user satisfaction variation, and
42% with the help of a few extra measurements on
dialog acts and subtasks. One critic, one can make
about PARADISE concern its cost (real user tests
are costly) and the use of subjective assessment.
The adaption of the DQR text understanding
evaluation methodology (Sabatier et al, 2000) to
speech resulted in a generic and qualitative proce-
dure. Each element of its test set holds three parts,
the Declaration to define the context, a Question
which bears on point present in the context and the
Response. The test set is organized through seven
levels of test, from basic explicit understanding
to semantic interpretation and reply pertinence as-
sessment. This protocol is task and system generic
but test set construction is not straightforward and
the bias introduced by the wording of the question
is difficult to assess.
Recently the GDR-13 work group of CNRS
on spoken dialog understanding, has proposed an
evaluation methodology for literal understanding.
According to (Antoine and al., 2002), DEFI tries
to remedy two important weaknesses of the MAD-
COW methodology, namely the lack of genericity
and the lack of diagnostic information, by craft-
ing system specific test sets from a primary set of
enunciations representative of the task (provided
by the developers). Secondary enunciations are
then derived from the primary ones in order to ex-
hibit particular language phenomena. Afterwards,
the systems are evaluated by their developers us-
ing specific test set and their own metrics. The
various results can be mapped over a generic ab-
stract architecture for comparison (although this
mapping is still unspecified at the time of writ-
ing). DEFI has already been used in one evalua-
tion campaign, with 5 systems presented by 4 lab-
oratories. (Antoine and al., 2002) has reported the
following weaknesses of the protocol: how to con-
trol the bias introduced by the derivation of enun-
ciations, how to guaranty that derived enunciation
will remain in the task scope (this prevented some
system from being evaluated over the complete
test set) and finally how to restrict and organize
the language phenomena used in the test set.
3 The PEACE paradigm
We first describe the paradigm and relate prelim-
inary experiments with PEACE. This paradigm
which is as basement for the MEDIA project will
be refined by all the partners and use for an evalua-
tion campaign between seven systems of industrial
and academic sites.
3.1 Description
The PEACE paradigm relies on the idea that for
database querying tasks, it is possible to define a
common semantic representation, onto which all
the systems are able to convert their own repre-
sentation (Moore, 1994). The paradigm based on
data extracted from real corpus, includes both lit-
eral and contextual understanding test sets. More
precisely, it provides:
 the definition of a semantic representation
(see 3.1.1),
 the definition of a model for dialogic contexts
(see 3.1.2),
 the definition and typology of linguistic phe-
nomena and dialogic functions used to selec-
tively diagnoze the system language capabil-
ities (anaphora resolution, constraints relax-
ation, etc.) (see 3.1.3),
 a data structuring method. The format of the
annotated data will be adapted to language
resource standard annotations implemented
(see 3.1.4),
 and evaluation metrics with the correspond-
ing evaluation tool (see 3.1.5).
3.1.1 Generic semantic representation
The difficulty of choosing a semantic represen-
tation lies in finding a complete and simple repre-
sentation of a user utterance meaning in a unified
format. A frame Attribute Value Representation
(AVR) has been chosen, allowing a fast and re-
liable annotation. The values are either numeric
units, proper names, or semantic classes, that
group together lexical units which are synonyms
for the task. The order of the (attribute, value)
pairs in the semantic representation matches their
respective position in the utterance. A modal in-
formation (positive (+) and negative(-)) is also as-
signed to each (attribute, value) pair. The semantic
representation of an utterance consists then in a list
of triplets of the form (mode, attribute, normalized
value). An example is given in figure 1. In order
to take into account for long-time dependencies or
to allow multiple referenced objects, the semantic
representation may be enriched by adding a refer-
ence value to each triplet for the representation of
links between 2 attributes of the utterance.
Attributes can grouped into different classes:
 the database attributes (the most frequent)
correspond to the attributes of the database
tables (e.g. category for an hotel);
 the modifier attributes are associated to
the database concepts. Their values are
used to modify the database concept in-
terpretation values (e.g. the attribute
category-modifier with possible val-
ues: >; <, =, Max, Min);
 the discursive attributes are introduced to
handle various aspects of dialogic interaction
User c?est pas Paris c?est Passy
Query it is not Paris it is Passy
(LU) AVR (-, place, Paris)
(+, place, Passy)
Figure 1: Example of a semantic representation of an ut-
terance with positive and negative information for the ARISE
task. Place is an database attribute,Paris and Passy are
values and +/- modal markers.
(e.g. commandwith values cancelation, cor-
rection, error specification: : :, or response
with values yes or no);
 the argument attribute which represents the
topic at the focus of the utterance.
When dealing with information retrieval appli-
cations, defining the database and modifier at-
tributes and the appropriate values can be done
in a rather straightforward way. Most of those
attributes are derived directly from the informa-
tion stored in the database. Furthermore, most of
the discursive attributes are domain-independent.
Some database attributes remain unchanged across
many tasks, such as those dealing with dates or
prices.
This semantic representation has been used at
LIMSI for PARIS-SITI TASK (touristic informa-
tion) and ARISE TASK (traintable information)
both with triplet representation. More recently in
the context of the AMITIES project, quadruplets
were used.
3.1.2 Contextual understanding modeling
Contextual understanding evaluation provides
information about the capability of the system
to take into account the dialog history in order
to properly interpret the user query. Contextual
understanding evaluation is rarely performed be-
cause of the dynamic nature of the dialog make
the dialog context depend on the system?s dialog
strategy.
Nevertheless PEACE proposes a system-
independent way to evaluate local contextual
interpretation. Given U
1
:::U
t
the user inter-
actions, and S
1
:::S
t
the answers of the agent
or system, the context a time t is a function
f(U
1
; S
1
; U
2
; S
2
; :::U
t
; S
t
). In the PEACE
paradigm, a paraphrase of the context is derived
from the semantic representation (Bonneau-
Maynard et al, 2000).
The dialog contexts are extracted from real di-
alogs in three steps. First, the internal semantic
frames representing the dialog contexts are auto-
matically extracted from the log files of the ses-
sion recordings. Secondly, the semantic frames
are converted into AVR format and then hand-
corrected to faithfully represent the dialog history.
The last step consists in the writing of a sentence
for each context (the context paraphrase), which
results in the same AVR representation as the one
of the dialog context.
Two possibilities may be investigated for build-
ing the paraphrase from the internal semantic rep-
resentation of the dialog context. A rule-based or
template-based natural language generation mod-
ule can be used to automatically produce the para-
phrase. The paraphrase can also be obtained
by concatenating the sentences preceding the ex-
tracted dialog state. In both cases, a manual veri-
fication is needed.
3.1.3 A typology of linguistic phenomena and
dialogic functions
For dialog system evaluation, it is essential to
build test sets randomly extracted from real cor-
pus. For dialog system diagnosis, it is also crucial
to build test sets labeled with the linguistic phe-
nomena and dialogic functions. Thus, the capabil-
ities of system?s contextual understanding can be
assessed for the main linguistic and dialogic dif-
ficulties such as, for instance, anaphora or ellipsis
resolution.
3.1.4 A data structuring method
Two types of units, one for literal understanding
(LU), the other for contextual understanding (CU)
are defined. The format of the annotated data will
be adapted to language resource standard annota-
tions implemented in XML, e.g. (Geoffrois et al,
2000), (Ide and Romary, 2002).
Each unit is extracted from a real dialog cor-
pus. LU units are composed of the user query,
the corresponding audio signal, an automatic tran-
scription obtained with a recognition system, and
finally the literal semantic representation of the ut-
terance (see Figure 1). CU units are composed of
Context je voudrais un ho?tel 4
paraphrase e?toiles dans le neuvie`me
I would like a 4 category
hotel in the ninth
(LU) AVR (+, argument, hotel)
(+, district, 9)
(+, category, 4)
User la me?me cate?gorie dans
query un autre arrondissement
the same category in
another district
(LU) AVR (+, other, district)
(+, same, category)
(CU) AVR (+, argument, hotel)
(-, district, 9)
(+, category, 4)
Figure 2: Example of a contextual understanding unit com-
posed of a context paraphrase, a user query and the resulting
AVR. AVR of context paraphrase and user query are given in
TYPEWRITING MODE. Ellipsis (?in the ninth?) and anaphora
(?same category?, ?another district?) may be observed.
the dialog context (given by the paraphrase), the
user query and the resulting AVR of the user query
in the given context (see Figure 2). Those units are
also labeled with linguistic and dialogic phenom-
ena.
3.1.5 Evaluation metrics and scoring tool
Common evaluation metrics are essential for
analyzing the system capabilities. The scoring tool
for AVR comparison is able to compare between
two AVR frame representation sets. For evalu-
ation, system outputs translated in AVR format
composed one set, the other one contains the AVR
references which are manually annotated. Both
frame sets have the form of a list of AVRs (fixed
length records). Each record is composed of three
or four fields (mode, attribute, value, reference).
The comparison consists in applying a set of pre-
defined operators each assigned with a cost value.
The comparison process looks for operator lists
to be applied to the test frame in order to obtain
the reference frame that minimizes the final cost
value. For a global evaluation, the classical opera-
tors from speech evaluation (DELetion, INSertion
and SUBstitution) may be used (as used for first
two values of Accuracy percentage in Table 1).
With our scoring tool the definition of new opera-
tors is quite easy. It is then also possible to distin-
guish between different types of errors by defining
specific operators (as used to estimate Topic iden-
tification in Table 1), or by using different cost val-
ues (for example a substitution is often considered
more costly for dialog management).
3.2 Example use of PEACE
In order to validate the evaluation paradigm, a
set of approximatively 1,700 literal units and a
set of 100 contextual units has been used for
the PARIS-SITI task (Bonneau-Maynard and Dev-
illers, 2000). Results for both literal and contex-
tual understanding test sets are given in Table 1. In
order to observe the ability of the systems to deal
with recognition errors, each literal understand-
ing unit also contains the ASR transcription of the
original user utterance. The various measures of
understanding accuracy are computed as the ratio
between the sum of the number of deleted, inserted
and substituted attributes, and the total number of
AVR attributes in the test set. The possibility of
an automatic evaluation of the LU accuracy and
the ability of the scoring tool to point out the er-
rors allowed us to easily improve the literal un-
derstanding accuracy from 89.0% to 93.5%. Due
to a 26.5% ASR error rate, the LU accuracy goes
down from 93.5% to 72% after ASR transcription.
The contextual understanding accuracy on the 100
test units is 82.6% on exact transcription. For
instance, anaphoric references are relatively well
solved, with 80.4% accuracy on the 50 units con-
taining at least one anaphoric reference. For each
example, the anaphoric referenced object is gen-
erally correctly identified and remaining errors are
often due to a bad history constraint management.
3.3 Discussing the PEACE paradigm
The PEACE paradigm enables automatic evalua-
tion of literal and contextual dialog understand-
ing. The evaluation paradigm makes the distinc-
tion between different types of errors, allowing a
qualitative and diagnostic analysis of the perfor-
mances of a speech understanding module. Very
few evaluation paradigms propose automatic di-
agnosis of contextual interpretation (Glass et al,
2000). The proposed methodology is based on
#Units #Attr. %Acc. Prec.
LU exact 1 681 3 991 93.5% 0.7
LU ASR . 1 681 3 991 72.0% 1.4
Topic id. 680 833 94.3% 1.6
Modifier id. 323 445 95.7% 1.9
CU exact 100 430 86.8% 3.2
Anaphoric 50 245 84.4% 4.5
resolution
Ellipsis 25 106 85.3% 6.7
resolution
Table 1: Literal understanding (LU) accuracy on both exact
and ASR transcription, and contextual understanding (CU)
accuracy. Second column indicates the number of units in-
cluded in the test set (i.e # of user utterances), third col-
umn gives the total number of attributes in the correct AVR
test sets. Details, using specific operators, are given for
argument (topic) and modifier identification for LU on
exact transcription, and for anaphoric reference and ellipsis
resolution for CU. Last column gives the 95% precision of
the accuracy estimation (Montacie? and Chollet, 1997)
.
semi-automatically built reference test sets, and
therefore is much more time effective than manual
evaluation. Furthermore, it provides reproducible
tests.
Although the semantic representation is task de-
pendent, the example described above shows the
feasibility of the paradigm for any dialog system
interfacing to a database. Robustness to many
linguistic phenomena such as repetitions, hesita-
tions or auto-corrections may be evaluated with
this method. XML coding will facilitate the gener-
icity and the reusability of the test sets, by al-
lowing the selection of the dialogic contexts to be
studied.
The representation of the dialog context with a
single paraphrase, derived from a ?flat? structured
AVR, may have some limitations in case of long-
time dialog dependencies. It does not allow for
memorizing all the steps of the dialog. For ex-
ample, if the speaker says first ?I would like a
2 star hotel?, then ?no I prefer 3 stars? and fi-
nally says ?give me again my first choice?, the
CU unit cannot take into account this succession
of queries. However, this kind of interaction is
rarely observed in dialogue corpora: the user usu-
ally repeats the constraint value (?give me again
a 2 star hotel?). To represent more precisely the
dialog state, the representation of the dialog con-
text should incorporate some meta-information in-
spired for example from the DAMSL annotation
standard 1 (Devillers et al, 2002b).
Another point is the representativity of the test
sets. This may be considered as a limitation as
far as PEACE paradigm is built on the idea that
the test units are extracted from real dialogs. Ob-
viously, the larger the test sets are, the better. A
diagnostic evaluation may need a very large test
corpora to validate system performance against the
wide range of phenomena present in spontaneous
dialog.
The ability to automatically diagnose the per-
formances of contextual understanding modules
on local difficulties such as ellipsis, negations,
anaphoric reference or constraint relaxation is one
of the major advantages of the PEACE paradigm,
which has not been investigated by other method-
ologies. This is why it has been chosen for the
MEDIA project described in the next section.
4 The MEDIA project
The MEDIA project proposes a paradigm based
on a reference task and on test sets extracted from
real corpora for evaluating literal and contextual
understanding in dialog systems. The PEACE
paradigm will serve as basis for the MEDIA
project. The consortium is composed of IRIT,
LIA, LIMSI, LORIA, VALORIA for the French
academic sites and France Telecom R&D and
TELIP for the industrial sites. The scientific com-
mittee contains representatives of AT&T (USA),
Tilburg University (Netherlands), IBM, IMAG,
LIUM and VECSYS (France).
The project has four main parts. First, the selec-
tion of reference task such as for example a task
of web-based travel agency. The reference task
has to correspond to a real-life application allow-
ing real user tests. Secondly, multi-level represen-
tation such as the semantic representation, the ty-
pology of linguistic phenomena and dialogic func-
tions, the dialog context model... will be com-
monly refined and adapted to the reference task.
The third part deals with the recording and la-
beling of a dialog corpus which will be used for
1http://www.cs.rochester.edu/research/trains/annotation
both system adaptation and test set selection. The
last part is the organisation of the evaluation cam-
paigns by ELRA/ELDA for the participating sites.
ELRA/ELDA is the coordinator of a larger
scope project: EVALDA which includes among
others, the MEDIA project. ELDA with VEC-
SYS will provide transcribed and annotated cor-
pora and evaluation tools according to consortium
specifications. The recording of 1200 French di-
alogs (240 speakers, 5 dialogs each, 15k user
queries) is planned. Three sets of LU and CU
units will be built from this corpus. A large size
adaptation set will be used by the participants to
adapt their system to the task and the semantic
representation. The development set (around 1K
LU (resp. CU) units) will be used to validate the
evaluation protocole. The size of the test set is
planned to be around 3K LU (resp. CU) units. Var-
ious approaches are currently used at the partici-
pating sites; stochastic or syntactic and semantic
rule-based modeling. The project started in Jan-
uary 2003 and will last two years.
5 Conclusion
Assessing the dialog system understanding capa-
bilities requires to evaluate the transition between
successive states of the dialog. At least, we must
be able to test a sequence of two states at any
point in the dialog. The dynamic and interac-
tive nature of the dialog makes construction and
reuse of test sets difficult. Furthermore, to eval-
uate one particular dialog transition, the system
has to be put in a particular state corresponding to
the original dialog context. The variable describ-
ing the dialog state can be composed of complex
information such as the current semantic frame
(list of triplets (mode,attribute,value) or quadru-
ples (mode, attribute, value, reference)), the dialog
history semantic frame and potentially other infor-
mation like recognition scores, dialog acts, etc.
The PEACE paradigm allows the evaluation of
two successive simplified dialog states. It has been
successfully tested with test samples focusing on
linguistic difficulties of literal and contextual un-
derstanding. For these tests, the dialog state is
the dialog history semantic frame. The contextual
understanding modeling in PEACE is system inde-
pendent since the context is given by a paraphrase
of queries. PEACE allows a diagnostic evaluation
of specific semantic attributes and particular lin-
guistic phenomena.
In our opinion, it is crucial for the dialog com-
munity to agree on a common reference task and
reference test sets in order to be able to compare
and diagnose dialog systems. Both evaluation with
real users and artificial simulation of successive
dialog states using test sets extracted from real cor-
pora have to be carried out in parallel. The use of
test sets reduces the global cost of dialog system
evaluation, moreover such tests are reproducible.
The PEACE protocol will be used as basis for
the French Technolangue MEDIA project in a two
year evaluation campaign where dialog systems
from both academia and industry will be evalu-
ated. In other domains, it could be related with
(Hirschman, 2000) propositions for Question An-
swering evaluation.
References
J.Y. Antoine and al. 2002. Predictive and objective evalua-
tion of speech understanding: the ?challenge? evaluation
campaign of the i3 speech workgroup of th french cnrs. In
LREC2002, Spain, May. ELRA.
H. Bonneau-Maynard and L. Devillers. 2000. A framework
for evaluating contextual understanding. In ICSLP.
H. Bonneau-Maynard, L. Devillers, and S. Rosset. 2000.
Predictive performance of dialog systems. In LREC2000,
volume 1, pages 177?181, Athens, Greece, May. ELRA.
J. Carletta. 1996. Assessing agreement on classification
tasks: the kappa statistics. Computational Linguistics,
2(22):249?254.
R. Winski D. Gibbon, R. Moore. 1997. Handbook of Stan-
dards and Ressources for Spoken Language Ressources.
Mouton de Gruyter, New York.
L. Devillers, H. Maynard, and P. Paroubek. 2002a.
Me?thodologies d?e?valuation des syste`mes de dia-
logue parle? : re?flexions et expe?riences autour de la
compre?hension. In Traitement Automatique des Langues,
volume 43, pages 155?184.
L. Devillers, S. Rosset, H. Bonneau-Maynard, and L. Lamel.
2002b. Annotations for dynamic diagnosis of the dialog
state. In LREC2002, Spain, May. ELRA.
L. Dybkj?r and al. 1998. The disc approach to spo-
ken language systems development and evaluation. In
LREC1998), volume 1, pages 185?189, Spain, May.
ELRA.
N. Fraser. 1998. Spoken Language System Assessment, vol-
ume 3. Mouton de Gruyter, New York.
E. Geoffrois, C. Barras, S. Bird, and Z. Wu. 2000. Tran-
scribing with annotation graphs. In LREC2000, volume 2,
pages 1517?1521, Greece, May. ELRA.
J. Glass, J. Polifroni, S. Seneff, and V. Zue. 2000. Data
collection and performance evaluation of spoken dialogue
systems: the MIT experience.
Lynette Hirschman. 2000. Reading comprehension and
question answering new evaluation paradigms for human
language technology. In LREC2000 Workshop ?Using
Evaluation within HLT Programs: Results and Trends?,
pages 54?59, Greece, May. ELRA.
N. Ide and L. Romary. 2002. Towards multimodal content
representation. In LREC 2002.
L. Lamel. 1998. Spoken language dialog system develop-
ment and evaluation at limsi. In Actes de l?International
Symposium on Spoken Dialogue, Sydney, Australia,
November.
MADCOW. 1992. Multi-site data collection for a spoken
language corpus. In DARPA Speech and Natural Lan-
guage Workshop.
C. Montacie? and G. Chollet. 1997. Syste`mes de re?fe?rence
pour l?e?valuation d?applications et la caracte?risation de
bases de donne?es en reconnaissance de la parole. In
16e`me JEP.
R.C. Moore. 1994. Semantic evaluation for spoken-language
systems. In DARPA Speech and Natural Language Work-
shop.
P. Sabatier, Ph. Blache, J. Guizol, F. Le?vy, A. Nazarenko,
and S. N?Guema. 2000. e?valuer des syste`mes de
compre?hension de textes. In Ressources et Evaluation en
Inge?nierie Linguistique, pages 265?275. Chibout K. et al
(Eds) Duculot.
M. Walker, D. Litman, C. Kamm, and A. Abella. 1998. Eval-
uating spoken dialogue agents with paradise: 2 cases stud-
ies. Computer Speech and Language, 3(12):317?347.
M. Walker, R. Passonneau, and J.E. Boland. 2001. Quantita-
tive and qualitative evaluation of darpa communicatorspo-
ken dialog systems. In Actes du 39me ACL, pages 515?
522, Toulouse, France, July. ACL.
S. Whittaker, L. Terveen, and B. Nardi. 2002. Reference task
agenda for HCI. In ISLE workshop 2002.
Coling 2008: Proceedings of the workshop on Human Judgements in Computational Linguistics, pages 17?23
Manchester, August 2008
Human judgment as a parameter in evaluation campaigns
Jean-Baptiste Berthelin and Cyril Grouin and Martine Hurault-Plantet and Patrick Paroubek
LIMSI-CNRS
BP 133
F-91403 Orsay Cedex
firstname.lastname@limsi.fr
Abstract
The relevance of human judgment in an
evaluation campaign is illustrated here
through the DEFT text mining campaigns.
In a first step, testing a topic for a cam-
paign among a limited number of human
evaluators informs us about the feasibility
of a task. This information comes from the
results obtained by the judges, as well as
from their personal impressions after pass-
ing the test.
In a second step, results from individual
judges, as well as their pairwise matching,
are used in order to adjust the task (choice
of a marking scale for DEFT?07 and selec-
tion of topical categories for DEFT?08).
Finally, the mutual comparison of com-
petitors? results, at the end of the evalu-
ation campaign, confirms the choices we
made at its starting point, and provides
means to redefine the task when we shall
launch a future campaign based on the
same topic.
1 Introduction
For the past four years, the DEFT1 (De?fi Fouille
de Texte) campaigns have been aiming to evalu-
ate methods and software developed by several re-
search teams in French text mining, on a variety of
topics.
The different editions concerned, in this or-
der, the identification of speakers in political
c
? 2008. Licensed under the Creative Commons
Attribution-Noncommercial-Share Alike 3.0 Unported li-
cense (http://creativecommons.org/licenses/by-nc-sa/3.0/).
Some rights reserved.
1See http://deft.limsi.fr/ for a presentation in
French.
speeches (2005), the topical segmentation of po-
litical, scientific and juridical corpora (2006), the
automatic affectation of opinion values to texts de-
veloping an argumented judgment (2007), and the
identification of the genre and topic of a docu-
ment (2008).
Human judgment was used during the prepara-
tion of the last two campaigns, to assess the dif-
ficulty of the task, and to see which parameters
could be modified. To do this, before the partic-
ipants start competing via their software, we put
human judges in front of versions of the task with
various sets of parameters. This allows us to adjust
the definition of the task according to which diffi-
culties were encountered, and how judges agree to-
gether. These human judges are in small number,
and belong to our team. However, results of the
campaign are automatically evaluated with refer-
ence to results attached to the corpus from the start.
This is because the evaluation of a campaign?s
results by human judges is expensive. For in-
stance, TREC2 international evaluation campaigns
are supported by the NIST institute and funded by
state agencies. In Europe, on the same domains,
the CLEF3 campaigns are funded by the Euro-
pean Commission, and in France, evaluation cam-
paigns are also funded by projects, such as Tech-
nolangue4. DEFT campaigns, however, are con-
ducted with small budgets. That means for us to
have selected corpora that contain the desired re-
sults. For instance, in a campaign for topical cat-
egorization, we must start with a topically tagged
corpus. By so doing, we also can, at the end of
a campaign, compare results from human judges
with results from competitors, using an identical
2http://trec.nist.gov
3http://www.clef-campaign.org
4http://www.technolangue.net
17
common reference.
In this paper, we describe experiments we per-
formed with human judgments when preparing
DEFT campaigns. We survey the various steps
in the preparation of the last two campaigns, and
we go through the detail of how human evalua-
tion, performed during these steps, led us to the
parametrization of these two campaigns. We also
present a comparative analysis of results found by
human judges and results submitted by competi-
tors in the challenge. We conclude about the rel-
evance of the human evaluation of a task, prior to
evaluating software dedicated to this task.
2 Parametrization of the campaign
We were competitors in the 2005 and 2006 edi-
tions, and became organisators for the 2007 and
2008 campaigns. For both challenges that we or-
ganized, we went through the classical steps of the
evaluation paradigm (Adda et al, 1999), to which
we systematically added a step of human test of the
task, in order to adjust those parameters that could
be modified. The steps, therefore, are following:
1. thinking about potential topics;
2. choice of a task and collection of corpora;
3. choice of measurements;
4. test of the task by human judges on an extract
of the corpus in order to precisely define its
parameters;
5. launching the task, recruiting participants;
6. testing period;
7. adjudication: possibility of complaints about
the results;
8. workshop that closes the campaign.
Whenever human judges have to evaluate the
results of participants in a campaign, the main
problems are about correctly defining the judging
criteria to be applied by judges, and that judges
be in sufficient number to vote on judging each
document. Hovy et al (2002) describe work to-
ward formalization of software evaluation method-
ology in NLP, developed in the EAGLES5 and
5http://www.ilc.cnr.it/EAGLES96/home.
html
ISLE6 projects. For cost-efficiency reasons, au-
tomatic evaluation is relevant, and its results have
sometimes been compared to results from human
judges. For instance, Eck and Hori (2005) com-
pare results of evaluation measurements used in
automatic translation with human judgments on
the same corpus. In (Burstein and Wolska, 2003),
the authors describe an experiment in the evalua-
tion of writing style and find a better agreement
between the automatic evaluation system and one
human judge, than between two human judges.
Returning to the DEFT campaign, once the task
is chosen, the corpora are collected, and evaluation
measurements are defined, there can remain some
necessity of adjusting parameters, according to the
expected difficulty of the task. This could be, for
instance, the level of granularity in a task of top-
ical segmentation, or which categories should be
relevant in a task of categorization. To get this ad-
justing done, we submit the task to human judges.
In 2007, the challenge was about the automatic
affectation of opinion values to texts developing an
argumented judgment (Grouin et al, 2007). We
collected opinion texts already tagged by an opin-
ion value, such as film reviews that, in addition
to a text giving the judgment of the critic on the
film, also feature a mark in the shape of a variable
number of stars. The adjustable parameter of the
task, therefore, is the scale of opinion values. The
task will be more or less difficult, according to the
range of this scale.
The 2008 campaign was about classifying a set
of documents by genre and topic (Hurault-Plantet
et al, 2008). The choice of genres and topics is
a crucial one. Some pairs of topics or genres are
more difficult to separate than other ones. We also
had to find different genres sharing a set of topical
categories, while corpora in French are not so very
abundant. So we selected two genres, encyclo-
pedia and daily newspaper, and about ten general
topical categories. The parameter we had to ad-
just was the set of categories to be matched against
each other.
3 Assessing the difficulty of a task
3.1 Calibration of an opinion value scale
In 2007, the challenge was about the automatic af-
fectation of opinion values to texts developing an
argumented judgment. In view of that, we col-
lected four corpora that covered various domains:
6http://www.ilc.cnr.it/EAGLES96/isle/
18
reviews of films and books, of video games and of
scientific papers, as well as parliamentary debates
about a draft law.
Each corpus had the interesting feature of com-
bining a mark or opinion with a descriptive text, as
the mark was used to sum up the judment in the
argumentative part of this text. Due to the diver-
sity of sources, we found as many marking scales
as involved copora:
? 2 values for parliamentary debates7 (the rep-
resentative who took part in the debate was
either in favour or in disfavour of the draft
law) ;
? 4 values for scientific paper reviews (accepted
as it stands ? accepted with minor changes
? accepted with major changes and second
overall reviewing ?rejected), based on a set of
criteria including interestingness, relevance
and originality of the paper?s content ;
? 5 values for film and book reviews8 (a mark
between 0 and 4, from bad to excellent) ;
? 20 values for video game reviews9 (a global
mark calculated from a set of advices about
various aspects of the game: graphics, playa-
bility, life span, sound track and scenario).
In order to, first, assess the feasibility of the task,
and to, secondly, define the scale of values to be
used in the evaluation campaign, we submitted hu-
man judges to several tests (Paek, 2001): they were
instructed to assign a mark on two kinds of scale, a
wide one with the original values, and a restricted
one with 2 or 3 values, depending on the corpus it
was applying to. The results from various judges
were evaluated in terms of precision and recall, and
matched to each other by way of the Kappa coeffi-
cient (Carletta, 1996) (Cohen, 1960).
We present hereunder the values of the ? coef-
ficient between pairs of human judges, and with
the reference, on the video game corpus. The wide
scale (Table 1) uses the original values (marks be-
tween 0 and 20), while the restricted scale (Ta-
ble 2) relies upon 3 values with following defini-
tions: class 0 for original marks between 0 and 10,
class 1 for marks between 11 and 14, and class 2
for marks between 15 and 20.
7http://www.assemblee-nationale.fr/12/
debats/
8http://www.avoir-alire.com
9http://www.jeuxvideo.com/etajvbis.htm
Judge Ref. 1 2 3
Ref. 0.17 0.12 0.07
1 0.17 0.03 0.05
2 0.12 0.03 0.07
3 0.07 0.05 0.07
Table 1: Video game corpus: wide scale, marks
from 0 to 20.
Judge Ref. 1 2 3
Ref. 0.74 0.79 0.69
1 0.74 0.74 0.54
2 0.79 0.74 0.69
3 0.69 0.54 0.69
Table 2: Video game corpus: restricted scale,
marks from 0 to 2.
Table 1 and 2 show that agreement between
judges varies widely when marking scales are
modified. Table 1 shows that there is an insuffi-
cient agreement among judges on the wide scale,
with ? coefficients lower than 0.20, while the
agreement between these same judges can be con-
sidered as good on the restricted scale, with ? co-
efficients between 0.54 and 0.79 (Table 2), the me-
dian being at 0.74.
In order to confirm the validity of the change
in scales, we used the ? to test how each judge
agreed with himself, between his two sets of re-
sults (Table 3). Therefore, we compared judg-
ments made by each judge using the initial value
scale and converted towards the restricted scale,
with judgments made by the same judge directly
using the restricted value scale. This measurement
shows the degree of correspondence between both
scales for each judge. Among the three judges who
took part in the test, the first and third one agree
well with themselves, while for the second one, the
agreement is only moderate.
Judge 1 2 3
1 0.74
2 0.46
3 0.70
Table 3: Video game corpus: agreement of each
judge with himself when scales change.
We did the same for a second corpus, of film re-
views. The test involved five judges, and the scale
19
change was smaller, since it was from five values
to three, and not from twenty to three. For this
scale change, we merged the two lowest values (0
and 1) into one (0), and the two highest ones (3
and 4) into one (2), and the middle value in the
wide scale (2) remained the intermediate one in
the restricted scale (1). This scale change was the
most relevant one, since, with 29.7% of the docu-
ments, the class of the middle mark (2) accounted
for almost one third of the corpus. However, the
two other groups of documents are less well bal-
anced. Indeed, the lowest mark concerns less doc-
uments than the highest one: 4.6% and 10.3% re-
spectively for the initial marks 0 and 1, while one
finds 39.8% and 15.6% of documents for the marks
3 and 4. Grouping the documents in only two
classes, by joining the middle class with the two
lowest ones, would have yielded a better balance
between classes, with 44.6% of documents for the
lower mark and 55.4% for the higher one, but that
would have been less meaningful.
Results from human judges are shown in the Ta-
bles 4 and 5 for both scales.
Judge Ref. 1 2 3 4 5
Ref. 0.10 0.29 0.39 0.46 0.47
1 0.10 0.37 0.49 0.48 0.35
2 0.29 0.37 0.36 0.30 0.43
3 0.39 0.49 0.36 0.49 0.54
4 0.46 0.48 0.30 0.49 0.60
5 0.47 0.35 0.43 0.54 0.60
Table 4: Film review corpus: wide scale, marks
from 0 to 4
Judge Ref. 1 2 3 4 5
Ref. 0.27 0.62 0.53 0.56 0.67
1 0.27 0.45 0.43 0.57 0.37
2 0.62 0.45 0.73 0.48 0.54
3 0.53 0.43 0.73 0.62 0.62
4 0.56 0.57 0.48 0.62 0.76
5 0.67 0.37 0.54 0.62 0.76
Table 5: Film review corpus: restricted scale,
marks from 0 to 2.
Agreements between human judges ranked from
bad to moderate for the wide scale (the five origi-
nal values in this corpus), while these agreements
rank from insufficient to good in the case of the
restricted scale with three values. We can see that
differences induced by the scale change are much
less important than with the video game corpus.
This agrees well with the scales being much closer
to each other.
By first performing a hand-made evaluation, and
secondly, matching between themselves the results
from the judges, we found a way to assess with
greater precision the difficulty of the evaluation
task we were about to launch. Concerning the
first two review corpora (films and books, video
games), we attached values good, average and bad
to the three selected classes. The scale for sci-
entific paper reviews was also restricted to three
classes for which following values were selected:
paper accepted as it stands or with minor edits, pa-
per accepted after major edits, paper rejected. Fi-
nally, since its original scale had only two values,
the corpus of parliamentary debates underwent no
change of scale.
3.2 Choice of a topical category set
In order to determine which topical categories
should be recognized in the 2008 task of classify-
ing documents by genre and topic, we performed a
manual evaluation of a sample of the corpus with 4
human judges. The sample included 30 Le Monde
papers for the journalistic genre, and 30 Wikipedia
entries for the encyclopedic genre. Only the title
and body of each article was kept in the sample,
and the tables were deleted. All marks of inclu-
sion in either corpus were also deleted (references
to Le Monde and Wikipedia tags).
The test ran this way: each article was put in a
separate file, and the evaluators had to identify the
genre and the topical category under which it was
published. All articles were included in one set,
which means evaluators had to choose, between all
categories and genres, which ones to match with
each document. This test was made with a first
selection of 8 categories, shared by both genres,
listed in Table 6.
Table 7 shows that results from human judges
in terms of precision and recall were excellent on
the identification of genre (F-scores between 0.94
and 1.00) and quite good on the identification of
categories (F-scores between 0.66 and 0.82).
We also proceeded to the pairwise matching of
results from human judges via the ? coefficient.
Results show an excellent agreement of judges
among themselves and with the reference for genre
identification (Table 8). The agreement is mod-
20
Le Monde Wikipedia
Notebook People
Economy Economy
France French Politics
International International Politics,
minus category
French Politic
Science Science
Society Society,
minus subcategories
Politics, People,
Sport, Media
Sport Sport
Television Television
Table 6: Correspondence between categories from
Le Monde and Wikipedia for the 8 categories in
the test.
Judge 1 2 3 4
Genres 1.00 0.98 0.97 0.94
Categories 0.79 0.77 0.82 0.66
Table 7: F-scores obtained by human judges on the
identification of genre and categories.
erate to good for categoy identification (Table 9).
These good results led us to keep the corpora as
they stood, since they appeared to constitute a
good reference for the defined task. However, we
made an exception for category Notebook (biogra-
phies of celebrities) which we discarded for two
reasons. First, it is more of a genre, namely, ?bi-
ography?, rather than a topical category. Secondly,
we found it rather difficult to assign a single cate-
gory to articles which could belong in two different
ones, as would be the case for the biography of a
sportsman, which would fall under both categories
Notebook et Sport.
Judge Re?f. 1 2 3 4
Re?f. 1.00 0.97 0.93 0.87
1 1.00 0.97 0.93 0.87
2 0.97 0.97 0.90 0.83
3 0.93 0.93 0.90 0.87
4 0.87 0.87 0.83 0.87
Table 8: ? coefficient between human judges and
the reference: Identification of genre.
Our task of genre and topic classification in-
Judge Re?f. 1 2 3 4
Re?f. 0.56 0.52 0.60 0.39
1 0.56 0.69 0.75 0.55
2 0.52 0.69 0.71 0.61
3 0.60 0.75 0.71 0.52
4 0.39 0.55 0.61 0.52
Table 9: ? coefficient between human judges and
the reference: Identification of categories.
cluded two subtasks, one being genre and topic
recognition for a first set of categories, the other
one being only topic recognition for a second set
of categories. Therefore, the corpus had to be di-
vided in two parts. In order to find which cate-
gories had to go into which subcorpus, we decided
to estimate, for each category, the difficulty of rec-
ognizing it. To do so, we calculated the precision
and recall of each evaluator for each category. This
measurement was obtained via a second evaluation
of human judges, with a wider set of categories (by
adding categories Art and Literature).
The ordering of categories by decreasing pre-
cision is following: Sport (1.00), International
(0.80), France (0.76), Literature (0.76), Art (0.74),
Television (0.71), Economy (0.58), Science (0.33),
Society (0.26). This means no document in the
Sport category was misclassified, and, contrari-
wise, categories Science and Society were the most
problematic ones.
The ordering by decreasing recall is slightly
different: International (0.87), Economy (0.80),
Sport (0.75), France (0.70), Art (0.62), Literature
(0.49), Television (0.46), Society (0.42), Science
(0.33). Hence, articles in the International cate-
gory were best identified. This ordering also con-
firms the difficulty felt by human judges concern-
ing the categories Society and Science.
We decided to distribute the categories for each
subtask according to a balance between easy and
diffucult ones in terms of human evaluation:
? Art, Economy, Sport, Television for the sub-
task with both genre and category recogni-
tion;
? France, International, Literature, Science,
Society for the subtask with only category
recognition. For this second subset, we put
together three categories which are topically
close (France, International and Society).
21
4 Human judgments and software
4.1 Confirming the difficulty of a task
The 2007 edition of DEFT highlighted two main
phenomena concerning the corpora involved in the
task.
First, each corpus yielded a different level of dif-
ficulty, and this gradation of difficulty among cor-
pora appeared both for human evaluators and com-
petitors in the challenge (Paroubek et al, 2007).
Judges Competitors
Debates 0.77/1.00 0.54/0.72
Game reviews 0.73/0.90 0.46/0.78
Film reviews 0.52/0.79 0.38/0.60
Paper reviews 0.41/0.58 0.40/0.57
Table 10: Minimal and maximal strict F-scores
between human evaluators and competitors in the
challenge, 2007 edition.
During human tests, judges mentioned the great
facility of finding about opinions expressed in the
corpus of parliamentary debate. Next came cor-
pora of video game reviews, and then of film and
book reviews, whose difficulty was considered av-
erage, and last, the corpus of scientific paper re-
views, which the judges perceived as particularly
difficult. This gradation of difficulty among cor-
pora was also found among competitors, following
the same ordering of three levels of difficulty.
Secondly, the difficulties met by human eval-
uators are also found in the case of competitors.
Upon finishing human tests, judges felt difficulties
in evaluating the corpus of scientific paper reviews,
yielding poor results. Now, the results of competi-
tors on the same corpus are quite as poor, occupy-
ing exactly the same value interval as for human
judges. Most competitors, by the way, obtained
their worst results on this corpus.
The alikeness of results between judges and
competitors reflects the complexity of the corpus:
when preparing the campaign, we observed that
reviews were quite short. Therefore, assigning a
value had to rely upon a small amount of data.
From that, we can derive a minimal size for docu-
ments to be used in this kind of evaluation. More-
over, a paper review can be seen as an aid for the
author, to be expressed as positively as possible,
even if it is also addressed to the Program Commit-
tee which has to accept or reject the paper. There-
fore, the mark could prove more negative than the
text of the review.
The case of comments about videogames is a
different one. Indeed, giving a global mark on a
scale of 20 is a difficult task. Therefore, this mark
comes most often from a sum of smaller marks
which rate either the whole document according
to various criteria, or parts of this document. In
our corpus, each reviewer rates the game accord-
ing to several criteria, namely, graphics, playa-
bility, life span, sound track and scenario, from
which a rather long text is produced, making the
judgment an easier task to perform. However, the
global mark differs from the sum of the smaller
ones from various criteria, hence the difficulty for
human judges to reckon this global mark on a scale
of 20.
4.2 Confirmation of the expected success of
competitors
Contrary to the 2007 edition, in which competi-
tors obtained results that confirmed those of human
judges, the 2008 edition gave them the opportunity
to reach a higher level than human evaluators.
While genre identification yielded no special
problem, either for human evaluators or for com-
petitors, and the results obtained by both groups
are similar, competitors reached better results than
human judges in topical categorization.
Concerning genre identification, strict F-scores
are situated between 0.94 and 1.00 for human
judges, and between 0.95 and 0.98 for the best
runs of competitors (each competitor was allowed
to submit up to three collections of results, only
the best one being used for the final ranking). As
for topical categorization, strict F-scores go from
0.66 to 0.82 for human evaluators, and from 0.84
to 0.89 for best runs from competitors.
The equivalence of results on genre identifica-
tion between judges and competitors can be ex-
plained by the fact that it was a simple, binary
choice (the newspaper Le Monde vs. Wikipedia).
Contrariwise, competitors obtained better re-
sults in topical categorization, since machines have
a stronger abstraction capacity than humans in
presence of the 9 topical categories we defined
(Art, Economy, France, International, Literature,
Science, Society, Sport and Television). However,
conditions were not quite similar, since human
judges had to pick a category among eight, and
not, like the automatic systems, a category within
two subsets of four and five categories. Indeed,
22
we dispatched the categories into two sets, by bal-
ancing categories that are easy or difficult for hu-
man evaluators. For the second set of categories,
we carefully put together three semantically close
ones, (France, International and Society, all three
of them being about political and societal con-
tents), to make the task more difficult. Although
the second set of categories seems more compli-
cated for human judges, half of the competitors ob-
tained better results in topical categorization of the
second set than of the first one.
5 Conclusion
The relevance of human judgment in an evaluation
campaign is present from the beginning to the end
of a campaign.
In a first step, testing a topic for a campaign
among a limited number of human evaluators al-
lows us to check the feasibility of a task. This
checking relies both on the results obtained by
judges (recall, precision, F-scores) and on their
personal impressions after passing the test.
In a second step, the study of both the results ob-
tained by the judges, and their pairwise matching
involving such a comparator as the ? coefficient
allows us to adjust the task (choice of a marking
scale for DEFT?07 and selection of topical cate-
gories for DEFT?08).
Finally, the mutual comparison of competitors?
results, at the end of the evaluation campaign, al-
lows us to validate the choices we made at its start-
ing point, and even to reposition the task when we
shall launch a future campaign based on the same
topic.
References
Adda, Gilles, Joseph Mariani, Patrick Paroubek, Mar-
tin Rajman, and Josette Lecomte. 1999. L?action
GRACE d?e?valuation de l?assignation des parties du
discours pour le franc?ais. Langues, 2(2):119?129,
juin.
Burstein, Jill and Magdalena Wolska. 2003. Toward
evaluation of writing style: Finding overly repetitive
word use in student essays. In 10th Conference of the
European Chapter of the Association for Computa-
tional Linguistics, EACL?03, pages 35?42, Budapest,
Hungary, april.
Carletta, J. 1996. Assessing agreement on classifica-
tion tasks: the kappa statistics. Computational Lin-
guistics, 2(22):249?254.
Cohen, Jacob. 1960. A coefficient of agreement
for nominal scales. Educational and Psychological
Measurement, (20):37?46.
Eck, Matthias and Chiori Hori. 2005. Overview of
the iwslt 2005 evaluation campaign. In International
Workshop on Spoken Language Translation, pages
5?14, Pittsburg, PA.
Grouin, Cyril, Jean-Baptiste Berthelin, Sarra El Ayari,
Thomas Heitz, Martine Hurault-Plantet, Miche`le
Jardino, Zohra Khalis, and Michel Lastes. 2007.
Pre?sentation de DEFT?07 (D ?Efi Fouille de Textes).
In Actes de l?atelier de clo?ture du 3e`me D ?Efi
Fouille de Textes, pages 1?8, Grenoble. Association
Franc?aise d?Intelligence Artificielle.
Hovy, Eduard, Margaret King, and Andrei Popescu-
Belis. 2002. Principles of context-based machine
translation evaluation. Machine Translation.
Hurault-Plantet, Martine, Jean-Baptiste Berthelin,
Sarra El Ayari, Cyril Grouin, Patrick Paroubek, and
Sylvain Loiseau. 2008. Re?sultats de l?e?dition 2008
du D ?Efi Fouille de Textes. In Actes TALN?08, Avi-
gnon. Association pour le Traitement Automatique
des Langues.
Paek, Tim. 2001. Empirical Methods for Evaluat-
ing Dialog Systems. In Proceedings of the ACL
2001 Workshop on Evaluation Methodologies for
Language and Dialogue Systems, pages 3?10.
Paroubek, Patrick, Jean-Baptiste Berthelin, Sarra El
Ayari, Cyril Grouin, Thomas Heitz, Martine Hurault-
Plantet, Miche`le Jardino, Zohra Khalis, and Michel
Lastes. 2007. Re?sultats de l?e?dition 2007 du D ?Efi
Fouille de Textes. In Actes de l?atelier de clo?ture du
3e`me D ?Efi Fouille de Textes, pages 9?17, Grenoble.
Association Franc?aise d?Intelligence Artificielle.
23
Coling 2008: Proceedings of the workshop on Cross-Framework and Cross-Domain Parser Evaluation, pages 36?43
Manchester, August 2008
Large Scale Production of Syntactic Annotations to Move Forward
Patrick Paroubek, Anne Vilnat, Sylvain Loiseau
LIMSI-CNRS
BP 133 91403 Orsay Cedex
France
prenom.nom@limsi.fr
Gil Francopoulo
Tagmatica
126 rue de Picpus 75012 Paris
France
gil.francopoulo@tagmatica.com
Olivier Hamon
ELDA and LIPN-P13
55-57 rue Brillat-Savarin 75013 Paris,
France
hamon@elda.org
Eric Villemonte de la Clergerie
Alpage-INRIA
Dom. de Voluceau Rocquencourt,
B.P. 105, 78153 Le Chesnay, France
Eric.De La Clergerie@inria.fr
Abstract
This article presents the methodology of
the PASSAGE project, aiming at syntacti-
cally annotating large corpora by compos-
ing annotations. It introduces the anno-
tation format and the syntactic annotation
specifications. It describes an important
component of the methodolgy, namely an
WEB-based evaluation service, deployed
in the context of the first PASSAGE parser
evaluation campaign.
1 Introduction
The last decade has seen, at the international level,
the emergence of a very strong trend of researches
on statistical methods in Natural Language Pro-
cessing. In our opinion, one of its origins, in
particular for English, is the availability of large
annotated corpora, such as the Penn Treebank
(1M words extracted from the Wall Street journal,
with syntactic annotations; 2
nd
release in 1995
1
,
the British National Corpus (100M words cover-
ing various styles annotated with parts of speech
2
),
or the Brown Corpus (1M words with morpho-
syntactic annotations). Such annotated corpora
were very valuable to extract stochastic grammars
or to parametrize disambiguation algorithms. For
instance (Miyao et al, 2004) report an experiment
where an HPSG grammar is semi-automatically
aquired from the Penn Treebank, by first annotat-
ing the treebank with partially specified derivation
c
? 2008. Licensed under the Creative Commons
Attribution-Noncommercial-Share Alike 3.0 Unported li-
cense (http://creativecommons.org/licenses/by-nc-sa/3.0/).
Some rights reserved.
1
http://www.cis.upenn.edu/
?
treebank/
2
http://www.natcorp.ox.ac.uk/
trees using heuristic rules , then by extracting lex-
ical entries with the application of inverse gram-
mar rules. (Cahill et al, 2004) managed to ex-
tract LFG subcategorisation frames and paths link-
ing long distance dependencies reentrancies from
f-structures generated automatically for the Penn-
II treebank trees and used them in an long distance
dependency resolution algorithm to parse new text.
They achieved around 80% f-score for fstructures
parsing on the WSJ part of the Penn-II treebank,
a score comparable to the ones of the state-of-
the-art hand-crafted grammars. With similar re-
sults, (Hockenmaier and Steedman, 2007) trans-
lated the Penn Treebank into a corpus of Combina-
tory Categorial Grammar (CCG) derivations aug-
mented with local and long-range word to word
dependencies and used it to train wide-coverage
statistical parsers. The development of the Penn
Treebank have led to many similar proposals of
corpus annotations
3
. However, the development of
such treebanks is very costly from an human point
of view and represents a long standing effort, in
particular for getting of rid of the annotation errors
or inconsistencies, unavoidable for any kind of hu-
man annotation. Despite the growing number of
annotated corpora, the volume of data that can be
manually annotated remains limited thus restrict-
ing the experiments that can be tried on automatic
grammar acquisition. Furthermore, designing an
annotated corpus involves choices that may block
future experiments from acquiring new kinds of
linguistic knowledge because they necessitate an-
notation incompatible or difficult to produce from
the existing ones.
With PASSAGE (de la Clergerie et al, 2008b),
we believe that a new option becomes possible.
3
http://www.ims.uni-stuttgart.de/
projekte/TIGER/related/links.shtml
36
Funded by the French ANR program on Data
Warehouses and Knowledge, PASSAGE is a 3-
year project (2007?2009), coordinated by INRIA
project-team Alpage. It builds up on the re-
sults of the EASy French parsing evaluation cam-
paign, funded by the French Technolangue pro-
gram, which has shown that French parsing sys-
tems are now available, ranging from shallow to
deep parsing. Some of these systems were nei-
ther based on statistics, nor extracted from a tree-
bank. While needing to be improved in robustness,
coverage, and accuracy, these systems has nev-
ertheless proved the feasibility to parse medium
amount of data (1M words). Preliminary experi-
ments made by some of the participants with deep
parsers (Sagot and Boullier, 2006) indicate that
processing more than 10 M words is not a prob-
lem, especially by relying on clusters of machines.
These figures can even be increased for shallow
parsers. In other words, there now exists sev-
eral French parsing systems that could parse (and
re-parse if needed) large corpora between 10 to
100 M words.
Passage aims at pursuing and extending the
line of research initiated by the EASy campaign
by using jointly 10 of the parsing systems that
have participated to EASy. They will be used to
parse and re-parse a French corpus of more than
100 M words along the following feedback loop
between parsing and resource creation as follows
(de la Clergerie et al, 2008a):
1. Parsing creates syntactic annotations;
2. Syntactic annotations create or enrich linguis-
tic resources such as lexicons, grammars or
annotated corpora;
3. Linguistic resources created or enriched on
the basis of the syntactic annotations are then
integrated into the existing parsers;
4. The enriched parsers are used to create richer
(e.g., syntactico-semantic) annotations;
5. etc. going back to step 1
In order to improve the set of parameters of
the parse combination algorithm (inspired from
the Recognizer Output Voting Error Reduction,
i.e. ROVER, experiments), two parsing evalu-
ation campaigns are planned during PASSAGE,
the first of these already took place at the end of
2007 (de la Clergerie et al, 2008b). In the follow-
ing, we present the annotation format specification
and the syntactic annotation specifications of PAS-
SAGE, then give an account of how the syntactic
annotations were compared in the first evaluation
campaign, by first describing the evaluation met-
rics and the web server infrastructure that was de-
ployed to process them. We conclude by showing
how the results so far achieved in PASSAGE will
contribute to the second part of the project, extract-
ing and refining enriched linguistic annotations.
2 PASSAGE Annotation Format
The aim is to allow an explicit representation of
syntactic annotations for French, whether such an-
notations come from human annotators or parsers.
The representation format is intended to be used
both in the evaluation of different parsers, so the
parses? representations should be easily compara-
ble, and in the construction of a large scale anno-
tation treebank which requires that all French con-
structions can be represented with enough details.
The format is based on three distinct specifica-
tions and requirements:
1. MAF (ISO 24611)
4
and SynAF (ISO 24615)
5
which are the ISO TC37 specifications for
morpho-syntactic and syntactic annotation
(Ide and Romary, 2002) (Declerck, 2006)
(Francopoulo, 2008). Let us note that these
specifications cannot be called ?standards?
because they are work in progress and these
documents do not yet have the status Pub-
lished Standard. Currently, their official sta-
tus is only Committee Draft.
2. The format used during the previous TECH-
NOLANGUE/EASY evaluation campaign
in order to minimize porting effort for the ex-
isting tools and corpora.
3. The degree of legibility of the XML tagging.
From a technical point of view, the format is a
compromise between ?standoff? and ?embedded?
notation. The fine grain level of tokens and words
is standoff (wrt the primary document) but higher
levels use embedded annotations. A standoff nota-
tion is usually considered more powerful but less
4
http://lirics.loria.fr/doc pub/maf.pdf
5
http://lirics.loria.fr/doc pub/
N421 SynAF CD ISO 24615.pdf
37
Figure 1: UML diagram of the structure of an an-
notated document
readable and not needed when the annotations fol-
low a (unambiguous) tree-like structure. Let us
add that, at all levels, great care has been taken to
ensure that the format is mappable onto MAF and
SynAF, which are basically standoff notations.
The structure of a PASSAGE annotated docu-
ment may be summarized with the UML diagram
in Figure1. The document begins by the declara-
tion of all the morpho-syntactic tagsets (MSTAG)
that will be used within the document. These dec-
larations respect the ISO Standard Feature Struc-
ture Representation (ISO 24610-1). Then, tokens
are declared. They are the smallest unit address-
able by other annotations. A token is unsplittable
and holds an identifier, a character range, and a
content made of the original character string. A
word form is an element referencing one or sev-
eral tokens. It has has two mandatory attributes:
an identifier and a list of tokens. Some optional at-
tributes are allowed like a part of speech, a lemma,
an inflected form (possibly after spelling correc-
tion or case normalization) and morpho-syntactic
tags. The following XML fragment shows how
the original fragment ?Les chaises? can be repre-
sented with all the optional attributes offered by
the PASSAGE annotation format :
<T id="t0" start="0" end="3">
Les
</T>
<W id="w0" tokens="t0"
pos="definiteArticle"
lemma="le"
form="les"
mstag="nP"/>
<T id="t1" start="4" end="11">
chaises
</T>
<W id="w1" tokens="t1"
pos="commonNoun"
lemma="chaise"
form="chaises"
mstag="nP gF"/>
Note that all parts of speech are taken from the
ISO registry
6
(Francopoulo et al, 2008). As in
MAF, a word may refer to several tokens in or-
der to represent multi-word units like ?pomme de
terre?. Conversely, a unique token may be refered
by two different words in order to represent results
of split based spelling correction like when ?un-
etable? is smartly separated into the words ?une?
and ?table?. The same configuration is required to
represent correctly agglutination in fused preposi-
tions like the token ?au? that may be rewritten into
the sequence of two words ?`a? ?le?. On the con-
trary of MAF, cross-reference in token-word links
for discontiguous spans is not allowed for the sake
of simplicity. Let us add that one of our require-
ment is to have PASSAGE annotations mappable
onto the MAF model and not to map all MAF an-
notations onto PASSAGE model. A G element de-
notes a syntactic group or a constituent (see details
in section 3). It may be recursive or non-recursive
and has an identifier, a type, and a content made of
word forms or groups, if recursive. All group type
values are taken from the ISO registry. Here is an
example :
<T id="t0" start="0" end="3">
Les
</T>
<T id="t1" start="4" end="11">
chaises
</T>
<G id="g0" type="GN">
<W id="w0" tokens="t0"/>
<W id="w1" tokens="t1"/>
</G>
A group may also hold optional attributes like syn-
tactic tagsets of MSTAG type. The syntactic re-
lations are represented with a standoff annotations
which refer to groups and word forms. A relation
is defined by an identifier, a type, a source, and a
target (see details in section 3. All relation types,
like ?subject? or ?direct object? are mappable onto
the ISO registry. An unrestricted number of com-
ments may be added to any element by means of
the mark element (i.e. M). Finally, a ?Sentence?
6
Data Category Registry, see http://syntax.
inist.fr
38
element gathers tokens, word forms, groups, rela-
tions and marks and all sentences are included in-
side a ?Document? element.
3 PASSAGE Syntactic Annotation
Specification
3.1 Introduction
The annotation formalism used in PASSAGE
7
is
based on the EASY one(Vilnat et al, 2004) which
whose first version was crafted in an experimental
project PEAS (Gendner et al, 2003), with inspira-
tion taken from the propositions of (Carroll et al,
2002). The definition has been completed with the
input of all the actors involved in the EASY evalu-
ation campaign (both parsers? developers and cor-
pus providers) and refined with the input of PAS-
SAGE participants. This formalism aims at mak-
ing possible the comparison of all kinds of syn-
tactic annotation (shallow or deep parsing, com-
plete or partial analysis), without giving any ad-
vantage to any particular approach. It has six
kinds of syntactic ?chunks?, we call constituents
and 14 kinds of relations The annotation formal-
ism allows the annotation of minimal, continuous
and non recursive constituents, as well as the en-
coding of relations wich represent syntactic func-
tions. These relations (all of them being binary, ex-
cept for the ternary coordination) have sources and
targets which may be either forms or constituents
(grouping several forms). Note that the PASSAGE
annotation formalism does not postulate any ex-
plicit lexical head.
3.2 Constituent annotations
For the PASSAGE campaigns, 6 kinds of con-
stituents (syntactic ?chunks?) have been consid-
ered and are illustrated in Table 3.2:
? the Noun Phrase (GN for Groupe Nominal)
may be made of a noun preceded by a de-
terminer and/or by an adjective with its own
modifiers, a proper noun or a pronoun;
? the prepositional phrase (GP, for groupe
pr?epositionnel ) may be made of a preposi-
tion and the GN it introduces, a contracted
determiner and preposition, followed by the
introduced GN, a preposition followed by an
adverb or a relative pronoun replacing a GP;
7
Annotation guide: http://www.limsi.fr/
Recherche/CORVAL/PASSAGE/eval 1/2007 10
05PEAS reference annotations v11.12.html
? the verb kernel (NV for noyau verbal ) in-
cludes a verb, the clitic pronouns and possible
particles attached to it. Verb kernels may have
different forms: conjugated tense, present or
past participle, or infinitive. When the con-
jugation produces compound forms, distinct
NVs are identified;
? the adjective phrase (GA for groupe adjec-
tival) contains an adjective when it is not
placed before the noun, or past or present par-
ticiples when they are used as adjectives;
? the adverb phrase (GR for groupe adverbial )
contains an adverb;
? the verb phrase introduced by a preposition
(PV) is a verb kernel with a verb not inflected
(infinitive, present participle,...), introduced
by a preposition. Some modifiers or adverbs
may also be included in PVs.
GN - la tr`es grande porte
8
(the very big door);
- Rouletabille
- eux (they), qui (who)
GP - de la chambre (from the bedroom),
- du pavillon (from the lodge)
- de l`a (from there), dont (whose)
NV - j?entendais (I heared)
- [on ne l?entendait]
9
plus
(we could no more hear her)
- Jean [viendra] (Jean will come)
- [d?esob?eissant] `a leurs parents
(disobeying their parents),
- [ferm?ee] `a clef (key closed)
- Il [ne veut] pas [venir]
(He doesn?t want to come),
- [ils n??etaient] pas [ferm?es]
(they were not closed),
GA - les barreaux [intacts] (the intact bars)
- la solution [retenue] fut...
(the chosen solution has been...),
- les enfants [d?esob?eissants]
(the disobeying children)
GR - aussi (also)
- vous n?auriez [pas] (you would not)
PV - [pour aller] `a Paris (for going to Paris),
- de vraiment bouger (to really move)
Table 1: Constituent examples
39
3.2.1 Syntactic Relation annotations
The dependencies establish all the links between
the minimal constituents described above. All par-
ticipants, corpus providers and campaign organiz-
ers agreed on a list of 14 kinds of dependencies
listed below:
1. subject-verb (SUJ V): may be inside the
same NV as between elle and ?etait in elle
?etait (she was), or between a GN and a NV as
between mademoiselle and appelait in Made-
moiselle appelait (Miss was calling);
2. auxiliary-verb (AUX V), between two NVs
as between a and construit in: on a construit
une maison (we have built a house);
3. direct object-verb (COD V): the relation is
annotated between a main verb (NV) and a
noun phrase (GN), as between construit and
la premi`ere automobile in: on a construit la
premi`ere automobile (we have built the first
car);
4. complement-verb (CPL V): to link to the
verb the complements expressed as GP or PV
which may be adjuncts or indirect objects, as
between en quelle ann?ee and construit in en
quelle ann?ee a-t on construit la premi`ere au-
tomobile (In which year did we build the first
car);
5. modifier-verb (MOD V): concerns the con-
stituants which certainly modify the verb,
and are not mandatory, as adverbs or adjunct
clauses, as between profond?ement or quand
la nuit tombe and dort in Jean dort pro-
fond?ement quand la nuit tombe (Jean deeply
sleeps when the night falls);
6. complementor (COMP): to link the intro-
ducer and the verb kernel of a subordinate
clause, as between qu? and viendra in Je
pense qu?il viendra (I think that he will
come); it is also used to link a preposition and
a noun phrase when they are not contiguous,
preventing us to annotate them as GP;
7. attribute-subject/object (ATB SO): between
the attribute and the verb kernel, and precis-
ing that the attribute is relative to (a) the sub-
ject as between grand and est in il est grand
), or (b) the object as between ?etrange and
trouve in il trouve cette explication ?etrange;
8. modifier-noun (MOD N): to link to the noun
all the constituents which modify it, as the ad-
jective, the genitive, the relative clause... This
dependency is annotated between unique and
fen?etre in l?unique fen?etre (the unique win-
dow) or between de la chambre and la porte
in la porte de la chambre (the bedroom door);
9. modifier-adjective (MOD A): to relate to the
adjective the constituents which modify it, as
between tr`es et belle in ?la tr`es belle collec-
tion (the very impressive collection) or be-
tween de son fils and fi`ere in elle est fi`ere de
son fils (she is proud of her son);
10. modifier-adverb (MOD R): the same kind of
dependency than MOD A for the adverbs, as
between tr`es and gentiment in elle vient tr`es
gentiment (she comes very kindly);
11. modifier-preposition (MOD P): to relate to
a preposition what modifies it, as between
peu and avant in elle vient peu avant lui (she
comes just before him);
12. coordination (COORD): to relate the coor-
dinate and the coordinated elements, as be-
tween Pierre, Paul and et in Pierre et Paul
arrivent (Paul and Pierre are arriving);
13. apposition (APP): to link the elements which
are placed side by side, when they refer to the
same object, as between le d?eput?e and Yves
Tavernier in Le d?eput?e Yves Tavernier ... (the
Deputy Yves Tavernier...);
14. juxtaposition (JUXT): to link constituents
which are neither coordinate nor in an appo-
sition relation, as in enumeration. It also links
clauses as on ne l?entendait et elle ?etait in
on ne l? entendait plus ... elle ?etait peut-?etre
morte (we did not hear her any more... per-
haps she was dead).
Some dependencies are illustrated in the two an-
notated sentences illutrated in figure . These anno-
tations have been made using EasyRef, a specific
Web annotation tool developed by INRIA.
4 PASSAGE First Evaluation Campaign
4.1 Evalution Service
The first PASSAGE evaluation campaign was
carried out in two steps. During the ini-
tial one-month development phase, a develop-
ment corpus was used to improve the quality of
40
Figure 2: Example of two sentences annotations
parsers. This development corpus from the TECH-
NOLANGUE/EASY is composed of 40,000 sen-
tences, out of which 4,000 sentences have been
manually annotated for the gold standard. Based
on these annotated sentences, an automatic WEB-
based evaluation server provides fast performance
feedback to the parsers? developers. At the end
of this first phase, each participant indicated what
he thought was his best parser run and got evalu-
ated on a new set of 400 sentences selected from
another part of the developement corpus which
meanwhile had been manually annotated for the
purpose and kept undisclosed.
The two phases represent a strong effort for the
evaluators. To avoid adding the cost of managing
the distribution and installation of the evaluation
package at each developer?s site, the solution of the
WEB evaluation service was chosen. A few infras-
tructures have been already experimented in NLP,
like GATE (Cunningham et al, 2002) infrastruc-
tures, but to our knowledge none has been used to
provide an WEB-based evaluation service as PAS-
SAGE did. The server was designed to manage
two categories of users: parser developers and or-
ganizers. To the developers, it provides, almost in
real time, confidential and secure access to the au-
tomatic evaluation of their submitted parses. To
the organizers, it give access to statistics enabling
them to follow the progress made by the develop-
ers, and easy management of the test phase. The
evaluation server provides, through a simple WEB
browser, access to both coarse and fine grain statis-
tics to a developer?s performance evaluation, glob-
ally for the whole corpus, at the level of a partic-
ular syntactic annotation or of a particular genre
specific subcorpus, and also at the level of a single
annotation for a particular word form.
Figure 3: Overall functional relations results
4.2 Performance Results
Ten systems participated to the constituents anno-
tation task. For most of the systems, F-measure is
up to 90% and only three systems are between 80%
and 90%. The trend is quite the same for Recall
and Precision. Around 96.5% of the constituents
returned by the best system are correct and it found
95.5% of the constituents present in gold standard.
Figure 3 shows the results of the seven systems that
participated to the functional relations annotation
task. Performance is lower than for constituents
and differences between systems are larger, an evi-
dence that the task remains more difficult. No sys-
tems gets a performance above 70% in F-measure,
three are above 60% and two above 50%. The last
two systems are above 40%.
4.3 Systems Improvements
The higher system gets increasing results from the
beginning of the development phase to the test
phase for both constituents and relations. How-
ever, although the increase for relations is rather
continuous, constituents results grow during the
first few development evaluations, then reach a
threshold from which results do not vary. This
can be explained by the fact that the constituent
scores are rather high, while for relations, scores
are lower and starting from low scores.
Using the evaluation server, system improves
its performance by 50% for the constituents and
600% for the relations, although performance vary
according to the type of relation or constituent.
Moreover, in repeating development evaluations,
another consequence was the convergence of pre-
cision and recall.
41
5 Parser?s outputs combination
The idea to combine the output of systems partic-
ipating to an evalauation campaign in order to ob-
tain a combination with better performance than
the best one was invented to our knowledge by J.
Fiscus (Fiscus, 1997) in a DARPA/NIST speech
recognition evaluation (ROVER/Reduced Output
Voting Error Reduction). By aligning the out-
put of the participating speech transcription sys-
tems and by selecting the hypothesis which was
proposed by the majority of the systems, he ob-
tained better performances than these of the best
system. The idea gained support in the speech pro-
cessing community(L?o?of et al, 2007) and in gen-
eral better results are obtained with keeping only
the output of the two or three best performing sys-
tems, in which case the relative improvement can
go up to 20% with respect to the best performance
(Schwenk and Gauvain, 2000). For text process-
ing, the ROVER procedure was applied to POS
tagging (Paroubek, 2000) and machine translation
(Matusov et al, 2006).
In our case, we will use the text itself to realign
the annotations provided by the various parser be-
fore computing their combination, as we did for
our first experiments with the EASY evaluation
campaign data (Paroubek et al, 2008). Since it
is very likely taht the different parsers do not use
the same word and sentence segmentation, we will
realign all the data along a common word and sen-
tence segmentation obtained by majority vote from
the different outputs.
But our motivation for using such procedure
is not only concerned with performance improve-
ment but also with the obtention of a confidence
measure for the annotation since if all systems
agree on a particular annotation, then it is very
likely to be true.
At this stage many options are open for the way
we want to apply the ROVER algorithm, since we
have both constituents and relations in our anno-
tations. We could vary the selection order (be-
tween constituents and relations), or use differ-
ent comparison functions for the sources/targets of
constituents/relations(Patrick Paroubek, 2006), or
perform incremental/global merging of the annoa-
tions, or explore different weightings/thresholding
strategies etc. In passage, ROVER experiments
are only beginning and we have yet to determine
which is the best strategy before applying it to
word and sentence free segmentation data. In the
early experiment we did with the ?EASy classic?
PASSAGE track which uses a fixed word and sen-
tence segmentation, we measured an improvement
in precision for some specific subcorpora and an-
notations but improvement in recall was harder to
get.
6 Conclusion
The definition of a common interchange syntactic
annotation format is an essential element of any
methodology aiming at the creation of large an-
notated corpora from the cooperation of parsing
systems to acquire new linguistic knowledge. But
the formalism aquires all of its value when backed-
up by the deployment of a WEB-based evaluation
service as the PASSAGE examples shows. 167
experiments were carried out during the develop-
ment phase (around 17 experiments per participant
in one month). The results of the test phase were
available less than one hour after the end of the de-
velopment phase. The service proved so success-
ful that the participants asked after the evaluation,
that the evaluation service be extended to support
evaluation as a perennial service
References
Cahill, Aoife, Michael Burke, Ruth O?Donovan, Josef
Van Genabith, and Andy Way. 2004. Long-distance
dependency resolution in automatically acquired
wide-coverage pcfg-based lfg approximations. In
Proceedings of the 42nd Meeting of the Association
for Computational Linguistics (ACL?04), Main Vol-
ume, pages 319?326, Barcelona, Spain, July.
Carroll, J., D. Lin, D. Prescher, and H. Uszkoreit.
2002. Proceedings of the workshop beyond parse-
val - toward improved evaluation measures for pars-
ing systems. In Proceedings of the 3rd International
Conference on Language Resources and Evaluation
(LREC), Las Palmas, Spain.
Cunningham, Hamish, Diana Maynard, Kalina
Bontcheva, and Valentin Tablan. 2002. Gate:
an architecture for development of robust hlt ap-
plications. In ACL ?02: Proceedings of the 40th
Annual Meeting on Association for Computational
Linguistics, pages 168?175, Morristown, NJ, USA.
Association for Computational Linguistics.
Declerck, T. 2006. Synaf: towards a standard for syn-
tactic annotation. In In proceedings of the fifth in-
ternational conference on Language Resources and
Evaluation (LREC 2006), Genoa, Italy, May. ELRA.
Fiscus, Jonathan G. 1997. A post-processing system
to yield reduced word error rates: recognizer output
voting error reduction (rover). In In proceedings of
42
the IEEE Workshop on Automatic Speech Recogni-
tion and Understanding, pages 347?357, Santa Bar-
bara, CA.
Francopoulo, G., T. Declerck, V. Sornlertlamvanich,
E. de la Clergerie, and M. Monachini. 2008. Data
category registry: Morpho-syntactic and syntactic
profiles. Marrakech. LREC.
Francopoulo, Gil. 2008. Tagparser: Well on the way
to iso-tc37 conformance. In In proceedings of the
International Conference on Global Interoperability
for Language Resources (ICGL), pages 82?88, Hong
Kong, January.
Gendner, V?eronique, Gabriel Illouz, Mich`ele Jardino,
Laura Monceaux, Patrick Paroubek, Isabelle Robba,
and Anne Vilnat. 2003. Peas the first instanciation
of a comparative framework for evaluating parsers of
french. In Proceedings of the 10
th
Conference of the
European Chapter fo the Association for Computa-
tional Linguistics, pages 95?98, Budapest, Hungary,
April. ACL. Companion Volume.
Hockenmaier, Julia and Mark Steedman. 2007. Ccg-
bank: A corpus of ccg derivations and dependency
structures extracted from the penn treebank. Com-
putational Linguistics, 33(3):355?396.
Ide, N. and L. Romary. 2002. Standards for language
ressources. Las Palmas. LREC.
L?o?of, J., C. Gollan, S. Hahn, G. Heigold, B. Hoffmeis-
ter, C. Plahl, D. Rybach, R. Schl?uter, , and H. Ney.
2007. The rwth 2007 tc-star evaluation system for
european english and spanish. In In proceedings of
the Interspeech Conference, pages 2145?2148.
Matusov, Evgeny, N. Ueffing, and Herman Ney. 2006.
Automatic sentence segmentation and punctuation
prediction for spoken language translation. In Pro-
ceedings of the International Workshop on Spo-
ken Language Translation (IWSLT), pages 158?165,
Trento, Italy.
de la Clergerie, Eric, Christelle Ayache, Ga?el de Chal-
endar, Gil Francopoulo, Claire Gardent, and Patrick
Paroubek. 2008a. Large scale production of syntac-
tic annotations for french. In In proceedings of the
First Workshop on Automated Syntactic Annotations
for Interoperable Language Resources at IGCL?08,
pages 45?52, Hong Kong, January.
de la Clergerie, Eric, Olivier Hamon, Djamel Mostefa,
Christelle Ayache, Patrick Paroubek, and Anne Vil-
nat. 2008b. Passage: from french parser evalua-
tion to large sized treebank. In ELRA, editor, In
proceedings of the sixth international conference on
Language Resources and Evaluation (LREC), Mar-
rakech, Morroco, May. ELRA.
Miyao, Yusuke, Takashi Ninomiya, and Jun?ichi Tsu-
jii. 2004. Corpus-oriented grammar development
for acquiring a head-driven phrase structure gram-
mar from the penn treebank. In In Proceedings of
the First International Joint Conference on Natural
Language Processing (IJCNLP-04).
Paroubek, Patrick, Isabelle Robba, Anne Vilnat, and
Christelle Ayache. 2008. Easy, evaluation of parsers
of french: what are the results? In Proceedings of
the 6
th
International Conference on Language Re-
sources and Evaluation (LREC), Marrakech, Mor-
roco.
Paroubek, Patrick. 2000. Language resources as by-
product of evaluation: the multitag example. In
In proceedings of the Second International Con-
ference on Language Resources and Evaluation
(LREC2000), volume 1, pages 151?154.
Patrick Paroubek, Isabelle Robba, Anne Vilnat Chris-
telle Ayache. 2006. Data, annotations and mea-
sures in easy - the evaluation campaign for parsers
of french. In ELRA, editor, In proceedings of
the fifth international conference on Language Re-
sources and Evaluation (LREC 2006), pages 315?
320, Genoa, Italy, May. ELRA.
Sagot, Beno??t and Pierre Boullier. 2006. Efficient
parsing of large corpora with a deep lfg parser. In
In proceedings of the sixth international conference
on Language Resources and Evaluation (LREC),
Genoa, Italy, May. ELDA.
Schwenk, Holger and Jean-Luc Gauvain. 2000. Im-
proved rover using language model information. In
In proceedings of the ISCA ITRW Workshop on Au-
tomatic Speech Recognition: Challenges for the new
Millenium, pages 47?52, Paris, September.
Vilnat, A., P. Paroubek, L. Monceaux, I. Robba,
V. Gendner, G. Illouz, and M. Jardino. 2004. The
ongoing evaluation campaign of syntactic parsing of
french: Easy. In Proceedings of the 4
th
International
Conference on Language Resources and Evaluation
(LREC), pages 2023?2026, Lisbonne, Portugal.
43
