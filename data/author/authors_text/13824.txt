Proceedings of the NAACL HLT 2010 Student Research Workshop, pages 46?51,
Los Angeles, California, June 2010. c?2010 Association for Computational Linguistics
 
Identifying Opinion Holders and Targets with Dependency Parser in 
Chinese News Texts 
Bin Lu 
Department of Chinese, Translation and Linguistics & 
Language Information Sciences Research Centre 
City University of Hong Kong 
Kowloon, Hong Kong 
lubin2010@gmail.com 
Abstract 
In this paper, we propose to identify 
opinion holders and targets with 
dependency parser in Chinese news texts, 
i.e. to identify opinion holders by means of 
reporting verbs and to identify opinion 
targets by considering both opinion holders 
and opinion-bearing words. The 
experiments with NTCIR-7 MOAT?s 
Chinese test data show that our approach 
provides better performance than the 
baselines and most systems reported at 
NTCIR-7. 
1 Introduction 
In recent years, sentiment analysis, which mines 
opinions from information sources such as news, 
blogs and product reviews, has drawn much 
attention in the NLP field (Hatzivassiloglou and 
McKeown, 1997; Pang et al, 2002; Turney, 2002; 
Hu and Liu, 2004).  
An opinion expressed in a text involves different 
components, including opinion expression, opinion 
holder and target (Wilson and Wiebe, 2003). 
Opinion holder is usually an entity that holds an 
opinion, and opinion target is what the opinion is 
about (Kim and Hovy, 2006). Although there have 
been research on identifying opinion holders and 
targets in  English product reviews and news texts, 
little work has been reported on similar tasks 
involving Chinese news texts.   
In this study, we investigate how dependency 
parsing can be used to help the task on opinion 
holder/target identification in Chinese news texts. 
Three possible contributions from this study are: 1) 
we propose that the existence of reporting verbs is 
a very important feature for identifying opinion 
holders in news texts, which has not been clearly 
indicated; 2) we argue that the identification of 
opinion targets should not be done alone without 
considering opinion holders, because opinion 
holders are much easier to be identified in news 
texts and the identified holders are quite useful for 
the identification of the associated targets. Our 
approach shows encouraging performance on 
opinion holder/target identification, and the results 
are much better than the baseline results and most 
results reported in NTCIR-7 (Seki et al, 2008).  
The paper is organized as follows. Sec. 2 
introduces related work. Sec. 3 gives the linguistic 
analysis of opinion holder/target. The proposed 
approach is described in Sec. 4, followed by the 
experiments in Sec. 5. Lastly we conclude in Sec. 6. 
2 Related Work 
Although document-level sentiment analysis 
(Turney, 2002; Pang et al, 2002) can provide the 
overall polarity of the whole text, it fails to detect 
the holders and targets of the sentiment in texts.  
2.1 Opinion Holders/ Target Identification 
For opinion mining of product reviews, opinion 
holder identification is usually omitted under the 
assumption that opinion holder is the review writer; 
and opinion targets are limited to the product 
discussed and its features (Hu and Liu, 2004). But 
in news texts, opinion holders/targets are more 
diverse: all named entities and noun phrases can be 
opinion holders; while opinion targets could be 
noun phrases, verb phrases or even clauses (Kim 
and Hovy, 2006; Ruppenhofer et al 2008).  
Bethard et al (2004) identify opinion 
propositions and their holders by semantic parsing 
techniques. Choi et al (2005) and Kim and Hovy 
(2005) identify only opinion holders on the MPQA 
corpus (Wilson and Wiebe, 2003). Kim and Hovy 
(2006) proposed to map the semantic frames of 
FrameNet into opinion holder and target for only 
adjectives and verbs. Kim et al (2008) proposed to 
46
 use syntactic structures for target identification 
without considering opinion holders. Stoyanov and 
Cardie (2008) define opinion topic and target and 
treat the task as a co-reference resolution problem. 
For the identification of opinion holders/targets 
in Chinese, there were several reports at NTCIR-7 
(Seki et al, 2008). Xu et al (2008) proposed to use 
some heuristic rules for opinion holder/target 
identification. Ku et al (2008) treated opinion 
holder identification as a binary classification 
problem of determining if a word was a part of an 
opinion holder. 
2.2 Chinese Dependency Parsing 
Dependency structures represent all sentence 
relationships uniformly as typed dependency 
relations between pairs of words. Some major 
dependency relations for Chinese (Ma et al, 2004) 
include ??  (Subject-Verb, SBV), ??  (Verb-Object, 
VOB), ??(Attributive-Noun, ATT), ?? (Quantifier, 
QUN) and ??? ? (Independent structure, IS). 
Consider the following Chinese sentence: 
a) ?? ?? ???? ???? ?? ?
?? ? ?? ?? ? ?? ? ? 
Russian Foreign Minister Ivanov said that 
NATO's eastward expansion was "Towards the 
wrong direction." 
Its dependency tree is shown in Figure1. Its head 
is the verb? (said), whose subject and object are 
respectively ???????? (Russian Foreign 
Minister Ivanov) and the embedded clause ???
????????????? (NATO's eastward 
expansion was "towards the wrong direction.").  
3 Linguistic Analysis of Opinions 
The opinions in news text may be explicitly 
mentioned or be expressed indirectly by the types 
of words and the style of language (Wilson and 
Wiebe, 2003). Two kinds of lexical clues are 
exploited here for opinion holder/target 
identification:  
Reporting verbs: verbs indicating speech 
events; 
Opinion-bearing Words: words or phrases 
containing polarity (i.e. positive, negative or 
neutral). 
In sentence a) above, the reporting verb ? (said) 
indicates a speech event expressing an opinion 
given by the holder ?? ?? ???? (Russian 
Foreign Minister Ivanov). Meanwhile, the opinion-
bearing word ? ?  (wrong) shows negative 
attitude towards the target ? ? ? ? ? ? 
(NATO's eastward expansion).  
Therefore, we assume that a large proportion of 
holders are governed by such reporting verbs, 
while targets are usually governed by opinion-
bearing words/phrases. 
Opinion holders are usually named entities, 
including, but not limited to, person names (e.g. ?
????? /economist Ol), organization names 
(e.g. ???? /UK government), and personal 
titles (e.g. ???? /the economist). Opinion 
holders can also be common noun phrases, such as 
??  (companies), ?????  (two thousand 
students). Pronouns1 can also be opinion holders, 
e.g. ? (he), ?? (they), ?(I). Opinion targets are 
more abstract and diverse, and could be agents, 
concrete objects, actions, events or even abstract 
ideas. In addition to noun phrases, opinion targets 
could also be verb phrases or embedded clauses.  
4 Identifying Opinion Holders/Targets  
In this section, we introduce our approach of 
identifying opinion holders/targets. We use the 
dependency parser in the HIT LTP package 
(http://ir.hit.edu.cn/) to get the dependency 
relations of the simplified Chinese sentences 
converted from the traditional Chinese ones.  
4.1 Lexical Resources 
The reporting verbs were firstly collected from the 
Chinese sample data of NTCIR-6 OAPT (Seki et 
al., 2007) in which the OPINION_OPR tag was 
used to mark them. We then use HowNet, 
WordNet and Tongyici Cilin to extend the 
reporting verbs from 68 to 308 words through 
manual synonym search. Some frequently used 
reporting verbs include ?(say), ??(express), ?
?(think), etc. Some of the reporting verbs could 
also convey opinions, such as ?? (criticize), ?
? (condemn), ?? (praise), etc. 
For opinion-bearing words/phrases, we use The 
Lexicon of Chinese Positive Words (Shi and Zhu, 
2006) and The Lexicon of Chinese Negative Words 
(Yang and Zhu, 2006), which consist of 5046 
positive items and 3499 negative ones, respectively. 
                                                          
1 The resolution of the anaphor or co-reference has not been 
dealt with yet, i.e. the identified holders of the sentence are 
assumed to be in the same form as it appears in the sentence. 
47
  
Figure 1. Dependency Tree for Sentence a) 
4.2 Chinese Sentence Preprocessing (SP) 
To enhance the robustness of the dependency 
parser, named entities are first recognized with a 
traditional Chinese word segmentation tool with 
access to the very large LIVAC dictionary 
(http://www.livac.org) collected from Chinese 
news published in Hong Kong and Taiwan. The 
identified named entities, as well as the collected 
reporting verbs and opinion-bearing words are 
added to the user dictionary of the HIT LTP 
package to help parsing.  
Before parsing, the parentheses enclosing only 
English words or numbers are removed in 
sentences, because the parser cannot properly 
process the parentheses which may greatly 
influence the parsing result. 
4.3 Identifying Opinion Holders with 
Reporting Verbs 
4.3.1 Holder Candidate Generation 
Two hypotheses are used to identify opinion 
holders in opinionated sentences: 1) the subject of 
reporting verbs will be the opinion holders; 2) if no 
reporting verb is found, the author could be the 
opinion holder. In addition to the two hypotheses 
above, the following heuristic rules (HR) are used: 
1) Other words having relations with reporting 
verbs 
If the subject of reporting verbs is not found in 
the sentence, we will find the word having 
relationship of ATT, VOB or IS with the reporting 
verbs, because sometimes the parser may wrongly 
marked the subject as other relations.  
2) Colon processing in Headlines 
If no reporting verbs are found in news 
headlines, we just pick up the noun before the 
colon as the target candidate in the headlines 
because the author usually replaces the reporting 
verb with a colon due to length limitation. E.g. in 
the headline ????????? (Morgan: 
Economic growth has been shut down), the noun
??  (Morgan) before colon is chosen as the 
opinion holder.  
3) Holder in the previous sentence  
If no opinion holder is found in the current 
clause and one holder candidate is found in the 
previous clause, we just choose the opinion holder 
of the previous clause as the holder candidate, 
because an opinion holder may express several 
ideas through consecutive sentences or clauses. 
4.3.2 Holder Candidate Expansion (EP) 
Through the procedure of candidate generation, we 
may find a holder candidate containing only one 
single word. But the holder may be a word 
sequence instead of a single word. Thus we further 
expand the holder candidates from the core head 
word by the following rules:  
1) Attributive modifier (ATT)  
E.g. in sentence a) mentioned in Sec. 2.2, the 
subject of the reporting verb? (said) is ????
(Ivanov), which has the attributive noun ?? 
(Foreign Minister) modified further by an 
attributive noun ??(Russia). Therefore, the final 
extended opinion holder would be ??????
(Russian Foreign Minister Ivanov). 
2) Quantifier modifier and ?/? (and/or) 
E.g. the quantifier modifier ??(some) in the 
noun phrase ?????? (some Asian countries) 
should be part of the opinion holder. Sometime, we 
need to extend the holder across ?/?(and/or), e.g.
???????????? (Suharto and two 
other army generals). 
Furthermore, time nouns, numbers and words 
only containing one Chinese character (except for 
pronouns) are removed from the candidates, as 
they are unlikely to be opinion holders. 
4.4 Identifying Opinion Targets with 
Opinion-bearing Words 
48
 Here we propose to use automatically identified 
reporting verbs and opinion holders to help opinion 
target identification. The heuristic rules (HR) are 
as follows. 
1) If a candidate of opinion holder is 
automatically identified with a reporting verb in an 
opinionated sentence, we will try to find the 
subject in the embedded clause as the target 
candidate by the following two steps: a) Find the 
subject of the object verb of the reporting verb. E.g. 
in sentence a) in Sec. 2.2, the opinion target ??
????  (NATO's eastward expansion) is the 
subject of the verb ?  (was) in the embedded 
clause which is in turn the object of the reporting 
verb? (said); b) If no target candidate is found in 
step a, we try to find after the reporting verb the 
subject whose parent is an opinion-bearing word as 
the target candidate. 
2) If no target candidate is found in step 1, and 
no opinion holder is found in the sentence, we find 
the subject of the sentence as the target candidate, 
because the author may be the opinion holder and 
the target could be the subject of the sentence. 
3) If still no target candidate is found in step 2, 
we find the object in the sentence as the target 
because the object could be the opinion target in 
case there is no subject and no opinion holder. 
Target candidate expansion (EP) is similar to 
holder candidate expansion described in Sec. 4.3.2. 
If an opinion target is in the opinion holder 
candidates (we call it holder conflict, HC), we 
remove it from the target candidates, and then try 
to find another using the above procedure.  
5 Experiments 
We use the traditional Chinese test data in NTCIR-
7 MOAT (Seki et al, 2008) for our experiments. 
Out of 4465 sentences, 2174 are annotated as 
opinionated by the lenient standard, and the 
opinion holders of some opinionated sentences are 
marked as POST_AUTHOR denoting the author of 
the news article. We use the final list given by the 
organizers as the gold standard.  
Baselines for opinion holder identification:  
Baseline 1: We just use the subject of reporting 
verbs as the opinion holder, without sentence 
preprocessing described in Sec. 4.2 and any 
heuristic rules introduced in Sec. 4.3.1. 
Baseline 2: We also implement the CRF model 
for detecting opinion holders (Choi et al, 2006) by 
using CRF++. The training data is the NTCIR-6 
Chinese test data. The labels used by CRF 
comprise Holder, Parent of Holder, None (not 
holder or parent) and the features for each word in 
our implementation include: basic features (i.e. 
word, POS-tag, whether the word itself is a 
reporting verb or not), dependency features (i.e. 
parent word, POS-tag of its parent, dependency 
relation with its parent, whether its parent is a 
reporting verb) and semantic features (i.e. WSD 
entry in Tongyici Cilin, WSD entry of its parent). 
Baseline for opinion target identification:  
Baseline 1: we try to find the subject or object of 
opinion-bearing words as the targets. If both a 
subject and an object are found, we just simply 
choose the subject as the target. 
We evaluate performance using 3 measures: 
exact match (EM), head match (HM), and partial 
match (PM), similar to Choi et al (2006). We use 
three evaluation metrics: recall (Rec), precision 
(Pre), and F1. For opinion holder identification, we 
consider two cases: 1) all opinionated sentences; 2) 
only the opinionated sentences whose opinion 
holders do not contain POST_AUTHOR. The 
metric ALL_Pre reported below is the precision in 
case 1 which is the same with recall and F1. 
5.1 Results for Opinion Holder Identification 
The results for holder identification are shown in 
Table 1, from which we can observe that our 
proposed approach significantly outperforms the 
two baseline methods, including the unsupervised 
baseline 1 and the supervised baseline 2.  
  ALL_Pre Pre Rec F1 
EM 52.4 46.8 31.6 37.8 
HM 67.1 80.2 54.2 64.7 Baseline1  PM 72.1 89.3 60.4 72.0 
EM 45.5 34.7 18.1 23.8 
HM 55.2 63.6 33.1 43.6 Baseline2 (CRF) PM 55.6 64.9 33.8 44.4 
EM 69.8 74.4 63.6 68.5 
HM 72.5 79.2 67.7 73.0 Our 
Approach PM 75.7 85.1 72.7 78.4 
Table 1. Results for Opinion Holders 
Unexpectedly, even the unsupervised baseline 1 
achieves better performance than baseline 2 (the 
CRF-based method). The possible reasons are: 1) 
the training data is not large enough to cover the 
cases in the test data, resulting in low recall of the 
CRF model; 2) the features used by the CRF model 
could be refined to improve the performance. 
49
 Here we also evaluate the influences of the 
following three factors on the performance: 
sentences preprocessing (SP) in Sec. 4.2, holder 
expansion (EP) in Sec. 4.3.2 and the heuristic rules 
(HR) in Sec. 4.3.1. The results are shown in Figure 
2 for different combinations, in which BL refers to 
baseline 1.  
Influence of Factors
35
40
45
50
55
60
65
70
75
80
85
BL BL+SP BL+EP BL+SP+EP Our Approach
(BL+SP+EP+HR)
Approaches
F1
EM
HM
PM
 
Figure 2. Influences of Factors on Opinion Holders 
From Figure 2, we can observe that: 1) All three 
factors have positive effects on performance 
compared to baseline 1, and our approach by 
integrating all factors achieves the best 
performance; 2) SP improve the performance in 
terms of all three metrics, showing that SP 
including named entity recognition and parenthesis 
removing are useful for holder identification; 3) 
The major improvement of EP lies in EM, showing 
that the main contribution of EP is to get the exact 
opinion holders by expanding the core head noun; 
4) SP+EP+HR improves the performance in terms 
of all three metrics compared with SP+HR, 
showing the heuristic rules are useful to improve 
the performance. 
5.2 Results for Opinion Target Identification 
The results for opinion target identification are 
shown in Table 2, from which we can observe that 
our proposed approach significantly outperforms 
the baseline method. 
  Pre Rec F1 
EM 11.1 9.2 10.1 
HM 24.0 19.9 21.8 Baseline 1  PM 39.4 32.7 35.8 
EM 29.3 28.5 28.9 
HM 38.4 38.0 38.2 
Our 
Approach 
 PM 59.3 58.7 59.0 
Table 2. Results for Opinion Targets 
We also investigate the influences of the 
following four factors on the performance: 
sentence preprocessing (SP) in Sec. 4.2, target 
expansion (EP) in Sec. 4.4, holder conflict (HC), 
the heuristic rules (HR) proposed in Sec. 4.4. The 
F1s for EM, HM and PM are shown in Figure 3, in 
which BL refers to baseline 1.  
Influence of Factors
8
18
28
38
48
58
68
BL BL+SP BL+EP BL+HC BL+SP
+EP+HC
Our Approach 
Approaches
F1
EM
HM
PM
 
Figure 3. Influences of Factors on Opinion Targets 
From Figure 3, we can observe that: 1) All four 
factors have positive effects on performance 
compared to the baseline, and our approach 
integrating all the factors achieves the best 
performance; 2)  EP significantly improves F1 of 
EM without much improvement on F1 of HM or 
PM, showing that EP?s major contribution lies in 
exact match; 3) The major contribution of HC is 
the improvement of F1s of HM and PM, showing 
the automatically identified opinion holders are 
quite helpful for finding opinion targets; 4) 
SP+EP+HC improves the performance in terms of 
all three metrics; and our approach further 
improves the performance by adding HR. 
5.3 Discussion 
Here we compare our results with those reported at 
NTCIR-7 MOAT traditional Chinese test (Seki et 
al., 2008). Without considering the errors in the 
previous step, the highest F1s for opinion holder 
analysis reported by the four participants were 
respectively 82.5%, 59.9%, 50.3% and 59.5%, and 
the highest F1s for target reported by the three 
participants were respectively 60.6%, 2.1% and 
3.6%. Compared to the results at NTCIR-7, our 
performances on both opinion holder identification 
in Table 1 and that on target identification in Table 
2 seem quite encouraging even by the EM metrics.  
Consider the evaluation for opinion 
holders/targets was semi-automatic at NTCIR-7. 
We should note that although the generated 
standard had been supplemented by the 
participants? submissions, some correct answers 
may still be missing, especially for targets since 
only three teams participated in the target 
50
 identification task and the recalls were not high. 
Thus the performance reported in Table 1 and 2 
may be underestimated. 
Here we also give an estimate on the 
percentages of opinionated sentences containing 
both opinion holders and at least one reporting 
verb in NTCIR-6 and NTCIR-7?s traditional 
Chinese test data, which are respectively 94.5% 
and 83.9%. The high percentages show that 
reporting verbs are very common in news report.  
6 Conclusion and Future Work 
In this paper, we investigate the problem of 
identifying opinion holders/targets in opinionated 
sentences of Chinese news texts based on Chinese 
dependency parser, reporting verbs and opinion-
bearing words. Our proposed approach shows 
encouraging performance on opinion holder/target 
identification with the NTCIR-7?s traditional 
Chinese test data, and outperforms most systems 
reported at NTCIR-7 and the baseline methods 
including the CRF-based model.  
The proposed approach is highly dependent on 
dependency parser, and we would like to further 
investigate machine learning approaches (including 
the CRF model) by treating dependency structures 
as one of the linguistic features, which could be 
more robust to parsing errors. Opinion targets are 
more difficult to be identified than opinion holders, 
and deserve more attention in the NLP field, and 
we also would extend the targets to verb phrases 
and embedded clauses in addition to noun phrases. 
To explore the effectiveness of our approach with 
English data such as MPQA is another direction. 
Acknowledgements 
We acknowledge the help of our colleagues 
(Professor Benjamin K. Tsou and Mr. Jiang Tao). 
Reference 
Steven Bethard, Hong Yu, Ashley Thornton, Vasileios 
Hatzivassiloglou, and Dan Jurafsky. 2004. Automatic 
Extraction of Opinion Propositions and their Holders, 
AAAI Spring Symposium on Exploring Attitude and 
Affect in Text: Theories and Applications. 
Yejin Choi, Claire Cardie, Ellen Riloff, and Siddharth 
Patwardhan. 2005. Identifying sources of opinions 
with conditional random fields and extraction 
patterns. In Proc. of HLT/EMNLP-05. 
Vasileios Hatzivassiloglou and Kathleen McKeown. 
1997. Predicting the Semantic Orientation of 
Adjectives. In Proc. of ACL-97. 174-181. 
Minqing Hu and Bing Liu. 2004. Mining Opinion 
Features in Customer Reviews. In Proc. of AAAI-04. 
Soo-Min Kim and Eduard Hovy. 2005. Identifying 
Opinion Holders for Question Answering in Opinion 
Texts, In Proc. of AAAI-05 Workshop on Question 
Answering in Restricted Domains. Pittsburgh, PA. 
Soo-Min Kim and Eduard Hovy. 2006. Extracting 
opinions, opinion holders, and topics expressed in 
online news media text, In Proc. of ACL Workshop 
on Sentiment and Subjectivity in Text.  
Youngho Kim, Seongchan Kim, and Sung-Hyon 
Myaeng. 2008. Extracting Topic-related Opinions 
and their Targets in NTCIR-7, Proc. of NTCIR-7 
Workshop, Tokyo, Japan. 
Lun-Wei Ku, I-Chien Liu, Chia-Ying Lee, Kuan-hua 
Chen and Hsin-Hsi Chen. 2008. Sentence-Level 
Opinion Analysis by CopeOpi in NTCIR-7. In Proc. 
of NTCIR-7 Workshop. Tokyo, Japan. 
Jinshan Ma, Yu Zhang, Ting Liu and Sheng Li. 2004. A 
statistical dependency parser of Chinese under small 
training data. IJCNLP-04 Workshop: Beyond shallow 
analyses - Formalisms and statistical modeling for 
deep analyses. 
Bo Pang, Lillian Lee, and Shivakumar Vaithyanathan. 
2002. Thumbs up? Sentiment classification using 
machine learning techniques. In Proc. of EMNLP-02. 
Josef Ruppenhofer, Swapna Somasundaran, Janyce 
Wiebe. 2008. Finding the Sources and Targets of 
Subjective Expressions. In Proc. of LREC 2008. 
Yohei Seki, David Kirk Evans, Lun-Wei Ku, and et al 
2007. Overview of Opinion Analysis Pilot Task at 
NTCIR-6. Proc. of the NTCIR-6 Workshop.  
Yohei Seki, David Kirk Evans, Lun-Wei Ku, and et al 
2008. Overview of Multilingual Opinion Analysis 
Task at NTCIR-7. Proc. of the NTCIR-7 Workshop. 
Japan. 2008. 12. 
Jilin Shi and Yinggui Zhu. 2006. The Lexicon of 
Chinese Positive Words (????? ). Sichuan 
Lexicon Press. 
Veselin Stoyanov and Claire Cardie. 2008. Topic 
Identification for Fine-Grained Opinion Analysis. In 
Proc. of COLING-08. 
Peter D. Turney. 2002. Thumbs up or thumbs down? 
Semantic orientation applied to unsupervised 
classification of reviews, In Proc. of ACL-02. 
Theresa Wilson and Janyce Wiebe. 2003. Annotating 
opinions in the world press. Proc. of the 4th ACL 
SIGdial Workshop on Discourse and Dialogue 
(SIGdial-03). 
Ruifeng Xu, Kam-Fai Wong and Yunqing Xia. 2008. 
Coarse-Fine Opinion Mining - WIA in NTCIR-7 
MOAT Task. Proc. of the 7th NTCIR Workshop. 
Ling Yang and Yinggui Zhu. 2006. The Lexicon of 
Chinese Negative Words (????? ). Sichuan 
Lexicon Press. 
51
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 320?330,
Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational Linguistics
 
 
Joint Bilingual Sentiment Classification with Unlabeled Parallel Corpora 
 
Bin Lu1,3*, Chenhao Tan2, Claire Cardie2 and Benjamin K. Tsou3,1 
1 Department of Chinese, Translation and Linguistics, City University of Hong Kong, Hong Kong 
2 Department of Computer Science, Cornell University, Ithaca, NY, USA 
3 Research Centre on Linguistics and Language Information Sciences,  
Hong Kong Institute of Education, Hong Kong  
lubin2010@gmail.com, {chenhao, cardie}@cs.cornell.edu, btsou99@gmail.com 
 
 
Abstract 
Most previous work on multilingual sentiment 
analysis has focused on methods to adapt 
sentiment resources from resource-rich 
languages to resource-poor languages. We 
present a novel approach for joint bilingual 
sentiment classification at the sentence level 
that augments available labeled data in each 
language with unlabeled parallel data. We rely 
on the intuition that the sentiment labels for 
parallel sentences should be similar and present 
a model that jointly learns improved mono-
lingual sentiment classifiers for each language. 
Experiments on multiple data sets show that the 
proposed approach (1) outperforms the mono-
lingual baselines, significantly improving the 
accuracy for both languages by 3.44%-8.12%; 
(2) outperforms two standard approaches for 
leveraging unlabeled data; and (3) produces 
(albeit smaller) performance gains when 
employing pseudo-parallel data from machine 
translation engines. 
1 Introduction 
The field of sentiment analysis has quickly 
attracted the attention of researchers and 
practitioners alike (e.g. Pang et al, 2002; Turney, 
2002; Hu and Liu, 2004; Wiebe et al, 2005; Breck 
et al, 2007; Pang and Lee, 2008). 1 Indeed, 
sentiment analysis systems, which mine opinions 
from textual sources (e.g. news, blogs, and 
reviews), can be used in a wide variety of 
                                                          
*The work was conducted when the first author was visiting 
Cornell University. 
applications, including interpreting product 
reviews, opinion retrieval and political polling.  
Not surprisingly, most methods for sentiment 
classification are supervised learning techniques, 
which require training data annotated with the 
appropriate sentiment labels (e.g. document-level 
or sentence-level positive vs. negative polarity).  
This data is difficult and costly to obtain, and must 
be acquired separately for each language under 
consideration.  
Previous work in multilingual sentiment analysis 
has therefore focused on methods to adapt 
sentiment resources (e.g. lexicons) from resource-
rich languages (typically English) to other 
languages, with the goal of transferring sentiment 
or subjectivity analysis capabilities from English to 
other languages (e.g. Mihalcea et al (2007); Banea 
et al (2008; 2010); Wan (2008; 2009); 
Prettenhofer and Stein (2010)). In recent years, 
however, sentiment-labeled data is gradually 
becoming available for languages other than 
English (e.g. Seki et al (2007; 2008); Nakagawa et 
al. (2010); Schulz et al (2010)). In addition, there 
is still much room for improvement in existing 
monolingual (including English) sentiment 
classifiers, especially at the sentence level (Pang 
and Lee, 2008).  
This paper tackles the task of bilingual 
sentiment analysis. In contrast to previous work, 
we (1) assume that some amount of sentiment-
labeled data is available for the language pair 
under study, and (2) investigate methods to 
simultaneously improve sentiment classification 
for both languages. Given the labeled data in each 
language, we propose an approach that exploits an 
unlabeled parallel corpus with the following 
320
  
intuition: two sentences or documents that are 
parallel (i.e. translations of one another) should 
exhibit the same sentiment ? their sentiment 
labels (e.g. polarity, subjectivity, intensity) should 
be similar. The proposed maximum entropy-based 
EM approach jointly learns two monolingual 
sentiment classifiers by treating the sentiment 
labels in the unlabeled parallel text as unobserved 
latent variables, and maximizes the regularized 
joint likelihood of the language-specific labeled 
data together with the inferred sentiment labels of 
the parallel text.  Although our approach should be 
applicable at the document-level and for additional 
sentiment tasks, we focus on sentence-level 
polarity classification in this work. 
We evaluate our approach for English and 
Chinese on two dataset combinations (see Section 
4) and find that the proposed approach outperforms 
the monolingual baselines (i.e. maximum entropy 
and SVM classifiers) as well as two alternative 
methods for leveraging unlabeled data 
(transductive SVMs (Joachims, 1999b) and co-
training (Blum and Mitchell, 1998)).  Accuracy is 
significantly improved for both languages, by 
3.44%-8.12%. We furthermore find that 
improvements, albeit smaller, are obtained when 
the parallel data is replaced with a pseudo-parallel 
(i.e. automatically translated) corpus. To our 
knowledge, this is the first multilingual sentiment 
analysis study to focus on methods for 
simultaneously improving sentiment classification 
for a pair of languages based on unlabeled data 
rather than resource adaptation from one language 
to another.  
The rest of the paper is organized as follows. 
Section 2 introduces related work. In Section 3, the 
proposed joint model is described. Sections 4 and 
5, respectively, provide the experimental setup and 
results; the conclusion (Section 6) follows. 
2 Related Work 
Multilingual Sentiment Analysis.  There is a 
growing body of work on multilingual sentiment 
analysis. Most approaches focus on resource 
adaptation from one language (usually English) to 
other languages with few sentiment resources. 
Mihalcea et al (2007), for example, generate 
subjectivity analysis resources in a new language 
from English sentiment resources by leveraging a 
bilingual dictionary or a parallel corpus. Banea et 
al. (2008; 2010) instead automatically translate the 
English resources using automatic machine 
translation engines for subjectivity classification. 
Prettenhofer and Stein (2010) investigate cross-
lingual sentiment classification from the 
perspective of domain adaptation based on 
structural correspondence learning (Blitzer et al, 
2006). 
Approaches that do not explicitly involve 
resource adaptation include Wan (2009), which 
uses co-training (Blum and Mitchell, 1998) with 
English vs. Chinese features comprising the two 
independent ?views? to exploit unlabeled Chinese 
data and a labeled English corpus and thereby 
improves Chinese sentiment classification. 
Another notable approach is the work of Boyd-
Graber and Resnik (2010), which presents a 
generative model --- supervised multilingual latent 
Dirichlet alocation --- that jointly models topics 
that are consistent across languages, and employs 
them to better predict sentiment ratings. 
Unlike the methods described above, we focus 
on simultaneously improving the performance of 
sentiment classification in a pair of languages by 
developing a model that relies on sentiment-
labeled data in each language as well as unlabeled 
parallel text for the language pair. 
Semi-supervised Learning.  Another line of 
related work is semi-supervised learning, which 
combines labeled and unlabeled data to improve 
the performance of the task of interest (Zhu and 
Goldberg, 2009). Among the popular semi-
supervised methods (e.g. EM on Na?ve Bayes 
(Nigam et al, 2000), co-training (Blum and 
Mitchell, 1998), transductive SVMs (Joachims, 
1999b), and co-regularization (Sindhwani et al, 
2005; Amini et al, 2010)), our approach employs 
the EM algorithm, extending it to the bilingual 
case based on maximum entropy. We compare to 
co-training and transductive SVMs in Section 5. 
Multilingual NLP for Other Tasks. Finally, 
there exists related work using bilingual resources 
to help other NLP tasks, such as word sense 
disambiguation (e.g. Ido and Itai (1994)), parsing 
(e.g. Burkett and Klein (2008); Zhao et al (2009); 
Burkett et al (2010)), information retrieval (Gao et 
al., 2009), named entity detection (Burkett et al, 
2010); topic extraction (e.g. Zhang et al, 2010), 
text classification (e.g. Amini et al, 2010), and 
hyponym-relation acquisition (e.g. Oh et al, 2009). 
321
  
In these cases, multilingual models increase 
performance because different languages contain 
different ambiguities and therefore present 
complementary views on the shared underlying 
labels.  Our work shares a similar motivation. 
3 A Joint Model with Unlabeled Parallel 
Text 
We propose a maximum entropy-based statistical 
model. Maximum entropy (MaxEnt) models1 have 
been widely used in many NLP tasks (Berger et al, 
1996; Ratnaparkhi, 1997; Smith, 2006). The 
models assign the conditional probability of the 
label   given the observation   as follows: 
          
 
 
                                 (1) 
where    is a real-valued vector of feature weights 
and    is a feature function that maps pairs       to 
a nonnegative real-valued feature vector. Each 
feature has an associated parameter,   , which is 
called its weight; and   is the corresponding 
normalization factor.  
Maximum likelihood parameter estimation 
(training) for such a model, with a set of labeled 
examples            
  , amounts to solving the 
following optimization problem: 
  
 
                   
 
                        (2) 
3.1 Problem Definition 
Given two languages    and   , suppose we have 
two distinct (i.e. not parallel) sets of sentiment-
labeled data,    and     written in    and     
respectively. In addition, we have unlabeled (w.r.t. 
sentiment) bilingual (in    and   ) parallel data   
that are defined as follows. 
               
    
     
     
               
    
     
    
     
    
       
     
   
   
 
   
where               denotes the polarity of 
the  -th instance    (positive or negative);    and    
are respectively the numbers of labeled instances 
in    and   ;   
   and   
   are parallel instances in    
and   , respectively (i.e. they are supposed to be 
                                                          
1They are sometimes referred to as log-linear models, but also 
known as exponential models, generalized linear models, or 
logistic regression. 
translations of one another), whose labels   
   and 
  
   are unobserved, but according to the intuition 
outlined in Section 1, should be similar.  
Given the input data        and  , our task is to 
jointly learn two monolingual sentiment classifiers 
? one for    and one for   . With MaxEnt, we 
learn from the input data:  
                   
 
     
 
  
where    
   and     
 
 are the vectors of feature weights 
for    and   , respectively (for brevity we denote 
them as    and    in the remaining sections). In this 
study, we focus on sentence-level sentiment 
classification, i.e. each    is a sentence, and   
   and 
  
   are parallel sentences. 
3.2 The Joint Model  
Given the problem definition above, we now 
present a novel model to exploit the 
correspondence of parallel sentences in unlabeled 
bilingual text. The model maximizes the following 
joint likelihood with respect to    and   : 
                                        
    
    
    
    
         
        
    
     
  
   
 
     
     
     
     
     
         
 
   (3) 
where          denotes    or   ; the first term on 
the right-hand side is the likelihood of labeled data 
for both    and   ; and the second term is the 
likelihood of the unlabeled parallel data  .  
If we assume that parallel sentences are perfect 
translations, the two sentences in each pair should 
have the same polarity label, which gives us:   
    
     
     
     
          
     
    
          
    
        
                          (4) 
where   
  is the unobserved class label for the  -th 
instance in the unlabeled data. This probability 
directly models the sentiment label agreement 
between   
   and   
  . 
However, there could be considerable noise in 
real-world parallel data, i.e. the sentence pairs may 
be noisily parallel (or even comparable) instead of 
fully parallel (Munteanu and Marcu, 2005). In such 
noisy cases, the labels (positive or negative) could 
be different for the two monolingual sentences in a 
sentence pair. Although we do not know the exact 
probability that a sentence pair exhibits the same 
label, we can approximate it using their translation 
322
  
probabilities, which can be computed using word 
alignment toolkits such as Giza++ (Och and Ney, 
2003) or the Berkeley word aligner (Liang et al, 
2006). The intuition here is that if the translation 
probability of two sentences is high, the probability 
that they have the same sentiment label should be 
high as well. Therefore, by considering the noise in 
parallel data, we get: 
    
     
     
     
           
          
    
         
    
            
              
    
             
               (5)                 
where       is the translation probability of the  -th 
sentence pair in  ;2    is the opposite of   ; the first 
term models the probability that   
   and   
   have 
the same label; and the second term models the 
probability that they have different labels.  
By further considering the weight to ascribe to 
the unlabeled data vs. the labeled data (and the 
weight for the L2-norm regularization), we get the 
following regularized joint log likelihood to be 
maximized: 
                                    
 
    
         
    
    
    
         
  
 
      
  
         (6) 
where the first term on the right-hand side is the 
log likelihood of the labeled data from both    and 
    the second is the log likelihood of the 
unlabeled parallel data  , multiplied by     , a 
constant that controls the contribution of the 
unlabeled data; and      is a regularization 
constant that penalizes model complexity or large 
feature weights. When    is 0, the algorithm 
ignores the unlabeled data and degenerates to two 
MaxEnt models trained on only the labeled data. 
3.3 The EM Algorithm on MaxEnt 
To solve the optimization problem for the model, 
we need to jointly estimate the optimal parameters 
for the two monolingual classifiers by finding: 
   
    
                                      (7) 
This can be done with an EM algorithm, whose 
steps are summarized in Algorithm 1. First, the 
MaxEnt parameters,    and   , are estimated from 
                                                          
2The probability should be rescaled within the range of [0, 1], 
where 0.5 means that we are completely unsure if the 
sentences are translations of each other or not, and only those 
translation pairs with a probability larger than 0.5 are 
meaningful for our purpose. 
just the labeled data. Then, in the E-step, the 
classifiers, based on current values of     and   , 
compute          for each labeled example and 
assign probabilistically-weighted class labels to 
each unlabeled example. Next, in the M-step, the 
parameters,    and   , are updated using both the 
original labeled data (   and   ) and the newly 
labeled data  . These last two steps are iterated 
until convergence or a predefined iteration limit  . 
Algorithm 1. The MaxEnt-based EM Algorithm for 
Multilingual Sentiment Classification 
Input: Labeled data    and    
Unlabeled parallel data   
Output: 
Two monolingual MaxEnt classifiers with 
parameters   
  and   
 , respectively 
1. Train two initial monolingual models 
Train and initialize   
   
 and   
   
 on the labeled data 
2. Jointly optimize two monolingual models 
for     to   do // T: number of iterations 
       E-Step: 
Compute         for each example in    ,    and   
based on   
     
 and   
     
; 
Compute the expectation of the log likelihood with 
respect to       ; 
M-Step: 
Find    
   
 and   
   
 by maximizing the regularized 
joint log likelihood; 
Convergence: 
 If the increase of the joint log likelihood is 
sufficiently small, break; 
      end for  
3. Output    
  as   
   
s, and   
  as    
   
  
In the M-step, we can optimize the regularized 
joint log likelihood using any gradient-based 
optimization technique (Malouf, 2002). The 
gradient for Equation 3 based on Equation 4 is 
shown in Appendix A; those for Equations 5 and 6 
can be derived similarly. In our experiments, we 
use the L-BFGS algorithm (Liu et al, 1989) and 
run EM until the change in regularized joint log 
likelihood is less than 1e-5 or we reach 100 
iterations.3 
                                                          
3Since the EM-based algorithm may find a local maximum of 
the objective function, the initialization of the parameters is 
important. Our experiments show that an effective maximum 
can usually be found by initializing the parameters with those 
learned from the labeled data; performance would be much 
worse if we initialize all the parameters to 0 or 1. 
323
  
3.4 Pseudo-Parallel Labeled and Unlabeled 
Data 
We also consider the case where a parallel corpus 
is not available: to obtain a pseudo-parallel corpus 
  (i.e. sentences in one language with their 
corresponding automatic translations), we use an 
automatic machine translation system (e.g. Google 
machine translation 4 ) to translate unlabeled in-
domain data from    to    or vice versa. 
Since previous work (Banea et al, 2008; 2010; 
Wan, 2009) has shown that it could be useful to 
automatically translate the labeled data from the 
source language into the target language, we can 
further incorporate such translated labeled data into 
the joint model by adding the following component 
into Equation 6: 
           
    
      
  
   
 
                  (8) 
where    is the alternative class of  ,   
   is the 
automatically translated example from   
 ; and  
     is a constant that controls the weight of the 
translated labeled data. 
4 Experimental Setup 
4.1 Data Sets and Preprocessing 
The following labeled datasets are used in our 
experiments. 
MPQA (Labeled English Data): The Multi-
Perspective Question Answering (MPQA) corpus 
(Wiebe et al, 2005) consists of newswire 
documents manually annotated with phrase-level 
subjectivity information. We extract all sentences 
containing strong (i.e. intensity is medium or 
higher), sentiment-bearing (i.e. polarity is positive 
or negative) expressions following Choi and 
Cardie (2008). Sentences with both positive and 
negative strong expressions are then discarded, and 
the polarity of each remaining sentence is set to 
that of its sentiment-bearing expression(s). 
NTCIR-EN (Labeled English Data) and 
NTCIR-CH (Labeled Chinese Data): The 
NTCIR Opinion Analysis task (Seki et al, 2007; 
2008) provides sentiment-labeled news data in 
Chinese, Japanese and English. Only those 
sentences with a polarity label (positive or 
negative) agreed to by at least two annotators are 
extracted. We use the Chinese data from NTCIR-6 
                                                          
4http://translate.google.com/ 
as our Chinese labeled data. Since far fewer 
sentences in the English data pass the annotator 
agreement filter, we combine the English data from 
NTCIR-6 and NTCIR-7. The Chinese sentences 
are segmented using the Stanford Chinese word 
segmenter (Tseng et al, 2005). 
The number of sentences in each of these 
datasets is shown in Table 1. In our experiments, 
we evaluate two settings of the data: (1) 
MPQA+NTCIR-CH, and (2) NTCIR-EN+NTCIR-
CH. In each setting, the English labeled data 
constitutes    and the Chinese labeled data,   .  
 MPQA NTCIR-EN NTCIR-CH 
Positive 1,471 (30%) 528 (30%) 2,378 (55%) 
Negative 3,487 (70%) 1,209 (70%) 1,916 (45%) 
Total 4,958 1,737 4,294 
Table 1: Sentence Counts for the Labeled Data 
Unlabeled Parallel Text and its Preprocessing. 
For the unlabeled parallel text, we use the ISI 
Chinese-English parallel corpus (Munteanu and 
Marcu, 2005), which was extracted automatically 
from news articles published by Xinhua News 
Agency in the Chinese Gigaword (2nd Edition) and 
English Gigaword (2nd Edition) collections. 
Because sentence pairs in the ISI corpus are quite 
noisy, we rely on Giza++ (Och and Ney, 2003) to 
obtain a new translation probability for each 
sentence pair, and select the 100,000 pairs with the 
highest translation probabilities.5  
We also try to remove neutral sentences from 
the parallel data since they can introduce noise into 
our model, which deals only with positive and 
negative examples. To do this, we train a single 
classifier from the combined Chinese and English 
labeled data for each data setting above by 
concatenating the original English and Chinese 
feature sets. We then classify each unlabeled 
sentence pair by combining the two sentences in 
each pair into one. We choose the most confidently 
predicted 10,000 positive and 10,000 negative 
pairs to constitute the unlabeled parallel corpus   
for each data setting. 
                                                          
5We removed sentence pairs with an original confidence score 
(given in the corpus) smaller than 0.98, and also removed the 
pairs that are too long (more than 60 characters in one 
sentence) to facilitate Giza++. We first obtain translation 
probabilities for both directions (i.e. Chinese to English and 
English to Chinese) with Giza++, take the log of the product 
of those two probabilities, and then divide it by the sum of 
lengths of the two sentences in each pair.  
324
  
4.2 Baseline Methods 
In our experiments, the proposed joint model is 
compared with the following baseline methods. 
MaxEnt: This method learns a MaxEnt 
classifier for each language given the monolingual 
labeled data; the unlabeled data is not used.  
SVM: This method learns an SVM classifier for 
each language given the monolingual labeled data; 
the unlabeled data is not used. SVM-light 
(Joachims, 1999a) is used for all the SVM-related 
experiments. 
Monolingual TSVM (TSVM-M): This method 
learns two transductive SVM (TSVM) classifiers 
given the monolingual labeled data and the 
monolingual unlabeled data for each language.  
Bilingual TSVM (TSVM-B): This method 
learns one TSVM classifier given the labeled 
training data in two languages together with the 
unlabeled sentences by combining the two 
sentences in each unlabeled pair into one. We 
expect this method to perform better than TSVM-
M since the combined (bilingual) unlabeled 
sentences could be more helpful than the unlabeled 
monolingual sentences. 
Co-Training with SVMs (Co-SVM): This 
method applies SVM-based co-training given both 
the labeled training data and the unlabeled parallel 
data following Wan (2009). First, two monolingual 
SVM classifiers are built based on only the 
corresponding labeled data, and then they are 
bootstrapped by adding the most confident 
predicted examples from the unlabeled data into 
the training set. We run bootstrapping for 100 
iterations. In each iteration, we select the most 
confidently predicted 50 positive and 50 negative 
sentences from each of the two classifiers, and take 
the union of the resulting 200 sentence pairs as the 
newly labeled training data. (Examples with 
conflicting labels within the pair are not included.) 
5 Results and Analysis 
In our experiments, the methods are tested in the 
two data settings with the corresponding unlabeled 
parallel corpus as mentioned in Section 4.6 We use 
                                                          
6 The results reported in this section employ Equation 4. 
Preliminary experiments showed that Equation 5 does not 
significantly improve the performance in our case, which is 
reasonable since we choose only sentence pairs with the 
highest translation probabilities to be our unlabeled data (see 
Section 4.1).      
5-fold cross-validation and report average accuracy 
(also MicroF1 in this case) and MacroF1 scores. 
Unigrams are used as binary features for all 
models, as Pang et al (2002) showed that binary 
features perform better than frequency features for 
sentiment classification. The weights for unlabeled 
data and regularization,    and   , are set to 1 
unless otherwise stated. Later, we will show that 
the proposed approach performs well with a wide 
range of parameter values.7 
5.1 Method Comparison 
We first compare the proposed joint model (Joint) 
with the baselines in Table 2. As seen from the 
table, the proposed approach outperforms all five 
baseline methods in terms of both accuracy and 
MacroF1 for both English and Chinese and in both 
of the data settings. 8  By making use of the 
unlabeled parallel data, our proposed approach 
improves the accuracy, compared to MaxEnt, by 
8.12% (or 33.27% error reduction) on English and 
3.44% (or 16.92% error reduction) on Chinese in 
the first setting, and by 5.07% (or 19.67% error 
reduction) on English and 3.87% (or 19.4% error 
reduction) on Chinese in the second setting. 
 Among the baselines, the best is Co-SVM; 
TSVMs do not always improve performance using 
the unlabeled data compared to the standalone 
SVM; and TSVM-B outperforms TSVM-M except 
for Chinese in the second setting. The MPQA data 
is more difficult in general compared to the NTCIR 
data. Without unlabeled parallel data, the 
performance on the Chinese data is better than on 
the English data, which is consistent with results 
reported in NTCIR-6 (Seki et al, 2007).  
Overall, the unlabeled parallel data improves 
classification accuracy for both languages when 
using our proposed joint model and Co-SVM. The 
joint model makes better use of the unlabeled 
parallel data than Co-SVM or TSVMs presumably 
because of its attempt to jointly optimize the two 
monolingual models via soft (probabilistic) 
assignments of the unlabeled instances to classes in 
each iteration, instead of the hard assignments in 
Co-SVM and TSVMs. Although English sentiment 
                                                          
7The code is at http://sites.google.com/site/lubin2010. 
8 Significance is tested using paired t-tests with  <0.05: ? 
denotes statistical significance compared to the corresponding 
performance of MaxEnt; * denotes statistical significance 
compared to SVM; and 
?
 denotes statistical significance 
compared to Co-SVM. 
325
  
classification alone is more difficult than Chinese 
for our datasets, we obtain greater performance 
gains for English by exploiting unlabeled parallel 
data as well as the Chinese labeled data.  
5.2 Varying the Weight and Amount of 
Unlabeled Data 
Figure 1 shows the accuracy curve of the proposed 
approach for the two data settings when varying 
the weight for the unlabeled data,   , from 0 to 1. 
When    is set to 0, the joint model degenerates to 
two MaxEnt models trained with only the labeled 
data.  
We can see that the performance gains for the 
proposed approach are quite remarkable even when 
   is set to 0.1; performance is largely stable after 
   reaches 0.4. Although MPQA is more difficult 
in general compared to the NTCIR data, we still 
see steady improvements in performance with 
unlabeled parallel data. Overall, the proposed 
approach performs quite well for a wide range of 
parameter values of   .  
Figure 2 shows the accuracy curve of the 
proposed approach for the two data settings when 
varying the amount of unlabeled data from 0 to 
20,000 instances. We see that the performance of 
the proposed approach improves steadily by adding 
more and more unlabeled data. However, even 
with only 2,000 unlabeled sentence pairs, the 
proposed approach still produces large 
performance gains.  
5.3 Results on Pseudo-Parallel Unlabeled 
Data 
As discussed in Section 3.4, we generate pseudo-
parallel data by translating the monolingual 
sentences in each setting using Google?s machine 
translation system. Figures 3 and 4 show the 
performance of our model using the pseudo-
parallel data versus the real parallel data, in the two 
settings, respectively. The EN->CH pseudo-
parallel data consists of the English unlabeled data 
and its automatic Chinese translation, and vice 
versa. 
Although not as significant as those with parallel 
data, we can still obtain improvements using the 
pseudo-parallel data, especially in the first setting. 
The difference between using parallel versus 
pseudo-parallel data is around 2-4% in Figures 3 
and 4, which is reasonable since the quality of the 
pseudo-parallel data is not as good as that of the 
parallel data. Therefore, the performance using 
pseudo-parallel data is better with a small weight 
(e.g.   = 0.1) in some cases.  
 
Setting 1: NTCIR-EN+NTCIR-CH Setting 2: MPQA+NTCIR-CH 
Accuracy MacroF1 Accuracy MacroF1 
English Chinese English Chinese English Chinese English Chinese 
MaxEnt 75.59 79.67 66.61* 79.34 74.22 79.67 65.09* 79.34 
SVM 76.34 81.02 61.12 80.75? 76.74? 81.02 61.35 80.75? 
TSVM-M 73.46 80.21 55.33 79.99 72.89 81.14 52.82 79.99 
TSVM-B 78.36 81.60? 65.53 81.42 76.42? 78.51 61.66 78.32 
Co-SVM 82.44?* 82.79? 72.61?* 82.67?* 78.18?* 82.63?* 68.03?* 82.51?* 
Joint 83.71?* 83.11?* 75.89?*? 82.97?* 79.29?*? 83.54?* 72.58?*? 83.37?* 
Table 2: Comparison of Results 
       
Figure 1. Accuracy vs. Weight of Unlabeled Data                Figure 2. Accuracy vs. Amount of Unlabeled Data 
 
0 0.2 0.4 0.6 0.8 1
72
74
76
78
80
82
84
86
Weight of Unlabeled Data
A
c
c
u
r
a
c
y
(
%
)
 
 
English on NTCIR-EN+NTCIR-CH
Chinese on NTCIR-EN+NTCIR-CH
English  on MPQA+NTCIR-CH
Chinese on MPQA+NTCIR-CH
0 0.5 1 1.5 2
72
74
76
78
80
82
84
86
Size of Unlabeled Data
A
c
c
u
r
a
c
y
 
(
%
)
 
 
English on NTCIR-EN+NTCIR-CH
Chinese on NTCIR-EN+NTCIR-CH
English  on MPQA+NTCIR-CH
Chinese on MPQA+NTCIR-CH
326
  
5.4 Adding Pseudo-Parallel Labeled Data 
In this section, we investigate how adding 
automatically translated labeled data might 
influence the performance as mentioned in Section 
3.4. We use only the translated labeled data to train 
classifiers, and then directly classify the test data. 
The average accuracies in setting 1 are 66.61% and 
63.11% on English and Chinese, respectively; 
while the accuracies in setting 2 are 58.43% and 
54.07% on English and Chinese, respectively. This 
result is reasonable because of the language gap 
between the original language and the translated 
language. In addition, the class distributions of the 
English labeled data and the Chinese are quite 
different (30% vs. 55% for positive as shown in 
Table 1).  
Figures 5 and 6 show the accuracies when 
varying the weight of the translated labeled data vs. 
the labeled data, with and without the unlabeled 
parallel data. From Figure 5 for setting 1, we can 
see that the translated data can be helpful given the 
labeled data and even the unlabeled data, as long as 
   is small; while in Figure 6, the translated data 
decreases the performance in most cases for setting 
2. One possible reason is that in the first data 
setting, the NTCIR English data covers the same 
topics as the NTCIR Chinese data and thus direct 
translation is helpful, while the English and 
Chinese topics are quite different in the second 
data setting, and thus direct translation hurts the 
performance given the existing labeled data in each 
language. 
5.5 Discussion 
To further understand what contributions our 
proposed approach makes to the performance gain, 
we look inside the parameters in the MaxEnt 
models learned before and after adding the parallel 
unlabeled data. Table 3 shows the features in the 
model learned from the labeled data that have the 
largest weight change after adding the parallel data;  
     
Figure 3. Accuracy with Pseudo-Parallel Unlabeled           Figure 4. Accuracy with Pseudo-Parallel Unlabeled 
 Data in Setting 1                                                         Data in Setting 2 
        
Figure 5. Accuracy with Pseudo-Parallel Labeled              Figure 6. Accuracy with Pseudo-Parallel Labeled  
Data in Setting 1                                                      Data in Setting 2 
 
 
0 0.2 0.4 0.6 0.8 1
74
76
78
80
82
84
86
Weight of Unlabeled Data
A
c
c
u
r
a
c
y
(
%
)
 
 
English on Parallel Data
Chinese on Parallel Data
English on EN->CH Pseudo-Parallel Data
Chinese on EN->CH Pseudo-Parallel Data
English on CH->EN Pseudo-Parallel Data
Chinese on CH->EN Pseudo-Parallel Data
0 0.2 0.4 0.6 0.8 1
65
70
75
80
85
Weight of Unlabeled Data
A
c
c
u
r
a
c
y
(
%
)
 
 
English on Parallel Data
Chinese on Parallel Data
English on EN->CH Pseudo-Parallel Data
Chinese on EN->CH Pseudo-Parallel Data
English on CH->EN Pseudo-Parallel Data
Chinese on CH->EN Pseudo-Parallel Data
0 0.2 0.4 0.6 0.8 1
70
72
74
76
78
80
82
84
6
Weight of Translated Labeled Data
A
c
c
u
r
a
c
y
(
%
)
 
 
English w/o Unlabeled Data
Chinese w/o Unlabeled Data
English with Unlabeled Data
Chinese with Unlabeled Data
0 0.2 0.4 0.6 0.8 1
68
70
72
74
76
78
8
82
84
86
Weight of Translated Labeled Data
A
c
c
u
r
a
c
y
(
%
)
 
 
English w/o Unlabeled Data
Chinese w/o Unlabeled Data
English with Unlabeled Data
Chinese with Unlabeled Data
327
  
Positive Negative 
Word Weight Word Weight 
friendly 0.701 german 0.783 
principles 0.684 arduous 0.531 
hopes 0.630 oppose 0.511 
hoped 0.553 administrations 0.431 
cooperative 0.552 oau9 0.408 
Table 4. New Features Learned from Unlabeled Data 
and Table 4 shows the newly learned features from 
the unlabeled data with the largest weights. 
From Table 3 10  we can see that the weight 
changes of the original features are quite 
reasonable, e.g. the top words in the positive class 
are obviously positive and the proposed approach 
gives them higher weights. The new features also 
seem reasonable given the knowledge that the 
labeled and unlabeled data includes negative news 
about for specific topics (e.g. Germany, Taiwan),. 
We also examine the process of joint training by 
checking the performance on test data and the 
agreement of the two monolingual models on the 
unlabeled parallel data in both settings. The 
average agreement across 5 folds is 85.06% and 
73.87% in settings 1 and 2, respectively, before the 
joint training, and increases to 100% and 99.89%, 
respectively, after 100 iterations of joint training. 
Although the average agreement has already 
increased to 99.50% and 99.02% in settings 1 and 
2, respectively, after 30 iterations, the performance 
on the test set steadily improves in both settings 
until around 50-60 iterations, and then becomes 
relatively stable after that. 
Examination of those sentence pairs in setting 2 
for which the two monolingual models still 
                                                          
9
This is an abbreviation for the Organization of African Unity. 
10The features and weights in Tables 3 and 4 are extracted 
from the English model in the first fold of setting 1. 
disagree after 100 iterations of joint training often 
produces sentences that are not quite parallel, e.g.: 
English: The two sides attach great importance to 
international cooperation on protection and promotion of 
human rights. 
Chinese: ????,????????????????,???
??????????????(Both sides agree that double 
standards on the issue of human rights are to be avoided, and 
are opposed to using pressure on human rights issues in 
international relations.) 
Since the two sentences discuss human rights 
from very different perspectives, it is reasonable 
that the two monolingual models will classify them 
with different polarities (i.e. positive for the 
English sentence and negative for the Chinese 
sentence) even after joint training.  
6 Conclusion 
In this paper, we study bilingual sentiment 
classification and propose a joint model to 
simultaneously learn better monolingual sentiment 
classifiers for each language by exploiting an 
unlabeled parallel corpus together with the labeled 
data available for each language. Our experiments 
show that the proposed approach can significantly 
improve sentiment classification for both 
languages. Moreover, the proposed approach 
continues to produce (albeit smaller) performance 
gains when employing pseudo-parallel data from 
machine translation engines. 
In future work, we would like to apply the joint 
learning idea to other learning frameworks (e.g. 
SVMs), and to extend the proposed model to 
handle word-level parallel information, e.g. 
bilingual dictionaries or word alignment 
information. Another issue is to investigate how to 
improve multilingual sentiment analysis by 
exploiting comparable corpora. 
Acknowledgments 
We thank Shuo Chen, Long Jiang, Thorsten 
Joachims, Lillian Lee, Myle Ott, Yan Song, 
Xiaojun Wan, Ainur Yessenalina, Jingbo Zhu and 
the anonymous reviewers for many useful 
comments and discussion. This work was 
supported in part by National Science Foundation 
Grants BCS-0904822, BCS-0624277, IIS-
0968450; and by a gift from Google. Chenhao Tan 
is supported by NSF (DMS-0808864), ONR (YIP-
N000140910911), and a grant from Microsoft.  
  
 Word 
Weight 
Before After Change 
Positive 
important 0.452 1.659 1.207 
cooperation 0.325 1.492 1.167 
support 0.533 1.483 0.950 
importance 0.450 1.193 0.742 
agreed 0.347 1.061 0.714 
Negative 
difficulties 0.018 0.663 0.645 
not 0.202 0.844 0.641 
never 0.245 0.879 0.634 
germany 0.035 0.664 0.629 
taiwan 0.590 1.216 0.626 
Table 3. Original Features with Largest Weight Change 
328
  
References 
Massih-Reza Amini, Cyril Goutte, and Nicolas Usunier. 
2010. Combining coregularization and consensus-
based self-training for multilingual text 
categorization. In Proceeding of SIGIR?10. 
Carmen Banea, Rada Mihalcea, and Janyce Wiebe. 
2010. Multilingual subjectivity: Are more languages 
better? In Proceedings of COLING?10. 
Carmen Banea, Rada Mihalcea, Janyce Wiebe, and 
Samer Hassan. 2008. Multilingual subjectivity 
analysis using machine translation. In Proceedings of 
EMNLP?08. 
Adam L. Berger, Stephen A. Della Pietra and Vincent J. 
Della Pietra. 1996. A maximum entropy approach to 
natural language processing. Computational 
Linguistics, 22(1). 
John Blitzer, Ryan McDonald, and Fernando Pereira. 
2006. Domain adaptation with structural correspond-
dence learning. In Proceedings of EMNLP?06. 
Avrim Blum and Tom Mitchell. 1998. Combining 
labeled and unlabeled data with co-training. In 
Proceedings of COLT?98. 
Jordan Boyd-Graber and Philip Resnik. 2010. Holistic 
sentiment analysis across languages: Multilingual 
supervised Latent Dirichlet Allocation. In 
Proceedings of EMNLP?10. 
Eric Breck, Yejin Choi, and Claire Cardie. 2007. 
Identifying expressions of opinion in context. In 
Proceedings of IJCAI?07.  
David Burkett, Slav Petrov, John Blitzer, and Dan 
Klein. 2010. Learning better monolingual models 
with unannotated bilingual text. In Proceedings of 
CoNLL?10. 
David Burkett and Dan Klein. 2008. Two languages are 
better than one (for syntactic parsing). In 
Proceedings of EMNLP?08. 
Yejin Choi and Claire Cardie. 2008. Learning with 
compositional semantics as structural inference for 
subsentential sentiment analysis. In Proceedings of 
EMNLP?08. 
Wei Gao, John Blitzer, Ming Zhou, and Kam-Fai Wong. 
2009. Exploiting bilingual information to improve 
web search. In Proceedings of ACL/IJCNLP?09. 
Minqing Hu and Bing Liu. 2004. Mining opinion 
features in customer reviews. In Proceedings of 
AAAI?04. 
Ido Dagan, and Alon Itai. 1994. Word sense 
disambiguation using a second language monolingual 
corpus, Computational Linguistics, 20(4): 563-596. 
Thorsten Joachims. 1999a. Making Large-Scale SVM 
Learning Practical. In: Advances in Kernel Methods - 
Support Vector Learning, B. Sch?lkopf, C. Burges, 
and A. Smola (ed.), MIT Press. 
Thorsten Joachims. 1999b. Transductive inference for 
text classification using support vector machines. In 
Proceedings of ICML?99. 
Percy Liang, Ben Taskar, and Dan Klein. 2006. 
Alignment by agreement. In Proceedings of 
NAACL?06. 
Dong C. Liu and  Jorge Nocedal. 1989. On the limited 
memory BFGS method for large scale optimization. 
Mathematical Programming, (45): 503?528. 
Robert Malouf. 2002. A comparison of algorithms for 
maximum entropy parameter estimation. In 
Proceedings of CoNLL?02. 
Rada Mihalcea, Carmen Banea, and Janyce Wiebe. 
2007. Learning multilingual subjective language via 
cross-lingual projections. In Proceedings of ACL?07. 
Dragos S. Munteanu and Daniel Marcu. 2005. 
Improving machine translation performance by 
exploiting non-parallel corpora. Computational 
Linguistics, 31(4): 477?504. 
Tetsuji Nakagawa, Kentaro Inui, and Sadao Kurohashi. 
2010. Dependency tree-based sentiment classification 
using CRFs with hidden variables. In Proceedings of 
NAACL/HLT ?10. 
Kamal Nigam, Andrew K. Mccallum, Sebastian Thrun, 
and Tom Mitchell. 2000. Text classification from 
labeled and unlabeled documents using EM. Machine 
Learning, 39(2): 103?134. 
Franz J. Och and Hermann Ney. 2003. A systematic 
comparison of various statistical alignment models. 
Computational Linguistics, 29(1): 19-51. 
Bo Pang and Lillian Lee. 2008. Opinion mining and 
sentiment analysis, Foundations and Trends in 
Information Retrieval, Now Publishers. 
Bo Pang, Lillian Lee, and Shivakumar Vaithyanathan. 
2002. Thumbs up? Sentiment classification using 
machine learning techniques. In Proceedings of 
EMNLP?02. 
Peter  Prettenhofer and  Benno Stein. 2010. Cross-
language text classification using structural 
correspondence learning. In Proceedings of ACL?10. 
Adwait Ratnaparkhi. 1997. A simple introduction to 
maximum entropy models for natural language 
processing. Technical Report 97-08, University of 
Pennsylvania. 
329
  
Julia M. Schulz, Christa Womser-Hacker, and Thomas 
Mandl. 2010. Multilingual corpus development for 
opinion mining. In Proceedings of LREC?10. 
Yohei Seki, David Kirk Evans, Lun-Wei Ku, Le Sun, 
Hsin-His Chen, and Noriko Kando. 2008. Overview 
of multilingual opinion analysis task at NTCIR-7. In 
Proceedings of the NTCIR-7 Workshop.  
Yohei Seki, David K. Evans, Lun-Wei Ku, Le Sun, 
Hsin-His Chen, Noriko Kando, and Chin-Yew Lin. 
2007. Overview of opinion analysis pilot task at 
NTCIR-6. In Proceedings of the NTCIR-6 Workshop. 
Vikas Sindhwani, Partha Niyogi, and Mikhail Belkin. 
2005. A co-regularization approach to semi-
supervised learning with multiple views. In 
Proceedings of ICML?05. 
Noah A. Smith. 2006. Novel estimation methods for 
unsupervised discovery of latent structure in natural 
language text. Ph.D. thesis, Department of Computer 
Science, Johns Hopkins University. 
Huihsin Tseng, Pichuan Chang, Galen Andrew, Daniel 
Jurafsky and Christopher Manning. 2005. A 
conditional random field word segmenter. In 
Proeedings of the 4th SIGHAN Workshop. 
Peter D. Turney. 2002. Thumbs up or thumbs down? 
Semantic orientation applied to unsupervised 
classification of reviews, In Proceedings of ACL?02. 
Xiaojun Wan. 2008. Using Bilingual Knowledge and 
Ensemble Techniques for Unsupervised Chinese 
Sentiment Analysis. In Proceedings of  EMNLP?08. 
Xiaojun Wan. 2009. Co-training for cross-lingual 
sentiment classification. In Proceedings of 
ACL/AFNLP?09. 
Janyce Wiebe, Theresa Wilson, and Claire Cardie. 
2005. Annotating expressions of opinions and 
emotions in language. Language Resources and 
Evaluation, 39(2- 3): 165-210. 
Duo Zhang, Qiaozhu Mei, and ChengXiang Zhai. 2010. 
Cross-lingual latent topic extraction, In Proceedings 
of ACL?10. 
Hai Zhao, Yan Song, Chunyu Kit, and Guodong Zhou. 
2009. Cross language dependency parsing using a 
bilingual lexicon. In Proceedings of 
ACL/IJCNLP?09.  
Xiaojin Zhu and Andrew B. Goldberg. 2009. 
Introduction to Semi-Supervised Learning. Morgan 
& Claypool Publishers. 
Appendix A. Equation Deduction 
In this appendix, we derive the gradient for the objective 
function in Equation 3, which is used in parameter 
estimation. As mentioned in Section 3.3, the parameters 
can be learned by finding: 
   
    
         
      
                 
       
      
                    
       
      
                           
         
     
     
     
         
 
        
Since the first term on the right-hand side is just the 
expression for the standard MaxEnt problem, we will 
focus on the gradient for the second term, and denote 
       
     
     
     
          as ( ). 
Let         denote    or   , and   
  be the  th weight 
in the vector   . For brevity, we drop the   in the above 
notation, and write   
  to denote   
  . Then the partial 
derivative of (*) based on Equation 4 with respect to   
  
is as follows: 
    
   
  
     
    
     
 
   
     
    
       
 
     
    
         
    
       
 
                              (1) 
Further, we obtain: 
 
   
     
    
      
 
   
 
             
   
   
              
   
   
  
 
                       
 
             
   
   
              
   
   
  
 
  
    
    
     
 
             
   
   
                      
   
   
  
  
               
    
     
    
    
       
     
    
        
    
    
        
    
       
    
    
       (2) 
Merge (2) into (1), we get: 
    
   
  
 
     
    
         
    
       
 
      
    
         
    
          
   
    
    
        
    
       
    
    
         
      
    
         
    
       
    
    
        
     
    
       
    
    
             
    
    
    
       
    
         
    
          
    
           
  
330
Proceedings of the 5th International Workshop on Semantic Evaluation, ACL 2010, pages 292?295,
Uppsala, Sweden, 15-16 July 2010. c?2010 Association for Computational Linguistics
CityU-DAC: Disambiguating Sentiment-Ambiguous Adjectives within 
Context 
 
Bin LU and Benjamin K. TSOU 
Department of Chinese, Translation and Linguistics &  
Language Information Sciences Research Centre 
City University of Hong Kong 
{lubin2010, rlbtsou}@gmail.com 
  
 
Abstract 
This paper describes our system 
participating in task 18 of SemEval-2010, 
i.e. disambiguating Sentiment-
Ambiguous Adjectives (SAAs). To 
disambiguating SAAs, we compare the 
machine learning-based and lexicon-
based methods in our submissions: 1) 
Maximum entropy is used to train  
classifiers based on the annotated 
Chinese data from the NTCIR opinion 
analysis tasks, and the clause-level and 
sentence-level classifiers are compared; 
2) For the lexicon-based method, we first 
classify the adjectives into two classes: 
intensifiers (i.e. adjectives intensifying 
the intensity of context) and suppressors 
(i.e. adjectives decreasing the intensity of 
context), and then use the polarity of 
context to get the SAAs? contextual 
polarity based on a sentiment lexicon. 
The results show that the performance of 
maximum entropy is not quite high due 
to little training data; on the other hand, 
the lexicon-based method could improve 
the precision by considering the polarity 
of context. 
1 Introduction 
In recent years, sentiment analysis, which mines 
opinions from information sources such as news, 
blogs, and product reviews, has drawn much 
attention in the NLP field (Hatzivassiloglou and 
McKeown, 1997; Pang et al, 2002; Turney, 
2002; Hu and Liu, 2004; Pang and Lee, 2008). It 
has many applications such as social media 
monitoring, market research, and public 
relations.  
Some adjectives are neutral in sentiment 
polarity out of context, but they could show 
positive, neutral or negative meaning within 
specific context. Such words can be called 
dynamic sentiment-ambiguous adjectives 
(SAAs). However, SAAs have not been 
intentionally tackled in the researches of 
sentiment analysis, and usually have been 
discarded or ignored by most previous work. Wu 
et al, (2008) presents an approach of combining 
collocation information and SVM to 
disambiguate SAAs, in which the collocation-
based method was first used to disambiguate 
adjectives within the context of collocation (i.e. a 
sub-sentence marked by comma), and then the 
SVM algorithm was explored for those instances 
not covered by the collocation-based method. 
According to their experiments, their supervised 
algorithm achieves encouraging performance. 
The task 18 at SemEval-2010 is intended to 
create a benchmark dataset for disambiguating 
SAAs. Given only 100 trial sentences, but not 
provided with any official training data, 
participants are required to tackle this problem 
data by unsupervised approaches or use their 
own training data. The task consists of 14 SAAs, 
which are all high-frequency words in Mandarin 
Chinese. They are ?|big, ?|small, ?|many, ?
|few, ?|high, ?|low, ?|thick, ?|thin, ?|deep, 
?|shallow, ?|heavy, ?|light, ??|huge, ??
|grave. This task deals with Chinese SAAs, but 
the disambiguating techniques should be 
language-independent. Please refer to (Wu and 
Jin, 2010) for more descriptions of the task. 
In our participating system, the annotated 
Chinese data from the NTCIR opinion analysis 
tasks is used as training data with the help of a 
combined sentiment lexicon. A machine 
learning-based method (namely maximum 
entropy) and the lexicon-based method are 
compared in our submissions. The results show 
that the performance of maximum entropy is not 
quite high due to little training data; on the other 
hand, the lexicon-based method could improve 
292
the precision by considering the context of 
SAAs. In Section 2, we briefly describe data 
preparation of sentiment lexicon and training 
data. Our approaches for disambiguating SAAs 
are given in Section 3. The experiment and 
results are presented in Section 4, followed by a 
conclusion in Section 5. 
2 Data Preparation 
2.1 Sentiment Lexicon 
Several traditional Chinese resources of polar 
words/phrases are collected, including NTU 
Sentiment Dictionary1, The Lexicon of Chinese 
Positive Words (Shi and Zhu, 2006), The Lexicon 
of Chinese Negative Words (Yang and Zhu, 2006) 
0, and CityU?s sentiment-bearing word/phrase 
list (Lu et al 2008), which were manually 
marked in the political news data by trained 
annotators (Benjamin and Lu, 2008). Sentiment-
bearing items marked with the SENTIMENT_KW 
tag (SKPI), including only positive and negative 
items but not neutral ones, were also 
automatically extracted from the Chinese sample 
data of NTCIR-6 OAPT (Seki et al, 2007). All 
these polar item lexicons were combined, and the 
combined polar item lexicon consists of 13,437 
positive items and 18,365 negative items, a total 
of 31,802 items.  
2.2 Training Data 
The training data is extracted from the Chinese 
sample and test data from the NTCIR opinion 
analysis task, including NTCIR-6 (Seki et al, 
2007), NTCIR-7 (Seki et al, 2008) and NTCIR-8 
(Seki et al, 2010). The NTCIR opinion analysis 
tasks provide an opportunity to evaluate the 
techniques used by different participants based 
on a common evaluation framework in Chinese 
(simplified and traditional), Japanese and 
English.  
For data from NTCIR-6 and NTCIR-7, three 
annotators manually marked the polarity of each 
opinionated sentence, and the lenient polarity is 
used here as the gold standard (please refer to 
Seki et al, 2008 for explanation of lenient and 
strict standard). For each opinionated sentence 
from NTCIR-8, only two annotators marked and 
the strict polarity is used as the gold standard. 
The traditional Chinese sentences are transferred 
into simplified Chinese. In total, there are about 
12K opinionated sentences annotated with 
polarity, out of which about 9K are marked as 
                                                          
1 http://nlg18.csie.ntu.edu.tw:8080/opinion/index.html  
positive or negative, and others neutral. All the 
9K sentences plus the 100 sentences from the 
trial data are used as the sentence-level training 
data. 
Meanwhile, we also try to get the clause-level 
training data since the context of collocation 
within sub-sentences are quite crucial for 
disambiguating SAAs according to Wu et al 
(2008). From the 9K positive/ negative sentences 
above, we automatically extract the clause for 
each occurrence of SAAs.  
Note the polarity for a whole sentence is not 
necessarily the same with that of the clause 
containing SAAs. Consider the sentence ? ?? 
? ?? ? ?? ? ? ?? ?? ?? ?? 
(In the current large circumstance of the world, 
China and Russia support each other). The 
polarity of the whole sentence is positive, while 
the clause ??????????(In the current 
large circumstance of the world) containing a 
SAA ? (large) is neutral, and the polarity lies in 
the second part of the whole sentence, i.e. ?? 
?? (support each other). 
Thus, we manually checked the polarity of 
clauses containing SAAs. Due to time limitation, 
we only checked 465 clauses. Plus the clauses 
extracted from 100 trial sentences, the final 
clause-level training data consist of 565 
positive/negative clauses containing SAAs. 
3 Our Approach for Disambiguating 
SAAs 
To disambiguating SAAs, we use the maximum 
entropy algorithm and the sentiment lexicon-
based method, and also combine them together. 
3.1 The Maximum Entropy-based Method 
Maximum entropy classification (MaxEnt) is a 
technique which has proven effective in a 
number of natural language processing 
applications (Berger et al, 1996). Le Zhang?s 
maximum entropy tool2 is used for classification. 
The Chinese sentences are segmented into 
words using a production segmentation system. 
Unigrams of words are used as basic features for 
classification. Bigrams are also tried, but does 
not show improvement, and thus are not 
described in details here. 
3.2 The Lexicon-based Method 
For the lexicon-based method, we first classify 
the 14 adjectives into two classes: intensifiers 
                                                          
2 http:// homepages.inf.ed.ac.uk/lzhang10/maxent_toolkit.html 
293
and suppressors. Intensifiers refer to adjectives 
intensifying the intensity of context, including ?
|big, ? |many, ? |high, ? |thick, ? |deep, ?
|heavy, ??|huge, ??|grave, while suppressors 
refer to adjectives decreasing the intensity of 
context, including ?|small, ?|few, ?|low, ?
|thin, ?|shallow, ?|light. 
Meanwhile, the collocation nouns are also 
classified into two classes: positive and negative. 
Positive nouns include ? ? |quality, ? ?
|standard, ? ? |level, ? ? |benefit, ? ?
|achievement, etc. Negative nouns include ??
|pressure, ?? |disparity, ?? |problem, ??
|risk, ??|pollution etc.  
The hypothesis here is that intensifiers will 
receive the polarity of their collocations while 
suppressors will get the opposite polarity of their 
collocations. For example, ?? |achievement 
could be collocated with one of the following 
intensifiers: ?|big, ?|many or ?|high, and the 
adjectives just receive the polarity of ??
|achievement, which is positive. Meanwhile, ?
?|pollution could be collocated with one of the 
following suppressors: ?|small, ?|few, ?|low, 
and the adjectives just receive the opposite 
polarity of??|pollution, which is also positive. 
 Based on this hypothesis, we could get the 
polarity of SAAs through theirs collocation 
nouns within the clauses containing SAAs. The 
context of SAAs is a sub-sentence marked by 
comma. The sentiment lexicon mentioned in 
Section 2.1 is used to find polarity of collocation 
nouns. 
3.3 Combining Maximum Entropy and 
Lexicon  
To combine the two methods above, the lexicon-
based method is first used to disambiguate the 
sentiment of SAAs, and the context of 
collocation is a sub-sentence marked by comma. 
Then for those instances that are not covered by 
the lexicon-based method, the maximum entropy 
algorithm is explored. 
4 Experiment and Results 
The dataset contains two parts: some sentences 
were extracted from Chinese Gigaword (LDC 
corpus: LDC2005T14), and other sentences were 
gathered through the search engine like Google. 
Firstly, these sentences were automatically 
segmented and POS-tagged, and then the 
ambiguous adjectives were manually annotated 
with the correct sentiment polarity within the 
sentence context. Two annotators annotated the 
sentences double blindly, and the third annotator 
checks the annotation. All the data of 2,917 
sentences is provided as the test set, and 
evaluation is performed in terms of micro 
accuracy and macro accuracy.  
We submitted 4 runs: run 1 is based on the 
sentence-level MaxEnt classifier; run 2 on the 
clause-level MaxEnt classifier; run 3 is got by 
combining the lexicon-based method and the 
sentence-level MaxEnt classifier; and run 4 by 
combining the lexicon-based method and the 
clause-level MaxEnt classifier. The official 
scores for the 4 runs are shown in Table 2. 
Table 2. Results of 4 Runs 
Run Micro Acc. (%) Macro Acc. (%)
1 61.98 67.89 
2 62.63 60.85 
3 71.55 75.54 
4 72.47 69.80 
From Table 2, we can observe that: 
1) Compared the highest scores achieved by 
other teams, the performance of maximum 
entropy (run 1 and 2) is not quite high due to 
little training data;  
2) By integrating the lexicon-based method 
and maximum entropy (run 3 and 4), we improve 
the accuracy by considering the context of SAAs;  
3) The sentence-level maximum entropy 
classifier shows better macro accuracy, and 
clause-level one better micro accuracy. 
In addition to the official scores, we also 
evaluate the performance of the lexicon-based 
method alone. The micro and macro accuracy are 
respectively 0.847 and 0.835665, showing that 
the lexicon-based method is more accurate than 
the maximum entropy algorithm (run 1 and 2). 
But it only covers 1,436 (49%) of 2,917 test 
instances.  
Because the data from the NTCIR opinion 
analysis task is not specifically annotated for this 
task, and the manually checked clauses are less 
than 600, the performance of our system is not 
quite high compared to the highest performance 
achieved by other teams. 
5 Conclusion 
To disambiguating SAAs, we compare machine 
learning-based and lexicon-based methods in our 
submissions: 1) Maximum entropy is used to 
train classifiers based on the annotated Chinese 
data from the NTCIR opinion analysis tasks, and  
the clause-level and sentence-level classifiers are 
294
compared; 2) For the lexicon-based method, we 
first classify the adjectives into two classes: 
intensifiers (i.e. adjectives intensifying the 
intensity of context) and suppressors (i.e. 
adjectives decreasing the intensity of context), 
and then use the polarity of context to get the 
SAAs? contextual polarity. The results show that 
the performance of maximum entropy is not 
quite high due to little training data; on the other 
hand, the lexicon-based method could improve 
the precision by considering the context of 
SAAs. 
 
References 
Adam L. Berger, Stephen A. Della Pietra, and Vincent 
J. Della Pietra. 1996. A maximum entropy 
approach to natural language processing. 
Computational Linguistics, 22(1):39-71. 
Vasileios Hatzivassiloglou and Kathleen McKeown. 
1997. Predicting the Semantic Orientation of 
Adjectives. Proceedings of ACL-97. 174-181. 
Minqing Hu and Bing Liu. 2004. Mining Opinion 
Features in Customer Reviews. In Proceedings of 
the 19th National Conference on Artificial 
Intelligence, pp. 755-760.  
Bin Lu, Benjamin K. Tsou and Oi Yee Kwong. 2008. 
Supervised Approaches and Ensemble Techniques 
for Chinese Opinion Analysis at NTCIR-7. In 
Proceedings of the Seventh NTCIR Workshop 
(NTCIR-7). pp. 218-225. Tokyo, Japan. 
Bo Pang and Lillian Lee. 2008. Opinion mining and 
sentiment analysis, Foundations and Trends in 
Information Retrieval, Now Publishers. 
Bo Pang, Lillian Lee, and Shivakumar Vaithyanathan. 
2002. Thumbs up? Sentiment classification using 
machine learning techniques. In Proceedings of 
EMNLP 2002, pp.79?86. 
Yohei Seki, David Kirk Evans, Lun-Wei Ku, Le Sun, 
Hsin-His Chen, Noriko Kando. 2007. Overview of 
Opinion Analysis Pilot Task at NTCIR-6. Proc. of 
the Seventh NTCIR Workshop. Japan. 2007.6. 
Yohei Seki, David Kirk Evans, Lun-Wei Ku, Le Sun, 
Hsin-His Chen, Noriko Kando and Chin-Yew Lin. 
2008. Overview of Multilingual Opinion Analysis 
Task at NTCIR-7. Proc. of the Seventh NTCIR 
Workshop. Japan. Dec. 2008. 
Yohei Seki, Lun-Wei Ku, Le Sun, Hsin-His Chen, 
Noriko Kando. 2010. Overview of Multilingual 
Opinion Analysis Task at NTCIR-8. Proc. of the 
Seventh NTCIR Workshop. Japan. June, 2010. 
Jilin Shi and Yinggui Zhu. 2006. The Lexicon of 
Chinese Positive Words (?????). Sichuan 
Lexicon Press. 
Benjamin K. Tsou and Bin Lu. 2008. A Political 
News Corpus in Chinese for Opinion Analysis. In 
Proceedings of the Second International Workshop 
on Evaluating Information Access (EVIA2008). pp. 
6-7. Tokyo, Japan. 
Peter D. Turney. 2002. Thumbs up or thumbs down? 
Semantic orientation applied to unsupervised 
classification of reviews, In Proceedings of ACL-
02, Philadelphia, Pennsylvania, 417-424. 
Yunfang Wu, Miao Wang, Peng Jin and Shiwen Yu. 
2008. Disambiguate sentiment ambiguous 
adjectives. In Proceedings of  IEEE International 
Conference on Natural Language Processing and 
Knowledge Engineering (NLP-KE?08). 
Yunfang Wu, and Peng Jin. 2010. SemEval-2010 
Task 18: Disambiguate sentiment ambiguous 
adjectives. In Proceedings of SemEval-2010. 
Ruifeng Xu, Kam-Fai Wong and Yunqing Xia. 2008. 
Coarse-Fine Opinion Mining - WIA in NTCIR-7 
MOAT Task. In Proceedings of the Seventh NTCIR 
Workshop (NTCIR-7). Tokyo, Japan, Dec. 16-19. 
Ling Yang and Yinggui Zhu. 2006. The Lexicon of 
Chinese Negative Words (?????). Sichuan 
Lexicon Press. 
 
 
295
Mining Large-scale Parallel Corpora from Multilingual Patents: 
An English-Chinese example and its application to SMT 
Bin Lu?, Benjamin K. Tsou??, Tao Jiang?, Oi Yee Kwong?, and Jingbo Zhu? 
?Department of Chinese, Translation & Linguistics, City University of Hong Kong 
?Research Centre on Linguistics and Language Information Sciences,  
Hong Kong Institute of Education 
?ChiLin Star Corp., Southern Software Park, Zhuhai, China 
?Natural Language Processing Lab, Northeastern University, Shenyang, China 
{lubin2010, rlbtsou, jiangtaoster}@gmail.com, 
rlolivia@cityu.edu.hk, zhujingbo@mail.neu.edu.cn 
 
Abstract 
In this paper, we demonstrate how to 
mine large-scale parallel corpora with 
multilingual patents, which have not 
been thoroughly explored before. We 
show how a large-scale English-Chinese 
parallel corpus containing over 14 
million sentence pairs with only 1-5% 
wrong can be mined from a large amount 
of English-Chinese bilingual patents. To 
our knowledge, this is the largest single 
parallel corpus in terms of sentence pairs. 
Moreover, we estimate the potential for 
mining multilingual parallel corpora 
involving English, Chinese, Japanese, 
Korean, German, etc., which would to 
some extent reduce the parallel data 
acquisition bottleneck in multilingual 
information processing. 
1 Introduction 
Multilingual data are critical resources for 
building many applications, such as machine 
translation (MT) and cross-lingual information 
retrieval. Many parallel corpora have been built, 
such as the Canadian Hansards (Gale and 
Church, 1991), the Europarl corpus (Koehn, 
2005), the Arabic-English and English-Chinese 
parallel corpora used in the NIST Open MT 
Evaluation.  
However, few parallel corpora exist for many 
language pairs, such as Chinese-Japanese, 
Japanese-Korean, Chinese- French or 
Japanese-German. Even for language pairs with 
several parallel corpora, such as Chinese-English 
and Arabic-English, the size of parallel corpora 
is still a major limitation for SMT systems to 
achieve higher performance. 
In this paper, we present a way which could, to 
some extent, reduce the parallel data acquisition 
bottleneck in multilingual language processing.  
Based on multilingual patents, we show how an 
enlarged English-Chinese parallel corpus 
containing over 14 million high-quality sentence 
pairs can be mined from a large number of 
comparable patents harvested from the Web. To 
our knowledge, this is the largest single parallel 
corpus in terms of parallel sentences. Some SMT 
experiments are also reported. Moreover, we 
investigate the potential to get large-scale 
parallel corpora for languages beyond the 
Canadian Hansards, Europarl and UN news used 
in NIST MT Evaluation by estimating the 
quantity of multilingual patents involving 
English, Chinese, Japanese, Korean, German, 
etc.  
Related work is introduced in Section 2. 
Patents, PCT patents, multilingual patents are 
described in Section 3. Then an English-Chinese 
parallel corpus, its mining process and 
application to SMT are introduced in Section 4, 
followed by the quantity estimation of 
multilingual patents involving other language 
pairs in Section 5. We discuss the results in 
Section 6, and conclude in Section 7. 
2 Related Work 
Parallel sentences could be extracted from 
parallel documents or comparable corpora. 
Different approaches have been proposed to 
align sentences in parallel documents consisting 
of the same content in different languages based 
on the following information: a) the sentence 
length in bilingual sentences (Brown et al 1991; 
Gale and Church, 1991); b) lexical information 
in bilingual dictionaries (Ma, 2006); c) statistical 
translation model (Chen, 1993), or the composite 
of more than one approach (Simard and 
Plamondon, 1998; Moore, 2002).  
To overcome the lack of parallel documents, 
comparable corpora are also used to mine 
parallel sentences, which raises further 
challenges since the bilingual contents are not 
strictly parallel. For instance, Zhao and Vogel 
(2002) investigated the mining of parallel 
sentences for Web bilingual news. Munteanu and 
Marcu (2005) presented a method for 
discovering parallel sentences in large Chinese, 
Arabic, and English comparable, non-parallel 
corpora based on a maximum entropy classifier. 
Cao et al, (2007) and Lin et al, (2008) proposed 
two different methods utilizing the parenthesis 
pattern to extract term translations from bilingual 
web pages. Jiang et al (2009) presented an 
adaptive pattern-based method which produced 
Chinese-English bilingual sentences and terms  
with over 80% accuracy. 
Only a few papers were found on the related 
work in the patent domain. Higuchi et al (2001) 
used the titles and abstracts of 32,000 
Japanese-English bilingual patents to extract 
bilingual terms. Utiyama and Isahara (2007) 
mined about 2 million parallel sentences by 
using two parts in the description section of 
Japanese-English comparable patents. Lu et al 
(2009) derived about 160K parallel sentences 
from Chinese-English comparable patents by 
aligning sentences and filtering alignments with 
the combination of different quality measures. 
Another closely related work is the 
English-Chinese parallel corpus (Lu et al, 
2010), which is largely extended by this work, in 
which both the number of patents and that of 
parallel sentences are augmented by about 
100%, and more SMT experiments are given. 
Moreover, we show the potential for mining 
parallel corpora from multilingual patents 
involving other languages. 
For statistical machine translation (SMT), 
tremendous strides have been made in two 
decades, including Brown et al (1993), Och and 
Ney (2004) and Chiang (2007). For the MT 
evaluation, NIST (Fujii et al, 2008; 2010) has 
been organizing open evaluations for years, and 
the performance of the participants has been 
improved rapidly.  
3 Patents and Multilingual Patents 
A patent is a legal document representing ?an 
official document granting the exclusive right to 
make, use, and sell an invention for a limited 
period? (Collins English Dictionary1). A patent 
application consists of different sections, and we 
focus on the text, i.e. only title, abstract, claims 
and description.  
3.1 PCT Patents 
Since the invention in a patent is only protected 
in the filing countries, a patent applicant who 
wishes to protect his invention outside the 
original country should file patents in other 
countries, which may involve other languages. 
The Patent Cooperation Treaty (PCT) system 
offers inventors and industry an advantageous 
route for obtaining patent protection 
internationally. By filing one ?international? 
patent application under the PCT via the World 
Intellectually Property Organization (WIPO), 
protection of an invention can be sought 
simultaneously (i.e. the priority date) in each of a 
large number of countries. 
The number of PCT international applications 
                                                          
1 Retrieved March 2010, from 
http://www.collinslanguage.com/ 
filed is more than 1.7 million 2 . A PCT 
international application may be filed in any 
language accepted by the relevant receiving 
office, but must be published in one of the 
official publication languages (Arabic, Chinese, 
English, French, German, Japanese, Korean, 
Russian and Spanish). Other highly used 
languages for filing include Italian, Dutch, 
Finnish, Swedish, etc. Table 1 3  shows the 
number of PCT applications for the most used 
languages of filing and publication.  
 Lang. of Filing 
Share 
(%) 
Lang. of 
Publication 
Share 
(%) 
English 895K 52.1 943K 54.9 
Japanese 198K 11.5 196K 11.4 
German 185K 10.8 184K 10.7 
French 55K 3.2 55K 3.2 
Korean 24K 1.4 3K4 0.2 
Chinese 24K 1.4 24K 1.4 
Other 336K 19.6 313K 18.2 
Total 1.72M 100 1.72M 100 
Table 1. PCT Application Numbers for Languages of 
Publication and Filing 
From Table 1, we can observe that English, 
Japanese and German are the top 3 languages in 
terms of PCT applications, and English accounts 
for over 50% of applications in terms of 
language of both publication and filing.  
3.2 Multilingual Patents 
A PCT application does not necessarily mean a 
multilingual patent. An applicant who has 
decided to proceed further with his PCT 
international application must fulfill the 
requirements for entry into the PCT national 
phase at the patent offices of countries where he 
seeks protection. For example, a Chinese 
company may first file a Chinese patent in China 
                                                          
2 Retrieved Apr., 2010 from 
http://www.wipo.int/pctdb/en/. The data below involving 
PCT patents comes from the website of WIPO. 
3 The data in this and other tables in the following sections 
involving PCT patents comes from the website of WIPO. 
4  Korean just became one of the official publication 
languages for the PCT system since 2009, and thus the 
number of PCT patents with Korean as language of 
publication is small. 
patent office and then file its international 
application also in Chinese under the PCT. Later 
on, it may have the patent translated into English 
and file it in USA patent office, which means the 
patent becomes bilingual. If the applicant 
continues to file it in Japan with Japanese, it 
would be trilingual. Even more, it would be 
quadrilingual or involve more languages when it 
is filed in other countries with more languages. 
Such multilingual patents are considered 
comparable (or noisy parallel) because they are 
not parallel in the strict sense but still closely 
related in terms of information conveyed 
(Higuchi et al, 2001; Lu et al, 2009). 
4 A Large English-Chinese Parallel 
Corpus Mined from Bilingual Patents 
In this section, we introduce the English-Chinese 
bilingual patents harvested from the Web and the 
method to mine parallel sentences from them. 
SMT experiments on the final parallel corpus are 
also described. 
4.1 Harvesting English-Chinese Bilingual 
Patents 
The official patent office in China is the State 
Intellectual Property Office (SIPO). In early 
2009,  by searching on its website, we found 
about 200K Chinese patents previously filed as 
PCT applications in English and crawled their 
bibliographical data, titles, abstracts and the 
major claim from the Web, and then other claims 
and descriptions were also added. Since some 
contents are in the image format, the images 
were OCRed and the texts recognized were 
manually verified. 
All PCT patent applications are filed through 
WIPO. With the Chinese patents mentioned 
above, the corresponding English patents were 
searched from the website of WIPO by the PCT 
publication numbers to obtain relevant sections 
of the English PCT applications, including 
bibliographical data, title, abstract, claims and 
description. About 80% (160K) out of the 
Chinese patents found their corresponding 
English ones. Some contents of the English 
patents were OCRed by WIPO. 
We automatically split the patents into 
individual sections according to the respective 
tags inside the patents, and segmented each 
section sentences according to punctuations. The 
statistics of each section for Chinese and 
English patents are shown in Table 2. 
Chinese English 
Sections 
#Char #Sent #Word #Sent 
Title 2.7M 157K 1.6M 157K 
Abstract 33M 596K 20M 784K 
Claim 367M 6.8M 217M 7.4M 
Desc. 2,467M 48.8M 1,353M 54.0M 
Total 2,870M 56.2M 1,591M 62.3M 
Table 2. Statistics of Comparable Patents 
4.2 Mining Parallel Sentences from 
Bilingual Patents 
The sentences in each section of Chinese patents 
were aligned with those in the corresponding 
section of the corresponding English patents to 
find parallel sentences after the Chinese 
sentences were segmented into words. 
Since the comparable patents are not strictly 
parallel, the individual alignment methods 
mentioned in Section 2 would be not effective: 1) 
the length-based method is not accurate since it 
does not consider content similarity; 2) the 
bilingual dictionary-based method cannot deal 
with new technical terms in the patents; 3) the 
translation model-based method would need 
training data to get a translation model. Thus, in 
this study we combine these three methods to 
mine high-quality parallel sentences from 
comparable patents. 
We first use a bilingual dictionary to 
preliminarily align the sentences in each section 
of the comparable patents. The dictionary-based 
similarity score dP  of a sentence pair is 
computed based on a bilingual dictionary as 
follows (Utiyama and Isahara, 2003):  
2/??
)deg()deg(
),(
),(
ce
Sw Sw ec
ec
ecd ll
ww
ww
SSp cc ee
+
=
? ?
? ?
?
 
where cw  and ew  are respectively the 
word types in Chinese sentence cS  and 
English sentence eS ; cl  and el  respectively 
denote the lengths of cS  and eS  in terms of 
the number of words; and ),( ec ww?  = 1 if 
cw  and ew  is a translation pair in the 
bilingual dictionary or are the same string, 
otherwise 0; and 
?
?
=
ee Sw
ecc www ),()deg( ?
?
?
=
ce Sw
ece www ),()deg( ? . 
For the bilingual dictionary, we combine three 
ones: namely, LDC_CE_DIC2.0 5  constructed 
by LDC, bilingual terms in HowNet and the 
bilingual lexicon in Champollion (Ma, 2006). 
We then remove sentence pairs using length 
filtering and ratio filtering: 1) for length filtering, 
if a sentence pair has more than 100 words in the 
English sentence or more than 333 characters in 
the Chinese one, it is removed; 2) for length ratio 
filtering, we discard the sentence pairs with 
Chinese-English length ratio outside the range of 
0.8 to 1.8. The parameters here are set 
empirically. 
We further filter the parallel sentence 
candidates by learning an IBM Model-1 on the 
remaining aligned sentences and compute the 
translation similarity score tP  of sentence 
pairs by combining the translation probability 
value of both directions (i.e. Chinese->English 
and English->Chinese) based on the trained 
IBM-1 model (Moore, 2002; Chen, 2003; Lu et 
al, 2009). It is computed as follows: 
ec
ecce
ect ll
)S(SPlog)S(SPlog
SSp
+
+
=
)|()|(
),(
 
where )SS(P ce | denotes the probability 
that a translator will produce eS  in English 
when presented with cS  in Chinese, and vice 
versa for )|(S ec SP . Sentence pairs with 
                                                          
5 http://projects.ldc.upenn.edu/Chinese/LDC_ch.htm 
similarity score tP  lower than a predefined 
threshold are filtered out as wrong aligned 
sentences. 
Table 3 shows the sentence numbers and the 
percentages of sentences kept in each step above 
with respect to all sentence pairs. In the first row 
of Table 3, 1.DICT denotes the first step of using 
the bilingual dictionary to align sentences; 2. FL 
denotes the length and ratio filtering; 3. TM 
refers to the third and final step of using 
translation models to filter sentence pairs. 
 1. DICT 2.FL 3. TM (final) 
Abstr. 503K 352K  (70%) 
166K  
(33%) 
Claims 6.0M 4.3M (72.1%) 
2.0M 
(33.4%) 
Desc. 38.6M 26.8M (69.4%) 
12.1M 
(31.3%) 
Total6 45.1M 31.5M (69.8%) 
14.3M 
(31.7%) 
Table 3. Numbers of Sentence Pairs 
Both the 31.5M parallel sentences after the 
second step FL and the final 14.3M after the third 
step TM are manually evaluated by randomly 
sampling 100 sentence pairs for each section. 
The evaluation metric follows the one in Lu et al 
(2009), which classifies each sentence pair into 
Correct, Partially Correct or Wrong. The results 
of manual evaluation are shown in Table 4. 
 Section Correct Partially Correct Wrong 
Abstr. 85% 7% 8% 
Claims 83% 10% 7% 2. FL 
Desc. 69% 15% 15% 
Abstr. 97% 2% 1% 
Claims 92% 3% 5% 3. TM  (final) 
Desc. 89% 8% 3% 
Table 4. Manual Evaluation of the Corpus 
From Table 4, we can see that: 1) In the final 
corpus, the percentages of correct parallel 
sentences are quite high, and the wrong 
percentages are no higher than 5%; 2) Without 
                                                          
6 Here the total number does not include the number of 
titles, which are directly treated as parallel. 
the final step of TM, the accuracies of 31.5M 
sentence pairs are between 69%-85%, and the 
percentages of wrong pairs are between 
7%-15%; 3) The abstract section shows the 
highest correct percentage, while the description 
section shows the lowest. 
Thus, we could conclude that the mined 14M 
parallel sentences are of high quality with only 
1%-5% wrong pairs, and our combination of 
bilingual dictionaries and translation models for 
mining parallel sentences are quite effective. 
4.3 Chinese-English Statistical Machine 
Translation 
A Chinese-English SMT system is setup using 
Moses (Koehn, 2007). We train models  based 
on different numbers of parallel sentences mined 
above. The test set contains 548 sentence pairs 
which are randomly selected and different from 
the training data. The sizes of the training data 
and BLEU scores for the models are shown in 
Table 5. 
System BLEU (%) #Sentence Pairs for training 
Model-A 17.94 300K 
Model-B 19.96 750K 
Model-C 20.09 1.5M 
Model-D 20.98 3M 
Model-E 22.60 6M 
Table 5. SMT Experimental Results 
From Table 5, we can see that the BLEU 
scores are improving steadily when the training 
data increases. When the training data is 
enlarged by 20 times from 300K to 6M, the 
BLEU score increases to 22.60 from 17.94, 
which is quite a significant improvement. We 
show the translations of one Chinese sample 
sentence in Table 6 below. 
CN  
Sent. 
?? ?? ?? ??? ?? ? ? ?? ? 
? ? 
Ref. 
the main shaft of the electric motor 
extends into the working cavity of the 
compressor shell , 
Model-A the motor main shaft into the compressor the chamber 
Model-B motor shaft into the compressor housing . the working chamber 
Model-C motor shaft into the compressor housing . the working chamber 
Model-D 
motor spindle extends into the 
compressor housing . the working 
chamber 
Model-E motor spindle extends into the working chamber of the compressor housing , 
Table 6. Translations of One Chinese Sentence 
From Table 6, we can see the translations 
given by Model-A to Model-C are lack of the 
main verb, the one given by Model-D has an 
ordering problem for the head noun and the 
modifier, and the one given by Model-E seems 
better than the others and its content is already 
quite similar to the reference despite the lexical 
difference. 
5 Multilingual Corpora for More 
Languages 
In this section, we describe the potential of 
building large-scale parallel corpora for more 
languages, especially Asian languages by using 
the 1.7 million PCT patent applications and their 
national correspondents. By using PCT 
applications as the pivot, we can build 
multilingual parallel corpora from multilingual 
patents, which would greatly enlarge parallel 
data we could obtain. 
The patent applications filed in one country 
should be in the official language(s) of the 
country, e.g. the applications filed in China 
should be in Chinese, those in Japan be in 
Japanese, and so on. In Table 7, the second 
column shows the total numbers of patent 
applications in different countries which were 
previously filed as PCT ones; and the third 
column shows the total numbers of applications 
in different countries, which were previously 
filed as PCT ones with English as language of 
publication. 
National Phase 
Country7 ALL 
English as Lang. 
of Publication 
                                                          
7 For the national phase of the PCT System, the statistics 
are based on data supplied to WIPO by national and 
Japan 424K 269K 
China 307K 188K 
Germany 32K 10K 
R. Korea 236K 134K 
China & Japan 189K 130K 
China & R. Korea 154K 91K 
Japan & R. Korea 158K 103K 
China & Japan  
& R. Korea 106K 73K 
Table 7. Estimated Numbers of Multilingual 
Patents 
The number of the Chinese-English bilingual 
patents (CE) in Table 7 is about 188K, which is 
consistent with the number of 160K found in 
Section 4.1 since the latter contains only the 
applications up to early 2009. Based on Table 7, 
we estimate below the rough sizes of bilingual 
corpora, trilingual corpora, and even 
quadrilingual corpora for different languages. 
1) Bilingual Corpora with English as one 
language 
Compared to CE (188K), the 
Japanese-English bilingual corpus (269K) could 
be 50% larger in terms of bilingual patents, the 
Korean-English one (134K) could be about 30% 
smaller, and the German-English one (10K) 
would be much smaller. 
2) Bilingual Corpora for Asian Languages  
The Japanese-Chinese bilingual corpus 
(189K) could be comparable to CE (188K) in 
terms of bilingual applications, the Chinese- 
Korean one (154K) could be about 20% smaller, 
and the Japanese-Korean one (158K) is quite 
similar to the Chinese-Korean one. 
3)  Trilingual Corpora 
In addition to bilingual corpora, we can also 
build trilingual corpora from trilingual patents. It 
is quite interesting to note that the trilingual 
corpora  could be quite large even compared to 
the bilingual corpora.  
The trilingual corpora for Chinese, Japanese 
and English (130K) could be only 30% smaller 
than CE in terms of patents. The trilingual corpus 
                                                                                      
regional patent Offices, received at WIPO often 6 months or 
more after the end of the year concerned, i.e. the numbers 
are not up-to-date . 
for Chinese, Korean and English (91K) and that 
for Japanese, Korean and English (103K) are 
also quite large. The number of the trilingual 
patents for the Asian languages of Chinese, 
Japanese and Korean (106K) is about 54% of 
that of CE. 
4) Quadrilingual Corpora 
The number of the quadrilingual patents for 
Chinese, Japanese, Korean and English (73K) is 
about 38% of that of CE. From these figures, we 
could say that a large proportion of the PCT 
applications published in English later have been 
filed in all the three Asian countries: China, 
Japan, and R. Korea. 
6 Discussion 
The websites from which the Chinese and 
English patents were downloaded were quite 
slow to access, and were occasionally down 
during access. To avoid too much workload for 
the websites, the downloading speed had been 
limited. Some large patents would cost much 
time for the websites to respond and had be 
specifically handled. It took considerable efforts 
to obtain these comparable patents.  
In addition our English-Chinese corpus mined 
in this study is at least one order of magnitude 
larger, we give some other differences between 
ours and those introduced in Section 2 (Higuchi 
et al, 2001; Utiyama and Isahara, 2007; Lu et al 
2009)  
1) Their bilingual patents were identified by 
the priority information in the US patents, and 
could not be easily extended to language pairs 
without English; while our method using PCT 
applications as the pivot could be easily 
extended to other language pairs as illustrated in 
Section 5. 
2) The translation process is different: their 
patents were filed in USA Patent Office in 
English by translating from Japanese or Chinese, 
while our patents were first filed in English as a 
PCT application, and later translated into 
Chinese. The different translation processes may 
have different characteristics. 
Since the PCT and multilingual patent 
applications increase rapidly in recent years as 
discussed in Section 3, we could expect more 
multilingual patents to enlarge the large-scale 
parallel corpora with the new applications and 
keep them up-to-date with new technical terms. 
On the other hand, patents are usually translated 
by patent agents or professionals, we could 
expect high quality translations from 
multilingual patents. We have been planning to 
build trilingual and quadrilingual corpora from 
multilingual patents. 
One possible limitation of patent corpora is 
that the sentences are all from technical domains 
and written in formal style, and thus it is 
interesting to know if the parallel sentences 
could improve the performance of SMT systems  
on NIST MT evaluation corpus containing news 
sentences and web sentences.  
7 Conclusion 
In this paper, we show how a large high-quality 
English-Chinese parallel corpus can be mined 
from a large amount of comparable patents 
harvested from the Web, which is the largest 
single parallel corpus in terms of the  number of 
parallel sentences. Some sampled parallel 
sentences are available at 
http://www.livac.org/smt/parpat.html, and more 
parallel sentences would be publicly available to 
the research community. 
With 1.7 million PCT patent applications and 
their corresponding national ones, there are 
considerable potentials of constructing 
large-scale high-quality parallel corpora for 
languages. We give an estimation on the sizes of 
multilingual parallel corpora which could be 
obtained from multilingual patents involving 
English, Chinese, Japanese, Korean, German, 
etc., which would to some extent reduce the 
parallel data acquisition bottleneck in 
multilingual information processing. 
Acknowledgements 
We wish to thank Mr. Long Jiang from 
Microsoft Research Asia and anonymous 
reviewers for their valuable comments. 
References 
Brown, Peter F., Jennifer C. Lai, and Robert L. 
Mercer. 1991. Aligning sentences in parallel 
corpora. In Proceedings of ACL. pp.169-176. 
Brown, Peter F., Stephen A. Della Pietra, Vincent J. 
Della Pietra, and Robert L. Mercer. 1993. 
Mathematics of statistical machine translation: 
Parameter estimation. Computational Linguistics, 
19(2), 263-311. 
Cao, Guihong, Jianfeng Gao and Jianyun Nie. 2007. 
A System to Mine Large-scale Bilingual 
Dictionaries from  Monolingual Web Pages. In 
Proceedings of MT Summit. pp. 57-64. 
Chen, Stanley F. 1993. Aligning sentences in 
bilingual corpora using lexical information. In 
Proceedings of ACL. pp. 9-16. 
Chiang, David. 2007. Hierarchical phrase-based 
translation. Computational Linguistics, 33(2), 
201?228. 
Fujii, Atsushi, Masao Utiyama, Mikio Yamamoto, 
and Takehito Utsuro. 2008. Overview of the patent 
translation task at the NTCIR-7 workshop. In 
Proceedings of the NTCIR-7 Workshop. pp. 
389-400. Tokyo, Japan. 
Fujii, Atsushi, Masao Utiyama, Mikio Yamamoto, 
Takehito Utsuro, Terumasa Ehara, Hiroshi 
Echizen-ya and Sayori Shimohata. 2010. 
Overview of the patent translation task at the 
NTCIR-8 workshop. In Proceedings of the 
NTCIR-8 Workshop. Tokyo, Japan. 
Gale, William A., and Kenneth W. Church. 1991. A 
program for aligning sentences in bilingual 
corpora. In Proceedings of ACL. pp.79-85. 
Higuchi, Shigeto, Masatoshi Fukui, Atsushi Fujii, and 
Tetsuya Ishikawa. PRIME: A System for 
Multi-lingual Patent Retrieval. In Proceedings of 
MT Summit VIII, pp.163-167, 2001. 
Koehn, Philipp. 2005. Europarl: A parallel corpus for 
statistical machine translation. In Proceedings of 
MT Summit X. 
Koehn, Philipp, Hieu Hoang, Alexandra Birch, Chris 
Callison-Burch, et al 2007. Moses: Open source 
toolkit for statistical machine translation. In 
Proceedings of ACL Demo Session. pp. 177-180. 
Lin, Dekang, Shaojun Zhao, Benjamin V. Durme and 
Marius Pasca. 2008. Mining Parenthetical 
Translations from the Web by Word Alignment. In 
Proceedings of ACL-08. pp. 994-1002. 
Jiang, Long, Shiquan Yang, Ming Zhou, Xiaohua Liu, 
and Qingsheng Zhu. 2009. Mining Bilingual Data 
from the Web with Adaptively Learnt Patterns. In 
Proceedings of ACL-IJCNLP. pp. 870-878. 
Lu, Bin, Benjamin K. Tsou, Jingbo Zhu, Tao Jiang, 
and Olivia Y. Kwong. 2009. The Construction of 
an English-Chinese Patent Parallel Corpus. MT 
Summit XII 3rd Workshop on Patent Translation. 
Lu, Bin, Tao Jiang, Kapo Chow and Benjamin K. 
Tsou. 2010. Building a Large English-Chinese 
Parallel Corpus from Comparable Patents and its 
Experimental Application to SMT. LREC 
Workshop on Building and Using Comparable 
Corpora. Malta. May, 2010. 
Ma, Xiaoyi. 2006. Champollion: A Robust Parallel 
Text Sentence Aligner. In Proceedings of the 5th 
International Conference on Language Resources 
and Evaluation (LREC). Genova, Italy. 
Moore, Robert C. 2002. Fast and Accurate Sentence 
Alignment of Bilingual Corpora. In Proceedings of 
AMTA. pp.135-144. 
Munteanu, Dragos S., and Daniel Marcu. 2005. 
Improving Machine Translation Performance by 
Exploiting Non-parallel Corpora. Computational 
Linguistics, 31(4), 477?504. 
Och, Franz J., and Hermann Ney. 2004. The 
Alignment Template Approach to Machine 
Translation. Computational Linguistics, 30(4), 
417-449. 
Simard, Michel, and Pierre Plamondon. 1998. 
Bilingual Sentence Alignment: Balancing 
Robustness and Accuracy. Machine Translation, 
13(1), 59-80. 
Utiyama, Masao, and Hitoshi Isahara. 2007. A 
Japanese-English patent parallel corpus. In 
Proceeding of MT Summit XI. pp. 475?482. 
Zhao, Bing, and Stephen Vogel. 2002. Adaptive 
Parallel Sentences Mining from Web Bilingual 
News Collection. In Proceedings of Second IEEE 
International Conference on Data Mining 
(ICDM?02). 
