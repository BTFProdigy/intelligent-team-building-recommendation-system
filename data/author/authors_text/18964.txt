Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 1134?1144,
Baltimore, Maryland, USA, June 23-25 2014.
c?2014 Association for Computational Linguistics
Are Two Heads Better than One? Crowdsourced Translation via a
Two-Step Collaboration of Non-Professional Translators and Editors
Rui Yan, Mingkun Gao, Ellie Pavlick, and Chris Callison-Burch
Computer and Information Science Department,
University of Pennsylvania, Philadelphia, PA 19104, U.S.A.
{ruiyan,gmingkun,epavlick}@seas.upenn.edu, ccb@cis.upenn.edu
Abstract
Crowdsourcing is a viable mechanism for
creating training data for machine trans-
lation. It provides a low cost, fast turn-
around way of processing large volumes
of data. However, when compared to pro-
fessional translation, naive collection of
translations from non-professionals yields
low-quality results. Careful quality con-
trol is necessary for crowdsourcing to
work well. In this paper, we examine
the challenges of a two-step collaboration
process with translation and post-editing
by non-professionals. We develop graph-
based ranking models that automatically
select the best output from multiple redun-
dant versions of translations and edits, and
improves translation quality closer to pro-
fessionals.
1 Introduction
Statistical machine translation (SMT) systems are
trained using bilingual sentence-aligned parallel
corpora. Theoretically, SMT can be applied to
any language pair, but in practice it produces the
state-of-art results only for language pairs with
ample training data, like English-Arabic, English-
Chinese, French-English, etc. SMT gets stuck
in a severe bottleneck for many minority or ?low
resource? languages with insufficient data. This
drastically limits which languages SMT can be
successfully applied to. Because of this, collect-
ing parallel corpora for minor languages has be-
come an interesting research challenge. There are
various options for creating training data for new
language pairs. Past approaches have examined
harvesting translated documents from the web
(Resnik and Smith, 2003; Uszkoreit et al, 2010;
Smith et al, 2013), or discovering parallel frag-
ments from comparable corpora (Munteanu and
Marcu, 2005; Abdul-Rauf and Schwenk, 2009;
Smith et al, 2010). Until relatively recently, lit-
tle consideration has been given to creating par-
allel data from scratch. This is because the cost
of hiring professional translators is prohibitively
high. For instance, Germann (2001) hoped to hire
professional translators to create a modest sized
100,000 word Tamil-English parallel corpus, but
were stymied by the costs and the difficulty of
finding good translators for a short-term commit-
ment.
Recently, crowdsourcing has opened the possi-
bility of translating large amounts of text at low
cost using non-professional translators. Facebook
localized its web site into different languages us-
ing volunteers (TechCrunch, 2008). DuoLingo
turns translation into an educational game, and
translates web content using its language learners
(von Ahn, 2013).
Rather than relying on volunteers or gamifica-
tion, NLP research into crowdsourcing transla-
tion has focused on hiring workers on the Ama-
zon Mechanical Turk (MTurk) platform (Callison-
Burch, 2009). This setup presents unique chal-
lenges, since it typically involves non-professional
translators whose language skills are varied, and
since it sometimes involves participants who try
to cheat to get the small financial reward (Zaidan
and Callison-Burch, 2011). A natural approach
for trying to shore up the skills of weak bilinguals
is to pair them with a native speaker of the tar-
get language to edit their translations. We review
relevant research from NLP and human-computer
interaction (HCI) on collaborative translation pro-
cesses in Section 2. To sort good translations from
bad, researchers often solicit multiple, redundant
translations and then build models to try to predict
which translations are the best, or which transla-
tors tend to produce the highest quality transla-
tions.
The contributions of this paper are:
1134
? An analysis of the difficulties posed by a two-
step collaboration between editors and trans-
lators in Mechanical Turk-style crowdsourc-
ing environments. Editors vary in quality,
and poor editing can be difficult to detect.
? A new graph-based algorithm for selecting
the best translation among multiple transla-
tions of the same input. This method takes
into account the collaborative relationship
between the translators and the editors.
2 Related work
In the HCI community, several researchers have
proposed protocols for collaborative translation
efforts (Morita and Ishida, 2009b; Morita and
Ishida, 2009a; Hu, 2009; Hu et al, 2010). These
have focused on an iterative collaboration between
monolingual speakers of the two languages, facil-
itated with a machine translation system. These
studies are similar to ours in that they rely on na-
tive speakers? understanding of the target language
to correct the disfluencies in poor translations. In
our setup the poor translations are produced by
bilingual individuals who are weak in the target
language, and in their experiments the translations
are the output of a machine translation system.
1
Another significant difference is that the HCI
studies assume cooperative participants. For in-
stance, Hu et al (2010) recruited volunteers from
the International Children?s Digital Library (Hour-
cade et al, 2003) who were all well intentioned
and participated out a sense of altruism and to
build a good reputation among the other volunteer
translators at childrenslibrary.org. Our
setup uses anonymous crowd workers hired on
Mechanical Turk, whose motivation to participate
is financial. Bernstein et al (2010) characterized
the problems with hiring editors via MTurk for a
word processing application. Workers were either
lazy (meaning they made only minimal edits) or
overly zealous (meaning they made many unnec-
essary edits). Bernstein et al (2010) addressed
this problem with a three step find-fix-verify pro-
cess. In the first step, workers click on one word
or phrase that needed to be corrected. In the next
step, a separate group of workers proposed correc-
1
A variety of HCI and NLP studies have confirmed the
efficacy of monolingual or bilingual individuals post-editing
of machine translation output (Callison-Burch, 2005; Koehn,
2010; Green et al, 2013). Past NLP work has also examined
automatic post-editing(Knight and Chander, 1994).
tions to problematic regions that had been identi-
fied by multiple workers in the first pass. In the
final step, other workers would validate whether
the proposed corrections were good.
Most NLP research into crowdsourcing has fo-
cused on Mechanical Turk, following pioneering
work by Snow et al (2008) who showed that the
platform was a viable way of collecting data for a
wide variety of NLP tasks at low cost and in large
volumes. They further showed that non-expert an-
notations are similar to expert annotations when
many non-expert labelings for the same input
are aggregated, through simple voting or through
weighting votes based on how closely non-experts
matched experts on a small amount of calibra-
tion data. MTurk has subsequently been widely
adopted by the NLP community and used for an
extensive range of speech and language applica-
tions (Callison-Burch and Dredze, 2010).
Although hiring professional translators to cre-
ate bilingual training data for machine translation
systems has been deemed infeasible, Mechanical
Turk has provided a low cost way of creating large
volumes of translations (Callison-Burch, 2009;
Ambati and Vogel, 2010). For instance, Zbib et
al. (2012; Zbib et al (2013) translated 1.5 mil-
lion words of Levine Arabic and Egyptian Arabic,
and showed that a statistical translation system
trained on the dialect data outperformed a system
trained on 100 times more MSA data. Post et al
(2012) used MTurk to create parallel corpora for
six Indian languages for less than $0.01 per word.
MTurk workers translated more than half a million
words worth of Malayalam in less than a week.
Several researchers have examined the use of ac-
tive learning to further reduce the cost of transla-
tion (Ambati et al, 2010; Ambati, 2012; Blood-
good and Callison-Burch, 2010). Crowdsourcing
allowed real studies to be conducted whereas most
past active learning were simulated. Pavlick et al
(2014) conducted a large-scale demographic study
of the languages spoken by workers on MTurk by
translating 10,000 words in each of 100 languages.
Chen and Dolan (2012) examined the steps neces-
sary to build a persistent multilingual workforce
on MTurk.
This paper is most closely related to previous
work by Zaidan and Callison-Burch (2011), who
showed that non-professional translators could ap-
proach the level of professional translators. They
solicited multiple redundant translations from dif-
1135
Urdu translator:
According to the territory?s people the pamphlets from
the Taaliban had been read in the announcements in all
the mosques of the Northern Wazeerastan.
English post-editor:
According to locals, the pamphlet released by the Taliban
was read out on the loudspeakers of all the mosques in
North Waziristan.
LDC professional:
According to the local people, the Taliban?s pamphlet
was read over the loudspeakers of all mosques in North
Waziristan.
Table 1: Different versions of translations.
ferent Turkers for a collection of Urdu sentences
that had been previously professionally translated
by the Linguistics Data Consortium. They built a
model to try to predict on a sentence-by-sentence
and Turker-by-Turker which was the best transla-
tion or translator. They also hired US-based Turk-
ers to edit the translations, since the translators
were largely based in Pakistan and exhibited er-
rors that are characteristic of speakers of English
as a language. Zaidan and Callison-Burch (2011)
observed only modest improvements when incor-
porating these edited translation into their model.
We attempt to analyze why this is, and we pro-
posed a new model to try to better leverage their
data.
3 Crowdsourcing Translation
Setup We conduct our experiments using the
data collected by Zaidan and Callison-Burch
(2011). This data set consists 1,792 Urdu sen-
tences from a variety of news and online sources,
each paired with English translations provided by
non-professional translators on Mechanical Turk.
Each Urdu sentence was translated redundantly
by 3 distinct translators, and each translation was
edited by 3 separate (native English-speaking) ed-
itors to correct for grammatical and stylistic er-
rors. In total, this gives us 12 non-professional
English candidate sentences (3 unedited, 9 edited)
per original Urdu sentence. 52 different Turkers
took part in the translation task, each translating
138 sentences on average. In the editing task, 320
Turkers participated, averaging 56 sentences each.
For comparison, the data also includes 4 differ-
ent reference translations for each source sentence,
produced by professional translators.
Table 1 gives an example of an unedited trans-
lation, an edited translation, and a professional
translation for the same sentence. The transla-
tions provided by translators on MTurk are gen-
erally done conscientiously, preserving the mean-
ing of the source sentence, but typically con-
tain simple mistakes like misspellings, typos, and
awkward word choice. English-speaking editors,
despite having no knowledge of the source lan-
guage, are able to fix these errors. In this work,
we show that the collaboration design of two
heads? non-professional Urdu translators and non-
professional English editors? yields better trans-
lated output than would either one working in iso-
lation, and can better approximate the quality of
professional translators.
Analysis We know from inspection that trans-
lations seem to improve with editing (Table 1).
Given the data from MTurk, we explore whether
this is the case in general: Do all translations im-
prove with editing? To what extent does the in-
dividual translator and the individual editor effect
the quality of the final sentence?
Figure 1: Relationship between editor aggressive-
ness and effectiveness. Each point represents an
editor/translation pair. Aggressiveness (x-axis) is
measured as the TER between the pre-edit and
post-edit version of the translation, and effective-
ness (y-axis) is measured as the average amount
by which the editing reduces the translation?s
TER
gold
. While many editors make only a few
changes, those who make many changes can bring
the translation substantially closer to professional
quality.
We use translation edit rate (TER) as a mea-
sure of translation similarity. TER represents the
amount of change necessary to transform one sen-
tence into another, so a low TER means the two
1136
0.020.05
0.07
? T
ER g
old
Editor ? TERgold 0.03  0.50
0.00
0.01
0.02
? T
ER g
old
Editor ? TERgold 0.01  0.03
-0.03
-0.01
0.01
? T
ER g
old
Editor ? TERgold -0.01  0.01
-0.08
-0.04
-0.00
? T
ER g
old
Editor ? TERgold -0.03  -0.01
0.3  0.9 0.2  0.3 0.2  0.2 0.1  0.2 0.0  0.1Translation TERgold
-0.30
-0.15
-0.01
? T
ER g
old
Editor ? TERgold -0.64  -0.03
Figure 2: Effect of editing on translations of vary-
ing quality. Rows reflect bins of editors, with the
worse editors (those whose changes result in in-
creased TER
gold
) on the top and the most effective
editors (those whose changes result in the largest
reduction in TER
gold
) on the bottom. Bars re-
flect bins of translations, with the highest TER
gold
translations on the left, and the lowest on the
right. We can see from the consistently negative
? TER
gold
in the bottom row that good editors are
able to improve both good and bad translations.
sentences are very similar. To capture the quality
(?professionalness?) of a translation, we take the
average TER of the translation against each of our
gold translations. That is, we define TER
gold
of
translation t as
TER
gold
=
1
4
4
?
i=1
TER(gold
i
, t) (1)
where a lower TER
gold
is indicative of a higher
quality (more professional-sounding) translation.
We first look at editors along two dimensions:
their aggressiveness and their effectiveness. Some
editors may be very aggressive (they make many
changes to the original translation) but still be in-
effective (they fail to bring the quality of the trans-
lation closer to that of a professional). We measure
aggressiveness by looking at the TER between
the pre- and post-edited versions of each editor?s
translations; higher TER implies more aggressive
editing. To measure effectiveness, we look at the
change in TER
gold
that results from the editing;
negative ?TER
gold
means the editor effectively
improved the quality of the translation, while pos-
itive ?TER
gold
means the editing actually brought
the translation further from our gold standard.
Figure 1 shows the relationship between these
two qualities for individual editor/translation
pairs. We see that while most translations re-
quire only a few edits, there are a large number
of translations which improve substantially after
heavy editing. This trend conforms to our intu-
ition that editing is most useful when the transla-
tion has much room for improvement, and opens
the question of whether good editors can offer im-
provements to translations of all qualities.
To address this question, we split our transla-
tions into 5 bins, based on their TER
gold
. We also
split our editors into 5 bins, based on their effec-
tiveness (i.e. the average amount by which their
editing reduces TER
gold
). Figure 2 shows the de-
gree to which editors at each level are able to im-
prove the translations from each bin. We see that
good editors are able to make improvements to
translations of all qualities, but that good editing
has the greatest impact on lower quality transla-
tions. This result suggests that finding good ed-
itor/translator pairs, rather than good editors and
good translators in isolation, should produce the
best translations overall. Figure 3 gives an exam-
ple of how an initially medium-quality translation,
when combined with good editing, produces a bet-
ter result than the higher-quality translation paired
with mediocre editing.
4 Problem Formulation
The problem definition of the crowdsourcing
translation task is straightforward: given a set of
candidate translations for a source sentence, we
want to choose the best output translation.
This output translation is the result of the com-
bined translation and editing stages. Therefore,
our method operates over a heterogeneous net-
work that includes translators and post-editors as
well as the translated sentences that they pro-
duce. We frame the problem as follows. We form
two graphs: the first graph (G
T
) represents Turk-
ers (translator/editor pairs) as nodes; the second
graph (G
C
) represents candidate translated and
1137
Figure 3: Three alternative translations (left) and the edited versions of each (right). Each edit on the
right was produced by a different editor. Order reflects the TER
gold
of each translation, with the lowest
TER
gold
on the top. Some translators receive low TER
gold
scores due to superficial errors, which can be
easily improved through editing. In the above example, the middle-ranked translation (green) becomes
the best translation after being revised by a good editor.
post-edited sentences (henceforth ?candidates?) as
nodes. These two graphs, G
T
and G
C
are com-
bined as subgraphs of a third graph (G
TC
). Edges
in G
TC
connect author pairs (nodes in G
T
) to the
candidate that they produced (nodes in G
C
). To-
gether, G
T
, G
C
, and G
TC
define a co-ranking
problem (Yan et al, 2012a; Yan et al, 2011b; Yan
et al, 2012b) with linkage establishment (Yan et
al., 2011a; Yan et al, 2012c), which we define for-
mally as follows.
Let G denote the heterogeneous graph with
nodes V and edges E. Let G = (V ,E) =
(V
T
, V
C
, E
T
, E
C
, E
TC
). G is divided into three
subgraphs, G
T
, G
C
, and G
TC
. G
C
= (V
C
, E
C
) is
a weighted undirected graph representing the can-
didates and their lexical relationships to one an-
other. Let V
C
denote a collection of translated
and edited candidates, and E
C
the lexical simi-
larity between the candidates (see Section 4.3 for
details). G
T
= (V
T
, E
T
) is a weighted undirected
graph representing collaborations between Turk-
ers. V
T
is the set of translator/editor pairs. Edges
E
T
connect translator/editor pairs in V
T
which
share a translator and/or editor. Each collabora-
tion (i.e. each node in V
T
) produces a candidate
(i.e. a node in V
C
). G
TC
= (V
TC
, E
TC
) is an
unweighted bipartite graph that ties G
T
and G
C
together and represents ?authorship?. The graph
G consists of nodes V
TC
= V
T
? V
C
and edges
E
TC
connecting each candidate with its authoring
translator/post-editor pair. The three sub-networks
(G
T
, G
C
, and G
TC
) are illustrated in Figure 4.
4.1 Inter-Graph Ranking
The framework includes three random walks, one
on G
T
, one on G
C
and one on G
TC
. A random
walk on a graph is a Markov chain, its states be-
ing the vertices of the graph. It can be described
by a stochastic square matrix, where the dimen-
sion is the number of vertices in the graph, and the
entries describe the transition probabilities from
one vertex to the next. The mutual reinforcement
framework couples the two random walks on G
T
and G
C
that rank candidates and Turkers in iso-
lation. The ranking method allows us to obtain
a global ranking by taking into account the intra-
/inter-component dependencies. In the following
sections, we describe how we obtain the rankings
on G
T
and G
C
, and then move on to discuss how
the two are coupled.
Our algorithm aims to capture the following in-
tuitions. A candidate is important if 1) it is similar
to many of the other proposed candidates and 2)
it is authored by better qualified translators and/or
post-editors. Analogously, a translator/editor pair
is believed to be better qualified if 1) the editor
is collaborating with a good translator and vice
versa and 2) the pair has authored important candi-
dates. This ranking schema is actually a reinforced
process across the heterogeneous graphs. We use
two vectors c = [pi(c)]
|c|?1
and t = [pi(t)]
|t|?1
to
denote the saliency scores pi(.) of candidates and
Turker pairs. The above-mentioned intuitions can
be formulated as follows:
? Homogeneity. We use adjacency matrix
1138
Figure 4: 2-step collaborative crowdsourcing translation model based on graph ranking framework in-
cluding three sub-networks. The undirected links between users denotes translation-editing collabora-
tion. The undirected links between candidate translations indicate lexical similarity between candidates.
A bipartite graph ties candidate and Turker networks together by authorship (to make the figure clearer,
some linkage is omitted). A dashed circle indicates the group of candidate translations for a single source
sentence to translate.
[M ]
|c|?|c|
to describe the homogeneous affinity
between candidates and [N ]
|t|?|t|
to describe the
affinity between Turkers.
c ?M
T
c, t ? N
T
t (2)
where c = |V
C
| is the number of vertices in the
candidate graph and t = |V
T
| is the number of ver-
tices in the Turker graph. The adjacency matrix
[M ] denotes the transition probabilities between
candidates, and analogously matrix [N ] denotes
the affinity between Turker collaboration pairs.
? Heterogeneity. We use an adjacency matrix
[
?
W ]
|c|?|t|
and [
?
W ]
|t|?|c|
to describe the authorship
between the output candidate and the producer
Turker pair from both of the candidate-to-Turker
and Turker-to-candidate perspectives.
c ?
?
W
T
t, t ?
?
W
T
c (3)
All affinity matrices will be defined in the next
section. By fusing the above equations, we can
have the following iterative calculation in matrix
forms. For numerical computation of the saliency
scores, the initial scores of all sentences and Turk-
ers are set to 1 and the following two steps are
alternated until convergence to select the best can-
didate.
Step 1: compute the saliency scores of candi-
dates, and then normalize using `-1 norm.
c
(n)
= (1? ?)M
T
c
(n?1)
+ ?
?
W t
(n?1)
c
(n)
= c
(n)
/||c
(n)
||
1
(4)
Step 2: compute the saliency scores of Turker
pairs, and then normalize using `-1 norm.
t
(n)
= (1? ?)N
T
t
(n?1)
+ ?
?
W c
(n?1)
t
(n)
= t
(n)
/||t
(n)
||
1
(5)
where ? specifies the relative contributions to the
saliency score trade-off between the homogeneous
affinity and the heterogeneous affinity. In order
to guarantee the convergence of the iterative form,
we must force the transition matrix to be stochastic
and irreducible. To this end, we must make the
c and t column stochastic (Langville and Meyer,
2004). c and t are therefore normalized after each
iteration of Equation (4) and (5).
4.2 Intra-Graph Ranking
The standard PageRank algorithm starts from an
arbitrary node and randomly selects to either fol-
low a random out-going edge (considering the
weighted transition matrix) or to jump to a random
node (treating all nodes with equal probability).
1139
In a simple random walk, it is assumed that all
nodes in the transitional matrix are equi-probable
before the walk starts. Then c and t are calculated
as:
c = ?M
T
c + (1? ?)
1
|V
C
|
(6)
and
t = ?N
T
t + (1? ?)
1
|V
T
|
(7)
where 1 is a vector with all elements equaling to 1
and the size is correspondent to the size of V
C
or
V
T
. ? is the damping factor usually set to 0.85, as
in the PageRank algorithm.
4.3 Affinity Matrix Establishment
We introduce the affinity matrix calculation, in-
cluding homogeneous affinity (i.e., M,N ) and
heterogeneous affinity (i.e.,
?
W,
?
W ).
As discussed, we model the collection of can-
didates as a weighted undirected graph, G
C
, in
which nodes in the graph represent candidate sen-
tences and edges represent lexical relatedness. We
define an edge?s weight to be the cosine similarity
between the candidates represented by the nodes
that it connects. The adjacency matrix M describes
such a graph, with each entry corresponding to the
weight of an edge.
F(c
i
, c
j
) =
c
i
? c
j
||c
i
||||c
j
||
M
ij
=
F(c
i
, c
j
)
?
k
F(c
i
, c
k
)
(8)
where F(.) is the cosine similarity and c is a term
vector corresponding to a candidate. We treat a
candidate as a short document and weight each
term with tf.idf (Manning et al, 2008), where tf
is the term frequency and idf is the inverse docu-
ment frequency.
The Turker graph, G
T
, is an undirected graph
whose edges represent ?collaboration.? Formally,
let t
i
and t
j
be two translator/editor pairs; we say
that pair t
i
?collaborates with? pair t
j
(and there-
fore, there is an edge between t
i
and t
j
) if t
i
and
t
j
share either a translator or an editor (or share
both a translator and an editor). Let the function
I(t
i
, t
j
) denote the number of ?collaborations?
(#col) between t
i
and t
j
.
I(t
i
, t
j
) =
{
#col (e
ij
? E
T
)
0 otherwise
, (9)
Then the adjacency matrix N is then defined as
N
ij
=
I(t
i
, t
j
)
?
k
I(t
i
, t
k
)
(10)
In the bipartite candidate-Turker graph G
TC
,
the entry E
TC
(i, j) is an indicator function denot-
ing whether the candidate c
i
is generated by t
j
:
A(c
i
, t
j
) =
{
1 (e
ij
? E
TC
)
0 otherwise
(11)
Through E
TC
we define the weight matrices
?
W
ij
and
?
W
ij
, containing the conditional probabil-
ities of transitions from c
i
to t
j
and vice versa:
?
W
ij
=
A(c
i
, t
j
)
?
k
A(c
i
, t
k
)
,
?
W
ij
=
A(c
i
, t
j
)
?
k
A(c
k
, t
j
)
(12)
5 Evaluation
We are interested in testing our random walk
method, which incorporates information from
both the candidate translations and from the Turk-
ers. We want to test two versions of our pro-
posed collaborative co-ranking method: 1) based
on the unedited translations only and 2) based on
the edited sentences after translator/editor collab-
orations.
Metric Since we have four professional transla-
tion sets, we can calculate the Bilingual Evalu-
ation Understudy (BLEU) score (Papineni et al,
2002) for one professional translator (P1) using
the other three (P2,3,4) as a reference set. We
repeat the process four times, scoring each pro-
fessional translator against the others, to calculate
the expected range of professional quality transla-
tion. In the following sections, we evaluate each of
our methods by calculating BLEU scores against
the same four sets of three reference translations.
Therefore, each number reported in our experi-
mental results is an average of four numbers, cor-
responding to the four possible ways of choosing 3
of the 4 reference sets. This allows us to compare
the BLEU score achieved by our methods against
the BLEU scores achievable by professional trans-
lators.
Baselines As a naive baseline, we choose one
candidate translation at random for each input
Urdu sentence. To establish an upper bound for
our methods, and to determine if there exist high-
quality Turker translations at all, we compute four
1140
Reference (Avg.) 42.51
Oracle (Seg-Trans) 44.93
Oracle (Seg-Trans+Edit) 48.44
Oracle (Turker-Trans) 38.66
Oracle (Turker-Trans+Edit) 39.16
Random 30.52
Lowest TER 35.78
Graph Ranking (Trans) 38.88
Graph Ranking (Trans+Edit) 41.43
Table 2: Overall BLEU performance for all
methods (with and without post-editing). The
highlighted result indicates the best performance,
which is based on both candidate sentences and
Turker information.
oracle scores. The first oracle operates at the seg-
ment level on the sentences produced by transla-
tors only: for each source segment, we choose
from the translations the one that scores highest
(in terms of BLEU) against the reference sen-
tences. The second oracle is applied similarly,
but chooses from the candidates produced by the
collaboration of translator/post-editor pairs. The
third oracle operates at the worker level: for each
source segment, we choose from the translations
the one provided by the worker whose transla-
tions (over all sentences) score the highest on
average. The fourth oracle also operates at the
worker level, but selects from sentences produced
by translator/post-editor collaborations. These or-
acle methods represent ideal solutions under our
scenario. We also examine two voting-inspired
methods. The first method selects the translation
with the minimum average TER (Snover et al,
2006) against the other translations; intuitively,
this would represent the ?consensus? translation.
The second method selects the translation gen-
erated by the Turker who, on average, provides
translations with the minimum average TER.
Results A summary of our results in given in Ta-
ble 2. As expected, random selection yields bad
performance, with a BLEU score of 30.52. The
oracles indicate that there is usually an acceptable
translation from the Turkers for any given sen-
tence. Since the oracles select from a small group
of only 4 translations per source segment, they are
not overly optimistic, and rather reflect the true po-
tential of the collected translations. On average,
the reference translations give a score of 42.38. To
put this in perspective, the output of a state-of-the-
Figure 5: Effect of candidate-Turker coupling (?)
on BLEU score.
art machine translation system (the syntax-based
variant of Joshua) achieves a score of 26.91, which
is reported in (Zaidan and Callison-Burch, 2011).
The approach which selects the translations with
the minimum average TER (Snover et al, 2006)
against the other three translations (the ?consen-
sus? translation) achieves BLEU scores of 35.78.
Using the raw translations without post-editing,
our graph-based ranking method achieves a BLEU
score of 38.89, compared to Zaidan and Callison-
Burch (2011)? s reported score of 28.13, which
they achieved using a linear feature-based classi-
fication. Their linear classifier achieved a reported
score of 39.06
2
when combining information from
both translators and editors. In contrast, our pro-
posed graph-based ranking framework achieves a
score of 41.43 when using the same information.
This boost in BLEU score confirms our intuition
that the hidden collaboration networks between
candidate translations and transltor/editor pairs are
indeed useful.
Parameter Tuning There are two parameters in
our experimental setups: ? controls the probability
of starting a new random walk and ? controls the
coupling between the candidate and Turker sub-
graphs. We set the damping factor ? to 0.85, fol-
lowing the standard PageRank paradigm. In order
to determine a value for ?, we used the average
BLEU, computed against the professional refer-
2
Note that the data we used in our experiments are slightly
different, by discarding nearly 100 NULL sentences in the
raw data. We do not re-implement this baseline but report the
results from the paper directly. According to our experiments,
most of the results generated by baselines and oracles are very
close to the previously reported values.
1141
Plain ranking 38.89
w/o collaboration 38.88
Shared translator 41.38
Shared post-editor 41.29
Shared Turker 41.43
Table 3: Variations of all component settings.
ence translations, as a tuning metric. We experi-
mented with values of ? ranging from 0 to 1, with
a step size of 0.05 (Figure 5). Small ? values place
little emphasis on the candidate/Turker coupling,
whereas larger values rely more heavily on the co-
ranking. Overall, we observed better performance
with values within the range of 0.05-0.15. This
suggests that both sources of information? the can-
didate itself and its authors? are important for the
crowdsourcing translation task. In all of our re-
ported results, we used the ? = 0.1.
Analysis We examine the relative contribution
of each component of our approach on the overall
performance. We first examine the centroid-based
ranking on the candidate sub-graph (G
C
) alone
to see the effect of voting among translated sen-
tences; we denote this strategy as plain ranking.
Then we incorporate the standard random walk on
the Turker graph (G
T
) to include the structural in-
formation but without yet including any collabo-
ration information; that is, we incorporate infor-
mation from G
T
and G
C
without including edges
linking the two together. The co-ranking paradigm
is exactly the same as the framework described in
Section 3.2, but with simplified structures.
Finally, we examine the two-step collaboration
based candidate-Turker graph using several varia-
tions on edge establishment. As before, the nodes
are the translator/post-editor working pairs. We
investigate three settings in which 1) edges con-
nect two nodes when they share only a transla-
tor, 2) edges connect two nodes when they share
only a post-editor, and 3) edges connect two nodes
when they share either a translator or a post-editor.
These results are summarized in Table 3.
Interestingly, we observe that when modeling
the linkage between the collaboration pairs, con-
necting Turker pairs which share either a transla-
tor or the post-editor achieves better performance
than connecting pairs that share only translators or
connecting pairs which share only editors. This
result supports the intuition that a denser collabo-
ration matrix will help propagate saliency to good
translators/post-editors and hence provides better
predictions for candidate quality.
6 Conclusion
We have proposed an algorithm for using a two-
step collaboration between non-professional trans-
lators and post-editors to obtain professional-
quality translations. Our method, based on a
co-ranking model, selects the best crowdsourced
translation from a set of candidates, and is capable
of selecting translations which near professional
quality.
Crowdsourcing can play a pivotal role in fu-
ture efforts to create parallel translation datasets.
In addition to its benefits of cost and scalabil-
ity, crowdsourcing provides access to languages
that currently fall outside the scope of statistical
machine translation research. In future work on
crowdsourced translation, further benefits in qual-
ity improvement and cost reduction could stem
from 1) building ground truth data sets based on
high-quality Turkers? translations and 2) identify-
ing when sufficient data has been collected for a
given input, to avoid soliciting unnecessary redun-
dant translations.
Acknowledgements
This material is based on research sponsored by
a DARPA Computer Science Study Panel phase 3
award entitled ?Crowdsourcing Translation? (con-
tract D12PC00368). The views and conclusions
contained in this publication are those of the au-
thors and should not be interpreted as represent-
ing official policies or endorsements by DARPA
or the U.S. Government. This research was sup-
ported by the Johns Hopkins University Human
Language Technology Center of Excellence and
through gifts from Microsoft, Google and Face-
book.
References
Sadaf Abdul-Rauf and Holger Schwenk. 2009. On the
use of comparable corpora to improve SMT perfor-
mance. In Proceedings of the 12th Conference of the
European Chapter of the ACL (EACL 2009), pages
16?23, March.
Vamshi Ambati and Stephan Vogel. 2010. Can crowds
build parallel corpora for machine translation sys-
tems? In Workshop on Creating Speech and Lan-
guage Data with MTurk.
1142
Vamshi Ambati, Stephan Vogel, and Jaime G Car-
bonell. 2010. Active learning and crowd-sourcing
for machine translation. In LREC, volume 11, pages
2169?2174. Citeseer.
Vamshi Ambati. 2012. Active Learning and Crowd-
sourcing for Machine Translation in Low Resource
Scenarios. Ph.D. thesis, Language Technologies In-
stitute, School of Computer Science, Carnegie Mel-
lon University, Pittsburgh, PA.
Michael S. Bernstein, Greg Little, Robert C. Miller,
Bjrn Hartmann, Mark S. Ackerman, David R.
Karger, David Crowell, and Katrina Panovich.
2010. Soylent: a word processor with a crowd in-
side. In Proceedings of the ACM Symposium on
User Interface Software and Technology (UIST).
Michael Bloodgood and Chris Callison-Burch. 2010.
Large-scale cost-focused active learning for statisti-
cal machine translation. In Proceedings of the 48th
Annual Meeting of the Association for Computa-
tional Linguistics.
Chris Callison-Burch and Mark Dredze. 2010. Cre-
ating speech and language data with Amazon?s Me-
chanical Turk. In Proceedings of the NAACL HLT
2010 Workshop on Creating Speech and Language
Data with Amazon?s Mechanical Turk, pages 1?12,
June.
Chris Callison-Burch. 2005. Linear B system de-
scription for the 2005 NIST MT evaluation exercise.
In Proceedings of Machine Translation Evaluation
Workshop.
Chris Callison-Burch. 2009. Fast, cheap, and creative:
Evaluating translation quality using amazon?s me-
chanical turk. In Proceedings of EMNLP.
David L. Chen and William B. Dolan. 2012. Build-
ing a persistent workforce on mechanical turk for
multilingual data collection. In Proceedings of the
Human Computer Interaction International Confer-
ence.
Ulrich Germann. 2001. Building a statistical machine
translation system from scratch: How much bang
for the buck can we expect? In Proceedings of
the Workshop on Data-driven Methods in Machine
Translation - Volume 14, DMMT ?01, pages 1?8.
Spence Green, Jeffrey Heer, and Christopher D. Man-
ning. 2013. The efficacy of human post-editing for
language translation. In Proceedings of the SIGCHI
Conference on Human Factors in Computing Sys-
tems, CHI ?13, pages 439?448.
Juan Pablo Hourcade, Benjamin B Bederson, Allison
Druin, Anne Rose, Allison Farber, and Yoshifumi
Takayama. 2003. The international children?s digi-
tal library: viewing digital books online. Interacting
with Computers, 15(2):151?167.
Chang Hu, Benjamin B. Bederson, and Philip Resnik.
2010. Translation by iterative collaboration be-
tween monolingual users. In Proceedings of
ACM SIGKDD Workshop on Human Computation
(HCOMP).
Chang Hu, Benjamin B. Bederson, Philip Resnik, and
Yakov Kronrod. 2011. Monotrans2: A new human
computation system to support monolingual trans-
lation. In Proceedings of the SIGCHI Conference
on Human Factors in Computing Systems, CHI ?11,
pages 1133?1136.
Chang Hu. 2009. Collaborative translation by mono-
lingual users. In CHI ?09 Extended Abstracts on Hu-
man Factors in Computing Systems, CHI EA ?09,
pages 3105?3108.
Martin Kay. 1998. The proper place of men and ma-
chines in language translation. Machine Transla-
tion, 12(1/2):3?23, January.
Kevin Knight and Ishwar Chander. 1994. Automated
postediting of documents. In In Proceedings of
AAAI.
Philipp Koehn. 2010. Enabling monolingual transla-
tors: Post-editing vs. options. In HLT-NAACL?10,
pages 537?545, June.
Amy N Langville and Carl D Meyer. 2004. Deeper
inside pagerank. Internet Mathematics, 1(3):335?
380.
Zhifei Li, Chris Callison-Burch, Chris Dyer, Juri Gan-
itkevitch, Ann Irvine, Sanjeev Khudanpur, Lane
Schwartz, Wren Thornton, Ziyuan Wang, Jonathan
Weese, and Omar Zaidan. 2010. Joshua 2.0: A
toolkit for parsing-based machine translation with
syntax, semirings, discriminative training and other
goodies. In Proceedings of the Joint Fifth Work-
shop on Statistical Machine Translation and Metric-
sMATR, pages 133?137, July.
Annie Louis and Ani Nenkova. 2013. What makes
writing great? first experiments on article quality
prediction in the science journalism domain. Trans-
actions of Association for Computational Linguis-
tics.
Christopher D Manning, Prabhakar Raghavan, and
Hinrich Sch?utze. 2008. Introduction to information
retrieval, volume 1. Cambridge University Press
Cambridge.
Daisuke Morita and Toru Ishida. 2009a. Collaborative
translation by monolinguals with machine transla-
tors. In Proceedings of the 14th International Con-
ference on Intelligent User Interfaces, IUI ?09, pages
361?366.
Daisuke Morita and Toru Ishida. 2009b. Designing
protocols for collaborative translation. In Interna-
tional Conference on Principles of Practice in Multi-
Agent Systems (PRIMA-09), pages 17?32. Springer.
1143
Dragos Stefan Munteanu and Daniel Marcu. 2005.
Improving machine translation performance by ex-
ploiting non-parallel corpora. Comput. Linguist.,
31(4):477?504, December.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: A method for automatic
evaluation of machine translation. In Proceedings
of the 40th Annual Meeting on Association for Com-
putational Linguistics, ACL ?02, pages 311?318.
Ellie Pavlick, Matt Post, Ann Irvine, Dmitry Kachaev,
and Chris Callison-Burch. 2014. The language de-
mographics of Amazon Mechanical Turk. Transac-
tions of the Association for Computational Linguis-
tics (TACL), 2(Feb):79?92.
Matt Post, Chris Callison-Burch, and Miles Osborne.
2012. Constructing parallel corpora for six Indian
languages via crowdsourcing. In Proceedings of the
Seventh Workshop on Statistical Machine Transla-
tion.
Philip Resnik and Noah A. Smith. 2003. The web
as a parallel corpus. Computational Linguistics,
29(3):349?380, September.
Jason R. Smith, Chris Quirk, and Kristina Toutanova.
2010. Extracting parallel sentences from compa-
rable corpora using document level alignment. In
HLT-NAACL?10, pages 403?411.
Jason Smith, Herve Saint-Amand, Magdalena Pla-
mada, Philipp Koehn, Chris Callison-Burch, and
Adam Lopez. 2013. Dirt cheap web-scale paral-
lel text from the Common Crawl. In Proceedings of
the 2013 Conference of the Association for Compu-
tational Linguistics (ACL 2013), July.
Matthew Snover, Bonnie Dorr, Richard Schwartz, Lin-
nea Micciulla, and John Makhoul. 2006. A study of
translation edit rate with targeted human annotation.
In Proceedings of association for machine transla-
tion in the Americas, pages 223?231.
Rion Snow, Brendan O?Connor, Daniel Jurafsky, and
Andrew Y. Ng. 2008. Cheap and fast?but is it
good?: Evaluating non-expert annotations for nat-
ural language tasks. In Proceedings of the Con-
ference on Empirical Methods in Natural Language
Processing, EMNLP ?08, pages 254?263.
TechCrunch. 2008. Facebook taps users to create
translated versions of site, January.
Jakob Uszkoreit, Jay M. Ponte, Ashok C. Popat, and
Moshe Dubiner. 2010. Large scale parallel docu-
ment mining for machine translation. In Proceed-
ings of the 23rd International Conference on Com-
putational Linguistics, COLING ?10, pages 1101?
1109.
Luis von Ahn. 2013. Duolingo: Learn a language for
free while helping to translate the web. In Proceed-
ings of the 2013 International Conference on Intel-
ligent User Interfaces, IUI ?13, pages 1?2.
Rui Yan, Liang Kong, Congrui Huang, Xiaojun Wan,
Xiaoming Li, and Yan Zhang. 2011a. Timeline gen-
eration through evolutionary trans-temporal summa-
rization. In Proceedings of the Conference on Em-
pirical Methods in Natural Language Processing,
EMNLP ?11, pages 433?443.
Rui Yan, Xiaojun Wan, Jahna Otterbacher, Liang Kong,
Xiaoming Li, and Yan Zhang. 2011b. Evolutionary
timeline summarization: A balanced optimization
framework via iterative substitution. In Proceed-
ings of the 34th International ACM SIGIR Confer-
ence on Research and Development in Information
Retrieval, SIGIR ?11, pages 745?754.
Rui Yan, Mirella Lapata, and Xiaoming Li. 2012a.
Tweet recommendation with graph co-ranking. In
Proceedings of the 50th Annual Meeting of the Asso-
ciation for Computational Linguistics: Long Papers
- Volume 1, ACL ?12, pages 516?525.
Rui Yan, Xiaojun Wan, Mirella Lapata, Wayne Xin
Zhao, Pu-Jen Cheng, and Xiaoming Li. 2012b.
Visualizing timelines: Evolutionary summarization
via iterative reinforcement between text and image
streams. In Proceedings of the 21st ACM Interna-
tional Conference on Information and Knowledge
Management, CIKM ?12, pages 275?284.
Rui Yan, Zi Yuan, Xiaojun Wan, Yan Zhang, and Xi-
aoming Li. 2012c. Hierarchical graph summariza-
tion: leveraging hybrid information through visible
and invisible linkage. In PAKDD?12, pages 97?108.
Springer.
Omar F. Zaidan and Chris Callison-Burch. 2011.
Crowdsourcing translation: Professional quality
from non-professionals. In Proceedings of the 49th
Annual Meeting of the Association for Computa-
tional Linguistics: Human Language Technologies,
pages 1220?1229.
Rabih Zbib, Erika Malchiodi, Jacob Devlin, David
Stallard, Spyros Matsoukas, Richard Schwartz, John
Makhoul, Omar F. Zaidan, and Chris Callison-
Burch. 2012. Machine translation of Arabic di-
alects. In The 2012 Conference of the North Amer-
ican Chapter of the Association for Computational
Linguistics.
Rabih Zbib, Gretchen Markiewicz, Spyros Matsoukas,
Richard Schwartz, and John Makhoul. 2013.
Systematic comparison of professional and crowd-
sourced reference translations for machine transla-
tion. In Proceedings of the 2013 Conference of
the North American Chapter of the Association for
Computational Linguistics: Human Language Tech-
nologies, Atlanta, Georgia.
Xin Wayne Zhao, Yanwei Guo, Rui Yan, Yulan He,
and Xiaoming Li. 2013. Timeline generation with
social attention. In Proceedings of the 36th Inter-
national ACM SIGIR Conference on Research and
Development in Information Retrieval, SIGIR ?13,
pages 1061?1064.
1144
The Language Demographics of Amazon Mechanical Turk
Ellie Pavlick1 Matt Post2 Ann Irvine2 Dmitry Kachaev2 Chris Callison-Burch1,2
1Computer and Information Science Department, University of Pennsylvania
2Human Language Technology Center of Excellence, Johns Hopkins University
Abstract
We present a large scale study of the languages
spoken by bilingual workers on Mechanical
Turk (MTurk). We establish a methodology
for determining the language skills of anony-
mous crowd workers that is more robust than
simple surveying. We validate workers? self-
reported language skill claims by measuring
their ability to correctly translate words, and
by geolocating workers to see if they reside in
countries where the languages are likely to be
spoken. Rather than posting a one-off survey,
we posted paid tasks consisting of 1,000 as-
signments to translate a total of 10,000 words
in each of 100 languages. Our study ran
for several months, and was highly visible on
the MTurk crowdsourcing platform, increas-
ing the chances that bilingual workers would
complete it. Our study was useful both to cre-
ate bilingual dictionaries and to act as cen-
sus of the bilingual speakers on MTurk. We
use this data to recommend languages with the
largest speaker populations as good candidates
for other researchers who want to develop
crowdsourced, multilingual technologies. To
further demonstrate the value of creating data
via crowdsourcing, we hire workers to create
bilingual parallel corpora in six Indian lan-
guages, and use them to train statistical ma-
chine translation systems.
1 Overview
Crowdsourcing is a promising new mechanism for
collecting data for natural language processing re-
search. Access to a fast, cheap, and flexible work-
force allows us to collect new types of data, poten-
tially enabling new language technologies. Because
crowdsourcing platforms like Amazon Mechanical
Turk (MTurk) give researchers access to a world-
wide workforce, one obvious application of crowd-
sourcing is the creation of multilingual technologies.
With an increasing number of active crowd workers
located outside of the United States, there is even the
potential to reach fluent speakers of lower resource
languages. In this paper, we investigate the feasi-
bility of hiring language informants on MTurk by
conducting the first large-scale demographic study
of the languages spoken by workers on the platform.
There are several complicating factors when try-
ing to take a census of workers on MTurk. The
workers? identities are anonymized, and Amazon
provides no information about their countries of ori-
gin or their language abilities. Posting a simple sur-
vey to have workers report this information may be
inadequate, since (a) many workers may never see
the survey, (b) many opt not to do one-off surveys
since potential payment is low, and (c) validating the
answers of respondents is not straightforward.
Our study establishes a methodology for deter-
mining the language demographics of anonymous
crowd workers that is more robust than simple sur-
veying. We ask workers what languages they speak
and what country they live in, and validate their
claims by measuring their ability to correctly trans-
late words and by recording their geolocation. To
increase the visibility and the desirability of our
tasks, we post 1,000 assignments in each of 100 lan-
guages. These tasks each consist of translating 10
foreign words into English. Two of the 10 words
have known translations, allowing us to validate that
the workers? translations are accurate. We construct
bilingual dictionaries with up to 10,000 entries, with
the majority of entries being new.
Surveying thousands of workers allows us to ana-
lyze current speaker populations for 100 languages.
79
Transactions of the Association for Computational Linguistics, 2 (2014) 79?92. Action Editor: Mirella Lapata.
Submitted 12/2013; Published 2/2014. c?2014 Association for Computational Linguistics.
11/26/13 turkermap.html
file:///Users/ellie/Documents/Research/turker-demographics/code/src/20130905/paper-rewrite/turkermap.html 1/1
1 1,998
Figure 1: The number of workers per country. This map was generated based on geolocating the IP address
of 4,983 workers in our study. Omitted are 60 workers who were located in more than one country during
the study, and 238 workers who could not be geolocated. The size of the circles represents the number
of workers from each country. The two largest are India (1,998 workers) and the United States (866). To
calibrate the sizes: the Philippines has 142 workers, Egypt has 25, Russia has 10, and Sri Lanka has 4.
The data also allows us to answer questions like:
How quickly is work completed in a given language?
Are crowdsourced translations reliably good? How
often do workers misrepresent their language abili-
ties to obtain financial rewards?
2 Background and Related Work
Amazon?s Mechanical Turk (MTurk) is an on-
line marketplace for work that gives employers
and researchers access to a large, low-cost work-
force. MTurk allows employers to provide micro-
payments in return for workers completing micro-
tasks. The basic units of work on MTurk are called
?Human Intelligence Tasks? (HITs). MTurk was de-
signed to accommodate tasks that are difficult for
computers, but simple for people. This facilitates
research into human computation, where people can
be treated as a function call (von Ahn, 2005; Little et
al., 2009; Quinn and Bederson, 2011). It has appli-
cation to research areas like human-computer inter-
action (Bigham et al., 2010; Bernstein et al., 2010),
computer vision (Sorokin and Forsyth, 2008; Deng
et al., 2010; Rashtchian et al., 2010), speech pro-
cessing (Marge et al., 2010; Lane et al., 2010; Parent
and Eskenazi, 2011; Eskenazi et al., 2013), and natu-
ral language processing (Snow et al., 2008; Callison-
Burch and Dredze, 2010; Laws et al., 2011).
On MTurk, researchers who need work completed
are called ?Requesters?, and workers are often re-
ferred to as ?Turkers?. MTurk is a true market, mean-
ing that Turkers are free to choose to complete the
HITs which interest them, and Requesters can price
their tasks competitively to try to attract workers and
have their tasks done quickly (Faridani et al., 2011;
Singer and Mittal, 2011). Turkers remain anony-
mous to Requesters, and all payment occurs through
Amazon. Requesters are able to accept submitted
work or reject work that does not meet their stan-
dards. Turkers are only paid if a Requester accepts
their work.
Several reports examine Mechanical Turk as an
economic market (Ipeirotis, 2010a; Lehdonvirta and
Ernkvist, 2011). When Amazon introduced MTurk,
it first offered payment only in Amazon credits, and
later offered direct payment in US dollars. More re-
cently, it has expanded to include one foreign cur-
rency, the Indian rupee. Despite its payments be-
ing limited to two currencies or Amazon credits,
MTurk claims over half a million workers from 190
countries (Amazon, 2013). This suggests that its
worker population should represent a diverse set of
languages.
80
A demographic study by Ipeirotis (2010b) fo-
cused on age, gender, martial status, income lev-
els, motivation for working on MTurk, and whether
workers used it as a primary or supplemental form
of income. The study contrasted Indian and US
workers. Ross et al. (2010) completed a longitudi-
nal follow-on study. A number of other studies have
informally investigated Turkers? language abilities.
Munro and Tily (2011) compiled survey responses
of 2,000 Turkers, revealing that four of the six most
represented languages come from India (the top six
being Hindi, Malayalam, Tamil, Spanish, French,
and Telugu). Irvine and Klementiev (2010) had
Turkers evaluate the accuracy of translations that
had been automatically inducted from monolingual
texts. They examined translations of 100 words in
42 low-resource languages, and reported geolocated
countries for their workers (India, the US, Romania,
Pakistan, Macedonia, Latvia, Bangladesh and the
Philippines). Irvine and Klementiev discussed the
difficulty of quality control and assessing the plausi-
bility of workers? language skills for rare languages,
which we address in this paper.
Several researchers have investigated using
MTurk to build bilingual parallel corpora for ma-
chine translation, a task which stands to benefit
low cost, high volume translation on demand (Ger-
mann, 2001). Ambati et al. (2010) conducted a pilot
study by posting 25 sentences to MTurk for Span-
ish, Chinese, Hindi, Telugu, Urdu, and Haitian Cre-
ole. In a study of 2000 Urdu sentences, Zaidan
and Callison-Burch (2011) presented methods for
achieving professional-level translation quality from
Turkers by soliciting multiple English translations
of each foreign sentence. Zbib et al. (2012) used
crowdsourcing to construct a 1.5 million word par-
allel corpus of dialect Arabic and English, train-
ing a statistical machine translation system that pro-
duced higher quality translations of dialect Arabic
than a system a trained on 100 times more Mod-
ern Standard Arabic-English parallel data. Zbib et
al. (2013) conducted a systematic study that showed
that training an MT system on crowdsourced trans-
lations resulted in the same performance as training
on professional translations, at 15 the cost. Hu etal. (2010; Hu et al. (2011) performed crowdsourced
translation by having monolingual speakers collab-
orate and iteratively improve MT output.
English 689 Tamil 253 Malayalam 219
Hindi 149 Spanish 131 Telugu 87
Chinese 86 Romanian 85 Portuguese 82
Arabic 74 Kannada 72 German 66
French 63 Polish 61 Urdu 56
Tagalog 54 Marathi 48 Russian 44
Italian 43 Bengali 41 Gujarati 39
Hebrew 38 Dutch 37 Turkish 35
Vietnamese 34 Macedonian 31 Cebuano 29
Swedish 26 Bulgarian 25 Swahili 23
Hungarian 23 Catalan 22 Thai 22
Lithuanian 21 Punjabi 21 Others ? 20
Table 1: Self-reported native language of 3,216
bilingual Turkers. Not shown are 49 languages with
?20 speakers. We omit 1,801 Turkers who did not
report their native language, 243 who reported 2 na-
tive languages, and 83 with ?3 native languages.
Several researchers have examined cost optimiza-
tion using active learning techniques to select the
most useful sentences or fragments to translate (Am-
bati and Vogel, 2010; Bloodgood and Callison-
Burch, 2010; Ambati, 2012).
To contrast our research with previous work, the
main contributions of this paper are: (1) a robust
methodology for assessing the bilingual skills of
anonymous workers, (2) the largest-scale census to
date of language skills of workers on MTurk, and (3)
a detailed analysis of the data gathered in our study.
3 Experimental Design
The central task in this study was to investigate Me-
chanical Turk?s bilingual population. We accom-
plished this through self-reported surveys combined
with a HIT to translate individual words for 100
languages. We evaluate the accuracy of the work-
ers? translations against known translations. In cases
where these were not exact matches, we used a sec-
ond pass monolingual HIT, which asked English
speakers to evaluate if a worker-provided translation
was a synonym of the known translation.
Demographic questionnaire At the start of each
HIT, Turkers were asked to complete a brief survey
about their language abilities. The survey asked the
following questions:
? Is [language] your native language?
? How many years have you spoken [language]?
81
? Is English your native language?
? How many years have you spoken English?
? What country do you live in?
We automatically collected each worker?s current lo-
cation by geolocating their IP address. A total of
5,281 unique workers completed our HITs. Of these,
3,625 provided answers to our survey questions, and
we were able to geolocate 5,043. Figure 1 plots
the location of workers across 106 countries. Table
1 gives the most common self-reported native lan-
guages.
Selection of languages We drew our data from the
different language versions of Wikipedia. We se-
lected the 100 languages with the largest number of
articles 1 (Table 2). For each language, we chose
the 1,000 most viewed articles over a 1 year period,2
and extracted the 10,000 most frequent words from
them. The resulting vocabularies served as the input
to our translation HIT.
Translation HIT For the translation task, we
asked Turkers to translate individual words. We
showed each word in the context of three sentences
that were drawn from Wikipedia. Turkers were al-
lowed to mark that they were unable to translate a
word. Each task contained 10 words, 8 of which
were words with unknown translations, and 2 of
which were quality control words with known trans-
lations. We gave special instruction for translat-
ing names of people and places, giving examples
of how to handle ?Barack Obama? and ?Australia?
using their interlanguage links. For languages with
non-Latin alphabets, names were transliterated.
The task paid $0.15 for the translation of 10
words. Each set of 10 words was independently
translated by three separate workers. 5,281 workers
completed 256,604 translation assignments, totaling
more than 3 million words, over a period of three
and a half months.
Gold standard translations A set of gold stan-
dard translations were automatically harvested from
1http://meta.wikimedia.org/wiki/List_of_
Wikipedias
2http://dumps.wikimedia.org/other/
pagecounts-raw/
500K+ ARTICLES: German (de), English (en), Spanish (es), French
(fr), Italian (it), Japanese (ja), Dutch (nl), Polish (pl), Portuguese
(pt), Russian (ru)
100K-500K ARTICLES: Arabic (ar), Bulgarian (bg), Catalan (ca),
Czech (cs), Danish (da), Esperanto (eo), Basque (eu), Persian (fa),
Finnish (fi), Hebrew (he), Hindi (hi), Croatian (hr), Hungarian (hu),
Indonesian (id), Korean (ko), Lithuanian (lt), Malay (ms), Norwe-
gian (Bokmal) (no), Romanian (ro), Slovak (sk), Slovenian (sl), Ser-
bian (sr), Swedish (sv), Turkish (tr), UKrainian (UK), Vietnamese
(vi), Waray-Waray (war), Chinese (zh)
10K-100K ARTICLES: Afrikaans (af) Amharic (am) Asturian (ast)
Azerbaijani (az) Belarusian (be) Bengali (bn) Bishnupriya Manipuri
(bpy) Breton (br) Bosnian (bs) Cebuano (ceb) Welsh (cy) Zazaki
(diq) Greek (el) West Frisian (fy) Irish (ga) Galician (gl) Gujarati
(gu) Haitian (ht) Armenian (hy) Icelandic (is) Javanese (jv) Geor-
gian (ka) Kannada (kn) Kurdish (ku) Luxembourgish (lb) Latvian
(lv) Malagasy (mg) Macedonian (mk) Malayalam (ml) Marathi
(mr) Neapolitan (nap) Low Saxon (nds) Nepali (ne) Newar / Nepal
Bhasa (new) Norwegian (Nynorsk) (nn) Piedmontese (pms) Sicil-
ian (scn) Serbo-Croatian (sh) Albanian (sq) Sundanese (su) Swahili
(sw) Tamil (ta) Telugu (te) Thai (th) Tagalog (tl) Urdu (ur) Yoruba
(yo)
<10K ARTICLES: Central Bicolano (bcl) Tibetan (bo) Ilokano (ilo)
Punjabi (pa) Kapampangan (pam) Pashto (ps) Sindhi (sd) Somali
(so) Uzbek (uz) Wolof (wo)
Table 2: A list of the languages that were used in our
study, grouped by the number of Wikipedia articles
in the language. Each language?s code is given in
parentheses. These language codes are used in other
figures throughout this paper.
Wikipedia for every language to use as embedded
controls. We used Wikipedia?s inter-language links
to pair titles of English articles with their corre-
sponding foreign article?s title. To get a more trans-
latable set of pairs, we excluded any pairs where: (1)
the English word was not present in the WordNet
ontology (Miller, 1995), (2) either article title was
longer than a single word, (3) the English Wikipedia
page was a subcategory of person or place, or (4)
the English and the foreign titles were identical or a
substring of the other.
Manual evaluation of non-identical translations
We counted all translations that exactly matched
the gold standard translation as correct. For non-
exact matches we created a second-pass quality as-
surance HIT. Turkers were shown a pair of En-
glish words, one of which was a Turker?s transla-
tion of the foreign word used for quality control,
and the other of which was the gold-standard trans-
lation of the foreign word. Evaluators were asked
whether the two words had the same meaning, and
chose between three answers: ?Yes?, ?No?, or ?Re-
82
Figure 2: Days to complete the translation HITs for
40 of the languages. Tick marks represent the com-
pletion of individual assignments.
lated but not synonymous.? Examples of mean-
ing equivalent pairs include: <petroglyphs, rock
paintings>, <demo, show> and <loam, loam: soil
rich in decaying matter>. Non-meaning equiva-
lents included: <assorted, minutes>, and <major,
URL of image>. Related items were things like
<sky, clouds>. Misspellings like <lactation, lac-
tiation > were judged to have same meaning, and
were marked as misspelled. Three separate Turkers
judged each pair, allowing majority votes for diffi-
cult cases.
We checked Turkers who were working on this
task by embedding pairs of words which were ei-
?? ?$ %??? ( ?? + ?$ ??? %?.? ?? ?? ???? 5 ?? ?????9 ???:? ?? ??<? 
In retribution pakistan also did six nuclear tests on 28 may 1998.
On 28 May Pakistan also conducted six nuclear tests as an act
of redressal.
Retaliating on this ?Pakistan? conducted Six(6) Nuclear Tests
on 28 May, 1998.
pakistan also did 6 nuclear test in retribution on 28 may, 1998
Figure 3: An example of the Turkers? translations of
a Hindi sentence. The translations are unedited and
contain fixable spelling, capitalization and grammat-
ical errors.
ther known to be synonyms (drawn from Word-
Net) or unrelated (randomly chosen from a corpus).
Automating approval/rejections for the second-pass
evaluation allowed the whole pipeline to be run au-
tomatically. Caching judgments meant that we ulti-
mately needed only 20,952 synonym tasks to judge
all of the submitted translations (a total of 74,572
non-matching word pairs). These were completed
by an additional 1,005 workers. Each of these as-
signments included 10 word pairs and paid $0.10.
Full sentence translations To demonstrate the
feasibility of using crowdsourcing to create multi-
lingual technologies, we hire Turkers to construct
bilingual parallel corpora from scratch for six In-
dian languages. Germann (2001) attempted to build
a Tamil-English translation system from scratch by
hiring professional translators, but found the cost
prohibitive. We created parallel corpora by trans-
lating the 100 most viewed Wikipedia pages in Ben-
gali, Malyalam, Hindi, Tamil, Telugu, and Urdu into
English. We collected four translations from differ-
ent Turkers for each source sentence.
Workers were paid $0.70 per HIT to translate
10 sentences. We accepted or rejected translations
based on a manual review of each worker?s submis-
sions, which included a comparison of the transla-
tions to a monotonic gloss (produced with a dic-
tionary), and metadata such as the amount of time
the worker took to complete the HIT and their geo-
graphic location.
Figure 3 shows an example of the translations we
obtained. The lack of a professionally translated
reference sentences prevented us from doing a sys-
tematic comparison between the quality of profes-
83
pt bs sh tl it sr ro es ms de af te hr id da nl tr gu sk fi he ml fr ja pa bg mk no gl ht ga sv cy lv hu kn az be lt ko ne eo ar pl mr ca cs sw ta hi bn nn ka so zh jv el ceb vi bcl is su uz lb bpy scn new ur sd br ps ru am wo bo
0.0
0.2
0.4
0.6
0.8
1.0
Figure 4: Translation quality for languages with at least 50 Turkers. The dark blue bars indicate the pro-
portion of translations which exactly matched gold standard translations, and light blue indicate translations
which were judged to be correct synonyms. Error bars show the 95% confidence intervals for each language.
sion and non-professional translations as Zaidan and
Callison-Burch (2011) did. Instead we evaluate the
quality of the data by using it to train SMT systems.
We present results in section 5.
4 Measuring Translation Quality
For single word translations, we calculate the qual-
ity of translations on the level of individual assign-
ments and aggregated over workers and languages.
We define an assignment?s quality as the proportion
of controls that are correct in a given assignment,
where correct means exactly correct or judged to be
synonymous.
Quality(ai) = 1ki
ki?
j=1
?(trij ? syns[gj]) (1)
where ai is the ith assignment, ki is the number of
controls in ai, trij is the Turker?s provided transla-
tion of control word j in assignment i, gj is the gold
standard translation of control word j, syns[gj] is
the set of words judged to be synonymous with gj
and includes gj , and ?(x) is Kronecker?s delta and
takes value 1 when x is true. Most assignments had
two known words embedded, so most assignments
had scores of either 0, 0.5, or 1.
Since computing overall quality for a language as
the average assignment quality score is biased to-
wards a small number of highly active Turkers, we
instead report language quality scores as the aver-
age per-Turker quality, where a Turker?s quality is
the average quality of all the assignments that she
completed:
Quality(ti) =
?
aj?assigns[i] Quality(aj)
| assigns[i] | (2)
where assigns[i] is the assignments completed
by Turker i, and Quality(a) is as above.
Quality for a language is then given by
Quality(li) =
?
tj?turkers[i] Quality(tj)
| turkers[i] | (3)
When a Turker completed assignments in more than
one language, their quality was computed separately
for each language. Figure 4 shows the transla-
tion quality for languages with contributions from
at least 50 workers.
Cheating using machine translation One obvi-
ous way for workers to cheat is to use available
online translation tools. Although we followed
best practices to deter copying-and-pasting into on-
line MT systems by rendering words and sentences
84
as images (Zaidan and Callison-Burch, 2011), this
strategy does not prevent workers from typing the
words into an MT system if they are able to type in
the language?s script.
To identify and remove workers who appeared to
be cheating by using Google Translate, we calcu-
lated each worker?s overlap with the Google transla-
tions. We used Google to translate all 10,000 words
for the 51 foreign languages that Google Trans-
late covered at the time of the study. We mea-
sured the percent of workers? translations that ex-
actly matched the translation returned from Google.
Figure 5a shows overlap between Turkers?s trans-
lations and Google Translate. When overlap is high,
it seems likely that those Turkers are cheating. It is
also reasonable to assume that honest workers will
overlap with Google some amount of the time as
Google?s translations are usually accurate. We di-
vide the workers into three groups: those with very
high overlap with Google (likely cheating by using
Google to translate words), those with reasonable
overlap, and those with no overlap (likely cheating
by other means, for instance, by submitting random
text).
Our gold-standard controls are designed to iden-
tify workers that fall into the third group (those who
are spamming or providing useless translations), but
they will not effectively flag workers who are cheat-
ing with Google Translate. We therefore remove the
500 Turkers with the highest overlap with Google.
This equates to removing all workers with greater
than 70% overlap. Figure 5b shows that removing
workers at or above the 70% threshold retains 90%
of the collected translations and over 90% of the
workers.
Quality scores reported throughout the paper re-
flect only translations from Turkers whose overlap
with Google falls below this 70% threshold.
5 Data Analysis
We performed an analysis of our data to address the
following questions:
? Do workers accurately represent their language
abilities? Should we constrain tasks by region?
? How quickly can we expect work to be com-
pleted in a particular language?
(a) Individual workers? overlap with Google Translate.
We removed the 500 workers with the highest overlap
(shaded region on the left) from our analyses, as it is rea-
sonable to assume these workers are cheating by submit-
ting translations from Google. Workers with no overlap
(shaded region on the right) are also likely to be cheating,
e.g. by submitting random text.
(b) Cumulative distribution of overlap with Google trans-
late for workers and translations. We see that eliminating
all workers with >70% overlap with google translate still
preserves 90% of translations and >90% of workers.
Figure 5
? Can Turkers? translations be used to train MT
systems?
? Do our dictionaries improve MT quality?
Language skills and location We measured the
average quality of workers who were in countries
that plausibly speak a language, versus workers from
countries that did not have large speaker populations
of that language. We used the Ethnologue (Lewis
85
Avg. Turker quality (# Ts) Primary locations Primary locations
In region Out of region of Turkers in region of Turkers out of region
Hindi 0.63 (296) 0.69 (7) India (284) UAE (5) UK (3) Saudi Arabia (2) Russia (1) Oman (1)
Tamil 0.65 (273) ** 0.25 (2) India (266) US (3) Canada (2) Tunisia (1) Egypt (1)
Malayalam 0.76 (234) 0.83 (2) India (223) UAE (6) US (3) Saudi Arabia (1) Maldives (1)
Spanish 0.81 (191) 0.84 (18) US (122) Mexico (16) Spain (14) India (15) New Zealand (1) Brazil (1)
French 0.75 (170) 0.82 (11) India (62) US (45) France (23) Greece (2) Netherlands (1) Japan (1)
Chinese 0.60 (116) 0.55 (21) US (75) Singapore (13) China (9) Hong Kong (6) Australia (3) Germany (2)
German 0.82 (91) 0.77 (41) Germany (48) US (25) Austria (7) India (34) Netherlands (1) Greece (1)
Italian 0.86 (90) * 0.80 (42) Italy (42) US (29) Romania (7) India (33) Ireland (2) Spain (2)
Amharic 0.14 (16) ** 0.01 (99) US (14) Ethiopia (2) India (70) Georgia (9) Macedonia (5)
Kannada 0.70 (105) NA (0) India (105)
Arabic 0.74 (60) ** 0.60 (45) Egypt (19) Jordan (16) Morocco (9) US (19) India (11) Canada (3)
Sindhi 0.19 (96) 0.06 (9) India (58) Pakistan (37) US (1) Macedonia (4) Georgia (2) Indonesia (2)
Portuguese 0.87 (101) 0.96 (3) Brazil (44) Portugal (31) US (15) Romania (1) Japan (1) Israel (1)
Turkish 0.76 (76) 0.80 (27) Turkey (38) US (18) Macedonia (8) India (19) Pakistan (4) Taiwan (1)
Telugu 0.80 (102) 0.50 (1) India (98) US (3) UAE (1) Saudi Arabia (1)
Irish 0.74 (54) 0.71 (47) US (39) Ireland (13) UK (2) India (36) Romania (5) Macedonia (2)
Swedish 0.73 (54) 0.71 (45) US (25) Sweden (22) Finland (3) India (23) Macedonia (6) Croatia (2)
Czech 0.71 (45) * 0.61 (50) US (17) Czech Republic (14) Serbia (5) Macedonia (22) India (10) UK (5)
Russian 0.15 (67) * 0.12 (27) US (36) Moldova (7) Russia (6) India (14) Macedonia (4) UK (3)
Breton 0.17 (3) 0.18 (89) US (3) India (83) Macedonia (2) China (1)
Table 3: Translation quality when partitioning the translations into two groups, one containing translations
submitted by Turkers whose location is within regions that plausibly speak the foreign language, and the
other containing translations from Turkers outside those regions. In general, in-region Turkers provide
higher quality translations. (**) indicates differences significant at p=0.05, (*) at p=0.10.
et al., 2013) to compile the list of countries where
each language is spoken. Table 3 compares the av-
erage translation quality of assignments completed
within the region of each language, and compares it
to the quality of assignments completed outside that
region.
Our workers reported speaking 95 languages na-
tively. US workers alone reported 61 native lan-
guages. Overall, 4,297 workers were located in a
region likely to speak the language from which they
were translating, and 2,778 workers were located
in countries considered out of region (meaning that
about a third of our 5,281 Turkers completed HITs
in multiple languages).
Table 3 shows the differences in translation qual-
ity when computed using in-region versus out-of-
region Turkers, for the languages with the greatest
number of workers. Within region workers typi-
cally produced higher quality translations. Given the
number of Indian workers on Mechanical Turk, it
is unsurprising that they represent majority of out-
of-region workers. For the languages that had more
than 75 out of region workers (Malay, Amharic, Ice-
landic, Sicilian, Wolof, and Breton), Indian workers
represented at least 70% of the out of region workers
in each language.
A few languages stand out for having suspiciously
strong performance by out of region workers, no-
tably Irish and Swedish, for which out of region
workers account for a near equivalent volume and
quality of translations to the in region workers. This
is admittedly implausible, considering the relatively
small number of Irish speakers worldwide, and the
very low number living in the countries in which our
Turkers were based (primarily India). Such results
highlight the fact that cheating using online transla-
tion resources is a real problem, and despite our best
efforts to remove workers using Google Translate,
some cheating is still evident. Restricting to within
region workers is an effective way to reduce the
prevalence of cheating. We discuss the languages
which are best supported by true native speakers in
section 6.
Speed of translation Figure 2 gives the comple-
tion times for 40 languages. The 10 languages to
finish in the shortest amount of time were: Tamil,
Malayalam, Telugu, Hindi, Macedonian, Spanish,
Serbian, Romanian, Gujarati, and Marathi. Seven of
the ten fastest languages are from India, which is un-
86
320 2 4 6 8 10 12 14 16 18 20 22 24 26 28 30
800,000
0
100,000
200,000
300,000
400,000
500,000
600,000
700,000
Malay
alam
Tamil
Telugu
Hindi
Urdu
Bengali
Figure 6: The total volume of translations (measured
in English words) as a function of elapsed days.
sentence English + dictionary
language pairs foreign words entries
Bengali 22k 732k 22k
Hindi 40k 1,488k 22k
Malayalam 32k 863k 23k
Tamil 38k 916k 25k
Telugu 46k 1,097k 21k
Urdu 35k 1,356k 20k
Table 4: Size of parallel corpora and bilingual dic-
tionaries collected for each language.
surprising given the geographic distribution of work-
ers. Some languages follow the pattern of having a
smattering of assignments completed early, with the
rate picking up later.
Figure 6 gives the throughput of the full-sentence
translation task for the six Indian languages. The
fastest language was Malayalam, for which we col-
lected half a million words of translations in just un-
der a week. Table 4 gives the size of the data set that
we created for each of these languages.
Training SMT systems We trained statistical
translation models from the parallel corpora that we
created for the six Indian languages using the Joshua
machine translation system (Post et al., 2012). Table
5 shows the translation performance when trained
on the bitexts alone, and when incorporating the
bilingual dictionaries created in our earlier HIT. The
scores reflect the performance when tested on held
out sentences from the training data. Adding the dic-
trained on bitext + BLEU
language bitexts alone dictionaries ?
Bengali 12.03 17.29 5.26
Hindi 16.19 18.10 1.91
Malayalam 6.65 9.72 3.07
Tamil 8.08 9.66 1.58
Telugu 11.94 13.70 1.76
Urdu 19.22 21.98 2.76
Table 5: BLEU scores for translating into English
using bilingual parallel corpora by themselves, and
with the addition of single-word dictionaries. Scores
are calculated using four reference translations and
represent the mean of three MERT runs.
tionaries to the training set produces consistent per-
formance gains, ranging from 1 to 5 BLEU points.
This represents a substantial improvement. It is
worth noting, however, that while the source doc-
uments for the full sentences used for testing were
kept disjoint from those used for training, there is
overlap between the source materials for the dictio-
naries and those from the test set, since both the dic-
tionaries and the bitext source sentences were drawn
from Wikipedia.
6 Discussion
Crowdsourcing platforms like Mechanical Turk give
researchers instant access to a diverse set of bilin-
gual workers. This opens up exciting new avenues
for researchers to develop new multilingual systems.
The demographics reported in this study are likely to
shift over time. Amazon may expand its payments to
new currencies. Posting long-running HITs in other
languages may recruit more speakers of those lan-
guages. New crowdsourcing platforms may emerge.
The data presented here provides a valuable snap-
shot of the current state of MTurk, and the methods
used can be applied generally in future research.
Based on our study, we can confidently recom-
mend 13 languages as good candidates for research
now: Dutch, French, German, Gujarati, Italian, Kan-
nada, Malayalam, Portuguese, Romanian, Serbian,
Spanish, Tagalog, and Telugu. These languages
have large Turker populations who complete tasks
quickly and accurately. Table 6 summarizes the
strengths and weaknesses of all 100 languages cov-
ered in our study. Several other languages are viable
87
workers quality speed
many high fast Dutch, French, German, Gu-
jarati, Italian, Kannada, Malay-
alam, Portuguese, Romanian,
Serbian, Spanish, Tagalog, Tel-
ugu
slow Arabic, Hebrew, Irish, Punjabi,
Swedish, Turkish
low fast Hindi, Marathi, Tamil, Urdu
or
medium
slow Bengali, Bishnupriya Ma-
nipuri, Cebuano, Chinese,
Nepali, Newar, Polish, Russian,
Sindhi, Tibetan
few high fast Bosnia, Croatian, Macedonian,
Malay, Serbo-Croatian
slow Afrikaans, Albanian,
Aragonese, Asturian, Basque,
Belarusian, Bulgarian, Central
Bicolano, Czech, Danish,
Finnish, Galacian, Greek,
Haitian, Hungarian, Icelandic,
Ilokano, Indonesian, Japanese,
Javanese, Kapampangan,
Kazakh, Korean, Lithuanian,
Low Saxon, Malagasy, Nor-
wegian (Bokmal), Sicilian,
Slovak, Slovenian, Thai, UKra-
nian, Uzbek, Waray-Waray,
West Frisian, Yoruba
low fast ?
or
medium
slow Amharic, Armenian, Azer-
baijani, Breton, Catalan,
Georgian, Latvian, Luxembour-
gish, Neapolitian, Norwegian
(Nynorsk), Pashto, Pied-
montese, Somali, Sudanese,
Swahili, Tatar, Vietnamese,
Walloon, Welsh
none low or
medium
slow Esperanto, Ido, Kurdish, Per-
sian, Quechua, Wolof, Zazaki
Table 6: The green box shows the best languages to
target on MTurk. These languages have many work-
ers who generate high quality results quickly. We
defined many workers as 50 or more active in-region
workers, high quality as?70% accuracy on the gold
standard controls, and fast if all of the 10,000 words
were completed within two weeks.
candidates provided adequate quality control mech-
anisms are used to select good workers.
Since Mechanical Turk provides financial incen-
tives for participation, many workers attempt to
complete tasks even if they do not have the lan-
guage skills necessary to do so. Since MTurk does
not provide any information about workers demo-
graphics, including their language competencies, it
can be hard to exclude such workers. As a result
naive data collection on MTurk may result in noisy
data. A variety of techniques should be incorporated
into crowdsourcing pipelines to ensure high quality
data. As a best practice, we suggest: (1) restricting
workers to countries that plausibly speak the foreign
language of interest, (2) embedding gold standard
controls or administering language pretests, rather
than relying solely on self-reported language skills,
and (3) excluding workers whose translations have
high overlap with online machine translation sys-
tems like Google translate. If cheating using exter-
nal resources is likely, then also consider (4) record-
ing information like time spent on a HIT (cumulative
and on individual items), patterns in keystroke logs,
tab/window focus, etc.
Although our study targeted bilingual workers on
Mechanical Turk, and neglected monolingual work-
ers, we believe our results reliably represent the cur-
rent speaker populations, since the vast majority of
the work available on the crowdsourced platform
is currently English-only. We therefore assume the
number of non-English speakers is small. In the fu-
ture, it may be desirable to recruit monolingual for-
eign workers. In such cases, we recommend other
tests to validate their language abilities in place of
our translation test. These could include perform-
ing narrative cloze, or listening to audio files con-
taining speech in different language and identifying
their language.
7 Data release
With the publication of this paper, we are releasing
all data and code used in this study. Our data release
includes the raw data, along with bilingual dictionar-
ies that are filtered to be high quality. It will include
256,604 translation assignments from 5,281 Turkers
and 20,952 synonym assignments from 1,005 Turk-
ers, along with meta information like geolocation
88
and time submitted, plus external dictionaries used
for validation. The dictionaries will contain 1.5M
total translated words in 100 languages, along with
code to filter the dictionaries based on different cri-
teria. The data also includes parallel corpora for six
Indian languages, ranging in size between 700,000
to 1.5 million words.
8 Acknowledgements
This material is based on research sponsored by
a DARPA Computer Science Study Panel phase 3
award entitled ?Crowdsourcing Translation? (con-
tract D12PC00368). The views and conclusions
contained in this publication are those of the authors
and should not be interpreted as representing offi-
cial policies or endorsements by DARPA or the U.S.
Government. This research was supported by the
Johns Hopkins University Human Language Tech-
nology Center of Excellence and through gifts from
Microsoft and Google.
The authors would like to thank the anonymous
reviewers for their thoughtful comments, which sub-
stantially improved this paper.
References
Amazon. 2013. Service summary tour for re-
questers on Amazon Mechanical Turk. https://
requester.mturk.com/tour.
Vamshi Ambati and Stephan Vogel. 2010. Can crowds
build parallel corpora for machine translation systems?
In Proceedings of the NAACL HLT 2010 Workshop on
Creating Speech and Language Data with Amazon?s
Mechanical Turk. Association for Computational Lin-
guistics.
Vamshi Ambati, Stephan Vogel, and Jaime Carbonell.
2010. Active learning and crowd-sourcing for ma-
chine translation. In Proceedings of the 7th Interna-
tional Conference on Language Resources and Evalu-
ation (LREC).
Vamshi Ambati. 2012. Active Learning and Crowd-
sourcing for Machine Translation in Low Resource
Scenarios. Ph.D. thesis, Language Technologies In-
stitute, School of Computer Science, Carnegie Mellon
University, Pittsburgh, PA.
Michael S. Bernstein, Greg Little, Robert C. Miller,
Bjrn Hartmann, Mark S. Ackerman, David R. Karger,
David Crowell, and Katrina Panovich. 2010. Soylent:
a word processor with a crowd inside. In Proceed-
ings of the ACM Symposium on User Interface Soft-
ware and Technology (UIST).
Jeffrey P. Bigham, Chandrika Jayant, Hanjie Ji, Greg Lit-
tle, Andrew Miller, Robert C. Miller, Robin Miller,
Aubrey Tatarowicz, Brandyn White, Samual White,
and Tom Yeh. 2010. VizWiz: nearly real-time an-
swers to visual questions. In Proceedings of the ACM
Symposium on User Interface Software and Technol-
ogy (UIST).
Michael Bloodgood and Chris Callison-Burch. 2010.
Large-scale cost-focused active learning for statisti-
cal machine translation. In Proceedings of the 48th
Annual Meeting of the Association for Computational
Linguistics.
Chris Callison-Burch and Mark Dredze. 2010. Creating
speech and language data with Amazon?s Mechanical
Turk. In Proceedings of the NAACL HLT 2010 Work-
shop on Creating Speech and Language Data with
Amazon?s Mechanical Turk, pages 1?12, Los Angeles,
June. Association for Computational Linguistics.
Jia Deng, Alexander Berg, Kai Li, and Li Fei-Fei. 2010.
What does classifying more than 10,000 image cate-
gories tell us? In Proceedings of the 12th European
Conference of Computer Vision (ECCV, pages 71?84.
Maxine Eskenazi, Gina-Anne Levow, Helen Meng,
Gabriel Parent, and David Suendermann. 2013.
Crowdsourcing for Speech Processing, Applications to
Data Collection, Transcription and Assessment. Wi-
ley.
Siamak Faridani, Bjo?rn Hartmann, and Panagiotis G.
Ipeirotis. 2011. What?s the right price? pricing tasks
for finishing on time. In Third AAAI Human Compu-
tation Workshop (HCOMP?11).
Ulrich Germann. 2001. Building a statistical machine
translation system from scratch: How much bang for
the buck can we expect? In ACL 2001 Workshop on
Data-Driven Machine Translation, Toulouse, France.
Chang Hu, Benjamin B. Bederson, and Philip Resnik.
2010. Translation by iterative collaboration between
monolingual users. In Proceedings of ACM SIGKDD
Workshop on Human Computation (HCOMP).
Chang Hu, Philip Resnik, Yakov Kronrod, Vladimir Ei-
delman, Olivia Buzek, and Benjamin B. Bederson.
2011. The value of monolingual crowdsourcing in
a real-world translation scenario: Simulation using
haitian creole emergency sms messages. In Pro-
ceedings of the Sixth Workshop on Statistical Ma-
chine Translation, pages 399?404, Edinburgh, Scot-
land, July. Association for Computational Linguistics.
Panagiotis G. Ipeirotis. 2010a. Analyzing the mechani-
cal turk marketplace. In ACM XRDS, December.
Panagiotis G. Ipeirotis. 2010b. Demographics of
Mechanical Turk. Technical Report Working paper
89
CeDER-10-01, New York University, Stern School of
Business.
Ann Irvine and Alexandre Klementiev. 2010. Using Me-
chanical Turk to annotate lexicons for less commonly
used languages. In Workshop on Creating Speech and
Language Data with MTurk.
Ian Lane, Matthias Eck, Kay Rottmann, and Alex
Waibel. 2010. Tools for collecting speech corpora
via mechanical-turk. In Proceedings of the NAACL
HLT 2010 Workshop on Creating Speech and Lan-
guage Data with Amazon?s Mechanical Turk, Los An-
geles.
Florian Laws, Christian Scheible, and Hinrich Schu?tze.
2011. Active learning with amazon mechanical turk.
In Proceedings of the 2011 Conference on Empirical
Methods in Natural Language Processing, Edinburgh,
Scotland.
Matthew Lease, Jessica Hullman, Jeffrey P. Bigham,
Juho Kim Michael S. Bernstein and, Walter Lasecki,
Saeideh Bakhshi, Tanushree Mitra, and Robert C.
Miller. 2013. Mechanical Turk is not anony-
mous. http://dx.doi.org/10.2139/ssrn.
2228728.
Vili Lehdonvirta and Mirko Ernkvist. 2011. Knowl-
edge map of the virtual economy: Converting
the virtual economy into development potential.
http://www.infodev.org/en/Document.
1056.pdf, April. An InfoDev Publication.
M. Paul Lewis, Gary F. Simons, and Charles D. Fennig
(eds.). 2013. Ethnologue: Languages of the world,
seventeenth edition. http://www.ethnologue.
com.
Greg Little, Lydia B. Chilton, Rob Miller, and Max Gold-
man. 2009. Turkit: Tools for iterative tasks on me-
chanical turk. In Proceedings of the Workshop on
Human Computation at the International Conference
on Knowledge Discovery and Data Mining (KDD-
HCOMP ?09), Paris.
Matthew Marge, Satanjeev Banerjee, and Alexander
Rudnicky. 2010. Using the Amazon Mechanical Turk
to transcribe and annotate meeting speech for extrac-
tive summarization. In Workshop on Creating Speech
and Language Data with MTurk.
George A. Miller. 1995. WordNet: a lexical database for
english. Communications of the ACM, 38(11):39?41.
Robert Munro and Hal Tily. 2011. The start of the
art: Introduction to the workshop on crowdsourcing
technologies for language and cognition studies. In
Crowdsourcing Technologies for Language and Cog-
nition Studies, Boulder.
Scott Novotney and Chris Callison-Burch. 2010. Cheap,
fast and good enough: Automatic speech recognition
with non-expert transcription. In Human Language
Technologies: The 2010 Annual Conference of the
North American Chapter of the Association for Com-
putational Linguistics, pages 207?215. Association for
Computational Linguistics.
Gabriel Parent and Maxine Eskenazi. 2011. Speaking
to the crowd: looking at past achievements in using
crowdsourcing for speech and predicting future chal-
lenges. In Proceedings Interspeech 2011, Special Ses-
sion on Crowdsourcing.
Matt Post, Chris Callison-Burch, and Miles Osborne.
2012. Constructing parallel corpora for six indian
languages via crowdsourcing. In Proceedings of the
Seventh Workshop on Statistical Machine Translation,
pages 401?409, Montre?al, Canada, June. Association
for Computational Linguistics.
Alexander J. Quinn and Benjamin B. Bederson. 2011.
Human computation: A survey and taxonomy of a
growing field. In Computer Human Interaction (CHI).
Cyrus Rashtchian, Peter Young, Micah Hodosh, and Ju-
lia Hockenmaier. 2010. Collecting image annotations
using Amazon?s Mechanical Turk. In Workshop on
Creating Speech and Language Data with MTurk.
Joel Ross, Lilly Irani, M. Six Silberman, Andrew Zal-
divar, and Bill Tomlinson. 2010. Who are the crowd-
workers?: Shifting demographics in Amazon Mechan-
ical Turk. In alt.CHI session of CHI 2010 extended
abstracts on human factors in computing systems, At-
lanta, Georgia.
Yaron Singer and Manas Mittal. 2011. Pricing mecha-
nisms for online labor markets. In Third AAAI Human
Computation Workshop (HCOMP?11).
Rion Snow, Brendan O?Connor, Daniel Jurafsky, and
Andrew Y. Ng. 2008. Cheap and fast - but is it
good? Evaluating non-expert annotations for natural
language tasks. In Proceedings of EMNLP.
Alexander Sorokin and David Forsyth. 2008. Utility
data annotation with amazon mechanical turk. In First
IEEE Workshop on Internet Vision at CVPR.
Luis von Ahn. 2005. Human Computation. Ph.D. thesis,
School of Computer Science, Carnegie Mellon Uni-
versity, Pittsburgh, PA.
Omar F. Zaidan and Chris Callison-Burch. 2011. Crowd-
sourcing translation: Professional quality from non-
professionals. In Proceedings of the 49th Annual
Meeting of the Association for Computational Lin-
guistics: Human Language Technologies, pages 1220?
1229. Association for Computational Linguistics.
Rabih Zbib, Erika Malchiodi, Jacob Devlin, David
Stallard, Spyros Matsoukas, Richard Schwartz, John
Makhoul, Omar F. Zaidan, and Chris Callison-Burch.
2012. Machine translation of Arabic dialects. In The
2012 Conference of the North American Chapter of
the Association for Computational Linguistics. Asso-
ciation for Computational Linguistics.
90
Rabih Zbib, Gretchen Markiewicz, Spyros Matsoukas,
Richard Schwartz, and John Makhoul. 2013. Sys-
tematic comparison of professional and crowdsourced
reference translations for machine translation. In Pro-
ceedings of the 2013 Conference of the North Amer-
ican Chapter of the Association for Computational
Linguistics: Human Language Technologies, Atlanta,
Georgia.
91
92
