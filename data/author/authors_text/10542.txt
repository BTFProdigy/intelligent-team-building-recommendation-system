Coling 2008: Companion volume ? Posters and Demonstrations, pages 39?42
Manchester, August 2008
The Impact of Reference Quality on Automatic MT Evaluation 
Olivier Hamon1,2 and Djamel Mostefa1 
(1) Evaluation and Language Resources Distribution Agency (ELDA) 
55-57 rue Brillat-Savarin, 75013 Paris, France 
(2) LIPN (UMR 7030) ? Universit? Paris 13 & CNRS 
99 av. J.-B. Cl?ment, 93430 Villetaneuse, France 
{hamon,mostefa}@elda.org 
 
Abstract 
Language resource quality is crucial in 
NLP. Many of the resources used are de-
rived from data created by human beings 
out of an NLP context, especially regard-
ing MT and reference translations. In-
deed, automatic evaluations need high-
quality data that allow the comparison of 
both automatic and human translations. 
The validation of these resources is 
widely recommended before being used. 
This paper describes the impact of using 
different-quality references on evalua-
tion. Surprisingly enough, similar scores 
are obtained in many cases regardless of 
the quality. Thus, the limitations of the 
automatic metrics used within MT are 
also discussed in this regard.  
1 Introduction 
Language resources (LRs) are essential compo-
nents in research and development of NLP sys-
tems. However, the production of most LRs is 
done by human beings and is therefore subject to 
errors or imperfections. The creation of LRs re-
quires a quality assurance procedure that helps 
control their quality and make sure that they 
comply with the specifications. 
The importance of validation criteria is even 
higher when it comes to evaluation, as reference 
LRs are used to measure system performance and, 
thus, quality. An evaluation must be done in a 
suitable qualitative framework and data used 
should be as good-quality as possible. Bearing 
                                                 
 
 ? 2008. Licensed under the Creative Commons At-
tribution-Noncommercial-Share Alike 3.0 Unported 
license (http://creativecommons.org/licenses/by-nc-
sa/3.0/). Some rights reserved. 
that in mind, validation standards have been de-
fined (Van den Heuvel et al, 2003) and re-
sources should follow the specifications as close 
as possible for that purpose. 
The problem also applies to reference transla-
tions in MT. Most of the automatic metrics used 
compare human reference translations to the out-
put obtained from MT systems. Generally, more 
than one reference is used to get multiple transla-
tion possibilities (Papineni et al, 2001), but the 
evaluation of sentences depends highly on the 
human reference(s) translation(s) used. However, 
only a few studies have gone deeper into a defi-
nition of quality and have tried to detail how to 
evaluate it (Van den Heuvel & Sanders, 2006). 
N-gram metrics give scores that strongly depend 
on the reference and, thus, we wonder how much 
scores computed with a poor reference transla-
tion diverge from the ones computed with a high 
quality reference translation. This paper focuses 
on two issues: 1) how to validate the quality of a 
human translation; 2) study of the impact of the 
quality of reference translations on MT evalua-
tions. The final objective behind this work is to 
find out to what an extent a validation is useful 
within the evaluation protocol. The building of 
reference translations is very time and money 
consuming, but the cost of validation should not 
be underestimated either (Fers?e et al, 2006). 
2 Context 
In our experiments, we used the material from 
the TC-STAR 1  second evaluation campaign 
(Mostefa et al, 2006) and the third one (Mostefa 
et al, 2007). For both campaigns, three language 
directions were used: English-to-Spanish (EnEs), 
Spanish-to-English (EsEn) and Chinese-to-
English (ZhEn). Data came from European Par-
liament Plenary Sessions (EPPS) for EnES and 
                                                 
1
 http://www.tc-star.org 
39
EsEn, Spanish Parliament Sessions (Cortes) for 
EsEn, and Voice of America (VoA) for ZhEn. 
Three kinds of input were considered: automatic 
transcriptions from Automatic Speech Recogni-
tion (ASR) systems, manual transcriptions (Ver-
batim) and Final Text Editions (FTE) provided 
by the European Parliament. This represents 14 
sets consisting of source documents, reference 
translations translated twice by two different 
agencies, and translations obtained from MT sys-
tems. Each set contains around 25,000 words. 
Therefore, we had an overall set of 28 reference 
translations on the evaluations, directions and 
inputs from both years. During the campaigns, 
MT systems have been evaluated with automatic 
metrics such as BLEU (Papineni et al, 2001). 
3 Validation 
3.1 Guidelines 
The quality of reference translations is consid-
ered in two ways. First, translation guidelines are 
given to the translators. Then, translated files are 
sent to a validation agency in order to check their 
quality according to the defined criteria. 
Guidelines were produced within the TC-
STAR project. They were discussed internally 
but also with the Linguistic Data Consortium 
(LDC) who has had the experience of producing 
many reference translations. 
Translation agencies are informed about the 
quality control and extra attention is paid to the 
recommendations given for translation quality: 
meaning and style should remain as close to the 
original source documents as possible; no addi-
tional annotations should be added to the transla-
tion; capitalization has to be carefully respected; 
the translation of neologisms and unknown 
words should take into account the speaker?s in-
tention; date format should also follow the estab-
lished conventions, etc. 
3.2 Criteria and Procedure 
For each reference translation of the three 
language directions, the Speech Processing 
EXpertise centre (SPEX)2  validated 600 words 
from contiguous segments randomly selected. 
Translations were checked by professional 
translators, who classified errors into categories. 
Points are given to references, according to the 
penalty scheme presented in Table 1. 
 
 
                                                 
2
 http://www.spex.nl 
Penalty points Error category 
Year 2 Year 3 
Syntactical 4 3 
Deviation from guidelines 3 - 
Lexical 2 3 
Poor usage 1 1 
Capitalization - 1 
Punctuation / spelling (max.10) 0.5 0.5 
Table 1. Translation errors penalties. 
 
In order to be considered valid, a reference 
translation must have less than 40 penalty points. 
A non-valid reference translation is sent back to 
the translation agency/ies to be proofread and 
improved with the help of an errors report. 
3.3 Typical Errors 
Most errors are lexical ones, followed by poor 
usage of the language. Syntactic and spelling 
category errors are considerably fewer. In terms 
on input type, the number of lexical, spelling and 
syntactic errors is higher for FTE than Verbatim. 
On the other hand, the number of errors for usage 
and deviation from guidelines (including global 
translation quality) is higher for Verbatim. 
Likewise, general errors are more frequent for 
English-to-Spanish than for Spanish-to-English. 
Chinese-to-English produces many more errors, 
in particular lexical ones. Syntactic errors could 
be wrong placement of adjective, wrong choice 
of person for pronouns, wrong use of verb tense 
or use of adjective as noun. Deviations from 
guidelines do not offer a wide variety: word/part 
of sentence omission, proper nouns mistransla-
tion or translation quality/odd sentence problems. 
Thus, they have been regrouped under the others 
for the 3rd year evaluation. Lexical errors show 
the widest variety, probably due to the specificity 
of the vocabulary: mistranslation of acronyms, 
wrong word order, missing plural, literal transla-
tion, bad terminology or approximation, wrong 
preposition or translation inconsistencies. Other 
errors are wrong punctuation or spelling errors. 
All these errors will lower the quality of the 
reference translations, which would imply a bi-
ased evaluation. The aim of the validation is then 
to reduce, as much as possible, the impact of 
mistranslation in order to improve a priori the 
assessment of the automatic translations. 
4 Results 
The following format is adopted for each set: 
?Year/Data-Input_Direction?, e.g. ?3/EPPS-FTE 
_EsEn? refers to the third-year set on Spanish-to-
English using the FTE input on EPPS data. 
40
4.1 Results of the Validation 
On the overall 14 sets, 10 sets have been trans-
lated again and revalidated at least once (for ei-
ther one or the two reference translations). Table 
2 gives the scores of validation for each of these 
sets together with the Word Error Rate with its 
respective validated reference. The left-hand side 
number gives the result for the first reference and 
the right-hand side one gives the result for the 
second reference. The different lines for each set 
give results for the different types of reference 
(from intermediate to final), thus showing the 
evolution of their validation. 
Validation score WER / final 
reference Set 
Ref 1 Ref 2 Ref 1 Ref 2 
3/EPPS-
FTE_EsEn 
59.5 
18 
18 
104.5 
73.5 
38 
12.5 
- 
8.2 
1.3 
3/Cortes-
FTE_EsEn 
43.5 
34 
34 
120.5 
70.5 
35 
6.2 
- 
5.1 
1.2 
3/Cortes-
Verb_EsEn 
54 
26.5 
67 
22.5 
0.5 0.3 
3/VoA-
Verb_ZhEn 
130 
53.5 
27 
129 
37 
37 
24.2 
6.3 
15.3 
- 
2/EPPS-
FTE_EnEs 
23 
23 
31 
23.5 
- 0.9 
2/EPPS-
FTE_EsEn 
18.5 
18.5 
33 
17 
- 2.6 
2/EPPS-
Verb_EnEs 
59.5 
11.5 
17 
17 
0.7 - 
2/Cortes-
FTE_EsEn 
42 
6 
61.5 
9 
0.2 5.3 
2/Cortes-
Verb_EsEn 
69 
18.5 
54 
0 
20.7 4.7 
2/VoA-
Verb_ZhEn 
84 
39.5 
38 
38 
17.0 - 
Table 2. Validation scores of reference translations 
and WER between intermediate (upper line) and 
final references (bottom line) for the first reference 
(Ref 1) and the second one (Ref 2). 
 
The mean score for the reference translations 
before any correction takes place is around 71, 
while after correction this is around 23. Thus, 
final translations are not perfect but their quality 
is sufficient to serve as reference. However, a 
maximum of 130 is obtained from the translation 
for Chinese-to-English, which seems to be more 
difficult than the other directions. WER was also 
computed between the non-validated translations 
and their corresponding validated versions. As it 
can be observed, are not necessarily very high 
and many WER values are below 1%. 
4.2 Intermediate vs. Final Reference 
When comparing the differences between the 
validation scores and WER for each translation, 
no correlation is found. The correlation coeffi-
cient between the score differences and the WER 
is around 58%. For instance, a score difference 
of 36 between non-validated and validated refer-
ences corresponds to a WER of 0.2, while an-
other difference of 26.5 corresponds to a WER of 
6.3. There is no direct correlation between the 
quality of the references and the WER scores. 
Thus, a priori, the quality of reference transla-
tions has no impact on the WER, which could be 
extended to the scoring of MT systems. Indeed, 
if WER does not reflect in a precise manner the 
quality increase of a human translation, how can 
it be useful/reliable for scoring MT systems? 
Figure 1 presents the correlation between these 
score differences and WER. It shows that quality 
is not necessarily well correlated with WER. The 
explanation is twofold: firstly, the improvement 
of a human translation does not necessarily imply 
many changes; secondly, WER does not reflect 
the quality of a translation accurately, as it does 
not seem to focus on essential language issues. 
0
5
10
15
20
25
0 20 40 60 80 100 120
Score difference
W
ER
 
Figure 1. Correlation of score difference  and WER 
between non-validated and validated references. 
4.3 BLEU Results of MT Systems 
BLEU scores have been computed for MT sys-
tem output, using each set and each reference 
(whether validated or not). Then, either scores 
are quite identical or scores are slightly diver-
gent. With the aim of studying this in detail, we 
assembled together the mean difference BLEU 
scores and the WER (against the final reference) 
for all the intermediate reference translations, as 
shown in Table 3. 
The correlation coefficient between the abso-
lute value of the mean difference score and mean 
of the WER is around 80%. Thus, the changes 
made into the references seem to have an impact 
on BLEU scores. However, given that quality is 
not correlated with WER, the absolute variation 
of the BLEU scores cannot be interpreted as a 
41
difference in MT system quality. It rather shows 
that comparing systems is the only plausible 
thing with BLEU, instead of evaluating systems 
in an absolute way. 
Set Mean diff. BLEU score 
WER / final 
reference 
3/EPPS-FTE_EsEn 1.18 
-0.08 
12.5 / 8.2 
- / 1.3 
3/Cortes-FTE_EsEn 0.67 0.035 
6.2 / 5.1 
- / 1.2 
3/Cortes-Verb_EsEn 0.021 0.5 / 0.3 
3/VoA-Verb_ZhEn -1.71 0.001 
24.2 / 15.3 
6.3 / - 
2/EPPS-FTE_EnEs 0.02 - / 0.9 
2/EPPS-FTE_EsEn 0.24 - / 2.6 
2/EPPS-Verb_EnEs -0.05 0.7 / - 
2/Cortes-FTE_EsEn 1.152 0.2 / 5.3 
2/Cortes-Verb_EsEn -2.21 20.7 / 4.7 
2/VoA-Verb_ZhEn 0.08 17.0 / - 
Table 3. Mean difference BLEU scores for each 
reference translation and WER between interme-
diate and final references. 
4.4 Correlations of systems? evaluations 
Correlations for BLEU scores were computed 
between 2 different-quality references. This al-
lowed us to obtain 2 correlation coefficients for 2 
non-validated references. For the correlation on 
scores, all coefficients are over 99%, so that even 
if scores increase or decrease, the distance be-
tween systems does not change. This is con-
firmed by the correlation on ranks, since the co-
efficients are between 96% and 100%. Thus, bet-
ter reference translations could hardly distinguish 
MT systems in an easier way during evaluation. 
5 Discussion and Conclusions 
This work has used the BLEU metric to score 
MT system output and has demonstrated that the 
quality of reference translations does not have a 
clear impact on WER, also using n-grams. Even 
when using lower-quality translations, scores 
remain similar from one reference to another and 
important modifications of the human translation  
do not affect strongly the scores of the MT sys-
tems. This behaviour concerns all the languages 
tested, and remains the same regardless of the 
input or language used. However, we should not 
forget that the context of this experiment con-
cerns actual automatic metrics. When reference 
translations have been modified, the impact on 
scores is not that clear, and even worse, this im-
pact could be argued, to a certain extent, when 
the aim is to compare systems. Indeed, we also 
observed changes into scores when references 
were modified. Moreover, the quality of MT sys-
tems should not be ignored: if the overall quality 
of a system output is low, changes in reference 
translation will certainly have a lower impact on 
their scores. 
Over the modification of the scores, the vali-
dation of the reference translation leads up to the 
validation criteria (although they are rigorously 
defined they are sometimes not very easy to ap-
ply by the validation team), the consistencies 
between agencies and translators (differences 
between reference translations show how the 
human translation quality may vary according to 
the translator) and some errors made by agencies 
(could be argued and validation can be difficult 
depending on the context, input, etc). Those 
points have to be carefully checked during a 
validation procedure and scores given by auto-
matic metrics should be studied in agreement 
with the variation of the quality and validation. 
Acknowledgements 
This work was supported by the TC-STAR pro-
ject (grant number IST-506738). We would like 
to thank SPEX, especially Henk Van den Heuvel 
and Eric Sanders, for the validation work. Our 
thanks also to Victoria Arranz for her help. 
References 
Fers?e H., Van den Heuvel H., Olsen S. 2006. Valida-
tion of third party Spoken and Written Language 
Resources Methods for performing Quick Quality 
Checks. Proceedings of Workshop ?Quality assur-
ance and quality measurement for language and 
speech resources?, LREC 2006, Genoa, Italy. 
Mostefa D., Garcia M-N., Hamon O. and Moreau N. 
2006. Evaluation report, Technology and Corpora 
for Speech to Speech Translation (TC-STAR) pro-
ject. Deliverable D16. 
Mostefa D., Hamon O., Moreau N. and Choukri K. 
2007. Evaluation report, Technology and Corpora 
for Speech to Speech Translation (TC-STAR) pro-
ject. Deliverable D30. 
Papineni K, Roukos S, Ward T, and Wei-Jing Z. 
2001. BLEU: A Method for Automatic Evaluation 
of Machine Translation. IBM Research Division, 
Thomas J. Watson Research Center. 
Van den Heuvel H., Choukri K., H?ge H., Maegaard 
B., Odijk J., Mapelli V. 2003. Quality Control of 
Language Resources at ELRA. Proceedings of Eu-
rospeech, Geneva, Switzerland, pp. 1541-1544. 
42
Proceedings of the 12th Conference of the European Chapter of the ACL, pages 345?353,
Athens, Greece, 30 March ? 3 April 2009. c?2009 Association for Computational Linguistics
End-to-End Evaluation in Simultaneous Translation
Olivier Hamon1,2, Christian F?gen3, Djamel Mostefa1, Victoria Arranz1,
Muntsin Kolss3, Alex Waibel3,4 and Khalid Choukri1
1Evaluations and Language Resources Distribution Agency (ELDA), Paris, France
2 LIPN (UMR 7030) ? Universit? Paris 13 & CNRS, Villetaneuse, France
3 Univerit?t Karlsruhe (TH), Germany
4 Carnegie Mellon University, Pittsburgh, USA
{hamon|mostefa|arranz|choukri}@elda.org,
{fuegen|kolss|waibel}@ira.uka.de
Abstract
This paper presents the end-to-end evalu-
ation of an automatic simultaneous trans-
lation system, built with state-of-the-art
components. It shows whether, and for
which situations, such a system might be
advantageous when compared to a human
interpreter. Using speeches in English
translated into Spanish, we present the
evaluation procedure and we discuss the
results both for the recognition and trans-
lation components as well as for the over-
all system. Even if the translation process
remains the Achilles? heel of the system,
the results show that the system can keep
at least half of the information, becoming
potentially useful for final users.
1 Introduction
Anyone speaking at least two different languages
knows that translation and especially simultaneous
interpretation are very challenging tasks. A human
translator has to cope with the special nature of
different languages, comprising phenomena like
terminology, compound words, idioms, dialect
terms or neologisms, unexplained acronyms or ab-
breviations, proper names, as well as stylistic and
punctuation differences. Further, translation or in-
terpretation are not a word-by-word rendition of
what was said or written in a source language. In-
stead, the meaning and intention of a given sen-
tence have to be reexpressed in a natural and fluent
way in another language.
Most professional full-time conference inter-
preters work for international organizations like
the United Nations, the European Union, or the
African Union, whereas the world?s largest em-
ployer of translators and interpreters is currently
the European Commission. In 2006, the European
Parliament spent about 300 million Euros, 30% of
its budget, on the interpretation and translation of
the parliament speeches and EU documents. Gen-
erally, about 1.1 billion Euros are spent per year
on the translating and interpreting services within
the European Union, which is around 1% of the
total EU-Budget (Volker Steinbiss, 2006).
This paper presents the end-to-end evaluation
of an automatic simultaneous translation system,
built with state-of-the-art components. It shows
whether, and in which cases, such a system might
be advantageous compared to human interpreters.
2 Challenges in Human Interpretation
According to Al-Khanji et al (2000), researchers
in the field of psychology, linguistics and interpre-
tation seem to agree that simultaneous interpre-
tation (SI) is a highly demanding cognitive task
involving a basic psycholinguistic process. This
process requires the interpreter to monitor, store
and retrieve the input of the source language in
a continuous manner in order to produce the oral
rendition of this input in the target language. It is
clear that this type of difficult linguistic and cog-
nitive operation will force even professional in-
terpreters to elaborate lexical or synthetic search
strategies.
Fatigue and stress have a negative effect on the
interpreter, leading to a decrease in simultaneous
interpretation quality. In a study by Moser-Mercer
et al (1998), in which professional speakers were
asked to work until they could no longer provide
acceptable quality, it was shown that (1) during
the first 20 minutes the frequency of errors rose
steadily, (2) the interpreters, however, seemed to
be unaware of this decline in quality, (3) after 60
minutes, all subjects made a total of 32.5 mean-
ing errors, and (4) in the category of nonsense the
number of errors almost doubled after 30 minutes
on the task.
Since the audience is only able to evaluate the
simultaneously interpreted discourse by its form,
345
the fluency of an interpretation is of utmost im-
portance. According to a study by Kopczynski
(1994), fluency and style were third on a list of
priorities (after content and terminology) of el-
ements rated by speakers and attendees as con-
tributing to quality. Following the overview in
(Yagi, 2000), an interpretation should be as natu-
ral and as authentic as possible, which means that
artificial pauses in the middle of a sentence, hes-
itations, and false-starts should be avoided, and
tempo and intensity of the speaker?s voice should
be imitated.
Another point to mention is the time span be-
tween a source language chunk and its target lan-
guage chunk, which is often referred to as ear-
voice-span. Following the summary in (Yagi,
2000), the ear-voice-span is variable in duration
depending on some source and target language
variables, like speech delivery rate, information
density, redundancy, word order, syntactic charac-
teristics, etc. Short delays are usually preferred for
several reasons. For example, the audience is irri-
tated when the delay is too large and is soon asking
whether there is a problem with the interpretation.
3 Automatic Simultaneous Translation
Given the explanations above on human interpre-
tation, one has to weigh two factors when consid-
ering the use of simultaneous translation systems:
translation quality and cost.
The major disadvantage of an automatic system
compared to human interpretation is its translation
quality, as we will see in the following sections.
Current state-of-the-art systems may reach satis-
factory quality for people not understanding the
lecturer at all, but are still worse than human inter-
pretation. Nevertheless, an automatic system may
have considerable advantages.
One such advantage is its considerable short-
term memory: storing long sequences of words is
not a problem for a computer system. Therefore,
compensatory strategies are not necessary, regard-
less of the speaking rate of the speaker. However,
depending on the system?s translation speed, la-
tency may increase. While it is possible for hu-
mans to compress the length of an utterance with-
out changing its meaning (summarization), it is
still a challenging task for automatic systems.
Human simultaneous interpretation is quite ex-
pensive, especially due to the fact that usually two
interpreters are necessary. In addition, human in-
terpreters require preparation time to become fa-
miliar with the topic. Moreover, simultaneous in-
terpretation requires a soundproof booth with au-
dio equipment, which adds an overall cost that is
unacceptable for all but the most elaborate multi-
lingual events. On the other hand, a simultaneous
translation system also needs time and effort for
preparation and adaptation towards the target ap-
plication, language and domain. However, once
adapted, it can be easily re-used in the same do-
main, language, etc. Another advantage is that the
transcript of a speech or lecture is produced for
free by using an automatic system in the source
and target languages.
3.1 The Simultaneous Translation System
Figure 1 shows a schematic overview of the si-
multaneous translation system developed at Uni-
versit?t Karlsruhe (TH) (F?gen et al, 2006b). The
speech of the lecturer is recorded with the help
of a close-talk microphone and processed by the
speech recognition component (ASR). The par-
tial hypotheses produced by the ASR module are
collected in the resegmentation component, for
merging and re-splitting at appropriate ?seman-
tic? boundaries. The resegmented hypotheses are
then transferred to one or more machine transla-
tion components (MT), at least one per language
pair. Different output technologies may be used
for presenting the translations to the audience. For
a detailed description of the components as well
as the client-server framework used for connect-
ing the components please refer to (F?gen et al,
2006b; F?gen et al, 2006a; Kolss et al, 2006; F?-
gen and Kolss, 2007; F?gen et al, 2001).
3.2 End-to-End Evaluation
The evaluation in speech-to-speech translation
jeopardises many concepts and implies a lot of
subjectivity. Three components are involved and
an overall system may grow the difficulty of esti-
mating the output quality. However, two criteria
are mainly accepted in the community: measuring
the information preservation and determining how
much of the translation is understandable.
Several end-to-end evaluations in speech-to-
speech translation have been carried out in the last
few years, in projects such as JANUS (Gates et
al., 1996), Verbmobil (N?bel, 1997) or TC-STAR
(Hamon et al, 2007). Those projects use the
main criteria depicted above, and protocols differ
in terms of data preparation, rating, procedure, etc.
346
Dictionary
Source
Hypothesis Translatable
Segment
Model
Source Boundary
Resegmen?
tationRecognition
Speech
Translation
Model Model
Target Language
Machine
Translation
Model
Source Acoustic
Model
Source Language
Output
Translated
Translation
Vocabulary
Audio Stream
Text
Output
(Subtitles)(Synthesis)
Spoken
Figure 1: Schematic overview and information flow of the simultaneous translation system. The main
components of the system are represented by cornered boxes and the models used for theses components
by ellipses. The different output forms are represented by rounded boxes.
To our opinion, to evaluate the performance of a
complete speech-to-speech translation system, we
need to compare the source speech used as input to
the translated output speech in the target language.
To that aim, we reused a large part of the evalua-
tion protocol from the TC-STAR project(Hamon
et al, 2007).
4 Evaluation Tasks
The evaluation is carried out on the simultaneously
translated speech of a single speaker?s talks and
lectures in the field of speech processing, given in
English, and translated into Spanish.
4.1 Data used
Two data sets were selected from the talks and
lectures. Each set contained three excerpts, no
longer than 6 minutes each and focusing on dif-
ferent topics. The former set deals with speech
recognition and the latter with the descriptions of
European speech research projects, both from the
same speaker. This represents around 7,200 En-
glish words. The excerpts were manually tran-
scribed to produce the reference for the ASR eval-
uation. Then, these transcriptions were manually
translated into Spanish by two different transla-
tors. Two reference translations were thus avail-
able for the spoken language translation (SLT)
evaluation. Finally, one human interpretation was
produced from the excerpts as reference for the
end-to-end evaluation. It should be noted that for
the translation system, speech synthesis was used
to produce the spoken output.
4.2 Evaluation Protocol
The system is evaluated as a whole (black box
evaluation) and component by component (glass
box evaluation):
ASR evaluation. The ASR module is evaluated
by computing the Word Error Rate (WER) in case
insensitive mode.
SLT evaluation. For the SLT evaluation, the au-
tomatically translated text from the ASR output is
compared with two manual reference translations
by means of automatic and human metrics. Two
automatic metrics are used: BLEU (Papineni et
al., 2001) and mWER (Niessen et al, 2000).
For the human evaluation, each segment is eval-
uated in relation to adequacy and fluency (White
and O?Connell, 1994). For the evaluation of ad-
equacy, the target segment is compared to a ref-
erence segment. For the evaluation of fluency,
the quality of the language is evaluated. The two
types of evaluation are done independently, but
each evaluator did both evaluations (first that of
fluency, then that of adequacy) for a certain num-
ber of segments. For the evaluation of fluency,
evaluators had to answer the question: ?Is the text
written in good Spanish??. For the evaluation of
adequacy, evaluators had to answer the question:
?How much of the meaning expressed in the ref-
erence translation is also expressed in the target
translation??.
For both evaluations, a five-point scale is pro-
posed to the evaluators, where only extreme val-
ues are explicitly defined. Three evaluations are
carried out per segment, done by three different
evaluators, and segments are divided randomly,
because evaluators must not recreate a ?story?
347
and thus be influenced by the context. The total
number of judges was 10, with around 100 seg-
ments per judge. Furthermore, the same number
of judges was recruited for both categories: ex-
perts, from the domain with a knowledge of the
technology, and non-experts, without that knowl-
edge.
End-to-End evaluation. The End-to-End eval-
uation consists in comparing the speech in the
source language to the output speech in the tar-
get language. Two important aspects should be
taken into account when assessing the quality of
a speech-to-speech system.
First, the information preservation is measured
by using ?comprehension questionnaires?. Ques-
tions are created from the source texts (the En-
glish excerpts), then questions and answers are
translated into Spanish by professional translators.
These questions are asked to human judges after
they have listened to the output speech in the tar-
get language (Spanish). At a second stage, the an-
swers are analysed: for each answer a Spanish val-
idator gives a score according to a binary scale (the
information is either correct or incorrect). This al-
lows us to measure the information preservation.
Three types of questions are used in order to di-
versify the difficulty of the questions and test the
system at different levels: simple Factual (70%),
yes/no (20%) and list (10%) questions. For in-
stance, questions were: What is the larynx respon-
sible for?, Have all sites participating in CHIL
built a CHIL room?, Which types of knowledge
sources are used by the decoder?, respectively.
The second important aspect of a speech-to-
speech system is the quality of the speech output
(hereafter quality evaluation). For assessing the
quality of the speech output one question is asked
to the judges at the end of each comprehension
questionnaire: ?Rate the overall quality of this au-
dio sample?, and values go from 1 (?1: Very bad,
unusable?) to 5 (?It is very useful?). Both auto-
matic system and interpreter outputs were evalu-
ated with the same methodology.
Human judges are real users and native Span-
ish speakers, experts and non-experts, but different
from those of the SLT evaluation. Twenty judges
were involved (12 excerpts, 10 evaluations per ex-
cerpt and 6 evaluations per judge) and each judge
evaluated both automatic and human excerpts on a
50/50 percent basis.
5 Components Results
5.1 Automatic Speech Recognition
The ASR output has been evaluated using the
manual transcriptions of the excerpts. The overall
Word Error Rate (WER) is 11.9%. Table 1 shows
the WER level for each excerpt.
Excerpts WER [%]
L043-1 14.5
L043-2 14.5
L043-3 9.6
T036-1 11.3
T036-2 11.7
T036-3 9.2
Overall 11.9
Table 1: Evaluation results for ASR.
T036 excerpts seem to be easier to recognize au-
tomatically than L043 ones, probably due to the
more general language of the former.
5.2 Machine Translation
5.2.1 Human Evaluation
Each segment within the human evaluation is eval-
uated 4 times, each by a different judge. This aims
at having a significant number of judgments and
measuring the consistency of the human evalua-
tions. The consistency is measured by computing
the Cohen?s Kappa coefficient (Cohen, 1960).
Results show a substantial agreement for flu-
ency (kappa of 0.64) and a moderate agreement
for adequacy (0.52).The overall results of the hu-
man evaluation are presented in Table 2. Regard-
ing both experts? and non-experts? details, agree-
ment is very similar (0.30 and 0.28, respectively).
All judges Experts Non experts
Fluency 3.13 2.84 3.42
Adequacy 3.26 3.21 3.31
Table 2: Average rating of human evalua-
tions [1<5].
Both fluency and adequacy results are over the
mean. They are lower for experts than for non-
experts. This may be due to the fact that experts
are more familiar with the domain and therefore
more demanding than non experts. Regarding the
detailed evaluation per judge, scores are generally
lower for non-experts than for experts.
348
5.2.2 Automatic Evaluation
Scores are computed using case-sensitive metrics.
Table 3 shows the detailed results per excerpt.
Excerpts BLEU [%] mWER [%]
L043-1 25.62 58.46
L043-2 22.60 62.47
L043-3 28.73 62.64
T036-1 34.46 55.13
T036-2 29.41 59.91
T036-3 35.17 50.77
Overall 28.94 58.66
Table 3: Automatic Evaluation results for SLT.
Scores are rather low, with a mWER of 58.66%,
meaning that more than half of the translation is
correct. According to the scoring, the T036 ex-
cerpts seem to be easier to translate than the L043
ones, the latter being of a more technical nature.
6 End-to-End Results
6.1 Evaluators Agreement
In this study, ten judges carried out the evaluation
for each excerpt. In order to observe the inter-
judges agreement, the global Fleiss?s Kappa co-
efficient was computed, which allows to measure
the agreement between m judges with r criteria of
judgment. This coefficient shows a global agree-
ment between all the judges, which goes beyond
Cohen?s Kappa coefficient. However, a low co-
efficient requires a more detailed analysis, for in-
stance, by using Kappa for each pair of judges.
Indeed, this allows to see how deviant judges are
from the typical judge behaviour. For m judges,
n evaluations and r criteria, the global Kappa is
defined as follows:
? = 1 ?
nm2 ?
?n
i=1
?r
j=1 X
2
ij
nm(m? 1) ?rj=1 Pj(1 ? Pj)
where:
Pj =
?n
i=1 Xij
nm
and: Xij is the number of judgments for the ith
evaluation and the jth criteria.
Regarding quality evaluation (n = 6, m = 10,
r = 5), Kappa values are low for both human in-
terpreters (? = 0.07) and the automatic system
(? = 0.01), meaning that judges agree poorly
(Landis and Koch, 1977). This is explained by
the extreme subjectivity of the evaluation and the
small number of evaluated excerpts. Looking at
each pair of judges and the Kappa coefficients
themselves, there is no real agreement, since most
of the Kappa values are around zero. However,
some judge pairs show fair agreement, and some
others show moderate or substantial agreement. It
is observed, though, that some criteria are not fre-
quently selected by the judges, which limits the
statistical significance of the Kappa coefficient.
The limitations are not the same for the com-
prehension evaluation (n = 60, m = 10, r = 2),
since the criteria are binary (i.e. true or false). Re-
garding the evaluated excerpts, Kappa values are
0.28 for the automatic system and 0.30 for the in-
terpreter. According to Landis and Koch (1977),
those values mean that judges agree fairly. In
order to go further, the Kappa coefficients were
computed for each pair of judges. Results were
slightly better for the interpreter than for the au-
tomatic system. Most of them were between 0.20
and 0.40, implying a fair agreement. Some judges
agreed moderately.
Furthermore, it was also observed that for the
120 available questions, 20 had been answered
correctly by all the judges (16 for the interpreter
evaluation and 4 for the automatic system one)
and 6 had been answered wrongly by all judges (1
for the former and 5 for the latter). That shows a
trend where the interpreter comprehension would
be easier than that of the automatic system, or at
least where the judgements are less questionable.
6.2 Quality Evaluation
Table 4 compares the quality evaluation results of
the interpreter to those of the automatic system.
Samples Interpreter Automatic system
L043-1 3.1 1.6
L043-2 2.9 2.3
L043-3 2.4 2.1
T036-1 3.6 3.1
T036-2 2.7 2.5
T036-3 3.5 2.5
Mean 3.03 2.35
Table 4: Quality evaluation results for the inter-
preter and the automatic system [1<5].
As can be seen, with a mean score of 3.03 even
for the interpreter, the excerpts were difficult to
interpret and translate. This is particularly so for
349
L043, which is more technical than T036. The
L043-3 excerpt is particularly technical, with for-
mulae and algorithm descriptions, and even a com-
plex description of the human articulatory system.
In fact, L043 provides a typical presentation with
an introduction, followed by a deeper description
of the topic. This increasing complexity is re-
flected on the quality scores of the three excerpts,
going from 3.1 to 2.4.
T036 is more fluent due to the less technical na-
ture of the speech and the more general vocabu-
lary used. However, the T036-2 and T036-3 ex-
cerpts get a lower quality score, due to the descrip-
tion of data collections or institutions, and thus the
use of named entities. The interpreter does not
seem to be at ease with them and is mispronounc-
ing some of them, such as ?Grenoble? pronounced
like in English instead of in Spanish. The inter-
preter seems to be influenced by the speaker, as
can also be seen in his use of the neologism ?el ce-
nario? (?the scenario?) instead of ?el escenario?.
Likewise, ?Karlsruhe? is pronounced three times
differently, showing some inconsistency of the in-
terpreter.
The general trend in quality errors is similar to
those of previous evaluations: lengthening words
(?seeee?ales?), hesitations, pauses between syl-
lables and catching breath (?caracter?s...ticas?),
careless mistakes (?probibilidad? instead of ?prob-
abilidad?), self-correction of wrong interpreting
(?reconocien-/reconocimiento?), etc.
An important issue concerns gender and num-
ber agreement. Those errors are explained by
the presence of morphological gender in Spanish,
like in ?estos se?ales? instead of ?estas se?ales?
(?these signals?) together with the speaker?s speed
of speech. The speaker seems to start by default
with a masculine determiner (which has no gen-
der in English), adjusting the gender afterward de-
pending on the noun following. A quick transla-
tion may also be the cause for this kind of errors,
like ?del se?al acustico? (?of the acoustic signal?)
with a masculine determiner, a feminine substan-
tive and ending in a masculine adjective. Some
translation errors are also present, for instance
?computerizar? instead of ?calcular? (?compute?).
The errors made by the interpreter help to un-
derstand how difficult oral translation is. This
should be taken into account for the evaluation of
the automatic system.
The automatic system results, like those of
the interpreter, are higher for T036 than for L043.
However, scores are lower, especially for the
L043-1 excerpt. This seems to be due to the
type of lexicon used by the speaker for this ex-
cerpt, more medical, since the speaker describes
the articulatory system. Moreover, his description
is sometimes metaphorical and uses a rather col-
loquial register. Therefore, while the interpreter
finds it easier to deal with these excerpts (known
vocabulary among others) and L043-3 seems to be
more complicated (domain-specific, technical as-
pect), the automatic system finds it more compli-
cated with the former and less with the latter. In
other words, the interpreter has to ?understand?
what is said in L043-3, contrary to the automatic
system, in order to translate.
Scores are higher for the T036 excerpts. In-
deed, there is a high lexical repetition, a large
number of named entities, and the quality of the
excerpt is very training-dependant. However, the
system runs into trouble to process foreign names,
which are very often not understandable. Differ-
ences between T036-1 and the other T036 excerpts
are mainly due to the change in topic. While the
former deals with a general vocabulary (i.e. de-
scription of projects), the other two excerpts de-
scribe the data collection, the evaluation metrics,
etc., thus increasing the complexity of translation.
Generally speaking, quality scores of the au-
tomatic system are mainly due to the transla-
tion component, and to a lesser extent to the
recognition component. Many English words are
not translated (?bush?, ?keyboards?, ?squeaking?,
etc.), and word ordering is not always correct.
This is the case for the sentence ?how we solve
it?, translated into ?c?mo nos resolvers lo? instead
of ?c?mo lo resolvemos?. Funnily enough, the
problems of gender (?maravillosos aplicaciones?
- masc. vs fem.) and number (?pueden real-
mente ser aplicado? - plu. vs sing.) the in-
terpreter has, are also found for the automatic
system. Moreover, the translation of compound
nouns often shows wrong word ordering, in partic-
ular when they are long, i.e. up to three words (e.g.
?reconocimiento de habla sistemas? for ?speech
recognition system? instead of ?sistemas de re-
conocimiento de habla?).
Finally, some error combinations result in fully
non-understandable sentences, such as:
?usted tramo se en emacs es squeaking
ruido y dries todos demencial?
350
where the following errors take place:
? tramo: this translation of ?stretch? results
from the choice of a substantive instead of a
verb, giving rise to two choices due to the lex-
ical ambiguity: ?estiramiento? and ?tramo?,
which is more a linear distance than a stretch
in that context;
? se: the pronoun ?it? becomes the reflexive
?se? instead of the personal pronoun ?lo?;
? emacs: the recognition module transcribed
the couple of words ?it makes? into ?emacs?,
not translated by the translation module;
? squeaking: the word is not translated by the
translation module;
? dries: again, two successive errors are made:
the word ?drives? is transcribed into ?dries?
by the recognition module, which is then left
untranslated.
The TTS component also contributes to decreas-
ing the output quality. The prosody module finds it
hard to make the sentences sound natural. Pauses
between words are not very frequent, but they do
not sound natural (i.e. like catching breath) and
they are not placed at specific points, as it would
be done by a human. For instance, the prosody
module does not link the noun and its determiner
(e.g. ?otros aplicaciones?). Finally, a not user-
friendly aspect of the TTS component is the rep-
etition of the same words always pronounced in
the same manner, what is quite disturbing for the
listener.
6.3 Comprehension Evaluation
Tables 5 and 6 present the results of the compre-
hension evaluation, for the interpreter and for the
automatic system, respectively. They provide the
following information:
identifiers of the excerpt: Source data are the
same for the interpreter and the automatic
system, namely the English speech;
subj. E2E: The subjective results of the end-to-
end evaluation are done by the same assessors
who did the quality evaluation. This shows
the percentage of good answers;
fair E2E: The objective verification of the an-
swers. The audio files are validated to check
whether they contain the answers to the ques-
tions or not (as the questions were created
from the English source). This shows the
maximum percentage of answers an evalua-
tor managed to find from either the interpreter
(speaker audio) or the automatic system out-
put (TTS) in Spanish. For instance, informa-
tion in English could have been missed by
the interpreter because he/she felt that this in-
formation was meaningless and could be dis-
carded. We consider those results as an ob-
jective evaluation.
SLT, ASR: Verification of the answers in each
component of the end-to-end process. In or-
der to determine where the information for
the automatic system is lost, files from each
component (recognised files for ASR, trans-
lated files for SLT, and synthesised files for
TTS in the ?fair E2E? column) are checked.
Excerpts subj. E2E fair E2E
L043-1 69 90
L043-2 75 80
L043-3 72 60
T036-1 80 100
T036-2 73 80
T036-3 76 100
Mean 74 85
Table 5: Comprehension evaluation results for the
interpreter [%].
Regarding Table 5, the interpreter loses 15%
of the information (i.e. 15% of the answers were
incorrect or not present in the interpreter?s trans-
lation) and judges correctly answered 74% of the
questions. Five documents get above 80% of cor-
rect results, while judges find almost above 70%
of the answers for the six documents.
Regarding the automatic system results (Table
6), the information rate found by judges is just
above 50% since, by extension, more than half the
questions were correctly answered. The lowest
excerpt, L043-1, gets a rate of 25%, the highest,
T036-1, a rate of 76%, which is in agreement with
the observation for the quality evaluation. Infor-
mation loss can be found in each component, es-
pecially for the SLT module (35% of the informa-
tion is lost here). It should be noticed that the TTS
module made also errors which prevented judges
351
Excerpts subj. E2E fair E2E SLT ASR
L043-1 25 30 30 70
L043-2 62 70 80 70
L043-3 43 40 60 100
T036-1 76 80 90 100
T036-2 61 70 60 80
T036-3 47 60 70 80
Mean 52 58 65 83
Table 6: Comprehension evaluation results for the
automatic system [%].
from answering related questions. Moreover, the
ASR module loses 17% of the information. Those
results are certainly due to the specific vocabulary
used in this experiment.
So as to objectively compare the interpreter with
the automatic system, we selected the questions
for which the answers were included in the inter-
preter files (i.e. those in the ?fair E2E? column
of Table 5). The goal was to compare the overall
quality of the speech-to-speech translation to in-
terpreters? quality, without the noise factor of the
information missing. The assumption is that the
interpreter translates the ?important information?
and skips the useless parts of the original speech.
This experiment is to measure the level of this in-
formation that is preserved by the automatic sys-
tem. So a new subset of results was obtained, on
the information kept by the interpreter. The same
study was repeated for the three components and
the results are shown in Tables 7 and 8.
Excerpts subj. E2E fair E2E SLT ASR
L043-1 27 33 33 78
L043-2 65 75 88 75
L043-3 37 67 83 100
T036-1 76 80 90 100
T036-2 69 88 75 100
T036-3 47 60 70 80
Mean 53 60 70 80
Table 7: Evaluation results for the automatic sys-
tem restricted to the questions for which answers
can be found in the interpreter speech [%].
Comparing the automatic system to the inter-
preter, the automatic system keeps 40% of the in-
formation where the interpreter translates the doc-
uments correctly. Those results confirm that ASR
loses a lot of information (20%), while SLT loses
10% further, and so does the TTS. Judges are quite
close to the objective validation and found most of
the answers they could possibly do.
Excerpts subj. E2E
L043-1 66
L043-2 90
L043-3 88
T036-1 80
T036-2 81
T036-3 76
Mean 80
Table 8: Evaluation results for interpreter, re-
stricted to the questions for which answers can be
found in the interpreter speech [%].
Subjective results for the restricted evaluation
are similar to the previous results, on the full data
(80% vs 74% of the information found by the
judges). Performance is good for the interpreter:
98% of the information correctly translated by the
automatic system is also correctly interpreted by
the human. Although we can not compare the
performance of the restricted automatic system to
that of the restricted interpreter (since data sets of
questions are different), it seems that of the inter-
preter is better. However, the loss due to subjective
evaluation seems to be higher for the interpreter
than for the automatic system.
7 Conclusions
Regarding the SLT evaluation, the results achieved
with the simultaneous translation system are still
rather low compared to the results achieved with
offline systems for translating European parlia-
ment speeches in TC-STAR. However, the offline
systems had almost no latency constraints, and
parliament speeches are much easier to recognize
and translate when compared to the more spon-
taneous talks and lectures focused in this paper.
This clearly shows the difficulty of the whole task.
However, the human end-to-end evaluation of the
system in which the system is compared with hu-
man interpretation shows that the current transla-
tion quality allows for understanding of at least
half of the content, and therefore, may be already
quite helpful for people not understanding the lan-
guage of the lecturer at all.
352
References
Rajai Al-Khanji, Said El-Shiyab, and Riyadh Hussein.
2000. On the Use of Compensatory Strategies in Si-
multaneous Interpretation. Meta : Journal des tra-
ducteurs, 45(3):544?557.
Jacob Cohen. 1960. A coefficient of agreement for
nominal scales. In Educational and Psychological
Measurement, volume 20, pages 37?46.
Christian F?gen and Muntsin Kolss. 2007. The influ-
ence of utterance chunking on machine translation
performance. In Proc. of the European Conference
on Speech Communication and Technology (INTER-
SPEECH), Antwerp, Belgium, August. ISCA.
Christian F?gen, Martin Westphal, Mike Schneider,
Tanja Schultz, and Alex Waibel. 2001. LingWear:
A Mobile Tourist Information System. In Proc. of
the Human Language Technology Conf. (HLT), San
Diego, California, March. NIST.
Christian F?gen, Shajith Ikbal, Florian Kraft, Kenichi
Kumatani, Kornel Laskowski, John W. McDonough,
Mari Ostendorf, Sebastian St?ker, and Matthias
W?lfel. 2006a. The isl rt-06s speech-to-text system.
In Steve Renals, Samy Bengio, and Jonathan Fiskus,
editors, Machine Learning for Multimodal Interac-
tion: Third International Workshop, MLMI 2006,
Bethesda, MD, USA, volume 4299 of Lecture Notes
in Computer Science, pages 407?418. Springer Ver-
lag Berlin/ Heidelberg.
Christian F?gen, Muntsin Kolss, Matthias Paulik, and
Alex Waibel. 2006b. Open Domain Speech Trans-
lation: From Seminars and Speeches to Lectures.
In TC-Star Speech to Speech Translation Workshop,
Barcelona, Spain, June.
Donna Gates, Alon Lavie, Lori Levin, Alex. Waibel,
Marsal Gavalda, Laura Mayfield, and Monika Wosz-
cyna. 1996. End-to-end evaluation in janus: A
speech-to-speech translation system. In Proceed-
ings of the 6th ECAI, Budapest.
Olivier Hamon, Djamel Mostefa, and Khalid Choukri.
2007. End-to-end evaluation of a speech-to-speech
translation system in tc-star. In Proceedings of the
MT Summit XI, Copenhagen, Denmark, September.
Muntsin Kolss, Bing Zhao, Stephan Vogel, Ashish
Venugopal, and Ying Zhang. 2006. The ISL Statis-
tical Machine Translation System for the TC-STAR
Spring 2006 Evaluations. In TC-Star Workshop
on Speech-to-Speech Translation, Barcelona, Spain,
December.
Andrzej Kopczynski, 1994. Bridging the Gap: Empiri-
cal Research in Simultaneous Interpretation, chapter
Quality in Conference Interpreting: Some Pragmatic
Problems, pages 87?100. John Benjamins, Amster-
dam/ Philadelphia.
J. Richard Landis and Gary G. Koch. 1977. The mea-
surement of observer agreement for categorical data.
In Biometrics, Vol. 33, No. 1 (Mar., 1977), pp. 159-
174.
Barbara Moser-Mercer, Alexander Kunzli, and Ma-
rina Korac. 1998. Prolonged turns in interpreting:
Effects on quality, physiological and psychological
stress (pilot study). Interpreting: International jour-
nal of research and practice in interpreting, 3(1):47?
64.
Sonja Niessen, Franz Josef Och, Gregor Leusch, and
Hermann Ney. 2000. An evaluation tool for ma-
chine translation: Fast evaluation for mt research.
In Proceedings of the 2nd International Conference
on Language Resources and Evaluation, Athens,
Greece.
Rita N?bel. 1997. End-to-end Evaluation in Verb-
mobil I. In Proceedings of the MT Summit VI, San
Diego.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2001. Bleu: a method for automatic
evaluation of machine translation. Technical Report
RC22176 (W0109-022), Research Report, Com-
puter Science IBM Research Division, T.J.Watson
Research Center.
Accipio Consulting Volker Steinbiss. 2006.
Sprachtechnologien f?r Europa. www.tc-star.
org/pubblicazioni/D17_HLT_DE.pdf.
John S. White and Theresa A. O?Connell. 1994.
Evaluation in the arpa machine translation program:
1993 methodology. In HLT ?94: Proceedings of the
workshop on Human Language Technology, pages
135?140, Morristown, NJ, USA. Association for
Computational Linguistics.
Sane M. Yagi. 2000. Studying Style in Simultane-
ous Interpretation. Meta : Journal des traducteurs,
45(3):520?547.
353
Proceedings of the EMNLP 2014 Workshop on Arabic Natural Langauge Processing (ANLP), pages 155?159,
October 25, 2014, Doha, Qatar.
c?2014 Association for Computational Linguistics
TECHLIMED system description for the Shared Task on Automatic
Arabic Error Correction
Djamel MOSTEFA
Techlimed
42 rue de l?Universit?e
Lyon, France
Omar ASBAYOU
Techlimed
42 rue de l?Universit?e
Lyon, France
{firstname.lastname}@techlimed.com
Ramzi ABBES
Techlimed
42 rue de l?Universit?e
Lyon, France
Abstract
This article is a system description paper
and reports on the participation of Tech-
limed in the ?QALB-2014 shared task? on
evaluation of automatic arabic error cor-
rection systems organized in conjunction
with the EMNLP 2014 Workshop on Ara-
bic Natural Language Processing. Cor-
recting automatically texts in Arabic is a
challenging task due to the complexity and
rich morphology of the Arabic language
and the lack of appropriate resources, (e.g.
publicly available corpora and tools). To
develop our systems, we considered sev-
eral approaches from rule based systems
to statistical methods. Our results on the
development set show that the statistical
system outperforms the lexicon driven ap-
proach with a precision of 71%, a recall of
50% and a F-measure of 59%.
1 Introduction
Automatic error correction is an important task
in Natural Language Processing (NLP). It can be
used in a wide range of applications such as word
processing tools (e.g. Microsoft Office, Openof-
fice, . . . ), machine translation, information re-
trieval, optical character recognition . . . Automatic
error correction tools on Arabic are underperform-
ing in comparison with other languages like En-
glish or French. This can be explained by the lack
of appropriate resources (e.g. publicly available
corpora and tools) and the complexity of the Ara-
bic language. Arabic is a challenging language for
any NLP tool for many reasons. Arabic has a rich
and complex morphology compared to other latin
languages. Short vowels are missing in the texts
but are mandatory from a grammatical point of
view. Moreover they are needed to disambiguate
between several possibilities of words. Arabic
is a rich language.There are many synonyms and
Arabic is a highly agglutinative, inflectional and
derivational language and uses clitics (proclitics
and enclitics). Arabic has many varieties. Mod-
ern Standard Arabic includes the way Arabic is
written in the news or in formal speech. Classi-
cal Arabic refers to religious and classical texts.
Dialectal Arabic has no standard rules for orthog-
raphy and is based on the pronunciation. There-
fore a same word can be written using many differ-
ent surface forms depending on the dialectal ori-
gin of the writer. Another very popular way of
writing Arabic on the Internet and the social me-
dia like Facebook or Tweeter is to use ?Arabizi?, a
latinized form of writing Arabic using latin letters
and digits (Aboelezz, 2009).
For our participation in this evaluation task, we
tried to implement two different approaches. The
first approach is a lexicon driven spell checker. For
this, we have plan to adapt and test state-of-the-
art spell checkers. The second approach is a pure
statistical approach by considering the correction
problem as a statical machine translation task.
The paper is organized as follows: section 2
gives an overview of the automatic error correction
evaluation task and resources provided by the or-
ganizers; section 3 describes the systems we have
developed for the evaluations; and finally in sec-
tion 4 we discuss the results and draw some con-
clusion.
2 Task description and language
resources
The aim of the QALB Shared Task on Automatic
Arabic Error Correction (Mohit, 2014) is to evalu-
ate automatic text correction systems for the Ara-
bic language. The objective of the task is to cor-
rect automatically texts in Arabic provided by the
organizers. The QALB corpus is used for the eval-
uation task. A training set and a development set
with gold standard is provided for system train-
155
ing and development. The training and develop-
ment sets are made of sentences with errors com-
ing from newspapers articles and the gold stan-
dard is made of manual annotations of the sen-
tences. The annotations were made by human
annotators who used a correction guidelines de-
scribed in (Zaghouani, 2014). The corrections are
made of substitutions, insertions, deletions, splits,
merges, moves of words and punctuation marks.
The training set is made of 19,411 sentences and
1M tokens. The development set includes 1,017
sentences for around 53k tokens.
The evaluation is performed by comparing the
gold standard with the hypothesis using the Lev-
enshtein edit distance (Levenshtein, 1966) and
the implementation of the M2 scorer (Dahlmeier,
2012). Then for each sentence the Precision, Re-
call and F-measure are calculated.
Finally a test set of 968 sentences for 52k tokens
with no gold standard has to be corrected automat-
ically for the evaluation.
3 System description
For our participation in this evaluation campaign,
we studied two main approaches. The first one
is a lexical driven approach using dictionaries to
correct the errors. Different lexicons were evalu-
ated using Hunspell as spellchecking and correc-
tion tool.
The second approach is a statistical machine trans-
lation point of view by considering the automatic
error correction problem as a translation task. For
this we used the statistical machine translation sys-
tem Moses (Koehn, 2007), to train a model on the
training data provided by the organizers.
3.1 Baseline system
Since this the time first we are trying to develop
a spellchecker and correction tool for Arabic, we
wanted to have some figures about the perfor-
mance of spellcheckers on Arabic.
We used the development set to test the per-
formance of various spellchecker and correction
tools. We corrected the development set automati-
cally using the spellchecker module of the follow-
ing softwares:
? Microsoft Word 2013
? OpenOffice 2014
? Hunspell
For Microsoft Word and OpenOffice we used
the default configuration for correcting Arabic text
and disabled the grammar correction.
Hunspell is an open source spellchecker widely
used in the open source community. It is the
spellchecker of many well-known applications
such as OpenOffice, LibreOffice, Firefox, Thun-
derbird, Chrome, etc. It is the next generation of
lexical based spellcheckers in line with Myspell,
Ispell and Aspell. It is highly configurable, sup-
ports Unicode and rich morphology languages like
Arabic or Hungarian. Hunspell uses mainly two
files for spellchecking and correction. The first
one is a dictionary file *.dic which contains ba-
sically a wordlist and for each word, a list of ap-
plicable rules that can be applied to the word. The
second one is an affix file *.aff which contains a
list of possible affixes and the rules of application.
More information on these files can be found in
the Hunspell manual
1
.
Hunspell is an interactive spellchecker. It takes
as an input a text to be corrected and for each word
that is not found using the loaded dictionary and
affix files, it gives a list of suggestions to correct
the word. For the correction which must be fully
automatic, we forced Hunspell to always correct
the word with the first suggestion without any hu-
man intervention.
The dictionaries/affixes used for the evalua-
tion is coming from the Ayaspell project(Ayaspell,
2008). The dictionary contains 52 725 entries and
the affix file contains 11859 rules.
The results are given in Table 1
Dictionary Precision Recall F-measure
Word 45.7 16.6 24.3
Hunspell 51.8 18.8 27.6
OpenOffice 56.1 20.7 30.2
Table 1: Results on the development set for Word,
Hunspell/Ayaspell and OpenOffice(in percentage)
The best results are the ones obtained by
OpenOffice with a precision of 56.1%, a recall of
20.7% and a F-measure of 30.2%.
We would like to mention that these spellcheck-
ers do not correct the punctuations which may ex-
plain the relative low recall scores.
1
http://sourceforge.net/projects/hunspell/files/Hunspell/Documentation/
156
3.2 Statistical machine translation system
Our second approach is to consider the automatic
correction problem as a translation problem by
considering the sentences to be corrected as a
source language and the correct sentences as a tar-
get language. Since the organizers provided us
with a 1 million tokens corpora with and with-
out spelling errors, we tried to build a statisti-
cal machine translation system using the parallel
data. We used the Moses (Koehn, 2007), a Statis-
tical Machine Translation (SMT) system to train
a phrase based translation model with the train-
ing data. The training data provided is made of
erroneous sentences and for each sentence a list
of corrections to be applied. To build the paral-
lel error/correct text corpus we applied the correc-
tions to the sentences. We came up with a par-
allel corpus of 19421 sentences and 102k tokens
for the error version and 112k tokens for the cor-
rected version. Moses requires a parallel corpus
to train a translation model, a development set to
tune the translation model and also a monolingual
language model in the target language. Since we
had to evaluate the performance on the develop-
ment data provided by the organizers, we had to
use part of the training data as a development data
for Moses. So we split the 20k sentences included
in the training data in a new training set of 18k
and a new development data of 2k sentences. We
trained standard phrase based models using the
surface word form with no morphological analy-
sis or segmentation. For the word alignment in the
training process, we used GIZA++ (Och, 2003).
The 2k sentences were used to tune the SMT mod-
els.
Corpus # Sentences Usage
train18k 18000 train
dev-train2k 1411 dev
dev 1017 test
Table 2: Bitexts used for the SMT system
For the language models we used corpora of
newspapers publicly available or collected by
Techlimed. The sources are coming from the
Open Source Arabic Corpora (Saad, 2010) (20M
words), the Adjir corpus (Adjir, 2005) (147M
words) and other corpora we collected from var-
ious online newspapers for a total of 300M
words. The language model was created with the
IRSTLM toolkit (Federico, 2008).
We evaluated the translation models on the de-
velopment set using different sizes of monolin-
gual corpus. The 3 systems were trained on the
same parallel corpus but with different size for fir
monolingual data for System100, System200 and
System300 with respectively 100M words, 200M
words and 300M words. The results are given in
table 3.
System Precision Recall F-measure
System100 70.7 48.8 57.8
System200 70.7 49.6 58.3
System300 70.8 50.1 58.7
Table 3: Results on the development set (in per-
centage) for the 3 SMT systems
We can see from table 3 that the size of the lan-
guage model has no impact on the precision but
increases slightly the recall of 1.3% in absolute
(2.6% in relative).
The BLEU scores (Papineni, 2002) measured
on Sytem100, System200, System300 are respec-
tively 65.45, 65.82 and 65.98.
We also tried to combine Hunspell/Ayaspell
with the SMT system by correcting the output of
the SMT system with Hunspell/Ayaspell but didn?t
get any improvement.
4 Discussion
The results obtained by the SMT system is much
more better than the ones obtained with Hun-
spell/Ayaspell with a F-measure of 58.7% for the
best SMT system and 27,6 for Hunspell/Ayaspell.
We have to mention that the training corpus pro-
vided by the organizers of 1 million words with
the manual annotations enabled us to train a statis-
tical system that learn automatically the correction
made by the annotators while Hunspell/Ayaspell
was not adapted to the correction guidelines. In
particular the punctuations are not corrected by
Hunspell/Ayaspell and this explains the difference
of recall between the SMT system (50.1%) and
Hunspell/Ayaspell (20.7%). If we have a look at
the gold standard of the development set, 38.6%
of the manual annotations concern punctuation
marks with 6266 punctuation marks annotations
for an overall total of 16,231 annotations. While
there are clear rules for strong punctuation marks
like period, question or exclamation marks, there
are no clear grammatical rules for the weak punc-
tuation marks, especially for commas which con-
157
cern 4,117 annotations of the gold standard of
the development set (25.4%). Another point that
we would like to mention is that a spell checker
and correction tool is usually used in an inter-
active mode by proposing n-best candidates for
the correction of a word. When looking at Hun-
spell/Ayspell correction candidates for an error,
we saw the correction was not in position 1 but
in the list of candidates. So it would be interesting
to compare the correction on the n-best candidates
and not only on the first candidate for Hunspell
and the SMT system.
5 Conclusion
This paper has reported on the participation of
Techlimed in the QALB Shared Task on Auto-
matic Arabic Error Correction. This is the first
time we tried to develop a spellchecker for Arabic
and have investigated two approaches. The first
one is a lexicon driven approach using Hunspell as
a spellchecker and correction tool and the second
one is a SMT systems using Moses for training a
statistical machine translation model on the 1 mil-
lion tokens corpus provided by the organizers. The
best results were obtained with the SMT system
which, especially, was able to deal with the punc-
tuation marks corrections. We also tested an hy-
brid system by combining Hunspell and the SMT
system but didn?t get better results than the SMT
system alone. Our perspective is to improve the
results by using hybrid systems based on the Di-
iNAR lexical database (Abbes, 2004) and also a
large arabic named entity dictionary, both owned
and developped by Techlimed We will also try to
used factored translation models with the Tech-
limed Part-Of-Speech taggers. And more training
data will also improve the quality of the correc-
tions.
Acknowledgments
We would like to thank the QALB Shared Task or-
ganizers for setting up this evaluation campaign on
automatic error correction tool for Arabic and for
providing us with the language resources and tools
that we used for the development of our systems.
References
Ramzi Abb`es, Joseph Dichy, and Mohamed Hassoun.
2004. The architecture of a standard arabic lexical
database: some figures, ratios and categories from
the Diinar. 1 source program. In Proceedings of the
Workshop on Computational Approaches to Arabic
Script-based Languages, pages 15?22. Association
for Computational Linguistics, 2004.
Mariam Aboelezz. 2009. Latinised arabic and connec-
tions to bilingual ability. In Papers from the Lan-
caster University Postgraduate Conference in Lin-
guistics and Language Teaching, 2009.
Ahmed Abdelali. 2005. http://aracorpus.e3rab.com/
Ayaspell Arabic dictionary project, 2008.
http://ayaspell.sourceforge.net
Daniel Dahlmeier and Hwee Tou Ng. 2012. Bet-
ter evaluation for grammatical error correction. In
Proceedings of the 2012 Conference of the North
American Chapter of the Association for Computa-
tional Linguistics: Human Language Technologies,
pages 568?572. Association for Computational Lin-
guistics, 2012.
Marcello Federico, Nicola Bertoldi, and Mauro Cet-
tolo. 2008. Irstlm: an open source toolkit for han-
dling large scale language models. In Interspeech,
pages 1618?1621, 2008.
Hunspell, 2007. http://hunspell.sourceforge.net/
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran,
Richard Zens, et al. 2007. Moses: Open source
toolkit for statistical machine translation. In Pro-
ceedings of the 45th Annual Meeting of the ACL
on Interactive Poster and Demonstration Sessions,
pages 177?180. Association for Computational Lin-
guistics, 2007.
Vladimir Levenshtein. 1966. Binary codes capable of
correcting deletions, insertions and reversals. In So-
viet physics doklady, volume 10, page 707, 1966.
Behrang Mohit, Alla Rozovskaya, Nizar Habash, Wa-
jdi Zaghouani, and Ossama Obeid. 2014. The First
QALB Shared Task on Automatic Text Correction
for Arabic. In Proceedings of EMNLP Workshop on
Arabic Natural Language Processing, Doha, Qatar,
October 2014.
Franz Josef Och and Hermann Ney. 2003. A sys-
tematic comparison of various statistical alignment
models. Computational Linguistics, 29(1):19?51,
2003.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: a method for automatic
evaluation of machine translation. In Proceedings of
the 40th annual meeting on association for compu-
tational linguistics, pages 311?318. Association for
Computational Linguistics, 2002.
Motaz K Saad and Wesam Ashour. 2010 Osac: Open
source arabic corpora. In 6th ArchEng Int. Sympo-
siums, EEECS, volume 10, 2010.
158
Wajdi Zaghouani, Behrang Mohit, Nizar Habash, Os-
sama Obeid, Nadi Tomeh, Alla Rozovskaya, Noura
Farra, Sarah Alkuhlani, and Kemal Oflazer. 2014.
Large scale arabic error annotation: Guidelines and
framework. In Proceedings of the Ninth Inter-
national Conference on Language Resources and
Evaluation (LREC?14), Reykjavik, Iceland, May
2014. European Language Resources Association
(ELRA).
159
