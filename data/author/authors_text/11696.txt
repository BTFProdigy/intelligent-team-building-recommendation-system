Proceedings of NAACL HLT 2009: Short Papers, pages 145?148,
Boulder, Colorado, June 2009. c?2009 Association for Computational Linguistics
Evaluating the Syntactic Transformations in Gold Standard Corpora  for Statistical Sentence Compression   Naman K. Gupta, Sourish Chaudhuri, Carolyn P. Ros? Language Technologies Institute Carnegie Mellon University Pittsburgh, PA 15213, USA {nkgupta,sourishc,cprose}@cs.cmu.edu   Abstract 
We present a policy-based error analysis approach that demonstrates a limitation to the current commonly adopted paradigm for sentence compression.  We demon-strate that these limitations arise from the strong assumption of locality of the deci-sion making process in the search for an acceptable derivation in this paradigm. 1 Introduction In this paper we present a policy-based error analy-sis approach that demonstrates a limitation to the current commonly adopted paradigm for sentence compression (Knight and Marcu, 2000; Turner and Charniak, 2005; McDonald, 2006; Clark and La-pata 2006).       Specifically, in typical statistical compression approaches, a simplifying assumption is made that compression is accomplished strictly by means of word deletion. Furthermore, each sequence of con-tiguous words that are dropped from a source sen-tence is considered independently of other sequences of words dropped from other portions of the sentence, so that the features that predict whether deleting a sequence of words is preferred or not is based solely on local considerations.  This simplistic approach allows all possible derivations to be modeled and decoded efficiently within the search space, using a dynamic programming algo-rithm.       In theory, it should be possible to learn how to generate effective compressions using a corpus of source-target sentence pairs, given enough exam-ples and sufficiently expressive features.  How-ever, our analysis casts doubt that this framework 
with its strong assumptions of locality is suffi-ciently powerful to learn the types of example compressions frequently found in corpora of hu-man generated gold standard compressions regard-less of how expressive the features are.     Work in sentence compression has been some-what hampered by the tremendous cost involved in producing a gold standard corpus.  Because of this tremendous cost, the same gold standard corpora are used in many different published studies almost as a black box.  This is done with little scrutiny of the limitations on the learnability of the desired target systems. These limitations result from in-consistencies due to the subtleties in the process by which humans generate the gold standard compres-sions from the source sentences, and from the strong locality assumptions inherent in the frame-works.    Typically, the humans who have participated in the construction of these corpora have been in-structed to preserve grammaticality and to produce compressions by deletion.  Human ratings of the gold standard compressions by separate judges confirm that the human developers have literally followed the instructions, and have produced com-pressions that are themselves largely grammatical.  Nevertheless, what we demonstrate with our error analysis is that they have used meaning preserving transformation that didn't consistently preserve the grammatical relations from the source sentence while transforming source sentences into target sentences.  This places limitations on how well the preferred patterns of compression can be learned using the current paradigm and existing corpora.     In the remainder of the paper, we discuss rele-vant work in sentence compression.  We then in-troduce our policy-based error analysis technique.  Next we discuss the error analysis itself and the conclusions we draw from it.  Finally, we conclude 
145
with future directions for broader application of this error analysis technique. 2 Related Work  Knight and Marcu (2000) present two approaches to the sentence compression problem- one using a noisy channel model and the other using a deci-sion-based model. Subsequent work (McDonald, 2006) has demonstrated an advantage for a soft constraint approach, where a discriminative model learns to make local decisions about dropping a sequence of words from the source sentence in or-der to produce the target compression.  Features in this system are defined over pairs of words in the source sentence, with the idea that the pair of words would appear adjacent in the resulting com-pression, with all intervening words dropped.  Thus, the features represent this transformation, and the feature weights are meant to indicate whether the transformation is associated with good compressions or not.      We use McDonald?s (2006) proposed model as a foundation for our work because its soft constraint approach allows for natural integration of a variety of classes of features, even overlapping features.  In our prior work we have explored the potential for improving the performance of a compression system by including additional, more sophisticated syntactically motivated features than those in-cluded in previously published models.  In this pa-per, we evaluate the gold standard corpus itself using similar syntactic grammar policies. 3 Grammar Policy Extraction In the domain of Sentence Compression, the cor-pus consists of source sentences each paired with a gold standard compressed sentence. Most of the above related work has been evaluated using the following 2 corpora, namely the Ziff-Davis (ZD) set (Knight and Marcu, 2002) consisting of 1055 sentences, and a partial Broadcast News Corpus (CL Corpus) (Clarke and Lapata, 2006) originally consisting of 1619 sentences, of which we used 1070 as the training set in our development work as well as in the error analysis below. Hence, we use these two popular corpora to present our work. We hypothesize certain grammar policies that in-tuitively should be followed while deriving the target-compressed sentence from the source sen-
tence if the mapping between source and target sentences is produced via grammatical transforma-tions. The basic idea behind these policies grows out of the same ideas motivating the syntactic fea-tures used in McDonald (2006). These policies, extracted using the MST (McDonald, 2005) de-pendency parse structure of the source sentence, are as follows:  1. The syntactic root word of a sentence should be retained in the compressed sen-tence. 2.  If a verb is retained in the compressed sentence, then the dependent subject of that verb should also be retained. 3. If a verb is retained in the compressed sen-tence, then the dependent object of that verb should also be retained. 4. If the verb is dropped in the compressed sentence then its arguments, namely sub-ject, object, prepositional phrases etc., should also be dropped. 5. If the Preposition in a Prepositional phrase (PP) is retained in the compressed sen-tence, then the dependent Noun Phrase (NP) of that Preposition should also be re-tained. 6. If the head noun of a Noun phrase (NP) within a Prepositional phrase is retained in the compressed sentence, then the syntac-tic parent Preposition of the NP should also be retained. 7. If a Preposition, the syntactic head of a Prepositional phrase (PP), is dropped in the compressed sentence, then the whole PP, including dependent Noun phrase in that PP, should also be dropped. 8. If the head noun of a Noun phrase within a Prepositional phrase (PP) is dropped in the compressed sentence, then the syntactic parent Preposition of the PP should also be dropped.  These grammar policies make predictions about where, in the phrase structure, constituents are likely to be dropped or retained in the compres-sion.  Thus, these policies have similar motivation to the syntactic features in the McDonald (2006) model. However, there is a fundamental difference in the way these policies are computed. In the McDonald (2006) model, the features are com-
146
puted locally over adjacent words yi-1 & yi in the compression and the words dropped from the original sentence between that word range yi-1 & yi. In cases where the syntactic structure of the in-volved words extends beyond this range, the ex-tracted features are not able to capture all of the relevant syntactic dependencies. On the other hand, in our analysis the policies are computed globally over the complete sentence without specifying any range of words. As an illustrative example, let us consider the following sentence from the CL Cor-pus (bold represents dropped words):  1. The1 leaflet2 given3 to4 Labour5 activists6 mentions7 none8 of9 these10 things11.  According to Policy 2, since the verb 'mentions' is retained, the subject of the verb ?the leaflet? should also be retained. In the McDonald (2006) model, by looking at the local range yi-1 = 5 and yi = 7 for the verb 'mentions', we will not be able to compute whether the subject(1,2) was retained in the compression or not. So this policy can be cap-tured only if the global context is taken into ac-count while evaluating the verb 'mentions'. Now we evaluate each sentence in the corpus to determine whether a particular policy was applica-ble and if applicable then whether it was violated. Table 1 shows the summary of the evaluation of all the sentences in the two corpora. Column 2 in the table shows the percentage of sentences in the ZD Corpus where the respective policies were applica-ble. And column 3 shows the percentage of sen-tences where the respective policies were violated, whenever applicable. Columns 4 and 5 show re-spective percentages for the CL corpus. 4 Evaluation In this section we discuss the results from evaluat-ing the 8 grammar policies discussed in Section 3 over the ZD and CL corpora, as discussed above.   The policies were evaluated with respect to whether they applied in a sentence, i.e., whether the premise of the ?if ? then? rule is true in the sentence, and whether the policy was broken when applied, i.e., if the premise is true but the conse-quent is false.  The striking finding is that for every one of the policies discussed in the previous sec-tion, they are violated for at least 10% of the sen-tences where they applied, and sometimes as much as 72%.  For most policies, the proportion of sen-tences where the policy is violated when applied is 
a minority of cases.  Thus, based on this, we can expect that grammar oriented features motivated by these policies and derived from a syntactic analysis of the source and/or target sentences in the gold standard could be used to improve the per-formance of compression systems that don?t make use of syntactic information to that extent.  How-ever, the noticeable proportion of violations with respect to some of the policies indicate that there is a limited extent to which these types of features can contribute towards improved performance. One observation we make from Table 1 is that while the proportion of sentences where the poli-cies (Columns 2 and 4) apply as well as the propor-tion of sentences where the policies are broken when applied (Columns 3 and 5) are highly corre-lated between the two corpora.  Nevertheless, the distributions are not identical. Thus, again, while we predict that using this style of dependency syn-tax features might improve performance of com-pression systems within a single corpus, we would not expect trained models that rely on these syntac-tic dependency features to generalize in an ideal way between corpora.   ZD (%  Appli-cable) ZD (% Viola-tions when Appli-cable) 
CL (%  Appli-cable) CL (%  Viola-tions when Appli-cable) Policy1 100% 34% 100% 14% Policy2 66% 18% 84% 18% Policy3 50% 10% 61% 24% Policy4 59% 59% 46% 72% Policy5 62% 17% 77% 27% Policy6 65% 22% 79% 29% Policy7 57% 25% 58% 40% Policy8 55% 16% 58% 36% Table 1: Summary of evaluation of grammar policies over the Ziff-Davis (ZD) training set and Clark-Lapata (CL) training set.  Beyond the above evaluation illustrating the extent to which grammar inspired policies are violated in human generated gold standard corpora, interesting insights into challenges that must be addressed in order to improve performance can be obtained by taking a close look at typical examples from the CL corpus where the policies are broken in the 
147
gold standard corpora (bold represents dropped words).  1. The attempt to put flesh and blood on the skeleton structure of a possible united Europe emerged. 2. Annely has used the gallery ?s three floors to divide the exhibits into three dis-tinct groups. 3. Labor has said it will scrap the system. 4. Montenegro ?s sudden rehabilitation of Nicholas ?s memory is a popular move.  In Sentence 1, retaining the dependent Noun struc-ture of the dropped Preposition on in the PP vio-lates Policy 7. Such a NP to Infinitive Phrase transformation changes the syntactic structure of the sentence. Sentence 2 also breaks several poli-cies, namely Policies 1, 4 and 7. The syntactic root has is dropped. Also the main verb has used is dropped while retaining the Subject Annely. In Sentence 3, breaking Policies 1, 2 and 4, the hu-man annotators replaced the pronoun it with the noun Labor, the subject of a dropped verb ?has said?. Such anaphora resolution cannot be done without relevant context, which is not available in strictly local paradigms of sentence compression. In Sentence 4, policies 3. 5 and 8 are violated. Transformations like substituting Nicholas?s mem-ory by the metonym Nicholas and popular move by popular need to be identified and analyzed. Such varied transformations, made in the syntactic struc-ture of the sentences by human annotators, are counter-intuitive, making them hard to be captured in the linear models learned in association with the syntactic features in current compression systems. 5 Conclusions and Current Directions In this paper we have introduced a policy-based error analysis technique that was used to investi-gate the potential impact and limitations of adding a particular style of dependency parse features to typical statistical compression systems.  We have argued that the reason for the limitation arises from the strong assumption of the local nature of the decisions that are made in obtaining the system-generated compression from a source sentence.       Other related technologies such as statistical machine translation and statistical paraphrase are based on similar paradigms with similar assump-
tions of the local nature of decisions that are made in the search for an acceptable derivation.  We con-jecture both that it is likely that the same issues related to the construction of the gold standard corpora likely apply and that a similar policy-based error analysis approach could be used in order to assess the extent to which this is true and identify possible directions for improving performance.  In our ongoing work, we plan to conduct a similar error analysis for these problems in order to evalu-ate the generality of the findings reported here.   Acknowledgments This work was funded in part by the Office of Na-val Research grant number N00014510043. References  James Clarke and Mirella Lapata. 2006. Constraint-Based Sentence Compression: An Integer Program-ming Approach. Proceedings of the COLING/ACL 2006 Main Conference Poster Sessions (ACL-2006), pages 144-151, 2006. James Clarke and Mirella Lapata. 2006. Models for Sen-tence Compression: A Comparison across Domains, Training Requirements and Evaluation Measures.  Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the Association for Computational Linguistics, 377-384. Sydney, Australia. Kevin Knight and Daniel Marcu. 2000. Statistics-Based Summarization ? Step One: Sentence Compression. Proceedings of AAAI-2000, Austin, TX, USA. Knight, Kevin and Daniel Marcu. 2002. Summarization beyond sentence extraction: a probabilistic approach to sentence compression. Artificial Intelligence 139(1):91?107. Ryan McDonald, Koby Crammer, and Fernando Pereira. 2005. Online large-margin training of de-pendency parsers. Proc. ACL. Ryan Mcdonald, 2006. Discriminative sentence com-pression with soft syntactic constraints. Proceedings of the 11th EACL. Trento, Italy, pages 297--304. Jenine Turner and Eugene Charniak. 2005. Supervised and unsupervised learning for sentence compression. Proc. ACL. 
148
Proceedings of the ACL-IJCNLP 2009 Conference Short Papers, pages 101?104,
Suntec, Singapore, 4 August 2009. c?2009 ACL and AFNLP
Leveraging Structural Relations for Fluent Compressions                    
at Multiple Compression Rates 
 
Sourish Chaudhuri, Naman K. Gupta, Noah A. Smith, Carolyn P. Ros? 
Language Technologies Institute, Carnegie Mellon University, Pittsburgh, PA-15213, USA. 
{sourishc, nkgupta, nasmith, cprose}@cs.cmu.edu 
 
Abstract 
Prior approaches to sentence compression 
have taken low level syntactic constraints into 
account in order to maintain grammaticality. 
We propose and successfully evaluate a more 
comprehensive, generalizable feature set that 
takes syntactic and structural relationships into 
account in order to sustain variable compres-
sion rates while making compressed sentences 
more coherent, grammatical and readable.  
1 Introduction 
We present an evaluation of the effect of syntac-
tic and structural constraints at multiple levels of 
granularity on the robustness of sentence com-
pression at varying compression rates.  Our eval-
uation demonstrates that the new feature set pro-
duces significantly improved compressions 
across a range of compression rates compared to 
existing state-of-the-art approaches. Thus, we 
name our system for generating compressions the 
Adjustable Rate Compressor (ARC).   
Knight and Marcu (2000) (K&M, henceforth) 
presented two approaches to the sentence com-
pression problem: one using a noisy channel 
model, the other using a decision-based model. 
The performances of the two models were com-
parable though their experiments suggested that 
the noisy channel model degraded more smooth-
ly than the decision-based model when tested on 
out-of-domain data. Riezler et al (2003) applied 
linguistically rich LFG grammars to a sentence 
compression system. Turner and Charniak (2005) 
achieved similar performance to K&M using an 
unsupervised approach that induced rules from 
the Penn Treebank.  
A variety of feature encodings have previous-
ly been explored for the problem of sentence 
compression.  Clarke and Lapata (2007) included 
discourse level features in their framework to 
leverage context for enhancing coherence. 
McDonald?s (2006) model (M06, henceforth) is 
similar to K&M except that it uses discriminative 
online learning to train feature weights. A key 
aspect of the M06 approach is a decoding algo-
rithm that searches the entire space of compres-
sions using dynamic programming to choose the 
best compression (details in Section 2). We use 
M06 as a foundation for this work because its 
soft constraint approach allows for natural inte-
gration of additional classes of features. Similar 
to most previous approaches, our approach com-
presses sentences by deleting words only. 
The remainder of the paper is organized as 
follows. Section 2 discusses the architectural 
framework.  Section 3 describes the innovations 
in the proposed model. We conclude after pre-
senting the results of our evaluation in Section 4. 
2 Experimental Paradigm 
Supervised approaches to sentence compression 
typically use parallel corpora consisting of origi-
nal and compressed sentences (paired corpus, 
henceforth). In this paper, we will refer to these 
pairs as a 2-tuple <x, y>, where x is the original 
sentence and y is the compressed sentence. 
We implemented the M06 system as an expe-
rimental framework in which to conduct our in-
vestigation. The system uses as input the paired 
corpus, the corresponding POS tagged corpus, 
the paired corpus parsed using the Charniak 
parser (Charniak, 2000), and dependency parses 
from the MST parser (McDonald et al, 2005). 
Features are extracted over adjacent pairs of 
words in the compressed sentence and weights 
are learnt at training time using the MIRA algo-
rithm (Crammer and Singer, 2003). We decode 
as follows to find the best compression:  
Let the score of a compression y for a sen-
tence x be s(x, y). This score is factored using a 
first-order Markov assumption over the words in 
the compressed sentence, and is defined by the 
dot product between a high dimensional feature 
representation and a corresponding weight vector 
(for details, refer to McDonald, 2006). The equa-
tions for decoding are as follows: 
 
1),,,(][max][
0.0]1[
iijxsjCiC
C
ij
 
101
where C is the dynamic programming table and 
C[i] represents the highest score for compres-
sions ending at word i for the sentence x. 
The M06 system takes the best scoring com-
pression from the set of all possible compres-
sions.  In the ARC system, the model determines 
the compression rate and enforces a target com-
pression length by altering the dynamic pro-
gramming algorithm as suggested by M06: 
 
1,]][1[
0.0]1][1[
rrC
C  
,1i  
),,(]1][[max]][[ ijxsrjCriC ij
 
 
where C is the dynamic programming table as 
before and C[i][r] is the score for the best com-
pression of length r that ends at position i in the 
sentence x. This algorithm runs in O (n2r) time.  
We define the rate of human generated com-
pressions in the training corpus as the gold stan-
dard compression rate (GSCR). We train a linear 
regression model over the training data to predict 
the GSCR for a sentence based on the ratio be-
tween the lengths of each compressed-original 
sentence pair in the training set. The predicted 
compression rate is used to force the system to 
compress sentences in the test set to a specific 
target length. Based on the computed regression, 
the formula for computing the Predicted Com-
pression Rate (PCR) from the Original Sentence 
Length (OSL) is as follows: 
 
OSLPCR 004.086.0  
 
In our work, enforcing specific compression 
rates serves two purposes. First, it allows us to 
make a more controlled comparison across ap-
proaches, since variation in compression rate 
across approaches confounds comparison of oth-
er aspects of performance.  Second, it allows us 
to investigate how alternative models work at 
higher compression rates. Here our primary con-
tribution is of robustness of the approach with 
respect to alternative feature spaces and com-
pression rates. 
3 Extended Feature Set 
A major focus of our work is the inclusion of 
new types of features derived from syntactic ana-
lyses in order to make the resulting compressions 
more grammatical and thus increase the versatili-
ty of the resulting compression models.   
The M06 system uses features extracted from 
the POS tagged paired corpus: POS bigrams, 
POS context of the words added to or dropped 
from the compression, and other information 
about the dropped words. For a more detailed 
description, please refer to McDonald, 2006.   
From the phrase structure trees, M06 extracts 
context information about nodes that subsume 
dropped words. These features attempt to ap-
proximately encode changes in the grammar 
rules between source and target sentences. De-
pendency features include information about the 
dropped words? parents as well as conjunction 
features of the word and the parent. 
Our extensions to the M06 feature set are in-
spired by an analysis of the compressions gener-
ated by it, and allow for a richer encoding of 
dropped words and phrases using properties of 
the words and their syntactic relations to the rest 
of the sentence. Consider this example (dropped 
words are marked as such):  
 
* 68000 Sweden AB of Uppsala , Sweden , intro-
duced the TeleServe , an integrated answering 
machine and voice-message handler that links a 
Macintosh to Touch-Tone phones . 
  
Note in the above example that the syntactic 
head of the sentence introduced has been 
dropped. Using the dependency parse, we add a 
class of features to be learned during training that 
lets the system decide when to drop the syntactic 
head of the sentence. Also note that answering 
machine in the original sentence was preceded 
by an while the word the was used with Tele-
serve (dropped in the compression). While POS 
information helps the system to learn that the 
answering machine is a good POS sequence, we 
do not have information that links the correct 
article to the noun. Information from the depen-
dency parse allows us to learn when we can drop 
words whose heads are retained and when we 
can drop a head and still retain the dependent.  
Now, consider the following example: 
 
Examples for editors are applicable to awk pat-
terns , grep and egrep .  
 
    Here, Examples has been dropped, while for 
editors which has Examples as a head is retained. 
Besides, in the sequence, editors are applica-
ble?, the word editors behaves as the subject of 
are although the correct compression would have 
examples as its subject. A change in the argu-
ments of the verbs will distort the meaning of the 
sentence. We augmented the feature set to in-
clude a class of features about structural informa-
tion that tells us when the subject (or object) of a 
verb can be dropped while the verb itself is re-
tained. Thus, now if the system does retain the 
102
are, it is more likely to retain the correct argu-
ments of the word from the original sentence. 
    The new classes of features use only the de-
pendency labels generated by the parser and are 
not lexicalized. Intuitively, these features help 
create units within the sentences that are tightly 
bound together, e.g., a subject and an object with 
its parent verb. We notice, as one would expect, 
that some dependency bindings are less strong 
than others. For instance, when faced with a 
choice, our system drops a relative pronoun thus 
breaking the dependency between the retained 
noun and the relative pronoun, rather than drop 
the noun, which was the retained subject. 
Below is a summary of the information that 
the new features in our system encode: 
[Parent-Child]- When a word is dropped, is its 
parent retained in the compression?  
[Dependent]- When a word is dropped, are 
other words dependent on it (its children) 
also dropped or are they retained?  
[Verb-Arg]- Information from the dependency 
parse about the subjects and objects of 
verbs can be used to encode more specific 
features (similar to the above) that say 
whether or not the subject (or object) was 
retained when the verb was dropped.  
[Sent-Head-Dep]- Is the syntactic head of a 
sentence dropped? 
4 Evaluation 
We evaluate our model in comparison with M06. 
At training time, compression rates were not en-
forced on the ARC or M06 model. Our evalua-
tion demonstrates that the proposed feature set 
produces more grammatical sentences across 
varying compression rates.  In this section, 
GSCR denotes gold standard compression rate 
(i.e., the compression rate found in training data), 
CR denotes compression rate.   
4.1 Corpora 
Sentence compression systems have been tested 
on product review data from the Ziff-Davis (ZD, 
henceforth) Corpus by Knight and Marcu (2000), 
general news articles by Clarke and Lapata (CL, 
henceforth) corpus (2007) and biomedical ar-
ticles (Lin and Wilbur, 2007). To evaluate our 
system, we used 2 test sets: Set 1 contained 50 
sentences; all 32 sentences from the ZD test set 
and 18 additional sentences chosen randomly 
from the CL test set; Set 2 contained 40 sen-
tences selected from the CL corpus, 20 of which 
were compressed at 75% of GSCR and 20 at 
50% of GSCR (the percentages denote the en-
forced compression rates). 
Three examples comparing compressed sen-
tences are given below:  
 
 
Original: Like FaceLift, much of ATM 's screen 
performance depends on the underlying applica-
tion. 
Human: Much of ATM 's performance depends 
on the underlying application . 
M06: 's screen performance depends on applica-
tion  
ARC: ATM 's screen performance depends on 
the underlying application . 
 
Original: The discounted package for the Sparc-
server 470 is priced at $89,900 , down from the 
regular $107,795 . 
Human: The Sparcserver 470 is priced at 
$89,900 , down from the regular $107,795 . 
M06: Sparcserver 470 is $89,900 regular 
$107,795 
ARC: The discounted package is priced at 
$89,900 , regular $107,795 .  
 
 
The example below has compressions at 50% 
compression rate for M06 and ARC systems: 
 
 
Original: Cutbacks in local defence establish-
ments is also a factor in some constituencies . 
M06: establishments is a factor in some consti-
tuencies . 
ARC: Cutbacks is a factor in some constituen-
cies .  
 
 
Note that the subject of is is correctly retained 
in the ARC system. 
4.2 User Study 
In order to evaluate the effect of the features that 
we added to create the ARC model, we con-
ducted a user study, adopting an experimental 
methodology similar to that used by K&M and 
M06.  Each of four human judges, who were na-
tive speakers of English and not involved in the 
research we report in this paper, were instructed 
to rate two different sets of compressions along 
two dimensions, namely Grammaticality and 
Completeness, on a scale of 1 to 5. We chose to 
replace Importance (used by K&M), which is a 
task specific and possibly user specific notion, 
with the more general notion of Completeness, 
defined as the extent to which the compressed 
sentence is a complete sentence and communi-
cates the main idea of the original sentence.  
For Set 1, raters were given the original sen-
tence and 4 compressed versions (presented in 
103
random order as in the M06 evaluation): the hu-
man compression, the compression produced by 
the original M06 system, the compression from 
the M06 system with GSCR, and the ARC sys-
tem with GSCR. For Set 2, raters were given the 
original sentence, this time with two compressed 
versions, one from the M06 system and one from 
the ARC system, which were presented in a ran-
dom order.  Table 1 presents all the results in 
terms of human ratings of Grammaticality and 
Completeness as well as automatically computed 
ROUGE F1 scores (Lin and Hovy, 2003). The 
scores in parentheses denote standard deviations. 
 
 Grammati-
cality 
(Human 
Scores) 
Com-
pleteness 
(Human 
Scores) 
 
ROUGE 
F1 
Gold 
Standard 
4.60 (0.69) 3.80(.99) 1.00 (0) 
ARC 
(GSCR) 
3.70 (1.10) 3.50(1.10) .72 (.18) 
M06 3.50 (1.30) 3.10(1.30) .70 (.20) 
M06 
(GSCR) 
3.10 (1.10) 3.10(1.10) .71 (.18) 
ARC 
(75%CR) 
2.60 (1.10) 2.60(1.10) .72 (.14) 
M06 
(75%CR) 
2.20 (1.20) 2.00(1.00) .67 (.20) 
ARC 
(50%CR) 
2.30 (1.30) 1.90(1.00) .54 (.22) 
M06 
(50%CR) 
1.90 (1.10) 1.80(1.00) .58 (.22) 
Table 1: Results of human judgments and ROUGE F1 
 
 ROUGE scores were determined to have a 
significant positive correlation both with Gram-
maticality (R = .46, p < .0001) and Completeness 
(R = .39, p < .0001) when averaging across the 4 
judges? ratings.  On Set 1, a 2-tailed paired t-test 
reveals similar patterns for Grammaticality and 
Completeness: the human compressions are sig-
nificantly better than any of the systems.  ARC is 
significantly better than M06, both with enforced 
GSCR and without. M06 without GSCR is sig-
nificantly better than M06 with GSCR.  In Set 2 
(with 75% and 50% GSCR enforced), the quality 
of compressions degrade as compression rate is 
made more severe; however, the ARC model 
consistently outperforms the M06 model with a 
statistically significant margin across compres-
sion rates on both evaluation criteria. 
5 Conclusions and Future Work 
In this paper, we designed a set of new classes of 
features to generate better compressions, and 
they were found to produce statistically signifi-
cant improvements over the state-of-the-art. 
However, although the user study demonstrates 
the expected positive impact of grammatical fea-
tures, an error analysis (Gupta et al, 2009) re-
veals some limitations to improvements that can 
be obtained using grammatical features that refer 
only to the source sentence structure, since the 
syntax of the source sentence is frequently not 
preserved in the gold standard compression. In 
our future work, we hope to explore alternative 
approaches that allow reordering or paraphrasing 
along with deleting words to make compressed 
sentences more grammatical and coherent. 
 
Acknowledgments 
The authors thank Kevin Knight and Daniel 
Marcu for sharing the Ziff-Davis corpus as well 
as the output of their systems, and the anonym-
ous reviewers for their comments. This work was 
supported by the Cognitive and Neural Sciences 
Division, grant number N00014-00-1-0600. 
References  
Eugene Charniak. 2000. A maximum-entropy-
inspired parser. In Proc. of  NAACL. 
James Clarke and Mirella Lapata, 2007. Modelling 
Compression With Discourse Constraints. In Proc. 
of EMNLP-CoNLL. 
Koby Crammer and Y. Singer. 2003. Ultraconserva-
tive online algorithms for multi-class problems. 
JMLR. 
Naman K. Gupta, Sourish Chaudhuri and Carolyn P. 
Ros?, 2009. Evaluating the Syntactic Transforma-
tions in Gold Standard Corpora for Statistical Sen-
tence Compression . In Proc. of HLT-NAACL. 
Kevin Knight and Daniel Marcu. 2000. Statistics-
Based Summarization ? Step One: Sentence Com-
pression. In Proc. of AAAI. 
Jimmy Lin and W. John Wilbur. 2007. Syntactic sen-
tence compression in the biomedical domain: faci-
litating access to related articles. Information Re-
trieval, 10(4):393-414. 
Chin-Yew Lin and Eduard H. Hovy 2003. Automatic 
Evaluation of Summaries Using N-gram Co-
occurrence Statistics. In Proc. of HLT-NAACL. 
Ryan McDonald, 2006. Discriminative sentence com-
pression with soft syntactic constraints. In Proc. of 
EACL.  
Ryan McDonald, Koby Crammer, and Fernando Pe-
reira. 2005. Online large-margin training of depen-
dency parsers. In Proc.of ACL. 
S. Riezler, T. H. King, R. Crouch, and A. Zaenen.  
2003. Statistical sentence condensation using am-
biguity packing and stochastic disambiguation me-
thods for lexical-functional grammar. In Proc. of 
HLT-NAACL. 
104
