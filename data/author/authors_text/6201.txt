Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational
Natural Language Learning, pp. 400?409, Prague, June 2007. c?2007 Association for Computational Linguistics
Flexible, Corpus-Based Modelling of Human Plausibility Judgements
Sebastian Pad? and Ulrike Pad?
Computational Linguistics
Saarland University
Saarbr?cken, Germany
{pado,ulrike}@coli.uni-sb.de
Katrin Erk
Dept. of Linguistics
University of Texas at Austin
Austin, Texas
katrin.erk@mail.utexas.edu
Abstract
In this paper, we consider the computational
modelling of human plausibility judgements
for verb-relation-argument triples, a task
equivalent to the computation of selectional
preferences. Such models have applications
both in psycholinguistics and in computa-
tional linguistics.
By extending a recent model, we obtain
a completely corpus-driven model for this
task which achieves significant correlations
with human judgements. It rivals or exceeds
deeper, resource-driven models while exhibit-
ing higher coverage. Moreover, we show that
our model can be combined with deeper mod-
els to obtain better predictions than from ei-
ther model alone.
1 Introduction
One fundamental and intuitive finding in experimen-
tal psycholinguistics is that humans judge the plau-
sibility of a verb-argument pair vastly differently de-
pending on the semantic relation in the pair. Table 1
lists example human judgements which McRae et
al. (1998) elicited by asking about the plausibility of,
e.g., a hunter shooting (relation agent) or being shot
(relation patient). McRae et al found that ?hunter? is
judged to be a very plausible agent of ?shoot? and
an implausible patient, while the reverse is true for
?deer?. In linguistics, this phenomenon is explained
by selectional preferences on verbs? argument po-
sitions; we use plausibility and fit with selectional
preferences interchangeably.
Verb Relation Noun Plausibility
shoot agent hunter 6.9
shoot patient hunter 2.8
shoot agent deer 1.0
shoot patient deer 6.4
Table 1: Verb-relation-noun triples with plausibility
judgements on a 7-point scale (McRae et al, 1998)
In this paper, we consider computational mod-
els that predict human plausibility ratings, or the
fit of selectional preferences and argument, for
such (verb, relation, argument), in short, (v, r, a),
triples. Being able to model this type of data is rel-
evant in a number of ways. From the point of view
of psycholinguistics, selectional preferences have an
important effect in human sentence processing (e.g.,
McRae et al (1998), Trueswell et al (1994)), and
models of selectional preferences are therefore nec-
essary to inform models of this process (Pad? et al,
2006). In computational linguistics, a multitude of
tasks is sensitive to selectional preferences, such as
the resolution of ambiguous attachments (Hindle and
Rooth, 1993), word sense disambiguation (McCarthy
and Carroll, 2003), semantic role labelling (Gildea
and Jurafsky, 2002), or testing the applicability of
inference rules (Pantel et al, 2007).
A number of approaches has been proposed to
model selectional preference data (Pad? et al, 2006;
Resnik, 1996; Clark and Weir, 2002; Abe and Li,
1996). These models generally operate by general-
ising from seen (v, r, a) triples to unseen ones. By
relying on resources like corpora with semantic role
annotation or the WordNet ontology, these models
400
generally share two problems: (a), limited coverage;
and (b), the resource (at least partially) predetermines
the generalisations that they can make.
In this paper, we investigate whether it is possi-
ble to predict the plausibility of (v, r, a) triples in
a completely corpus-driven way. We build on a re-
cent selectional preference model (Erk, 2007) that
bases its generalisations on word similarity in a vec-
tor space. While that model relies on corpora with
semantic role annotation, we show that it is possible
to predict plausibility ratings solely on the basis of a
parsed corpus, by using shallow cues and a suitable
vector space specification.
For evaluation, we use two balanced data sets of
human plausibility judgements, i.e., datasets where
each verb is paired both with a good agent and a good
patient, and where both nouns are presented in either
semantic relation (as in Table 1). Using balanced test
data is a particularly difficult task, since it forces
the models to account reliably both for the influence
of the semantic relation (agent/patient) and of the
argument head (?hunter?/?deer?).
We obtain three main results: (a), our model is able
to match the superior performance of the model pro-
posed by Pad? et al (2006), while retaining the high
coverage of the model proposed by Resnik (1996);
(b), using parsing as a preprocessing step improves
the model?s performance significantly; and (c), a com-
bination of our model with the Pad? model exceeds
both individual models in accuracy.
Plan of the paper. In Section 2, we give an
overview of existing selectional preferences and vec-
tor space models. Section 3 introduces our model and
discusses its parameters. Sections 4 and 5 present our
experimental setup and results. Section 6 concludes.
2 Related Work
Modelling Selectional Preferences with Gram-
matical Functions. The idea of inducing selec-
tional preferences from corpora was introduced by
Resnik (1996). He approximated the semantic verb-
argument relations in (v, r, a) triples by grammatical
functions, which are readily available for large train-
ing corpora. His basic two-step procedure was fol-
lowed by all later approaches: (1), extract argument
headwords for a given predicate and relation from
a corpus; (2), generalise to other, similar words us-
ing the WordNet noun hierarchy. Other models also
relying on the WordNet resource include Abe and
Li (1996) and Clark and Weir (2002).
We present Resnik?s model in some detail, since
we will use it for comparison below. Resnik first
computes the overall selectional preference strength
for each verb-relation pair, i.e. the degree of ?con-
strainedness? of each relation. This quantity is esti-
mated as the difference (in terms of the Kullback-
Leibler divergence D) between the distribution over
WordNet argument classes given the relation, p(c|r),
and the distribution of argument classes given the
current verb-relation combination, p(c|v, r). The in-
tuition is that a verb-relation pair that only allows
for a limited range of argument heads will have a
probability distribution over argument classes that
strongly diverges from the prior distribution.
Next, the selectional association of the triple,
A(v, r, c), is computed as the ratio of the selectional
preference strength for this particular class, divided
by the overall selectional preference strength of the
verb-relation pair. This is shown in Equation 1.
A(v, r, c) =
p(c|v, r)log p(c|v,r)p(c|r)
D(p(c|r)||p(c|v, r))
(1)
Finally, the selectional preference between a verb,
a relation, and an argument head is taken to be the
selectional association of the verb and relation with
the most strongly associated WordNet ancestor class
of the argument.
WordNet-based approaches however face two
problems. One is a coverage problem due to the lim-
ited size of the resource (see the task-based evalu-
ation in Gildea and Jurafsky (2002)). The other is
that the shape of the WordNet hierarchy determines
the generalisations that the models make. These are
not always intuitive. For example, Resnik (1996) ob-
serves that (answer, obj, tragedy) receives a high
preference because ?tragedy? in WordNet is a type
of written communication, which is a preferred argu-
ment class of ?answer?.
Rooth et al (1999) present a fundamentally dif-
ferent approach to selectional preference induction
which uses soft clustering to form classes for general-
isation and does not take recourse to any hand-crafted
resource. We will argue in Section 6 that our model
allows more control over the generalisations made.
401
Modelling Selectional Preferences with Thematic
Roles. Pad? et al (2006) present a deeper model
for the plausibility of (v, r, a) triples that approxi-
mates the relations with thematic roles. It estimates
the selectional preferences of a verb-role pair with
a generative probability model that equates the plau-
sibility of a (v, r, a) triple with the joint probability
of seeing the thematic role with the verb-argument
pair. In addition, the model also considers the verb?s
sense s and the grammatical function gf of the ar-
gument; however, since the model is generative, it
can make predictions even when not all variables are
instantiated. The final model is shown in Equation 2.
Plausibilityv,r,a = P (v, s, r, a, gf ) (2)
The induction of this model from the FrameNet cor-
pus of semantically annotated training data (Fillmore
et al, 2003) encounters a serious sparse data prob-
lem, which is approached by the application of word-
class-based and Good-Turing re-estimation smooth-
ing. The resulting model?s plausibility predictions
are significantly correlated to human judgements, but
because of the use of verb-specific thematic roles,
the model?s coverage is still restricted by the verb
coverage of the training corpus.
Vector Space Models. Another class of models
that has found wide application in lexical semantics
is the family of vector space models. In a vector space
model, each target word is represented as a vector,
typically constructed from co-occurrence counts with
context words in a large corpus (the so-called basis
elements). The underlying assumption is that words
with similar meanings occur in similar contexts, and
will be assigned similar vectors. Thus, the distance
between the vectors of two target words, as given by
some distance measure (e.g., Cosine or Jaccard), is a
measure of their semantic similarity.
Vector space models are simple to construct, and
the semantic similarity they provide has found a wide
range of applications. Examples in NLP include in-
formation retrieval (Salton et al, 1975), automatic
thesaurus extraction (Grefenstette, 1994), and pre-
dominant sense identification (McCarthy et al, 2004).
In cognitive science, they have been used to account
for the influence of context on human lexical pro-
cessing (McDonald and Brew, 2004), and to model
lexical priming (Lowe and McDonald, 2000).
A drawback of vector space models is the diffi-
culty of interpreting what some degree of ?generic
semantic similarity? between two target words means
in linguistic terms. In particular, this similarity is
not sensitive to selectional preferences over specific
semantic relations, and thus cannot model the plau-
sibility data we are interested in. The next section
demonstrates how the integration of ideas from se-
lectional preference induction makes this distinction
possible.
3 The Vector Similarity Model:
Corpus-Based Modelling of Plausibility
3.1 Model Architecture
Our model builds on the architecture of Erk (2007). It
combines the idea underlying the selectional prefer-
ence models from Section 2, namely to predict plau-
sibility by generalising over head words, with vector
space similarity. The fundamental idea of our model
is to model the plausibility of the triple (v, r, a) by
comparing the argument head a to other headwords
a? which we have already seen in a corpus for the
same verb-relation pair (v, r), and which we there-
fore assume to be plausible. We write Seenr(v) for
the set of seen headwords. Our intuition is that if a
is similar to the words in Seenr(v), then the triple
(v, r, a) is plausible; conversely, if it is very dissimi-
lar, then the triple is implausible.
Concretely, we judge the plausibility of the triple
by averaging over the similarity of the vector for a to
all vectors for the seen headwords in Seenr(v):
Pl(v, r, a) =
?
a??Seenr(v)
w(a?) ? sim(a, a?)
|Seenr(v)|
(3)
where w is a weight factor specific to each a?. w can
be used to implement different weighting schemes
that encode prior knowledge, e.g., about the reliabil-
ity of different words in Seenr(v). In this paper, we
only consider a very simple weighting factor, namely
the frequency of the seen headwords. This encodes
the assumption that similarity to frequent head words
is more important than similarity to infrequent ones:
Pl(v, r, a) =
?
a??Seenr(v)
f(a?) ? sim(a, a?)
|Seenr(v)|
(4)
402
deer
lion
hunter
poacher
director
seen patients
of "shoot"
seen agents
of "shoot"
Figure 1: A vector space for estimating the
plausibilities of (shoot, agent, hunter) and
(shoot, patient, hunter).
This model can be seen as a straightforward imple-
mentation of the selectional preference induction pro-
cess of generalising from seen headwords to other,
similar words. By using vector space representations
to judge the similarity of words, we obtain a com-
pletely corpus-driven model that does not require any
additional resources and is very flexible. A comple-
mentary view on this model is as a generalisation of
traditional vector space models that computes simi-
larity not between two vectors, but between a vector
and a set of other vectors. By using the vectors for
seen headwords of a given relation as this set, the
similarity we compute is specific to this relation.
Example. Figure 1 shows an example vector space.
Consider v = ?shoot?, r = agent, and a = ?hunter?.
In order to judge whether a hunter is a plausible agent
of ?shoot?, the vector space representation of ?hunter?
is compared to all representations of known agents
of "shoot?, namely ?poacher? and ?director?. Due
to the nearness of the vector for ?hunter? to these
two vectors, ?hunter? will be judged a fairly good
agent of ?shoot?. Compare this with the result for the
role patient : ?hunter? is further away from ?lion? and
?deer?, and will therefore be found to be a rather bad
patient of ?shoot?. However, ?hunter? is still more
plausible as a patient of ?shoot? than e.g., ?director?.
3.2 Instantiating the Model: Unparsed
vs. Parsed Corpora
The two major tasks which need to be addressed to
obtain an instance of this model are (a), determining
the sets of seen head words Seenr(v), and (b), the
construction of a vector space. Erk (2007) extracted
the set of seen head words from corpora with se-
mantic role annotation, and used only a single vector
space representation. In this paper, we eliminate the
reliance on special annotation by considering shallow
approximations of the semantic relations in question.
In addition, we discuss in detail which properties of
the vector space are crucial for the prediction of plau-
sibility ratings, a much more fine-grained task than
the pseudo-word disambiguation task presented in
Erk (2007) that is more closely related to semantic
role labelling. The goal of our exposition is thus to
develop a model that can use more training data, and
represent the corpus information optimally in order
to obtain superior coverage.
In fact, tasks (a) and (b) can be solved on the basis
of unparsed corpora, but we would expect the results
to be rather noisy. Fortunately, the state of the art in
broad-coverage (Lin, 1993) and unsupervised (Klein
and Manning, 2004) dependency parsing allows us to
treat dependency parsing merely as a preprocessing
step. We therefore describe two instantiations of our
model: one based on an unprocessed corpus, and one
based on a dependency-based parsed corpus. By com-
paring the models, we can gauge whether syntactic
preprocessing improves model performance. In the
following, we describe the strategies the two models
adopt for (a) and (b).
Identifying seen head words for relations. Re-
call that the set Seenr(v) is supposed to contain
known head words a that are observed in the corpus
as triples (v, r, a). In a parsed corpus, we can approx-
imate the relation agent by the dependency relation
of subject provided by the parser, and the relation
patient by the dependency relation of object. In
an unparsed corpus, these grammatical relations are
unavailable, and the only straightforward evidence
we can use is word order. In this case, we assume
that words directly adjacent to the left of a predicate
are subjects, and therefore agents, whereas words
directly to its right are objects, and thus patients.
Vector space topology. The success of our method
depends directly on the topology of the vector space.
More specifically, two words should only be assigned
similar vectors if they are in fact of similar plausibil-
ity. If this is not the case, there is no guarantee that a
word a that is similar to the words in Seenr(v) forms
403
``````````````Basis elements
Target
deer hunter
shoot 10 10
escape 12 12
``````````````Basis elements
Target
deer hunter
shoot-SUBJ 0 8
shoot-OBJ 10 2
escape-SUBJ 10 5
escape-OBJ 2 7
Figure 2: Two vector spaces, using as basis elements
either context words (above) or words paired with
grammatical functions (below)
a plausible triple (v, r, a) itself (cf. Figure 1).
The topology, in turn, is related to the choice of
basis elements. Traditional vector space models use
context words as basis elements of the space. The
top table in Figure 2 illustrates our intuition that such
spaces are problematic: ?deer? and ?hunter? receive
identical vectors, even though they show complemen-
tary plausibility ratings (cf. Table 1). The reason is
that ?deer? and ?hunter? often co-occur quite closely
to one another (e.g., in the vicinity of ?shoot?), and
thus show a very similar profile in terms of context
words. In preliminary experiments, we found that vec-
tor spaces with context words as basis elements are
in fact unable to distinguish such word pairs reliably.
In contrast, the bottom table in Figure 2 indicates
that this problem can be alleviated by using context
words combined with the grammatical relation to
the target word as basis elements. Target words now
receive different representations, depending on the
grammatical function in which they occur with con-
text words. In consequence, resulting spaces can dis-
tinguish, for example, between ?hunter? and ?deer?.
We adopt word-function pairs as basis elements for
the vector spaces in all our models. In a dependency-
parsed corpus, the basis elements can be directly read
off the syntactic structure. In an unparsed corpus, we
again fall back on word order, appending to each
context word its relative position to the target word.
4 Experimental Setup
Experimental Materials. In order to make our
evaluation comparable to the earlier modelling study
by Pad? et al (2006), we present evaluations on the
two plausibility judgement datasets used there.1
The first dataset consists of 100 data points from
McRae et al (1998). Our example in Table 1, which
is taken from this dataset, demonstrates its balanced
structure: 25 verbs are paired with two arguments
and two relations each, such that each argument is
highly plausible in one relation, but implausible in
the other. The resulting distribution of ratings is thus
highly bimodal. Models can only reliably predict the
human ratings in this data set if they can capture the
difference between verb argument slots as well as as
between individual fillers.
The second, larger dataset is less strictly balanced,
since its triples are constructed on the basis of corpus
co-occurrences (Pad? et al, 2006). 18 verbs are com-
bined with the three most frequent subjects and ob-
jects from both the Penn Treebank and the FrameNet
corpus. Each verb-argument pair was rated both as
an agent and as a patient, which leads to a total of
24 rated triples per verb. The dataset contains ratings
for a total of 414 triples, due to overlap between cor-
pora. The resulting judgements show a more even
distribution of ratings than the McRae data.
Vector Similarity Models. Following our exposi-
tion in the last section, we construct two instantia-
tions of our vector similarity model, one using un-
parsed and one parsed data. Both are trained on the
complete British National Corpus (Burnard, 1995,
BNC) with more than six million sentences.
The unparsed model (Unparsed) uses the BNC
without any pre-processing. We first construct the
set of known headwords, Seenr(v), as follows: All
words up to 2 words to the left of instances of v
are assumed to be subjects, and thus agents; vice
versa for patients to the right. Then, we construct
semantic space representations for the experimental
arguments and known headwords, adopting optimal
parameter settings from the literature (Pad? and Lap-
ata, 2007). This means a context window of 5 words
to either side and 2,000 basis elements (dimensions),
which are formed by the most frequent 1,000 words
1We are grateful to Ken McRae for his dataset.
404
in the BNC, combined with each of the relations
agent and patient. All counts are log-likelihood trans-
formed (Lowe, 2001).
To construct the parsed model (Parsed), we
dependency-parsed the BNC with Minipar (Lin,
1993). We first obtain the seen headwords Seenr(v)
by using all subjects and objects of v as agents and pa-
tients, respectively. We then construct a vector space
for the experimental arguments and known head-
words.2 We use 2,000 dimensions again, but adopt the
most frequent (head , grammatical function) pairs
in the BNC as basis elements. The context window
is formed by subject and object dependencies.
All counts are log-likelihood transformed.
We experiment with two distance measures to com-
pute vector similarity, namely the Jaccard Coefficient
and Cosine Distance, both of which have been shown
to yield good performance in NLP tasks (Lee, 1999;
McDonald and Lowe, 1998).
Evaluation Procedure. We evaluate our models
by correlating the predicted plausibility values with
the human judgements, which range between 1 and
7. Since the human judgement data is not normally
distributed, we use Spearman?s ?, a non-parametric
rank-order test. We determine the statistical signif-
icance of differences in correlation strength using
the method described in Raghunathan (2003). This
method can deal with missing values and thus allows
us to compare models with different coverage.
It is difficult to specify a straightforward baseline
for our correlation-based evaluation. In contrast to
classification tasks, where models choose one out of
a fixed number of classes, our model predicts contin-
uous data. This task is more difficult to approximate,
e.g., using frequency information.
With respect to upper bounds, we hold that au-
tomatic models of plausibility cannot be expected
to surpass the typical agreement on the plausibility
judgement task between human participants. Thus,
we assume an upper bound of ? ? 0.7.
Comparison against Other Models. We compare
our performance to two models from the literature dis-
cussed in Section 2. The first model (Pado) is the the-
2This space was computed using the
DependencyVectors software described in Pad? and
Lapata (2007). This software can be downloaded from http:
//www.coli.uni-saarland.de/~pado/dv.html.
Model Coverage Spearman?s ?
Unparsed Cosine 90% 0.023, ns
Unparsed Jaccard 90% 0.044, ns
Parsed Cosine 91% 0.218, *
Parsed Jaccard 91% 0.129, ns
Resnik 94% 0.028, ns
Pado 56% 0.415, **
Table 2: Model performance on McRae data.
*: p < 0.05, **: p < 0.01
matic role-based model by Pad? et al (2006) trained
on the FrameNet (Fillmore et al, 2003) release 1.2 ex-
ample sentences, a subset of the BNC annotated with
semantic roles. This corpus contains about 57,000
sentences, which corresponds to roughly 1% of the
BNC data.
The second model (Resnik) is the WordNet-based
selectional preference model by Resnik (1996),
trained on the dependency-parsed BNC (see above).
5 Experimental Evaluation
The McRae Dataset. Table 2 summarises our re-
sults on the McRae dataset. The upper part shows
the results for our two vector similarity models
(Parsed/Unparsed), combined with the two distance
measures (Cosine/Jaccard). The lower part shows the
two resource-based models we use for comparison.
We find that all vector similarity models exhibit
high coverage (above 90%), and one model (Parsed
Cosine) can predict human judgements with a signifi-
cant correlation. The instantiation of the model has
a significant impact on the performance: The Parsed
models clearly outperform the Unparsed models. The
effect of the distance measure is less clear-cut, since
the Unparsed models perform better with Jaccard,
while the Parsed models prefer Cosine.
The deep semantic plausibility model (Pado)
makes predictions only for slightly more than half of
the data. This low coverage is a direct result of the
small overlap in verbs between the McRae dataset
and the FrameNet corpus. However, on the data
points it covers, it achieves a significant correlation
to human judgements. The correlation coefficient is
numerically much higher than that of the Parsed Co-
sine model, but due to the large coverage difference,
the two models are not statistically distinguishable.
405
Model Coverage Spearman?s ?
Unparsed Cosine 98% 0.117, *
Unparsed Jaccard 98% 0.149, **
Parsed Cosine 98% 0.479, ***
Parsed Jaccard 98% 0.120, *
Resnik 98% 0.237, ***
Pado 97% 0.515, ***
Table 3: Model performance on Pado data.
*: p < 0.05, **: p < 0.01, ***: p < 0.001
Resnik?s WordNet-based model shows a coverage
that is comparable to the vector similarity models,
but does not achieve a significant correlation to the
human judgements.
The Pado Dataset. Table 3 summarises the results
for the Pado dataset. Since all verbs in this dataset are
covered in FrameNet, the deep Pado model shows a
coverage comparable to all other models, at >95%.
The main difference to the McRae dataset lies in
the models? performance. We find that all models,
including the Unparsed vector models and Resnik,
manage to achieve significant correlations with the
human judgements. Within the vector similarity mod-
els, the same trends hold as for the McRae dataset:
Parsed outperforms Unparsed, and the best combina-
tion is Parsed Cosine. The models fall into two clearly
separated groups: The Pado and Parsed Cosine mod-
els achieve a highly significant correlation, and are
statistically indistinguishable. They significantly out-
perform the second group (p < 0.001), formed by
all other models. Within this second group, Resnik is
numerically the best model and shows a significant
correlation with human data; nevertheless, the differ-
ence to the first group is evident from its substantially
lower correlation coefficient.
The construction of the Pado dataset alows a fur-
ther analysis. As mentioned in Section 4, the dataset
consists of verb-argument pairs drawn from two dif-
ferent corpora. Therefore, each verb is combined
both with some arguments that are seen in FrameNet,
and some that are not. Our hypothesis is that the
FrameNet-trained Pado model performs consider-
ably better on the 216 ?FN-Seen? data points (verb-
argument pairs observed in FrameNet in at least one
relation) than on the 198 ?FN-Unseen? data points
(verb-argument pairs unseen in both relations).
Table 4 shows the results of this analysis for the
best-performing models. We observe a pattern corre-
sponding to our expectations: The performance of the
Pado model is clearly worse for FN-Unseen than for
FN-Seen, while the Resnik and Parsed Cosine mod-
els perform more evenly across both datasets. While
the Pado model is significantly better on the FN-Seen
dataset, it is numerically outperformed by the Parsed
Cosine model for the FN-Unseen data points. We
conclude that the deep model is more accurate within
the coverage of its resources, but loses its advantage
when it has to resort to smoothing.
Model combination. Our last analysis indicates
that the models have complementary strengths: the
thematic role-based Pado model is the best plausi-
bility predictor on the data points it has seen, while
the Parsed cosine model overall predicts human data
only numerically worse, and with better coverage.
We therefore suggest to combine the predictions of
the two models to combine their respective strengths.
For the moment, we only consider a naive backoff
scheme: For each data point, we use the prediction
of the Pado model if the data point is ?FN-Seen? (cf.
the last paragraph), and the prediction of the Parsed
Cosine model otherwise. Note that this criterion does
not consider the predictions of the models themselves,
only properties of the underlying training set.
The actual combination requires a normalisation
of the respective predictions, since one of the models
(Pado) is probabilistic, while the other one (Parsed
Cosine) is similarity-based, and their predictions are
not directly comparable. We perform a simple nor-
malisation by z-transforming the complete predic-
tions of each model.3 The combination of the scaled
predictions in fact results in an improved correlation
with the human data. The correlation coefficient of
?=0.552 numerically exceeds either base model, and
the coverage of 98% corresponds to the coverage of
the more robust Parsed Cosine model.
We take this result as evidence that even a simple
combination technique can lead to improved predic-
tions. Unfortunately, our naive backoff scheme does
not directly carry over to the McRae dataset, where
only 2 out of 100 data points are ?FN-Seen?, and the
Pado model would thus hardly contribute.
3The z transformation scales a dataset to a mean of 0 and a
standard deviation of 1.
406
Model FN-Seen Data FN-Unseen Data
Parsed Cosine 94% 0.426, *** 100% 0.461, ***
Resnik 96% 0.217, ** 100% 0.263, ***
Pado 97% 0.569, *** 96% 0.383, ***
Table 4: Performance on data points seen and unseen in FrameNet (Pado dataset). **: p < 0.01 ***: p < 0.001
Discussion. We have verified experimentally that
our vector similarity model is able to match the per-
formance of a deep plausibility model, exceeding it
in coverage, and to outperform a WordNet-based se-
lectional preference model. We conclude that a com-
pletely corpus-driven approach constitutes a viable
alternative to resource-based models.
One insight from our experiments is that vec-
tor similarity models constructed from dependency-
parsed corpora perform significantly better than un-
parsed models. This indicates that dependency rela-
tions like subject and object are reliable syntac-
tic correlates of semantic relations like agent and pa-
tient, but that their approximation in terms of word or-
der introduces considerable noise. The Parsed models
are best combined with Cosine Distance. We surmise
that Cosine, which tends to consider low-frequency
words more than Jaccard, is more susceptible to the
additional noise in unparsed corpora.
Furthermore, the choice of basis elements for the
vector space is vital: Plausibilities could only be pre-
dicted successfully with word-relation pairs as basis
elements. This is in contrast to recent results on pre-
dominant sense acquisition, the task of identifying
the most frequent sense for a given word in an un-
supervised manner (McCarthy et al, 2004). On that
task, Pad? and Lapata (2007) found vector spaces
with words as basis elements are in fact competitive
with models using word-relation pairs. This diver-
gence underlines an interesting difference between
the two tasks. Evidently, predominant senses identi-
fication, as a WSD-related task, can succeed on the
basis of topical information, which is represented
well in word-based spaces. In contrast, plausibility
judgments can only be predicted by a space based
on word-relation pairs which can represent the finer-
grained distinctions arising from different relations
between verb and noun.
A second important finding is that the relative per-
formance of the different models is the same on the
McRae and Pado datasets. The Pado model performs
best, followed by our Parsed Cosine vector similarity
model, followed by the Unparsed and Resnik models.
The McRae dataset, however, is much more diffi-
cult to account for than the Pado data, independent of
the model. This effect was already noted by Pad? et
al. (2006), who attributed it to the very limited over-
lap between the McRae dataset and FrameNet. While
this explanation can account for the difference for the
Pado model, we observe the same pattern across all
models. This suggests that a more general frequency
effect is at work here: The median frequency of the
hand-selected McRae nouns is 1,356 in the BNC, as
opposed to 8,184 for the corpus-derived Pado nouns.
The resulting sparseness affects all model families,
since all ultimately rely on co-occurrences.
The performance difference between the two
datasets is particularly large for the WordNet-based
selectional preference model (Resnik). A further
analysis of the model?s predictions shows that
the model has difficulty in distinguishing between
verb-relation-argument triples that differ only in
the argument, such as (shoot, agent, hunter) and
(shoot, agent, deer). Recall that it is crucial for the
prediction of the McRae data to make this distinc-
tion, since the arguments for each relation are cho-
sen to differ widely in plausibility. The reason for
the Resnik model?s difficulty is that arguments are
mapped onto WordNet synsets, and whenever two
arguments are mapped onto closely related synsets,
their plausibility ratings are similar. This problem is
graver for the McRae test set, where all arguments are
animates, and thus more similar in terms of WordNet,
than for the Pado set, which also contains a portion of
inanimate arguments with animate counterparts. This
analysis highlights again the fundamental problem
of resource-based models, where design decisions of
the underlying resource may limit, or even mislead,
the models? generalisations.
Finally, we have shown in a first experiment that
407
the syntax-based vector similarity model can be com-
bined with the role-base model to obtain a combined
model that performs superior to both. In this com-
bined model, the shallowmodel?s better coverage sup-
plements the accurate predictions of the deep model.
6 Conclusions
In this paper, we have considered the computational
modelling of human plausibility judgements for verb-
relation-argument triples, a task equivalent to the
computation of selectional preferences. We have ex-
tended a recent proposal (Erk, 2007) which com-
bines ideas from selectional preference induction and
vector space models. Our model can be constructed
from a large corpus with partial syntactic information
(specifically, subject and object relations) from which
it builds an optimally informative vector space.
We have demonstrated that the successful evalua-
tion of the model in Erk (2007) on the coarse-grained
pseudo-word disambiguation task carries over to the
prediction of human plausibility judgments which re-
quires relatively fine-grained, relation-based distinc-
tions. Our model is competitive with existing ?deep?
models while exhibiting a higher coverage. We have
also shown that our vector similarity model can be
combined with a ?deep? model so that the combined
model outperforms both base models. A thorough
investigation of strategies for prediction combination
and scaling remains future work.
The strategy of our model to derive generalisations
directly from corpus data, without recourse to re-
sources, is similar to another family of corpus-driven
selectional preference models, namely EM-based
clustering models (Rooth et al, 1999). However, we
believe that our model has a number of advantages.
(1), It is conceptually simple and implements the
intuition behind selectional preference models, ?gen-
eralise from known headwords to unknown ones?,
particularly directly through the comparison of new
headwords to known ones according to a given defini-
tion of similarity. (2), The separation of the similarity
computation and the acquisition of seen headwords
gives the experimenter fine-grained control over the
types and sources of information which inform the
construction of the model. (3), The instantiation of
the similarity computation with a vector space makes
it possible to integrate additional linguistic informa-
tion beyond verb-argument co-occurrences into the
model, building on a large body of work in vector
space construction. In sum, our modular model pro-
vides a higher degree of control than one-step models
like the EM-based proposal.
An important avenue of further research is the
ability of the vector plausibility model to model finer-
grained distinctions between semantic relations be-
yond the agent/patient dichotomy, as thematic role-
based models are able to. Excluding the direct use of
role-annotated corpora like FrameNet for coverage
reasons, the most promising strategy is to extend our
present scheme of approximating semantic relations
by grammatical realisations. How much noise this
approximation introduces when finer role sets are
used is an open research question.
Acknowledgments. The work presented in this pa-
per was supported by the financial support of DFG
(grants Pi-154/9-2 and IRTG ?Language Technology
and Cognitive Systems?).
References
Naoki Abe and Hang Li. 1996. Learning word associa-
tion norms using tree cut pair models. In Proceedings
of ICML 1996, pages 3?11.
Lou Burnard, 1995. User?s guide for the British National
Corpus. British National Corpus Consortium, Oxford
University Computing Services.
Stephen Clark and David Weir. 2002. Class-based prob-
ability estimation using a semantic hierarchy. Compu-
tational Linguistics, 28(2):187?206.
Katrin Erk. 2007. A simple, similarity-based model for
selectional preferences. In Proceedings of the 45th
ACL, Prague, Czech Republic.
Charles J. Fillmore, Christopher R. Johnson, and
Miriam R.L. Petruck. 2003. Background to FrameNet.
International Journal of Lexicography, 16:235?250.
Daniel Gildea and Daniel Jurafsky. 2002. Automatic la-
beling of semantic roles. Computational Linguistics,
28(3):245?288.
Gregory Grefenstette. 1994. Explorations in Automatic
Thesaurus Discovery. Kluwer Academic Publishers.
Donald Hindle and Mats Rooth. 1993. Structural ambi-
guity and lexical relations. Computational Linguistics,
19(1):103?120.
408
Dan Klein and Christopher Manning. 2004. Corpus-
based induction of syntactic structure: Models of de-
pendency and constituency. In Proceedings of the 42th
ACL, pages 478?485, Barcelona, Spain.
Lillian Lee. 1999. Measures of distributional similarity.
In Proceedings of the 37th ACL, pages 25?32, College
Park, MA.
Dekang Lin. 1993. Principle-based parsing without over-
generation. In Proceedings of the 31st ACL, pages
112?120, Columbus, OH.
Will Lowe and Scott McDonald. 2000. The direct route:
Mediated priming in semantic space. In Proceedings
of the 22nd CogSci, pages 675?680, Philadelphia, PA.
Will Lowe. 2001. Towards a theory of semantic space.
In Proceedings of the 23rd CogSci, pages 576?581, Ed-
inburgh, UK.
Diana McCarthy and John Carroll. 2003. Disambiguat-
ing nouns, verbs and adjectives using automatically ac-
quired selectional preferences. Computatinal Linguis-
tics, 29(4):639?654.
Diana McCarthy, Rob Koeling, Julie Weeds, and John
Carroll. 2004. Finding predominant word senses in
untagged text. In Proceedings of the 42th ACL, pages
279?286, Barcelona, Spain.
Scott McDonald and Chris Brew. 2004. A distributional
model of semantic context effects in lexical process-
ing. In Proceedings of the 42th ACL, pages 17?24,
Barcelona, Spain.
Scott McDonald and Will Lowe. 1998. Modelling func-
tional priming and the associative boost. In Proceed-
ings of the 20th CogSci, pages 675?680, Madison, WI.
Ken McRae, Michael Spivey-Knowlton, and Michael
Tanenhaus. 1998. Modeling the influence of thematic
fit (and other constraints) in on-line sentence compre-
hension. Journal of Memory and Language, 38:283?
312.
Sebastian Pad? and Mirella Lapata. 2007. Dependency-
based construction of semantic space models. Compu-
tational Linguistics, 33(2).
Ulrike Pad?, Frank Keller, and Matthew W. Crocker.
2006. Combining syntax and thematic fit in a proba-
bilistic model of sentence processing. In Proceedings
of the 28th CogSci, pages 657?662, Vancouver, BC.
Patrick Pantel, Rahul Bhagat, Bonaventura Coppola, Tim-
othy Chklovski, and Eduard Hovy. 2007. ISP: Learn-
ing inferential selectional preferences. In Proceedings
of NAACL 2007, Rochester, NY.
Trivellore Raghunathan. 2003. An approximate test for
homogeneity of correlated correlations. Quality and
Quantity, 37:99?110.
Philip Resnik. 1996. Selectional constraints: An
information-theoretic model and its computational re-
alization. Cognition, 61:127?159.
Mats Rooth, Stefan Riezler, Detlef Prescher, Glenn Car-
roll, and Franz Beil. 1999. Inducing an semanti-
cally annotated lexicon via EM-based clustering. In
Proceedings of the 37th ACL, pages 104?111, College
Park, MA.
Gerard Salton, Anita Wong, and Chung-Shu Yang. 1975.
A vector-space model for information retrieval. Jour-
nal of the American Society for Information Science,
18:613?620.
John Trueswell, Michael Tanenhaus, and Susan Garnsey.
1994. Semantic influences on parsing: Use of the-
matic role information in syntactic ambiguity resolu-
tion. Journal of Memory and Language, 33:285?318.
409
Modelling Semantic Role Plausibility in Human Sentence Processing
Ulrike Pad? and Matthew Crocker
Computational Linguistics
Saarland University
66041 Saarbr?cken
Germany
{ulrike,crocker}@coli.uni-sb.de
Frank Keller
School of Informatics
University of Edinburgh
2 Buccleuch Place
Edinburgh EH8 9LW, UK
keller@inf.ed.ac.uk
Abstract
We present the psycholinguistically moti-
vated task of predicting human plausibility
judgements for verb-role-argument triples
and introduce a probabilistic model that
solves it. We also evaluate our model on
the related role-labelling task, and com-
pare it with a standard role labeller. For
both tasks, our model benefits from class-
based smoothing, which allows it to make
correct argument-specific predictions de-
spite a severe sparse data problem. The
standard labeller suffers from sparse data
and a strong reliance on syntactic cues, es-
pecially in the prediction task.
1 Introduction
Computational psycholinguistics is concerned
with modelling human language processing.
Much work has gone into the exploration of sen-
tence comprehension. Syntactic preferences that
unfold during the course of the sentence have been
successfully modelled using incremental proba-
bilistic context-free parsing models (e.g., Jurafsky,
1996; Crocker and Brants, 2000). These models
assume that humans prefer the most likely struc-
tural alternative at each point in the sentence. If
the preferred structure changes during processing,
such models correctly predict processing difficulty
for a range of experimentally investigated con-
structions. They do not, however, incorporate an
explicit notion of semantic processing, while there
are many phenomena in human sentence process-
ing that demonstrate a non-trivial interaction of
syntactic preferences and semantic plausibility.
Consider, for example, the well-studied case of
reduced relative clause constructions. When incre-
mentally processing the sentence The deer shot by
the hunter was used as a trophy, there is a local
ambiguity at shot between continuation as a main
clause (as in The deer shot the hunter) or as a re-
duced relative clause modifying deer (equivalent
to The deer which was shot . . . ). The main clause
continuation is syntactically more likely.
However, there is a second, semantic clue pro-
vided by the high plausibility of deer being shot
and the low plausibility of them shooting. This
influences readers to choose the syntactically dis-
preferred reduced relative reading which interprets
the deer as an object of shot (McRae et al, 1998).
Plausibility has overridden the syntactic default.
On the other hand, for a sentence like The hunter
shot by the teenager was only 30 years old, se-
mantic plausibility initially reinforces the syntac-
tic main clause preference and readers show diffi-
culty accommodating the subsequent disambigua-
tion towards the reduced relative.
In order to model effects like these, we need
to extend existing models of sentence process-
ing by introducing a semantic dimension. Pos-
sible ways of integrating different sources of in-
formation have been presented e.g. by McRae
et al (1998) and Narayanan and Jurafsky (2002).
Our aim is to formulate a model that reliably pre-
dicts human plausibility judgements from corpus
resources, in parallel to the standard practice of
basing the syntax component of psycholinguistic
models on corpus probabilities or even probabilis-
tic treebank grammars. We can then use both the
syntactic likelihood and the semantic plausibility
score to predict the preferred syntactic alterna-
tive, thus accounting for the effects shown e.g. by
McRae et al (1998).
Independent of a syntactic model, we want any
semantic model we define to satisfy two criteria:
First, it needs to be able to make predictions in-
345
crementally, in parallel with the syntactic model.
This entails dealing with incomplete or unspeci-
fied (syntactic) information. Second, we want to
extend to semantics the assumption made in syn-
tactic models that the most probable alternative is
the one preferred by humans. The model therefore
must be probabilistic.
We present such a probabilistic model that can
assign roles incrementally as soon as a predicate-
argument pair is seen. It uses the likelihood of the-
matic role assignments to model human interpre-
tation of verb-argument relations. Thematic roles
are a description of the link between verb and ar-
gument at the interface between syntax and se-
mantics. Thus, they provide a shallow level of
sentence semantics which can be learnt from an-
notated corpora.
We evaluate our model by verifying that it in-
deed correctly predicts human judgements, and by
comparing its performance with that of a standard
role labeller in terms of both judgement prediction
and role assignment. Our model has two advan-
tages over the standard labeller: It does not rely
on syntactic features (which can be hard to come
by in an incremental task) and our smoothing ap-
proach allows it to make argument-specific role
predictions in spite of extremely sparse training
data. We conclude that (a) our model solves the
task we set, and (b) our model is better equipped
for our task than a standard role labeller.
The outline of the paper is as follows: After
defining the prediction task more concretely (Sec-
tion 2), we present our simple probabilistic model
that is tailoured to the task (Section 3). We in-
troduce our test and training data in Section 4. It
becomes evident immediately that we face a se-
vere sparse data problem, which we tackle on two
levels: By smoothing the distribution and by ac-
quiring additional counts for sparse cases. The
smoothed model succeeds on the prediction task
(Section 5). Finally, in Section 6, we compare our
model to a standard role labeller.
2 The Judgement Prediction Task
We can measure our intuitions about the plau-
sibility of hunters shooting and deer being shot
in terms of plausibility judgements for verb-role-
argument triples. Two example items from McRae
et al (1998) are presented in Table 1. The judge-
ments were gathered by asking raters to assign a
value on a scale from 1 (not plausible) to 7 (very
Verb Noun Role Rating
shoot hunter agent 6.9
shoot hunter patient 2.8
shoot deer agent 1.0
shoot deer patient 6.4
Table 1: Test items: Verb-noun pairs with ratings
on a 7 point scale from McRae et al (1998).
plausible) to questions like How common is it for
a hunter to shoot something? (subject reading:
hunter must be agent) or How common is it for a
hunter to be shot? (object reading: hunter must be
patient). The number of ratings available in each
of our three sets of ratings is given in Table 2 (see
also Section 4).
The task for our model is to correctly predict the
plausibility of each verb-role-argument triple. We
evaluate this by correlating the model?s predicted
values and the judgements. The judgement data
is not normally distributed, so we correlate using
Spearman?s ? (a non-parametric rank-order test).
The ? value ranges between 0 and 1 and indicates
the strength of association between the two vari-
ables. A significant positive value indicates that
the model?s predictions are accurate.
3 A Model of Human Plausibility
Judgements
We can formulate a model to solve the prediction
task if we equate the plausibility of a role assign-
ment to a verb-argument pair with its probability,
as suggested above. This value is influenced as
well by the verb?s semantic class and the grammat-
ical function of the argument. The plausibility for
a verb-role-argument triple can thus be estimated
as the joint probability of the argument head a, the
role r, the verb v, the verb?s semantic class c and
the grammatical function g f of a:
Plausibilityv,r,a = P(r,a,v,c,g f )
This joint probability cannot be easily estimated
from co-occurrence counts due to lack of data.
But we can decompose this term into a number
of subterms that approximate intuitively impor-
tant information such as syntactic subcategorisa-
tion (P(g f |v,c)), the syntactic realisation of a se-
mantic role (P(r|v,c,g f )) and selectional prefer-
ences (P(a|v,c,g f ,r)):
Plausibilityv,r,a = P(r,a,v,c,g f ) =
P(v) ?P(c|v) ?P(g f |v,c) ?
P(r|v,c,g f ) ?P(a|v,c,g f ,r)
346
shoot.02: [The hunter Arg0] shot [the deer Arg1].
Killing: [The hunter Killer] shot [the deer Victim].
Figure 1: Example annotation: PropBank (above)
and FrameNet (below).
Each of these subterms can be estimated more eas-
ily from the semantically annotated training data
simply using the maximum likelihood estimate.
However, we still need to smooth our estimates,
especially as the P(a|v,c,g f ,r) term remains very
sparse. We describe our use of two complemen-
tary smoothing methods in Section 5.
Our model fulfils the requirements we have
specified: It is probabilistic, able to work incre-
mentally as soon as a single verb-argument pair
is available, and can make predictions even if the
input information is incomplete. The model gen-
erates the missing values if, e.g., the grammatical
function or the verb?s semantic class are not spec-
ified. This means that we can immediately evalu-
ate on the judgement data without needing further
verb sense or syntactic information.
4 Test and Training data
Training Data To date, there are two main
annotation efforts that have produced semanti-
cally annotated corpora: PropBank (PB) and
FrameNet (FN). Their approaches to annotation
differ enough to warrant a comparison of the cor-
pora as training resources. Figure 1 gives an exam-
ple sentence annotated in PropBank and FrameNet
style. The PropBank corpus (c. 120,000 propo-
sitions, c. 3,000 verbs) adds semantic annotation
to the Wall Street Journal part of the Penn Tree-
bank. Arguments and adjuncts are annotated for
every verbal proposition in the corpus. A common
set of argument labels Arg0 to Arg5 and ArgM
(adjuncts) is interpreted in a verb-specific way.
Some consistency in mapping has been achieved,
so that Arg0 generally denotes agents and Arg1
patients/themes.
The FrameNet corpus (c. 58,000 verbal propo-
sitions, c. 1,500 verbs in release 1.1) groups verbs
with similar meanings together into frames (i.e.
descriptions of situations) with a set of frame-
specific roles for participants and items involved
(e.g. a killer, instrument and victim in the Killing
frame). Both the definition of frames as semantic
verb classes and the semantic characterisation of
frame-specific roles introduces a level of informa-
tion that is not present in PropBank. Since corpus
annotation is frame-driven, only some senses of a
verb may be present and word frequencies may not
be representative of English.
Test Data Our main data set consists of 160 data
points from McRae et al (1998) that were split
randomly into a 60 data point development set and
a 100 data point test set. The data is made up of
two arguments per verb and two ratings for each
verb-argument pair, one for the subject and one
for the object reading of the argument (see Section
2). Each argument is highly plausible in one of
the readings, but implausible in the other (recall
Table 1). Human ratings are on a 7-point scale.
In order to further test the coverage of our
model, we also include 76 items from Trueswell
et al (1994) with one highly plausible object per
verb and a rating each for the subject and object
reading of the argument. The data were gath-
ered in the same rating study as the McRae et
al. data, so we can assume consistency of the rat-
ings. However, in comparison to the McRae data
set, the data is impoverished as it lacks ratings for
plausible agents (in terms of the example in Ta-
ble 1, this means there are no ratings for hunter).
Lastly, we use 180 items from Keller and Lapata
(2003). In contrast with the previous two studies,
the verbs and nouns for these data were not hand-
selected for the plausibility of their combination.
Rather, they were extracted from the BNC corpus
by frequency criteria: Half the verb-noun combi-
nations are seen in the BNC with high, medium
and low frequency, half are unseen combinations
of the verb set with nouns from the BNC. The
data consists of ratings for 30 verbs and 6 argu-
ments each, interpreted as objects. The human
ratings were gathered using the Magnitude Esti-
mation technique (Bard et al, 1996). This data
set alows us to test on items that were not hand-
selected for a psycholinguistic study, even though
the data lacks agenthood ratings and the items are
poorly covered by the FrameNet corpus.
All test pairs were hand-annotated with
FrameNet and PropBank roles following the
specifications in the FrameNet on-line database
and the PropBank frames files.1
The judgement prediction task is very hard to
solve if the verb is unseen during training. Back-
ing off to syntactic information or a frequency
1Although a single annotator assigned the roles, the anno-
tation should be reliable as roles were mostly unambiguous
and the annotated corpora were used for reference.
347
Total Revised
Source FN PB
McRae et al (1998) 100 64 (64%) 92 (92%)
Trueswell et al (1994) 76 52 (68.4%) 72 (94.7%)
Keller and Lapata (2003) 180 ? 162 (90%)
Table 2: Test sets: Total number of ratings and size of revised test sets containing only ratings for seen
verbs (% of total ratings). ?: Coverage too low (26.7%).
baseline only works if the role set is small and syn-
tactically motivated, which is the case for Prop-
Bank, but not FrameNet. We present results both
for the complete test sets and and for revised sets
containing only items with seen verbs. Exclud-
ing unseen verbs seems justified for FrameNet and
has little effect for the PropBank corpus, since its
coverage is generally much better. Table 2 shows
the total number of ratings for each test set and
the sizes of the revised test sets containing only
items with seen verbs. FrameNet alays has sub-
stantially lower coverage. Since only 27% of the
verbs in the Keller & Lapata items are covered in
FrameNet, we do not test this combination.
5 Experiment 1: Smoothing Methods
We now turn to evaluating our model. It is im-
mediately clear that we have a severe sparse data
problem. Even if all the verbs are seen, the com-
binations of verbs and arguments are still mostly
unseen in training for all data sets.
We describe two complementary approaches
to smoothing sparse training data. One, Good-
Turing smoothing, approaches the problem of un-
seen data points by assigning them a small proba-
bility. The other, class-based smoothing, attempts
to arrive at semantic generalisations for words.
These serve to identify equivalent verb-argument
pairs that furnish additional counts for the estima-
tion of P(a|v,c,g f ,r).
5.1 Good-Turing Smoothing and Linear
Interpolation
We first tackle the sparse data problem by smooth-
ing the distribution of co-occurrence counts. We
use the Good-Turing re-estimate on zero and one
counts to assign a small probability to unseen
events. This method relies on re-estimating the
probability of seen and unseen events based on
knowledge about more frequent events.
Adding Linear Interpolation We also exper-
imented with the linear interpolation method,
which is typically used for smoothing n-gram
models. It re-estimates the probability of the n-
gram in question as a weighted combination of the
n-gram, the n-1-gram and the n-2-gram. For ex-
ample, P(a|v,c,g f ,r) is interpolated as
P(a|v,c,g f ,r) = ?1P(a|v,c,g f ,r)+
?2P(a|v,c,r)+?3P(a|v,c)
The ? values were estimated on the training
data, separately for each of the model?s four con-
ditional probability terms, by maximising five-fold
cross-validation likelihood to avoid overfitting.
We smoothed all model terms using the Good-
Turing method and then interpolated the smoothed
terms. Table 3 lists the test results for both train-
ing corpora and all test sets when Good-Turing
smoothing (GT) is used alone and with linear in-
terpolation (GT/LI). We also give the unsmoothed
coverage and correlation. The need for smoothing
is obvious: Coverage is so low that we can only
compute correlations in two cases, and even for
those, less than 20% of the data are covered.
GT smoothing alone always outperforms the
combination of GT and LI smoothing, especially
for the FrameNet training set. Maximising the
data likelihood during ? estimation does not ap-
proximate our final task well enough: The log
likelihood of the test data is duly improved from
?797.1 to ?772.2 for the PropBank data and from
?501.9 to ?446.3 for the FrameNet data. How-
ever, especially for the FrameNet training data,
performance on the correlation task diminishes as
data probability rises. A better solution might be
to use the correlation task directly as a ? estima-
tion criterion, but this is much more complex, re-
quiring us to estimate all ? terms simultaneously.
Also, the main problem seems to be that the ? in-
terpolation smoothes by de-emphasising the most
specific (and sparsest) term, so that, on our final
task, the all-important argument-specific informa-
tion is not used efficiently when it is available. We
therefore restrict ourselves to GT smoothing.
348
Smoothed Unsmoothed
Train Smoothing Test Coverage ? Coverage ?
PB
GT
McRae 93.5% (86%) 0.112, ns 2% (2%) ?
Trueswell 100% (94.7%) 0.454, ** 17% (16%) ns
Keller&Lapata 100% (90%) 0.285, ** 5% (4%) 0.727, *
GT/LI
McRae 93.5% (86%) 0.110, ns 2% (2%) ?
Trueswell 100% (94.7%) 0.404, ** 17% (16%) ns
Keller&Lapata 100% (90%) 0.284, ** 5% (4%) 0.727, *
FN
GT McRae 87.5% (56%) 0.164, ns 6% (4%) ?Trueswell 76.9% (52.6%) 0.046, ns 6% (4%) ?
GT/LI McRae 87.5% (56%) 0.042, ns 6% (4%) ?Trueswell 76.9% (52.6%) 0.009, ns 6% (4%) ?
Table 3: Experiment 1, GT and Interpolation smoothing. Coverage on seen verbs (and on all items) and
correlation strength (Spearman?s ? for PB and FN data on all test sets. ?: too few data points, ns: not
significant, *: p < 0.05, **: p < 0.01.
Model Performance Both versions of the
smoothed model make predictions for all seen
verbs; the remaining uncovered data points are
those where the correct role is not accounted for
in the training data (the verb may be very sparse
or only seen in a different FrameNet frame). For
the FrameNet training data, there are no significant
correlations, but for the PropBank data, we see
correlations for the Trueswell and Keller&Lapata
sets. One reason for the good performance of
the PB-Trueswell and PB-Keller&Lapata combi-
nations is that in the PropBank training data, the
object role generally seems to be the most likely
one. If the most specific probability term is sparse
and expresses no role preference (which is the case
for most items: see Unsmoothed Coverage), our
model is biased towards the most likely role given
the verb, semantic class and grammatical function.
Recall that the Trueswell and Keller&Lapata data
contain ratings for (plausible) objects only, so that
preferring the patient role is a good strategy. This
also explains why the model performs worse for
the McRae et al data, which also has ratings for
good agents (and bad patients). On FrameNet, this
preference for ?patient? roles is not as marked, so
the FN-Trueswell case does not behave like the
PB-Trueswell case.
5.2 Class-Based Smoothing
In addition to smoothing the training distribution,
we also attempt to acquire more counts to es-
timate each P(a|v,c,g f ,r) by generalising from
tokens to word classes. The term we estimate
becomes P(classa|classv,g f ,r). This allows us
to make argument-specific predictions as we do
not rely on a uniform smoothed term for unseen
P(a|v,c,g f ,r) terms. We use lexicographic noun
classes from WordNet and verb classes induced
by soft unsupervised clustering, which outperform
lexicographic verb classes.
Noun Classes We tested both the coarsest and
the finest noun classification available in Word-
Net, namely the top-level ontology and the noun
synsets which contain only synonyms of the target
word.2 The top-level ontology proved to overgen-
erate alternative nouns, which raises coverage but
does not produce meaningful role predictions. We
therefore use the noun synsets below.
Verb Classes Verbs are clustered according to
linguistic context information, namely argument
head lemmas, the syntactic configuration of verb
and argument, the verb?s semantic class, the gold
role information and a combined feature of gold
role and syntactic configuration. The evaluation of
the clustering task itself is task-based: We choose
the clustering configuration that produces optimal
results in the the prediction task on the McRae de-
velopment set. The base corpus for clustering was
always used for frequency estimation.
We used an implementation of two soft clus-
tering algorithms derived from information the-
ory (Marx, 2004): the Information Distortion (ID)
(Gedeon et al, 2003) and Information Bottleneck
(IB) (Tishby et al, 1999) methods. Soft cluster-
ing allows us to take verb polysemy into account
that is often characterised by different patterns of
syntactic behaviour for each verb meaning.
2For ambiguous nouns, we chose the sense that led to the
highest probability for the current role assignment.
349
A number of parameters were set on the devel-
opment set, namely the clustering algorithm, the
smoothing method within the algorithms and the
number of clusters within each run. For our task,
the IB algorithm generally yielded better results.
We decided which clustering parametrisations
should be tried on the test sets based on the notion
of stability: Both algorithms increase the number
of clusters by one at each iteration. Thus, each
parametrisation yields a series of cluster configu-
rations as the number of iterations increases. We
chose those parametrisations where a series of at
least three consecutive cluster configurations re-
turned significant correlations on the development
set. This should be an indication of a generalisable
success, rather than a fluke caused by peculiarities
of the data. On the test sets, results are reported
for the configuration (characterised by the itera-
tion number) that returned the first significant re-
sult in such a series on the development set, as this
is the most general grouping.
5.3 Combining the Smoothing Methods
We now present results for combining the GT
and class-based smoothing methods. We use in-
duced verb classes and WordNet noun synsets for
class-based smoothing of P(a|v,c,g f ,r), and rely
on GT smoothing if the counts for this term are
still sparse. All other model terms are always
smoothed using the GT method. Table 4 contains
results for three clustering configurations each for
the PropBank and FrameNet data that have proven
stable on the development set. We characterise
them by the clustering algorithm (IB or ID) and
number of clusters. Note that the upper bound for
our ? values, human agreement or inter-rater cor-
relation, is below 1 (as indicated by a correlation
of Pearson?s r = .640 for the seen pairs from the
Keller and Lapata (2003) data).
For the FrameNet data, there is a marked in-
crease in performance for both test sets. The hu-
man judgements are now reliably predicted with
good coverage in five out of six cases. Clearly,
equivalent verb-argument counts have furnished
accurate item-specific estimates. On the PropBank
data set, class-based smoothing is less helpful: ?
values generally drop slightly. Apparently, the
FrameNet style of annotation allows us to induce
informative verb classes, whereas the PropBank
classes introduce noise at most.
6 Experiment 2: Role Labelling
We have shown that our model performs well on
its intended task of predicting plausibility judge-
ments, once we have proper smoothing methods
in place. But since this task has some similarity
to role labelling, we can also compare the model
to a standard role labeller on both the prediction
and role labelling tasks. The questions are: How
well do we do labelling, and does a standard role
labeller also predict human judgements?
Beginning with work by Gildea and Jurafsky
(2002), there has been a large interest in se-
mantic role labelling, as evidenced by its adop-
tion as a shared task in the Senseval-III compe-
tition (FrameNet data, Litkowski, 2004) and at
the CoNLL-2004 and 2005 conference (PropBank
data, Carreras and M?rquez, 2005). As our model
currently focuses on noun phrase arguments only,
we do not adopt these test sets but continue to use
ours, defining the correct role label to be the one
with the higher probability judgement. We evalu-
ate the model on the McRae test set (recall that the
other sets only contain good patients/themes and
are therefore susceptible to labeller biases).
We formulate frequency baselines for our train-
ing data. For PropBank, always assigning Arg1
results in F = 45.7 (43.8 on the full test set). For
FrameNet, we assign the most frequent role given
the verb, so the baseline is F = 34.4 (26.8).
We base our standard role labelling system on
the SVM labeller described in Giuglea and Mos-
chitti (2004), although without integrating infor-
mation from PropBank and VerbNet for FrameNet
classification as presented in their paper. Thus, we
are left with a set of fairly standard features, such
as phrase type, voice, governing category or path
through parse tree from predicate. These are used
to train two classifiers, one which decides which
phrases should be considered arguments and one
which assigns role labels to these arguments. The
SVM labeller?s F score on an unseen test set is
F = 80.5 for FrameNet data when using gold ar-
gument boundaries. We also trained the labeller
on the PropBank data, resulting in an F score of
F = 98.6 on Section 23, again on gold boundaries.
We also evaluate the SVM labeller on the cor-
relation task by normalising the scores that the la-
beller assigns to each role and then correlating the
normalised scores to the human ratings.
In order to extract features for the SVM labeller,
we had to present the verb-noun pairs in full sen-
350
Train Test Verb Clusters Coverage ?
PB
McRae
ID 4 93.5% (86%) 0.097, ns
IB 10 93.5% (86%) 0.104, ns
IB 5 93.5% (86%) 0.107, ns
Trueswell
ID 4 100% (94.7%) 0.419, **
IB 10 100% (94.7%) 0.366, **
IB 5 100% (94.7%) 0.439, **
Keller&Lapata
ID 4 100% (90%) 0.300, **
IB 10 100% (90%) 0.255, **
IB 5 100% (90%) 0.297, **
FN
McRae
ID 4 87.5% (56%) 0.304, *
IB 9 87.5% (56%) 0.275, *
IB 10 87.5% (56%) 0.267, *
Trueswell
ID 4 76.9% (52.6%) 0.256, ns
IB 9 76.9% (52.6%) 0.342, *
IB 10 76.9% (52.6%) 0.365, *
Table 4: Experiment 1: Combining the smoothing methods. Coverage on seen verbs (and on all items)
and correlation strength (Spearman?s ?) for PB and FN data. WN synsets as noun classes. Verb classes:
IB/ID: smoothing algorithm, followed by number of clusters. ns: not significant, *: p<0.05, **: p<0.01
tences, as the labeller relies on a number of fea-
tures from parse trees. We used the experimental
items from the McRae et al study, which are all
disambiguated towards a reduced relative reading
(object interpretation: The hunter shot by the ...)
of the argument. In doing this, we are potentially
biasing the SVM labeller towards one label, de-
pending on the influence of syntactic features on
role assignment. We therefore also created a main
clause reading of the verb-argument pairs (sub-
ject interpretation: The hunter shot the ...) and
present the results for comparison. For our model,
we have previously not specified the grammatical
function of the argument, but in order to put both
models on a level playing field, we now supply the
grammatical function of Ext (external argument),
which applies for both formulations of the items.
Table 5 shows that for the labelling task, our
model outperforms the labelling baseline and the
SVM labeller on the FrameNet data by at least
16 points F score while the correlation with hu-
man data remains significant. For the PropBank
data, labelling performance is on baseline level,
below the better of the two SVM labeller condi-
tions. This result underscores the usefulness of
argument-specific plausibility estimates furnished
by class-based smoothing for the FrameNet data.
For the PropBank data, our model essentially as-
signs the most frequent role for the verb.
The performance of the SVM labeller suggests
a strong influence of syntactic features: On the
PropBank data set, it always assigns the Arg0 la-
bel if the argument was presented as a subject
(this is correct in 50% of cases) and mostly the
appropriate ArgN label if the argument was pre-
sented as an object. On FrameNet, performance
again is above baseline only for the subject condi-
tion, where there is also a clear trend for assign-
ing agent-style roles. (The object condition is less
clear-cut.) This strong reliance on syntactic cues,
which may be misleading for our data, makes the
labeller perform much worse than on the standard
test sets. For both training corpora, it does not
take word-specific plausibility into account due to
data sparseness and usually assigns the same role
to both arguments of a verb. This precludes a sig-
nificant correlation with the human ratings.
Comparing the training corpora, we find that
both models perform better on the FrameNet data
even though there are many more role labels in
FrameNet, and the SVM labeller does not profit
from the greater smoothing power of FrameNet
verb clusters. Overall, FrameNet has proven more
useful to us, despite its smaller size.
In sum, our model does about as well (PB data)
or better (FN data) on the labelling task as the
SVM labeller, while the labeller does not solve
the prediction task. The success of our model, es-
pecially on the prediction task, stems partly from
the absence of global syntactic features that bias
the standard labeller strongly. This also makes our
model suited for an incremental task. Instead of
351
Train Model Coverage ? Labelling F Labelling Cov.
PB
Baseline ? ? 45.7 (43.8%) 100%
SVM Labeller (subj) 100% (92%) ns 50 (47.9%) 100%
SVM Labeller (obj) 100% (92%) ns 45.7 (43.8%) 100%
IB 5 (subj/obj) 93.5% (86%) ns 45.7 (43.8%) 100%
FN
Baseline ? ? 34.4 (26.8%) 100%
SVM Labeller (subj) 87.5% (56%) ns 40.6 (31.7%) 100%
SVM Labeller (obj) 87.5% (56%) ns 34.4 (26.8%) 100%
ID 4 (subj/obj) 87.5% (56%) 0.271, * 56.3 (43.9%) 100%
Table 5: Experiment 2: Standard SVM labeller vs our model. Coverage on seen verbs (and on all items),
correlation strength (Spearman?s ?), labelling F score and labelling coverage on seen verbs (and on all
items, if different) for PB and FN data on the McRae test set. ns: not significant, *: p<0.05.
syntactic cues, we successfully rely on argument-
specific plausibility estimates furnished by class-
based smoothing. Our joint probability model has
the further advantage of being conceptually much
simpler than the SVM labeller, which relies on a
sophisticated machine learning paradigm. Also,
we need to compute only about one-fifth of the
number of SVM features.
7 Conclusions
We have defined the psycholinguistically moti-
vated task of predicting human plausibility ratings
for verb-role-argument triples. To solve it, we
have presented an incremental probabilistic model
of human plausibility judgements. When we em-
ploy two complementary smoothing methods, the
model achieves both good coverage and reliable
correlations with human data. Our model per-
forms as well as or better than a standard role la-
beller on the task of assigning the preferred role to
each item in our test set. Further, the standard la-
beller does not succeed on the prediction task, as it
cannot overcome the extreme sparse data problem.
Acknowledgements Ulrike Pad? acknowledges
a DFG studentship in the International Post-
Graduate College ?Language Technology and
Cognitive Systems?. We thank Ana-Maria Giu-
glea, Alessandro Moschitti and Zvika Marx for
making their software available and are grateful to
Amit Dubey, Katrin Erk, Mirella Lapata and Se-
bastian Pad? for comments and discussions.
References
Bard, E. G., Robertson, D., and Sorace, A. (1996). Magnitude
estimation of linguistic acceptability. Language, 72(1),
32?68.
Carreras, X. and M?rquez, L. (2005). Introduction to the
CoNLL-2005 shared task: Semantic role labeling. In Pro-
ceedings of CoNLL-2005.
Crocker, M. and Brants, T. (2000). Wide-coverage proba-
bilistic sentence processing. Journal of Psycholinguistic
Research, 29(6), 647?669.
Gedeon, T., Parker, A., and Dimitrov, A. (2003). Information
distortion and neural coding. Canadian Applied Mathe-
matics Quarterly, 10(1), 33?70.
Gildea, D. and Jurafsky, D. (2002). Automatic labeling of
semantic roles. Computational Linguistics, 28(3), 245?
288.
Giuglea, A.-M. and Moschitti, A. (2004). Knowledge discov-
ery using FrameNet, VerbNet and PropBank. In Proceed-
ings of the Workshop on Ontology and Knowledge Discov-
ering at ECML 2004.
Jurafsky, D. (1996). A probabilistic model of lexical and syn-
tactic access and disambiguation. Cognitive Science, 20,
137?194.
Keller, F. and Lapata, M. (2003). Using the web to obtain fre-
quencies for unseen bigrams. Computational Linguistics,
29(3), 459?484.
Litkowski, K. (2004). Senseval-3 task: Automatic labeling of
semantic roles. In Proceedings of Senseval-3: The Third
International Workshop on the Evaluation of Systems for
the Semantic Analysis of Text.
Marx, Z. (2004). Structure-Based computational aspects of
similarity and analogy in natural language. Ph.D. thesis,
Hebrew University, Jerusalem.
McRae, K., Spivey-Knowlton, M., and Tanenhaus, M.
(1998). Modeling the influence of thematic fit (and other
constraints) in on-line sentence comprehension. Journal
of Memory and Language, 38, 283?312.
Narayanan, S. and Jurafsky, D. (2002). A Bayesian model
predicts human parse preference and reading time in sen-
tence processing. In S. B. T. G. Dietterich and Z. Ghahra-
mani, editors, Advances in Neural Information Processing
Systems 14, pages 59?65. MIT Press.
Tishby, N., Pereira, F. C., and Bialek, W. (1999). The in-
formation bottleneck method. In Proc. of the 37-th An-
nual Allerton Conference on Communication, Control and
Computing, pages 368?377.
Trueswell, J., Tanenhaus, M., and Garnsey, S. (1994). Seman-
tic influences on parsing: Use of thematic role information
in syntactic ambiguity resolution. Journal of Memory and
Language, 33, 285?318.
352
Proceedings of the NAACL HLT Workshop on Innovative Use of NLP for Building Educational Applications, pages 1?9,
Boulder, Colorado, June 2009. c?2009 Association for Computational Linguistics
Automatic Assessment of Spoken Modern Standard Arabic 
Jian Cheng, Jared Bernstein, Ulrike Pado, Masanori Suzuki 
Pearson Knowledge Technologies 
299 California Ave, Palo Alto, CA 94306 
jian.cheng@pearson.com
 
 
 
Abstract 
 Proficiency testing is an important ingredient 
in successful language teaching. However, re-
peated testing for course placement, over the 
course of instruction or for certification can be 
time-consuming and costly. We present the 
design and validation of the Versant Arabic 
Test, a fully automated test of spoken Modern 
Standard Arabic, that evaluates test-takers' fa-
cility in listening and speaking. Experimental 
data shows the test to be highly reliable (test-
retest r=0.97) and to strongly predict perform-
ance on the ILR OPI (r=0.87), a standard in-
terview test that assesses oral proficiency. 
1 Introduction 
Traditional high-stakes testing of spoken profi-
ciency often evaluates the test-taker's ability to ac-
complish communicative tasks in a conversational 
setting. For example, learners may introduce them-
selves, respond to requests for information, or ac-
complish daily tasks in a role-play. 
Testing oral proficiency in this way can be 
time-consuming and costly, since at least one 
trained interviewer is needed for each student. For 
example, the standard oral proficiency test used by 
the United States government agencies (the Inter-
agency Language Roundtable Oral Proficiency 
Interview or ILR OPI) is usually administered by 
two certified interviewers for approximately 30-45 
minutes per candidate. 
The great effort involved in oral proficiency in-
terview (OPI) testing makes automated testing an 
attractive alternative. Work has been reported on 
fully automated scoring of speaking ability (e.g., 
Bernstein & Barbier, 2001; Zechner et al, 2007, 
for English; Balogh & Bernstein, 2007, for English 
and Spanish). Automated testing systems do not 
aim to simulate a conversation with the test-taker 
and therefore do not directly observe interactive 
human communication. Bernstein and Barbier 
(2001) describe a system that might be used in 
qualifying simultaneous interpreters; Zechner et al 
(2007) describe an automated scoring system that 
assesses performance according to the TOEFL iBT 
speaking rubrics. Balogh and Bernstein (2007) fo-
cus on evaluating facility in a spoken language, a 
separate test construct that relates to oral profi-
ciency. 
?Facility in a spoken language? is defined as 
?the ability to understand a spoken language on 
everyday topics and to respond appropriately and 
intelligibly at a native-like conversational pace? 
(Balogh & Bernstein, 2007, p. 272). This ability is 
assumed to underlie high performance in commu-
nicative settings, since learners have to understand 
their interlocutors correctly and efficiently in real 
time to be able to respond. Equally, learners have 
to be able to formulate and articulate a comprehen-
sible answer without undue delay. Testing for oral 
proficiency, on the other hand, conventionally in-
cludes additional aspects such as correct interpreta-
tion of the pragmatics of the conversation, socially 
and culturally appropriate wording and content and 
knowledge of the subject matter under discussion. 
In this paper, we describe the design and valida-
tion of the Versant Arabic Test (VAT), a fully 
automated test of facility with spoken Modern 
Standard Arabic (MSA). Focusing on facility 
rather than communication-based oral proficiency 
enables the creation of an efficient yet informative 
automated test of listening and speaking ability. 
The automated test can be administered over the 
telephone or on a computer in approximately 17 
minutes. Despite its much shorter format and con-
strained tasks, test-taker scores on the VAT 
1
strongly correspond to their scores from an ILR 
Oral Proficiency Interview. 
The paper is structured as follows: After re-
viewing related work, we describe Modern Stan-
dard Arabic and introduce the test construct (i.e., 
what the test is intended to measure) in detail (Sec-
tion 3). We then describe the structure and devel-
opment of the VAT in Section 4 and present 
evidence for its reliability and validity in Section 5. 
2 Related Work 
The use of automatic speech recognition appeared 
earliest in pronunciation tutoring systems in the 
field of language learning. Examples include SRI's 
AUTOGRADER (Bernstein et al, 1990), the CMU 
FLUENCY system (Eskenazi, 1996; Eskenazi & 
Hansma, 1998) and SRI's commercial EduSpeak 
system (Franco et al, 2000). In such systems, 
learner speech is typically evaluated by comparing 
features like phone duration, spectral characteris-
tics of phones and rate-of-speech to a model of 
native speaker performances. Systems evaluate 
learners? pronunciation and give some feedback. 
Automated measurement of more comprehen-
sive speaking and listening ability was first re-
ported by Townshend et al (1998), describing the 
early PhonePass test development at Ordinate. The 
PhonePass tests returned five diagnostic scores, 
including reading fluency, repeat fluency and lis-
tening vocabulary. Ordinate?s Spoken Spanish Test 
also included automatically scored passage re-
tellings that used an adapted form of latent seman-
tic analysis to estimate vocabulary scores.  
More recently at ETS, Zechner et al (2007) de-
scribe experiments in automatic scoring of test-
taker responses in a TOEFL iBT practice environ-
ment, focusing mostly on fluency features. Zechner 
and Xi (2008) report work on similar algorithms to 
score item types with varying degrees of response 
predictability, including items with a very re-
stricted range of possible answers (e.g., reading 
aloud) as well as item types with progressively less 
restricted answers (e.g., describing a picture ? rela-
tively predictable, or stating an opinion ? less pre-
dictable). The scoring mechanism in Zechner and 
Xi (2008) employs features such as the average 
number of word types or silences for fluency esti-
mation, the ASR HMM log-likelihood for pronun-
ciation or a vector-based similarity measure to 
assess vocabulary and content. Zechner and Xi 
present correlations of machine scores with human 
scores for two tasks: r=0.50 for an opinion task and 
r=0.69 for picture description, which are compara-
ble to the modest human rater agreement figures in 
this data. 
Balogh and Bernstein (2007) describe opera-
tional automated tests of spoken Spanish and Eng-
lish that return an overall ability score and four 
diagnostic subscores (sentence mastery, vocabu-
lary, fluency, pronunciation). The tests measure a 
learner's facility in listening to and speaking a for-
eign language. The facility construct can be tested 
by observing performance on many kinds of tasks 
that elicit responses in real time with varying, but 
generally high, predictability. More predictable 
items have two important advantages: As with do-
main restricted speech recognition tasks in general, 
the recognition of response content is more accu-
rate, but a higher precision scoring system is also 
possible as an independent effect beyond the 
greater recognition accuracy. Scoring is based on 
features like word stress, segmental form, latency 
or rate of speaking for the fluency and pronuncia-
tion subscores, and on response fidelity with ex-
pected responses for the two content subscores.  
Balogh and Bernstein report that their tests are 
highly reliable (r>0.95 for both English and Span-
ish) and that test scores strongly predict human 
ratings of oral proficiency based on Common 
European Framework of Reference language abil-
ity descriptors (r=0.88 English, r=0.90 Spanish).  
3 Versant Arabic Test: Facility in Mod-
ern Standard Arabic 
We describe a fully operational test of spoken 
MSA that follows the tests described in Balogh and 
Bernstein (2007) in structure and method, and in 
using the facility construct. There are two impor-
tant dimensions to the test's construct: One is the 
definition of what comprises MSA, and the other 
the definition of facility. 
3.1 Target Language: Modern Standard 
Arabic 
Modern Standard Arabic is a non-colloquial lan-
guage used throughout the Arabic-speaking world 
for writing and in spoken communication within 
public, literary, and educational settings. It differs 
from the colloquial dialects of Arabic that are spo-
ken in the countries of North Africa and the Mid-
2
dle East in lexicon and in syntax, for example in 
the use of explicit case and mood marking. 
Written MSA can be identified by its specific 
syntactic style and lexical forms. However, since 
all short vowels are omitted in normal printed ma-
terial, the word-final short vowels indicating case 
and mood are provided by the speaker, even when 
reading MSA aloud. This means that a text that is 
syntactically and lexically MSA can be read in a 
way that exhibits features of the regional dialect of 
the speaker if case and mood vowels are omitted or 
phonemes are realized in regional pronunciations. 
Also, a speaker's dialectal and educational back-
ground may influence the choice of lexical items 
and syntactic structures in spontaneous speech. 
The MSA spoken on radio and television in the 
Arab world therefore shows a significant variation 
of syntax, phonology, and lexicon. 
3.2 Facility 
We define facility in spoken MSA as the ability to 
understand and speak contemporary MSA as it is 
used in international communication for broadcast, 
for commerce, and for professional collaboration. 
Listening and speaking skills are assessed by ob-
serving test-taker performance on spoken tasks that 
demand understanding a spoken prompt, and for-
mulating and articulating a response in real time. 
Success on the real-time language tasks de-
pends on whether the test-taker can process spoken 
material efficiently. Automaticity is an important 
underlying factor in such efficient language proc-
essing (Cutler, 2003). Automaticity is the ability to 
access and retrieve lexical items, to build phrases 
and clause structures, and to articulate responses 
without conscious attention to the linguistic code 
(Cutler, 2003; Jescheniak et al, 2003; Levelt, 
2001). If processing is automatic, the lis-
tener/speaker can focus on the communicative con-
tent rather than on how the language code is 
structured. Latency and pace of the spoken re-
sponse can be seen as partial manifestation of the 
test-taker?s automaticity.  
Unlike the oral proficiency construct that coor-
dinates with the structure and scoring of OPI tests, 
the facility construct does not extend to social 
skills, higher cognitive functions (e.g., persuasion), 
or world knowledge. However, we show below 
that test scores for language facility predict almost 
all of the reliable variance in test scores for an in-
terview-based test of language and communication.  
4 Versant Arabic Test 
The VAT consists of five tasks with a total of 69 
items. Four diagnostic subscores as well as an 
overall score are returned. Test administration and 
scoring is fully automated and utilizes speech 
processing technology to estimate features of the 
speech signal and extract response content. 
4.1 Test Design 
The VAT items were designed to represent core 
syntactic constructions of MSA and probe a wide 
range of ability levels. To make sure that the VAT 
items used realistic language structures, texts were 
adapted from spontaneous spoken utterances found 
in international televised broadcasts with the vo-
cabulary altered to contain common words that a 
learner of Arabic may have encountered.  
Four educated native Arabic speakers wrote the 
items and five dialectically distinct native Arabic 
speakers (Arabic linguist/teachers) independently 
reviewed the items for correctness and appropri-
ateness of content.  Finally, fifteen educated native 
Arabic speakers (eight men and seven women) 
from seven different countries recorded the vetted 
items at a conversational pace, providing a range 
of native accents and MSA speaking styles in the 
item prompts.  
4.2 Test Tasks and Structure 
The VAT has five task types that are arranged in 
six sections (Parts A through F): Readings, Repeats 
(presented in two sections), Short Answer Ques-
tions, Sentence Builds, and Passage Retellings. 
These item types provide multiple, fully independ-
ent measures that underlie facility with spoken 
MSA, including phonological fluency, sentence 
construction and comprehension, passive and ac-
tive vocabulary use, and pronunciation of rhythmic 
and segmental units. 
Part A: Reading (6 items) In this task, test-
takers read six (out of eight) printed sentences, one 
at a time, in the order requested by the examiner 
voice. Reading items are printed in Arabic script 
with short vowels indicated as they would be in a 
basal school reader. Test-takers have the opportu-
nity to familiarize themselves with the reading 
items before the test begins. The sentences are 
relatively simple in structure and vocabulary, so 
they can be read easily and fluently by people edu-
3
cated in MSA.  For test-takers with little facility in 
spoken Arabic but with some reading skills, this 
task provides samples of pronunciation and oral 
rea
rly 
aut
the 
dem
                                                          
ding fluency. 
Parts B and E: Repeats (2x15 items) Test-
takers hear sentences and are asked to repeat them 
verbatim. The sentences were recorded by native 
speakers of Arabic at a conversational pace. Sen-
tences range in length from three words to at most 
twelve words, although few items are longer than 
nine words. To repeat a sentence longer than about 
seven syllables, the test-taker has to recognize the 
words as produced in a continuous stream of 
speech (Miller & Isard, 1963). Generally, the abil-
ity to repeat material is constrained by the size of 
the linguistic unit that a person can process in an 
automatic or nearly automatic fashion. The ability 
to repeat longer and longer items indicates more 
and more advanced language skills ? particula
omaticity with phrase and clause structures.  
Part C: Short Answer Questions (20 items) 
Test-takers listen to spoken questions in MSA and 
answer each question with a single word or short 
phrase. Each question asks for basic information or 
requires simple inferences based on time, se-
quence, number, lexical content, or logic. The 
questions are designed not to presume any special-
ist knowledge of specific facts of Arabic culture or 
other subject matter. An English example1 of a 
Short Answer Question would be ?Do you get milk 
from a bottle or a newspaper?? To answer the 
questions, the test-taker needs to identify the words 
in phonological and syntactic context, infer 
and proposition and formulate the answer. 
Part D: Sentence Building (10 items) Test-
takers are presented with three short phrases. The 
phrases are presented in a random order (excluding 
the original, naturally occurring phrase order), and 
the test-taker is asked to respond with a reasonable 
sentence that comprises exactly the three given 
phrases. An English example would be a prompt of 
?was reading - my mother - her favorite maga-
zine?, with the correct response: ?My mother was 
reading her favorite magazine.? In this task, the 
test-taker has to understand the possible meanings 
of each phrase and know how the phrases might be 
combined with the other phrasal material, both 
with regard to syntax and semantics. The length 
and complexity of the sentence that can be built is 
 (e.g., a syllable, a word 
or 
ly, 
 scored in this test. 
e within 
mpleted. 
 of facility with spoken MSA. The sub-
sc s
? 
 phrases and clauses in 
? 
ontext and 
? 
tructing, reading and re-
? 
 in a native-like manner 
1 See Pearson (2009) for Arabic example items. 
constrained by the size of the linguistic units with 
which the test-taker represents the prompt phrases 
in verbal working memory
a multi-word phrase). 
Part F: Passage Retelling (3 items) In this fi-
nal task, test-takers listen to a spoken passage 
(usually a story) and then are asked to retell the 
passage in their own words. Test-takers are en-
couraged to retell as much of the passage as they 
can, including the situation, characters, actions and 
ending. The passages are from 19 to 50 words 
long.  Passage Retellings require listening compre-
hension of extended speech and also provide addi-
tional samples of spontaneous speech. Current
this task is not automatically
4.3 Test Administration 
Administration of the test takes about 17 minutes 
and the test can be taken over the phone or via a 
computer. A single examiner voice presents all the 
spoken instructions in either English or Arabic and 
all the spoken instructions are also printed verba-
tim on a test paper or displayed on the computer 
screen. Test items are presented in Arabic by na-
tive speaker voices that are distinct from the exam-
iner voice. Each test administration contains 69 
items selected by a stratified random draw from a 
large item pool. Scores are available onlin
a few minutes after the test is co
4.4 Scoring Dimensions 
The VAT provides four diagnostic subscores that 
indicate the test-taker's ability profile over various 
dimensions
ore  are 
Sentence Mastery: Understanding, recalling,   
and producing MSA
complete sentences. 
Vocabulary: Understanding common words 
spoken in continuous sentence c
producing such words as needed. 
Fluency: Appropriate rhythm, phrasing and 
timing when cons
peating sentences. 
Pronunciation: Producing consonants, vow-
els, and lexical stress
in sentence context. 
4
The VAT also reports an Overall score, which 
is a weighted average of the four subscores (Sen-
, Vocabulary 20%, 
tion 20%). 
m was trained 
on 
ent 
val
onse networks for each 
ite
wo
 answers with ob-
ser
 the can-
did
linear 
mo
 Sentence 
Building items and Vocabulary is based on re-
n rt Answer Questions. 
inconsistent measure-
me
tence Mastery contributes 30%
Fluency 30%, and Pronuncia
4.5 Automated Scoring 
The VAT?s automated scoring syste
native and non-native responses to the test items 
as well as human ability judgments. 
Data Collection For the development of the 
VAT, a total of 246 hours of speech in response to 
the test items was collected from natives and learn-
ers and was transcribed by educated native speak-
ers of Arabic. Subsets of the response data were 
also rated for proficiency. Three trained native 
speakers produced about 7,500 judgments for each 
of the Fluency and the Pronunciation subscores (on 
a scale from 1-6, with 0 indicating missing data). 
The raters agreed well with one another at r?0.8 
(r=0.79 for Pronunciation, r=0.83 for Fluency). All 
test administrations included in the concurr
idation study (cf. Section 5 below) were ex-
cluded from the training of the scoring system. 
Automatic Speech Recognition Recognition is 
performed by an HMM-based recognizer built us-
ing the HTK toolkit (Young et al, 2000). Three-
state triphone acoustic models were trained on 130 
hours of non-native and 116 hours of native MSA 
speech. The expected resp
m were induced from the transcriptions of native 
and non-native responses. 
Since standard written Arabic does not mark 
short vowels, the pronunciation and meaning of 
written words is often ambiguous and words do not 
show case and mood markings. This is a challenge 
to Arabic ASR, since it complicates the creation of 
pronunciation dictionaries that link a word's sound 
to its written form. Words were represented with 
their fully voweled pronunciation (cf., Vergyri et 
al., 2008; Soltau et al, 2007).  We relied on hand-
corrected automatic diacritization of the standard 
written transcriptions to create fully-voweled 
rds from which phonemic representations were 
automatically created. 
The orthographic transcript of a test-taker utter-
ance in standard, unvoweled form is still ambigu-
ous with regard to the actual words uttered, since 
the same consonant string can have different mean-
ings depending on the vowels that are inserted. 
Moreover, the different words written in this way 
are usually semantically related, making them po-
tentially confusable for language learners. There-
fore, for system development, we transcribed 
words with full vowel marks whenever a vowel 
change would cause a change of meaning. This 
partial voweling procedure deviates from the stan-
dard way of writing, but it facilitated system-
internal comparison of target
ved test-taker utterances since the target pro-
nunciation was made explicit. 
Scoring Methods The Sentence Mastery and 
Vocabulary scores are derived from the accuracy 
of the test-taker's response (in terms of number of 
words inserted, deleted, or substituted by
ate), and the presence or absence of expected 
words in correct sequences, respectively. 
The Fluency and Pronunciation subscores are 
calculated by measuring the latency of the re-
sponse, the rate of speaking, the position and 
length of pauses, the stress and segmental forms of 
the words, and the pronunciation of the segments 
in the words within their lexical and phrasal con-
text. The final subscores are based on a non-linear 
combination of these features. The non-
del is trained on feature values and human 
judgments for native and non-native speech. 
Figure 1 shows how each subscore draws on re-
sponses from the different task types to yield a sta-
ble estimate of test-taker ability. The Pronunciation 
score is estimated from responses to Reading, Re-
peat and Sentence Build items. The Fluency score 
uses the same set of responses as for Pronuncia-
tion, but a different set of acoustic features are ex-
tracted and combined in the score. Sentence 
Mastery is derived from Repeat and
spo ses to the Sho
5 Evaluation 
For any test to be meaningful, two properties are 
crucial: Reliability and validity. Reliability repre-
sents how consistent and replicable the test scores 
are. Validity represents the extent to which one can 
justify making certain inferences or decisions on 
the basis of test scores. Reliability is a necessary 
condition for validity, since 
nts cannot support inferences that would justify 
real-world decision making. 
To investigate the reliability and the validity of 
the VAT, a concurrent validation study was con-
ducted in which a group of test-takers took both 
5
the VAT and the ILR OPI.  If the VAT scores are 
parable t  traditional 
his will be a 
tive functioning in the target language. 
Th
ning Arabic 
co .S., and at least 11 were gradu-
ter for Arabic Studies Abroad 
between one rater and 
the
e taker took the VAT twice, we can 
                                                          
com o scores from a reliable
measure of oral proficiency in MSA, t
piece of evidence that the VAT indeed captures 
important aspects of test-takers' abilities in using 
spoken MSA. 
As additional evidence to establish the validity 
of the VAT, we examined the performance of the 
native and non-native speaker groups. Since the 
test claims to measure facility in understanding and 
speaking MSA, most educated native speakers 
should do quite well on the test, whereas the scores 
of the non-native test-takers should spread out ac-
cording to their ability level. Furthermore, one 
would also expect that educated native speakers 
would perform equally well regardless of specific 
national dialect backgrounds and no important 
score differences among different national groups 
of educated native speakers should be observed.  
5.1 Concurrent Validation Study 
ILR OPIs.  The ILR Oral Proficiency Interview is 
a well-established test of spoken language per-
formance, and serves as the standard evaluation 
tool used by United States government agencies 
(see www.govtilr.org). The test is a structured in-
terview that elicits spoken performances that are 
graded according to the ILR skill levels. These 
levels describe the test-taker?s ability in terms of 
communica
e OPI test construct is therefore different from 
that of the VAT, which measures facility with spo-
ken Arabic, and not communicative ability, as 
such. 
Concurrent Sample. A total of 118 test-takers 
(112 non-natives and six Arabic natives) took two 
VATs and two ILR OPIs. Each test-taker com-
pleted all four tests within a 15 day window. The 
mean age of the test-takers was 27 years old (SD = 
7) and the male-to-female split was 60-to-58. Of 
the non-native speakers in this concurrent testing 
sample, at least 20 test-takers were lear
at a llege in the U
ates from the Cen
program. Nine test-takers were recruited at a lan-
guage school in Cairo, Egypt, and the remainder 
were current or former students of Arabic recruited 
in the US. 
Seven active government-certified oral profi-
ciency interviewers conducted the ILR OPIs over 
the telephone. Each OPI was administered by two 
interviewers who submitted the performance rat-
ings independently after each interview. The aver-
age inter-rater correlation 
 average score given by the other two raters 
administering the same test-taker's other interview 
was 0.90. 
The test scores used in the concurrent study are 
the VAT Overall score, reported here in a range 
from 10 to 90, and the ILR OPI scores with levels 
{0, 0+, 1, 1+, 2, 2+, 3, 3+, 4, 4+, 5}2. 
5.2 Reliability 
Sinc each test-
estimate the VAT?s reliability using the test-retest 
method (e.g., Crocker & Algina, 1986: 133). The 
2 All plus ratings (e.g., 1+, 2+, etc) were converted with 0.5 
(e.g, 1.5, 2.5, etc) in the analysis reported in this paper. 
Figure 1: Relation of su
 
bscores to item types. 
6
correlation between the scores from the first ad-
ministration and the scores from the second ad-
mi
 are reliable at r=0.91 
(th
ased test of oral proficiency in 
M .
the V  MSA 
sp k
h
with ILR OPI scores, despite the difference in con-
dict native performance)  
score distributions of test-taker 
nistration was found to be at r=0.97, indicating 
high reliability of the VAT test. The scores from 
one test administration explain 0.972=94% of the 
score variance in another test administration to the 
same group of test-takers. 
We also compute the reliability of the ILR OPI 
scores for each test taker by correlating the aver-
ages of the ratings for each of the two test admini-
strations. The OPI scores
us 83% of the variance in the test scores are 
shared by the scores of another administration). 
This indicates that the OPI procedure implemented 
in the validation study was relatively consistent. 
5.3 Validity 
Evidence here for VAT score validity comes from 
two sources: the prediction of ILR OPI scores (as-
sumed for now to be valid) and the performance 
distribution of native and non-native test takers. 
Prediction of ILR OPI Test Scores.  For the 
comparison of the VAT to the ILR OPI, a scaled 
average OPI score was computed for each test-
taker from all the available ILR OPI ratings. The 
scaling was performed using a computer program, 
FACETS, which takes into account rater severity 
and test-taker ability and therefore produces a 
fairer estimate than a simple average (Linacre et 
al., 1990; Linacre, 2003).  
Figure 2 is a scatterplot of the ILR OPI scores 
and VAT scores for the concurrent validation sam-
ple (N=118). IRT scaling of the ILR scores allows 
a mapping of the scaled OPI scores and the VAT 
scores onto the original OPI levels, which are 
given on the inside of the plot axes. The correlation 
coefficient of the two test scores is r=0.87. This is 
roughly in the same range as both the ILR OPI re-
liability and the average ILR OPI inter-rater corre-
lation. The test scores on the VAT account for 76% 
of the variation in the ILR OPI scores (in contrast 
to 83% accounted for by another ILR OPI test ad-
ministration and 81% accounted for by one other 
ILR OPI interviewer). 
The VAT accounts for most of the variance in 
the interview-b
SA  This is one form of confirming evidence that 
AT captures important aspects of
ea ing and listening ability. 
T e close correspondence of the VAT scores 
struct, may come about because candidates easily 
transfer basic social and communicative skills ac-
quired in their native language, as long as they are 
able to correctly and efficiently process (i.e., com-
prehend and produce) the second language. Also, 
highly proficient learners have most likely ac-
quired their skills at least to some extent in social 
interaction with native speakers of their second 
language and therefore know how to interact ap-
propriately. 
Group Performance.  Finally, we examine the 
score distributions for different groups of test-
takers to investigate whether three basic expecta-
tions are met:  
? Native speakers all perform well, while non-
natives show a range of ability levels 
? Non-native speakers spread widely across 
the scoring scale (the test can distinguish 
well between a range of non-native ability   
levels)  
? Native speakers from different countries per-
form similarly (national origin does not pre-
We compare the 
groups in the training data set, which contains 
1309 native and 1337 non-native tests. For each 
test in the data set, an Overall score is computed by 
the trained scoring system on the basis of the re-
corded responses. Figure 3 presents cumulative 
distribution functions of the VAT overall scores, 
showing for each score which percentage of test-
takers performs at or below that level. This figure 
compares two speaker groups: Educated native 
speakers of Arabic and learners of Arabic. The 
Figure 2: Test-takers' ILR OPI scores as a function 
of VAT scores (r=0.87; N=118). 
 
7
score distributions of the native speakers and the 
learner sample are clearly different. For example, 
fewer than 5% of the native speakers score below  
70, while fewer than 10% of the learners score 
above 70. Further, the shape of the learner curve 
bution of scores, suggesting 
that the VAT discriminates well in the range of 
. The Mo-
ccan speakers are slightly separate from the other 
native speakers, but only a negligible number of 
them scores lower than 70, a score that less than 
10% of learners achieve.  This finding supports the 
notion that the VAT scores reflect a speaker's facil-
ity in spoken MSA, irrespective of the speaker's 
country of origin. 
6 Conclusion 
We have presented an automatically scored test of 
facility with spoken Modern Standard Arabic 
(MSA). The test yields an ability profile over four 
subscores, Fluency and Pronunciation (manner-of-
speaking) as well as Sentence Mastery and Vo-
cabulary (content), and generates a single Overall 
score as the weighted average of the subscores. We 
have presented data from a validation study with 
native and non-native test-takers that shows the 
VAT to be highly reliable (test-retest r=0.97). We 
also have presented validity evidence for justifying 
the use of VAT scores as a measure of oral profi-
ciency in MSA.  While educated native speakers of 
Arabic can score high on the test regardless of their 
country of origin because they all possess high fa-
cility in spoken MSA, learners of Arabic score dif-
ferently according to their ability levels; the VAT 
test scores account for most of the variance in the 
interview-based ILR OPI for MSA, indicating that 
the VAT captures a major feature of oral profi-
ciency.  
In summary, the empirical validation data sug-
gests that the VAT can be an efficient, practical 
alternative to interview-based proficiency testing 
in many settings, and that VAT scores can be used 
to inform decisions in which a person?s listening 
and speaking ability in Modern Standard Arabic 
should play a part. 
Acknowledgments 
The reported work was conducted under contract 
W912SU-06-P-0041 from the U.S. Dept. of the 
Army.  The authors thank Andy Freeman for pro-
viding diacritic markings, and to Waheed Samy, 
Naima Bousofara Omar, Eli Andrews, Mohamed 
Al-Saffar, Nazir Kikhia, Rula Kikhia, and Linda 
Istanbulli for support with item development and 
data collection/transcription in Arabic.  
 
Figure 4: Score distributions for native speakers 
of different countries of origin. 
indicates a wide distri
abilities of learners of Arabic as a foreign lan-
guage.  
Figure 4 is also a cumulative distribution func-
tions, but it shows score distributions for native 
speakers by country of origin (showing only coun-
tries with at least 40 test-takers). The curves for 
Egyptian, Syrian, Iraqi, Palestinian, Saudi and 
Yemeni speakers are indistinguishable
ro
Figure 3: Score distributions for native and non-
native speakers. 
8
References 
Jennifer Balogh and Jared Bernstein. 2007. Workable 
models of standard performance in English and 
Spanish. In Y. Matsumoto, D. Oshima, O. Robinson, 
and P. Sells, editors, Diversity in Language: Per-
spectives and Implications (CSLI Lecture Notes, 
176), 271-292. CSLI, Stanford, CA. 
Jared Bernstein and Isabella Barbier. 2001. Design and 
development parameters for a rapid automatic 
screening test for prospective simultaneous inter-
preters. Interpreting, International Journal of Re-
search and Practice in Interpreting, 5(2): 221-238. 
Jared Bernstein, Michael Cohen, Hy Murveit, Dmitry 
Rtischev, and Mitch Weintraub. 1990. Automatic 
evaluation and training in English pronunciation. In 
Proceedings of ICSLP, 1185-1188. 
Linda Crocker and James Algina. 1986. Introduction to 
Classical & Modern Test Theory. Harcourt Brace 
Jovanovich, Orland, FL. 
Anne Cutler. 2003. Lexical access. In L. Nadel, editor, 
Encyclopedia of Cognitive Science, volume 2, pp. 
858-864. Nature Publishing Group. 
Maxine Eskenazi. 1996. Detection of foreign speakers? 
pronunciation errors for second language training ? 
preliminary results. In Proceedings of ICSLP ?96. 
Maxine Eskenazi and Scott Hansma. 1998. The fluency 
pronunciation trainer. In Proceedings of the STiLL 
Workshop.  
Horacio Franco, Victor Abrash, Kristin Precoda, Harry 
Bratt, Raman Rao, John Butzberger, Romain Ross-
ier, and Federico Cesar. 2000. The SRI EduSpeak 
system: Recognition and pronunciation scoring for 
language learning. In Proceedings of InSTiLL, 123-
128. 
J?rg Jescheniak, Anja Hahne, and Herbert Schriefers. 
2003. Information flow in the mental lexicon during 
speech planning: Evidence from event-related poten-
tials. Cognitive Brain Research, 15(3):858-864. 
Willem Levelt. 2001. Spoken word production: A the-
ory of lexical access. Proceedings of the National 
Academy of Sciences, 98(23):13464-13471. 
John Linacre. 2003. FACETS Rasch measurement com-
puter program. Winstep, Chicago, IL.  
John Linacre, Benjamin Wright, and Mary Lunz. 1990. 
A Facets model for judgmental scoring. Memo 61. 
MESA Psychometric Laboratory. University of Chi-
cago. Retrieved April 14, 2009, from http:// 
http://www.rasch.org/memo61.htm. 
George Miller and Stephen Isard. 1963. Some percep-
tual consequences of linguistic rules. Journal of 
Verbal Learning and Verbal Behavior, 2:217-228. 
 
 
 
Pearson. 2009. Versant Arabic test ? test description 
and validation summary. Pearson. Retrieved April 
14, 2009, from 
http://www.ordinate.com/technology/VersantArabic
TestValidation.pdf. 
Hagen Soltau, George Saon, Daniel Povy, Lidia Mangu, 
Brian Kingsbury, Jeff Kuo, Mohamed Omar, and 
Geoffrey Zweig. 2007. The IBM 2006 GALE Arabic 
ASR system. In Proceedings of ICASSP 2007, 349-
352. 
Brent Townshend, Jared Bernstein, Ognjen Todic & 
Eryk Warren. 1998. Estimation of Spoken Language 
Proficiency. In STiLL: Speech Technology in Lan-
guage Learning, 177-180.  
Dimitra Vergyri, Arindam Mandal, Wen Wang, An-
dreas Stolcke, Jing Zheng, Martin Graciarena, David 
Rybach, Christian Gollan, Ralf Schl?ter, Karin 
Kirchhoff, Arlo Faria, and Nelson Morgan. 2008. 
Development of the SRI/Nightingale Arabic ASR 
system. In Proceedings of Interspeech 2008, 1437-
1440. 
Steve Young, Dan Kershaw, Julian Odell, Dave Ol-
lason, Valtcho Valtchev, and Phil Woodland. 2000. 
The HTK Book Version 3.0. Cambridge University 
Press, Cambridge, UK. 
Klaus Zechner and Xiaoming Xi. 2008. Towards auto-
matic scoring of a test of spoken language with het-
erogeneous task types. In Proceedings of the Third 
Workshop on Innovative Use of NLP for Building 
Educational Applications, 98-106. 
Klaus Zechner, Derrick Higgins, and Xiaoming Xi. 
2007. SpeechRater?: A construct-driven approach 
to score spontaneous non-native speech. In Proceed-
ings of the Workshop of the ISCA SIG on Speech and 
Language Technology in Education. 
9
A Flexible, Corpus-Driven Model of Regular
and Inverse Selectional Preferences
Katrin Erk?
University of Texas at Austin
Sebastian Pad???
Heidelberg University
Ulrike Pad??
Vico Research and Consulting GmbH
We present a vector space?based model for selectional preferences that predicts plausibility
scores for argument headwords. It does not require any lexical resources (such as WordNet). It
can be trained either on one corpus with syntactic annotation, or on a combination of a small
semantically annotated primary corpus and a large, syntactically analyzed generalization cor-
pus. Our model is able to predict inverse selectional preferences, that is, plausibility scores for
predicates given argument heads.
We evaluate our model on one NLP task (pseudo-disambiguation) and one cognitive task
(prediction of human plausibility judgments), gauging the influence of different parameters and
comparing our model against other model classes. We obtain consistent benefits from using the
disambiguation and semantic role information provided by a semantically tagged primary cor-
pus. As for parameters, we identify settings that yield good performance across a range of experi-
mental conditions. However, frequency remains a major influence of prediction quality, and
we also identify more robust parameter settings suitable for applications with many infrequent
items.
1. Introduction
Selectional preferences or selectional constraints describe knowledge about possible
and plausible fillers for a predicate?s argument positions. They model the fact that there
is often a semantically coherent set of concepts that can fill a given argument posi-
tion. Selectional preferences can help for many text analysis tasks which involve com-
paring different attachment decisions. Examples include syntactic disambiguation
(Hindle and Rooth 1993; Toutanova et al 2005), word sense disambiguation (WSD,
? Department of Linguistics, Calhoun Hall 512, 1 University Station B 5100, Austin, TX 78712.
E-mail: katrin.erk@mail.utexas.edu.
?? E-mail: pado@cl.uni-heidelberg.de.
? E-mail: ulrike.pado@vico-research.com.
Submission received: 26 November 2009; revised submission received: 6 May 2010; accepted for publication:
29 June 2010. Part of the work reported in this article was done while S. P. was a postdoctoral scholar and U. P.
a visiting scholar at Stanford University.
? 2010 Association for Computational Linguistics
Computational Linguistics Volume 36, Number 4
McCarthy and Carroll 2003), semantic role labeling (SRL, Gildea and Jurafsky 2002), and
characterizing the conditions under which entailment holds between two predicates
(Zanzotto, Pennacchiotti, and Pazienza 2006; Pantel et al 2007). Furthermore, selec-
tional preferences are also helpful for determining linguistic properties of predicates
and predicate?argument combinations, for example in compositionality assessment
(McCarthy, Venkatapathy, and Joshi 2007) or the detection of diathesis alternations
(McCarthy 2000). In psycholinguistics, selectional preferences predict human plausibil-
ity judgments for predicate?argument combinations (Resnik 1996) and effects in human
sentence reading times (Pad?, Crocker, and Keller 2009).
All these applications rely on the availability of broad-coverage, reliable selectional
preferences for predicates and their argument positions. Given the immense effort nec-
essary for manual semantic lexicon building and its associated reliability problems (see,
e.g., Briscoe and Boguraev 1989), all contemporary models of selectional preferences
acquire selectional preferences automatically from large corpora.
The simplest strategy is to extract triples (v, r, a) of a predicate, role, and argument
headword (or filler) from a corpus, and then to compute selectional preference as
relative frequencies. However, due to the Zipfian nature of word frequencies, the first
step on its own results in a very sparse list of headwords, in particular for less frequent
predicates. As an example, the verb anglicize only appears with nine direct objects in
the 100-million word British National Corpus (BNC, Burnard 1995). Only one of them,
name, appears more than once. Many highly plausible fillers are missing from the list,
such as word or spelling.
In order to make sensible predictions for triples that are unseen at training time,
it is crucial to add a generalization step that infers a degree of preference for new,
unseen headwords for a given predicate and role.1 The result is, in the ideal case, an
assignment to every possible headword of some degree of compatibility (or plausibil-
ity) with the predicate?s preferences. In the case of anglicize, the desired result would be
a high plausibility for words like the (previously seen) wordlist and surname as well as
the (unseen) word and spelling, and a low plausibility for (likewise unseen) words like
cow and machine.
The predominant approach to generalizing over headwords, first introduced by
Resnik (1996), is based on semantic hierarchies such as WordNet (Miller et al 1990). The
idea is to map all observed headwords onto synsets, and then generalize to a characteri-
zation of the selectional preference in terms of the WordNet noun hierarchy. This can be
achieved in many different ways (Abe and Li 1996; Resnik 1996; Ciaramita and Johnson
2000; Clark and Weir 2001). The performance of these models relies on the coverage
of the lexical resources, which can be a problem even for English (Gildea and Jurafsky
2002). An alternative approach to generalization uses co-occurrence information, either
in the form of distributional models or through a clustering approach. These models,
which avoid dependence on lexical resources, use corpus data for generalization
(Dagan, Lee, and Pereira 1999; Rooth et al 1999; Bergsma, Lin, and Goebel 2008).
In this article, we present a lightweight model for the acquisition and representa-
tion of selectional preferences. Our model is fully distributional and does not require
any knowledge sources beyond a large corpus where subjects and objects can be iden-
tified with reasonable accuracy. Its key point is to use vector space similarity (Lund
and Burgess 1996; Laundauer and Dumais 1997) to generalize from seen to unseen
1 Some approaches also fix a role and headword list and generalize from seen predicates to other, similar
predicates.
724
Erk, Pad?, and Pad? A Flexible, Corpus-Driven Model of Selectional Preferences
headwords. The vector space representations which serve as a basis for computing
similarity can in principle be computed from any arbitrary corpus, given that it is large
enough. In particular, this need not be the same corpus as the one on which we observe
predicate?headword co-occurrences. Our model thus distinguishes between a primary
corpus, from which the predicate?role?headword triples are extracted, and a generali-
zation corpus for computing the vector space representations. This distinction makes
it possible to apply our model to primary corpora with rich information that are too
small for efficient generalization, such as domain-specific corpora or corpora with
deeper linguistic analysis, as long as a larger, even if potentially noisier, generalization
corpus is available. We empirically demonstrate the benefit of this distinction. We use
FrameNet (Fillmore, Johnson, and Petruck 2003) as primary corpus and the BNC as
generalization corpus, modeling selectional preferences for semantic roles with near-
perfect coverage and low error rate.2
We evaluate our model on two tasks. The first task is pseudo-disambiguation
(Yarowsky 1993), where the model decides which of two randomly chosen words is a
better filler for the given argument position. This task tests model properties that are
needed for concrete semantic analysis tasks, most notably word sense disambiguation,
but also for semantic role labeling. The second task is the prediction of human
plausibility ratings, which is a standard task-independent benchmark for the quality
of selectional preferences. We test our model across a range of parameter settings to
identify best-practice values and show that it robustly outperforms both WordNet-
based and other distributional models on both tasks.
Finally, we investigate inverse preferences, that is, preferences that arguments
have for their predicates. Although there is ample cognitive evidence for the existence
of such preferences (e.g., McRae et al 2005), to our knowledge, they have not been in-
vestigated systematically in linguistics. However, statistics about inverse preferences
have been used implicitly in computational linguistics (e.g., Hindle 1990; Rooth et al
1999). We investigate the properties of inverse selectional preferences in comparison to
regular selectional preferences, and show that it is possible to predict inverse prefer-
ences with our selectional preference model as well.
The model that we discuss in this article, EPP, was first introduced in Erk (2007)
(using a pseudo-disambiguation task for evaluation) and further studied by Pad?, Pad?,
and Erk (2007) (evaluating against human plausibility judgments). In the current text,
we perform a more extensive evaluation and analysis, including the new evaluation on
inverse preferences, and we introduce a new similarity measure, nGCM, which achieves
excellent performance in many settings.
2. Computational Models of Selectional Preferences
In this section, we provide an overview of corpus-based models of selectional prefer-
ences. See Table 1 for a summary of the notation that we use.
2 As descriptions of semantic classes of participants in events, selectional preferences are most naturally
applied to semantic argument positions, that is, semantic roles (such as agent or patient). In contrast,
syntactic argument positions (like subject and object) can comprise several semantic argument positions,
due to the presence of diathesis alternations, and thus show less consistent selectional preferences.
Nevertheless, work in computational linguistics also makes use of selectional preferences for syntactic
argument positions, considering them noisy approximations of semantic argument positions.
725
Computational Linguistics Volume 36, Number 4
Table 1
Notation used throughout the article.
w ? Lemmas Word. We assume lemmatization throughout.
v ? Preds Predicate. Preds may be a subset of Lemmas, or a set of
semantic classes.
r ? Roles Role/Argument slot. Roles may be a set of grammatical
functions, or of semantic roles.
a ? Args ? Lemmas (Potential) argument headword.
c ? C Semantic class on which selectional preferences are
conditioned, for example, WordNet sense, FrameNet frame,
or latent semantic class.
VS = (DTrans,
Basis, sim, STrans)
Vector space. Basis is a set of basis elements, sima similarity
measure, DTrans a transformation of raw counts, and STrans
a transformation of the space.
We write w = ?wb1 , . . . , wbn? for the representation of w ?
Lemmas in a vector space with Basis = {b1, . . . , bn}.
wtr,v(a) Weight of argument headword a for predicate v and role r.
2.1 Historical Models
In formal linguistics, selectional restrictions were employed as strict Boolean restrictions
by Katz and Postal (Katz and Fodor 1963; Katz and Postal 1964) as input to a mutual dis-
ambiguation process between predicates and their modifiers. Sentences are semantically
anomalous if there are no mutually consistent readings for the two words. Semantically
anomalous sentences would receive no reading, whereas ambiguous sentences would
receive several readings.
The strict dismissal as meaningless of sentences that violate selectional restrictions
was later criticized. A case in point is metaphors, which often combine predicates and
arguments from different domains (Lakoff and Johnson 1980). Wilks (1975:329) stated
that ?rejecting utterances is just what humans do not. They try to understand them.?
He proposes to reconceptualize selectional restrictions as preferences whose violation
is dispreferred, but not fatal. His proposal for a semantic interpretation mechanism still
uses semantic primitives, but always produces a single most plausible interpretation by
choosing the senses of each word that maximize the compatibility between selectional
preferences and semantic types. In this manner, he is able to compute semantic repre-
sentations for sentences that violate selectional restrictions, including metaphors such
as ?my car drinks gasoline.?
2.2 Semantic Hierarchy?Based Models
The first broad-coverage computational model of selectional preferences, and still one
of the best-known ones, namely that of Resnik (1996), belongs to the class of semantic
hierarchy?based models. These models generalize over observed headwords using a
semantic hierarchy or ontology for nouns. The two main advantages of such models are
that (a) they can make predictions for all words covered by the hierarchy, even for very
infrequent ones for which distributional representations tend to be unreliable; and (b)
the hierarchy robustly guides generalization even for few observed headwords.
Resnik?s model instantiates the set of relations Roles with grammatical functions
which can be observed in syntactically analyzed corpora. More specifically, it concen-
726
Erk, Pad?, and Pad? A Flexible, Corpus-Driven Model of Selectional Preferences
trates on selectional preferences for subjects and objects. For the generalization step,
Resnik?s model maps all headwords onto WordNet synsets (or classes) c. Resnik first
computes the overall selectional preference strength for each verb?relation pair (v, r),
that is, the degree to which the pair constrains possible fillers. To estimate this quantity,
the distribution of WordNet synsets for this particular verb?relation pair is compared
to the distribution of synsets over all verbs, given the relation r. Technically, this is
achieved using Kullback?Leibler divergence:
SelStr(v, r) = D(P(c|v, r)||P(c|r)) =
?
c?C
P(c|v, r)log(P(c|v, r)
P(c|r) ) (1)
The parameters P(c|v, r) and P(c|r) are estimated from the corpus frequencies of tuples
(v, r, a) and the membership of nouns a in WordNet classes c: The observed frequency
of (v, r, a) is split equally among all WordNet classes for a. This avoids word sense
disambiguration, but incurs a certain share of wrong attributions. The intuition of
SelStr(v, r) is that a verb?relation pair that only allows a limited range of argument heads
will have a posterior distribution over classes that strongly diverges from the prior.
Next, the selectional association of the triple, SelAssoc(v, r, c), is computed as the
ratio of the selectional preference strength for this particular class c to the overall selec-
tional preference strength of the verb?relation pair (v, r). This is shown in Equation (2).
SelAssoc(v, r, c) =
P(c|v, r)logP(c|v,r)P(c|r)
SelStr(v, r)
(2)
Finally, the selectional preference between a verb, a relation, and an argument head
is defined as the maximal selectional association of the verb, the relation, and any
WordNet class c that the argument can instantiate. We will refer to this model as
RESNIK herein.
In subsequent years, a number of WordNet-based models were developed that
differ from Resnik?s model in the details of how the generalization in the WordNet
hierarchy is performed. Abe and Li (1996) characterize selectional preferences by a
tree cut through the WordNet noun hierarchy that minimizes tree cut length while
maximizing accuracy of prediction. Clark and Weir (2001) perform generalization by
ascending the WordNet noun hierarchy as long as the degree of selectional preference
among siblings is not significantly different. Ciaramita and Johnson (2000) encode
WordNet in a Bayesian Network to take advantage of the Bayes nets? ability to ?ex-
plain away? ambiguity. Grishman and Sterling (1992) perform generalization on the
basis of a manually constructed semantic hierarchy specifically developed on the same
corpus.
2.3 Distributional Models
Distributional models do not make use of any lexicon resource for the generalization
step. Instead, they use word co-occurrence?typically obtained from the same corpus
as the observed headwords?for generalization. This independence from manually
constructed resources gives distributional models a good cost?benefit ratio and makes
them especially attractive for domain-specific applications. These models, like the
727
Computational Linguistics Volume 36, Number 4
semantic hierarchy?based models, usually use grammatical functions as the set Roles
for which selectional preferences are predicted.
Pereira, Tishby, and Lee (1993) and Rooth et al (1999) generalize by discovering
latent classes of noun?verb pairs with soft clustering. They model the probability of
a word a as the argument of a predicate v as the probability of generating v and a
independently from the latent classes c:
P(v, a) =
?
c?C
P(c, v, a) =
?
c?C
P(c)P(v|c)P(a|c) (3)
Pereira, Tishby, and Lee (1993) develop a task-specific procedure to optimize P(c),
P(v|c), and P(a|c). Their procedure supports hierarchical clustering and can optimize
the number of clusters. Rooth et al (1999) present a simpler Expectation Maximization?
based estimation procedure which takes the number of clusters as input parameter. We
refer to this model as ROOTH ET AL. herein.
Dagan, Lee, and Pereira (1999) introduce a general model for computing co-
occurrence probabilities with similarity-based smoothing. Although not intended as a
model of selectional preferences, it can also be interpreted as such. Given a similarity
measure sim defined on word pairs, they compute the smoothed occurrence probability
of a word w2 given w1 as
Psim(w2|w1) =
?
w?Simset(w1)
sim(w1, w)
Z(w1)
P(w2|w) (4)
where Simset(w) is the set of words most similar to w according to sim, and Z(w1) =
?
w?Simset(w1) sim(w1, w) is a normalizing factor. This model predicts w2 given w1 by
backing off from w1 to words w similar to w1. The contribution of each w in predicting
P(w2|w1) is weighted by sim(w1, w). The similarity sim(w1, w) is computed on vector
space representations.
Recently, Bergsma, Lin, and Goebel (2008) have adopted a discriminative ap-
proach to the prediction of selectional preferences. The features they use are mainly co-
occurrence statistics, enriched with morphological context features to alleviate sparse
data problems for low-frequency argument heads. They train one SVM per verb?
argument position pair, using unobserved verb?argument combinations as negative
examples, which makes their approach independent of manually annotated training
data. Schulte im Walde et al (2008) present a model that combines features of the
semantic hierarchy?based and the distributional approaches by integrating WordNet
into an EM-based clustering model; Schulte im Walde (2010) shows that integrating
noun?modifier relations improves the prediction of human plausibility judgments.
2.4 Semantic Role?Based Models
The third class of models takes advantage of semantic resources beyond simple seman-
tic hierarchies, notably of corpora with semantic role annotation. Such corpora allow the
prediction of selectional preferences for semantic roles rather than grammatical func-
tions. From a linguistic perspective, semantic roles represent a more appropriate level
for defining selectional preferences. For that reason, the role annotation provides cleaner
and more specific training data than even a manually syntactically annotated corpus
728
Erk, Pad?, and Pad? A Flexible, Corpus-Driven Model of Selectional Preferences
would. These advantages, however, come at the cost of considerably greater sparsity
issues.
Pad?, Crocker, and Keller (2009) present a model based on FrameNet (Fillmore,
Johnson, and Petruck 2003). This model estimates selectional preferences with a gen-
erative probability model that equates the plausibility of a (v, r, a) triple with the joint
probability of observing the thematic role r, the verb v, and the argument a, plus the
verb?s FrameNet sense c and the grammatical function gf of the argument. This joint
probability can be decomposed using the chain rule:
P(v, c, r, gf , a) = P(v)P(c|v)P(r|v, c)P(gf |r, v, c)P(a|gf , r, v, c) (5)
The model does not make any independence assumptions. To counteract sparse data
issues for the more complex terms, the model applies WordNet-based generalization
(for nouns), distributional clustering (for verbs), and Good?Turing smoothing. We refer
to this model as PADO ET AL. Another semantic role?based model was proposed by
Vandekerckhove, Sandra, and Daelemans (2009). It acquires selectional preferences for
PropBank roles from a PropBank-labeled corpus, generalizing to unseen headwords
with memory-based learning.
3. A Distributional Exemplar-Based Model of Selectional Preferences: EPP
We now present the EPP model of selectional preferences. It falls into the category of
distributional models. More specifically, it is an exemplar model that remembers all
seen headwords for a given argument position and computes the degree of plausibility
for a new headword candidate through its similarity to the stored exemplars. Exemplars
are modeled as vectors in a semantic space.
Exemplar models are a well-known modeling framework that is used in psychol-
ogy (Nosofsky 1986), in computational linguistics (under the name of memory-based
learning [Daelemans and van der Bosch 2005]), and in linguistics, particularly phonet-
ics (Hay, Nolan, and Drager 2006). The appeal of exemplar models is that they provide
a cognitively plausible process of learning as storing exemplars, and categorization as
similarity computation that is grounded in features of the exemplars (e.g., formants in
phonetics, and contexts in lexical semantics).
The representation of selectional preferences through feature vectors also fits in well
with work in psycholinguistics by McRae, Ferretti, and Amyote (1997), who studied the
characterization of verb selectional preferences through features elicited from human
subjects. They found high overlap between features used to characterize the selectional
preferences on the one hand, and features listed for typical role fillers on the other hand.
For example, features generated for the agent role of frighten include mean, scary, and
ugly, features that were also highly relevant for the typical filler noun monster.
As briefly mentioned in Section 1, we consider selectional preferences to be charac-
terizations of typical fillers for the semantic roles of a predicate. Still, we keep our model
modular to different notions of argumenthood, such that it is also applicable to the
computation of selectional preferences for syntactic dependents of a predicate, as this is
an important case for computational applications. When we compute selectional prefer-
ences for syntactic dependents rather than semantic roles, we view syntactic argument
positions as noisy approximations of semantic roles.
729
Computational Linguistics Volume 36, Number 4
3.1 The Model
As stated previously, we assume that we have two corpora which assume different func-
tions in the model: the primary corpus, which provides information about predicate?
argument co-occurrences but may be too sparse for generalization; and the large, but
potentially noisy, generalization corpus, from which we obtain reliable semantic simi-
larity estimates.
Thus, the first step is the extraction of triples (v, r, a) of a predicate v ? Preds, a
relation r ? Roles, and a headword a ? Args from the primary corpus. Let Seenargs(r, v)
be the set of argument headwords seen with an argument position r of a predicate v
in the primary corpus. Given these triples, we predict the plausibility for an arbitrary
noun a0 in position (v, r) through the semantic similarity of a0 to all the members
of Seenargs(r, v). We obtain these similarity ratings by first computing vector space
representations for both and the members of seen(r, v) from the generalization corpus,
and then using a standard vector space similarity measure. We compute the plausibility
for a0 as
SelprefEPPr,v(a0) =
?
a?Seenargs(r,v)
wtr,v(a)
Zr,v
? sim(a0, a) (6)
where sim(a0, a) is the similarity between the vector space representations of a0 and
a, wtr,v(a) a weight for the seen headword a, and Zr,v a normalization constant, Zr,v =
?
a?Seenargs(r,v) wtr,v(a), so that the number of observed exemplars for each (v, r) pair does
not matter. Because SelprefEPP is basically a weighted average over similarity values, the
range of SelprefEPP is identical to the range of the employed similarity function sim. For
example, the range is [?1, 1] for cosine similarity, or [0, 1] for the Jaccard coefficient (cf.
Section 3.3). We discuss possible choices of both the similarity sim and the weight wtr,v
in Section 3.3.
3.2 Vector Space Representations
We use vector space representations for generalization. In a vector space model, each
target word is represented as a vector, typically constructed from co-occurrence counts
with context words in a large corpus (the so-called basis elements). The underlying
assumption, which goes back to Firth (1957) and Harris (1968), is that words with similar
meanings occur in similar contexts and will be assigned similar vectors. Thus, the
distance between the vectors of two target words, as given by some distance measure
(e.g., Cosine or Jaccard), reflects their semantic similarity.
Vector space models are simple to construct, and the semantic similarity they pro-
vide has found a wide range of applications. Examples in NLP include information
retrieval (Salton, Wong, and Yang 1975), automatic thesaurus extraction (Grefenstette
1994), and predominant sense identification (McCarthy et al 2004). Lexical resources
based on distributional similarity (e.g., Lin [1998]?s thesaurus) are used in a wide range
of applications that profit from knowledge about word similarity. In cognitive science,
they have been used, for example, to account for the influence of context on human
lexical processing (McDonald and Brew 2004) and lexical priming (Lowe and McDonald
2000).
An idealized example for a semantic space representation of selectional preferences
is shown in Figure 1(a). The two ellipses represent the exemplar clouds formed by the
730
Erk, Pad?, and Pad? A Flexible, Corpus-Driven Model of Selectional Preferences
Figure 1
An idealized vector space for the plausibilities of (shoot, agent, hunter) and (shoot, patient, hunter).
fillers of the agent and patient position of shoot, respectively. In order to judge whether a
hunter is a plausible agent of shoot, the vector space representation of hunter is compared
to the members of the exemplar cloud for the agent position?namely, poacher, policeman,
and director. Due to the high average similarity of the hunter vector to these vectors,
hunter will be judged a fairly good agent of shoot. Compare this with the result for the
patient role: hunter is rather distant from roe, deer, and buck, and is therefore predicted
to be a bad patient of shoot. However, note that hunter is still more plausible as a patient
of shoot than, for example, director.
3.3 Formalization and Parameter Choice
Vector space models have been formalized by Lowe (2001) as tuples VS = (DTrans,
Basis, sim, STrans), where Basis is a set of basis elements or dimensions, DTrans is a
transformation of raw co-occurrence counts, sim is a similarity measure, and STrans is
a transformation of the whole space, typically dimensionality reduction. An additional
parameter that becomes relevant for our use of vector spaces (cf. Equation [6]) is the
weighting function wt that determines the contribution of each exemplar to the overall
similarity. We discuss the parameters in turn and discuss our reasons for either explor-
ing them or fixing them.
Basis elements Basis. Traditionally, context words are used as basis elements, and co-
occurrence is defined in terms of a surface window. Such bag-of-words spaces tend to
group words by topics. They ignore the syntactic relation between context items and
the target, which is a problem for selectional preference modeling. The top table in
Figure 1(b) illustrates the problem: deer and hunter receive identical vectors, even though
they show complementary plausibility ratings. The reason is that deer and hunter often
co-occur in similar lexical bag-of-words contexts (namely, hunting-related activities).
The bottom table in Figure 1(b) indicates a way out of this problem, namely the use
of word-relation pairs as basis elements (Grefenstette 1994; Pad? and Lapata 2007).
This space splits the co-occurrences with context words such as shoot based on the
grammatical relation between target and context word, and this split looks different
for different words: whereas deer occurs exclusively as the object of shoot, hunter pre-
dominantly occurs as the subject. We find the reverse pattern for escape. In consequence,
731
Computational Linguistics Volume 36, Number 4
Table 2
Similarity measures explored in this article. Notation: We assume Basis = {b1, . . . , bn}. We write I
for mutual information, and BE(a) for the set of basis elements that co-occur at least once with a.
simLin(a, a?) =
?
(r,v)?BE(a)?BE(a? ) I(a,r,v)+I(a
?,r,v)
?
(r,v)?BE(a) I(a,r,v)
?
(r,v)?BE(a? ) I(a
? ,r,v) simcosine(a, a
?) =
?n
i=1 abi
?a?bi
||a||?||a?||
simDice(a, a?) =
2?|BE(a)?BE(a? )|
|BE(a)|+|BE(a? )| simJaccard(a, a
?) = |BE(a)?BE(a
? )|
|BE(a)?BE(a? )|
simnGCM(a, a?) = exp
(
?
?
?n
i=1 (
abi
||a|| ?
a?bi
||a?||
)2
)
where ||a|| =
?
?n
i=1 a
2
bi
simHindle(a, a?) =
?n
i=1 simHindle(a, a
?, i) where
simHindle(a, a?, i) =
{
min(I(a,bi ),I(a
?,bi )) if I(a, bi) > 0 and I(a?, bi) > 0
abs(max(I(a,bi ),I(a
? ,bi ))) if I(a, bi) < 0 and I(a?, bi) < 0
0 else
the resulting spaces gain the ability to distinguish between words like hunter and deer,
based on differences in typical occurrences in argument positions.
On the downside, dependency-based spaces are more expensive to compute than
word-based spaces because they require a corpus with syntactic analysis. Thus, we
explore both options. The word-based space records co-occurrences within a surface
window of 10 (lemmatized) words.3 We refer to it as WORDSPACE. The dependency-
based space, called DEPSPACE, has basis elements consisting of a grammatical function
concatenated with a word, as in the bottom example in Figure 1(b) (Pad? and Lapata
2007). Following earlier experiments on the representation of selectional preferences
in word-dependency-relation spaces (Pad?, Pad?, and Erk 2007), we use a subject?
object context specification that only considers co-occurrences between verbs and their
subjects and direct objects.4 In each case, we adopt the 2,000 most frequent context items
as basis elements.
Similarity measure sim. In principle, any similarity measure for vectors can be plugged
into our model. Previous studies that compared similarity measures came to various
conclusions about the usefulness of different measures. Cosine similarity is very popu-
lar in Information Retrieval. Lee (1999) obtains good results for the Jaccard coefficient
in pseudo-disambiguation. In the synonymy prediction task of Curran (2004), Dice
emerged in first place. Pad? and Lapata (2007) found good results with Lin?s measure
for predominant word sense identification.
Because it is unclear whether the findings about best similarity measures general-
ize to new tasks, we will investigate a range of similarity measures shown in Table 2:
Cosine, the Dice and Jaccard coefficients, Hindle?s (1990) and Lin?s (1998) mutual
information-based metrics, and an adaptation of Nosofsky?s (1986) Generalized Context
Model (GCM), a model for exemplar-based similarity from psychology. The original
GCM includes normalization by summed similarity over all classes of exemplars, which
introduces competition between categories. Our version, which we call nGCM, instead
normalizes by vector length to alleviate the influence of overall target frequency, but
3 We do not remove stop words for reasons of simplicity, as there is no unequivocal definition of this set,
and we do not wish to remove potentially informative contexts.
4 This context specification is available as soonly in the DependencyVectors software package
(http://www.nlpado.de/?sebastian/dv.html) starting from Release 2.5.
732
Erk, Pad?, and Pad? A Flexible, Corpus-Driven Model of Selectional Preferences
preserves the central idea that similarity decreases exponentially with distance (Shepard
1987).
All similarity measures from Table 2 are applicable to semantic spaces with arbitrary
basis elements, with the exception of the Lin measure, whose definition applies only
to dependency-based spaces. The reason is that it decomposes the basis elements into
relation?word pairs (r, v). For semantic spaces with words as basis elements, the Lin
measure can be adapted by omitting the random variable r (cf. Pad? and Lapata 2007).
Transformations DTrans and STrans. Next, we come to transformations on counts and vec-
tor spaces. Concerning the count transformations DTrans, all counts are log-likelihood
transformed (Dunning 1993), a standard procedure for word-based semantic space
models which alleviates the problematic effects of the Zipfian distribution of lexical
items, as proposed by Lowe (2001). As for transformations on the complete space STrans,
many studies do not perform dimensionality reduction at all. Others, like the LSA fam-
ily of vector spaces (Landauer and Dumais 1997), regard it as a crucial ingredient. To
gauge the impact of STrans, we compare unreduced spaces (2,000 dimensions) to 500-
dimensional spaces created using Principal Component Analysis (PCA), a standard
method for dimensionality reduction that identifies the directions of highest variance
in a high-dimensional space.
Weight functions wt. Exemplar-based models are usually applied in conjunction with a
function that can assign each exemplar an individual weight, which can be interpreted
cognitively as degree of activation (Nosofsky 1986). We assess a small number of
weight functions to investigate their importance within the EPP model. The first one,
UNI, assumes a uniform distribution, wtr,v(a) = 1. The second one, FREQ, uses the co-
occurrence frequency as weight, wtr,v(a) = freq(a, r, v), with the intuition that more fre-
quent exemplars should be both more activated and more reliable. Finally, we consider
a weight function that is an analogue of inverse document frequency in Information
Retrieval. It weights words higher that occur with a smaller number of verb?role pairs:
wtr,v(a) = log
|
?
a? Seenrv (a
? )|
|Seenrv (a)| , where we write Seenrv(a) for the set of verb?role pairs (r, v)
for which a occurs as a headword.5 We abbreviate this weight function by DISCR for
?discrimination?.
3.4 Discussion
Our EPP model can be seen as a straightforward implementation of the intuition to
model selectional preference by generalizing from seen headwords to other, similar,
words. We use vector space representations to judge the similarity of words, obtaining
a completely corpus-driven model that does not require any additional resources and is
very flexible. A complementary view on this model is as a generalization of traditional
vector space models that represent semantic similarities between pairs of words. The
EPP model goes beyond this by computing similarity between a vector and a set of other
vectors. By instantiating the set with the vectors for seen headwords of some relation r,
the similarity turns into a plausibility prediction that is specific to this relation.
Like other distributional models, the EPP model is applicable whenever corpus
data are available; no lexical resource is required. Additionally, it does not require the
headword observation step and the generalization step (cf. Section 1) to use the same
5 By keeping the constant |
?
a? Seenrv(a
? )|, we guarantee that the fraction remains larger than one, and
wtr,v(a) remains positive. This is to ensure that the weighted average in Equation (6) yields correct results.
733
Computational Linguistics Volume 36, Number 4
corpus.6 This allows us to work with a relatively small and deeply linguistically ana-
lyzed corpus of seen headwords, the FrameNet corpus, while using a much bigger data
set to generalize over seen headwords. It also allows us to make predictions for the
potentially deeper relations annotated in the primary corpus, for example, semantic
roles. We will investigate the potential of this setup in our Experiments 1 and 2.
As a distributional model, EPP avoids the two pitfalls of resource-based models.
One is a coverage problem due to the limited size of the resource (see the task-based
evaluation in Gildea and Jurafsky [2002]). For example, the semantic role?based PADO
ET AL. model resorts to class-based smoothing methods to improve coverage, which
EPP does not need. The other problem of resource-based models is that the shape of the
WordNet hierarchy determines the generalizations that the models make. These are not
always intuitive. For example, Resnik (1996) observes that (answer, obj, tragedy) receives
a high preference because tragedy in WordNet is a type of written communication, which
is a preferred argument class of answer.
The ROOTH ET AL. model (Rooth et al 1999) shares the resource independence of
EPP, but has complementary benefits and problems. Querying the probabilistic ROOTH
ET AL. model takes only constant time, whereas querying the exemplar-based EPP
model takes time linear in the number of seen arguments for the argument position.
However, the ROOTH ET AL. model requires a dedicated training phase with a space
complexity linear in the total number of verbs and nouns, which can lead to practical
problems for large corpora (cf. Section 5.1). The separation of similarity computation
and headword observation in EPP also gives the experimenter more fine-grained control
over the types and sources of information in the model.
The EPP model looks superficially similar to the model of Dagan, Lee, and Pereira
(1999). However, they differ in the role of the similarity measure: The Dagan, Lee, and
Pereira model computes a co-occurrence probability, and it uses similarity as a weight-
ing scheme. The EPP model computes similarity (of a word to the typical fillers of an
argument position), and its weighting schemes are separate from the similarity measure.
The two models also differ in the kinds of items they consider as a basis for generaliza-
tion (or smoothing): In computing the probability of seeing a word w2 after w1, the sum
in the Dagan, Lee, and Pereira model runs over all words that are similar to w1, whereas
the sum in the EPP model runs over all words that have been seen as headwords in the
argument position in question. Given that occurrence in an argument position is a form
of co-occurrence, and similarity (in both models) is computed on the basis of vectors
derived from co-occurrence counts, one could say that the sum in the EPP model runs
over words determined by first-order co-occurrence, whereas the sum in Dagan, Lee,
and Pereira runs over words chosen through second-order co-occurrence (where w1 and
w2 are second-order co-occurring if they both tend to occur with the same words w3).
4. Design of the Experimental Evaluation
In this section, we give a high-level overview over the experiments and experimental
settings we will use subsequently. Details will be provided in the following sections.
We evaluate the EPP model in three ways: We test the prediction of verbal
selectional preference models with a pseudo-disambiguation task (Experiment 1).
Then, we address the task of predicting human verb?argument plausibility ratings
(Experiment 2). Finally, we investigate inverse selectional preferences?preferences of
6 Dagan, Lee, and Pereira (1999) could in principle do the same, but do not explore this option.
734
Erk, Pad?, and Pad? A Flexible, Corpus-Driven Model of Selectional Preferences
nouns for the predicates that they co-occur with?again using pseudo-disambiguation
(Experiment 3).
We compare the EPP model to models from the three model categories presented in
Section 2: RESNIK as a hierarchical model; ROOTH ET AL. as a distributional model; and
PADO ET AL. as a semantic role?based model. As both Brockmann and Lapata (2003)
and Pad? (2007) have argued, no WordNet-based model systematically outperforms
the others, and the RESNIK model shows the most consistent behavior across different
scenarios. Among the distributional models, we choose ROOTH ET AL. as a model that
performs soft clustering and thus shows a marked difference to the EPP model. To our
knowledge, this is the first comparison of all three generalization paradigms: semantic
hierarchy?based, distributional, and semantic role?based.7
As mentioned earlier, we employ two tasks to evaluate the four models: pseudo-
disambiguation and the prediction of human plausibility ratings. The pseudo-
disambiguation task (Yarowsky 1993) has become a standard evaluation measure for
selectional preference models (Dagan, Lee, and Pereira 1999; Rooth et al 1999). Given
a choice of two potential headwords, the task of a selectional preference model is to
pick the more plausible one to fill a particular argument position of a given predicate.
Pseudo-disambiguation can be viewed as a word sense disambiguation task in which
the two potential headwords together form a ?pseudo-word,? for example herb/struggle
from the original words herb and struggle. The task is to ?disambiguate? the pseudo-
word to the word that fits better in the given context. It can also be viewed as an in vitro
version of semantic role labeling and dependency parsing (depending on whether the
relations are semantic roles or grammatical functions) (Zapirain, Agirre, and M?rquez
2009). In this case, the scenario is that of a sentence containing a predicate and two
words that could potentially fill an argument position of that predicate, for example, the
predicate recommend with the potential headwords herb and struggle for the grammatical
relation of direct object. The task is to decide which of the two potential headwords is
better suited to fill the argument position.
Human plausibility ratings, on the other hand, make considerably more fine-
grained distinctions than those occurring in pseudo-disambiguation tasks. Here, mod-
els predict the exact human ratings for verb?argument?role triples. Ratings are collected
to further control carefully selected experimental items for psycholinguistic studies
(Trueswell, Tanenhaus, and Garnsey 1994; McRae, Spivey-Knowlton, and Tanenhaus
1998), or are solicited for corpus-derived triples specifically to create evaluation data for
plausibility models (Brockmann and Lapata 2003; Pad? 2007).
We contrast two different levels of semantic analysis for the predicates and argu-
ment positions. In the SEM PRIMARY setting, the predicates are FrameNet frames, each
of them potentially instantiated by multiple different verbs. The argument positions in
these settings are frame-semantic roles. This setting most closely matches the notion
of selectional preferences as characterizations of semantic arguments of an event. In
addition, we study the SYN PRIMARY setting, where predicates are verbs, and argument
positions are grammatical functions (subject and direct object). Viewing grammatical
functions as shallow approximations of semantic roles, we can expect the selectional
preference models for this setting to yield noisier estimates than in the SEM PRIMARY
setting. The two settings will differ only in the choice of primary corpus, but will use
the same generalization corpus.
7 Erk (2007) has a comparison between hierarchy-based and distributional models, but does not include a
semantic role?based model.
735
Computational Linguistics Volume 36, Number 4
Table 3 illustrates the difference between the SEM PRIMARY setting and the SYN
PRIMARY setting on an example from a pseudo-disambiguation task: The SEM PRIMARY
setting has predicates like the FrameNet frame (predicate sense) ADORNING, with the
semantic role THEME as argument position. In contrast, the SYN PRIMARY setting has
predicates that are verb lemmas, such as cause, and argument positions that are gram-
matical functions (subj). In both settings, the two potential headwords (here called
headword and confounder, to be explained in more detail in the next section) to be
distinguished in the pseudo-disambiguation task are noun lemmas.
The verb?dependency?headword tuples of the SYN PRIMARY setting yield much
more coarse-grained and noisy characterizations of selectional preferences; however,
they can be extracted from corpora with only syntactic annotation. We are therefore
able to use the 100-million word BNC (Burnard 1995) as the primary corpus for this
setting by parsing it with the Minipar dependency parser (Lin 1993). Minipar could
parse almost all of the corpus, resulting in 6,005,130 parsed sentences.
For the SEM PRIMARY setting, we require a primary corpus with role-semantic
annotation. We use the much smaller FrameNet corpus (Fillmore, Johnson, and Petruck
2003). FrameNet is a semantic lexicon for English that groups words in semantic classes
called frames and lists fine-grained semantic argument roles for each frame. Ambiguity
is expressed by membership of a word in multiple frames. Each frame is exemplified
with annotated example sentences extracted from the BNC. The FrameNet release 1.2
comprises 131,582 annotated sentences (roughly three million words). To determine
headwords of the semantic roles, the corpus was parsed using the Collins (1997) parser.
As generalization corpus, we use the Minipar-parsed BNC in both settings. The ex-
perimentation with two different primary corpora allows us to directly study the influ-
ence of the disambiguation of predicates and the semantic characterization of argument
positions on the performance of selectional preference models. Note, however, that the
comparison is complicated by differences between the two corpora: The primary corpus
for the SYN PRIMARY setting is parsed automatically, which can introduce noise in the
determination of predicates, grammatical functions, and headwords. The primary cor-
pus for the SEM PRIMARY setting is manually annotated for semantics but is parsed
automatically to determine headwords. This can introduce noise in the headwords, but
not in the determination of predicates and semantic roles. Also, the primary corpus for
the SYN PRIMARY setting is much larger than the one used in the SEM PRIMARY setting.
5. Experiment 1: Pseudo-Disambiguation
The first experiment uses a pseudo-disambiguation task to evaluate the models? perfor-
mance on modeling the plausibility of nouns as headwords of argument positions of
verbal predicates.
Table 3
Pseudo-disambiguation items for the SYN PRIMARY setting and the SEM PRIMARY setting.
Setting Predicate (v) Arg. pos. (r) Headword (a) Confounder (a?)
SYN cause subj succession island
appear subj feasibility desire
SEM ADORNING THEME illustration axe
ROPE_MANIPULATION ROPE cord literature
736
Erk, Pad?, and Pad? A Flexible, Corpus-Driven Model of Selectional Preferences
Require: Some corpus T: a list of triples (v, r, a) of seen predicates, roles, and arguments.
Require: Some corpus N: a list of noun lemmas, along with a function freqN : N ?  
that associates each noun n ? N with its corpus frequency.
1: Nmid = {n ? N | freqN(n) ? 30 and freqN(n) ? 3, 000}
2: We define a probability distribution pN over the n ? Nmid by pN(n) = freqN (n)?
m freqN (m)
3: conf = { } # set of headword/confounder mappings, starts empty
4: AT = {a | (v, r, a) ? T} # set of seen headwords
5: for every a in AT do
6: choose a confounder a? ? Nmid according to pN
7: conf = conf ? { a ? a? }
8: end for
9: Return: conf
Figure 2
Algorithm for choosing confounders.
5.1 Setup
Task and data. In a data set of tuples (v, r, a) of a predicate v, argument position r, and
headword a, each tuple is paired with a confounder a?. The task is to pick the original
headword by comparing the tuples (v, r, a) and (v, r, a?). Table 3 shows some examples.
We begin by collecting all triples (v, r, a) observed in the respective primary corpus.
In the SYN PRIMARY setting, this corresponds to all headwords observed in subject or
direct object position of a verbal predicate in the BNC, and in the SEM PRIMARY setting,
to all nouns observed as headword of some semantic role in a frame introduced by a
verb. From this set of triples (v, r, a) for a given primary corpus, we draw an evaluation
sample that is balanced by the corpus frequency of predicates and argument position.
As test set, we choose 100 (v, r) pairs at random, drawing 20 pairs each from five fre-
quency bands: 50?100 occurrences; 100?200 occurrences; 200?500; 500?1,000; and more
than 1,000 occurrences. For any chosen predicate?relation pair, we sample triples (v, r, a)
equally from six frequency bands of arguments a: 1?50 occurrences; 50?100; 100?200;
200?500; 500-1,000; and more than 1,000 occurrences. These evaluation samples contain
a total of 213,929 (SYN) and 65,902 (SEM) tuples.
Next, we pair each headword with a confounder sampled from the primary corpus
as described in Figure 2.8 In the literature, there have been two different approaches to
choosing confounders for pseudo-disambiguation tasks: The first approach, used by
Dagan, Lee, and Pereira (1999), chooses confounders to match the headword a in
frequency. The second approach, used in Rooth et al (1999), sets the probability that
a word is drawn as a confounder to its relative frequency. The advantage and dis-
advantage of the first approach is that it largely eliminates the frequency bias that is
a general problem of vector space-based approaches. This is an advantage in that it
allows the generalization achieved by the model to be evaluated without any distortion
from frequency bias. It is a disadvantage in that in any practical application making
use of selectional preferences, the data will not be frequency-balanced. For example,
selectional preferences could be used by a dependency parser to decide which word in
the sentence to link to a given verb via a subject edge, or selectional preferences could
8 The confounder is the same for all instances of the headword a in the evaluation sample, regardless of the
values for r and v. As confounder candidates, we only use words with between 30 and 3,000 occurrences
in the BNC, following Rooth et al (1999).
737
Computational Linguistics Volume 36, Number 4
be used by a semantic role labeler to decide which constituent is the overall best filler
for the AGENT role for a given predicate. In such cases, it does not appear warranted to
assume that the frequencies of different headword candidates are balanced. We choose
the second option for our experiments, using relative corpus frequency to approximate
the probability of encountering different headword candidates.
Training of models. As stated earlier, we evaluate all models in the SYN PRIMARY setting
and the SEM PRIMARY setting. In all experiments herein, we perform two 2-fold cross-
validations runs. In each run, we randomly split the respective (SYN or SEM) evaluation
sample into a training and a test set at the token level. Figure 3 describes the experimen-
tal procedure in pseudo-code.
The EPP, RESNIK, and PADO ET AL. models are trained on the training split of the
evaluation sample. The EPP model additionally uses the BNC as generalization corpus
in both the SYN PRIMARY setting and the SEM PRIMARY setting. This generalization
corpus is used to compute either a WORDSPACE or a DEPSPACE vector space, as
discussed in Section 3.3. For the ROOTH ET AL. model, we had to employ a frequency
Require: A set Formalisms of formalisms to test
Require: A primary corpus T: a list of triples (v, r, a) of seen predicates, argument
positions, and arguments, along with a function freqT : T ?   that associates each
triple (v, r, a) ? T with its corpus frequency
Require: A mapping conf : Lemmas ? Lemmas of headwords to confounders such that
{a | (v, r, a) ? T} ? Domain(conf )
1: eval_results = { }
2: for splitno in 1:2 do
3: # prepare two independent splits
4: half1 = { }, half2 = { } # mappings from headwords to counts
5: for each tuple t in T do
6: # decide how many occurrences of t to put in half1, half2 by drawing from the binomial
distribution
7: Sample k ? B( freqT(t), 0.5)
8: half1 = half1 ? { t ? k }, half2 = half2 ? { t ? freqT(t) ? k }
9: end for
10: splits = { (half1, half2), (half2, half1) }
11: for ( ftrain, ftest) in splits do
12: for each formalism F in Formalisms do
13: train a model mF according to formalism F using the training set defined by
the frequency function ftrain.
14: for each tuple (v, r, a) in T do
15: for i in 1:ftest(v, r, a) do
16: Evaluate the performance of mF on the tuple (v, r, a, conf (a)) and add the
result to eval_results
17: end for
18: end for
19: end for
20: end for
21: end for
22: Return: eval_results
Figure 3
Algorithm for running a pseudo-disambiguation experiment
738
Erk, Pad?, and Pad? A Flexible, Corpus-Driven Model of Selectional Preferences
cutoff of five in the SYN PRIMARY setting to reduce the amount of training data due to
memory limitations. The PADO ET AL. model is only used in the SEM PRIMARY setting:
FrameNet is an integral part of this model, and it cannot be used in a syntax-only
setting without major changes. For details on training, see Section 2.4. Note that no
verb classes had to be induced from the data, because the predicates v are already
instantiated by verb classes, namely, FrameNet frames (see Table 3).
Finally, we report three baselines. The first baseline, headword frequency (HW), is
very simple. It decides between the headword a and the confounder a? by comparing
the frequencies f (a) and f (a?). The second, more informed, baseline is triple frequency
(TRIPLE). It votes for a if f (v, r, a) > f (v, r, a?), and vice versa. The third baseline, a bigram
language model (LM), was constructed by training a 2-gram language model from the
large English ukWAC Web corpus (Baroni et al 2009) using the SRILM toolkit (Stolcke
2002) with default Good?Turing smoothing. We retained only verbs, nouns, adjectives,
and adverbs in order to maximize the proximity between verbs and their subjects and
objects. We defined the preference score for verb?subject triples as the probability of the
sequence av, that is, Pref (v, subj, a) = P(v|a). Conversely, the preference score for verb?
object triples was defined as the probability of the sequence va, that is, Pref (v, obj, a) =
P(a|v). Again, the model compares Pref (v, r, a) and Pref (v, r, a?) to make its decision.
Evaluation. For all models, we report two evaluation figures. One is coverage: A tuple
is covered if the model assigns some preference to both a and a?, and the preferences are
not equal. The second is error rate, which is the relative frequency, among all covered
tuples, of instances where the confounder was at least equally preferred. Both coverage
and error rate are averages over the 2 x 2 cross-validation runs in each setting.
We determine the statistical significance of differences between error rates using
bootstrap resampling (Efron and Tibshirani 1994). This procedure samples correspond-
ing model predictions with replacement from the set of predictions made by the models
to be compared and computes the difference in error rates. On the basis of n such
samples (n = 1,000), the empirical 95% confidence interval for the difference in strength
on the basis of all observed differences is computed. If the interval includes 0, the
difference is not statistically significant.
5.2 SYN PRIMARY Setting: Results
Table 4 shows the results for the SYN PRIMARY setting. The overall best error rate is
achieved by a variant of the EPP model, with the RESNIK model coming in second
(the performance difference is significant at the 0.05 level). The EPP variants also show
near-perfect coverage, whereas the RESNIK model delivers results only for 63% of the
data points. We found a very high error rate and a comparatively low coverage for
ROOTH ET AL., which most likely stems from the data pruning necessary to reduce the
training data (compare the subsequent results in the SEM PRIMARY setting). The PADO
ET AL. model was not tested in the SEM PRIMARY setting, because it requires semantic
role annotation. The HW baseline is somewhat below chance (50%), which is an effect of
our by-token sampling procedure, according to which confounders often have higher
corpus frequencies than the real arguments. The TRIPLE baseline has a better error rate
than the LM baseline, but has very low coverage. Both the RESNIK and the EPP models
outperform the baselines in terms of error rate. That they outperform the TRIPLE
baseline in terms of error rate indicates that we sometimes have confounders that have
actually been seen more often with the verb?argument pair than the headword, but that
739
Computational Linguistics Volume 36, Number 4
Table 4
SYN PRIMARY setting: Pseudo-disambiguation results for different weighting schemes.
Model Similarity Error rate (%) Coverage (%)
UNI FREQ DISCR
EPP:DEPSPACE
Cosine 32.8 30.3 31.2 98.5
Dice 49.4 48.2 47.5 97.1
nGCM 27.6 27.5 25.7 98.5
Hindle 53.7 52.3 52.8 96.6
Jaccard 49.5 48.2 47.6 97.1
Lin 35.5 34.3 33.2 98.8
EPP:DEPSPACE, PCA
Cosine 30.2 28.7 28.8 98.1
Dice 29.9 30.8 28.6 98.2
nGCM 26.4 26.4 25.6 98.1
Hindle 45.0 44.4 44.2 95.7
Jaccard 29.7 30.7 28.5 98.2
Lin 28.7 29.1 26.7 97.7
EPP:WORDSPACE
Cosine 35.3 35.8 34.0 97.4
Dice 51.0 50.7 50.3 96.0
nGCM 33.2 34.7 31.8 97.4
Hindle 52.7 52.8 52.4 96.0
Jaccard 51.8 52.0 51.3 96.0
Lin 32.0 31.8 31.4 98.2
EPP:WORDSPACE, PCA
Cosine 30.3 31.3 29.4 97.1
Dice 31.3 32.4 30.5 97.8
nGCM 30.0 30.9 29.0 97.1
Hindle 40.2 41.0 40.4 95.3
Jaccard 31.0 32.1 30.2 97.8
Lin 27.8 29.8 26.9 97.3
RESNIK 28.1 63.4
ROOTH ET AL. 58.1 61.5
PADO ET AL. ? ?
HW 60.0 100.0
TRIPLE 32.0 4.0
LM 37.0 86.0
are dissimilar from other seen headwords, which allows RESNIK and EPP to identify
them as confounders in spite of their higher co-occurrence frequency.
We now turn to a comparison of the EPP variants. The coverage of all EPP models is
very high (0.95 or higher), independent of space, similarity measure, and dimensionality
reduction. We generally observe that error rates are lower when word meaning is
represented in DEPSPACE, and when discrimination weighting is used. In DEPSPACE,
nGCM works best, yielding the overall best result with an error rate of 25.6?25.7%.
In WORDSPACE, the Lin measure shows the best error rates with an error rate of just
below 27%. These results hold both for the unreduced and the reduced spaces and are
highly significant (p ? 0.01). Hindle is clearly the worst measure at around random
performance.
740
Erk, Pad?, and Pad? A Flexible, Corpus-Driven Model of Selectional Preferences
The difference between UNI and DISCR is significant throughout; the difference
between FREQ and DISCR is less uniform. In DEPSPACE, the difference between the best
measure with and without PCA (nGCM in both cases) is not significant; in WORDSPACE,
the difference between the best measure with and without PCA (Lin in both cases) is
significant (p ? 0.01).
For both WORDSPACEs and DEPSPACEs without PCA, the similarity measures divide
into two distinct groups: Lin, nGCM, and Cosine on the one hand and Jaccard, Dice, and
Hindle on the other, with a significant difference in performance between the groups
(p ? 0.01). The use of dimensionality reduction through PCA improves performance
for all similarity measures, in WORDSPACE as well as DEPSPACE. The improvement is
especially marked for the Dice and Jaccard measures, which perform at the level of
a random baseline for unreduced spaces. We assume that these set intersection-based
measures benefit from the independent dimensions that PCA produces. For the simi-
larity measures with best performance, the improvement through PCA is less marked.
Thus, PCA-reduced spaces show more similar error rates across similarity measures.
After PCA, only nGCM and Lin still significantly (p ? 0.01) outperform the others
in DEPSPACE, and in WORDSPACE, Lin is the only measure that performs significantly
differently from the rest (p ? 0.01).
As arguments are sampled from six frequency bins, we can inspect the effect of
argument frequency on error rate. Figure 4 examines the performance of the EPP model
with different similarity measures and weighting schemes by argument frequency bins
(cf. the subsection Task and Data in Section 5). We find that the overall best weighting
scheme, DISCR, also works best for all except the highest argument frequency bin. In
the DEPSPACE setting (upper row), all similarity measures show a frequency bias in that
Figure 4
SYN PRIMARY setting: Error rate by argument frequency bin. Bins: 1 = 1?50; 2 = 50?100;
3 = 100?200; 4 = 200?500; 5 = 500?1,000; 6 > 1,000.
741
Computational Linguistics Volume 36, Number 4
Figure 5
SYN PRIMARY setting: Error rate by predicate frequency bin: DISCR weighting. Bins: 1 = 50?100;
2 = 100?200; 3 = 200?500; 4 = 500?1,000; 5 > 1,000.
error rate is lower for more frequent arguments, but this bias is much less pronounced
in Cosine and nGCM than in the other measures, with error rates varying between 45%
and 25% rather than 80% and 20%. (Dice and Hindle, not shown here, exhibit similar
behavior to Jaccard.) In PCA-transformed DEPSPACE (middle row), this frequency bias
largely disappears for all similarity measures. In WORDSPACE (bottom row), although
there is again a frequency bias in all similarity measures, Lin now joins Cosine and
nGCM in being much less biased than Jaccard, Dice, and Hindle. For WORDSPACE
with PCA-transformation, not shown here, the curves resemble those of DEPSPACE with
PCA-transformation.
Figure 5 examines the effect of (predicate, argument position) pair frequency
on error rate. Predicate?argument position pairs were sampled from five frequency
bins. The figure shows DISCR weighting only. In the spaces without dimensionality
reduction, there is a clear division between Cosine, nGCM, and Lin on the one hand,
and Jaccard, Dice, and Hindle on the other. In PCA spaces, all measures except for
Hindle are similar in their performance. In both DEPSPACE conditions, error rate
decreases towards the higher frequency predicate bins, although this is not so in
742
Erk, Pad?, and Pad? A Flexible, Corpus-Driven Model of Selectional Preferences
WORDSPACE. It seems that in the sparser DEPSPACE, models can still profit from the
additional seen headwords in the highest predicate frequency bins, whereas in the less
sparse but noisier WORDSPACE, the added noise is stronger than the added signal in
the highest predicate frequency bins. For the lowest predicate frequency bins, the best
results in WORDSPACE are better than those in DEPSPACE.
5.3 SEM PRIMARY Setting: Results
Table 5 shows the results for the SEM PRIMARY setting, where we predict head words for
pairs of a frame (predicate sense) and semantic role. In comparison to the SYN PRIMARY
setting (Table 4), error rates are lower across the board. The difference for the EPP models
is on average around 10%.
Table 5
SEM PRIMARY setting: Pseudo-disambiguation results for different weighting schemes.
Model Similarity Error rate (%) Coverage (%)
UNI FREQ DISCR
EPP:DEPSPACE
Cosine 19.8 16.9 19.0 97.1
Dice 42.3 32.4 39.5 96.3
nGCM 20.2 16.3 20.8 97.1
Hindle 48.3 46.8 47.8 93.4
Jaccard 41.5 31.5 38.5 96.3
Lin 31.1 20.2 29.0 97.7
EPP:DEPSPACE, PCA
Cosine 18.5 17.0 17.8 96.9
Dice 19.3 19.2 18.0 97.6
nGCM 16.9 15.7 16.4 96.9
Hindle 44.9 45.6 44.7 89.3
Jaccard 18.2 18.5 17.5 97.6
Lin 18.8 19.5 18.3 98.9
EPP:WORDSPACE
Cosine 24.1 20.4 23.4 93.1
Dice 23.7 24.5 22.5 89.6
nGCM 21.1 17.8 19.4 93.1
Hindle 31.8 33.1 31.8 83.1
Jaccard 24.8 26.5 24.2 89.6
Lin 22.3 18.4 21.9 92.8
EPP:WORDSPACE, PCA
Cosine 21.0 17.6 20.5 93.1
Dice 18.5 16.4 17.8 96.8
nGCM 19.7 16.4 19.3 93.1
Hindle 41.0 39.8 40.7 90.6
Jaccard 18.1 16.2 17.6 96.8
Lin 21.3 17.1 20.7 98.3
RESNIK 16.5 62.8
ROOTH ET AL. 24.9 100.0
PADO ET AL. 7.1 59.0
HW 65.0 100.0
TRIPLE 44.0 2.0
LM NA NA
743
Computational Linguistics Volume 36, Number 4
The error rate of the PADO ET AL. model, at 7%, is the best by a large margin. We
attribute this to the extensive generalization mechanisms that the model uses, which
draw on an array of lexical?semantic resources. However, with a coverage of 59%,
the model is still unable to make predictions for many of the test items. Error rates
for the RESNIK and the EPP models are comparable, at 16.5% for RESNIK and 15.7% for
the best EPP variant. The two models differ sharply in coverage, however: 62.8% for
RESNIK, consistent with the findings of Gildea and Jurafsky (2002), and between 90%
and 98% for EPP variants. The RESNIK model also profits from the presence of semantic
disambiguation in the SEM PRIMARY setting (in the SYN PRIMARY setting its error
rate was 28%), which underlines the substantial impact that properties of the training
data have on semantic hierarchy?based models of selectional preferences. ROOTH
ET AL. now has perfect coverage, affirming our assumption that the very bad results
of the ROOTH ET AL. model in the SYN PRIMARY setting were an artifact of the data
sampling necessary for that data set. Although its error rate of 24.9% is a substantial
improvement over all baselines, the EPP model achieves error rates that are up to
9 points lower at a comparable coverage. Among the baselines, HW shows that here, as
in the SYN PRIMARY setting, arguments have some tendency of having lower frequency
than the confounders. The TRIPLE baseline shows near-random performance, at very
low coverage, a result of the very small size of the corpus. Because there is no large
corpus with frame-semantic roles, nor is the annotation easily linearizable, we could
not compute a LM baseline in the SEM PRIMARY setting.
Among EPP models, the DEPSPACEs and WORDSPACEs perform comparably, with a
non-significant advantage for DEPSPACE among the best models. Overall error rates
show the same clear divide between the three high-performing similarity measures
(Cosine, nGCM, and Lin) and the three weaker ones (Dice, Jaccard, and Hindle). Di-
mensionality reduction again dramatically improves the weaker models, with Jaccard
yielding the best result for the PCA-reduced WORDSPACE.9 Whereas all best parame-
trizations in the SYN PRIMARY setting used DISCR weighting, it is now FREQ weighting
that yields the best results.
Figure 6 again analyzes the influence of argument frequency on performance by
showing the performance of different variants of the EPP model over six argument
frequency bins. The upper row shows DEPSPACE without dimensionality reduction.
Note that FREQ weighting now works especially well for the lowest argument frequency
bin, much better than DISCR and PLAIN. This is the opposite of what we saw for the
SYN PRIMARY setting in Figure 4. With DISCR and PLAIN weighting, Jaccard and Lin
again have noticeable problems with the lowest argument frequency bins?as in the SYN
PRIMARY setting?but not with FREQ weighting. With DEPSPACE and dimensionality
reduction (middle row), we get error rates of ? 26% for all settings and all frequency
bins. On the lowest frequency bin, we again see a large advantage of FREQ weighting
over the two other weighting schemes. The bottom row shows WORDSPACE without
dimensionality reduction. Note that there is much less variation in error rates across
frequency bins here than in unreduced DEPSPACE.
Figure 7 charts error rate by predicate frequency bin, showing FREQ weighting
only, as this showed the best results on this data set. The figure clearly illustrates the
divide between the top and the bottom three similarity measures in DEPSPACE, as well
as the disappearance of this divide for both PCA settings. In unreduced WORDSPACE,
9 The differences to other similarity metrics in the FREQ setting are insignificant, with the exception
of Hindle.
744
Erk, Pad?, and Pad? A Flexible, Corpus-Driven Model of Selectional Preferences
the divide is not as clearly visible. The figure also indicates a slight tendency for error
rates to rise for the lowest-frequency as well as the highest-frequency predicates, across
all spaces.
5.4 Discussion
The resource-based approaches that we tested, RESNIK and PADO ET AL., show superior
performance when they have coverage (which coincides with findings in other lexical
semantics tasks that supervised data, when available, always increases performance),
but showed low coverage, at most 63% (RESNIK, SYN PRIMARY setting). The EPP model
achieves near-perfect coverage at good error rates: In the SYN PRIMARY setting, the
RESNIK model achieved an error rate of 28%, and the best EPP variant was at 26%. In
the SEM PRIMARY setting, error rates were 7% for the PADO ET AL. model, 16.5% for
the RESNIK model, and 16% for the best EPP variant. Comparing the EPP and ROOTH
ET AL. models in the SEM PRIMARY setting, we find that the use of an additional gen-
eralization corpus in the EPP model seems to offset any advantages introduced by the
joint clustering of predicates and arguments.
The difference in model performance on the two primary corpora (SYN and SEM)
is striking. Even though the FrameNet corpus is smaller and a sparse data prob-
lem might be expected, models perform at considerably lower error rates in the SEM
PRIMARY setting than when the primary corpus is the larger BNC. This underscores
the point that selectional preferences belong to a predicate sense rather than a predicate
lemma, and that they describe the semantics of fillers of semantic roles rather than of
Figure 6
SEM PRIMARY setting: Error rate by argument frequency bin. Bins: 1 = 1?50; 2 = 50?100;
3 = 100?200; 4 = 200?500; 5 = 500?1,000; 6 > 1,000.
745
Computational Linguistics Volume 36, Number 4
Figure 7
SEM PRIMARY setting: Error rate by predicate frequency bin: FREQ weighting. Bins: 1 = 50?100;
2 = 100?200; 3 = 200?500; 4 = 500?1,000; 5 > 1,000.
syntactic dependents (recall that in this setting, we predict head words for pairs of a
predicate sense and semantic role). In the SEM PRIMARY setting, the data is cleaner, so
it is expected that seen headwords of an argument position will be more semantically
uniform. This has a strong influence on model performance. Another factor contributing
to the difference in performance between the two data sets may be that the primary
corpus in the SYN PRIMARY setting is parsed automatically, whereas manual annotation
is available in the FrameNet corpus. However, although this manual annotation iden-
tifies predicate senses, role headwords are still determined through automatic parsing.
The division of the training data into a primary and a secondary corpus allows us to
successfully use FrameNet as the basis for semantic space?based similarity estimates
despite the fact that this corpus alone would be too small to sustain the construction of a
robust space.
In terms of model parameters for EPP, the following patterns stand out. Cosine,
Lin, and nGCM show good performance across all spaces and parameter settings; Dice
and Jaccard work comparably only on spaces that use dimensionality reduction. The
Hindle measure is an underperformer in all conditions. With Lin, Jaccard, Dice, and
Hindle, error rates rise sharply for less frequent arguments in many spaces. Although
Cosine and nGCM also have some frequency bias, it is much less pronounced. nGCM
seems to work well with sparse data sets that are not too noisy, as evidenced by the
fact that it has the best performance among all EPP variants on both DEPSPACEs in
746
Erk, Pad?, and Pad? A Flexible, Corpus-Driven Model of Selectional Preferences
Table 6
Verb?argument position?noun triples with plausibility judgments on a 7-point scale (McRae
et al, 1998).
Verb Argument position Noun Plausibility
shoot agent hunter 6.9
shoot patient hunter 2.8
shoot agent deer 1.0
shoot patient deer 6.4
the SYN PRIMARY setting, as well as in all SEM conditions except reduced WORDSPACE.
The Lin measure seems to work well with noisier data: It is the best EPP model when
using WORDSPACE in the SYN PRIMARY setting. Cosine, although never showing the
top performance, is among the best models in any setting. Although dimensionality
reduction only improves the overall error rates of the best models by a few points, it
has two important consequences: First, dimensionality reduction reduces dependence
of the results on the exact similarity measure chosen, as all measures except Hindle
show nearly indistinguishable error rates on reduced spaces (Figures 5 and 7). Second,
low-frequency arguments profit by a huge margin when PCA is used (Figures 4 and 6).
Among weighting schemes, DISCR weighting seems to be most useful when the data
is sparse but somewhat noisy (as is the case in the lower argument frequency bins in
the SYN PRIMARY setting). Frequency weighting seems to work best when the data is
either not sparse (as in the highest argument frequency bin in the SYN PRIMARY setting)
or very clean but sparse (as in the lowest argument frequency bin in the SEM PRIMARY
setting). A comparison of the two vector spaces, DEPSPACE and WORDSPACE, shows
no clear winner. When the collections of seen headwords are noisier, as they are in the
SYN PRIMARY setting, DEPSPACE, with its more aggressive filtering, yields the better
results. Sets of headwords collected by predicate sense, as in the SEM PRIMARY setting,
are sparser but cleaner, and WORDSPACE shows lower error rates.
6. Experiment 2: Human Plausibility Judgments
Experimental psycholinguistics affords a second perspective on selectional preferences:
The plausibility of verb?argument pairs has been shown to have an important effect
on human sentence processing (e.g., Trueswell, Tanenhaus, and Gransey 1994; Garnsey
et al 1997; McRae, Spivey-Knowlton, and Tanenhaus 1998). In these studies, plausibility
was operationalized as the thematic fit or selectional preference between a verb and its
argument in a specific argument position. Models of human sentence processing there-
fore need selectional preference models (Pad?, Crocker, and Keller 2009). Conversely,
psycholinguistic plausibility judgments can be used to evaluate computational models
of selectional preferences.
6.1 Experimental Materials
We present evaluations on two plausibility judgment data sets used in recent studies.
747
Computational Linguistics Volume 36, Number 4
The first data set consists of 100 data points10 from McRae, Spivey-Knowlton, and
Tanenhaus (1998). Our example in Table 6, which is taken from this data set, was
elicited by asking study participants to rate the plausibility of, for example, a hunter
shooting (AGENT) or being shot (PATIENT). The data point demonstrates the McRae set?s
balanced structure: 25 verbs are paired with two argument headwords in two argument
positions each, such that each argument is highly plausible in one argument position
but implausible in the other (hunters shoot, but are seldom shot, and vice versa for deer).
The resulting distribution of ratings is thus highly bimodal. Models can only reliably
predict the human ratings in this data set if they can capture the difference between
verb argument positions as well as between individual fillers. However, because the
verb?argument pairs were created by hand and with strict requirements, many of the
arguments are infrequent in standard corpora (e.g., wimp, bellboy, or knight). When
FrameNet is used to annotate senses for the verbs, no appropriate senses are available
for 28 of the 100 verb?argument pairs, reducing the test set to 72 data points.
The second, larger data set addresses this sparseness issue. Its triples are con-
structed on the basis of corpus co-occurrences (Pad? 2007). Eighteen verbs are combined
with their three most frequent subjects and objects found in the Penn Treebank and
FrameNet corpora, respectively, up to a total of 12 arguments. Each verb?argument pair
was rated both as an agent and as a patient (i.e., both in the observed and an unobserved
argument position), which leads to a total of 24 rated triples per verb. The data set
contains ratings for 414 triples. The resulting judgments show a more even distribution
of data. With FrameNet annotation for the verbs, appropriate senses are not attested for
six verb?argument pairs, reducing the test set to 408 data points.
6.2 Setup
We evaluate the same four models as in Experiment 1: EPP, the WordNet-based RESNIK
model, the distributional ROOTH ET AL. model, and the semantic role?based PADO
ET AL. model. We again compare a SYN PRIMARY setting, where the models make pre-
dictions for pairs of a verb and a grammatical function, with a SEM PRIMARY setting,
for which the two test data sets were annotated with verb sense and semantic roles in
the FrameNet paradigm (Pad? 2007) and where models make predictions for pairs of a
frame and a semantic role. As before, the PADO ET AL. model is only tested in the SEM
PRIMARY setting.11 For the EPP model, we focus on parsed, dimensionally unreduced
spaces and DISCR weighting, following earlier results (Pad?, Pad?, and Erk 2007). We
provide results for the best WORDSPACE models from Experiment 1 for comparison.
The primary corpora for training selectional preference models were prepared as in
Experiment 1 (cf. Section 5.1). The generalization corpus for EPP was again the BNC.
For the ROOTH ET AL. model in the SYN PRIMARY setting, we again used a frequency
cutoff. We found the RESNIK model to perform better when using just a subset of the
BNC (namely, all the triples for verbs present in the test set).
6.3 Evaluation Procedure
We evaluate our models by correlating the predicted plausibility values with the human
judgments, which range between 1 and 7. Because we do not assume a priori that there
10 The original data set has 60 data points more, which were used as the development set for the PADO
ET AL. model.
11 The PADO ET AL. model now uses automatically induced verb clusters instead of FrameNet frames.
748
Erk, Pad?, and Pad? A Flexible, Corpus-Driven Model of Selectional Preferences
Table 7
Comparison of EPP DEPSPACE models on McRae data. Unreduced spaces, DISCR weighting.
***p < 0.001.
SEM SYN
Sim Coverage Spearman?s ? Coverage Spearman?s ?
Dice 100% 0.038 ns 98% 0.148 ns
Jaccard 100% 0.045 ns 98% 0.153 ns
Cosine 100% 0.162 ns 98% 0.197 ns
Hindle 100% 0.060 ns 98% 0.108 ns
Lin 100% 0.085 ns 98% 0.094 ns
nGCM 100% 0.154 ns 98% 0.325 ***
is a linear correlation between the two variables, we do not use Pearson?s product-
moment correlation, but instead Spearman?s ?, a non-parametric rank-order correlation
coefficient.12 Note that significance is harder to reach the smaller the number of data
points is.
In line with Experiment 1, we include a simple frequency baseline FREQ, which
predicts the plausibility of each item as its frequency in the BNC (SYN) and in FrameNet
(SEM), respectively. With regard to an upper bound, we assume that automatic models
of plausibility should not be expected to surpass the typical human agreement on the
plausibility judgment. This is roughly ? ? 0.7 for the Pado data set.
6.4 McRae Data Set: Results and Discussion
Table 7 focuses on EPP variants with unreduced DEPSPACE for the McRae data set. We
see that this data set is rather difficult to model. None of the models trained in the SEM
PRIMARY setting achieves a significant correlation.13 Apparently, the FrameNet corpus
is too small to acquire selectional preferences that generalize well to the infrequent
items that make up the McRae data set. In the SYN PRIMARY setting, the nGCM model?s
predictions reach significance.
Table 8 shows results on the McRae data set for all selectional preference models
that we are considering. For EPP, we only show nGCM as the best-performing similarity
measure from the pseudo-disambiguation task, and Cosine as a widely used vanilla
measure. The results for the SEM PRIMARY setting (left-hand side) mirror the results
for the SEM PRIMARY setting in Experiment 1: The deep PADO ET AL. model shows the
best correlation (it is the only model to predict human judgments significantly). It
overcomes the sparseness in the FrameNet corpus by using semantic verb classes that
are particularly geared towards grouping the existing verb occurrences in the way
that is most meaningful for this task. It covers about 80% of the test data. EPP has
full coverage, and although it does not make statistically significant predictions, it
shows substantially higher correlation coefficients than ROOTH ET AL. and RESNIK.
12 A second concern is the computation of significance values: The methods most widely used for the
Pearson coefficient (Student?s t-distribution, Fisher transformation) assume that the variables are
normally distributed, which is not the case in our data set. For Spearman?s ?, we use the algorithm by
Best and Roberts (1975), which does not make this assumption.
13 Significance here refers to significance of correlation with the human data, not significance of differences
between models.
749
Computational Linguistics Volume 36, Number 4
Table 8
Comparison across models on McRae data. **p < 0.01, ***p < 0.001.
SEM SYN
Model Coverage Spearman?s ? Coverage Spearman?s ?
EPP (DEPSPACE nGCM) 100% 0.154 ns 98% 0.325 ***
EPP (DEPSPACE Cosine) 100% 0.162 ns 98% 0.197 ns
RESNIK 100% ?0.041 ns 100% 0.123 ns
ROOTH ET AL. 67% 0.078 ns 48% 0.465 ***
PADO ET AL. 78% 0.415 ** ? ?
EPP (WORDSPACE Lin) 100% 0.138 ns 98% 0.062 ns
EPP (WORDSPACE nGCM) 100% 0.167 ns 98% 0.110 ns
FREQ 18% 0.087 ns 36% 0.103 ns
The DEPSPACE and WORDSPACE variants of EPP perform similarly here, and the simple
frequency baseline has very low coverage and correlation.
As the right-hand side of Table 8 shows, both ROOTH ET AL. and EPP achieve
better results in the SYN PRIMARY setting than in the SEM PRIMARY setting. The ROOTH
ET AL. model obtains a highly significant correlation. The combination of infrequent
headwords in the McRae data set and the large primary corpus brings out the benefits
that the ROOTH ET AL. model can derive from generalizing from verbs and nouns to
the latent classes via soft clustering. Unfortunately, its coverage is still quite low (48%),
and for this reason, the difference from the best EPP model is not significant.14 In the
SYN PRIMARY setting, the EPP DEPSPACE models clearly outperform the WORDSPACE
because of the DEPSPACE models? more aggressive filtering. Interestingly, RESNIK
still performs poorly in the SYN PRIMARY setting: WordNet does not make the right
generalizations to capture the selectional preferences at play in the McRae data, no
matter how much training data is available. This is underscored by an analysis of which
WordNet classes were most frequently determined as the strongest association with
the target verbs: The classes entity, person, and physical object are assigned in 60 out of
100 test cases for the McRae data (SYN PRIMARY setting), a data set where plausibility
is determined by factors much more fine-grained than animacy. (In the SEM PRIMARY
setting, the picture is similar with classes person, organism, and entity assigned in 48 out
of 72 test cases.) The frequency baseline again performs badly.
6.5 Pado Data Set: Results and Discussion
We now turn to the Pado data set. Again, we first focus on the performance of differ-
ent similarity measures in EPP using unreduced DEPSPACE (Table 9). Correlation with
human judgments is much better than for the McRae data set, and highly significant
for all SEM PRIMARY setting models and three of the SYN PRIMARY setting models. In
both settings, Cosine and Lin are the best measures (difference not significant), followed
by nGCM. Hindle comes out worst once more. The difference between the strong and
14 As in Experiment 1, we apply bootstrap resampling to determine the significance of differences between
models. This procedure also takes differences in coverage into account?specifically, a significant
difference becomes harder to achieve as the number of data points shared between the models shrinks.
750
Erk, Pad?, and Pad? A Flexible, Corpus-Driven Model of Selectional Preferences
Table 9
Comparison of EPP DEPSPACE parametrizations on Pad? data. Unreduced spaces, DISCR
weighting. **p < 0.01; ***p < 0.001.
SEM SYN
Sim Coverage Spearman?s ? Coverage Spearman?s ?
Dice 100% 0.289 *** 100% 0.026 ns
Jaccard 100% 0.285 *** 100% 0.023 ns
Cosine 100% 0.508 *** 100% 0.403 ***
Hindle 100% 0.160 ** 100% ?0.004 ns
Lin 100% 0.498 *** 100% 0.229 ***
nGCM 100% 0.384 *** 100% 0.156 **
weak measures is more pronounced for the SYN PRIMARY setting, compared with the
SEM PRIMARY setting. Coverage is at or close to 100% throughout.
Table 10 shows results on the Pado data set for all selectional preference models that
we consider. In the SEM PRIMARY setting (where both the data and the primary corpus
have FrameNet annotation), EPP and the deep PADO ET AL. model predict the human
judgments similarly well (difference not significant). Because all verbs in this data set
are covered by FrameNet, the PADO ET AL. model also shows a nearly perfect cover-
age. EPP and PADO ET AL. do much better than ROOTH ET AL. (differences significant
at p ? 0.01). ROOTH ET AL. has the lowest coverage at 88%, but this is still higher than
its coverage of the McRae data. As with the McRae data, ROOTH ET AL. achieves better
correlation in the SYN PRIMARY setting than the SEM PRIMARY setting, indicating that
the frequency cutoff does not harm performance as much in Experiment 2 as it did in
Experiment 1. However, the coverage of ROOTH ET AL. is lower in the SYN PRIMARY
setting, perhaps because the SEM PRIMARY setting smoothes rare verbs by grouping
them in frames with other verbs. RESNIK also achieves better correlation in the SYN
PRIMARY setting, but recall that it was trained on a subset of the BNC only to reduce
noise in the training data?when trained on the whole BNC set, performance degrades
to ? = 0.060. The difference from the best EPP model remains numerically large. As for
Table 10
Comparison across models on Pad? data. ***p < 0.001.
SEM SYN
Model Coverage Spearman?s ? Coverage Spearman?s ?
EPP (DEPSPACE Cosine) 100% 0.489 *** 98% 0.470 ***
EPP (DEPSPACE nGCM) 100% 0.393 *** 98% 0.328 ***
RESNIK 98% 0.230 *** 97% 0.317 ***
ROOTH ET AL. 88% 0.060 ns 58% 0.200 ***
PADO ET AL. 97% 0.515 *** ? ?
EPP (WORDSPACE Lin) 100% 0.254 *** 100% 0.056 ns
EPP (WORDSPACE nGCM) 100% 0.192 *** 100% 0.078 ns
FREQ 32% ?0.041 ns 69% 0.090 ns
751
Computational Linguistics Volume 36, Number 4
the McRae data set, the EPP WORDSPACE models show much worse performance than
the DEPSPACE models, and do not significantly predict the human plausibility ratings.
The frequency baseline shows a considerably better coverage for this data set, but
its correlations hover around zero, which underlines our intuition that verb?argument
combinations can be plausible without being frequent in corpora. An example is the
combination (to) embarrass (an) official, which is rated as highly plausible, but occurs
only once each in the BNC and FrameNet.
6.6 Discussion
The McRae data set seems in general more difficult to account for than the Pado data
set, as noted by Pad?, Pad?, and Erk (2007). They explain it by a general frequency effect
in the BNC data (which are a superset of the FrameNet data): The median frequency of
the hand-selected McRae nouns in the BNC is 1,356, as opposed to 8,184 for the corpus-
derived Pado nouns.
Comparing all selectional preference models, we find that the RESNIK and the
ROOTH ET AL. models generally do worse than EPP both in terms of coverage and
quality of predictions. One notable exception is the excellent performance of the ROOTH
ET AL. model on the McRae data in the SYN PRIMARY setting, which comes, however,
with a low coverage of less than 50%. A closer inspection of the predictions showed
that ROOTH ET AL. makes many predictions for verb?object pairs but abstains from
subjects, thus reducing the complexity of the task. For only 20% of verbs, predictions
are made for subjects and objects. As noted in Pad?, Pad?, and Erk (2007), the relatively
poor performance of the RESNIK model may be explained by the fact that its ability to
generalize is limited to the structure of WordNet, where some semantic distinctions are
easier to make than others. For example, a fairly easy distinction to make for WordNet-
based models is animate vs. inanimate. Because the Pado set contains a portion of
inanimate arguments with animate counterparts, the RESNIK model does well on those.
In contrast, in the McRae test set, all arguments are animates, and thus similar to one
another in terms of WordNet.
The deep PADO ET AL. model achieves the best correlation with the human judg-
ments on both data sets, but it is limited to the SEM PRIMARY setting. Although the
best model is not always among the EPP DEPSPACE models, they consistently show a
coverage of close to 100%, and are generally statistically indistinguishable from the best
model. Unlike ROOTH ET AL. and RESNIK, whose performance varies widely between
the SEM PRIMARY setting and the SYN PRIMARY setting, the correlation coefficients for
the EPP models are generally similar across settings. We take this as evidence that EPP
models can extract relevant information from deeper annotation on small corpora as
well as from large, but noisy and shallow, training data.
Finally, we consider the different similarity measures for the EPP model evaluated
on unreduced DEPSPACE. The picture differs somewhat between the two data sets, but
the Cosine measure performs well overall, with Lin and nGCM generally in second
and third place. So, the group of the three best similarity measures is the same as in
Experiment 1, but Cosine shows better performance. One possible reason for this lies
in the verb frequency, which is relatively high in both data sets: 68% of the McRae
verbs and 83% of the Pado verbs have BNC frequencies of 1,000 and more, whereas
Experiment 1 used an equal number of predicates from five frequency bins, the highest
being 1,000 and more occurrences. In that highest predicate frequency bin, Cosine
consistently performed as well as Lin or better in Experiment 1 (Figures 5 and 7).
752
Erk, Pad?, and Pad? A Flexible, Corpus-Driven Model of Selectional Preferences
7. Experiment 3: Inverse Selectional Preferences
The term selectional preference is typically used to describe the semantic constraints
that predicates place on their arguments. In this section, we will investigate how nominal
arguments place semantic constraints or expectations on the predicates with which they
occur. Such expectations can be thought of as typical events that involve the given
object. For example, a noun like apple could be said to have preferences about its inverse
subject position, that is, the verbs that can take it as a plausible subject. Examples might
be verbs like grow or fall; for its inverse object position, apple probably prefers verbs
like eat, cut, or plant. We will use the term inverse selectional preference to refer to
preferences of nouns for their predicates, distinguishing them from regular selectional
preferences.
It is clear that not all verbs will be equally likely to occur with a given noun?
role pair. Still, inverse selectional preferences warrant a closer look: To what extent do
inverse selectional preferences differ from regular ones? And are the tasks of predicting
regular and inverse selectional preferences equally difficult? We start in Section 7.2 with
an exploratory data analysis of inverse selectional preferences, which shows that inverse
selectional preferences show semantically coherent patterns like regular selectional
preferences, but that, in contrast to most verbs, nouns tend to occur with multiple
semantic groups of verbs. In Sections 7.3?7.5, we test the EPP model on a pseudo-
disambiguation task for inverse selectional preferences.
7.1 Related Work
In computational linguistics, some approaches to characterizing selectional preferences
have used the symmetric nature of their models to characterize nouns in terms of the
verbs that they use (Hindle 1990; Rooth et al 1999). However, they do not explicitly
compare the two types of preferences. Also, there are approaches using selectional
preference information, in particular for word sense disambiguation and related tasks,
that could be characterized as using regular along with inverse selectional preferences
(Dligach and Palmer 2008; Erk and Pad? 2008; Nastase 2008). By comparing selectional
preference model performance on the tasks of predicting inverse and regular selectional
preferences in Sections 7.3?7.5, we hope to contribute to an understanding of what can
be achieved by using inverse preferences in word sense analysis tasks.
At the same time, inverse selectional preferences have been the object of fruitful
research in both psycholinguistics and theoretical linguistics. In psycholinguistics, a
particularly plausible argument for the existence of expectations of nouns for their
predicates in human language processing is head-final word order (as in Japanese or
in German subordinate clauses), where hearers may encounter all objects before the
head. It is likely that these objects are immediately integrated into a preliminary event
structure with an assumed predicate instead of being stored in short-term memory until
the predicate is encountered (Konieczny and D?ring 2003; Nakatani and Gibson 2009).
Another strand of work is McRae et al (2001, 2005), who have studied priming of verbs
from nouns. They found that a noun engenders priming of verbs for which it is a typical
agent, patient, instrument, or location.
In theoretical linguistics, the idea of event knowledge being encoded in the lex-
ical entries of nouns has been formulated in the context of Pustejovsky?s generative
lexicon (Pustejovsky 1995), where the qualia roles TELIC and AGENTIVE provide infor-
mation about the typical use of an object (book: read) and its construction (book: write),
753
Computational Linguistics Volume 36, Number 4
respectively. Pustejovsky uses this knowledge to account, for example, for the interpre-
tation of logical metonymy (begin a book). Although qualia roles are instantiated with
individual predicates rather than characterizations of all possible events, construction
and use are arguably two very salient events for an object. Through the data exploration
in Section 7.2, we hope to contribute to a linguistic characterization of inverse selectional
preferences.
7.2 Empirical Analysis of Inverse Selectional Preferences
The first question we ask concerns the selectional preference strength of regular and
inverse selectional preferences, using the measure introduced by Resnik (1996) to de-
termine the degree to which verbs select for nouns, and vice versa. As verb?role pairs,
we re-use the same 100 pairs that were used for the pseudo-disambiguation task in
Experiment 1. For the comparison, we randomly sample a total of 100 noun/inverse-
role pairs from the BNC, using the same five frequency bands as for the verbs (50?
100, 100?200, 200?500, 500?1,000, >1,000). The sample contains approximately the same
number of (inverse) subject and object roles.
We adapt the selectional preference strength measure from Equation (1) to our
case: Unlike Resnik, we compute KL divergence not on a distribution across WordNet
synsets, but on a distribution across lemmas.
SelStr(w1, r) = D(P(w2|w1, r)||P(w2|r)) (7)
For regular selectional preferences, w1 is a verb lemma, w2 a noun lemma, and r a role.
For inverse preferences, w1 is a noun lemma, w2 a verb lemma, and r an inverse role.
SelStr(w1, r) can be interpreted as a measure of the degree to which w1 has selectional
preferences concerning the role r. We induce the probability distributions through
maximum likelihood estimation on the BNC.
We can expect to see the same overall tendency in regular and inverse selec-
tional preference strength. It is not possible that inverse selectional preference strength
would be uniform throughout if regular selectional preference strength varied between
verbs. After all, if we fix the relation r for the time being, P(v|n) and P(n|v) are re-
lated through Bayes? formula. Instead, the questions we will ask are more specific.
Are regular and inverse preference strengths similar in size? Are regular and inverse
preference strengths similar by frequency band?that is, do frequent nouns behave
similarly to frequent verbs? And what effects do we see of the prior distributions P(n|r)
and P(v|r)?
Table 11 shows the range of selectional preference strengths found in each frequency
band for verbs and nouns. As expected, we see substantial strengths in both regular
and inverse preferences. Both parts of speech show the same pattern of decreasing KL
divergences for higher-frequency words, presumably because frequent words tend to be
polysemous, and can combine with many different words. However, the strengths for
inverse selectional preferences are in general lower than those for regular preferences.
One possible reason for this is that the number of nouns seen with a verb?role
pair might differ, in general, from the number of verbs seen with each noun?role
pair. However, we find that verbs and nouns occur with roughly the same number of
associates in the frequency bands up to the 200?500 band. In the band 500?1,000, verbs
appear with roughly one third more nouns than nouns appear with verbs, and in the
band of 1,000 occurrences or more, verbs appear with twice as many nouns (on average)
754
Erk, Pad?, and Pad? A Flexible, Corpus-Driven Model of Selectional Preferences
Table 11
Minimal, median, and maximal selectional preference strength (measured in terms of KL
divergence) in a sample of 100 verbs and 100 nouns (20 lemmas each per frequency band).
Band Verbs Nouns
min median max min median max
50?100 4.5 7.4 8.8 3.7 4.8 6.3
100?200 3.9 5.8 7.6 2.7 3.8 5.0
200?500 3.3 5.2 6.9 2.4 3.3 4.7
500?1,000 2.4 4.4 5.9 1.9 2.9 4.1
1,000? 1.8 3.6 6.2 1.4 2.3 3.7
as nouns appear with verbs in this band (1,189 vs. 636). Incidentally, the fact that the
highest-frequency verbs (which also tend to be the most ambiguous) appear in a much
larger number of contexts than the highest-frequency nouns could be a contributing
factor to the well-known problem that verbs are harder to disambiguate than nouns.
For the lower frequency bands, number of associates is unlikely to be the reason for
the weaker inverse preferences. Instead, a more likely reason for the overall weaker
inverse preferences lies in the overall distributions of nouns and verbs in the BNC.
Both show a Zipfian distribution, but there are 15,570 verbs as opposed to 455,173
nouns. Recall that KL divergence will be high whenever the individual terms p(w2|w1,r)p(w2|r)
to be summed are large. This, in turn, is the case when p(w2|r) is small. And p(w2|r) may
be small when the distribution p(w2|r) ranges over a larger number of words w2. For
regular selectional preferences, the w2 are nouns, and for inverse preferences the w2 are
verbs. Because there are many more nouns than verbs, the denominator p(w2|r) tends to
be smaller for regular preferences.
To get a clearer understanding of how inverse selectional preferences compare to
regular selectional preferences, we next do a qualitative analysis, looking at association
strength SelAssoc for individual triples verb?role?noun and noun?inverse-role?verb.
We adapt Equation (2) to the lexicon-free case and obtain
SelAssoc(w1, r, w2) =
1
SelStr(w1, r)
P(w2|w1, r) log
P(w2|w1, r)
P(w2|r)
(8)
Table 12 shows the five strongest associates for one verb?role pair and one noun?role
pair from each frequency band. The associates on both sides of the table generally
are semantically coherent and make intuitive sense. However, there is an interesting
difference between the verbs and nouns: We find that the nouns? preferred verbs can
often be grouped loosely into several meaning clusters, whereas the verbs? associates
tend to group into one cluster per grammatical function. For example, predicates taking
wheat as objects fall into those describing production (grow, sow) and those describing
processing (shred, grind, mill). Similarly, the predicates found for pill either concern
ingestion (take, swallow, pop), prescription, or idiomatic usage. In contrast, the objects of
rebut describe different kinds of statements, and the objects of celebrate are anniversaries
and other special events. Another observation that we can make in Table 12 is that
the nouns? most preferred associates have a similarly large share in the nouns? overall
selectional preference strength as the verbs? most preferred associates have in the verbs?
selectional preference strength. This indicates that the distribution of selectional prefer-
ences is similarly skewed towards the most preferred associate for verbs and nouns.
755
Computational Linguistics Volume 36, Number 4
Table 12
Examples of regular and inverse selectional preferences from different frequency bands for
argument positions of nouns and verbs: overall selectional preference strength SelStr and most
highly associated fillers with association strengths SelAssoc.
Band Verbs Nouns
50?100
rebut?obj, SelStr(w)= 7.43 wreckage?obj?1, SelStr(w)= 5.91
presumption 0.283 survey 0.126
allegation 0.088 examine 0.089
charge 0.082 sift 0.075
criticism 0.049 clear 0.056
claim 0.041 sight 0.051
100?200
enunciate?obj, SelStr(w)= 6.89 wheat?obj?1, SelStr(w)= 5.00
principle 0.242 grow 0.184
word 0.085 shred 0.049
theory 0.034 grind 0.049
philosophy 0.034 mill 0.042
policy 0.029 sow 0.040
200?500
break_with?obj, SelStr(w)= 6.92 pill?obj?1, SelStr(w)= 4.15
tradition 0.237 take 0.290
past 0.054 swallow 0.165
precedent 0.035 sweeten 0.070
convention 0.035 prescribe 0.049
Rome 0.022 pop 0.028
500?1,000
commence?obj, SelStr(w)= 5.92 dividend?obj?1, SelStr(w)= 4.10
proceedings 0.185 pay 0.508
action 0.051 declare 0.064
work 0.043 receive 0.064
proceeding 0.041 recommend 0.054
operation 0.033 raise 0.023
1,000?
celebrate?obj, SelStr(w)= 6.23 requirement?obj?1, SelStr(w)= 3.24
anniversary 0.177 meet 0.332
birthday 0.170 satisfy 0.015
centenary 0.046 comply_with 0.093
victory 0.033 fulfill 0.061
mass 0.028 impose 0.028
In sum, we find that inverse selectional preferences have weaker overall selectional
preference strength than regular preferences, but that may be due more to specifics of
the formula used rather than the skewness towards preferred role fillers. Two differ-
ences do emerge, though. First, noun selectional preferences show more semantic filler
sets than verb preferences. Second, the highest frequency verbs appear with many more
different associates than the highest frequency nouns.
7.3 Modeling Inverse Selectional Preferences
In the rest of this section, we test selectional preference models on the task of pre-
dicting inverse selectional preferences in a pseudo-disambiguation task, and compare
the results to the performance on predicting regular preferences in Experiment 1. We
do not repeat Experiment 2 even though it would have been technically possible to
756
Erk, Pad?, and Pad? A Flexible, Corpus-Driven Model of Selectional Preferences
re-use the McRae and Pado data sets and predict plausibility judgments through inverse
preferences. However, the data sets combine each verb with both plausible and im-
plausible nouns, but they do not combine each noun with different verbs in a balanced
fashion, so a repetition of Experiment 2 with inverse preferences would not be very
informative.
For the pseudo-disambiguation experiment, we focus on the EPP model. Distri-
butional models can, in general, be used straightforwardly to model both regular
and inverse selectional preferences. This is different for models like RESNIK that use
the WordNet noun hierarchy to represent regular selectional preferences. To model
inverse preferences, it would be necessary to use the WordNet verb hierarchy. However,
WordNet organizes verbs in a comparatively flat, unconnected hierarchy with a high
branching factor formed by the hypernymy/troponymy (?type of?) relation. This makes
effective generalization difficult, in particular in conjunction with the marked variation
in the set of preferred predicates that we observed for inverse selectional preferences in
Section 7.2.
We adapt the formulation of the EPP model to the inverse selectional preference
case as follows. Let a stand for a noun, r for an inverse argument position of this
noun, and Seenpreds(r, a) for the set of predicates seen with noun a and role r. Then the
selectional preference SelprefEPP of (r, a) for a verb v0 is defined in parallel to Equation (6)
as weighted average similarity to seen verbs:
SelprefEPPr,a(v0) =
?
v?Seenpreds(r,a)
wtr,a(v)
Zr,a
sim(v, v0) (9)
with Zr,a =
?
v?Seenpreds(r,a) wtr,a(v) as the normalization constant.
7.4 Pseudo-Disambiguation: Experimental Setup
We evaluate inverse selectional preferences on a pseudo-disambiguation task that is
set up completely analogously to our experiments on regular preferences in Section 5:
given a noun, an inverse argument position, one verb observed in this position, and a
confounder verb, distinguish between the two verbs. We use the 100 nouns sampled
across five frequency bands that we already used in Section 7.2. We experiment with
both WORDSPACE and DEPSPACE models, but restrict our attention to DISCR weighting,
which showed good results in Experiment 1.
In Section 5, we experimented on two different primary corpora, the BNC (SYN
PRIMARY setting) and FrameNet (SEM PRIMARY setting). Subsequently, we will use the
SYN PRIMARY setting again, but not the SEM PRIMARY setting. In the SEM PRIMARY
setting, the roles are FrameNet frame elements (semantic roles). However, frame ele-
ments are specific to a single frame, for example, the frame element ROPE belongs to
the frame ROPE_MANIPULATION.15 It would thus be pointless to predict a verb frame
given a noun and a frame element name, as the frame element already gives away the
frame.
15 It is possible for multiple frame elements to share a name, for example there are multiple frames with a
frame element named THEME. However, conceptually, this is only a shared name, not a shared role across
frames.
757
Computational Linguistics Volume 36, Number 4
7.5 Pseudo-Disambiguation: Results and Discussion
Table 13 shows the results of testing the EPP model for inverse selectional preferences
on pseudo-disambiguation. Coverage is very good for all model variants, similarly to
Experiment 1. The error rates, as well, are close to those for the regular preferences in
the SYN PRIMARY setting (cf. Table 4). The best model there (DEPSPACE, PCA, nGCM
with DISCR weighting) achieved an error rate of 25.6%, and the best model for inverse
preferences (WORDSPACE, Lin with DISCR weighting) reaches an error rate of 27.2%
here. Lin shows the best error rates in all conditions, closely followed by nGCM (the
difference is significant in WORDSPACE and the reduced DEPSPACE, but not significant
in the unreduced DEPSPACE). The Hindle similarity measure again brings up the rear.
In PCA-transformed spaces, the error rates are similar across all similarity measures
except for Hindle, as in Experiment 1.
WORDSPACEs yield better results than DEPSPACEs here, in contrast to Experiment 1.
The best WORDSPACE model (Lin without PCA) reaches significantly better error rates
(p ? 0.01) than the best DEPSPACE model (Lin with PCA). We think that the reason for
this lies in the fact that for inverse selectional preferences, the true associate and the
confounder that need to be distinguished in the pseudo-disambiguation task are verbs
rather than nouns. A noun will probably have more other nouns in a bag-of-words
context window than a verb would other verbs, which will make it easier to distinguish
verbs in a WORDSPACE than to distinguish nouns. A DEPSPACE, in contrast, will bring
out differences in the immediate syntactic neighborhood of nouns even if they occur in
the same sentence.
8. Conclusion
In this article, we have presented a similarity-based model of selectional preferences,
EPP. It computes the selectional fit of a candidate role filler as a weighted sum of seman-
tic similarities to headwords observed in a corpus, in a straightforward implementation
Table 13
Pseudo-disambiguation results for inverse selectional preferences (BNC as primary and
secondary corpus, DISCR weighting). ER = Error rate; Cov = Coverage.
Dimensions Similarity DEPSPACE WORDSPACE
ER (%) Cov (%) ER (%) Cov (%)
Original
2,000 dimensions
Cosine 37.4 99.0 34.0 99.1
Dice 42.4 98.8 43.4 98.7
nGCM 33.7 99.3 31.5 99.3
Hindle 48.8 96.0 52.2 94.6
Jaccard 36.7 99.4 44.9 98.7
Lin 32.8 98.9 27.2 98.9
PCA
500 dimensions
Cosine 35.2 99.0 31.3 99.4
Dice 35.0 99.6 32.9 99.8
nGCM 32.4 99.2 30.3 99.6
Hindle 44.2 99.0 48.7 99.1
Jaccard 34.8 99.6 32.6 99.8
Lin 30.6 99.8 28.8 99.8
758
Erk, Pad?, and Pad? A Flexible, Corpus-Driven Model of Selectional Preferences
of the intuition that plausibility judgments should generalize to fillers with similar
meaning. Our model is simple and easy to compute. In common with other distri-
butional models like Rooth et al (1999), it does not depend on lexical resources. Our
model derives additional flexibility from distinguishing between a primary corpus (for
observing headwords) and a generalization corpus (for inducing semantic similarities).
This allows it to use primary corpora with deeper semantic annotation that are too small
as a basis for computing vector space representations.
We have evaluated the EPP model on two tasks, a pseudo-disambiguation task
that can be viewed as an abstraction of both word sense disambiguation and semantic
role labeling, as well as on the prediction of human plausibility judgments. The model
achieves similar error rates to the semantic hierarchy?based RESNIK model, at consid-
erably higher coverage, and it achieves lower error rates than the ROOTH ET AL. soft
clustering model. The semantic role?based PADO ET AL. model, although highly accu-
rate in its predictions, has much lower coverage and needs a semantically annotated
corpus as a basis. We have also demonstrated that our model is able to meaningfully
model inverse selectional preferences, that is, expectations of nouns about verbs for
which they appear as arguments.
With respect to parameter settings of the EPP model, we find consistent patterns
across the three tasks we have considered. nGCM, Lin, and Cosine are the best-
performing similarity measures throughout. The good performance of the nGCM mea-
sure, an exponential similarity measure, is particularly noteworthy. We found it to work
well on data sets that are sparse and not too noisy, whereas the Lin similarity measure
achieved better performance when the data was noisy (see Section 5.4 for details). Di-
mensionality reduction (PCA) on the vector space raises the performance of the Jaccard
and Dice similarity measures to a similar level as the best three. More importantly, PCA
neutralizes a strong frequency bias that otherwise leads to a large performance drop
on rare arguments. Concerning weighting schemes, we found that frequency-based
weighting works well when the data is either clean or not too sparse. In the face of sparse
noisy data, DISCR weighting (a variant of tf/idf) is helpful. Comparing bag-of-words?
based and dependency-based vector spaces, DEPSPACEs are sparser but cleaner than
WORDSPACEs. Accordingly, DEPSPACEs are at an advantage when many headwords are
available, making efficient use of this information, whereas WORDSPACEs work better
for predicates with few seen headwords because they are less affected by sparseness.
We conclude with two open questions. The first question concerns the appropriate
representation of selectional preferences for polysemous verbs such as address, whose
direct object can either be a person, or a problem. Polysemy leads to headwords with
lower similarity among them than for non-polysemous verbs, which in turn can lead
to artificially low plausibilities for all fillers. In the SEM PRIMARY setting, occurrences
of polysemous verbs are separated into different frames. In future work, we hope to
improve our SYN PRIMARY setting models by clustering the seen headwords, and then
computing plausibility of new headwords relative to the nearest cluster.
A second question is the usefulness of inverse selectional preferences for the ac-
quisition of fine-grained information about nouns. As we discussed in Section 7, the
preferred verbs for a noun can often be grouped into meaning clusters. In future work,
we plan to investigate whether there are groups of predicates that recur across similar
nouns, and how they can be characterized. We expect some groups to correspond to
Pustejovsky?s qualia (Pustejovsky 1995), which constitute particularly salient events
for an object, namely, their creation and typical use. However, we expect corpus data
to yield a more complex picture of the events connected to a noun, which manifest
themselves in the form of additional, more specific meaning clusters.
759
Computational Linguistics Volume 36, Number 4
Acknowledgments
We would like to thank Detlef Prescher
and Carsten Brockmann for their
implementations of the ROOTH ET AL.
and RESNIK models, respectively. We
are also grateful to Nate Chambers and
Yves Peirsman, as well as to the
anonymous reviewers, for their
comments and suggestions.
References
Abe, Naoki and Hang Li. 1996. Learning
word association norms using tree cut pair
models. In Proceedings of the 10th
International Conference on Machine
Learning, pages 3?11, Bari.
Baroni, Marco, Silvia Bernardini, Adriano
Ferraresi, and Eros Zanchetta. 2009. The
wacky wide Web: A collection of very
large linguistically processed Web-crawled
corpora. Language Resources and Evaluation,
43(3):209?222.
Bergsma, Shane, Dekang Lin, and Randy
Goebel. 2008. Discriminative learning of
selectional preference from unlabeled text.
In Proceedings of the 13th Conference on
Empirical Methods in Natural Language
Processing, pages 59?68, Honolulu, HI.
Best, John and D. E. Roberts. 1975. Algorithm
AS 89: The upper tail probabilities of
Spearman?s Rho. Applied Statistics,
24:377?379.
Briscoe, Ted and Bran Boguraev, editors.
1989. Computational Lexicography for
Natural Language Processing. Longman
Publishing Group, New York.
Brockmann, Carsten and Mirella Lapata.
2003. Evaluating and combining
approaches to selectional preference
acquisition. In Proceedings of the 16th
Meeting of the European Chapter of the
Association for Computational Linguistics,
pages 27?34, Budapest.
Burnard, Lou, 1995. User?s Guide for the
British National Corpus. British National
Corpus Consortium, Oxford University
Computing Services.
Ciaramita, Massimiliano and Mark Johnson.
2000. Explaining away ambiguity:
Learning verb selectional preference
with Bayesian networks. In Proceedings
of the 18th International Conference on
Computational Linguistics, pages 187?193,
Saarbr?cken.
Clark, Stephen and David Weir. 2001.
Class-based probability estimation using a
semantic hierarchy. In Proceedings of the
2nd Annual Meeting of the North American
Chapter of the Association for Computational
Linguistics, pages 95?102, Pittsburgh, PA.
Collins, Michael. 1997. Three generative,
lexicalised models for statistical parsing.
In Proceedings of the 35th Annual Meeting of
the Association for Computational Linguistics,
pages 16?23, Madrid.
Curran, James. 2004. From Distributional to
Semantic Similarity. Ph.D. thesis, University
of Edinburgh.
Daelemans, Walter and Antal van den
Bosch. 2005. Memory-Based Language
Processing. Cambridge University Press,
Cambridge, UK.
Dagan, Ido, Lillian Lee, and Fernando C. N.
Pereira. 1999. Similarity-based models
of word cooccurrence probabilities.
Machine Learning, 34(1):34?69.
Dligach, Dmitriy and Martha Palmer. 2008.
Novel semantic features for verb sense
disambiguation. In Proceedings of the
46th Annual Meeting of the Association for
Computational Linguistics:Human Language
Technologies, Short Papers, pages 29?32,
Columbus, OH.
Dunning, Ted. 1993. Accurate methods for
the statistics of surprise and coincidence.
Computational Linguistics, 19(1):61?74.
Efron, Bradley and Robert Tibshirani. 1994.
An Introduction to the Bootstrap.
Monographs on Statistics and Applied
Probability 57. Chapman & Hall, London.
Erk, Katrin. 2007. A simple, similarity-based
model for selectional preferences. In
Proceedings of the Annual Meeting of the
Association for Computational Linguistics,
pages 216?223, Prague.
Erk, Katrin and Sebastian Pad?. 2008.
A structured vector space model for
word meaning in context. In Proceedings
of the 13th Conference on Empirical
Methods in Natural Language Processing,
pages 897?906, Honolulu, HI.
Fillmore, Charles J., Christopher R. Johnson,
and Miriam R. L. Petruck. 2003.
Background to FrameNet. International
Journal of Lexicography, 16:235?250.
Firth, John Rupert. 1957. A synopsis of
linguistic theory, 1930?1955. In Philological
Society, editor, Studies in Linguistic
Analysis. Blackwell, Oxford, pages 1?32.
Garnsey, Susan, Neal Pearlmutter, Elizabeth
Myers, and Melanie Lotocky. 1997. The
contributions of verb bias and plausibility
to the comprehension of temporarily
ambiguous sentences. Journal of Memory
and Language, 37:58?93.
Gildea, Daniel and Daniel Jurafsky. 2002.
Automatic labeling of semantic roles.
Computational Linguistics, 28(3):245?288.
760
Erk, Pad?, and Pad? A Flexible, Corpus-Driven Model of Selectional Preferences
Grefenstette, Gregory. 1994. Explorations in
Automatic Thesaurus Discovery. Kluwer
Academic Publishers, Dordrecht.
Grishman, Ralph and John Sterling. 1992.
Acquisition of selectional patterns. In
Proceedings of the 14th International
Conference on Computational Linguistics,
pages 658?664, Nantes.
Harris, Zellig. 1968. Mathematical Structure of
Language. Wiley, New York.
Hay, Jennifer, Aaron Nolan, and Katie
Drager. 2006. From fush to feesh: Exemplar
priming in speech perception. The
Linguistic Review, 23(3):351?379.
Hindle, Donald. 1990. Noun classification
from predicate-argument structures. In
Proceedings of the 28th Annual Meeting of the
Association for Computational Linguistics,
pages 268?275, Pittsburgh, PA.
Hindle, Donald and Mats Rooth. 1993.
Structural ambiguity and lexical relations.
Computational Linguistics, 19(1):103?120.
Katz, Jerrold J. and Jerry A. Fodor. 1963. The
structure of a semantic theory. Language,
39(2):170?210.
Katz, Jerrold J. and Paul M. Postal. 1964. An
Integrated Theory of Linguistic Descriptions.
Research Monograph No. 26. MIT Press,
Cambridge, MA.
Konieczny, Lars and Philipp D?ring. 2003.
Anticipation of clause-final heads.
Evidence from eye-tracking and SRNs.
In Proceedings of the 4th International
Conference on Cognitive Science,
pages 330?335, Sydney.
Lakoff, George and Mark Johnson. 1980.
Metaphors We Live By. University of
Chicago Press, Chicago, IL.
Landauer, Thomas and Susan Dumais. 1997.
A solution to Plato?s problem: The latent
semantic analysis theory of acquisition,
induction, and representation of
knowledge. Psychological Review,
104(2):211?240.
Lee, Lillian. 1999. Measures of distributional
similarity. In Proceedings of the 31st Annual
Meeting of the Association for Computational
Linguistics, pages 25?32, College Park, MA.
Lin, Dekang. 1993. Principle-based parsing
without overgeneration. In Proceedings of
the 31st Annual Meeting of the Association for
Computational Linguistics, pages 112?120,
Columbus, OH.
Lin, Dekang. 1998. Automatic retrieval and
clustering of similar words. In Proceedings
of the Joint Annual Meeting of the Association
for Computational Linguistics and
International Conference on Computational
Linguistics, pages 768?774, Montreal.
Lowe, Will. 2001. Towards a theory of
semantic space. In Proceedings of the 23rd
Annual Conference of the Cognitive Science
Society, pages 576?581, Edinburgh.
Lowe, Will and Scott McDonald. 2000. The
direct route: Mediated priming in semantic
space. In Proceedings of the 22nd Annual
Conference of the Cognitive Science Society,
pages 675?680, Philadelphia, PA.
Lund, Kevin and Curt Burgess. 1996.
Producing high-dimensional semantic
spaces from lexical co-occurrence. Behavior
Research Methods, Instruments, and
Computers, 28:203?208.
McCarthy, Diana. 2000. Using semantic
preferences to identify verbal participation
in role switching alternations. In
Proceedings of the 1st Annual Meeting of the
North American Chapter of the Association for
Computational Linguistics, pages 256?263,
Seattle, WA.
McCarthy, Diana and John Carroll. 2003.
Disambiguating nouns, verbs, and
adjectives using automatically acquired
selectional preferences. Computational
Linguistics, 29(4):639?654.
McCarthy, Diana, Rob Koeling, Julie Weeds,
and John Carroll. 2004. Finding
predominant word senses in untagged
text. In Proceedings of the 42th Annual
Meeting of the Association for Computational
Linguistics, pages 279?286, Barcelona.
McCarthy, Diana, Sriram Venkatapathy, and
Aravind K. Joshi. 2007. Detecting
compositionality of verb-object
combinations using selectional
preferences. In Proceedings of the 12th Joint
Conference on Empirical Methods in Natural
Language Processing and Conference on
Natural Language Learning, pages 369?379,
Prague.
McDonald, Scott and Chris Brew. 2004. A
distributional model of semantic context
effects in lexical processing. In Proceedings
of the 42th Annual Meeting of the Association
for Computational Linguistics, pages 17?24,
Barcelona.
McRae, Ken, Todd Ferretti, and Liane
Amyote. 1997. Thematic roles as
verb-specific concepts. Language and
Cognitive Processes, 12(2/3):137?176.
McRae, Ken, Mary Hare, Jeffrey Elman, and
Todd Ferretti. 2005. A basis for generating
expectancies for verbs from nouns.
Memory and Cognition, 33(7):1174?1184.
McRae, Ken, Mary Hare, Todd Ferretti, and
Jeffrey Elman. 2001. Activating verbs
typical agents, patients, instruments, and
locations via event schemas. In Proceedings
761
Computational Linguistics Volume 36, Number 4
of the Twenty-Third Annual Conference of the
Cognitive Science Society, pages 617?622,
Mahwah, NJ.
McRae, Ken, Michael Spivey-Knowlton, and
Michael Tanenhaus. 1998. Modeling the
influence of thematic fit (and other
constraints) in on-line sentence
comprehension. Journal of Memory and
Language, 38:283?312.
Miller, George, Richard Beckwith, Christiane
Fellbaum, Derek Gross, and Katherine
Miller. 1990. Five papers on WordNet.
International Journal of Lexicography,
3(4):235?312.
Nakatani, Kentaro and Edward Gibson. 2009.
An on-line study of Japanese nesting
complexity. Cognitive Science, 1(34):94?112.
Nastase, Vivi. 2008. Unsupervised all-words
word sense disambiguation with
grammatical dependencies. In Proceedings
of the 3rd International Joint Conference
on Natural Language Processing,
pages 757?762, Honolulu, HI.
Nosofsky, Robert. 1986. Attention, similarity,
and the identification-categorization
relationship. Journal of Experimental
Psychology: General, 115(1):39?57.
Pad?, Sebastian and Mirella Lapata. 2007.
Dependency-based construction of
semantic space models. Computational
Linguistics, 33(2):161?199.
Pad?, Sebastian, Ulrike Pad?, and Katrin Erk.
2007. Flexible, corpus-based modelling of
human plausibility judgements. In
Proceedings of the 12th Joint Conference on
Empirical Methods in Natural Language
Processing and Conference on Natural
Language Learning, pages 400?409, Prague.
Pad?, Ulrike. 2007. The Integration of Syntax
and Semantic Plausibility in a Wide-Coverage
Model of Human Sentence Processing. Ph.D.
thesis, Saarland University, Saarbr?cken,
Germany.
Pad?, Ulrike, Matthew W. Crocker, and
Frank Keller. 2009. A probabilistic model
of semantic plausibility in sentence
processing. Cognitive Science, 33:794?838.
Pantel, Patrick, Rahul Bhagat, Bonaventura
Coppola, Timothy Chklovski, and
Eduard Hovy. 2007. ISP: Learning
inferential selectional preferences. In
Proceedings of the Joint Human Language
Technology Conference and Annual Meeting
of the North American Chapter of the
Association for Computational Linguistics,
pages 564?571, Rochester, NY.
Pereira, Fernando, Naftali Tishby, and Lillian
Lee. 1993. Distributional clustering of
English words. In Proceedings of the 31st
Annual Meeting of the Association for
Computational Linguistics, pages 183?190,
Columbus, OH.
Pustejovsky, James. 1995. The Generative
Lexicon. MIT Press, Cambridge, MA.
Resnik, Philip. 1996. Selectional constraints:
An information-theoretic model and its
computational realization. Cognition,
61:127?159.
Rooth, Mats, Stefan Riezler, Detlef Prescher,
Glenn Carroll, and Franz Beil. 1999.
Inducing a semantically annotated lexicon
via EM-based clustering. In Proceedings of
the 37th Annual Meeting of the Association for
Computational Linguistics, pages 104?111,
College Park, MA.
Salton, Gerard, Anita Wong, and Chung-Shu
Yang. 1975. A vector-space model for
information retrieval. Journal of the
American Society for Information Science,
18:613?620.
Schulte im Walde, Sabine. 2010. Comparing
computational approaches to selectional
preferences: Second-order co-occurrence
vs. latent semantic clusters. In Proceedings
of the 7th International Conference on
Language Resources and Evaluation,
pages 1381?1388, Valleta.
Schulte im Walde, Sabine, Christian Hying,
Christian Scheible, and Helmut Schmid.
2008. Combining EM training and the
MDL principle for an automatic verb
classification incorporating selectional
preferences. In Proceedings of the 46th
Annual Meeting of the Association for
Computational Linguistics, pages 496?504,
Columbus, OH.
Shepard, Roger. 1987. Towards a universal
law of generalization for psychological
science. Science, 237(4820):1317?1323.
Stolcke, Andreas. 2002. SRILM?an
extensible language modeling toolkit. In
Proceedings of the International Conference on
Spoken Language Processing, pages 901?904,
Denver, CO.
Toutanova, Kristina, Christoper D. Manning,
Dan Flickinger, and Stephan Oepen. 2005.
Stochastic HPSG parse selection using the
Redwoods corpus. Journal of Research on
Language and Computation, 3(1):83?105.
Trueswell, John, Michael Tanenhaus, and
Susan Garnsey. 1994. Semantic influences
on parsing: Use of thematic role
information in syntactic ambiguity
resolution. Journal of Memory and Language,
33:285?318.
Vandekerckhove, Bram, Dominiek Sandra,
and Walter Daelemans. 2009. A robust and
extensible exemplar-based model of
762
Erk, Pad?, and Pad? A Flexible, Corpus-Driven Model of Selectional Preferences
thematic fit. In Proceedings of the 12th
Meeting of the European Chapter of the
Association for Computational Linguistics,
pages 826?834, Athens.
Wilks, Yorick. 1975. Preference semantics.
In E. Keenan, editor, Formal Semantics of
Natural Language. Cambridge University
Press, Cambridge, UK, pages 329?350.
Yarowsky, David. 1993. One sense per
collocation. In Proceedings of the ARPA
Human Language Technology Workshop,
pages 266?271, Princeton, NJ.
Zanzotto, Fabio Massimo, Marco
Pennacchiotti, and Maria Teresa Pazienza.
2006. Discovering asymmetric entailment
relations between verbs using selectional
preferences. In Proceedings of the Joint
Annual Meeting of the Association for
Computational Linguistics and International
Conference on Computational Linguistics,
pages 849?856, Sydney.
Zapirain, Be?at, Eneko Agirre, and Llu?s
M?rquez. 2009. Generalizing over lexical
features: Selectional preferences for
semantic role classification. In Proceedings
of the 47th Annual Meeting of the Association
for Computational Linguistics, pages 73?76,
Singapore.
763

