Proceedings of the EACL 2009 Workshop on Computational Approaches to Semitic Languages, pages 45?52,
Athens, Greece, 31 March, 2009. c?2009 Association for Computational Linguistics
Automatic Treebank-Based Acquisition of Arabic LFG Dependency
Structures
Lamia Tounsi Mohammed Attia
NCLT, School of Computing, Dublin City University, Ireland
{lamia.tounsi, mattia, josef}@computing.dcu.ie
Josef van Genabith
Abstract
A number of papers have reported on meth-
ods for the automatic acquisition of large-scale,
probabilistic LFG-based grammatical resources
from treebanks for English (Cahill and al., 2002),
(Cahill and al., 2004), German (Cahill and al.,
2003), Chinese (Burke, 2004), (Guo and al.,
2007), Spanish (O?Donovan, 2004), (Chrupala
and van Genabith, 2006) and French (Schluter
and van Genabith, 2008). Here, we extend the
LFG grammar acquisition approach to Arabic and
the Penn Arabic Treebank (ATB) (Maamouri and
Bies, 2004), adapting and extending the methodol-
ogy of (Cahill and al., 2004) originally developed
for English. Arabic is challenging because of its
morphological richness and syntactic complexity.
Currently 98% of ATB trees (without FRAG and
X) produce a covering and connected f-structure.
We conduct a qualitative evaluation of our annota-
tion against a gold standard and achieve an f-score
of 95%.
1 Introduction
Treebank-based statistical parsers tend to achieve
greater coverage and robustness compared to ap-
proaches using handcrafted grammars. However,
they are criticised for being too shallow to mark
important syntactic and semantic dependencies
needed for meaning-sensitive applications (Ka-
plan, 2004). To treat this deficiency, a number
of researchers have concentrated on enriching
shallow parsers with deep dependency informa-
tion. (Cahill and al., 2002), (Cahill and al., 2004)
outlined an approach which exploits information
encoded in the Penn-II Treebank (PTB) trees to
automatically annotate each node in each tree
with LFG f-structure equations representing deep
predicate-argument structure relations. From this
LFG annotated treebank, large-scale unification
grammar resources were automatically extracted
and used in parsing (Cahill and al., 2008) and
generation (Cahill and van Genabith, 2006).
This approach was subsequently extended to
other languages including German (Cahill and
al., 2003), Chinese (Burke, 2004), (Guo and al.,
2007), Spanish (O?Donovan, 2004), (Chrupala
and van Genabith, 2006) and French (Schluter
and van Genabith, 2008).
Arabic is a semitic language and is well-known
for its morphological richness and syntactic
complexity. In this paper we describe the porting
of the LFG annotation methodology to Arabic in
order to induce LFG f-structures from the Penn
Arabic Treebank (ATB) (Bies, 2003), (Maamouri
and Bies, 2004). We evaluate both the coverage
and quality of the automatic f-structure annotation
of the ATB. Ultimately, our goal is to use the f-
structure annotated ATB to derive wide-coverage
resources for parsing and generating unrestricted
Arabic text. In this paper we concentrate on the
annotation algorithm.
The paper first provides a brief overview of
Lexical Functional Grammar, and the Penn
Arabic Treebank (ATB). The next section presents
the architecture of the f-structure annotation
algorithm for the acquisition of f-structures from
the Arabic treebank. The last section provides
an evaluation of the quality and coverage of the
annotation algorithm.
1.1 Lexical Functional Grammar
Lexical-Functional Grammar (LFG) (Kaplan and
Bresnan, 1982); (Bresnan, 2001), (Falk, 2001)
2001, (Sells, 1985) is a constraint-based theory
of grammar. LFG rejects concepts of configura-
tionality and movement familiar from generative
grammar, and provides a non-derivational alterna-
tive of parallel structures of which phrase structure
trees are only one component.
LFG involves two basic, parallel forms of
45
knowledge representation: c(onstituent)-structure,
which is represented by (f-structure annotated)
phrase structure trees; and f(unctional)-structure,
represented by a matrix of attribute-value pairs.
While c-structure accounts for language-specific
lexical idiosyncrasies, syntactic surface config-
urations and word order variations, f-structure
provides a more abstract level of representation
(grammatical functions/ labeled dependencies),
abstracting from some cross-linguistic syntactic
differences. Languages may differ typologically
as regards surface structural representations, but
may still encode similar syntactic functions (such
as, subject, object, adjunct, etc.). For a recent
overview on LFG-based analyses of Arabic see
(Attia, 2008) who presents a hand-crafted Arabic
LFG parser using the XLE (Xerox Linguistics En-
vironment).
1.2 The Penn Arabic Treebank (ATB)
The Penn Arabic Treebank project started in
2001 with the aim of describing written Modern
Standard Arabic newswire. The Treebank consists
of 23611 sentences (Bies, 2003), (Maamouri and
Bies, 2004) .
Arabic is a subject pro-drop language: a null
category (pro) is allowed in the subject position
of a finite clause if the agreement features on
the verb are rich enough to enable content to be
recovered (Baptista, 1995), (Chomsky, 1981).
This is represented in the ATB annotation by an
empty node after the verb marked with a -SBJ
functional tag. The ATB annotation, following
the Penn-II Treebank, utilises the concept of
empty nodes and traces to mark long distance
dependencies, as in relative clauses and questions.
The default word order in Arabic is VSO. When
the subject precedes the verb (SVO), the con-
struction is considered as topicalized. Modern
Standard Arabic also allows VOS word order
under certain conditions, e.g. when the object is
a pronoun. The ATB annotation scheme involves
24 basic POS-tags (497 different tags with mor-
phological information ), 22 phrasal tags, and 20
individual functional tags (52 different combined
tags).
The relatively free word order of Arabic means
that phrase structural position is not an indicator
of grammatical function, a feature of English
which was heavily exploited in the automatic LFG
annotation of the Penn-II Treebank (Cahill and
al., 2002). Instead, in the ATB functional tags are
used to mark the subject as well as the object.
The syntactic annotation style of the ATB follows,
as much as possible, the methodologies and
bracketing guidelines already used for the English
Penn-II Treebank. For example, in the Penn
English Treebank (PTB) (Marcus, 1994), small
clauses are considered sentences composed of
a subject and a predicate, without traces for an
omitted verb or any sort of control relationship, as
in example (1) for the sentence ?I consider Kris a
fool?.
(1) (S (NP-SBJ I)
(VP consider
(S (NP-SBJ Kris)
(NP-PRD a fool))))
The team working on the ATB found this
approach very convenient for copula construc-
tions in Arabic, which are mainly verbless
(Maamouri and Bies, 2004). Therefore they used
a similar analysis without assuming a deleted
copula verb or control relationship, as in (2).
(2) (S (NP-SBJ Al-mas>alatu

??

A??
?
@)
(ADJ-PRD basiyTatuN

??J


?fl
.
))

??J


?fl
.

??

A??
?
@
Al-mas>alatu basiyTatuN
the-question simple
?The question is simple.?
2 Architecture of the Arabic Automatic
Annotation Algorithm
The annotation algorithm for Arabic is based on
and substantially revises the methodology used for
English.
For English, f-structure annotation is very much
driven by configurational information: e.g. the
leftmost NP sister of a VP is likely to be a direct
object and hence annotated ? OBJ =?. This infor-
mation is captured in the format of left-right anno-
tation matrices, which specify annotations for left
or right sisters relative to a local head.
By contrast, Arabic is a lot less configurational and
has much richer morphology. In addition, com-
pared to the Penn-II treebank, the ATB features a
larger functional tag set. This is reflected in the de-
sign of the Arabic f-structure annotation algorithm
46
(Figure 1), where left-right annotation matrices
play a much smaller role than for English. The
annotation algorithm recursively traverses trees in
the ATB. It exploits ATB morpho-syntactic fea-
tures, ATB functional tags, and (some) configura-
tional information in the local subtrees.
We first mask (conflate) some of the complex
morphological information available in the pre-
terminal nodes to be able to state generalisations
for some of the annotation components. We then
head-lexicalise ATB trees identifying local heads.
Lexical macros exploit the full morphological an-
notations available in the ATB and map them to
corresponding f-structure equations. We then ex-
ploit ATB functional tags mapping them to SUBJ,
OBJ, OBL, OBJ2, TOPIC and ADJUNCT etc.
grammatical functions. The remaining functions
(COMP, XCOMP, SPEC etc.) as well as some
cases of SUBJ, OBJ, OBL, OBJ2, TOPIC and AD-
JUNCT, which could not be identified by ATB
tags, are treated in terms of left-right context anno-
tation matrices. Coordination is treated in a sepa-
rate component to keep the other components sim-
ple. Catch-all & Clean-Up corrects overgenerali-
sations in the previous modules and uses defaults
for remaining unannotated nodes. Finally, non-
local dependencies are handled by a Traces com-
ponent.
The next sub-sections describe the main modules
of the annotation algorithm.
2.1 Conflation
ATB preterminals are very fine-grained, encod-
ing extensive morpho-syntactic details in addi-
tion to POS information. For example, the word
	
?

?
	
J? sanaqifu ?[we will] stand? is tagged as
(FUT+IV1P+IV+IVSUFF MOOD:I) denoting an
imperfective (I) verb (V) in the future tense (FUT),
and is first person (1) plural (P) with indicative
mood (IVSUFF MOOD:I). In total there are over
460 preterminal types in the treebank. This level
of fine-grainedness is an important issue for the
annotation as we cannot state grammatical func-
tion (dependency) generalizations about heads and
left and right contexts for such a large tag set. To
deal with this problem, for some of the annotation
algorithm components we masked the morpho-
syntactic details in preterminals, thereby conflat-
ing them into more generic POS tags. For exam-
ple, the above-mentioned tag will be conflated as
VERB.
Figure 1: Architecture of the Arabic annotation al-
gorithm
2.2 Lexical Macros
Lexical macros, by contrast, utilise the de-
tailed morpho-syntactic information encoded in
the preterminal nodes of the Penn Arabic Tree-
bank trees and provide the required functional an-
notations accordingly. These tags usually include
information related to person, number, gender,
definiteness, case, tense, aspect, mood, etc.
Table 1 lists common tags for nouns and verbs and
shows the LFG functional annotation assigned to
each tag.
2.3 Functional Tags
In addition to monadic POS categories, the ATB
treebank contains a set of labels (called functional
tags or functional labels) associated with func-
tional information, such as -SBJ for ?subject? and
-OBJ for ?object?. The functional tags module
translates these functional labels into LFG func-
tional equations, e.g. -OBJ is assigned the anno-
tation ?OBJ=?. An f-structure equation look-up
table assigns default f-structure equations to each
functional label in the ATB (Table 2).
A particular treatment is applied for the tag -PRD
(predicate). This functional tag is used with cop-
ula complements, as in (3) and the correspond-
ing c-structure in Figure 2. Copula complements
47
Tag Annotation
Nouns
MASC ? GEND = masc (masculine)
FEM ? GEND = fem (feminine)
SG ? NUM = sg (singular)
DU ? NUM = dual
PL ? NUM = pl (plural)
ACC ? CASE = acc (accusative)
NOM ? CASE = nom (nominative)
GEN ? CASE = gen (genitive)
Verbs
1 ? PERS = 1
2 ? PERS = 2
3 ? PERS = 3
S ? NUM = sg
D ? NUM = dual
P ? NUM = pl
F ? GEND = masc
M ? GEND = fem
Table 1: Morpho-syntactic tags and their functional anno-
tations
Functional Label Annotation
-SBJ (subject) ? SUBJ = ?
-OBJ (object) ? OBJ = ?
-DTV (dative), ? OBJ2 =?
-BNF (Benefactive)
-TPC (topicalized) ? TOPIC=?
-CLR (clearly related) ? OBL =?
-LOC (locative),
-MNR (manner),
-DIR (direction), ??? ADJUNCT
-TMP (temporal),
-ADV (adverbial)
-PRP (purpose),
Table 2: Functional tags used in the ATP Treebank and their
default annotations
correspond to the open complement grammatical
function XCOMP in LFG and the ATB tag -PRD
is associated with the annotation in (4) in order to
produce the f-structure in Figure 3. The resulting
analysis includes a main predicator ?null be? and
specifies the control relationship through a func-
tional equation stating that the main subject is co-
indexed with the subject of the XCOMP.
(3)

?

K


P?Q?
	
?

?
	
KY?? @
Al-hudonapu Daruwriy?apN
the-truce necessary
?The truce is necessary.?
(4) ? PRED = ?null be?
? XCOMP = ?
? SUBJ= ? SUBJ
S
NP-SBJ
N
Alhudonapu
NP-PRD
N
DaruwriyapN
Figure 2: C-structure for example (3)
2
6
6
6
6
6
6
6
6
6
6
6
6
6
6
6
6
6
6
6
6
6
6
6
6
4
PRED ?null be
D
SUBJ , XCOMP
E
?
SUBJ
2
6
6
6
6
6
4
PRED ?Al-hudonapu?
NUM sg
GEND fem
DEF +
CASE nom
3
7
7
7
7
7
5
1
XCOMP
2
6
6
6
6
6
6
6
6
4
PRED ?Daruwriy?apN?
NUM sg
GEND fem
DEF -
CASE nom
SUBJ
h i
1
3
7
7
7
7
7
7
7
7
5
3
7
7
7
7
7
7
7
7
7
7
7
7
7
7
7
7
7
7
7
7
7
7
7
7
5
Figure 3: F-structure for example (3)
2.4 Left-Right Context Rules
The left-right context annotation module is based
on a tripartite division of local subtrees into a left-
hand-side context (LHS) followed by a head (H)
followed by a right-hand-side context (RHS). We
developed our own head finding, or head lexical-
ization, rules based on a variety of heuristics and
manual inspection of the PS rules.
Initially, we extracted 45785 Phrase Structure (PS)
rules from the treebank. The reason for the rela-
tively large number of PS rules is the fine-grained
nature of the tags encoding morphological infor-
mation for pre-terminal nodes. When we conflate
pre-terminals containing morphological informa-
tion to basic POS tags, the set of PS rules is re-
duced to 9731.
Treebanks grammars follow the Zipfian law: for
each category, there is a small number of highly
frequent rules expanding that category, followed
by a large number of rules with a very low fre-
quency. Therefore, for each LHS category we se-
lect the most frequent rules which together give
85% coverage. This results is a reduced set of 339
(most frequent) PS rules. These rules are manu-
ally examined and used to construct left-right LFG
f-structure annotation matrices for the treebank.
The annotation matrices encode information about
48
the left and right context of a rule?s head and state
generalisations about the functional annotation of
constituents to the left and right of the local head.
Consider sentence (5), where an NP is expanded
as NP NP ADJP. The first NP is considered the
head and is given the annotation ?=?. The second
NP and the ADJP are located to the left (Arabic
reading) of the head (LHS). The left-right context
matrix for NP constituents analyses these phrases
as adjuncts and assigns them the annotation ? ? ?
ADJUNCT.
(5)

?

J


??m
.
Proceedings of the 5th Workshop on Important Unresolved Matters, pages 65?72,
Prague, Czech Republic, June 2007. c?2007 Association for Computational Linguistics
Arabic Tokenization System
Mohammed A. Attia
School of Informatics / The University of Manchester, PO Box 
88, Sackville Street, Manchester M60 1QD, UK
mohammed.attia@postgrad.manchester.ac.uk
Abstract
Tokenization is a necessary and non-trivial 
step in natural language processing. In the 
case of Arabic, where a single word can 
comprise up to four independent tokens, 
morphological knowledge needs to be in-
corporated into the tokenizer. In this paper 
we describe a rule-based tokenizer that 
handles tokenization as a full-rounded 
process with a preprocessing stage (white 
space normalizer), and a post-processing 
stage (token filter). We also show how it 
handles multiword expressions, and how 
ambiguity is resolved.
1 Introduction
Tokenization is a non-trivial problem as it is 
?closely related to the morphological analysis? 
(Chanod and Tapanainen 1994). This is even more 
the case with languages with rich and complex 
morphology such as Arabic. The function of a to-
kenizer is to split a running text into tokens, so that 
they can be fed into a morphological transducer or 
POS tagger for further processing. The tokenizer is 
responsible for defining word boundaries, demar-
cating clitics, multiword expressions, abbreviations 
and numbers.
    Clitics are syntactic units that do not have free 
forms but are instead attached to other words. De-
ciding whether a morpheme is an affix or a clitic 
can be confusing. However, we can generally say 
that affixes carry morpho-syntactic features (such 
as tense, person, gender or number), while clitics 
serve syntactic functions (such as negation, defini-
tion, conjunction or preposition) that would other-
wise be served by an independent lexical item. 
Therefore tokenization is a crucial step for a syn-
tactic parser that needs to build a tree from syntac-
tic units. An example of clitics in English is the 
genitive suffix ??s? in the student?s book.
    Arabic clitics, however, are not as easily recog-
nizable. Clitics use the same alphabet as that of 
words with no demarcating mark as the English 
apostrophe, and they can be concatenated one after 
the other. Without sufficient morphological knowl-
edge, it is impossible to detect and mark clitics. In 
this paper we will show different levels of imple-
mentation of the Arabic tokenizer, according to the 
levels of linguistic depth involved.
     Arabic Tokenization has been described in vari-
ous researches and implemented in many solutions 
as it is a required preliminary stage for further 
processing. These solutions include morphological 
analysis (Beesley 2001; Buckwalter 2002), diacri-
tization (Nelken and Shieber 2005), Information 
Retrieval (Larkey and Connell 2002), and POS 
Tagging (Diab et al2004; Habash and Rambow 
2005). None of these projects, however, show how 
multiword expressions are treated, or how ambigu-
ity is filtered out.
     In our research, tokenization is handled in a 
rule-based system as an independent process. We
show how the tokenizer interacts with other trans-
ducers, and how multiword expressions are identi-
fied and delimited. We also show how incorrect 
tokenizations are filtered out, and how undesired 
tokenizations are marked. All tools in this research 
are developed in Finite State Technology (Beesley 
and Karttunen 2003). These tools have been devel-
oped to serve an Arabic Lexical Functional Gram-
mar parser using XLE (Xerox Linguistics Envi-
ronment) platform as part of the ParGram Project
(Butt et al2002).
65
2 Arabic Tokens
     A token is the minimal syntactic unit; it can be a 
word, a part of a word (or a clitic), a multiword 
expression, or a punctuation mark. A tokenizer 
needs to know a list of all word boundaries, such 
as white spaces and punctuation marks, and also 
information about the token boundaries inside 
words when a word is composed of a stem and cli-
tics. Throughout this research full form words, i.e. 
stems with or without clitics, as well as numbers 
will be termed main tokens. All main tokens are 
delimited either by a white space or a punctuation 
mark. Full form words can then be divided into 
sub-tokens, where clitics and stems are separated.
2.1 Main Tokens
     A tokenizer relies mainly on white spaces and 
punctuation marks as delimiters of word bounda-
ries (or main tokens). Additional punctuation 
marks are used in Arabic such as the comma ???, 
question mark ??? and semicolon ???. Numbers are 
also considered as main tokens. A few Arab coun-
tries use the Arabic numerals as in English, while 
most Arab countries use the Hindi numerals such 
as ?2? (2) and ?3? (3). Therefore a list of all 
punctuation marks and number characters must be 
fed to the system to allow it to demarcate main 
tokens in the text.
2.2 Sub-Tokens
     Arabic morphotactics allow words to be pre-
fixed or suffixed with clitics (Attia 2006b). Clitics 
themselves can be concatenated one after the other. 
Furthermore, clitics undergo assimilation with 
word stems and with each other, which makes 
them even harder to handle in any superficial way. 
A verb can comprise up four sub-tokens (a con-
junction, a complementizer, a verb stem and an 
object pronoun) as illustrated by Figure 1.
Figure 1: Possible sub-tokens in Arabic verbs
    Similarly a noun can comprise up to four sub-
tokens. Although Figure 2 shows five sub-tokens 
but we must note that the definite article and the 
genitive pronoun are mutually exclusive. 
Figure 2: Possible sub-tokens in Arabic nouns
     Moreover there are various rules that govern the 
combination of words with affixes and clitics. 
These rules are called grammar-lexis specifications
(Abb?s et al2004; Dichy 2001; Dichy and Fargaly 
2003). An example of these specifications is a rule 
that states that adjectives and proper nouns do not 
combine with possessive pronouns.
3 Development in Finite State Technology
     Finite state technology has successfully been 
used in developing morphologies and text process-
ing tools for many natural languages, including 
Semitic languages. We will explain briefly how 
finite state technology works, then we will proceed 
into showing how different tokenization models 
are implemented.
(1) LEXICON Proclitic
al@U.Def.On@ Root;
Root;
LEXICON Root
kitab Enclitic;
LEXICON Suffix
an Enclitic;
Enclitic;
LEXICON Enclitic
hi@U.Def.Off@ #;
     In a standard finite state system, lexical entries 
along with all possible affixes and clitics are en-
coded in the lexc language which is a right recur-
sive phrase structure grammar (Beesley and Kart-
tunen 2003). A lexc file contains a number of lexi-
cons connected through what is known as ?con-
tinuation classes? which determine the path of 
concatenation. In example (1) above the lexicon 
Proclitic has a lexical form al, which is linked to a 
66
continuation class named Root. This means that the 
forms in Root will be appended to the right of al. 
The lexicon Proclitic also has an empty string, 
which means that Proclitic itself is optional and 
that the path can proceed without it. The bulk of all 
lexical entries are presumably listed under Root in 
the example.
     Sometimes an affix or a clitic requires or for-
bids the existence of another affix or clitic. This is 
what is termed ?long distance dependencies? 
(Beesley and Karttunen 2003). So Flag Diacritics 
are introduced to serve as filters on possible con-
catenations to a stem. As we want to prevent Pro-
clitic and Enclitic from co-occurring, for the defi-
nite article and the possessive pronoun are mutu-
ally excusive, we add a Flag Diacritic to each of 
them with the same feature name ?U.Def?, but 
with different value ?On/Off?, as shown in (1)
above. At the end we have a transducer with a bi-
nary relation between two sets of strings: the lower 
language that contains the surface forms, and the 
upper language that contains the analysis, as shown 
in (2) for the noun ?????? kitaban (two books).
(2) Lower Language: ??????
Upper Language: ????+noun+dual+sg
4 Tokenization Solutions
     There are different levels at which an Arabic 
tokenizer can be developed, depending on the 
depth of the linguistic analysis involved. During 
our work with the Arabic grammar we developed 
three different solutions, or three models, for Ara-
bic tokenization. These models vary greatly in their 
robustness, compliance with the concept of modu-
larity, and the ability to avoid unnecessary ambi-
guities.
     The tokenizer relies on white spaces and punc-
tuation marks to demarcate main tokens. In demar-
cating sub-tokens, however, the tokenizer needs 
more morphological information. This information 
is provided either deterministically by a morpho-
logical transducer, or indeterministically by a to-
ken guesser. Eventually both main tokens and sub-
tokens are marked by the same token boundary, 
which is the sign ?@? throughout this paper. The 
classification into main and sub-tokens is a con-
ceptual idea that helps in assigning the task of 
identification to different components.
     Identifying main tokens is considered a straight-
forward process that looks for white spaces and 
punctuation marks and divides the text accord-
ingly. No further details of main tokens are given 
beyond this point. The three models described be-
low are different ways to identify and divide sub-
tokens, or clitics and stems within a full form 
word.
4.1 Model 1: Tokenization Combined with 
Morphological Analysis 
     In this implementation the tokenizer and the 
morphological analyzer are one and the same. A 
single transducer provides both morphological 
analysis and tokenization. Examples of the token-
izer/analyser output are shown in (3). The ?+? sign 
precedes morphological features, while the ?@?
sign indicates token boundaries.
(3) ?????? (waliyashkur: and to thank)
?+conj@?+comp@+verb+pres+sg???@
     This sort of implementation is the most linguis-
tically motivated. This is also the most common 
form of implementation for Arabic tokenization 
(Habash and Rambow 2005).  However, it violates 
the design concept of modularity which requires 
systems to have separate modules for undertaking 
separate tasks. For a syntactic parser that requires
the existence of a tokenizer besides a morphologi-
cal analyzer, this implementation is not workable, 
and either Model 2 or Model 3 is used instead.
4.2 Model 2: Tokenization Guesser
    In this model tokenization is separated from 
morphological analysis. The tokenizer only detects 
and demarcates clitic boundaries. Yet information 
on what may constitute a clitic is still needed. This 
is why two additional components are required: a 
clitics guesser to be integrated with the tokenizer, 
and a clitics transducer to be integrated with the 
morphological transducer.
    Clitics Guesser. We developed a guesser for 
Arabic words with all possible clitics and all possi-
ble assimilations. Please refer to (Beesley and 
Karttunen 2003) on how to create a basic guesser. 
The core idea of a guesser is to assume that a stem 
is composed of any arbitrary sequence of Arabic 
alphabets, and this stem can be prefixed or/and 
suffixed with a limited set of tokens. This guesser 
is then used by the tokenizer to mark clitic bounda-
67
ries. Due to the nondeterministic nature of a 
guesser, there will be increased tokenization ambi-
guities. 
(4) ?????? (and to the man)
???@??@?@?@
?????@?@?@
?????@?@
??????@
     Clitics Transducer. We must note that Arabic 
clitics do not occur individually in natural texts. 
They are always attached to words. Therefore a 
specialized small-scale morphological transducer is 
needed to handle these newly separated forms. We 
developed a lexc transducer for clitics only, treat-
ing them as separate words. The purpose of this 
transducer is to provide analysis for morphemes 
that do not occur independently.
(5) ?+conj
?+prep
??+art+def
     This small-scale specialized transducer is then 
unioned (or integrated) with the main morphologi-
cal transducer. Before making the union it is nec-
essary to remove all paths that contain any clitics 
in the main morphological transducer to eliminate 
redundancies. 
     In our opinion this is the best model, the advan-
tages are robustness as it is able to deal with any 
words whether they are known to the morphologi-
cal transducer or not, and abiding by the concept of 
modularity as it separates the process of tokeniza-
tion from morphological analysis.
     There are disadvantages, however, for this 
model, and among them is that the morphological 
analyzer and the syntactic parser have to deal with 
increased tokenization ambiguities. The tokenizer 
is highly non-deterministic as it depends on a 
guesser which, by definition, is non-deterministic. 
For a simple sentence of three words, we are faced 
with eight different tokenization solutions. None-
theless, this can be handled as explained in subsec-
tion 5.1 on discarding spurious ambiguities.
4.3 Model 3: Tokenization Dependent on the 
Morphological Analyser
     In the above solution, the tokenizer defines the 
possible Arabic stem as any arbitrary sequence of 
Arabic letters. In this solution, however, the word 
stem is not guessed, but taken as a list of actual 
words. A possible word in the tokenizer in this 
model is any word found in the morphological 
transducer. The morphological transducer here is 
the same as the one described in subsection 4.1 but 
with one difference, that is the output does not in-
clude any morphological features, but only token 
boundaries between clitics and stems.
     This is a relatively deterministic tokenizer that 
handles clitics properly. The main downfall is that 
only words found in the morphological transducer 
are tokenized. It is not robust, yet it may be more 
convenient during grammar debugging, as it pro-
vides much fewer analyses than model 2. Here 
spurious ambiguities are successfully avoided. 
(6) ?????? (and to the man)
???@??@?@?@
     One advantage of this implementation is that 
the tool becomes more deterministic and more 
manageable in debugging. Its lack of robustness, 
however, makes it mostly inapplicable as no single 
morphological transducer can claim to comprise all 
the words in a language. In our XLE grammar, this 
model is only 0.05% faster than Model 2. This is 
not statistically significant advantage compared to 
its limitations.
4.4 Tokenizing Multiword Expressions
     Multiword Expressions (MWEs) are two or 
more words that behave like a single word syntac-
tically and semantically. They are defined, more 
formally, as ?idiosyncratic interpretations that 
cross word boundaries? (Sag et al2001). MWEs 
cover expressions that are traditionally classified as 
idioms (e.g. down the drain), prepositional verbs 
(e.g. rely on), verbs with particles (e.g. give up), 
compound nouns (e.g. traffic lights) and colloca-
tions (e.g. do a favour).
     With regard to syntactic and morphological
flexibility, MWEs are classified into three types: 
fixed, semi-fixed and syntactically flexible expres-
sions (Baldwin 2004; Oflazer et al2004; Sag et al
2001).
     a. Fixed Expressions. These expressions are 
lexically, syntactically and morphologically rigid. 
An expression of this type is considered as a word 
with spaces (a single word that happens to contain 
68
spaces), such as ??????? ???? al-sharq al-awsat (the 
Middle East) and ??? ??? bait lahem (Bethlehem).
     b. Semi-Fixed Expressions. These expressions 
can undergo variations, but still the components of 
the expression are adjacent. The variations are of 
two types, morphological variations where lexical 
items can express person, number, tense, gender, 
etc., such as the examples in (7), and lexical varia-
tions, where one word can be replaced by another
as in (8).
(7.a) ???? ????????
fatratah intiqaliyyah
translational.sg.fem period.sg.fem
(7.b) ?????? ??????????
fatratan intiqaliyyatan
translational.dual.fem period.dual.fem
(8) ???????/??? ?????/??? ???
ala zahr/wajh al-ard/al-basitah 
on the face/surface of the land/earth 
(on the face of the earth)
     c. Syntactically Flexible Expressions. These 
are the expressions that can either undergo reorder-
ing, such as passivization (e.g. the cat was let out 
of the bag), or allow external elements to intervene 
between the components such as (9.b), where the 
adjacency of the MWE is disrupted.
(9.a) ????? ?????
      darrajah nariyyah
      bike fiery (motorbike)
(9.b) ????? ????? ???????
     darrajat al-walad al-nariyyah
     the-bike the-boy the-fiery (the boy?s motorbike)
     Fixed and semi-fixed expressions are identified 
and marked by the tokenizer, while syntactically 
flexible expressions can only be handled by a syn-
tactic parser (Attia 2006a).
     The tokenizer is responsible for treating MWEs 
in a special way. They should be marked as single 
tokens with the inner space(s) preserved. For this 
purpose, as well as for the purpose of morphologi-
cal analysis, a specialized transducer is developed 
for MWEs that lists all variations of MWEs and 
provides analyses for them (Attia 2006a).
     One way to allow the tokenizer to handle 
MWEs is to embed the MWEs in the Tokenizer 
(Beesley and Karttunen 2003). Yet a better ap-
proach, described by (Karttunen et al1996), is to 
develop one or several multiword transducers or 
?staplers? that are composed on the tokenizer. We 
will explain here how this is implemented in our 
solution, where the list of MWEs is extracted from 
the MWE transducer and composed on the token-
izer. Let?s look at the composition regular expres-
sion:
(10) 1    singleTokens.i 
     2    .o. ?* 0:"[[[" (MweTokens.l) 0:"]]]" ?* 
       3    .o. "@" -> " " || "[[[" [Alphabet* | "@"*]  _ 
       4    .o. "[[[" -> [] .o. "]]]" -> []].i;
Single words separated by the ?@? sign are defined 
in the variable singleTokens and the MWE trans-
ducer is defined in MweTokens. In the MWE 
transducer all spaces in the lower language are re-
placed by ?@? so that the lower language can be 
matched against singleTokens. In line 1 the single-
Tokens is inverted (the upper language is shifted 
down) by the operator ?.i? so that composition 
goes on the side that contains the relevant strings. 
From the MWE transducer we take only the lower 
language (or the surface form) by the operator ?.l? 
in line 2. Single words are searched and if they 
contain any MWEs, the expressions will (option-
ally) be enclosed by three brackets on either side. 
Line 3 replaces all ?@? signs with spaces in side 
MWEs only. The two compositions in line 4 re-
move the intermediary brackets.
     Let?s now show this with a working example. 
For the phrase in (11), the tokenizer first gives the 
output in (12). Then after the MWEs are composed 
with the tokenizer, we obtain the result in (13) with 
the MWE identified as a single token.
(11) ?????? ????????
     wa-liwazir kharijiyatiha
     and-to-foreign minister-its 
     (and to its foreign minister)
(12) ??@??????@????@?@?@   
     (approx. and@to@foreign@minister@its@)
(13)  ??@???? ??????@?@?   
    (approx. and@to@foreign minister@its@)
4.5 Normalizing White Spaces
     White space normalization is a preliminary 
stage to tokenization where redundant and mis-
placed white spaces are corrected, to enable the 
tokenizer to work on a clean and predictable text.
69
In real-life data spaces may not be as regularly and 
consistently used as expected. There may be two or 
more spaces, or even tabs, instead of a single 
space. Spaces might even be added before or after 
punctuation marks in the wrong manner. There-
fore, there is a need for a tool that eliminates in-
consistency in using white spaces, so that when the
text is fed into a tokenizer or morphological ana-
lyzer, words and expressions can be correctly iden-
tified and analyzed. Table 1 shows where spaces 
are not expected before or after some punctuation 
marks.
No Space Before No Space After
) (
} {
] [
? ?
Table 1. Space distribution with some punctuation 
marks
     We have developed a white space normalizer 
whose function is to go through real-life texts and
correct mistakes related to the placement of white 
spaces. When it is fed an input such as the one in 
(14.a) in which additional spaces are inserted and 
some spaces are misplaced, it corrects the errors 
and gives the output in (14.b):
(14.a)  ??????     ????? ??? )??????????? ( ???.
(14.b)  ????? ??? ??????)???????????(??? .
5 Resolving Ambiguity
     There are different types of ambiguity. There 
are spurious ambiguities created by the guesser. 
There are also ambiguities which do not exist in 
the text before tokenization but are only created 
during the tokenization process. Finally there are 
real ambiguities, where a form can be read as a 
single word or two sub-tokens, or where an MWE 
has a compositional reading. These three types are 
treated by the following three subsections respec-
tively.
5.1 Discarding Spurious Ambiguities
Tokenization Model 2 discussed above in subsec-
tion 4.2 is chosen as the optimal implementation 
due to its efficiency and robustness, yet it is highly 
nondeterministic and produces a large number of 
spurious ambiguities. Therefore, a morphological 
transducer is needed to filter out the tokenization 
paths that contain incorrect sub-tokens. Recall ex-
ample (4) which contained the output of the nonde-
terministic tokenizer. In (15) below, after the out-
put is fed into a morphological transducer, only 
one solution is accepted and the rest are discarded, 
as underlined words do not constitute valid stems.
(15) ?????? (and to the man)
???@??@?@?@ - Passed. 
@?@?@????? - Discarded. 
@?@????? - Discarded. 
@?????? - Discarded. 
5.2 Handling Tokenization Ambiguities
     Among the function of a tokenizer is separate 
clitics from stems. Some clitics, however, when 
separated, become ambiguous with other clitics 
and also with other free forms. For example the 
word ?????? kitabahum has only one morphological 
reading (meaning their book), but after tokeniza-
tion ??@????  there are three different readings, as 
the second token ?? can either be a clitic genitive 
pronoun (the intended reading) or a free pronoun 
they (a book, they) or a noun meaning worry
(forming the compound book of worry). 
     This problem is solved by inserting a mark that 
precedes enclitics and follows proclitics to distin-
guish them from each other as well as from free 
forms (Ron M. Kaplan and Martin Forst, personal 
communications, Oxford, UK, 20 September 
2006). The mark we choose is the Arabic elonga-
tion short line called cashida which is originally 
used for graphical decorative purposes and looks 
natural with most clitics. To illustrate the usage, a 
two-word string (16.a) will be rendered without 
cashidas as in (16.b), and a single-word string that 
contains clitics (17.a) will be rendered with a dis-
tinctive cashida before the enclitic pronoun as in 
(17.b). This indicates that the pronoun is attached 
to the preceding word and not standing alone.
(16.a) ???? ??
       kitab hum/hamm (book of worry/a book, they)
(16.b) ??@ ????
(17.a) ?????? kitabuhum (their book)
(17.b) ???@????
70
     This implementation will also resolve a similar 
ambiguity, that is ambiguity arising between pro-
clitics and enclitics. The proclitic preposition ? ka 
(as) always occurs initially. There is a homo-
graphic enclitic object pronoun ? ka (you) that 
always occurs in the final position. This can create 
ambiguity in instances such as the made-up sen-
tence in (18.a). The sentence has the initial tokeni-
zation of (18.b) without a cashida, and therefore 
the central token becomes ambiguous as it can now 
be attached either to the preceding or following 
word leading either to the readings in (18.a) or 
(18.c). The cashida placement, however, resolves 
this ambiguity as in (18.d). The cashida is added 
after the token, indicating that it is attached to the 
following word and now only the reading in (18.a) 
is possible.
(18.a) ????? ???????
a?taitu ka-lamir (I gave like a prince)
(18.b) ??????@?@?????
(18.c) ?????? ??????
a?taitu-ka alamir (I gave you the prince)
(18.d) ??????@??@?????
5.3 Handling Real Ambiguities
     Some tokenization readings are legal, yet highly 
infrequent and undesired in real-life data. These 
undesired readings create onerous ambiguities, as 
they are confused with more common and more 
acceptable forms. For example the Arabic preposi-
tion ??? ba?d (after) has the possible remote reading 
of being split into two tokens ??@?? , which is made 
of two elements: ?? bi (with) and ?? ?add (counting). 
Similarly ??? baina (between) has the possible re-
mote reading ??@?? , which is made of two tokens 
as well: ?? bi (with) and ?? yin (Yen). 
     The same problem occurs with MWEs. The op-
timal handling of MWEs is to treat them as single 
tokens and leave internal spaces intact. Yet a non-
deterministic tokenizer allows MWEs to be ana-
lysed compositionally as individual words. So the 
MWE ??? ?????? hazr al-tajawwul (curfew) has 
two analyses, as in (19), although the composi-
tional reading in (19.b) is undesired.
(19.a) ??? ??????@  hazr al-tajawwul (curfew)
(19.b) ??????@???
hazr (forbidding) al-tajawwul (walking)
     The solution to this problem is to mark the un-
desired readings. This is implemented by develop-
ing a filter, or a finite state transducer that contains 
all possible undesired tokenization possibilities and 
attaches the ?+undesired? tag to each one of them. 
     Undesired tokens, such as ??@??  and ??@?? ,  
explained above, can be included in a custom list 
in the token filter. As for MWEs, the token filter 
imports a list from the MWE transducer and re-
places the spaces with the token delimiter ?@? to 
denote the undesired tokenization solutions. The 
token filter then matches the lists against the output 
of the tokenizer. If the output contains a matching 
string a mark is added, giving the output in (20). 
Notice how (20.b) is marked with the ?+undesired? 
tag.
(20.a) ??? ??????@ [hazr al-tajawwul (curfew)]
(20.b) ??????@??? +undesired
     This transducer or filter is composed on top of 
the core tokenizer. The overall design of the token-
izer and its interaction with other finite state com-
ponents is shown in Figure 3. WE must note that 
the tokenizer, in its interaction with the morpho-
logical transducer and the MWE transducer, does 
not seek morpho-syntactic information, but it que-
ries for lists and possible combinations.
Figure 3: Design of the Arabic Tokenizer
6 Conclusion
Tokenization is a process that is closely connected 
to and dependent on morphological analysis. In our 
research we show how different models of tokeni-
zation are implemented at different levels of lin-
guistic depth. We also explain how the tokenizer 
71
interacts with other components1, and how it re-
solves complexity and filters ambiguity. By apply-
ing token filters we gain control over the tokeniza-
tion output.
References
Abb?s R, Dichy J, Hassoun M (2004): The Architecture 
of a Standard Arabic lexical database: some figures, 
ratios and categories from the DIINAR.1 source pro-
gram, The Workshop on Computational Approaches 
to Arabic Script-based Languages, COLING 2004. 
Geneva, Switzerland.
Attia M (2006a): Accommodating Multiword Expres-
sions in an Arabic LFG Grammar. In Salakoski T, 
Ginter F, Pyysalo S, Pahikkala T (eds), Advances in 
Natural Language Processing, 5th International Con-
ference on NLP, FinTAL 2006, Turku, Finland, Vol 
4139. Turku, Finland: Springer-Verlag Berlin Hei-
delberg, pp 87-98.
Attia M (2006b): An Ambiguity-Controlled Morpho-
logical Analyzer for Modern Standard Arabic Model-
ling Finite State Networks, The Challenge of Arabic 
for NLP/MT Conference. The British Computer So-
ciety, London, UK.
Baldwin T (2004): Multiword Expressions, an Ad-
vanced Course, The Australasian Language Technol-
ogy Summer School (ALTSS 2004). Sydney, Austra-
lia.
Beesley KR (2001): Finite-State Morphological Analy-
sis and Generation of Arabic at Xerox Research: 
Status and Plans in 2001, Proceedings of the Arabic 
Language Processing: Status and Prospect--39th An-
nual Meeting of the Association for Computational 
Linguistics. Toulouse, France.
Beesley KR, Karttunen L (2003): Finite State Morphol-
ogy. Stanford, Calif.: CSLI.
Buckwalter T (2002): Buckwalter Arabic Morphologi-
cal Analyzer Version 1.0., Linguistic Data Consor-
tium. Catalog number LDC2002L49, and ISBN 1-
58563-257-0.
Butt M, Dyvik H, King TH, Masuichi H, Rohrer C 
(2002): The Parallel Grammar Project, COLING-
2002 Workshop on Grammar Engineering and 
Evaluation. Taipei, Taiwan.
                                                
1 The tokenizer along with a number of other Arabic 
finite state tools are made available for evaluation on 
the website: www.attiapace.com
Chanod J-P, Tapanainen P (1994): A Non-Deterministic 
Tokenizer for Finite-State Parsing, ECAI'96. Buda-
pest, Hungary.
Diab M, Hacioglu K, Jurafsky D (2004): Automatic 
Tagging of Arabic Text: From Raw Text to Base 
Phrase Chunks, Proceedings of NAACL-HLT 2004. 
Boston.
Dichy J (2001): On lemmatization in Arabic. A formal 
definition of the Arabic entries of multilingual lexical 
databases, ACL 39th Annual Meeting. Workshop on 
Arabic Language Processing; Status and Prospect. 
Toulouse, pp 23-30.
Dichy J, Fargaly A (2003): Roots & Patterns vs. Stems 
plus Grammar-Lexis Specifications: on what basis 
should a multilingual lexical database centred on 
Arabic be built?, Proceedings of the MT-Summit IX 
workshop on Machine Translation for Semitic Lan-
guages. New-Orleans.
Habash N, Rambow O (2005): Arabic Tokenization, 
Part-of-Speech Tagging and Morphological Disam-
biguation in One Fell Swoop, Proceedings of ACL 
2005. Michigan.
Karttunen L, Chanod J-P, Grefenstette G, Schiller A 
(1996): Regular expressions for language engineer-
ing. Natural Language Engineering 2:305-328.
Larkey LS, Connell ME (2002): Arabic Information 
Retrieval at UMass. In Voorhees EM, Harman DK 
(eds), The Tenth Text Retrieval Conference, TREC 
2001. Maryland: NIST Special Publication, pp 562-
570.
Nelken R, Shieber SM (2005): Arabic Diacritization 
Using Weighted Finite-State Transducers, Proceed-
ings of the 2005 ACL Workshop on Computational 
Approaches to Semitic Languages. Michigan.
Oflazer K, Uglu ??, Say B (2004): Integrating Mor-
phology with Multi-word Expression Processing in 
Turkish, Second ACL Workshop on Multiword Ex-
pressions: Integrating Processing. Spain, pp 64-71.
Sag IA, Baldwin T, Bond F, Copestake A, Flickinger D 
(2001): Multi-word Expressions: A Pain in the Neck 
for NLP, LinGO Working Papers. Stanford Univer-
sity, CA.
72
Proceedings of the NAACL HLT 2010 First Workshop on Statistical Parsing of Morphologically-Rich Languages, pages 67?75,
Los Angeles, California, June 2010. c?2010 Association for Computational Linguistics
Handling Unknown Words in Statistical Latent-Variable Parsing Models for
Arabic, English and French
Mohammed Attia, Jennifer Foster, Deirdre Hogan, Joseph Le Roux, Lamia Tounsi,
Josef van Genabith?
National Centre for Language Technology
School of Computing, Dublin City University
{mattia,jfoster,dhogan,jleroux,ltounsi,josef}@computing.dcu.ie
Abstract
This paper presents a study of the impact
of using simple and complex morphological
clues to improve the classification of rare and
unknown words for parsing. We compare
this approach to a language-independent tech-
nique often used in parsers which is based
solely on word frequencies. This study is ap-
plied to three languages that exhibit different
levels of morphological expressiveness: Ara-
bic, French and English. We integrate infor-
mation about Arabic affixes and morphotac-
tics into a PCFG-LA parser and obtain state-
of-the-art accuracy. We also show that these
morphological clues can be learnt automati-
cally from an annotated corpus.
1 Introduction
For a parser to do a reasonable job of analysing free
text, it must have a strategy for assigning part-of-
speech tags to words which are not in its lexicon.
This problem, also known as the problem of un-
known words, has received relatively little attention
in the vast literature on Wall-Street-Journal (WSJ)
statistical parsing. This is likely due to the fact that
the proportion of unknown words in the standard
English test set, Section 23 of the WSJ section of
Penn Treebank, is quite small. The problem mani-
fests itself when the text to be analysed comes from
a different domain to the text upon which the parser
has been trained, when the treebank upon which the
parser has been trained is limited in size and when
?Author names are listed in alphabetical order. For further
correspondence, contact L. Tounsi, D. Hogan or J. Foster.
the language to be parsed is heavily inflected. We
concentrate on the latter case, and examine the prob-
lem of unknown words for two languages which lie
on opposite ends of the spectrum of morphologi-
cal expressiveness and for one language which lies
somewhere in between: Arabic, English and French.
In our experiments we use a Berkeley-style latent-
variable PCFG parser and we contrast two tech-
niques for handling unknown words within the gen-
erative parsing model: one in which no language-
specific information is employed and one in which
morphological clues (or signatures) are exploited.
We find that the improvement accrued from look-
ing at a word?s morphology is greater for Arabic
and French than for English. The morphological
clues we use for English are taken directly from the
Berkeley parser (Petrov et al, 2006) and those for
French from recent work on French statistical pars-
ing with the Berkeley parser (Crabbe? and Candito,
2008; Candito et al, 2009). For Arabic, we present
our own set of heuristics to extract these signatures
and demonstrate a statistically significant improve-
ment of 3.25% over the baseline model which does
not employ morphological information.
We next try to establish to what extent these clues
can be learnt automatically by extracting affixes
from the words in the training data and ranking these
using information gain. We show that this automatic
method performs quite well for all three languages.
The paper is organised as follows: In Section 2
we describe latent variable PCFG parsing models.
This is followed in Section 3 by a description of our
three datasets, including statistics on the extent of
the unknown word problem in each. In Section 4, we
67
present results on applying a version of the parser
which uses a simple, language-agnostic, unknown-
word handling technique to our three languages. In
Section 5, we show how this technique is extended
to include morphological information and present
parsing results for English and French. In Section 6,
we describe the Arabic morphological system and
explain how we used heuristic rules to cluster words
into word-classes or signatures. We present parsing
results for the version of the parser which uses this
information. In Section 7, we describe our attempts
to automatically determine the signatures for a lan-
guage and present parsing results for the three lan-
guages. Finally, in Section 8, we discuss how this
work might be fruitfully extended.
2 Latent Variable PCFG Parsing
Johnson (1998) showed that refining treebank cate-
gories with parent information leads to more accu-
rate grammars. This was followed by a collection of
linguistically motivated propositions for manual or
semi-automatic modifications of categories in tree-
banks (Klein and Manning, 2003). In PCFG-LAs,
first introduced by Matsuzaki et al (2005), the re-
fined categories are learnt from the treebank us-
ing unsupervised techniques. Each base category
? and this includes part-of-speech tags ? is aug-
mented with an annotation that refines its distribu-
tional properties.
Following Petrov et al (2006) latent annotations
and probabilities for the associated rules are learnt
incrementally following an iterative process consist-
ing of the repetition of three steps.
1. Split each annotation of each symbol into n
(usually 2) new annotations and create rules
with the new annotated symbols. Estimate1 the
probabilities of the newly created rules.
2. Evaluate the impact of the newly created anno-
tations and discard the least useful ones. Re-
estimate probabilities with the new set of anno-
tations.
3. Smooth the probabilities to prevent overfitting.
We use our own parser which trains a PCFG-LA us-
ing the above procedure and parses using the max-
1Estimation of the parameters is performed by running Ex-
pectation/Maximisation on the training corpus.
rule parsing algorithm (Petrov et al, 2006; Petrov
and Klein, 2007). PCFG-LA parsing is relatively
language-independent but has been shown to be very
effective on several languages (Petrov, 2009). For
our experiments, we set the number of iterations to
be 5 and we test on sentences less than or equal to
40 words in length. All our experiments, apart from
the final one, are carried out on the development sets
of our three languages.
3 The Datasets
Arabic We use the the Penn Arabic Treebank
(ATB) (Bies and Maamouri, 2003; Maamouri and
Bies., 2004). The ATB describes written Modern
Standard Arabic newswire and follows the style and
guidelines of the English Penn-II treebank. We use
the part-of-speech tagset defined by Bikel and Bies
(Bikel, 2004). We employ the usual treebank split
(80% training, 10% development and 10% test).
English We use the Wall Street Journal section of
the Penn-II Treebank (Marcus et al, 1994). We train
our parser on sections 2-21 and use section 22 con-
catenated with section 24 as our development set.
Final testing is carried out on Section 23.
French We use the French Treebank (Abeille? et
al., 2003) and divide it into 80% for training, 10%
for development and 10% for final results. We fol-
low the methodology defined by Crabbe? and Can-
dito (2008): compound words are merged and the
tagset consists of base categories augmented with
morphological information in some cases2.
Table 1 gives basic unknown word statistics for
our three datasets. We calculate the proportion of
words in our development sets which are unknown
or rare (specified by the cutoff value) in the corre-
sponding training set. To control for training set
size, we also provide statistics when the English
training set is reduced to the size of the Arabic and
French training sets and when the Arabic training set
is reduced to the size of the French training set. In an
ideal world where training set sizes are the same for
all languages, the problem of unknown words will
be greatest for Arabic and smallest for English. It is
2This is called the CC tagset: base categories with verbal
moods and extraction features
68
language cutoff #train #dev #unk %unk language #train #dev #unk %unk
Arabic 0 594,683 70,188 3794 5.40 Reduced English 597,999 72,970 2627 3.60
- 1 - - 6023 8.58 (Arabic Size) - - 3849 5.27
- 5 - - 11,347 16.17 - - - 6700 9.18
- 10 - - 15,035 21.42 - - - 9083 12.45
English 0 950,028 72,970 2062 2.83 Reduced Arabic 266,132 70,188 7027 10.01
- 1 - - 2983 4.09 (French Size) - - 10,208 14.54
- 5 - - 5306 7.27 - - - 16,977 24.19
- 10 - - 7230 9.91 - - - 21,434 30.54
French 0 268,842 35,374 2116 5.98 Reduced English 265,464 72,970 4188 5.74
- 1 - - 3136 8.89 (French Size) - - 5894 8.08
- 5 - - 5697 16.11 - - - 10,105 13.85
- 10 - - 7584 21.44 - - - 13,053 17.89
Table 1: Basic Unknown Word Statistics for Arabic, French and English
reasonable to assume that the levels of inflectional
richness have a role to play in these differences.
4 A Simple Lexical Probability Model
The simplest method for handling unknown words
within a generative probabilistic parsing/tagging
model is to reserve a proportion of the lexical rule
probability mass for such cases. This is done by
mapping rare words in the training data to a spe-
cial UNKNOWN terminal symbol and estimating rule
probabilities in the usual way. We illustrate the pro-
cess with the toy unannotated PCFG in Figures 1
and 2. The lexical rules in Fig. 1 are the original
rules and the ones in Fig. 2 are the result of apply-
ing the rare-word-to-unknown-symbol transforma-
tion. Given the input sentence The shares recovered,
the word recovered is mapped to the UNKNOWN to-
ken and the three edges corresponding to the rules
NNS ? UNKNOWN, V BD ? UNKNOWN and
JJ ? UNKNOWN are added to the chart at this posi-
tion. The disadvantage of this simple approach is ob-
vious: all unknown words are treated equally and the
tag whose probability distribution is most dominated
by rare words in the training will be deemed the
most likely (JJ for this example), regardless of the
characteristics of the individual word. Apart from
its ease of implementation, its main advantage is its
language-independence - it can be used off-the-shelf
for any language for which a PCFG is available.3
One parameter along which the simple lexical
3Our simple lexical model is equivalent to the Berkeley sim-
pleLexicon option.
probability model can vary is the threshold used to
decide whether a word in the training data is rare or
?unknown?. When the threshold is set to n, a word
in the training data is considered to be unknown if it
occurs n or fewer times. We experiment with three
thresholds: 1, 5 and 10. The result of this experi-
ment for our three languages is shown in Table 2.
The general trend we see in Table 2 is that the
number of training set words considered to be un-
known should be minimized. For all three lan-
guages, the worst performing grammar is the one
obtained when the threshold is increased to 10. This
result is not unexpected. With this simple lexical
probability model, there is a trade-off between ob-
taining good guesses for words which do not occur
in the training data and obtaining reliable statistics
for words which do. The greater the proportion of
the probability mass that we reserve for the unknown
word section of the grammar, the more performance
suffers on the known yet rare words since these are
the words which are mapped to the UNKNOWN sym-
bol. For example, assume the word restructuring oc-
curs 10 times in the training data, always tagged as
a VBG. If the unknown threshold is less than ten and
if the word occurs in the sentence to be parsed, a
VBG edge will be added to the chart at this word?s
position with the probability 10/#VBG. If, however,
the threshold is set to 10, the word (in the training set
and the input sentence) will be mapped to UNKNOWN
and more possibilities will be explored (an edge for
each TAG ? UNKNOWN rule in the grammar). We
can see from Table 1 that at threshold 10, one fifth
69
VBD -> fell 50/153
VBD -> reoriented 2/153
VBD -> went 100/153
VBD -> latched 1/153
NNS -> photofinishers 1/201
NNS -> shares 200/201
JJ -> financial 20/24
JJ -> centrist 4/24
DT -> the 170/170
Figure 1: The original toy PCFG
VBD -> fell 50/153
VBD -> UNKNOWN 3/153
VBD -> went 100/153
NNS -> UNKNOWN 1/201
NNS -> shares 200/201
JJ -> financial 20/24
JJ -> UNKNOWN 4/24
DT -> the 170/170
Figure 2: Rare ? UNKNOWN
VBD -> fell 50/153
VBD -> UNK-ed 3/153
VBD -> went 100/153
NNS -> UNK-s 1/201
NNS -> shares 200/201
JJ -> financial 20/24
JJ -> UNK-ist 4/24
DT -> the 170/170
Figure 3: Rare ? UN-
KNOWN+SIGNATURE
Unknown Threshold Recall Precision F-Score Tagging Accuracy
Arabic
1 78.60 80.49 79.53 94.03
5 77.17 79.81 78.47 91.16
10 75.32 78.69 76.97 89.06
English
1 89.20 89.73 89.47 95.60
5 88.91 89.74 89.33 94.66
10 88.00 88.97 88.48 93.61
French
1 83.60 84.17 83.88 94.90
5 82.31 83.10 82.70 92.99
10 80.87 82.05 81.45 91.56
Table 2: Varying the Unknown Threshold with the Simple Lexical Probability Model
of the words in the Arabic and French development
sets are unknown, and this is reflected in the drop in
parsing performance at these thresholds.
5 Making use of Morphology
Unknown words are not all the same. We exploit this
fact by examining the effect on parsing accuracy of
clustering rare training set words using cues from
the word?s morphological structure. Affixes have
been shown to be useful in part-of-speech tagging
(Schmid, 1994; Tseng et al, 2005) and have been
used in the Charniak (Charniak, 2000), Stanford
(Klein and Manning, 2003) and Berkeley (Petrov et
al., 2006) parsers. In this section, we contrast the
effect on parsing accuracy of making use of such in-
formation for our three languages of interest.
Returning to our toy English example in Figures 1
and 2, and given the input sentence The shares re-
covered, we would like to use the fact that the un-
known word recovered ends with the past tense
suffix -ed to boost the probability of the lexical
rule V BD ? UNKNOWN. If we specialise the
UNKNOWN terminal using information from English
morphology, we can do just that, resulting in the
grammar in Figure 3. Now the word recovered is
mapped to the symbol UNK-ed and the only edge
which is added to the chart at this position is the one
corresponding to the rule V BD ? UNK-ed.
For our English experiments we use the unknown
word classes (or signatures) which are used in the
Berkeley parser. A signature indicates whether a
words contains a digit or a hyphen, if a word starts
with a capital letter or ends with one of the following
English suffixes (both derivational and inflectional):
-s, -ed, -ing, -ion, -er, -est, -ly, -ity, -y and -al.
For our French experiments we employ the same
signature list as Crabbe? and Candito (2008), which
itself was adapted from Arun and Keller (2005).
This list consists of (a) conjugation suffixes of regu-
70
lar verbs for common tenses (eg. -ons, -ez, -ent. . . )
and (b) derivational suffixes for nouns, adverbs and
adjectives (eg. -tion, -ment, -able. . . ).
The result of employing signature information
for French and English is shown in Table 3. Be-
side each f-score the absolute improvement over the
UNKNOWN baseline (Table 2) is given. For both
languages there is an improvement at all unknown
thresholds. The improvement for English is statis-
tically significant at unknown thresholds 1 and 10.4
The improvement is more marked for French and is
statistically significant at all levels.
In the next section, we experiment with signature
lists for Arabic.5
6 Arabic Signatures
In order to use morphological clues for Arabic we
go further than just looking at suffixes. We exploit
all the richness of the morphology of this language
which can be expressed through morphotactics.
6.1 Handling Arabic Morphotactics
Morphotactics refers to the way morphemes com-
bine together to form words (Beesley, 1998; Beesley
and Karttunen, 2003). Generally speaking, morpho-
tactics can be concatenative, with morphemes either
prefixed or suffixed to stems, or non-concatenative,
with stems undergoing internal alternations to con-
vey morphosyntactic information. Arabic is consid-
ered a typical example of a language that employs
non-concatenative morphotactics.
Arabic words are traditionally classified into three
types: verbs, nouns and particles. Adjectives take
almost all the morphological forms of, and share the
same templatic structures with, nouns. Adjectives,
for example, can be definite, and are inflected for
case, number and gender.
There are a number of indicators that tell us
whether the word is a verb or a noun. Among
4Statistical significance was determined using the strati-
fied shuffling method. The software used to perform the test
was downloaded from http://www.cis.upenn.edu/
?
dbikel/software.html.
5An inspection of the Berkeley Arabic grammar (available
at http://code.google.com/p/berkeleyparser/
downloads/list) shows that no Arabic-specific signatures
were employed. The Stanford parser uses 9 signatures for Ara-
bic, designed for use with unvocalised text. An immediate fu-
ture goal is to test this signature list with our parser.
these indicators are prefixes, suffixes and word tem-
plates. A template (Beesley and Karttunen, 2003) is
a kind of vocalization mould in which a word fits. In
derivational morphology Arabic words are formed
through the amalgamation of two tiers, namely, root
and template. A root is a sequence of three (rarely
two or four) consonants which are called radicals,
and the template is a pattern of vowels, or a com-
bination of consonants and vowels, with slots into
which the radicals of the root are inserted.
For the purpose of detection we use the reverse
of this information. Given that we have a word, we
try to extract the stem, by removing prefixes and suf-
fixes, and match the word against a number of verbal
and nominal templates. We found that most Ara-
bic templatic structures are in complementary dis-
tribution, i.e. they are either restricted to nominal
or verbal usage, and with simple regular expression
matching we can decide whether a word form is a
noun or a verb.
6.2 Noun Indicators
In order to detect that a word form is a noun (or ad-
jective), we employ heuristic rules related to Arabic
prefixes/suffixes and if none of these rules apply we
attempt to match the word against templatic struc-
tures. Using this methodology, we are able to detect
95% of ATB nouns.6
We define a list of 42 noun templates which are
used to indicate active/passive participle nouns, ver-
bal nouns, nouns of instrument and broken plural
nouns (see Table 4 for some examples). Note that
templates ending with taa marboutah ?ap? or start-
ing with meem madmoumah ?mu? are not consid-
ered since they are covered by our suffix/prefix rules,
which are as follows:
1- The definite article prefix ?  or in Buckwalter
transliteration ?Al?.
2- The tanween suffix

, 

,

 or ?N?, ?F?, ?K?, ?AF?.
3- The feminine plural suffix HA, or ?+At?.
4- The taa marboutah ending ? or ?ap? whether as a
6The heuristics we developed are designed to work on dia-
critized texts. Although diacritics are generally ignored in mod-
ern writing, the issue of restoring diacritics has been satisfac-
torily addressed by different researchers. For example, Nelken
and Shieber (2005) presented an algorithm for restoring diacrit-
ics to undiacritized MSA texts with an accuracy of over 90%
and Habasah et al (2009) reported on a freely-available toolkit
(MADA-TOKAN) an accuracy of over 96%.
71
Unknown Threshold Recall Precision F-Score Tagging Accuracy
Arabic
1 80.67 82.19 *81.42 (+ 1.89) 96.32
5 80.66 82.81 *81.72 (+ 3.25) 95.15
10 79.86 82.49 *81.15 (+ 4.18) 94.38
English
1 ***89.64 89.95 89.79 (+ 0.32) 96.44
5 89.16 89.80 89.48 (+ 0.15) 96.32
10 89.14 89.78 **89.46 (+ 0.98) 96.21
French
1 85.15 85.77 *85.46 (+ 1.58) 96.13
5 84.08 84.80 *84.44 (+ 1.74) 95.54
10 84.21 84.78 *84.49 (+ 3.04) 94.68
Table 3: Baseline Signatures for Arabic, French and English
statistically significant with *:p < 10?4, **: p < 10?3, ***: p < 0.004,
Template Name Regular Specification
Arabic Buckwalter Expression
?A

?
	
?

	
K 

{inofiEAl {ino.i.A. verbal noun (masdar)
?A

?
	
??

mifoEAl mi.o.A. noun instrument
??

	
?


J?

? musotafoEil musota.o.i. noun participle
?J


?

A

	
?

? mafAEiyl ma.A.iy. noun plural
?

?
	
?


J?

{isotafoEal {isota.o.a. verb
??

?

	
? fuwEil .uw.i. verb passive
Table 4: Sample Arabic Templatic Structures for Nouns and Verbs
feminine marker suffix or part of the word.
5- The genitive case marking kasrah 

, or ?+i?.
6- Words of length of at least five characters ending
with doubled yaa ?


or ?y??.
7- Words of length of at least six characters ending
with alif mamdoudah and hamzah Z  or ?A??.
8- Words of length of at least seven characters start-
ing with meem madmoumah ? or ?mu?.
6.3 Verb Indicators
In the same way, we define a list of 16 templates and
we combine them with heuristic rules related to Ara-
bic prefixes/suffixes to detect whether a word form
is exclusively a verb. The prefix/suffix heuristics are
as follows:
9-The plural marker suffix  ? or ?uwA? indicates a
verb.
10- The prefixes H , ?


,
	
?
,



,

? or ?sa?, ?>a?,
?>u?, ?na?, ?nu?, ?ya?, ?yu?, ?ta?, ?tu? indicate im-
prefective verb.
The verbal templates are less in number than the
noun templates yet they are no less effective in de-
tecting the word class (see Table 4 for examples).
Using these heuristics we are able to detect 85% of
ATB verbs.
6.4 Arabic Signatures
We map the 72 noun/verb classes that are identi-
fied using our hand-crafted heuristics into sets of
signatures of varying sizes: 4, 6, 14, 21, 25, 28
and 72. The very coarse-grained set considers just
4 signatures UNK-noun, UNK-verb, UNK-num,
and UNK and the most fine-grained set of 72 signa-
tures associates one signature per heuristic. In ad-
dition, we have evaluated the effect of reordering
rules and templates and also the effect of collating
all signatures satisfying an unknown word. The re-
sults of using these various signatures sets in parsing
72
UNK
NUM NOUN VERB
digits (see section 6.2) (see section 6.3)
Al definiteness tashkil At suffix ap suffix imperfect
rule 1 rules 2 and 5 rule 3 rule 4 rule 10
y? suffix A? suffix mu prefix verbal noun templates suffixes
rule 6 rule 7 rule 8 3 groupings dual/plural suffixes
plural templates participle active templates participle passive templates instrument templates passive templates
4 groupings
other templates verbal templates
5 groupings
Table 6: Arabic signatures
Cutoff 1 5 10
4 80.78 80.71 80.09
6 81.14 81.16 81.06
14 80.88 81.45 81.19
14 reorder 81.39 81.01 80.81
21 81.38 81.55 81.35
21 reorder 81.20 81.13 80.58
21 collect 80.94 80.56 79.63
25 81.18 81.25 81.26
28 81.42 81.72 (+ 3.25) 81.15
72 79.64 78.87 77.58
Table 5: Baseline Signatures for Arabic
our Arabic development set are presented in Table 5.
We achieve our best labeled bracketing f-score using
28 signatures with an unknown threshold of five. In
fact we get an improvement of 3.25% over using no
signatures at all (see Table 2). Table 3 describes in
more detail the scores obtained using the 28 signa-
tures present in Table 6. Apart from the set contain-
ing 72 signatures, all of the baseline signature sets in
Table 5 yield a statistically significant improvement
over the generic UNKNOWN results (p < 10?4).
7 Using Information Gain to Determine
Signatures
It is clear that dividing the UNKNOWN terminal into
more fine-grained categories based on morpholog-
ical information helps parsing for our three lan-
guages. In this section we explore whether useful
morphological clues can be learnt automatically. If
they can, it means that a latent-variable PCFG parser
can be adapted to any language without knowledge
of the language in question since the only language-
specific component in such a parser is the unknown-
signature specification.
In a nutshell, we extract affix features from train-
ing set words7 and then use information gain to rank
these features in terms of their predictive power in a
POS-tagging task. The features deemed most dis-
criminative are then used as signatures, replacing
our baseline signatures described in Sections 5 and
6. We are not going as far as actual POS-tagging,
but rather seeing whether the affixes that make good
features for a part-of-speech tagger also make good
unknown word signatures.
We experiment with English and French suffixes
of length 1-3 and Arabic prefixes and suffixes of var-
ious lengths as well as stem prefixes and suffixes of
length 2, 4 and 6. For each of our languages we
experiment with several information gain thresholds
on our development sets and we fix on an English
signature list containing 24 suffixes, a French list
containing 48 suffixes and an Arabic list containing
38 prefixes and suffixes.
Our development set results are presented in Ta-
ble 7. For all three languages, the information gain
signatures perform at a comparable level to the base-
line hand-crafted signatures (Table 3). For each
of the three unknown-word handling techniques, no
signature (UNKNOWN), hand-crafted signatures and
information gain signatures, we select the best un-
known threshold for each language?s development
set and apply these grammars to our test sets. The
f-scores are presented in Table 8, along with the up-
per bounds obtained by parsing with these grammars
in gold-tag mode. For French, the effect of tagging
accuracy on overall parse accuracy is striking. The
improvements that we get from using morphological
signatures are greatest for Arabic8 and smallest for
7We omit all function words and high frequency words be-
cause we are interested in the behaviour of words which are
likely to be similar to rare words.
8Bikel?s parser trained on the same Arabic data and tested
on the same input achieves an f-score of 76.50%. We trained
a 5-split-merge-iteration Berkeley grammar and parsed with the
73
Unknown Threshold Recall Precision F-Score Tagging Accuracy
Arabic IG
1 80.10 82.15 *81.11 (+ 1.58) 96.53
5 80.03 82.49 *81.32 (+ 2.85) 95.30
10 80.17 82.40 *81.27 (+ 4.3) 94.66
English IG
1 89.38 89.87 89.63 (+ 0.16) 96.45
5 89.54 90.22 ***89.88 (+ 0.55) 96.41
10 89.22 90.05 *89.63 (+ 1.15) 96.19
French IG
1 84.78 85.36 *85.07 (+ 1.19) 96.17
5 84.63 85.24 **84.93 (+ 2.23) 95.30
10 84.18 84.80 *84.49 (+ 3.09) 94.68
Table 7: Information Gain Signature Results
statistically significant with *:p < 10?4, **: p < 2 ? 10?4, ***: p < 0.005
Language No Sig Baseline Sig IG Sig
Arabic 78.34 *81.59 *81.33
Arabic Gold Tag 81.46 82.43 81.90
English 89.48 89.65 89.77
English Gold Tag 89.94 90.10 90.23
French 83.74 *85.77 **85.55
French Gold Tag 88.82 88.41 88.86
statistically significant with *: p < 10?4, **: p < 10?3
Table 8: F-Scores on Test Sets
English. The results for the information gain signa-
tures are promising and warrant further exploration.
8 Conclusion
We experiment with two unknown-word-handling
techniques in a statistical generative parsing model,
applying them to Arabic, French and English. One
technique is language-agnostic and the other makes
use of some morphological information (signatures)
in assigning part-of-speech tags to unknown words.
The performance differences from the two tech-
niques are smallest for English, the language with
the sparsest morphology of the three and the small-
est proportion of unknown words in its development
set. As a result of carrying out these experiments,
we have developed a list of Arabic signatures which
can be used with any statistical parser which does
Berkeley parser, achieving an f-score of 75.28%. We trained the
Berkeley parser with the -treebank SINGLEFILE option so that
English signatures were not employed.
its own tagging. We also present results which show
that signatures can be learnt automatically.
Our experiments have been carried out using gold
tokens. Tokenisation is an issue particularly for Ara-
bic, but also for French (since the treebank contains
merged compounds) and to a much lesser extent for
English (unedited text with missing apostrophes). It
is important that the experiments in this paper are re-
peated on untokenised text using automatic tokeni-
sation methods (e.g. MADA-TOKAN).
The performance improvements that we demon-
strate for Arabic unknown-word handling are obvi-
ously just the tip of the iceberg in terms of what can
be done to improve performance on a morpholog-
ically rich language. The simple generative lexical
probability model we use can be improved by adopt-
ing a more sophisticated approach in which known
and unknown word counts are combined when esti-
mating lexical rule probabilities for rare words (see
Huang and Harper (2009) and the Berkeley sophis-
ticatedLexicon training option). Further work will
also include making use of a lexical resource exter-
nal to the treebank (Goldberg et al, 2009; Habash,
2008) and investigating clustering techniques to re-
duce data sparseness (Candito and Crabbe?, 2009).
Acknowledgements
This research is funded by Enterprise Ireland
(CFTD/07/229 and PC/09/037) and the Irish Re-
search Council for Science Engineering and Tech-
nology (IRCSET). We thank Marie Candito and our
three reviewers for their very helpful suggestions.
74
References
Anne Abeille?, Lionel Cle?ment, and Franc?ois Toussenel,
2003. Treebanks: Building and Using Parsed
Corpora, chapter Building a Treebank for French.
Kluwer, Dordrecht.
Abhishek Arun and Frank Keller. 2005. Lexicalization
in crosslinguistic probabilistic parsing: The case of
French. In ACL. The Association for Computer Lin-
guistics.
Kenneth R. Beesley and Lauri Karttunen. 2003. Finite
State Morphology. CSLI studies in computational lin-
guistics.
Kenneth R. Beesley. 1998. Arabic morphology using
only finite-state operations. In The Workshop on Com-
putational Approaches to Semitic Languages.
Ann Bies and Mohammed Maamouri. 2003. Penn Ara-
bic Treebank guidelines. Technical Report TB-1-28-
03.
Dan Bikel. 2004. On the Parameter Space of Generative
Lexicalized Parsing Models. Ph.D. thesis, University
of Pennslyvania.
Marie Candito and Benoit Crabbe?. 2009. Improving gen-
erative statistical parsing with semi-supervised word
clustering. In Proceedings of IWPT?09.
Marie Candito, Beno??t Crabbe?, and Djame? Seddah. 2009.
On statistical parsing of French with supervised and
semi-supervised strategies. In Proceedings of the
EACL 2009 Workshop on Computational Linguis-
tic Aspects of Grammatical Inference, pages 49?57,
Athens, Greece, March.
Eugene Charniak. 2000. A maximum-entropy-inspired
parser. In Proceedings of the Annual Meeting of the
North American Association for Computational Lin-
guistics (NAACL-00), pages 132?139, Seattle, Wash-
ington.
Beno??t Crabbe? and Marie Candito. 2008. Expe?riences
d?analyse syntaxique statistique du franc?ais. In Actes
de TALN.
Yoav Goldberg, Reut Tsarfaty, Meni Adler, and Michael
Elhadad. 2009. Enhancing unlexicalized parsing per-
formance using a wide coverage lexicon, fuzzy tag-set
mapping, and EM-HMM-based lexical probabilities.
In EACL, pages 327?335. The Association for Com-
puter Linguistics.
Nizar Habash, Owen Rambow, and Ryan Roth. 2009.
Mada+tokan: A toolkit for Arabic tokenization, di-
acritization, morphological disambiguation, pos tag-
ging, stemming and lemmatization. In Proceedings of
the 2nd International Conference on Arabic Language
Resources and Tools (MEDAR).
Nizar Habash. 2008. Four techniques for online handling
of out-of-vocabulary words in arabic-english statistical
machine translation. In Proceedings of Association for
Computational Linguistics, pages 57?60.
Zhongqiang Huang and Mary Harper. 2009. Self-
training pcfg grammars with latent annotations across
languages. In Proceedings of the 2009 Conference on
Empirical Methods in Natural Language Processing,
Singapore, August.
Mark Johnson. 1998. PCFG models of linguis-
tic tree representations. Computational Linguistics,
24(4):613?632.
Dan Klein and Chris Manning. 2003. Accurate unlex-
icalised parsing. In Proceedings of the 41st Annual
Meeting of the ACL.
Mohammed Maamouri and Ann Bies. 2004. Developing
an Arabic Treebank: Methods, guidelines, procedures,
and tools. In Workshop on Computational Approaches
to Arabic Script-based Languages, COLING.
Mitchell Marcus, Grace Kim, Mary Ann Marcinkiewicz,
Robert MacIntyre, Ann Bies, Mark Ferguson, Karen
Katz, and Britta Schasberger. 1994. The Penn Tree-
bank: Annotating predicate argument structure. In
Proceedings of the 1994 ARPA Speech and Natural
Language Workshop, pages 114?119, Princeton, New
Jersey.
Takuya Matsuzaki, Yusuke Miyao, and Jun?ichi Tsujii.
2005. Probabilistic CFG with latent annotations. In
Proceedings of the 43rd Annual Meeting of the ACL,
pages 75?82, Ann Arbor, June.
Rani Nelken and Stuart M. Shieber. 2005. Arabic dia-
critization using weighted finite-state transducers. In
ACL-05 Workshop on Computational Approaches to
Semitic Languages.
Slav Petrov and Dan Klein. 2007. Improved inference
for unlexicalized parsing. In Proceedings of HLT-
NAACL, Rochester, NY, April.
Slav Petrov, Leon Barrett, Romain Thibaux, and Dan
Klein. 2006. Learning accurate, compact and inter-
pretable tree annotation. In Proceedings of the 21st
International Conference on Computational Linguis-
tics and the 44th Annual Meeting of the ACL, Sydney,
Australia, July.
Slav Petrov. 2009. Coarse-to-Fine Natural Language
Processing. Ph.D. thesis, University of California at
Berkeley, Berkeley, CA, USA.
Helmut Schmid. 1994. Probabilistic part-of-speech tag-
ging using decision trees. In Proceedings of the In-
ternational Conference on New Methods in Language
Processing (NeMLaP-1), pages 44?49.
Huihsin Tseng, Daniel Jurafsky, and Christopher Man-
ning. 2005. Morphological features help POS tagging
of unknown words across language varieties. In Pro-
ceedings of the Fourth SIGHAN Workshop on Chinese
Language Processing.
75
Proceedings of the Multiword Expressions: From Theory to Applications (MWE 2010), pages 19?27,
Beijing, August 2010
Automatic Extraction of Arabic Multiword Expressions
Mohammed Attia, Antonio Toral, Lamia Tounsi, Pavel Pecina and Josef van Genabith
School of Computing, Dublin City University
{mattia,atoral,ltounsi,ppecina,josef}@computing.dcu.ie
Abstract
In this paper we investigate the automatic
acquisition of Arabic Multiword Expres-
sions (MWE). We propose three com-
plementary approaches to extract MWEs
from available data resources. The first
approach relies on the correspondence
asymmetries between Arabic Wikipedia
titles and titles in 21 different languages.
The second approach collects English
MWEs from Princeton WordNet 3.0,
translates the collection into Arabic us-
ing Google Translate, and utilizes differ-
ent search engines to validate the output.
The third uses lexical association mea-
sures to extract MWEs from a large unan-
notated corpus. We experimentally ex-
plore the feasibility of each approach and
measure the quality and coverage of the
output against gold standards.
1 Introduction
A lexicon of multiword expressions (MWEs) has
a significant importance as a linguistic resource
because MWEs cannot usually be analyzed lit-
erally, or word-for-word. In this paper we ap-
ply three approaches to the extraction of Arabic
MWEs from multilingual, bilingual, and monolin-
gual data sources. We rely on linguistic informa-
tion, frequency counts, and statistical measures to
create a refined list of candidates. We validate the
results with manual and automatic testing.
The paper is organized as follows: in this intro-
duction we describe MWEs and provide a sum-
mary of previous related research. Section 2 gives
a brief description of the data sources used. Sec-
tion 3 presents the three approaches used in our
experiments, and each approach is tested and eval-
uated in its relevant sub-section. In Section 4 we
discuss the results of the experiments. Finally, we
conclude in Section 5.
1.1 What Are Multiword Expressions?
Multiword expressions (MWEs) are defined
as idiosyncratic interpretations that cross word
boundaries or spaces (Sag et al, 2002). The exact
meaning of an MWE is not directly obtained from
its component parts. Accommodating MWEs in
NLP applications has been reported to improve
tasks, such as text mining (SanJuan and Ibekwe-
SanJuan, 2006), syntactic parsing (Nivre and Nils-
son, 2004; Attia, 2006), and Machine Translation
(Deksne, 2008).
There are two basic criteria for identifying
MWEs: first, component words exhibit statisti-
cally significant co-occurrence, and second, they
show a certain level of semantic opaqueness or
non-compositionality. Statistically significant co-
occurrence can give a good indication of how
likely a sequence of words is to form an MWE.
This is particularly interesting for statistical tech-
niques which utilize the fact that a large number
of MWEs are composed of words that co-occur to-
gether more often than can be expected by chance.
The compositionality, or decomposabil-
ity (Villavicencio et al 2004), of MWEs is also
a core issue that presents a challenge for NLP ap-
plications because the meaning of the expression
is not directly predicted from the meaning of the
component words. In this respect, composition-
alily varies between phrases that are highly com-
19
positional, such as,       	  
 	   qfla-
?idatun ?askariyyatun, ?military base?, and those
that show a degree of idiomaticity, such as,       

   Proceedings of the 10th International Workshop on Finite State Methods and Natural Language Processing, pages 20?24,
Donostia?San Sebastia?n, July 23?25, 2012. c?2012 Association for Computational Linguistics
Handling Unknown Words in Arabic FST Morphology 
  Khaled Shaalan and Mohammed Attia Faculty of Engineering & IT, The British University in Dubai khaled.shaalan@buid.ac.ae mohammed.attia@buid.ac.ae         Abstract 
A morphological analyser only recognizes words that it already knows in the lexical database. It needs, however, a way of sensing significant changes in the language in the form of newly borrowed or coined words with high frequency. We develop a finite-state morphological guesser in a pipelined methodology for extracting unknown words, lemmatizing them, and giving them a priority weight for inclusion in a lexicon. The processing is performed on a large contemporary corpus of 1,089,111,204 words and passed through a machine-learning-based annotation tool. Our method is tested on a manually-annotated gold standard of 1,310 forms and yields good results despite the complexity of the task. Our work shows the usability of a highly non-deterministic finite state guesser in a practical and complex application. 
1 Introduction Due to the complex and semi-algorithmic nature of the Arabic morphology, it has always been a challenge for computational processing and analysis (Kiraz, 2001; Beesley 2003; Shaalan et al, 2012). A lexicon is an indispensable part of a morphological analyser (Dichy and Farghaly, 2003; Attia, 2006; Buckwalter, 2004; Beesley, 2001), and the coverage of the lexical database is a key factor in the coverage of the morphological analyser. This is why an automatic method for updating a lexical database is crucially important. 
 We present the first attempt, to the best of our knowledge, to address lemmatization of Arabic unknown words. The specific problem with lemmatizing unknown words is that they cannot be matched against a morphological lexicon. We develop a rule-based finite-state morphological guesser and use a machine learning disambiguator, MADA (Roth et al, 2008), in a pipelined approach to lemmatization.   This paper is structured as follows. The remainder of the introduction reviews previous work on Arabic unknown word extraction and lemmatization, and explains the data used in our experiments. Section 2 presents the methodology followed in extracting and analysing unknown words. Section 3 provides details on the morphological guesser we have developed to help deal with the problem. Section 4 shows and discusses the testing and evaluation results, and finally Section 5 gives the conclusion. 1.1 Previous Work Lemmatization of Arabic words has been addressed in (Roth et al, 2008; Dichy, 2001). Lemmatization of unknown words has been addressed for Slovene in (Erjavec and D?erosk, 2004), for Hebrew in (Adler at al., 2008) and for English, Finnish, Swedish and Swahili in (Lind?n, 2008). Lemmatization means the normalization of text data by reducing surface forms to their canonical underlying representations, which, in Arabic, means verbs in their perfective, indicative, 3rd person, masculine, singular forms, such as  ?????? 
20
$akara ?to thank?; and nominals in their nominative, singular, masculine forms, such as ?????? TAlib ?student?; and nominative plural for pluralia tantum nouns (or nouns that appear only in the plural form and are not derived from a singular word), such as ???? nAs ?people?. To the best of our knowledge, the study presented here is the first to address lemmatization of Arabic unknown words. The specific problem with lemmatizing unknown words is that they cannot be matched against a lexicon. In our method, we use a machine learning disambiguator, develop a rule-based finite-state morphological guesser, and combine them in a pipelined process of lemmatization. We test our method against a manually created gold standard of 1,310 types (unique forms) and show a significant improvement over the baseline. Furthermore, we develop an algorithm for weighting and prioritizing new words for inclusion in a lexicon depending on three factors: number of form variations of the lemmas, cumulative frequency of the forms, and POS tags.  1.2 Data Used In our work we rely on a large-scale corpus of 1,089,111,204 words, consisting of 925,461,707 words from the Arabic Gigaword Fourth Edition (Parker et al, 2009), and 163,649,497 words from news articles collected from the Al-Jazeera web site.1 In this corpus, unknown words appear at a rate of between 2% of word tokens (when we ignore possible spelling variants) and 9% of word tokens (when possible spelling variants are included).  2 Methodology To deal with unknown words, or out-of-vocabulary words (OOVs), we use a pipelined approach, which predicts part-of-speech tags and morpho-syntactic features before lemmatization. First, a machine learning, context-sensitive tool is used. This tool, MADA (Roth et al, 2008), performs POS tagging and morpho-syntactic analysis and disambiguation of words in context. MADA internally uses the Standard Arabic Morphological Analyser (SAMA) (Maamouri et al, 2010), an updated version of Buckalter Arabic                                                             1 http://aljazeera.net/portal. Collected in January 2010. 
Morphological  Analyser (BAMA) (Buckwalter, 2004). Second, we develop a finite-state morphological guesser that gives all possible interpretations of a given word. The morphological guesser first takes an Arabic form as a whole and then strips off all possible affixes and clitics one by one until all potential analyses are exhausted. As the morphological guesser is highly non-deterministic, all the interpretations are matched against the morphological analysis of MADA that receives the highest probabilistic scores. The guesser?s analysis that bears the closest resemblance (in terms of morphological features) with the MADA analysis is selected.  These are the steps followed in extracting and lemmatizing Arabic unknown words: ? A corpus of 1,089,111,204 is analysed with MADA. The number of types for which MADA could not find an analysis in SAMA is 2,116,180.  ? These unknown types are spell checked by the Microsoft Arabic spell checker using MS Office 2010. Among the unknown types, the number of types accepted as correctly spelt is 208,188. ? We then select types with frequency of 10 or more. This leave us with 40,277 types. ? We randomly select 1,310 types and manually annotate them with the gold lemma, the gold POS and lexicographic preference for inclusion in a dictionary. ? We use the full POS tags and morpho-syntactic features produced by MADA. ? We use the finite-state morphological guesser to produce all possible morphological inter-pretations and corresponding lemmatizations. ? We compare the POS tags and morpho-syntactic features in MADA output with the output of the morphological guesser and choose the one with the highest matching score.  3 Morphological Guesser We develop a morphological guesser for Arabic that analyses unknown words with all possible clitics, morpho-syntactic affixes and all relevant alteration operations that include insertion, assimilation, and deletion. Beesley and Karttunen 
21
(2003) show how to create a basic guesser. The core idea of a guesser is to assume that a stem is composed of any arbitrary sequence of Arabic non-numeric characters, and this stem can be prefixed and/or suffixed with a predefined set of prefixes, suffixes or clitics. The guesser marks clitic boundaries and tries to return the stem to its underlying representation, the lemma. Due to the nondeterministic nature of the guesser, there will be a large number of possible lemmas for each form.   The XFST finite-state compiler (Beesley and Karttunen, 2003) uses the ?substitute defined? command for creating the guesser. The XFST commands in our guesser are stated as follows.   define PossNounStem [[Alphabet]^{2,24}] "+Guess":0; define PossVerbStem [[Alphabet]^{2,6}] "+Guess":0;  This rule states that a possible noun stem is defined as any sequence of Arabic non-numeric characters of length between 2 and 24 characters.  A possible verb stem is between 2 and 6 characters. The length is the only constraint applied to an Arabic word stem. This word stem is surrounded by prefixes, suffixes, proclitics and enclitics. Clitics are considered as independent tokens and are separated by the ?@? sign, while prefixes and suffixes are considered as morpho-syntactic features and are interpreted with tags preceded by the ?+? sign. Below we present the analysis of the unknown noun  ????? ???????????? wa-Al-musaw~iquwna ?and-the-marketers?.  MADA output: form:wAlmswqwn num:p gen:m per:na case:n asp:na mod:na vox:na pos:noun prc0:Al_det prc1:0 prc2:wa_conj prc3:0 enc0:0 stt:d  Finite-state guesser output: ???????????? +adj??????????+Guess+masc+pl+nom@ ???????????? +adj????????????+Guess+sg@ ???????????? +noun??????????+Guess+masc+pl+nom@ ???????????? +noun????????????+Guess+sg@ ?? ????????????+conj@????+defArt@+adj?????  +Guess+masc+pl+nom@ ?? ????????????+conj@????+defArt@+adj??????? 
 +Guess+sg@ ?? ????????????+conj@????+defArt@+noun?????  +Guess+masc+pl+nom@ ?? ????????????+conj@????+defArt@+noun???????  +Guess+sg@ ?? ????????????+conj@+adj????????+Guess+masc  +pl+nom@ ?? ????????????+conj@+adj??????????+Guess+sg@ ?? ????????????+conj@+noun????????+Guess+masc  +pl+nom@ ?? ????????????+conj@+noun??????????+Guess+sg@  For a list of 40,277 word types, the morphological guesser gives an average of 12.6 possible interpretations per word. This is highly non-deterministic when compared to AraComLex morphological analyser (Attia et al 2011) which has an average of 2.1 solutions per word. We also note that 97% of the gold lemmas are found among the finite-state guesser's choices.  4 Testing and Evaluation To evaluate our methodology we create a manually annotated gold standard test suite of randomly selected surface form types. For these surface forms, the gold lemma and part of speech are manually given. Besides, the human annotator gives a preference on whether or not to include the entry in a dictionary. This feature helps to evaluate our lemma weighting equation. The annotator tends to include nouns, verbs and adjectives, and only proper nouns that have a high frequency. The size of the test suite is 1,310.   4.1 Evaluating Lemmatization In the evaluation experiment we measure accuracy calculated as the number of correct tags divided by the count of all tags. The baseline is given by the assumption that new words appear in their base form, i.e., we do not need to lemmatize them. The baseline accuracy is 45% as shown in Table 1. The POS tagging baseline proposes the most frequent tag (proper name) for all unknown words. In our test data this stands at 45%. We notice that MADA POS tagging accuracy is unexpectedly low (60%). We use Voted POS Tagging, that is when a lemma gets a different POS tag with a higher frequency, the new tag replaces the old low frequency tag. 
22
This method has improved the tagging results significantly (69%).    Accuracy  POS tagging 1 POS Tagging baseline 45% 2 MADA POS tagging 60% 3 Voted POS Tagging 69% Table 1. Evaluation of POS tagging  As for the lemmatization process itself, we notice that our experiment in the pipelined lemmatization approach gains a higher (54%) score than the baseline (45%) as shown in Table 2. This score significantly rises to 63% when the difference in the definite article ?Al? is ignored. The testing results indicate significant improvements over the baseline.   Lemmatization 1 Lemmas found among corpus forms 64% 2 Lemmas found among FST guesser forms 97% 3 Lemma first-order baseline 45% 4 Pipelined lemmatization (first-order decision) with strict definite article matching 54% 5 Pipelined lemmatization  (first-order decision) ignoring definite article matching 63% Table 2. Evaluation of lemmatization  4.2 Evaluating Lemma Weighting In our data we have 40,277 unknown token types. After lemmatization they are reduced to 18,399 types (that is 54% reduction of the surface forms) which are presumably ready for manual validation before being included in a lexicon. This number is still too big for manual inspection. In order to facilitate human revision, we devise a weighting algorithm for ranking so that the top n number of words will include the most lexicographically relevant words. We call surface forms that share the same lemma ?sister forms?, and we call the lemma that they share the ?mother lemma?. This weighting algorithm is based on three criteria: frequency of the sister forms, number of sister forms, and a POS factor which penalizes proper nouns (due to their disproportionate high frequency). The parameters of the weighting 
algorithm has been tuned through several rounds of experimentation.  Word Weight = ((number of sister forms having the same mother lemma * 800) + cumulative sum of frequencies of sister forms having the same mother lemma) / 2 + POS factor  Good words In top 100 In bottom 100 relying on Frequency alone (baseline) 63 50 relying on number of sister forms * 800 87 28 relying on POS factor 58 30 using the combined criteria 78 15 Table 3. Evaluation of lemma weighting and ranking  Table 3 shows the evaluation of the weighting  criteria. We notice that the combined criteria gives the best balance between increasing the number of good words in the top 100 words and reducing the number of good words in the bottom 100 words.  5 Conclusion We develop a methodology for automatically extracting unknown words in Arabic and lemmatizing them in order to relate multiple surface forms to their base underlying representation using a finite-state guesser and a machine learning tool for disambiguation. We develop a weighting mechanism for simulating a human decision on whether or not to include the new words in a general-domain lexical database. We show the feasibility of a highly non-deterministic finite state guesser in an essential and practical application.  Out of a word list of 40,255 unknown words, we create a lexicon of 18,399 lemmatized, POS-tagged and weighted entries. We make our unknown word lexicon available as a free open-source resource2. Acknowledgments This research is funded by the UAE National Research Foundation (NRF) (Grant No. 0514/2011).                                                             2 http://arabic-unknowns.sourceforge.net/ 
23
References  Adler, M., Goldberg, Y., Gabay, D. and Elhadad, M. 2008. Unsupervised Lexicon-Based Resolution of Unknown Words for Full Morpholological Analysis. In: Proceedings of Association for Computational Linguistics (ACL), Columbus, Ohio. Attia, M. 2006. An Ambiguity-Controlled Morpho-logical Analyzer for Modern Standard Arabic Modelling Finite State Networks. In: Challenges of Arabic for NLP/MT Conference, The British Computer Society, London, UK. Attia, Mohammed, Pavel Pecina, Lamia Tounsi, Antonio Toral, Josef van Genabith. 2011. An Open-Source Finite State Morphological Transducer for Modern Standard Arabic. International Workshop on Finite State Methods and Natural Language Processing (FSMNLP). Blois, France. Beesley, K. R. 2001. Finite-State Morphological Analysis and Generation of Arabic at Xerox Research: Status and Plans in 2001. In: The ACL 2001 Workshop on Arabic Language Processing: Status and Prospects, Toulouse, France. Beesley, K. R., and Karttunen, L.. 2003. Finite State Morphology: CSLI studies in computational linguistics. Stanford, Calif.: Csli. Buckwalter, T. 2004. Buckwalter Arabic Morphological Analyzer (BAMA) Version 2.0. Linguistic Data Consortium (LDC) catalogue number LDC2004L02, ISBN1-58563-324-0 Dichy, J. 2001. On lemmatization in Arabic, A formal definition of the Arabic entries of multilingual lexical databases. ACL/EACL 2001 Workshop on Arabic Language Processing: Status and Prospects. Toulouse, France. Dichy, J., and Farghaly, A. 2003. Roots & Patterns vs. Stems plus Grammar-Lexis Specifications: on what basis should a multilingual lexical database centred on Arabic be built? In: The MT-Summit IX workshop on Machine Translation for Semitic Languages, New Orleans. Erjavec, T., and D?erosk, S. 2004. Machine Learning of Morphosyntactic Structure: Lemmatizing Unknown Slovene Words. Applied Artificial Intelligence, 18:17?41. Kiraz, G. A. 2001. Computational Nonlinear Morphology: With Emphasis on Semitic Languages. Cambridge University Press. Lind?n, K. 2008. A Probabilistic Model for Guessing Base Forms of New Words by Analogy. In CICling-2008, 9th International Conference on Intelligent 
Text Processing and Computational Linguistics, Haifa, Israel, pp. 106-116. Maamouri, M., Graff, D., Bouziri, B., Krouna, S., and Kulick, S. 2010. LDC Standard Arabic Morphological Analyzer (SAMA) v. 3.1. LDC Catalog No. LDC2010L01. ISBN: 1-58563-555-3. Parker, R., Graff, D., Chen, K., Kong, J., and Maeda, K. 2009. Arabic Gigaword Fourth Edition. LDC Catalog No. LDC2009T30. ISBN: 1-58563-532-4. Roth, R., Rambow, O., Habash, N., Diab, M., and Rudin, C. 2008. Arabic Morphological Tagging, Diacritization, and Lemmatization Using Lexeme Models and Feature Ranking. In: Proceedings of Association for Computational Linguistics (ACL), Columbus, Ohio. Shaalan, K., Magdy, M., Fahmy, A., Morphological Analysis of Il-formed Arabic Verbs for Second Language Learners, In Eds. McCarthy P., Boonthum, C., Applied Natural Language Processing: Identification, Investigation and Resolution, PP. 383-397, IGI Global, PA, USA, 2012.  
24
Proceedings of the EMNLP 2014 Workshop on Arabic Natural Langauge Processing (ANLP), pages 48?56,
October 25, 2014, Doha, Qatar. c?2014 Association for Computational Linguistics
A Framework for the Classification and Annotation of Multiword  Expressions in Dialectal Arabic 
  Abdelati Hawwari, Mohammed Attia, Mona Diab Department of Computer Science The George Washington University {Abhawwari,mohattia,mtdiab}@gwu.edu 
 
    Abstract In this paper we describe a framework for classifying and annotating Egyptian Ara-bic Multiword Expressions (EMWE) in a specialized computational lexical re-source. The framework intends to en-compass comprehensive linguistic infor-mation for each MWE including: a. pho-nological and orthographic information; b. POS tags; c. structural information for the phrase structure of the expression; d. lexicographic classification; e. semantic classification covering semantic fields and semantic relations; f. degree of idio-maticity where we adopt a three-level rat-ing scale; g. pragmatic information in the form of usage labels; h. Modern Standard Arabic equivalents and English transla-tions, thereby rendering our resource a three-way ? Egyptian Arabic, Modern Standard Arabic and English ? repository for MWEs. 1 Introduction Multiword expressions (MWEs) comprise a wide range of diverse, arbitrary and yet linguistically related phenomena that share the characteristic of crossing word boundaries (Sag et al., 2002). MWEs are computationally challenging because the exact interpretation of an MWE is not direct-ly obtained from its component parts. MWEs are intrinsically single units on the deep conceptual and semantic levels, but on the surface (lexical and syntactic) levels they are expressed as multi-ple units. MWEs vary in their syntactic category, morphological behavior, and degree of semantic opaqueness. MWEs are pervasively present in natural texts, which makes it imperative to tackle them explicitly if we aspire to make large-scale, 
linguistically-motivated, and precise processing of a human language.  Integrating MWEs in NLP applications has evidently and consistently shown to improve the performance in tasks such as Information Re-trieval (Acosta et al. 2011; da Silva and Souza, 2012), Text Mining (SanJuan and Ibekwe-SanJuan, 2006), Syntactic Parsing (Eryi?it et al., 2011; Nivre and Nilsson, 2004; Attia, 2006; Korkontzelos and Manandhar, 2010), Machine Translation (Deksne, 2008; Carpuat and Diab, 2010; Ghoneim and Diab 2013; Bouamor et al., 2011), Question Answering, and Named-Entity extraction (Bu et al., 2011). In the current work, we propose guidelines for detailed linguistic annotation of an MWE lexicon for dialectal (Egyptian) Arabic that covers, among other types, expressions that are tradi-tionally classified as idioms (e.g. ??? ??????? EalaY Alriyq 1  ?on an empty stomach?), prepositional verbs (e.g. ???? ??? tawak~al EalaY ?rely on?), compound nouns (e.g. ???????? ?????? <i$Arap muruwr ?traffic light?), and collocations (e.g. ???? ???? >axad du$~ ?to take a shower?).  Creating a repository of annotated MWEs that is focused on dialects is essential for computa-tional linguistics research as it provides a crucial resource that is conducive to better analysis and understanding of the user-generated content rife in the social media (such as Facebook, Twitter, blogs, and forums). Moreover, it helps in under-standing he correspondences between different languages and their representation of the seman-tic space. We hope that the multilingual data in this repository will lead to a significant en-hancement in the processing of comparable and parallel corpora. We believe that our proposed framework will contribute to the sustainability of                                                 1 In this paper, we use the Buckwalter Transliteration Scheme for rendering Romanized Arabic as described in www.qamus.com. 
48
MWE research in general, and provide a blue print for research on MWEs in dialects, informal vernaculars, as well as morphologically rich lan-guages.  MWE are not only interesting from an NLP perspective but also from a linguistic perspec-tive, as MWE can help in understanding the link between lexicon, syntax and semantics. Until now, this is hampered by the lack of comprehen-sive resources for MWEs with fine-grained clas-sification on different dimensions related to se-mantic roles and syntactic functions. Arabic comprises numerous divergent dialects, and hav-ing an annotated MWE lexical resource in dia-lects and Modern Standard Arabic (MSA) will allow for studying transformation, change and development in this language. From a theoretical linguistic point of view, our work will be interesting particularly in studies related to Diglossia. Diglossia (Walters, 1996) is where two languages or dialects exist side by side within a community, where typically one is used in formal contexts while the other is used in informal communications and interactions. Stud-ying the MWE space for dialects and MSA as a continuum will lead to deeper insights into varia-tions as we note intersection and overlap be-tween the two. In many instances, we see that MSA MWEs and their dialectal equivalents are not necessarily shared as they occupy comple-mentary linguistic spaces. However, the nature of this complementarity and its cultural and social implications will need more exploration and in-vestigation, which will be possible once a com-plete resource becomes available. In the current work, we give detailed descrip-tion of our methodology and guidelines for anno-tating phonological, morphological, syntactic, semantic and pragmatic information of an Egyp-tian Multiword Expressions (EMWE) lexical resource. Our annotation scheme covers the fol-lowing areas. a) Phonological and orthographic information;  b) POS tag, based on the observation of how an MWE functions as a whole lexical unit; c) Syntactic variability and structural composi-tion;  d) Lexicographic types, which includes the classifications followed in the dictionary-writing domain (idioms, support verbs, com-pound nouns, etc.); e) Semantic information, where we cover se-mantic fields and relations; 
f) Idiomaticity Degree; we adopt a three level rating scale (Mel??uk, 1998) to measure the degree of semantic opaqueness; g) Degree of morphological, lexical and syntac-tic flexibility (Sag et al., 2002); h) Pragmatic information, which includes add-ing usage labels to MWEs where applicable; i) Translation, which includes the MSA and English equivalents, either as an MWE in MSA and English if available or as a para-phrase otherwise.   2 Previous Work There are four main areas of research on MWEs: extraction from structured and unstructured data, construction of lexicons for specific languages, integration in NLP applications, and the con-struction of guidelines and best practices. A sig-nificant amount of research has focused on the identification and extraction of MWEs (Ramisch et al., 2010; Dubremetz and Nivre, 2014; Attia et al., 2010; Weller and Heid, 2010; Schneider et al., 2014). Description and specifications of MWE lexical resources have been presented for Japanese (Shudo et al. 2011), Italian (Zaninello and Nissim, 2010), Dutch (Gr?goire, 2010; Odijk, 2013), and Modern Standard Arabic (Hawwari et al., 2012). Moreover, Calzolari et al. (2002) presented a project that attempted to in-troduce best practice recommendations for the treatment of MWE in mono- and multi-lingual computational lexicons that incorporate both syntactic and semantic information, but the limi-tation of their work is that they focus on only two types of MWEs, namely, support verbs and noun compounds. Apart from Schneider et al. (2014), who fo-cused on the language of the social web, none of these projects dealt with informal or dialectal languages, which are rampant in user-generated content (UGC). With the explosion of social me-dia, the language of Web 2.0 is undergoing fun-damental changes: English is no longer dominat-ing the web, and UGC is outpacing professional-ly edited content.  UGC is re-shaping the way people are con-suming and dealing with information, as the user is no longer a passive recipient, but has now turned into an active participant, and in many instances, a source or producer of information. Social media have empowered users to be more creative and interactive, and allowed them to 
49
voice their opinions on events and products and exert powerful influence on the behavior and opinion of others. Yet, the current overflow of UGC poses significant challenges in data gather-ing, annotation and presentation.  3 MWE Taxonomy Although the importance of the MWEs has been acknowledged by many researchers in the field of NLP as evident by the large number of research papers and dedicated workshops in the past decade, the theory of MWEs is still underdeveloped (Sag et al., 2002). There is critical need for studying MWEs both from the theoretical and practical point of views. MWEs have diverse categories, varying degrees of idiomaticity, different syntactic compositions, and different morphological, lexical and syntactic behavior, and dealing with them is complicated even further by the fact that there is no ?watertight criteria? for distinguishing them them (Atkins and Rundell, 2008). Moreover, there is no universally-agreed tax-onomy of MWEs (Ramisch, 2012), and different researchers proposed different typology for this phenomena. Fillmore et al. (1988) proposed three types based on lexical and syntactic familiarity: a) unfamiliar pieces familiarly combined, b) fa-miliar pieces unfamiliarly combined, and c) fa-miliar pieces familiarly combined. Mel'?uk (1989), on the other hand, introduced three different classes: a) complete phraseme, b) semi-phraseme, c) and quasi-phraseme. Sag et al. introduced two classes: institutionalized phrases and lexicalized phrases, with lexicalized phrases subdivided into fixed, semi-fixed and syntactically flexible expressions. Ramisch (2012) introduced yet another set of classes: nominal, verbal and adverbial expressions.  From the lexicographic point of view, the leg-acy three-way division of MWEs proved to be too coarse-grained to cater for the needs of lexi-cographers who need to identify the large array of sub-types that fall under the umbrella of ?MWEs?.  Atkins and Rundell (2008) empha-sized the need for lexicographers to be able to recognize MWE types such as fixed phrases, transparent collocations, similes, catch phrases, proverbs, quotations, greetings, phatic expres-sions, compounds, phrasal verbs, and support verbs. When we look deeply into the different classi-fications, we notice that each approach has 
looked at the phenomenon from a different angle, either focusing on its syntactic regularity, seman-tic and pragmatic properties, meaning composi-tionality, surface flexibility, POS (part of speech) category, or lexicographic relevance. What we propose is that it is not possible to come up with a hard and fast classification that cuts through all levels of representation. All afore-mentioned classifications are valid and can work parallel to each other, instead of substituting for each other. The assumption that we follow in this paper is that MWEs have different classifications at dif-ferent levels of representation from the very deep level of semantics and pragmatics to the very shallow level of morphology and phonetics.      The details of our annotation scheme are ex-plained in the following section. It is worth noting that in our current work, we move the focus away from edited text to the challenging and creative language found in UGC and by trying to close the language resource gap between edited and unedited text. We handle this gap by focusing on dialects, the language used in informal communications such as emails, chat rooms, and in social media in general. We cover the full range of MWEs (nominal, verbal, adver-bial, adjectival and prepositional expressions) in Egyptian Arabic, covering 7,331 MWEs (col-lected from corpora and paper dictionaries).   4 Annotation of Linguistic Features in MWE In this section, we provide a comprehensive specification of MWE types and the detailed lin-guistic information, including the phonological, orthographical, syntactic, semantic and pragmat-ic features.   4.1 Phonological Each MWE is provided in full diacritization to indicate its common pronunciation in Cairene Arabic accent, such as  ?????? ?? ??  ?? ? ?? ?? ?  EalaY kaf~ Eaforiyt ?at high risk?, ?lit. on the palm of a  de-mon?. We also list other phonological variants when available.  4.2 Orthography Since dialects do not have a standard orthogra-phy, we follow the CODA style (Habash et al., 2012), which is a devised standard for conven-tionalizing the orthography of dialectal Arabic. CODA takes canonical forms and etymological 
50
facts into consideration. For example, the Egyp-tian expression ???? ????? >axad bAluh ?to pay atten-tion? is rendered in CODA as ???? ????? >axa* bAluh. 4.3 POS At this level of annotation we consider the POS of the entire MWE when regarded as one unit from a functional perspective. We annotate each MWE with a POS tag from a predefined tagset. We define the POS tag based on the headword POS in the MWE. Our POS tagset includes verb, noun, adjective, adverb, interjection, proper noun, and preposition. The list of POS tags used along with examples is shown in Table 1.   POS Example 1 verb ???? ??? ?????????   jar~ EalaY AlHisAb  ?pay later? 2 noun ?????? ??????????  >akol AlEay$  ?making ends meet? [lit. eating bread] 3 adjective ??????? ??????????  >a$okAl wa->alowAn  ?various shapes and colors? 4 adverb ?????? ??????? >axorip Al-matam~ap ?at the end? 5 interjection ??? ???? ??????????  yA nAs yAhuwh  ?anybody there? 6 proper nouns ??????? ????   $ajarip Aldur~  ?Shajar al-Durr? 7 preposition  ?????? ?????   bi-gaD~ AlnaZar Eano  ?irrespective of? Table 1: MWE Examples with their POS Tags  4.4 Syntactic Annotation A syntactic variable is a slot that intervenes be-tween the component parts of an MWE, without being itself a part of it, but fills a syntactic gap. Syntactic variables are added, when needed, to MWEs to represent the syntactic behavior of an MWE and they exemplify how the MWE inter-acts with other elements within its scope. We create a tagset of syntactic variables reflecting the argument structure of an MWE. Examples are shown in Table 2.  
No Syntactic  Variable Example 1  ?????? somebody (masc_ nominative) 
??? (??????) ??????  jas~ (fulAn) AlnaboD ? (somebody) tested the waters? 2  ?????? somebody (fem_ accusative) 
 ?????) ?????????(????   >akal (fulAnap) bi-Eaynayh ?he devoured (some woman/girl) with his eyes? 3  ???????? people  (genitive) 
???? ???? (???????) ???????  daq~ bayn (Alqawom) <isofiyn ?he drove a wedge between (some people)?  4  ??????? some matter (accusative) ?? (??????) ?? ?????? Hat~ (Al>amora) fiy HisAbihi  ?he took (some matter) into consideration? 5  ??????? something (nominative) 
??????) ????? ??????) (Al$ayo') mitofaS~al Ealayh ?(something) fits him perfectly? Table 2: Syntactic variables and example usages  4.5 Lexicographic Annotation In the dictionary market there are specialized dictionaries for idioms, phrasal verbs, proverbs and quotations. However, general domain dic-tionaries try to avoid the use of too technical terms in the description of MWEs and use for the sake of simplicity a general term like ?phrase? to denote them to users. Yet, in the meta language of the dictionary compiling profession, lexicog-raphers make a more fine-grained distinction be-tween the various types of MWEs. Our lexico-graphic classification of MWEs is adapted from Atkins and Rundell (2008) and includes the fol-lowing tags. Examples are listed in Table 3.  1. Idiom: An idiom is an MWE whose mean-ing is fully or partially unpredictable from the meanings of its components (Nunberg et al., 1994); 2. Support verb, or ?light verbs?, may be defined as semantically empty verbs, which share their arguments with a noun (Meyers et al., 2004); 
51
3. Prepositional verb: These are verbs fol-lowed by prepositions with impact on the meaning; 4. Compound noun: A compound noun is a lexeme that consists of more than one noun; 5. Compound term: This is a technical com-pound noun used in a specific technical field; 6. Compound named entity: This is a multi-word proper name; 7. Phatic expression: an expression that is in-tended for performing a social function (such as greeting or well-wishing) rather than conveying information;  8. Proverb: We consider proverbs as multi-word expression if they are used as lexical units;  9. Quotation: We list only quotations that have gained currency in the language and have become familiar to the majority of the community. 10.  Classification Example 1 Idiom ??? ??????? ?? ?????   biyiEomil min AlHab~ap qub~ap  ?to make a mountain out of a molehill? 2 Support verb ?????? ???? >axad tAr  ?to take revenge? 3 Prepositional verb ??? ?????? DiHik Ealayh  ?to play a joke on? [lit. laugh on him]? 4 Compound noun ???? ???????? >abuw qirodAn  ?Cattle egret? 5 Compound term ???? ?????? Eiroq AlnisA  ?Sciatica? 6 Compound named entity ?????? ???????? >abuw Alhuwl  ?the Sphinx? 7 Phatic expres-sion ?????? ???? ?????  >a$uwf wu$~ak bi-xayr ?see you later? 8 Quotation ??? ?????? ??? ??????  yA mawolAyA kamA xa-laqotiniy ?penniless? 9 Proverb ?????? ??????  AlEaqol ziynap  ?wisdom is a blessing? Table 3: Examples of Lexical Types 4.6 Structural Classification We provide the syntactic phrase structure com-position of the expressions, giving the MWE pat-tern or the POS of its component elements. The purpose is to show the normal productive syntac-
tic patterns underlying the expressions. Table 4 shows the list of possible structural pattern in Egyptian MWEs.   Structure Example 1 adjective + conjunction + adjective 
?????????? ???? ?????   rayiq wa-fayiq  ?happy and relaxed? 
2 adjective + noun ?????? ?????????  tanaboliq Al-sulotAn ?couch potatoes? [lit. Sultan dependents]? 3 noun + noun  ??????? ???  kilomiq Haq~  ?word of truth? 4 adjective + preposition + noun ?????? ???????  garoqAn li-$uw$otuh  ?up to his ears? 5 adverb + noun ???? ???????  bayn nArayn  ?confused? [lit. between two fires] 6 adverb + verb  ????????? ?????????   HasobamA Ait~afaq ?haphazardly? [lit. as happens] 7 noun + adjec-tive ???? ???????  nafoxap kad~Abap  ?false pride/arrogance? [lit. false blow] 8  verb + con-junction + verb 
 ??????????? ???????????  yilit~ wa-yiEojin  ?to babble? [lit. knead and fold] 9  verb + verb  ????????? ??   Aimo$iy Ainojar~  ?get moving/get out? [lit. walk and drag] 10  verb + preposition + noun   
??????? ????? ??????  tawak~al EalaY Allah ?rely on Allah/go away? 11  preposition + noun  ??? ????????? EalaY AlTabo-TAb ?effortlessly? [lit. on ease] 12  verb + noun  ??? ???????  nafa$ riy$uh  ?show pride? [lit. stretched his feathers] 13  noun + verb  ???? ???????!  Allah yiroHamuh  ?Allah have mercy on him? Table 4: Examples Syntactic Classification  4.7 Semantic Fields The entries in the current lexical resource are classified into semantic fields based on their se-mantic contents. The objective is to assign one semantic field tag for each MWE in the lexicon. Organizing Lexical data in semantic field format brings many theoretical and practical benefits, one of those is to allow the current lexical re-source to function both as a lexicon and a thesau-
52
rus. In Table 5 we show a sample of our seman-tic field classification.   Semantic Field Example 1 Social Relation ??? ??? ???  samon EalaY Easal  ?getting on well? [lit. ghee on honey] 2 Oath and Em-phasis ?????? ????????  wa-Allah AlEaZiym  ?I swear by Allah? 3 Occasions ?????? ?? ????  yitrab~aY fiyEiz~ak ?congratulations on the new baby? [lit. may he grow up in your wealth] 4 Death ????? ????????  rab~inA Aifotakaruh ?he died?  [lit. the Lord remem-bered him] 5  wishing and cursing ?? ??? ???????  baEod Al$ar~ ?God forbid? [lit. may the evil be far away] 6 trickery ?????? ??????  lab~isuh AlEim~ap  ?to hoodwink? [lit. put the turban on him] 7 Occultism ???? ??????  Darab Alramol  ?to practice divination? [lit. to strike the sand]? Table 5: Semantic fields  4.8 Semantic Relations Aiming at presenting detailed lexical semantic information, we further classify our entries based on semantic relations like synonymy, antonymy and polysemy.  ? Synonymy: MWE synonyms are grouped together; as the following expressions which all mean ?to practice divination? ???? ????????? qarA AlfinojAn [lit. read the cup], ???? ???????? Darab AlwadaE [lit. hit the shells], ????? ????? qarA Alkaf~ [lit. read the hand palm]. ? Antonymy: MWE antonyms are two MWE having the opposite meaning to each other. For examples, ??????? ????? <iyduh nA$ofap ?ava-ricious? [lit. his hand is dry] is the antonym of ??????? ??????? <iyduh maxoruwmap ?wasteful? [lit. his hand has a hole in it]. 
? Polysemy. This is when an MWE has more than one meaning. For example, ???????? ??????? <iyduh Tawiylap [lit. his hand is long] can mean either a ?powerful person? or a ?thief?.  4.9 Idiomaticity Degree Mel??uk (1998) classified MWEs with regards to idiomaticity into three types: full phrasemes, quasi-phrasemes and semi-phrasemes.  ? Full phrasemes are when the meaning of the expression does not match the meaning of the component words, such as ???????? ???? Wa-halum~ jar~A ?and so on?.  ? Quasi-phrasemes are when the meaning of the expression matches the meaning of the component words in addition to an extra piece of meaning that is not directly derived from either components, such as ???? ?????? majolis Al$aEob ?people's assembly?.  ? Semi-phrasemes are when the meaning of the MWE is partially directly derived from one component and partially indirectly indi-cated by the other component, such as  ??????????????? dirAsAt EuloyA ?higher studies?.  4.10 Morpho-lexico-grammatical flexibility A scale of three levels is used to measure the de-gree of morphological, lexical and grammatical flexibility of a MWE, adopted from Sag et al. (2002). The three levels are as follows: ? Fixed MWE: An MWE is considered as a fixed expression if it does not have any de-gree of syntactic, morphological or lexical flexibility, and its meaning cannot be pre-dicted from its component elements, for ex-ample, ?????? ?????? sadAH madAH ?slapdash?. ? Semi-Fixed MWE: Semi-fixed expressions allow for a certain degree of morphological and lexical variation, but they are fixed in terms of the syntactic word order, for ex-ample, ????????\???????? ??? ?? ????????\??????  mA$oyap/mA$oyiyn EalaY Hal~ $aEo-rahA/$aEoruhum [lit. living by letting down her/their hair] ?whore/whores? or ?loose women?. ? Syntactically flexible MWE: A syntacti-cally flexible MWE is a frequent combina-tion of two words or more, characterized by high degree of morphological and syntactic flexibility. Example, ?????? (?????) ???? <id~aY 
53
(fulAn) du$~ ?to scold someone harshly? [lit. give someone a shower].   4.11 Pragmatic Annotation (Usage Labels) The reason we provide usage labels is inspired by the CALLHOME Egyptian Arabic corpus (Gadalla et al., 1997)), which is a collection of data gathered from spoken colloquial language. The usage labels present specifications on who uses an MWE and how it is used. The usage label tagset in our lexicon includes labels such vulgar, youth, aggressive or taboo, as exemplified in Table 6.  Who or how Example youth  ?????? ??????? ??? ???????  yisuwq Alhabal fiy Aljabal  ?to act foolishly? [lit. to act madly in the mountain]? women / girls ?????????? ????? ???? ?????  Al$ATrap tigozil birijol HumAr ?make do with what you have? [lit. a clever girl will knit with a donkey?s leg]? Aggressive ??????? ?? ???? >ad~iyk fiy wi$~ak  ?I shall slap you in the face? Table 6: Pragmatic annotation  5 Status of the current resource The Egyptian MWE lexical resource at the cur-rent stage contains 7,331 entries, and work is still on going in the linguistic annotation of the dic-tionary. Table 7 presents the current annotation progress statistics regarding the various classifi-cations and features.   Feature Completion 1 Diacritization 34.10% 2 Syntactic Variables 25.92% 3 MSA Equivalent 27.28% 4 POS  34.10% 5 Syntactic Classification 23.58% 6 English Equivalent 27.28% 7 Lexical Type 98.94% 8 Pragmatics Usage 4.09% 9 Synonymous 0.14% 10 Idiomaticity Degree 12.82% 11 Semantic-Field 2.29% Table 7: Annotation work progress 
6 Conclusion We have described the annotation guidelines for a lexical database of MWE for dialectal Arabic. We provide descriptive specifications of MWE at the phonological, orthographical, syntactic and semantic levels. The main contribution of this paper is that it is the first description of a classi-fication and annotation scheme of a lexical data-base for dialects, which can be extended for in-formal languages and with direct applicability on user-generated content. Acknowledgement This work was supported by the Defense Ad-vanced Research Projects Agency (DARPA) Contract No. HR0011-12-C-0014, BOLT pro-gram with subcontract from Raytheon BBN. References Acosta, Otavio Costa, Aline Villavicencio, Viviane P. Moreira. (2011) Identification and Treatment of Multiword Expressions applied to Information Re-trieval. Proceedings of the Workshop on Multi-word Expressions: from Parsing and Generation to the Real World (MWE 2011), pages 101?109, Portland, Oregon, USA, 23 June 2011. Atkins, B. T. S. and M. Rundell. 2008. The Oxford Guide to Practical Lexicography. Oxford University Press. Attia, Mohammed, Antonio Toral, Lamia Tounsi, Pavel Pecina and Josef van Genabith. 2010. Auto-matic Extraction of Arabic Multiword Expressions. COLING 2010 Workshop on Multiword Expres-sions: from Theory to Applications. Beijing, China  Attia, Mohammed. (2006) Accommodating Multi-word Expressions in an Arabic LFG Grammar. In T. Salakoski et al. (Eds.): Advances in Natural Language Processing. FinTAL 2006, Lecture Notes in Computer Science. Vol. 4139, pp. 87 - 98, 2006. Springer-Verlag Berlin Heidelberg. Baldwin, T. (2005a). The deep lexical acquisition of English verb-particles. Computer Speech and Lan-guage, Special Issue on Multiword Expressions 19 (4), 398?414. Baldwin, T. (2005b). Looking for prepositional verbs in corpus data. In Proceedings of the 2nd ACL-SIGSEM Workshop on the Linguistic Dimensions of Prepositions and their Use in Computational Linguistics Formalisms and Applications, Colches-ter, UK, pp. 115?126. Baldwin, Timothy and Su Nam Kim. 2009. Multi-word expressions. In Nitin Indurkhya and Fred J. Damerau, editors, Handbook of Natural Language 
54
Processing, pages 267?292. CRC Press, Boca Ra-ton, USA, 2nd edition. Bannard, C. 2007. A Measure of Syntactic Flexibil-ity for Automatically Identifying Multi Word Ex-pressions in Corpora. Proceedings of A Broader Perspective on Multiword Expressions, Workshop at the ACL 2007 Conference: 1?8. Benson, M. 1990. Collocations and general-purpose dictionaries. International Journal of Lexicogra-phy, 3(1):23--35. Bouamor, Dhouha, Nasredine Semmar and Pierre Zweigenbaum. (2011) Improved Statistical Ma-chine Translation Using MultiWord Expressions. International Workshop on Using Linguistic In-formation for Hybrid Machine Translation LIHMT. Barcelona, November 2011 Bu, Fan, Xiao-Yan Zhu, and Ming Li. (2011) A New Multiword Expression Metric and Its Applications. In Journal of Computer Science and Technology. 26(1): 3-13, Jan. 2011. Calzolari, Nicoletta, Charles J. Fillmore, Ralph Grishman, Nancy Ide, Alessandro Lenci, Catherine MacLeod, Antonio Zampolli. (2002) Towards Best Practice for Multiword Expressions in Computa-tional Lexicons. In Proceedings of the Third Inter-national Conference on Language Resources and Evaluation (LREC 2002), Las Palmas, Canary Is-lands, pp. 1934-1940 Carpuat, Marine and Mona Diab. (2010) Task-based evaluation of multiword expressions: a pilot study in statistical machine translation. Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics. Los Angeles, CA. Pp. 242-245. Chafe, Wallace1968. Idiomaticity as an Anomaly in the Chomskyan Paradigm. Foundations of Lan-guage 4.109-127. da Silva, Edson Marchetti and Renato Rocha Souza. (2012) Information retrieval system using Multi-words Expressions (MWE) as descriptors. JISTEM - Journal of Information Systems and Technology Management. Vol.9 no.2 S?o Paulo May/Aug. 2012. Deksne, Daiga, Raivis Skadi??, and Inguna Skadi?a. a. 2008. Dictionary of Multiword Expressions for Translation into Highly Infliected Languages. In the 6th International Conference on Language Re-sources and Evaluation (LREC 2008), Marrakech, Morocco. Dubremetz, Marie and Joakim Nivre. (2014) Extrac-tion of Nominal Multiword Expressions in French. In proceedings of the 10thWorkshop on Multiword Expressions (MWE 2014), the 14th Conference of the European Chapter of the Association for Com-
putational Linguistics. 26-27 April 2014. Gothen-burg, Sweden Eryi?it, G?l?en, Tugay ?Ilbay Ozan and Arkan Can. (2011) Multiword Expressions in Statistical De-pendency Parsing. Proceedings of the Second Workshop on Statistical Parsing of Morphological-ly Rich Languages. Fillmore, C.J., P. Kay, M. O?Connor. (1988) Regu-larity and idiomaticity in grammatical construc-tions: the case of let alone. Language, 64, 3, 501?538. Gadalla, Hassan, Hanaa Kilany, Howaida Arram, Ashraf Yacoub, Alaa El-Habashi, Amr Shalaby, Krisjanis Karins, Everett Rowson, Robert Mac-Intyre, Paul Kingsbury, David Graff, Cynthia McLemore. (1997) CALLHOME Egyptian Arabic Transcripts. LDC catalog number LDC97T19, ISBN 1-58563-115-9. Ghoneim, Mahmoud and Mona Diab. (2013) Multi-word Expressions in the context of Statistical Ma-chine Translation. In the Proceedings of IJCNLP 2013, October, Nagoya, Japan. Gibbs, R. W. (1980). Spilling the beans on under-standing and memory for idioms. Memory & Cog-nition, 8, 449?456. Gr?goire, Nicole. (2010) DuELME: a Dutch electron-ic lexicon of multiword expressions. In Language Resources and Evaluation, 44(1-2):23-39 (2010) Gross, Maurice, 1986. Lexicon-Grammar. The Repre-sentation of Compound Words. In COLING-1986 Proceedings, Bonn, pp. 1-6. Habash, Nizar, Mona Diab, Owen Rambow (2012). CODA: A Conventional Orthography for Dialectal Arabic. Proceedings of LREC, Istanbul Turkey, May 2012. Hawwari, Abdelati, Kfir Bar, and Mona Diab (2012). Building an Arabic Multiword Expressions Reposi-tory. Proceedings of the NAACL-HLT 2012 Workshop on Computational Linguistics for Lit-erature, Montreal, Canada, June 2012.  Jackendoff, R. (1973). The base rules for preposition-al phrases. In A Festschrift for Morris Halle, pp. 345?356. New York, USA: Rinehart and Winston. Korkontzelos, Ioannis, and Suresh Manandhar. (2010) Can Recognising Multiword Expressions Improve Shallow Parsing? In proceedings of Human Lan-guage Technologies: The 2010 Annual Conference of the North American Chapter of the ACL, pages 636?644, Los Angeles, California, June 2010. Mel??uk, I. (1998) Collocations and Lexical Func-tions. In A.P. Cowie (ed.): Phraseology. Theory, Analysis, and Applications, Oxford: Clarendon Press, 23-53. 
55
Mel??uk, Igor (2004) Verbes supports sans peine. Lingvistic? Investigationes 27: 2, 203-217. Nivre, Joakim and Jens Nilsson. 2004. Multiword Units in Syntactic Parsing. InWorkshop on Meth-odologies and Evaluation of Multiword Units in Real-World Applications, the 4th International Conference on Language Resources and Evaluation (LREC 2004), pp. 39-46. Lisbon, Portugal. Odijk, Jan. (2013) Identification and Lexical Repre-sentation of Multiword Expressions. In P. Spyns and J. Odijk (eds.), Essential Speech and Language Technology for Dutch, Theory and Applications of Natural Language Processing Palmer, Martha, Dan Gildea, Paul Kingsbury. (2005) The Proposition Bank: An Annotated Corpus of Semantic Roles. Computational Linguistics, 31:1., pp. 71-105.  Ramisch, Carlos, Aline Villavicencio, Christian Boi-tet, "mwetoolkit: a Framework for Multiword Ex-pression Identification", Proceedings of the Sev-enth International Conference on Language Re-sources and Evaluation (LREC 2010), Valetta, Malta, May, 2010. Sag, I. A., Baldwin, T., Bond, F., Copestake, A., and Flickinger, D. 2002. Multiword Expressions: A Pain in the Neck for NLP. Proceedings of the 3rd International Conference on Intelligent Text Pro-cessing and Computational Linguistics, CI-CLING2002: 1?15. SanJuan, Eric and Fidelia Ibekwe-SanJuan. 2006. Text mining without document context. In Infor-mation Processing and Management. Volume 42, Issue 6, pp. 1532-1552. Schneider, Nathan, Spencer Onuffer, Nora Kazour, Emily Danchik, Michael T. Mordowanec, Henriet-ta Conrad, and Noah A. Smith. (2014) Comprehen-sive Annotation of Multiword Expressions in a So-cial Web Corpus. In Proceedings of the Language Resources and Evaluation Conference (LREC 2014), Reykjavik, Iceland, May 2014. Shudo, Kosho, Akira Kurahone, and Toshifumi Tana-be. (2011) A Comprehensive Dictionary of Multi-word Expressions. Proceedings of HLT '11 Pro-ceedings of the 49th Annual Meeting of the Asso-ciation for Computational Linguistics: Human Language Technologies, Portland, Oregon. Volume 1, pp. 161-170 Walters, Keith. Diglossia, linguistic variation, and language change in Arabic. 1996. In Eid, Mushira, Perspectives on Arabic Linguistics VIII. John Ben-jamins. 1996 Weller, Marion, Ulrich Heid. (2010) Extraction of German Multiword Expressions from Parsed Cor-pora Using Context Features. In proceedings of the 
seventh international conference on Language Re-sources and Evaluation (LREC), Val-letta, Malta. Zaninello, Andrea, Malvina Nissim. (2010) Creation of lexical resources for a characterisation of multi-word expressions in Italian. In proceedings of the seventh international conference on Language Re-sources and Evaluation (LREC), Valletta, Malta  
56
Proceedings of the EMNLP 2014 Workshop on Arabic Natural Langauge Processing (ANLP), pages 148?154,
October 25, 2014, Doha, Qatar. c?2014 Association for Computational Linguistics
GWU-HASP: Hybrid Arabic Spelling and Punctuation Corrector1 
  Mohammed Attia, Mohamed Al-Badrashiny, Mona Diab Department of Computer Science The George Washington University {Mohattia;badrashiny;mtdiab}@gwu.edu 
 
    Abstract In this paper, we describe our Hybrid Ar-abic Spelling and Punctuation Corrector (HASP). HASP was one of the systems participating in the QALB-2014 Shared Task on Arabic Error Correction. The system uses a CRF (Conditional Random Fields) classifier for correcting punctua-tion errors, an open-source dictionary (or word list) for detecting errors and gener-ating and filtering candidates, an n-gram language model for selecting the best candidates, and a set of deterministic rules for text normalization (such as re-moving diacritics and kashida and con-verting Hindi numbers into Arabic nu-merals). We also experiment with word alignment for spelling correction at the character level and report some prelimi-nary results. 1 Introduction In this paper1 we describe our system for Arabic spelling error detection and correction, Hybrid Arabic Spelling and Punctuation Corrector (HASP). We participate with HASP in the QALB-2014 Shared Task on Arabic Error Cor-rection (Mohit et al., 2014) as part of the Arabic Natural Language Processing Workshop (ANLP) taking place at EMNLP 2014.      The shared task data deals with ?errors? in the general sense which comprise: a) punctuation errors; b) non-word errors; c) real-word spelling errors; d) grammatical errors; and, e) orthograph-ical errors such as elongation (kashida) and speech effects such as character multiplication                                                 1 This work was supported by the Defense Advanced Research Projects Agency (DARPA) Contract No. HR0011-12-C-0014, BOLT program with subcontract from Raytheon BBN. 
for emphasis. HASP in its current stage only handles types (a), (b), and (e) errors. We assume that the various error types are too distinct to be treated with the same computational technique. Therefore, we treat each problem separately, and for each problem we select the approach that seems most efficient, and ultimately all compo-nents are integrated in a single framework.  1.1 Previous Work Detecting spelling errors in typing is one of the earliest NLP applications, and it has been re-searched extensively over the years, particularly for English (Damerau, 1964; Church and Gale, 1991; Kukich, 1992; Brill and Moore, 2000; Van Delden et al., 2004; Golding, 1995; Golding and Roth, 1996; Fossati and Di Eugenio, 2007; Islam in Inkpen, 2009; Han and Baldwin, 2011; Wu et al., 2013).  The problem of Arabic spelling error correc-tion has been investigated in a number of papers (Haddad and Yaseen, 2007; Alfaifi and Atwell, 2012; Hassan et al., 2008; Shaalan et al., 2012; Attia et al., 2012; Alkanhal et al., 2012).  In our research, we address the spelling error detection and correction problem with a focus on non-word errors. Our work is different from pre-vious work on Arabic in that we cover punctua-tion errors as well. Furthermore, we fine-tune a Language Model (LM) disambiguator by adding probability scores for candidates using forward-backward tracking, which yielded better results than the default Viterbi. We also develop a new and more efficient splitting algorithm for merged words. 1.2 Arabic Morphology, Orthography and Punctuation Arabic has a rich and complex morphology as it applies both concatenative and non-concatenative morphotactics (Ratcliffe, 1998; Beesley, 1998; Habash, 2010), yielding a wealth of morphemes that express various morpho-
148
syntactic features, such as tense, person, number, gender, voice and mood.      Arabic has a large array of orthographic varia-tions, leading to what is called ?typographic er-rors? or ?orthographic variations? (Buckwalter, 2004a), and sometimes referred to as sub-standard spellings, or spelling soft errors. These errors are basically related to the possible over-lap between orthographically similar letters in three categories: a) the various shapes of ham-zahs (?? A2, ?? ,' ? ,{ ?? ,| ?? ,> ?? ,< ?? &); b) taa mar-boutah and haa ?? p, ?? h); and c) yaa and alif maqsoura (?? y, ?? Y).        Ancient Arabic manuscripts were written in scriptura continua, meaning running words without punctuation marks. Punctuation marks were introduced to Arabic mainly through bor-rowing from European languages via translation (Alqinai, 2013). Although punctuation marks in Arabic are gaining popularity and writers are becoming more aware of their importance, yet many writers still do not follow punctuation con-ventions as strictly and consistently as English writers. For example, we investigated contempo-raneous same sized tokenized (simple tokeniza-tion with separation of punctuation) English and Modern Standard Arabic Gigaword edited newswire corpora, we found that 10% of the to-kens in the English Gigaword corresponded to punctuation marks, compared to only 3% of the tokens in the Arabic counterpart.    Train. % Dev. % Word Count 925,643 -- 48,471 -- Total Errors 306,757 33.14 16,659 34.37 Word errors 187,040 60.97 9,878 59.30 Punc. errors 618,886 39.03 6,781 40.70 Split 10,869 3.48 612 3.67 Add_before 99,258 32.36 5,704 34.24 Delete 6,778 2.21 338 2.03 Edit 169,769 55.34 8,914 53.51 Merge 18,267 5.95 994 5.97 Add_after 20 0.01 2 0.01 Move 427 0.14 13 0.08 Table 1. Distribution Statistics on Error Types 1.3 Data Analysis In our work, we use the QALB corpus (Zag-houani et al. 2014), and the training and devel-opment set provided in the QALB shared task (Mohit et. al 2014). The shared task addresses a large array of errors, and not just typical spelling                                                 2 In this paper, we use the Buckwalter Transliteration Scheme as described in www.qamus.com. 
errors. For instance, as Table 1 illustrates punc-tuation errors make up to 40% of all the errors in the shared task. For further investigation, we annotated 1,100 words from the development set for error types, and found that 85% of the word errors (excluding punctuation marks) are typical spelling errors (or non-word errors), while 15% are real-word er-rors, or lexical ambiguities (that is, they are valid words outside of their context), and they range between dialectal words, grammatical errors, semantic errors, speech effects and elongation, examples shown in Table 2.  Error Type Example Correction dialectal words bhAy ??????  ?by this? [Syrian] bh*h ??????  ?by this? [MSA] grammatical errors kbyr ?????  ?big.masc? kbyrp ???????  ?big.fem? semantic  errors |tyh ???????  ?come to him? |typ ??????  ?coming? speech  effects ??????????????   AlrjAAAAl ?men? ????????  AlrjAl ?men? elongation dm__A' ??????  ?blood? dmA' ?????  ?blood? Table 2. Examples of real word errors  2 Our Methodology Due to the complexity and variability of errors in the shared task, we treat each problem individu-ally and use different approaches that prove to be most appropriate for each problem. We specifi-cally address three subtypes of errors: ortho-graphical errors; punctuation errors; and non-word errors. 2.1 Orthographical Errors There are many instances in the shared task?s data that can be treated using simple and straight-forward conversion via regular expression re-place rules. We estimate that these instances cover 10% of the non-punctuation errors in the development set. In HASP we use deterministic heuristic rules to normalize the text, including the following: 1. Hindi numbers (?????????????????) are converted into Arabic numerals [0-9] (occurs 495 in the training data times); 2. Speech effects are removed. For example, ?????????????? AlrjAAAAl ?men? is converted to ???????? AlrjAl. As a general rule letters repeated three times or more are reduced to one letter (715 times); 3. Elongation or kashida is removed. For ex-ample, ???????  dm__A' ?blood? is converted to 
149
????? dmA' (906 times); 4. Special character U+06CC, the Farsi yeh: ?? is converted to U+0649, the visually similar Arabic alif maqsoura ?? Y (293 times). 2.2 Punctuation Errors Punctuation errors constitute 40% of the errors in the QALB Arabic data. It is worth noting that by comparison, punctuation errors only constituted 4% of the English data in CoNLL 2013 Shared Task on English Grammatical Error Correction (Ng et al., 2013) and were not evaluated or han-dled by any participant. In HASP, we focus on 6 punctuation marks: comma, colon, semi-colon, exclamation mark, question mark and period. The ?column? file in the QALB shared task da-ta comes preprocessed with the MADAMIRA morphological analyzer version 04092014-1.0-beta (Pasha et al., 2014). The features that we utilize in our punctuation classification experi-ments are all extracted from the ?column? file, and they are as follows: (1) The original word, that is the word as it ap-pears in the text without any further pro-cessing, (e.g., ????????? llt$Awr ?for consulting?); (2) The tokenized word using the Penn Arabic Treebank (PATB) tokenization (e.g., ?????????? +?? l+Alt$Awr); (3) Kulick POS tag (e.g., IN+DT+NN). (4) Buckwalter POS tag (e.g., PREP+DET+ NOUN+CASE_DEF_GN) as produced by MADAMIRA; (5) Classes to be predicted: colon_after, com-ma_after, exclmark_after, period_after, qmark_after, semicolon_after and NA (when no punctuation marks are used);  Window Size Recall Precision F-measure 4 36.24 54.09 43.40 5 37.95 59.61 46.37 6 36.65 59.99 45.50 7 34.50 59.53 43.68 Table 3. Yamcha results on the development set       For classification, we experiment with Sup-port Vector Machines (SVM) as implemented in Yamcha (Kudo and Matsumoto, 2003) and Con-ditional Random Field (CRF++) classifiers  (Laf-ferty et al. 2001). In our investigation, we vary the context window size from 4 to 8 and we use all 5 features listed for every word in the win-dow. As Tables 3 and 4 show, we found that window size 5 gives the best f-score by both Yamcha and CRF. When we strip clitics from 
tokenized tag, reducing it to stems only, the per-formance of the system improved. Overall CRF yields significantly higher results using the same experimental setup. We assume that the perfor-mance advantage of CRF is a result of the way words in the context and their features are inter-connected in a neat grid in the template file.  # Window Size Recall Precision f-measure 1 4 44.03 74.33 55.31 2 5 44.50 75.49 55.99 3 6 44.22 74.93 55.62 4 7 43.81 75.09 55.34 5 8 43.49 75.41 55.17 6 8* 43.31 75.37 55.00 Table 4. CRF results on the development set * with full tokens; other experiments use stems only, i.e., clitics are removed. 2.3. Non Word Errors This type of errors comprises different subtypes: merges where two or more words are merged together; splits where a space is inserted within a single word; or misspelled words (which under-went substitution, deletion, insertion or transpo-sition) that should be corrected. We handle these problems as follows. 2.3.1. Word Merges Merged words are when the space(s) between two or more words is deleted, such as  ???????????????h*AAlnZAm ?this system?, which should be  ??????????????? h*A AlnZAm. They constitute 3.67% and 3.48% of the error types in the shared task?s de-velopment and training data, respectively. Attia et al. (2012) used an algorithm for dealing with merged words in Arabic, that is, ? ? 3, where l is the length of a word. For a 7-letter word, their algorithm generates 4 candidates as it allows on-ly a single space to be inserted in a string. Their algorithm, however, is too restricted. By contrast Alkanhal et al. (2012) developed an algorithm with more generative power, that is 2???. Their algorithm, however, is in practice too general and leads to a huge fan out. For a 7-letter word, it generates 64 solutions. We develop a splitting algorithm by taking into account that the mini-mum length of words in Arabic is two. Our mod-ified algorithm is 2???, which creates an effec-tive balance between comprehensiveness and compactness. For the 7-letter word, it generates 8 candidates. However, from Table 5 on merged words and their gold splits, one would question 
150
the feasibility of producing more than two splits for any given string. Our splitting algorithm is evaluated in 2.3.3.1.c and compared to Attia et al.?s (2012) algorithm.   Development Training Total Count 631 11,054 1 split 611 10,575 2 splits 15 404 3 splits 3 57 4 splits 1 13 5 splits 1 5 Table 5. Merged words and their splits 2.3.2. Word Splits Beside the problem of merged words, there is also the problem of split words, where one or more spaces are inserted within a word, such as ?? ???? Sm Am ?valve? (correction is ????? SmAm). This error constitutes 6% of the shared task?s both training and development set. We found that the vast majority of instances of this type of error involve the clitic conjunction waw ?and?, which should be represented as a word prefix. Among the 18,267 splits in the training data 15,548 of them involved the waw, corresponding to 85.12%. Similarly among the 994 splits in the development data, 760 of them involved the waw (76.46%).     Therefore, we opted to handle this problem in our work in a partial and shallow manner using deterministic rules addressing specifically the following two phenomena:  1. Separated conjunction morpheme waw ?? w ?and? is attached to the succeeding word (oc-curs 15,915 times in the training data); 2. Literal strings attached to numbers are sepa-rated with space(s). For example, ?????????2000?????? ?dmA'2000$hydF? ?blood of 2000 martyrs? is converted to ????????? 2000 ?????? ?dmA' 2000 $hydF? (824 times). 2.3.3. Misspelled Word Errors This is more akin to the typical spelling correc-tion problem where a word has the wrong letters, rendering it a non-word. We address this prob-lem using two approaches: Dictionary-LM Cor-rection, and Alignment Based Correction.  2.3.3.1. Dictionary-LM Correction Spelling error detection and correction mainly consists of three phases: a) error detection; b) candidate generation; and c) error correction, or best candidate selection.   
a. Error Detection For non-word spelling error detection and candi-date generation we use AraComLex Extended, an open-source reference dictionary (or word list) of full-form words. The dictionary is devel-oped by Attia et al. (2012) through an amalgama-tion of various resources, such as a wordlist from the Arabic Gigaword corpus, wordlist generated from the Buckwalter morphological analyzer, and AraComLex (Attia et al., 2011), a finite-state morphological transducer. AraComLex Extended consists of 9.2M words and, as far as we know, is the largest wordlist for Arabic reported in the literature to date. We enhance the AraComLex Extended dic-tionary by utilizing the annotated data in the shared task?s training data. We add 776 new val-id words to the dictionary and remove 4,810 mis-spelt words, leading to significant improvement in the dictionary?s ability to make decisions on words. Table 6 shows the dictionary?s perfor-mance on the training and development set in the shared task as applied only to non-words and excluding grammatical, semantic and punctua-tion errors.  data set R P F Training 98.84 96.34 97.57 Development 98.72 96.04 97.36 Table 6. Results of dictionary error detection  b. Candidate Generation For candidate generation we use Foma (Hulden, 2009), a finite state compiler that is capable of producing candidates from a wordlist (compiled as an FST network) within a certain edit distance from an error word. Foma allows the ranking of candidates according to customizable transfor-mation rules.   # Error Type Count Ratio % 1.  ?? > typed as ?? A 59,507 31.82 2.  Insert 28.945 15.48 3.  ?? < typed as ?? A 25.392 13.58 4.  Delete 18.246 9.76 5.  ?? p typed as ?? h 14.639 7.83 6.  Split 11.419 6.11 7.  ?? y typed as ?? Y 6.419 3.43 Table 7. Error types in the training set  We develop a re-ranker based on our observa-tion of the error types in the shared task?s train-ing data (as shown in Table 7) and examining the character transformations between the misspelt words and their gold corrections. Our statistics 
151
shows that soft errors (or variants as explained in Section 1.2) account for more than 62% of all errors in the training data.  c. Error Correction For error correction, namely selecting the best solution among the list of candidates, we use an n-gram language model (LM), as implemented in the SRILM package (Stolcke et al., 2011). We use the ?disambig? tool for selecting candidates from a map file where erroneous words are pro-vided with a list of possible corrections. We also use the ?ngram? utility in post-processing for de-ciding on whether a split-word solution has a better probability than a single word solution. Our bigram language model is trained on the Gi-gaword Corpus 4th edition (Parker et al., 2009).     For the LM disambiguation we use the ??fb? option (forward-backward tracking), and we pro-vide candidates with probability scores. We gen-erate these probability scores by converting the edit distance scores produced by the Foma FST re-ranker explained above. Both of the forward-backward tracking and the probability scores in in tandem yield better results than the default values. We evaluate the performance of our sys-tem against the gold standard using the Max-Match (M2) method for evaluating grammatical error correction by Dahlmeier and Ng (2012).     The best f-score achieved in our system is ob-tained when we combine the CRF punctuation classifier (merged with the original punctuations found in data), knowledge-based normalization (norm), dictionary-LM disambiguation and split-1, as shown in Table 8. The option split-1 refers to using the splitting algorithm ? ? 3  as ex-plained in Section 2.3.1, while split-2 refers to using the splitting algorithm 2???.  # Experiment R P F 1 LM+split-1 33.32 73.71 45.89 2 +CRF_punc+split-1 49.74 65.38 56.50 3 + norm+split-1 38.81 69.08 49.70 4 +CRF_punc+norm +split-1 54.79 67.65 60.55 5 +CRF_punc+norm +orig_punc+split-1 53.18 73.15 61.59 6 +CRF_punc+norm +orig_punc+split-2 53.13 73.01 61.50 Table 8. LM correction with 3 candidates       In the QALB Shared Task evaluation, we submit two systems: System 1 is configuration 5 in Table 8, and System 2 corresponds to configu-ration 6, and the results on the test set are shown 
in Table 9. As Table 9 shows, the best scores are obtained by System 1, which is ranked 5th among the 9 systems participating in the shared task.  # Experiment R P F 1 System 1 52.98 75.47 62.25 2 System 2 52.99 75.34 62.22 Table 9. Final official results on the test set pro-vided by the Shared Task 2.3.3.2. Alignment-Based Correction We formatted the data for alignment using a window of 4 words: one word to each side (forming the contextual boundary) and two words in the middle. The two words in the mid-dle are split into characters so that character transformations can be observed and learned by the aligner. The alignment tool we use is Giza++ (Och and Ney, 2003). Results are reported in Ta-ble 10. 	 ? # Experiment  R P F 1 for all error types 36.05 45.13 37.99 2 excluding punc 32.37 54.65 40.66 3 2 + CRF_punc+norm 46.11 62.02 52.90 Table 10. Results of character-based alignment 	 ?Although these preliminary results from Align-ment are significantly below results yielded from the Dictionary-LM approach, we believe that there are several potential improvements that need to be explored:  ? Using LM on the output of the alignment; ? Determining the type of errors that the alignment is most successful at handling: punctuation, grammar, non-words, etc; ? Parsing training data errors with the Diction-ary-LM disambiguation and retraining, so in-stead of training data consisting of errors and gold corrections, it will consist of corrected errors and gold corrections. 3 Conclusion We have described our system HASP for the au-tomatic correction of spelling and punctuation mistakes in Arabic. To our knowledge, this is the first system to handle punctuation errors. We utilize and improve on an open-source full-form dictionary, introduce better algorithm for hand-ing merged word errors, tune the LM parameters, and combine the various components together, leading to cumulative improved results. 
152
References Alfaifi, A., and Atwell, E. (2012) Arabic Learner Corpora (ALC): a taxonomy of coding errors. In Proceedings of the 8th International Computing Conference in Arabic (ICCA 2012), Cairo, Egypt. Alkanhal, Mohamed I., Mohamed A. Al-Badrashiny, Mansour M. Alghamdi, and Abdulaziz O. Al-Qabbany. (2012) Automatic Stochastic Arabic Spelling Correction With Emphasis on Space In-sertions and Deletions. IEEE Transactions on Au-dio, Speech, and Language Processing, Vol. 20, No. 7, September 2012. Alqinai, Jamal. (2013) Mediating punctuation in Eng-lish Arabic translation. Linguistica Atlantica.  Vol. 32. Attia, M., Pecina, P., Tounsi, L., Toral, A., and van Genabith, J. (2011) An Open-Source Finite State Morphological Transducer for Modern Standard Arabic. International Workshop on Finite State Methods and Natural Language Processing (FSMNLP). Blois, France. Attia, Mohammed, Pavel Pecina, Younes Samih, Khaled Shaalan, Josef van Genabith. 2012. Im-proved Spelling Error Detection and Correction for Arabic. COLING 2012, Bumbai, India.  Beesley, Kenneth R. (1998). Arabic Morphology Us-ing Only Finite-State Operations. In The Workshop on Computational Approaches to Semitic lan-guages, Montreal, Quebec, pp. 50?57. Ben Othmane Zribi, C. and Ben Ahmed, M. (2003) Efficient Automatic Correction of Misspelled Ara-bic Words Based on Contextual Information, Lec-ture Notes in Computer Science, Springer, Vol. 2773, pp.770?777. Brill, Eric and Moore, Robert C. (2000) An improved error model for noisy channel spelling correction. Proceedings of the 38th Annual Meeting of the As-sociation for Computational Linguistics, Hong Kong, pp. 286?293. Brown, P. F., Della Pietra, V. J., de Souza, P. V., Lai, J. C. and Mercer, R. L. (1992) Class-Based n-gram Models of Natural Language. Computational Lin-guistics, 18(4), 467?479. Buckwalter, T. (2004b) Buckwalter Arabic Morpho-logical  Analyzer (BAMA) Version 2.0. Linguistic Data Consortium (LDC) catalogue number: LDC2004L02,  ISBN1-58563-324-0. Buckwalter, Tim. (2004a) Issues in Arabic orthogra-phy and morphology analysis. Proceedings of the Workshop on Computational Approaches to Arabic Script-based Languages. Pages 31-34. Association for Computational Linguistics Stroudsburg, PA, USA. 
Church, Kenneth W. and William A. Gale. (1991) Probability scoring for spelling correction. Statis-tics and Computing, 1, pp. 93?103. Dahlmeier, Daniel and Ng, Hwee Tou. 2012. Better evaluation for grammatical error correction. In Proceedings of NAACL. Damerau, Fred J. (1964) A Technique for Computer Detection and Correction of Spelling Errors. Communications of the ACM, Volum 7, issue 3, pp. 171?176. Gao, Jianfeng, Xiaolong Li, Daniel Micol, Chris Quirk, and Xu Sun. (2010) A large scale ranker-based system for search query spelling correction. Proceedings of the 23rd International Conference on Computational Linguistics (COLING 2010), pages 358?366, Beijing, China Golding, Andrew R. A Bayesian Hybrid Method for Context-sensitive Spelling Correction. In Proceed-ings of the Third Workshop on Very Large Corpo-ra. MIT, Cambridge, Massachusetts, USA. 1995, pp.39?53. Golding, Andrew R., and Dan Roth. (1996) Applying Winnow to Context-Sensitive Spelling Correction. In Proceedings of the Thirteenth International Con-ference on Machine Learning, Stroudsburg, PA, USA, pp. 182?190 Habash, Nizar Y. (2010) Introduction to Arabic Natu-ral Language Processing. Synthesis Lectures on Human Language Technologies 3.1: 1-187. Haddad, B., and Yaseen, M. (2007) Detection and Correction of Non-Words in Arabic: A Hybrid Ap-proach. International Journal of Computer Pro-cessing of Oriental Languages. Vol. 20, No. 4. Han, Bo and Timothy Baldwin. (2011) Lexical Nor-malisation of Short Text Messages: Makn Sens a #twitter. Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 368?378, Portland, Oregon, June 19-24, 2011 Hassan, A, Noeman, S., and Hassan, H. (2008) Lan-guage Independent Text Correction using Finite State Automata. IJCNLP. Hyderabad, India. Hulden, M. (2009) Foma: a Finite-state compiler and library. EACL '09 Proceedings of the 12th Confer-ence of the European Chapter of the Association for Computational Linguistics. Association for Computational Linguistics Stroudsburg, PA, USA Islam, Aminul, Diana Inkpen. (2009) Real-Word Spelling Correction using Google Web 1T n-gram with Backoff. International Conference on Natural Language Processing and Knowledge Engineering, Dalian, China, pp. 1?8. 
153
Kiraz, G. A. (2001) Computational Nonlinear Mor-phology: With Emphasis on Semitic Languages. Cambridge University Press. Kudo, Taku, Yuji Matsumoto. (2003) Fast Methods for Kernel-Based Text Analysis. 41st Annual Meeting of the Association for Computational Lin-guistics (ACL-2003), Sapporo, Japan. Kukich, Karen. (1992) Techniques for automatically correcting words in text. Computing Surveys, 24(4), pp. 377?439. Lafferty, John, Andrew McCallum, and Fernando Pereira. (2001) Conditional random fields: Proba-bilistic models for segmenting and labeling se-quence data, In Proceedings of the International Conference on Machine Learning (ICML 2001), , MA, USA, pp. 282-289. Levenshtein, V. I. (1966) Binary codes capable of correcting deletions, insertions, and reversals. In: Soviet Physics Doklady, pp. 707-710. Magdy, W., and Darwish, K. (2006) Arabic OCR er-ror correction using character segment correction, language modeling, and shallow morphology. EMNLP '06 Proceedings of the 2006 Conference on Empirical Methods in Natural Language Pro-cessing. Mohit, Behrang, Alla Rozovskaya, Nizar Habash, Wajdi Zaghouani, and Ossama Obeid, 2014.  The First QALB Shared Task on Automatic Text Cor-rection for Arabic. In Proceedings of EMNLP workshop on Arabic Natural Language Processing. Doha, Qatar. Ng, Hwee Tou, Siew Mei Wu, Yuanbin Wu, Christian Hadiwinoto, and Joel Tetreault. (2013) The CoNLL-2013 Shared Task on Grammatical Error Correction. Proceedings of the Seventeenth Con-ference on Computational Natural Language Learning: Shared Task, pages 1?12, Sofia, Bulgar-ia, August 8-9 2013. Norvig, P. (2009) Natural language corpus data. In Beautiful Data, edited by Toby Segaran and Jeff Hammerbacher, pp. 219-?-242. Sebastopol, Ca-lif.: O'Reilly. Och, Franz Josef, Hermann Ney. (2003) A Systematic Comparison of Various Statistical Alignment Models. In Computational Linguistics, volume 29, number 1, pp. 19-51 March 2003. Parker, R., Graff, D., Chen, K., Kong, J., and Maeda, K. (2009) Arabic Gigaword Fifth Edition. LDC Catalog No.: LDC2009T30, ISBN: 1-58563-532-4. Parker, R., Graff, D., Chen, K., Kong, J., and Maeda, K. (2011) Arabic Gigaword Fifth Edition. LDC Catalog No.: LDC2011T11, ISBN: 1-58563-595-2. Pasha, Arfath, Mohamed Al-Badrashiny, Ahmed El Kholy, Ramy Eskander, Mona Diab, Nizar Habash, 
Manoj Pooleery, Owen Rambow, Ryan Roth. (2014) Madamira: A fast, comprehensive tool for morphological analysis and disambiguation of Ar-abic. In Proceedings of the 9th International Con-ference on Language Resources and Evaluation, Reykjavik, Iceland. Ratcliffe, Robert R. (1998) The Broken Plural Prob-lem in Arabic and Comparative Semitic: Allo-morphy and Analogy in Non-concatenative Mor-phology. Amsterdam studies in the theory and his-tory of linguistic science. Series IV, Current issues in linguistic theory ; v. 168. Amsterdam ; Philadel-phia: J. Benjamins. Roth, R. Rambow, O., Habash, N., Diab, M., and Rudin, C. (2008) Arabic Morphological Tagging, Diacritization, and Lemmatization Using Lexeme Models and Feature Ranking. Proceedings of ACL-08: HLT, Short Papers, pp. 117?120. Shaalan, K., Samih, Y., Attia, M., Pecina, P., and van Genabith, J. (2012) Arabic Word Generation and Modelling for Spell Checking. Language Re-sources and Evaluation (LREC). Istanbul, Turkey. pp. 719?725. Stolcke, A., Zheng, J., Wang, W., and Abrash, V. (2011) SRILM at sixteen: Update and outlook. in Proc. IEEE Automatic Speech Recognition and Understanding Workshop. Waikoloa, Hawaii. van Delden, Sebastian, David B. Bracewell, and Fer-nando Gomez. (2004) Supervised and Unsuper-vised Automatic Spelling Correction Algorithms. In proceeding of Information Reuse and Integration (IRI). Proceedings of the 2004 IEEE International Conference on Web Services, pp. 530?535. Wu, Jian-cheng, Hsun-wen Chiu, and Jason S. Chang. (2013) Integrating Dictionary and Web N-grams for Chinese Spell Checking. Computational Lin-guistics and Chinese Language Processing. Vol. 18, No. 4, December 2013, pp. 17?30. Zaghouani, Wajdi, Behrang Mohit, Nizar Habash, Ossama Obeid, Nadi Tomeh, Alla Rozovskaya, Noura Farra, Sarah Alkuhlani, and Kemal Oflazer. 2014. Large Scale Arabic Error Annotation: Guide-lines and Framework. In Proceedings of the Ninth International Conference on Language Resources and Evaluation (LREC?14), Reykjavik, Iceland.  
154
