Proceedings of NAACL HLT 2007, pages 524?531,
Rochester, NY, April 2007. c?2007 Association for Computational Linguistics
A Probabilistic Framework for Answer Selection in Question Answering
Jeongwoo Ko1, Luo Si2, Eric Nyberg1
1Language Technologies Institute, Carnegie Mellon, Pittsburgh, PA 15213
2Department of Computer Science, Purdue University, West Lafayette, IN 47907
jko@cs.cmu.edu, lsi@cs.purdue.edu, ehn@cs.cmu.edu
Abstract
This paper describes a probabilistic an-
swer selection framework for question an-
swering. In contrast with previous work
using individual resources such as ontolo-
gies and the Web to validate answer can-
didates, our work focuses on developing
a unified framework that not only uses
multiple resources for validating answer
candidates, but also considers evidence of
similarity among answer candidates in or-
der to boost the ranking of the correct an-
swer. This framework has been used to se-
lect answers from candidates generated by
four different answer extraction methods.
An extensive set of empirical results based
on TREC factoid questions demonstrates
the effectiveness of the unified framework.
1 Introduction
Question answering aims at finding exact answers
to a user?s natural language question from a large
collection of documents. Most QA systems com-
bine information retrieval with extraction techniques
to identify a set of likely candidates and then uti-
lize some selection strategy to generate the final
answers (Prager et al, 2000; Clarke et al, 2001;
Harabagiu et al, 2001). Since answer extractors
may be based on imprecise empirical methods, the
selection process can be very challenging, as it often
entails identifying correct answer(s) amongst many
incorrect ones.
Questio
n Ques
tion Analys
isQu
ery D
ocumen
t
Retriev
al Corpus
Docs
Answe
r
Extract
ionAn
swer candida
tes An
swer Selecti
on Answe
r
Shang
hai
FT94
2-20
16
0.5
Taiw
an
FBIS
3-4532
0
0.4
Shang
hai
FBIS
3-58
0.64
Shang
hai
WSJ9
2011
0-00
13
0.65
Hong
 Kon
g
AP88
0603
-026
8
0.7
Beijin
g
Docu
ment
 
extra
cted
Score
Answ
er 
cand
idate
s
Whic
h city
 in C
hina 
has t
he 
large
st nu
mber
 of fo
reign
 
finan
cial c
omp
anies
?
Figure 1: A traditional QA pipeline architecture
Figure 1 shows a traditional QA architecture with
an example question. Given the question ?Which
city in China has the largest number of foreign fi-
nancial companies??, the answer extraction com-
ponent produces a ranked list of five answer can-
didates. Due to imprecision in answer extraction,
an incorrect answer (?Beijing?) was ranked at the
top position. The correct answer (?Shanghai?) was
extracted from two documents with different confi-
dence scores and ranked at the third and the fifth po-
sitions. In order to select ?Shanghai? as the final
answer, we need to address two issues:
? Answer Validation. How do we identify correct
answer(s) amongst incorrect ones? Validating
an answer may involve searching for facts in
a knowledge base, e.g. IS-A(Shanghai,
city), IS-IN(Shanghai, China).
? Answer Similarity. How do we exploit evi-
dence of similarity among answer candidates?
524
For example, when there are redundant an-
swers (?Shanghai?, as above) or several an-
swers which represent a single instance (e.g.
?Clinton, Bill? and ?William Jefferson Clin-
ton?) in the candidate list, how much should we
boost the answer candidate scores?
To address the first issue, several answer selec-
tion approaches have used semantic resources. One
of the most common approaches relies on Word-
Net, CYC and gazetteers for answer validation or
answer reranking; answer candidates are pruned
or discounted if they are not found within a re-
source?s hierarchy corresponding to the expected an-
swer type (Xu et al, 2003; Moldovan et al, 2003;
Prager et al, 2004). In addition, the Web has been
used for answer reranking by exploiting search en-
gine results produced by queries containing the an-
swer candidate and question keywords (Magnini et
al., 2002), and Wikipedia?s structured information
has been used for answer type checking (Buscaldi
and Rosso, 2006).
To use more than one resource for answer
type checking of location questions, Schlobach
et al (2004) combined WordNet with geographi-
cal databases. However, in their experiments the
combination actually hurt performance because of
the increased semantic ambiguity that accompanies
broader coverage of location names. This demon-
strates that the method used to combine potential
answers may matter as much as the choice of re-
sources.
To address the second issue we must determine
how to detect and exploit answer similarity. As an-
swer candidates are extracted from different docu-
ments, they may contain identical, similar or com-
plementary text snippets. For example, the United
States may be represented by the strings ?U.S.?,
?United States? or ?USA? in different documents. It
is important to detect this type of similarity and ex-
ploit it to boost answer confidence, especially for list
questions that require a set of unique answers. One
approach is to incorporate answer clustering (Kwok
et al, 2001; Nyberg et al, 2003; Jijkoun et al,
2006). For example, we might merge ?April 1912?
and ?14 Apr 1912? into a cluster and then choose
one answer as the cluster head. However, clustering
raises new issues: how to choose the cluster head
and how to calculate the scores of the clustered an-
swers.
Although many QA systems individually address
these issues in answer selection, there has been lit-
tle research on generating a generalized probabilistic
framework that allows any validation and similarity
features to be easily incorporated.
In this paper we describe a probabilistic answer
selection framework to address the two issues. The
framework uses logistic regression to estimate the
probability that an answer candidate is correct given
multiple answer validation features and answer sim-
ilarity features. Experimental results on TREC
factoid questions (Voorhees, 2004) show that our
framework significantly improved answer selection
performance for four different extraction techniques,
when compared to default selection using the indi-
vidual candidate scores produced by each extractor.
This paper is organized as follows: Section 2 de-
scribes our answer selection framework and Section
3 lists the features that generate similarity and va-
lidity scores for factoid questions. In Section 4, we
describe the experimental methodology and the re-
sults. Section 5 describes how we intend to extend
our framework to handle complex questions. Finally
Section 6 concludes with suggestions for future re-
search.
2 Method
Answer validation is based on an estimate of the
probability P (correct(Ai)|Ai, Q), where Q is a
question and Ai is an answer candidate to the ques-
tion. Answer similarity is is based on an estimate
of the probability P (correct(Ai)|Ai, Aj), where Aj
is similar to Ai. Since both probabilities influ-
ence answer selection performance, it is important
to combine them in a unified framework and es-
timate the probability of an answer candidate as:
P (correct(Ai)|Q,A1, ..., An).
In this paper, we propose a proba-
bilistic framework that directly estimates
P (correct(Ai)|Q,A1, ..., An) using multiple
answer validation features and answer similarity
features. The framework was implemented with
logistic regression, which is a statistical machine
learning technique used to predict the probability
of a binary variable from input variables. Logistic
525
P (correct(Ai)|Q,A1, ..., An) (1)
? P (correct(Ai)|val1(Ai), ..., valK1(Ai), sim1(Ai), ..., simK2(Ai))
=
exp(?0 +
K1?
k=1
?kvalk(Ai) +
K2?
k=1
?ksimk(Ai))
1 + exp(?0 +
K1?
k=1
?kvalk(Ai) +
K2?
k=1
?ksimk(Ai))
where, simk(Ai) =
N?
j=1(j 6=i)
sim?k(Ai, Aj).
~?, ~?,~? = argmax
~?,~?,~?
R?
j=1
Nj?
i=1
logP (correct(Ai)|val1(Ai), ..., valK1(Ai), sim1(Ai), ..., simK2(Ai)) (2)
regression has been successfully employed in many
applications including multilingual document merg-
ing (Si and Callan, 2005). In our previous work (Ko
et al, 2006), we showed that logistic regression
performed well in merging three resources to vali-
date answers to location and proper name questions.
We extended this approach to combine multiple
similarity features with multiple answer validation
features. The extended framework estimates the
probability that an answer candidate is correct given
the degree of answer correctness and the amount
of supporting evidence provided in a set of answer
candidates (Equation 1).
In Equation 1, each valk(Ai) is a feature function
used to produce an answer validity score for an an-
swer candidate Ai. Each sim?k(Ai, Aj) is a similar-
ity function used to calculate an answer similarity
between Ai and Aj . K1 and K2 are the number of
answer validation and answer similarity features, re-
spectively. N is the number of answer candidates.
To incorporate multiple similarity features, each
simk(Ai) is obtained from an individual similarity
metric. For example, if Levenshtein distance is used
as one similarity metric, simk(Ai) is calculated by
summing N-1 Levenshtein distances between one
answer candidate and all other candidates. As some
string similarity metrics (e.g. Levenshtein distance)
produce a number between 0 and 1 (where 1 means
two strings are identical and 0 means they are differ-
ent), similarity scores less than some threshold value
are ignored.
The parameters ?, ?, ? were estimated from train-
ing data by maximizing the log likelihood as shown
in Equation 2, where R is the number of training
questions and Nj is the number of answer candidates
for each question Qj . For parameter estimation, we
used the Quasi-Newton algorithm (Minka, 2003).
To select correct answers, the initial answer candi-
date set is reranked according to the estimated prob-
ability of each candidate. For factoid questions, the
top answer is selected as the final answer to the ques-
tion. As logistic regression can be used for binary
classification with a default threshold of 0.5, we can
also use the framework to classify incorrect answers:
if the probability of an answer candidate is lower
than 0.5, it is considered to be a wrong answer and
is filtered out of the answer list. This is useful in
deciding whether or not a valid answer exists in the
corpus, an important aspect of the TREC QA evalu-
ation (Voorhees, 2004).
3 Feature Representation
This section details the features used to generate an-
swer validity scores and answer similarity scores for
our answer selection framework.
526
3.1 Answer Validation Features
Each answer validation feature produces a validity
score which predicts whether or not an answer can-
didate is a correct answer for the question. This task
can be done by exploiting external QA resources
such as the Web, databases, and ontologies. For fac-
toid questions, we used gazetteers and WordNet in a
knowledge-based approach; we also used Wikipedia
and Google in a data-driven approach.
3.1.1 Knowledge-based Features
In order to generate answer validity scores using
gazetteers and WordNet, we reused the algorithms
described in our previous work (Ko et al, 2006).
Gazetteers: Gazetteers provide geographic
information, which allows us to identify
strings as instances of countries, their cities,
continents, capitals, etc. For answer selec-
tion, we used three gazetteer resources: the
Tipster Gazetteer, the CIA World Factbook
(https://www.cia.gov/cia/publications/factbook/inde
x.html) and information about the US states pro-
vided by 50states.com (http://www.50states.com).
These resources were used to assign an answer
validity score between -1 and 1 to each candidate
(Figure 2). A score of 0 means the gazetteers did
not contribute to the answer selection process for
that candidate. For some numeric questions, range
checking was added to validate numeric questions
similarly to Prager et al (2004). For example, given
the question ?How many people live in Chile??,
if an answer candidate is within ? 10% of the
population stated in the CIA World Factbook, it
receives a score of 1.0. If it is in the range of 20%,
its score is 0.5. If it significantly differs by more
than 20%, it receives a score of -1.0. The threshold
may vary based on when the document was written
and when the census was taken1.
WordNet: The WordNet lexical database includes
English words organized in synonym sets, called
synsets (Fellbaum, 1998). We used WordNet in or-
der to produce an answer validity score between -1
and 1, following the algorithm in Figure 3. A score
1The ranges used here were found to work effectively, but
were not explicitly validated or tuned.
 
 
  
1)  If the answer candidate directly matches the gazetteer 
answer for the question, its gazetteer score is 1.0. (e.g. 
Given the question ?What continent is Togo on??, the 
candidate ?Africa? receives a score of 1.0.) 
2)  If the answer candidate occurs in the gazetteer within 
the subcategory of the expected answer type, its score 
is 0.5. (e.g., Given the question ?Which city in China 
has the largest number of foreign financial 
companies??, the candidates ?Shanghai? and ?Boston? 
receive a score of 0.5 because they are both cities.) 
3)  If the answer candidate is not the correct semantic 
type, its score is -1. (e.g., Given the question ?Which 
city in China has the largest number of foreign 
financial companies??, the candidate ?Taiwan? 
receives a score of -1 because it is not a city.) 
4) Otherwise, the score is 0.0. 
Figure 2: Validity scoring with gazetteers.
 
 
 
 
 
 
 
 
 
 
 
1)  If the answer candidate directly matches WordNet, its 
WordNet score is 1.0. (e.g. Given the question ?What is 
the capital of Uruguay??, the candidate ?Montevideo? 
receives a score of 1.0.) 
2)  If the answer candidate?s hypernyms include a 
subcategory of the expected answer type, its score is 
0.5. (e.g., Given the question ?Who wrote the book 
?Song of Solomon??", the candidate ?Mark Twain? 
receives a score of 0.5 because its hypernyms include 
?writer?.) 
3)  If the answer candidate is not the correct semantic 
type, this candidate receives a score of -1. (e.g., Given 
the question ?What state is Niagara Falls located in??, 
the candidate ?Toronto? gets a score of -1 because it is 
not a state.) 
4) Otherwise, the score is 0.0. 
Figure 3: Validity scoring with WordNet.
of 0 means that WordNet does not contribute to the
answer selection process for a candidate.
3.1.2 Data-driven Features
Wikipedia and Google were used in a data-driven
approach to generate answer validity scores.
Wikipedia: Wikipedia (http://www.wikipedia.org)
is a multilingual free on-line encyclopedia. Fig-
ure 4 shows the algorithm used to generate an
answer validity score from Wikipedia. If there
is a Wikipedia document whose title matches an
answer candidate, the document is analyzed to
obtain the term frequency (tf) and the inverse term
527
For
 eac
h an
swe
r ca
ndi
date
 A i,
1. I
niti
aliz
e th
e W
ikip
edia
 sco
re: 
ws(
A i) 
= 0
2. S
earc
h fo
r a 
Wik
iped
ia d
ocu
men
t wh
ose
 titl
e is
 A i
3. I
f a 
doc
um
ent 
is fo
und
, ca
lcul
ate 
tf.id
fsc
ore
 of 
A ii
n th
e 
retr
ieve
d W
ikip
edia
 doc
um
ent
ws(
A i) 
= (1
+lo
g(tf
)) ?
 (1+
log
(idf
))
4. I
f no
t, fo
r ea
ch q
ues
tion
 key
wor
d K
j,
4.1.
 Sea
rch
 for
 a W
ikip
edia
 doc
um
ent 
that
 inc
lud
es K
j
4.2.
 If a
 doc
um
ent 
is fo
und
, ca
lcul
ate 
tf.id
fsc
ore
 of 
A i 
ws(
A i) 
+= 
(1+
log
(tf)
) ? 
(1+
log
(idf
))
Figure 4: Validity scoring with Wikipedia
1 )
1( 2)(
)(
?
+
?
=
d
scs
scs
For
 eac
h an
swe
r ca
ndi
date
 A i,
1. I
niti
aliz
e th
e G
oog
le s
cor
e: g
s(A
i) =
 0
2. F
or e
ach
 sni
ppe
t s:
2.1.
 Ini
tial
ize 
the 
snip
pet 
co-
occ
urre
nce
 sco
re: 
cs(s
) = 
1
2.2.
 Fo
r ea
ch q
ues
tion
 key
wor
d k
in s
: 
2.2.
1 C
om
put
e di
stan
ce d
, th
e m
inim
um
 num
ber
 of 
wor
ds b
etw
een
 ka
nd t
he a
nsw
er c
and
idat
e 
2.2.
2 U
pda
te th
e sn
ipp
et c
o-o
ccu
rren
ce s
cor
e:
2.3.
 gs(
A i) 
= g
s(A
i) +
 cs(
s)
3. N
orm
aliz
e th
e G
oog
le s
cor
e (d
ivid
ing
 it b
y a 
con
stan
t C
)
Figure 5: Validity scoring with Google
frequency (idf) of the candidate, from which a
tf.idf score is calculated. When there is no matched
document, each question keyword is also processed
as a back-off strategy, and the answer validity score
is calculated by summing the tf.idf scores. To
calculate word frequency, the TREC Web Corpus
(http://ir.dcs.gla.ac.uk/test collections/wt10g.html)
was used as a large background corpus.
Google: Following Magnini et al (2002), we used
Google to generate a numeric score. A query con-
sisting of an answer candidate and question key-
words was sent to the Google search engine. To
calculate a score, the top 10 text snippets returned
by Google were then analyzed using the algorithm
in Figure 5.
3.2 Answer Similarity Features
We calculate the similarity between two answer can-
didates using multiple string distance metrics and a
list of synonyms.
3.2.1 String Distance Metrics
There are several different string distance metrics
to calculate the similarity of short strings. We used
five popular string distance metrics: Levenshtein,
Jaccard, Jaro, Jaro-Winkler, and Cosine similarity.
3.2.2 Synonyms
Synonyms can be used as another metric to calcu-
late answer similarity. We defined a binary similar-
ity score for synonyms.
sim(Ai, Aj) =
{
1, if Ai is a synonym of Aj
0, otherwise
To get a list of synonyms, we used three knowl-
edge bases: WordNet, Wikipedia and the CIA World
Factbook. WordNet includes synonyms for English
words. Wikipedia redirection is used to obtain an-
other set of synonyms. For example, ?Calif.? is redi-
rected to ?California? in Wikipedia, and ?William
Jefferson Clinton? is redirected to ?Bill Clinton?.
The CIA World Factbook includes five different
names for a country: conventional long form, con-
ventional short form, local long form, local short
form and former name. For example, the conven-
tional long form of Egypt is ?Arab Republic of
Egypt?, the conventional short form is ?Egypt?, the
local short form is ?Misr?, the local long form is
?Jumhuriyat Misr al-Arabiyah? and the former name
is ?United Arab Republic (with Syria)?. All are con-
sidered to be synonyms of ?Egypt?.
In addition, manually generated rules are used to
obtain synonyms for different types of answer can-
didates (Nyberg et al, 2003):
? Dates are converted into the ISO 8601 date for-
mat (YYYY-MM-DD) (e.g., ?April 12 1914?
and ?12th Apr. 1914? are converted into ?1914-
04-12? and considered as synonyms).
? Temporal expressions are converted into the
HH:MM:SS format (e.g., ?six thirty five p.m.?
and ?6:35 pm? are converted into ?18:35:xx?
and considered as synonyms).
? Numeric expression are converted into sci-
entific notation (e.g, ?one million? and
?1,000,000? are converted into ?1e+06? and
considered as synonyms).
528
? Representative entities are converted into the
represented entity when the expected answer
type is COUNTRY (e.g., ?the Egyptian govern-
ment? is changed to ?Egypt? and ?Clinton ad-
ministration? is changed to ?U.S.?).
4 Experiment
This section describes the experiments we used
to evaluate our answer selection framework. The
JAVELIN QA system (Nyberg et al, 2006) was used
as a testbed for the evaluation.
4.1 Experimental Setup
A total of 1760 factoid questions from the TREC8-
12 QA evaluations served as a dataset, with 5-fold
cross validation.
To better understand how the performance of our
framework varies for different extraction techniques,
we tested it with four JAVELIN answer extraction
modules: FST, LIGHTv1, LIGHTv2 and SVM (Ny-
berg et al, 2006). FST is an answer extractor based
on finite state transducers that incorporate a set of
extraction patterns (both manually-created and gen-
eralized patterns). LIGHTv1 is an extractor that se-
lects answer candidates using a non-linear distance
heuristic between the keywords and an answer can-
didate. LIGHTv2 is another extractor based on a
different distance heuristic, originally developed as
part of a multilingual QA system. SVM is an extrac-
tor that uses Support Vector Machines to discrimi-
nate between correct and incorrect answers.
Answer selection performance was measured by
average accuracy: the number of correct top answers
divided by the number of questions where at least
one correct answer exists in the candidate list pro-
vided by an extractor. The baseline was calculated
with the answer candidate scores provided by each
individual extractor; the answer with the best extrac-
tor score was chosen, and no validation or similarity
processing was performed. For Wikipedia, we used
a version downloaded in Nov. 2005, which con-
tained 1,811,554 articles.
4.2 Results and Analysis
We first analyzed the average accuracy when us-
ing individual validation features. Figure 6 shows
the effect of the individual answer validation fea-
tures on different extraction outputs. The combina-
0.00.10.20.30.40.50.60.70.80.91.0
AL
L
GL
W
IKI
W
N
GZ
Ba
se
lin
e
Average Accuracy
 FS
T
 Li
gh
tV1
 Li
gh
tV2
 SV
M
Figure 6: Average accuracy of individual answer
validation features (GZ: gazetteers, WN: WordNet,
WIKI: Wikipedia, GL: Google, ALL: combination
of all features).
tion of all features significantly improved the per-
formance when compared to answer selection using
a single feature. Comparing the data-driven features
with the knowledge-based features, the data-driven
features (such as Wikipedia and Google) increased
performance more than the knowledge-based fea-
tures (such as gazetteers and WordNet); our intuition
is that the knowledge-based features covered fewer
questions. The biggest improvement was found with
candidates produced by the SVM extractor: a 242%
improvement over the baseline. It was mostly be-
cause SVM tended to produce several answer can-
didates with the same or very similar confidence
scores, but our framework could select the correct
answer among many incorrect ones by exploiting
answer validation features.
Table 1 shows the effect of individual similarity
features on different extractors when using 0.3 and
0.5 as a similarity threshold, respectively. When
comparing five different string similarity features
(Levenshtein, Jaro, Jaro-Winkler, Jaccard and Co-
sine similarity), Levenshtein and Jaccard tended to
perform better than the others. When comparing
synonym features with string similarity features,
synonyms performed slightly better.
We also analyzed answer selection performance
when combining all six similarity features (?All? in
Table 1). Combining all similarity features did not
improve the performance except for the FST extrac-
tor, because including five string similarity features
529
Similarity FST LIGHTv1 LIGHTv2 SVM
feature 0.3 0.5 0.3 0.5 0.3 0.5 0.3 0.5
Levenshtein 0.728 0.728 0.471 0.455 0.399 0.400 0.381 0.383
Jaro 0.708 0.705 0.422 0.440 0.373 0.378 0.274 0.282
Jaro-Winkler 0.701 0.705 0.426 0.442 0.374 0.379 0.277 0.275
Jaccard 0.738 0.738 0.438 0.448 0.452 0.448 0.382 0.390
Cosine 0.738 0.738 0.436 0.435 0.418 0.422 0.380 0.378
Synonyms 0.745 0.745 0.458 0.458 0.442 0.442 0.412 0.412
Lev+Syn 0.748 0.751 0.460 0.466 0.445 0.448 0.420 0.412
Jac+Syn 0.742 0.742 0.456 0.465 0.440 0.445 0.396 0.396
All 0.755 0.755 0.405 0.425 0.435 0.431 0.303 0.302
Table 1: Average accuracy using individual similarity features under different thresholds: 0.3 and 0.5
(?Lev+Syn?: the combination of Levenshtein with synonyms, ?Jac+Syn?: the combination of Jaccard and
synonyms, ?All?: the combination of all similarity metrics)
Baseline Sim Val All
FST 0.658 0.751 0.855 0.877
LIGHTv1 0.394 0.466 0.612 0.628
LIGHTv2 0.343 0.448 0.578 0.582
SVM 0.169 0.420 0.578 0.586
Table 2: Average accuracy of individual features
(Sim: merging similarity features, Val: merging val-
idation features, ALL: combination of all features).
provided too much redundancy to the logistic regres-
sion. We also compared the combination of Leven-
shtein with synonyms and the combination of Jac-
card with synonyms, and then chose Levenshtein
and synonyms as the two best similarity features in
our framework.
We also analyzed the degree to which the average
accuracy was affected by answer similarity and val-
idation features. Table 2 compares the average ac-
curacy using the baseline, the answer similarity fea-
tures, the answer validation features and all feature
combinations. As can be seen, the similarity fea-
tures significantly improved performance, so we can
conclude that exploiting answer similarity improves
answer selection performance. The validation fea-
tures also significantly improved the performance.
When combining both sets of features together,
the answer selection performance increased for all
four extractors: an average of 102% over the base-
line, 30% over the similarity features and 1.82%
over the validation features. Adding the similarity
features to the validation features generated small
but consistent improvement in all configurations.
We expect more performance gain from similar-
ity features when merging similar answers returned
from all four extractors.
5 Extensions for Complex Questions
Although we conducted our experiments on fac-
toid questions, our framework can be easily ex-
tended to handle complex questions, which require
longer answers representing facts or relations (e.g.,
?What is the relationship between Alan Greenspan
and Robert Rubin??). As answer candidates are
long text snippets, different features should be used
for answer selection. Possible validation features
include question keyword inclusion and predicate
structure match (Nyberg et al, 2005). For exam-
ple, given the question ?Did Egypt sell Scud mis-
siles to Syria??, the key predicate from the ques-
tion is Sell(Egypt, Syria, Scud missile). If there is
a sentence which contains the predicate structure
Buy(Syria, Scud missile, Egypt), we can calculate
the predicate structure distance and use it as a val-
idation feature. For answer similarity, we intend to
explore novelty detection approaches evaluated in
Allan et al (2003).
6 Conclusion
In this paper, we described our answer selection
framework for estimating the probability that an an-
swer candidate is correct given multiple answer vali-
530
dation and similarity features. We conducted a series
of experiments to evaluate the performance of the
framework and analyzed the effect of individual val-
idation and similarity features. Empirical results on
TREC questions show that our framework improved
answer selection performance in the JAVELIN QA
system by an average of 102% over the baseline,
30% over the similarity features alone and 1.82%
over the validation features alone.
We plan to improve our framework by adding reg-
ularization and selecting the final answers among
candidates returned from all extractors. As our
current framework is based on the assumption that
each answer is independent, we are building another
probabilistic framework which does not require any
independence assumption, and uses an undirected
graphical model to estimate the joint probability of
all answer candidates.
7 Acknowledgments
This work was supported in part by ARDA/DTO
Advanced Question Answering for Intelli-
gence (AQUAINT) program award number
NBCHC040164.
References
J. Allan, C. Wade, and A. Bolivar. 2003. Retrieval and
novelty detection at the sentence level. In Proceedings
of SIGIR.
D. Buscaldi and P. Rosso. 2006. Mining Knowledge
from Wikipedia for the Question Answering task. In
Proceedings of the International Conference on Lan-
guage Resources and Evaluation.
C. Clarke, G. Cormack, and T. Lynam. 2001. Exploiting
redundancy in question answering. In Proceedings of
SIGIR.
C. Fellbaum. 1998. WordNet: An Electronic Lexical
Database. MIT Press.
S. Harabagiu, D. Moldovan, M. Pasca, R. Mihalcea,
M. Surdeanu, R. Bunescu, R. Grju, V. Rus, and
P. Morarescu. 2001. FALCON: Boosting knowledge
for answer engines. In Proceedings of TREC.
V. Jijkoun, J. van Rantwijk, D. Ahn, E. Tjong Kim Sang,
and M. de Rijke. 2006. The University of Amsterdam
at CLEF@QA 2006. In Working Notes CLEF.
J. Ko, L. Hiyakumoto, and E. Nyberg. 2006. Exploit-
ing semantic resources for answer selection. In Pro-
ceedings of the International Conference on Language
Resources and Evaluation.
C. Kwok, O. Etzioni, and D. S. Weld. 2001. Scal-
ing question answering to the web. In Proceedings of
WWW10 Conference.
B. Magnini, M. Negri, R. Pervete, and H. Tanev. 2002.
Comparing statistical and content-based techniques for
answer validation on the web. In Proceedings of the
VIII Convegno AI*IA.
T. Minka. 2003. A Comparison of Numerical Optimizers
for Logistic Regression. Unpublished draft.
D. Moldovan, D. Clark, S. Harabagiu, and S. Maiorano.
2003. Cogex: A logic prover for question answering.
In Proceedings of HLT-NAACL.
E. Nyberg, T. Mitamura, J. Carbonell, J. Callan,
K. Collins-Thompson, K. Czuba, M. Duggan,
L. Hiyakumoto, N. Hu, Y. Huang, J. Ko, L. Lita,
S. Murtagh, V. Pedro, and D. Svoboda. 2003. The
JAVELIN Question-Answering System at TREC 2002.
In Proceedings of the Text REtrieval Conference.
E. Nyberg, T. Mitamura, R. Frederking, M. Bilotti,
K. Hannan, L. Hiyakumoto, J. Ko, F. Lin, V. Pedro,
and A. Schlaikjer. 2006. JAVELIN I and II Systems at
TREC 2005. In Proceedings of TREC.
E. Nyberg, T. Mitamura, R. Frederking, V. Pedro,
M. Bilotti, A. Schlaikjer, and K. Hannan. 2005. Ex-
tending the javelin qa system with domain semantics.
In Proceedings of AAAI-05 Workshop on Question An-
swering in Restricted Domains.
J. Prager, E. Brown, A. Coden, and D. Radev. 2000.
Question answering by predictive annotation. In Pro-
ceedings of SIGIR.
J. Prager, J. Chu-Carroll, K. Czuba, C. Welty, A. Itty-
cheriah, and R. Mahindru. 2004 IBM?s Piquant in
Trec2003. In Proceedings of TREC.
S. Schlobach, M. Olsthoorn, and M. de Rijke. 2004.
Type checking in open-domain question answering. In
Proceedings of European Conference on Artificial In-
telligence.
L. Si and J. Callan. 2005 CLEF2005: Multilingual
retrieval by combining multiple multilingual ranked
lists. In Proceedings of Cross-Language Evaluation
Forum.
E. Voorhees. 2004. Overview of the TREC 2003 ques-
tion answering track. In Proceedings of TREC.
J. Xu, A. Licuanan, J. May, S. Miller, and R. Weischedel.
2003. TREC 2002 QA at BBN: Answer Selection and
Confidence Estimation. In Proceedings of TREC.
531
Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 784?791,
Prague, Czech Republic, June 2007. c?2007 Association for Computational Linguistics
Language-independent Probabilistic Answer Ranking
for Question Answering
Jeongwoo Ko, Teruko Mitamura, Eric Nyberg
Language Technologies Institute
School of Computer Science
Carnegie Mellon University
{jko, teruko, ehn}@cs.cmu.edu
Abstract
This paper presents a language-independent
probabilistic answer ranking framework for
question answering. The framework esti-
mates the probability of an individual an-
swer candidate given the degree of answer
relevance and the amount of supporting evi-
dence provided in the set of answer candi-
dates for the question. Our approach was
evaluated by comparing the candidate an-
swer sets generated by Chinese and Japanese
answer extractors with the re-ranked answer
sets produced by the answer ranking frame-
work. Empirical results from testing on NT-
CIR factoid questions show a 40% perfor-
mance improvement in Chinese answer se-
lection and a 45% improvement in Japanese
answer selection.
1 Introduction
Question answering (QA) systems aim at find-
ing precise answers to natural language questions
from large document collections. Typical QA sys-
tems (Prager et al, 2000; Clarke et al, 2001;
Harabagiu et al, 2000) adopt a pipeline architec-
ture that incorporates four major steps: (1) question
analysis, (2) document retrieval, (3) answer extrac-
tion and (4) answer selection. Question analysis is
a process which analyzes a question and produces a
list of keywords. Document retrieval is a step that
searches for relevant documents or passages. An-
swer extraction extracts a list of answer candidates
from the retrieved documents. Answer selection is a
process which pinpoints correct answer(s) from the
extracted candidate answers.
Since the first three steps in the QA pipeline may
produce erroneous outputs, the final answer selec-
tion step often entails identifying correct answer(s)
amongst many incorrect ones. For example, given
the question ?Which Chinese city has the largest
number of foreign financial companies??, the an-
swer extraction component produces a ranked list of
five answer candidates: Beijing (AP880603-0268)1,
Hong Kong (WSJ920110-0013), Shanghai (FBIS3-
58), Taiwan (FT942-2016) and Shanghai (FBIS3-
45320). Due to imprecision in answer extraction,
an incorrect answer (?Beijing?) can be ranked in
the first position, and the correct answer (?Shang-
hai?) was extracted from two different documents
and ranked in the third and the fifth positions. In or-
der to rank ?Shanghai? in the top position, we have
to address two interesting challenges:
? Answer Similarity. How do we exploit simi-
larity among answer candidates? For example,
when the candidates list contains redundant an-
swers (e.g., ?Shanghai? as above) or several an-
swers which represent a single instance (e.g.
?U.S.A.? and ?the United States?), how much
should we boost the rank of the redundant an-
swers?
? Answer Relevance. How do we identify
relevant answer(s) amongst irrelevant ones?
This task may involve searching for evi-
dence of a relationship between the answer
1Answer candidates are shown with the identifier of the
TREC document where they were found.
784
and the answer type or a question key-
word. For example, we might wish to query
a knowledge base to determine if ?Shang-
hai? is a city (IS-A(Shanghai, city)),
or to determine if Shanghai is in China
(IS-IN(Shanghai, China)).
The first challenge is to exploit redundancy in the
set of answer candidates. As answer candidates are
extracted from different documents, they may con-
tain identical, similar or complementary text snip-
pets. For example, ?U.S.? can appear as ?United
States? or ?USA? in different documents. It is im-
portant to detect redundant information and boost
answer confidence, especially for list questions that
require a set of unique answers. One approach is
to perform answer clustering (Nyberg et al, 2002;
Jijkoun et al, 2006). However, the use of cluster-
ing raises additional questions: how to calculate the
score of the clustered answers, and how to select the
cluster label.
To address the second question, several answer
selection approaches have used external knowledge
resources such as WordNet, CYC and gazetteers for
answer validation or answer reranking. Answer can-
didates are either removed or discounted if they are
not of the expected answer type (Xu et al, 2002;
Moldovan et al, 2003; Chu-Carroll et al, 2003;
Echihabi et al, 2004). The Web also has been used
for answer reranking by exploiting search engine re-
sults produced by queries containing the answer can-
didate and question keywords (Magnini et al, 2002).
This approach has been used in various languages
for answer validation. Wikipedia?s structured in-
formation was used for Spanish answer type check-
ing (Buscaldi and Rosso, 2006).
Although many QA systems have incorporated in-
dividual features and/or resources for answer selec-
tion in a single language, there has been little re-
search on a generalized probabilistic framework that
supports answer ranking in multiple languages using
any answer relevance and answer similarity features
that are appropriate for the language in question.
In this paper, we describe a probabilistic answer
ranking framework for multiple languages. The
framework uses logistic regression to estimate the
probability that an answer candidate is correct given
multiple answer relevance features and answer sim-
ilarity features. An existing framework which was
originally developed for English (Ko et al, 2007)
was extended for Chinese and Japanese answer
ranking by incorporating language-specific features.
Empirical results on NTCIR Chinese and Japanese
factoid questions show that the framework signifi-
cantly improved answer selection performance; Chi-
nese performance improved by 40% over the base-
line, and Japanese performance improved by 45%
over the baseline.
The remainder of this paper is organized as fol-
lows: Section 2 contains an overview of the answer
ranking task. Section 3 summarizes the answer rank-
ing framework. In Section 4, we explain how we
extended the framework by incorporating language-
specific features. Section 5 describes the experimen-
tal methodology and results. Finally, Section 6 con-
cludes with suggestions for future research.
2 Answer Ranking Task
The relevance of an answer to a question can be es-
timated by the probability P(correct(Ai) |Ai, Q),
where Q is a question and Ai is an answer can-
didate. To exploit answer similarity, we estimate
the probability P (correct(Ai) |Ai, Aj), where Aj
is similar to Ai. Since both probabilities influence
overall answer ranking performance, it is important
to combine them in a unified framework and es-
timate the probability of an answer candidate as:
P (correct(Ai)|Q,A1, ..., An).
The estimated probability is used to rank answer
candidates and select final answers from the list. For
factoid questions, the top answer is selected as a fi-
nal answer to the question. In addition, we can use
the estimated probability to classify incorrect an-
swers: if the probability of an answer candidate is
lower than 0.5, it is considered to be a wrong answer
and is filtered out of the answer list. This is useful
in deciding whether or not a valid answer to a ques-
tion exists in a given corpus (Voorhees, 2002). The
estimated probability can also be used in conjunc-
tion with a cutoff threshold when selecting multiple
answers to list questions.
3 Answer Ranking Framework
This section summarizes our answer ranking frame-
work, originally developed for English answers (Ko
785
P (correct(Ai)|Q,A1, ..., An)
? P (correct(Ai)|rel1(Ai), ..., relK1(Ai), sim1(Ai), ..., simK2(Ai))
=
exp(?0 +
K1?
k=1
?krelk(Ai) +
K2?
k=1
?ksimk(Ai))
1 + exp(?0 +
K1?
k=1
?krelk(Ai) +
K2?
k=1
?ksimk(Ai))
where, simk(Ai) =
N?
j=1(j 6=i)
sim?k(Ai, Aj).
Figure 1: Estimating correctness of an answer candidate given a question and a set of answer candidates
et al, 2007). The model uses logistic regression
to estimate the probability of an answer candidate
(Figure 1). Each relk(Ai) is a feature function used
to produce an answer relevance score for an an-
swer candidate Ai. Each sim?k(Ai, Aj) is a similar-
ity function used to calculate an answer similarity
between Ai and Aj . K1 and K2 are the number of
answer relevance and answer similarity features, re-
spectively. N is the number of answer candidates.
To incorporate multiple similarity features, each
simk(Ai) is obtained from an individual similarity
metric, sim?k(Ai, Aj). For example, if Levenshtein
distance is used as one similarity metric, simk(Ai)
is calculated by summing N-1 Levenshtein distances
between one answer candidate and all other candi-
dates.
The parameters ?, ?, ? were estimated from train-
ing data by maximizing the log likelihood. We used
the Quasi-Newton algorithm (Minka, 2003) for pa-
rameter estimation.
Multiple features were used to generate answer
relevance scores and answer similarity scores; these
are discussed below.
3.1 Answer Relevance Features
Answer relevance features can be classified into
knowledge-based features or data-driven features.
1) Knowledge-based features
Gazetteers: Gazetteers provide geographic infor-
mation, which allows us to identify strings as in-
stances of countries, their cities, continents, capitals,
etc. For answer ranking, three gazetteer resources
were used: the Tipster Gazetteer, the CIA World
Factbook and information about the US states pro-
vided by 50states.com. These resources were used
to assign an answer relevance score between -1 and
1 to each candidate. For example, given the question
?Which city in China has the largest number of for-
eign financial companies??, the candidate ?Shang-
hai? receives a score of 0.5 because it is a city in the
gazetteers. But ?Taiwan? receives a score of -1.0 be-
cause it is not a city in the gazetteers. A score of 0
means the gazetteers did not contribute to the answer
selection process for that candidate.
Ontology: Ontologies such as WordNet contain
information about relationships between words and
general meaning types (synsets, semantic categories,
etc.). WordNet was used to identify answer rele-
vance in a manner analogous to the use of gazetteers.
For example, given the question ?Who wrote the
book ?Song of Solomon???, the candidate ?Mark
Twain? receives a score of 0.5 because its hyper-
nyms include ?writer?.
2) Data-driven features
Wikipedia: Wikipedia was used to generate an an-
swer relevance score. If there is a Wikipedia docu-
ment whose title matches an answer candidate, the
document is analyzed to obtain the term frequency
(tf) and the inverse term frequency (idf) of the can-
didate, from which a tf.idf score is calculated. When
there is no matched document, each question key-
word is also processed as a back-off strategy, and the
answer relevance score is calculated by summing the
tf.idf scores obtained from individual keywords.
Google: Following Magnini et al (2002), a query
consisting of an answer candidate and question key-
786
words was sent to the Google search engine. Then
the top 10 text snippets returned by Google were
analyzed to generate an answer relevance score by
computing the minimum number of words between
a keyword and the answer candidate.
3.2 Answer Similarity Features
Answer similarity is calculated using multiple string
distance metrics and a list of synonyms.
String Distance Metrics: String distance metrics
such as Levenshtein, Jaro-Winkler, and Cosine sim-
ilarity were used to calculate the similarity between
two English answer candidates.
Synonyms: Synonyms can be used as another
metric to calculate answer similarity. If one answer
is synonym of another answer, the score is 1. Other-
wise the score is 0. To get a list of synonyms, three
knowledge bases were used: WordNet, Wikipedia
and the CIA World Factbook. In addition, manually
generated rules were used to obtain synonyms for
different types of answer candidates. For example,
?April 12 1914? and ?12th Apr. 1914? are converted
into ?1914-04-12? and treated as synonyms.
4 Extensions for Multiple Languages
We extended the framework for Chinese and
Japanese QA. This section details how we incor-
porated language-specific resources into the frame-
work. As logistic regression is based on a proba-
bilistic framework, the model does not need to be
changed to support other languages. We only re-
trained the model for individual languages. To sup-
port Chinese and Japanese QA, we incorporated new
features for individual languages.
4.1 Answer Relevance Features
We replaced the English gazetteers and WordNet
with language-specific resources for Japanese and
Chinese. As Wikipedia and the Web support mul-
tiple languages, the same algorithm was used in
searching language-specific corpora for the two lan-
guages.
1) Knowledge-based features
The knowledge-based features involve searching for
facts in a knowledge base such as gazetteers and
WordNet. We utilized comparable resources for
Chinese and Japanese. Using language-specific re-
#Articles
Language Nov. 2005 Aug. 2006
English 1,811,554 3,583,699
Japanese 201,703 446,122
Chinese 69,936 197,447
Table 1: Articles in Wikipedia for different lan-
guages
sources, the same algorithms were applied to gener-
ate an answer relevance score between -1 and 1.
Gazetteers: There are few available gazetteers
for Chinese and Japanese. Therefore, we extracted
location data from language-specific resources. For
Japanese, we extracted Japanese location informa-
tion from Yahoo2, which contains many location
names in Japan and the relationships among them.
For Chinese, we extracted location names from the
Web. In addition, we translated country names pro-
vided by the CIA World Factbook and the Tipster
gazetteers into Chinese and Japanese names. As
there is more than one translation, top 3 translations
were used.
Ontology: For Chinese, we used HowNet (Dong,
2000) which is a Chinese version of WordNet.
It contains 65,000 Chinese concepts and 75,000
corresponding English equivalents. For Japanese,
we used semantic classes provided by Gengo
GoiTaikei3. Gengo GoiTaikei is a Japanese lexicon
containing 300,000 Japanese words with their asso-
ciated 3,000 semantic classes. The semantic infor-
mation provided by HowNet and Gengo GoiTaikei
was used to assign an answer relevance score be-
tween -1 and 1.
2) Data-driven features
Wikipedia: As Wikipedia supports more than 200
language editions, the approach used in English can
be used for different languages without any modifi-
cation. Table 1 shows the number of text articles in
three different languages. Wikipedia?s current cov-
erage in Japanese and Chinese does not match its
coverage in English, but coverage in these languages
continues to improve.
To supplement the small corpus of Chi-
nese documents available, we used Baidu
2http://map.yahoo.co.jp/
3http://www.kecl.ntt.co.jp/mtg/resources/GoiTaikei
787
(http://baike.baidu.com), which is similar to
Wikipedia but contains more articles written in
Chinese. We first search for Chinese Wikipedia.
When there is no matching document in Wikipedia,
each answer candidate is sent to Baidu and the
retrieved document is analyzed in the same way to
analyze Wikipedia documents.
The idf score was calculated using word statis-
tics from Japanese Yomiuri newspaper corpus and
the NTCIR Chinese corpus.
Google: The same algorithm was applied to ana-
lyze Japanese and Chinese snippets returned from
Google. But we restricted the language to Chi-
nese or Japanese so that Google returned only Chi-
nese or Japanese documents. To calculate the dis-
tance between an answer candidate and question
keywords, segmentation was done with linguistic
tools. For Japanese, Chasen4 was used. For Chinese
segmentation, a maximum-entropy based parser was
used (Wang et al, 2006).
3) Manual Filtering
Other than the features mentioned above, we man-
ually created many rules for numeric and temporal
questions to filter out invalid answers. For example,
when the question is looking for a year as an answer,
an answer candidate which contains only the month
receives a score of -1. Otherwise, the score is 0.
4.2 Answer Similarity Features
The same features used for English were applied
to calculate the similarity of Chinese/Japanese an-
swer candidates. To identify synonyms, Wikipedia
were used for both Chinese and Japanese. EIJIRO
dictionary was used to obtain Japanese synonyms.
EIJIRO is a English-Japanese dictionary contain-
ing 1,576,138 words and provides synonyms for
Japanese words.
As there are several different ways to represent
temporal and numeric expressions (Nyberg et al,
2002; Greenwood, 2006), language-specific conver-
sion rules were applied to convert them into a canon-
ical format; for example, a rule to convert Japanese
Kanji characters to Arabic numbers is shown in Fig-
ure 2.
4http://chasen.aist-nara.ac.jp/hiki/ChaSen
0.25
??
??
1993-
07-04
1993 
? 7 ?
4 ?
50 %
??
1993-
07-04
??
??
?
??
??
3E+11
 ?
3,000
??
3E+11
 ?
??
 ? ?
Norm
alized
 answ
er stri
ng
Origin
al ans
wer st
ring
Figure 2: Example of normalized answer strings
5 Experiments
This section describes the experiments to evaluate
the extended answer ranking framework for Chinese
and Japanese QA.
5.1 Experimental Setup
We used Chinese and Japanese questions provided
by the NTCIR (NII Test Collection for IR Sys-
tems), which focuses on evaluating cross-lingual
and monolingual QA tasks for Chinese, Japanese
and English. For Chinese, a total of 550 fac-
toid questions from the NTCIR5-6 QA evaluations
served as the dataset. Among them, 200 questions
were used to train the Chinese answer extractor and
350 questions were used to evaluate our answer
ranking framework. For Japanese, 700 questions
from the NTCIR5-6 QA evaluations served as the
dataset. Among them, 300 questions were used to
train the Japanese answer extractor and 400 ques-
tions were used to evaluate our framework.
Both the Chinese and Japanese answer extractors
use maximum-entropy to extract answer candidates
based on multiple features such as named entity, de-
pendency structures and some language-dependent
features.
Performance of the answer ranking framework
was measured by average answer accuracy: the
number of correct top answers divided by the num-
ber of questions where at least one correct answer
exists in the candidate list provided by an extrac-
tor. Mean Reciprocal Rank (MRR5) was also used
to calculate the average reciprocal rank of the first
correct answer in the top 5 answers.
The baseline for average answer accuracy was
calculated using the answer candidate likelihood
scores provided by each individual extractor; the
788
TO
P1
TO
P3
MR
R5
0.00.10.20.30.40.50.60.70.80.91.0
Ja
pa
ne
se
 A
ns
we
r S
ele
cti
on
 
 B
as
eli
ne
 Fr
am
ew
ork
TO
P1
TO
P3
MR
R5
0.00.10.20.30.40.50.60.70.80.91.0
Ch
ine
se
 A
ns
we
r S
ele
cti
on
 
Avgerage Accuracy
 B
as
eli
ne
 Fr
am
ew
ork
Figure 3: Performance of the answer ranking framework for Chinese and Japanese answer selection (TOP1:
average accuracy of top answer, TOP3: average accuracy of top 3 answers, MRR5: average of mean recip-
rocal rank of top 5 answers)
answer with the best extractor score was chosen,
and no validation or similarity processing was per-
formed.
3-fold cross-validation was performed, and we
used a version of Wikipedia downloaded in Aug
2006.
5.2 Results and Analysis
We first analyzed the average accuracy of top 1, top3
and top 5 answers. Figure 3 compares the average
accuracy using the baseline and the answer selec-
tion framework. As can be seen, the answer rank-
ing framework significantly improved performance
on both Chinese and Japanese answer selection. As
for the average top answer accuracy, there were 40%
improvement over the baseline (Chinese) and 45%
improvement over the baseline (Japanese).
We also analyzed the degree to which the average
accuracy was affected by answer similarity and rel-
evance features. Table 2 compares the average top
answer accuracy using the baseline, the answer rel-
evance features, the answer similarity features and
all feature combinations. Both the similarity and the
relevance features significantly improved answer se-
lection performance compared to the baseline, and
combining both sets of features together produced
the best performance.
We further analyzed the utility of individual rele-
vance features (Figure 4). For both languages, filter-
ing was useful in ruling out wrong answers. The im-
Baseline Rel Sim All
Chinese 0.442 0.482 0.597 0.619
Japanese 0.367 0.463 0.502 0.532
Table 2: Average top answer accuracy of individ-
ual features (Rel: merging relevance features, Sim:
merging similarity features, ALL: merging all fea-
tures).
pact of the ontology was more positive for Japanese;
we assume that this is because the Chinese ontol-
ogy (HowNet) contains much less information over-
all than the Japanese ontology (Gengo GoiTaikei).
The comparative impact of Wikipedia was similar.
For Chinese, there were many fewer Wikipedia doc-
uments available. Even though we used Baidu as a
supplemental resource for Chinese, this did not im-
prove answer selection performance. On the other
hand, the use of Wikipedia was very helpful for
Japanese, improving performance by 26% over the
baseline. This shows that the quality of answer
relevance estimation is significantly affected by re-
source coverage.
When comparing the data-driven features with the
knowledge-based features, the data-driven features
(such as Wikipedia and Google) tended to increase
performance more than the knowledge-based fea-
tures (such as gazetteers and WordNet).
Table 3 shows the effect of individual similar-
ity features on Chinese and Japanese answer selec-
789
Ba
se
lin
eF
IL
ON
T
GA
Z
GL
W
IK
I
All
0.3
0
0.3
5
0.4
0
0.4
5
0.5
0
0.5
5
Avg. Top Answer Accuracy
 C
hin
es
e
 Ja
pa
ne
se
Figure 4: Average top answer accuracy of individ-
ual answer relevance features.(FIL: filtering, ONT,
ontology, GAZ: gazetteers, GL: Google, WIKI:
Wikipedia, ALL: combination of all relevance fea-
tures)
Chinese Japanese
0.3 0.5 0.3 0.5
Cosine 0.597 0.597 0.488 0.488
Jaro-Winkler 0.544 0.518 0.410 0.415
Levenshtein 0.558 0.544 0.434 0.449
Synonyms 0.527 0.527 0.493 0.493
All 0.588 0.580 0.502 0.488
Table 3: Average accuracy using individual similar-
ity features under different thresholds: 0.3 and 0.5
(?All?: combination of all similarity metrics)
tion. As some string similarity features (e.g., Lev-
enshtein distance) produce a number between 0 and
1 (where 1 means two strings are identical and 0
means they are different), similarity scores less than
a threshold can be ignored. We used two thresh-
olds: 0.3 and 0.5. In our experiments, using 0.3
as a threshold produced better results in Chinese.
In Japanese, 0,5 was a better threshold for individ-
ual features. Among three different string similar-
ity features (Levenshtein, Jaro-Winkler and Cosine
similarity), cosine similarity tended to perform bet-
ter than the others.
When comparing synonym features with string
similarity features, synonyms performed better than
string similarity in Japanese, but not in Chinese. We
had many more synonyms available for Japanese
Data-driven features All features
Chinese 0.606 0.619
Japanese 0.517 0.532
Table 4: Average top answer accuracy when using
data-driven features v.s. when using all features.
and they helped the system to better exploit answer
redundancy.
We also analyzed answer selection performance
when combining all four similarity features (?All?
in Table 3). Combining all similarity features im-
proved the performance in Japanese, but hurt the
performance in Chinese, because adding a small set
of synonyms to the string metrics worsened the per-
formance of logistic regression.
5.3 Utility of data-driven features
In our experiments we used data-driven fea-
tures as well as knowledge-based features. As
knowledge-based features need manual effort to ac-
cess language-specific resources for individual lan-
guages, we conducted an additional experiment only
with data-driven features in order to see how much
performance gain is available without the manual
work. As Google, Wikipedia and string similarity
metrics can be used without any additional manual
effort when extended to other languages, we used
these three features and compared the performance.
Table 4 shows the performance when using data-
driven features v.s. all features. It can be seen that
data-driven features alone achieved significant im-
provement over the baseline. This indicates that the
framework can easily be extended to any language
where appropriate data resources are available, even
if knowledge-based features and resources for the
language are still under development.
6 Conclusion
In this paper, we presented a generalized answer se-
lection framework which was applied to Chinese and
Japanese question answering. An empirical evalu-
ation using NTCIR test questions showed that the
framework significantly improves baseline answer
selection performance. For Chinese, the perfor-
mance improved by 40% over the baseline. For
Japanese, the performance improved by 45% over
790
the baseline. This shows that our probabilistic
framework can be easily extended for multiple lan-
guages by reusing data-driven features (with new
corpora) and adding language-specific resources
(ontologies, gazetteers) for knowledge-based fea-
tures.
In our previous work, we evaluated the perfor-
mance of the framework for English QA using ques-
tions from past TREC evaluations (Ko et al, 2007).
The experimental results showed that the combina-
tion of all answer ranking features improved per-
formance by an average of 102% over the baseline.
The relevance features improved performance by an
average of 99% over the baseline, and the similar-
ity features improved performance by an average of
46% over the baseline. Our hypothesis is that answer
relevance features had a greater impact for English
QA because the quality and coverage of the data re-
sources available for English answer validation is
much higher than the quality and coverage of ex-
isting resources for Japanese and Chinese. In future
work, we will continue to evaluate the robustness of
the framework. It is also clear from our comparison
with English QA that more work can and should be
done in acquiring data resources for answer valida-
tion in Chinese and Japanese.
Acknowledgments
We would like to thank Hideki Shima, Mengqiu
Wang, Frank Lin, Justin Betteridge, Matthew
Bilotti, Andrew Schlaikjer and Luo Si for their valu-
able support. This work was supported in part
by ARDA/DTO AQUAINT program award number
NBCHC040164.
References
D. Buscaldi and P. Rosso. 2006. Mining Knowledge
from Wikipedia for the Question Answering task. In
Proceedings of the International Conference on Lan-
guage Resources and Evaluation.
J. Chu-Carroll, J. Prager, C. Welty, K. Czuba, and D. Fer-
rucci. 2003. A Multi-Strategy and Multi-Source Ap-
proach to Question Answering. In Proceedings of Text
REtrieval Conference.
C. Clarke, G. Cormack, and T. Lynam. 2001. Exploiting
redundancy in question answering. In Proceedings of
SIGIR.
Zhendong Dong. 2000. Hownet:
http://www.keenage.com.
A. Echihabi, U. Hermjakob, E. Hovy, D. Marcu, E. Melz,
and D. Ravichandran. 2004. How to select an answer
string? In T. Strzalkowski and S. Harabagiu, editors,
Advances in Textual Question Answering. Kluwer.
Mark A. Greenwood. 2006. Open-Domain Question An-
swering. Thesis.
S. Harabagiu, D. Moldovan, M. Pasca, R. Mihalcea,
M. Surdeanu, R. Bunsecu, R. Girju, V. Rus, and
P. Morarescu. 2000. Falcon: Boosting knowledge for
answer engines. In Proceedings of TREC.
V. Jijkoun, J. van Rantwijk, D. Ahn, E. Tjong Kim Sang,
and M. de Rijke. 2006. The University of Amsterdam
at CLEF@QA 2006. In Working Notes CLEF.
J. Ko, L. Si, and E. Nyberg. 2007. A Probabilistic Frame-
work for Answer Selection in Question Answering. In
Proceedings of NAACL/HLT.
B. Magnini, M. Negri, R. Pervete, and H. Tanev. 2002.
Comparing statistical and content-based techniques for
answer validation on the web. In Proceedings of the
VIII Convegno AI*IA.
T. Minka. 2003. A Comparison of Numerical Optimizers
for Logistic Regression. Unpublished draft.
D. Moldovan, D. Clark, S. Harabagiu, and S. Maiorano.
2003. Cogex: A logic prover for question answering.
In Proceedings of HLT-NAACL.
E. Nyberg, T. Mitamura, J. Carbonell, J. Callan,
K. Collins-Thompson, K. Czuba, M. Duggan,
L. Hiyakumoto, N. Hu, Y. Huang, J. Ko, L. Lita,
S. Murtagh, V. Pedro, and D. Svoboda. 2002. The
JAVELIN Question-Answering System at TREC 2002.
In Proceedings of Text REtrieval Conference.
J. Prager, E. Brown, A. Coden, and D. Radev. 2000.
Question answering by predictive annotation. In Pro-
ceedings of SIGIR.
E. Voorhees. 2002. Overview of the TREC 2002 ques-
tion answering track. In Proceedings of Text REtrieval
Conference.
M. Wang, K. Sagae, and T. Mitamura. 2006. A Fast, Ac-
curate Deterministic Parser for Chinese. In Proceed-
ings of COLING/ACL.
J. Xu, A. Licuanan, J. May, S. Miller, and R. Weischedel.
2002. TREC 2002 QA at BBN: Answer Selection and
Confidence Estimation. In Proceedings of Text RE-
trieval Conference.
791
