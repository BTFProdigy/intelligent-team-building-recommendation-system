Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014), pages 785?789,
Dublin, Ireland, August 23-24, 2014.
UoW: NLP Techniques Developed at the University of Wolverhampton for
Semantic Similarity and Textual Entailment
Rohit Gupta, Hanna B
?
echara, Ismail El Maarouf and Constantin Or
?
asan
Research Group in Computational Linguistics,
Research Institute of Information and Language Processing,
University of Wolverhampton, UK
{R.Gupta, Hanna.Bechara, I.El-Maarouf, C.Orasan}@wlv.ac.uk
Abstract
This paper presents the system submit-
ted by University of Wolverhampton for
SemEval-2014 task 1. We proposed a ma-
chine learning approach which is based
on features extracted using Typed Depen-
dencies, Paraphrasing, Machine Transla-
tion evaluation metrics, Quality Estima-
tion metrics and Corpus Pattern Analysis.
Our system performed satisfactorily and
obtained 0.711 Pearson correlation for the
semantic relatedness task and 78.52% ac-
curacy for the textual entailment task.
1 Introduction
The SemEval task 1 (Marelli et al., 2014a) in-
volves two subtasks: predicting the degree of re-
latedness between two sentences and detecting the
entailment relation holding between them. The
task uses SICK dataset (Marelli et al., 2014b),
consisting of 10000 pairs, each annotated with re-
latedness in meaning and entailment relationship
holding between them. Similarity measures be-
tween sentences are required in a wide variety of
NLP applications. In applications like Informa-
tion Retrieval (IR), measuring similarity is a vi-
tal step in order to determine the best result for
a related query. Other applications such as Para-
phrasing and Translation Memory (TM) rely on
similarity measures to weight results. However,
computing semantic similarity between sentences
is a complex and difficult task, due to the fact that
the same meaning can be expressed in a variety of
ways. For this reason it is necessary to have more
than a surface-form comparison.
We present a method based on machine learning
which exploits available NLP technology. Our ap-
This work is licensed under a Creative Commons At-
tribution 4.0 International Licence. Page numbers and pro-
ceedings footer are added by the organisers. Licence details:
http://creativecommons.org/licenses/by/4.0/
proach relies on features inspired by deep seman-
tics (such as parsing and paraphrasing), machine
translation quality estimation, machine translation
evaluation and Corpus Pattern Analysis (CPA
1
).
We use the same features to measure both se-
mantic relatedness and textual entailment. Our hy-
pothesis is that each feature covers a particular as-
pect of implicit similarity and entailment informa-
tion contained within the pair of sentences. Train-
ing is performed in a regression framework for se-
mantic relatedness and in a classification frame-
work for textual entailment.
The remainder of the paper is structured as fol-
lows. In Section 2, we review the work related
to our study and the existing NLP technologies
used to measure sentence similarity. In Sections 3
and 4, we describe our approach and the similarity
measures we used. In Section 5, we present the re-
sults and an analysis of our runs based on the test
and training data provided by the SemEval-2014
task. Finally, our work is summed up in Section 6
with perspectives for future work we would like to
explore.
2 Related Work
The areas of semantic relatedness and entailment
have received extensive interest from the research
community in the last decade. Earlier work in
relatedness (Banerjee and Pedersen, 2003; Li et
al., 2006) exploited WordNet in various ways to
extract the semantic relatedness. Banerjee and
Pedersen (2003) presented a measure using ex-
tended gloss overlap. This measure takes two
WordNet synsets as input and uses the overlap
of their WordNet glosses to compute their degree
of semantic relatedness. Li et al. (2006) pre-
sented a semantic similarity metric based on the
semantic similarity of words in a sentence. Re-
cently, Wang and Cer (2012) presented an ap-
1
http://pdev.org.uk
785
proach that uses probabilistic edit-distance to mea-
sure semantic similarity. The approach uses prob-
abilistic finite state and pushdown automata to
model weighted edit-distance where state transi-
tions correspond to edit-operations. In some as-
pects, our work is similar to B?ar et al. (2012),
who presented an approach which combines var-
ious text similarity measures using a log-linear re-
gression model.
Entailment has been modelled using various ap-
proaches. The main approaches are based on
logic inferencing (Moldovan et al., 2003), ma-
chine learning (Hickl et al., 2006; Castillo, 2010)
and tree edit-distance (Kouylekov and Magnini,
2005). Most of the recent approaches employ var-
ious syntactic or tree edit models (Heilman and
Smith, 2010; Mai et al., 2011; Rios and Gelbukh,
2012; Alabbas and Ramsay, 2013). Recently, Al-
abbas and Ramsay (2013) presented a modified
tree edit distance approach, which extends tree
edit distance to the level of subtrees. The ap-
proach extends Zhang-Shasha?s algorithm (Zhang
and Shasha, 1989).
3 Features
Our system uses the same 31 features for both sub-
tasks. This section explains them and the code
which implements most of them can be found on
GitHub
2
.
3.1 Language Technology Features
We used existing language processing tools to ex-
tract features. Stanford CoreNLP
3
toolkit provides
lemma, parts of speech (POS), named entities, de-
pendencies relations of words in each sentence.
We calculated Jaccard similarity on surface
form, lemma, dependencies relations, POS and
named entities to get the feature values. The Jac-
card similarity computes sentence similarity by di-
viding the overlap of words on the total number of
words of both sentences.
Sim(s1, s2) =
|s1 ? s2|
|s1 ? s2|
(1)
where in equation (1), Sim(s1, s2) is the Jaccard
similarity between sets of words s1 and s2.
We used the same toolkit to identify corefer-
ence relations and determine clusters of corefer-
ential entities. The coreference feature value was
2
https://github.com/rohitguptacs/wlvsimilarity
3
http://nlp.stanford.edu/software/corenlp.shtml
calculated using clusters of coreferential entities.
The intuition is that sentences containing corefer-
ential entities should have some semantic related-
ness. In order to extract clusters of coreferential
entities, the pair of sentences was treated as a doc-
ument. The coreference feature value using these
clusters was calculated as follows:
V alue
coref
=
CC
TC
(2)
where CC is the number of clusters formed by the
participation of entities (at least one entity from
each sentence of the pair) in both sentences and
TC is the total number of clusters.
We calculated two separate feature values for
dependency relations: the first feature concate-
nated the words involved in a dependency relation
and the second used grammatical relation tags. For
example, for the sentence pair ?the kids are play-
ing outdoors? and ?the students are playing out-
doors? the Jaccard similarity is calculated based
on concatenated words ?kids::the, playing::kids,
playing::are, ROOT::playing, playing::outdoors?
and ?students::the, playing::students, playing::are,
ROOT::playing, playing::outdoors? to get the
value for the first feature and ?det, nsubj, aux, root,
dobj? and ?det, nsubj, aux, root, dobj? to get the
value for the second feature.
These language technology features try to cap-
ture the token based similarity and grammatical
similarity between a pair of sentences.
3.2 Paraphrasing Features
We used the PPDB paraphrase database (Ganitke-
vitch et al., 2013) to get the paraphrases. We used
lexical and phrasal paraphrases of ?L? size. For
each sentence of the pair, we created two sets of
bags of n-grams (1 ? n ? length of the sentence).
We extended each set with paraphrases for each n-
gram available from paraphrase database. We then
calculated the Jaccard similarity (see Section 3.1)
between these extended bag of n-grams to get the
feature value. This feature capture the cases where
one sentence is a paraphrase of the other.
3.3 Negation Feature
Our system does not attempt to model similar-
ity with negation, but since negation is an impor-
tant feature for contradiction in textual entailment,
we designed a non-similarity feature. The system
checks for the presence of a negation word such as
?no?, ?never? and ?not? in the pair of sentences and
786
returns ?1? (?0? otherwise) if both or none of the
sentences contain any of these words.
3.4 Machine Translation Quality Estimation
Features
Seventeen of the features consist of Machine
Translation Quality Estimation (QE) features,
based on the work of (Specia et al., 2009) and used
as a baseline in recent QE tasks (such as (Callison-
Burch et al., 2012)). We extracted these features
by treating the first set of sentences as the Machine
Translation (MT) ?source?, and the second set of
sentences as the MT ?target?. In Machine Trans-
lation, these features are used to access the quality
of MT ?target?. The QE features include shallow
surface features such as the number of punctua-
tion marks, the average length of words, the num-
ber of words. Furthermore, these features include
n-gram frequencies and language model probabil-
ities. A full list of the QE features is provided in
the documentation of the QE system
4
(Specia et
al., 2009).
QE features relate to well-formedness and syn-
tax, and are not usually used to compute seman-
tic relatedness between sentences. We have used
them in the hope that the surface features at least
will show us some structural similarity between
sentences.
3.5 Machine Translation Evaluation Features
Additionally, we used BLEU (Papineni et al.,
2002), a very popular machine translation evalu-
ation metric, as a feature. BLEU is based on n-
gram counts. It is meant to capture the similarity
between translated text and references for machine
translation evaluation. The BLEU score over sur-
face, lemma and POS was calculated to get three
feature values. In a pair of sentences, one side was
treated as a translation and another as a reference.
We applied it at the sentence level to capture the
similarity between two sentences.
3.6 Corpus Pattern Analysis Features
Corpus Pattern Analysis (CPA) (Hanks, 2013) is
a procedure in corpus linguistics that associates
word meaning with word use by means of seman-
tic patterns. CPA is a new technique for map-
ping meaning onto words in text. It is currently
being used to build a ?Pattern Dictionary of En-
glish Verbs?(PDEV
5
). It is based on the Theory of
4
https://github.com/lspecia/quest
5
http://pdev.org.uk
Norms and Exploitations (Hanks, 2013).
There are two features extracted from PDEV.
They both make use of a derived resource called
the CPA network (Bradbury and El Maarouf,
2013). The CPA network links verbs according
to similar semantic patterns (e.g. both ?pour? and
?trickle? share an intransitive use where the subject
is ?liquid?).
The first feature value compares the main verbs
in both sentences. When both verbs share a pat-
tern, the system returns a value of ?1? (otherwise
?0?). The second feature extends the CPA network
to compute the probability of a PDEV pattern,
given a word. This probability is computed over
the portion of the British National Corpus which is
manually tagged with PDEV patterns. The prob-
ability of a pattern given each word of a sentence
of the dataset is obtained by the product of those
probabilities. The feature value is the (normalised)
number of common patterns from the three most
probable patterns in each sentence. These features
try to capture similarity based on semantic pat-
terns.
4 Predicting Through Machine Learning
4.1 Model Description
We used a support vector machine in order to build
a regression model to predict semantic relatedness
and a classification model to predict textual entail-
ment. For the actual implementation we used Lib-
SVM
6
(Chang and Lin, 2011).
We used a regression model for the related-
ness task that estimates a continuous score be-
tween 1 and 5 for each sentence. For the entail-
ment task, we trained a classification model which
assigns one of three different labels (ENTAIL-
MENT, CONTRADICTION, NEUTRAL) to each
sentence pair. We trained both systems on the
4500 sentence training set, augmented with the
500 sentence trial data. The values of C and ?
have been optimised through a grid-search which
uses a 5-fold cross-validation method.
The RBF kernel proved to be the best for both
tasks.
5 Results and Analysis
We submitted 4 runs of our system (Run-1 to Run-
4). Run-1 was submitted as primary run. Run-2,
Run-3 and Run-4 systems were identical except
6
http://www.csie.ntu.edu.tw/ cjlin/libsvm/
787
Run-1 Run-2 Run-3 Run-4
C 8 8 2 2
? 0.0441 0.0441 0.125 0.125
Pearson 0.7111 0.7166 0.6968 0.6975
Table 1: Results: Relatedness.
for some parameter differences for SVM train-
ing and the replacement of the values which were
outside the boundaries (1-5). If relatedness val-
ues predicted by the system were less than 1 or
greater than 5, these values were replaced by 1
and 5 respectively for Run-1, Run-2 and Run-4
and 1.5 and 4.5 respectively for Run-3. Our pri-
mary run also used one extra feature for related-
ness, which was obtained by considering entail-
ment judgement as a feature. Our hypothesis was
that entailment judgement may help in measur-
ing relatedness. In the actual test this feature was
not helpful and we obtained Pearson correlation of
0.711 for the primary run, compared to 0.716 for
Run-2. The details of runs are given in Table 1 and
2.
After training both models, we ran a feature
selection algorithm to determine which features
yielded the highest accuracy, and therefore had the
highest impact on our system. Perhaps unsurpris-
ingly, the QE features were not very useful in pre-
dicting semantic similarity or entailment. How-
ever, despite their focus on fluency rather than se-
mantic correctness, the QE features still managed
to contribute to some improvements in the textual
entailment task (increasing accuracy by 1%), and
the semantic relatedness task (0.027 increase in
Pearson correlation).
In the entailment (classification) task, the
strongest feature proved to be the negation fea-
ture with 70% accuracy (on the training set) when
training on this feature only. This suggests that
some measure of negation is crucial in determin-
ing whether a sentence contradicts or entails an-
other sentence. Other strong features were lemma,
paraphrasing and dependencies.
In the relatedness (regression) task, the lemma,
surface, paraphrasing, dependencies, PDEV fea-
tures were the strongest contributors to accuracy.
Run-1 Run-2 Run-3 Run-4
C 16 16 8 8
? 0.0625 0.0625 0.5 0.5
Accuracy 78.526 78.526 78.343 78.343
Table 2: Results: Entailment.
6 Conclusion and Future Work
We have presented an efficient approach to calcu-
late semantic relatedness and textual entailment.
One noticeable point of our approach is that we
have used the same features for both tasks and
our system performed well in each of these tasks.
Therefore, our system captures reasonably good
models to compute semantic relatedness and tex-
tual entailment.
In the future we would like to explore more fea-
tures and particularly those based on tree edit dis-
tance, WordNet and PDEV. Our intuition suggests
that tree edit distance seems to be more helpful for
entailment, whereas WordNet and PDEV seem to
be more helpful for similarity measurement. Ad-
ditionally, we would like to combine our tech-
niques for measuring relatedness and entailment
with MT evaluation techniques. We would fur-
ther like to apply these techniques cross-lingually,
moving into other areas like machine translation
evaluation and quality estimation.
Acknowledgement
The research leading to these results has received
funding from the People Programme (Marie Curie
Actions) of the European Union?s Seventh Frame-
work Programme FP7/2007-2013/ under REA
grant agreement no. 317471 and partly supported
by an AHRC grant ?Disambiguating Verbs by Col-
location project, AH/J005940/1, 2012-2015?.
References
Maytham Alabbas and Allan Ramsay. 2013. Natural
language inference for Arabic using extended tree
edit distance with subtrees. Journal of Artificial In-
telligence Research, 48:1?22.
Satanjeev Banerjee and Ted Pedersen. 2003. Extended
gloss overlaps as a measure of semantic relatedness.
In IJCAI, volume 3, pages 805?810.
Daniel B?ar, Chris Biemann, Iryna Gurevych, and
Torsten Zesch. 2012. Ukp: Computing seman-
tic textual similarity by combining multiple content
similarity measures. In First Joint Conference on
788
Lexical and Computational Semantics, Association
for Computational Linguistics, pages 435?440.
Jane Bradbury and Isma??l El Maarouf. 2013. An
empirical classification of verbs based on Semantic
Types: the case of the ?poison? verbs. In Proceed-
ings of the Joint Symposium on Semantic Process-
ing. Textual Inference and Structures in Corpora,
pages 70?74.
Chris Callison-Burch, Philipp Koehn, Christof Monz,
Matt Post, Radu Soricut, and Lucia Specia, editors.
2012. Proceedings of the Seventh Workshop on Sta-
tistical Machine Translation. Association for Com-
putational Linguistics, Montr?eal, Canada, June.
Julio J. Castillo. 2010. Recognizing textual en-
tailment: experiments with machine learning al-
gorithms and RTE corpora. Special issue: Natu-
ral Language Processings and its Applications, Re-
search in Computing Science, 46:155?164.
Chih-Chung Chang and Chih-Jen Lin. 2011. LIB-
SVM: A library for support vector machines. ACM
Transactions on Intelligent Systems and Technology,
2:27:1?27:27.
Juri Ganitkevitch, Van Durme Benjamin, and Chris
Callison-Burch. 2013. Ppdb: The paraphrase
database. In Proceedings of NAACL-HLT, pages
758?764, Atlanta, Georgia.
Patrick Hanks. 2013. Lexical Analysis: Norms and
Exploitations. Mit Press.
Michael Heilman and Noah A. Smith. 2010. Tree edit
models for recognizing textual entailments, para-
phrases, and answers to questions. In The 2010 An-
nual Conference of the North American Chapter of
the ACL, number June, pages 1011?1019.
Andrew Hickl, Jeremy Bensley, John Williams, Kirk
Roberts, Bryan Rink, and Ying Shi. 2006. Rec-
ognizing textual entailment with LCC?s GROUND-
HOG system. In Proceedings of the Second PAS-
CAL Challenges Workshop.
Milen Kouylekov and Bernardo Magnini. 2005. Rec-
ognizing textual entailment with tree edit distance
algorithms. In Proceedings of the First Challenge
Workshop Recognising Textual Entailment, pages
17?20.
Yuhua Li, David McLean, Zuhair A Bandar, James D
O?shea, and Keeley Crockett. 2006. Sentence sim-
ilarity based on semantic nets and corpus statistics.
Knowledge and Data Engineering, IEEE Transac-
tions on, 18(8):1138?1150.
Zhewei Mai, Y Zhang, and Donghong Ji. 2011. Rec-
ognizing text entailment via syntactic tree match-
ing. In Proceedings of NTCIR-9 Workshop Meeting,
pages 361?364, Tokyo, Japan.
Marco Marelli, Luisa Bentivogli, Marco Baroni, Raf-
faella Bernardi, Stefano Menini, and Roberto Zam-
parelli. 2014a. Semeval-2014 task 1: Evaluation of
compositional distributional semantic models on full
sentences through semantic relatedness and textual
entailment. In Proceedings of the 8th International
Workshop on Semantic Evaluation (SemEval-2014).
Marco Marelli, Stefano Menini, Marco Baroni, Luisa
Bentivogli, Raffaella Bernardi, and Roberto Zam-
parelli. 2014b. A sick cure for the evaluation of
compositional distributional semantic models. In
Proceedings of LREC 2014.
Dan Moldovan, Christine Clark, Sanda Harabagiu, and
Steve Maiorano. 2003. COGEX : A Logic Prover
for Question Answering. In Proceedings of HLT-
NAACL, number June, pages 87?93.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: a method for automatic eval-
uation of machine translation. In Proceedings of the
ACL, pages 311?318.
Miguel Rios and Alexander Gelbukh. 2012. Recog-
nizing Textual Entailment with a Semantic Edit Dis-
tance Metric. In 11th Mexican International Confer-
ence on Artificial Intelligence, pages 15?20. IEEE.
Lucia Specia, Marco Turchi, Nicola Cancedda, Marc
Dymetman, and Nello Cristianini. 2009. Estimat-
ing the sentence-level quality of machine translation
systems. In 13th Conference of the European Asso-
ciation for Machine Translation, pages 28?37.
Mengqiu Wang and Daniel Cer. 2012. Stanford: prob-
abilistic edit distance metrics for STS. In Proceed-
ings of the First Joint Conference on Lexical and
Computational Semantics, pages 648?654.
Kaizhong Zhang and Dennis Shasha. 1989. Simple
Fast Algorithms for the Editing Distance between
Trees and Related Problems. SIAM Journal on Com-
puting, 18(6):1245?1262.
789
Proceedings of the Second Workshop on Hybrid Approaches to Translation, pages 34?41,
Sofia, Bulgaria, August 8, 2013. c?2013 Association for Computational Linguistics
Reordering rules for English-Hindi SMT 
 
 
Raj Nath Patel, Rohit Gupta, Prakash B. Pimpale and Sasikumar M 
CDAC Mumbai, Gulmohar Cross Road No. 9, 
Juhu, Mumbai-400049 
India 
{rajnathp,rohitg,prakash,sasi}@cdac.in 
 
  
Abstract 
Reordering is pre-processing stage for Statisti-
cal Machine Translation (SMT) system where 
the words of the source sentence are re-
ordered as per the syntax of the target lan-
guage. We are proposing a rich set of rules for 
better reordering. The idea is to facilitate the 
training process by better alignments and par-
allel phrase extraction for a phrase based SMT 
system.  Reordering also helps the decoding 
process and hence improving the machine 
translation quality. We have observed signifi-
cant improvements in the translation quality 
by using our approach over the baseline SMT. 
We have used BLEU, NIST, multi-reference 
word error rate, multi-reference position inde-
pendent error rate for judging the improve-
ments. We have exploited open source SMT 
toolkit MOSES to develop the system.   
1 Introduction 
This paper describes syntactic reordering rules to 
reorder English sentences as per the Hindi lan-
guage structure. Generally in reordering ap-
proach, the source sentence is parsed(E) and syn-
tactic reordering rules are applied to form reor-
dered sentence(E`). The training of SMT system 
is performed using parallel corpus having source 
side reordered(E`) and target side. The decoding 
is done by supplying reordered source sentences. 
The source sentences prior to decoding are reor-
dered using the same syntactic rules as applied 
for the training data. So, this process works as a 
preprocessing stage for the phrase-based SMT 
system. It has been observed that reordering as a 
pre-processing stage is beneficial for developing 
English-Hindi phrase based SMT system (Rama-
nathan et al, 2008; Rama et al, 2008). This pa-
per describes a rich set of rules for the structural 
transformation of English sentence to Hindi lan-
guage structure using Stanford (De et al, 2006) 
parse tree on source side. These rules are manu-
ally extracted based on analysis of source sen-
tence tree and Hindi translation. 
For the evaluation purpose we have trained 
and evaluated three different phrase based SMT 
systems using MOSES toolkit (Koehn et al 
2007) and GIZA++(Och and Ney, 2003). The 
first system was non-reordered baseline (Brown 
et al, 1990; Marcu and Wong, 2002; Koehn et 
al., 2003), second using limited reordering de-
scribed in Ramanathan et al (2008) and third 
using improved reordering technique proposed in 
the paper. Evaluation has been carried out for 
end to end English-Hindi translation outputs us-
ing BLEU score (Papineni et al, 2001), NIST 
score (Doddington, 2002), multi-reference posi-
tion-independent word error rate (Tillmann et al, 
1997), multi-reference word error rate (Nie?en et 
al., 2000). We have observed improvement in 
each of these evaluation metrics used. Next sec-
tion discusses related work. Section 3 describes 
our reordering approach followed by experi-
ments and results in section 4 and conclusion in 
section 5. 
2 Related Work 
Various pre-processing approaches have been 
proposed for handling syntax within SMT sys-
tems. These proposed methods reconcile the 
word-order differences between the source and 
target language sentences by reordering the 
source prior to the SMT training and decoding 
stages. For English-Hindi statistical machine 
translation reordering approach is used by Ra-
manathan et al (2008) and Rama et al (2008). 
This approach (Ramanathan et al 2008) has 
shown significant improvements over baseline 
(Brown et al, 1990; Marcu and Wong, 2002; 
Koehn et al, 2003). The BLEU score for the sys-
tem has increased from 12.10 to 16.90 after reor-
dering. The same reordering approach (Rama-
nathan et al, 2008) used by us has shown slight 
improvement in BLEU score of 0.64 over base-
line i.e. BLEU score increased from 21.55 to 
34
22.19 compare to +4.8 BLEU point increase in 
the previous case. The reason can be, when the 
system is able to get bigger chunks from the 
phrase table itself the local reordering (within 
phrase) is not needed and the long distance reor-
dering employed in the earlier approach will be 
helpful for overall better translation. It may not 
be able to show significant improvements when 
local reordering is not captured by the translation 
model.    
Other language pairs have also shown signifi-
cant improvement when reordering is employed. 
Xia and Mc-Cord (2004) have observed im-
provement for French-English and Chao et al 
(2007) for Chinese-English language pairs. 
Nie?en and Ney (2004) have proposed sentence 
restructuring whereas Collins et al (2005) have 
proposed clause restructuring to improve Ger-
man-English SMT. Popovic and Ney (2006) 
have also reported the use of simple local trans-
formation rules for Spanish-English and Serbian-
English translation. 
Recently, Khalilov and Fonollosa (2011) pro-
posed a reordering technique using deterministic 
approach for long distance reordering and non-
deterministic approach for short distance reorder-
ing exploiting morphological information. Some 
reordering approaches are also presented exploit-
ing the SMT itself (Gupta et al, 2012; Dlougach 
and Galinskaya, 2012).     
Various evaluation techniques are available 
for reordering and overall machine translation 
evaluation. Particularly for reordering Birch and 
Osborne (2010) have proposed LRScore, a lan-
guage independent metric for evaluating the lexi-
cal and word reordering quality. The translation 
evaluation metrics include BLEU (Papineni et. 
al., 2002), Meteor (Lavie and Denkowski, 2009), 
NIST (Doddington, 2002), etc. 
3 Reordering approach  
Our reordering approach is based on syntactic 
transformation of the English sentence parse tree 
according to the target language (Hindi) struc-
ture. It is similar to Ramanathan et al (2008) but 
the transformation rules are not restricted to 
?SVO to SOV? and ?pre-modifier to post-
modifier? transformations only.  
The idea was to come up with generic syntac-
tic transformation rules to match the target lan-
guage grammatical structure. The motivation 
came from the fact that if words are already in a 
correct place with respect to other words in the 
sentence, system doesn?t need to do the extra 
work of reordering at the decoding time. This 
problem becomes even more complicated when 
system doesn?t able to get bigger phrases for 
translating a sentence. Assuming an 18 words 
sentence, if system is able to get only 2 word 
length phrases, there are 362880(9!) translations 
(permutations) possible (still ignoring the case 
where one phrase having more than one transla-
tion options) for a sentence.  
The source and the target sentences are manu-
ally analyzed to derive the tree transformation 
rules. From the generated set of rules we have 
selected rules which seemed to be more generic. 
There are cases where we have found more than 
one possible correct transformations for an Eng-
lish sentence as the target language (Hindi) is a 
free word order language within certain limits. In 
such cases word order close to English structure 
is preferred over possible word orders with re-
spect to Hindi. 
We identified 5 categories which are most 
prominent candidates for reordering. These in-
clude VPs (verb phrases), NPs (noun phrases), 
ADJPs (adjective phrase), PPs (preposition 
phrase) and ADVPs (adverb phrase). In the fol-
lowing subsections, we have described rules for 
these in more detail. 
 
Tag Description(Penn tags) 
dcP   Any, parser generated phrase 
pp  Prepositional Phrase(PP) 
whP WH Phrase(WHNP, 
WHADVP, WHADJP, WHPP) 
vp Verb Phrase(VP) 
sbar Subordinate clause(SBAR) 
np Noun phrase(NP) 
vpw Verb words(VBN, VBP, VB,VBG, 
MD, VBZ, VBD) 
prep Preposition words(IN, 
TO,VBN,VBG) 
adv Adverbial words(RB, RBR, RBS) 
adj Adjunct word(JJ,JJR,JJS) 
advP Adverb phrase(ADVP) 
punct Punctuation(,) 
adjP Adjective phrase(ADJP) 
OP  advP, np and/or pp 
Tag* One or more occurrences of Tag 
Tag? Zero or one occurrence of Tag 
 
Table 1: Tag description 
 
The format for writing the rules is as follows: 
Type_of_phrase(tag1 tag2 tag 3: tag2 tag1 tag3) 
35
This means that ?tag1 tag2 tag3?, structure 
has been transformed to ?tag2 tag1 tag3? for the 
type_of_phrase. This type_of_phrase denotes our 
category (NP, VP, ADJP, ADVP, PP) in which 
rule fall. The table given above explains about 
various tags and corresponding Penn tags used in 
writing these rules. 
The following subsections explain the reorder-
ing rules. The higher precedence rule is written 
prior to the lower precedence. In general the 
more specific rules have high precedence. Each 
rule is followed by an example with intermediate 
steps of parsing and transformation as per the 
Hindi sentence structure. ?Partial Reordered? 
shows the effect of the particular rule whereas 
?Reordered? shows impact of the whole reorder-
ing approach. The Hindi (transliterated) sentence 
is also provided as a reference for the corre-
sponding English sentence.  
3.1 Noun Phrase Rules 
NP (np1 PP[ prep NP[ np2 sbar]] : np2 prep 
np1 sbar)            (1) 
English: The time of the year when nature 
dawns all its colorful splendor, is beautiful. 
Parse: [NP (np1 the time) [PP (prep of) [NP 
(np2 the year) (sbar when nature dawns all its 
colorful splendor)]]] , is beautiful . 
Partial Reordered: (np2 the year) (prep of) 
(np1 the time) (sbar when nature dawns all its 
colorful splendor) , is beautiful . 
Reordered: (np2 the year) (prep of) (np1 the 
time) (sbar when nature all its colorful splendor 
dawns) , beautiful is . 
Hindi: varsh ka samay jab prakriti apne sabhi 
rang-birange vabahv failati hai, sundar hai . 
 
NP(np SBAR[ S[ dcP ]] :dcP  np)        (2) 
English: September to march is the best sea-
son to visit Udaipur. 
Parse: September to March is [NP (np the 
best season) [SBAR [S (dcP to visit Udaipur)]]] . 
Partial Reordered: September to March is 
(dcP to visit Udaipur) (np the best season) .  
Reordered: September to March (dcP Udai-
pur visit to) (np the best season) is .  
Hindi: september se march udaipur ghumane 
ka sabse achcha samay hai . 
 
NP(np punct advP : advP punct np)                (3) 
English: The modern town of Mumbai,  
about 50 km south of Navi Mumbai is Khar-
ghar. 
Parse: The modern town of [NP (np Mumbai) 
(punct ,) (advP about 50 km south of Navi 
Mumbai)] is Kharghar . 
Partial Reordered: (advP about 50 km 
south of Navi Mumbai)) (punct ,) (dcP The 
modern town of Mumbai) is kharghar . 
Reordered: (advP Navi Mumbai of about 50 
km south) (punct ,) (dcP Mumbai of the modern 
town) kharghar is . 
Hindi: navi mumbai ke 50 km dakshin me 
mumbai ka adhunic sahar kharghar hai . 
 
NP( np  vp : vp np)                                           (4) 
English: The main attraction is a divine tree 
called as 'Kalptaru'. 
Parse: The main attraction is [NP (np a divine 
tree) (vp called as 'Kalptaru') ] . 
Partial Reordered: The main attraction is (vp 
` called as 'Kalptaru') (np a divine tree) . 
Reordered: The main attraction (vp ` Kalptaru 
' as called) (np a divine tree) is . 
Hindi: iska mukhya akarshan kalptaru namak 
ek divya vriksh hai . 
3.2 Verb Phrase Rules 
VP( vpw PP [ prep NP[ np  punct? SBAR[ whP 
dcP ]]] : np prep vpw punct? whP dcP)          (5) 
English: The best time to visit is in the after-
noon when the crowd thins out. 
Parse: The best time to visit [VP (vpw is) PP[ 
(prep in) NP[ (np the afternoon) [SBAR (whP 
when) (dcP the crowd thins out)]]] . 
Partial Reordered: The best time to visit (np 
the afternoon) (prep in) (vpw is) (whP when) 
(dcP the crowd thins out) .  
Reordered: visit to The best time (np the af-
ternoon) (prep in) (vpw is) (whP when) (dcP the 
crowd thins out) .  
Hindi: bhraman karane ka sabase achcha 
samay dopahar me hai jab bhid kam ho jati hai . 
 
VP( vpw NP[ np punct? SBAR[ whP dcP ]] : np 
vpw punct? whP dcP)                                       (6) 
English: Jaswant Thada is a white marble 
monument which was built in 1899 in the 
memory of Maharaja Jaswant Singh II. 
Parse: jaswant thada [VP (vpw is) [NP (np a 
white marble monument) [SBAR (whP which) 
(dcP was built in 1899 in the memory of Maha-
raja Jaswant Singh II)]] . 
Partial Reordered: Jaswant Thada (np a 
white marble monument) (vpw is) (whP which) 
(dcP was built in 1899 in the memory of Maha-
raja Jaswant Singh II) .  
36
Reordered: Jaswant Thada (np a white mar-
ble monument) (vpw is) (whP which) (dcP Ma-
haraja Jaswant Singh II of the memory in 1899 in 
built was) .  
Hindi: jaswant thada ek safed sangmarmar ka 
smarak hai jo ki maharaja jaswant singh dwitiya 
ki yad me 1889 me banwaya gaya tha . 
 
VP(vpw OP sbar : OP vpw sbar )        (7) 
English: Temples in Bhubaneshwar are built 
beautifully on a common plan as prescribed by 
Hindu norms.  
Parse: Temples in Bhubaneshwar are [VP 
(vpw built) (advP beautifully) (pp on a common 
plan) (sbar as prescribed by Hindu norms)] . 
Partial Reordered: Bhubaneshwar in Tem-
ples are (advP beautifully) (pp a common plan 
on) (vpw built) (sbar as prescribed by Hindu 
norms) . 
Reordered: Bhubaneshwar in Temples (advP 
beautifully) (pp a common plan on) (vpw built) 
are (sbar as Hindu norms by prescribed) . 
Hindi: bhubaneswar ke mandir hindu niya-
mon dwara nirdharit samanya yojana ke anusar 
banaye gaye hain . 
 
VP(vpw pp1 pp*2: pp*2 pp1 vpw)                  (8) 
English: Avalanche is located at a distance 
of 28 Kms from Ooty. 
Parse: Avalanche is [VP (vpw located) (pp1 at 
a distance of 28 kms) (pp2 from Ooty)] . 
Partial Reordered: Avalanche is (pp2 from 
Ooty) (pp1 at a distance of 28 kms) (vpw locat-
ed) . 
Reordered: Avalanche (pp2 Ooty from ) (pp1 
28 kms of a distance at) (vpw located) is . 
Hindi: avalanche ooty se 28 km ki duri par 
sthit hai . 
 
VP(vpw np pp : np pp vbw)         (9) 
English: Taxis and city buses available out-
side the station, facilitate access to the city. 
Parse: Taxis and city buses available outside 
the station , [VP (vpw facilitate) (np access) (pp 
to the city)] . 
Partial Reordered: Taxis and city buses 
available outside the station , (pp to the city) (np 
access) (vpw facilitate) .  
Reordered: Taxis and city buses the station 
outside available , (pp the city to) (np access) 
(vpw facilitate) .  
Hindi: station ke baahar sahar jane  ke liye 
taksi aur bus ki suvidha upalabdha hai . 
 
VP ( prep dcP : dcP prep)        (10) 
English: A wall was built to protect it. 
Parse: A wall was built [VP (prep to) (dcP 
protect it)] . 
Partial Reordered: A wall was built (protect 
it) (prep to) .  
Reordered: A wall (dcP it protect) (prep to) 
built was .  
Hindi: ek diwar ise surakshit karane ke liye 
banayi gayi thi . 
 
VP(adv vpw dcphrase: dcphrase adv vpw)    (11) 
English: Modern artist such as French sculp-
tor Bartholdi is best known by his famous 
work. 
Parse: Modern artists such as French sculptor 
Bartholdi is [VP (adv best) (vpw known) (dcP by 
his famous work)] . 
Partial Reordered: Modern artists such as 
French sculptor Bartholdi is (dcP by his famous 
work) (adv best) (vpw known) . 
Reordered: such as French sculptor Bartholdi 
Modern artists (dcP his famous work by) (adv 
best) (vpw known) is . 
Hindi: french shilpkar bartholdi jaise aa-
dhunik kalakar apane prashidha kam ke liye 
vishesh rup se jane jate hain . 
 
VP(advP vpw dcP: advP dcP vpw)           (12) 
English: Bikaner, popularly known as the 
camel county is located in Rajasthan. 
Parse: Bikaner , [VP (advP popularly) (vpw 
known) (dcP as the camel country)] is located in 
Rajsthan . 
Partial Reordered: Bikaner , (advP popular-
ly) (dcP as the camel country) (vpw known) is 
located in Rajsthan . 
Reordered: Bikaner , (advP popularly) (dcP 
the camel country as) (vpw known) Rajsthan in 
located is . 
Hindi: bikaner , jo aam taur par unton ke 
desh ke naam se jana jata hai, rajasthan me sthit 
hai .  
 
VP(vpw adv? adjP? dcP: dcP adjP? adv? vpw) 
           (13) 
English: This palace has been beautiful from 
many years.  
Parse: This palace has [VP (vpw been) (adjP 
beautiful) (dcP from many years)] . 
Partial Reordered: This palace has (dcP 
from many years) (adjP beautiful) (vpw been) . 
Reordered: This palace (dcP many years 
from) (adjP beautiful) (vpw been) has .  
Hindi: yah mahal kai varson se sunder raha 
hai . 
37
3.3 Adjective and Adverb Phrase Rules 
 
ADJP( vpw pp : pp vpw )        (14) 
English: The temple is decorated with paint-
ings depicting incidents. 
Parse: The temple is [ADJP (vpw decorated) 
(pp with paintings depicting incidents )] . 
Partial Reordered: The temple is (pp with 
paintings depicting incidents) (vpw decorat-
ed) . 
Reordered: The temple (pp incidents depict-
ing paintings with) (vpw decorated) is . 
Hindi: mandir ghatnao ko darshate hue chit-
ron se sajaya gya hai . 
 
ADJP( adjP pp : pp adjP )        (15) 
English: As a resul, temperatures are now 
higher than ever before . 
Parse: As a result , temperatures are now 
[ADJP (adjP higher) (pp than ever)] before . 
Partial Reordered: As a result , temperatures 
are now (pp than ever) (adj higher) before . 
Reordered: a result As , temperatures now 
before (pp ever than) (adj higher) are . 
Hindi: parinam swarup taapman ab pahle se 
bhi adhik hai . 
 
ADJP( adj dcP : dcP adj )        (16) 
English: The Kanha National park is open to 
visitors. 
Parse: The Kanha National park is [ADJP 
(adj open) (dcP to visitors)] . 
Partial Reordered: The Kanha National park 
is (pp to visitors ) (adj open)  . 
Reordered: The Kanha National park (pp vis-
itors to) (adj open) is . 
Hindi: kanha national park paryatakon ke liye 
khula hai . 
 
ADVP( adv dcP: dcP adv )        (17) 
English: The temple is most favored spot for 
tourists apart from the pilgrims. 
Parse: The temple is most favored spot for 
tourists [ADVP (adv apart) (dcP from the pil-
grims)] . 
Partial Reordered: The temple is most fa-
vored spot for tourists (dcP from the pilgrims ) 
(adv apart)  . 
Reordered: The temple most favored spot 
(dcP the pilgrims from) (adv apart) is . 
Hindi: mandir teerth yatriyon ke alawa par-
yatkon ke liye bhi lokpriya sthal hai . 
3.4 Preposition Phrase Rules 
PP( adv prep? dcP : dcP prep? adv )       (18) 
English: Does kalajar occur because of sun? 
Parse: Does kalajar occur [PP (adv because) 
(prep? of) (dcP sun)] ? 
Partial Reordered: Does kalajar occur (dcp 
sun) (prep? of) (adv because) ? 
Reordered: Does kalajar (dcp sun) (prep? of) 
(adv because) occur? 
Hindi: kya kalajar dhup ke karan hota hai ? 
 
 
input Ahmedabad was named after the sultan Ahmed Shah, who built the city in 1411. 
baseline ahmedabad was named after the sultan ahmed shah, who built the city in 1411. 
???????? ?? ??? ?? ??? ??? ??????? ???? shah, ???? ??? 1411. 
ahamdabad ke nam par rakha gaya sultan ahamad shah, wale shahar 1411.    
limited re-
ordering 
ahmedabad the sultan ahmed shah , who the city 1411 in built after named was . 
???????? ?? ??? ??????? ??????? ?? , ????? ???? ??? ??? ?????? ?? ??? ?? 
??? ??? ?? ?  
ahamdabad ka nam sultan ahamadshah ke , jisane 1411 me shahar banawaya ke 
nam par rakha gaya tha . 
our ap-
proach 
ahmedabad the sultan ahmed shah after named was , who 1411 in the city built . 
???????? ?? ??? ??????? ??????? ?? ??? ?? ??? ?? ????? ???? ??? ??? 
?????? ?? ?  
ahamadabad ka nam sultan ahamadshah ke nam se pada tha jisane 1411 me sha-
har banawaya tha . 
reference ???????? ?? ??? ??????? ??????? ?? ??? ?? ??? ??, ????? ???? ??? ??? 
?????? ?? ? 
ahamadabad ka nam sultan ahamadshah ke nam par pada tha jisane 1411 me 
shahar banawaya tha . 
Table 2: Comparison of translation on a sentence from test corpus
38
4 Experiments and Results 
The experiments were carried out on the corpus 
described in Table 3 below. 
 
 #Sentences #Words 
Training 94926 1235163 
Tuning 1446 23600 
Test 500 9792 
 
Table 3: Corpus distribution 
 
The baseline system was setup by using the 
phrase-based model (Brown et al, 1990; Marcu 
and Wong, 2002; Koehn et al, 2003). For the 
language model, we carried out experiments and 
found on comparison that 5-gram model with 
modified Kneser-Ney smoothing (Chen and 
Goodman, 1998) to be the best performing. Tar-
get Hindi corpus from the training set was used 
for creating the language model. The KenLM 
(Heafield., 2011) toolkit was used for the lan-
guage modeling experiments. The tuning corpus 
was used to set weights for the language models, 
distortion model, phrase translation model etc. 
using minimum error rate training (Och, 2003). 
Decoding was performed using the MOSES de-
coder. Stanford constituency parser (De et al, 
2006) was used for parsing. 
Table 2 above describes with the help of an 
example how the reordering and hence the trans-
lation quality has improved. From the example it 
can be seen that the translation by system using 
our approach is better than the other two sys-
tems. The output translation is structurally more 
correct in our approach and conveys the same 
meaning with respect to the reference translation. 
 
phra
se-
lengt
h 
#phrases #distinct-phrases(distinct on source) 
baseline limited re-
ordering/ 
%IOBL/ 
IOBL 
our approach/ 
%IOBL/  
IOBL 
baseline limited re-
ordering/ 
%IOBL/ 
IOBL 
our approach/ 
%IOBL/ 
IOBL 
2 537017 579878/ 
7.98/ 
42861 
579630/ 
9.98/ 
42613 
208988 249847/ 
19.55/ 
40859 
254393/ 
21.72/ 
45405 
3 504810 590265/ 
16.92/ 
85455 
616381/ 
22.10/ 
111571 
292183 384518/ 
31.62/ 
92335 
408240/ 
39.72/ 
116057 
4 406069 493637/ 
21.56/ 
87568 
531904/ 
30.98/ 
125835 
268431 372282/ 
38.68/ 
103851 
409966/ 
52.72/ 
141535 
5 313368 391490/ 
24.92/ 
78122 
431135/ 
37.58/ 
117766 
221228 313723/ 
41.80/ 
92495 
354273/ 
60.13/ 
133045 
6  231146 292899/ 
26.71/ 
61753 
327192/ 
41.55/ 
96046 
170852 244643/ 
43.19/ 
73791 
279723/ 
63.72/ 
108871 
7 154800 196679/ 
27.05/ 
41879 
220868/ 
42.67/ 
66068 
119628 170108/ 
42.19/ 
50480 
194881/ 
62.90/ 
75253 
 
Table 4: Phrase count analysis 
 
The Table 5 below lists four different evalua-
tions of the systems under study. For BLEU and 
NIST higher score is considered as better and for 
mWER and mPER  lower score is desirable. Ta-
ble 5 shows the results of comparative evaluation 
of baseline, limited reordering and our approach 
with improved reordering. We find that addition 
of more reordering rules show substantial im-
provements over the baseline phrase based sys-
tem and the limited reordering system (Rama-
nathan et al, 2008). The impact of improved 
syntactic reordering can be seen as the BLEU 
and NIST scores have increased whereas mWER 
and mPER scores have decreased. 
 
  
39
 BLEU NIST mWER 
% 
mPER 
% 
baseline 21.55 5.72 68.08 45.54 
limited 
reordering 
22.19 5.74 66.44 44.70 
our      
approach 
24.47 5.88 64.71 43.89 
 
Table 5: Evaluation scores 
 
Table 4 above shows the count of overall 
phrases and distinct phrases (distinct on source) 
for baseline, limited reordering approach and our 
improved reordering approach. The table also 
shows increase over baseline (IOBL) and per-
centage increase over baseline(%IOBL) for lim-
ited reordering and improved reordering. We 
have observed that no. of distinct phrases ex-
tracted from the training corpus get increased. 
The %IOBL for bigger phrases is more compare 
to shorter phrases. This can be attributed to the 
better alignments resulting in extraction of more 
phrases (Koehn et al, 2003).  
We have also observed that the overall in-
crease is even lesser than the increase in no. of 
distinct phrases (distinct on source) for all the 
phrase-lengths in our approach (e.g. 42613 and 
45405 for phrase-length 2) which shows that re-
ordering makes word alignments more consistent 
and reduces multiple entries for the same source 
phrase. The training was done on maximum 
phrase-length 7(default).   
5 Conclusion  
It can be seen that addition of more reordering 
rules improve translation quality. As of now we 
have tried these rules only for English-Hindi 
pair, but the plan is to employ similar reordering 
rules in other English-Indian language pairs as 
most Indian languages are structurally similar to 
Hindi. Also plans are there to go for comparative 
study of improved reordering system and hierar-
chical model. 
References  
Alexandra Birch , Miles Osborne and Phil Blunsom. 
2010. Metrics for MT evaluation: evaluating reor-
dering. Machine Translation 24, no. 1: 15-26. 
Peter  F. Brown, John Cocke, Stephen A. Della Pietra, 
Vincent J. Della Pietra, Fredrick Jelinek, John D. 
Lafferty, Robert L. Mercer, and Paul S. Roossin. 
1990. A statistical approach to machine translation. 
Computational linguistics 16(2): 79?85. 
Wang Chao, Michael Collins, and Philipp Koehn. 
2007. Chinese syntactic reordering for statistical 
machine translation. In Proceedings of the 2007 
Joint Conference on Empirical Methods in 
Natural Language Processing and Computa-
tional Natural Language Learning (EMNLP-
CoNLL).  
Stanley F. Chen, Joshua Goodman. 1996. An empiri-
cal study of smoothing techniques for language 
modeling. In Proceedings of the 34th annual 
meeting on Association for Computational 
Linguistics. Association for Computational Lin-
guistics. 
Michael Collins, Philipp Koehn, and Ivona Ku?erov?. 
2005. Clause restructuring for statistical machine 
translation. In Proceedings of the 43rd Annual 
Meeting on Association for Computational 
Linguistics. 
Marneffe De, Marie-Catherine,  Bill MacCartney, and 
Christopher D. Manning. 2006. Generating typed 
dependency parses from phrase structure parses. In 
Proceedings of LREC, vol. 6, pp. 449-454. 
Jacob Dlougach and Irina Galinskaya. 2012. Building 
a reordering system using tree-to-string hierar-
chical model. In Proceedings of the First Work-
shop on Reordering for Statistical Machine 
Translation at COLING, Mumbai, India. 
George Doddington. 2002. Automatic evaluation of 
machine translation quality using n-gram co-
occurrence statistics. In Proceedings of the sec-
ond international conference on Human Lan-
guage Technology Research. Morgan Kaufmann 
Publishers Inc. 
Rohit Gupta, Raj N. Patel and Ritesh Shah. 2012. 
Learning Improved Reordering Models for Urdu, 
Farsi and Italian using SMT. In Proceedings of 
the first workshop on Reordering for Statisti-
cal Machine Translation, COLING 2012, 
Mumbai, India. 
Kenneth Heafield. 2011. KenLM: Faster and smaller 
language model queries. In Proceedings of the 
Sixth Workshop on Statistical Machine Trans-
lation, Association for Computational Linguis-
tics. 
Philipp Koehn, Franz Josef Och, and Daniel Marcu. 
2003. Statistical phrase-based translation. In Pro-
ceedings of the 2003 Conference of the North 
American  Chapter of the Association for 
Computational Linguistics on Human Lan-
guage Technology-Volume 1.  
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris 
Callison-Burch, Marcello Federico, Nicola Ber-
toldi, Brooke Cowan et al 2007. Moses: Open 
40
source toolkit for statistical machine translation. In 
Proceedings of the 45th Annual Meeting of the 
ACL on Interactive Poster and Demonstration Ses-
sions. 
Daniel Marcu, and William Wong. 2002. A phrase-
based, joint probability model for statistical ma-
chine translation. Proceedings of EMNLP. 
Sonja Nie?en, Franz J. Och, Gregor Leusch, and 
Hermann Ney. 2000. An Evaluation Tool for Ma-
chine Translation: Fast Evaluation for MT Re-
search. International Conference on Language Re-
sources and Evaluation. 
Franz J. Och, and Hermann Ney. 2003. A systematic 
comparison of various statistical alignment models. 
Computational linguistics, Volume 29, number 
1:19-51. 
Franz Josef Och. 2003.  Minimum error rate training 
in statistical machine translation. In Proceedings 
of the 41st Annual Meeting on Association for 
Computational Linguistics-Volume 1:pp. 160-
167. 
Kishore Papineni, Salim Roukos, Todd Ward, Wei-
Jing Zhu. 2001. BLEU: a Method for Automatic 
Evaluation of Machine Translation. IBM Re-
search Report, Thomas J. Watson Research 
Center. 
Taraka Rama, Karthik Gali and Avinesh PVS. 2008. 
Does Syntactic Knowledge help English-Hindi 
SMT ?. Proceedings of the NLP Tools contest, 
ICON. 
Ananthakrishnan Ramanathan, Pushpak 
Bhattacharyya, Jayprasad Hegde, Ritesh M. Shah, 
and M. Sasikumar. 2008. Simple syntactic and 
morphological processing can help English-Hindi 
statistical machine translation. In International 
Joint Conference on NLP (IJCNLP08). 
Christoph Tillmann, Stephan Vogel, Hermann Ney, 
Alex Zubiaga, and Hassan Sawaf. 1997. Accelerat-
ed DP based search for statistical translation. In 
European Conf. on Speech Communication 
and Technology. 
Fei Xia and Michael McCord. 2004. Improving a sta-
tistical MT system with automatically learned re-
write patterns.  In Proceedings of the 20th inter-
national conference on Computational Lin-
guistics, p. 508. Association for Computational 
Linguistics. 
 
 
41
