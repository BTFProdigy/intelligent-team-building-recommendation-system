BiFrameNet: Bilingual Frame Semantics Resource Construction by 
Cross-lingual Induction 
Pascale Fung and Benfeng Chen 
Human Language Technology Center,  
University of Science & Technology (HKUST), 
Clear Water Bay, Hong Kong 
{pascale,bfchen}@ee.ust.hk 
 
Abstract 
We present a novel automatic approach to 
constructing a bilingual semantic network?the 
BiFrameNet, to enhance statistical and 
transfer-based machine translation systems. 
BiFrameNet is a frame semantic representation, 
and contains semantic structure transfers 
between English and Chinese. The English 
FrameNet and the Chinese HowNet provide us 
with two different views of the semantic 
distribution of lexicon by linguists. We propose 
to induce the mapping between the English 
lexical entries in FrameNet to Chinese word 
senses in HowNet, furnishing a bilingual 
semantic lexicon which simulates the ?concept 
lexicon? supposedly used by human translators, 
and which can thus be beneficial to machine 
translation systems.  BiFrameNet alo contains 
bilingual example sentences that have the same 
semantic roles. We automatically induce 
Chinese example sentences and their semantic 
roles, based on semantic structure alignment 
from the first stage of our work, as well as 
shallow syntactic structure. In addition to its 
utility for machine-aided and machine 
translations, our work is also related to the 
spatial models proposed by cognitive scientists 
in the framework of artifactual simulations of 
the translation process. 
1. Introduction 
The merits of translation at the word level or 
the concept level have long been a cause for debate 
among linguists. Some linguists suggest that the two 
languages of a bilingual speaker share a common 
semantic system (Illes and Francis 1999; Ikeda 1998) 
and hence translation is carried out at the concept 
level.  
Meanwhile, there has been a gradual 
convergence of statistical and transfer approaches in 
machine translation recently (Wu 2003). Statistical 
MT systems are based on a stochastic mapping 
between lexical items, assuming the underlying 
semantic transfer is hidden. Transfer systems use 
explicit lexical, syntactic and semantic transfer rules. 
Consequently, cognitive scientists and 
computational linguists alike have been interested in 
the study of semantic mapping between languages 
(Ploux and Ji, 2003, Dorr et al, 2002, Ngai et al, 
2002, Boas 2002, Palmer and Wu, 1995). We 
propose to automatically construct a bilingual lexical 
semantic network with word sense and semantic role 
mapping between English and Chinese, simulating 
the ?concept lexicon?, suggested by cognitive 
scientists, of a bilingual person.  
 
 
  
Figure 1. BiFrameNet lexicon and example 
sentence induction 
 
The linguists-defined ontologies?-FrameNet 
(Baker et al, 1998), HowNet (Dong and Dong, 
2000), and bilingual dictionaries are the basis for the 
induction of the mapping. We automatically estimate 
the semantic transfer likelihoods between English 
FrameNet lexical entries and the Chinese word 
senses in HowNet, and align those frames and 
lexical pairs with high likelihood values. In addition, 
we propose to induce Chinese example sentences 
automatically to match English annotated sentences 
provided in the FrameNet. The BiFrameNet thus 
induced provides an additional resource for 
machine-aided or machine translation systems. It can 
also serve as a reference to be compared to cognitive 
studies of the translation process.  
: le x ic a l e n t ry  in  F ra m e N e t ; : c o n c e p t  in  H o w N e t  
:  F ra m e N e t  f r a m e ;               :H o w N e t  c a te g o ry
: lin k s  f ro m  F ra m e N e t  to  H o w N e t
:  l in k s  o f  th e  f ra m e  F ;      : t r a n s la t io n s  o f  
 i s  a  r a n k e d  li s t ;  
F
F H
L
L T
R
?
? ?
?
[ ]  m e a n s  th e  to p -  e le m e n t
:p o s s ib le   lin k e d  to  
: H o w N e t  c a te g o r ie s  r e la te d  to  f ra m e  
( ) :  b in a ry  fu n c t io n ,  r e tu rn  1  i f  in p u t  i s  t ru e ;
           o th e rw is e  r e tu rn  0 .
F o r  e a c h   
1 = { tr a n s la t io n
F
F
R k k
V F
F
x
T ?
? ?
?
?
?
?
W _ C G _ C P O S
s  o f   in  H o w N e t}
2 = { tra n s la t io n s  o f   in  d ic t io n a ry }
 1 2
{ ( , ) | . , . = . }
F o r  e a c h  
   = { |(u , ) ,  }
   F o r  e a c h    
       ( ) ( , )
    i s  th e  ra n k e d  l i s t  o f  H  s o
F
F
T
T T T
L L T
F
V L F
H
f H V v H
R
?
? ? ?
?
?
?
?
? ? ? ? ?
? ? ?
? ?
=
? ?
? ?
= ? ??
U
U
r t e d  b y  ( )  
   [1] [ 2 ] . . . [ ]
  { [ ] | ( [ ] , [ ] ) ,
            1, . . , 1, . .}
   { ( , ) | , }
F
F F
F F F
f H
R R R N
R k S im R l R k th r e s h o ld
l N k N
L F V? ? ? ?
? =
? ? ? >
= = +
= ? ? ?
U U
U
I
 Figure 2. BiFrameNet ontology induction  
 
Ploux and Ji, (2003) proposed a spatial model for 
matching semantic values between French and 
English. Palmer and Wu (1995) studied the mapping 
of change-of-state English verbs to Chinese. Dorr et 
al. (2002) described a technique for the construction 
of a Chinese-English verb lexicon based on HowNet 
and the English LCS Verb Database (LVD). They 
created links between HowNet concepts and LVD 
verb classes using both statistics and a manually 
constructed ?seed mapping? of thematic classes 
between HowNet and LVD. Ngai et al (2002) 
employed a word-vector based approach to create the 
alignment between WordNet and HowNet classes 
without any manual annotation. Boas (2002) outlined 
a number of issues surrounding the planning and 
design of GermanFrameNet (GFN), a bilingual 
FrameNet dictionary which, when complete, will 
have a corpus-based German lexicon following the 
FrameNet structure. 
2.1. FrameNet and HowNet 
The Berkeley FrameNet database consists of 
frame-semantic descriptions of more than 7000 
English lexical items, together with example 
sentences annotated with semantic roles (Baker et al, 
1998). There is currently no frame semantic 
representation of Chinese. However, the Chinese 
HowNet (Dong and Dong 2000) represents a 
hierarchical view of lexical semantics in Chinese.   
 This paper is organized as follows: Section 2 
describes the algorithm for estimating transfer 
relations between FrameNet and HowNet structures.  
Section 3 presents our method for selecting 
BiFrameNet example sentences for a particular 
frame and automatically inducing semantic role 
annotations. We conclude in Section 4, followed by 
a discussion in Section 5.  
FrameNet is a collection of lexical entries grouped 
by frame semantics. Each lexical entry represents an 
individual word sense, and is associated with 
semantic roles and some annotated sentences. 
Lexical entries with the same semantic roles are 
grouped into a ?frame? and the semantic roles are 
called ?frame elements?. For example: 
 
Frame: Cause_harm 
Frame Elements: agent, body_part, cause, event, 
instrument, iterations, purpose, reason, result, 
victim?.. 
Lexical Entries: 
bash.v, batter.v, bayonet.v, beat.v, belt.v, 
bludgeon.v, boil.v, break.v, bruise.v, buffet.v, 
burn.v,?. 
Example annotated sentence of lexical entry 
?beat.v?: 
[agent I] lay down on him and beat [victim at him] 
[means with my fists].  
2. Lexical semantic mapping in BiFrameNet  
Dorr et al (2002) uses a manual seed mapping of 
semantic roles between FrameNet and LVD to 
induce a bilingual verb lexicon. In this paper, we 
propose a method of automatically mapping the 
English FrameNet lexical entries to HowNet 
concepts, resulting in the BiFrameNet ontology. We 
also make use of two bilingual English-Chinese 
lexicons for this induction. In this section 2, we use 
an example FrameNet lexical entry ?beat.v? in the 
?cause_harm? frame to illustrate the main steps of 
our algorithm.  
HowNet is a Chinese ontology with a graph 
structure of word senses called ?concepts?, and each 
concept contains 7 fields including lexical entries in 
Chinese, English gloss, POS tags for the word in 
Chinese and English, and a definition of the concept 
including its category and semantic relations (Dong 
and Dong, 2000). For example, one translation for 
?beat.v? is ?: 
In this work, we make use of contextual lexical 
entries from the same semantic frame, as illustrated 
above. In this example, the ?cause_harm? frame 
contains two lexical entries??beat.v? and ?strike.v?. 
From the previous step, ?beat.v? and ?strike.v? is 
each linked to a number of Chinese candidates. 
?beat.v? is linked to ??? with membership in two 
different HowNet categories, namely ??|beat? and 
? ?? |associate?. To disambiguate between the 
above these 2 candidate categories, we make use of 
the other lexical entries in ?cause_harm?, in this case 
?strike.v? which is linked to ???,  in the ??|beat? 
HowNet category. Now, ?|beat? receives two votes 
(from ?? and from ??), and ?|associate? only 
one (from ??). We therefore choose the HowNet 
category ?|beat? to be aligned to the frame 
?cause_harm?, and eliminate the sense of ???in the 
? ? ? |associate? category.  Consequently, 
?beat.v? in ?cause_harm? is linked to all HowNet 
concepts that are translations of ?beat? which are 
verbs, and which also belong to the HowNet category 
?|beat? (but not ?|associate?).  
 
NO. = 17645 
W_C =? 
G_C =V 
E_C =~??~??~??~???~??~??~?? 
W_E=attack 
G_E=V 
E_E= 
DEF=fight|?? 
 
Whereas HowNet concepts correspond roughly to 
FrameNet lexical entries, its semantic relations do not 
correspond directly to FrameNet semantic roles. 
 
2.2. Initial mapping based on bilingual lexicon 
(step 1) 
We use the bilingual lexicon from HowNet and 
LDC dictionary to first create all possible mappings 
between FrameNet lexical entries and HowNet 
concepts whose part-of-speech (POS) tags are the 
same. Here we assume that syntactic classification 
for the majority of FrameNet lexical entries (i.e. 
verbs and adjectives) are semantically motivated and 
are mostly preserved across different languages. For 
example ?beat? can be translated into {?, ?, ??, 
??, ??, ??, ?, ???} in HowNet and {?, 
??, ?, ??, ??} in the LDC English-Chinese 
dictionary.   ?beat.v? is then linked to all HowNet 
concepts whose Chinese word/phrase is one of the 
translations and the part of speech is verb ?v?. 
 
In our example, HowNet concepts under two 
HowNet categories??beat? and ?damage? are linked 
to the ?cause_harm? frame in FrameNet. Only the 
concepts in the top N categories are considered as 
correctly linked to the lexical entries in the 
?cause_harm? frame. We heuristically chose N to be 
three in our algorithm. 
2.4. Final mapping adjusted by taxonomy 
distance (step 3) 
Using frame context alone in the above step can 
effectively prune out incorrect links, but it also 
prunes some correct links whose HowNet categories 
are not in the top three categories. In this next step, 
we aim to recover this kind of pruned links by finding 
other categories with high similarity to the chosen 
categories. We introduce the category similarity 
score (Liu and Li, 2002), which is based on the 
HowNet taxonomy distance:  
2.3. Refined mapping based on semantic 
contexts in both languages (step 2) 
At this stage, each FrameNet lexical entry has links 
to multiple HowNet concepts and categories. For 
example, ?beat.v? in ?cause_harm? frame is linked to 
??? in both the ?beat? category and the ?associate? 
category (as in????/make a phone call?). We need 
to choose the correct HowNet concept (word sense). 
Many word sense disambiguation algorithms use 
contextual words in a sentence as disambiguating 
features.  
Sim(category1,category2) = 
+d
?
?  
Where d is the path length from category1 to 
category2 in the taxonomy. ? is an adjusting 
parameter, which controls the curvature of the 
similarity score. We set ?=1.6 in our work following 
the experiment results in Liu and Li (2002). If the 
similarity of category p and one of the top three 
categories is higher than a threshold t, the category p 
is also considered as a valid category for the frame. 
 
 
 
In our example, some valid categories, such as 
?firing|??? is not selected in the previous step even 
though it is related to the ?cause_harm? frame. Based 
on the HowNet taxonomy, the similarity score 
between ?firing|? and ?beat|? is 1.0, which is 
above the threshold set.  Hence, ?firing|? is also 
chosen as a valid category and the concepts in this 
category are linked to the ?beat.v? lexical entry in the 
?cause_harm? frame. However, using taxonomy 
distance can cause errors such as ? in the ?weave? 
category to be aligned to ?beat.v? in the 
?cause_harm? frame. 
2.5. BiFrameNet lexicon evaluation   
We evaluate our work by comparing the results to 
a manually set golden standard of transfer links for 
some lexical entries in FrameNet, and use the 
precision and recall rate as evaluation criteria. 
Manual evaluation of all lexical entries is a slow 
process and is currently still on-going.  However, 
to show the lower bound of the system performance, 
we chose FrameNet lexical entries with the highest 
number of transfer links to HowNet concepts as the 
test set. Since each link is a word sense, these lexical 
entries have most ambiguous translations.  Since 
the number of lexical entries in a FrameNet parent 
frame (i.e. frame size) is an important factor in the 
disambiguation step, we analyze our results by 
distinguishing between ?small frames? (a frame with 
less than 5 lexical entries) and ?large frames?.  
24% of the frames are ?small frames?. Referring to 
Tables 2 and 3, we can see a weighted average of 
(0.649*0.24+0.874*0.76) =82% F-measure. 
 
lexical 
entry 
Parent frame #candidate 
HowNet 
links 
#lexical 
entries in 
parent 
frame 
beat.v cause_harm 144 51 
move.v motion 132 10 
bright.a light_emission 126 44 
hold.v containing 145 2 
fall.v motion_directional 127 5 
issue.v emanating 124 4 
Table1. Lexical entries test set 
 
lexical 
entry 
Precision 
step3/step1 
Recall  
step3/step1  
F-measure 
step3/step1 
beat.v 88.9/36.8% 90.6/100% 89.7/53.8% 
move.v 100/49.2 % 72.3/100% 83.9/66.0% 
bright.a 79.1/54.0% 100/100% 88.3/70.1% 
Overall 87.1/46.3% 87.6/100% 87.4/52.3% 
Table 2.Performance on large frames 
lexical 
entry 
Precision 
step3/step1 
Recall  
step3/step1  
F-measure 
step3/step1 
hold,v 22.4/7.6% 100/100% 36.7/14.1% 
fall,v 87.0/49.2% 81.1/100% 83.9/66.0% 
issue.v 31.1/12.3% 100/100% 47.5/20.3% 
Overall 52.1/25.0% 85.9/100% 64.9/40.0% 
Table 3. Performance on small frames 
  Step 1 Step 2 Step 3 
Precision 36.81% 95.24% 88.89% 
Recall 100% 75.47% 90.56% 
F-measure 53.81% 84.21% 89.72% 
Table 4. Average performance on ?beat.v? at 
each step of the algorithm 
 
Table 4 shows the system performance in each step 
of the alignment between the FrameNet ?beat.v? to 
HowNet concepts with the final F-measure at 89.72. 
3. Cross-lingual induction of example 
annotated sentences in BiFrameNet 
In the second stage of our proposed work, we aim 
to automatically induce Chinese example sentences 
that are appropriate for each semantic frame. 
Together with English example sentences that 
already exist in the English FrameNet, they form 
part of the BiFrameNet, and serve to provide 
concrete examples of bilingual usage of semantic 
roles. They can be used either as a resource for 
machine-aided translation or training data for 
machine translation.  
 
FrameNet is a collection of over 100-million 
words of samples of written and spoken language 
from a wide range of sources, including British and 
American English. All the example sentences are 
chosen by linguists for their representative-ness of 
particular semantic roles, grammatical functions, and 
phrase type.  The current FrameNet contains on 
average 30 annotated example sentences per 
predicate, which is still inadequate for automatic 
semantic parsing systems (Fleischman et al, 2003). 
Each FrameNet example sentence contains a 
predicate. The semantic roles of the related frame 
elements are manually labeled. The syntactic phrase 
type (e.g. NP, PP) and their grammatical function 
(e.g. external argument, object argument) are also 
labeled. An example annotated sentence containing 
the predicate ?beat.v?, in the ?cause_harm? frame,  
is  shown below: 
 
Example sentence type: trans-simple  
We are fighting a barbarian, and [agent: we] must 
[predicate: beat] [victim: him].  
 
In order to provide a representative set of Chinese 
example sentences automatically for a particular 
frame, our method must fulfill the following criteria: 
1) It must find real sentences occurring naturally in 
Chinese texts; 
2) It should find sentences that cover as many 
different usage and domain as possible; 
3) It must find sentences that have the same 
semantic roles as the English example sentences; 
 
F F
  
    
;
;
English sentence Chinese sentence
for frame for frame 
Candidate for frame 
:  Dynamic Programming alignment (Figure 5)
: : 
? :  ? :  
:  
For each 
  { | , , ( , ) }
  For each 
  
F
F F
F
DP
CA
F
CA F u v L? ?= ? ? ?
??
e c
e c
c
c c
e
{ }
F
?    = argmax ( )
?    { }
CA
F F
DP
?
? ? ?
c
c e, c
cU
 
Figure 3. BiFrameNet example sentence induction 
 
4) It should require no manual annotation of any 
kind. 
 
There are at least three different (semi-)automatic 
approaches for mining Chinese example sentences: 
 
i) Translate all English example sentences into 
Chinese by automatic means, and annotate the 
semantic roles by word alignment; 
 
This approach is not appropriate because machine 
translation can be erroneous and this method does 
not satisfy criteria (1) and (2). 
 
ii) Construct an English semantic parser and a 
Chinese parser independently, and use them to 
annotate the sentences in a sentence aligned, 
parallel corpus; 
 
Apart from the high cost of building two semantic 
parsers, which itself requires semantically annotated 
Chinese data; it would be necessary to create 
artificial links between independent human 
annotations manually.  
 
iii) Mine Chinese sentences from a monolingual 
corpus that are syntactically similar to the English 
example sentence, and induce semantic roles from 
the syntactic transfer function between English and 
Chinese. 
 
This is the approach we take. Inspired by previous 
work on syntax-driven semantic parsing (Gildea and 
Jurafsky, 2002; Fleischman et al, 2003), and 
syntax-based machine translation (Wu, 1997; 
Cuerzan and Yarowsky, 2002), we postulate that 
syntactically similar sentences with the same 
predicate also share similar semantic roles. In this 
paper, we present our first experiments on inducing 
semantic roles based on shallow syntactic 
information. We mine Chinese example sentences 
from naturally occurring monolingual corpus, and 
rank them by their syntactic similarity to our English 
example sentences. A dynamic programming 
algorithm then annotates the aligned syntactic units 
with the same semantic roles. The example Chinese 
sentences are not translations of the English 
sentences. Therefore, the set of example sentences 
within a frame is enriched, providing better coverage 
for MT and CLIR systems. 
3.1. Induction from aligned predicate bilingual 
lexical pair 
Since frames are disjoint, we propose a method 
for finding example sentences one frame at a time.  
In this paper, we focus on finding Chinese example 
sentences for the largest frame ?cause_harm? and 
the main semantic roles in this frame??agent?, 
?predicate? and ?victim?1.  
 
For each English lexical entry and its target 
translation candidates in the BiFrameNet, we first 
extract sentences that contain the translation 
candidates from a large Chinese monolingual 
corpus. Figure 4 shows some initial Chinese 
example sentence candidates under ?beat.v?. There 
are many sentences that do not have the 
?agent-predicate-victim? structure. Our next step is 
to find the Chinese sentences that have the 
?agent?, ?predicate? and ?victim? semantic roles and 
annotate them automatically.  
 
????????????????? (the southern 
army killed and maimed hundreds of government soldiers) 
??????????????  (soldiers harmed 
innocent civilians during the attack) 
??????????????  (farmers cut down 
more than 70 trees) 
??????? (use the needle to prick the squash) 
*??????????(the media exposed/produced 
an investigation report) 
*???????????????? (some 
publishers adopt a ?idiom? method) 
Figure 4. Some Chinese example sentence and 
glosses 
3.2. Inducing semantic roles from cross-lingual 
POS transfer  
Among all the Chinese sentences containing the 
target predicate words, we need to identify those that 
contain the same semantic roles as those of the 
English example sentences in FrameNet. Current 
automatic semantic parsing algorithms (Gildea and 
Jurafsky 2003, Fleischman et al, 2003) are all based 
on syntactic parse trees showing a close coupling of 
semantic and syntactic structures.  
Without carrying out full syntactic parsing of the 
Chinese sentences, we postulate that the semantic 
                                                        
1 As an example, for ?beat.v?, 73% of the English example 
sentences have these three semantic roles, only 27% also have 
other semantic roles such as ?tools?. 
roles of a sentence are generated by the underlying 
shallow syntactic structure of the sentence such as 
POS tag sequences. We therefore focus on finding 
bilingual sentence pairs that are comparable in POS 
structure, though not necessarily having any lexical 
comparability. Note that this constitutes only a 
subset of all possible Chinese example sentences for 
each frame. The expansion of this set remains the 
objective of our future research  
English POS  Chinese POS  ( , )e c?  
PRP  N 3.16-e2 
NN  N 4.0-e6 
JJ  N 1.74-e4 
NNP Nr 4.257-e2 
JJS V 2.15-e4 
VB V 7.2-e5 
VBG Ad 1.34-e3 
VBG m  6.74-e3 
Table 5. Example POS tag transfer  
Given an English example sentence, its semantic 
role sequence, and its POS tag sequence; and a set of 
Chinese sentences and their POS tag sequence, we 
use a dynamic programming method (Figure 5) to 
find the Chinese sentence whose POS sequence is 
most likely to be generated from the English POS 
sequence, and the alignment path. The Chinese word 
aligned to the English word will assume the latter?s 
semantic role.  
 
[agent ????]?[predicate ????][victim ??
??????] 
[agent ??]????[predicate ??]?[victim ??
???] 
[agent ??][predicate ??][victim ??????
????] 
??[agent ?][predicate ??][victim ??] 
Figure 6. Example Chinese annotated sentences 
 
3.3. BiFrameNet example sentence evaluation Initialization 
[0,0] 0; [0, -1] ( , ); [ -1,0] ( , )j iS S j c S i e? ? ?= + + ?  
Recursion 
[ -1, ] ( , )
[ , ] [ 1, 1] ( , )
[ , 1] ( , )
[ -1, ] ( , )
[ , ] arg [ 1, 1] ( , )
[ , 1] ( , )
i
i j
j
i
i j
j
S i j e
S i j max S i j e c
S i j c
S i j e
T i j max S i j e c
S i j c
? ?
?
? ?
? ?
?
? ?
? +?= ? ? +?? ? +?
? +?= ? ? +?? ? +?
 
where 
( , )i je c?  is the transfer cost of an English 
POS tag from a Chinese POS tag; ? is an empty word. 
M and N are the lengths of the English and Chinese 
POS sequences respectively; 1< i <M; 1<j<N; 
Termination 
[ , ], [ , ]S N M T N M  are the final alignment score and 
final point on the path; 
Path Backtracking  
Output the final English-Chinese POS alignment path 
by tracing back from the terminal points. Also output 
the final alignment score normalized by the path length. 
We estimate the syntactic POS transfer 
probabilities from the HK News Corpus. We use two 
state-of-the-art POS taggers?a maximum entropy 
based English POS tagger (Ratnaparkhi, 1996), and 
an HMM based Chinese POS tagger.2 We perform 
two sets of experiments: (1) For each example 
English sentence in the ?cause_harm? frame from 
FrameNet, we extract a corresponding Chinese 
sentence annotated with the same semantic roles; (2) 
rank all the Chinese sentences that have been 
aligned to the English sentences by alignment score. 
The highest ranking Chinese sentences are used for 
the BiFrameNet. Table 6 shows that the average 
annotation accuracy of all top Chinese sentence 
candidates for each English example sentence is 
68%. Table 7 shows that the annotation accuracy of 
the top 100 Chinese example sentences, sorted by DP 
score, is 71.8%. 
 
Semantic roles Accuracy 
Predicate 77.63% 
Agent 68.75% 
Victim 52.72% 
(Overall) 68% 
Figure 5. Dynamic programming (DP) alignment 
 
We train ( , )e c?  in Figure 5 from a sentence aligned, 
POS tagged, parallel corpus (Hong Kong News), and a 
bilingual dictionary. For each bilingual word pair in the 
dictionary, we estimate the prior distributions of the 
POS tags of the Chinese words from the Chinese side 
of the parallel corpus, and that of the English words 
from the English side.  A V x W POS tag ?confusion 
matrix? is generated, where V is the vocabulary of the 
Chinese POS tags, and W is the vocabulary of the 
English POS tags. Table 5 shows some example 
English-Chinese POS mapping and Figure 6 shows 
some example annotated sentences in Chinese. 
Table 6. Annotation accuracy of the selected 
Chinese sentences 
Semantic roles Accuracy 
Predicate 81.69% 
Agent 63.24% 
Victim 70.77% 
(Overall) 71.8% 
Table 7. Annotation accuracy of the top 100 
Chinese sentences with the highest DP alignment 
scores 
                                                         
2 http://mtgroup.ict.ac.cn/~zhp/ICTCLAS/index.html 
4. Conclusion  
We have presented a first quantitative and 
automatic approach of constructing a bilingual 
lexical semantic resource?the BiFrameNet. 
BiFrameNet consists of mappings between 
FrameNet semantic frames and HowNet concepts, as 
well as English and Chinese example sentences for a 
particular frame, with annotated semantic roles in 
the English FrameNet labels. Evaluation results 
show that we achieve a promising 82% average 
F-measure on lexical entry alignment, for the most 
ambiguous lexical entries; and a 68-72% accuracy in 
Chinese example sentence induction, for the largest 
frame. The initial results are available at 
http://www.cs.ust.hk/~hltc/BiFrameNet and will be 
updated as further improvements and evaluations are 
implemented.  
5. Discussion  
There are a number of possible directions for 
future work. One obvious extension is to use 
syntactic parse tree representations instead of POS 
sequences in example sentence alignment. Second, 
there are many other Chinese sentences that share 
the same semantic roles, but not the same POS 
sequences, which are not included. Using additional 
features to correctly identify these sentences and the 
constituent semantic roles is a topic of our ongoing 
research. Moreover, we note that Chinese is a highly 
idiomatic and metaphoric language. Compounded by 
the ambiguity of word boundaries, many predicate 
usages in Chinese are highly unexpected. It is worth 
considering using other Chinese linguistic resources 
to enhance the example sentence extraction and 
annotation. Finally, BiFrameNet needs to be further 
evaluated and manual post-processing is perhaps 
required.  
We expect the final complete BiFrameNet, in 
addition to the various FrameNet and PropBank 
resources being developed manually, will be a 
valuable resource for statistical and interlingua 
transfer-based MT systems, as well as to human 
translators in an machine-aided translation scenario. 
We are also motivated to investigate the relationship 
between our results and those of semantic mapping 
models proposed by cognitive scientists.  
6. Acknowledgement  
This work is partly supported by grants CERG# 
HKUST6206/03E and CERG#HKUST6213/02E of the 
Hong Kong Research Grants Council.  
References 
Collin F. Baker, Charles J. Fillmore and John B. Lowe. 
(1998).The Berkeley FrameNet project. In 
Proceedings of the COLING-ACL, Montreal, 
Canada.  
Hans C. Boas. Bilingual FrameNet Dictionaries for 
Machine Translation. In Proceedings of the Third 
International Conference on Language Resources 
and Evaluation. Las Palmas, Spain. Vol. IV: 
1364-1371 2002. 
Silviu Cucerzan and David Yarowsky. Bootstrapping a 
multilingual part-of-speech tagger in one person-day. 
In Proceedings of the Sixth Conference on Natural 
Language Learning (CoNLL). Taipei, Taiwan. 2002.  
Dong, Zhendong., and Dong, Qiang. HowNet [online 
2002]. Available at 
http://www.keenage.com/zhiwang/e_zhiwang.html 
Bonnie J. Dorr, Gina-Anne Levow, and Dekang 
Lin.(2002).Construction of a Chinese-English Verb 
Lexicon for Machine Translation. In Machine 
Translation, Special Issue on Embedded MT, 17:1-2.  
Michael Fleischman, Namhee Kwon and Eduard Hovy. 
Maximum Entropy Models for FrameNet 
Classification. In Proceedings of ACL 2003, 
Sapporo.  
Daniel Gildea and Daniel Jurafsky.(2002).Automatic 
Labeling of Semantic Roles. In Computational 
Linguistics, Vol 28.3: 245-288.  
Judy Illes and Wendy S. Francis. Convergent cortical 
representation of semantic processing in bilinguals. 
In Brain and Language, 70(3):347-363, 1999.  
Liu Qun and Li, Sujian. Word Similarity Computing 
Based on How-net. In Computational Linguistics 
and Chinese Language Processing?Vol.7, No.2, 
August 2002, pp.59-76 
Grace Ngai, Marine Carpuat, and Pascale Fung. 
Identifying Concepts Across Languages: A First 
Step towards a Corpus-based Approach to 
Automatic Ontology Alignment. In Proceedings of 
COLING-02, Taipei, Taiwan. 
Martha Palmer and Wu Zhibiao. Verb Semantics for 
English-Chinese Translation. In Machine 
Translation 10: 59-92, 1995.  
Sabine Ploux and Hyungsuk Ji. A Model for Matching 
Semantic Maps between Languages (French/English, 
English/French). In Computational Linguistics 
29(2):155-178, 2003. 
Adwait Ratnaparkhi. A Maximum Entropy 
Part-Of-Speech Tagger. In Proceedings of EMNLP 
2003, May 17-18, 1996. University of Pennsylvania 
Satoko Ikeda. Manual response set in a stroop-like task 
involving categorization of English and Japanese 
words indicates a common semantic representation. 
In Perceptual and Motor Skills, 87(2):467-474, 
1998. 
Dekai Wu. Stochastic inversion transduction grammars 
and bilingual parsing of parallel corpora. In 
Computational Linguistics 23(3):377-404, Sep 1997 
Dekai Wu. The HKUST leading question translation 
system. MT-Summit 2003. New Orleans, Sep 2003. 
 
Multi-level Bootstrapping for Extracting Parallel Sentences from a 
Quasi-Comparable Corpus 
Pascale Fung and Percy Cheung 
Human Language Technology Center, 
Department of Electrical & Electronic Engineering, HKUST, 
Clear Water Bay, Hong Kong 
{pascale,eepercy}@ee.ust.hk  
 
Abstract 
We propose a completely unsupervised 
method for mining parallel sentences from 
quasi-comparable bilingual texts which 
have very different sizes, and which 
include both in-topic and off-topic 
documents. We discuss and analyze 
different bilingual corpora with various 
levels of comparability. We propose that 
while better document matching leads to 
better parallel sentence extraction, better 
sentence matching also leads to better 
document matching. Based on this, we use 
multi-level bootstrapping to improve the 
alignments between documents, sentences, 
and bilingual word pairs, iteratively. Our 
method is the first method that does not 
rely on any supervised training data, such 
as a sentence-aligned corpus, or temporal 
information, such as the publishing date of 
a news article.  It is validated by 
experimental results that show a 23% 
improvement over a method without 
multilevel bootstrapping.   
1 Introduction 
Sentence-aligned parallel corpus is an important 
resource for empirical natural language tasks such 
as statistical machine translation and cross-lingual 
information retrieval. Recent work has shown that 
even parallel sentences extracted from comparable 
corpora helps improve machine translation 
qualities (Munteanu and Marcu, 2004). Many 
different methods have been previously proposed 
to mine parallel sentences from multilingual 
corpora. Many of these algorithms are described in 
detail in (Manning and Sch?tze, 1999, Dale et al, 
2000, Veronis 2001). The challenge of these tasks 
varies according to the degree of comparability of 
the input multilingual documents. Existing work 
extract parallel sentences from parallel, noisy 
parallel or comparable corpora based on the 
assumption that parallel sentences should be 
similar in sentence length, sentence order and bi-
lexical context. In our work, we try to find parallel 
sentences from a quasi-comparable corpus, and we 
find that many of assumptions in previous work are 
no longer applicable in this case. Alternatively, we 
propose an effective, multi-level bootstrapping 
approach to accomplish this task (Figure 1).  
 
 
Figure1. Multi-level bootstrapping for parallel 
sentence extraction 
 
Extraction of matching bilingual segments from 
non-parallel data has remained a challenging task 
after almost a decade. Previously, the author and 
other researchers had suggested that bi-lexical 
information based on context can still be used to 
find correspondences between passages, sentences, 
or words, in non-parallel, comparable texts of the 
same topic (Fung and McKeown 1995, Rapp 1995, 
Grefenstette 1998, Fung and Lo 1998, Kikui 1999). 
More recent works on parallel sentence extraction 
from comparable data align documents first, before 
extracting sentences from the aligned documents 
(Munteanu and Marcu, 2002, Zhao and Vogel, 
2002). Both work used a translation model trained 
from parallel corpus and adaptively extract more 
parallel sentences and bilingual lexicon in the 
comparable corpus. In Zhao and Vogel (2002), the 
comparable corpus consists of Chinese and English 
versions of new stories from the Xinhua News 
agency. Munteanu and Marcu (2002) used 
unaligned segments from the French-English 
Hansard corpus and finds parallel sentences among 
them.  Zhao and Vogel (2002) used a generative 
statistical machine translation alignment model, 
Munteanu and Marcu (2002) used suffix trees-
based alignment model, and Munteanu and Marcu 
(2004) used a maximum entropy based classifier 
trained from parallel corpus to extract matching 
sentences from a comparable corpus of Arabic and 
English news. The comparable corpora used in all 
these work consist of documents on the same topic. 
Our challenge is to find matching bilingual 
sentences from documents that might or might not 
be on the same topic.  
2 Bilingual Sentence Alignment 
There have been various definitions of the term 
?parallel corpora? in the research community. In 
this paper, we compare and analyze different 
bilingual corpora, ranging from the parallel, noisy 
parallel, comparable, to quasi-comparable. 
 
A parallel corpus is a sentence-aligned corpus 
containing bilingual translations of the same 
document. The Hong Kong Laws Corpus is a 
parallel corpus with sentence level alignment; and 
is used as a parallel sentence resource for statistical 
machine translation systems. There are 313,659 
sentence pairs in Chinese and English. Alignment 
of parallel sentences from this type of database has 
been the focus of research throughout the last 
decade and can be accomplished by many off-the-
shelf, publicly available alignment tools.  
 
A noisy parallel and comparable corpus contains 
non-aligned sentences that are nevertheless mostly 
bilingual translations of the same document. 
Previous works have extracted bilingual word 
senses, lexicon and parallel sentence pairs from 
noisy parallel corpora (Fung and McKeown 1995, 
Fung and Lo 1998). Corpora such as the Hong 
Kong News are in fact rough translations of each 
other, focused on the same thematic topics, with 
some insertions and deletions of paragraphs.  
 
Another type of comparable corpus is one that 
contains non-sentence-aligned, non-translated 
bilingual documents that are topic-aligned. For 
example, newspaper articles from two sources in 
different languages, within the same window of 
published dates, can constitute a comparable 
corpus. Note that many existing algorithms for 
sentence alignment from comparable corpus are, in 
fact, methods for noisy parallel corpus. 
 
On the other hand, a quasi-comparable corpus is 
one that contains non-aligned, and non-translated  
bilingual documents that could either be on the 
same topic (in-topic) or not (off-topic). TDT3 
Corpus is a good source of truly non-parallel and 
quasi-comparable corpus. It contains transcriptions 
of various news stories from radio broadcasting or 
TV news report from 1998-2000 in English and 
Chinese.  In this corpus, there are about 7,500 
Chinese and 12,400 English documents, covering 
more than 60 different topics.  Among these, 1,200 
Chinese and 4,500 English documents are 
manually marked as being in-topic. The remaining 
documents are marked as off-topic as they are 
either only weakly relevant to a topic or irrelevant 
to all topics in the existing documents. From the 
in-topic documents, most are found to be 
comparable. A few of the Chinese and English 
passages are almost translations of each other. 
Nevertheless, the existence of considerable amount 
of off-topic document gives rise to more variety of 
sentences in terms of content and structure.  
Overall, the TDT 3 corpus contains 110,000 
Chinese sentences and 290,000 English sentences. 
A very small number of the bilingual sentences are 
translations of each other, while some others are 
bilingual paraphrases. In this paper, we describe a 
method to extract translated and paraphrased 
bilingual sentence pairs from this quasi-
comparable corpus. 
2.1 Comparing bilingual corpora 
We explore the usability of different bilingual 
corpora for the purpose of multilingual natural 
language processing. We argue that the usability of 
bilingual corpus depends how well the sentences 
are aligned. To quantify this corpus characteristic, 
we propose using a lexical alignment score 
computed from the bilingual word pairs distributed 
throughout the bilingual sentence pairs. 
 
We first identify bilingual word pairs that appear in 
the aligned sentence pairs by using a bilingual 
lexicon (bilexicon). Lexical alignment score is then 
defined as the sum of the mutual information score 
of all word pairs that appear in the corpus:  
?=
=
),(
),(
)()(
),(),(
ec WWall
ec
ec
ec
ec
WWSS
WfWf
WWfWWS
 
where f(Wc,We) is the co-occurrence frequency of 
bilexicon pair (Wc,We) in the aligned sentence pairs. 
f(Wc) and f(We) are the occurrence frequencies of 
Chinese word Wc and English word We, in the 
bilingual corpus. 
 
Table 1 shows the lexical alignment scores of   
parallel sentences extracted from a parallel corpus 
(Hong Kong Law), a comparable noisy parallel 
corpus (Hong Kong News), and a non-parallel, 
quasi-comparable corpus (TDT 3). We can see that 
the scores are in direct proportion to the parallel-
ness or comparability of the corpus.    
 
Corpus Parallel Comparable Quasi- 
Comparable
Bilexicon 
score 
359.1 253.8 160.3 
Table 1: Bilingual lexicon scores of different 
corpora 
 
2.2 Comparing alignment assumptions 
All previous work on sentence alignment from 
parallel corpus makes use of one or multiple of the 
following nine (albeit imperfect) assumptions, as 
described in the literature (Somers 2001, Manning 
& Sch?tze, 1999), and summarized as below: 
 
1. There are no missing translations in the 
target document; 
2. Sentence lengths: a bilingual sentence pair 
are similarly long in the two languages; 
3. Sentence position: Sentences are assumed 
to correspond to those roughly at the same 
position in the other language.  
4. Bi-lexical context: A pair of bilingual 
sentences which contain more words that 
are translations of each other tend to be 
translations themselves. 
 
For noisy parallel corpora without sentence 
delimiters, assumptions made previously for 
bilingual word pairs are as follows: 
 
5. Occurrence frequencies of bilingual word 
pairs are similar; 
6. The positions of bilingual word pairs are 
similar; 
7. Words have one sense per corpus; 
8. Following 7, words have a single 
translation per corpus; 
9. Following 4, the sentence contexts in two 
languages of a bilingual word pair are 
similar. 
 
Different sentence alignment algorithms based on 
both sentence and lexical information can be found 
in Manning and Sch?tze (1999), Wu (2000), Dale 
et al (2001), Veronis (2002), and Somers (2002). 
These methods have also been applied recently in a 
sentence alignment shared task at NAACL 20031. 
We have learned that as bilingual corpora become 
less parallel, it is better to rely on information 
about word translations rather than sentence length 
and position.  
 
For comparable corpora, previous bilingual 
sentence or word pair extraction works are based 
soly on bilexical context assumption (Fung & 
McKeown 1995, Rapp 1995, Grefenstette 1998, 
Fung and Lo 1998, Kikui 1999, Barzilay and 
Elhadad 2003, Masao and Hitoshi 2003, Kenji and 
Hideki 2002). Similarly, for quasi-comparable 
corpora, we cannot rely on any other sentence level 
or word level statistics but the bi-lexical context 
assumption. We also postulate one additional 
assumption: 
 
10. Seed parallel sentences: Documents and 
passages that are found to contain at least 
one pair of parallel sentences are likely to 
contain more parallel sentences. 
 
3 Our approach: Multi-level Bootstrapping 
Existing algorithms (Zhao and Vogel, 2002, 
Munteanu and Marcu, 2002) for extracting parallel 
sentences from comparable documents seem to 
follow the 2 steps: (1) extract comparable 
documents (2) extract parallel corpus from 
comparable documents. Other work on 
monolingual, comparable sentence alignment by 
(Barzilay and Elhadad 2003) also supports that it is 
advantageous to first align comparable passages 
and then align the bilingual sentences within the 
aligned passages. The algorithms proposed by 
Zhao and Vogel, and by Munteanu and Marcu 
differ in the training and computation of document 
similarity scores and sentence similarity scores.  
Examples of document similarity computation 
include counting word overlap and cosine 
similarity. Examples of sentence similarity 
computation include word overlap count, cosine 
similarity, and classification scores of a binary 
classifier trained from parallel corpora, generative 
alignment classifier. In our work, we use simple 
cosine similarity measures and we dispense with 
using parallel corpora to train an alignment 
classifier. In addition, we do not make any 
                                                     
1 http://www.cs.unt.edu/~rada/wpt/ 
document position assumptions since such 
information is not always available. 
 
In addition to assumption 10 on the seed sentence 
pairs, we propose that while better document 
matching leads to better parallel sentence 
extraction, better sentence matching leads to better 
bilingual lexical extraction, better bilingual lexicon 
yields better glossing words, which improve the 
document and sentence match. We can iterate this 
whole process for incrementally improved results 
using a multi-level bootstrapping algorithm. Figure 
2 outlines the algorithm in more detail. In the 
following sections 3.1-3.4, we describe the four 
different steps of our algorithm. 
3.1 Extract comparable documents 
The aim of this step is to extract the Chinese-
English documents pairs that are comparable, and 
therefore should have similar term distributions. 
 
The documents are word segmented with the 
Language Data Consortium (LDC) Chinese-
English dictionary 2.0. The Chinese document is 
then glossed using all the dictionary entries. 
Multiple translations of a Chinese word is 
disambiguated by looking at the context of the 
sentences this word appears in (Fung et al, 1999). 
  
Both the glossed Chinese document and the 
English document are then represented in word 
vectors, with term weighting. We evaluated 
different combinations of term weighting of each 
word in the corpus: term freuency (tf), inverse 
document frequency (idf), tf.idf, the product of a 
function of tf and idf.  The  
?documents? here are sentences. We find that 
using idf  alone gives the best sentence pair rank. 
This is due to the fact that frequencies of bilingual 
word pairs are not comparable in a non-parallel, 
quasi-comparable corpus. 
1. Extract comparable documents   
For all documents in the comparable corpus D: 
a. Gloss Chinese documents using the bilingual lexicon (Bilex); 
b. For every pair of glossed Chinese and English documents, compute document similarity 
=>S(i,j); 
c. Obtain all matched bilingual document pairs whose S(i,j)> threshold1=>C 
2. Extract parallel sentences 
For each document pair in C: 
a. For every pair of glossed Chinese sentence and English sentence, compute sentence similarity 
=>S2(i,j); 
b. Obtain all matched bilingual sentence pairs whose S2(i,j)> threshold2=>C2 
3. Update bilingual lexicon with unknown word translations 
For each bilingual word pair in C2; 
a. Compute correlation scores of all bilingual word pairs =>S3(i,j);  
b. Obtain all bilingual word pairs previously unseen in Bilex and whose S3(i,j)> threshold3=>C3 
and update Bilex; 
c. Compute alignment score=>S4; if (S4> threshold4) return C3 otherwise continue; 
4. Update comparable document pairs  
a. Find all pairs of glossed Chinese and English documents which contain parallel sentences 
(anchor sentences) from C2=>C4;  
b. Expand C4 by finding documents similar to each of the document in C4; 
c. C:=C4; 
d. Goto 2; 
Figure 2. Multi-level bootstrapping algorithm 
 
Pair-wise similarities are calculated for all possible 
Chinese-English document pairs, and bilingual 
documents with similarities above a certain 
threshold are considered to be comparable. For 
quasi-comparable corpora, this document 
alignment step also serves as topic alignment.  
3.2 Extract parallel sentences 
In this step, we extract parallel sentences from the 
matched English and Chinese documents in the 
previous section. Each sentence is again 
represented as word vectors. For each extracted 
document pair, the pair-wise cosine similarities are 
calculated for all possible Chinese-English 
sentence pairs. Sentence pairs above a set threshold 
are considered parallel and extracted from the 
documents.  
 
We have only used one criterion to determine the 
parallel-ness of sentences at this stage, namely the 
number of words in the two sentences that are 
translations of each other. Further extensions are 
discussed in the final section of this paper.  
 
3.3  Update bilingual lexicon 
Step 3 updates the bilingual lexicon according to 
the intermediate results of parallel sentence 
extraction.  
 
The occurrence of unknown words can adversely 
affect parallel sentence extraction by introducing 
erroneous word segmentations. This is particularly 
notorious for Chinese to English translation.  For 
example, ?? ? ?? (?Olympic Committee?) is not 
found in the bilingual lexicon so the Chinese is 
segmented into three separate words in the original 
corpus, each word with an erroneous English gloss.  
Note that this occurs for unknown words in general, 
not just transliterated words.  
 
Hence, we need to refine bi-lexicon by learning 
new word translations from the intermediate output 
of parallel sentences extraction. In this work, we 
focus on learning translations for name entities 
since these are the words most likely missing in 
our baseline lexicon. The Chinese name entities are 
extracted with the system described in (Zhai et al
2004). New bilingual word pairs are learned from 
the extracted sentence pairs based on (Fung and Lo 
98) as follows:  
 
1. Extract new Chinese name entities (Zhai et 
al 2004);  
2. For each new Chinese name entity:  
z Extract all sentences that it appears in,  
from the original Chinese corpus, and 
build a context word vector;  
z For all English words, collect all 
sentences it appears in from the 
original corpus, and build the context 
vectors; 
z Calculate the similarity  between the 
Chinese word  and each of the English 
word vectors  
??
?
?=
j
j
i
j
j
AEC
i
wECw
EwCw
ECSim ji
)(
)(.)(
),( )(
KK  
where A is the aligned bilexicon pair 
between the two word vector. 
 
z Rank the English candidate according 
to the similarity score. 
 
Sometimes a Chinese named entity might be 
translated into a multi-word English collocation. In 
such a case, we search for and accept the English 
collocation candidate that does appear in the 
English documents. 
Below are some examples of unknown name 
entities that have been translated (or transliterated) 
correctly: 
????.  Augusto Pinochet (transliteration) 
???   Space Shuttle Endeavor (translation) 
???  Olympic Committee (translation) 
???? Benjamin Netanyahu (transliteration) 
3.4 Update comparable documents  
This step replaces the original corpus by the set of 
documents that are found to contain at least one 
pair of parallel sentences. Other documents that are 
comparable to this set are also included since we 
believe that even though they were judged to be 
not similar at the document level, they might still 
contain one or two parallel sentences. The 
algorithm then iterates to refine document 
extraction and parallel sentence extraction. An 
alignment score is computed in each iteration, 
which counts, on average, how many known 
bilingual word pairs actually co-occur in the 
extracted ?parallel? sentences. The alignment score 
is high when these sentence pairs are really 
translations of each other. 
4 Evaluation 
We evaluate our algorithm on a quasi-comparable 
corpus of TDT3 data, which contains various news 
stories transcription of radio broadcasting or TV 
news report from 1998-2000 in English and 
Chinese Channels.   
 
4.2. Baseline method 
 
The baseline method shares the same 
preprocessing, document matching and sentence 
matching with our proposed method. However, it 
does not iterate to update the comparable 
document set, the parallel sentence set, or the 
bilingual lexicon.  
 
Human evaluators then manually check whether 
the matched sentence pairs are indeed parallel. The 
precision of the parallel sentences extracted is 43% 
for the top 2,500 pairs, ranked by sentence 
similarity scores. 
 
4.3 Multi-level bootstrapping 
 
There are 110,000 Chinese sentences and  290,000 
English sentences,  which lead to more than 30 
billion  possible sentence pairs. Few of the 
sentence pairs turn out to be parallel, but many are 
paraphrasing sentence pairs. For example, in the 
following extracted sentence pair, 
? ?? ? ?? ??? ? ?? ?? ?  
(Hun Sen becomes Cambodia ' s sole 
prime minister) 
? Under the agreement, Hun Sen becomes 
Cambodia ' s sole prime minister .  
the English sentence has the extra phrase ?under 
the agreement?. 
 
The precision of parallel sentences extraction is 
67% for the top 2,500 pairs using our method, 
which is 24% higher than the baseline. In addition, 
we also found that the precision of parallel 
sentence pair extraction increases steadily over 
each iteration, until convergence. 
 
For another evaluation, we use the bilingual lexical 
score as described in Section 2.1 again as a 
measure of the quality of the extracted bilingual 
sentence pairs from the parallel corpus, 
comparable corpus, and quasi-comparable corpus. 
Word pairs common to all corpora are used in the 
lexical alignment score. Table 2 shows that the 
quality of the extracted parallel sentences from the 
quasi-comparable corpus is similar to those from 
noisy parallel and comparable corpus, even though 
both are understandably inferior in terms of 
parallel-ness when compared to the manually 
aligned parallel corpus. It is worth noting that the 
lexical alignment score for the extracted sentence 
pairs from the quasi-comparable corpus is similar 
to that for the comparable corpus. This is because 
we must evaluate different corpora by using word 
pairs that appear in all corpora. This has eliminated 
many word pairs some of which are likely to 
contribute significantly to the alignment score.  
 
Table 2: Lexical alignment scores of extracted 
parallel sentences, based on a common lexicon 
 
Figure 3 shows two pairs of parallel sentences 
from a parallel corpus and a comparable corpus, 
showing that the latter are closer to bilingual 
paraphrases rather than literal translations.  
Parallel sentence from parallel corpus: 
?? ?? ?? ??? ?? ?? ?? ?
??? ?  
Chinese president Jiang_Zemin arrived in 
Japan today for a landmark state visit. 
Parallel sentence from comparable 
corpus: 
? ?? ?? ?? ?? ?? ?? ?
? ?  
Mr Jiang is the first Chinese head of state to 
visit the island country.  
 
Figure 3. Example parallel sentences  
5 Conclusion 
We explore the usability of different bilingual 
corpora for the purpose of multilingual natural 
language processing. We compare and contrast a 
number of bilingual corpora, ranging from the 
parallel, to comparable, and to non-parallel corpora. 
The usability of each type of corpus is then 
evaluated by a lexical alignment score calculated 
for the bi-lexicon pair in the aligned bilingual 
sentence pairs.  
 
We compared different alignment assumptions for 
mining parallel sentences from these different 
types of bilingual corpora and proposed new 
assumptions for quasi-comparable corpora. By 
postulating additional assumptions on seed parallel 
sentences of comparable documents, we propose a 
multi-level bootstrapping algorithm to extract 
useful material, such as parallel sentences and 
bilexicon, from quasi-comparable corpora. This is 
a completely unsupervised method. Evaluation 
results show that our approach achieves 67% 
accuracy and a 23% improvement from baseline.  
This shows that the proposed assumptions and 
algorithm are promising for the final objective. The 
lexical alignment score for the comparable 
sentences extracted with our unsupervised method 
is found to be very close to that of the parallel 
corpus. This shows that our extraction method is 
effective. 
Corpus Alignment 
method  
Bilexicon 
alignment score  
Parallel  
 
manual 3.924949 
Comparable  DP on sentence 
position  
1.3685069 
Comparable Absolute sentence 
position  
1.0636631 
Quasi-
comparable  
Multi-level 
bootstrapping 
2.649668 
 
Quasi-
comparable 
Cosine similarity 1.507132  
The main contributions of our work lie in steps 3 
and 4 and in the iterative process.  Step 3 updates 
the bilingual lexicon from the intermediate results 
of parallel sentence extraction. Step 4 replaces the 
original corpus by the set of documents that are 
found to contain parallel sentences. The algorithm 
then iterates to refine document extraction and 
parallel sentence extraction. An alignment score is 
computed at each iteration, which counts, on 
average, how many known bilingual word pairs 
actually co-occur in the extracted parallel 
sentences. The alignment score is high when these 
sentence pairs are really translations of each other. 
By using the correct alignment assumptions, we 
have demonstrated that a bootstrapping iterative 
process is also possible for finding parallel 
sentences and new word translations from 
comparable corpus. 
6 Acknowledgements 
This work is partly supported by grants CERG# 
HKUST6206/03E and CERG#HKUST6213/02E 
of the Hong Kong Research Grants Council.  
References  
Regina Barzilay and Noemie Elhadad, Sentence 
Alignment for Monolingual Comparable 
Corpora, Proc. of EMNLP, 2003, Sapporo, 
Japan. 
Christopher D. Manning and Hinrich Sch?tze. 
Foundations of Statistical Natural Language 
Processing. The MIT Press. 
Robert Dale, Hermann Moisl, and Harold Somers 
(editors), Handbook of Natural Language 
Processing. 
Pascale Fung and Kathleen Mckeown. Finding 
terminology translations from non-parallel 
corpora. In The 5th Annual Workshop on Very 
Large Corpora. pages 192--202, Hong Kong, 
Aug. 1997.", 
Pascale Fung and Lo Yuen Yee. An IR Approach 
for Translating New Words from Nonparallel, 
Comparable Texts.  In COLING/ACL  1998 
Gale, W A and Kenneth W.Church. A Program for 
Aligning Sentences in Bilingual Corpora. 
Computatinal Linguistics. vol.19 No.1 March, 
1993. 
 Pascale Fung, Liu, Xiaohu, and Cheung, Chi Shun. 
Mixed-language Query Disambiguation. In 
Proceedings of ACL ?99, Maryland: June 1999 
Gregory Grefenstette, editor. Cross-Language 
Information Retrieval. Kluwer Academic 
Publishers, 1998. 
Hiroyuki Kaji, Word sense acquisition from 
bilingual comparable corpora, in Proceedings of 
the NAACL, 2003, Edmonton, Canada, pp 111-
118. 
Genichiro Kikui. Resolving translation ambiguity 
using non-parallel bilingual corpora. In 
Proceedings of ACL99 Workshop on 
Unsupervised Learning in Natural Language 
Dragos Stefan Munteanu, Daniel Marcu, 2002. 
Processing Comparable Corpora With Bilingual 
Suffix Trees. In Proceedings of the 2002 
Conference on Empirical Methods in Natural 
Language Processing (EMNLP 2002).  
Dragos Stefan Munteanu, Daniel Marcu, 2004. 
Improved Machine Translation Performace via 
Parallel Sentence Extraction from Comparable 
Corpora. In Proceedings of the Human 
Language Technology and North American 
Association for Computational Linguistics 
Conference (HLT/NAACL 2004). 
Reinhard Rapp. Identifying word translations in 
non-parallel texts. Proceedings of the 33rd 
Meeting of the Association for Computational 
Linguistics. Cambridge, MA, 1995. 320-322 
Philip Resnik and Noah A. Smith. The Web as a 
Parallel Corpus. Computational Linguistics 
29(3), pp. 349-380, September 2003. 
Frank Smadja. Retrieving collocations from text: 
Xtract. In Computational Linguistics, 19(1):143-
177,1993 
Harold Somers. Bilingual Parallel Corpora and 
Language Engineering. Anglo-Indian workshop 
"Language Engineering for South-Asian 
languages" (LESAL), (Mumbai, April 2001).  
Jean Veronis (editor). Parallel Text Processing: 
Alignment and Use of Translation Corpora. 
Dordrecht: Kluwer. ISBN 0-7923-6546-1. Aug 
2000.  
Dekai Wu. Alignment. In Robert Dale, Hermann 
Moisl, and Harold Somers (editors), Handbook 
of Natural Language Processing. 415-458. New 
York: Marcel Dekker. ISBN 0-8247-9000-6. Jul 
2000.  
Bing Zhao, Stephan Vogel. Processing 
Comparable Corpora With Bilingual Suffix 
Trees. In Proceedings of the EMNLP 2002.  
Zhai, Lufeng, Pascale Fung. Richard Schwartz, 
Marine Carpuat and Dekai Wu. Using N-best list 
for Named Entity Recognition from Chinese 
Speech. To appear in the Proceedings of the 
NAACL 2004.  
Inversion Transduction Grammar Constraints
for Mining Parallel Sentences from
Quasi-Comparable Corpora
Dekai Wu1 and Pascale Fung2
1 Human Language Technology Center, HKUST,
Department of Computer Science
2 Department of Electrical and Electronic Engineering,
University of Science and Technology, Clear Water Bay, Hong Kong
dekai@cs.ust.hk, pascale@ee.ust.hk
Abstract. We present a new implication of Wu?s (1997) Inversion
Transduction Grammar (ITG) Hypothesis, on the problem of retriev-
ing truly parallel sentence translations from large collections of highly
non-parallel documents. Our approach leverages a strong language uni-
versal constraint posited by the ITG Hypothesis, that can serve as a
strong inductive bias for various language learning problems, resulting
in both efficiency and accuracy gains. The task we attack is highly prac-
tical since non-parallel multilingual data exists in far greater quantities
than parallel corpora, but parallel sentences are a much more useful re-
source. Our aim here is to mine truly parallel sentences, as opposed to
comparable sentence pairs or loose translations as in most previous work.
The method we introduce exploits Bracketing ITGs to produce the first
known results for this problem. Experiments show that it obtains large
accuracy gains on this task compared to the expected performance of
state-of-the-art models that were developed for the less stringent task of
mining comparable sentence pairs.
1 Introduction
Parallel sentences are a relatively scarce but extremely useful resource for many
applications including cross-lingual retrieval and statistical machine translation.
Parallel sentences, or bi-sentences for short, can be exploited for a wealth of
applications ranging from mining term translations for cross-lingual applications,
to training paraphrase models and inducing structured terms for indexing, query
processing, and retrieval.
Unfortunately, far more is available in the way of monolingual data. High-
quality parallel corpora are currently largely limited to specialized collections
of government (especially UN) and certain newswire collections, and even then
relatively few bi-sentences are available in tight sentence-by-sentence translation.
 This work was supported in part by the Hong Kong Research Grants Council through
grants RGC6083/99E, RGC6256/00E, DAG03/04.EG09, and RGC6206/03E.
R. Dale et al (Eds.): IJCNLP 2005, LNAI 3651, pp. 257?268, 2005.
c
? Springer-Verlag Berlin Heidelberg 2005
258 D. Wu and P. Fung
Increasingly sophisticated methods for extracting loose translations from non-
parallel monolingual corpora?and in particular, what have been called com-
parable sentence pairs?have also recently become available. But while loose
translations by themselves already have numerous applications, truly parallel
sentence translations provide invaluable types of information for the aforemen-
tioned types of mining and induction, which cannot easily be obtained from
merely loose translations or comparable sentence pairs. In particular, truly par-
allel bi-sentences are especially useful for extracting more precise syntactic and
semantic relations within word sequences.
We present a new method that exploits a novel application of Inversion
Transduction Grammar or ITG expressiveness constraints (Wu 1995 [1], Wu
1997 [2]) for mining monolingual data to obtain tight sentence translation pairs,
yielding accuracy significantly higher than previous known methods. We focus
here on very non-parallel quasi-comparable monolingual corpora, which are avail-
able in far larger quantities but are significantly more difficult to mine than either
noisy parallel corpora or comparable corpora. The majority of previous work has
concerned noisy parallel corpora (sometimes imprecisely also called ?compara-
ble corpora?), which contain non-aligned sentences that are nevertheless mostly
bilingual translations of the same document. More recent work has examined
comparable corpora, which contain non-sentence-aligned, non-translated bilin-
gual documents that are topic-aligned. Still relatively few methods attempt to
mine quasi-comparable corpora, which contain far more heterogeneous, very non-
parallel bilingual documents that could be either on the same topic (in-topic) or
not (off-topic).
Our approach is motivated by a number of desirable characteristics of ITGs,
which historically were developed for translation and alignment purposes, rather
than mining applications of the kind discussed in this paper. The ITG Hypothesis
posits a strong language universal constraint that can act as a strong inductive
bias for various language learning problems, resulting in both efficiency and accu-
racy gains. Specifically, the hypothesis asserts that sentence translation between
any two natural languages can be accomplished within ITG expressiveness (sub-
ject to certain conditions). So-called Bracketing ITGs (BITG) are particularly
interesting in certain applications such as the problem we consider here, because
they impose ITG constraints in language-independent fashion, and do not re-
quire any language-specific linguistic grammar. (As discussed below, Bracketing
ITGs are the simplest form of ITGs, where the grammar uses only a single,
undifferentiated non-terminal.)
The key modeling property of bracketing ITGs that is most relevant to the
task of identifying parallel bi-sentences is that they assign strong preference to
candidate sentence pairs in which nested constituent subtrees can be recursively
aligned with a minimum of constituent boundary violations. Unlike language-
specific linguistic approaches, however, the shape of the trees are driven in un-
supervised fashion by the data. One way to view this is that the trees are hid-
den explanatory variables. This not only provides significantly higher robustness
than more highly constrained manually constructed grammars, but also makes
Inversion Transduction Grammar Constraints for Mining Parallel Sentences 259
the model widely applicable across languages in economical fashion without a
large investment in manually constructed resources.
Moreover, for reasons discussed by Wu [2], ITGs possess an interesting in-
trinsic combinatorial property of permitting roughly up to four arguments of any
frame to be transposed freely, but not more. This matches suprisingly closely the
preponderance of linguistic verb frame theories from diverse linguistic traditions
that all allow up to four arguments per frame. Again, this property falls naturally
out of ITGs in language-independent fashion, without any hardcoded language-
specific knowledge. This further suggests that ITGs should do well at picking out
translation pairs where the order of up to four arguments per frame may vary
freely between the two languages. Conversely, ITGs should do well at rejecting
candidates where (1) too many words in one sentence find no correspondence in
the other, (2) frames do not nest in similar ways in the candidate sentence pair,
or (3) too many arguments must be transposed to achieve an alignment?all of
which would suggest that the sentences probably express different ideas.
Various forms of empirical confirmation for the ITG Hypothesis have emerged
recently, which quantitatively support the qualitative cross-linguistic character-
istics just described across a variety of language pairs and tasks. Zens and Ney
(2003) [3] show that ITG constraints yield significantly better alignment coverage
than the constraints used in IBM statistical machine translation models on both
German-English (Verbmobil corpus) and French-English (Canadian Hansards
corpus). Zhang and Gildea (2004) [4] found that unsupervised alignment using
Bracketing ITGs produces significantly lower Chinese-English alignment error
rates than a syntactically supervised tree-to-string model [5]. Zhang and Gildea
(2005) [6] show that lexicalized ITGs can further improve alignment accuracy.
With regard to translation rather than alignment accuracy, Zens et al (2004)
[7] show that decoding under ITG constraints yields significantly lower word
error rates and BLEU scores than the IBM constraints. Chiang (2005) [8] ob-
tains significant BLEU score improvements via unsupervised induction of hi-
erarchical phrasal bracketing ITGs. Such results partly motivate the work we
discuss here.
We will begin by surveying recent related work and reviewing the formal
properties of ITGs. Subsequently we describe the architecture of our new
method, which relies on multiple stages so as to balance efficiency and accuracy
considerations. Finally we discuss experimental results on a quasi-comparable
corpus of Chinese and English from the topic detection task.
2 Recent Approaches to Mining Non-parallel Corpora
Recent work (Fung and Cheung 2004 [9]; Munteanu et al 2004 [10]; Zhao and
Vogel 2002 [11]) on extracting bi-sentences from comparable corpora is largely
based on finding on-topic documents first through similarity matching and time
alignment.
However, Zhao and Vogel used a corpus of Chinese and English versions of
news stories from the Xinhua News agency, with ?roughly similar sentence order
260 D. Wu and P. Fung
of content?. This corpus can be more accurately described as a noisy parallel cor-
pus. Munteanu et al used comparable corpora of news articles published within
the same 5-day window. In both cases, the corpora contain documents on the
same matching topics; unlike our present objective of mining quasi-comparable
corpora, these other methods assume corpora of on-topic documents.
Munteanu et al first identify on-topic document pairs by looking at publication
date and word overlap, then classify all sentence pairs as being parallel or not par-
allel, using a maximum entropy classifier trained on parallel corpora. In contrast,
the method we will propose identifies candidate sentence pairs without assuming
that publication date information is available, and then uses the ITG constraints
to automatically find parallel sentence pairs without requiring any training.
It is also difficult to relate Munteanu et al?s work to our present objective
because they do not directly evaluate the quality of the extracted bi-sentences
(they instead look at performance of their machine translation application);
however, as with Fung and Cheung, they noted that the sentences extracted
were not truly parallel on the whole.
In this work, we aim to find parallel sentences from much more heterogenous,
very non-parallel quasi-comparable corpora. Since many more multilingual text
collections available today contain documents that do not match documents
in the other language, we propose finding more parallel sentences from off-topic
documents, as well as on-topic documents. An example is the TDT corpus, which
is an aggregation of multiple news sources from different time periods.
3 Inversion Transduction Grammars
Formally, within the expressiveness hierarchy of transduction grammars, the
ITG level of expressiveness has highly unusual intrinsic properties as seen in
Figure 1. Wu [2] showed that the ITG class is an equivalence class of subsets
of syntax-directed transduction grammars or SDTGs (Lewis and Stearns 1968
[12]), equivalently defined by meeting any of the following three conditions: (1) all
rules are of rank 2, (2) all rules are of rank 3, or (3) all rules are either of straight
or inverted orientation (and may have any rank). Ordinary unrestricted SDTGs
allow any permutation of the symbols on the right-hand side to be specified when
translating from the input language to the output language. In contrast, ITGs
only allow two out of the possible permutations. If a rule is straight, the order of
its right-hand symbols must be the same for both languages (just as in a simple
SDTG or SSDTG). On the other hand, if a rule is inverted, then the order is left-
to-right for the input language and right-to-left for the output language. Since
inversion is permitted at any level of rule expansion, a derivation may intermix
productions of either orientation within the parse tree. The ability to compose
multiple levels of straight and inverted constituents gives ITGs much greater
expressiveness than might seem at first blush, as indicated by the growing body
of empirical results mentioned earlier.
A simple example may be useful to fix ideas. Consider the following pair of
parse trees for sentence translations:
Inversion Transduction Grammar Constraints for Mining Parallel Sentences 261
Fig. 1. The ITG level of expressiveness constitutes a surprisingly broad equivalence class
within the expressiveness hierarchy of transduction grammars. The simple monolingual
notion of ?context-free? is too coarse to adequately categorize the bilingual case of trans-
duction grammars. The expressiveness of a transduction grammar depends on the max-
imum rank k of rules, i.e., the maximum number of nonterminals on the right-hand-side.
SDTG-k is always more expressive than SDTG-(k-1), except for the special case of the
ITG class which includes both SDTG-2 and SDTG-3. In contrast, for monolingual CFGs,
expressiveness is not affected by rank, as shown by the existence of a binary Chomsky
normal form for any CFG. A binary normal form exists for ITGs but not SDTGs.
[[[The Authority]
NP
[will [[be accountable]
VV
[to [the [[Financial Secretary]
NN
]
NNN
]
NP
]
PP
]
VP
]
VP
]
SP
.]
S
[[[]
NP
[ [[ [[[ ]
NN
]
NNN
]
NP
]
PP
[]
VV
]
VP
]
VP
]
SP
]
S
Even though the order of constituents under the inner VP is inverted between
the languages, an ITG can capture the common structure of the two sentences.
This is compactly shown by writing the parse tree together for both sentences
with the aid of an ?? angle bracket notation marking parse tree nodes that
instantiate rules of inverted orientation:
[[[The/?Authority/]NP [will/ ?[be/?accountable/]VV [to/ [the/?
[[Financial/Secretary/]NN ]NNN ]NP ]PP ?VP ]VP ]SP./]S
In a weighted or stochastic ITG (SITG), a weight or a probability is associ-
ated with each rewrite rule. Following the standard convention, we use a and b
to denote probabilities for syntactic and lexical rules, respectively. For example,
the probability of the rule NN
0.4? [A N] is aNN?[A N] = 0.4. The probability of a
lexical rule A
0.001? x/y is bA(x, y) = 0.001. Let W1, W2 be the vocabulary sizes
of the two languages, and N = {A1, . . . , AN} be the set of nonterminals with
indices 1, . . . , N .
Polynomial-time algorithms are possible for various tasks including transla-
tion using ITGs, as well as bilingual parsing or biparsing, where the task is to
build the highest-scored parse tree given an input bi-sentence.
262 D. Wu and P. Fung
For present purposes we can employ the special case of Bracketing ITGs,
where the grammar employs only one single, undistinguished ?dummy? nonter-
minal category for any non-lexical rule. Designating this category A, a Bracketing
ITG has the following form (where, as usual, lexical transductions of the form
A ? e/f may possibly be singletons of the form A ? e/ or A ? /f).
A ? [AA]
A ? ?AA?
A ? , 
A ? e1/f1
. . .
A ? ei/fj
Broadly speaking, Bracketing ITGs are useful when we wish to make use of
the structural properties of ITGs discussed above, without requiring any addi-
tional linguistic information as constraints. Since they lack differentiated syn-
tactic categories, Bracketing ITGs merely constrain the shape of the trees that
align various nested portions of a sentence pair. The only linguistic knowledge
used in Bracketing ITGs is the purely lexical set of collocation translations.
Nevertheless, the ITG Hypothesis implies that biparsing truly parallel sentence
pairs with a Bracketing ITG should typically yield high scores. Conversely, some
non-parallel sentence pairs could be ITG-alignable, but any significant departure
violating constituent boundaries will be downgraded.
As an illustrative example, in the models employed by most previous work
on mining bi-sentences from non-parallel corpora, the following pair of sentences
(found in actual data arising in our experiments below) would receive an inap-
propriately high score, because of the high lexical similarity between the two
sentences:
Chinese president Jiang Zemin arrived in Japan today for a landmark state visit .
            
(Jiang Zemin will be the first Chinese national president to pay a state vist to
Japan.)
However, the ITG based model is sensitive enough to the differences in the
constituent structure (reflecting underlying differences in the predicate argument
structure) so that our experiments show that it assigns a low score. On the
other hand, the experiments also show that it successfully assigns a high score
to other candidate bi-sentences representing a true Chinese translation of the
same English sentence, as well as a true English translation of the same Chinese
sentence.
4 Candidate Generation
An extremely large set of pairs of monolingual sentences from the quasi-
comparable monolingual corpora will need to be scanned to obtain a useful
M
Inversion Transduction Grammar Constraints for Mining Parallel Sentences 263
Fig. 2. Candidate generation overview. The iterative bootstrapping algorithm first
mines loosely parallel sentence pairs from quasi-comparable corpora that contain both
on-topic and off-topic documents. In a preprocessing step, documents that are believed
to be on the same topic according to their similarity score are extracted, then ?parallel?
pairs are mined from these matched documents. The extracted sentences are used to
bootstrap the entire process iteratively in two ways: (1) they are used to update a
bilingual lexicon, which is then used again to reprocess the documents to be matched
again; (2) any document pairs that are found to contain at least one ?parallel? sentence
pairs are considered to be on-topic, and added to the matched document set. Note
that step (2) adds to the on-topic document set certain document pairs that are not
considered to be on-topic by document matching scores.
number of parallel sentences, since obviously, the overwhelming majority of the
n2 possible sentence pairs will not be parallel. It is infeasible to run the ITG
biparsing algorithm on n2 candidate sentence pairs. Therefore a multi-stage al-
gorithm is needed that first generates likely candidates using faster heuristics,
and then biparses the candidates to obtain the final high-precision results.
We base our candidate generation on a method that Fung and Cheung (2004)
developed for extracting loose translations (comparable sentence pairs) from
quasi-comparable corpora [9], as shown in Figure 2. We selected this model
because it produces the highest known accuracy on that task.
Figure 3 outlines the algorithm in greater detail. In the following sections, we
describe the document pre-processing step followed by each of the subsequent
iterative steps of the algorithm.
264 D. Wu and P. Fung
1. Initial document matching
For all documents in the comparable corpus D:
? Gloss Chinese documents using the bilingual lexicon (Bilex)
? For every pair of glossed Chinese document and English documents:
? compute document similarity => S(i,j)
? Obtain all matched bilingual document pairs whose S(i,j) > threshold1 => D2
2. Sentence matching
For each document pair in D2:
? For every pair of glossed Chinese sentence and English sentence:
? compute sentence similarity => S2(i,j)
? Obtain all matched bilingual sentence pairs whose S2(i,j) > threshold2 => C1
3. EM learning of new word translations
For all bilingual sentences pairs in C1, do:
? Compute translation lexicon probabilities of all bilingual word pairs =>S3(i,j)
? Obtain all bilingual word pairs previously unseen in Bilex and whose S3(i,j) > threshold3
=> L1, and update Bilex
? Compute sentence alignment scores => S4; if S4 does not change then return C1 and L1,
otherwise continue
4. Document re-matching
? Find all pairs of glossed Chinese and English documents which contain parallel sentences
(anchor sentences) from C1 => D3
? Expand D2 by finding documents similar to each of the document in D2
? D2 := D3
5. Goto 2 if termination criterion not met
Fig. 3. Candidate generation algorithm
Document preprocessing. The documents are word segmented with the Linguistic
Data Consortium (LDC) Chinese-English dictionary 2.0. The Chinese document
is then glossed using all the dictionary entries. When a Chinese word has multiple
possible translations in English, it is disambiguated using an extension of Fung
et al?s (1999) method [13].
Initial document matching. The aim of this step is to roughly match the Chinese-
English documents pairs that are on-topic, in order to extract parallel sentences
from them. Following previous work, cosine similarity between document vectors
is used to judge whether a bilingual document pair is on-topic or off-topic.
Both the glossed Chinese document and English are represented in word
vectors, with term weights. Pair-wise similarities are calculated for all possible
Chinese-English document pairs, and bilingual documents with similarities above
a certain threshold are considered to be comparable. Comparable documents are
often on-topic.
Sentence matching. All sentence pair combinations within the on-topic docu-
ments are considered next in the selection process. Each sentence is again rep-
resented as word vectors. For each extracted document pair, pair-wise cosine
similarities are calculated for all possible Chinese-English sentence pairs. Sen-
tence pairs above a set threshold are considered parallel and extracted from the
documents. Since cosine similarity is computed on translated word pairs within
the sentence pairs, the better our bilingual lexicon is, the more accurate the
sentence similarity will be. In the following section, we discuss how to find new
word translations.
Inversion Transduction Grammar Constraints for Mining Parallel Sentences 265
EM lexical learning from matched sentence pairs. This step updates the bilingual
lexicon according to the intermediate results of parallel sentence extraction. New
bilingual word pairs are learned from the extracted sentence pairs based on an
EM learning method. In our experience any common method can be used for this
purpose; for the experiments below we used the GIZA++ [14] implementation
of the IBM statistical translation lexicon Model 4 of Brown et al (1993) [15].
This model is based on the conditional probability of a source word being
generated by the target word in the other language, based on EM estimation
from aligned sentences. Zhao and Vogel (2002) showed that this model lends
itself to adaptation and can provide better vocabulary coverage and better sen-
tence alignment probability estimation [11]. In our work, we use this model on
the intermediate results of parallel sentence extraction, i.e., on a set of aligned
sentence pairs that may or may not truly correspond to each other.
We found that sentence pairs with high alignment scores are not necessarily
more similar than others. This might be due to the fact that EM estimation
at each intermediate step is not reliable, since we only have a small amount of
aligned sentences that are truly parallel. The EM learner is therefore weak when
applied to bilingual sentences from very non-parallel quasi-comparable corpora.
Document re-matching. This step implements a ?find-one-get-more? principle,
by augmenting the earlier matched documents with document pairs that are
found to contain at least one parallel sentence pair. We further find other doc-
uments that are similar to each of the monolingual documents found. The algo-
rithm then iterates to refine document matching and parallel sentence extraction.
Convergence. The IBM model parameters, including sentence alignment score
and word alignment scores, are computed in each iteration. The parameter values
eventually stay unchanged and the set of extracted bi-sentence candidates also
converges to a fixed size. The iteration then terminates and returns the last set
of bilingual sentence pairs as the generated candidate sentences.
5 ITG Scoring
The ITG model computes scores upon the set of candidates generated in the
preceding stage. A variant of the approach used by Leusch et al (2003) [16]
allows us to forego training to estimate true probabilities; instead, rules are
simply given unit weights. This allows the scores computed by ITG biparsing to
be interpreted as a generalization of classical Levenshtein string edit distance,
where inverted block transpositions are also allowed. Even without probability
estimation, Leusch et al found excellent correlation with human judgment of
similarity between translated paraphrases.
As mentioned earlier, biparsing for ITGs can be accomplished efficiently in
polynomial time, rather than the exponential time required for classical SDTGs.
The biparsing algorithm employs a dynamic programming approach described
by Wu [2]. The time complexity of the algorithm in the general case is ?
(
T 3V 3
)
where T and V are the lengths of the two sentences. This is a factor of V 3 more
266 D. Wu and P. Fung
than monolingual chart parsing, but has turned out to remain quite practical
for corpus analysis, where parsing need not be real-time.
6 Experiments
Method. For our experiments we extracted the bi-sentences from a very non-
parallel, quasi-comparable corpus of TDT3 data which consists of transcriptions
of news stories from radio and TV broadcasts in both English and Chinese chan-
nels during the period 1998-2000. This corpus contained approximately 290,000
English sentences and 110,000 Chinese sentences. This yields over 30 billion
possible sentence pairs, so a multi-stage approach is clearly necessary.
Experience showed that the lexicon learned in the candidate generation stage,
while adequate for candidate generation, is not of sufficient quality for biparsing
due to the non-parallel nature of the training data. However, any translation
lexicon of reasonable accuracy can be used. For these experiments we employed
the LDC Chinese-English dictionary 2.0.
To conduct as blind an evaluation as possible, an independent annotator
separately produced gold standard labels for a random sample of approximately
300 of the top 2,500 candidate sentence pairs proposed by the generation stage.
The annotator was instructed to accept any semantically equivalent translations,
including non-literal ones. Inspection had shown that sentence pair candidates
longer than about 15 words were practically never truly parallel translations,
so these were a priori excluded by the sampling in order to ensure that preci-
sion/recall scores would be more meaningful.
Results. Under our method any desired tradeoff between precision and recall
can be obtained. Therefore, rather than arbitrarily setting a threshold, we are
interested in evaluation metrics that can show whether the ITG model is highly
effective at any desired tradeoff points. Thus, we assess the contribution of ITG
ranking by computing standard uninterpolated average precision scores used to
evaluate the effectiveness of ranking methods. Specifically, in this case, this is
the expected value of precision over the rank positions of the correctly identified
truly parallel bi-sentences:
uninterpolated average precision =
1
| T |
?
i?T
precision at rank (i) (1)
where T is the set of correctly identified bi-sentences.
Our method yielded an uninterpolated average precision of 64.7%. No di-
rect comparison of this figure is possible since previous work has focused on
the rather different objectives of mining noisy parallel or comparable corpora
to extract comparable sentence pairs and loose translations. However, we can
understand the improvement by comparing against scores obtained using the
cosine-based lexical similarity metric which is typical of the majority of previ-
ous methods for mining non-parallel corpora, including that of Fung and Cheung
(2004)[9]. Evaluating the ranking produced under this more typical score yielded
Inversion Transduction Grammar Constraints for Mining Parallel Sentences 267
Fig. 4. Precision-recall curves for the ITG model (upper curve) versus traditional cosine
model (lower curve); see text
an uninterpolated average precision of 24.6%. This suggests that the ITG based
method could produce significant accuracy gains if applied to many of the ex-
isting non-parallel corpus mining methods.
Figure 4 compares precision versus recall curves obtained with rankings from
the ITG model compared with the more traditional cosine lexical similarity
model. The graph reveals that at all levels, much higher precision can be obtained
using the ITG model. Up to 20% recall, the ITG ranking produces bi-sentences
with perfect precision; in contrast, the cosine model produces 30% precision.
Even at 50% recall, the ITG ranked bi-sentences have above 65% precision, as
compared with 21% for the cosine model.
As can be seen from the following examples of extracted bi-sentences (shown
with rough word glosses), the ITG constraints are able to accommodate nested
inversions accounting for the cross-linguistic differences in constituent order:
7 Conclusion
We have introduced a new method that exploits generic bracketing Inversion
Transduction Grammars giving the first known results for the new task of min-
ing truly parallel sentences from very non-parallel quasi-comparable corpora.
It is time to break the silence.
?( b , / S4 ?? ? ? ? 
(Now topical , is break silence genitive time aspectual .)
I think that?s what people were saying tonight.
 ?: ? / ?? ?Z @ ? ?? 
(I think this is people today by say genitive words .)
If the suspects are convicted, they will serve their time in Scotland.
?? $  ?? ? ? $ 	j , 1 ? ( ?<p  
(If two classifier suspected person bei-particle sentence guilty, then must in Scot-
land serve time .)
268 D. Wu and P. Fung
The method takes the strong language universal constraint posited by the ITG
Hypothesis as an inductive bias on the bi-sentence extraction task which we
anticipate will become a key stage in unsupervised learning for numerous more
specific models. Experiments show that the method obtains large accuracy gains
on this task compared to the performance that could be expected if state-of-the-
art models for the less stringent task of mining comparable sentence pairs were
applied to this task instead. From a practical standpoint, the method has the
dual advantages of neither requiring expensive training nor requiring language-
specific grammatical resources, while producing high accuracy results.
References
1. Wu, D.: An algorithm for simultaneously bracketing parallel texts by aligning
words. In: ACL-95, Cambridge, MA (1995)
2. Wu, D.: Stochastic inversion transduction grammars and bilingual parsing of par-
allel corpora. Computational Linguistics 23 (1997)
3. Zens, R., Ney, H.: A comparative study on reordering constraints in statistical
machine translation. In: ACL-03, Sapporo (2003) 192?202
4. Zhang, H., Gildea, D.: Syntax-based alignment: Supervised or unsupervised? In:
COLING-04, Geneva (2004)
5. Yamada, K., Knight, K.: A syntax-based statistical translation model. In: ACL-01,
Toulouse, France (2001)
6. Zhang, H., Gildea, D.: Stochastic lexicalized inversion transduction grammar for
alignment. In: ACL-05, Ann Arbor (2005) 475?482
7. Zens, R., Ney, H., Watanabe, T., Sumita, E.: Reordering constraints for phrase-
based statistical machine translation. In: COLING-04, Geneva (2004)
8. Chiang, D.: A hierarchical phrase-based model for statistical machine translation.
In: ACL-05, Ann Arbor (2005) 263?270
9. Fung, P., Cheung, P.: Mining very-non-parallel corpora: Parallel sentence and
lexicon extraction via bootstrapping and em. In: EMNLP-2004, Barcelona (2004)
10. Munteanu, D.S., Fraser, A., Marcu, D.: Improved machine translation performance
via parallel sentence extraction from comparable corpora. In: NAACL-04. (2004)
11. Zhao, B., Vogel, S.: Adaptive parallel sentences mining from web bilingual news
collections. In: IEEE Workshop on Data Mining. (2002)
12. Lewis, P.M., Stearns, R.E.: Syntax-directed transduction. Journal of the Associa-
tion for Computing Machinery 15 (1968) 465?488
13. Fung, P., Liu, X., Cheung, C.S.: Mixed-language query disambiguation. In: ACL-
99, Maryland (1999)
14. Och, F.J., Ney, H.: Improved statistical alignment models. In: ACL-2000, Hong
Kong (2000)
15. Brown, P.F., DellaPietra, S.A., DellaPietra, V.J., Mercer, R.L.: The mathematics
of statistical machine translation. Computational Linguistics 19 (1993) 263?311
16. Leusch, G., Ueffing, N., Ney, H.: A novel string-to-string distance measure with
applications to machine translation evaluation. In: MT Summit IX. (2003)
Automatic Construction of an English-Chinese Bilingual FrameNet  
Chen, Benfeng and Pascale Fung 
Human Language Technology Center, 
Department of Electrical& Electronic Engineering, 
University of Science and Technology (HKUST), 
Clear Water Bay, Hong Kong 
{bfchen,pascale}@ee.ust.hk 
 
 
Abstract 
We propose a method of automatically con-
structing an English-Chinese bilingual Fra-
meNet where the English FrameNet lexical 
entries are linked to the appropriate Chinese 
word senses. This resource can be used in ma-
chine translation and cross-lingual IR systems. 
We coerce the English FrameNet into Chinese 
using a bilingual lexicon, frame context in 
FrameNet and taxonomy structure in HowNet. 
Our approach does not require any manual 
mapping between FrameNet and HowNet 
semantic roles. Evaluation results show that 
we achieve a promising 82% average F-
measure for the most ambiguous lexical 
entries.  
1 Introduction 
Since the early 90?s, automatic alignment of bilingual 
documents and sentences based on lexical and syntactic 
information has been a major focus of the statistical 
NLP community as their results are a valuable resource 
for statistical machine translation, cross-lingual question 
answering, and other bilingual or cross-lingual tasks. 
Recently, there has been an increasing trend of using 
semantic information for these tasks spurred by the 
availability of various ontology databases such as 
WordNet, FrameNet, PropBank, etc. Among these, the 
Berkeley FrameNet database is a semantic lexical re-
source consisting of frame-semantic descriptions of 
more than 7000 English lexical items, together with 
example sentences annotated with semantic roles (Baker 
et al, 1998). The current version of FrameNet has been 
applied successfully to English question answering sys-
tems (Gildea, 2002). However, the manual development 
of FrameNet in other languages has been on a small 
scale (e.g. German, Spanish, Japanese) or unfinished 
(e.g. Chinese). Since manually annotation is rather time 
consuming, the main objective of our work is to auto-
matically create multilingual FrameNet to enable se-
mantic analysis in multiple languages rather than in 
English. Another objective is to quantify the mapping 
between semantic structures across language pairs for 
statistical NLP systems. Our basic idea is to coerce the 
English FrameNet into another language using existing 
semantic resources and a bilingual lexicon. Our initial 
target language is Chinese. However, we expect that our 
technique is applicable to other languages as well. There 
are two Chinese semantic resources available today--
Cilin (tong2yi4ci2ci2lin2) (Mei et al, 1982) and 
HowNet (Dong and Dong, 2000). Much like WordNet, 
Cilin is a thesaurus with a hierarchical structure of word 
clusters, but it does not describe any semantic relation-
ship between words and categories. HowNet, on the 
other hand, is an ontology with a graph structure of in-
ter-concept relations and inter-attribute relations. In 
addition, HowNet has been widely used in resolving 
NLP problems, such as word sense disambiguation 
(Dang et al, 2002) and machine translation (Dorr et al, 
2002). For our work, we choose to align HowNet con-
cepts to lexical entries in FrameNet in order to construct 
the English-Chinese bilingual FrameNet.   
 
(Dorr et al, 2002) describes a technique for the con-
struction of a Chinese-English verb lexicon based on 
HowNet and an English verb database called the LCS 
Verb Database (LVD). They created links between Chi-
nese concepts in HowNet and English verb classes in 
LVD using both statistics and a manually constructed 
?seed mapping? of thematic classes between HowNet 
and LVD. Ngai et al (2002) employed a word-vector 
based approach to create the alignment between Word-
Net and HowNet classes without any manual annotation. 
In this paper, we present a fully automatic approach to 
create links between FrameNet semantic frames and 
HowNet concepts. We also plan to release an on-line 
demonstration for the community to access the bilingual 
FrameNet we built. 
2 FrameNet and HowNet 
FrameNet and HowNet are ontologies with different 
structures and different semantic role/relation defini-
tions. FrameNet is a collection of lexical entries 
grouped by frame semantics. Each lexical entry repre-
sents an individual word sense, and is associated with 
semantic roles and some annotated sentences. Lexical 
entries with the same semantic roles are grouped into a 
?frame? and the semantic roles are called ?frame ele-
ments?. For example: 
 
Frame: Cause_harm 
Frame Elements: agent, body_part, cause, event, in-
strument, iterations, purpose, reason, result, victim?.. 
Lexical Entries in ?cause_harm? Frame: 
bash.v, batter.v, bayonet.v, beat.v, belt.v, bludgeon.v, 
boil.v, break.v, bruise.v, buffet.v, burn.v,?. 
An annotated sentence of lexical entry ?beat.v?:  
[agent I] lay down on him and beat [victim at him] 
[means with my fists].  
 
HowNet is a Chinese ontology with a graph structure of 
word senses called ?concepts?, and each concept con-
tains 7 fields including lexical entries in Chinese, Eng-
lish gloss, POS tags for the word in Chinese and English, 
and a definition of the concept including its category 
and semantic relations (Dong and Dong, 2000). For 
example, one translation for ?beat.v? is ?: 
NO. = 17645 
W_C =? 
G_C =V 
E_C =~??~??~??~???~??~??~?? 
W_E=attack 
G_E=V 
E_E= 
DEF=fight|?? 
 
Whereas HowNet concepts correspond roughly to Fra-
meNet lexical entries, its semantic relations do not cor-
respond directly to FrameNet semantic roles. 
3 Construction of the English-Chinese Bi-
lingual FrameNet  
(Dorr et al 2002) uses a manual seed mapping of se-
mantic roles between FrameNet and LVD. In this paper, 
we propose a method of automatically linking the Eng-
lish FrameNet lexical entries to HowNet concepts, re-
sulting in a bilingual FrameNet. We make use of two 
bilingual English-Chinese lexicons, as well as HowNet 
and FrameNet. In the following sections 3.1 to 3.3, we 
use an example FrameNet lexical entry ?beat.v? in the 
?cause_harm? frame to illustrate the main steps of our 
algorithm in Figure 1. 
For each lexical entry l in FrameNet 
    Find translations T1 of l in HowNet translations. 
    Find translations T2 of l in LDC dictionary. 
    Combine the T1 and T2 together as T. T= T1?T2 
Link l to all HowNet concepts LC whose W_C field   
is in T. LC= {c|c.W_C ? T}, c is any HowNet  
concept. 
For each frame F in FrameNet  
Group all the HowNet concepts together FC which  
are linked to the lexical entries in F. FC= {c|  
link(c,l)=true and l ? F}. 
    Compute the frequency of HowNet categories in FC. 
Select the top 3 HowNet categories as valid  
categories VA for frame F. 
For each HowNet categories a  
  If the similarity score between a and one of the top  
  3 categories is greater than threshold t. Sim(a, ta) > 
  t, ta is any of the top 3 categories. 
          Add a into VA. VA = VA?{a}. 
    For each lexical entry l in frame F 
         For each HowNet concept c linked to l 
              If the categories of c is not in VA 
prune this link. 
Figure 1. The algorithm.  
 3.1 Baseline mapping based on bilingual lexicon  
We use the bilingual lexicon from HowNet and LDC 
dictionary to first create all possible mappings between 
FrameNet lexical entries and HowNet concepts whose 
part-of-speech (POS) tags are the same. Here we as-
sume that syntactic classification for the majority of 
FrameNet lexical entries (i.e. verbs and adjectives) are 
semantically motivated and are mostly preserved across 
different languages. For example ?beat? can be trans-
lated into {?, ?, ??, ??, ??, ??, ?, ?
??} in HowNet and {?, ??, ?, ??, ??} in 
the LDC English-Chinese dictionary.   ?beat.v? is then 
linked to all HowNet concepts whose Chinese 
word/phrase is one of the translations and the part of 
speech is verb ?v?.  Figure 2 shows some examples of 
HowNet concepts that are linked to ?beat.v?. 
 
Figure 2. Partial initial alignment of ?beat.v? to 
HowNet concepts with 144 candidate links 
 
3.2 Disambiguation by semantic contexts in both 
languages  
At this stage, each FrameNet lexical entry has links to 
multiple HowNet concepts and categories. For example, 
?beat.v? in ?cause_harm? frame is linked to ??? in 
both the ?beat? category and the ?associate? category 
(as in????/make a phone call?). We need to choose 
the correct HowNet concept (word sense). Many word 
sense disambiguation algorithms use contextual words 
in a sentence as disambiguating features. In this work, 
we make use of contextual lexical entries from the same 
semantic frame, as illustrated below: 
 
 
 
 
 
 
 
 
 
 
 
To disambiguate between the above two candidate cate-
gories, we make use of the other lexical entries in 
?cause_harm?, such as ???, and their linked categories 
in HowNet, such as ?beat? again. Each target HowNet 
category receives a vote from the candidate links. In our 
example, ?beat? receives two votes (from ??? and from 
???), and ?associate? only one (from ???). We choose 
the HowNet category with the most votes and its con-
stituent concepts to be the valid word sense links to the 
source FrameNet lexical entry. Consequently, ?beat.v? 
in ?cause_harm? is linked to all HowNet concepts that 
are translations of ?beat? which are verbs, and which 
also belong to the HowNet category ?beat? (vs. ?associ-
ate?).  
Figure 3. Disambiguating HowNet candidates for 
?beat.v? with 42 candidate links 
 
 
In our example, Figure 3 shows the top 14 examples of 
HowNet concepts belonging to two HowNet catego-
ries??beat? and ?damage? that are linked to the 
?cause_harm? frame in FrameNet. Only the concepts in 
the top N categories are considered as correctly linked 
to the lexical entries in the ?cause_harm? frame. We 
heuristically chose N to be three in our algorithm. 
3.3 Compensating links by HowNet taxonomy struc-
ture 
Using frame context alone in the above step can effec-
tively prune out incorrect links, but it also prunes some 
correct links whose HowNet categories are not in the 
top three categories but are similar to them. In this next 
step, we aim to recover this kind of pruned links. We 
introduce the category similarity score, which is based 
on the HowNet taxonomy distance (Liu and Li, 2002):  
Sim(category1,category2) = 
+d
?
?  
Where d is the path length from category1 to category2 
in the taxonomy. ? is an adjusting parameter, which 
controls the curvature of the similarity score. We set 
?=1.6 in our work following the experiment results in  
(Liu and Li, 2002). If the similarity of category p and 
one of the top three categories is higher than a threshold 
t, the category p is also considered as a valid category 
for the frame. 
 
In our example, some valid categories, such as ?firing|
??? is not selected in the previous step even though it 
is related to the ?cause_harm? frame. Based on the 
HowNet taxonomy, the similarity score between ?firing|
??? and ?beat|?? is 1.0, which we consider as high.  
Hence, ?firing|??? is also chosen as a valid category 
and the concepts in this category are linked to the 
?beat.v? lexical entry in the ?cause_harm? frame. How-
ever, using taxonomy distance can cause erros such as 
? in the ?weave? category to be aligned to ?beat.v? in 
the ?cause_harm? frame. 
 
Figure 4. Final HowNet candidates for ?beat.v? with 
54 candidate links 
 
cause_ 
harm 
beat.v 
strike.v 
? 
? 
?? 
|associate
?|beat 
FrameNet HowNet 
4 Evaluation   
We evaluate our work by comparing the results to a 
manually set golden standard of links for the most am-
biguous lexical entries in FrameNet, and use the preci-
sion and recall rate as evaluation criteria. To show the 
lower bound of the system performance, we chose six 
FrameNet lexical entries with the most links to HowNet 
concepts as the test set. Since each link is a word sense, 
these lexical entries have the most ambiguous transla-
tions.  Such lexical entries also turned out to be mostly 
verbs. Since the number of lexical entries in a FrameNet 
parent frame (i.e. frame size) is an important factor in 
the disambiguation step, we analyze our results by dis-
tinguishing between ?small frame?s (a frame with less 
than 5 lexical entries) and ?large frame?s. 24% of the 
frames are ?small frames?. Results in Tables 2 and 3 
have a weighted average of 
(0.649*0.24+0.874*0.76)=82% F-measure.  
 
lexical 
entry 
Parent frame #candidate 
HowNet 
links 
#lexical 
entries in 
parent 
frame 
beat.v cause_harm 144 51 
move.v motion 132 10 
bright.a light_emission 126 44 
hold.v containing 145 2 
fall.v motion_directional 127 5 
issue.v emanating 124 4 
Table1. Lexical entries test set  
 
lexical 
entry 
Precision 
best/baseline 
Recall  
best/baseline  
F-measure 
best/baseline
beat.v 88.9/36.8% 90.6/100% 89.7/53.8% 
move.v 100/49.2 % 72.3/100% 83.9/66.0% 
bright.a 79.1/54.0% 100/100% 88.3/70.1% 
Overall 87.1/46.3% 87.6/100% 87.4/52.3% 
Table 2. Performance on large frames 
 
lexical 
entry 
Precision 
step3/step1 
Recall  
best/baseline  
F-measure 
best/baseline
hold,v 22.4/7.6% 100/100% 36.7/14.1% 
fall,v 87.0/ 49.2 % 81.1/100% 83.9/66.0% 
issue.v 31.1/12.3% 100/100% 47.5/20.3% 
Overall 52.1/25.0% 85.9/100% 64.9/40.0% 
Table 3. Performance on small frames 
 
  Baseline 
Alignment 
Category 
Ranking 
Category Rank-
ing+ Taxonomy
Precision 36.81% 95.24% 88.89% 
Recall 100% 75.47% 90.56% 
F-measure 53.81% 84.21% 89.72% 
Table 4. Average performance on ?beat.v? at each 
step of the algorithm. 
 
Table 4 shows the system performance in each step of 
the alignment between the most ambiguous FrameNet 
lexical entry ?beat.v? to HowNet concepts with the final 
F-measure at 89.72. 
5 Conclusion and Discussion  
The alignment results can be found at 
http://www.cs.ust.hk/~hltc/BiFrameNet. Our evaluation 
shows that our method has achieved an 82% average F-
measure in aligning the most ambiguous FrameNet lexi-
cal entries to HowNet concepts. This paper describes 
the first stage in our project towards creating a bi-
lingual English-Chinese FrameNet, by aligning lexical 
entries between FrameNet and HowNet. The next step is 
to automatically extract semantically annotated Chinese 
sentences based on the annotated English sentences in 
FrameNet, the aligned FrameNet lexical entries, and 
bilingual corpora. We expect the final bilingual Frame-
Net will provide a valuable resource for multi-lingual or 
cross-lingual natural language processing. 
Acknowledgment 
This work is partly supported by CERG 
#HKUST6213/02E of the Hong Kong Research Grants 
Council (RGC).  
References 
Collin F. Baker, Charles J. Fillmore and John B. Lowe. 
(1998).The Berkeley FrameNet project. In Proceedings of the 
COLING-ACL, Montreal, Canada.  
Hoa Trang Dang, Ching-yi Chia, Martha Palmer, and Fu-Dong 
Chiou. Simple Features for Chinese Word Sense Disambigua-
tion. In Proceedings of COLING-2002, Taipei Taiwan, August 
24 - September 1, 2002. 
Dong, Zhendong., and Dong, Qiang.(2000). HowNet [online]. 
Available at 
http://www.keenage.com/zhiwang/e_zhiwang.html  
Bonnie J. Dorr, Gina-Anne Levow, and Dekang 
Lin.(2002).Construction of a Chinese-English Verb Lexicon 
for Machine Translation. In Machine Translation, Special Is-
sue on Embedded MT, 17:1-2.  
Daniel Gildea and Daniel Jurafsky.(2002).Automatic Labeling of 
Semantic Roles. In Computational Linguistics, Vol 28.3: 245-
288.  
Liu Qun, Li, Sujian.(2002).Word Similarity Computing Based on 
How-net. In Computational Linguistics and Chinese Language 
Processing?Vol.7, No.2, August 2002, pp.59-76 
Mei Jiaju and Gao Yunqi.(1983). tong2yi4ci2ci2lin2. Shanghai 
Dictionary Press. 
Grace Ngai, Marine Carpuat, Pascale Fung.(2002).Identifying 
Concepts Across Languages: A First Step towards a Corpus-
based Approach to Automatic Ontology Alignment". In Pro-
ceedings of COLING-02, Taipei, Taiwan. 
 
Using N-best Lists for Named Entity Recognition from Chinese Speech 
Lufeng ZHAI*, Pascale FUNG*, Richard SCHWARTZ?, Marine CARPUAT?, Dekai WU? 
 
* HKUST 
Human Language Technology Center 
Electrical & Electronic Engineering 
University of Science and Technology 
Clear Water Bay, Hong Kong 
{lfzhai,pascale}@ee.ust.hk 
? BBN Technologies 
9861 Broken Land Parkway  
Columbia, MD 21046 
                     U.S.A 
schwartz@bbn.com 
? HKUST 
Human Language Technology Center 
Department of Computer Science 
University of Science and Technology 
Clear Water Bay, Hong Kong 
{marine,dekai}@cs.ust.hk 
 
Abstract 
We present the first known result for named 
entity recognition (NER) in realistic large-
vocabulary spoken Chinese.  We establish this 
result by applying a maximum entropy model, 
currently the single best known approach for 
textual Chinese NER, to the recognition 
output of the BBN LVCSR system on Chinese 
Broadcast News utterances. Our results 
support the claim that transferring NER 
approaches from text to spoken language is a 
significantly more difficult task for Chinese 
than for English.  We propose re-segmenting 
the ASR hypotheses as well as applying post-
classification to improve the performance. 
Finally, we introduce a method of using n-best 
hypotheses that yields a small but nevertheless 
useful improvement NER accuracy.  We use 
acoustic, phonetic, language model, NER and 
other scores as confidence measure.  
Experimental results show an average of 6.7% 
relative improvement in precision and 1.7% 
relative improvement in F-measure. 
1. Introduction 
Named Entity Recognition (NER) is the first step for 
many tasks in the fields of natural language processing 
and information retrieval. It is a designated task in a 
number of conferences, including the Message 
Understanding Conference (MUC), the Information 
Retrieval and Extraction Conference (IREX), the 
Conferences on Natural Language Learning (CoNLL) 
and the recent Automatic Content Extraction 
Conference (ACE).   
 
There has been a considerable amount of work on 
English NER yielding good performance (Tjong Kim 
Sang et al 2002, 2003; Cucerzan & Yarowsky 1999; 
Wu et al 2003). However, Chinese NER is more 
difficult, especially on speech output, due to two 
reasons. First, Chinese has a large number of homonyms 
and the vocabulary used in Chinese person names is an 
open set so more characters/words are unseen in the 
training data. Second, there is no standard definition of 
Chinese words. Word segmentation errors made by 
recognizers may lead to NER errors. Previous work on 
Chinese textual NER includes Jing et al (2003) and Sun 
et al (2003) but there has been no published work on 
NER in spoken Chinese.  
 
Named Entity Recognition for speech is more difficult 
than for text, since the most reliable features for textual 
NER (punctuation, capitalization, and syntactic 
patterns) are often not available in speech output. NER 
on automatically recognized broadcast news was first 
conducted by MITRE in 1997, and was subsequently 
added to Hub-4 evaluation as a task. Palmer et al 
(1999) used error modeling, and Horlock & King (2003) 
proposed discriminative training to handle NER errors; 
both used a hidden Markov model (HMM). Miller et al 
(1999) also reported results in English speech NER 
using an HMM model. In a NIST 1999 evaluation, it 
was found that NER errors on speech arise from a 
combination of ASR errors and errors of the underlying 
NER system. 
 
In this work, we investigate whether the NIST finding 
holds for Chinese speech NER as well. We present the 
first known result for recognizing named entities in 
realistic large-vocabulary spoken Chinese. We propose 
to use the best-known model for Chinese textual NER?
a maximum entropy model?on Chinese speech NER. 
We also propose using re-segmentation and post-
classification to improve this model. Finally, we 
propose to integrate the ASR and NER components to 
optimize NER performance by making use of the n-best 
ASR output.  
2. A Spoken Chinese NER Model 
2.1 LVCSR output  
We use the ASR output from BBN?s Byblos system on  
broadcast news data from the Xinhua News Agency, 
which has 1046 sentences. This system has a character 
error rate of 7%. We had manually annotated them with 
named entities as an evaluation set according to the PFR 
corpus annotation guideline (PFR 2001). 
2.2 A maximum-entropy NER model with post- 
classification  
To establish a baseline spoken Chinese NER model, we 
selected a maximum entropy (MaxEnt) approach since 
this is currently the single most accurate approach 
known for recognizing named entities in text (Tjong 
Kim Sang et al, 2002, 2003, Jing et al, 2003)1. In the 
CoNLL 2003 NER evaluation, 5 out of 16 systems use 
MaxEnt models and the top 3 results for English and top 
2 results for German were obtained by systems that use 
MaxEnt.  
 
Natural language can be viewed as a stochastic process. 
We can use p(y|x) to denote the probability distribution 
of what we try to predict y (.e.g. part-of-speech tag, 
Named Entity tag) conditioned on what we observe x 
(e.g. previous POS or the actual word). The Maximum 
Entropy principle can be stated as follows: given some 
set of constrains from observations, find the most 
uniform probability distribution (Maximum Entropy) 
p(y|x) that satisfies these constrains:  
0
0 0
* arg max ( | )
1( | ) exp( ( , ))
( )
( ) exp( ( , ))
yi i i
m
i i j j i i
ji
l m
i j j i k
k j
y P y x
P y x f x y
Z x
Z x f x y
?
?
=
= =
=
= ?
= ?
?
? ?
 
In the above equations, fj(xi,yk) is a binary valued feature 
function, and ?j is a weight that indicates how important 
feature fj is for the model. Z(xi) is a normalization factor. 
We estimate the weights using the improved iterative 
scaling (IIS) algorithm.  
 
For our task, we first compare a character-based 
MaxEnt model to a word-based model. Since 
recognition errors also lead to segmentation errors 
which in turn have an adverse effect on the NER 
performance, we experiment with disregarding the word 
boundaries in the ASR hypothesis and instead re-
segment using a MaxEnt segmenter. We also compare 
an approach of one-pass identification/classification to a 
two-pass approach where the identified NE candidates 
are classified later. In addition, we propose a hybrid 
approach of using one-pass identification/classification 
results, discarding the extracted NE tags, and re-
classifying the extracted NE in a second pass.  
                                                 
1 We exclude from the present focus the slight improvements 
that are usually possible to obtain by combination of multiple 
models, usually through ad hoc methods such as voting. 
2.3 Experimental setup  
We use two annotated corpora for training. One is a 
corpus of People?s Daily newspaper from January 1998, 
annotated by the Institute of Computational Linguistics 
of Beijing University (the ?PFR? corpus). This corpus 
consists of about 20k sentences, annotated with word 
segmentation, part-of-speech tags and three named-
entity tags including person (PER), location (LOC) and 
organization (ORG) . We use the first 6k sentences to 
train our NER system. Our system is then evaluated on 
2k sentences from People?s Daily and 1k sentences from 
the BBN ASR output. The results are shown in Tables 1 
and 3. 
  
To compare our system to the IBM baseline described 
in (Jing et al 2003), we need to evaluate our system on 
the same corpus as they used. Among the data they used, 
the only publicly available corpus is a human-generated 
transcription of broadcast news, provided by NIST for 
the Information Extraction ? Entity Recognition 
evaluation (the ?IEER? corpus). This corpus consists of 
10 hours of training data and 1 hour of test data. Ten 
categories of NEs were annotated, including person 
names, location, organization, date, duration, and 
measure. A comparison of results is shown in Table 2. 
2.4 Results and discussion 
From text to speech 
Table 1 compares the NER performances of the same 
MaxEnt model on the Chinese textual PFR test data and 
the one-best BBN ASR hypotheses. We can see a 
significant drop in performance in the latter. These 
results support the claim that transferring NER 
approaches from text to spoken language is a 
significantly more difficult task for Chinese than for 
English.  We argue that this is due to the combination of 
different factors specific to spoken Chinese. First, 
Chinese has a large number of homonyms that leads to a 
degradation in speech recognition accuracy which in 
turn leads to low NER accuracy. Second, the vocabulary 
used in Chinese person names is an open set so many 
characters/words are unseen in the training data.  
 
Comparison to IBM baseline 
Table 2 compares results on IEER data from our 
baseline word-based MaxEnt model compared with that 
of IBM?s HMM word-based model. These two models 
achieved almost the same results, which show that our 
NER system based on MaxEnt is state-of-the-art.  
 
Re-segmentation effect 
Table 3 shows that by discarding word boundaries from 
the ASR hypothesis, and then re-segmenting using our 
MaxEnt segmenter, we obtained a better performance in 
most cases. We believe that some reduction in 
segmentation errors due to recognition errors is obtained 
this way; for example, in the ASR output, two words 
??  ? ? in ???  ?  ?  ???   ?  ? ? are 
misrecognized as one word ????, which can be 
corrected by re-segmentation. 
 
Post-classification effect 
Table 3 also shows that the one-pass 
identification/classification method yields better results 
than the two-pass method. However, there are still 
errors in the one-pass output where the bracketing is 
correct, but the NE classification is wrong. In particular, 
the type ORG is easily confusable with LOC in Chinese. 
Both types of NEs tend to be rather long. We propose a 
hybrid approach by first using the one-pass method to 
extract NEs, and then removing all type information, 
combining words of one NE to a whole NE-word and 
post-classifying all the NE-words again. Our results in 
Figure 1 show that the post-classification combined 
with the one-pass approach performs much better on all 
types. 
Table 1. NER results on Chinese speech data are worse 
than on Chinese text data. 
Table 2. Our NER baseline is comparable to the IBM 
baseline. 
Table 3. The character model is better than the word 
model, and one-pass NER is better than two-pass. 
3. Using N-Best Lists to Improve NER 
Miller et al (1999) performed NER on the one-best 
hypothesis of English Broadcast News data.  Palmer & 
Ostendorf (2001) and Horlock & King (2003) carried 
out English NER on word lattices. We are interested in 
investigating how to best utilize the n-best hypothesis 
from the ASR system to improve NER performances. 
From Figure 1, we can see that recall increases as the 
number of hypotheses increases. Thus it would appear 
possible to find a way to make use of the n-best ASR 
output, in order to improve the NER performance. 
However, we can expect it to be difficult to get 
significant improvement since the same figure (Figure 1) 
shows that precision drops much more quickly than 
recall. This is because the nth hypothesis tends to have 
more character errors than the (n-1)th hypothesis, which 
may lead to more NER errors. Therefore the question is, 
given n NE-tagged hypotheses, what is the best way to 
use them to obtain a better NER overall performance 
than by using the one-best hypothesis alone? 
                                                                                                                
One simple approach is to allow all the hypotheses to 
vote on a possible NE output. In simple voting, a 
recognized named-entity is considered correct only 
when it appears in more than 30 percent of the total 
number of all the hypotheses for one utterance. The 
result of this simple voting is shown in Table 4. Next, 
we propose a mechanism of weighted voting using 
confidence measure for each hypothesis. In one 
experiment, we use the MaxEnt NER score as 
confidence measure. In another experiment, we use all 
the six scores (acoustic, language model, number of 
words, number of phones, number of silence, or NER 
score) provided by the BBN ASR system as confidence 
measure. During implementation, an optimizer based on 
Powell?s algorithm is used to find the 6 weights (?k) for 
each score (Sk). For any given hypothesis, confidence 
measure is given by: 
PER LOC ORG  P R F P R F P R F 
Newspaper 
text .86 .76 .81 .87 .75 .81 .83 .83 .83 
1-best ASR 
hypothesis  .22 .18 .20 .75 .79 .77 .43 .35 .39 
Model Precision Recall F-measure
IBM HMM 77.51% 65.22% 70.83% 
MaxEnt 77.3% 65.4% 70.9% 
6
1
k k
k
W S ?
=
= ??  
The above confidence measure is then normalized into a 
final confidence measure for each hypothesis: 
1
exp( )?
exp( )
i
i N
l
l
WW
W
=
=
?
 
Finally, an NE output is considered valid if  
1
? ( ) 0.
N
i i
i
W NE?
=
? >? 3  
1,
( )
0,i
NE occurs in the i th hypothesis
NE
Otherwise
? ??= ??
 
PER LOC ORG  
P R F P R F P R F 
2-pass, 
word  .23  .18  .20  .75  .79  .77  .43 .35 .39  
1-pass, 
word .25  .20  .21  .76  .84  .80  .70 .25 .36  
2-pass, 
character  .53  .43  .48  .67  .70  .68  .75 .59 .66 
1-pass, 
character  .60  .45  .52  .56  .69  .62  .55 .35 .43 
3.1 Experimental setup 
We use the n-best hypothesis of 1,046 Broadcast News 
Chinese utterances from the BBN LVCSR system. n 
ranges from one to 300, averaging at 68. Each utterance 
has a reference transcription with no recognition error. 
3.2 Results and discussion 
Table 4 presents the NER results for the reference 
sentence, one best hypothesis, and different n-best 
voting methods. Results for the reference sentences 
show the upper bound performance (68% F-measure) of 
applying a MaxEnt NER system trained from the 
Chinese text corpus (e.g., PFR) to Chinese speech 
output (e.g., Broadcast News). From Table 4, we can 
conclude that it is possible to improve NER precision by 
using n-best hypothesis by finding the optimized 
combination of different acoustic, language model, 
NER, and other scores. In particular, since most errors 
in Chinese ASR seem to be for person names, using 
NER score on the n-best hypotheses can improve 
recognition results by a relative 6.7% in precision and 
1.7% in F-measure.   
 
PER LOC ORG Results 
F P F P F P 
Reference 
sentence 
0.71 0.75 0.78 0.77 0.56 0.72 
One best 0.46 0.50 0.75 0.74 0.54 0.69 
n-best  
simple vote 
0.45 0.59 0.76 0.75 0.56 0.71 
n-best 
weighted vote 
(NE score) 
0.46 0.59 0.77 0.76 0.55 0.71 
n-best 
weighted vote 
(all scores) 
0.48 0.53 0.75 0.73 0.55 0.69 
 
Table 4. n-best weighted voting with NE score gives the 
best performance.                                                                                                   
Recall 
 
Precision 
 
F-measure 
 
Figure 1. Post-classification improves NER performance. 
4. Conclusion 
We present the first known result for named entity 
recognition (NER) in realistic large-vocabulary spoken 
Chinese.  We apply a maximum entropy (MaxEnt) 
based system to the n-best output of the BBN LVCSR 
system on Chinese Broadcast News utterances. Our 
results support the claim that transferring NER 
approaches from text to spoken language is a 
significantly more difficult task for Chinese than for 
English.  We show that re-segmenting the ASR 
hypotheses improves the NER performance by 24%. We 
also show that applying post-classification improves the 
NER performance by 13%. Finally, we introduce a 
method of using n-best hypotheses that yields a useful 
6.7% relative improvement in NER precision, and 1.7% 
relative improvement in F-measure, by weighted voting.  
 
Institute of Computational Linguistics, Beijing University. 2001. The PFR 
corpus.  http://icl.pku.edu.cn/research/corpus/shengming.htm. 
Hongyan JING, Radu FLORIAN, Xiaoqiang LUO, Tong ZHANG and Abraham 
ITTYCHERIAH. 2003. HowtogetaChineseName(Entity): Segmentation and 
combination issues. Proceedings of EMNLP. Sapporo, Japan: July 2003. 
 
David MILLER, Richard SCHWARTZ, Ralph WEISCHEDEL and Rebecca STONE. 
1999. Named entity extraction from broadcast news. Proceedings of the 
DARPA Broadcast News Workshop. Herndon, Virginia: 1999. 37-40. 
 David D. PALMER, Mari OSTENDORF and John D. BURGER. 1999. Robust 
information extraction from spoken language data. Proceedings of 
Eurospeech 1999. Sep 1999. 
Acknowledgements. We would like to thank the Hong Kong 
Research Grants Council (RGC) for supporting this research 
in part via grants HKUST6206/03E, HKUST6256/00E, 
HKUST6083/99E, DAG03/04.EG30, and DAG03/04.EG09.  
Jian SUN, Ming ZHOU and Jianfeng GAO. 2003. A class-based language model 
approach to Chinese named entity identification. Computational Linguistics 
and Chinese Language Processing. 2003. 
References Erik F. TJONG KIM SANG. 2002. Introduction to the CoNLL-2002 Shared Task: Language-independent named entity recognition. Proceedings of CoNLL-
2002. Taipei, Taiwan: 2002. 155-158.  
Silviu CUCERZAN and David YAROWSKY. 1999. Language independent named 
entity recognition combining morphological and contextual evidence. 
Proceedings of the 1999 Joint SIGDAT Conference on EMNLP and VLC. 
University of Maryland, MD. 
Erik F. TJONG KIM SANG and Fien DE MEULDER. 2003. Introduction to the 
CoNLL-2003 Shared Task: Language-Independent Named Entity 
Recognition. Proceedings of CoNLL-2003. Edmonton, Canada. 142-147.  
James HORLOCK and Simon KING. 2003. Discriminative Methods for Improving 
Named Entity Extraction on Speech Data. Proceedings of Eurospeech 2003. 
Geneva. 
Dekai WU, Grace NGAI and Marine CARPUAT. 2003. A Stacked, Voted, Stacked 
Model for Named Entity Recognition. Proceedings of CoNLL-2003. 
Edmonton, Canada: 2003. 200-203.  
Proceedings of NAACL HLT 2007, Companion Volume, pages 213?216,
Rochester, NY, April 2007. c?2007 Association for Computational Linguistics
Speech Summarization Without Lexical Features
for Mandarin Broadcast News
Jian Zhang
Human Language Technology Center
Electronic and Computer Engineering
University of Science and Technology
Clear Water Bay,Hong Kong
zjustin@ust.hk
Pascale Fung
Human Language Technology Center
Electronic and Computer Engineering
University of Science and Technology
Clear Water Bay,Hong Kong
pascale@ee.ust.hk
Abstract
We present the first known empirical study
on speech summarization without lexi-
cal features for Mandarin broadcast news.
We evaluate acoustic, lexical and struc-
tural features as predictors of summary
sentences. We find that the summarizer
yields good performance at the average F-
measure of 0.5646 even by using the com-
bination of acoustic and structural features
alone, which are independent of lexical
features. In addition, we show that struc-
tural features are superior to lexical fea-
tures and our summarizer performs sur-
prisingly well at the average F-measure
of 0.3914 by using only acoustic features.
These findings enable us to summarize
speech without placing a stringent demand
on speech recognition accuracy.
1 Introduction
Speech summarization, a technique of extracting
key segments that convey the main content from
a spoken document or audio document, has be-
come a new area of study in the last few years.
There has been much significant progress made in
speech summarization for English or Japanese text
and audio sources (Hori and Furui, 2003; Inoue et
al., 2004; Koumpis and Renals, 2005; Maskey and
Hirschberg, 2003; Maskey and Hirschberg, 2005).
Some research efforts have focused on summariz-
ing Mandarin sources (Chen et al, 2006; Huang
et al, 2005), which are dependent on lexical fea-
tures. Considering the difficulty in obtaining high
quality transcriptions, some researchers proposed
speech summarization systems with non-lexical fea-
tures (Inoue et al, 2004; Koumpis and Renals,
2005; Maskey and Hirschberg, 2003; Maskey and
Hirschberg, 2006). However, there does not exist
any empirical study on speech summarization with-
out lexical features for Mandarin Chinese sources.
In this paper, we construct our summarizer with
acoustic and structural features, which are indepen-
dent of lexical features, and compare acoustic and
structural features against lexical features as predic-
tors of summary sentences.
In Section 2 we review previous work on broad-
cast news summarization. We describe the Mandarin
broadcast news corpus on which our system operates
in Section 3. In Section 4 we describe our summa-
rizer and these features used in experiments. We set
up our experiments and evaluate the results in Sec-
tion 5, followed by our conclusion in Section 6.
2 Previous Work
There have been many research efforts on speech
summarization. Some methods dependent on lexi-
cal features are presented (Inoue et al, 2004; Chen
et al, 2006; Huang et al, 2005). (Inoue et al,
2004) uses statistical methods to identify words to
include in a summary, based on linguistic and acous-
tic/prosodic features of the Japanese broadcast news
transcriptions; while (Chen et al, 2006) proposes
the use of probabilistic latent topical information for
extractive summarization of Mandarin spoken docu-
ments. (Huang et al, 2005) presents Mandarin spo-
213
ken document summarization scheme using acous-
tic, prosodic, and semantic information. Alterna-
tively, some methods which are independent of lex-
ical features are presented (Maskey and Hirschberg,
2003; Maskey and Hirschberg, 2006). (Maskey
and Hirschberg, 2003) extracts structural informa-
tion from audio documents to help summarization.
(Maskey and Hirschberg, 2006) focuses on how to
use acoustic information alone to help predict sen-
tences to be included in a summary and shows a
novel way of using continuous HMMs for summa-
rizing speech documents without transcriptions.
It is advantageous to build speech summarization
models without using lexical features: we can sum-
marize speech data without placing a stringent de-
mand on the speech recognition accuracy. In this pa-
per, we propose one such model on Mandarin broad-
cast news and compare the effectiveness of acous-
tic and structural features against lexical features as
predictors of summary sentences.
3 The Corpus and Manual Summaries
We use a portion of the 1997 Hub4 Mandarin corpus
available via LDC as experiment data. The related
audio data were recorded from China Central Tele-
vision(CCTV) International News programs. They
include 23-day broadcast from 14th January, 1997
to 21st April, 1997, which contain 593 stories and
weather forecasts. Each broadcast lasts approxi-
mately 32 minutes, and has been hand-segmented
into speaker turns. For evaluation, we manually
annotated these broadcast news, and extracted seg-
ments as reference summaries. We divide these
broadcast news stories into 3 types: one-turn news,
weather forecast, and several-turns news. The con-
tent of each several-turn news is presented by more
than one reporter, and sometimes interviewees. We
evaluate our summarizer on the several-turns news
corpus. The corpus has 347 stories which contain
4748 sentences in total.
4 Features and Methodology
4.1 Acoustic/Prosodic Features
Acoustic/prosodic features in speech summarization
system are usually extracted from audio data. Re-
searchers commonly use acoustic/prosodic variation
? changes in pitch, intensity, speaking rate ? and du-
ration of pause for tagging the important contents
of their speeches (Hirschberg, 2002). We also use
these features for predicting summary sentences on
Mandarin broadcast news.
Our acoustic feature set contains thirteen features:
DurationI, DurationII, SpeakingRate, F0I, F0II,
F0III, F0IV, F0V, EI, EII, EIII, EIV and EV. Du-
rationI is the sentence duration. DurationII is the
average phoneme duration. General phonetic stud-
ies consider that the speaking rate of sentence is re-
flected in syllable duration. So we use average syl-
lable duration for representing SpeakingRate. F0I is
F0?s minimum value. F0II is F0?s maximum value.
F0III equals to the difference between F0II and F0I.
F0IV is the mean of F0. F0V is F0 slope. EI is min-
imum energy value. EII is maximum energy value.
EIII equals to the difference between EII and EI.
EIV is the mean of energy value. EV is energy slope.
We calculate DurationI from the annotated manual
transcriptions that align the audio documents. We
then obtain DurationII and SpeakingRate by pho-
netic forced alignment. Next we extract F0 fea-
tures and energy features from audio data by using
Praat (Boersma and Weenink, 1996).
4.2 Structural Features
Each broadcast news of the 1997 Hub4 Mandarin
corpus has similar structure, which starts with an an-
chor, followed by the formal report of the story by
other reporters or interviewees.
Our structural feature set consists of 4 features:
Position, TurnI, TurnII and TurnIII. Position is de-
fined as follows: one news has k sentences, then we
set (1? (0/k)) as Position value of the first sentence
in the news, and set (1?((i?1)/k)) as Position value
of the ith sentence. TurnI is defined as follows: one
news has m turns, then we set (1? (0/m)) as TurnI
value of the sentences which belong to the first turn?s
content, and set (1?((j?1)/m)) as TurnI values of
the sentences which belong to the jth turn?s content.
TurnII is the previous turn?s TurnI value. TurnIII is
the next turn?s TurnI value.
4.3 Reference Lexical Features
Most methods for text summarization mainly utilize
lexical features. We are interested in investigating
the role of lexical features in comparison to other
features. All reference lexical features are extracted
214
from the manual transcriptions.
Our lexical feature set contains eight features:
LenI, LenII, LenIII, NEI, NEII, NEIII, TFIDF
and Cosine. For a sentence, we set the number of
words in the sentence as LenI value. LenII is the
previous sentence?s LenI value. LenIII is the next
sentence?s LenI value. For a sentence, we set the
number of Named Entities in the sentence as the
NEI value. We define the number of Named Enti-
ties which appear in the sentence at the first time in
a news as NEII value. NEIII value equals to the ra-
tio of the number of unique Named Entities to the
number of all Named Entities.
TFIDF is the product of tf and idf. tf is the frac-
tion: the numerator is the number of occurrences
of the considered word and the denominator is the
number of occurrences of all words in a story. idf is
the logarithm of the fraction: the numerator is the to-
tal number of sentences in the considered news and
the denominator is the number of sentences where
the considered word appears. Cosine means cosine
similarity measure between two sentence vectors.
4.4 Summarizer
Our summarizer contains the preprocessing stage
and the estimating stage. The preprocessing stage
extracts features and normalizes all features by
equation (1).
Nj = wj ?mean(wj)dev(wj) (1)
Here, wj is the original value of feature j which is
used to describe sentence i; mean(wj) is the mean
value of feature j in our training set or test set;
dev(wj) is the standard deviation value of feature
j in our training set or test set.
The estimating stage predicts whether each sen-
tence of the broadcast news is in a summary or not.
We use Radial Basis Function(RBF) kernel for con-
structing SVM classifier as our estimator referring to
LIBSVM (Chang and Lin, 2001), which is a library
for support vector machines.
5 Experiments and Evaluation
We use the several-turn news corpus, described in
Section 3, in our experiments. We use 70% of the
corpus consisting of 3294 sentences as training set
Table 1: Feature set Evaluation by F-measure
Feature Set SR10% SR15% SR20% Ave
Ac+St+Le .5961 .546 .5544 .5655
Ac+St .5888 .5489 .5562 .5646
St .5951 .5616 .537 .5645
Le .5175 .5219 .5329 .5241
Ac .3068 .4092 .4582 .3914
Baseline .21 .32 .43 .32
Ac: Acoustic; St: Structural; Le: Lexical
and the remaining 1454 sentences as held-out test
set, upon which our summarizer is tested.
We measure our summarizer?s performance by
precision, recall, and F-measure (Jing et al, 1998).
We explain these metrics as follows:
precision = Sman
?Ssum
Ssum (2)
recall = Sman
?Ssum
Sman (3)
F-Measure = 2? precision ? recallprecision + recall (4)
In equation (2), (3) and (4), Sman is the sentence
set of manual summaries or reference summaries;
Ssum is the sentence set of predicted summaries pro-
vided by our summarizer.
We have three versions of reference summaries
based on summarization ratio(SR): 10%, 15% and
20% respectively. So we build three baselines re-
ferring to different versions of reference summaries.
When using SR 10% summaries, we build the base-
lines by choosing the first 10% of sentences from
each story. Our baseline results in F-measure score
are given in Table 1.
We perform three sets of experiments with differ-
ent summarization ratios.
By using acoustic and structural features alone,
the summarizer produces the same performance as
by using all features. We can find the evidence from
Table 1 and Figure 1. On average, the combination
of acoustic and structural features yields good per-
formance: F-measure of 0.5646, 24.46% higher than
the baseline, only 0.09% lower than the average F-
measure produced by using all features. This find-
ing makes it possible to summarize speech without
215
A L AS S ALS
0.00
0.05
0.10
0.15
0.20
0.25
0.30
0.35
0.40
0.45
0.50
0.55
0.60
0.65
A:Acoustic, S: Structural, L:Lexical
 Positive Precision
 Positive Recall
 Positive F-measure
S
c
o
r
e
Feature sets
Figure 1: Performance comparison on SR10%
placing a stringent demand on the speech recogni-
tion accuracy.
In the same Mandarin broadcast program, the dis-
tribution and flow of summary sentences are rela-
tively consistent. Therefore, compared with speech
summarization on English sources, we can achieve
the different finding that structural features play
a key role in speech summarization for Mandarin
broadcast news. Table 1 shows the evidence. On
average, structural features are superior to lexical
features: F-measure of 0.5645, 24.45% higher than
the baseline and 4,04% higher than the average F-
measure produced by using lexical features.
Another conclusion we can draw from Table 1
is that acoustic features are important for speech
summarization on Mandarin broadcast news. On
average, even by using acoustic features alone our
summarizer yields competitive result: F-measure of
0.3914, 7.14% higher than the baseline. The similar
conclusion also holds for speech summarization on
English sources (Maskey and Hirschberg, 2006).
6 Conclusion
In this paper, we have presented the results of an
empirical study on speech summarization for Man-
darin broadcast news. From these results, we found
that by using acoustic and structural features alone,
the summarizer produces good performance: aver-
age F-measure of 0.5646, the same as by using all
features. We also found that structural features make
more important contribution than lexical features to
speech summarization because of the relatively con-
sistent distribution and flow of summary sentences
in the same Mandarin broadcast program. Moreover,
we have shown that our summarizer performed sur-
prisingly well by using only acoustic features: av-
erage F-measure of 0.3914, 7.14% higher than the
baseline. These findings also suggest that high qual-
ity speech summarization can be achieved without
stringent requirement on speech recognition accu-
racy.
References
P. Boersma and D. Weenink. 1996. Praat, a system for doing
phonetics by computer, version 3.4. Institute of Phonetic
Sciences of the University of Amsterdam, Report, 132:182.
Chih-Chung Chang and Chih-Jen Lin, 2001. LIBSVM: a library
for support vector machines.
B. Chen, Y.M. Yeh, Y.M. Huang, and Y.T. Chen. 2006. Chi-
nese Spoken Document Summarization Using Probabilistic
Latent Topical Information. Proc. ICASSP.
J. Hirschberg. 2002. Communication and prosody: Functional
aspects of prosody. Speech Communication, 36(1):31?43.
C. Hori and S. Furui. 2003. A new approach to automatic
speech summarization. Multimedia, IEEE Transactions on,
5(3):368?378.
C.L. Huang, C.H. Hsieh, and C.H. Wu. 2005. Spoken Docu-
ment Summarization Using Acoustic, Prosodic and Seman-
tic Information. Multimedia and Expo, 2005. ICME 2005.
IEEE International Conference on, pages 434?437.
A. Inoue, T. Mikami, and Y. Yamashita. 2004. Improvement of
Speech Summarization Using Prosodic Information. Proc.
of Speech Prosody.
H. Jing, R. Barzilay, K. McKeown, and M. Elhadad. 1998.
Summarization evaluation methods: Experiments and anal-
ysis. AAAI Symposium on Intelligent Summarization.
K. Koumpis and S. Renals. 2005. Automatic summariza-
tion of voicemail messages using lexical and prosodic fea-
tures. ACM Transactions on Speech and Language Process-
ing (TSLP), 2(1):1?24.
S. Maskey and J. Hirschberg. 2003. Automatic summarization
of broadcast news using structural features. Proceedings of
Eurospeech 2003.
S. Maskey and J. Hirschberg. 2005. Comparing lexical, acous-
tic/prosodic, structural and discourse features for speech
summarization. Interspeech 2005 (Eurospeech).
S. Maskey and J. Hirschberg. 2006. Summarizing Speech
Without Text Using Hidden Markov Models. Proc. NAACL.
216
Proceedings of NAACL HLT 2009: Short Papers, pages 13?16,
Boulder, Colorado, June 2009. c?2009 Association for Computational Linguistics
Semantic Roles for SMT: A Hybrid Two-Pass Model Dekai WU1          Pascale FUNG2 Human Language Technology Center HKUST 1Department of Computer Science and Engineering 2Department of Electronic and Computer Engineering University of Science and Technology, Clear Water Bay, Hong Kong dekai@cs.ust.hk    pascale@ee.ust.hk  Abstract We present results on a novel hybrid semantic SMT model that incorporates the strengths of both semantic role labeling and phrase-based statistical machine translation.  The approach avoids major complexity limitations via a two-pass architecture.  The first pass is per-formed using a conventional phrase-based SMT model.  The second pass is performed by a re-ordering strategy guided by shallow se-mantic parsers that produce both semantic frame and role labels.  Evaluation on a Wall Street Journal newswire genre test set showed the hybrid model to yield an improvement of roughly half a point in BLEU score over a strong pure phrase-based SMT baseline ? to our knowledge, the first successful application of semantic role labeling to SMT. 1 Introduction Many of the most glaring errors made by to-day?s statistical machine translation systems are those resulting from confusion of semantic roles.  Translation errors of this type frequently result in critical misunderstandings of the essential meaning of the original input language sentences ? who did what to whom, for whom or what, how, where, when, and why. Semantic role confusions are errors of adequacy rather than fluency.  It has often been noted that the dominance of lexically-oriented, precision-based metrics such as BLEU (Papineni et al 2002) tend to reward fluency more than adequacy.  The length penalty in the BLEU metric, in particular, is only an indirect and weak indicator of adequacy.  As a result, SMT work has been driven to optimize 
systems such that they often produce translations that contain significant role confusion errors de-spite reading fluently. The present work is inspired by the question of whether we can improve translation utility via a strategy of favoring semantic adequacy slightly more ? possibly at the expense of slight degrada-tions in lexical fluency. Shallow semantic parsing models have attained increasing levels of accuracy in recent years (Gildea and Jurafsky 2000; Sun and Jurafsky 2004; Pradhan et al 2004, 2005; Pradhan 2005; Fung et al 2006, 2007; Gim?nez and M?rquez 2007a, 2008).  Such models, which identify semantic frames within input sentences by marking its predicates, and labeling their arguments with the semantic roles that they fill. Evidence has begun to accumulate that semantic frames ? predicates and semantic roles ? tend to preserve consistency across translations better than syntactic roles do.  This is, of course, by design; it follows from the definition of semantic roles, which are less language-dependent than syntactic roles.  Across Chinese and English, for example, it has been reported that approximately 84% of se-mantic roles are preserved consistently (Fung et al 2006).  Of these, roughly 15% do not preserve syn-tactic roles consistently. Since this directly targets the task of determin-ing semantic correctness, we believe that the ade-quacy of MT output could be improved by leveraging the predictions of semantic parsers.  We would like to exploit automatic semantic parsers to identify inconsistent semantic frame and role map-pings between the input source sentences and their output translations. However, we take note of the difficult experi-ence in making syntactic and semantic models con-
13
tribute to improving SMT accuracy.  On the one hand, there is reason to be optimistic.  Over the past decade, we have seen an accumulation of evi-dence that SMT accuracy can be improved via tree-structured and syntactic models (e.g., Wu 1997; Wu and Chiang 2009), and more recently, work from lexical semantics has also at long last been successfully applied to increasing SMT accu-racy, in the form of techniques adapted from word sense disambiguation models (Chan et al 2007; Gim?nez and M?rquez  2007b; Carpuat and Wu 2007). On the other hand, both directions saw un-expected disappointments along the way (e.g., Och et al 2003; Carpuat and Wu 2005).  We are there-fore forewarned that it is likely to be at least as difficult to successfully adapt the even more com-plex types of lexical semantics modeling from se-mantic parsing and role labeling to the translation task. In this paper, we present a novel hybrid model that, for the first time to our knowledge, success-fully applies semantic parsing technology to the challenge of improving the quality of Chinese-English statistical machine translation.  The model makes use of a typical representative SMT system based on Moses, plus shallow semantic parsers for both English and Chinese. 2 Hybrid two-pass semantic SMT While the accuracy of shallow semantic parsers has been approaching reasonably high levels in recent years for well-studied languages like Eng-lish, and to a lesser extent, Chinese, the problem of excessive computational complexity is one of the primary challenges in adapting semantic parsing technology to the translation task. Semantic parses, by definition, are less likely than syntactic parses to obey clearly nested hierar-chical composition rules.  Moreover, the semantic parses are less likely to share an exactly isomor-phic structure across the input and output lan-guages, since the raison d??tre of semantic parsing is to capture semantic frame and role regularities independent of syntactic variation ? monolingually and cross-lingually. This makes it difficult to incorporate semantic parsing into SMT merely by applying the sort of dynamic programming techniques found in current syntactic and tree-structured SMT models, most of which rely on being able to factor the computation 
into independent computations on the subtrees.  In other words, the key computational obstacle is that the semantic parse of a larger string (or string pair, in the case of translation) is not in general strictly mechanically composable from the semantic parses of its smaller substrings (or substring pairs). In fact, the lack of easy compositionality is the reason that today?s most accurate shallow semantic parsers rely not primarily on compositional parsing techniques, but rather on ensembles of predictors that independently rate/rank a wide variety of fac-tors supporting the role assignments given a broad sentence-wide range of context features.  But while this improves semantic parsing accuracy, it poses a major obstacle for efficient tight integration into the sub-hypothesis construction and maintenance loops within SMT decoders. To circumvent this computational obstacle, the hybrid two-pass model defers application of the non-compositional semantic parsing information until a second error-correcting pass.  This imposes a division of labor between the two passes.   1. Apply a semantic parser for the input language to the input source sentence.  2. Apply a semantic parser for the output language to the baseline translation that was output by the first pass.  Note: this also pro-duces a shallow syntactic parse as a byproduct.  3. If the semantic frames (target predicates and their associated semantic roles) are all consistent between the input and output sentences, and are aligned to each other by the phrase alignments from the first pass, then finish immediately and output the base-line translation.  4. Segment the baseline translation by introducing segment boundaries around every constituent phrase whose shallow syn-tactic parse category (from step 2) was V, NP, or PP.  This breaks the baseline translation into a small number of coarse chunks to consider during re-ordering, instead of a large number of individual words.  5. Generate a set of candidate re-ordered translation hypotheses by iteratively moving constituent phrases whose predicate or se-mantic role label was mismatched to the input sentence.  Each new candidate generated may in turn spawn a further set of can-didates (especially since moving one constituent phrase may cause another?s predicate or semantic role label to change from matched to mismatched).  This search is performed breadth-first to favor fewer re-orderings (in case the hypothesis generation grows beyond allotted time).  6. Apply a semantic parser for the output language to each candi-date re-ordered translation hypothesis as it is generated.   7. Return the re-ordered translation hypothesis with the maximum match of semantic predicates and arguments.  Figure 1.  Algorithm for second pass. 
14
 Figure 2.  Example, showing translations after SMT first pass and after re-ordering second pass.  The first pass is performed using a conventional phrase-based SMT model. The phrase-based SMT model is assigned to the tasks of (a) providing an initial baseline hypothesis translation, and (b) fix-ing the lexical choice decisions.  Note that the lexi-cal choice decisions are not only at the single-word level, but are in general at the phrasal level. The second pass takes the output of the first pass, and re-orders constituent phrases correspond-ing to semantic predicates and arguments, seeking to maximize the cross-lingual match of the seman-tic parse of the re-ordered translation to that of the original input sentence.  The second pass algorithm performs the error correction shown in Figure 1. The design decision to allow the first pass to fix all lexical choices follows an insight inspired by an empirical observation from our error analyses:  the lexical choice decisions being made by today?s SMT models have attained fairly reasonable levels, and are not where the major problems of adequacy lie.  Rather, the ordering of arguments in relation to their predicates is often where the main failures of adequacy occur.  By avoiding lexical choice variations while considering re-ordering hypothe-ses, a significantly larger amount of re-ordering can be done without further increasing computa-tional complexity.  So we sacrifice a small amount of fluency by allowing re-ordering without com-pensating lexical choice ? in exchange for gaining potentially a larger amount of fluency by getting the predicate-argument structure right. The model has a similar rationale for employing a re-ordering pass instead of re-ranking n-best lists or lattices.  Oracle analysis of n-best lists and lat-tices show that they often focus on lexical choice alternatives rather than re-ordering / role variations which are more important to semantic adequacy. 
3 Experiment A Chinese-English experiment was conducted on the two-pass hybrid model. A phrase-based SMT baseline model was built by augmenting the open source statistical machine translation decoder Moses (Koehn et al 2007) with additional pre-processors.  English and Chinese shallow semantic parsers followed those discussed in Section 1. The model was trained on LDC newswire paral-lel text consisting of 3.42 million sentence pairs, containing 64.1 million English words and 56.9 million Chinese words. The English was tokenized and case-normalized; the Chinese was tokenized via a maximum-entropy model (Fung et al 2004). Phrase translations were extracted via the grow-diag-final heuristic. The language model is a 6-gram model trained with Kneser-Ney smoothing using the SRI lan-guage modeling toolkit (Stolcke 2002). The test set of Wall Street Journal newswire sentences was randomly extracted from the Chi-nese-English Bilingual Propbank.  Although we did not make use of the Propbank annotations, this would potentially allow other types of analyses in the future. The phrase-based SMT model used for the first pass achieves a BLEU score of 42.99, establishing a fairly strong baseline to begin with. In comparison, the automatically error-corrected translations that are output by the second pass achieve a BLEU score of 43.51.  This repre-sents approximately half a point improvement over the strong baseline. An example is seen in Figure 2.  The SMT first pass translation has an ARG0 National Develop-ment Bank of Japan in the capital market which is badly mismatched to both the input sentence?s 
15
ARG0 ?? ?? ?? and ARGM-LOC ? ?? ?? ??.  The second pass ends up re-ordering the constituent phrase corresponding to the mis-matched ARGM-LOC, of Japan in the capital market, to follow the PRED issued, where the new English semantic parse now assigns most of its words the correctly matched ARGM-LOC seman-tic role label.  Similarly, samurai bonds 30 billion yen is re-ordered to 30 billion yen samurai bonds. 4 Discussion and conclusion To our knowledge, this is a first result demonstrat-ing that shallow semantic parsing can improve translation accuracy of SMT models.  We note that accuracy here was measured via BLEU, and it has been widely observed that the negative impacts of semantic predicate-argument errors on the utility of the translation are underestimated by evaluation metrics based on lexical criteria such as BLEU. We conjecture that more expensive manual evalua-tion techniques which directly measure translation utility could even more strongly reveal improve-ment in role confusion errors. The hybrid two-pass approach can be compared with the greedy re-ordering based strategy of the ReWrite decoder (Germann et al 2001), although our search is breadth-first rather than purely greedy.  Whereas ReWrite was based on word-level re-ordering, however, our approach is based on constituent phrase re-ordering, and the phrases to be re-ordered are more selectively chosen via the semantic parse labels.  Moreover, the objective function being maximized by ReWrite is still the SMT model score; whereas in our case the new objective function is cross-lingual semantic predi-cate-argument match (plus an implicit search bias toward fewer re-orderings). The hybrid two-pass approach can also be com-pared with serial combination architectures for hy-brid MT (e.g., Ueffing et al 2008).  But whereas Ueffing et al take the output from a first-pass rule-based MT system, and then correct it using a sec-ond-pass SMT system, our two-pass semantic SMT model does the reverse: it takes the output from a first-pass SMT system, and then corrects it with the aid of semantic analyzers.  Acknowledgments.  Thanks to Chi-kiu Lo and Zhaojun Wu.  This material is based upon work supported in part by the Defense Advanced Research Projects Agency (DARPA) under GALE Contract No. HR0011-06-C-0023, and by the Hong Kong Research Grants Council (RGC) research grants GRF621008, GRF612806, DAG03/04.EG09, RGC6256/00E, and RGC6083/99E. 
References Marine Carpuat and Dekai Wu. 2005. Word sense disambiguation vs. statistical machine translation. 43rd Annual Meeting of the Association for Computa-tional Linguistics (ACL-2005). Ann Arbor, MI: Jun 2005. Marine Carpuat and Dekai Wu. 2007. Improving statistical machine translation using word sense disambiguation. 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Lan-guage Learning (EMNLP-CoNLL 2007). Prague: Jun 2007. 61-72. Yee Seng Chan, Hwee Tou Ng and David Chiang 2007. Word sense disam-biguation improves statistical machine translation. 45th Annual Meeting of the Association for Computational Linguistics (ACL-07), Prague: Jun 2007. Pascale Fung, Grace Ngai, Yongsheng Yang and Benfeng Chen. 2004. A maxi-mum-entropy Chinese parser augmented by transformation-based learning. ACM Transactions on Asian Language Information Processing (TALIP) 3(2): 159-168. Pascale Fung, Zhaojun Wu, Yongsheng Yang and Dekai Wu. 2006. Automatic learning of Chinese/English semantic structure mapping. IEEE/ACL 2006 Workshop on Spoken Language Technology (SLT 2006). Aruba: Dec 2006. 230-233. Pascale Fung, Zhaojun Wu, Yongsheng Yang and Dekai Wu. 2007. Learning Bilingual Semantic Frames: Shallow Semantic Parsing vs. Semantic Role Projection.  11th Conference on Theoretical and Methodological Issues in Machine Translation (TMI 2007). Sk?vde, Sweden: Sep 2007. 75-84. Ulrich Germann, Michael Jahr, Kevin Knight, Daniel Marcu and Kenji Yamada. Fast Decoding and Optimal Decoding for Machine Translation. 2001. 39th Annual Meeting of the Association for Computational Linguistics (ACL-2001). Toulouse: July 2001. Daniel Gildea and Daniel Jurafsky. 2000. Automatic Labeling of Semantic Roles. 38th Annual Conference of the Association for Computational Lin-guistics (ACL-2000). 512?520, Hong Kong: Oct 2000. Jes?s Gim?nez and Llu?s M?rquez. 2007. Linguistic Features for Automatic Evaluation of Heterogeneous MT Systems. WMT 2007 (ACL'07). Jes?s Gim?nez and Llu?s M?rquez. 2007. Discriminative Phrase Selection for Statistical Machine Translation. Learning Machine Translation. NIPS Workshop Series. MIT Press.  Jes?s Gim?nez and Llu?s M?rquez. 2008. A Smorgasbord of Features for Auto-matic MT Evaluation. 3rd ACL Workshop on Statistical Machine Transla-tion (shared evaluation task). Pages 195-198, Columbus, Ohio: Jun 2008. Alessandro Moschitti and Roberto Basili. 2005. Verb subcategorization kernels for automatic semantic labeling. ACL-SIGLEX Workshop on Deep Lexical Acquisition. Ann Arbor: Jun 2005. 10-17. Franz Josef Och, Daniel Gildea, Sanjeev Khudanpur, Anoop Sarkar, Kenji Yamada, Alex Fraser, Shankar Kumar, Libin Shen, David Smith, Katherine Eng, Viren Jain, Zhen Jin and Dragomir Radev. 2004. A smorgasbord of features for statistical machine translation. Human Language Technology Conference of the North American Chapter of the Association for Computa-tional Linguistics (HLT/NAACL-2004). Boston: May 2004. Kishore Papineni, Salim Roukos, Todd Ward and Wei-Jing Zhu. 2002. BLEU: A method for automatic evaluation of machine translation. 40th Annual Meeting of the Association for Computational Linguistics (ACL-2002). Sameer Pradhan. 2005. ASSERT: Automatic Statistical SEmantic Role Tagger. http://oak.colorado.edu/assert/. Sameer Pradhan, Kadri Hacioglu, Valerie Krugler, Wayne Ward, James H. Martin and Daniel Jurafsky. 2005. Support Vector Learning for Semantic Argument Classification. Machine Learning 60(1-3): 11-39. Sameer Pradhan, Wayne Ward, Kadri Hacioglu, James Martin and Daniel Juraf-sky. 2004. Shallow Semantic Parsing using Support Vector Machines. Hu-man Language Technology/North American Chapter of the Association for Computational Linguistics (HLT/NAACL-2004). Boston: May 2004. Andreas Stolcke. 2002. SRILM ? An Extensible Language Modeling Toolkit. International Conference on Spoken Language Processing (ICSLP-2002). Denver, Colorado: Sep 2002. Honglin Sun and Daniel Jurafsky. 2004. Shallow Semantic Parsing of Chinese. Human Language Technology Conference of the North American Chapter of the Association for Computational Linguistics (HLT/NAACL-2004). 249-256. Boston: May 2004.  Nicola Ueffing, Jens Stephan, Evgeny Matusov, Lo?c Dugast, George Foster, Roland Kuhn, Jean Senellart and Jin Yang. 2008. Tighter Integration of Rule-based and Statistical MT in Serial System Combination. 22nd Interna-tional Conference on Computational Linguistics (COLING 2008). Manches-ter: Aug 2008. Dekai Wu. 1997. Stochastic Inversion Transduction Grammars and bilingual parsing of parallel corpora. Computational Linguistics 23(3): 377-404. Dekai Wu and David Chiang (eds). 2009. Proceedings of SSST-3, Third Work-shop on Syntax and Structure in Statistical Translation (NAACL-HLT 2009). Boulder, CO: Jun 2009. Nianwen Xue and Martha Palmer. 2005. Automatic Semantic Role Labeling for Chinese Verbs. 19th International Joint Conference on Artificial Intelli-gence. Edinburgh, Scotland. 
16
Proceedings of the COLING/ACL 2006 Main Conference Poster Sessions, pages 239?246,
Sydney, July 2006. c?2006 Association for Computational Linguistics
Robust Word Sense Translation by EM Learning of Frame Semantics   
Pascale Fung and Benfeng Chen 
Human Language Technology Center 
Department of Electrical & Electronic Engineering 
University of Science & Technology (HKUST) 
Clear Water Bay 
Hong Kong 
{pascale,bfchen}@ee.ust.hk 
Abstract 
We propose a robust method of auto-
matically constructing a bilingual word 
sense dictionary from readily available 
monolingual ontologies by using esti-
mation-maximization, without any an-
notated training data or manual tuning. 
We demonstrate our method on the 
English FrameNet and Chinese HowNet 
structures. Owing to the robustness of 
EM iterations in improving translation 
likelihoods, our word sense translation 
accuracies are very high, at 82% on av-
erage, for the 11 most ambiguous words 
in the English FrameNet with 5 senses 
or more. We also carried out a pilot 
study on using this automatically gener-
ated bilingual word sense dictionary to 
choose the best translation candidates 
and show the first significant evidence 
that frame semantics are useful for 
translation disambiguation. Translation 
disambiguation accuracy using frame 
semantics is 75%, compared to 15% by 
using dictionary glossing only.  These 
results demonstrate the great potential 
for future application of bilingual frame 
semantics to machine translation tasks.  
1 Introduction 
As early as in the 1950s, semantic nets were in-
vented as an ?interlingua? for machine transla-
tion.  
The ?semantic net? or ?semantic map? that 
humans possess in the cognitive process is a 
structure of concept classes and lexicon (Illes and 
Francis 1999). In addition, the frame-semantic 
representation of predicate-argument relations 
has gained much attention in the research com-
munity. The Berkeley FrameNet (Baker et al 
1998) is such an example.  
We suggest that in addition to dictionaries, bi-
lingual frame semantics (word sense dictionary) 
is a useful resource for lexical selection in the 
translation process of a statistical machine trans-
lation system. Manual inspection of the contras-
tive error analysis data from a state-of-the-art 
SMT system showed that around 20% of the er-
ror sentences produced could have been avoided 
if the correct predicate argument information was 
used (Och et al 2003). Therefore, frame seman-
tics can provide another layer of translation dis-
ambiguation in these systems. 
We therefore propose to generate a bilingual 
frame semantics mapping (word sense diction-
ary), simulating the ?semantic map? in a bilin-
gual speaker. Other questions of interest to us 
include how concept classes in English and Chi-
nese break down and map to each other.  
This paper is organized as follows. In section 
2, we present the one-frame-two-languages idea 
of bilingual frame semantics representation. In 
section 3, we explain the EM algorithm for gen-
erating a bilingual ontology fully automatically. 
In section 4, we present an evaluation on word 
sense translation. Section 5 describes an evalua-
tion on how well  bilingual frame semantics can 
improve translation disambiguation. We then 
discuss related work in section 6, conclude in 
section 7, and finally discuss future work in sec-
tion 8. 
2 One Frame Two Languages  
The challenge of translation disambiguation is to 
select the target word cl* with the correct seman-
tic frame f--(cl,f), among the multitude of transla-
tion candidates Pr(cl|el). We suggest that while a 
source word in the input sentence might have 
multiple translation candidates, the correct target 
word must have the same sense, i.e., belong to 
the same semantic frame, as the source word (i.e. 
Pr(cl,f|el,f) is high). For example, ?burn| ?
239
(tang)? carries the ?cause_harm|damage? 
sense, whereas ?burn|? (shao)? carries the 
?heat|cooking? sense. The source sentence ?My 
hands are burned? has the 
?cause_harm|damage? sense, therefore the cor-
rect translation of ?burn? is ??(tang)? not 
??(shao)?. The frame semantics information 
of the source word can thus lead to the best trans-
lation candidate. 
Whereas some translation ambiguities are 
preserved over languages, most are not. In par-
ticular, for languages as different as English and 
Chinese, there is little overlap between how lexi-
con is broken-down (Ploux and Ji 2003). Some 
cognitive scientists suggest that a bilingual 
speaker tends to group concepts in a single se-
mantic map and simply attach different words in 
English and Chinese to the categories in this 
map.  
Based on the above, we propose the one-
frame-two-languages idea for constructing a bi-
lingual word sense dictionary from monolingual 
ontologies.  
FrameNet (Baker et al 1998) is a collection of 
lexical entries grouped by frame semantics. Each 
lexical entry represents an individual word sense, 
and is associated with semantic roles and some 
annotated sentences. Lexical entries with the 
same semantic roles are grouped into a ?frame? 
and the semantic roles are called ?frame ele-
ments?. Each frame in FrameNet is a concept 
class and a single word sense belongs to only one 
frame. However, the Chinese HowNet represents 
a hierarchical view of lexical semantics in Chi-
nese.  
HowNet (Dong and Dong 2000) is a Chinese 
ontology with a graph structure of word senses 
called ?concepts?, and each concept contains 7 
fields including lexical entries in Chinese, Eng-
lish gloss, POS tags for the word in Chinese and 
English, and a definition of the concept including 
its category and semantic relations (Dong and 
Dong, 2000). Whereas HowNet concepts corre-
spond roughly to FrameNet lexical entries, its 
semantic relations do not correspond directly to 
FrameNet semantic roles.  
A bilingual frame, as shown in Figure 1, 
simulates the semantic system of a bilingual 
speaker by having lexical items in two languages 
attached to the frame.   
3 Automatic Generation of Bilingual 
Frame Semantics 
To choose ?burn|?(tang)? instead of ?burn|?
(shao)?in the translation of ?My hands are 
burned?, we need to know that  ??(tang)? 
belongs to the ?cause_harm? frame, but ??
(shao)? belongs to the ?heat? frame. In other 
words, we need to have a bilingual frame seman-
tics ontology. Much like a dictionary, this bilin-
gual ontology forms part of the translation 
?lexicon?, and can be used either by human 
translators or automatic translation systems.  
Such a bilingual frame semantics ontology 
also provides a simulation of the ?concept lexi-
con" of a bilingual person, as suggested by cog-
nitive scientists.  
Figure 1 shows an example of a bilingual 
frame that possibly corresponds to the semantic 
structure in a bilingual person. 
 
 
Figure 1. An example bilingual frame 
 
 
We previously proposed using the Chinese 
HowNet and a bilingual lexicon to map the Eng-
lish FrameNet into a bilingual BiFrameNet (Fung 
and Chen 2004). We used a combination of 
frame size thresholding and taxonomy distance 
to obtain the final alignment between FrameNet 
frames and HowNet categories, to generate the 
BiFrameNet.  
Our previous algorithm had the disadvantage 
of requiring the ad hoc tuning of thresholds. This 
results in poor performance on lexical entries 
from small frames (i.e. frames with very few 
lexical entries). The tuning process also means 
that a development set of annotated data is 
needed. In this paper, we propose a fully auto-
matic estimation-maximization algorithm in-
stead, to generate a similar FrameNet to HowNet 
240
bilingual ontology, without requiring any anno-
tated data or manual tuning. As such, our method 
can be applied to ontologies of any structure, and 
is not restricted to FrameNet or HowNet.  
Our approach is based on the following as-
sumptions: 
1. A source semantic frame is mapped to a tar-
get semantic frame if many word senses in 
the two frames translate to each other; 
2. A source word sense translates into a target 
word sense if their parent frames map to each 
other.  
The semantic frame in FrameNet is defined 
as a single frame, whereas in HowNet it is de-
fined as the category.  The formulae of our pro-
posed algorithm are listed in Figure 2.  
 
Variable definitions:  
cl : Chinese lexeme . 
cf : Chinese frame. 
(cl, cf) : the word sense entry in cf . 
el : English lexeme . 
ef : English frame.  
(el, ef) : the word sense entry in ef . 
(All variables are assumed to be independent of 
each other.) 
 
Model  parameters: 
Pr(cl|el): bilingual word pair probability from 
dictionary 
Pr(cf|ef): Chinese to English frame mapping 
probability. 
Pr(cl,cf|el,ef): Chinese to English word sense 
translation probability. 
 
(1) Word senses that belong to mapped frames  
are translated to each other: 
( ) ( )( )
clcl
efelcfcl
efcfelclefelcfcl
cf
?=
?= ?
?
   1)Pr(
y   probabilit priori a  theassume  wewhere
,|,Pr
|Pr)|Pr(,|,Pr
 
 
(2) Frames that have translated word senses 
are mapped to each other: 
???
??
? ? ?
? ?=
cf el cl
el cl
efelcfcl
efelcfcl
efcf
),|,Pr(
),|,Pr(
)|Pr(  
 
Figure 2. The bilingual frame semantics formulae 
 
In the initialization step of our EM algorithm, all 
English words in FrameNet are glossed into Chi-
nese using a bilingual lexicon with uniform 
probabilities Pr(cl|el). Next, we apply the EM 
algorithm to align FrameNet frames and HowNet 
categories. By using EM, we improve the prob-
abilities of frame mapping in Pr(cf|ef) and word 
sense translations in Pr(cl,cf|el,ef) iteratively: We 
estimate sense translations based on uniform bi-
lingual dictionary probabilities Pr(cl|el) first. 
The frame mappings are maximized by using the 
estimated sense translation. The a priori lexical 
probability Pr(cl) is assumed to be one for all 
Chinese words. Underlining the correctness of 
our algorithm, we note that the overall likeli-
hoods of the model parameters in our algorithm 
improve until convergence after 11 iterations. 
We use the alignment output after the conver-
gence step. That is, we obtain all word sense 
translations and frame mapping from the EM 
algorithm: 
 ( )
efefcfcf
efelefelcfclcfcl
cf
cfcl
?=
?=
  )|Pr(maxarg*
),(   ,|,Prmaxarg)*,(
),(
 
 
The mapping between FrameNet frames and 
HowNet categories is obviously not one-to-one 
since the two languages are different. The initial 
and final mappings before and after EM itera-
tions are shown in Figures 3a,b and 4a,b. Each 
point (i,j) in Figures 3a and b represents an 
alignment between FrameNet frame i to HowNet 
category j.  Before EM iterations, each English 
lexical item is glossed into its (multiple) Chinese 
translations by a bilingual dictionary. The parent 
frame of the English lexical item and those of all 
its Chinese translations are aligned to form an 
initial mapping. This initial mapping shows that 
each English FrameNet frame is aligned to an 
average of 56 Chinese HowNet categories. This 
mapping is clearly noisy. After EM iterations, 
each English frame is aligned to 5 Chinese cate-
gories on average, and each Chinese category is 
aligned to 1.58 English frames on average. 
241
 
Figure 3a. FrameNet to HowNet mapping before EM 
iterations. 
 
 
Figure 3b. FrameNet to HowNet mapping after EM 
iterations. 
 
We also plot the histograms of one-to-X 
mapping between FrameNet frames and HowNet 
categories before and after EM iterations in Fig-
ure 4. The horizontal axis is the number X in 
one-to-X mapping between English and Chinese 
frames. The vertical axis is the occurrence fre-
quency. For example, point (i,j) represents that 
there are j frames in English mapping to i catego-
ries in Chinese. Figure 4 shows that using lexical 
glossing only, there are a large number of frames 
that are aligned to over 150 of Chinese catego-
ries, while only a small number of English 
frames align to relatively few Chinese categories. 
After EM iterations, the majority of the English 
frames align to only a few Chinese categories, 
significantly improving the frame mapping 
across the two languages.  
  
Figure 4a. Histogram of one-to-X mappings between 
English frames and Chinese categories. Most English 
frames align to a lot of Chinese categories before EM 
learning.    
 
 
Figure 4b. Histograms of one-to-X mappings between 
English frames and Chinese categories. Most English 
frames only align to a few Chinese categories after 
EM learning.  
 
The above plots demonstrate the difference 
between FrameNet and HowNet structures. For 
example, ?boy.n? belongs to ?attention_getting? 
and ?people? frames in FrameNet. 
?boy.n|attention_getting? should translate into ?
??/waiter? in Chinese, whereas ?boy.n|people? 
has the sense of ???/male child?. However, 
in HowNet, both ???/waiter? and ???/male 
child? belong to the same category, human|?.  
 
burn.v,cause_harm --> ?.v,damage|??   
burn.v,cause_harm --> ?.v,damage|??  
burn.v,cause_harm --> ?.v,damage|??  
burn.v,cause_harm --> ?.v,damage|??  
burn.v,experience_bodily_harm --> ?
?.v,wounded|?? 
burn.v,heat --> ?.v,cook|??  
 
Figure 5. Example word sense translation of the 
English verb ?burn? in our bilingual frame se-
mantics mapping. 
 
242
An example of word sense translation from our 
algorithm output is shown in Figure 5. The word 
sense translations of the FrameNet lexical entries 
represent the simulated semantic world of a bi-
lingual person who uses the same semantic struc-
ture but with lexical access in two languages. For 
example, the frame ?cause_harm? now contains 
the bilingual word sense pair 
?burn.v,cause_harm --> ?.v,damage|?? ?; 
and the frame ?experience_bodily_harm? con-
tains the bilingual word sense pair 
?burn.v,experience_bodily_harm --> ?
?.v,wounded|???.   
4 Robust Word Sense Translation Using 
Frame Semantics 
We evaluate the accuracy of word sense transla-
tion in our automatically generated bilingual on-
tology,  by testing on the most ambiguous lexical 
entries in FrameNet, i.e. words with the highest 
number of frames. These words and some of 
their sense translations are shown in Table 1 be-
low.   
 
 
tie.n,clothing -> ?.n,part|??  
tie.v,cause_confinement -> ??.v,restrain|??  
tie.v,cognitive_connection -> ??.v,connect|?
? 
 
make.n,type -> ??.n,attribute|??  
make.v,building -> ??.v,build|??  
make.v,causation -> ?.v,CauseToDo|??  
 
roll.v,body-movement -> ??.v,wave|??  
roll.v,mass_motion -> ??.v,roll|?  
roll.v,reshaping -> ?.v,FormChange|?? 
 
feel.n,sensation -> ??.n,experience|??  
feel.v,perception_active -> ??.v,perception|?
? 
feel.v,seeking -> ?.v,LookFor|? 
 
Table 1. Example word sense translation out-
put 
 
The word sense translation accuracies of the 
above words are shown in Table 2. The results 
are highly positive given that those from previ-
ous work in word translation disambiguation us-
ing bootstrapping methods (Li and Li, 2003; 
Yarowsky 1995) achieved 80-90% accuracy in 
disambiguating between only two senses per 
word1.  
The only susceptibility of our algorithm is in 
its reliance on bilingual dictionaries. The sense 
translations of the words ?tie?, ?roll?, and ?look? 
are relatively less accurate due to the absence of 
certain translations in the dictionaries we used. 
For example, the ?bread/food? sense of the word 
?roll? is not found in the bilingual dictionaries at 
all.  
 
 
 
English 
word 
Number of 
frames/senses 
in FrameNet 
Sense 
translation 
accuracy 
tie 8 64% 
make 7 100% 
roll 6 55% 
feel 6 88% 
can 5 81% 
run 5 100% 
shower 5 100% 
burn 5 91% 
pack 5 85% 
drop 5 76% 
look 5 64% 
Average 5.6 82% 
Table 2. Translation accuracies of the most am-
biguous words in FrameNet 
 
We compare our results to that of our previ-
ous work (Fung and Chen 2004), by using the 
same bilingual lexicon. Table 3 shows that we 
have improved the accuracy of word sense trans-
lation using the current method.  
 
lexical 
entry 
Parent frame Accuracy 
(Fung & 
Chen 
2004) 
Accuracy
(this pa-
per) 
beat.v cause_harm 88.9% 100% 
move.v motion 100% 100% 
bright.a light_emission 79.1% 100% 
hold.v containing 22.4% 100% 
fall.v mo-
tion_directional 
87% 100% 
issue.v emanating 31.1% 100% 
Table 3. Our method improves word sense 
translation precision over Fung and Chen (2004).  
We note in particular that whereas the previ-
ous algorithm in Fung and Chen (2004) does not 
                                                          
1 We are not able to evaluate our algorithm on the same 
set of words as in (Li & Li 2003; Yarowsky 1995) since 
these words do not have entries in FrameNet.  
 
243
perform well on lexical entries from small frames 
(e.g. on ?hold.v? and ?issue.v?) due to ad hoc 
manual thresholding, the current method is fully 
automatic and therefore more robust.  In Fung 
and Chen (2004), semantic frames are mapped to 
each other if their lexical entries translate to each 
other above a certain threshold. If the frames are 
small and therefore do not contain many lexical 
entries, then these frames might not be correctly 
mapped. If the parent concept classes are not cor-
rectly mapped, then word sense translation accu-
racy suffers.  
The main advantage of our algorithm over our 
work in 2004 lies in the hill-climbing iterations 
of the EM algorithm. In the proposed algorithm, 
all concept classes are mapped with a certain 
probability, so no mapping is filtered out prema-
turely. As the algorithm iterates, it is more prob-
able for the correct bilingual word sense to be 
translated to each other, and it is also more prob-
able for the bilingual concept classes to be 
mapped to each other. After convergence of the 
algorithm, the output probabilities are optimal 
and the translation results are more accurate.  
5 Towards Translation Disambiguation 
using Frame Semantics  
As translation disambiguation forms the core of 
various machine translation strategies, we are 
interested in studying whether the generated bi-
lingual frame semantics can complement existing 
resources, such as bilingual dictionaries, for 
translation disambiguation.  
The semantic frame of the predicate verb and 
the argument structures in a sentence can be 
identified by the syntactic structure, part-of-
speech tags, head word, and other features in the 
sentence. The predicate verb translation corre-
sponds to the word sense translation we de-
scribed in the previous sections, Pr(cl,cf | el,ef).   
We intend to evaluate the effectiveness of bi-
lingual frame semantics mapping in disambiguat-
ing between translation candidates. For the 
evaluation set, we use 202 randomly selected 
example sentences from FrameNet, which have 
been annotated with predicate-argument struc-
tures.  
In the first step of the experiment, for each 
predicate word (el,ef), we find all its translation 
candidates of the predicate word in each sen-
tence, and annotate them with their HowNet 
categories to form a translated word sense 
Pr(cl,cf|el,ef). For the example sentence in Fig-
ure 6, there are altogether 147 word sense trans-
lations for (hold,detaining).  
 
Under South African law police could HOLD the man 
for questioning for up to 48 hours before seeking the 
permission of magistrates for an extension 
 ##HOLD,detaining 
# ??,engage|??-- 
# ?,guide|??-- 
# ??,regard|??-- 
# ??,restrain|??-- 
# ?,load|?? -- 
# ?,pretend|??-- 
# ??,hold|?-- 
? 
# ?,hold|? -- 
# ?,occupy|?? -- 
#  ??,hold|? -- 
# ?,occupy|?? -- 
? 
# ?,hold|? -- 
# ?,hold|? -- 
? 
# ?,hold|? -- 
# ?,speak|? -- 
# ?,KeepOn|??? -- 
? 
# ?,function|?? -- 
# ?,manage|?? -- 
# ??,detain|?? ++  
# ??,facilities|?? -- 
#  ??,own|? ? 
 
Figure 6. A FrameNet example sentence and predicate verb 
translations {Pr(cl,cf|el,ef)}.  
 
We then find the word sense translation with 
the highest probability among all HowNet and 
FrameNet class mappings from our EM algo-
rithm: 
  
( )
( )
( )?
?
?=
=
cf
cl
cl
efelcfcl
efcfelcl
efelcfclcl
,|,Pr
|Pr)|Pr(maxarg
,|,Prmaxarg*
 
 An example (el,ef) is (hold, detaining) and the 
cl*=argmax P(cl,cf|el,ef) found by our program 
is??. (cl,cf)* in this case is (??,detain|??). 
Human evaluators then look at the set of {cl*} 
and mark cl* as either true translations or erro-
neous. The accuracy of word sense translations 
on this evaluation set of example sentences is at 
74.9%.  
In comparison, we also look at Pr(cl|el), trans-
lation based on bilingual dictionary only, and 
find 
244
( ) ( )elclelclcl
clcl
,Prmaxarg|Prmaxarg* ==  
The translation accuracy of using bilingual 
dictionary only, is at a predictable low 15.8%.  
Our results are the first significant evidence 
of, in addition to bilingual dictionaries, bilingual 
frame semantics is a useful resource for the 
translation disambiguation task.  
6 Related Work 
The most relevant previous works include word 
sense translation and translation disambiguation 
(Li & Li 2003; Cao & Li 2002; Koehn and 
Knight 2000; Kikui 1999; Fung et al, 1999), 
frame semantic induction (Green et al, 2004; 
Fung & Chen 2004), and bilingual semantic 
mapping (Fung & Chen 2004; Huang et al 2004; 
Ploux & Ji, 2003, Ngai et al, 2002; Palmer & 
Wu 1995).  Other than the English FrameNet 
(Baker et al 1998), we also note the construction 
of the Spanish FrameNet (Subirats & Petruck, 
2003), the Japanese FrameNet (Ikeda 1998), and 
the German FrameNet (Boas, 2002). In terms of 
learning method, Chen and Palmer (2004) also 
used EM learning to cluster Chinese verb senses.  
Word Sense Translation  
Previous word sense translation methods are 
based on using context information to improve 
translation. These methods look at the context 
words and discourse surrounding the source 
word and use methods ranging from  boostrap-
ping (Li & Li 2003), EM iterations (Cao and Li, 
2002; Koehn and Knight 2000),  and the cohe-
sive relation between the source sentence and 
translation candidates (Fung et al 1999; Kikui 
1999).   
Our proposed translation disambiguation 
method compares favorably to (Li & Li 2003) in 
that we obtain an average of 82% precision on 
words with multiple senses, whereas they ob-
tained precisions of 80-90% on words with two 
senses. Our results also compare favorably to 
(Fung et al 1999; Kikui 1999) as the precision of 
our predicate verb in the input sentence transla-
tion disambiguation is about 75% whereas their 
precisions range from 40% to 80%, albeit on an 
independent set of words.  
Automatic Generation of Frame Semantics  
Green et al (2004) induced SemFrame automati-
cally and compared it favorably to the hand-
constructed FrameNet (83.2% precision in cover-
ing the FrameNet frames). They map WordNet 
and LDOCE, two semantic resources, to obtain 
SemFrame. Burchardt et al (2005) used Frame-
Net in combination with WordNet to extend cov-
erage. 
Bilingual Semantic Mapping 
Ploux and Ji, (2003) proposed a spatial model for 
matching semantic values between French and 
English. Palmer and Wu (1995) studied the map-
ping of change-of-state English verbs to Chinese. 
Dorr et al (2002) described a technique for the 
construction of a Chinese-English verb lexicon 
based on HowNet and the English LCS Verb Da-
tabase (LVD). They created links between 
HowNet concepts and LVD verb classes using 
both statistics and a manually constructed ?seed 
mapping? of thematic classes between HowNet 
and LVD. Ngai et al (2002) induced bilingual 
semantic network from WordNet and HowNet. 
They used lexical neighborhood information in a 
word-vector based approach to create the align-
ment between WordNet and HowNet classes 
without any manual annotation. 
7 Conclusion 
Based on the one-frame-two-languages idea, 
which stems from the hypothesis of the mind of a 
bilingual speaker, we propose automatically gen-
erating a bilingual word sense dictionary or on-
tology. The bilingual ontology is generated from 
iteratively estimating and maximizing the prob-
ability of a word translation given frame map-
ping, and that of frame mapping given word 
translations. We have shown that for the most 
ambiguous 11 words in the English FrameNet, 
the average word sense translation accuracy is 
82%. Applying the bilingual ontology mapping 
to translation disambiguation of predicate verbs 
in another evaluation, the accuracy of our 
method is at an encouraging 75%, significantly 
better than the 15% accuracy of using bilingual 
dictionary only. Most importantly, we have dem-
onstrated that bilingual frame semantics is poten-
tially useful for cross-lingual retrieval, machine-
aided and machine translation.  
8 Future Work 
Our evaluation exercise has shown the promise 
of using bilingual frame semantics for translation 
task. We are currently carrying out further work 
in the aspects of (1) improving the accuracy of 
source word frame identification and (2) incorpo-
rating bilingual frame semantics in a full fledged 
245
machine translation system. In addition, Frame-
Net has a relatively poor coverage of lexical en-
tries. It would be necessary to apply either semi-
automatic or automatic methods such as those in 
(Burchardt et al 2005, Green et al2004) to ex-
tend FrameNet coverage for final application to 
machine translation tasks. Last but not the least, 
we are interested in applying our method to other 
ontologies such as the one used for the Propbank 
data, as well as to other language pairs.  
Acknowledgement 
This work was partially supported by CERG# 
HKUST6206/03E and CERG# HKUST6213/02E 
of the Hong Kong Research Grants Council. We 
thank Yang, Yongsheng for his help in the final 
draft of the paper, and the anonymous reviewers 
for their useful comments.  
References 
Collin F. Baker, Charles J. Fillmore and John B. 
Lowe. (1998).The Berkeley FrameNet project. 
In Proceedings of the COLING-ACL, Montreal, 
Canada.  
Hans C. Boas. (2002). Bilingual FrameNet Diction-
aries for Machine Translation. In Proceedings of 
the Third International Conference on Language 
Resources and Evaluation. Las Palmas, Spain. 
Vol. IV: 1364-1371 2002. 
A. Burchardt, K. Erk, A. Frank. (2005). A WordNet 
Detour to FrameNet.  In 
Proceedings of the 2nd GermaNet Workshop, 
2005. 
Cao, Yunbo and Hang Li. (2002). Base Noun 
Phrase Translation Using Web Data and the EM 
Algorithm. In COLING 2002. Taipei, 2002. 
Jinying Chen and Martha Palmer. (2004). Chinese 
Verb Sense Discrimination Using EM Clustering 
Model with Rich Linguistic Features. In ACL 
2004. Barcelona, Spain, 2004. 
Dong, Zhendong., and Dong, Qiang. (2002) 
HowNet [online]. Available at 
http://www.keenage.com/zhiwang/e_zhiwang.ht
ml 
Bonnie J. Dorr, Gina-Anne Levow, and Dekang 
Lin.(2002).Construction of a Chinese-English 
Verb Lexicon for Machine Translation. In Ma-
chine Translation, Special Issue on Embedded 
MT, 17:1-2.  
Pascale Fung and Benfeng Chen. (2004). BiFra-
meNet: Bilingual Frame Semantics Resource 
Construction by Cross-lingual Induction. In 
COLING 2004. Geneva, Switzerland. August 
2004. 
Pascale Fung, Liu, Xiaohu, and Cheung, Chi Shun. 
(1999). Mixed-Language Query Disambiguation. 
In Proceedings of ACL ?99, Maryland: June 
1999. 
Daniel Gildea and Daniel Juraf-
sky.(2002).Automatic Labeling of Semantic 
Roles. In Computational Linguistics, Vol 28.3: 
245-288.  
Rebecca Green, Bonnie Dorr, Philip Resnik. 
(2004). Inducing Frame Semantic Verb Classes 
from WordNet and LDOCE. In ACL 2004.  
Fran?ois Grosjean in Lesley Milroy and Pieter 
Muysken (editors). One Speaker, Two Lan-
guages. Cambridge University Press, 1995.  
Chu-Ren Huang, Ru-Yng Chang, Hsiang-Pin Lee. 
(2004).Sinica BOW (Bilingual Ontological 
Wordnet): Integration of Bilingual WordNet and 
SUMO. In Proceedings of the 4th International 
Conference on 
Language Resources and Evaluation, (2004). 
Judy Illes and Wendy S. Francis. (1999). Conver-
gent cortical representation of semantic process-
ing in bilinguals. In Brain and Language, 
70(3):347-363, 1999.  
Satoko Ikeda. (1998). Manual response set in a 
stroop-like task involving categorization of Eng-
lish and Japanese words indicates a common 
semantic representation. In Perceptual and Mo-
tor Skills, 87(2):467-474, 1998. 
Liu Qun and Li, Sujian. (2002). Word Similarity 
Computing Based on How-net. In Computa-
tional Linguistics and Chinese Language Proc-
essing?Vol.7, No.2, August 2002, pp.59-76 
Philipp Koehn and Kevin Knight. (2000). Estimat-
ing Word Translation Probabilities from Unre-
lated Monolingual Corpora Using the EM 
Algorithm. In AAAI/IAAI 2000: 711-715 
Grace Ngai, Marine Carpuat, and Pascale Fung. 
(2002). Identifying Concepts Across Languages: 
A First Step towards a Corpus-based Approach 
to Automatic Ontology Alignment. In Proceed-
ings of COLING-02, Taipei, Taiwan. 
Franz Och et al (2003). 
http://www.clsp.jhu.edu/ws2003/groups/translate
/ 
Martha Palmer and Wu Zhibiao. (1995).Verb Se-
mantics for English-Chinese Translation. In Ma-
chine Translation 10: 59-92, 1995.  
Sabine Ploux and Hyungsuk Ji. (2003). A Model 
for Matching Semantic Maps between Lan-
guages (French/English, English/French). In 
Computational Linguistics 29(2):155-178, 2003. 
Carlos Subitrats and Miriam Petriuck. (2003). Su-
prirse: Spanish FrameNet. Workshop on Frame 
Semantics, International Congress of Linguists, 
July 2003. 
 
246
 
		Combining Optimal Clustering and Hidden Markov Models for
Extractive Summarization
Pascale Fung

Human Language Technology Center,
Dept. of Electrical & Electronic
Engineering,
University of Science & Technology
(HKUST)
Clear Water Bay, Hong Kong
pascale@ee.ust.hk
Grace Ngai

Dept. of Computing,
Hong Kong Polytechnic
University,
Kowloon, Hong Kong
csgngai@polyu.edu.hk
CHEUNG, Chi-Shun

Human Language Technology Center,
Dept. of Electrical & Electronic
Engineering,
University of Science & Technology
(HKUST)
Clear Water Bay, Hong Kong
eepercy@ee.ust.hk
Abstract
We propose Hidden Markov models with
unsupervised training for extractive sum-
marization. Extractive summarization se-
lects salient sentences from documents to
be included in a summary. Unsupervised
clustering combined with heuristics is a
popular approach because no annotated
data is required. However, conventional
clustering methods such as K-means do
not take text cohesion into consideration.
Probabilistic methods are more rigorous
and robust, but they usually require su-
pervised training with annotated data. Our
method incorporates unsupervised train-
ing with clustering, into a probabilistic
framework. Clustering is done by modi-
fied K-means (MKM)--a method that
yields more optimal clusters than the con-
ventional K-means method. Text cohesion
is modeled by the transition probabilities
of an HMM, and term distribution is
modeled by the emission probabilities.
The final decoding process tags sentences
in a text with theme class labels. Parame-
ter training is carried out by the segmental
K-means (SKM) algorithm. The output of
our system can be used to extract salient
sentences for summaries, or used for topic
detection. Content-based evaluation
shows that our method outperforms an ex-
isting extractive summarizer by 22.8% in
terms of relative similarity, and outper-
forms a baseline summarizer that selects
the top N sentences as salient sentences
by 46.3%.
1 Introduction
Multi-document summarization (MDS) is the
summarization of a collection of related documents
(Mani (1999)). Its application includes the summa-
rization of a news story from different sources
where document sources are related by the theme
or topic of the story. Another application is the
tracking of news stories from the single source
over different time frame. In this case, documents
are related by topic over time.

Multi-document summarization is also an exten-
sion of single document summarization. One of the
most robust and domain-independent summariza-
tion approaches is extraction-based or shallow
summarization (Mani (1999)). In extraction-based
summarization, salient sentences are automatically
extracted to form a summary directly  (Kupiec et.
al, (1995), Myaeng & Jang (1999), Jing et. al,
(2000), Nomoto & Matsumoto (2001,2002), Zha
(2002), Osborne (2002)), or followed by a synthe-
sis stage to generate a more natural summary
(McKeown & Radev (1999), Hovy & Lin (1999)).
Summarization therefore involves some theme or
topic identification and then extraction of salient
segments in a document. 

Story segmentation, document and sentence and
classification can often be accomplished by unsu-
pervised, clustering methods, with little or no re-
quirement of human labeled data (Deerwester
(1991), White & Cardie (2002), Jing et. al (2000)).
Unsupervised methods or hybrids of supervised
and unsupervised methods for extractive summari-
zation have been found to yield promising results
that are either comparable or superior to supervised
methods (Nomoto & Matsumoto (2001,2002)). In
these works, vector space models are used and
document or sentence vectors are clustered to-
gether according to some similarity measure
(Deerwester (1991), Dagan et al (1997)).

The disadvantage of clustering methods lies in
their ad hoc nature. Since sentence vectors are con-
sidered to be independent sample points, the sen-
tence order information is lost. Various heuristics
and revision strategies have been applied to the
general sentence selection schema to take into con-
sideration text cohesion (White & Cardie (2002),
Mani and Bloedorn (1999), Aone et. al (1999), Zha
(2002), Barzilay et al, (2001)). We would like to
preserve the natural linear cohesion of sentences in
a text as a baseline prior to the application of any
revision strategies.

To compensate for the ad hoc nature of vector
space models, probabilistic approaches have re-
gained some interests in information retrieval in
recent years (Knight & Marcu (2000), Berger &
Lafferty (1999), Miller et al, (1999)).  These re-
cent probabilistic methods in information retrieval
are largely inspired by the success of probabilistic
models in machine translation in the early 90s
(Brown et. al), and regard information retrieval as
a noisy channel problem. Hidden Markov Models
proposed by Miller et al (1999), and have shown
to outperform tf, idf in TREC information retrieval
tasks. The advantage of probabilistic models is that
they provide a more rigorous and robust frame-
work to model query-document relations than ad
hoc information retrieval. Nevertheless, such prob-
abilistic IR models still require annotated training
data.

In this paper, we propose an iterative unsupervised
training method for multi-document extractive
summarization, combining vectors space model
with a probabilistic model. We iteratively classify
news articles, then paragraphs within articles, and
finally sentences within paragraphs into common
story themes, by using modified K-means (MKM)
clustering and segmental K-means (SKM) decod-
ing. We obtain an initial clustering of article
classes by MKM, which determines the inherent
number of theme classes of all news articles. Next,
we use SKM to classify paragraphs and then sen-
tences. SKM iterates between a k-means clustering
step, and a Viterbi decoding step, to obtain a final
classification of sentences into theme classes. Our
MKM-SKM paradigm combines vector space clus-
tering model with a probabilistic framework, pre-
serving some of the natural sentence cohesion,
without the requirement of annotated data. Our
method also avoids any arbitrary or ad hoc setting
of parameters.

In section 2, we introduce the modified K-means
algorithm as a better alternative than conventional
K-means for document clustering. In section 3 we
present the stochastic framework of theme classifi-
cation and sentence extraction. We describe the
training algorithm in section 4, where details of the
model parameters and Viterbi scoring are pre-
sented. Our sentence selection algorithm is de-
scribed in Section 5. Section 6 describes our
evaluation experiments. We discuss the results and
conclude in section 7.
2 Story Segmentation using Modified K-
means (MKM) Clustering
The first step in multi-document summarization is
to segment and classify documents that have a
common theme or story. Vector space models can
be used to compare documents (Ando et al (2000),
Deerwester et al (1991)). K-means clustering is
commonly used to cluster related document or sen-
tence vectors together. A typical k-means cluster-
ing algorithm for summarization is as follows:

1. Arbitrarily choose K vectors as initial centroids;
2. Assign vectors closest to each centroid to its cluster;
3. Update centroid using all vectors assigned to each
cluster;
4. Iterate until average intra-cluster distance falls be-
low a threshold;

We have found three problems with the standard k-
means algorithm for sentence clustering. First, the
initial number of clusters k, has to be set arbitrarily
by humans. Second, the initial partition of a cluster
is arbitrarily set by thresholding. Hence, the initial
set of centroids is arbitrary. Finally, during cluster-
ing, the centroids are selected as the sentence
among a group of sentences that has the least aver-
age distance to other sentences in the cluster. All
these characteristics of K-means can be the cause
of a non-optimal cluster configuration at the final
stage.

To avoid the above problems, we propose using
modified K-means (MKM) clustering algorithm
(Wilpon & Rabiner(1985)), coupled with virtual
document centroids. MKM starts from a global
centroid and splits the clusters top down until the
clusters stabilize:

1. Compute the centroid of the entire training set;
2. Assign vectors closest to each centroid to its cluster;
3. Update centroid using all vectors assigned to each
cluster;
4. Iterate 2-4 until vectors stop moving between clus-
ters;
5. Stop if clusters stabilizes, and output final clusters,
else goto step 6;
6. Split the cluster with largest intra-cluster distance
into two by finding the pair of vectors with largest
distance in the cluster. Use these two vectors as new
centroids, and repeat steps 2-5.

In addition, we do not use any existing document
in the collection as the selected centroid. Rather,
we introduce virtual centroids that contain the ex-
pected value of all documents in a cluster. An ele-
ment of the centroid is the average weight of the
same index term in all documents within that clus-
ter:

M
w
M
m
m
i
i

=
=
1?

The vectors are document vectors in this step. The
number of clusters is determined after the clusters
are stabilized. The resultant cluster configuration is
more optimal and balanced than that from using
conventional k-means clustering.   Using the MKM
algorithm with virtual centroids, we segment the
collection of news articles into clusters of related
articles. Articles covering the same story from dif-
ferent sources now carry the same theme label.
Articles from the same source over different time
period also carry the same theme label. In the next
stage, we iteratively re-classify each paragraph,
and then re-classify each sentence in each para-
graph into final theme classes.

3 A Stochastic Process of Theme Classifi-
cation
After we have obtained story labels of each article,
we need to classify the paragraphs and then the
sentences according to these labels. Each para-
graph in the article is assigned the cluster number
of that article, as we assume all paragraphs in the
same article share the same story theme.

We suggest that the entire text generation process
can be considered as a stochastic process that starts
in some theme class, generates sentences one after
another for that theme class, then goes to the next
theme and generates the sentences, so on and so
forth, until it reaches the final theme class in a
document, and finishes generating sentences in that
class. This is an approximation of the authoring
process where a writer thinks of a certain structure
for his/her article, starts from the first section,
writes sentences in that section, proceeds to the
next section, etc., until s/he finishes the last sen-
tence in the last section.

Given a document of sentences, the task of sum-
mary extraction involves discovering the underly-
ing theme class transitions at the sentence
boundaries, classify each sentence according to
these theme concepts, and then extract the salient
sentences in each theme class cluster.

We want to find )|(maxarg DCP
C

  where D

 is a
document consisting of linearly ordered sentence
sequences ))(,),(,),2(),1(( TstsssD 

= , and C

is
a theme class sequence which consists of the class
labels of all the sentences in D

 ,
)))((,)),((,)),2(()),1((( TsctscscscC 

= .

Following Bayes Rule gives us
)(/)()|()|( DPCPCDPDCP  = . We assume )(DP  is
equally likely for all documents, so that finding the
best class sequence becomes:

)))(()),((,)),2(()),1(((
)))((),()),((),(,)),2((),2()),1((),1((maxarg
)()|(maxarg)|(maxarg
TsctscscscP
TscTstsctsscsscsP
CPCDPDCP
C
CC





?=
?

Note that the total number of theme classes is far
fewer than the total number of sentences in a
document and the mapping is not one-to-one. Our
task is similar to the concept of discourse parsing
(Marcu (1997)), where discourse structures are
extracted from the text. In our case, we are carry-
ing out discourse tagging, whereby we assign the
class labels or tags to each sentence in the docu-
ment.

We use Hidden Markov Model for this stochastic
process, where the classes are assumed to be hid-
den states.

We make the following assumptions:

? The probability of the sentence given its past only
depends on its theme class (emission probabilities);
? The probability of the theme class only depends on
the theme classes of the previous N sentences (tran-
sition probabilities).


The above assumptions lead to a Hidden Markov
Model with M states representing M different
theme classes. Each state can generate different
sentences according to some probability distribu-
tion?the emission probabilities. These states are
hidden as we only observe the generated sentences
in the text.  Every theme/state can transit to any
other theme/state, or to itself, according to some
probabilities?the transition probabilities.

2C
1C 3C
jC
?
=
L
i
jt CiSP
0
)|)(( 



Figure 1: An ergodic HMM for theme tagging

Our theme tagging task then becomes a search
problem for HMM: Given the observation se-
quence ))(,),(,),2(),1(( TstsssD 

= , and the
model ? , how do we choose a corresponding state
sequence )))((,)),((,)),2(()),1((( TsctscscscC 

=  ,
that best explains the sentence sequence?


To train the model parameter ? , we need to solve
another problem in HMM: How do we adjust the
model parameters ),,( pi? BA= , the transition,
emission and initial probabilities, to maximize the
likelihood of the observation sentence sequences
given our model?

In a supervised training paradigm, we can obtain
human labeled class-sentence pairs and carry out a
relative frequency count for training the emission
and transition probabilities. However, hand label-
ing some large collection of texts with theme
classes is very tedious. One main reason is that
there is a considerable amount of disagreement
between humans on manual annotation of themes
and topics. How many themes should there be?
Where should each theme start and end? 

It is therefore desirable to decode the hidden theme
or topic states using an unsupervised training
method without manually annotated data. Conse-
quently, we only need to cluster and label the ini-
tial document according to cluster number. In the
HMM framework, we then improve upon this ini-
tial clustering by iteratively estimate ),,( pi? BA= ,
and maximize )|( DCP   using a Viterbi decoder.

3.1 Sentence Feature Vector and Similarity
Measure
Prior to the training process, we need to define sen-
tence feature vector and the similarity measure for
comparing two sentence vectors.

As we consider a document D

 to be a sequence of
sentences, the sentences themselves are repre-
sented as feature vectors )(ts  of length L, where t
is the position of the sentence in the document and
L is the size of the vocabulary. Each element of the
vector )(ts  is an index term in the sentence,
weighted by its text frequency (tf) and inverse
document frequency (idf) where tf is defined as the
frequency of the word in that particular sentence,
and idf  is the inverse frequency of the word in the
larger document collection
N
dflog? where df is
the number of sentences this particular word ap-
pears in and N is the total number of sentences in
the training corpus.  We select the sentences as
documents in computing the tf and idf because we
are comparing sentence against sentence.

In the initial clustering and subsequent Viterbi
training process, sentence feature vectors need to
be compared to the centroid of each cluster. Vari-
ous similarity measures and metrics include the
cosine measure, Dice coefficient, Jaccard coeffi-
cient, inclusion coefficient, Euclidean distance, KL
convergence, information radius, etc (Manning &
Sch   tze (1999), Dagan et al (1997), Salton and
McGill (1983)).  We chose the cosine similarity
measure for its ease in computation:

 

= =
=
?
?
=
L
i
L
i
ii
L
i
ii
vsts
vsts
vsts
1 1
22
1
))(())((
)()(
))(),(cos(
4 Segmental K-means Clustering for Pa-
rameter Training
In this section, we describe an iterative training
process for estimation of our HMM parameters.
We consider the output of the MKM clustering
process in Section 2 as an initial  segmentation of
text into class sequences. To improve upon this
initial segmentation, we use an iterative Viterbi
training method that is similar to the segmental k-
means clustering for speech processing (Rabiner &
Juang(1993)). All articles in the same story cluster
are processed as follows: 

1. Initialization: All paragraphs in the same story class
are clustered again. Then all sentences in the same
paragraph shares the same class label as that para-
graph. This is the initial class-sentence segmentation.
Initial class transitions are counted.
2. (Re-)clustering: Sentence vectors with their class
labels are repartitioned into K clusters (K is obtained
from the MKM step previously) using the K-means
algorithm. This step is iterated until the clusters sta-
bilize.
3. (Re-)estimation of probabilities: The centroids of
each cluster are estimated. Update emission prob-
abilities from the new clusters.
4. (Re-)classification by decoding: the updated set of
model parameters from step 2 are used to rescore the
(unlabeled) training documents into sequences of
class given sentences, using Viterbi decoding. Up-
date class transitions from this output.
5. Iteration: Stop if convergence conditions are met,
else repeat steps 2-4.

The segmental clustering algorithm is iterated until
the decoding likelihood converges. The final
trained Viterbi decoder is then used to tag un-
annotated data sets into class-sentence pairs. 

In the following Sections 4.1 and 4.2, we discuss in
more detail steps 3 and 4.
4.1 Estimation of Probabilities
We need to train the parameters of our HMM such
that the model can best describe the training data.
During the iterative process, the probabilities are
estimated from class-sentence pair sequences ei-
ther from the initialization stage or the re-
classification stage.
4.1.1 Transition Probabilities: Text Cohesion
and Text Segmentation
Text cohesion (Halliday and Hasan (1996)) is an
important concept in summarization as it under-
lines the theme of a text segment based on connec-
tivity patterns between sentences (Mani (2002)).
When an author writes from theme to theme in a
linear text, s/he generates sentences that are tightly
linked together within a theme. When s/he pro-
ceeds to the next theme, the sentences that are gen-
erated next are quite separate from the previous
theme of sentences but are they themselves tightly
linked again.

As mentioned in the introduction, most extraction-
based summarization approaches give certain con-
sideration to the linearity between sentences in a
text. For example, Mani (1999) uses spread activa-
tion weight between sentence links,  (Barzilay et al
2001) uses a cohesion constraint that led to im-
provement in summary quality. Anone et al (1999)
uses linguistic knowledge such as aliases, syno-
nyms, and morphological variations to link lexical
items together across sentences.

Term distribution has been studied by many NLP
researchers. Manning & Sch?tze (1999) gives a
good overview of various probability distributions
used to describe how a term appears in a text. The
distributions are in general non-Gaussian in nature.

Our Hidden Markov Model provides a unified
framework to incorporate text cohesion and term
distribution information in the transition probabili-
ties of theme classes. The class of a sentence de-
pends on the class labels of the previous N
sentences. The linearity of the text is hence pre-
served in our model.  In the preliminary experi-
ment, we set N to be one, that is, we are using a bi-
gram class model.

4.1.2 Emission Probabilities: Poisson distribu-
tion of terms
For the emission probabilities, there are a number
of possible formulations. We cannot use relative
frequency counts of number of sentences in clus-
ters divided by the total sentences in the cluster
since most sentences occur only once in the entire
corpus. Looking at the sentence feature vector, we
take the view that the probability of a sentence
vector being generated by a particular cluster is the
product of the probabilities of the index terms in
the sentence occurring in that cluster according to
some distribution, and that these term distribution
probabilities are independent of each other.

For a sentence vector of length L, where L is the
total size of the vocabulary, its elements?the in-
dex terms?have certain probability density func-
tion (pdf). In speech processing, spectral features
are assumed to follow independent Gaussian dis-
tributions. In language processing, several models
have been proposed for term distribution, including
the Poisson distribution, the two-Poisson model for
content and non-content words (Bookstein and
Swanson (1975)), the negative binomial (Mosteller
and Wallace (1984), Church and Gale (1995)) and
Katz?s k-mixture (Katz (1996)). We adopt two
schemes for comparison (1) the unigram distribu-
tion of each index term in the clusters; (2) the Pois-
son distribution as pdf. for modeling the term
emission probabilities:
!
);(
k
ekp
k
i
i
i
?? ??=

At each estimation step of the training process, the
? for the Poisson distribution is estimated from the
centroid of each theme cluster. 1
                                                         
1

Strictly speaking, we ought to re-estimate the IDF in the k-mixture
during each iteration by using the re-estimated clusters from the k-
means step as the documents. However, we simplify the process by
using the pre-computed IDF from all training documents.
4.2 Viterbi Decoding: Re-classification with
sentence cohesion

After each re-estimation, we use a Viterbi decoder
to find the best class sequence given a document
containing sentence sequences.  The ?time se-
quence? corresponds to the sequence of sentences
in a document whereas the states are the theme
classes.

At each node of the trellis, the probability of a sen-
tence given any class state is computed from the
transition probabilities and the emission probabili-
ties. After Viterbi backtracking, the best class se-
quence of a document is found and the sentences
are relabeled by the class tags.

5 Salient Sentence Extraction
The SKM algorithm is iterated until the decoding
likelihood converges. The final trained Viterbi de-
coder is then used to tag un-annotated data sets
into class-sentence pairs. We can then extract sali-
ent sentences from each class to be included in a
summary, or for question-answering.

To evaluate the effectiveness of our method as a
foundation for extractive summarization, we ex-
tract sentences from each theme class in each
document using four features, namely:
(1) the position of the sentence
n
p 1= -- the further it is
from the title, the less important it is supposed to be;
(2) the cosine similarity of the sentence with the centroid of
its class ?1;
(3) its similarity with the first sentence in the article ?2; and
(4) the so-called Z model (Zechner (1996), Nomoto & Ma-
tsumoto (2000)), where the mass of a sentence is com-
puted as the sum of tf, idf values of index terms in that
sentence and the center of mass  is chosen as the salient
sentence to be included in a summary.

))(())))((log(1(maxarg
1
tsidftstfz i
L
i
i
s

=
?+= 

The above features are linearly combined to yield a
final saliency score for every sentence:

zwwwpwsw ?+?+?+?= 423121)( ??
                                                                                         

Our features are similar to those in an existing sys-
tem (Radev 2002), with the difference in the cen-
troid computation (and cluster definition), resulting
from our stochastic system.

6 Experiments
Many researchers have proposed various evalua-
tion methods for summarization. We find that ex-
trinsic, task-oriented evaluation method to be most
easily automated, and quantifiable (Radev 2000).
We choose to evaluate our stochastic theme classi-
fication system (STCS) on a multi-document
summarization task, among other possible tasks
We choose a content-based method to evaluate the
summaries extracted by our system, compared to
those by another extraction-based system MEAD
(Radev 2002), and against a baseline system that
chooses the top N sentence in each document as
salient sentences.  All three systems are considered
unsupervised.

The evaluation corpus we use is a segment of the
English part of HKSAR news from the LDC, con-
sisting of 215 articles. We first use MEAD to ex-
tract summaries from 1%-20% compression ratio.
We then use our system to extract the same num-
ber of salient sentences as MEAD, according to the
sentence weights. The baseline system also ex-
tracts the same amount of data as the other two
systems. We plot the cosine similarities of the
original 215 documents with each individual ex-
tracted summaries from these three systems. The
following figure shows a plot of cosine similarity
scores against compression ratio of each extracted
summary. In terms of relative similarity score, our
system is 22.8% higher on average than MEAD,
and 46.3% higher on average than the base-
line.
0
0. 1
0. 2
0. 3
0. 4
0. 5
0. 6
0. 7
0. 8
0. 9
0% 10% 20% 30%
Our  syst em
MEAD
TOP-N Sent

Figure 2: Our system outperforms an existing multi-document
summarizer (MEAD) by 22.8% on average, and outperforms
the baseline top-N sentence selection system by 46.3% on
average.

We would like to note that in our comparative
evaluation, it is necessary to normalize all variable
factors that might affect the system performance,
other than the intrinsic algorithm in each system.
For example, we ensure that the sentence segmen-
tation function is identical in all three systems. In
addition, index term weights need to be properly
trained within their own document clusters. Since
MEAD discards all sentences below the length 9,
the other two systems also discard such sentences.
The feature weights in both our system and MEAD
are all set to the default value one. Since all other
features are the same between our system and
MEAD, the difference in performance is attributed
to the core clustering and centroid computation
algorithms in both systems.
7 Conclusion and Discussion
We have presented a stochastic HMM framework
with modified K-means and segmental K-means
algorithms for extractive summarization. Our
method uses an unsupervised, probabilistic ap-
proach to find class centroids, class sequences and
class boundaries in linear, unrestricted texts in or-
der to yield salient sentences and topic segments
for summarization and question and answer tasks.
We define a class to be a group of connected sen-
tences that corresponds to one or multiple topics in
the text. Such topics can be answers to a user
query, or simply one concept to be included in the
summary. We define a Markov model where the
states correspond to the different classes, and the
observations are continuous sequences of sen-
tences in a document. Transition probabilities are
the class transitions obtained from a training cor-
pus. Emission probabilities are the probabilities of
an observed sentence given a specific class, fol-
lowing a Poisson distribution.  Unlike conventional
methods where texts are treated as independent
sentences to be clustered together, our method in-
corporates text cohesion information in the class
transition probabilities. Unlike other HMM and
noisy channel, probabilistic approaches for infor-
mation retrieval, our method does not require an-
notated data as it is unsupervised.

We also suggest using modified K-means cluster-
ing algorithm to avoid ad hoc choices of initial
cluster set as in the conventional K-means algo-
rithm. For unsupervised training, we use a segmen-
tal K-means training method to iteratively improve
the clusters. Experimental results show that the
content-based performance of our system is 22.8%
above that of an existing extractive summarization
system, and 46.3% above that of simple top-N sen-
tence selection system.  Even though the evalua-
tion on the training set is not a close evaluation
since the training is unsupervised, we will also
evaluate on testing data not included in the training
set as our trained decoder can be used to classify
sentences in unseen texts. Our framework serves as
a foundation for future incorporation of other sta-
tistical and linguistic information as vector features,
such as part-of-speech tags, name aliases, syno-
nyms, and morphological variations.
References
Chinatsu Aone, James Gorlinsky, Bjornar Larsen, and Mary Ellen Oku-
rowski, 1999. A trainable summarizer with knowledge acquired from
robust NLP techniques. In Advances in automatic text summarization,
ed. Inderjeet Mani and Mark T. Maybury. pp 71-80.
Michele Banko, Vibhu O. Mittal & Michael J. Witbrock. 2000. Headline
Generation Based on Statistical Translation. In Proc. Of the Associa-
tion for Computational Linguistics. 
Regina Barzilay,  Noemie Elhadad & Kathleen R. McKeown. 2001. Sen-
tence Ordering in Multi-document Summarization. In  Proceedings of
the 1st Human Language Technology Conference. pp 149-156. San
Diego, CA, US.
Adam Berger & John Lafferty. 1999. Information Retrieval as Statistical
Translation. .  . In Proc. Of the 22nd  ACM SIGIR Conference (SIGIR-
99). pp 222-229. Berkeley, CA, USA.
Branimir Boguraev & Christopher Kennedy. 1999. Salience-Based Content
Characterisation of Text Documents.  In Advances in automatic text
summarization / edited. Inderjeet Mani and Mark T. Maybury. pp 99-
110.
Kenneth Ward Church. 1988. A Stochastic Parts Program and Noun Phrase
Parser for Unrestricted Text.  In  Proceedings of the Second Conference
on Applied Natural Language Processing. pp 136--143.  Austin, TX.
Ido Dagan, Lillian Lee, & Fernando Pereira. 1997. Similarity-based meth-
ods for word sense disambiguation. In Proc. Of the 32nd  Conference of
the Association of Computational Linguistics, pp 56-63.
Halliday & Hasan, 1976. Cohesion in English. London: Longman.
Marti A. Hearst. 1994. Multi-Paragraph Segmentation of       Expository
Text.  In  Proc. Of the Association for Computational Linguistics. pp 9-
16.  Las Cruces, NM.
Eduard Hovy & Chin-Yew Lin. 1999. Automated Text Summarization in
SUMMARIST In Advances in automatic text summarization / edited.
Inderjeet Mani and Mark T. Maybury.  pp 81-97.
H. Jing. Dragomir R. Radev and M. Budzikowska. Centroid-based summa-
rization of multiple documents: sentence extaction, utility-based
evaluation and user studies. In Proceedings of ANLP/NAACL-2000.
Slava M. Katz. 1996. Distribution of content words and phrases in text and
language modeling. In Natural Language Engineering, Vol.2 Part.1,
pp15-60.
Kevin Knight & Daniel Marcu. 2000. Statistics-Based Summarization ?
Step One: Sentence Compression. In Proc. Of the 17th Annual Confer-
ence of the American Association for Artificial Intelligence. pp  703-
710. Austin, Texas, US.
Julian Kupiec, Jan Pedersen & Francine Chen. 1995. A Trainable Docu-
ment Summarizer. In Proc. Of the 18th ACM-SIRGIR Conference. pp
68-73.
Inderjeet Mani & Eric Bloedorn. 1999. Summarizing Similarities and
Differences Among Related Documents.  In Information Retrieval, 1.
pp 35-67.
Christopher D. Manning & Hinrich Sch   tze 1999. Foundations of statisti-
cal natural language processing. The MIT Press  Cambridge, Massachu-
setts. London, England.
Daniel Marcu. 1997. The Rhetorical Parsing of Natural Language Texts.
In Proc. of the 35th Annual Meeting of Association for Computational
Linguistics and 8th Conference of European Chapter of Association for
Computational Linguistics. pp 96-103.  Madrid, Spain.
Bernard Merialdo. 1994. Tagging English Text with a Probabilistic Model.
In Computational Linguistics, 20.2. pp 155-172.
David R.H. Miller, Tim Leek & Richard M. Schwartz. 1999. A Hidden
Markov Model Information Retrieval System. In Proc. Of the
SIGIR?99   pp 214?221.  Berkley, CA, US.
Sung Hyon Myaeng & Dong-Hyun Jang. 1999. Development and Evalua-
tion of a Statistically-Based Document Summarization System. In Ad-
vances in automatic text summarization / edited. Inderjeet Mani and
Mark T. Maybury. MIT Press. pp 61-70.
Tadashi Nomoto & Yuji Matsumoto. 2002. Supervised ranking in open
domain summarization. In Proc. Of the 40th  Conference of the Associa-
tion of Computational Linguistics, pp.. Pennsylvania, US.
Tadashi Nomoto & Yuji Matsumoto.  2001.  A New Approach to Unsu-
pervised Text Summarization. In  Proc. Of the SIGIR?01, pp 26-34  New
Orleans, Louisiana, USA.
Jahna C. Otterbacher, Dragomir R. Radev & Airong Luo. 2002. Revision
that Improve Cohesion in Multi-document Summaries. In  Proc. Of the
Workshop on Automatic Summarization (including DUC 2002), pp 27-
36. Association for Computational Linguistics. Philadelphia, US.
Miles Osborne. 2002. Using Maximum Entropy for Sentence Extraction.
In  Proc. Of the Workshop on Automatic Summarization (including
DUC 2002), pp 1-8. Association for Computational Linguistics. Phila-
delphia, US.
Dragomir Radev, Adam Winkel & Michael Topper . 2002. Multi-
Document Centroid-based Text Summarization.. In Proc. Of the ACL-
02 Demonstration Session, pp112-113. Pennsylvania, US.
Michael White & Claire Cardie. 2002. Selecting Sentences for Multidocu-
ment Summaries using Randomized Local Search.  In Proc. Of the
Workshop on Automatic Summarization (including DUC 2002), pp 9-
18. Association for Computational Linguistics. Philadelphia, US.
J.G. Wilpon & L.R. Rabiner 1985. A modified K-means clustering algo-
rithm for use in isolated word recognition. In IEEE Trans. Acoustics,
Speech, Signal Proc. ASSP-33(3), pp 587-594.
K. Zechner. 1996. Fast generation of abstracts from general domain text
corpora by extracting relevant sentences. In Proc. Of the 16th  Interna-
tional Conference on Computational Linguistics, pp 986-989. Copen-
hagen, Denmark.
Hongyuan Zha. 2002. Generic Summarization and Keyphrase Extraction
Using Mutual Reinforcement Principle and Sentence Clustering.  In
Proc. Of the SIGIR?02. pp 113-120.  Tampere, Finland Endre Boros,
Paul B. Kantor & David J. Neu. 2001. A Clustering Based Approach to
Creating Multi-Document Summaries.
Mining Very-Non-Parallel Corpora:  
Parallel Sentence and Lexicon Extraction via Bootstrapping and EM  
Pascale Fung and Percy Cheung 
Human Language Technology Center,  
University of Science & Technology (HKUST), 
Clear Water Bay, Hong Kong 
{pascale,eepercy}@ee.ust.hk 
 
 
Abstract 
We present a method capable of extracting 
parallel sentences from far more disparate 
?very-non-parallel corpora? than previous 
?comparable corpora? methods, by exploiting 
bootstrapping on top of IBM Model 4 EM. Step 
1 of our method, like previous methods, uses 
similarity measures to find matching documents 
in a corpus first, and then extracts parallel 
sentences as well as new word translations from 
these documents. But unlike previous methods, 
we extend this with an iterative bootstrapping 
framework based on the principle of 
?find-one-get-more?, which claims that 
documents found to contain one pair of parallel 
sentences must contain others even if the 
documents are judged to be of low similarity. 
We re-match documents based on extracted 
sentence pairs, and refine the mining process 
iteratively until convergence. This novel 
?find-one-get-more? principle allows us to add 
more parallel sentences from dissimilar 
documents, to the baseline set. Experimental 
results show that our proposed method is nearly 
50% more effective than the baseline method 
without iteration. We also show that our method 
is effective in boosting the performance of the 
IBM Model 4 EM lexical learner as the latter, 
though stronger than Model 1 used in previous 
work, does not perform well on data from 
very-non-parallel corpus. 
Figure1. Parallel sentence and lexicon extraction 
via Bootstrapping and EM 
 
The most challenging task is to extract bilingual 
sentences and lexicon from very-non-parallel data. 
Recent work (Munteanu et al, 2004, Zhao and Vogel, 
2002) on extracting parallel sentences from 
comparable data, and others on extracting 
paraphrasing sentences from monolingual corpora 
(Barzilay and Elhadad 2003) are based on the 
?find-topic-extract-sentence? principle which claims 
that parallel sentences only exist in document pairs 
with high similarity. They all use lexical information 
(e.g. word overlap, cosine similarity) to match 
documents first, before extracting sentences from 
these documents.  
1. Introduction 
Parallel sentences are important resources for 
training and improving statistical machine translation 
and cross-lingual information retrieval systems. 
Various methods have been previously proposed to 
extract parallel sentences from multilingual corpora. 
Some of them are described in detail in (Manning 
and Sch?tze, 1999, Wu, 2001, Veronis 2001). The 
challenge of these tasks varies by the degree of 
parallel-ness of the input multilingual documents.  
 
However, the non-parallel corpora used so far in 
the previous work tend to be quite comparable. Zhao 
and Vogel (2002) used a corpus of Chinese and 
English versions of news stories from the Xinhua 
News agency, with ?roughly similar sentence order 
 
of content?. This corpus can be more accurately 
described as noisy parallel corpus. Barzilay and 
Elhadad (2003) mined paraphrasing sentences from 
weather reports. Munteanu et al, (2004) used news 
articles published within the same 5-day window. All 
these corpora have documents in the same, matching 
topics. They can be described as on-topic 
documents. In fact, both Zhao and Vogel (2002) and 
Barzilay and Elhadad (2003) assume similar 
sentence orders and applied dynamic programming 
in their work.  
 
In our work, we try to find parallel sentences from 
far more disparate, very-non-parallel corpora than in 
any previous work. Since many more multilingual 
texts available today contain documents that do not 
have matching documents in the other language, we 
propose finding more parallel sentences from 
off-topic documents, as well as on-topic documents. 
An example is the TDT corpus, which is an 
aggregation of multiple news sources from different 
time periods. We suggest the ?find-one-get-more? 
principle, which claims that as long as two 
documents are found to contain one pair of parallel 
sentence, they must contain others as well. Based on 
this principle, we propose an effective Bootstrapping 
method to accomplish our task (Figure 1).  
 
We also apply the IBM Model 4 EM lexical 
learning to find unknown word translations from the 
extracted parallel sentences from our system. The 
IBM models are commonly used for word alignment 
in statistical MT systems. This EM method differs 
from some previous work, which used a seed-word 
lexicon to extract new word translations or word 
senses from comparable corpora (Rapp 1995, Fung 
& McKeown 1997, Grefenstette 1998, Fung and Lo 
1998, Kikui 1999, Kaji 2003).  
2. Bilingual Sentence Alignment 
There have been conflicting definitions of the 
term ?comparable corpora? in the research 
community. In this paper, we contrast and analyze 
different bilingual corpora, ranging from the 
parallel, noisy parallel, comparable, to 
very-non-parallel corpora. 
 
A parallel corpus is a sentence-aligned corpus 
containing bilingual translations of the same 
document. The Hong Kong Laws Corpus is a 
parallel corpus with manually aligned sentences, and 
is used as a parallel sentence resource for statistical 
machine translation systems. There are 313,659 
sentence pairs in Chinese and English. Alignment of 
parallel sentences from this type of database has 
been the focus of research throughout the last decade 
and can be accomplished by many off-the-shelf, 
publicly available alignment tools.  
 
A noisy parallel corpus, sometimes also called a 
?comparable? corpus, contains non-aligned 
sentences that are nevertheless mostly bilingual 
translations of the same document. (Fung and 
McKeown 1997, Kikui 1999, Zhao and Vogel 2002) 
extracted bilingual word senses, lexicon and parallel 
sentence pairs from such corpora. A corpus such as 
Hong Kong News contains documents that are in 
fact rough translations of each other, focused on the 
same thematic topics, with some insertions and 
deletions of paragraphs.  
 
Another type of comparable corpus is one that 
contains non-sentence-aligned, non-translated 
bilingual documents that are topic-aligned. For 
example, newspaper articles from two sources in 
different languages, within the same window of 
published dates, can constitute a comparable corpus. 
Rapp (1995), Grefenstette (1998), Fung and Lo 
(1998), and Kaji (2003) derived bilingual lexicons or 
word senses from such corpora. Munteanu et al, 
(2004) constructed a comparable corpus of Arabic 
and English news stories by matching the publishing 
dates of the articles.  
 
Finally, a very-non-parallel corpus is one that 
contains far more disparate, very-non-parallel 
bilingual documents that could either be on the same 
topic (in-topic) or not (off-topic). The TDT3 Corpus 
is such a corpus. It contains transcriptions of various 
news stories from radio broadcasting or TV news 
report from 1998-2000 in English and Chinese.  In 
this corpus, there are about 7,500 Chinese and 
12,400 English documents, covering more around 60 
different topics.  Among these, 1,200 Chinese and 
4,500 English documents are manually marked as 
being in-topic. The remaining documents are marked 
as off-topic as they are either only weakly relevant 
to a topic or irrelevant to all topics in the existing 
documents. From the in-topic documents, most are 
found to have high similarity. A few of the Chinese 
and English passages are almost translations of each 
other. Nevertheless, the existence of a considerable 
amount of off-topic document gives rise to more 
variety of sentences in terms of content and 
structure.  Overall, the TDT 3 corpus contains 
110,000 Chinese sentences and 290,000 English 
sentences. Some of the bilingual sentences are 
translations of each other, while some others are 
bilingual paraphrases. Our proposed method is a first 
approach that can extract bilingual sentence pairs 
from this type of very-non-parallel corpus. 
3. Comparing bilingual corpora 
To quantify the parallel-ness or comparability of 
bilingual corpora, we propose using a lexical 
matching score computed from the bilingual word 
pairs occurring in the bilingual sentence pairs. 
Matching bilingual sentence pairs are extracted from 
different corpora using existing and the proposed 
methods.  
 
We then identify bilingual word pairs that appear 
in the matched sentence pairs by using a bilingual 
lexicon (bilexicon). The lexical matching score is 
then defined as the sum of the mutual information 
score of a known set of word pairs that appear in the 
corpus:  
 
?=
=
),(
),(
)()(
),(),(
ec WWall
ec
ec
ec
ec
WWSS
WfWf
WWfWWS
 
 
where f(Wc,We) is the co-occurrence frequency of 
bilexicon pair (Wc,We) in the matched sentence 
pairs. f(Wc) and f(We) are the occurrence 
frequencies of Chinese word Wc and English word 
We, in the bilingual corpus. 
 
Corpus Parallel Comparable Quasi- 
Comparable 
Lexical 
matching 
score 
359.1 253.8 160.3 
Table 1: Bilingual lexical matching scores of 
different corpora  
Table 1 shows the lexical matching scores of the 
parallel corpus (Hong Kong Law), a comparable 
noisy parallel corpus (Hong Kong News), and a 
very-non-parallel corpus (TDT 3). We can see that 
the more parallel or comparable the corpus, the 
higher the overall lexical matching score is. 
4. Comparing alignment principles 
It is well known that existing work on sentence 
alignment from parallel corpus makes use of one or 
multiple of the following principles (Manning and 
Sch?tze, 1999, Somers 2001): 
? A bilingual sentence pair are similar in length in 
the two languages; 
? Sentences are assumed to correspond to those 
roughly at the same position in the other 
language; 
? A pair of bilingual sentences which contain 
more words that are translations of each other 
tend to be translations themselves. Conversely, 
the context sentences of translated word pairs 
are similar. 
 
For noisy parallel corpora, sentence alignment is 
based on embedded content words. The word 
alignment principles used in previous work are as 
follows: 
? Occurrence frequencies of bilingual word pairs 
are similar; 
? The positions of bilingual word pairs are similar; 
? Words have one dominant sense/translation per 
corpus. 
 
Different sentence alignment algorithms based on 
the above principles can be found in Manning and 
Sch?tze (1999), Somers (2001), Wu (2000), and 
 
1. Initial document matching   
For all documents in the comparable corpus D: 
Gloss Chinese documents using the bilingual lexicon (Bilex); 
For every pair of glossed Chinese document and English documents,  
compute document similarity =>S(i,j); 
Obtain all matched bilingual document pairs whose S(i,j)> threshold1=>D2 
2. Sentence matching 
For each document pair in D2: 
For every pair of glossed Chinese sentence and English sentence,  
compute sentence similarity =>S2(i,j); 
Obtain all matched bilingual sentence pairs whose S2(i,j)> threshold2=>C1 
3. EM learning of new word translations 
For all bilingual sentences pairs in C1, do: 
Compute translation lexicon probabilities of all bilingual word pairs =>S3(i,j);  
Obtain all bilingual word pairs previously unseen in Bilex and whose S3(i,j)> threshold3=>L1, and update Bilex; 
Compute sentence alignment scores=>S4; if (S4 does not change) return C1 and L1, otherwise continue; 
4. Document re-matching  
Find all pairs of glossed Chinese and English documents which contain parallel sentences (anchor sentences) from 
C1=>D3;  
  Expand D2 by finding documents similar to each of the document in D2; 
  D2:=D3; 
Goto 2; 
Figure 2. Bootstrapping with EM 
Veronis (2002). These methods have also been 
applied recently in a sentence alignment shared task 
at NAACL 20031. We have also learned that as 
bilingual corpora become less parallel, it is better to 
rely on lexical information rather than sentence 
length and position information. 
 
For comparable corpora, the alignment principle 
made in previous work is as follows: 
? Parallel sentences only exist in document pairs 
with high similarity scores ? 
?find-topic-extract-sentence? 
 
We take a step further and propose a new 
principle for our task: 
? Documents that are found to contain at least one 
pair of parallel sentences are likely to contain 
more parallel sentences ? ?find-one-get-more? 
5. Extracting Bilingual Sentences from 
Very-Non-Parallel Corpora  
Existing algorithms such as Zhao and Vogel, 
(2002), Barzilay and Elhadad, (2003), Munteanu et 
al., (2004) for extracting parallel or paraphrasing 
sentences from comparable documents, are based on 
the ?find-topic-extract-sentence? principle which 
looks for document pairs with high similarities, and 
then look for parallel sentences only from these 
documents.  
 
Based on our proposed ?find-one-get-more? 
principle, we suggest that there are other, dissimilar 
documents that might contain more parallel 
sentences. We can iterate this whole process for 
improved results using a Bootstrapping method. 
Figure 2 outlines the algorithm in more detail. In the 
following sections 5.1-5.5, we describe the 
document pre-processing step followed by the four 
subsequent iterative steps of our algorithm. 
5.1. Document preprocessing  
The documents are word segmented with the 
Language Data Consortium (LDC) Chinese-English 
dictionary 2.0.Then the Chinese document is glossed 
using all the dictionary entries. When a Chinese 
word has multiple possible translations in English, it 
is disambiguated by a method extended from (Fung 
et al 1999). 
5.2. Initial document matching 
This initial step is based on the same 
?find-topic-extract-sentence? principle as in earlier 
works.  The aim of this step is to roughly match the 
Chinese-English documents pairs that have the same 
topic, in order to extract parallel sentences from 
                                                        
1 http://www.cs.unt.edu/~rada/wpt/ 
them. Similar to previous work, comparability is 
defined by cosine similarity between document 
vectors.   
 
Both the glossed Chinese document and English 
are represented in word vectors, with term weights. 
We evaluated different combinations of term 
weighting of each word in the corpus: term 
frequency (tf); inverse document frequency (idf); 
tf.idf; and the product of a function of tf and idf.  
The ?documents? here are sentences. We find that 
using idf alone gives the best sentence pair rank. 
This is probably due to the fact that frequencies of 
bilingual word pairs are not comparable in a 
very-non-parallel corpus. 
 
Pair-wise similarities are calculated for all 
possible Chinese-English document pairs, and 
bilingual documents with similarities above a certain 
threshold are considered to be comparable. For 
very-non-parallel corpora, this document-matching 
step also serves as topic alignment.  
5.3. Sentence matching  
Again based on the ?find-topic-extract-sentence? 
principle, we extract parallel sentences from the 
matched English and Chinese documents. Each 
sentence is again represented as word vectors. For 
each extracted document pair, pair-wise cosine 
similarities are calculated for all possible 
Chinese-English sentence pairs. Sentence pairs 
above a set threshold are considered parallel and 
extracted from the documents. Sentence similarity is 
based on the number of words in the two sentences 
that are translations of each other. The better our 
bilingual lexicon is, the more accurate the sentence 
similarity will be. In the following section, we 
discuss how to find new word translations. 
5.4. EM lexical learning from matched sentence 
pairs 
This step updates the bilingual lexicon according 
to the intermediate results of parallel sentence 
extraction. New bilingual word pairs are learned 
from the extracted sentence pairs based on an EM 
learning method. We use the GIZA++ (Och and 
Ney, 2000) implementation of the IBM statistical 
translation lexicon Model 4 (Brown et al, 1993) for 
this purpose. 
 
This model is based on the conditional probability 
of a source word being generated by the target word 
in the other language, based on EM estimation from 
aligned sentences. Zhao and Vogel (2002) showed 
that this model lends itself to adaptation and can 
provide better vocabulary coverage and better 
sentence alignment probability estimation. In our 
work, we use this model on the intermediate results 
of parallel sentence extraction, i.e. on a set of 
aligned sentence pairs that may or may not truly 
correspond to each other.  
 
We found that sentence pairs with high alignment 
scores are not necessarily more similar than others. 
This might be due to the fact that EM estimation at 
each intermediate step is not reliable, since we only 
have a small amount of aligned sentences that are 
truly parallel. The EM learner is therefore weak 
when applied to bilingual sentences from 
very-non-parallel corpus. We decided to try using 
parallel corpora to initialize the EM estimation, as in 
Zhao and Vogel (2002). The results are discussed in 
Section 6. 
5.5. Document re-matching: find-one-get-more 
This step augments the earlier matched documents 
by the ?find-one-get-more? principle. From the set 
of aligned sentence pairs, we look for other 
documents, judged to be dissimilar in the first step, 
that contain one or more of these sentence pairs. We 
further find other documents that are similar to each 
of the monolingual documents found. This new set 
of documents is likely to be off-topic, yet contains 
segments that are on-topic.  Following our new 
alignment principle, we believe that these documents 
might still contain more parallel sentence candidates 
for subsequent iterations. The algorithm then iterates 
to refine document matching and parallel sentence 
extraction.  
5.6. Convergence 
The IBM model parameters, including sentence 
alignment score and word alignment scores, are 
computed in each iteration. The parameter values 
eventually stay unchanged and the set of extracted 
bilingual sentence pairs also converges to a fixed 
size. The system then stops and gives the last set of 
bilingual sentence pairs as the final output.  
6. Evaluation 
We evaluate our algorithm on a very-non-parallel 
corpus of TDT3 data, which contains various news 
stories transcription of radio broadcasting or TV 
news report from 1998-2000 in English and Chinese 
Channels.  We compare the results of our proposed 
method against a baseline method that is based on 
the conventional, ?find-topic-extract-sentence? 
principle only. We investigate the performance of 
the IBM Model 4 EM lexical learner on data from 
very-non-parallel corpus, and evaluate how our 
method can boost its performance. The results are 
described in the following sub-sections. 
6.1. Baseline method 
Since previous works were carried out on different 
corpora, in different language pairs, we cannot 
directly compare our method against them. 
However, we implement a baseline method that 
follows the same ?find-topic-extract-sentence? 
principle as in earlier work. The baseline method 
shares the same preprocessing, document matching 
and sentence matching steps with our proposed 
method. However, it does not iterate to update the 
comparable document set, the parallel sentence set, 
or the bilingual lexicon.  
 
Human evaluators manually check whether the 
matched sentence pairs are indeed parallel. The 
precision of the parallel sentences extracted is 42.8% 
for the top 2,500 pairs, ranked by sentence similarity 
scores. 
6.2. Bootstrapping performs much better 
There are 110,000 Chinese sentences and  
290,000 English sentences in TDT3,  which lead to 
more than 30 billion  possible sentence pairs. Few 
of the sentence pairs turn out to be exact translations 
of each other, but many are bilingual paraphrases. 
For example, in the following extracted sentence 
pair, the English sentence has the extra phrase 
?under the agreement?, which is missing from the 
Chinese sentence: 
? ??????????? ??  
(Hun Sen becomes Cambodia ' s sole prime 
minister) 
? Under the agreement, Hun Sen becomes 
Cambodia ' s sole prime minister.  
 
Another example of translation versus bilingual 
paraphrases is as follows: 
 
? ???????????????????
(The Chinese president Jiang Zemin arrived in 
Japan today for a state visit) 
(Translation) Chinese president Jiang Zemin 
arrived in Japan today for a landmark state visit.  
? ???????????????(This is a 
first visit by a Chinese head of state to Japan) 
(Paraphrase) Mr Jiang is the first Chinese head of 
state to visit the island country.  
 
The precision of parallel sentences extraction is 
65.7% for the top 2,500 pairs using our method, 
which has a 50% improvement over the baseline. In 
addition, we also found that the precision of parallel 
sentence pair extraction increases steadily over each 
iteration, until convergence. 
 
 
6.3. Bootstrapping can boost a weak EM lexical 
learner  
6.4. Bootstrapping is significantly more useful 
than new word translations for mining 
parallel sentences In this section, we discuss experimental results 
that lead to the claim that our proposed method can 
boost a weak IBM Model 4 EM lexical learner.  
It is important for us to gauge the effects of the 
two main ideas in our algorithm, Bootstrapping and 
EM lexicon learning, on the extraction parallel 
sentences from very-non-parallel corpora. The 
baseline experiment shows that without iteration, the 
performance is at 42.8%. We carried out another set 
of experiment of using Bootstrapping where the 
bilingual lexicon is not updated in each iteration.  
The bilingual sentence extraction accuracy of the top 
2500 sentence pairs in this case dropped to 65.2%, 
with only 1% relative degradation.  
6.3.1. EM lexical learning is weak on bilingual 
sentences from very-non-parallel corpora  
We compare the performances of the IBM Model 
4 EM lexical learning on parallel data (130k 
sentence pairs from Hong Kong News) and 
very-non-parallel data (7200 sentence pairs from 
TDT3) by looking at a common set of source words 
and their top-N translation candidates extracted. We 
found that the IBM Model 4 EM learning performs 
much worse on TDT3 data. Figure 3 shows that the 
EM learner performs about 30% worse on average 
on the TDT3 data.  
 
Based on the above, we conclude that EM lexical 
learning has little effect on the overall parallel 
sentence extraction output. This is probably due to 
the fact that whereas EM does find new word 
translations (such as ????/Pinochet), this has 
little effect on the overall glossing of the Chinese 
document since such new words are rare.   
 
 
7. Conclusion 
Previous work on extracting bilingual or 
monolingual sentence pairs from comparable 
corpora has only been applied to documents that are 
within the same topic, or have very similar 
publication dates. One principle for previous 
methods is ?find-topic-extract-sentence? which 
claims that parallel or similar sentences can only be 
found in document pairs with high similarity. We 
propose a new, ?find-one-get-more? principle which 
claims that document pairs that contain at least one 
pair of matched sentences must contain others, even 
if these document pairs do not have high similarity 
scores. Based on this, we propose a novel 
Bootstrapping method that successfully extracts 
parallel sentences from a far more disparate and 
very-non-parallel corpus than reported in previous 
work. This very-non-parallel corpus, TDT3 data, 
includes documents that are off-topic, i.e. documents 
with no corresponding topic in the other language. 
This is a completely unsupervised method. 
Evaluation results show that our approach achieves 
65.7% accuracy and a 50% relative improvement 
from baseline.  This shows that the proposed 
method is promising. We also find that the IBM 
Model 4 lexical learner is weak on data from 
very-non-parallel corpus, and that its performance 
can be boosted by our Multilevel Bootstrapping 
method, whereas using parallel corpus for adaptation 
is not nearly as useful.   
 Figure 3. EM lexical learning performance 
6.3.2. Multilevel Bootstrapping is significantly 
better than adaptation data in boosting the 
weak EM lexical learner  
Since the IBM model parameters can be better 
estimated if the input sentences are more parallel, we 
have tried to add parallel sentences to the extracted 
sentence pairs in each iteration step, as proposed by 
Zhao and Vogel (2002). However, our experiments 
showed that adding parallel corpus gives no 
improvement on the final output. This is likely due 
to (1) the parallel corpus is not in the same domain 
as the TDT corpus; and (2) there are simply not 
enough parallel sentences extracted at each step for 
the reliable estimation of model parameters.  
 
In contrast, Figure 3 shows that when we apply 
Bootstrapping to the EM lexical learner, the 
bilingual lexicon extraction accuracy is improved by 
20% on the average, evaluated on top-N translation 
candidates of the same source words, showing that 
our proposed method can boost a weak EM lexical 
learner even on data from very-non-parallel corpus.  
 
In addition, we compare and contrast a number of 
bilingual corpora, ranging from the parallel, to  
comparable, and to very-non-parallel corpora. The 
parallel-ness of each type of corpus is quantified by 
a lexical matching score calculated for the bi-lexicon 
pair distributed in the aligned bilingual sentence 
pairs. We show that this scores increases as the 
parallel-ness or comparability of the corpus 
increases.  
 
Finally, we would like to suggest that 
Bootstrapping can in the future be used in 
conjunction with other sentence or word alignment 
learning methods to provide better mining results. 
For example, methods for learning a classifier to 
determine sentence parallel-ness such as that 
proposed by Munteanu et al, (2004) can be 
incorporated into our Bootstrapping framework.  
References  
Regina Barzilay and Noemie Elhadad, Sentence 
Alignment for Monolingual Comparable Corpora, 
Proc. of EMNLP, 2003, Sapporo, Japan. 
Peter F. Brown, S.A. Della Pietra, V.J. Della Pietra, 
and R.L. Mercer. The mathematics of statistical 
machine translation: parameter estimation, in 
Computational Linguistics, 19-2, 1993. 
Pascale Fung and Kathleen Mckeown. Finding 
terminology translations from non-parallel 
corpora. In The 5th Annual Workshop on Very 
Large Corpora. pages 192--202, Hong Kong, 
Aug. 1997. 
Pascale Fung and Lo Yuen Yee. An IR Approach for 
Translating New Words from Nonparallel, 
Comparable Texts.  In COLING/ACL  1998 
Pascale Fung, Liu, Xiaohu, and Cheung, Chi Shun. 
Mixed-language Query Disambiguation. In 
Proceedings of ACL ?99, Maryland: June 1999 
Gale, W A and Kenneth W.Church. A Program for 
Aligning Sentences in Bilingual Corpora. 
Computatinal Linguistics. vol.19 No.1 March, 
1993. 
 Gregory Grefenstette, editor. Cross-Language 
Information Retrieval. Kluwer Academic 
Publishers, 1998. 
Hiroyuki Kaji, Word sense acquisition from 
bilingual comparable corpora, in Proceedings of 
the NAACL, 2003, Edmonton, Canada, pp 
111-118. 
Genichiro Kikui. Resolving translation ambiguity 
using non-parallel bilingual corpora. In 
Proceedings of ACL99 Workshop on 
Unsupervised Learning in Natural Language 
Christopher D. Manning and Hinrich Sch?tze. 
Foundations of Statistical Natural Language 
Processing. The MIT Press. 
Dragos Stefan Munteanu, Alexander Fraser, Daniel 
Marcu, 2004. Improved Machine Translation 
Performance via Parallel Sentence Extraction 
from Comparable Corpora. In Proceedings of the 
Human Language Technology and North 
American Association for Computational 
Linguistics Conference (HLT/NAACL 2004).  
Franz Josef Och and Hermann Ney. Improved 
statistical alignment models, in Proceedings of 
ACL-2000.  
Reinhard Rapp. Identifying word translations in 
non-parallel texts. Proceedings of the 33rd 
Meeting of the Association for Computational 
Linguistics. Cambridge, MA, 1995. 320-322 
Philip Resnik and Noah A. Smith. The Web as a 
Parallel Corpus. Computational Linguistics 
29(3), pp. 349-380, September 2003. 
Frank Smadja. Retrieving collocations from text: 
Xtract. In Computational Linguistics, 
19(1):143-177,1993 
Harold Somers. Bilingual Parallel Corpora and 
Language Engineering. Anglo-Indian workshop 
"Language Engineering for South-Asian 
languages" (LESAL), (Mumbai, April 2001).  
Jean Veronis (editor), Parallel Text Processing: 
Alignment and Use of Translation Corpora. 
Dordrecht: Kluwer. ISBN 0-7923-6546-1. Aug 
2000.  
Dekai Wu. Alignment. In Robert Dale, Hermann 
Moisl, and Harold Somers (editors), Handbook of 
Natural Language Processing. 415-458. New 
York: Marcel Dekker. ISBN 0-8247-9000-6. Jul 
2000.  
Bing Zhao, Stephan Vogel. Adaptive Parallel 
Sentences Mining from Web Bilingual News 
Collections. In Proceedings of the IEEE 
Workshop on Data Mining 2002.  
 
Proceedings of the 2nd Workshop on Building and Using Comparable Corpora, ACL-IJCNLP 2009, pages 23?26,
Suntec, Singapore, 6 August 2009. c?2009 ACL and AFNLP
Active Learning of Extractive Reference Summaries
for Lecture Speech Summarization
Justin Jian Zhang and Pascale Fung
Human Language Technology Center
Department of Electronic and Computer Engineering
University of Science and Technology (HKUST)
Clear Water Bay,Hong Kong
{zjustin,pascale}@ece.ust.hk
Abstract
We propose using active learning for tag-
ging extractive reference summary of lec-
ture speech. The training process of
feature-based summarization model usu-
ally requires a large amount of train-
ing data with high-quality reference sum-
maries. Human production of such sum-
maries is tedious, and since inter-labeler
agreement is low, very unreliable. Ac-
tive learning helps assuage this problem by
automatically selecting a small amount of
unlabeled documents for humans to hand
correct. Our method chooses the unla-
beled documents according to the similar-
ity score between the document and the
comparable resource?PowerPoint slides.
After manual correction, the selected doc-
uments are returned to the training pool.
Summarization results show an increasing
learning curve of ROUGE-L F-measure,
from 0.44 to 0.514, consistently higher
than that of using randomly chosen train-
ing samples.
Index Terms: active learning, summarization
1 Introduction
The need for the summarization of classroom lec-
tures, conference speeches, political speeches is
ever increasing with the advent of remote learning,
distributed collaboration and electronic archiving.
These user needs cannot be sufficiently met by
short abstracts. In recent years, virtually all sum-
marization systems are extractive - compiling bul-
let points from the document using some saliency
criteria. Reference summaries are often manu-
ally compiled by one or multiple human annota-
tors (Fujii et al, 2008; Nenkova et al, 2007). Un-
like for speech recognition where the reference
sentence is clear and unambiguous, and unlike
for machine translation where there are guidelines
for manual translating reference sentences, there
is no clear guideline for compiling a good ref-
erence summary. As a result, one of the most
important challenges in speech summarization re-
mains the difficulty to compile, evaluate and thus
to learn what a good summary is. Human judges
tend to agree on obviously good and very bad
summaries but cannot agree on borderline cases.
Consequently, annotator agreement is low. Refer-
ence summary generation is a tedious and low ef-
ficiency task. On the other hand, supervised learn-
ing of extractive summarization requires a large
amount of training data of reference summaries.
To reduce the amount of human annotation effort
and improve annotator agreement on the reference
summaries, we propose that active learning (selec-
tive sampling) is one possible solution.
Active learning has been applied to NLP tasks
such as spoken language understanding (Tur et al,
2005), information extraction (Shen et al, 2004),
and text classification (Lewis and Catlett, 1994;
McCallum and Nigam, 1998; Tong and Koller,
2002). Different from supervised learning which
needs the entire corpus with manual labeling re-
sult, active learning selects the most useful exam-
ples for labeling and requires manual labeling of
training dataset to re-train model.
In this paper, we suggest a framework of refer-
ence summary annotation with relatively high in-
ter labeler agreement based on the rhetorical struc-
ture in presentation slides. Based on this frame-
work, we further propose a certainty-based active
learning method to alleviate the burden of human
annotation of training data.
The rest of this paper is organized as follows:
Section 2 depicts the corpus for our experiments,
the extractive summarizer, and outlines the acous-
tic/prosodic, and linguistic feature sets for repre-
senting each sentence. Section 3 depicts how to
23
compile reference summaries with high inter la-
beler agreement by using the RDTW algorithm
and our active learning algorithm for tagging ex-
tractive reference summary. We describe our ex-
periments and evaluate the results in Section 4.
Our conclusion follows in Section 5.
2 Experimental Setup
2.1 The Corpus
Our lecture speech corpus (Zhang et al, 2008)
contains 111 presentations recorded from the
NCMMSC2005 and NCMMSC2007 conferences
for evaluating our approach. The man-
ual transcriptions and the comparable corpus?
PowerPoint slides are also collected. Each presen-
tation lasts for 15 minutes on average. We select
71 of the 111 presentations with well organized
PowerPoint slides that always have clear sketches
and evidently aligned with the transcriptions. We
use about 90% of the lecture corpus from the 65
presentations as original unlabeled data U and the
remaining 6 presentations as held-out test set. We
randomly select 5 presentations from U as our
seed presentations. Reference summaries of the
seed presentations and the presentations of test set
are generated from the PowerPoint slides and pre-
sentation transcriptions using RDTW followed by
manual correction, as described in Section 3.
2.2 SVM Classifier and the Feature Set
While (Ribeiro and de Matos, 2007) has shown
that MMR (maximum marginal relevance) ap-
proach is superior to feature-based classifica-
tion for summarizing Portuguese broadcast news
data, another work on Japanese lecture speech
drew the opposite conclusion (Fujii et al, 2008)
that feature-based classification method is bet-
ter. Therefore we continue to use the feature-
based method in our work. We consider the ex-
tractive summarization as a binary classification
problem, we predict whether each sentence of the
lecture transcription should be in a summary or
not. We use Radial Basis Function (RBF) ker-
nel for constructing SVM classifier, which is pro-
vided by LIBSVM, a library for support vector
machines (Chang and Lin, 2001). We represent
each sentence by a feature vector which consists of
acoustic features: duration of the sentence, aver-
age syllable Duration, F0 information features, en-
ergy information features; and linguistic features:
length of the sentence counted by word and TFIDF
information features, as shown in (Zhang et al,
2008). We then build the SVM classifier as our
summarizer based on these sentence feature vec-
tors.
3 Active Learning for Tagging Reference
Summary and Summarization
Similar to (Hayama et al, 2005; Kan, 2007), we
have previously proposed how presentation slides
are used to compile reference summaries automat-
ically (Zhang et al, 2008). The motivations be-
hind this procedure are:
? presentation slides are compiled by the au-
thors themselves and therefore provide a
good standard summary of their work;
? presentation slides contain the hierarchical
rhetorical structure of lecture speech as the ti-
tles, subtitles, page breaks, bullet points pro-
vide an enriched set of discourse information
that are otherwise not apparent in the spoken
lecture transcriptions.
We propose a Relaxed Dynamic Time Warping
(RDTW) procedure, which is identical to Dy-
namic Programming and Edit Distance, to align
sentences from the slides to those in the lecture
speech transcriptions, resulting in automatically
extracted reference summaries.
We calculate the similarity scores
matrix Sim = (sij), where sij =
similarity(Senttrans[i], Sentslides[j]), be-
tween the sentences in the transcription and
the sentences in the slides. We then obtain
the distance matrix Dist = (dij), where
dij = 1?sij . We calculate the initial warp path P:
P = (pini1 , ..., pinin , ..., piniN ) by DTW, where pinin
is represented by sentence pair(iinin , jinin ): one
from transcription, the other from slides. Con-
sidering that the lecturer often doesn?t follow the
flow of his/her slides strictly, we adopt Relaxed
Dynamic Time Warping (RDTW) for finding the
optimal warp path, by the following equation.
?
??
??
ioptn = iinin
joptn =
jinin +Cargmin
j=jinin ?C
dioptn ,j
(1)
We consider the transcription sentences on this
path as reference summary sentences. We then
obtain the optimal path (popt1 , ..., poptn , ..., poptN ),
where poptn is represented by (ioptn , joptn ) and C
24
is the capacity to relax the path. We then select
the sentences ioptn of the transcription whose sim-
ilarity scores of sentence pairs: (ioptn , joptn ), are
higher than the pre-defined threshold as the refer-
ence summary sentences. The advantage of using
these summaries as references is that it circum-
vents the disagreement between multiple human
annotators.
We have compared these reference summaries
to human-labeled summaries. When asked to ?se-
lect the most salient sentences for a summary?, we
found that inter-annotator agreement ranges from
30% to 50% only. Sometimes even a single per-
son might choose different sentences at different
times (Nenkova et al, 2007). However, when in-
structed to follow the structure and points in the
presentation slides, inter-annotator agreement in-
creased to 80%. The agreement between auto-
matically extracted reference summary and hu-
mans also reaches 75%. Based on this high degree
of agreement, we generate reference summaries
by asking a human to manually correct those ex-
tracted by the RDTW algorithm. Our reference
summaries therefore make for more reliable train-
ing and test data.
For a transcribed presentation D with a se-
quence of recognized sentences {s1, s2, ..., sN},
we want to find the sentences to be classified
as summary sentences by using the salient sen-
tence classification function c(). In a probabilis-
tic framework, the extractive summarization task
is equivalent to estimating P (c(??s n) = 1|D) of
each sentence sn, where ??s n is the feature vec-
tor with acoustic and linguistic features of the sen-
tence sn.
We propose an active learning approach where a
small set of transcriptions as seeds with reference
summaries, created by the RDTW algorithm and
human correction, are used to train the seed model
for the summarization classifier, and then the clas-
sifier is used to label data from a unlabel pool. At
each iteration, human annotators choose the unla-
beled documents whose similarity scores between
the extracted summary sentences and the Power-
Point slides sentences are top-N highest for label-
ing summary sentences. Formally, this approach
is described in Algorithm 1.
Given document D: {s1, s2, ..., sN}, we cal-
culate the similarity score between the extracted
summary sentences: {s?1, s?2, ..., s?K} and the Pow-
erPoint slide sentences: {ppts1, ppts2, ..., pptsL},
by equation 2.
Scoresim(D) = 1K
K?
n=1
L?
j=1
Sim(s?n, pptsj) (2)
4 Experimental Results and Evaluation
Algorithm 1 Active learning for tagging extrac-
tive reference summary and summarization
Initialization
For an unlabeled data set: Uall, i = 0
(1) Randomly choose a small set of data X{i}
from Uall; U{i} = Uall ?X{i}
(2) Manually label each sentence in X{i} as
summary or non-summary by RDTW and hu-
man correction and save these sentences and
their labels in L{i}
Active Learning Process
(3) X{i} = null
(4) Train the classifier M{i} using L{i}
(5) Test U{i} by M{i}
(6) Calculate similarity score of given docu-
ment D between the extracted summary sen-
tences and the PowerPoint slides sentences by
equation 2
(7) Select the documents with top-five highest
similarity scores from U{i}
(8) Save selected samples into X{i}
(9) Manually correct each sentence label in
X{i} as summary or non-summary
(10) L{i + 1} = L{i} + X{i}
(11) U{i + 1} = U{i} ?X{i}
(12) Evaluate M{i} on the testing set E
(13) i = i+1, and repeat from (3) until U{i} is
empty or M{i} obtains satisfying performance
(14) M{i} is produced and the process ends
We start our experiments by randomly choosing
six documents for manual labeling. We gradually
increase the training data pool by choosing five
more documents each time for manual correction.
We carry out two sets of experiments for compar-
ing our algorithm and random selection. We evalu-
ate the summarizer by ROUGE-L (summary-level
Longest Common Subsequence) F-measure (Lin,
2004).
The performance of our algorithm is illustrated
by the increasing ROUGE-L F-measure curve in
Figure 1. It is shown to be consistently higher than
25
Figure 1: Active learning vs. random selection
using randomly chosen samples. We also find that
by using only 51 documents for training, the per-
formance of the summarization model achieved
by our approach is better than that of the model
trained by random selection using all 65 presen-
tations (0.514 vs. 0.512 ROUGE-L F-measure).
This shows that our active learning approach re-
quires 22% less training data. Besides, acoustic
features can improve the performance of active
learning of speech summarization. Without acous-
tic features, our summarizer only performs 0.47
ROUGE-L F-measure.
5 Conclusion and Discussion
In this paper, we propose using active learning re-
duce the need for human annotation for tagging
extractive reference summary of lecture speech
summarization. We use RDTW to extract sen-
tences from transcriptions according to Power-
Point slides, and these sentences are then hand
corrected as reference summaries. The unlabeled
documents are selected whose similarity scores
between the extracted summary sentences and the
PowerPoint slides sentences are top-N highest for
labeling summary sentences. We then use an SVM
classifier to extract summary sentences. Summa-
rization results show an increasing learning curve
of F-measure, from 0.44 to 0.514, consistently
higher than that of using randomly chosen train-
ing data samples. Besides, acoustic features play
a significant role in active learning of speech sum-
marization. In our future work, we will try to ap-
ply different criteria, such as uncertainty-based or
committee-based criteria, for selecting samples to
be labeled, and compare the effectiveness of them.
6 Acknowledgements
This work is partially supported by GRF612806 of
the Hong Kong RGC.
References
C.C. Chang and C.J. Lin. 2001. LIBSVM: a library for sup-
port vector machines. Software available at http://www.
csie. ntu. edu. tw/cjlin/libsvm, 80:604?611.
Y. Fujii, K. Yamamoto, N. Kitaoka, and S. Nakagawa. 2008.
Class Lecture Summarization Taking into Account Con-
secutiveness of Important Sentences. In Proceedings of
Interspeech, pages 2438?2441.
T. Hayama, H. Nanba, and S. Kunifuji. 2005. Alignment
between a technical paper and presentation sheets using
a hidden markov model. In Active Media Technology,
2005.(AMT 2005). Proceedings of the 2005 International
Conference on, pages 102?106.
M.Y. Kan. 2007. SlideSeer: A digital library of aligned
document and presentation pairs. In Proceedings of the
7th ACM/IEEE-CS joint conference on Digital libraries,
pages 81?90. ACM New York, NY, USA.
D.D. Lewis and J. Catlett. 1994. Heterogeneous uncertainty
sampling for supervised learning. In Proceedings of the
Eleventh International Conference on Machine Learning,
pages 148?156. Morgan Kaufmann.
C.Y. Lin. 2004. Rouge: A Package for Automatic Evalua-
tion of Summaries. Proceedings of the Workshop on Text
Summarization Branches Out (WAS 2004), pages 25?26.
A. McCallum and K. Nigam. 1998. Employing EM in Pool-
based Active Learning for Text Classification. In Proceed-
ings of ICML, pages 350?358.
A. Nenkova, R. Passonneau, and K. McKeown. 2007. The
Pyramid Method: Incorporating human content selection
variation in summarization evaluation. ACM Transactions
on Speech and Language Processing (TSLP), 4(2).
R. Ribeiro and D.M. de Matos. 2007. Extractive Summa-
rization of Broadcast News: Comparing Strategies for Eu-
ropean Portuguese. Lecture Notes in Computer Science,
4629:115.
D. Shen, J. Zhang, J. Su, G. Zhou, and C.L. Tan. 2004.
Multi-criteria-based Active Learning for Named Entity
Recognition. In Proceedings of 42th Annual Meeting of
the Association for Computational Linguistics. Associa-
tion for Computational Linguistics Morristown, NJ, USA.
S. Tong and D. Koller. 2002. Support vector machine ac-
tive learning with applications to text classification. The
Journal of Machine Learning Research, 2:45?66.
G. Tur, D. Hakkani-Tr, and R. E. Schapiro. 2005. Combin-
ing Active and Semi-supervised Learning for Spoken Lan-
guage Understanding. Speech Communications, 45:171?
186.
J.J. Zhang, S. Huang, and P. Fung. 2008. RSHMM++ for
extractive lecture speech summarization. In IEEE Spoken
Language Technology Workshop, 2008. SLT 2008, pages
161?164.
26
Identifying Concepts Across Languages:
A First Step towards a Corpus-based Approach to Automatic Ontology
Alignment
Grace Ngai
 
Marine Carpuat

Pascale Fung
  
grace@intendi.com eemarine@ust.hk pascale@ee.ust.hk
 
Intendi Inc.
Hong Kong

Human Language Technology Center
HKUST
Clear Water Bay, Hong Kong
1 Introduction
The growing importance of multilingual informa-
tion retrieval and machine translation has made mul-
tilingual ontologies an extremely valuable resource.
Since the construction of an ontology from scratch
is a very expensive and time consuming undertak-
ing, it is attractive to consider ways of automatically
aligning monolingual ontologies, which already ex-
ist for many of the world?s major languages.
This paper presents a first step towards the cre-
ation of a bilingual ontology through the alignment
of two monolingual ontologies: the American En-
glish WordNet and the Mandarin Chinese HowNet.
These two ontologies have structures which are very
different from each other, as well as being con-
structed for two very different languages, which
makes this an appropriate and challenging task for
our algorithm.
2 Alignment of Ontologies
In this paper, we address the problem of automatic
multilingual ontology alignment. Multilingual on-
tologies are very useful, but are also very time-
consuming and expensive to build. For example,
Euro WordNet (Vossen, 1998), a multilingual on-
tology for 8 European languages, involved 11 aca-
demic and commercial institutions and took 3 years
to complete. Furthermore, for many of the world?s
major languages, monolingual ontologies already
exist in some shape or form. Therefore, it is reason-
able and attractive to investigate whether a multi-
lingual ontology could be quickly and robustly con-
structed from monolingual resources.
Given the easy availability of bilingual dictionar-
ies, the task might seem easy at a first blush. How-
ever, given two independently constructed ontolo-
gies, there always exists some difference in their
structure that makes it difficult to perform a purely
structural alignment. These differences arise from
different approaches and philosophies taken during
the construction of the ontology; and for ontologies
in different languages, differences which stem from
dissimilarities between the languages concerned.
In addition, multilingual ontology alignment also
has to deal with machine translation issues. Since
an ontology arranges words in a semantic hierar-
chy, it is possible for a word to appear in several
different places in the hierarchy depending on its
semantic sense. However, words and concepts in a
given language do not always translate cleanly into a
second language; a word often has multiple transla-
tions, and they do not always share the same mean-
ings. In the absence of any ambiguity resolution,
synonym sets in one ontology will be erroneously
aligned to multiple synonym sets in the second on-
tology. This is a serious problem: an investigative
experiment with two ontologies, the American En-
glish WordNet and the Mandarin Chinese HowNet,
found that, in the absence of any word sense disam-
biguation, each HowNet definition (the equivalent
of a synonym set from WordNet) corresponded to
an average of 8.1 WordNet synonym sets.
The approach taken in this paper works upon the
assumption that even though a word may have dif-
ferent translations that correspond to different se-
mantic senses, it is not likely that its synonyms will
have the same exact set of translations. Given a syn-
onym set, or synset, in one ontology, our approach
considers the average similarity between its words
and words from all potential alignment candidates:
Given two ontologies  and  , a synonym set
(synset) 	
 , and a similarity score Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010), pages 197?205,
Beijing, August 2010
Unsupervised Synthesis of Multilingual Wikipedia Articles 
 
Chen Yuncong 
The Human Language Technology Center 
The Hong Kong University of Science and 
Technology 
ee_cyxab@stu.ust.hk 
Pascale Fung 
The Human Language Technology Center 
The Hong Kong University of Science and 
Technology 
pascale@ee.ust.hk 
 
Abstract 
In this paper, we propose an 
unsupervised approach to automatically 
synthesize Wikipedia articles in 
multiple languages. Taking an existing 
high-quality version of any entry as 
content guideline, we extract keywords 
from it and use the translated keywords 
to query the monolingual web of the 
target language. Candidate excerpts or 
sentences are selected based on an 
iterative ranking function and 
eventually synthesized into a complete 
article that resembles the reference 
version closely. 16 English and Chinese 
articles across 5 domains are evaluated 
to show that our algorithm is domain-
independent. Both subjective 
evaluations by native Chinese readers 
and ROUGE-L scores computed with 
respect to standard reference articles 
demonstrate that synthesized articles 
outperform existing Chinese versions or 
MT texts in both content richness and 
readability. In practice our method can 
generate prototype texts for Wikipedia 
that facilitate later human authoring. 
1 Introduction 
Wikipedia has over 260 versions in different 
languages, but the great disparity in their scope 
and quality is hindering the effective spread of 
knowledge. The English version is currently the 
dominant one with over 3 million articles while 
the Chinese version, for example, has only one 
tenth the amount. Most Chinese articles suffer 
from content incoherence and lack of details 
compared to their English counterparts. Some 
of these articles are human-authored translation 
of the English version with varying degrees of 
accuracy and completeness, and others are ill-
arranged combinations of excerpts directly 
adapted from external sources. The former 
takes considerable human effort and the latter 
tends to produce fragmented and incomplete 
texts. The intuitive solution of machine 
translation is also not feasible because it hardly 
provides satisfactory readability. 
These problems call for a synthesis approach. 
In order to present the information conveyed by 
an English article in Chinese, instead of 
literally translate it, we build a topic-template 
expressed by the keywords extracted from the 
English article. Machine-translation of these 
keywords helps to yield the topic-template in 
Chinese. Using the topic-template in Chinese, 
we form a pool of candidate excerpts by 
retrieving Chinese documents from the Internet. 
These online documents are usually human-
authored and have optimal readability and 
coherence. Candidate excerpts are further split 
into segments as synthesis unit. For segment 
selection, we propose an iterative ranking 
function that aims to maximize textual 
similarity, keywords coverage, and content 
coherence, while penalizes information 
redundancy. 
A feature of our approach is the use of bi-
lingual resources throughout the synthesis 
process. We calculate similarity scores of two 
texts based on both English and Chinese 
versions of them, which forms a more precise 
measure than using either version alone. 
For the sake of clarity, we will use English and 
Chinese as examples of source and target 
language respectively when describing the 
methodology. Nonetheless, our approach is not 
constrained to any specific language pair and 
supports both direction of synthesis. 
197
2 Related Work 
Much work has been done to explore the 
multilingualism of Wikipedia. (Adafre et al 
2006) investigated two approaches to identify 
similarity between articles in different 
languages for automatic generation of parallel 
corpus, including a machine-translation based 
approach and one using a bilingual lexicon 
derived from the hyperlink structure underlying 
Wikipedia articles. Both methods rely on pair-
wise comparisons made at the sentential level, 
which hardly account for similarity or 
coherence in the paragraph scope. Besides it is 
not a generative algorithm and thus 
inapplicable to our problem where comparable 
sentences in Chinese are simply not available. 
A generative approach was proposed by 
(Sauper and Barzilay, 2009) to create highly-
structured Wikipedia articles (e.g. descriptions 
of diseases) composed of information drawn 
from the Internet. It uses an automatically-
induced domain-specific template, and the 
perceptron algorithm augmented with a global 
integer linear programming (ILP) formulation 
to optimize both local fit of information into 
each section and global coherence across the 
entire article. This method works only for 
specific domains where articles have obviously 
separable sections (e.g. Causes and Symptoms) 
and it requires a training corpus for each 
domain to induce the template. Moreover, the 
synthesis units they use are complete excerpts 
rather than individual sentences as in our 
approach. Their choice is based on the 
assumption that texts on the Internet appear in 
complete paragraphs, with structure strictly 
adhere to the fixed training templates, which 
may be true for specific domains they test on, 
but fails to hold for domain-independent 
application. Instead, our algorithm aims to 
synthesize the article in the sentential level. We 
select sentences to fit the source content at run 
time, regardless to whether a pre-determined 
structural template exists or not. Therefore the 
requirement on the structures of source articles 
becomes very flexible, enabling our system to 
work for arbitrary domain. In a sense, rather 
than being a structure-aware approach, our 
algorithm performs in a content-aware manner. 
This also makes maintaining coherence 
throughout article a lot more challenging. 
Works on monolingual extractive text 
summarization also lend insights into our 
problem. (Goldstein et al, 2000) used 
sequential sentence selection based on Maximal 
Marginal Relevance Multi-Document (MMR-
MD) score to form summarizations for multiple 
documents, with the constraint of sentence 
count. Since our problem does not have this 
constraint, we employ a variant of MMR-MD 
and introduced new terms specific to this task. 
(Takamura and Okumura, 2009) formulated a 
text summarization task as a maximum 
coverage problem with knapsack constraint and 
proposed a variety of combinatorial 
mathematics-based algorithms for solving the 
optimization problem. 
For multi-lingual summarization, (Evans, 2005) 
applied the concept of multi-lingual text 
similarity to summarization and improved 
readability of English summaries of Arabic text 
by replacing machine translated Arabic 
sentences with highly similar English sentences 
whenever possible. 
 
3 Methodology 
Figure 1 describes the high-level algorithm of 
our approach. The system takes as input the 
English Wikipedia page and outputs an article 
in Chinese. 
First, the structured English article is extracted 
from the Wikipedia page. Due to the relative 
independence of contents in different sections 
in typical Wikipedia articles (e.g. childhood, 
early writings), a separate synthesis task is 
performed on each section and all synthesized 
sections are eventually combined in the original 
order to form the Chinese article. 
For each section, keywords are extracted from 
the English text using both tf-idf and the graph-
based TextRank algorithm. Named entities, 
time indicators, and terms with Wikipedia 
hyperlinks are also included. These keywords 
express the topics of the current section and are 
regarded as the content guideline. We then use 
Google Translate and Google Dictionary to 
198
obtain the Chinese translations of these 
keywords and thereby convert the content 
guideline into Chinese. The Chinese keywords 
are then combined with the translated subject 
term and section title to form queries that are 
used to retrieve online Chinese documents by 
Google search. The returned Chinese 
documents are clustered and filtered based on 
both their format and content. The remaining 
candidate excerpts are further split using the 
TextTiling algorithm (Hearst, 1997) into 
segments that constitutes the text units for 
synthesis. This unit size ensures both semantic 
completeness within each unit and flexibility of 
combining multiple units into coherent 
paragraphs. Segments are chosen according to 
scores computed iteratively by a variant of the 
MMR-MD scoring function that considers not 
only the relevance of an individual segment to 
the source section but also its impact on the 
provisional synthesized section as a whole. 
3.1 Wikipedia Page Preprocessing 
The source Wikipedia page is parsed to remove 
non-textual page elements (e.g. images, info-
boxes and side-bars). Only texts and headings 
are extracted and their structures are maintained 
as templates for final integration of synthesized 
sections. 
3.2 Keyword Extraction 
The keyword set K for a section is the union of 
6 categories of content-bearing terms. 
  ?    
  : set of terms with high tf-idf score (top 5%) 
  : set of terms with high TextRank score (top 
5%) 
  : set of named entities 
  : set of temporal indicators (e.g. June, 1860) 
  : set of terms with Wikipedia links 
  : section title 
 
For   , tf-idf scores are computed by: 
 
       ?       (
 
   
  )  
      
where     is the term frequency of term i in the 
section and     is the document frequency of 
term i in a corpus consists of 2725 high-quality 
English Wikipedia articles 1 , which well 
represent the language style of Wikipedia. 
 
For   , we compute TextRank scores 
according to (Mihalcea and Tarau, 2004). It is a 
graph-based model where words as vertices 
recursively vote for the weights of their linked 
neighbors (e.g. words appear in the same 
sentence as them) using the formula: 
 
  (  )  
(   )    ?
   
?          (  )
     (  )   (  )  
 
                                                          
1
  http://evanjones.ca/software/wikipedia2text.html 
Input:  
English version of an entry 
Output:  
Synthesized Chinese version 
Algorithm: 
1: Parse the English Wikipedia page to extract the structured texts. 
2: For each section: 
2.1: Extract keywords. 
2.2: Use Chinese translation of keywords to search online Chinese texts. 
2.3: Filter retrieved Chinese texts and split them into segments. 
2.4: Synthesize the current section using candidate segments. 
3: Generate the Chinese Wikipedia page by combining synthesized sections according 
to the original structure of English version. 
 
Figure 1. High-level algorithm of the synthesis approach 
199
Where   (  ) is the set of vertices with forward 
links to i,    (  )  is the set of vertices 
receiving links from i,     is the weight of edge 
between    and   . In the case of a word graph, 
we simplify this formula by assuming the graph 
to be undirected and unweighted. Each pair of 
words occurring in the same sentence share an 
edge between them and all word vertices have 
initial weights of 1. 
 
Unlike tf-idf which considers only word-
specific values and tends to give higher weights 
for rare words, TextRank uses global 
information about how a word is used in its 
context to induce its importance and has the 
advantage of highlighting keywords that are 
relatively common but highly relevant. In this 
sense, these two measures complement each 
other. Named entities are recognized using the 
named entity chunker provided by the NLTK 
(Natural Language ToolKit) package2. 
3.3 Keyword Translation 
Keywords are then translated using Google 
Dictionary to form Chinese queries. Usually 
one English keyword has several translations 
and they will be used jointly when forming the 
search query. 
Google Dictionary often fails to generate 
correct transliteration for rare names, so we 
augment it with a function of parenthesized 
phrase translation. We basically seeks named-
entity strings from online documents that are in 
the format of ?CHINESE (ENGLISH)? and 
extracts the Chinese transliteration from the 
pattern using regular expression combined with 
a Pinyin (Chinese Romanization) 3 /English 
pronunciation lookup table. Since Chinese 
words are not spaced in documents, the 
Pinyin/English lookup is helpful to determine 
the boundary of the Chinese transliteration 
based on the fact that most Chinese 
transliterations start with characters pronounced 
similar to the initial syllables in corresponding 
English names. This function is relatively 
simple but works surprisingly well as many 
                                                          
2 The package is available at http://www.nltk.org 
3 Pinyin information is obtained from Unicode Han 
Database at http://www.unicode.org/reports/tr38/ 
rare named entities are available in this pattern 
on the Web. 
3.4 Web Search 
Keywords in Chinese alternatively form query 
pairs with the Wikipedia subject term. Each 
pair is used to retrieve a set of (16 in our 
experiments) Chinese documents containing 
both words with Google Search. If a keyword 
has multiple translations, they are joined by the 
string ?OR? in the query which is the way to 
specify alternatives in Google logic. If a 
keyword is a named entity, its English version 
is also used as an alternative in order to acquire 
documents in which the subject is referred to by 
its English name instead of transliterations. For 
the subject ?Chekhov/????, a keyword with 
two transliterations ?Taganrog/????/??
? ? ? and another keyword with two 
transliterations ?father/??/??? will result 
in two query pairs: ?Chekhov OR ??? 
Taganrog OR ???? OR ????? and 
?Chekhov OR ??? ?? OR ???. 
3.5 Candidate Filtering 
The retrieved excerpts are filtered first by 
criteria on format include text length and the 
percentage of white-space and non-Chinese 
characters. Pair-wise similarity is then 
computed among all the remaining excerpts and 
those above a certain threshold are clustered. 
Within a cluster only the centroid excerpt with 
maximum similarity with the source section 
will be selected. This stage typically eliminates 
? of the documents that are either not 
sufficiently relevant or redundant. The 
similarity measure we use is a combination of 
both English and Chinese versions of cosine 
similarity and Jaccard index. 
   (   )           (   )           (   )  
                                (   )           (   )  
For Chinese excerpts, English similarity is 
computed by first translating them into English 
by Google Translate and taking tf-idf as token 
weights. Similar procedure works for 
computing Chinese similarity for English 
excerpts, except that Chinese texts need to be 
200
segmented4  first and weights are based on tf 
only. These machine translations do not require 
grammatical correctness since they are 
essentially used as bags of words in both cosine 
similarity and Jaccard index. During this stage, 
every excerpt acquires bi-lingual versions, 
which is important for the extended similarity 
measure in the iterative ranking function. 
Filtered excerpts are further split into segments 
using the TextTiling algorithm. After clustering 
the remaining segments form the candidate 
units for synthesis of the current section. 
3.6 Iterative Scoring Function 
Based on the idea that the ?goodness? of a 
segment should be evaluated both on its 
individual relevance to the source and the 
overall impact on the synthesized section, we 
summarize four factors for scoring a segment: 
(1) Intuitively a segment scores higher if it has 
higher similarity to the source section; (2) A 
segment makes positive contribution to 
synthesized section if it introduces some 
keywords mentioned in the source; (3) A 
segment tends to improve the coherence of 
synthesized section if it comes from the same 
excerpts as the other segments in synthesized 
section; (4) A sentence should be penalized if 
its content is redundant with the synthesized 
section. 
Integrating the four factors above, we propose 
that for source text r, the score of the ith 
candidate segment si in the nth iteration is 
formulated as: 
   (  )       (  )        (  )  
                         (  )       (  )  
This formula is composed of 4 terms 
corresponding to the ?goodness? factors:   (  ) 
for similarity,    (  )  for keyword coverage, 
  (  )  for coherence, and    (  )  for 
redundancy. The corresponding weights are 
tuned in a large number of experiments as to 
                                                          
4 The segmentation tool using forward maximum 
matching is obtained at 
http://technology.chtsai.org/mmseg 
achieve optimal performance. This function is a 
variant of the original MMR-MD score tailored 
for our application. 
  (  ) is a comprehensive similarity measure of 
segment si to the reference text r. 
  (  )        (    )        (    )  
                      (    )        (    )  
where p is the parent section of r and    is the 
parent excerpt of   . Similarities between parent 
excerpts are also examined because sometimes 
two segments, especially short segments, 
despite their textual similarity actually come 
from very different contexts and exhibit 
different focuses. In this case, the latter three 
terms will suppress the score between these two 
segments which would otherwise be 
erroneously high and therefore produce a more 
precise measure of similarity. 
   (  )  measures the contribution of    in 
terms of uncovered keywords. 
   (  )  ?    ( )
     
         
 
         ?   
     
 
where    is the winner set in the nth iteration. 
   is the set of keywords in the reference text 
and    is the set of keywords in the selected 
segment   .     represents the set of keywords 
in the reference that are not yet been covered 
by the provisional synthesized text in the nth 
iteration.    (  )  quantifies the keyword 
contribution as the sum of idf values of 
uncovered keywords. The subject term is 
excluded because it as a keyword does not 
reflect any topic bias and is therefore not a 
good indicator for coverage. 
  (  ) is a term that reflects the coherence and 
readability in the synthesized text.  
  (  )  |{  |           }| 
201
Input: 
 Sn: candidate set in iteration n 
 r: the reference text 
Define: 
 n: iteration index 
Dn: winner set in iteration n 
Csel-segment:            (    )            
Csel-sentence:            (    )            
           (    )     
                           (    )            
Cbreak:                (    )            
Algorithm: 
     ,     
 while     : 
                        (  ) 
     if Cbreak: 
         return    
     else if Csel-segment: 
                   
     else if Csel-sentence: 
                                                  
                                                               
                    
               
Output: 
 Synthesized text for the reference r 
where    is the parent excerpt of    and    is the 
parent excerpt of   . Segments from the same 
excerpts tend to be less redundant and more 
coherent. Therefore candidates that share the 
same parent excerpts as segments in winner set 
are more favorable and rewarded by this term. 
This is a major difference from the original 
MMR-MD function in which sentences from 
different documents are favored. This is 
because their formula is targeted for automatic 
summarization where more emphasis is put on 
diversity rather than coherence. 
  (  )  measures the redundancy of the 
synthesized text if    is included. It is quantified 
as the maximum similarity of    with all 
selected segments. 
  (  )     
     
 (     ) 
3.7 Segment Selection Algorithm 
 
Figure 2 describes the segment selection 
algorithm. Starting with a candidate set and an 
empty winner set, we iteratively rank the 
candidates by Q and in each iteration the top-
ranked segment is examined. There are two 
circumstances a segment would be selected for 
the winner set: 
 
(1) if the segment scores sufficiently high 
(2) the segment does not score high enough for 
an unconditional selection, but as long as it 
introduces uncovered keywords,  its 
contribution to the overall content quality 
may still overweigh the compromised 
similarity 
In the second circumstance however, since we 
are only interested in the uncovered keywords, 
it may not be necessary for the entire segment 
to be included in the synthesized text. Instead, 
we only include the sentences in this segment 
that contain those keywords. Therefore we 
propose two conditions:  
? Csel-segment: condition for selecting a segment 
   (    )           
? Csel-sentence: condition for selecting sentences 
   (     )                   (     )  
          (    )            
 
Thresholds in both conditions are not static but 
dependent on the highest score of all candidates 
in order to accommodate diversity in score 
range for different texts. Finally if no more 
candidates are able to meet the lowered score 
threshold, even if they might carry new 
keywords, we assume they are not suitable for 
synthesis and return the current winner set. This 
break condition is formulated as Cbreak: 
? Cbreak: condition to finish selection 
   (    )            
4 Evaluation 
4.1 Experiment Setup 
We evaluate our system on 16 Wikipedia 
subjects across 5 different domains as listed in 
Table 1. 
Figure 2. Segment selection algorithm 
202
The subjects are selected from ?the List of 
Articles Every Wikipedia Should Have? 5 
published by Wikimedia. These subjects are 
especially appropriate for our evaluation 
because we can (1) use a subset of such articles 
that have high quality in both English and 
Chinese as standard reference for evaluation; (2) 
safely assume Chinese information about these 
subjects is widely available on the Internet; (3) 
take subjects currently without satisfactory 
versions in Chinese as our challenge. 
Human Evaluation 
We presented the synthesized articles of these 
subjects to 5 native Chinese readers who 
compare synthesized articles with MT results 
and existing Chinese versions on Wikipedia 
which range from translated stubs to human-
authored segments. We asked the reviewers to 
score them on a 5-point scale in terms of four 
quality indicators: structural similarity to the 
English version, keyword coverage, fluency, 
and conciseness. 
Automatic Evaluation 
 
In addition to human evaluation, we also 
compare synthesized articles to several high-
quality Chinese Wikipedia articles using 
ROUGE-L (C.Y. Lin, 2004). We assume these 
                                                          
5
http://meta.wikimedia.org/wiki/List_of_articles_every_W
ikipedia_should_have/Version_1.2 
Chinese versions are the goals for our synthesis 
system and greater resemblance with these 
standard references indicates better synthesis. 
ROUGE-L measures the longest common 
subsequence (LCS) similarity between two 
documents, rather than simply word overlap so 
it to some degree reflects fluency. 
4.2 Result Analysis 
Human Evaluation 
Human evaluator feedbacks for articles in 
different categories are shown in Table 2. 
Machine-translated versions are judged to have 
the highest score for structural similarity, but 
erroneous grammar and word choices make 
their readability so poor even within sentences 
and therefore of no practical use. 
 
Generally, articles synthesized by our system 
outperform most existing Chinese versions in 
terms of both structural and content similarity. 
Many existing Chinese versions completely 
ignore important sections that appear in English 
versions, while our system tries to offer 
information with as much fidelity to the 
English version as possible and is usually able 
to produce information for every section. 
Synthesized articles however, tend to be less 
fluent and more redundant than human-
authored versions. 
 
Performance varies in different domains. 
Synthesis works better for subjects in Person 
category, because the biographical structure 
provides a specific and fairly unrelated content 
in each section, making the synthesis less 
redundancy-prone. On the other hand, there is 
arbitrariness when organizing articles in Event 
and Culture category. This makes it difficult to 
find online text organized in the same way as 
the English Wikipedia version, therefore 
introducing a greater challenge in sentence 
selection for each section. Articles in the 
Science category usually include rare 
terminologies, and formatted texts like 
diagrams and formula, which impede correct 
translation and successful extraction of 
keywords. 
Category Subjects 
Person Anton Chekhov 
Abu Nuwas 
Joseph Haydn 
Li Bai 
Organization HKUST 
IMF 
WTO 
Events Woodstock Festival 
Invasion of Normandy 
Decembrist Revolt 
Science El Nino 
Gamma Ray 
Stingray 
Culture Ceramic Art 
Spiderman 
Terrorism 
 
Table 1. Subjects used for evaluation 
203
 
Automatic Evaluation 
 
Using ROUGE-L to measure the quality of 
both synthesized and MT articles against 
human-authored standard references, we find 
synthesized articles generally score higher than 
MT versions. The results are shown in Table 3. 
 
The synthesized articles, extracted from high 
quality human-authored monolingual texts, are 
generally better in precision than the MT 
articles because there is less erroneous word 
choice or grammatical mistakes. Most 
synthesized articles also have higher recall than 
MT versions because usually a substantial 
portion of the high-quality Chinese excerpts, 
after being retrieved by search engine, will be 
judged by our system as good candidate texts 
and included into the synthesized article. This 
naturally increases the resemblance of 
synthesized articles to standard references, and 
thus the F-scores. Note that since our method is 
unsupervised, the inclusion of the standard 
Chinese articles underscores the precision and 
recall of our method. 
 
5 Conclusion 
 
In this paper, we proposed an unsupervised 
approach of synthesizing Wikipedia articles in 
multiple languages based on an existing high-
quality version of any entry. By extracting 
keywords from the source article and retrieving 
relevant texts from the monolingual Web in a 
target language, we generate new articles using 
an iterative scoring function. 
 
Synthesis results for several subjects across 
various domains confirmed that our method is 
able to produce satisfactory articles with high 
resemblance to the source English article. For 
many of the testing subjects that are in ?stub? 
status, our synthesized articles can act as either 
replacement or supplement to existing Chinese 
versions. For other relatively well-written ones, 
our system can help provide content prototypes 
for missing sections and missing topics, 
bootstrapping later human editing. 
 
A weakness of our system is the insufficient 
control over coherence and fluency in 
paragraph synthesis within each section, new 
methods are being developed to determine the 
proper order of chosen segments and optimize 
the readability. 
 
We are working to extend our work to a system 
that supports conversion between major 
languages such as German, French and Spanish. 
The employment of mostly statistical methods 
in our approach facilitates the extension. We 
have also released a downloadable desktop 
application and a web application based on this 
system to assist Wikipedia users.  
Cat. Structural Similarity Coverage Fluency Conciseness 
 Synt.  Orig. MT Synt. Orig. MT Synt. Orig. MT Synt. Orig. MT 
Psn. 2.85 1.49 5 2.94 1.84 4.51 2.71 4.58 0.83 1.74 4.47 n/a 
Org. 1.96 1.22 5 2.51 2.10 4.46 2.10 4.42 1.06 0.99 4.53 n/a 
Evt. 1.37 1.13 5 2.56 1.94 4.40 2.45 4.46 0.81 0.80 4.40 n/a 
Sci. 2.43 1.30 5 2.68 2.14 4.42 2.53 4.51 1.02 1.05 4.50 n/a 
Cul. 1.39 1.35 5 2.2 2.21 4.54 2.32 4.54 0.94 1.34 4.59 n/a 
Avg. 2.02 1.30 5 2.58 2.05 4.47 2.42 4.50 0.93 1.22 4.50 n/a 
 
 
Table 2. Result of human evaluation against English source articles (out of 5 points; Synt: 
synthesized articles; Orig: the existing human-authored Chinese Wikipedia versions; MT: Chinese 
versions generated by Google Translate) 
Category Recall Precision F-score 
 Synt. MT Synt. MT Synt. MT 
Psn. 0.48 0.30 0.20 0.16 0.28 0.22 
Org. 0.40 0.29 0.16 0.13 0.23 0.18 
Evt. 0.36 0.26 0.13 0.15 0.19 0.19 
Sci. 0.31 0.22 0.14 0.11 0.19 0.15 
Cul. 0.37 0.27 0.13 0.12 0.24 0.17 
Avg. 0.38 0.27 0.15 0.13 0.23 0.18 
 
Table 3. Results of automatic evaluation 
against gold Chinese reference articles (Synt: 
synthesized articles; MT: Chinese versions 
generated by Google Translate) 
204
Reference 
Adafre, Sisay F. and Maarten de Rijke, ?Finding 
Similar Sentences across Multiple Languages in 
Wikipedia?, Proceedings of the EACL Workshop on 
New Text, Trento, Italy, 2006 
Bird, Steven, E. Klein, and E. Loper, Natural 
Language Processing with Python --- Analyzing 
Text with the Natural Language Toolkit, O'Reilly 
Media, 2009 
 
Evans, David K., ?Identifying Similarity in Text: 
Multi-Lingual Analysis for Summarization?, PhD 
thesis, Columbia University, 2005. 
 
Goldstein, Jade, Vibhu Mittal, Jaime Carbonell and 
Mark Kantrowitz, ?Multi-document summarization 
by sentence extraction?, NAACL-ANLP 2000 
Workshop on Automatic summarization, pages 40-
48, 2000 
 
Hearst, Marti A., ?TextTiling: Segmenting Text into 
Multi-paragraph Subtopic Passages?, Computational 
Linguistics, Volume 23, Issue 1, pp. 33-64, 1997 
 
Lin, Chin-Yew, ?ROUGE: A Package for 
Automatic Evaluation of Summaries?, Proceedings 
of Workshop on Text Summarization Branches Out, 
Post-Conference Workshop of ACL 2004, Barcelona, 
Spain. 
 
Mihalcea, Rada and Paul Tarau, ?TextRank: 
Bringing order into texts?, Proceedings of EMNLP, 
pages 404?411 Barcelona, Spain, 2004 
 
Sauper, Christina and Regina Barzilay, 
?Automatically Generating Wikipedia Articles: a 
Structure-Aware Approach?, Proceedings of the 
47th Annual Meeting of the ACL and the 4th 
IJCNLP of the AFNLP, pages 208?216, Suntec, 
Singapore, 2-7 August 2009. 
Takamura, Hiroya and Manabu Okumura, ?Text 
Summarization Model based on Maximum 
Coverage Problem and its Variant?, Proceedings of 
the 12th Conference of the European Chapter of the 
Association for Computational Linguistics, pages 
781-789, 2009 
 
 
205
Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010), pages 1299?1307,
Beijing, August 2010
 	
 	   
 	Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural
Language Learning, pages 766?776, Jeju Island, Korea, 12?14 July 2012. c?2012 Association for Computational Linguistics
Cross-Lingual Language Modeling with Syntactic Reordering for
Low-Resource Speech Recognition
Ping Xu and Pascale Fung
Human Language Technology Center
Department of Electronic and Computer Engineering
The Hong Kong University of Science and Technology, Clear Water Bay, Hong Kong
xuping@ust.hk, pascale@ece.ust.hk
Abstract
This paper proposes cross-lingual language
modeling for transcribing source resource-
poor languages and translating them into tar-
get resource-rich languages if necessary. Our
focus is to improve the speech recognition
performance of low-resource languages by
leveraging the language model statistics from
resource-rich languages. The most challeng-
ing work of cross-lingual language modeling
is to solve the syntactic discrepancies between
the source and target languages. We therefore
propose syntactic reordering for cross-lingual
language modeling, and present a first result
that compares inversion transduction grammar
(ITG) reordering constraints to IBM and lo-
cal constraints in an integrated speech tran-
scription and translation system. Evaluations
on resource-poor Cantonese speech transcrip-
tion and Cantonese to resource-rich Mandarin
translation tasks show that our proposed ap-
proach improves the system performance sig-
nificantly, up to 3.4% relative WER reduction
in Cantonese transcription and 13.3% relative
bilingual evaluation understudy (BLEU) score
improvement in Mandarin transcription com-
pared with the system without reordering.
1 Introduction
Statistical language modeling techniques have
achieved remarkable success in speech and language
processing (Clarkson and Rosenfeld, 1997; Stolcke,
2002). However, this success largely depends on the
availability of a large amount of suitable text data in
a language. Without sufficient text data for training,
it is very difficult to build a practical and usable sta-
tistical language model. Therefore, most of the ad-
vances have been reported in so called resource-rich
language such as English, Mandarin and Japanese,
after creating linguistic resources of these languages
at considerable cost. Today there are more than
6000 living languages spoken in the world (Gordon
et al2005), and most of them have little transcribed
texts and are considered as resource-poor languages
(Nakov and Ng, 2009). Many of these languages are
actually spoken by a huge number of speakers (e.g.
some Chinese and Indian languages), and thus there
is still a great demand to build speech and language
processing systems for these languages.
Owing to data scarcity, most often an interpo-
lation (Bellegarda, 2004) of language models be-
tween a resource-poor language and a resource-rich
language is used in most low-resource ASR sys-
tems. Some researchers have proposed transform-
ing resource-rich language models to resource-poor
language models by word-level transduction, either
in a context-independent or context-dependent man-
ner (Hori et al2003; Akita and Kawahara, 2006;
Jensson et al2009; Neubig et al2010). In (Jens-
son et al2009), a simple dictionary based context-
independent transduction from a resource-rich lan-
guage to a resource-poor language is exploited to
improve speech recognition of the resource-poor
language. In (Hori et al2003; Akita and Kawahara,
2006; Neubig et al2010), context-dependent trans-
duction is exploited. In their case, the resource-poor
language is a spoken language, and the resource-rich
language is a written language. They carried out lan-
guage model transformation since the input speech
766
is in speaking-style and the output text is in written-
style.
Others have investigated cross-lingual informa-
tion between a resource-poor language and a
resource-rich language. In (Khudanpur and Kim,
2002), cross-language cues are used to improve a
language model of a resource-poor language. They
used cross-lingual unigram probabilities trained
from a story-specific parallel corpus of the resource-
poor and resource-rich languages. They interpo-
late the language model of the resource-poor lan-
guage with those unigram probabilities. In (Kim and
Khudanpur, 2003), an n-gram language model in a
resource-poor language is interpolated with cross-
lingual unigram trigger probabilities. These triggers
are word pairs of the resource-poor and resource-
rich languages with the highest mutual information
across these two languages. Another way of esti-
mating those unigram probabilities is using latent
semantic analysis by measuring cosine similarities
from a document-aligned corpus for any given word
pair (Kim and Khudanpur, 2004).
Both interpolation and word-level transduction
approaches fail to meet the challenge of syntac-
tic discrepancies between the resource-poor and
resource-rich languages. This syntactic discrepan-
cies exist, for example, even between the Sinitic lan-
guages and Indian languages1 of the same family.
Sinitic languages such as Cantonese/Yue, Shang-
hai/Wu, etc. are officially considered as ?dialects?
of the standard Chinese Mandarin (or Putonghua)2.
However, they differ greatly from Mandarin in all
aspects and are not mutually comprehensible. For
instance, in addition to lexical and pronunciation
differences, Cantonese Chinese (Lee, 2011) differs
syntactically from Mandarin as well - we found that
there are approximately 10% syntactic inversions
between sentences of the two forms of Chinese.
We suggest that a better approach than interpo-
lation and word-level transduction is to use cross-
lingual language modeling with syntactic reorder-
1For example, Hindi and Malayalam (Geethakumary, 2002).
2Since Cantonese does not have an official written form,
there are very few written texts available for training language
models. In this paper, we treat Cantonese as a typical resource-
poor language and Mandarin as a typical resource-rich lan-
guage. This language pair will be used for illustration purposes
throughout this paper.
ing. A reordering model with reordering constraints,
such as ITG constraints (Wu, 1997), IBM con-
straints (Berger et al1996), and local constraints
(Kumar and Byrne, 2005) can account for the syn-
tactic differences. It has been shown in (Zens and
Ney, 2003; Kanthak et al2005; Dreyer et al2007)
that ITG constraints perform better than other con-
straints when tackling the reordering between many
language pairs. Previous work on weighted finite-
state transducer (WFST) based speech translation
such as (Casacuberta et al2004; Zhou et al2005;
Zhou et al2006; Mathias and Byrne, 2006; Ma-
tusov et al2006; Saon and Picheny, 2007) only
train the reordering model using IBM constraints,
local constraints or ad hoc rules. We will use
ITG constraints, which have only been applied to
text translation tasks before, to model the syntactic
differences in cross-lingual language modeling for
speech recognition.
We will implement a cross-lingual language
model using WFSTs, and integrate it into a WFST-
based speech recognition search space to give both
resource-poor language and resource-rich language
transcriptions. This creates an integrated speech
transcription and translation framework.
This paper is organized as follows: Section 2
presents our proposed cross-lingual language mod-
eling with syntactic reordering. In Section 3, we dis-
cuss speech recognition with cross-lingual language
models. Section 4 and 5 give the experimental setup
and results. We conclude our work at the end of this
paper.
2 Cross-lingual Language Modeling with
Syntactic Reordering
In automatic speech recognition (ASR), given an ob-
served source speech vector X, the decoding pro-
cess searches the best word sequence v?I1 (consists
of words v1, v2, ..., vI ) by maximizing the posterior
probability P (vI1 |X), where vI1 is the source tran-
script representing the transcription of the source
speech (see Eq. (1)). According to Bayes? law,
we can decompose P (vI1 |X) into an acoustic model
P (X|vI1) and a language model P (vI1). If a source
language Lv is a resource-rich language, then the
language model P (vI1) can be well estimated from
sufficient training texts. However, if the source lan-
767
guage Lv is a resource-poor language, then the lan-
guage model P (vI1) cannot be reliably or robustly
estimated due to lack of training texts.
v?I1 = argmax
vI1
P (vI1 |X) (1)
= argmax
vI1
P (X|vI1)P (vI1)
= argmax
vI1
P (X|vI1)
?
wJ1
P (vI1 |wJ1 )P (wJ1 )
? argmax
vI1
P (X|vI1)max
wJ1
P (vI1 |wJ1 )P (wJ1 )
Since this paper tackles the language modeling
challenge for low-resource speech recognition, here
we just assume that the source language Lv is a
resource-poor language. We further assume that
there is a target language Lw, which is a resource-
rich language closely related to the language Lv.
In order to improve the language model P (vI1)
of the resource-poor language Lv, we introduce
cross-lingual language modeling by decomposing
the language model P (vI1) into a translation model
P (vI1 |wJ1 ) and a language model P (wJ1 ) of the
resource-rich language Lw (see Eq. (1)). wJ1 is
the target resource-rich language transcript that con-
sists of words w1, w2, ..., wJ . P (vI1 |wJ1 )P (wJ1 ) is
defined as a cross-lingual language model. It lever-
ages the abundant statistics from the language model
P (wJ1 ) to improve the language model P (vI1) of the
resource-poor language.
The translation model P (vI1 |wJ1 ) can be esti-
mated by addressing the discrepancies between the
resource-poor language Lv and the resource-rich
language Lw, which can be modeled from a paral-
lel corpus of the Lv transcript vI1 and the Lw tran-
script wJ1 . For the syntactic inversions, we reorder
the word or phrase positions of the Lw language
model into those of the Lv language model. We
have observed that most of the words are aligned
monotonically between Lv and Lw within a phrase.
This paper, therefore only considers phrase-level re-
ordering, which effectively preserves the monotonic
word sequences within phrases, and significantly re-
duces the number of reordering paths compared with
word-level reordering.
2.1 Preprocessing: Phrase Extraction and
Segmentation
Our discussion starts with phrase extraction from the
parallel corpus. We define a phrase sequence v?K1
(consists of phrases v?1, v?2, ..., v?K ) segmented from
the word-level Lv transcript vI1 and w?K1 (consists of
phrases w?1, w?2, ..., w?K ) segmented from the word-
level Lw transcript wJ1 . Furthermore, we define a
reordering sequence rK1 , of which the detail can be
found in Section 2.2.
The phrase-level translation model P (vI1 |wJ1 ) is
decomposed into four components (see Eq. (2)):
segmentation model P (w?K1 |wJ1 ), phrasal reorder-
ing model P (rK1 |w?K1 , wJ1 ), phrase-to-phrase trans-
duction model P (v?K1 |rK1 , w?K1 , wJ1 ) and reconstruc-
tion model P (vI1 |v?K1 , rK1 , w?K1 , wJ1 ). Before present-
ing each component model, we need to extract two
phrase tables for the Lv transcript and the Lw tran-
script, respectively.
P (vI1 |wJ1 ) ? max
v?K1 ,r
K
1 ,w?
K
1
P (w?K1 |wJ1 ) ?
P (rK1 |w?K1 , wJ1 ) ?
P (v?K1 |rK1 , w?K1 , wJ1 ) ?
P (vI1 |v?K1 , rK1 , w?K1 , wJ1 ) (2)
The phrase extraction is based on word-to-word
alignments of the parallel corpus. We train word
alignments in both directions with GIZA++, and
then symmetrize the two alignments using the re-
fined method (Och and Ney, 2003). Figure 1 shows
an example of word-to-word alignment results be-
tween an Lv transcript (Cantonese) and an Lw
transcript (Mandarin), from which phrase-to-phrase
alignments are derived by identifying deletion, sub-
stitution, insertion and inversion.
Prior to phrasal reordering, the segmentation
model P (w?K1 |wJ1 ) implemented by a segmentation
WFST Sw is applied to segment a word sequence
wJ1 in the Lw language model into a phrase sequence
{w?1, w?2, ..., w?K}. The maximum number of words
that can be segmented into one phrase is controlled
by a segmentation order s. An example of Sw is
shown in Figure 3(a1). It segments a word sequence
{w1, w2, w3} into a phrase sequence {w1, w2 w3}
after performing composition (Mohri, 2009) with the
target Lw language model (see Figure 3(b1 & b2))3.
3The ? ? symbol is used to indicate the concatenation of con-
768
DeletionSubstitution Inversion Inversion & Insertion
i
iv
ji 
'
~
kv
kw
~
jw
j
4-6
4-7
1 2
1-1 2-2
??? ?
k?=1
3
3-4
?
k?=2
4
?
k?=3
5
5-8
??
k?=4
6
6-5
?
k?=5
7 8
7-3 8-3
? ?
k?=6
??
k=1
21
?
k=2
3
?
k=3
4
?
k=4
5
? ?
k=5
6 7
??
k=6
8
Substitution Substitution
Figure 1: An example (in English: Please give me an ad-
dress first) of phrase extraction from word-to-word align-
ments. i and j are word indexes. k? and k are phrase
indexes. i?j represents the word-to-word alignment.
k?k? represents the indentified phrase-to-phrase align-
ment.
2.2 Phrasal Reordering Model
Given a phrase sequence {w?1, w?2, ..., w?K} of the
Lw transcript, the role of the reordering model
P (rK1 |w?K1 , wJ1 ) is to reorder phrase positions of the
Lw transcript into those of the Lv transcript by per-
mutation of w?K1 according to a reordering sequence
{rK1 : rk ? {1, 2, ...,K}, rk 6= rk? 6=k}. The
phrase sequence {w?1, w?2, ..., w?K} is therefore re-
ordered into {w?r1 , w?r2 , ..., w?rK } consequently (see
Figure 2 where K = 3). Since arbitrary permuta-
tions of K phrases are NP-hard (Knight, 1999), re-
ordering constraints have to be set over rK1 to reduce
the number of permutations.
There are three reordering constraints widely used
in statistical machine translation, namely local con-
straints, IBM constraints and ITG constraints. Here
we would like to point out that this is the first
time that reordering constraints have been incorpo-
rated into a cross-lingual language model for speech
recognition.
Reordering Constraints
Local constraints make the restriction that one
phrase can jump at most L?1 phrases either forward
or backward, where L is the reordering distance (or
window size of permutation)4 . The generation of rK1
under local constraints can be viewed as solving of
the following problem (Kl?ve, 2009):
secutive words forming a phrase.
4The concept of reordering distance also applies to other
constraints.
How many permutations of
{1, 2, . . . k . . . ,K} satisfy |rk ? k| < L
for all k?
IBM constraints, a superset of local constraints
(Dreyer et al2007), generate permutations rK1 de-
viate from the monotonic phrase order {rK1 : rk =
k}. More specifically, any phrase position rk can be
selected from the positions of the first m yet uncov-
ered phrases (see Eq. (3)). A typical value of m is 4
(Zens and Ney, 2003), and we write IBM constraints
with m = 4 as IBM(4).
rk ?
?
?
?
?
?
?
?
?
?
?
?
{1, 2, ..., k ? 1 +m; rk 6= rk? 6=k}
if k ? K + 1?m,
{1, 2, ...,K; rk 6= rk? 6=k}
if K + 1?m < k ? K.
(3)
ITG constraints provide a more faithful coverage
of syntactic reordering in the parallel data than lo-
cal constraints and IBM constraints. Our presenta-
tion of ITG constraints starts with defining of some
permutation sets. Let SK be the set of permuta-
tions on {1,2,. . . ,K}. A permutation rK1 ? SK ,
where rK1 = r1r2 . . . rK , contains a subsequence
of type ? ? SM if and only if a sequence of in-
dices 1 ? i1 < i2 < . . . < iM ? K exists such
that ri1ri2 . . . riM has all the same pairwise compar-
isons as ? . We denote the set of permutations of SK
not containing subsequences of type ? by SK(?). If
we have sets SK(?1), . . . , SK(?p), we denote the set
SK(?1)? . . .?SK(?p) by SK(?1, . . . , ?p) (Barcucci
et al2000). ITG constraints allow the permutation
set SK(3142, 2413), which forbids subsequence of
type (3, 1, 4, 2) and its dual (2, 4, 1, 3). Explicitly,
ITG constraints avoid any permutation rK1 satisfy-
ing either ri2 < ri4 < ri1 < ri3 or ri3 < ri1 <
ri4 < ri2 , where 1 ? i1 < i2 < i3 < i4 ? K . In
(Wu, 1997), these forbidden subsequences are called
?inside-out? transpositions. They are fairly distorted
matchings, and hardly observed in real parallel data.
In order to get an intuitive sense of the reordering
capability of those three constraints, we list the num-
ber of permutations under local constraints, IBM
constraints as well as ITG constraints5 in Table 1.
5Interestingly, when K = L, the number of permuta-
tions under ITG constraints NITG = |SK(3142, 2413)|, and
|SK(3142, 2413)| equals the K?1-th Schro?der numbers sK?1
(Ehrenfeucht et al1998)
769
Table 1: Comparison of permutation number under local constraints (NLocal), IBM constraints (NIBM(4)) and ITG
constraints (NITG). The comparison is constrained by the phrase number K and the reordering distance L.
K=2 K=3 K=4 K=5 K=6 K=7 K=8 K=9 K=10
NLocal 2 3 5 8 13 21 34 55 89
L=2 NIBM(4) 2 3 5 8 13 21 34 55 89
NITG 2 3 5 8 13 21 34 55 89
NLocal 2 6 14 31 73 172 400 932 2177
L=3 NIBM(4) 2 6 14 31 73 172 400 932 2177
NITG 2 6 12 25 57 124 268 588 1285
NLocal 2 6 24 78 230 675 2069 6404 19708
L=4 NIBM(4) 2 6 24 78 230 675 2069 6404 19708
NITG 2 6 22 52 122 321 885 2304 5880
NLocal 2 6 24 120 504 1902 6902 25231 95401
L=5 NIBM(4) 2 6 24 96 330 1066 3451 11581 39264
NITG 2 6 22 90 236 602 1714 5269 16385
NLocal 2 6 24 120 720 3720 17304 76110 329462
L=6 NIBM(4) 2 6 24 96 384 1374 4718 16275 57749
NITG 2 6 22 90 394 1108 3014 9038 29618
We can see that given the same K (K ? 10) and
L (L ? 6), IBM constraints have less permutations
than local constraints, and ITG constraints have less
permutations than IBM constraints in general (only
one exception when K = L = 6). These obser-
vations indicate that ITG constraints can filter out
more unlikely permutations for a fixed reordering
distance, resulting in longer distance reordering ca-
pability.
Table 1 also tells us that the phrase number K
and the reordering distance L for any of the con-
straints cannot be too large for practical implemen-
tation. For instance, if L = 6 and K goes from 6 to
7, the order of magnitude of NLocal, NIBM(4) and
NITG increases from 2 to 3. Hence, phrases for per-
mutation should be selective to cover the most pos-
sible re-orderings. If long reordering distances are
allowed, unlikely permutations should be pruned so
that the memory consumption becomes manageable.
Reordering Sequence Distribution
So far we have discussed the issue that how to
generate permutations for the reordering model us-
ing reordering constraints. Another issue is how to
parameterize the reordering sequence distribution.
Both ITG constraints and other constraints assume
that all permutations are equally probable. However,
it makes sense to restrict those non-monotonic re-
orderings when performing the translation. This not
only helps the search of the most likely permutation,
but also guides the pruning of unlikely permutations.
P (rK1 |w?K1 , wJ1 ) = P (r1)
K
?
k=2
P (rk|rk?1, w?K1 )
= P (r1)
K
?
k=2
P (rk|rk?1) (4)
We make a first order Markov assumption over the
phrasal reordering model P (rK1 |w?K1 , wJ1 ) (see Eq.
(4)). The reordering sequence distribution is param-
eterized to assign decreasing likelihood to phrase re-
orderings {w?r1 , w?r2 , . . . , w?rK} that diverge from the
original word order (Och et al1999; Kumar et al
2005). Suppose w?rk = wl
?
l and w?rk?1 = w
q?
q , the
reordering sequence distribution is set as Eq. (5),
where p0 is a tuning factor. We normalize the proba-
bilities P (rk|rk?1) such that
?K
k?=1,k? 6=rk?1 P (rk =
k?|rk?1) = 1.
P (rk|rk?1) = p|l?q
??1|
0
P (r1 = k) =
1
K ; k ? {1, 2, ...,K}
(5)
770
Assume that we have a phrase sequence
{w?1, w?2, w?3}, Figure 2 shows the phrasal reordering
model implemented by a reordering WFST ?r under
the first order Markov assumption for this phrase se-
quence.
Figure 3(a2) gives one more example of ?r,
which reorders the phrase sequence {w1, w2 w3}
into {w2 w3, w1}6. Within the WFST paradigm, re-
ordering models under any of those constraints can
be integrated into the cross-lingual language model.
)(/~:~ 111 rPwwr )|(/
~:~ 1222 rrPwwr )|(/
~:~ 2333 rrPwwr
Figure 2: An example of reordering WFST ?r imple-
menting the phrasal reordering model under the first or-
der Markov assumption.
2.3 Phrase-to-Phrase Transduction Model
Once the phrase sequence of the Lw transcript
is reordered into the Lv transcript order, we use
the phrase-to-phrase transduction model specified in
Eq. (6) to perform the cross-language transduction.
Given sufficient parallel training data, the context-
dependent phrase-to-phrase transduction model can
be estimated using the GIATI method (Casacu-
berta and Vidal, 2004). However, for the trans-
lation task with scarce training data, the context-
dependent transduction probabilities may not be re-
liably estimated. Therefore, we assume that a phrase
v?k is generated independently by each phrase w?rk .
C(v?k, w?rk) is the number of times that phrase v?k is
aligned to w?rk in the parallel corpus. This model can
be implemented by a WFST Tvw which transduces
v?k to w?rk . Figure 3(a3) shows an example of Tvw
transducing v2 v3 to w2 w3.
P (v?K1 |rK1 , w?K1 , wJ1 ) = P (v?K1 |rK1 , w?K1 )
=
K
?
k=1
Pk(v?k|w?rk)
=
K
?
k=1
C(v?k, w?rk)
?
v?k
C(v?k, w?rk)
(6)
2.4 Reconstruction Model
Reconstruction model P (vI1 |v?K1 , rK1 , w?K1 , wJ1 ) oper-
ates in the opposite direction as the segmentation
6For simplicity, reordering sequence distributions are not
shown there.
model. It generates a word sequence vI1 from a
phrase sequence v?K1 . The reconstruction model can
be implemented by a WFST Rv. An example of
Rv is shown in Figure 3(a4), which reconstructs a
phrase v2 v3 into a word sequence {v2, v3}.
3 Speech Recognition with Cross-Lingual
Language Models
The translation model P (vI1 |wJ1 ) can be constructed
via WFST composition (denoted by ?) (Mohri,
2009) of all the component models as shown in Eq.
(7) and Figure 3, where T is the final composed
WFST that transduces vI1 to wJ1 .
T = Rv ? Tvw ? ?r ? Sw (7)
The cross-lingual language model Gcl is con-
structed through composition (see Eq. (8)) of
the translation model and a resource-rich language
model G.
Gcl = T ?G = Rv ? Tvw ? ?r ? Sw ?G (8)
As the way of integrating a resource-rich lan-
guage model G into ASR search space (Mohri et al
2008), we can integrate the cross-lingual language
model Gcl into ASR search space in a globally op-
timized way as well. The search space can be im-
plemented using a transducer ASR, which is for-
mulated with a unified WFST approach as shown
in Eq. (9). Here H transduces HMM states to
context-dependent phones. C represents a trans-
duction from context-dependent phones to context-
independent phones. L is a lexicon transducer which
maps context-independent phone sequences to word
strings restricted to the input symbols of the cross-
lingual language model transducer Gcl.
ASR = H ? C ? L ?Gcl (9)
Eq. (9) outputs the recognition result in a resource-
rich language. If recognition system requires recog-
nition outputs in a resource-poor language, then the
search space should be constructed as Eq. (10),
where ? is a projection (Mohri, 2009) operator
which projects the input label to the output label.
Before decoding, the recognition transducer ASR
can be optimized by a determinization operation
right after each composition.
ASR = H ? C ? L ? ?(Gcl) (10)
771




  
  
(a1) Segmentation WFST Sw (b1) Written-style language model G











0 1w 1 : w 1
2w 2 : w 2
3
w 2 _ w 3 : w 2 4
w 3 : w 3
- :w3
(a2) Reordering WFST ?r (b2) Sw ?G





		
		




 	








(a3) Phrase-to-phrase transduction WFST Tvw (b3) ?r ? Sw ?G
 






		
	




		
	





	



	





0
1w 1 : w 1
2v2_v3:w1
3
w 1 _ # 1 : w 1
4
v2_v3:w2
w 1 : w 2
v 2 _ v 3 _ # 1 : w 2
5w 2 : w 2
6
- :w3
w 3 : w 3
(a4) Reconstruction WFST Rv (b4) Tvw ? ?r ? Sw ?G
 



	













	







(b5) Rv ? Tvw ? ?r ? Sw ?G
Figure 3: Illustration of constructing a cross-lingual language model via WFSTs: a word sequence {w1, w2, w3}
represented by the Lw language model G (b1) is segmented into a phrase sequence {w1, w2 w3} (b2); {w1, w2 w3} is
reordered into {w2 w3, w1} (b3); phrase w2 w3 is transduced to v2 v3 (b4); phrase v2 v3 is reconstructed into a word
sequence {v2, v3} (b5). wk and vk represent wk and vk, respectively. ?-? refers to ? or null symbol. Auxiliary symbols
#1,#2, ? ? ? are used to make the WFST determinizable (Mohri, 2009) such that the transducer can be optimized by a
determinization (Mohri, 2009) operation which significantly reduces the search network size.
772
4 Experimental Setup
4.1 Corpus and Model Training
To investigate the performance of our proposed
cross-lingual language models, we have chosen
Cantonese as a resource-poor language and Man-
darin as a resource-rich language. We have col-
lected Cantonese parliamentary speech from the
Hong Kong Legislative Council. Currently we only
have 4152 parallel transcribed sentences containing
19.4 hours of speech. It is separated into three sets,
a training set (11.9 hours, 2700 sentences), a de-
velopment set (3.7 hours, 788 sentences), and an
evaluation set (3.8 hours, 664 sentences). The sen-
tences in the evaluation set are a bit longer than
those in the development set. The parallel transcrip-
tions of the training set constitute a parallel cor-
pus, which includes Cantonese transcription (man-
ual transcription) of 106k words and Mandarin tran-
scription (Hansard7 transcription) of 80k words. The
statistics of substitutions, insertions, deletions and
inversions identified in the parallel corpus are shown
in Table 2. Besides the parallel corpus, we have a
set of additional Mandarin transcriptions, which has
31M words.
Table 2: No. of substitutions, insertions, deletions and
inversions identified in the parallel corpus with different
segmentation order s.
Segmentation Order s = 2 s = 3 s = 4 s = 5
Substitutions 30921 22723 19011 17106
Insertions 4657 3820 3641 3295
Deletions 1365 1158 1066 1030
Inversions 3000 2876 2814 2779
Total 39943 30577 26532 24210
The training set is used for training an acous-
tic model (including H and C) using a Maximum
Likelihood criterion. It adopts 13 MFCC coeffi-
cients, together with 13 delta coefficients and 13 ac-
celeration coefficients as the acoustic features. The
acoustic model comprises 73 Hidden Markov Mod-
els (HMMs) to represent 70 Cantonese phonemes as
well as silence, short pause, and noise. During the
acoustic model training, tied-state cross-word tri-
phones are constructed by decision tree clustering.
7Hansard is a name of the printed transcripts of parliamen-
tary debates.
The parallel corpus is used for training the trans-
lation model T . Together with the parallel corpus,
the additional Mandarin transcriptions are used for
training an interpolated word-level trigram language
model G, where the lexicon size is about 28K. A
modified scheme of Kneser-Ney discounting is ap-
plied for the language model G with a back-off
threshold of 1 for unigram and 2 for bigram. The
cross-lingual language model Gcl can be obtained
by composition of T and G.
4.2 Decoding and Evaluation Method
Decoding of the speech recognition search space
ASR is performed by T 3 Decoder (Dixon et
al., 2009), which is a state-of-the-art WFST-based
LVCSR speech decoder. Decoding of ASR in Eq.
(9) gives Mandarin outputs. Decoding of ASR in
Eq. (10) gives Cantonese outputs.
In our experiments, we use the following evalua-
tion criteria:
WER (word error rate). The WER is computed
as the minimum number of substitution, insertion
and deletion operations that have to be performed
to convert the generated sentence into the reference
sentence (Zens et al2004). The WER relates the
speech recognition accuracy. The lower WER, the
better.
BLEU (bilingual evaluation understudy) score.
The BLEU score measures the precision of n-grams
(unigrams, bigrams, trigrams and fourgrams) with
respect to a reference translation with a penalty for
too short sentences (Papineni et al2002). The
BLEU score reflects the translation accuracy. The
larger BLEU score, the better.
We perform WER evaluation of decoding out-
puts of Eq. (10) and BLEU score evaluation of
decoding outputs of Eq. (9) using the evaluation
set. The WER evaluation is on the Cantonese output
against the Cantonese reference transcription (man-
ual transcription). The BLEU score evaluation is on
the Mandarin output against the Mandarin reference
transcription (Hansard transcription).
4.3 Parameter Settings
The performance of our proposed cross-lingual lan-
guage models is sensitive to many parameters.
Firstly, segmentation order s affects phrase extrac-
tion. The optimal value depends on the language
773
Table 3: WER and BLEU score for decoding results of H ? C ? L ?G, H ? C ? L ? ?(Gcl) without reordering, and
H ? C ? L ? ?(Gcl) with reordering under various constraints.
Models H ? C ? L ?G
H ? C ? L ? ?(Gcl) H ? C ? L ? ?(Gcl)
Gcl = T3 ?G Gcl = T3 ?G,T3 = Rv ? Tvw ??r ? Sw
T3 = Rv ? Tvw ? Sw Local Constraints IBM Constraints ITG Constraints
WER(%) 29.85 27.05 26.35 26.20 26.13
BLEU N/A 29.23 32.29 32.81 33.12
pair and the size of corpus. Secondly, p0 in the
first order Markov assumption affects the decoding
results. Thirdly, the number of reordering permu-
tations or paths are formidable when the reorder-
ing distance L is long as suggested by Table 1.
Therefore, we apply histogram pruning to reorder-
ing paths, which only maintains top N most likely
ones. The development set is used for tuning param-
eters p0 and N .
5 Experimental Results
The evaluation results of the proposed cross-lingual
language models Gcl with reordering under various
constraints are presented in Table 3, where Gcl =
Ts?G = T3?G.8 In general, reordering has a signif-
icant effect on enhancing the performance of recog-
nition and translation in the sense of WER reduc-
tion and BLEU improvement. Compared with the
cross-lingual language model without reordering,
the cross-lingual language model with reordering
under local constraints gives 0.70% absolute WER
reduction and 3.06 absolute BLEU improvement.
The cross-lingual language model with reordering
under IBM constraints gives 0.85% absolute WER
reduction and 3.58 absolute BLEU improvement.
The cross-lingual language model with reordering
under ITG constraints yields the best performance,
with 0.92% absolute WER reduction and 3.89 abso-
lute BLEU improvement. All WER improvements
pointed out here are statistically significant at 99%
confidence according to a two-proportional z-test,
and all BLEU improvements are statistically signifi-
cant at 95% confidence according to a paired student
t-test using bootstrap resampling.
8We have chosen segmentation order s = 3 because it works
the best in our system.
6 Conclusions
We have proposed cross-lingual language model-
ing with phrase-level syntactic reordering for low-
resource speech recognition. The cross-lingual lan-
guage modeling enriches a resource-poor language
model by leveraging the language model from a
closely related resource-rich language. It provides
an effective method to solve the low-resource lan-
guage modeling challenge by using a large amount
of resource-rich language (e.g. Mandarin) data
and a small amount of resource-poor language (e.g.
Cantonese) data, as well as some parallel data of
resource-poor and resource-rich languages. With
a cross-lingual language model, our ASR system
can decode speech into transcriptions, either in a
resource-poor language or a resource-rich language,
using a single WFST-based speech decoder.
We have presented a first end-to-end WFST
source to target language transcription and transla-
tion system with syntactic reordering and global op-
timization. Our work is the first to use ITG con-
straints for the syntactic reordering in such an in-
tegrated system. We also did comparative study
of ITG constraints, IBM constraints and local con-
straints in the reordering model, for completeness.
We have also presented the determinizable design of
each transducer for composing a cross-lingual lan-
guage model such that we can optimize the search
network by determinization. This is crucially im-
portant to successfully build a practical integrated
system, and, of course, the work is extremely chal-
lenging.
Experiments on Cantonese recognition and Can-
tonese to Mandarin translation tasks have shown that
our proposed cross-lingual language model substan-
tially improves the performance of the recognition
and translation. The best system gives 12.5% rel-
ative WER reduction in Cantonese (resource-poor
774
language) transcriptions over the system using inter-
polation. The best reordering model gives 3.4% rela-
tive WER reduction and 13.3% relative BLEU score
improvement in Mandarin (resource-rich language)
transcriptions over the system without reordering.
The improvements have been found to be statisti-
cally significant.
Even though the objective of our work is for
speech recognition, our proposed cross-lingual lan-
guage modeling can be easily applied to speech
translation of other language pairs for efficient di-
rect decoding from source speech to target text.
7 Acknowledgments
This work is partially supported by ITS/189/09 and
CERG#612211. The authors would like to thank Dr.
Tasuku Oonishi for providing access to the T 3 de-
coder, and thank Prof. Sadaoki Furui and his team
for useful discussions. Thanks should go to Yue Yu
and Percy Cheung for collecting the Cantonese and
Mandarin parallel data. Thanks also go to Ricky
Chan for training the Cantonese acoustic model and
Dr. Markus Saers for helping on training the GIZA
word-to-word alignment models.
References
Y. Akita and T. Kawahara. 2006. Efficient estimation
of language model statistics of spontaneous speech via
statistical transformation model. In Proceedings of the
IEEE International Conference on Acoustics, Speech
and Signal Processing, volume 1, pages 1049?1052.
E. Barcucci, A. Del Lungo, E. Pergola, and R. Pinzani.
2000. Permutations avoiding an increasing number
of length-increasing forbidden subsequences. Dis-
crete Mathematics and Theoretical Computer Science,
4(1):31?44.
J.R. Bellegarda. 2004. Statistical language model adap-
tation: review and perspectives. Speech communica-
tion, 42(1):93?108.
A.L. Berger, P.F. Brown, S.A. Della Pietra, V.J.
Della Pietra, A.S. Kehler, and R.L. Mercer. 1996.
Language translation apparatus and method us-
ing context-based translation models. US Patent
5,510,981.
F. Casacuberta and E. Vidal. 2004. Machine translation
with inferred stochastic finite-state transducers. Com-
putational Linguistics, 30(2):205?225.
F. Casacuberta, H. Ney, F.J. Och, et al004. Some ap-
proaches to statistical and finite-state speech-to-speech
translation. Computer Speech & Language, 18(1):25?
47.
P. Clarkson and R. Rosenfeld. 1997. Statistical lan-
guage modeling using the cmu-cambridge toolkit. In
5th European Conference on Speech Communication
and Technology.
P.R. Dixon, T. Oonishi, K. Iwano, and S. Furui. 2009.
Recent development of wfst-based speech recognition
decoder. In Proceedings of 2009 APSIPA Annual Sum-
mit and Conference, pages 138?147, Sapporo, Japan.
M. Dreyer, K. Hall, and S. Khudanpur. 2007. Compar-
ing reordering constraints for smt using efficient bleu
oracle computation. In Proceedings of SSST, NAACL-
HLT 2007 / AMTA Workshop on Syntax and Structure
in Statistical Translation, pages 103?110, Rochester,
New York.
A. Ehrenfeucht, T. Harju, P. Ten Pas, and G. Rozenberg.
1998. Permutations, parenthesis words, and schro?der
numbers. Discrete mathematics, 190(1):259?264.
V. Geethakumary. 2002. A contrastive analysis of hindi
and malayalam. Language in India.
R.G. Gordon, B.F. Grimes, and Summer Institute of Lin-
guistics. 2005. Ethnologue: Languages of the world,
volume 15. SIL International, Dallas TX, USA.
T. Hori, D. Willett, and Y. Minami. 2003. Lan-
guage model adaptation using wfst-based speaking-
style translation. In Proceedings of the IEEE Inter-
national Conference on Acoustics, Speech and Signal
Processing, volume 1, pages 228?231.
A.T. Jensson, T. Oonishi, K. Iwano, and S. Furui. 2009.
Development of a wfst based speech recognition sys-
tem for a resource deficient language using machine
translation. In Proceedings of APSIPA ASC 2009:
Asia-Pacific Signal and Information Processing Asso-
ciation, 2009 Annual Summit and Conference, pages
50?56.
S. Kanthak, D. Vilar, E. Matusov, R. Zens, and H. Ney.
2005. Novel reordering approaches in phrase-based
statistical machine translation. In Proceedings of the
ACL Workshop on Building and Using Parallel Texts,
pages 167?174. Association for Computational Lin-
guistics.
S. Khudanpur and W. Kim. 2002. Using cross-language
cues for story-specific language modeling. In 7th In-
ternational Conference on Spoken Language Process-
ing.
W. Kim and S. Khudanpur. 2003. Cross-lingual lexical
triggers in statistical language modeling. In Proceed-
ings of the 2003 conference on Empirical methods in
natural language processing, pages 17?24. Associa-
tion for Computational Linguistics.
W. Kim and S. Khudanpur. 2004. Cross-lingual latent
semantic analysis for language modeling. In Proceed-
775
ings of the IEEE International Conference on Acous-
tics, Speech and Signal Processing, volume 1, pages
I257?I260. IEEE.
T. Kl?ve. 2009. Generating functions for the number
of permutations with limited displacement. The Elec-
tronic Journal of Combinatorics, 16(R104).
K. Knight. 1999. Decoding complexity in word-
replacement translation models. Computational Lin-
guistics, 25(4):607?615.
S. Kumar and W. Byrne. 2005. Local phrase reorder-
ing models for statistical machine translation. In
Proceedings of Human Language Technology Confer-
ence / Conference on Empirical Methods in Natural
Language Processing (HLT/EMNLP), pages 161?168,
Vancouver, Canada.
S. Kumar, Y. Deng, and W. Byrne. 2005. A weighted
finite state transducer translation template model for
statistical machine translation. Natural Language En-
gineering, 12(1):35?75.
J. Lee. 2011. Toward a parallel corpus of spoken can-
tonese and written chinese. In Proceedings of the 5th
International Joint Conference on Natural Language
Processing, pages 1462?1466, Chiang Mai, Thailand.
L. Mathias and W. Byrne. 2006. Statistical phrase-based
speech translation. In Proceedings of the IEEE Inter-
national Conference on Acoustics, Speech and Signal
Processing, volume 1, pages 561?564.
E. Matusov, S. Kanthak, and H. Ney. 2006. Integrating
speech recognition and machine translation: Where do
we stand? In Proceedings of the IEEE International
Conference on Acoustics, Speech and Signal Process-
ing, volume 5, pages V1217?V1220. IEEE.
M. Mohri, F. C. N. Pereira, and M. Riley. 2008.
Speech recognition with weighted finite-state trans-
ducers. Handbook on Speech Processing and Speech
Communication, Part E: Speech Recognition.
M. Mohri. 2009. Weighted automata algorithms. Hand-
book of Weighted Automata, pages 213?254.
P. Nakov and H.T. Ng. 2009. Improved statistical ma-
chine translation for resource-poor languages using re-
lated resource-rich languages. In Proceedings of the
2009 Conference on Empirical Methods in Natural
Language Processing, volume 3, pages 1358?1367.
Association for Computational Linguistics.
G. Neubig, Y. Akita, S. Mori, and T. Kawahara. 2010.
Improved statistical models for smt-based speaking
style transformation. In Proceedings of the IEEE In-
ternational Conference on Acoustics, Speech and Sig-
nal Processing, pages 5206?5209.
F.J. Och and H. Ney. 2003. A systematic comparison of
various statistical alignment models. Computational
Linguistics, 29(1):19?51.
F.J. Och, C. Tillmann, and H. Ney. 1999. Improved
alignment models for statistical machine translation.
In Proceedings of the Joint SIGDAT Conf. on EMNLP
and VLC, pages 20?28, College Park, MD, USA.
K. Papineni, S. Roukos, T. Ward, and W.J. Zhu. 2002.
Bleu: a method for automatic evaluation of machine
translation. In Proceedings of the 40th annual meet-
ing on association for computational linguistics, pages
311?318. Association for Computational Linguistics.
G. Saon and M. Picheny. 2007. Lattice-based viterbi
decoding techniques for speech translation. In Au-
tomatic Speech Recognition & Understanding, 2007.
ASRU. IEEE Workshop on, pages 386?389. IEEE.
A. Stolcke. 2002. Srilm-an extensible language model-
ing toolkit. In 7th International Conference on Spoken
Language Processing.
D. Wu. 1997. Stochastic inversion transduction gram-
mars and bilingual parsing of parallel corpora. Com-
putational Linguistics, 23(3):377?403.
R. Zens and H. Ney. 2003. A comparative study on re-
ordering constraints in statistical machine translation.
In Proceedings of the 41st Annual Meeting on Associ-
ation for Computational Linguistics, pages 144?151,
Sapporo, Japan. Association for Computational Lin-
guistics.
R. Zens, H. Ney, T. Watanabe, and E. Sumita. 2004.
Reordering constraints for phrase-based statistical ma-
chine translation. In Proceedings of the 20th interna-
tional conference on Computational Linguistics, pages
205?211, Geneva, Switzerland. Association for Com-
putational Linguistics.
B. Zhou, S.F. Chen, and Y. Gao. 2005. Constrained
phrase-based translation using weighted finite-state
transducers. In Proceedings of the IEEE International
Conference on Acoustics, Speech and Signal Process-
ing, volume 1, pages 1017?1020.
B. Zhou, S. F. Chen, and Y. Gao. 2006. Folsom: A fast
and memory-efficient phrase-based approach to statis-
tical machine translation. In Spoken Language Tech-
nology Workshop, pages 226?229. IEEE.
776
Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 907?916,
October 25-29, 2014, Doha, Qatar.
c?2014 Association for Computational Linguistics
Language Modeling with Functional Head Constraint for Code Switching
Speech Recognition
Ying Li and Pascale Fung
Human Language Technology Center
Department of Electronic and Computer Engineering
The Hong Kong University of Science and Technology
eewing@ee.ust.hk, pascale@ece.ust.hk
Abstract
In this paper, we propose novel struc-
tured language modeling methods for code
mixing speech recognition by incorporat-
ing a well-known syntactic constraint for
switching code, namely the Functional
Head Constraint (FHC). Code mixing data
is not abundantly available for training
language models. Our proposed meth-
ods successfully alleviate this core prob-
lem for code mixing speech recognition
by using bilingual data to train a struc-
tured language model with syntactic con-
straint. Linguists and bilingual speakers
found that code switch do not happen be-
tween the functional head and its comple-
ments. We propose to learn the code mix-
ing language model from bilingual data
with this constraint in a weighted finite
state transducer (WFST) framework. The
constrained code switch language model is
obtained by first expanding the search net-
work with a translation model, and then
using parsing to restrict paths to those per-
missible under the constraint. We im-
plement and compare two approaches -
lattice parsing enables a sequential cou-
pling whereas partial parsing enables a
tight coupling between parsing and fil-
tering. We tested our system on a lec-
ture speech dataset with 16% embedded
second language, and on a lunch conver-
sation dataset with 20% embedded lan-
guage. Our language models with lattice
parsing and partial parsing reduce word
error rates from a baseline mixed lan-
guage model by 3.8% and 3.9% in terms
of word error rate relatively on the aver-
age on the first and second tasks respec-
tively. It outperforms the interpolated lan-
guage model by 3.7% and 5.6% in terms of
word error rate relatively, and outperforms
the adapted language model by 2.6% and
4.6% relatively. Our proposed approach
avoids making early decisions on code-
switch boundaries and is therefore more
robust. We address the code switch data
scarcity challenge by using bilingual data
with syntactic structure.
1 Introduction
In multilingual communities, it is common for
people to mix two or more languages in their
speech. A single sentence spoken by bilingual
speakers often contains the main, matrix language
and an embedded second language. This type
of linguistic phenomenon is called ?code switch-
ing? by linguists. It is increasingly important for
automatic speech recognition (ASR) systems to
recognize code switching speech as they exist in
scenarios such as meeting and interview speech,
lecture speech, and conversational speech. Code
switching is common among bilingual speakers of
Spanish-English, Hindi-English, Chinese-English,
and Arabic-English, among others. In China,
lectures, meetings and conversations with techni-
cal contents are frequently peppered with English
terms even though the general population is not
considered bilingual in Chinese and English. Un-
like the thousands and tens of thousands of hours
of monolingual data available to train, for exam-
ple, voice search engines, transcribed code switch
data necessary for training language models is
hard to come by. Code switch language modeling
is therefore an even harder problem than acoustic
modeling.
One approach for code switch speech recogni-
tion is to explicitly recognizing the code switch
points by language identification first using pho-
netic or acoustic information, before applying
speech recognizers for the matrix and embed-
ded languages (Chan et. al, 2004; Shia et. al,
907
2004; Lyu and Lyu, 2008). This approach is ex-
tremely error-prone as language identification at
each frame of the speech is necessary and any er-
ror will be propagated in the second speech recog-
nition stage leading to fatal and irrecoverable er-
rors.
Meanwhile, there are two general approaches to
solve the problem of lack of training data for lan-
guage modeling. In a first approach, two language
models are trained from both the matrix and em-
bedded language separately and then interpolated
together (Vu et. al, 2012; Chan et. al, 2006). How-
ever, an interpolated language model effectively
allows code switch at all word boundaries without
much of a constraint. Another approach is to adapt
the matrix language language model with a small
amount of code switch data (Tsai et. al, 2010; Yeh
et. al, 2010; Bhuvanagiri and Kopparapu, 2010;
Cao et. al, 2010). The effectiveness of adapta-
tion is also limited as positions of code switch-
ing points are not generalizable from the limited
data. Significant progress in speech recognition
has been made by using deep neural networks for
acoustic modeling and language model. However,
improvement thus gained on code switch speech
recognition remains very small. Again, we pro-
pose that syntactic constraints of the code switch-
ing phenomenon can help improve performance
and model accuracy. Previous work of using part-
of-speech tags (Zhang et. al, 2008; Vu et al 2012)
and our previous work using syntactic constraints
(Li and Fung, 2012, 2013) have made progress
in this area. Part-of-speech is relatively weak in
predicting code switching points. It is generally
accepted by linguists that code switching follows
the so-called Functional Head Constraint, where
words on the nodes of a syntactic sub tree must
follow the language of that of the headword. If the
headword is in the matrix language then none of
its complements can switch to the embedded lan-
guage.
In this work, we propose two ways to incorpo-
rate the Functional Head Constraint into speech
recognition and compare them. We suggest two
approaches of introducing syntactic constraints
into the speech recognition system. One is to ap-
ply the knowledge sources in a sequential order.
The acoustic model and a monolingual language
model are used first to produce an intermediate
lattice, then a second pass choose the best result
using the syntactic constraints. Another approach
uses tight coupling. We propose using structured
language model (Chelba and Jelinek, 2000) to
build the syntactic structure incrementally.
Following our previous work, we suggest in-
corporating the acoustic model, the monolingual
language model and a translation model into a
WFST framework. Using a translation model al-
lows us to learn what happens when a language
switches to another with context information. We
will motivate and describe this WFST framework
for code switching speech recognition in the next
section. The Functional Head Constraint is de-
scribed in Section 3. The proposed code switch
language models and speech recognition coupling
is described in Section 4. Experimental setup and
results are presented in Section 5. Finally we con-
clude in Section 6.
2 Code Switch Language Modeling in a
WFST Framework
As code switch text data is scarce, we do not have
enough data to train the language model for code
switch speech recognition. We propose instead to
incorporate language model trained in the matrix
language with a translation model to obtain a code
switch language model. We propose to integrate a
bilingual acoustic model (Li et. al, 2011) and the
code switch language model in a weighted finite
state transducer framework as follows.
Suppose X denotes the observed code switch
speech vector, w
J
1
denotes a word sequence in the
matrix language, the hypothesis transcript v
I
1
is as
follows:
v?
I
1
= argmax
v
I
1
P (v
I
1
|X)
= argmax
v
I
1
P (X|v
I
1
)P (v
I
1
)
= argmax
v
I
1
P (X|v
I
1
)
?
w
J
1
P (v
I
1
|w
J
1
)P (w
J
1
)
?
=
argmax
v
I
1
P (X|v
I
1
)P (v
I
1
|w
J
1
)P (w
J
1
) (1)
where P (X|v
I
1
) is the acoustic model and P (v
I
1
)
is the language model in the mixed language.
Our code switch language model is obtained
from a translation model P (v
I
1
|w
J
1
) from the ma-
trix language to the mixed language, and the lan-
guage model in the matrix language P (w
J
1
).
Instead of word-to-word translation, the trans-
duction of the context dependent lexicon trans-
fer is constrained by previous words. Assume the
transduction depends on the previous n words:
908
P (v
I
1
|w
J
1
) =
I
?
i=1
P (v
i
|v
i?1
1
, w
i
1
)
?
=
I
?
i=1
P (v
i?1
i?n+1
|w
i
i?n+1
)
=
I
?
i=1
P (v
i
, w
i
|v
i?1
i?n+1
, w
i?1
i?n+1
)
P (w
i
|v
i?1
i?n+1
, w
i?1
i?n+1
)
=
I
?
i=1
P (v
i
, w
i
|v
i?1
i?n+1
, w
i?1
i?n+1
)
P (w
i
|
?
v
i
v
i?1
i?n+1
, w
i?1
i?n+1
)
(2)
There are C-level and H-level search networks
in the WFST framework. The C-level search net-
work is composed of the universal phone model
P , the context model C, the lexicon L, and the
grammar G
N = P ? C ? L ?G (3)
The H-level search network is composed of the
state model H , the phoneme model P , the context
model C, the lexicon L, and the grammar G
N = H ? P ? C ? L ?G (4)
The C-level requires less memory then the H-level
search network. We propose to use a weighted fi-
nite state transducer framework incorporating the
bilingual acoustic model P , the context model C,
the lexicon L, and the code switching language
models G
CS
into a C-level search network for
mixed language speech recognition. The output
of the recognition result is in the mixed language
after projection pi(G
CS
).
N = P ? C ? L ? pi(G
CS
) (5)
The WFST implementation to obtain the code
switch language model G
CS
is as follows:
G
cs
= T ?G
(6)
where T is the translation model
P (v?
L
1
|w
J
1
) =
L
?
l=1
P
l
(v?
l
|w
l
) (7)
P
l
(v?
l
|w
l
) is the probability ofw
l
translated into v?
l
.
In order to make use of the text data in the ma-
trix language to recognize speech in the mixed lan-
guage, the translation model P (v
I
1
|w
J
1
) transduce
the language model in the matrix language to the
mixed language.
P (v
I
1
|w
J
1
) =
?
v?
L
1
,c
L
1
,r
K
1
,w?
K
1
P (w?
K
1
|w
J
1
)
?P (r
K
1
|w?
K
1
, w
J
1
)
?P (c
L
1
, r
K
1
, w?
K
1
, w
J
1
)
?P (v?
K
1
|c
L
1
, r
K
1
, w?
K
1
, w
J
1
)
?P (v
I
1
|v?
K
1
, r
K
1
, w?
K
1
, w
J
1
) (8)
where P (w?
K
1
|w
J
1
) is the word-to-phrase segmen-
tation model, P (r
K
1
|w?
K
1
, w
J
1
) is the phrasal re-
ordering model, P (c
L
1
, r
K
1
, w?
K
1
, w
J
1
) is the chunk
segmentation model, P (v?
K
1
|c
L
1
, r
K
1
, w?
K
1
, w
J
1
)
is the chunk-to-chunk transduction model,
P (v
I
1
|v?
K
1
, r
K
1
, w?
K
1
, w
J
1
) is the chunk-to-word
reconstruction model.
The word-to-phrase segmentation model ex-
tracts a table of phrases {v?
1
, v?
2
, ..., v?
K
} for
the transcript in the embedded language and
{w?
1
, w?
2
, ..., w?
K
} for the transcript in the ma-
trix language based on word-to-word alignments
trained in both directions with GIZA++ (Och and
Ney, 2003). The chunk segmentation model per-
forms the segmentation of a phrase sequence w?
K
1
into L phrases {c
1
, c
2
, ..., c
L
} using a segmenta-
tion weighted finite-state transducer. Assumes that
a chunk c
l
is code-switched to the embedded lan-
guage independently by each chunk, the chunk-
to-chunk transduction model is the probability of
a chunk to be code switched to the embedded lan-
guage trained on parallel data. The reconstruction
model generates word sequence from chunk se-
quences and operates in the opposite direction to
the segmentation model.
3 Functional Head Constraint
Many linguistics (Abney 1986; Belazi et. al, 1994;
Bhatt 1994) have discovered the so-called Func-
tional Head Constraint in code switching. They
have found that code switches between a func-
tional head (a complementizer, a determiner, an
inflection, etc.) and its complement (sentence,
noun-phrase, verb-phrase) do not happen in natu-
ral speech. In addition, the Functional Head Con-
straint is language independent.
In this work, we propose to investigate and
incorporate the Functional Head Constraint into
code switching language modeling in a WFST
framework. Figure 1 shows one of the Functional
Head Constraint examples. Functional heads are
909
the roots of the sub trees and complements are part
of the sub trees. Actual words are the leaf nodes.
According to the Functional Head Constraint, the
leave nodes of a sub tree must be in either the
matrix language or embedded language, following
the language of the functional head. For instance,
the third word ???/something? is the head of
the constituents ???/very ???/important ?
?/something?. These three constituent words
cannot be switched. Thus, it is not permissible
to code switch in the constituent. More precisely,
the language of the constituent is constrained to be
the same as the language of the headword. In the
following sections, we describe the integration of
the Functional Head Constraint and the language
model.
We have found this constraint to be empirically
sound as we look into our collected code mixing
speech and language data. The only violation of
the constraint comes from rare cases of borrowed
words such as brand names with no translation in
the local, matrix language. Borrowed words are
used even by monolingual speakers so they are in
general part of the matrix language lexicon and
require little, if any, special treatment in speech
recognition.
In the following sections, we describe the inte-
gration of Functional Head Constraint and the lan-
guage model.
4 Code Switching Language Modeling
with Functional Head Constraint
We propose two approaches of language model-
ing with Functional Head Constraint: 1) lattice-
parsing and sequential-coupling (Chapplerler et.
al, 1999); 2) partial-parsing and tight-coupling
(Chapplerler et. al, 1999). The two approaches
will be described in the followed sections.
4.1 Sequential-coupling by Lattice-based
Parsing
In this first approach, the acoustic models, the
code switch language model and the syntactic con-
straint are incorporated in a sequential order to
progressively constrain the search. The acoustic
models and the matrix language model are used
first to produce an intermediate output. The in-
termediate output is a lattice in which word se-
quences are compactly presented. Lattice-based
parsing is used to expand the word lattice gener-
ated from the first decoding step according to the
Functional Head Constraint.
We have reasons to use word lattice instead
of N-best hypothesis. The number of hypothesis
of word lattice is larger than N-best hypothesis.
Moreover, different kinds of errors correspond to
the language model would be observed if N-best
list is extracted after the first decoding step. The
second pass run over the N-best list will prevent
the language model with Functional Head Con-
straint from correcting the errors. In order to ob-
tain a computational feasible number of hypothe-
ses without bias to the language model in the first
decoding step, word lattice is used as the interme-
diate output of the first decoding step.
A Probabilistic Context-Free Grammar (PCFG)
parser is trained on Penn Treebank data. The
PCFG parser is generalized to take the lattice gen-
erated by the recognizer as the input. Figure 2 il-
lustrates a word lattice which is a compact repre-
sentation of the hypothesis transcriptions of a an
input sentence. All the nodes of the word-lattice
are ordered by increasing depth.
A CYK table is obtained by associating the arcs
with their start and end states in the lattice instead
of their sentence position and initialized all the
cells in the table corresponding to the arcs (Chap-
plerler et. al, 1999). Each cell C
k,j
of the ta-
ble is filled by a n-tuple of the non-terminal A,
the length k and the starting position of the word
sequence w
j
...w
j+k
if there exists a PCFG rule
A? w
j
...w
j+k
, where A is a non-terminal which
parse sequences of words w
j
...w
j+k
. In order to
allow all hypothesis transcriptions of word lattice
to be taken into account, multiple word sequences
of the same length and starting point are initialized
in the same cell. Figure 2 mapped the word lattice
of the example to the table, where the starting node
label of the arc is the column index and the length
of the arc is the row index.
The sequential-coupling by lattice-parsing con-
sists of the standard cell-filling and the self-filling
steps. First, the cells C
k,j
and C
i?k,j+k
are com-
bined to produce a new interpretation for cell C
i,j
. In order to handle the unary context-free produc-
tion A ? B and update the cells after the stan-
dard cell-filling, a n-tuple of A, i and j is added
for each n-tuple of the non-terminal B, the length
i and the start j in the cell C
i,j
. The parse trees
extracted are associated with the input lattice from
the table starting from the non-terminal label of
the top cell. After the parse tree is obtained, we re-
910
??	 ?
DT	 ?
this 
??	 ?
NN	 ?
theory 
????	 ?
NN	 ?
EM 
?	 ?
VC	 ?
is 
???	 ?
JJ	 ?
important 
??
AD	 ?
very 
??	 ?
NP	 ?
theory 
??	 ?
NP	 ?
this 
?	 ?
VP	 ?
is ?	 ?
VP	 ?
is 
Hypotheses:*
*EM*	.*

 EM*theory.*
 this*EM*theory.*
 is*this*EM*theory.*
 something*is*this*EM*theory.*(not*permissible)*
.*
.*
.*
??	 ?
NN	 ?
something 
???	 ?
JJ	 ?
important 
??	 ?
NN	 ?
something 
Figure 1: A Functional Head Constraint example.
0	 ? 1	 ? 2	 ? 6	 ? 7	 ? 8	 ?
??/very	 ?
?/very	 ? ???/important	 ?
???/	 ?
important	 ?
??/	 ?
something	 ? ?/is	 ?
?/is	 ?
??/	 ?
conclude	 ? ??/	 ?
this	 ?
????/	 ?
EM	 ?
??/	 ?
theory	 ?
4	 ?3	 ? 5	 ?
Figure 2: An example word lattice in the matrix language.
2	 ? 3	 ? 5	 ?4	 ? 6	 ? 7	 ?1	 ?
! ! ! ! ! ! ! !
! ???
"#$%&'(
)*(!
! ?"#+! ! ! ! !
??
",-'.!
?!
???
"#$%&'(
)*(!
??
"+&$-(
/#*0!
?"#+! ??
"#*1234
-!
??
"(/#+!
???
?"56!
??
"(/-&'.!
!
Figure 3: The mapping of the example word lattice to the table.
911
cursively enumerate all its subtrees. Each subtree
is able to code-switch to the embedded language
with a translation probability P
l
(v?
l
|w
l
).
The lattice parsing operation consists of the an
encoding of a given word sequence along with
a parse tree (W,T ) and a sequence of elemen-
tary model actions. In order to obtain a correct
probability assignment P (W,T ) one simply as-
sign proper conditional probabilities to each tran-
sition in the weighted finite states.
The probability of a parse T of a word sequence
WP (W,T ) can be calculated as the product of the
probabilities of the subtrees.
P (W,T ) =
n+1
?
k=1
[P (wk|W
k?1
T
k?1
) (9)
Where W
k
= w
0
...w
k
is the first k words in the
sentence, and (W
k
, T
k
) is the word-and-parse k-
prefix. The probability of the n-tuple of the non-
terminal A, the length i and the starting position j
is the probability of the subtree corresponding to
A parsing throughout the sequence w
j
...w
j+i?1
.
The probability of the partial parsing is the product
of probabilities of the subtree parses it is made of.
The probability of an n-tuple is the maximum over
the probabilities of probable parsing path.
The N most probable parses are obtained during
the lattice-parsing.
The probability of a sentence is computed by
adding on the probability of each new context-free
rule in the sentences.
4.2 Tight-coupling by Incremental Parsing
To integrate the acoustic models, language model
and the syntactic constraint in time synchronous
decoding, an incremental operation is used in this
approach. The final word-level probability as-
signed by our model is calculated using the acous-
tic models, the matrix language model, the struc-
tured language model and the translation model.
The structured language model uses probabilistic
parameterization of a shift-reduce parse (Chelba
and Jelinek, 2000). The tight-coupled language
model consists of three transducers, the word pre-
dictor, the tagger and the constructor. As shown
in Figure 3, W
k
= w0...wk is the first k words of
the sentence, T
k
contains only those binary sub-
trees whose leaves are completely included inW
k
,
excluding w
0
=<s>. Single words along with
their POS tag can be regarded as root-only trees.
The exposed head h
k
is a pair of the headword
of the constituent W
k
and the non-terminal label.
The exposed head of single words are pairs of the
words and their POS tags.
Given the word-and-parse (k-1)-prefix
W
k?1
T
k?1
, the new word w
k
is predicted by
the word-predictor P (w
k
|W
k?1
T
k?1
). Taking
the word-and-parse k ? 1-prefix and the next
word as input, the tagger P (t
k
|w
k
,W
k?1
T
k?1
)
gives the POS tag t
k
of the word w
k
. Constructor
P (p
k
i
|W
k
T
k
) assigns a non-terminal label to the
constituent W
k+1
. The headword of the newly
built constituent is inherited from either the
headword of the constituent W
k
or the next word
w
k+1
.
P (w
k
|W
k?1
T
k?1
)
= P (w
k
|[W
k?1
T
k?1
])
= P (w
k
|h
0
, h
?1
) (10)
P (t
k
|w
k
,W
k?1
T
k?1
)
= P (t
k
|w
k
, [W
k?1
T
k?1
])
= P (t
k
|w
k
, h
0
.tag, h
?1
.tag) (11)
P (p
k
i
|W
k
T
k
)
= P (p
k
i
|[W
k
T
k
])
= P (p
k
i
|h
0
, h
1
) (12)
The probability of a parse tree T P (W,T ) of a
word sequence W and a complete parse T can be
calculated as:
P (W,T ) =
n+1
?
k=1
[P (w
k
|W
k?1
T
k?1
)
P (t
k
|W
k?1
T
k?1
, w
k
)
P (T
k
|W
k?1
T
k?1
, w
k
, t
k
)](13)
P (T
k
k?1
|W
k?1
T
k?1
, w
k
, t
k
)
=
N
k
?
i=1
P (p
k
|W
k?1
T
k?1
, w
k
, t
k
, p
k
1
...p
k
i?1
)
(14)
Where w
k
is the word predicted by the word-
predictor, t
k
is the POS tag of the word w
k
pre-
dicted by the tagger, W
k?1
T
k?1
is the word-parse
(k - 1)-prefix, T
k
k?1
is the incremental parse struc-
ture that generates T
k
= T
k?1
||T
k
k?1
when at-
tached to T
k?1
; it is the parse structure built on
top of T
k?1
and the newly predicted word wk; the
|| notation stands for concatenation; N
k?1
is the
number of operations the constructor executes at
912
Figure 4: A word-and-parse example.
position k of the input string before passing con-
trol to the word-predictor (the N
k
th operation at
position k is the null transition); N
k
is a function
of T ; p
k
i
denotes the i th constructor action carried
out at position k in the word string.
The probability models of word-predictor, tag-
ger and constructor are initialized from the Upenn
Treebank with headword percolation and bina-
rization. The headwords are percolated using a
context-free approach based on rules of predict-
ing the position of the headword of the constituent.
The approach consists of three steps. First a parse
tree is decomposed to phrase constituents. Then
the headword position is identified and filled in
with the actual word percolated up from the leaves
of the tree recursively.
Instead of the UPenn Treebank-style, we use a
more convenient binary branching tree. The parse
trees are binarized using a rule-based approach.
The probability models of the word-predictor,
tagger and constructor are trained in a maximiza-
tion likelihood manner. The possible POS tag as-
signments, binary branching parse, non-terminal
labels and the head-word annotation for a given
sentence are hidden. We re-estimate them using
EM algorithm.
Instead of generating only the complete parse,
all parses for all the subsequences of the sen-
tence are produced. The headwords of the subtrees
are code switched to the embedded language with
a translation probability P
l
(v?
l
|w
l
) as well as the
leaves.
4.3 Decoding by Translation
Using either lattice parsing or partial parsing, a
two-pass decoding is needed to recognize code
switch speech. A computationally feasible first
pass generates an intermediate result so that the
language model with Functional Head constraint
can be used in the second pass. The first decoding
pass composes of the transducer of the universal
phoneme model P , the transducerC from context-
dependent phones to context-independent phones,
the lexicon transducer L which maps context-
independent phone sequences to word strings and
the transducer of the language model G. A T3 de-
coder is used in the first pass.
ASR
1
= P ? C ? L ?G (15)
Instead of N-best list, word lattice is used as the
intermediate output of the first decoding step.
The language model G
CS
of the transducer in
the second pass is improved from G by compos-
ing with the translation model P
l
(v?
l
|w
l
). Finally,
the recognition transducer is optimized by deter-
mination and minimization operations.
ASR
2
= P?C?min(det(L?min(det(pi(G
CS
)))))
(16)
5 Experiments
5.1 Experimental Setup
The bilingual acoustic model used for our mixed
language ASR is trained from 160 hours of speech
from GALE Phase 1 Chinese broadcast conver-
sation, 40 hours of speech from GALE Phase 1
English broadcast conversation, and 3 hours of
in-house nonnative English data. The acoustic
features used in our experiments consist of 39
components (13MFCC, 13MFCC, 13MFCC us-
ing cepstral mean normalization), which are an-
alyzed at a 10msec frame rate with a 25msec win-
dow size. The acoustic models used throughout
our paper are state-clustered crossword tri-phone
HMMs with 16 Gaussian mixture output densi-
ties per state. We use the phone set consists of
21 Mandarin standard initials, 37 Mandarin finals,
6 zero initials and 6 extended English phones. The
pronunciation dictionary is obtained by modify-
ing Mandarin and English dictionaries using the
phone set. The acoustic models are reconstructed
913
Table 1: Code switching point detection evaluation (Precision/Recall/F-measure)
Lecture speech Lunch conversation
MixedLM 0.61/0.64/0.64 0.54/0.63/0.58
InterpolatedLM 0.62/0.66/0.64 0.55/0.63/0.58
AdaptedLM 0.63/0.71/0.67 0.54/0.63/0.58
Sequential coupling 0.66/0.71/0.68 0.55/0.70/0.61
Tight coupling 0.68/0.71/0.70 0.56/0.70/0.62
by decision tree tying. We also collected two
speech databases with Chinese to English code
switching - namely, 20 hours of lecture speech cor-
pus (Data 1) and 3 hours of lunch conversation
corpus (Data 2). 18 hours of Data 1 is used for
acoustic model adaptation and 1 hour of data are
used as the test set (Test 1). 2 hours of Data 2 con-
taining 2389 utterances is used to adapt the acous-
tic model and 280 utterances are used as the test
set (Test 2). To train the parser, we use Chinese
Treebank Version 5.0 which consists of 500 thou-
sand words and use the standard data split (Petrov
and Klein, 2007).
For the language models, transcriptions of 18
hours of Data 1 are trained as a baseline mixed
language model for the lecture speech domain.
250,000 sentences from Chinese speech confer-
ence papers, power point slides and web data
are used for training a baseline Chinese matrix
language model for the lecture speech domain
(LM 1). Transcriptions of 2 hours of Data 2 are
used as the baseline mixed language model in the
lunch conversation domain. 250,000 sentences of
the GALE Phase 1 Chinese conversational speech
transcriptions are used to train a Chinese ma-
trix language model (LM 2). 250,000 of GALE
Phase 1 English conversational speech transcrip-
tion are used to train the English embedded lan-
guage model (LM 3). To train the bilingual trans-
lation model, the Chinese Gale Phase 1 conversa-
tional speech transcriptions are used to generate
a bilingual corpus using machine translation. For
comparison, an interpolated language model for
the lunch conversation domain is trained from in-
terpolating LM 2 with LM 3. Also for comparison,
an adapted language model for lecture speech is
trained from LM 1 and transcriptions of 18 hours
of Data 1. An adapted language mode l for conver-
sation is trained from LM 2 and 2 hours of Data 2.
The size of the vocabulary for recognition is 20k
words. The perplexity of the baseline language
model trained on the code switching speech tran-
scription is 236 on the lecture speech and 279 on
the conversation speech test sets.
5.2 Experimental Results
Table 1 reports precision, recall and F-measure
of code switching point in the recognition results
of the baseline and our proposed language mod-
els. Our proposed code switching language mod-
els with functional head constraint improve both
precision and recall of the code switching point
detection on the code switching lecture speech and
lunch conversation 4.48%. Our method by tight-
coupling increases the F-measure by 9.38% rela-
tively on the lecture speech and by 6.90% rela-
tively on the lunch conversation compared to the
baseline adapted language model.
The Table 2 shows the word error rates (WERs)
of experiments on the code switching lecture
speech and Table 3 shows the WERs on the code
switching lunch conversations. Our proposed code
switching language model with Functional Head
Constraints by sequential-coupling reduces the
WERs in the baseline mixed language model by
3.72% relative on Test 1, and 5.85% on Test 2. Our
method by tight-coupling also reduces WER by
2.51% relative compared to the baseline language
model on Test 1, and by 4.57% on Test 2. We
use the speech recognition scoring toolkit (SCTK)
developed by the National Institute of Standards
and Technology to compute the significance lev-
els, which is based on two-proportion z-test com-
paring the difference between the recognition re-
sults of our proposed approach and the baseline.
All the WER reductions are statistically signifi-
cant. For our reference, we also compare the per-
formance of using Functional Head Constraint to
that of using inversion constraint in (Li and Fung,
2012, 2013) and found that the present model re-
duces WER by 0.85% on Test 2 but gives no im-
provement on Test 1. We hypothesize that since
914
Table 2: Our proposed system outperforms the baselines in terms of WER on the lecture speech
Matrix Embedded Overall
MixedLM 34.41% 39.16% 35.17%
InterpolatedLM 34.11% 40.28% 35.10%
AdaptedLM 35.11% 38.41% 34.73%
Sequential coupling 33.17% 36.84% 33.76%
Tight coupling 33.14% 36.65% 33.70%
Table 3: Our proposed system outperforms the baselines in terms of WER on the lunch conversation
Matrix Embedded Overall
MixedLM 46.4% 48.55% 46.83%
InterpolatedLM 46.04% 49.04% 46.64%
AdaptedLM 46.64% 48.39% 46.20%
Sequential coupling 43.24% 46.27% 43.89%
Tight coupling 42.97% 46.03% 43.58%
Test 1 has mostly Chinese words, the proposed
method is not as advantageous compared to our
previous work. Another future direction is for us
to improve the lattice parser as we believe it will
lead to further improvement on the final result of
our proposed method.
6 Conclusion
In this paper, we propose using lattice parsing and
partial parsing to incorporate a well-known syn-
tactic constraint for code mixing speech, namely
the Functional Head Constraint, into a continu-
ous speech recognition system. Under the Func-
tional Head Constraint, code switch cannot occur
between the functional head and its complements.
Since code mixing speech data is scarce, we pro-
pose to instead learn the code mixing language
model from bilingual data with this constraint.
The constrained code switching language model
is obtained by first expanding the search network
with a translation model, and then using parsing to
restrict paths to those permissible under the con-
straint. Lattice parsing enables a sequential cou-
pling of parsing then constraint filtering whereas
partial parsing enables a tight coupling between
parsing and filtering. A WFST-based decoder
then combines a bilingual acoustic model and the
proposed code-switch language model in an inte-
grated approach. Lattice-based parsing and partial
parsing are used to provide the syntactic structure
of the matrix language. Matrix words at the leave
nodes of the syntax tree are permitted to switch to
the embedded language if the switch does not vio-
late the Functional Head Constraint. This reduces
the permissible search paths from those expanded
by the bilingual language model. We tested our
system on a lecture speech dataset with 16% em-
bedded second language, and on a lunch conversa-
tion dataset with 20% embedded second language.
Our language models with lattice parsing and par-
tial parsing reduce word error rates from a baseline
mixed language model by 3.72% to 3.89% rela-
tive in the first task, and by 5.85% to 5.97% in
the second task. They are reduced from an inter-
polated language model by 3.69% to 3.74%, and
by 5.46% to 5.77% in the first and second task re-
spectively. WER reductions from an adapted lan-
guage model are 2.51% to 2.63%, and by 4.47%
to 4.74% in the two tasks. The F-measure for code
switch point detection is improved from 0.64 by
the interpolated model to 0.68, and from 0.67 by
the adapted model to 0.70 by our method. Our
proposed approach avoids making early decisions
on code-switch boundaries and is therefore more
robust. Our approach also avoids the bottleneck of
code switch data scarcity by using bilingual data
with syntactic structure. Moreover, our method re-
duces word error rates for both the matrix and the
embedded language.
Acknowledgments
This work is partially supported by grant number
RGF 612211 of the Hong Kong Research Grants
Council, by 1314159-0PAFT20F003 of the Ping
An Research Institute and by 13140910 of the
Huawei Noah?s Ark Lab.
915
References
J.J. Gumperz, ?Discourse strategies?, Cambridge Uni-
versity Press, 1, 1982.
Coulmas, F., ?The handbook of sociolinguistics?,
Wiley-Blackwell, 1998.
Vu, N.T. and Lyu, D.C. and Weiner, J. and Telaar, D.
and Schlippe, T. and Blaicher, F. and Chng, E.S. and
Schultz, T. and Li, H. A first speech recognition
system for Mandarin-English code-switch conversa-
tional speech?, ICASSP, 2012
J.Y.C. Chan and PC Ching and T. Lee and H.M. Meng
?Detection of language boundary in code-switching
utterances by bi-phone probabilities? Chinese Spo-
ken Language Processing, 2004 International Sym-
posium on, 293?296.
C.J. Shia and Y.H. Chiu and J.H. Hsieh and C.H.
Wu ?Language boundary detection and identifica-
tion of mixed-language speech based on MAP es-
timation??, ICASSP 2004.
D.C. Lyu and R.Y. Lyu ?Language identification
on code-switching utterances using multiple cues?
Ninth Annual Conference of the International
Speech Communication Association, 2008.
Tsai, T.L. and Chiang, C.Y. and Yu, H.M. and Lo,
L.S. and Wang, Y.R. and Chen, S.H. ?A study on
Hakka and mixed Hakka-Mandarin speech recogni-
tion? Chinese Spoken Language Processing (ISC-
SLP), 2010 7th International Symposium on, 199?
204
Yeh, C.F. and Huang, C.Y. and Sun, L.C. and
Lee, L.S. ?An integrated framework for transcrib-
ing Mandarin-English code-mixed lectures with im-
proved acoustic and language modeling? Chinese
Spoken Language Processing (ISCSLP), 2010 7th
International Symposium on, 214?219
K. Bhuvanagiri and S. Kopparapu, ?An Approach to
Mixed Language Automatic Speech Recognition?,
Oriental COCOSDA, Kathmandu, Nepal, 2010
Cao, H. and Ching, PC and Lee, T. and Ye-
ung, Y.T. ?Semantics-based language modeling for
Cantonese-English code-mixing speech recognition
Chinese Spoken Language Processing (ISCSLP),
2010 7th International Symposium on,246?250
Chelba, Ciprian, and Frederick Jelinek. ?Structured
language modeling.? Computer Speech & Language
14, no. 4 (2000): 283-332.
Imseng, D. and Bourlard, H. and Magimai-Doss,
M. and Dines, J., ?Language dependent universal
phoneme posterior estimation for mixed language
speech recognition?, ICASSP, 2011.
Q. Zhang and J. Pan and Y. Yan, ?Mandarin-English
bilingual speech recognition for real world music re-
trieval?, ICASSP, 2008.
Bouselmi, G. and Fohr, D. and Illina, I., ?Combined
acoustic and pronunciation modelling for non-native
speech recognition?, Eighth Annual Conference of
the International Speech Communication Associa-
tion, 2007.
Woolford, E., ?Bilingual code-switching and syntactic
theory?, in Linguistic Inquiry, 14(3):520?536, JS-
TOR, 1983.
MacSwan, J., ?13 Code-switching and grammatical
theory?, in The Handbook of Bilingualism and Mul-
tilingualism, 323 Wiley-Blackwell, 2012.
Poplack, S. and Sankoff, D., ?A formal grammar for
code-switching?, in Papers in Linguistics: Inter-
national Journal of Human Communication, 3?45,
1980.
Moore, Robert C and Lewis, William, ?Intelligent se-
lection of language model training data? Proceed-
ings of the ACL 2010 Conference Short Papers,
220?224.
Belazi, Heidi; Edward Rubin; Almeida Jacqueline
Toribio ?Code switching and X-Bar theory: The
functional head constraint?. Linguistic Inquiry 25
(2): 221-37, 1994.
Bhatt, Rakesh M., ?Code-switching and the functional
head constraint? In Janet Fuller et al. Proceedings of
the Eleventh Eastern States Conference on Linguis-
tics. Ithaca, NY: Department of Modern Languages
and Linguistics. pp. 1-12, 1995
Chappelier, Jean-C?dric, et al., ?Lattice parsing for
speech recognition.? TALN 1999.
916
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 1327?1335,
Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational Linguistics
Rare Word Translation Extraction from Aligned Comparable Documents
Emmanuel Prochasson and Pascale Fung
Human Language Technology Center
Hong Kong University of Science and Technology
Clear Water Bay, Kowloon, Hong Kong
{eemmanuel,pascale}@ust.hk
Abstract
We present a first known result of high pre-
cision rare word bilingual extraction from
comparable corpora, using aligned compara-
ble documents and supervised classification.
We incorporate two features, a context-vector
similarity and a co-occurrence model between
words in aligned documents in a machine
learning approach. We test our hypothesis
on different pairs of languages and corpora.
We obtain very high F-Measure between 80%
and 98% for recognizing and extracting cor-
rect translations for rare terms (from 1 to 5 oc-
currences). Moreover, we show that our sys-
tem can be trained on a pair of languages and
test on a different pair of languages, obtain-
ing a F-Measure of 77% for the classification
of Chinese-English translations using a train-
ing corpus of Spanish-French. Our method is
therefore even potentially applicable to low re-
sources languages without training data.
1 Introduction
Rare words have long been a challenge to translate
automatically using statistical methods due to their
low occurrences. However, the Zipf?s Law claims
that, for any corpus of natural language text, the fre-
quency of a word wn (n being its rank in the fre-
quency table) will be roughly twice as high as the
frequency of word wn+1. The logical consequence
is that in any corpus, there are very few frequent
words and many rare words.
We propose a novel approach to extract rare word
translations from comparable corpora, relying on
two main features.
The first feature is the context-vector similar-
ity (Fung, 2000; Chiao and Zweigenbaum, 2002;
Laroche and Langlais, 2010): each word is charac-
terized by its context in both source and target cor-
pora, words in translation should have similar con-
text in both languages.
The second feature follows the assumption that
specific terms and their translations should appear
together often in documents on the same topic, and
rarely in non-related documents. This is the gen-
eral assumption behind early work on bilingual lex-
icon extraction from parallel documents using sen-
tence boundary as the context window size for co-
occurrence computation, we suggest to extend it to
aligned comparable documents using document as
the context window. This document context is too
large for co-occurrence computation of functional
words or high frequency content words, but we show
through observations and experiments that this win-
dow size is appropriate for rare words.
Both these features are unreliable when the num-
ber of occurrences of words are low. We sug-
gest however that they are complementary and can
be used together in a machine learning approach.
Moreover, we suggest that the model trained for one
pair of languages can be successfully applied to ex-
tract translations from another pair of languages.
This paper is organized as follows. In the next
section, we discuss the challenge of rare lexicon
extraction, explaining the reasons why classic ap-
proaches on comparable corpora fail at dealing with
rare words. We then discuss in section 3 the con-
cept of aligned comparable documents and how we
exploited those documents for bilingual lexicon ex-
traction in section 4. We present our resources and
implementation in section 5 then carry out and com-
ment several experiments in section 6.
1327
2 The challenge of rare lexicon extraction
There are few previous works focusing on the ex-
traction of rare word translations, especially from
comparable corpora. One of the earliest works is
from (Pekar et al, 2006). They emphasized the
fact that the context-vector based approach, used for
processing comparable corpora, perform quite un-
reliably on all but the most frequent words. In a
nutshell1, this approach proceeds by gathering the
context of words in source and target languages in-
side context-vectors, then compares source and tar-
get context-vectors using similarity measures. In
a monolingual context, such an approach is used
to automatically get synonymy relationship between
words to build thesaurus (Grefenstette, 1994). In the
multilingual case, it is used to extract translations,
that is, pairs of words with the same meaning in
source and target corpora. It relies on the Firthien
hypothesis that you shall know a word by the com-
pany it keeps (Firth, 1957).
To show that the frequency of a word influences
its alignment, (Pekar et al, 2006) used six pairs of
comparable corpora, ranking translations according
to their frequencies. The less frequent words are
ranked around 100-160 by their algorithm, while the
most frequent ones typically appear at rank 20-40.
We ran a similar experiment using a French-
English comparable corpus containing medical doc-
uments, all related to the topic of breast cancer,
all manually classified as scientific discourse. The
French part contains about 530,000 words while the
English part contains about 7.4 millions words. For
this experiment though, we sampled the English part
to obtain a 530,000-words large corpus, matching
the size of the French part.
Using an implementation of the context-vector
similarity, we show in figure 1 that frequent words
(above 400 occurrences in the corpus) reach a 60%
precision whereas rare words (below 15 occur-
rences) are correctly aligned in only 5% of the time.
These results can be explained by the fact that, for
the vector comparison to be efficient, the informa-
tion they store has to be relevant and discriminatory.
If there are not enough occurrences of a word, it is
1Detailed presentations can be found for example in (Fung,
2000; Chiao and Zweigenbaum, 2002; Laroche and Langlais,
2010).
Figure 1: Results for context-vector based translations
extraction with respect to word frequency. The vertical
axis is the amount of correct translations found for Top1,
and the horizontal axis is the word occurrences in the cor-
pus.
impossible to get a precise description of the typical
context of this word, and therefore its description
is likely to be very different for source and target
words in translation.
We confirmed this result with another observa-
tion on the full English part of the previous cor-
pus, randomly split in 14 samples of the same size.
The context-vectors for very frequent words, such
as cancer (between 3,000 and 4,000 occurrences in
each sample) are very similar across the subsets.
Less frequent words, such as abnormality (between
70 and 16 occurrences in each sample) have very
unstable context-vectors, hence a lower similarity
across the subsets. This observation actually indi-
cates that it will be difficult to align abnormality
with itself.
3 Aligned comparable documents
A pair of aligned comparable documents is a par-
ticular case of comparable corpus: two compara-
ble documents share the same topic and domain;
they both relate the same information but are not
mutual translations; although they might share par-
allel chunks (Munteanu and Marcu, 2005) ? para-
graphs, sentences or phrases ? in the general case
they were written independently. These compara-
ble documents, when concatenated together in order,
form an aligned comparable corpus.
1328
Examples of such aligned documents can be
found, for example in (Munteanu and Marcu, 2005):
they aligned comparable documents with close pub-
lication dates. (Tao and Zhai, 2005) used an iter-
ative, bootstrapping approach to align comparable
documents using examples of already aligned cor-
pora. (Smith et al, 2010) aligned documents from
Wikipedia following the interlingual links provided
on articles.
We take advantage of this alignment between doc-
uments: by looking at what is common between
two aligned documents and what is different in
other documents, we obtain more precise informa-
tion about terms than when using a larger compa-
rable corpus without alignment. This is especially
interesting in the case of rare lexicon as the clas-
sic context-vector similarity is not discriminatory
enough and fails at raising interesting translation for
rare words.
4 Rare word translations from aligned
comparable documents
4.1 Co-occurrence model
Different approaches have been proposed for bilin-
gual lexicon extraction from parallel corpora, rely-
ing on the assumption that a word has one sense, one
translation, no missing translation, and that its trans-
lation appears in aligned parallel sentences (Fung,
2000). Therefore, translations can be extracted by
comparing the distribution of words across the sen-
tences. For example, (Gale and Church, 1991) used
a derivative of the ?2 statistics to evaluate the as-
sociation between words in aligned region of paral-
lel documents. Such association scores evaluate the
strength of the relation between events. In the case
of parallel sentences and lexicon extraction, they
measure how often two words appear in aligned sen-
tences, and how often one appears without the other.
More precisely, they will compare their number of
co-occurrences against the expected number of co-
occurrences under the null-hypothesis that words are
randomly distributed. If they appear together more
often than expected, they are considered as associ-
ated (Evert, 2008).
We focus in this work on rare words, more pre-
cisely on specialized terminology. We define them
as the set of terms that appear from 1 (hapaxes)
to 5 times. We use a strategy similar to the one
applied on parallel sentences, but rely on aligned
documents. Our hypothesis is very similar: words
in translation should appear in aligned comparable
documents. We used the Jaccard similarity (eq. 1)
to evaluate the association between words among
aligned comparable documents. In the general case,
this measure would not give relevant scores due to
frequency issue: it produces the same scores for
two words that appear always together, and never
one without the other, disregarding the fact that they
appear 500 times or one time only. Other associ-
ation scores generally rely on occurrence and co-
occurrence counts to tackle this issue (such as the
log-likelihood, eq. 2). In our case, the number of
co-occurrences will be limited by the number of oc-
currences of the words, from 1 to 5. Therefore, the
Jaccard similarity efficiently reflects what we want
to observe.
J(wi, wj) =
|Ai ?Aj |
|Ai ?Aj |
;Ai = {d : wi ? d} (1)
A score of 1 indicates a perfect association
(words always appear together, never one without
the other), the more one word appears without the
other, the lower the score.
4.2 Context-vector similarity
We implemented the context-vector similarity in a
way similar to (Morin et al, 2007). In all experi-
ments, we used the same set of parameters, as they
yielded the best results on our corpora. We built the
context-vectors using nouns only as seed lexicon,
with a window size of 20. Source context-vectors
are translated in the target language using the re-
sources presented in the next section. We used the
log-likelihood (Dunning, 1993, eq. 2) for context-
vector normalization (O is the observed number of
co-occurrence in the corpus, E is the expected num-
ber of co-occurrences under the null hypothesis).
We used the Cosine similarity (eq. 3) for context-
vector comparisons.
ll(wi, wj) = 2
?
ij
Oijlog
Oij
Eij
(2)
Cosine(A,B) = A ?B?A?2 + ?B?2 ?A ?B (3)
1329
4.3 Binary classification of rare translations
We suggest to incorporate both the context-vector
similarity and the co-occurrence features in a ma-
chine learning approach. This approach consists of
training a classifier on positive examples of transla-
tion pairs, and negative examples of non-translations
pairs. The trained model (in our case, a decision
tree) is then used to tag an unknown pair of words as
either ?Translation? or ?Non-Translation?.
One potential problem for building the training
set, as pointed out for example by (Zhao and Ng,
2007) is this: we have a limited number of pos-
itive examples, but a very large amount of non-
translation examples as obviously is the case for
rare word translations in any training corpus. In-
cluding two many negative examples in the training
set would lead the classifier to label every pairs as
?Non-Translation?.
To tackle this problem, (Zhao and Ng, 2007)
tuned the imbalance of positive/negative ratio by re-
sampling the positive examples in the training set.
We chose to reduce the set of negative examples,
and found that a ratio of five negative examples to
one positive is optimal in our case. A lower ratio
improves precision but reduces recall for the ?Trans-
lation? class.
It is also desirable that the classifier focuses on
discriminating between confusing pairs of transla-
tions. As most of the negative examples have a
null co-occurrence score and a null context-vector
similarity, they are excluded from the training set.
The negative examples are randomly chosen among
those that fulfill the following constraints:
? non-null features ;
? ratio of number of occurrences between
source/target words higher than 0.2 and lower
than 5.
We use the J48 decision tree algorithm, in the
Weka environment (Hall et al, 2009). Features are
computed using the Jaccard similarity (section 3)
for the co-occurrence model, and the implementa-
tion of the context-vector similarity presented in sec-
tion 4.2.
4.4 Extension to another pair of languages
Even though the context vector similarity has been
shown to achieve different accuracy depending on
the pair of languages involved, the co-occurrence
model is totally language independent. In the case of
binary classification of translations, the two models
are complementary to each other: word pairs with
null co-occurrence are not considered by the context
model while the context vector model gives more se-
mantic information than the co-occurrence model.
For these reasons, we suggest that it is possible
to use a decision tree trained on one pair of lan-
guages to extract translations from another pair of
languages. A similar approach is proposed in (Al-
fonseca et al, 2008): they present a word decom-
position model designed for German language that
they successfully applied to other compounding lan-
guages. Our approach consists in training a decision
tree on a pair of languages and applying this model
to the classification of unknown pairs of words in
another pair of languages. Such an approach is es-
pecially useful for prospecting new translations from
less known languages, using a well known language
as training.
We used the same algorithms and same features as
in the previous sections, but used the data computed
from one pair of languages as the training set, and
the data computed from another pair of languages as
the testing set.
5 Experimental setup
5.1 Corpora
We built several corpora using two different strate-
gies. The first set was built using Wikipedia and the
interlingual links available on articles (that points
to another version of the same article in another
language). We started from the list of all French
articles2 and randomly selected articles that pro-
vide a link to Spanish and English versions. We
downloaded those, and clean them by removing the
wikipedia formatting tags to obtain raw UTF8 texts.
Articles were not selected based on their sizes, the
vocabulary used, nor a particular topic. We obtained
about 20,000 aligned documents for each language.
A second set was built using an in-house system
2Available on http://download.wikimedia.org/.
1330
[WP] French [WP] English [WP] Es [CLIR] En [CLIR] Zh
#documents 20,169 20,169 20,169 15,3247 15,3247
#tokens 4,008,284 5,470,661 2,741,789 1,334,071 1,228,330
#unique tokens 120,238 128,831 103,398 30,984 60,015
Table 1: Statistics for all parts of all corpora.
(unpublished) that seeks for comparable and paral-
lel documents from the web. Starting from a list of
Chinese documents (in this case, mostly news arti-
cles), we automatically selected English target docu-
ments using Cross Language Information Retrieval.
About 85% of the paired documents obtained are di-
rect translations (header/footer of web pages apart).
However, they will be processed just like aligned
comparable documents, that is, we will not take ad-
vantage of the structure of the parallel contents to
improve accuracy, but will use the exact same ap-
proach that we applied for the Wikipedia documents.
We gathered about 15,000 pairs of documents em-
ploying this method.
All corpora were processed using Tree-Tagger3
for segmentation and Part-of-Speech tagging. We
focused on nouns only and discarded all other to-
kens. We would record the lemmatized form of
tokens when available, otherwise we would record
the original form. Table 1 summarizes main statis-
tics for each corpus; [WP] refers to the Wikipedia
corpora, [CLIR] to the Chinese-English corpora ex-
tracted through cross language information retrieval.
5.2 Dictionaries
We need a bilingual seed lexicon for the context-
vector similarity. We used a French-English lex-
icon obtained from the Web. It contains about
67,000 entries. The Spanish-English and Spanish-
French dictionaries were extracted from the linguis-
tic resources of the Apertium project4. We ob-
tained approximately 22,500 Spanish-English trans-
lations and 12,000 for Spanish-French. Finally, for
Chinese-English we used the LDC2002L27 resource
from the Linguistic Data Consortium5 with about
122,000 entries.
3http://www.ims.uni-stuttgart.
de/projekte/corplex/TreeTagger/
DecisionTreeTagger.html
4http://www.apertium.org
5http://www.ldc.upenn.edu
5.3 Evaluation lists
To evaluate our approach, we needed evaluation lists
of terms for which translations are already known.
We used the Medical Subject Headlines, from the
UMLS meta-thesaurus6 which provides a lexicon of
specialized, medical terminology, notably in Span-
ish, English and French. We used the LDC lexi-
con presented in the previous section for Chinese-
English.
From these resources, we selected all the source
words that appears from 1 to 5 times in the corpora
in order to build the evaluation lists.
5.4 Oracle translations
We looked at the corpora to evaluate how many
translation pairs from the evaluation lists can be
found across the aligned comparable documents.
Those translations are hereafter the oracle transla-
tions. For French/English, French/Spanish and En-
glish/Spanish, about 60% of the translation pairs can
be found. For Chinese/English, this ratio reaches
45%. The main reason for this lower result is the
inaccuracy of the segmentation tool used to process
Chinese. Segmentation tools usually rely on a train-
ing corpus and typically fail at handling rare words
which, by definition, were unlikely to be found in the
training examples. Therefore, some rare Chinese to-
kens found in our corpus are the results of faulty seg-
mentation, and the translation of those faulty words
can not be found in related documents. We encoun-
tered the same issue but at a much lower degree for
other languages because of spelling mistakes and/or
improper Part-of-Speech tagging.
6 Experiments
We ran three different experiments. Experiment I
compares the accuracy of the context-vector sim-
ilarity and the co-occurrence model. Experiment
II uses supervised classification with both features.
6http://www.nlm.nih.gov/research/umls/
1331
Figure 2: Experiment I: comparison of accuracy obtained for the Top10 with the context-vector similarity and the
co-occurrence model, for hapaxes (left) and words that appear 2 to 5 times (right).
Experiment III extracts translation from a pair of
languages, using a classifier trained on another pair
of languages.
6.1 Experiment I: co-occurrence model vs.
context-vector similarity
We split the French-English part of the Wikipedia
corpus into different samples: the first sample con-
tains 500 pairs of documents. We then aggregated
more documents to this initial sample to test differ-
ent sizes of corpora. We built the sample in order to
ensure hapaxes in the whole corpus are hapaxes in
all subsets. That is, we ensured the 431 hapaxes in
the evaluation lists are represented in the 500 docu-
ments subset.
We extracted translations in two different ways:
1. using the co-occurrence model;
2. using the context-vector based approach, with
the same evaluation lists.
The accuracy is computed on 1,000 pairs of trans-
lations from the set of oracle translations, and mea-
sures the amount of correct translations found for the
10 best ranks (Top10) after ranking the candidates
according to their score (context-vector similarity or
co-occurrence model). The results are presented in
figure 2.
We can draw two conclusions out of these results.
First, the size of the corpus influences the quality
of the bilingual lexicon extraction when using the
co-occurrence model. This is especially interesting
with hapaxes, for which frequency does not change
with the increase of the size of the corpora. The ac-
curacy is improved by adding more information to
the corpus, even if this additional information does
not cover the pairs of translations we are looking for.
The added documents will weaken the association
of incorrect translations, without changing the as-
sociation for rare terms translations. For example,
the precision for hapaxes using the co-occurrence
model ranges from less than 1% when using only
500 pairs of documents, to about 13% when using
all documents. The second conclusion is that the
co-occurrence model outperforms the context-vector
similarity.
However, both these approaches still perform
poorly. In the next experiment, we propose to com-
bine them using supervised classification.
6.2 Experiment II: binary classification of
translation
For each corpus or combination of corpora ?
English-Spanish, English-French, Spanish-French
and Chinese-English, we ran three experiments, us-
ing the following features for supervised learning of
translations:
? the context-vector similarity;
? the co-occurrence model;
? both features together.
The parameters are discussed in section 4.3. We
used all the oracle translations to train the positive
values. Results are presented in table 2, they are
computed using a 10-folds cross validation. Class
T refers to ?Translation?, ?T to ?Non-Translation?.
The evaluation of precision/recall/F-Measure for the
class ?Translation? are given in equation 4 to 6.
1332
Precision Recall F-Measure Cl.
English-Spanish
context- 0.0% 0.0% 0.0% T
vectors 83.3% 99.9% 90.8% ?T
co-occ. 66.2% 44.2% 53.0% T
model 89.5% 95.5% 92.4% ?T
both 98.6% 88.6% 93.4% T97.8% 99.8% 98.7% ?T
French-English
context- 76.5% 10.3% 18.1% T
vectors 90.9% 99.6% 95.1% ?T
co-occ. 85.7% 1.2% 2.4% T
model 90.1% 100% 94.8% ?T
both 81.0% 80.2% 80.6% T94.9% 98.7% 96.8% ?T
French-Spanish
context- 0.0% 0.0% 0.0% T
vectors 81.0% 100% 89.5% ?T
co-occ. 64.2% 46.5% 53.9% T
model 88.2% 93.9% 91.0% ?T
both 98.7% 94.6% 96.7% T98.8% 99.7% 99.2% ?T
Chinese-English
context- 69.6% 13.3% 22.3% T
vectors 91.0% 93.1% 92.1% ?T
co-occ. 73.8% 32.5% 45.1% T
model 85.2% 97.1% 90.8% ?T
both 86.7% 74.7% 80.3% T96.3% 98.3% 97.3% ?T
Table 2: Experiment II: results of binary classification for
?Translation? and ?Non-Translation?.
precisionT =
|T ? oracle|
|T | (4)
recallT =
|T ? oracle|
|oracle| (5)
FMeasure = 2? precision? recallprecision+ recall (6)
These results show first that one feature is gen-
erally not discriminatory enough to discern correct
translation and non-translation pairs. For example
with Spanish-English, by using context-vector sim-
ilarity only, we obtained very high recall/precision
for the classification of ?Non-Translation?, but null
precision/recall for the classification of ?Transla-
tion?. In some other cases, we obtained high pre-
cision but poor recall with one feature only, which is
not a usefully result as well since most of the correct
translations are still labeled as ?Non-Translation?.
However, when using both features, the precision
is strongly improved up to 98% (English-Spanish
or French-Spanish) with a high recall of about 90%
for class T. We also achieved about 86%/75% pre-
cision/recall in the case of Chinese-English, even
though they are very distant languages. This last re-
sult is also very promising since it has been obtained
from a fully automatically built corpus. Table 3
shows some examples of correctly labeled ?Trans-
lation?.
The decision trees obtained indicate that, in gen-
eral, word pairs with very high co-occurrence model
scores are translations, and that the context-vector
similarity disambiguate candidates with lower co-
occurrence model scores. Interestingly, the trained
decision trees are very similar between the different
pairs of languages, which inspired the next experi-
ment.
6.3 Experiment III: extension to another pair
of languages
In the last experiment, we focused on using the
knowledge acquired with a given pair of languages
to recognize proper translation pairs using a dif-
ferent pair of languages. For this experiment, we
used the data from one corpus to train the classifier,
and used the data from another combination of lan-
guages as the test set. Results are displayed in ta-
ble 4.
These last results are of great interest because
they show that translation pairs can be correctly
classified even with a classifier trained on another
pair of languages. This is very promising be-
cause it allows one to prospect new languages using
knowledge acquired on a known pairs of languages.
As an example, we reached a 77% F-Measure for
Chinese-English alignment using a classifier trained
on Spanish-French features. This not only confirms
the precision/recall of our approach in general, but
also shows that the model obtained by training tends
to be very stable and accurate across different pairs
of languages and different corpora.
1333
Tested with
Trained with Sp-En Sp-Fr Fr-En Zh-En
Sp-En 98.6/88.8/93.5 98.7/94.9/96.8 91.5/48.3/63.2 99.3/63.0/77.1
Sp-Fr 89.5/77.9/83.9 90.4/82.9/86.5 75.4/53.5/62.6 98.7/63.3/77.1
Fr-En 89.5/77.9/83.9 90.4/82.9/86.5 85.2/80.0/82.6 81.0/87.6/84.2
Zh-En 96.6/89.2/92.7 97.7/94.9/96.3 81.1/50.9/62.5 97.4/65.1/78.1
Table 4: Experiment III: Precision/Recall/F-Measure for label ?Translation?, obtained for all training/testing set com-
binations.
English French
myometrium myome`tre
lysergide lysergide
hyoscyamus jusquiame
lysichiton lysichiton
brassicaceae brassicace?es
yarrow achille?e
spikemoss se?laginelle
leiomyoma fibromyome
ryegrass ivraie
English Spanish
spirometry espirometr??a
lolium lolium
omentum epiplo?n
pilocarpine pilocarpina
chickenpox varicela
bruxism bruxismo
psittaciformes psittaciformes
commodification mercantilizacio?n
talus astra?galo
English Chinese
hooliganism ??
kindergarten ???
oyster ??
fascism ?????
taxonomy ???
mongolian ???
subpoena ??
rupee ??
archbishop ???
serfdom ??
typhoid ??
Table 3: Experiment II and III: examples of rare word
translations found by our algorithm. Note that even
though some words such as ?kindergarten? are not rare
in general, they occur with very low frequency in the test
corpus.
7 Conclusion
We presented a new approach for extracting transla-
tions of rare words among aligned comparable doc-
uments. To the best of our knowledge, this is one
of the first high accuracy extraction of rare lexi-
con from non-parallel documents. We obtained a F-
Measure ranging from about 80% (French-English,
Chinese-English) to 97% (French-Spanish). We also
obtained good results for extracting lexicon for a
pair of languages, using a decision tree trained with
the data computed on another pair of languages.
We yielded a 77% F-Measure for the extraction of
Chinese-English lexicon, using Spanish-French for
training the model.
On top of these promising results, our approach
presents several other advantages. First, we showed
that it works well on automatically built corpora
which require minimal human intervention. Aligned
comparable documents can easily be collected and
are available in large volumes. Moreover, the pro-
posed machine learning method incorporating both
context-vector and co-occurrence model has shown
to give good results on pairs of languages that are
very different from each other, such as Chinese-
English. It is also applicable across different train-
ing and testing language pairs, making it possible
for us to find rare word translations even for lan-
guages without training data. The co-occurrence
model is completely language independent and have
been shown to give good results on various pairs of
languages, including Chinese-English.
Acknowledgments
The authors would like to thank Emmanuel Morin
(LINA CNRS 6241) for providing us the compa-
rable corpus used for the experiment in section 2,
Simon Shi for extracting and providing the corpus
1334
described in section 5.1, and the anonymous re-
viewers for their valuable comments. This research
is partly supported by ITS/189/09 AND BBNX02-
20F00310/11PN.
References
Enrique Alfonseca, Slaven Bilac, and Stefan Pharies.
2008. Decompounding query keywords from com-
pounding languages. In Proceedings of the 46th An-
nual Meeting of the Association for Computational
Linguistics (ACL?08), pages 253?256.
Yun-Chuang Chiao and Pierre Zweigenbaum. 2002.
Looking for candidate translational equivalents in spe-
cialized, comparable corpora. In Proceedings of the
19th International Conference on Computational Lin-
guistics (COLING?02), pages 1208?1212.
Ted Dunning. 1993. Accurate Methods for the Statistics
of Surprise and Coincidence. Computational Linguis-
tics, 19(1):61?74.
Stefan Evert. 2008. Corpora and collocations. In
A. Ludeling and M. Kyto, editors, Corpus Linguis-
tics. An International Handbook, chapter 58. Mouton
de Gruyter, Berlin.
John Firth. 1957. A synopsis of linguistic theory 1930-
1955. Studies in Linguistic Analysis, Philological.
Longman.
Pascale Fung. 2000. A statistical view on bilingual lex-
icon extraction?from parallel corpora to non-parallel
corpora. In Jean Ve?ronis, editor, Parallel Text Pro-
cessing, page 428. Kluwer Academic Publishers.
William A. Gale and Kenneth W. Church. 1991. Iden-
tifying word correspondence in parallel texts. In
Proceedings of the workshop on Speech and Natural
Language, HLT?91, pages 152?157, Morristown, NJ,
USA. Association for Computational Linguistics.
Gregory Grefenstette. 1994. Explorations in Automatic
Thesaurus Discovery. Kluwer Academic Publisher.
Mark Hall, Eibe Frank, Geoffrey Holmes, Bernhard
Pfahringer, Peter Reutemann, and Ian H. Witten.
2009. The weka data mining software: An update.
SIGKDD Explorations, 11.
Audrey Laroche and Philippe Langlais. 2010. Revisiting
context-based projection methods for term-translation
spotting in comparable corpora. In 23rd Interna-
tional Conference on Computational Linguistics (Col-
ing 2010), pages 617?625, Beijing, China, Aug.
Emmanuel Morin, Be?atrice Daille, Koichi Takeuchi, and
Kyo Kageura. 2007. Bilingual Terminology Mining ?
Using Brain, not brawn comparable corpora. In Pro-
ceedings of the 45th Annual Meeting of the Association
for Computational Linguistics (ACL?07), pages 664?
671, Prague, Czech Republic.
Dragos Stefan Munteanu and Daniel Marcu. 2005. Im-
proving Machine Translation Performance by Exploit-
ing Non-Parallel Corpora. Computational Linguistics,
31(4):477?504.
Viktor Pekar, Ruslan Mitkov, Dimitar Blagoev, and An-
drea Mulloni. 2006. Finding translations for low-
frequency words in comparable corpora. Machine
Translation, 20(4):247?266.
Jason R. Smith, Chris Quirk, and Kristina Toutanova.
2010. Extracting parallel sentences from comparable
corpora using document level alignment. In Human
Language Technologies: The 2010 Annual Conference
of the North American Chapter of the ACL, pages 403?
411.
Tao Tao and ChengXiang Zhai. 2005. Mining compa-
rable bilingual text corpora for cross-language infor-
mation integration. In KDD ?05: Proceedings of the
eleventh ACM SIGKDD international conference on
Knowledge discovery in data mining, pages 691?696,
New York, NY, USA. ACM.
Shanheng Zhao and Hwee Tou Ng. 2007. Identifi-
cation and resolution of Chinese zero pronouns: A
machine learning approach. In Proceedings of the
2007 Joint Conference on Empirical Methods in Natu-
ral Language Processing and Computational Natural
Language Learning (EMNLP-CoNLL), Prague, Czech
Republic.
1335
Proceedings of The First Workshop on Computational Approaches to Code Switching, pages 62?72,
October 25, 2014, Doha, Qatar.
c
?2014 Association for Computational Linguistics
Overview for the First Shared Task on
Language Identification in Code-Switched Data
Thamar Solorio
Dept. of Computer Science
University of Houston
Houston, TX, 77004
solorio@cs.uh.edu
Elizabeth Blair, Suraj Maharjan, Steven Bethard
Dept. of Computer and Information Sciences
University of Alabama at Birmingham
Birmingham, AL, 35294
{eablair,suraj,bethard}@uab.edu
Mona Diab, Mahmoud Gohneim, Abdelati Hawwari, Fahad AlGhamdi
Dept. of Computer Science
George Washington University
Washington, DC 20052
{mtdiab,mghoneim,abhawwari,fghamdi}@gwu.edu
Julia Hirschberg and Alison Chang
Dept. of Computer Science
Columbia University
New York, NY 10027
julia@cs.columbia.edu
ayc2135@columbia.edu
Pascale Fung
Dept. of Electronic & Computer Engineering
Hong Kong University of Science and Technology
Clear Water Bay, Kowloon, Hong Kong
pascale@ece.ust.hk
Abstract
We present an overview of the first shared
task on language identification on code-
switched data. The shared task in-
cluded code-switched data from four lan-
guage pairs: Modern Standard Arabic-
Dialectal Arabic (MSA-DA), Mandarin-
English (MAN-EN), Nepali-English (NEP-
EN), and Spanish-English (SPA-EN). A to-
tal of seven teams participated in the task
and submitted 42 system runs. The evalua-
tion showed that language identification at
the token level is more difficult when the
languages present are closely related, as in
the case of MSA-DA, where the prediction
performance was the lowest among all lan-
guage pairs. In contrast, the language pairs
with the higest F-measure where SPA-EN
and NEP-EN. The task made evident that
language identification in code-switched
data is still far from solved and warrants
further research.
1 Introduction
The main goal of this language identification shared
task is to increase awareness of the outstanding
challenges in the automated processing of Code-
Switched (CS) data and motivate more research in
this direction. We define CS broadly as a commu-
nication act, whether spoken or written, where two
or more languages are being used interchangeably.
In its spoken form, CS has probably been around
ever since different languages first came in contact.
Linguists have studied this phenomenon since the
mid 1900s. In contrast, the Natural Language Pro-
cessing (NLP) community has only recently started
to pay attention to CS, with the earliest work in
this area dating back to Joshi?s theoretical work
proposing an approach to parsing CS data (Joshi,
1982) based on the Matrix and Embedded language
framework. With the wide-spread use of social me-
dia, CS is now being used more and more in written
language and thus we are seeing an increase in pub-
lished papers dealing with CS. We are specifically
interested in intrasentential code switched phenom-
ena. As a result of this task, we have successfully
created the first set of annotated data for several
language pairs with a coherent set of labels across
the languages. As the shared task results show,
CS poses new research questions that warrant new
NLP approaches, and thus we expect to see a sig-
nificant increase in NLP work in the coming years
addressing CS phenomena in data.
The shared task covers four language pairs and
is focused on social media data. We provided par-
ticipants with annotated data from Twitter for the
62
language pairs: Modern Standard Arabic-Arabic
dialects (MSA-DA), Mandarin-English (MAN-
EN), NEP-EN (NEP-EN), and SPA-EN (SPA-EN).
These language pairs represent a good variety in
terms of language typology and relatedness among
pairs. They also cover languages with different rep-
resentation in terms of number of speakers world
wide. Participants were asked to make predictions
on unseen Twitter data for each language pair. We
also provided participants with test data from a
?surprise genre? with the objective of assessing the
robustness of language identification systems to
genre variation.
2 Task Description
The task consists of labeling each token/word in
the input file with one of six labels: lang1, lang2,
other, ambiguous, mixed, and named entities NE.
The lang1, lang2 labels refer to the two languages
addressed in the subtask, for example for the lan-
guage pair MSA-DA, lang1 would be an MSA and
lang2 is DA. The other category is a label used to
tag all punctuation marks, emoticons, numbers, and
similar tokens that do not represent actual words in
any of the given languages. The ambiguous label
is for instances where it is not possible to assign
a language with certainty, for example, a lexical
form that belongs to both languages, appearing in a
context that does not indicate one language over the
other. The mixed category is for words composed
of CS morphemes, such as the word snapchateando
?to chat? from SPA-EN, the word overai from NEP-
EN, or the word hayqwlwn
1
?they will say?, from
MSA-DA, where the ?ha? is a DA future morpheme
and the stem ?yqwlwn? is MSA.The NE label is
included in this task in an effort to allow for a more
focused analysis of CS data with the exclusion of
proper nouns. NEs have a very different behavior
than most other words in a language vocabulary
and thus from our perspective they need to be iden-
tified to be handled properly.
Table 1 shows Twitter examples taken from the
training data. The annotation guidelines are posted
on the workshop website
2
. We post the ones used
for SPA-EN as for the other language pairs the only
differences are the examples provided.
1
We use Buckwalter transliteration scheme http://
www.qamus.org/transliteration.htm
2
http://emnlp2014.org/workshops/
CodeSwitch/call.html
Language Pair Example
MSA-DA AlnhArdp AlsAEp 11 hAkwn Dyf >.
HAfZ AlmyrAzy ElY qnAp drym llHdyv
En >wlwyAt Alvwrp fy AlmrHlp Al-
HAlyp wqDyp tSHyH msAr Alvwrp
Al<ElAmy
(Today O?Clock 11 I will be
[a ]guest[ of] Mr. Hafez
AlMirazi on Channel Dream
to talk about [the ]priorities[ of]
the revolution in the stage the current
and [the ]issue[ of] correcting
[the ]path[ of] the revolution Media)
NEP-EN My car at the workshop for a much
needed repairs... ABA pocket khali
hune bho
(My car at the workshop for a much
needed repairs. . . now my pocket will
be empty)
SPA-EN Por primera vez veo a @username ac-
tually being hateful! it was beautiful:)
(For the first time I get to see @user-
name actually being hateful! it was
beautiful:)
Table 1: Examples of Twitter data used in the
shared task.
3 Related Work
In the past, most language identification research
has been done at the document level. Some re-
searchers, however, have developed methods to
identify languages within multilingual documents
(Singh and Gorla, 2007; Nguyen and Do?gru?oz,
2013; King and Abney, 2013). Their test data
comes from a variety of sources, including web
pages, bilingual forum posts, and jumbled data
from monolingual sources, but none of them are
trained on code-switched data, opting instead for a
monolingual training set per language. This could
prove to be a problem when working on code-
switched data, particularly in shorter samples such
as social media data, as the code-switching context
is not present in training material.
One system tackled both the problems of code-
switching and social media in language and code-
switched status identification (Lignos and Marcus,
2013). Lignos and Marcus gathered millions of
monolingual tweets in both English and Spanish in
order to model the two languages, and used crowd-
sourcing to annotate tens of thousands of Span-
ish tweets, approximately 11% of which contained
code-switched content. This system was able to
achieve 96.9% word-level accuracy and a 0.936
F-measure in identifying code-switched tweets.
The issue still stands that relatively little code-
switching data, such as that used in Lignos and
63
Marcus? research, is readily available. Even in
their data, the percentage of code-switched tweets
was barely over a tenth of the total test data. There
have been other corpora built, particularly for other
language pairs such as Mandarin-English (Li et
al., 2012; Lyu et al., 2010), but the amount of data
available and the percentage of code-switching data
within that data are not up to the standards of other
areas of the natural language processing field. With
this in mind, we sought to provide corpora for mul-
tiple language pairs, each with a better distribution
of code-switching phenomena.
4 Data Sets
Most of the data for the shared task comes form
Twitter. However, we also collected and annotated
data from other social media sources, including
Facebook, web forums, and blogs. These additional
sources of data were used as the surprise data. In
this section we describe briefly the corpora curated
for the shared task.
Language-pair Training Test Surprise
MAN-EN 1000 313 n/a
MSA-DA 5,838 2332, 1,777 12,017
NEP-EN 9,993 3,018 (2,874) 1,087
SPA-EN 11,400 3,060 (1,626) 1,102
Table 2: Statistics of the shared task data sets
per language pairs. The numbers are according to
what was actually annotated, numbers in parenthe-
sis show what the participating systems were able
to crawl from Twitter. The Surprise genre comes
from various sources, other than Twitter.
Table 2 shows some statistics about the differ-
ent datasets used in this task. We strive to provide
dataset sizes that would allow a robust analysis of
results. However, an unexpected challenge was
the rate at which tweets became unavailable. Dif-
ferent language pairs had different attrition rates
with SPA-EN being the most affected language and
MSA-DA and NEP-EN the least affected. Note
that we provided two test datasets for MSA-DA.
Since we separated the data on a per user basis, the
first test set had a highly skewed distribution. The
second test set was distributed to participants to
allow a comparison with a data set having a class
distribution more similar to the training set.
4.1 SPA-EN data
Developing the corpus involved two primary steps:
locating code-switching tweets and using crowd-
sourcing to annotate their tokens with language
tags. A small portion of the tweets were annotated
in-lab and this was used as the gold data for quality
control in the crowdsourcing annotation.
To avoid biasing the data used in this task, we
used a two step process to select the tweets: first we
identified CS tweets by doing a keyword search on
Twitter?s API. We selected a few frequently used
English words and restricted the search to tweets
identified by Twitter as Spanish from users in Cali-
fornia and Texas. An additional set of tweets was
then collected by using frequent Spanish words in
an all English tweet, from users in the same loca-
tions. We filtered these tweets to remove tweets
containing URLs, duplicates, spam tweets and
retweets.
In-lab annotators labeled the filtered tweets using
the guidelines referenced above. From this set of
labeled data we then ranked the users in this set by
the percentage of CS tweets. We selected the 12
most prolific CS users and then pulled all of their
available tweets. These 12 users contributed the
tweets used in the shared task. The tweets were
labeled using CrowdFlower
3
. After analyzing the
number and content distribution of the tweets, the
SPA-EN data was split into a 11,400 tweet training
set and a 3,014 tweet test set.
The SPA-EN Surprise Genre (SPA-EN-SG) in-
cluded Facebook comments from the Veteranas
community
4
and the Chicanas community
5
and
blog data from the Albino Bean
6
. Data was col-
lected using Python scripts that implemented the
Beautiful Soup library and the third-party Python
Facebook SDK (for Blogger and Facebook respec-
tively). Post and comment IDs were used to iden-
tify Facebook posts, and URLs were used to iden-
tify Blogger posts. The collected posts were format-
ted to match those collected from Twitter. In-lab
annotators were used to annotate approximately 1K
tokens. All the data we collected in this manner
was released as surprise data to all participants.
4.2 NEP-EN data
The collection of NEP-EN data followed a simi-
lar approach to that of SPA-EN. We first focused
on finding users that switched frequently between
3
http://www.crowdflower.com/
4
https://www.facebook.com/
VeteranaPinup
5
https://www.facebook.com/pages/
Chicanas/444483772293893
6
http://thealbinobean.blogspot.com/
64
Nepali and English. In addition, the users must
not be using Devnagari script as done by Nepalese
to write Nepali, but must have used its Roman-
ized form. We started by manually reading tweets
from some of our Nepali friends. We then crawled
their followers who corresponded with them using
code-switched tweets or replies. We found that
a lot of these users were regular code-switchers
themselves. We repeated the same process with the
followers and collected nearly 30 such users. We
then collected about 2,000 tweets each from these
users using the Twitter API. We filtered out all the
retweets and the tweets with URLs, following the
same process that was used for SPA-EN.
For the surprise test data, we crawled code-
switched data from Facebook comments and posts.
We found that most Nepalese comments had a rich
amount of code-switched data. However, we could
not crawl their data because of privacy issues. Nev-
ertheless, we could crawl data from public Face-
book pages. We identified some public Nepali Face-
book pages where anyone could comment. These
pages include FM, news and public figures? public
Facebook pages. We crawled the latest 10 feeds
from these public pages using the Facebook API
and gathered about 12,000 comments and posts for
the shared task.
Initially, we sought out help from Nepali gradu-
ate students at the University of Alabama at Birm-
ingham to annotate 100 tweets (1739 tokens). We
gave the same annotation file to two annotators to
do the annotation. We found that they agreed with
an accuracy of 95.34%. These tweets were then re-
viewed and used as initial gold data in Crowdflower
to annotate the first 1000 tweets. The annotation
job was enabled only in Nepal and Bhutan. We
disabled India, even though people living in some
regions of India (Darjeeling, Sikkim) also speak
and write in Nepali, as most spammers were com-
ing from India. We then ran two batches of 5000
tweets and one batch of 3000 tweets along with the
initial 1,000 tweets as the gold data. This NEP-EN
data was then split into a 9,993 tweet training set
and a 2,874 tweet test set. No Twitter user appeared
in both sets.
4.3 MAN-EN data
The MAN-EN tweets were collected from Twitter
with the Twitter API. Users were selected from
lists of most followed Twitter accounts in Taiwan
(where Mandarin Chinese is the official language).
These users? tweets were checked for Mandarin En-
glish bilingualism and added to our data collection
if they contained both languages.
The next round of usernames came from the
lists of users that our original top accounts were
following. The tweets written by this new set of
users were then examined for Mandarin English
code switching and stored as data if they matched
the criteria.
The jieba tokenizer
7
was used to segment the
Mandarin sections of the tweets and compute off-
sets of each segment. We format the code switch-
ing tweets into columns including language type,
labels, and offsets. Named entities were labeled
manually by a single annotator.
The data was split by user into 1000 tweets for
training and 313 for testing. No MAN-EN surprise
data for the current shared task.
4.4 MSA-DA data
For the MSA-DA language pair, we selected Egyp-
tian Arabic (EGY) as the Arabic dialect. We har-
vested data from two social media sources: Twitter
[TWT] and Blog commentaries [COM]. The TWT
data served as the main gold standard data for the
task where we provided fully annotated data for
Training/Tuning and Test. We provided two TWT
data sets for the test data that exemplified different
tag distributions. The COM data set comprised
only test data and it served as the Arabic surprise
data set.
To reduce the potential of TWT data attrition
from users deleting their accounts or tweets, we
selected tweets that are less prone to deletion and/or
change. Thereby we harvested tweets by a select
set of Egyptian Public Figures. The percentage
of deleted tweets and deactivated accounts among
those users is significantly lower if we compare it
to the tweets crawled from random Egyptian users.
We used the ?Tweepy? library to crawl the time-
lines of 12 Public Figures. Similar to other lan-
guage pairs, we excluded all re-tweets, tweets with
URLs, tweets mentioning other users, and tweets
containing Latin characters. We accepted 9,947
tweets, for each we extracted the tweet-id and user-
id. Using these IDs, we retrieved the tweets text,
tokenized it and assigned character offsets. To guar-
antee consistency and avoid any misalignment is-
sues, we compiled the full pipeline into the ?Arabic
Tweets Token Assigner? package which is made
7
https://github.com/fxsjy/jieba
65
available through the workshop website
8
.
For COM, we selected 6723 commentaries (half
MSA and half DA) from ?youm7?
9
commen-
taries provided by the Arabic Online Commentary
Dataset (Zaidan and Callison-Burch, 2011). The
COM data set was processed (12017 total tokens)
using the same pipeline created for the task. We
also provided the participants with the data format-
ted with character offsets to maintain consistency
across data sets in the Arabic subtask.
The annotation of MSA-DA language pair data
is based on two sets of guidelines. The first set
is a generic set of guidelines for code switching
in general across different language pairs. These
guidelines provide the overarching framework for
annotating code switched data on the morpholog-
ical, lexical, syntactic, and pragmatic levels. The
second set of guidelines is language pair specific.
We created the guidelines for the Arabic language
specifically. We enlisted the help of 3 annotators
in addition to a super annotator, hence resulting
in 4 annotators overall for the whole collection of
the data. All the annotators are native speakers
of Egyptian Arabic with excellent proficiency in
MSA. The super annotator only annotated 10% of
the overall data and served as the adjudicator. The
annotation process was iterative with several repe-
titions of the cycle of training, annotation, revision,
adjudication until we approached a stable Inter An-
notator Agreement (IAA) of over 90% pairwise
agreement.
5 Survey of Shared Task Systems
We received submissions from seven different
teams. Each participating system had the freedom
to submit responses to any of the language pairs
covered in the shared task. All seven participants
submitted system responses for SPA-EN, making
this language pair the most popular in this shared
task and MAN-EN the least popular.
All but one participating system used a machine
learning algorithm or language models, or even a
combination of both, as part of their configuration.
A couple of the participating systems used hand-
crafted rules of some sort, either at the intermediate
steps or as the final post-processing step. We also
observed a good number of systems using exter-
nal resources, in the form of labeled monolingual
8
http://emnlp2014.org/workshops/
CodeSwitch/call.html
9
An Egyptian newspaper, www.youm7.com
corpora, language specific gazetteers, off the shelf
tools (NE recognizers, language id systems, or mor-
phological analyzers) and even unsupervised data
crawled from the same users present in the data
sets provided. Affixes were also used in some form
by different systems.
The architecture of the different systems ranged
from a simple approach based on frequencies of
character n-grams combined in a rule-based system,
to more complex approaches using word embed-
dings, extended Markov Models, and CRF autoen-
coders. The majority of the systems that partici-
pated in more than one language pair did little to no
customization to account for the morphological dif-
ferences of the specific language pairs beyond lan-
guage specific parameter-tuning, which probably
reflects participants? goal to develop a multilingual
id system.
Due to the presence of the NE label, several
systems included a component for NE recognition
where there was one available for the specific lan-
guage. In addition, many systems also included
case information. One unexpected finding from
the shared task was that no participating system
tried to embed in their models some form of lin-
guistic theory or framework about CS. Only one
system made an explicit reference to CS theories
(Chittaranjan et al., 2014) in their motivation to use
contextual information, which can be considered
as a loose embedding of CS theory. While system
performance was competitive (see next section),
there is still room for improvement and perhaps
some of that improvement can come out of adding
this kind of knowledge into the models. Lastly, we
were surprised to see that not all systems made use
of character encoding information, even though for
Mandarin-English that would have been a strong
indicator. In Table 3 we present a summary high-
lighting some of the design choices of participating
systems.
6 Results
We used the following evaluation metrics: Accu-
racy, Precision, Recall, and F-measure. We use
F-measure to provide a ranking of systems. In the
evaluation at the tweet level we use the standard
f-measure. For the evaluation at the token level
we use instead the average weighted f-measure to
account for the highly imbalanced distribution of
classes.
To provide a fair evaluation, we only scored pre-
66
System
Machine
Learning
Rules Case
Character
Encoding
External Resources LM Affixes Context
(Chittaranjan et al., 2014)
CRF
4
4 dbpedia dumps, online sources
? 3
(Shrestha, 2014) 4
4 spell checker
(Jain and Bhat, 2014)
CRF
4
4 English dictionary
4 4 ? 2
(King et al., 2014)
eMM
ANERgazet, TwitterNLP, Stan-
ford NER
4 4 4
(Bar and Dershowitz, 2014)
SVM
4
Illocution Twitter Lexicon,
monolingual corpora (NE lists)
4 4 ? 2
(Lin et al., 2014)
CRF
4
4
Hindi-Nepali Wikipedia, JRC,
CoNLL 2003 shared task, lang
id predictors: cld2 and ldig
4 4
(Barman et al., 2014)
kNN, SVM
4 4
BNC, LexNorm
4 ? 1
Table 3: Comparison of shared task participating system algorithm choices. CRF stands for Conditional
Random Fields, SVM for Support Vector Machines and LM for Language Models.
dictions on tweets submitted by all teams. All
systems were compared to a simple lexicon-based
baseline. The lexicon was gathered from the train-
ing data for classes lang1 and lang2 only. Emoti-
cons, punctuation marks, usernames and URLs are
by default tagged as other. In the case of a tie or a
new token, the baseline system assigns the majority
class for that language pair.
Figure 1 shows prediction performance on the
Twitter test data for each language pair at the tweet
level. The system predictions for this task are taken
directly from the individual token predictions in
the following manner: if the system predictions for
the same tweet contain at least one tag from each
language (lang1 and lang2), the tweet is labeled
as code-switched, otherwise it is labeled as mono-
lingual. As illustrated, each language pair shows
different patterns. Comparing the systems that par-
ticipated in all language pairs, there is no clear
winner across the board. However, (Chittaranjan et
al., 2014) was in the top three places in at least one
test file for each language pair. Table 4 shows the
results at the token level by label. Here again the
figures show F-measure per class label and the last
column is the weighted average f-measure (Avg-F).
One of the few general trends on these results is
that most participating systems were not able to
correctly identify the minority classes ?ambiguous?
and ?other?. There are only few instances of these
labels in the training set and some test sets did not
have one of these classes present. The impact on
final system performance from these classes is not
significant. However, to study CS patterns we will
need to have these labels identified properly.
The MAN-EN pair received four system re-
sponses and all four of them reached an F-measure
>80% and outperformed the simple baseline by a
considerable margin. We expected this language
pair to be the easiest one for the shared task since
each language uses a different encoding script. A
very rough but accurate distinction between Man-
darin and English could be achieved by looking
at the character encoding. However, according to
the system descriptions provided, not all systems
used encoding information. The best performing
systems for MAN-EN are (King et al., 2014) and
(Chittaranjan et al., 2014). The former slightly
outperformed the latter at the Tweet level (see Fig-
ure 1a) task while the opposite was true at the token
level (see Table 4 rows 4 and 5).
In the case of SPA-EN, all seven systems out-
performed the simple baseline. The best perform-
ing system in all SPA-EN tasks was (Bar and
Dershowitz, 2014). This system achieved an F-
measure of 82.2%, 2.9 percentage points above the
second best system (Lin et al., 2014) on the tweet
level task (see Figure 1(d)). In the token level
evaluation, (Bar and Dershowitz, 2014) reached
an Avg. F-measure of 94%. This top performing
system uses a sequential classification approach
where the labels from the preceding words are used
as features in the model. Another design choice
that might have given the edge to this system is the
fact that their model combines character- and word-
based language models in what the authors call
?intra- and inter-word level? features. Both types
of language models are trained on large amounts
of monolingual data and NE lists, which again pro-
vides additional knowledge that other systems are
not exploiting. For instance, the NE lexicons might
account for the best results in the NE class in both
the Twitter data and the Surprise genre (see Table 4
last row for SPA-EN and second to last for SPA-
EN Surprise). Most systems showed considerable
67
(Jain and
Bhat, 2014)
(Lin et
al., 2014)
(Chittaranjan
et al., 2014)
(King et
al., 2014)
0.6
0.7
0.8
0.9
1
0.838
0.888
0.892
0.894
F
-
m
e
a
s
u
r
e
Baseline
(a) MAN-EN
(Chittaranjan
et al., 2014)
(King et
al., 2014)
(Jain and
Bhat, 2014)
(Elfardy et
al., 2014)
(Lin et
al., 2014)
0.1
0.2
0.3
0.4
0.152
0.118
0.048
0.044
0.095
0.196
0.260
0.338
0.360
0.417
F
-
m
e
a
s
u
r
e
Baseline Test1
Baseline Test2
(b) MSA-DA. Dark gray bars show performance on Test1 and light gray bars show performance for Test2
(King et
al., 2014)
(Lin et
al., 2014)
(Jain and
Bhat, 2014)
(Shrestha,
2014)
(Chittaranjan
et al., 2014)
(Barman et
al., 2014)
0.8
0.9
1
0.952
0.962
0.972
0.974
0.975
0.977
F
-
m
e
a
s
u
r
e
Baseline
(c) NEP-EN
(Shrestha,
2014)
(King et
al., 2014)
(Chittaranjan
et al., 2014)
(Barman et
al., 2014)
(Jain and
Bhat, 2014)
(Lin et
al., 2014)
(Bar and
Dershowitz,
2014)
0.6
0.7
0.8
0.9
1
0.634
0.703
0.753
0.754
0.783
0.793
0.822
F
-
m
e
a
s
u
r
e
Baseline
(d) SPA-EN
Figure 1: Prediction results on language identification at the tweet level. This is a binary task to distinguish
between a monolingual and a CS tweet. We show performance of participating systems using F-measure
as the evaluation metric. The solid line shows the lexicon baseline performance.
differences in prediction performance in both gen-
res. In all cases the Avg. F-measure was higher
on the Twitter test data than on the surprise genre.
Although the surprise genre is too small to draw
strong conclusions, all language pairs with surprise
genre test data showed a decrease in performance
of around 10%.
We analyzed system outputs and found some
consistent sources of error. Lexical forms that exist
in both languages were frequently mislabeled by
68
most systems. For example the word for ?he? was
frequently mislabeled by at least one system. In
most of the cases systems were predicting EN as
label when the target language was SPA. Cases like
this were even more prone to errors when these
words fell in the CS point, as in this tweet: ni el
header he hecho (I haven?t even done the header).
Tweets like this one, with just one token from the
other language, were difficult for most systems.
Named entities were also frequent sources of error,
especially when they were spelled with lower cases
letters.
By far the hardest language pair in this shared
task was MSA-DA, as anticipated. Especially when
considering the typological similarities between
MSA and DA. This is mainly due to the fact that
DA and MSA are close variants of one another and
hence they share considerable amount of lexical
items. The shared lexical items could be simple
cognates of one another, or faux amis where they
are homographs or homophones, but have com-
pletely different meaning. Both categories con-
stitute a significant challenge. Accordingly, the
baseline system had the lowest performance from
all language pairs in both test sets. We note chal-
lenges in this language pair on each linguistic level
where CS occurs especially for the shared lexical
items.
On the phonological level, DA writers tend to
mimic the MSA script for DA words even if they
are pronounced differently. For example: ?heart? is
pronounced in DA Alob and in MSA as qalob but
commonly written in MSA as ?qalob? in DA data.
Also many phonological differences are in short
vowels that are underspecified in written Arabic,
adding another layer of ambiguity.
On the morphological level, there is no avail-
able morphological analyzer able to recognize such
shared words and hence they are mostly misclassi-
fied. Language identification for MSA-DA CS text
highly depends on the context. Typically some Ara-
bic variety word serves as a marker for a context
switch such as mElh$ for DA or mn? for MSA. But
if shared lexical items are used, it is challenging
to identify the Arabic variant. An example from
the training data is qlb meaning either heart as a
noun or change as a verb in the phrase lw qlb mjrm,
corresponding to ?If the heart of a criminal? or ?if
he changes into a criminal?. These challenges ren-
der language identification for CS MSA-DA data
far from solved as evident by the fact that the high-
est scoring system reached an F-measure of only
41.7% in Test2 for CS identification. Moreover,
this is the only language pair where at least one
system was not able to outperform the baseline and
in the case of Test2 only one system (Lin et al.,
2014) outperformed the baseline.
Most teams did well for the NEP-EN shared task,
and all teams outperformed the baseline. The rea-
son for the high performance might be the high
number of codeswitched tweets in the training and
test data for NEP-EN (much higher than other lan-
guage pairs). This allowed systems to have more
samples of CS instances. The other reason for good
performance by most participants in both evalua-
tions might be that Nepali and English are two very
different languages. The structure of the words and
syntax of word formation are very different. We
suspect, for instance, that there is a much lower
overlap of character n-grams in this language pair
than in SPA-EN, which makes for an easier task. At
the Tweet level, system performance ranged over
a small set of values, the lowest F-measure was
95.2% while the highest was 97.7%. Looking at
the numbers in Table 4, we can see that even NE
recognition seemed to be a much easier task for this
language pair than for SPA-EN (compare results
for the NE category in both SPA-EN sets to those
of both NEP-EN data sets). The best performing
system for the Twitter test data is (Barman et al.,
2014) with an F-measure of 97.7%. The results
trend in the surprise genre is not consistent with
what we observed for the Twitter test data. The
top ranked system for Twitter sunk to the 4th place
with an F-measure or 59.6%, a considerable drop
of almost 40 percentage points. In this case, the
overall numbers indicate a much wider difference
in the genres than what we observed for other lan-
guages, such as SPA-EN, for example. We should
note that the class distribution in the surprise data is
considerably different from what the models used
for training, and from that of the test data as well.
In the Twitter data there was a larger number of CS
tweets than monolingual ones, while in the surprise
genre the majority class was monolingual. This
will account for a good portion of the differences
in performance. But here as well, the small number
of labeled instances makes it hard to draw strong
conclusions.
69
Test Set System lang1 lang2 NE other ambiguous mixed Avg-F
MAN-EN
Baseline 0.9 0.47 0 0.29 - 0 0.761
(Jain and Bhat, 2014) 0.97 0.66 0.52 0.33 - 0 0.871
(Lin et al., 2014) 0.98 0.73 0.62 0.34 - 0 0.886
(King et al., 2014) 0.98 0.74 0.58 0.30 - 0 0.884
(Chittaranjan et al., 2014) 0.98 0.76 0.66 0.34 - 0 0.892
MSA-DA Test 1
(King et al., 2014) 0.88 0.14 0.05 0 0 - 0.720
Baseline 0.92 0.06 0 0.89 0 - 0.819
(Chittaranjan et al., 2014) 0.94 0.15 0.57 0.91 0 - 0.898
(Jain and Bhat, 2014) 0.93 0.05 0.73 0.87 0 - 0.909
(Lin et al., 2014) 0.94 0.09 0.74 0.98 0 - 0.922
(Elfardy et al., 2014)* 0.94 0.05 0.85 0.99 0 - 0.936
MSA-DA Test 2
Baseline 0.54 0.27 0 0.94 0 0 0.385
(King et al., 2014) 0.59 0.59 0.13 0.01 0 0 0.477
(Chittaranjan et al., 2014) 0.58 0.50 0.42 0.43 0.01 0 0.513
(Jain and Bhat, 2014) 0.62 0.49 0.67 0.75 0 0 0.580
(Elfardy et al., 2014)* 0.73 0.73 0.91 0.98 0 0.01 0.777
(Lin et al., 2014) 0.76 0.81 0.73 0.98 0 0 0.799
MSA-DA Surprise
(King et al., 2014) 0.48 0.60 0.05 0.02 0 0 0.467
(Jain and Bhat, 2014) 0.53 0.61 0.62 0.96 0 0 0.626
(Chittaranjan et al., 2014) 0.56 0.69 0.33 0.96 0 0 0.654
(Lin et al., 2014) 0.68 0.82 0.61 0.97 0 0 0.778
(Elfardy et al., 2014)* 0.66 0.81 0.87 0.99 0 0 0.801
NEP-EN
Baseline 0.67 0.76 0 0.61 - 0 0.678
(King et al., 2014) 0.87 0.80 0.51 0.34 - 0.03 0.707
(Lin et al., 2014) 0.93 0.91 0.49 0.95 - 0.02 0.917
(Jain and Bhat, 2014) 0.94 0.96 0.52 0.94 - 0 0.942
(Shrestha, 2014) 0.94 0.96 0.57 0.95 - 0 0.944
(Chittaranjan et al., 2014) 0.94 0.96 0.45 0.97 - 0 0.948
(Barman et al., 2014) 0.96 0.97 0.58 0.97 - 0.06 0.959
NEP-EN Surprise
(Lin et al., 2014) 0.83 0.73 0.46 0.65 - - 0.712
(King et al., 2014) 0.82 0.88 0.43 0.12 - - 0.761
(Chittaranjan et al., 2014) 0.78 0.87 0.37 0.80 - - 0.796
(Jain and Bhat, 2014) 0.83 0.91 0.50 0.87 - - 0.850
(Barman et al., 2014) 0.87 0.90 0.61 0.74 - - 0.853
(Shrestha, 2014) 0.85 0.92 0.53 0.78 - - 0.855
SPA-EN
Baseline 0.72 0.56 0 0.75 0 0 0.704
(Shrestha, 2014) 0.88 0.85 0.35 0.92 0 0 0.873
(Jain and Bhat, 2014) 0.92 0.92 0.36 0.90 0 0 0.905
(Lin et al., 2014) 0.93 0.93 0.32 0.91 0.03 0 0.913
(Barman et al., 2014) 0.93 0.92 0.47 0.93 0.03 0 0.921
(King et al., 2014) 0.94 0.93 0.54 0.92 0 0 0.923
(Chittaranjan et al., 2014) 0.94 0.93 0.28 0.95 0 0 0.926
(Bar and Dershowitz, 2014) 0.95 0.95 0.56 0.94 0.04 0 0.940
SPA-EN Surprise
(Shrestha, 2014) 0.80 0.78 0.23 0.81 0 0 0.778
(Jain and Bhat, 2014) 0.83 0.84 0.22 0.79 0 0 0.811
(Lin et al., 2014) 0.83 0.86 0.19 0.80 0.03 0 0.816
(Barman et al., 2014) 0.84 0.85 0.31 0.82 0.03 0 0.823
(Chittaranjan et al., 2014) 0.94 0.86 0.14 0.83 0 0 0.824
(King et al., 2014) 0.84 0.85 0.35 0.81 0 0 0.828
(Bar and Dershowitz, 2014) 0.85 0.87 0.37 0.83 0.03 0 0.839
Table 4: Performance results on language identification at the token level. A ?-? indicates there were no
tokens of this class in the test set. We ranked systems using weighted averaged f-measure (Avg-F). The ?*?
marks the system by (Elfardy et al., 2014). This system was not considered in the ranking for the shared
task as it was developed by co-organizers of the task.
7 Lessons Learned
Among the things we want to improve for future
shared tasks is the issue of data loss due to re-
moval of tweets or users deleting their accounts.
We decided to use Twitter data to have a relevant
corpus. However, the trade-off is the lack of rights
to distribute the data ourselves. This is not just a
burden for the participants. It is an awful waste of
resources as the data that was expensive to gather
and label is not being used beyond the small group
of researchers involved in the creation of the cor-
pus. This will deter us from using Twitter data for
future shared tasks, at least until a better solution
is identified.
70
(Chittaranjan
et al., 2014)
(King et
al., 2014)
(Jain and
Bhat, 2014)
(Lin et
al., 2014)
(Elfardy et
al., 2014)
0
0.1
0.2
0.3
0.170
0.194
0.222
0.276
0.277
F
-
m
e
a
s
u
r
e
(a) MSA-DA Surprise Genre Results
(Chittaranjan
et al., 2014)
(Jain and
Bhat, 2014)
(Barman et
al., 2014)
(King et
al., 2014)
(Shrestha,
2014)
(Lin et
al., 2014)
0.4
0.5
0.6
0.7
0.554
0.571
0.596
0.604
0.632
0.702
F
-
m
e
a
s
u
r
e
(b) NEP-EN Surprise Genre Results
(Shrestha,
2014)
(King et
al., 2014)
(Chittaranjan
et al., 2014)
(Barman et
al., 2014)
(Jain and
Bhat, 2014)
(Lin et
al., 2014)
(Bar and
Dershowitz,
2014)
0.4
0.5
0.6
0.7
0.8
0.633
0.640
0.704
0.710
0.725
0.727
0.753
F
-
m
e
a
s
u
r
e
(c) SPA-EN Surprise Genre Results
Figure 2: Prediction results on language identification at the document level for the surprise genre. This
is a binary task to distinguish between a monolingual and a code-switched text. We show performance of
participating systems using F-measure as the evaluation metric.
Using crowdsourcing for annotating the data is a
cheap and easy way for generating resources. But
we found out that even when following best prac-
tices for quality control, there was a substantial
amount of noise in the gold data. We plan to con-
tinue working on refining the annotation guidelines
and quality control processes to reduce the amount
of noise in gold annotations.
8 Conclusion
This is the first shared task on language identifica-
tion in CS data. Yet, the response was quite positive
as we received 42 system runs from seven different
teams, plus submissions for MSA-AD from a sub
group of the task organizers (Elfardy et al., 2014).
The systems presented are overall robust and with
interesting differences from one another. Although
we did not see a single system ranking in the top
places across all language pairs and tasks, we did
see systems showing robust performance indicat-
ing some level of language independence. But the
results are not consistent at the tweet/document
level. The language pair that proved to be the most
difficult for the task was MSA-DA, where the lexi-
con baseline system was hard to beat even with an
F-measure of 47.1%.
This shared task showed that language identifica-
tion in code-switched data is still an open problem
that warrants further investigation. Perhaps in the
near future we will see systems that embed some
form of linguistic theory about CS and maybe that
would result in more accurate predictions.
Our goal is to support new research addressing
CS data. Discussions about the challenge for the
next shared task are already underway. One pos-
sibility might be parsing. We plan to investigate
the challenges in parsing CS data and we will start
by exploring the hardships in manually annotating
CS with syntactic information. We would also like
to explore the possibility of classifying CS points
according to their socio-pragmatic role.
71
Acknowledgments
We would like to thank all shared task partici-
pants. We also thank Brian Hester and Mohamed
Elbadrashiny for their invaluable support in the
development of the gold standard data and analy-
sis of results. We also thank the in-lab annotators
and the CrowdFlower contributors. This work was
partly funded by NSF under awards 1205475 and
1205556.
References
Kfir Bar and Nachum Dershowitz. 2014. Tel Aviv Uni-
versity system description for the code-switching
workshop shared task. In Proceedings of the First
Workshop on Computational Approaches to Code-
Switching, Doha, Qatar, October. ACL.
Utsab Barman, Joachim Wagner, Grzegorz Chrupala,
and Jennifer Foster. 2014. DCU-UVT: Word-
level language classification with code-mixed data.
In Proceedings of the First Workshop on Computa-
tional Approaches to Code-Switching, Doha, Qatar,
October. ACL.
Gokul Chittaranjan, Yogarshi Vyas, Kalika Bali, and
Monojit Choudhury. 2014. A framework to label
code-mixed sentences in social media. In Proceed-
ings of the First Workshop on Computational Ap-
proaches to Code-Switching, Doha, Qatar, October.
ACL.
Heba Elfardy, Mohamed Al-Badrashiny, and Mona
Diab. 2014. AIDA: Identifying code switching in
informal Arabic text. In Proceedings of the First
Workshop on Computational Approaches to Code-
Switching, Doha, Qatar, October. ACL.
Naman Jain and Riyaz Ahmad Bhat. 2014. Language
identification in codeswitching scenario. In Pro-
ceedings of the First Workshop on Computational
Approaches to Code-Switching, Doha, Qatar, Octo-
ber. ACL.
A. Joshi. 1982. Processing of sentences with in-
trasentential code-switching. In J?an Horeck?y, editor,
COLING-82, pages 145?150, Prague, July.
Ben King and Steven Abney. 2013. Labeling the lan-
guages of words in mixed-language documents us-
ing weakly supervised methods. In Proceedings of
the 2013 Conference of the North American Chap-
ter of the Association for Computational Linguis-
tics: Human Language Technologies, pages 1110?
1119, Atlanta, Georgia, June. Association for Com-
putational Linguistics.
Levi King, Eric Baucom, Tim Gilmanov, Sandra
K?ubler, Dan Whyatt, Wolfgang Maier, and Paul Ro-
drigues. 2014. The IUCL+ system: Word-level
language identification via extended Markov models.
In Proceedings of the First Workshop on Computa-
tional Approaches to Code-Switching, Doha, Qatar,
October. ACL.
Ying Li, Yue Yu, and Pascale Fung. 2012. A
Mandarin-English code-switching corpus. In Nico-
letta Calzolari, Khalid Choukri, Thierry Declerck,
Mehmet U?gur Do?gan, Bente Maegaard, Joseph Mar-
iani, Jan Odijk, and Stelios Piperidis, editors, Pro-
ceedings of the Eighth International Conference on
Language Resources and Evaluation (LREC-2012),
pages 2515?2519, Istanbul, Turkey, May. European
Language Resources Association (ELRA). ACL An-
thology Identifier: L12-1573.
Constantine Lignos and Mitch Marcus. 2013. Toward
web-scale analysis of codeswitching. In 87th An-
nual Meeting of the Linguistic Society of America.
Chu-Cheng Lin, Waleed Ammar, Chris Dyer, and Lori
Levin. 2014. The CMU submission for the shared
task on language identification in code-switched
data. In Proceedings of the First Workshop on Com-
putational Approaches to Code-Switching, Doha,
Qatar, October. ACL.
D.C. Lyu, T.P. Tan, E. Chng, and H. Li. 2010. SEAME:
a Mandarin-English code-switching speech corpus
in South-East Asia. In INTERSPEECH, volume 10,
pages 1986?1989.
Dong Nguyen and A. Seza Do?gru?oz. 2013. Word level
language identification in online multilingual com-
munication. In Proceedings of the 2013 Conference
on Empirical Methods in Natural Language Process-
ing, pages 857?862, Seattle, Washington, USA, Oc-
tober. Association for Computational Linguistics.
Prajwol Shrestha. 2014. An incremental approach for
language identification in codeswitched text. In Pro-
ceedings of the First Workshop on Computational
Approaches to Code-Switching, Doha, Qatar, Octo-
ber. ACL.
Anil Kumar Singh and Jagadeesh Gorla. 2007. Identifi-
cation of languages and encodings in a multilingual
document. In Proceedings of ACL-SIGWAC?s Web
As Corpus3, Belgium.
Omar F. Zaidan and Chris Callison-Burch. 2011. The
Arabic online commentary dataset: An annotated
dataset of informal Arabic with high dialectal con-
tent. In Proceedings of the 49th Annual Meeting of
the Association for Computational Linguistics: Hu-
man Language Technologies: Short Papers - Volume
2, HLT ?11, pages 37?41, Stroudsburg, PA, USA.
Association for Computational Linguistics.
72
