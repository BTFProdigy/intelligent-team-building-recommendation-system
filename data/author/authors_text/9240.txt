Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the ACL, pages 584?592,
Boulder, Colorado, June 2009. c?2009 Association for Computational Linguistics
Using Citations to Generate Surveys of Scientific Paradigms
Saif Mohammad??, Bonnie Dorr???, Melissa Egan??, Ahmed Hassan?,
Pradeep Muthukrishan?, Vahed Qazvinian?, Dragomir Radev??, David Zajic?
Institute for Advanced Computer Studies? and Computer Science?, University of Maryland.
Human Language Technology Center of Excellence?. Center for Advanced Study of Language.
{saif,bonnie,mkegan,dmzajic}@umiacs.umd.edu
Department of Electrical Engineering and Computer Science?
School of Information?, University of Michigan.
{hassanam,mpradeep,vahed,radev}@umich.edu
Abstract
The number of research publications in var-
ious disciplines is growing exponentially.
Researchers and scientists are increasingly
finding themselves in the position of having
to quickly understand large amounts of tech-
nical material. In this paper we present the
first steps in producing an automatically gen-
erated, readily consumable, technical survey.
Specifically we explore the combination of
citation information and summarization tech-
niques. Even though prior work (Teufel et
al., 2006) argues that citation text is unsuitable
for summarization, we show that in the frame-
work of multi-document survey creation, cita-
tion texts can play a crucial role.
1 Introduction
In today?s rapidly expanding disciplines, scientists
and scholars are constantly faced with the daunting
task of keeping up with knowledge in their field. In
addition, the increasingly interconnected nature of
real-world tasks often requires experts in one dis-
cipline to rapidly learn about other areas in a short
amount of time.
Cross-disciplinary research requires scientists in
areas such as linguistics, biology, and sociology
to learn about computational approaches and appli-
cations, e.g., computational linguistics, biological
modeling, social networks. Authors of journal ar-
ticles and books must write accurate surveys of pre-
vious work, ranging from short summaries of related
research to in-depth historical notes.
Interdisciplinary review panels are often called
upon to review proposals in a wide range of areas,
some of which may be unfamiliar to panelists. Thus,
they must learn about a new discipline ?on the fly?
in order to relate their own expertise to the proposal.
Our goal is to effectively serve these needs by
combining two currently available technologies: (1)
bibliometric lexical link mining that exploits the
structure of citations and relations among citations;
and (2) summarization techniques that exploit the
content of the material in both the citing and cited
papers.
It is generally agreed upon that manually written
abstracts are good summaries of individual papers.
More recently, Qazvinian and Radev (2008) argue
that citation texts are useful in creating a summary
of the important contributions of a research paper.
The citation text of a target paper is the set of sen-
tences in other technical papers that explicitly refer
to it (Elkiss et al, 2008a). However, Teufel (2005)
argues that using citation text directly is not suitable
for document summarization.
In this paper, we compare and contrast the use-
fulness of abstracts and of citation text in automati-
cally generating a technical survey on a given topic
from multiple research papers. The next section pro-
vides the background for this work, including the
primary features of a technical survey and also the
types of input that are used in our study (full pa-
pers, abstracts, and citation texts). Following this,
we describe related work and point out the advances
of our work over previous work. We then describe
how citation texts are used as a new input for multi-
document summarization to produce surveys of a
given technical area. We apply four different sum-
marization techniques to data in the ACL Anthol-
584
ogy and evaluate our results using both automatic
(ROUGE) and human-mediated (nugget-based pyra-
mid) measures. We observe that, as expected, ab-
stracts are useful in survey creation, but, notably, we
also conclude that citation texts have crucial survey-
worthy information not present in (or at least, not
easily extractable from) abstracts. We further dis-
cover that abstracts are author-biased and thus com-
plementary to the broader perspective inherent in ci-
tation texts; these differences enable the use of a
range of different levels and types of information in
the survey?the extent of which is subject to survey
length restrictions (if any).
2 Background
Automatically creating technical surveys is sig-
nificantly distinct from that of traditional multi-
document summarization. Below we describe pri-
mary characteristics of a technical survey and we
present three types of input texts that we used for
the production of surveys.
2.1 Technical Survey
In the case of multi-document summarization, the
goal is to produce a readable presentation of mul-
tiple documents, whereas in the case of technical
survey creation, the goal is to convey the key fea-
tures of a particular field, basic underpinnings of the
field, early and late developments, important con-
tributions and findings, contradicting positions that
may reverse trends or start new sub-fields, and ba-
sic definitions and examples that enable rapid un-
derstanding of a field by non-experts.
A prototypical example of a technical survey is
that of ?chapter notes,? i.e., short (50?500 word)
descriptions of sub-areas found at the end of chap-
ters of textbook, such as Jurafsky and Martin (2008).
One might imagine producing such descriptions au-
tomatically, then hand-editing them and refining
them for use in an actual textbook.
We conducted a human analysis of these chapter
notes that revealed a set of conventions, an outline
of which is provided here (with example sentences
in italics):
1. Introductory/opening statement: The earliest
computational use of X was in Y, considered by
many to be the foundational work in this area.
2. Definitional follow up: X is def ined as Y.
3. Elaboration of definition (e.g., with an exam-
ple): Most early algorithms were based on Z.
4. Deeper elaboration, e.g., pointing out issues
with initial approaches: Unfortunately, this
model seems to be wrong.
5. Contrasting definition: Algorithms since then...
6. Introduction of additional specific instances /
historical background with citations: Two clas-
sic approaches are described in Q.
7. References to other summaries: R provides a
comprehensive guide to the details behind X.
The notion of text level categories or zoning
of technical papers?related to the survey compo-
nents enumerated above?has been investigated pre-
viously in the work of Nanba and Kan (2004b) and
Teufel (2002). These earlier works focused on the
analysis of scientific papers based on their rhetori-
cal structure and on determining the portions of pa-
pers that contain new results, comparisons to ear-
lier work, etc. The work described in this paper fo-
cuses on the synthesis of technical surveys based on
knowledge gleaned from rhetorical structure not un-
like that of the work of these earlier researchers, but
perhaps guided by structural patterns along the lines
of the conventions listed above.
Although our current approach to survey creation
does not yet incorporate a fully pattern-based com-
ponent, our ultimate objective is to apply these pat-
terns to guide the creation and refinement of the final
output. As a first step toward this goal, we use cita-
tion texts (closest in structure to the patterns iden-
tified by convention 7 above) to pick out the most
important content for survey creation.
2.2 Full papers, abstracts, and citation texts
Published research on a particular topic can be sum-
marized from two different kinds of sources: (1)
where an author describes her own work and (2)
where others describe an author?s work (usually in
relation to their own work). The author?s descrip-
tion of her own work can be found in her paper. How
others perceive her work is spread across other pa-
pers that cite her work. We will refer to the set of
sentences that explicitly mention a target paper Y as
the citation text of Y.
585
Traditionally, technical survey generation has
been tackled by summarizing a set of research pa-
pers pertaining to the topic. However, individual re-
search papers usually come with manually-created
?summaries??their abstracts. The abstract of a pa-
per may have sentences that set the context, state the
problem statement, mention how the problem is ap-
proached, and the bottom-line results?all in 200 to
500 words. Thus, using only the abstracts (instead
of full papers) as input to a summarization system is
worth exploring.
Whereas the abstract of a paper presents what the
authors think to be the important contributions of a
paper, the citation text of a paper captures what oth-
ers in the field perceive as the contributions of the
paper. The two perspectives are expected to have
some overlap in their content, but the citation text
also contains additional information not found in ab-
stracts (Elkiss et al, 2008a). For example, how a
particular methodology (described in one paper) was
combined with another (described in a different pa-
per) to overcome some of the drawbacks of each.
A citation text is also an indicator of what contri-
butions described in a paper were more influential
over time. Another distinguishing feature of citation
texts in contrast to abstracts is that a citation text
tends to have a certain amount of redundant informa-
tion. This is because multiple papers may describe
the same contributions of a target paper. This redun-
dancy can be exploited to determine the important
contributions of the target paper.
Our goal is to test the hypothesis that an ef-
fective technical survey will reflect information on
research not only from the perspective of its au-
thors but also from the perspective of others who
use/commend/discredit/add to it. Before describ-
ing our experiments with technical papers, abstracts,
and citation texts, we first summarize relevant prior
work that used these sources of information as input.
3 Related work
Previous work has focused on the analysis of cita-
tion and collaboration networks (Teufel et al, 2006;
Newman, 2001) and scientific article summarization
(Teufel and Moens, 2002). Bradshaw (2003) used
citation texts to determine the content of articles and
improve the results of a search engine. Citation
texts have also been used to create summaries of sin-
gle scientific articles in Qazvinian and Radev (2008)
and Mei and Zhai (2008). However, there is no pre-
vious work that uses the text of the citations to pro-
duce a multi-document survey of scientific articles.
Furthermore, there is no study contrasting the qual-
ity of surveys generated from citation summaries?
both automatically and manually produced?to sur-
veys generated from other forms of input such as the
abstracts or full texts of the source articles.
Nanba and Okumura (1999) discuss citation cate-
gorization to support a system for writing a survey.
Nanba et al (2004a) automatically categorize cita-
tion sentences into three groups using pre-defined
phrase-based rules. Based on this categorization a
survey generation tool is introduced in Nanba et al
(2004b). They report that co-citation (where both
papers are cited by many other papers) implies sim-
ilarity by showing that the textual similarity of co-
cited papers is proportional to the proximity of their
citations in the citing article.
Elkiss et al (2008b) conducted several exper-
iments on a set of 2,497 articles from the free
PubMed Central (PMC) repository.1 Results from
this experiment confirmed that the cohesion of a ci-
tation text of an article is consistently higher than
the that of its abstract. They also concluded that ci-
tation texts contain additional information are more
focused than abstracts.
Nakov et al (2004) use sentences surrounding ci-
tations to create training and testing data for seman-
tic analysis, synonym set creation, database cura-
tion, document summarization, and information re-
trieval. Kan et al (2002) use annotated bibliogra-
phies to cover certain aspects of summarization and
suggest using metadata and critical document fea-
tures as well as the prominent content-based features
to summarize documents. Kupiec et al (1995) use a
statistical method and show how extracts can be used
to create summaries but use no annotated metadata
in summarization.
Siddharthan and Teufel (2007) describe a new
reference task and show high human agreement as
well as an improvement in the performance of ar-
gumentative zoning (Teufel, 2005). In argumenta-
tive zoning?a rhetorical classification task?seven
1http://www.pubmedcentral.gov
586
classes (Own, Other, Background, Textual, Aim,
Basis, and Contrast) are used to label sentences ac-
cording to their role in the author?s argument.
Our aim is not only to determine the utility of cita-
tion texts for survey creation, but also to examine the
quality distinctions between this form of input and
others such as abstracts and full texts?comparing
the results to human-generated surveys using both
automatic and nugget-based pyramid evaluation
(Lin and Demner-Fushman, 2006; Nenkova and Pas-
sonneau, 2004; Lin, 2004).
4 Summarization systems
We used four summarization systems for our
survey-creation approach: Trimmer, LexRank, C-
LexRank, and C-RR. Trimmer is a syntactically-
motivated parse-and-trim approach. LexRank is a
graph-based similarity approach. C-LexRank and C-
RR use graph clustering (?C? stands for clustering).
We describe each of these, in turn, below.
4.1 Trimmer
Trimmer is a sentence-compression tool that extends
the scope of an extractive summarization system by
generating multiple alternative sentence compres-
sions of the most important sentences in target doc-
uments (Zajic et al, 2007). Trimmer compressions
are generated by applying linguistically-motivated
rules to mask syntactic components of a parse of a
source sentence. The rules can be applied iteratively
to compress sentences below a configurable length
threshold, or can be applied in all combinations to
generate the full space of compressions.
Trimmer can leverage the output of any con-
stituency parser that uses the Penn Treebank con-
ventions. At present, the Stanford Parser (Klein and
Manning, 2003) is used. The set of compressions
is ranked according to a set of features that may in-
clude metadata about the source sentences, details of
the compression process that generated the compres-
sion, and externally calculated features of the com-
pression.
Summaries are constructed from the highest scor-
ing compressions, using the metadata and maximal
marginal relevance (Carbonell and Goldstein, 1998)
to avoid redundancy and over-representation of a
single source.
4.2 LexRank
We also used LexRank (Erkan and Radev, 2004), a
state-of-the-art multidocument summarization sys-
tem, to generate summaries. LexRank first builds a
graph of all the candidate sentences. Two candidate
sentences are connected with an edge if the similar-
ity between them is above a threshold. We used co-
sine as the similarity metric with a threshold of 0.15.
Once the network is built, the system finds the most
central sentences by performing a random walk on
the graph.
The salience of a node is recursively defined on
the salience of adjacent nodes. This is similar to
the concept of prestige in social networks, where the
prestige of a person is dependent on the prestige of
the people he/she knows. However, since random
walk may get caught in cycles or in disconnected
components, we reserve a low probability to jump
to random nodes instead of neighbors (a technique
suggested by Langville and Meyer (2006)).
Note also that unlike the original PageRank
method, the graph of sentences is undirected. This
updated measure of sentence salience is called as
LexRank. The sentences with the highest LexRank
scores form the summary.
4.3 Cluster Summarizers: C-LexRank, C-RR
Two clustering methods proposed by Qazvinian and
Radev (2008)?C-RR and C-LexRank?were used
to create summaries. Both create a fully connected
network in which nodes are sentences and edges are
cosine similarities. A cutoff value of 0.1 is applied
to prune the graph and make a binary network. The
largest connected component of the network is then
extracted and clustered.
Both of the mentioned summarizers cluster the
network similarly but use different approaches to se-
lect sentences from different communities. In C-
RR sentences are picked from different clusters in
a round robin (RR) fashion. C-LexRank first calcu-
lates LexRank within each cluster to find the most
salient sentences of each community. Then it picks
the most salient sentence of each cluster, and then
the second most salient and so forth until the sum-
mary length limit is reached.
587
Most of work in QA and paraphrasing focused on folding paraphrasing knowledge into question analyzer or answer
locator Rinaldi et al 2003; Tomuro, 2003. In addition, number of researchers have built systems to take reading
comprehension examinations designed to evaluate children?s reading levels Charniak et al 2000; Hirschman et al
1999; Ng et al 2000; Riloff and Thelen, 2000; Wang et al 2000. so-called ? definition ? or ? other ?
questions at recent TREC evalua - tions Voorhees, 2005 serve as good examples. To better facilitate user
information needs, recent trends in QA research have shifted towards complex, context-based, and interactive
question answering Voorhees, 2001; Small et al 2003; Harabagiu et al 2005. [And so on.]
Table 1: First few sentences of the QA citation texts survey generated by Trimmer.
5 Data
The ACL Anthology is a collection of papers from
the Computational Linguistics journal, and proceed-
ings of ACL conferences and workshops. It has
almost 11,000 papers. To produce the ACL An-
thology Network (AAN), Joseph and Radev (2007)
manually parsed the references before automatically
compiling the network metadata, and generating ci-
tation and author collaboration networks. The AAN
includes all citation and collaboration data within
the ACL papers, with the citation network consist-
ing of 11,773 nodes and 38,765 directed edges.
Our evaluation experiments are on a set of papers
in the research area of Question Answering (QA)
and another set of papers on Dependency parsing
(DP). The two sets of papers were compiled by se-
lecting all the papers in AAN that had the words
Question Answering and Dependency Parsing, re-
spectively, in the title and the content. There were
10 papers in the QA set and 16 papers in the DP set.
We also compiled the citation texts for the 10 QA
papers and the citation texts for the 16 DP papers.
6 Experiments
We automatically generated surveys for both QA
and DP from three different types of documents: (1)
full papers from the QA and DP sets?QA and DP
full papers (PA), (2) only the abstracts of the QA
and DP papers?QA and DP abstracts (AB), and
(3) the citation texts corresponding to the QA and
DP papers?QA and DP citations texts (CT).
We generated twenty four (4x3x2) surveys,
each of length 250 words, by applying Trimmer,
LexRank, C-LexRank and C-RR on the three data
types (citation texts, abstracts, and full papers) for
both QA and DP. (Table 1 shows a fragment of one
of the surveys automatically generated from QA ci-
tation texts.) We created six (3x2) additional 250-
word surveys by randomly choosing sentences from
the citation texts, abstracts, and full papers of QA
and DP. We will refer to them as random surveys.
6.1 Evaluation
Our goal was to determine if citation texts do in-
deed have useful information that one will want to
put in a survey and if so, how much of this infor-
mation is not available in the original papers and
their abstracts. For this we evaluated each of the
automatically generated surveys using two separate
approaches: nugget-based pyramid evaluation and
ROUGE (described in the two subsections below).
Two sets of gold standard data were manually cre-
ated from the QA and DP citation texts and abstracts,
respectively:2 (1) We asked two impartial judges to
identify important nuggets of information worth in-
cluding in a survey. (2) We asked four fluent speak-
ers of English to create 250-word surveys of the
datasets. Then we determined how well the differ-
ent automatically generated surveys perform against
these gold standards. If the citation texts have only
redundant information with respect to the abstracts
and original papers, then the surveys of citation texts
will not perform better than others.
6.1.1 Nugget-Based Pyramid Evaluation
For our first approach we used a nugget-based
evaluation methodology (Lin and Demner-Fushman,
2006; Nenkova and Passonneau, 2004; Hildebrandt
et al, 2004; Voorhees, 2003). We asked three impar-
tial annotators (knowledgeable in NLP but not affil-
iated with the project) to review the citation texts
and/or abstract sets for each of the papers in the QA
and DP sets and manually extract prioritized lists
2Creating gold standard data from complete papers is fairly
arduous, and was not pursued.
588
of 2?8 ?nuggets,? or main contributions, supplied
by each paper. Each nugget was assigned a weight
based on the frequency with which it was listed by
annotators as well as the priority it was assigned
in each case. Our automatically generated surveys
were then scored based on the number and weight
of the nuggets that they covered. This evaluation ap-
proach is similar to the one adopted by Qazvinian
and Radev (2008), but adapted here for use in the
multi-document case.
The annotators had two distinct tasks for the QA
set, and one for the DP set: (1) extract nuggets for
each of the 10 QA papers, based only on the citation
texts for those papers; (2) extract nuggets for each
of the 10 QA papers, based only on the abstracts of
those papers; and (3) extract nuggets for each of the
16 DP papers, based only on the citation texts for
those papers.3
We obtained a weight for each nugget by revers-
ing its priority out of 8 (e.g., a nugget listed with
priority 1 was assigned a weight of 8) and summing
the weights over each listing of that nugget.4
To evaluate a given survey, we counted the num-
ber and weight of nuggets that it covered. Nuggets
were detected via the combined use of annotator-
provided regular expressions and careful human re-
view. Recall was calculated by dividing the com-
bined weight of covered nuggets by the combined
weight of all nuggets in the nugget set. Precision
was calculated by dividing the number of distinct
nuggets covered in a survey by the number of sen-
tences constituting that survey, with a cap of 1. F-
measure, the weighted harmonic mean of precision
and recall, was calculated with a beta value of 3 in
order to assign the greatest weight to recall. Recall
is favored because it rewards surveys that include
highly weighted (important) facts, rather than just a
3We first experimented using only the QA set. Then to show
that the results apply to other datasets, we asked human anno-
tators for gold standard data on the DP citation texts. Addi-
tional experiments on DP abstracts were not pursued because
this would have required additional human annotation effort to
establish a point we had already made with the QA set, i.e., that
abstracts are useful for survey creation.
4Results obtained with other weighting schemes that ig-
nored priority ratings and multiple mentions of a nugget by a
single annotator showed the same trends as the ones shown by
the selected weighting scheme, but the latter was a stronger dis-
tinguisher among the four systems.
Human Performance: Pyramid F-measureHuman1 Human2 Human3 Human4 Average
Input: QA citation surveysQA?CT nuggets 0.524 0.711 0.468 0.695 0.599QA?AB nuggets 0.495 0.606 0.423 0.608 0.533Input: QA abstract surveysQA?CT nuggets 0.542 0.675 0.581 0.669 0.617QA?AB nuggets 0.646 0.841 0.673 0.790 0.738Input: DP citation surveysDP?CT nuggets 0.245 0.475 0.378 0.555 0.413
Table 2: Pyramid F-measure scores of human-created
surveys of QA and DP data. The surveys are evaluated
using nuggets drawn from QA citation texts (QA?CT),
QA abstracts (QA?AB), and DP citation texts (DP?CT).
great number of facts.
Table 2 gives the F-measure values of the 250-
word surveys manually generated by humans. The
surveys were evaluated using the nuggets drawn
from the QA citation texts, QA abstracts, and DP ci-
tation texts. The average of their scores (listed in the
rightmost column) may be considered a good score
to aim for by the automatic summarization methods.
Table 3 gives the F-measure values of the surveys
generated by the four automatic summarizers, evalu-
ated using nuggets drawn from the QA citation texts,
QA abstracts, and DP citation texts. The table also
includes results for the baseline random summaries.
When we used the nuggets from the abstracts
set for evaluation, the surveys created from ab-
stracts scored higher than the corresponding surveys
created from citation texts and papers. Further, the
best surveys generated from citation texts outscored
the best surveys generated from papers. When we
used the nuggets from citation sets for evaluation,
the best automatic surveys generated from citation
texts outperform those generated from abstracts and
full papers. All these pyramid results demonstrate
that citation texts can contain useful information that
is not available in the abstracts or the original papers,
and that abstracts can contain useful information that
is not available in the citation texts or full papers.
Among the various automatic summarizers, Trim-
mer performed best at this task, in two cases ex-
ceeding the average human performance. Note also
that the random summarizer outscored the automatic
summarizers in cases where the nuggets were taken
from a source different from that used to generate
the survey. However, one or two summarizers still
tended to do well. This indicates a difficulty in ex-
589
System Performance: Pyramid F-measure
Random C-LexRank C-RR LexRank Trimmer
Input: QA citation surveys
QA?CT nuggets 0.321 0.434 0.268 0.295 0.616
QA?AB nuggets 0.305 0.388 0.349 0.320 0.543
Input: QA abstract surveys
QA?CT nuggets 0.452 0.383 0.480 0.441 0.404
QA?AB nuggets 0.623 0.484 0.574 0.606 0.622
Input: QA full paper surveys
QA?CT nuggets 0.239 0.446 0.299 0.190 0.199
QA?AB nuggets 0.294 0.520 0.387 0.301 0.290
Input: DP citation surveys
DP?CT nuggets 0.219 0.231 0.170 0.372 0.136
Input: DP abstract surveys
DP?CT nuggets 0.321 0.301 0.263 0.311 0.312
Input: DP full paper surveys
DP?CT nuggets 0.032 0.000 0.144 * 0.280
Table 3: Pyramid F-measure scores of automatic surveys of QA and DP data. The surveys are evaluated using nuggets
drawn from QA citation texts (QA?CT), QA abstracts (QA?AB), and DP citation texts (DP?CT).
* LexRank is computationally intensive and so was not run on the DP-PA dataset (about 4000 sentences).
Human Performance: ROUGE-2human1 human2 human3 human4 average
Input: QA citation surveysQA?CT refs. 0.1807 0.1956 0.0756 0.2019 0.1635QA?AB refs. 0.1116 0.1399 0.0711 0.1576 0.1201Input: QA abstract surveysQA?CT refs. 0.1315 0.1104 0.1216 0.1151 0.1197QA-AB refs. 0.2648 0.1977 0.1802 0.2544 0.2243Input: DP citation surveysDP?CT refs. 0.1550 0.1259 0.1200 0.1654 0.1416
Table 4: ROUGE-2 scores obtained for each of the manu-
ally created surveys by using the other three as reference.
ROUGE-1 and ROUGE-L followed similar patterns.
tracting the overlapping survey-worthy information
across the two sources.
6.1.2 ROUGE evaluation
Table 4 presents ROUGE scores (Lin, 2004) of
each of human-generated 250-word surveys against
each other. The average (last column) is what the au-
tomatic surveys can aim for. We then evaluated each
of the random surveys and those generated by the
four summarization systems against the references.
Table 5 lists ROUGE scores of surveys when the
manually created 250-word survey of the QA cita-
tion texts, survey of the QA abstracts, and the survey
of the DP citation texts, were used as gold standard.
When we use manually created citation text
surveys as reference, then the surveys gener-
ated from citation texts obtained significantly bet-
ter ROUGE scores than the surveys generated from
abstracts and full papers (p < 0.05) [RESULT 1].
This shows that crucial survey-worthy information
present in citation texts is not available, or hard to
extract, from abstracts and papers alone. Further,
the surveys generated from abstracts performed sig-
nificantly better than those generated from the full
papers (p < 0.05) [RESULT 2]. This shows that ab-
stracts and citation texts are generally denser in sur-
vey worthy information than full papers.
When we use manually created abstract sur-
veys as reference, then the surveys generated
from abstracts obtained significantly better ROUGE
scores than the surveys generated from citation texts
and full papers (p < 0.05) [RESULT 3]. Further, and
more importantly, the surveys generated from cita-
tion texts performed significantly better than those
generated from the full papers (p < 0.05) [RESULT
4]. Again, this shows that abstracts and citation texts
are richer in survey-worthy information. These re-
sults also show that abstracts of papers and citation
texts have some overlapping information (RESULT
2 and RESULT 4), but they also have a signifi-
cant amount of unique survey-worthy information
(RESULT 1 and RESULT 3).
Among the automatic summarizers, C-LexRank
and LexRank perform best. This is unlike the results
found through the nugget-evaluation method, where
Trimmer performed best. This suggests that Trim-
590
System Performance: ROUGE-2
Random C-LexRank C-RR LexRank Trimmer
Input: QA citation surveys
QA?CT refs. 0.11561 0.17013 0.09522 0.13501 0.16984
QA?AB refs. 0.08264 0.11653 0.07600 0.07013 0.10336
Input: QA abstract surveys
QA?CT refs. 0.04516 0.05892 0.06149 0.05369 0.04114
QA?AB refs. 0.12085 0.13634 0.12190 0.20311 0.13357
Input: QA full paper surveys
QA?CT refs. 0.03042 0.03606 0.03599 0.28244 0.03986
QA?AB refs. 0.04621 0.05901 0.04976 0.10540 0.07505
Input: DP citation surveys
DP?CT refs. 0.10690 0.13164 0.08748 0.04901 0.10052
Input: DP abstract surveys
DP?CT refs. 0.07027 0.07321 0.05318 0.20311 0.07176
Input: DP full paper surveys
DP?CT refs. 0.03770 0.02511 0.03433 * 0.04554
Table 5: ROUGE-2 scores of automatic surveys of QA and DP data. The surveys are evaluated by using human
references created from QA citation texts (QA?CT), QA abstracts (QA?AB), and DP citation texts (DP?CT). These
results are obtained after Jack-knifing the human references so that the values can be compared to those in Table 4.
* LexRank is computationally intensive and so was not run on the DP full papers set (about 4000 sentences).
mer is better at identifying more useful nuggets of
information, but C-LexRank and LexRank are bet-
ter at producing unigrams and bigrams expected in
a survey. To some extent this may be due to the fact
that Trimmer uses smaller (trimmed) fragments of
source sentences in its summaries.
7 Conclusion
In this paper, we investigated the usefulness of di-
rectly summarizing citation texts (sentences that cite
other papers) in the automatic creation of technical
surveys. We generated surveys of a set of Ques-
tion Answering (QA) and Dependency Parsing (DP)
papers, their abstracts, and their citation texts us-
ing four state-of-the-art summarization systems (C-
LexRank, C-RR, LexRank, and Trimmer). We then
used two separate approaches, nugget-based pyra-
mid and ROUGE, to evaluate the surveys. The re-
sults from both approaches and all four summa-
rization systems show that both citation texts and
abstracts have unique survey-worthy information.
These results also demonstrate that, unlike single
document summarization (where citing sentences
have been suggested to be inappropriate (Teufel
et al, 2006)), multidocument summarization?
especially technical survey creation?benefits con-
siderably from citation texts.
We next plan to generate surveys using both cita-
tion texts and abstracts together as input. Given the
overlapping content of abstracts and citation texts,
discovered in the current study, it is clear that re-
dundancy detection will be an integral component of
this future work. Creating readily consumable sur-
veys is a hard task, especially when using only raw
text and simple summarization techniques. There-
fore we intend to combine these summarization and
bibliometric techniques with suitable visualization
methods towards the creation of iterative technical
survey tools?systems that present surveys and bib-
liometric links in a visually convenient manner and
which incorporate user feedback to produce even
better surveys.
Acknowledgments
This work was supported, in part, by the National
Science Foundation under Grant No. IIS-0705832
(iOPENER: Information Organization for PENning
Expositions on Research) and Grant No. 0534323
(Collaborative Research: BlogoCenter - Infrastruc-
ture for Collecting, Mining and Accessing Blogs),
in part, by the Human Language Technology Cen-
ter of Excellence, and in part, by the Center for Ad-
vanced Study of Language (CASL). Any opinions,
findings, and conclusions or recommendations ex-
pressed in this material are those of the authors and
do not necessarily reflect the views of the sponsors.
591
References
Shannon Bradshaw. 2003. Reference directed indexing:
Redeeming relevance for subject search in citation in-
dexes. In Proceedings of the 7th European Conference
on Research and Advanced Technology for Digital Li-
braries.
Jaime G. Carbonell and Jade Goldstein. 1998. The use
of mmr, diversity-based reranking for reordering doc-
uments and producing summaries. In Proceedings of
21st Annual International ACM SIGIR Conference on
Research and Development in Information Retrieval,
pages 335?336, Melbourne, Australia.
Aaron Elkiss, Siwei Shen, Anthony Fader, Gu?nes? Erkan,
David States, and Dragomir R. Radev. 2008a. Blind
men and elephants: What do citation summaries tell
us about a research article? Journal of the Ameri-
can Society for Information Science and Technology,
59(1):51?62.
Aaron Elkiss, Siwei Shen, Anthony Fader, Gu?nes? Erkan,
David States, and Dragomir R. Radev. 2008b. Blind
men and elephants: What do citation summaries tell
us about a research article? Journal of the Ameri-
can Society for Information Science and Technology,
59(1):51?62.
Gu?nes? Erkan and Dragomir R. Radev. 2004. Lexrank:
Graph-based centrality as salience in text summariza-
tion. Journal of Artificial Intelligence Research.
Wesley Hildebrandt, Boris Katz, and Jimmy Lin. 2004.
Overview of the trec 2003 question-answering track.
In Proceedings of the 2004 Human Language Tech-
nology Conference and the North American Chapter
of the Association for Computational Linguistics An-
nual Meeting (HLT/NAACL 2004).
Mark Joseph and Dragomir Radev. 2007. Citation analy-
sis, centrality, and the ACL Anthology. Technical Re-
port CSE-TR-535-07, University of Michigan. Dept.
of Electrical Engineering and Computer Science.
Daniel Jurafsky and James H. Martin. 2008. Speech
and Language Processing: An Introduction to Natural
Language Processing, Speech Recognition, and Com-
putational Linguistics (2nd edition). Prentice-Hall.
Min-Yen Kan, Judith L. Klavans, and Kathleen R. McK-
eown. 2002. Using the Annotated Bibliography as a
Resource for Indicative Summarization. In Proceed-
ings of LREC 2002, Las Palmas, Spain.
Dan Klein and Christopher D. Manning. 2003. Accu-
rate unlexicalized parsing. In Proceedings of the 41st
Meeting of ACL, pages 423?430.
Julian Kupiec, Jan Pedersen, and Francine Chen. 1995.
A trainable document summarizer. In SIGIR ?95,
pages 68?73, New York, NY, USA. ACM.
Amy Langville and Carl Meyer. 2006. Google?s PageR-
ank and Beyond: The Science of Search Engine Rank-
ings. Princeton University Press.
Jimmy J. Lin and Dina Demner-Fushman. 2006. Meth-
ods for automatically evaluating answers to complex
questions. Information Retrieval, 9(5):565?587.
Chin-Yew Lin. 2004. Rouge: A package for automatic
evaluation of summaries. In Proceedings of the ACL
workshop on Text Summarization Branches Out.
Qiaozhu Mei and ChengXiang Zhai. 2008. Generating
impact-based summaries for scientific literature. In
Proceedings of ACL ?08, pages 816?824.
Preslav I. Nakov, Schwartz S. Ariel, and Hearst A. Marti.
2004. Citances: Citation sentences for semantic anal-
ysis of bioscience text. In Workshop on Search and
Discovery in Bioinformatics.
Hidetsugu Nanba and Manabu Okumura. 1999. Towards
multi-paper summarization using reference informa-
tion. In IJCAI1999, pages 926?931.
Hidetsugu Nanba, Takeshi Abekawa, Manabu Okumura,
and Suguru Saito. 2004a. Bilingual presri: Integration
of multiple research paper databases. In Proceedings
of RIAO 2004, pages 195?211, Avignon, France.
Hidetsugu Nanba, Noriko Kando, and Manabu Okumura.
2004b. Classification of research papers using cita-
tion links and citation types: Towards automatic re-
view article generation. In Proceedings of the 11th
SIG Classification Research Workshop, pages 117?
134, Chicago, USA.
Ani Nenkova and Rebecca Passonneau. 2004. Evaluat-
ing content selection in summarization: The pyramid
method. Proceedings of the HLT-NAACL conference.
Mark E. J. Newman. 2001. The structure of scientific
collaboration networks. PNAS, 98(2):404?409.
Vahed Qazvinian and Dragomir R. Radev. 2008. Scien-
tific paper summarization using citation summary net-
works. In COLING 2008, Manchester, UK.
Advaith Siddharthan and Simone Teufel. 2007. Whose
idea was this, and why does it matter? attribut-
ing scientific work to citations. In Proceedings of
NAACL/HLT-07.
Simone Teufel and Marc Moens. 2002. Summariz-
ing scientific articles: experiments with relevance and
rhetorical status. Comput. Linguist., 28(4):409?445.
Simone Teufel, Advaith Siddharthan, and Dan Tidhar.
2006. Automatic classification of citation function. In
Proceedings of EMNLP, pages 103?110, Australia.
Simone Teufel. 2005. Argumentative Zoning for Im-
proved Citation Indexing. Computing Attitude and Af-
fect in Text: Theory and Applications, pages 159?170.
Ellen M. Voorhees. 2003. Overview of the trec 2003
question answering track. In Proceedings of the
Twelfth Text Retrieval Conference (TREC 2003).
David M. Zajic, Bonnie J. Dorr, Jimmy Lin, and Richard
Schwartz. 2007. Multi-candidate reduction: Sentence
compression as a tool for document summarization
tasks. Information Processing and Management (Spe-
cial Issue on Summarization).
592
Hedge Trimmer: A Parse-and-Trim Approach to Headline Generation 
Bonnie Dorr, David Zajic 
University of Maryland 
bonnie, dmzajic@umiacs.umd.edu 
Richard Schwartz 
BBN 
schwartz@bbn.com 
 
 
Abstract 
This paper presents Hedge Trimmer, a HEaDline 
GEneration system that creates a headline for a newspa-
per story using linguistically-motivated heuristics to 
guide the choice of a potential headline.  We present 
feasibility tests used to establish the validity of an ap-
proach that constructs a headline by selecting words in 
order from a story.  In addition, we describe experimen-
tal results that demonstrate the effectiveness of our lin-
guistically-motivated approach over a HMM-based 
model, using both human evaluation and automatic met-
rics for comparing the two approaches. 
1 Introduction 
 In this paper we present Hedge Trimmer, a HEaD-
line GEneration system that creates a headline for a 
newspaper story by removing constituents from a parse 
tree of the first sentence until a length threshold has 
been reached.  Linguistically-motivated heuristics guide 
the choice of which constituents of a story should be 
preserved, and which ones should be deleted.  Our focus 
is on headline generation for English newspaper texts, 
with an eye toward the production of document surro-
gates?for cross-language information retrieval?and 
the eventual generation of readable headlines from 
speech broadcasts. 
In contrast to original newspaper headlines, which 
are often intended only to catch the eye, our approach 
produces informative abstracts describing the main 
theme or event of the newspaper article.    We claim that 
the construction of informative abstracts requires access 
to deeper linguistic knowledge, in order to make sub-
stantial improvements over purely statistical ap-
proaches. 
In this paper, we present our technique for produc-
ing headlines using a parse-and-trim approach based on 
the BBN Parser. As described in Miller et al (1998), the 
BBN parser builds augmented parse trees according to a 
process similar to that   described in Collins (1997).  
The BBN parser has been used successfully for the task 
of information extraction in the SIFT system (Miller et 
al., 2000). 
The next section presents previous work in the area 
of automatic generation of abstracts.  Following this, we 
present feasibility tests used to establish the validity of 
an approach that constructs headlines from words in a 
story, taken in order and focusing on the earlier part of 
the story.  Next, we describe the application of the 
parse-and-trim approach to the problem of headline 
generation.  We discuss the linguistically-motivated 
heuristics we use to produce results that are headline-
like.  Finally, we evaluate Hedge Trimmer by compar-
ing it to our earlier work on headline generation, a prob-
abilistic model for automatic headline generation (Zajic 
et al 2002).  In this paper we will refer to this statistical 
system as HMM Hedge  We demonstrate the effective-
ness of our linguistically-motivated approach, Hedge 
Trimmer, over the probabilistic model, HMM Hedge, 
using both human evaluation and automatic metrics. 
2 Previous Work 
 Other researchers have investigated the topic of 
automatic generation of abstracts, but the focus has been 
different, e.g., sentence extraction (Edmundson, 1969; 
Johnson et al 1993; Kupiec et al, 1995; Mann et al, 
1992; Teufel and Moens, 1997; Zechner, 1995), proc-
essing of structured templates (Paice and Jones, 1993), 
sentence compression (Hori et al, 2002; Knight and 
Marcu, 2001; Grefenstette, 1998, Luhn, 1958), and gen-
eration of abstracts from multiple sources (Radev and 
McKeown, 1998).  We focus instead on the construction 
of headline-style abstracts from a single story. 
 Headline generation can be viewed as analogous to 
statistical machine translation, where a concise docu-
ment is generated from a verbose one using a Noisy 
Channel Model and the Viterbi search to select the most 
likely summarization.  This approach has been explored 
in (Zajic et al, 2002) and (Banko et al, 2000). 
 The approach we use in Hedge is most similar to 
that of (Knight and Marcu, 2001), where a single sen-
tence is shortened using statistical compression. As in 
this work, we select headline words from story words in 
the order that they appear in the story?in particular, the 
first sentence of the story.  However, we use linguisti-
cally motivated heuristics for shortening the sentence; 
there is no statistical model, which means we do not 
require any prior training on a large corpus of 
story/headline pairs. 
 Linguistically motivated heuristics have been used 
by (McKeown et al 2002) to distinguish constituents of 
parse trees which can be removed without affecting 
grammaticality or correctness.  GLEANS (Daum? et al 
2002) uses parsing and named entity tagging to fill val-
ues in headline templates. 
 Consider the following excerpt from a news story: 
 
(1) Story Words: Kurdish guerilla forces moving 
with lightning speed poured into Kirkuk today 
immediately after Iraqi troops, fleeing relent-
less U.S. airstrikes, abandoned the hub of Iraq?s 
rich northern oil fields. 
 
Generated Headline: Kurdish guerilla forces 
poured into Kirkuk after Iraqi troops abandoned 
oil fields. 
 
 In this case, the words in bold form a fluent and 
accurate headline for the story.  Italicized words are 
deleted based on information provided in a parse-tree 
representation of the sentence.   
3 Feasibility Testing 
 Our approach is based on the selection of words 
from the original story, in the order that they appear in 
the story, and allowing for morphological variation.  To 
determine the feasibility of our headline-generation ap-
proach, we first attempted to apply our ?select-words-
in-order? technique by hand.  We asked two subjects to 
write headline headlines for 73 AP stories from the 
TIPSTER corpus for January 1, 1989, by selecting 
words in order from the story.  Of the 146 headlines, 2 
did not meet the ?select-words-in-order? criteria be-
cause of accidental word reordering.  We found that at 
least one fluent and accurate headline meeting the crite-
ria was created for each of the stories.  The average 
length of the headlines was 10.76 words. 
 Later we examined the distribution of the headline 
words among the sentences of the stories, i.e. how many 
came from the first sentence of a story, how many from 
the second sentence, etc.  The results of this study are 
shown in Figure 1.  We observe that 86.8% of the head-
line words were chosen from the first sentence of their 
stories.  We performed a subsequent study in which two 
subjects created 100 headlines for 100 AP stories from 
August 6, 1990.  51.4% of the headline words in the 
second set were chosen from the first sentence.  The 
distribution of headline words for the second set shown 
in Figure 2. 
 Although humans do not always select headline 
words from the first sentence, we observe that a large 
percentage of headline words are often found in the first 
sentence. 
0.00%
10.00%
20.00%
30.00%
40.00%
50.00%
60.00%
70.00%
80.00%
90.00%
100.00%
N=
1
N=
2
N=
3
N=
4
N=
5
N=
6
N=
7
N=
8
N=
9
N>
=
10
 
Figure 1: Percentage of words from human-generated 
headlines drawn from Nth sentence of story (Set 1) 
0.00%
10.00%
20.00%
30.00%
40.00%
50.00%
60.00%
N=
1
N=
2
N=
3
N=
4
N=
5
N=
6
N=
7
N=
8
N=
9
N>
=
10
 
Figure 2:  Percentage of words from human-generated head-
lines drawn from Nth sentence of story (Set 2) 
 
4 Approach 
The input to Hedge is a story, whose first sentence is 
immediately passed through the BBN parser.  The 
parse-tree result serves as input to a linguistically-
motivated module that selects story words to form head-
lines based on key insights gained from our observa-
tions of human-constructed headlines.  That is, we 
conducted a human inspection of the 73 TIPSTER sto-
ries mentioned in Section 3 for the purpose of develop-
ing the Hedge Trimmer algorithm. 
 Based on our observations of human-produced 
headlines, we developed the following algorithm for 
parse-tree trimming: 
 
1. Choose lowest leftmost S with NP,VP 
2. Remove low content units 
o some determiners 
o time expressions 
3. Iterative shortening: 
o XP Reduction 
o Remove preposed adjuncts 
o Remove trailing PPs 
o Remove trailing SBARs 
 
 More recently, we conducted an automatic analysis 
of the human-generated headlines that supports several 
of the insights gleaned from this initial study. We parsed 
218 human-produced headlines using the BBN parser 
and analyzed the results. For this analysis, we used 72 
headlines produced by a third participant.1 The parsing 
results included 957 noun phrases (NP) and 315 clauses 
(S).  
 We calculated percentages based on headline-level, 
NP-level, and Sentence-level structures in the parsing 
results.  That is, we counted: 
 
? The percentage of the 957 NPs containing de-
terminers and relative clauses 
? The percentage of the 218 headlines containing 
preposed adjuncts and conjoined S or VPs 
? The percentage of the 315 S nodes containing 
trailing time expressions, SBARs, and PPs 
 
Figure 3 summarizes the results of this automatic analy-
sis.  In our initial human inspection, we considered each 
of these categories to be reasonable candidates for dele-
tion in our parse tree and this automatic analysis indi-
cates that we have made reasonable choices for deletion, 
with the possible exception of trailing PPs, which show 
up in over half of the human-generated headlines.  This 
suggests that we should proceed with caution with re-
spect to the deletion of trailing PPs; thus we consider 
this to be an option only if no other is available. 
 
HEADLINE-LEVEL PERCENTAGES 
preposed adjuncts = 0/218 (0%)  
conjoined S = 1/218 ( .5%) 
conjoined VP = 7/218 (3%) 
NP-LEVEL PERCENTAGES 
relative clauses = 3/957 (.3%)  
determiners = 31/957 (3%); of these, only 
16 were ?a? or ?the? (1.6% overall) 
S-LEVEL PERCENTAGES2 
time expressions = 5/315 (1.5%) 
trailing PPs = 165/315 (52%) 
trailing SBARs = 24/315 (8%) 
Figure 3: Percentages found in human-generated headlines 
                                                          
1
 No response was given for one of the 73 stories. 
2
 Trailing constituents (SBARs and PPs) are computed by 
counting the number of SBARs (or PPs) not designated as an 
argument of (contained in) a verb phrase. 
 For a comparison, we conducted a second analysis 
in which we used the same parser on just the first sen-
tence of each of the 73 stories.  In this second analysis, 
the parsing results included 817 noun phrases (NP) and 
316 clauses (S).  A summary of these results is shown in 
Figure 4.  Note that, across the board, the percentages 
are higher in this analysis than in the results shown in 
Figure 3 (ranging from 12% higher?in the case of trail-
ing PPs?to 1500% higher in the case of time expres-
sions), indicating that our choices of deletion in the 
Hedge Trimmer algorithm are well-grounded. 
 
HEADLINE-LEVEL PERCENTAGES 
preposed adjuncts = 2/73 (2.7%) 
conjoined S = 3/73 (4%) 
conjoined VP = 20/73 (27%) 
NP-LEVEL PERCENTAGES 
relative clauses = 29/817 (3.5%)  
determiners = 205/817 (25%); of these, 
only 171 were ?a? or ?the? (21% overall) 
S-LEVEL PERCENTAGES 
time expressions = 77/316 (24%) 
trailing PPs = 184/316 (58%) 
trailing SBARs =  49/316 (16%) 
Figure 4: Percentages found in first sentence of 
each story. 
4.1 Choose the Correct S Node 
 The first step relies on what is referred to as the 
Projection Principle in linguistic theory (Chomsky, 
1981): Predicates project a subject (both dominated by 
S) in the surface structure.  Our human-generated head-
lines always conformed to this rule; thus, we adopted it 
as a constraint in our algorithm. 
 An example of the application of step 1 above is 
the following, where boldfaced material from the parse 
tree representation is retained and italicized material is 
eliminated: 
 
(2) Input: Rebels agree to talks with government of-
ficials said Tuesday. 
 
Parse: [S [S [NP Rebels] [VP agree to talks 
with government]] officials said Tuesday.] 
 
Output of step 1: Rebels agree to talks with gov-
ernment. 
 
When the parser produces a correct tree, this step pro-
vides a grammatical headline.  However, the parser of-
ten produces an incorrect output.  Human inspection of 
our 624-sentence DUC-2003 evaluation set revealed 
that there were two such scenarios, illustrated by the 
following cases:  
 
(3) [S [SBAR What started as a local contro-
versy] [VP has evolved into an international 
scandal.]] 
 
(4) [NP [NP Bangladesh] [CC and] [NP [NP In-
dia] [VP signed a water sharing accord.]]] 
  
In the first case, an S exists, but it does not conform 
to the requirements of step 1.  This occurred in 2.6% of 
the sentences in the DUC-2003 evaluation data.  We 
resolve this by selecting the lowest leftmost S, i.e., the 
entire string ?What started as a local controversy has 
evolved into an international scandal? in the example 
above. 
In the second case, there is no S available.  This oc-
curred in 3.4% of the sentences in the evaluation data.  
We resolve this by selecting the root of the parse tree; 
this would be the entire string ?Bangladesh and India 
signed a water sharing accord? above.  No other parser 
errors were encountered in the DUC-2003 evaluation 
data. 
4.2 Removal of Low Content Nodes  
 Step 2 of our algorithm eliminates low-content 
units.  We start with the simplest low-content units: the 
determiners a and the.  Other determiners were not con-
sidered for deletion because our analysis of the human-
constructed headlines revealed that most of the other 
determiners provide important information, e.g., nega-
tion (not), quantifiers (each, many, several), and deictics 
(this, that). 
 Beyond these, we found that the human-generated 
headlines contained very few time expressions which, 
although certainly not content-free, do not contribute 
toward conveying the overall ?who/what content? of the 
story.  Since our goal is to provide an informative head-
line (i.e., the action and its participants), the identifica-
tion and elimination of time expressions provided a 
significant boost in the performance of our automatic 
headline generator. 
 We identified time expressions in the stories using 
BBN?s IdentiFinder? (Bikel et al 1999). We imple-
mented the elimination of time expressions as a two-
step process: 
 
? Use IdentiFinder to mark time expressions 
? Remove [PP ? [NP [X] ?] ?] and [NP [X]] 
where X is tagged as part of a time expression 
 
The following examples illustrate the application of 
this step: 
 
(5) Input: The State Department on Friday lifted the 
ban it had imposed on foreign fliers. 
 
Parse:  [Det The] State Department [PP [IN 
on] [NP [NNP Friday]]] lifted [Det the] ban it 
had imposed on foreign fliers.  
 
Output of step 2: State Department lifted ban it 
has imposed on foreign fliers. 
 
(6) Input: An international relief agency announced 
Wednesday that it is withdrawing from North 
Korea. 
 
Parse:  [Det An] international relief agency an-
nounced [NP [NNP Wednesday]] that it is with-
drawing from North Korea. 
 
Output of step 2: International relief agency an-
nounced that it is withdrawing from North Korea. 
 
 We found that 53.2% of the stories we examined 
contained at least one time expression which could be 
deleted.  Human inspection of the 50 deleted time ex-
pressions showed that 38 were desirable deletions, 10 
were locally undesirable because they introduced an 
ungrammatical fragment,3 and 2 were undesirable be-
cause they removed a potentially relevant constituent.  
However, even an undesirable deletion often pans out 
for two reasons: (1) the ungrammatical fragment is fre-
quently deleted later by some other rule; and (2) every 
time a constituent is removed it makes room under the 
threshold for some other, possibly more relevant con-
stituent.  Consider the following examples. 
 
(7) At least two people were killed Sunday. 
 
(8) At least two people were killed when single-
engine airplane crashed. 
 
Example (7) was produced by a system which did not 
remove time expressions.  Example (8) shows that if the 
time expression Sunday were removed, it would make 
room below the 10-word threshold for another impor-
tant piece of information. 
4.3 Iterative Shortening 
 The final step, iterative shortening, removes lin-
guistically peripheral material?through successive de-
letions?until the sentence is shorter than a given 
threshold.  We took the threshold to be 10 for the DUC 
task, but it is a configurable parameter.  Also, given that 
the human-generated headlines tended to retain earlier 
material more often than later material, much of our 
                                                          
3
 Two examples of genuinely undesirable time expression deletion 
are: 
? The attack came on the heels of [New Year?s Day]. 
? [New Year?s Day] brought a foot of snow to the region. 
iterative shortening is focused on deleting the rightmost 
phrasal categories until the length is below threshold. 
 There are four types of iterative shortening rules. 
The first type is a rule we call ?XP-over-XP,? which is 
implemented as follows: 
 
In constructions of the form [XP [XP ?] ?] re-
move the other children of the higher XP, where 
XP is NP, VP or S. 
 
This is a linguistic generalization that allowed us apply 
a single rule to capture three different phenomena (rela-
tive clauses, verb-phrase conjunction, and sentential 
conjunction).   The rule is applied iteratively, from the 
deepest rightmost applicable node backwards, until the 
length threshold is reached. 
The impact of XP-over-XP can be seen in these ex-
amples of NP-over-NP (relative clauses), VP-over-VP 
(verb-phrase conjunction), and S-over-S (sentential con-
junction), respectively: 
 
(9) Input: A fire killed a firefighter who was fatally 
injured as he searched the house. 
 
Parse:  [S [Det A] fire killed [Det a]  [NP [NP 
firefighter] [SBAR who was fatally injured as 
he searched the house] ]] 
 
Output of NP-over-NP: fire killed firefighter 
 
(10) Input: Illegal fireworks injured hundreds of peo-
ple and started six fires.  
 
Parse:  [S Illegal fireworks [VP [VP injured 
hundreds of people] [CC and] [VP started six 
fires] ]] 
 
Output of VP-over-VP: Illegal fireworks injured 
hundreds of people 
 
(11) Input: A company offering blood cholesterol 
tests in grocery stores says medical technology 
has outpaced state laws, but the state says the 
company doesn?t have the proper licenses. 
 
Parse:  [S [Det A] company offering blood cho-
lesterol tests in grocery stores says [S [S medi-
cal technology has outpaced state laws], [CC 
but] [S [Det the] state stays [Det the] company 
doesn?t have [Det the] proper licenses.]] ]  
 
Output of S-over-S: Company offering blood 
cholesterol tests in grocery store says medical 
technology has outpaced state laws 
 
 The second type of iterative shortening is the re-
moval of preposed adjuncts.  The motivation for this 
type of shortening is that all of the human-generated 
headlines ignored what we refer to as the preamble of 
the story.  Assuming the Projection principle has been 
satisfied, the preamble is viewed as the phrasal material 
occurring before the subject of the sentence. Thus, ad-
juncts are identified linguistically as any XP unit pre-
ceding the first NP (the subject) under the S chosen by 
step 1. This type of phrasal modifier is invisible to the 
XP-over-XP rule, which deletes material under a node 
only if it dominates another node of the same phrasal 
category. 
 The impact of this type of shortening can be seen in 
the following example:  
 
(12) Input: According to a now finalized blueprint de-
scribed by U.S. officials and other sources, the 
Bush administration plans to take complete, unilat-
eral control of a post-Saddam Hussein Iraq 
 
Parse:  [S [PP According to a now-finalized blue-
print described by U.S. officials and other sources] 
[Det the] Bush administration plans to take 
complete, unilateral control of [Det a] post-
Saddam Hussein Iraq ]  
 
Output of Preposed Adjunct Removal: Bush ad-
ministration plans to take complete unilateral con-
trol of post-Saddam Hussein Iraq 
 
 The third and fourth types of iterative shortening 
are the removal of trailing PPs and SBARs, respec-
tively: 
 
? Remove PPs from deepest rightmost node back-
ward until length is below threshold. 
? Remove SBARs from deepest rightmost node 
backward until length is below threshold. 
 
  These are the riskiest of the iterative shortening rules, 
as indicated in our analysis of the human-generated 
headlines.  Thus, we apply these conservatively, only 
when there are no other categories of rules to apply.  
Moreover, these rules are applied with a backoff option 
to avoid over-trimming the parse tree. First the PP 
shortening rule is applied.  If the threshold has been 
reached, no more shortening is done.  However, if the 
threshold has not been reached, the system reverts to the 
parse tree as it was before any PPs were removed, and 
applies the SBAR shortening rule.  If the threshold still 
has not been reached, the PP rule is applied to the result 
of the SBAR rule.   
 Other sequences of shortening rules are possible.  
The one above was observed to produce the best results 
on a 73-sentence development set of stories from the 
TIPSTER corpus.  The intuition is that, when removing 
constituents from a parse tree, it?s best to remove 
smaller portions during each iteration, to avoid produc-
ing trees with undesirably few words.  PPs tend to rep-
resent small parts of the tree while SBARs represent 
large parts of the tree.  Thus we try to reach the thresh-
old by removing small constituents, but if we can?t 
reach the threshold that way, we restore the small con-
stituents, remove a large constituent and resume the 
deletion of small constituents. 
The impact of these two types of shortening can be 
seen in the following examples:  
 
(13) Input: More oil-covered sea birds were found 
over the weekend. 
 
Parse:  [S More oil-covered sea birds were 
found [PP over the weekend]]     
 
Output of PP Removal: More oil-covered sea 
birds were found. 
 
(14) Input: Visiting China Interpol chief expressed 
confidence in Hong Kong?s smooth transition 
while assuring closer cooperation after Hong 
Kong returns.  
 
Parse:  [S Visiting China Interpol chief ex-
pressed confidence in Hong Kong?s smooth 
transition [SBAR while assuring closer coopera-
tion after Hong Kong returns]]   
 
Output of SBAR Removal: Visiting China Inter-
pol chief expressed confidence in Hong Kong?s 
smooth transition 
 
5 Evaluation 
We conducted two evaluations.  One was an informal 
human assessment and one was a formal automatic 
evaluation.  
5.1 HMM Hedge 
We compared our current system to a statistical 
headline generation system we presented at the 2001 
DUC Summarization Workshop (Zajic et al, 2002), 
which we will refer to as HMM Hedge.  HMM Hedge 
treats the summarization problem as analogous to statis-
tical machine translation.  The verbose language, arti-
cles, is treated as the result of a concise language, 
headlines, being transmitted through a noisy channel.  
The result of the transmission is that extra words are 
added and some morphological variations occur.  The 
Viterbi algorithm is used to calculate the most likely 
unseen headline to have generated the seen article.  The 
Viterbi algorithm is biased to favor headline-like char-
acteristics gleaned from observation of human perform-
ance of the headline-construction task.  Since the 2002 
Workshop, HMM Hedge has been enhanced by incorpo-
rating part of speech of information into the decoding 
process, rejecting headlines that do not contain a word 
that was used as a verb in the story, and allowing mor-
phological variation only on words that were used as 
verbs in the story.  HMM Hedge was trained on 700,000 
news articles and headlines from the TIPSTER corpus. 
5.2 Bleu: Automatic Evaluation 
 
BLEU (Papineni et al 2002) is a system for auto-
matic evaluation of machine translation.  BLEU uses a 
modified n-gram precision measure to compare machine 
translations to reference human translations.  We treat 
summarization as a type of translation from a verbose 
language to a concise one, and compare automatically 
generated headlines to human generated headlines. 
For this evaluation we used 100 headlines created 
for 100 AP stories from the TIPSTER collection for 
August 6, 1990 as reference summarizations for those 
stories.  These 100 stories had never been run through 
either system or evaluated by the authors prior to this 
evaluation.  We also used the 2496 manual abstracts for 
the DUC2003 10-word summarization task as reference 
translations for the 624 test documents of that task.  We 
used two variants of HMM Hedge, one which selects 
headline words from the first 60 words of the story, and 
one which selects words from the first sentence of the 
story.  Table 1 shows the BLEU score using trigrams, 
and the 95% confidence interval for the score. 
 
 AP900806 DUC2003 
HMM60 0.0997 ? 0.0322 
avg len: 8.62 
0.1050 ? 0.0154 
avg len: 8.54 
HMM1Sent 0.0998 ? 0.0354 
avg len: 8.78 
0.1115 ? 0.0173 
avg len: 8.95 
HedgeTr 0.1067 ? 0.0301 
avg len: 8.27 
0.1341 ? 0.0181 
avg len: 8.50 
Table 1 
These results show that although Hedge Trimmer 
scores slightly higher than HMM Hedge on both data 
sets, the results are not statistically significant.  How-
ever, we believe that the difference in the quality of the 
systems is not adequately reflected by this automatic 
evaluation. 
5.3 Human Evaluation 
Human evaluation indicates significantly higher 
scores than might be guessed from the automatic 
evaluation.  For the 100 AP stories from the TIPSTER 
corpus for August 6, 1990, the output of Hedge Trim-
mer and HMM Hedge was evaluated by one human.  
Each headline was given a subjective score from 1 to 5, 
with 1 being the worst and 5 being the best.  The aver-
age score of HMM Hedge was 3.01 with standard devia-
tion of 1.11.  The average score of Hedge Trimmer was 
3.72 with standard deviation of 1.26.  Using a t-score, 
the difference is significant with greater than 99.9% 
confidence. 
The types of problems exhibited by the two systems are 
qualitatively different.  The probabilistic system is more 
likely to produce an ungrammatical result or omit a nec-
essary argument, as in the examples below. 
 
(15) HMM60: Nearly drowns in satisfactory condi-
tion satisfactory condition. 
 
(16) HMM60: A county jail inmate who noticed. 
 
 In contrast, the parser-based system is more likely 
to fail by producing a grammatical but semantically 
useless headline. 
 
(17) HedgeTr:  It may not be everyone?s idea espe-
cially coming on heels. 
 
 Finally, even when both systems produce accept-
able output, Hedge Trimmer usually produces headlines 
which are more fluent or include more useful informa-
tion. 
 
(18)   a. HMM60:  New Year?s eve capsizing 
 b. HedgeTr:  Sightseeing cruise boat capsized 
and sank. 
 
(19)   a. HMM60:  hundreds of Tibetan students 
demonstrate in Lhasa. 
 b. HedgeTr:  Hundreds demonstrated in Lhasa 
demanding that Chinese authorities respect cul-
ture. 
6 Conclusions and Future Work 
 We have shown the effectiveness of constructing 
headlines by selecting words in order from a newspaper 
story.  The practice of selecting words from the early 
part of the document has been justified by analyzing the 
behavior of humans doing the task, and by automatic 
evaluation of a system operating on a similar principle. 
 We have compared two systems that use this basic 
technique, one taking a statistical approach and the 
other a linguistic approach.  The results of the linguisti-
cally motivated approach show that we can build a 
working system with minimal linguistic knowledge and 
circumvent the need for large amounts of training data.  
We should be able to quickly produce a comparable 
system for other languages, especially in light of current 
multi-lingual initiatives that include automatic parser 
induction for new languages, e.g. the TIDES initiative. 
 We plan to enhance Hedge Trimmer by using a 
language model of Headlinese, the language of newspa-
per headlines (M?rdh 1980) to guide the system in 
which constituents to remove.  We Also we plan to al-
low for morphological variation in verbs to produce the 
present tense headlines typical of Headlinese.   
 Hedge Trimmer will be installed in a translingual 
detection system for enhanced display of document sur-
rogates for cross-language question answering.  This 
system will be evaluated in upcoming iCLEF confer-
ences. 
7 Acknowledgements 
The University of Maryland authors are supported, 
in part, by BBNT Contract 020124-7157, DARPA/ITO 
Contract N66001-97-C-8540, and NSF CISE Research 
Infrastructure Award EIA0130422.  We would like to 
thank Naomi Chang and Jon Teske for generating refer-
ence headlines. 
References 
Banko, M., Mittal, V., Witbrock, M. (2000).  Headline 
Generation Based on Statistical Translation.  In Pro-
ceedings of 38th Meeting of Association for Computa-
tion Linguistics, Hong Kong, pp. 218-325. 
Bikel, D., Schwartz, R., and Weischedel, R. (1999). An 
algorithm that learns what?s in a name.  Machine Learn-
ing, 34(1/3), February 
Chomsky, Noam A. (1981). Lectures on Government 
and Binding, Foris Publications, Dordrecht, Holland. 
Collins, M. (1997). Three generative lexicalised models 
for statistical parsing. In Proceedings of the 35th ACL, 
1997. 
Daum?, H., Echihabi, A., Marcu, D., Munteanu, D., 
Soricut, R. (2002).  GLEANS: A Generator of Logical 
Extracts and Abstracts for Nice Summaries, In Work-
shop on Automatic Summarization, Philadelphia, PA, 
pp. 9-14. 
Edmundson, H. (1969). ?New methods in automatic 
extracting.? Journal of the ACM, 16(2).  
Grefenstett, G. (1998).  Producing intelligent tele-
graphic text reduction to provide an audio scanning ser-
vice for the blind.  In Working Notes of the AIII Spring 
Symposium on Intelligent Text Summarization, Stanford 
University, CA, pp. 111-118. 
Hori, C., Furui, S., Malkin, R., Yu, H., Waibel, A. 
(2002).  Automatic Speech Summarization Applied to 
English Broadcast News Speech.  In Proceedings of 
2002 International Conference on Acoustics, Speech 
and Signal Processing, Istanbul, pp. 9-12. 
Johnson, F. C., Paice, C. D., Black, W. J., and Neal, A. 
P. (1993). ?The application of linguistic processing to 
automatic abstract generation.? Journal of Document 
and Text Management, 1(3):215-42. 
Knight, K. and Marcu, D. (2001).  ?Statistics-Based 
Summarization Step One: Sentence Compression,? In 
Proceedings of AAAI-2001. 
Kupiec, J., Pedersen, J., and Chen, F. (1995).  ?A train-
able document summarizer.?  In Proceedings of the 18th 
ACM-SIGIR Conference. 
Luhn, H. P. (1958).  "The automatic creation of litera-
ture abstracts." IBM Journal of Research and Develop-
ment, 2(2). 
Mann, W.C., Matthiesen, C.M.I.M., and Thomspson, 
S.A. (1992).  Rhetorical structure theory and text analy-
sis.  In Mann, W.C. and Thompson, S.A., editors, Dis-
course Description.  J. Benjamins Pub. Co., Amsterdam. 
M?rdh, I. (1980).  Headlinese:  On the Grammar of 
English Front Page Headlines, Malmo. 
McKeown, K.,  Barzilay, R.,  Blair-Goldensohn, S., 
Evans, D.,  Hatzivassiloglou, V., Klavans, J., Nenkova, 
A., Schiffman, B.,  and Sigelman, S. (2002).  ?The Co-
lumbia Multi-Document Summarizer for DUC 2002,?  
In Workshop on Automatic Summarization, Philadel-
phia, PA, pp. 1-8. 
Miller, S., Crystal, M., Fox, H., Ramshaw, L., Schwartz, 
R., Stone, R., Weischedel, R. and Annotation Group, the 
(1998). Algorithms that Learn to Extract Information; 
BBN: Description of the SIFT System as Used for 
MUC-7. In Proceedings of the MUC-7. 
Miller, S., Ramshaw, L., Fox, H., and Weischedel, R. 
(2000). ?A Novel Use of Statistical Parsing to Extract 
Information from Text,? In Proceedings of 1st Meeting 
of the North American Chapter of the ACL, Seattle, 
WA, pp.226-233. 
Paice, C. D. and Jones, A. P. (1993).  ?The identifica-
tion of important concepts in highly structured technical 
papers.?  In Proceedings of the Sixteenth Annual Inter-
national ACM SIGIR conference on research and de-
velopment in IR. 
Papineni, K., Roukos, S., Ward, T., and Zhu, W. (2002).  
?BLEU: a Method for Automatic Evaluation of Ma-
chine Translation,? In Proceedings of 40th Annual 
Meeting of the Association for Computational Linguis-
tics, Philadelphia, PA, pp. 331-318 
Radev, Dragomir R. and Kathleen R. McKeown (1998). 
?Generating Natural Language Summaries from Multi-
ple On-Line Sources.? Computational Linguistics, 
24(3):469--500, September 1998. 
Teufel, Simone and Marc Moens (1997).  ?Sentence 
extraction as a classification task,? In Proceedings of 
the Workshop on Intelligent and scalable Text summari-
zation, ACL/EACL-1997, Madrid, Spain. 
Zajic, D., Dorr, B., Schwartz, R. (2002) ?Automatic 
Headline Generation for Newspaper Stories,? In Work-
shop on Automatic Summarization, Philadelphia, PA, 
pp. 78-85. 
Zechner, K. (1995). ?Automatic text abstracting by se-
lecting relevant passages.?  Master's thesis, Centre for 
Cognitive Science, University of Edinburgh. 
 
Proceedings of the ACL Workshop on Intrinsic and Extrinsic Evaluation Measures for Machine Translation
and/or Summarization, pages 1?8, Ann Arbor, June 2005. c?2005 Association for Computational Linguistics
A Methodology for Extrinsic Evaluation of Text Summarization:
Does ROUGE Correlate?
Bonnie J. Dorr and Christof Monz and Stacy President and Richard Schwartz? and David Zajic
Department of Computer Science and UMIACS
University of Maryland
College Park, MD 20742
{bonnie,christof,stacypre,dmzajic}@umiacs.umd.edu
?BBN Technologies
9861 Broken Land Parkway
Columbia, Maryland 21046
schwartz@bbn.com
Abstract
This paper demonstrates the usefulness of sum-
maries in an extrinsic task of relevance judgment
based on a new method for measuring agree-
ment, Relevance-Prediction, which compares sub-
jects? judgments on summaries with their own judg-
ments on full text documents. We demonstrate that,
because this measure is more reliable than previ-
ous gold-standard measures, we are able to make
stronger statistical statements about the benefits of
summarization. We found positive correlations be-
tween ROUGE scores and two different summary
types, where only weak or negative correlations
were found using other agreement measures. How-
ever, we show that ROUGE may be sensitive to the
choice of summarization style. We discuss the im-
portance of these results and the implications for fu-
ture summarization evaluations.
1 Introduction
People often prefer to read a summary of a text document,
e.g., news headlines, scientific abstracts, movie previews
and reviews, and meeting minutes. Correspondingly, the
explosion of online textual material has prompted ad-
vanced research in document summarization. Although
researchers have demonstrated that users can read sum-
maries faster than full text (Mani et al, 2002) with some
loss of accuracy, researchers have found it difficult to
draw strong conclusions about the usefulness of summa-
rization due to the low level of interannotator agreement
in the gold standards that they have used. Definitive con-
clusions about the usefulness of summaries would pro-
vide justification for continued research and development
of new summarization methods.
To investigate the question of whether text summariza-
tion is useful in an extrinsic task, we examined human
performance in a relevance assessment task using a hu-
man text surrogate (i.e. text intended to stand in the place
of a document). We use single-document English sum-
maries as these are sufficient for investigating task-based
usefulness, although more elaborate surrogates are possi-
ble, e.g., those that span more than one document (Radev
and McKeown, 1998; Mani and Bloedorn, 1998).
The next section motivates the need for develop-
ing a new framework for measuring task-based useful-
ness. Section 3 presents a novel extrinsic measure called
Relevance-Prediction. Section 4 demonstrates that this is
a more reliable measure than that of previous gold stan-
dard methods, e.g., the LDC-Agreement method used for
SUMMAC-style evaluations, and that this reliability al-
lows us to make stronger statistical statements about the
benefits of summarization. We expect these findings to
be important for future summarization evaluations.
Section 5 presents the results of correlation between
task usefulness and the Recall Oriented Understudy for
Gisting Evaluation (ROUGE) metric (Lin and Hovy,
2003).1 While we show that ROUGE correlates with task
usefulness (using our Relevance-Prediction measure), we
detect a slight difference between informative, extractive
headlines (containing words from the full document) and
less informative, non-extractive ?eye-catchers? (contain-
ing words that might not appear in the full document, and
intended to entice a reader to read the entire document).
Section 6 further highlights the importance of this
point and discusses the implications for automatic eval-
uation of non-extractive summaries. To evaluate non-
extractive summaries reliably, an automatic measure may
require knowledge of sophisticated meaning units.2 It is
our hope that the conclusions drawn herein will prompt
investigation into more sophisticated automatic metrics
as researchers shift their focus to non-extractive sum-
maries.
1ROUGE has been previously used as the primary automatic
evaluation metric by NIST in the 2003 and 2004 DUC Evalua-
tions.
2The content units proposed in recent methods (Nenkova
and Passonneau, 2004) are a first step in this direction.
1
2 Background
In the past, assessments of usefulness involved a wide
range of both intrinsic and extrinsic (task-based) mea-
sures (Sparck-Jones and Gallier, 1996). Intrinsic evalu-
ations focus on coherence and informativeness (Jing et
al., 1998) and often involve quality comparisons between
automatic summaries and reference summaries that are
pre-determined to be of high quality. Human intrinsic
measures determine quality by assessing document accu-
racy, fluency, and clarity. Automatic intrinsic measures
such as ROUGE use n-gram scoring to produce rankings
of summarization methods.
Extrinsic evaluations concentrate on the use of sum-
maries in a specific task, e.g., executing instructions, in-
formation retrieval, question answering, and relevance
assessments (Mani, 2001). In relevance assessments, a
user reads a topic or event description and judges rele-
vance of a document to the topic/event based solely on its
summary.3 These have been used in many large-scale ex-
trinsic evaluations, e.g., SUMMAC (Mani et al, 2002)
and the Document Understanding Conference (DUC)
(Harman and Over, 2004). The task chosen for such eval-
uations must support a very high degree of interannota-
tor agreement, i.e., consistent relevance decisions across
subjects with respect to a predefined gold standard.
Unfortunately, a consistent gold standard has not yet
been reported. For example, in two previous studies
(Mani, 2001; Tombros and Sanderson, 1998), users?
judgments were compared to ?gold standard judgments?
produced by members of the University of Pennsylva-
nia?s Linguistic Data Consortium. Although these judg-
ments were supposed to represent the correct relevance
judgments for each of the documents associated with an
event, both studies reported that annotators? judgments
varied greatly and that this was a significant issue for the
evaluations. In the SUMMAC experiments, the Kappa
score (Carletta, 1996; Eugenio and Glass, 2004) for in-
terannotator agreement was reported to be 0.38 (Mani et
al., 2002). In fact, large variations have been found in the
initial summary scoring of an individual participant and a
subsequent scoring that occurs a few weeks later (Mani,
2001; van Halteren and Teufel, 2003).
This paper attempts to overcome the problem of in-
terannotator inconsistency by measuring summary effec-
tiveness in an extrinsic task using a much more consistent
form of user judgment instead of a gold standard. Us-
ing Relevance-Prediction increases the confidence in our
results and strengthens the statistical statements we can
make about the benefits of summarization.
The next section describes an alternative approach to
measuring task-based usefulness, where the usage of ex-
ternal judgments as a gold standard is replaced by the
3A topic is an event or activity, along with all directly re-
lated events and activities. An event is something that happens
at some specific time and place, and the unavoidable conse-
quences.
user?s own decisions on the full text. Following the lead
of earlier evaluations (Oka and Ueda, 2000; Mani et al,
2002; Sakai and Sparck-Jones, 2001), we focus on rele-
vance assessment as our extrinsic task.
3 Evaluation of Usefulness of Summaries
We define a new extrinsic measure of task-based useful-
ness called Relevance-Prediction, where we compare a
summary-based decision to the subject?s own full-text de-
cision rather than to a different subject?s decision. Our
findings differ from that of the SUMMAC results (Mani
et al, 2002) in that using Relevance-Prediction as an al-
ternative to comparision to a gold standard is a more re-
alistic agreement measure for assessing usefulness in a
relevance assessment task. For example, users perform-
ing browsing tasks must examine document surrogates,
but open the full-text only if they expect the document to
be interesting to them. They are not trying to decide if
the document will be interesting to someone else.
To determine the usefulness of summarization, we fo-
cus on two questions:
? Can users make judgments on summaries that are
consistent with their full-text judgments?
? Can users make judgments on summaries more
quickly than on full document text?
First we describe the Relevance-Prediction measure for
determining whether users can make accurate judgments
with a summary. Following this, we describe our exper-
iments and results using this measure, including the tim-
ing results of summaries compared to full documents.
3.1 Relevance-Prediction Measure
To answer the first question above, we define a mea-
sure called Relevance-Prediction, where subjects build
their own ?gold standard? based on the full-text docu-
ments. Agreement is measured by comparing subjects?
surrogate-based judgments against their own judgments
on the corresponding texts. The subject?s judgment is as-
signed a value of 1 if his/her surrogate judgment is the
same as the corresponding full-text judgment, and 0 oth-
erwise. These values were summed over all judgments
for a surrogate type and were divided by the total num-
ber of judgments for that surrogate type to determine the
effectiveness of the associated summary method.
Formally, given a summary/document pair (s, d), if
subjects make the same judgment on s that they did on
d, we say j(s, d) = 1. If subjects change their judg-
ment between s and d, we say j(s, d) = 0. Given a set
of summary/document pairs DSi associated with event i,
the Relevance-Prediction score is computed as follows:
Relevance-Prediction(i) =
?
s,d?DSi
j(s, d)
|DSi|
This approach provides a more reliable comparison
mechanism than gold standard judgments provided by
2
other individuals. Specifically, Relevance-Prediction is
more helpful in illuminating the usefulness of summaries
for a real-world scenario, e.g., a browsing environment,
where credit is given when an individual subject would
choose (or reject) a document under both conditions. To
our knowledge, this subject-driven approach to testing
usefulness has never before been used.
3.2 Experiment Design
Ten human subjects were recruited to evaluate full-text
documents and two summary types.4 The original text
documents were taken from the Topic Detection and
Tracking 3 (TDT-3) corpus (Allan et al, 1999) which
contains news stories and headlines, topic and event de-
scriptions, and a mapping between news stories and their
related topic and/or events. Although the TDT-3 collec-
tion contains transcribed speech documents, our investi-
gation was restricted to documents that were originally
text, i.e., newspaper or newswire, not broadcast news.
For our experiment we selected three distinct events
and related document sets5 from TDT-3. For each event,
the subjects were given a description of the event (writ-
ten by LDC) and then asked to judge relevance of a set
of 20 documents associated with that event (using three
different presentation types to be discussed below).
The events used from the TDT data set were events
from world news occurring in 1998. It is possible that
the subjects had some prior knowledge about the events,
yet we believe that this would not affect their ability to
complete the task. Subjects? background knowledge of an
event can also make this task more similar to real-world
browsing tasks, in which subjects are often familiar with
the event or topic they are searching for.
The 20 documents were retrieved by a search engine.
We used a constrained subset where exactly half (10)
were judged relevant by the LDC annotators. Because all
20 documents were somewhat similar to the event, this
approach ensured that our task would be more difficult
than it would be if we had chosen documents from com-
pletely unrelated events (where the choice of relevance
would be obvious even from a poorly written summary).
Each document was pre-annotated with the headline
associated with the original newswire source. These
headlines were used as the first summary type. We re-
fer to them as HEAD (Headline Surrogate). The average
length of the HEAD surrogates was 53 characters. In ad-
dition, we commissioned human-generated summaries6
of each document as the second summary type; we refer
4We required all human subjects to be native-English speak-
ers to ensure that the accuracy of judgments was not degraded
by language barriers.
5The three event and related document sets contained
enough data points to achieve statistically significant results.
6The human summarizers were instructed to create a sum-
mary no greater than 75 characters for each specified full text
document. The summaries were not compared for writing style
or quality.
to this as HUM (Human Surrogate). The average length
of the HUM surrogates was 72 characters. Although nei-
ther of these summaries was produced automatically, our
experiment allowed us to focus on the question of sum-
mary usefulness and to learn about the differences in pre-
sentation style as a first step toward experimentation with
the output of automatic summarization systems.
Two main factors were measured: (1) differences
in judgments for the three presentation types (HEAD,
HUM, and the full-text document) and (2) judgment time.
Each subject made a total of 60 judgments for each pre-
sentation type since there were 3 distinct events and 20
documents per event. To facilitate the analysis of the data,
the subjects? judgments were constrained to two possibil-
ities, relevant or not relevant.7
Although the HEAD and HUM surrogates were both
produced by humans, they differed in style. The HEAD
surrogates were shorter than the HUM surrogates by
26%. Many of these were ?eye-catchers? designed to en-
tice the reader to examine the entire document (i.e., pur-
chase the newspaper); that is, the HEAD surrogates were
not intended to stand in the place of the full document.
By contrast, the writers of the HUM surrogates were in-
structed to write text that conveyed what happened in the
full document. We observed that the HUM surrogates
used more words and phrases extracted from the full doc-
uments than the HEAD surrogates.
Experiments were conducted using a web browser (In-
ternet Explorer) on a PC in the presence of the experi-
menter. Subjects were given written and verbal instruc-
tions for completing their task and were asked to make
relevance judgments on a practice event set. The judg-
ments from the practice event set were not included in
our experimental results or used in our analyses. The
written instructions were given to aid subjects in deter-
mining requirements for relevance. For example, in an
Election event documents describing new people in of-
fice, new public officials, change in governments or par-
liaments were suggested as evidence for relevance.
Each of the ten subjects made judgments on 20 doc-
uments for each of three different events. After reading
each document or summary, the subjects clicked on a ra-
dio button corresponding to their judgment and clicked
a submit button to move to the next document descrip-
tion. Subjects were not allowed to move to the next sum-
mary/document until a valid selection was made. No
backing up was allowed. Judgment time was computed
as the number of seconds it took the subject to read the
full text document or surrogate, comprehend it, compare
it to the event description, and make a judgment (timed
up until the subject clicked the submit button).
7If we allowed subjects to make additional judgments such
as somewhat relevant, this could possibly encourage subjects to
always choose this when they were the least bit unsure. Previ-
ous experiments indicate that this additional selection method
may increase the level of variability in judgments (Zajic et al,
2004).
3
3.3 Order of Document/Surrogate Presentation
One concern with our evaluation methodology was the
issue of possible memory effects or priming: if the same
subjects saw a summary and a full document about the
same event, their answers might be tainted. Thus, prior to
the full experiment, we conducted pre-experiments (us-
ing 4 participants) with an extreme form of influence: we
presented the summary and full text in immediate suc-
cession. In these experiments, we compared two docu-
ment presentation approaches, termed ?Drill Down? and
?Complete Set.? In the ?Drill Down? document presen-
tation approach all three presentation types were shown
for each document, in sequence: first a single HEAD sur-
rogate, followed by the corresponding HUM surrogate,
followed by the full text document. This process was re-
peated 10 times.
In the ?Complete Set? document-presentation ap-
proach we presented the complete set of documents us-
ing one surrogate type, followed by the complete set us-
ing another surrogate type, and so on. That is, the 10
HEAD surrogates were displayed all at once, followed
by the corresponding 10 HUM surrogates, followed by
the corresponding 10 full-text documents.
The results indicated that there was almost no effect
between the two document-presentation approaches. The
performance varied only slightly and neither approach
consistently allowed subjects to perform better than the
other. Therefore, we determined that the subjects were
not associating a given summary with its corresponding
full-text documents. This may be due, in part, to the fact
that all 20 documents were related to the event?and ac-
cording to the LDC relevance judgments half of these
were actually about the same event.
Given that the variations were insignificant in these
pre-experiments, we selected only the Complete-Set ap-
proach (no Drill-Down) for the full experiment. How-
ever, we still needed to vary the ordering for the two sur-
rogate presentation types associated with each full-text
document. Thus, each 20-document set was divided in
half for each subject. In the first half, the subject saw the
first 10 documents as: (1) HEAD surrogates, then HUM
surrogates and then the full-text document; or (2) HUM
surrogates, then HEAD surrogates, and then the full-text
document. In the second half, the subject saw the alter-
native ordering, e.g., if a subject saw HEAD surrogates
before HUM surrogates in the first half, he/she saw the
HUM surrogates before HEAD surrogates for the sec-
ond half. Either way, the full-text document was always
shown last so as not to introduce judgment effects asso-
ciated with reading the entire document before either sur-
rogate type.
In addition to varying the ordering for the surrogate
type, the ordering of the surrogates and full documents
within the events were also varied. The subjects were
grouped in pairs, and each pair viewed the surrogates and
documents in a different order than the other pairs.
3.4 Experimental Hypotheses
We hypothesized that the summaries would allow sub-
jects to achieve a Relevance-Prediction rate of 70?90%.
Since these summaries were significantly shorter than the
original document text, we expected that the rate would
not be 100% compared to the judgments made on the full
document text. However, we expected higher than a 50%
ratio, i.e., higher than that of random judgments on all of
the surrogates. We also expected high performance be-
cause the meaning of the original document text is best
preserved when written by a human (Mani, 2001).
A second hypothesis is that the HEAD surrogates
would yield a significantly lower agreement rate than that
of the HUM surrogates. Our commissioned HUM surro-
gates were written to stand in place of the full document,
whereas the HEAD surrogates were written to catch a
reader?s interest. This suggests that the HEAD surrogates
might not provide as informative a description of the orig-
inal documents as the HUM surrogates.
We also tested a third hypothesis: that our Relevance-
Prediction measure would be more reliable than that of
the LDC-Agreement method used for SUMMAC-style
evaluations (thus providing a more stable framework for
evaluating summarization techniques). LDC-Agreement
compares a subject?s judgment on a surrogate or full text
against the ?correct? judgments as assigned by the TDT
corpus annotators (Linguistic Data Consortium 2001).
Finally, we tested the hypothesis that using a text sum-
mary for judging relevance would take considerably less
time than using the corresponding full-text document.
4 Experimental Results
Table 1 shows the subjects? judgments using both
Relevance-Prediction and LDC-Agreement for each of
three events. Using our Relevance-Prediction measure,
the HUM surrogates yield averages between 79% and
86%, with an overall average of 81%, thus confirming
our first hypothesis.
However, we failed to confirm our second hypothe-
sis. The HEAD Relevance-Prediction rates were between
71% and 82%, with an overall average of 76%, which
was lower than the rates for HUM, but the difference
was not statistically significant. It appeared that subjects
were able to make consistent relevance decisions from the
non-extractive HEAD surrogates, even though these were
shorter and less informative than the HUM surrogates.
A closer look reveals that the HEAD summaries some-
times contained enough information to judge relevance,
yielding almost the same number of true positives (and
true negatives) as the HUM summaries. For example, a
document about the formation of a coalition government
to avoid violence in Cambodia has the HEAD surrogate
Cambodians hope new government can avoid past mis-
takes. By contrast, the HUM surrogate for this same event
was Rival parties to form a coalition government to avoid
violence in Cambodia. Although the HEAD surrogate
4
Surrogate EVENT 1 EVENT 2 EVENT 3 Overall Avg Avg Time
LDC RP LDC RP LDC RP LDC RP (seconds)
HEAD 67% 76% 66% 71% 70% 82% 67% 76% 4.60
HUM 69% 80% 73% 86% 62% 79% 68% 81% 4.57
DOC ? ? ? ? ? ? ? ? 13.38
Table 1: Relevance-Prediction (RP) and LDC-Agreement (LDC) Rates for HEAD and HUM Surrogates for each Event
uses words that do not appear in the original document
(hope and mistakes), the subject may infer the relevance
of this surrogate by relating hope to the notion of forming
a coalition government and mistakes to violence.
On the other hand, we found that the lower degree of
informativeness of HEAD surrogates gave rise to over
50% more false negatives than the HUM summaries. This
statistically significant difference will be discussed fur-
ther in Section 6.
As for our third hypothesis, Table 1 illustrates a
substantial difference between the two agreement mea-
sures. For each of the three events, the Relevance-
Prediction rate is at least five percent higher than that
of the LDC-Agreement approach, with an average of
8.8% increase for the HEAD summary and a 13.3% aver-
age increase for the HUM summary. The average rates
across events show a statistically significant difference
between LDC-Agreement and Relevance-Prediction for
both HUM summaries with p<0.01 and HEAD sum-
maries with p<0.05. This significance was determined
through use of a single factor ANOVA statistical analysis.
The higher Relevance-Prediction rate supports our state-
ment that this approach provides a more stable framework
for evaluating different summarization techniques.
Finally, the average timing results shown in Table 1
confirm our fourth hypothesis. The subjects took 4-5 sec-
onds (on average) to make judgments on both the HEAD
and HUM summaries, as compared to about 13.4 seconds
to make judgments on full text documents. This shows
that it takes subjects almost 3 times longer to make judg-
ments on full text documents as it took to make judgments
on the summaries (HEAD and HUM). This finding is not
surprising since text summaries are an order of magnitude
shorter than full-text documents.
5 Correlation with Intrinsic Evaluation
Metric: ROUGE
We now turn to the task of correlating our extrinsic task
performance with scores produced by an intrinsic evalu-
ation measure. We used the Recall Oriented Understudy
for Gisting Evaluation (ROUGE) metric version 1.2.1. In
previous studies (Dorr et al, 2004) ROUGE was shown
to have a very low correlation with the LDC-Agreement
measurement results of the extrinsic task. This was at-
tributed to low interannotator agreement in the gold stan-
dard. Our goal was to test whether our new Relevance-
Prediction technique would allow us to induce higher cor-
relations with ROUGE.
5.1 Extrinsic Agreement Data
To reduce the effect of outliers on the correlation between
ROUGE and the human judgments, we averaged over all
judgments for each subject (20 judgments ? 3 events) to
produce 60 data points. These data points were then par-
titioned into either 1, 2, or 4 partitions of equal size. (Par-
titions of size four have 15 data points, partitions of size
two have 30 data points, and partitions of size one have
60 data points per subject?or a total of 600 datapoints
across all 10 subjects). To ensure that the correlation did
not depend on a specific partition, we repeated this same
process using 10,000 different (randomly generated) par-
titions for each of the three partition sizes.
Partitioned data points of size four provided a high de-
gree of noise reduction without compromising the size
of the data set (15 points). Larger partition sizes would
result in too few data points and compromise the statis-
tical significance of our correlation results. In order to
show the variation within a single partition, we used the
partitioning of size 4 with the smallest mean square er-
ror on the human headline compared to the other parti-
tionings as a representative partition. For this represen-
tative partitioning, the individual data points P1?P15 of
that partition are shown for each of the two agreement
measures in Tables 2 and 3. This shows that, across parti-
tions, the maximum and minimum Relevance-Prediction
rates for HEAD (93% and 60%) are higher than the cor-
responding LDC-Agreement rates (85% and 50%). The
same trend is seen with the HUM surrogates: Relevance-
Prediction maximum of 98%, minimum of 68%; and
LDC-Agreement maximum 88%, minimum of 55%.
5.2 Intrinsic ROUGE Score
To correlate the partitioned agreement scores above with
our intrinsic measure, we first ran ROUGE on all 120 sur-
rogates in our experiment (i.e., the HUM and HEAD sur-
rogates for each of the 60 event/document pairs) and then
averaged the ROUGE scores for all surrogates belong-
ing to the same partitions (for each of the three partition
sizes). These partitioned ROUGE values were then used
for detecting correlations with the corresponding parti-
tioned agreement scores described above.
Table 4 shows the ROUGE scores, based on 3 ref-
erence summaries per document, for partitions P1?P15
used in the previous tables.8 For brevity, we include
8We commissioned a total of 180 human-generated refer-
ence summaries (3 for each of 60 documents) (in addition to
the human generated summaries used in the experiment).
5
Surrogate P1 P2 P3 P4 P5 P6 P7 P8 P9 P10 P11 P12 P13 P14 P15
HEAD 80% 80% 85% 70% 73% 60% 80% 75% 60% 75% 88% 68% 80% 93% 83%
HUM 83% 88% 85% 68% 75% 75% 93% 75% 98% 90% 75% 70% 80% 90% 78%
Table 2: Relevance-Prediction Rates for HEAD and HUM Surrogates (Representative Partition of Size 4)
Surrogate P1 P2 P3 P4 P5 P6 P7 P8 P9 P10 P11 P12 P13 P14 P15
HEAD 70% 73% 85% 70% 63% 60% 60% 85% 50% 73% 70% 78% 65% 63% 73%
HUM 68% 75% 58% 68% 75% 70% 68% 80% 88% 58% 63% 55% 55% 60% 78%
Table 3: LDC-Agreement Rates for HEAD and HUM Surrogates (Representative Partition of Size 4)
Surrogate P1 P2 P3 P4 P5 P6 P7 P8 P9 P10 P11 P12 P13 P14 P15 Avg
HEAD .10 .23 .13 .27 .20 .24 .26 .22 .13 .08 .30 .16 .26 .27 .30 .211
HUM .16 .22 .17 .23 .19 .36 .39 .29 .28 .25 .37 .22 .22 .39 .27 .269
Table 4: Average Rouge-1 Scores for HEAD and HUM Surrogates (Representative Partition of Size 4)
only ROUGE 1-gram measurement (R1).9 The ROUGE
scores for HEAD surrogates were slightly lower than
those for HUM surrogates. This is consistent with
our statements earlier about the difference between non-
extractive ?eye-catchers? and informative headlines. Be-
cause ROUGE measures whether a particular summary
has the same words (or n-grams) as a reference summary,
a more constrained choice of words (as found in the ex-
tractive HUM surrogates) makes it more likely that the
summary would match the reference.
A summary in which the word choice is less
constrained?as in the non-extractive HEAD
surrogates?is less likely to share n-grams with the
reference. Thus, we may see non-extractive summaries
that have almost identical meanings, but very different
words. This raises the concern that ROUGE may be
sensitive to the style of summarization that is used.
Section 6 discusses this point further.
5.3 Intrinsic and Extrinsic Correlation
To test whether ROUGE correlates more highly with
Relevance-Prediction than with LDC-Agreement, we cal-
culated the correlation for the results of both techniques
using Pearson?s r (Siegel and Castellan, 1988):
?n
i=1(ri ? r?)(si ? s?)??n
i=1(ri ? r?)
2
??n
i=1(si ? s?)
2
where ri is the ROUGE score of surrogate i, r? is the av-
erage ROUGE score of all data points, si is the agree-
ment score of summary i (using Relevance-Prediction or
LDC-Agreement), and s? is the average agreement score.
Pearson?s statistics is commonly used in summarization
and machine translation evaluation, see e.g. (Lin, 2004;
Lin and Och, 2004).
As one might expect, there is some variability in the
correlation between ROUGE and human judgments for
9We also computed ROUGE 2-gram, ROUGE L and
ROUGE W, but the trend for these did not differ from ROUGE-
1.
Figure 1: Distribution of the Correlation Variation for
Relevance-Prediction on HEAD and HUM
the different partitions. However, the boxplots for both
HEAD and HUM indicate that the first and third quartile
were relatively close to the median (see Figure 1).
Table 5 shows the Pearson Correlations with ROUGE-
1 using Relevance-Prediction and LDC-Agreement. For
Relevance-Prediction, we observed a positive correlation
for both surrogate types, with a slightly higher corre-
lation for HEAD than HUM. For LDC-Agreement, we
observed no correlation (or a minimally negative one)
with ROUGE-1 scores, for both the HEAD and HUM
surrogates. The highest correlation was observed for
Relevance-Prediction on HEAD.
We conclude that ROUGE correlates more highly with
the Relevance-Prediction measurement than the LDC-
Agreement measurement, although we should add that
none of the correlations in Table 5 were statistically sig-
nificant at p < 0.05. The low LDC-Agreement scores are
consistent with previous studies where poor correlations
6
Surrogate P = 1 P = 2 P = 4
HEAD (RP) 0.1270 0.1943 0.3140
HUM (RP) 0.0632 0.1096 0.1391
HEAD (LDC) -0.0968 -0.0660 -0.0099
HUM (LDC) -0.0395 -0.0236 -0.0187
Table 5: Pearson Correlations with ROUGE-1 for
Relevance-Prediction (RP) and LDC-Agreement (LDC),
where Partition size (P) = 1, 2, and 4
were attributed to low interannotator agreement rates.
6 Discussion
Our results suggest that ROUGE may be sensitive to the
style of summarization that is used. As we observed
above, many of the HEAD surrogates were not actually
summaries of the full text, but were eye-catchers. Of-
ten, these surrogates did not allow the subject to judge
relevance correctly, resulting in lower agreement. In ad-
dition, these same surrogates often did not use a high per-
centage of words that were actually from the story, result-
ing in low ROUGE scores. (We noticed that most words
in the HUM surrogates appeared in the corresponding
stories.) There were three consequences of this difference
between HEAD and HUM: (1) The rate of agreement was
lower for HEAD than for HUM; (2) The average ROUGE
score was lower for HEAD than for HUM; and (3) The
correlation of ROUGE scores with agreement was higher
for HEAD than for HUM.
A further analysis supports the (somewhat counterin-
tuitive) third point above. Although the ROUGE scores
of true positives (and true negatives) were significantly
lower for HEAD surrogates (0.2127 and 0.2162) than
for HUM surrogates (0.2696 and 0.2715), the number of
false negatives was substantially higher for HEAD sur-
rogates than for HUM surrogates. These cases corre-
sponded to much lower ROUGE scores for HEAD sur-
rogates (0.1996) than for HUM (0.2586) surrogates.
A summary of this analysis is given in Table 6, where
true positives and negatives are indicated by Rel/Rel
and NonRel/NonRel, respectively, and false positives and
negatives are indicated by Rel/NonRel and NonRel/Rel,
respectively.10 The numbers in parentheses after each
ROUGE score refer to the standard deviation for that
10We also included (average) elapsed times for summary
judgments in each of the four categories. One might expect a
?relevant? judgment to be much quicker than a ?non-relevant?
judgment (since the latter might require reading the full sum-
mary). However, it turned out non-relevant judgments did not
always take longer. In fact, the NonRel/NonRel cases took con-
siderably less time than the Rel/Rel and Rel/NonRel cases. On
the other hand, the NonRel/Rel cases took considerably more
time?almost as much time as reading the full text documents?
an indication that the subjects may have re-read the summary a
number of times, perhaps vacillating back and forth. Still, the
overall time savings was significant, given that the vast major-
ity of the non-relevant judgments were in the NonRel/NonRel
category.
score. This was computed as follows:
Std .-Dev . =
?
?N
i=1(xi ? x?)
2
N
where N is the number of surrogates in a particular judg-
ment category (e.g., N = 245 for the HEAD-based Non-
Rel/Rel judgments), xi is the ROUGE score for the ith
surrogate, and r? is the average of all ROUGE scores in
that category.
Although there were very few false positives (less than
6% for both HEAD and HUM), the number of false nega-
tives (NonRel/Rel) was particularly high for HEAD (50%
higher than for HUM). This difference was statistically
significant at p<0.01 using the t-test. The large number
of false negatives with HEAD may be attributed to the
eye-catching nature of these surrogates. A subject may
be misled into thinking that this surrogate is not related
to an event because the surrogate does not contain words
from the event description and is too broad for the subject
to extract definitive information (e.g., the surrogate There
he goes again!). Because the false negatives were associ-
ated with the lowest average ROUGE score (0.1996), we
speculate that, if a correlation exists between Relevance-
Prediction and ROUGE, the false negatives may be a ma-
jor contributing factor.
Based on this experiment, we conjecture that ROUGE
may not be a good method for measuring the useful-
ness of summaries when the summaries are not extrac-
tive. That is, if someone intentionally writes summaries
that contain different words than the story, the summaries
will also likely contain different words than a reference
summary, resulting in low ROUGE scores. However,
the summaries, if well-written, could still result in high
agreement with the judgments made on the full text.
7 Conclusion
We have shown that two types of human summaries,
HEAD and HUM, can be useful for relevance assessment
in that they help a user achieve 70-85% agreement in rel-
evance judgments. We observed a 65% reduction in judg-
ment time between full texts and summaries. These find-
ings are important in that they establish the usefulness
of summarization and they support research and devel-
opment of additional summarization methods, including
automatic methods.
We introduced a new method for measuring agree-
ment, Relevance-Prediction, which takes a subject?s
full-text judgment as the standard against which the
same subject?s summary judgment is measured. Be-
cause Relevance-Prediction was more reliable than LDC-
Agreement judgments, we encourage others to use this
measure in future summarization evaluations.
Using this new method, we were able to find positive
correlations between relevance assessments and ROUGE
scores for HUM and HEAD surrogates, where only
7
Judgment HEAD HUM
(Surr/Doc) Raw R1-Avg Avg Time Raw R1-Avg Avg Time
Rel/Rel 211 (35%) 0.2127 (?0.120) 4.6 251 (42%) 0.2696 (?0.130) 4.2
Rel/NonRel 27 (5%) 0.2115 (?0.110) 7.1 35 (6%) 0.2725 (?0.131) 4.6
NonRel/Rel 117 (19%) 0.1996 (?0.127) 8.5 77 (13%) 0.2586 (?0.120) 13.8
NonRel/NonRel 245 (41%) 0.2162 (?0.126) 2.5 237 (39%) 0.2715 (?0.131) 1.9
TOTAL 600 (100%) 0.2115 (?0.124) 4.6 600 (100%) 0.2691 (?0.129) 4.6
Table 6: Subjects? Judgments and Corresponding Average ROUGE 1 Scores
negative correlations were found using LDC-Agreement
scores. We found that both the Relevance-Prediction and
the ROUGE-1 scores were higher for human-generated
summaries than for the original headlines. It appears
that most of the difference is induced by surrogates that
are eye-catchers (rather than true summaries), where both
agreement and ROUGE scores are low.
Our future work will include further experimentation
with automatic summarization methods to determine the
level of Relevance-Prediction. We aim to determine how
well automatic summarizers help users complete tasks,
and to investigate which automatic summarizers perform
better than others. We also plan to test for correlations
between ROUGE and human task performance with auto-
matic summaries, to further investigate whether ROUGE
is a good predictor of human task performance.
Acknowledgements
This work was supported in part by DARPA TIDES Cooperative
Agreement N66001-00-2-8910.
References
James Allan, Hubert Jin, Martin Rajman, Charles Wayne,
Daniel Gildea, Victor Lavrenko, Rose Hoberman, and David
Caputo. 1999. Topic-based Novelty Detection. Technical
Report 1999 Summer Workshop at CLSP Final Report, Johns
Hopkins, Maryland.
Jean Carletta. 1996. Assessing Agreement on Classification
Tasks: The Kappa Statistic. Computational Lingusitics,
22(2):249?254, June.
Bonnie J. Dorr, Christof Monz, Douglas Oard, Stacy President,
and David Zajic. 2004. Extrinsic Evaluation of Automatic
Metrics for Summarization. Technical report, University of
Maryland, College Park, MD. LAMP-TR-115, CAR-TR-
999, CS-TR-4610, UMIACS-TR-2004-48.
Barbara Di Eugenio and Michael Glass. 2004. Squibs and Dis-
cussions - The Kappa Statistic: A Second Look. Computa-
tional Linguistics, pages 95?101.
Donna Harman and Paul Over. 2004. Proceedings of the DUC
2004. Boston, MA.
Hongyan Jing, Regina Barzilay, Kathleen McKeown, and
Michael Elhadad. 1998. Summarization evaluation meth-
ods: Experiments and analysis. In Proceedings of the AAAI
Symposium on Intelligent Summarization, Stanford Univer-
sity, CA, March 23-25.
Chin-Yew Lin and Eduard Hovy. 2003. Automatic Evaluation
of Summaries Using N-gram Co-Occurrence Statistics. In
Proceedings of HLT-NAACL 2003 Workshop, pages 71?78,
Edmonton Canada, May-June.
Chin-Yew Lin and Franz Joseph Och. 2004. ORANGE: a
Method for Evaluating Automatic Evaluation Metrics for
Machine Translation. In Proceedings of the 20th Interna-
tional Conference on Computational Linguistics (COLING
2004), Geneva, Switzerland, August 23?27.
Chin-Yew Lin. 2004. ROUGE: a Package for Automatic Eval-
uation of Summaries. In Proceedings of the Workshop on
Text Summarization Branches Out (WAS 2004), Barcelona,
Spain, July 25?26.
I. Mani and E. Bloedorn. 1998. Summarizing Similarities and
Differences Among Related Documents. Information Re-
trieval, 1(1):35?67.
Inderjeet Mani, Gary Klein, David House, and Lynette
Hirschman. 2002. SUMMAC: a text summarization eval-
uation. Natural Language Engineering, 8(1):43?68.
Inderjeet Mani. 2001. Summarization Evaluation: An
Overview. In Proceedings of the NAACL 2001 Workshop on
Automatic Summarization.
Ani Nenkova and Rebecca J. Passonneau. 2004. Evaluating
Content Selection in Summarization: The Pyramid Method.
In Proceedings of the NAACL 2004, Boston, MA.
Mamiko Oka and Yoshihiro Ueda. 2000. Evaluation of Phrase-
Representation Summarization Based on an Information Re-
trieval Task. In Proceedings of the ANLP/NAACL 2000
Workshop on Automatic Summarization, pages 59?68, New
Brunswick, NJ.
Dragomir Radev and Kathleen McKeown. 1998. Generat-
ing Natural Language Summaries from Multiple On-Line
Sources. Computational Linguistics, pages 469?500.
Tetsuya Sakai and Karen Sparck-Jones. 2001. Generic Sum-
maries for Indexing in Information Retrieval - Detailed Test
Results. Technical Report TR513, Computer Laboratory,
University of Cambridge.
Sidney Siegel and N. John Castellan, Jr. 1988. Nonparametric
Statistics for the Behavioral Sciences. McGraw-Hill, New
York, second edition.
Karen Sparck-Jones and J.R. Gallier. 1996. Evaluating Natu-
ral Language Processing Systems: An Analysis and Review.
Springer, Berlin.
Anastasios Tombros and Mark Sanderson. 1998. Advantages
of query biased summaries in information retrieval. In Pro-
ceedings of the 21st Annual International ACM SIGIR Con-
ference on Research and Development in Information Re-
trieval, pages 2?10.
Hans van Halteren and Simone Teufel. 2003. Examining the
Consensus Between Human Summaries: Initial Experiments
with Factoid Analysis. In Proceedings of the HLT-NAACL
03 Text Summarization Workshop.
David Zajic, Bonnie J. Dorr, Richard Schwartz, and Stacy Presi-
dent. 2004. Headline Evaluation Experiment Results. Tech-
nical report, University of Maryland, College Park, MD.
UMIACS-TR-2004-18.
8
Proceedings of the Workshop on Innovative Hybrid Approaches to the Processing of Textual Data (Hybrid2012), EACL 2012, pages 78?86,
Avignon, France, April 23 2012. c?2012 Association for Computational Linguistics
A random forest system combination approach for error detection in
digital dictionaries
Michael Bloodgood and Peng Ye and Paul Rodrigues
and David Zajic and David Doermann
University of Maryland
College Park, MD
meb@umd.edu, pengye@umiacs.umd.edu, prr@umd.edu,
dzajic@casl.umd.edu, doermann@umiacs.umd.edu
Abstract
When digitizing a print bilingual dictionary,
whether via optical character recognition or
manual entry, it is inevitable that errors are
introduced into the electronic version that is
created. We investigate automating the pro-
cess of detecting errors in an XML repre-
sentation of a digitized print dictionary us-
ing a hybrid approach that combines rule-
based, feature-based, and language model-
based methods. We investigate combin-
ing methods and show that using random
forests is a promising approach. We find
that in isolation, unsupervised methods ri-
val the performance of supervised methods.
Random forests typically require training
data so we investigate how we can apply
random forests to combine individual base
methods that are themselves unsupervised
without requiring large amounts of training
data. Experiments reveal empirically that
a relatively small amount of data is suffi-
cient and can potentially be further reduced
through specific selection criteria.
1 Introduction
Digital versions of bilingual dictionaries often
have errors that need to be fixed. For example,
Figures 1 through 5 show an example of an er-
ror that occurred in one of our development dic-
tionaries and how the error should be corrected.
Figure 1 shows the entry for the word ?turfah? as
it appeared in the original print copy of (Qureshi
and Haq, 1991). We see this word has three senses
with slightly different meanings. The third sense
is ?rare?. In the original digitized XML version
of (Qureshi and Haq, 1991) depicted in Figure 2,
this was misrepresented as not being the meaning
Figure 1: Example dictionary entry
Figure 2: Example of error in XML
of ?turfah? but instead being a usage note that fre-
quency of use of the third sense was rare. Figure 3
shows the tree corresponding to this XML repre-
sentation. The corrected digital XML representa-
tion is depicted in Figure 4 and the corresponding
corrected tree is shown in Figure 5.
Zajic et al (2011) presented a method for re-
pairing a digital dictionary in an XML format us-
ing a dictionary markup language called DML. It
remains time-consuming and error-prone however
to have a human read through and manually cor-
rect a digital version of a dictionary, even with
languages such as DML available. We therefore
investigate automating the detection of errors.
We investigate the use of three individual meth-
ods. The first is a supervised feature-based
method trained using SVMs (Support Vector Ma-
chines). The second is a language-modeling
78
.ENTRY
. .? ? ?
.
.SENSE
.USG
.rare
.? ? ?FORM
. .PRON
.t?r?fah
ORTH
.????
Figure 3: Tree structure of error
Figure 4: Example of error in XML, fixed
.ENTRY
. .? ? ?
.
.SENSE
.TRANS
.TR
.rare
.
.? ? ?FORM
. .PRON
.t?r?fah
ORTH
.????
Figure 5: Tree structure of error, fixed
method that replicates the method presented in
(Rodrigues et al, 2011). The third is a simple
rule inference method. The three individual meth-
ods have different performances. So we investi-
gate how we can combine the methods most effec-
tively. We experiment with majority vote, score
combination, and random forest methods and find
that random forest combinations work the best.
For many dictionaries, training data will not be
available in large quantities a priori and therefore
methods that require only small amounts of train-
ing data are desirable. Interestingly, for automati-
cally detecting errors in dictionaries, we find that
the unsupervised methods have performance that
rivals that of the supervised feature-based method
trained using SVMs. Moreover, when we com-
bine methods using the random forest method, the
combination of unsupervised methods works bet-
ter than the supervised method in isolation and al-
most as well as the combination of all available
methods. A potential drawback of using the ran-
dom forest combination method however is that it
requires training data. We investigated how much
training data is needed and find that the amount
of training data required is modest. Furthermore,
by selecting the training data to be labeled with
the use of specific selection methods reminiscent
of active learning, it may be possible to train the
random forest system combination method with
even less data without sacrificing performance.
In section 2 we discuss previous related work
and in section 3 we explain the three individual
methods we use for our application. In section 4
we explain the three methods we explored for
combining methods; in section 5 we present and
discuss experimental results and in section 6 we
conclude and discuss future work.
2 Related Work
Classifier combination techniques can be broadly
classified into two categories: mathematical and
behavioral (Tulyakov et al, 2008). In the first
category, functions or rules combine normalized
classifier scores from individual classifiers. Ex-
amples of techniques in this category include Ma-
jority Voting (Lam and Suen, 1997), as well as
simple score combination rules such as: sum rule,
min rule, max rule and product rule (Kittler et al,
1998; Ross and Jain, 2003; Jain et al, 2005). In
the second category, the output of individual clas-
sifiers are combined to form a feature vector as
79
the input to a generic classifier such as classifi-
cation trees (P. and Chollet, 1999; Ross and Jain,
2003) or the k-nearest neighbors classifier (P. and
Chollet, 1999). Our method falls into the second
category, where we use a random forest for sys-
tem combination.
The random forest method is described in
(Breiman, 2001). It is an ensemble classifier con-
sisting of a collection of decision trees (called a
random forest) and the output of the random for-
est is the mode of the classes output by the indi-
vidual trees. Each single tree is trained as follows:
1) a random set of samples from the initial train-
ing set is selected as a training set and 2) at each
node of the tree, a random subset of the features is
selected, and the locally optimal split is based on
only this feature subset. The tree is fully grown
without pruning. Ma et al (2005) used random
forests for combining scores of several biometric
devices for identity verification and have shown
encouraging results. They use all fully supervised
methods. In contrast, we explore minimizing the
amount of training data needed to train a random
forest of unsupervised methods.
The use of active learning in order to re-
duce training data requirements without sacri-
ficing model performance has been reported on
extensively in the literature (e.g., (Seung et al,
1992; Cohn et al, 1994; Lewis and Gale, 1994;
Cohn et al, 1996; Freund et al, 1997)). When
training our random forest combination of indi-
vidual methods that are themselves unsupervised,
we explore how to select the data so that only
small amounts of training data are needed because
for many dictionaries, gathering training data may
be expensive and labor-intensive.
3 Three Single Method Approaches for
Error Detection
Before we discuss our approaches for combining
systems, we briefly explain the three individual
systems that form the foundation of our combined
system.
First, we use a supervised approach where we
train a model using SVMlight (Joachims, 1999)
with a linear kernel and default regularization pa-
rameters. We use a depth first traversal of the
XML tree and use unigrams and bigrams of the
tags that occur as features for each subtree to
make a classification decision.
We also explore two unsupervised approaches.
The first unsupervised approach learns rules for
when to classify nodes as errors or not. The rule-
based method computes an anomaly score based
on the probability of subtree structures. Given
a structure A and its probability P(A), the event
that A occurs has anomaly score 1-P(A) and the
event that A does not occur has anomaly score
P(A). The basic idea is if a certain structure hap-
pens rarely, i.e. P(A) is very small, then the oc-
currence of A should have a high anomaly score.
On the other hand, if A occurs frequently, then
the absence of A indicates anomaly. To obtain
the anomaly score of a tree, we simply take the
maximal scores of all events induced by subtrees
within this tree.
The second unsupervised approach uses a reim-
plementation of the language modeling method
described in (Rodrigues et al, 2011). Briefly,
this methods works by calculating the probabil-
ity a flattened XML branch can occur, given a
probability model trained on the XML branches
from the original dictionary. We used (Stolcke,
2002) to generate bigram models using Good Tur-
ing smoothing and Katz back off, and evaluated
the log probability of the XML branches, ranking
the likelihood. The first 1000 branches were sub-
mitted to the hybrid system marked as an error,
and the remaining were submitted as a non-error.
Results for the individual classifiers are presented
in section 5.
4 Three Methods for Combining
Systems
We investigate three methods for combining the
three individual methods. As a baseline, we in-
vestigate simple majority vote. This method takes
the classification decisions of the three methods
and assigns the final classification as the classifi-
cation that the majority of the methods predicted.
A drawback of majority vote is that it does not
weight the votes at all. However, it might make
sense to weight the votes according to factors such
as the strength of the classification score. For ex-
ample, all of our classifiers make binary decisions
but output scores that are indicative of the confi-
dence of their classifications. Therefore we also
explore a score combination method that consid-
ers these scores. Since measures from the differ-
ent systems are in different ranges, we normal-
ize these measurements before combining them
(Jain et al, 2005). We use z-score which com-
80
putes the arithmetic mean and standard deviation
of the given data for score normalization. We then
take the summation of normalized measures as
the final measure. Classification is performed by
thresholding this final measure.1
Another approach would be to weight them by
the performance level of the various constituent
classifiers in the ensemble. Weighting based on
performance level of the individual classifiers is
difficult because it would require extra labeled
data to estimate the various performance lev-
els. It is not clear how to translate the differ-
ent performance estimates into weights, or how
to have those weights interact with weights based
on strengths of classification. Therefore, we did
not weigh based on performance level explicitly.
We believe that our third combination method,
the use of random forests, implicitly cap-
tures weighting based on performance level and
strengths of classifications. Our random forest ap-
proach uses three features, one for each of the in-
dividual systems we use. With random forests,
strengths of classification are taken into account
because they form the values of the three fea-
tures we use. In addition, the performance level
is taken into account because the training data
used to train the decision trees that form the for-
est help to guide binning of the feature values into
appropriate ranges where classification decisions
are made correctly. This will be discussed further
in section 5.
5 Experiments
This section explains the details of the experi-
ments we conducted testing the performance of
the various individual and combined systems.
Subsection 5.1 explains the details of the data we
experiment on; subsection 5.2 provides a sum-
mary of the main results of our experiments; and
subsection 5.3 discusses the results.
5.1 Experimental Setup
We obtained the data for our experiments using
a digitized version of (Qureshi and Haq, 1991),
the same Urdu-English dictionary that Zajic et
al. (2011) had used. Zajic et al (2011) pre-
sented DML, a programming language used to
fix errors in XML documents that contain lexico-
graphic data. A team of language experts used
1In our experiments we used 0 as the threshold.
Recall Precision F1-Measure Accuracy
LM 11.97 89.90 21.13 57.53
RULE 99.79 70.83 82.85 80.37
FV 35.34 93.68 51.32 68.14
Table 1: Performance of individual systems at
ENTRY tier.
DML to correct errors in a digital, XML repre-
sentation of the Kitabistan Urdu dictionary. The
current research compared the source XML doc-
ument and the DML commands to identify the el-
ements that the language experts decided to mod-
ify. We consider those elements to be errors. This
is the ground truth used for training and evalua-
tion. We evaluate at two tiers, corresponding to
two node types in the XML representation of the
dictionary: ENTRY and SENSE. The example de-
picted in Figures 1 through 5 shows an example of
SENSE. The intuition of the tier is that errors are
detectable (or learnable) from observing the ele-
ments within a tier, and do not cross tier bound-
aries. These tiers are specific to the Kitabistan
Urdu dictionary, and we selected them by observ-
ing the data. A limitation of our work is that we do
not know at this time whether they are generally
useful across dictionaries. Future work will be
to automatically discover the meaningful evalua-
tion tiers for a new dictionary. After this process,
we have a dataset with 15,808 Entries, of which
47.53% are marked as errors and 78,919 Senses,
of which 10.79% are marked as errors. We per-
form tenfold cross-validation in all experiments.
In our random forest experiments, we use 12 de-
cision trees, each with only 1 feature.
5.2 Results
This section presents experimental results, first
for individual systems and then for combined sys-
tems.
5.2.1 Performance of individual systems
Tables 1 and 2 show the performance of lan-
guage modeling-based method (LM), rule-based
method (RULE) and the supervised feature-based
method (FV) at different tiers. As can be seen,
at the ENTRY tier, RULE obtains the highest F1-
Measure and accuracy, while at the SENSE tier,
FV performs the best.
81
Recall Precision F1-Measure Accuracy
LM 9.85 94.00 17.83 90.20
RULE 84.59 58.86 69.42 91.96
FV 72.44 98.66 83.54 96.92
Table 2: Performance of individual systems at
SENSE tier.
5.2.2 Improving individual systems using
random forests
In this section, we show that by applying ran-
dom forests on top of the output of individual sys-
tems, we can have gains (absolute gains, not rel-
ative) in accuracy of 4.34% to 6.39% and gains
(again absolute, not relative) in F1-measure of
3.64% to 11.39%. Tables 3 and 4 show our ex-
perimental results at ENTRY and SENSE tiers
when applying random forests with the rule-based
method.2 These results are all obtained from 100
iterations of the experiments with different parti-
tions of the training data chosen at each iteration.
Mean values of different evaluation measures and
their standard deviations are shown in these ta-
bles. We change the percentage of training data
and repeat the experiments to see how the amount
of training data affects performance.
It might be surprising to see the gains in per-
formance that can be achieved by using a ran-
dom forest of decision trees created using only
the rule-based scores as features. To shed light
on why this is so, we show the distribution of
RULE-based output scores for anomaly nodes and
clean nodes in Figure 6. They are well separated
and this explains why RULE alone can have good
performance. Recall RULE classifies nodes with
anomaly scores larger than 0.9 as errors. How-
ever, in Figure 6, we can see that there are many
clean nodes with anomaly scores larger than 0.9.
Thus, the simple thresholding strategy will bring
in errors. Applying random forest will help us
identify these errorful regions to improve the per-
formance. Another method for helping to identify
these errorful regions and classify them correctly
is to apply random forest of RULE combined with
the other methods, which we will see will even
further boost the performance.
2We also applied random forests to our language mod-
eling and feature-based methods, and saw similar gains in
performance.
0.55 0.6 0.65 0.7 0.75 0.8 0.85 0.9 0.95 1
0
500
1000
1500
output score of rule-based system
o
c
c
u
r
r
e
n
c
e
s
 
 
anomaly
clean
Figure 6: Output anomalies score from RULE
(ENTRY tier).
5.2.3 System combination
In this section, we explore different methods
for combining measures from the three systems.
Table 5 shows the results of majority voting and
score combination at the ENTRY tier. As can
be seen, majority voting performs poorly. This
may be due to the fact that the performances of
the three systems are very different. RULE sig-
nificantly outperforms the other two systems, and
as discussed in Section 4 neither majority voting
nor score combination weights this higher perfor-
mance appropriately.
Tables 6 and 7 show the results of combining
RULE and LM. This is of particular interest since
these two systems are unsupervised. Combin-
ing these two unsupervised systems works better
than the individual methods, including supervised
methods. Tables 8 and 9 show the results for com-
binations of all available systems. This yields the
highest performance, but only slightly higher than
the combination of only unsupervised base meth-
ods.
The random forest combination technique does
require labeled data even if the underlying base
methods are unsupervised. Based on the ob-
servation in Figure 6, we further study whether
choosing more training data from the most error-
ful regions will help to improve the performance.
Experimental results in Table 10 show how the
choice of training data affects performance. It
appears that there may be a weak trend toward
higher performance when we force the selection
of the majority of the training data to be from
ENTRY nodes whose RULE anomaly scores are
82
Training % Recall Precision F1-Measure Accuracy
0.1 78.17( 14.83) 75.87( 3.96) 76.18( 7.99) 77.68( 5.11)
1 82.46( 4.81) 81.34( 2.14) 81.79( 2.20) 82.61( 1.69)
10 87.30( 1.96) 84.11( 1.29) 85.64( 0.46) 86.10( 0.35)
50 89.19( 1.75) 83.99( 1.20) 86.49( 0.34) 86.76( 0.28)
Table 3: Mean and std of evaluation measures from 100 iterations of experiments using RULE+RF.
(ENTRY tier)
Training % Recall Precision F1-Measure Accuracy
0.1 60.22( 12.95) 69.66( 9.54) 63.29( 7.92) 92.61( 1.57)
1 70.28( 3.48) 86.26( 3.69) 77.31( 1.39) 95.55( 0.25)
10 71.52( 1.23) 91.26( 1.39) 80.18( 0.41) 96.18( 0.07)
50 72.11( 0.75) 91.90( 0.64) 80.81( 0.39) 96.30( 0.06)
Table 4: Mean and std of evaluation measures from 100 iterations of experiments using RULE+RF.
(SENSE tier)
larger than 0.9. However, the magnitudes of the
observed differences in performance are within a
single standard deviation so it remains for future
work to determine if there are ways to select the
training data for our random forest combination
in ways that substantially improve upon random
selection.
5.3 Discussion
Majority voting (at the entry level) performs
poorly, since the performance of the three individ-
ual systems are very different and majority voting
does not weight votes at all. Score combination
is a type of weighted voting. It takes into account
the confidence level of output from different sys-
tems, which enables it to perform better than ma-
jority voting. However, score combination does
not take into account the performance levels of
the different systems, and we believe this limits its
performance compared with random forest com-
binations.
Random forest combinations perform the best,
but the cost is that it is a supervised combination
method. We investigated how the amount of train-
ing data affects the performance, and found that a
small amount of labeled data is all that the random
forest needs in order to be successful. Moreover,
although this requires further exploration, there is
weak evidence that the size of the labeled data can
potentially be reduced by choosing it carefully
from the region that is expected to be most error-
ful. For our application with a rule-based system,
this is the high-anomaly scoring region because
although it is true that anomalies are often errors,
it is also the case that some structures occur rarely
but are not errorful.
RULE+LM with random forest is a little bet-
ter than RULE with random forest, with gain of
about 0.7% on F1-measure when evaluated at the
ENTRY level using 10% data for training.
An examination of examples that are marked as
being errors in our ground truth but that were not
detected to be errors by any of our systems sug-
gests that some examples are decided on the ba-
sis of features not yet considered by any system.
For example, in Figure 7 the second FORM is
well-formed structurally, but the Urdu text in the
first FORM is the beginning of the phrase translit-
erated in the second FORM. Automatic systems
detected that the first FORM was an error, how-
ever did not mark the second FORM as an error
whereas our ground truth marked both as errors.
Examination of false negatives also revealed
cases where the systems were correct that there
was no error but our ground truth wrongly indi-
cated that there was an error. These were due to
our semi-automated method for producing ground
truth that considers elements mentioned in DML
commands to be errors. We discovered instances
in which merely mentioning an element in a DML
command does not imply that the element is an er-
ror. These cases are useful for making refinements
to how ground truth is generated from DML com-
mands.
Examination of false positives revealed two
categories. One was where the element is indeed
an error but was not marked as an element in our
ground truth because it was part of a larger error
83
Method Recall Precision F1-Measure Accuracy
Majority voting 36.71 90.90 52.30 68.18
Score combination 76.48 75.82 76.15 77.23
Table 5: LM+RULE+FV (ENTRY tier)
Training % Recall Precision F1-Measure Accuracy
0.1 77.43( 15.14) 72.77( 6.03) 74.26( 8.68) 75.32( 6.71)
1 86.50( 3.59) 80.41( 1.95) 83.27( 1.33) 83.51( 1.11)
10 88.12( 1.12) 84.65( 0.57) 86.34( 0.46) 86.76( 0.39)
50 89.12( 0.62) 87.39( 0.56) 88.25( 0.30) 88.72( 0.29)
Table 6: System combination based on random forest (LM+RULE). (ENTRY tier, mean (std))
Training % Recall Precision F1-Measure Accuracy
0.1 65.85( 12.70) 71.96( 7.63) 67.68( 7.06) 93.38( 1.03)
1 80.29( 3.58) 84.97( 3.13) 82.45( 1.36) 96.31( 0.28)
10 82.68( 2.49) 90.91( 2.37) 86.53( 0.41) 97.22( 0.07)
50 83.22( 2.43) 92.21( 2.29) 87.42( 0.35) 97.42( 0.04)
Table 7: System combination based on random forest (LM+RULE). (SENSE tier, mean (std))
Training % Recall Precision F1-Measure Accuracy
20 91.57( 0.55) 87.77( 0.43) 89.63( 0.23) 89.93( 0.22)
50 92.04( 0.54) 88.85( 0.48) 90.41( 0.29) 90.72( 0.28)
Table 8: System combination based on random forest (LM+RULE+FV). (ENTRY tier, mean (std))
Training % Recall Precision F1-Measure Accuracy
20 86.47( 1.01) 90.67( 1.02) 88.51( 0.26) 97.58( 0.06)
50 86.50( 0.81) 92.04( 0.85) 89.18( 0.30) 97.73( 0.06)
Table 9: System combination based on random forest (LM+RULE+FV). (SENSE tier, mean (std))
Recall Precision F1-Measure Accuracy
50% 85.40( 4.65) 80.71( 3.49) 82.82( 1.57) 82.63( 1.54)
70% 86.13( 3.94) 80.97( 2.64) 83.36( 1.33) 83.30( 1.21)
90% 85.77( 3.61) 81.82( 2.72) 83.65( 1.45) 83.69( 1.35)
95% 85.93( 3.46) 82.14( 2.98) 83.89( 1.32) 83.94( 1.18)
random 86.50( 3.59) 80.41( 1.95) 83.27( 1.33) 83.51( 1.11)
Table 10: Effect of choice of training data based on rule based method (Mean evaluation measures
from 100 iterations of experiments using RULE+LM at ENTRY tier). We choose 1% of the data for
training and the first column in the table specifies the percentage of training data chosen from Entries
with anomalous score larger than 0.9.
84
Figure 7: Example of error in XML
that got deleted and therefore no DML command
ever mentioned the smaller element but lexicog-
raphers upon inspection agree that the smaller el-
ement is indeed errorful. The other category was
where there were actual errors that the dictionary
editors didn?t repair with DML but that should
have been repaired.
A major limitation of our work is testing how
well it generalizes to detecting errors in other dic-
tionaries besides the Urdu-English one (Qureshi
and Haq, 1991) that we conducted our experi-
ments on.
6 Conclusions
We explored hybrid approaches for the applica-
tion of automatically detecting errors in digitized
copies of dictionaries. The base methods we
explored consisted of a variety of unsupervised
and supervised methods. The combination meth-
ods we explored also consisted of some methods
which required labeled data and some which did
not.
We found that our base methods had differ-
ent levels of performance and with this scenario
majority voting and score combination methods,
though appealing since they require no labeled
data, did not perform well since they do not
weight votes well.
We found that random forests of decision trees
was the best combination method. We hypothe-
size that this is due to the nature of our task and
base systems. Random forests were able to help
tease apart the high-error region (where anoma-
lies take place). A drawback of random forests
as a combination method is that they require la-
beled data. However, experiments reveal empiri-
cally that a relatively small amount of data is suf-
ficient and the amount might be able to be further
reduced through specific selection criteria.
Acknowledgments
This material is based upon work supported, in
whole or in part, with funding from the United
States Government. Any opinions, findings and
conclusions, or recommendations expressed in
this material are those of the author(s) and do not
necessarily reflect the views of the University of
Maryland, College Park and/or any agency or en-
tity of the United States Government. Nothing
in this report is intended to be and shall not be
treated or construed as an endorsement or recom-
mendation by the University of Maryland, United
States Government, or the authors of the product,
process, or service that is the subject of this re-
port. No one may use any information contained
or based on this report in advertisements or pro-
motional materials related to any company prod-
uct, process, or service or in support of other com-
mercial purposes.
References
Leo Breiman. 2001. Random forests. Machine
Learning, 45:5?32. 10.1023/A:1010933404324.
David A. Cohn, Les Atlas, and Richard Ladner. 1994.
Improving generalization with active learning. Ma-
chine Learning, 15:201?221.
David A. Cohn, Zoubin Ghahramani, and Michael I.
Jordan. 1996. Active learning with statistical mod-
els. Journal of Artificial Intelligence Research,
4:129?145.
Yoav Freund, H. Sebastian Seung, Eli Shamir, and
Naftali Tishby. 1997. Selective sampling using the
query by committee algorithm. Machine Learning,
28:133?168.
Anil K. Jain, Karthik Nandakumar, and Arun Ross.
2005. Score normalization in multimodal biometric
systems. Pattern Recognition, pages 2270?2285.
Thorsten Joachims. 1999. Making large-scale SVM
learning practical. In Bernhard Scho?lkopf, Christo-
pher J. Burges, and Alexander J. Smola, editors, Ad-
vances in Kernel Methods ? Support Vector Learn-
ing, chapter 11, pages 169?184. The MIT Press,
Cambridge, US.
J. Kittler, M. Hatef, R.P.W. Duin, and J. Matas.
1998. On combining classifiers. Pattern Analysis
and Machine Intelligence, IEEE Transactions on,
20(3):226 ?239, mar.
L. Lam and S.Y. Suen. 1997. Application of majority
voting to pattern recognition: an analysis of its be-
havior and performance. Systems, Man and Cyber-
netics, Part A: Systems and Humans, IEEE Trans-
actions on, 27(5):553 ?568, sep.
David D. Lewis and William A. Gale. 1994. A se-
quential algorithm for training text classifiers. In
SIGIR ?94: Proceedings of the 17th annual inter-
national ACM SIGIR conference on Research and
development in information retrieval, pages 3?12,
85
New York, NY, USA. Springer-Verlag New York,
Inc.
Yan Ma, Bojan Cukic, and Harshinder Singh. 2005.
A classification approach to multi-biometric score
fusion. In AVBPA?05, pages 484?493.
Verlinde P. and G. Chollet. 1999. Comparing deci-
sion fusion paradigms using k-nn based classifiers,
decision trees and logistic regression in a multi-
modal identity verification application. In Proceed-
ings of the 2nd International Conference on Audio
and Video-Based Biometric Person Authentication
(AVBPA), pages 189?193.
Bashir Ahmad Qureshi and Abdul Haq. 1991. Stan-
dard Twenty First Century Urdu-English Dictio-
nary. Educational Publishing House, Delhi.
Paul Rodrigues, David Zajic, David Doermann,
Michael Bloodgood, and Peng Ye. 2011. Detect-
ing structural irregularity in electronic dictionaries
using language modeling. In Proceedings of the
Conference on Electronic Lexicography in the 21st
Century, pages 227?232, Bled, Slovenia, Novem-
ber. Trojina, Institute for Applied Slovene Studies.
Arun Ross and Anil Jain. 2003. Information fusion in
biometrics. Pattern Recognition Letters, 24:2115?
2125.
H. S. Seung, M. Opper, and H. Sompolinsky. 1992.
Query by committee. In COLT ?92: Proceedings of
the fifth annual workshop on Computational learn-
ing theory, pages 287?294, New York, NY, USA.
ACM.
Andreas Stolcke. 2002. Srilm - an extensible language
modeling toolkit. In Proceedings of the Interna-
tional Conference on Spoken Language Processing.
Sergey Tulyakov, Stefan Jaeger, Venu Govindaraju,
and David Doermann. 2008. Review of classi-
fier combination methods. In Machine Learning in
Document Analysis and Recognition, volume 90 of
Studies in Computational Intelligence, pages 361?
386. Springer Berlin / Heidelberg.
David Zajic, Michael Maxwell, David Doermann, Paul
Rodrigues, and Michael Bloodgood. 2011. Cor-
recting errors in digital lexicographic resources us-
ing a dictionary manipulation language. In Pro-
ceedings of the Conference on Electronic Lexicog-
raphy in the 21st Century, pages 297?301, Bled,
Slovenia, November. Trojina, Institute for Applied
Slovene Studies.
86
