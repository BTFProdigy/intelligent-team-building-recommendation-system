Proceedings of the 22nd International Conference on Computational Linguistics (Coling 2008), pages 81?88
Manchester, August 2008
A Concept-Centered Approach to Noun-Compound Interpretation 
Cristina Butnariu 
School of Computer Science and Informatics 
University College Dublin 
Belfield, Dublin 4 
Ioana.Butnariu@ucd.ie 
Tony Veale 
School of Computer Science and Informatics 
University College Dublin 
Belfield, Dublin 4 
Tony.Veale@ucd.ie 
 Abstract 
A noun-compound is a compressed 
proposition that requires an audience to 
recover the implicit relationship between 
two concepts that are expressed as nouns. 
Listeners recover this relationship by 
considering the most typical relations 
afforded by each concept. These 
relational possibilities are evident at a 
linguistic level in the syntagmatic 
patterns that connect nouns to the verbal 
actions that act upon, or are facilitated 
by, these nouns. We present a model of 
noun-compound interpretation that first 
learns the relational possibilities for 
individual nouns from corpora, and 
which then uses these to hypothesize 
about the most likely relationship that 
underpins a noun compound. 
1 Introduction 
Noun compounds hide a remarkable depth of 
conceptual machinery behind a simple syntactic 
form, Noun-Noun, and thus pose a considerable 
problem for the computational processing of 
language (Johnston and Busa, 1996). It is not just 
that compounds are commonplace in language, 
or that their interpretation requires a synthesis of 
lexical, semantic, and pragmatic information 
sources (Finin, 1980); compounds provide a 
highly compressed picture of the workings of 
concept combination, so there are as many ways 
of interpreting a noun compound as there are 
ways of combining the underlying concepts 
(Gagn?, 2002). Linguists have thus attempted to 
                                                 
? 2008. Licensed under the Creative Commons 
Attribution-Noncommercial-Share Alike 3.0 Unported 
license (http://creativecommons.org/licenses/by-nc-
sa/3.0/). Some rights reserved. 
 
understand noun-compounds as full propositions 
in which a phrase with two nouns connected by 
an explicit relation ? usually expressed as a verb 
and a preposition ? is compressed into a pair of 
nouns (Levi, 1978). Since these noun-pairs must 
allow an audience to reconstruct the 
decompressed proposition, there must be some 
systematic means by which the missing relation 
can easily be inferred. 
This framing of the problem as a search for a 
missing relation suggests two broad strategies for 
the interpretation of compounds. In the first, the 
top-down strategy, we assume that there are only 
so many ways of combining two concepts; by 
enumerating these ways, we can view the 
problem of interpretation as a problem of 
classification, in which compounds are placed 
into separate classes that each correspond to a 
single manner of concept connection (Kim and 
Baldwin, 2006), (Nastase and Szpakowicz, 
2003). This strategy explicitly shaped the 
SemEval task on classifying semantic relations 
between nominals (Girju et al, 2007) and so is 
employed by all of the systems that participated 
in that task. In the second, the bottom-up 
strategy, we assume that it is futile to try and 
enumerate the many ways in which concepts can 
relationally combine, but look instead to large 
corpora to discover the ways in which different 
word combinations are explicitly framed by 
language (Nakov, 2006), (Turney, 2006a). 
In this paper we describe an approach that 
employs the bottom-up strategy with an open- 
rather than closed-inventory of inter-concept 
relations. These relations are acquired from the 
analysis of large corpora, such as the Web IT 
corpus of Google n-grams (Brants and Franz, 
2006). We argue that an understanding of noun-
compounds requires an understanding of 
lexicalized concept combination, which in turn 
requires an understanding of how lexical 
concepts can be used and connected to others. As 
such, we do not use corpora as a means of 
81
characterizing noun-compounds themselves, but 
as a means of characterizing the action 
possibilities of the individual nouns that can 
participate in a compound. In other words, we 
attempt to characterize those properties of 
different concepts denoted by nouns to help 
predict how those nouns will combine with 
others, and through which relations. For instance, 
?diamonds? can be used to cover and encrust 
jewelry or to provide a sharp tip for various 
tools; we see the former usage in ?diamond 
bracelet? and the latter in ?diamond saw?. 
Likewise, ?cheese? is a solid substance which 
can be cut, as in ?cheese knife?, an edible 
substance that can used as a filling, as in ?cheese 
sandwich?, and a substance that can be melted as 
a topping, as in ?cheese pizza?. It follows that a 
sandwich can be filled, a pizza can be topped, 
knives can cut and a bracelet can have a covering 
of gems. We use relational possibilities as a 
general term for what are sometimes called the 
qualia of a word (Pustejovsky, 1995), and learn 
the linguistic relational possibilities of nouns by 
seeking out specific textual patterns in corpora. 
In section 2 we consider the most currently 
relevant elements of the substantial body of past 
work in this area. In section 3 we describe how 
corpus analysis is used to identify the most 
common lexico-semantic relational possibilities 
of nouns, while in section 4 we describe how 
these relations are used, in conjunction with web-
based validation, to interpret specific noun-
compounds. We present an evaluation of this 
approach in section 5 and conclude the paper 
with some final remarks in section 6. 
2 Related Work 
Machine-learning and example-based approaches 
to noun-compounds generally favor the top-down 
strategy for defining relations, since it allows 
training data and exemplars/cases to be labeled 
using a fixed inventory of relational classes. As 
noted earlier, this strategy is characteristic of the 
systems that participated in the SemEval task on 
classifying semantic relations between nominals 
(Girju et al, 2007), such as Butnariu and Veale 
(2007). Though the inventory is fixed in size, it 
can be defined using varying levels of 
abstraction; for instance, Nastase and 
Szpakowicz (2003) use an inventory of 35 
relations, 5 of which are top level relations with 
the remaining 30 at the lower level. The top-
down strategy pre-dates these computational 
approaches, and is a key aspect of the 
foundational work of Levi (1978) and of 
subsequent work by Gagn? and Shoben (1997), 
both of whom posit a small set of semantic 
relations as underpinning all noun compounds. 
More recently, Kim and Baldwin (2005) use a 
fixed inventory of semantic relations to annotate 
a case-base of examples; new compounds are 
understood by determining their lexical similarity 
to the closest annotated compounds in the case-
base. Kim and Baldwin (2006) link their 
relations to specific seed verbs that linguistically 
convey these relations, and then train a classifier 
to recognize which semantic relation is implied 
by a pair of nouns connected by a given 
intervening verb. This approach appears to be 
sensitive to the number of seed verbs; on a test 
involving 453 noun compounds, an accuracy of 
52.6% is achieved with 84 seed verbs, but just 
46.6% with 57 seed verbs. 
Verbs understandably play a key role in the 
interpretation of compounds, since some kind of 
predicate must be recovered to link both nouns. 
For instance, Levi (1978) uses verbs to make 
explicit the implicit relation between the nouns 
of a compound, while Finin (1980) characterizes 
the relation in a noun-noun compound using an 
inventory of all the possible verbs that can link 
both nouns; thus, e.g. salt water is interpreted 
using a relation like dissolved_in. Nakov (2006) 
takes a similar approach, and uses verb-centred 
paraphrases to express the semantic relations 
between the nouns of a compound. He argues 
that the meaning of a compound is best 
expressed via a collection of appropriate verbs 
rather than via the abstract relations (such as 
Cause, Location) that are used in more 
traditional approaches, such as those of Levi 
(1978) and Gagn? (2002).  
Nakov (2006) pursues a bottom-up strategy in 
which an open-ended inventory of relations is 
discovered using linguistic evidence. Turney 
(2006a, 2006b) similarly pursues a bottom-up, 
data-driven approach, in which semantic 
relations are expressed via representative lexico-
syntactic patterns that are mined from large text 
corpora. Turney (2006a) sorts these relational 
patterns by pertinence, a measure that reflects the 
similarity of the noun pairs in the corpus in 
which each pattern is observed to occur. Patterns 
which are relatively unambiguous and which 
serve to cluster noun pairs with similar meanings 
have higher pertinence than those that do not.  
The approach described here is similarly corpus-
based and verb-centric, but it is also noun-centric 
rather than pair-centric, which is to say, we use 
82
corpus analysis to learn about the relational 
behavior of individual nouns rather than pairs of 
nouns. Like many other authors, from Finin 
(1980) to Nakov (2006), we see the problem of 
compound interpretation as a problem of 
paraphrase generation, in which a suitable verb 
(with an optional preposition) is used to 
linguistically re-frame the compound as a 
complete proposition. This linguistic frame is a 
relational possibilities of one of the nouns that is 
apt for the other. Following Gagn? and Shoben 
(1997), this relational possibilities is frequently 
suggested by the modifier noun, but as we now 
describe, it may also be suggested by the head. 
3 Acquisition of Relational Possibilities  
The meaning of a noun compound can be 
paraphrased in a variety of ways. For instance, 
consider the compound ?headache pill?, which 
might be paraphrased as follows: 
 
P1: headache-inducing pill 
P2: headache prevention pill 
P3: pill for treating headaches 
P4: pill that causes headaches 
P5: pill that is prescribed for 
headaches 
P6: pill that prevents headaches 
 
Some paraphrases are syntactic variants of others 
(e.g., P2 and P6), others employ lexical variation 
(e.g., P1 and P4) and others are co-descriptions 
of the same event (e.g., P3 and P5 or P5 and P6). 
It thus seems unreasonable to try and reduce 
these meanings to a single semantic relation, 
since the compound can be used to mean several 
of P1 ? P6 simultaneously. Rather than try to 
construct an inventory of logical relations, closed 
or otherwise, we shall instead treat linguistic 
frames like ?for treating X?, ?that prevents X?. 
etc. as proxies for the relations themselves, while 
retaining the capacity to treat syntactic variants 
as proxies for the same relation. Moreover, these 
linguistic frames are relational possibilities of 
specific words, so that ?-inducing X? is a 
relational possibility of ?headache? while ?for 
treating X? is a relational possibility of ?pill?. 
Thus, a compound of the form ?headache X? 
might be re-framed as ?headache-inducing X? 
and a compound of the form ?X pill? might be 
re-framed as ?pill that prevents X?, ?pill that 
causes X? or ?pill for treating X?. 
The relational possibilities of individual words 
can be acquired from a large n-gram corpus like 
that of Brants and Franz (2006), as derived from 
Google?s web index. Table 1 summarizes the 
linguistic relational possibilities that can be 
derived from specific n-gram patterns. 
Google n-gram 
pattern 
Relational  
possibilities 
Logical Form 
X ? Verb+ing X Verb+ing Y verb(X, Y) 
X ? Verb+ed X Verb+ed Y verb(X, Y) 
Verb+ed prep X Y Verb+ed prep X verb_prep(Y, X)
X Verb+ed X Verb+ed prep Y verb_prep(X, Y)
for Verb+ing X Y for Verb+ing X verb(Y, X) 
X for Verb+ing X for Verb+ing Y verb(X, Y) 
that Verb+s X Y that Verb+s X verb(Y, X) 
X that Verb+s X that Verb+s Y verb(X, Y) 
 
Table 1. For an anchor noun X, the n-gram 
(left) suggests relational possibilities (middle) to 
link to a generic noun Y; different linguistic 
relational possibilities can have the same logical 
form (right). 
 
For example, we extract the following 
linguistic relational possibilities for the noun 
?diamond?, where Google frequencies are given 
in parentheses:  
accented_with_diamonds(4224), 
encrusted_with_diamonds(3990), 
decorated_with_diamonds(2616), 
based_on_diamond(2148), 
covered_with_diamonds(2018), 
filled_with_diamonds(1942), 
adorned_with_diamonds(1462), 
coated_with_diamond(1150), 
for_buying_diamonds(618), 
for_grading_diamonds(342), 
for_cutting_diamonds(430), 
dusted_in_diamond(168), 
bedecked_with_diamonds(140), 
tipped_with_diamond(108), 
crowned_with_diamonds(98), 
for_exporting_diamonds(98), 
embossed_with_diamond(90), 
edged_with_diamonds(82), 
drilled_with_diamond(86), 
that_sells_diamonds(44)?
A hat may be crowned with diamonds, a watch 
decorated with diamonds, a bracelet covered 
with diamonds, a throne encrusted with 
diamonds and a king bedecked with diamonds ? 
each is an elaboration of a basic covering 
relation, but each adds nuances of its own that 
we do not want to lose in an interpretation that is 
83
maximally specific to the nouns concerned. We 
therefore take the view that relations should be as 
open-ended and nuanced as the linguistic 
evidence suggests; if one needs to see two 
different relations as broadly equivalent, 
resources like WordNet (Fellbaum, 1998) can be 
used to make the generalization. 
4 Interpreting Noun Compounds 
We see interpretation of a compound M-H as a 
two-stage process of divergent generation 
followed by convergent validation. The 
generation process simply considers the 
relational possibilities associated either with the 
modifier M or the head H and generates a 
paraphrase from each. Consider the compound 
?yeast bread? where M = ?yeast? and H = 
?bread?; the relational possibilities for ?yeast? 
and ?bread? and used to generate a set of 
potential paraphrases as shown in Table 2. For 
clarity, ?M? and ?H? denote the parts of each 
paraphrase frame that will be filled with the 
modifier and head respectively. 
 
Relational possibilities 
for M 
Paraphrases for M-
H 
H Verb+ed prep M 
e.g., H derived from yeast 
H Verb+ed prep M 
e.g., bread derived 
from yeast 
H that Verb+s M 
e.g., H that contains yeast 
H that Verb+s M 
e.g., bread that 
contains yeast 
Relational possibilities 
for H 
Paraphrase for M-H 
H Verb+ed prep M 
e.g., bread prepared with 
M 
H Verb+ed prep M 
e.g., bread prepared 
with yeast 
H that Verb+s M 
e.g., bread that has M 
H that Verb+s M 
e.g., bread that has 
yeast 
Table 2. Relational possibilities of the head (H) 
and modifier (M) nouns used for paraphrasing. 
 
The relational possibilities for the head noun 
?bread? yield the following paraphrases, where 
numbers in parentheses are Google frequencies 
for the original n-grams on which each 
paraphrase is based: 
 
?bread made from yeast? (6335), 
?bread topped with yeast? 
(6043), ?bread made with yeast? 
(4726), ?bread stuffed with 
yeast? (3871), ?bread baked in 
yeast? (3341), bread made of 
yeast? (3064), ?bread served 
with yeast? (3012), ?bread 
soaked in yeast? (2975), ?bread 
dipped in yeast? (2873), ?bread 
filled with yeast? (2783), ... 
 
Similarly, the relational possibilities for the 
modifier noun ?yeast? yield the following 
paraphrases: 
 
?bread expressed in yeast? 
(14058), ?bread leavened with 
yeast? (10816), ?bread derived 
from yeast? (2562), ?bread 
based on yeast? (1200), ?bread 
fermented with yeast? (842), 
?bread raised with yeast? 
(736), ?bread induced in yeast? 
(342) , ?bread infected with 
yeast? (262), ?bread filled 
with yeast? (120), ? 
 
While these two sets of relational possibilities 
capture the most salient activities in which 
?yeast? and  ?bread? participate, many of the 
paraphrases listed here are inappropriate for 
?yeast bread?. Candidate paraphrases for a noun 
compound are useful only when one has a means 
of determining the degree to which paraphrases 
are meaningful and apt and of rejecting those 
which are not. This process typically assumes 
that a meaningful paraphrase is one for which 
evidence of prior usage can be found in a large 
corpus (like the web); the greater this evidence, 
the more favored a given paraphrase should be. 
This assumption is central to Nakov (2006), who 
uses templates to find paraphrases for a noun 
compound on the web. These templates use the 
Google wildcard * to indicate the position of a 
verb so that the specific verbs at the heart of a 
paraphrase can be mined from the snippets that 
are returned. Nakov (2007) uses the schematic 
patterns ?N1 that * N2?,  ?N2 that * N1?,  ?N1 * 
N2? and ?N2 * N1?, where the wildcard can 
stand for as many a eight contiguous words. 
Relational possibilities allow us, in the first 
divergent stage of interpretation, to generate 
fully-formed paraphrases that do not require 
wildcards, so the second convergent stage of 
interpretation simply needs to validate these 
paraphrases by finding one or more instances of 
each on the web. Indeed, an especially 
compelling paraphrase may be found in the 
Google n-grams themselves, without recourse to 
the web. For instance, the paraphrase ?bread 
leavened with yeast? has a frequency of 56 in the 
database of Google 4-grams, while the 
84
paraphrase ?bread based on yeast? has such a 
low web frequency of 2 hits that it can be 
validated only by going to the web.  
But web-based validation has its limitations: it 
cannot account for novel and creative 
compounds, nor can it account for conventional 
compounds whose meaning is not echoed in an 
expanded paraphrase-form on the web. Thus, we 
also consider an alternate validation procedure 
for those paraphrases that can  be generated both 
from a modifier noun relational possibility and 
from a head noun relational possibility. For 
example, ?bread filled with yeast? can be derived 
from the head relational possibility ?bread filled 
with X? which has a frequency of 2783, and 
from the modifier relational possibility ?X filled 
with yeast? which has a frequency of just 120. 
This dual basis for generation provides evidence 
that the paraphrase is meaningful without the 
need to actually find the paraphrase on the web. 
We refer to the validation of paraphrases in this 
way as validation by matching relational 
possibilities of the modifier and head nouns. 
This matching relational possibilities 
procedure does not require web validation, and 
so does not produce a web frequency for each 
paraphrase. We thus need to assign a score to a 
paraphrase based on the web frequencies of the 
matching relational possibilities that give rise to 
it. For simplicity, we add the web frequency of 
the head relational possibility (e.g., 2783 from 
?bread filled with X?) to the frequency of the 
modifier relational possibility (e.g., 120 from ?X 
filled with yeast?) to obtain an invented 
frequency for the generated paraphrase (e.g., 
2903 for ?bread filled with yeast?). 
The third and more restricted validation 
procedure we employ is a hybrid one, based on 
the intersection of the two procedures above: we 
require web-validation of paraphrases that are 
already validated by virtue of arising from 
matching head and modifier relational 
possibilities. In this case, we rank the 
paraphrases by their actual web frequency. The 
set of paraphrases validated by the hybrid 
approach will be a subset of the paraphrases 
validated by the other two validation methods; 
the size of this subset will be informative about 
the relative utility of each procedure. 
5 Empirical Evaluation 
To evaluate the relational possibility approach to 
noun-noun interpretation, we perform two 
experiments: one to consider how well the set of 
validated paraphrases can be mapped to the 
abstract relations used by (Nastase and 
Szpakowicz, 2003) to annotate their noun-noun 
compounds, and one to consider how well these 
paraphrases match the paraphrases offered by 
humans for the same noun compounds. To 
understand the role of different validation 
strategies, we use three variants of the model that 
correspond to the three means of validating 
paraphrases: model-1 uses the presence of the 
relational possibility on the web as the mark of a 
valid paraphrase; model-2 uses the matching 
relational possibilities procedure to validate a 
paraphrase (i.e., the paraphrase must arise from 
both an relational possibility of the modifier and 
of the head); a third model, model-3 intersects 
both validation procedures. In each case, 
validated paraphrases are ranked by their 
frequency scores, as found explicitly on the web 
in the case of model-1 and model-3, or as 
invented in model-2.   
5.1 Mapping compounds to abstract relations 
In the first experiment, we test the relational 
possibility model on a set of noun-noun 
compounds from Nastase and Szpakowicz 
(2003), whose data is pre-classified into abstract 
classes of semantic relations (i.e., Agent, 
Instrument, Location). We perform a manual 
analysis on the paraphrases that are generated 
and validated for each noun pair, to measure how 
accurately each paraphrase matches the pre-
classified abstract semantic relation. The Nastase 
and Szpakowicz (2003) dataset comprises 600 
word pairs of the form adj-noun, adv-noun and 
noun-noun; for this experiment we use only the 
329 noun-noun pairs, which are each pre-labeled 
with one of 28 different semantic relations. 
We consider and quantify two eventualities here: 
those situations in which the relational possibility 
model generates and validates a paraphrase that 
closely corresponds to the semantic relation 
assigned by Nastase and Szpakowicz (2003); and 
those situations in which the relational possibility 
model generates and validates an interpretation 
that a human judge considers a plausible and 
sensible interpretation of a compound regardless 
of Nastase and Szpakowicz (2003)?s 
interpretation. Table 3 presents validated 
relational possibilities for the compound ?olive 
oil?, where those that match the pre-classified 
relation are in bold, and those that are otherwise 
plausible are italicized. 
 
85
Paraphrases generated by 
web-based validation 
Paraphrases generated by 
matching relational possibilities 
Paraphrases generated by 
Nakov (2007) 
extracted from (189), obtained from 
(132), mixed with (87), made from 
(75), produced from (38), pressed 
from (35), colored (25), infused (20), 
enriched with (16), made of (14), 
flavored (13), made with (12), derived 
(10), based (10), produced by (9), 
blended with (8), coloured (7), based 
on (7), combined with (6), found in 
(6), dissolved in (6), served with (6), 
contained in (5), replaced by (4), 
flavoured (3), come from (3)?  
used in (25839), obtained from 
(15352), extracted from (14561), 
made from (11627), found in 
(11524), used for (9919), mixed with 
(9781), produced from (7794), 
produced by (6776), made with 
(5423), used as (4880),  are in (4577), 
contained in (4551), come from 
(4241), based on (4135), combined 
with (4029), added to (3848), made in 
(3608) ? 
come from (13), be 
obtained from (11), be 
extracted from (10), be 
made from (9), be 
produced from (7), be 
released from (4), taste 
like (4), be beaten from 
(3), be produced with (3) , 
emerge from (3) 
Table 3. Validated paraphrases for ?olive oil?; matches with Nastase and Szpakowicz are in 
bold; other sensible interpretations are italicized. 
 
We also consider the rank of the paraphrases 
that match the relations assigned by Nastase and 
Szpakowicz (2003) to this data set. Figure 1 
graphs the F-measure for the relational 
possibilities approach when this relation is the 
top-ranked validated paraphrase, when it is in the 
top two validated paraphrases, and  more 
generally, when  it is in the top n validated 
paraphrases, n <= 20. Model-1 (web-based 
validation) out-performs Model-2 (matching 
relational possibilities, with no web validation) 
when we consider just a small window of top 
ranked paraphrases, but this situation reverses as 
the window (whose size is given on the x-axis) is 
enlarged.  
F-measure (%) for top ranked paraphrases
0
10
20
30
40
50
60
70
80
90
1 3 5 7 9 11 13 15 17 19
Model 1
Model 2
Model 3
  
Figure 1. F-measure for target semantic 
relations of top n ranked paraphrases generated 
with Model-1, Model-2 and Model-3. 
Model 3 (which requires both matching 
relational possibilities and web validation) shows 
similar results to Model 2 (web validation only), 
which suggests that the matching relational 
possibilities criterion is strongly predictive of 
web-validation. This further suggests that 
matching relational possibilities alone can 
reliably validate a paraphrase even when web 
evidence is lacking, as will be the case in 
creative noun compounds. 
During this evaluation process, we observe a 
tendency for specific paraphrases to co-occur 
when conveying a certain relation. For instance, 
Y obtained from X typically co-occurs with Y 
produced from X to indicate Nastase and 
Szpakowicz?s Source relation, while Y caused by 
X co-occurs with Y induced by X to convey their 
Effect relation, and Y owned by X co-occur with 
Y held by X to indicate their Possessor relation. 
This observation is similar to that of Nakov 
(2007), who performs a manual analysis of 
paraphrases obtains from web-mining. The 
results he reports are similar to those obtained 
using the relational possibilities approach, as 
shown in Table 3. 
5.2 Comparing human-generated paraphrases 
In the second experiment, we compared the 
paraphrases validated by the relational 
possibilities approach to human-generated 
paraphrases reported by Nakov (2007) and to the 
paraphrases generated by Nakov?s own web-
mining approach to this task. Nakov (2007) 
collected human paraphrases for each noun-
compound in his data-set (250 noun compounds 
listed in the appendix of Levi, 1978) by asking 
subjects to rephrase a noun-compound using a 
relative-clause centred around a single verb with 
an optional preposition. This rephrasing elicited 
human-generated paraphrases like the following: 
 
'neck vein is a vein that comes 
from the neck'  
'neck vein is a vein that drains 
the neck' 
 
86
Nakov then extracted normalized verbs and 
prepositions from these paraphrases to obtain a 
reduced verb-based form for each, e.g., to obtain 
the reduced forms come from and drain from the 
above examples. He used 174 subjects for this 
task, to generate around 17,000 reduced forms, 
or 71 forms per compound.  
For each of his 250 noun pairs we constructed 
three vectors h, w, and a, using human-generated 
paraphrase verbs and their frequencies (h), 
Nakov's web-extracted verbs and their 
frequencies (w) and the verbs of the paraphrases 
obtained using the relational possibilities 
approach and their frequencies (a). Following 
Nakov (2007), we then calculated the cosine 
correlation between two frequency vectors using 
the formula: 
simcos(h,w) = ?hiwi   ? ??hi2 ?? wi2 
For ease of comparison, the a vector is 
populated with verbs and frequencies from just 
two patterns, Y Verb+ed Prep X and Y that 
Verb+s X. In Table 4 we report the average 
cosine correlation across the vectors for all 250 
noun pairs, to compare for the three validation 
models the relational possibilities-based and 
Nakov?s web-generated paraphrases and the 
relational possibilities -based and human-elicited 
paraphrases. Also shown, in the last row, is the 
average cosine correlation  between Nakov?s 
web-mined paraphrases and human-elicited 
paraphrases, as reported in Nakov (2007). 
 
Model 1 (web-validation)  
correlation to humans 26.8 % 
correlation to web-mined approach 27.1 % 
Model 2 (matching relational possibilities) 
correlation to humans 17 % 
correlation to web-mined approach 14.25 % 
Model 3 (intersection of Model-1 and Model-2) 
correlation to humans 27.9 % 
correlation to web-mined approach 28 % 
Web-mining (Nakov, 2007) 
correlation to humans 31.8% 
Table 4. Average correlation between web-
mined paraphrases and relational possibilities-
based paraphrases with human elicitations. 
 
The results show the difference in quality of 
the paraphrases validated by each of our models. 
The matching-relational possibilities model 
(Model 2) yields the largest number of 
paraphrases. In the first experiment we showed 
that this model outperforms the other two when 
we consider just top-ranked paraphrases, but here 
it appears that this wider range of potentially 
creative interpretations diminishes the cosine 
correlation with human-elicited interpretations. 
But the most plausible paraphrases come to the 
fore in the hybrid model (Model 3), whose 
paraphrases are a subset of those of Models 1 and 
2. This hybrid approach also outperforms Model 
1 and compares well with the results obtained by 
web mining. 
The difference in cosine correlation between 
human-elicited and relational possibitlies-based 
paraphrases in Model-3 (27.9%) and Nakov?s 
web-mined and human-elicited paraphrases 
(31.8%) can be justified both by the type of 
patterns used in the comparison, and by the type 
of patterns used to validate paraphrases. For one, 
we consider paraphrases generated using just two 
forms of relational possibilities, Y Verb+ed Prep 
X and Y that Verb+s X, since these can be 
directly compared to the type of relative-clause 
paraphrases used in this experiment. 
Furthermore, relational possibilities are derived 
from Google n-grams where n < 6, we allow up 
to four words to intervene between the modifier 
and the head in a paraphrase, while the web-
mining paraphrases benefit from a larger window 
of intervening words (up to 8). Nonetheless, in 
88 out of the 250 pairs, the correlation between 
relational possibilities-based and human-elicited 
paraphrases is larger than that observed for the 
web-mining approach. 
6 Conclusions 
Since the meaning of noun compounds arises 
from a combination of individual noun meanings, 
it follows that the key input to the process of 
compound interpretation is detailed linguistic 
knowledge about how these nouns are 
conventionally used in language. This point may 
seem obvious, but a model of compounding can 
place so much emphasis on the behavior of noun-
pairs that the linguistic behavior of nouns in 
isolation is easily over-looked.  
We have presented a model of noun-
compounding that places nouns and their specific 
linguistic relational possibilities at the centre of 
processing. When one considers that linguistic 
relational possibilities capture aspects of noun 
meaning such as purpose, constitution and 
agency, their realization here can be viewed as a 
87
generalized and lexicalized aspect of qualia 
structure in the sense of Pustejovsky (1995) and 
Johnston and Busa (1996). Indeed, the n-gram 
patterns used to extract these relational 
possibilities from corpora are not unlike the 
patterns used by Cimiano and Wenderoth (2007) 
to harvest qualia structures from the web. 
We conclude from the empirical observation 
that the hybrid model outperforms the web-based 
model (albeit slimly) in experiment 2 while both 
perform equally well in experiment 1, is that the 
modifier and head are of comparable 
performance when paraphrasing the 
interpretations of noun compounds. Recall that 
the web-validation approach (Model-1) generates 
interpretations from either the modifier or the 
head, while the matching-relational possibilities 
and hybrid models require both to contribute 
equally. 
Necessary extensions to the approach include 
the acquisition of more relational possibilities of 
greater linguistic complexity, the ability to 
organize relational possibilities hierarchically 
according to their underlying semantic meanings, 
and the ability to recognize an implication 
structure among different but related relational 
possibilities. 
Acknowledgement 
We would like to thank Preslav Nakov for 
providing us the data used in the second 
experiment. 
References 
Brants, T., and Franz, A. 2006. Web 1t 5-gram 
version 1, Linguistic Data Consortium. 
Butnariu, C., and Veale T. 2007. A hybrid model for 
detecting semantic relations between noun pairs in 
text. In Proc. of the Fourth International Workshop 
on Semantic Evaluations (SemEval-2007), Prague,. 
Association for Computational Linguistics. 
Cimiano, P., and Wenderoth, J. 2007. Automatic 
Acquisition of Ranked Qualia Structures from the 
Web. In Proc. of the 45th Annual Meeting of the 
ACL, pp 888-895. 
Fellbaum, C. 1998. WordNet, an electronic lexical 
database. Cambridge: MIT Press. 
Finin, T. 1980. The semantic interpretation of 
compound nominals. Urbana, Illinois: University of 
Illinois dissertation. 
Gagn?, C. L., and Shoben, E. J. 1997. Influence of 
thematic relations on the comprehension of 
modifier-noun combinations. Journal of 
Experimental Psychology: Learning, Memory, and 
Cognition, 23, 71?87  
Gagn?, C. L. 2002. Lexical and Relational Influences 
on the Processing of Novel Compounds. Brain and 
Language 81(1-3), pp 723-735. 
Girju, R., Nakov, P. Nastase, V., Szpakowicz, S., 
Turney, P., and Yuret, D. 2007. Semeval-2007 task 
04: Classification of semantic relations between 
nominals. In Proc. of the Fourth International 
Workshop on Semantic Evaluations (SemEval-
2007), 13?18, Prague, Czech Republic. ACL. 
Johnston, M., and Busa, F. 1996. Qualia structure and 
the compositional interpretation of compounds. In 
Proc. of the ACL SIGLEX workshop on breadth 
and depth of semantic lexicons, Santa Cruz, CA. 
Kim, S. N., and Baldwin, T. 2006. Interpreting 
semantic relations in noun compounds via verb 
semantics. In Proc. of the 21st International 
Conference on Computational Linguistics and the 
44th annual meeting of the ACL, pp 491?498, NJ, 
USA. 
Kim, S. N., and Baldwin, T. 2005. Automatic 
interpretation of noun compounds using WordNet 
similarity. In Proc. of the 2nd International Joint 
Conference On Natural Language Processing, pp 
945?956, Cheju, Korea. 
Levi, J. 1978. The syntax and semantics of complex 
nominals. NY: Academic Press. 
Nakov, P., and Hearst, M. A. 2006. Using verbs to 
characterize noun-noun relations. In AIMSA, 
Jerome Euzenat and John Domingue (eds.), vol. 
4183 of Lecture Notes in Computer Science, pp 
233?244. Springer. 
Nakov, P. 2007. Using the Web as an Implicit 
Training Set: Application to Noun Compound 
Syntax and Semantics. Ph.D. Dissertation, 
University of California at Berkeley. 
Nastase, V., and Szpakowicz, S. 2003. Exploring 
noun-modifier semantic relations. In Proc. of the 
5th International Workshop on Computational 
Semantics (IWCS-5), pp 285?301, Tilburg, The 
Netherlands. 
Pustejovsky, J. 1995. The Generative Lexicon. The 
MIT Press, Cambridge, MA. 
Turney, P. 2006a. Expressing implicit semantic 
relations without supervision. In Proc. of the 21st 
International Conference on Computational 
Linguistics and the 44th annual meeting of the 
ACL, pp 313?320, NJ, USA. 
Turney, P. D. 2006b. Similarity of semantic relations. 
Computational Linguistics, 32, pp 379?416. 
88
Proceedings of the 4th International Workshop on Semantic Evaluations (SemEval-2007), pages 378?381,
Prague, June 2007. c?2007 Association for Computational Linguistics
UCD-S1: A hybrid model for detecting semantic relations between noun 
pairs in text
Cristina Butnariu
School of Computer Science and Informatics
University College Dublin
Belfield, Dublin 4, Ireland.
ioana.butnariu@UCD.ie
Tony Veale
School of Computer Science and Informatics
University College Dublin
Belfield, Dublin 4, Ireland.
tony.veale@UCD.ie
Abstract
We describe a supervised learning approach to 
categorizing  inter-noun  relations,  based  on 
Support Vector Machines, that builds a differ-
ent classifier for each of seven semantic rela-
tions.  Each  model  uses  the  same  learning 
strategy,  while  a  simple  voting  procedure 
based on five trained discriminators with vari-
ous  blends  of  features  determines  the  final 
categorization.  The features that  characterize 
each of the noun pairs are a blend of lexical-
semantic  categories extracted  from WordNet 
and  several  flavors  of  syntactic  patterns  ex-
tracted  from  various  corpora,  including 
Wikipedia and the WMTS corpus.
1 Introduction
The  SemEval  task  for  classifying  inter-noun 
semantic  relations  employs  seven  semantic 
relations  that  are  not  exhaustive:  Cause-Effect, 
Instrument-Agency,  Product-Producer  Origin-
Entity,  Theme-Tool,  Part-Whole  and  Content-
Container.  The  task  is  to  classify  the  relations 
between pairs of concepts that are part of the same 
syntactic  structure  in  a  given  sentence.  This 
approach  employs  a  context-dependent 
classification,  as  opposed to  usual  out-of-context 
approaches  in  classifying  semantic  relations 
between noun pairs (e.g., (Turney, 2005), (Nastase 
et. al., 2006)).
Our  approach  is  based  on  the  Support  Vector 
Machines  learning  paradigm  (Vapnik,  1995),  in 
which supervised machine learning is used to find 
the most  salient combination of features for each 
semantic relation. These features include semantic 
generalizations of  the noun-senses as encoded as 
WordNet (WN) hyponyms,  some manually selec-
ted  linguistic  features  (e.g.,  agentive,  gerundive, 
etc.) as well as the observed relational behaviour of 
the given nouns in three different corpora: the col-
lected  glosses  of  WordNet;  the  collected  text  of 
Wikipedia; and the WMTS corpus.
One can find similar approaches in the literature 
to the semantic classification of noun compounds. 
Turney (2005) uses automatically extracted para-
phrases to build a similarity measure between pairs 
of concepts, while  Nastase et. al. (2006) proposes 
separate models for two different word representa-
tions  when  determining  the  semantic  relation  in 
modifier-noun compounds:  a model  based on the 
lexico-semantic aspects of words and a model that 
uses contextual information from corpora. Our ap-
proach is different in that we use all the available 
features of word representations and concept inter-
actions in a single hybrid model.
2 System description
Our  system,  named  the  Semantic  Relation  Dis-
criminator (or SRD), takes as input a set of noun 
pairs that are manually classified as positive/negat-
ive for a given semantic relation and produces as 
output  a  discriminator  for  that  semantic  relation. 
We used SRD to learn different models for each of 
the  seven  semantic  relations  in  the  classification 
scheme for task 4 in the SemEval Workshop. The 
SRD system relies  on several  data-resources  and 
tools:  the  WN  noun-sense  hierarchy,  a  corpus 
made up of the WordNet glosses, the complete text 
of  Wikipedia  (downloaded June,  2005),  a  search 
engine indexing a very large corpus of text, and the 
WEKA  Data  Mining  software  package  (version 
3.5). 
SRD combines  two types  of  features  for  each 
noun pair:  semantic  features  extracted  from  WN 
noun-sense hierarchy, for which the WN synset-id 
378
information of each noun is used and syntactic fea-
tures extracted from the unlabeled and unstructured 
corpora mentioned above for which a shallow pars-
ing approach is employed. 
2.1 Feature acquisition 
SRD follows four steps in acquiring features: 
? Select  semantic  generalizations.  For  each 
noun-sense in a pair, SRD extracts all hyper-
nyms  at  depth  8 or  higher  in  the  WordNet 
noun-sense hierarchy.
? Extract  syntactic  phrases.  SRD  looks  for 
phrases in corpora that occur before or after 
each noun in a pair and which obey one of 
several  syntactic  templates.  SRD also looks 
for  joining  phrases  between  each  pair  of 
nouns that contain 5 words or less.
? Clean-up these phrases. SRD lemmatizes the 
words in each phrase and removes function 
words such as articles, possessive pronouns, 
adjective and adverbs. 
? Record  observed  patterns.  For  each  noun 
pair, SRD records the following types of syn-
tactic patterns together with their corpus fre-
quencies: joining terms that comprise at least 
one verb; phrases that are composed of one 
verb  and  one  preposition;  and  phrases  that 
are composed of a simple verb or a phrasal 
verb.
2.2 Selecting the features
Due to the large number of  features extracted in 
these  steps,  SRD employs  five  different  models 
that  use  different  combination  of  features  and 
which pool their votes to determine a single predic-
ation for each learning task. We describe below the 
feature sets used for each component. The features 
have binary values: 1 if the feature is present for a 
noun pair, and 0 otherwise. 
Each model employs WordNet hypernyms (from 
the  top  8  layers  of  the  noun  hierarchy)  of  both 
noun-senses as semantic features, while models 1 
and 2 employ the following additional features for 
each noun pair (N1, N2):
1. The  most  frequent  syntactic  patterns  that 
appear between N1 and N2 in corpora
2. The  most  frequent  syntactic  patterns  that 
appear between N2 and N1 in corpora
Model 1 and Model 2 differ only in the syntactic 
templates  used  to  validate  inter-noun  patterns. 
Model  1  fixates  on  patterns  that  contain  a  verb, 
while Model 2 accepts patterns that contain either a 
preposition or a verb, or both. This yields, on aver-
age, 5,000 binary features for Model 1 for each of 
the seven relation types, and an average of 10,000 
binary features for Model 2. 
In addition to WN-derived hypernymic-features, 
models 3 and 4 employ the following: 
1. The  most  frequent  syntactic  patterns  that 
immediately precede N1 in a corpus
2. The  most  frequent  syntactic  patterns  that 
immediately follow N1 in a corpus
3. The  most  frequent  syntactic  patterns  that 
immediately precede N2 in a corpus
4. The  most  frequent  syntactic  patterns  that 
immediately follow N2 in a corpus
In Model  3 each syntactic  pattern comprises a 
hyphenated  verb,  while  the  syntactic  patterns  in 
Model 4 each contain a preposition or a verb. SRD 
generates,  on  average,  1,500  binary  features  in 
Model 3 and 2,500 features in Model 4 for each re-
lation-type.
In addition to WN-derived hypernymic-features, 
model 5 employs the following:
1. A set of linguistic features for N1, indicat-
ing  whether  this  noun  is  a  nominalized 
verb, or whether it frequently appears in a 
specific semantic case role (e.g., agent).
2. The same set of linguistic features as de-
termined for N2.
SRD generates, on average, approximately 700 
binary features for each relation-type in Model 5. 
2.3 Building the models
The  SVM  learning  paradigm  seems  particularly 
suitable  to  our  task  for  a  number  of  reasons. 
Firstly, it  behaves robustly for all  seven learning 
tasks, ignoring the noise in the training set. This is 
important, since e.g., some training pairs for the In-
strument-Agency relation were labeled as both true 
and false. Secondly, SVM has an automated mech-
anism  for  parameter  tuning,  which  reduces  the 
overall computational effort. 
SRD employs  polynomial  SVMs because  they 
appear  to  perform  better  for  this  task  compared 
379
with simple linear SVMs or radial-basis functions. 
We used the WEKA implementation of John Plat-
t?s Sequential Minimal Optimization method (Platt, 
1998) to train the feature weights on all the avail-
able training data. Using SMO to train the polyno-
mial SVM takes approx. 2.8 CPU sec. per model.
The motivation for a multiple model scheme ap-
proach comes from empirical results. SRD yields 
higher  results  relative  to  the  five  single  models 
schemes that compose our system when evaluated 
using 10-fold cross validation on the training data. 
3 Experiments and Results
The SemEval  data-set  for  each  of  the  seven  se-
mantic relations comprises 140 annotated instances 
for training and between 70 to 90 for testing. Each 
instance  is  manually  labelled  with  the  part  of 
speech of each concept in a pair, as well as the WN 
synset-id of the intended word-sense and a sample 
sentential context. SRD?s predictions fall into eval-
uation category B, as the system uses WN synset-
id but not the query pattern used to originally pop-
ulate the data-sets with instances. SRD also skips 
those  training  instances  where  WN sense-ids  are 
not provided, so that the actual number of training 
instances used ranges from 129 to 138 manually la-
belled examples per relation-type.
SRD?s  precision,  recall,  F-score  and  accuracy 
for each relation is given by Table 1.
P R F1 Acc #t inst.
Cause-Effect 69.8 73.2 71.4 70.0 80
Instrument-Agency 72.5 76.3 74.4 74.4 78
Product-Producer 80.6 87.1 83.7 77.4 93
Origin-Entity 60.0 50.0 54.5 63.0 81
Theme-Tool 50.0 34.5 40.8 59.2 71
Part-Whole 71.4 57.7 63.8 76.4 72
Content-Container 84.8 73.7 78.9 79.7 74
Average 69.9 64.6 66.8 71.4 78.4
Table1. Results for SRD across the seven learning tasks
To  assess  the  effect  of  varying  quantities  of 
training  data,  the  model  was  tested  on  different 
fractions of the training data: dataset B1 comprises 
the first quarter of the training data, dataset B2 the 
first  half,   while  B3  dataset  comprises  the  first 
three  quarters  and  B4  comprises  the  complete 
training dataset. We report the behavior of SRD in 
predicting the unseen test data when learning from 
these datasets in table 2. The measures of table 2 
represent an average of SRD?s performance across 
all relation-types.
P R F1 Acc
Dataset B1  65.4 53.3 56.4 66.2
Dataset B2 67.8 63.8 63.5 69.6
Dataset B3 71.7 64.0 66.8 71.6
Dataset B4 69.9 64.6 66.8 71.4
Table2. Results for SRD on different training datasets
3.1 Error analysis
Three types of baseline values were proposed for 
this  task.  Baseline  1 (?majority baseline?)  is  ob-
tained by always guessing either "true" or "false", 
according to whichever is the majority category in 
the testing data-set for the given relation. Baseline 
2 (?alltrue baseline?) is achieved by always guess-
ing  ?true?.  Baseline  3  (?probmatch  baseline?)  is 
obtained  by  randomly  guessing  "true"  or  "false" 
with  a  probability  matching  the  distribution  of 
"true" or "false" in the testing dataset.
0
10
20
30
40
50
60
70
80
90
class1 class2 class3 class4 class5 class6 class7
SRD Baseline1 Baseline2 Baseline3
Figure1.  Comparison  of  SRD?s  F-scores  for  each  se-
mantic relation and the corresponding baselines.
Figure 1 plots the F-scores obtained for each se-
mantic relation. We observe that SRD has exhibits 
poor performance on two particular relations, Ori-
gin-Entity and Theme-Tool, denoted ?class4? and 
?class5? in the plot of Figure 1. SRD achieves the 
same  F-measure  score  as  the  random prediction 
baseline for Theme-Tool class, suggesting that the 
features used are simply not capable of building a 
discriminator for this semantic relation. SRD?s F-
score for Origin-Entity class is 10% higher than the 
random baseline, but still performs below the other 
two baselines. SRD?s best performance is achieved 
for Product-Producer and Part-Whole,  with an F-
score 11% higher than the highest baseline.
380
Table3. SRD F-measures using different feature sets
3.2 Improvements
One obvious problem with SRD is that we use a 
high-dimensional feature-space to train each mod-
el. Research in text categorization (e.g., Dumais et  
al., 1998) shows that feature selection algorithms 
like information gain can identify the most produc-
tive dimensions of the feature space and simultane-
ously boost classification accuracy.
To explore  this  potential  for  improvement,  we 
applied two types of feature selection filters (using 
WEKA): the InfoGainAttrEval filter that evaluates 
the utility of  a feature  by measuring information 
gain w.r.t. the class; and the  CfsSubsetEval filter, 
which evaluates the utility of a subset of features 
by considering the individual predictive ability of 
each individually and the degree of redundancy be-
tween  them  collectively.  Results  of  our  experi-
ments with SRD using different subsets of feature 
sets are displayed in Table 3. Set 1 is the complete 
set of all features. Set 2 is the subset obtained with 
the  top  n  features  as  ranked  by the  InfoGainAt-
trEval filter  (n is  determined using 10-fold cross 
validation on the training data). Set 3 is a tailored 
feature-set created for each relation-type using the 
CfsSubsetEval filter. Set 4 is the subset of all fea-
tures extracted from WN. 
We find that feature-filtering boosts the perfor-
mance of some learning tasks by up to 14 % (e.g., 
the Theme-Tool relation), but it can also decrease 
performance by the same amount (e.g., the Origin-
Entity relation). SRD achieves its best performance 
-- an overall F-measure of 71.7% -- when using a 
feature set that is tailored to each of the semantic 
relation classification tasks (e.g., Set 4 (WN only) 
for Origin-Entity, Set 1 (all) for Product-Producer 
and Container-Content, Set 4 and Set 3 (relation-
specific subsets) for everything else).
4 Conclusions
SRD  is  an  SVM-based  approach  to  classifying 
noun-pairs into categories that best reflect the se-
mantic relationship underlying each pair. Without 
feature-filtering, SRD shows modest classification 
capability, performing better than the highest base-
lines for five of the seven relational classes. Exper-
iments  with  feature  filtering  encourage  us  to  try 
and refine SRD?s feature space to focus on more 
discriminatory and semantically-revealing features 
of nouns. Feature-filtering can diminish as well as 
improve performance, and thus, should ideally be 
linked to an insightful theory of how particular fea-
tures  contribute  to  the  human-understanding  of 
noun-noun  pairs.  Filtering  techniques  provide  a 
good basis for formulating feature-based hypothe-
ses, but the most productive feature sets will come, 
we hope, from a cognitive and conceptual under-
standing of  the  processes  of  phrase  construction, 
rather than from an exhaustive and largely theory-
free exploration of different feature-sets.
Acknowledgments
We would like to thank Peter Turney for granting 
us access to the NRC copy of the WMTS.
References
Joachims,  T.  (1998)  Text  categorization  with  support 
vector  machines:  learning  with  many  relevant  fea-
tures. Proceedings of ECML-98, 10th European Con-
ference on Machine Learning.
Dumais,  S.  T.,  Platt,  J.,  Heckerman  D.,  Sahami  M., 
(1998) Inductive learning algorithms and representa-
tions  for  text  categorization,  Proceedings  of  ACM-
CIKM98
Nastase, V., Sayyad-Shirabad, J., Sokolova, M., and Sz-
pakowicz, S. (2006). Learning noun-modifier seman-
tic  relations  with  corpus-based and WordNet-based 
features. In Proceedings of the 21st National Confer-
ence on Artificial Intelligence, Boston, MA.
Platt, J. (1998), Fast Training of SVMs Using Sequen-
tial Minimal Optimization,  Support Vector Machine 
Learning, MIT Press, Cambridge.
Turney, P.D. (2005). Measuring semantic similarity by 
latent relational analysis. In Proceedings of the Nine-
teenth  International  Joint  Conference  on  Artificial  
Intelligence, Edinburgh, Scotland.
Vapnik, V. (1995). The Nature of Statistical  Learning 
Theory, Springer-Verlag, New York
Feature 
Set1
Feature 
Set2
Feature 
Set3
Feature 
Set4
Cause-Effect 71.4 72.7 75.7 61.3
Instrument-Agency 74.4 74.6 76.3 72
Product-Producer 83.7 81.3 80.5 77
Origin-Entity 54.5 44.8 38 61.5
Theme-Tool 40.8 42.8 53.8 42.5
Part-Whole 63.8 72.3 62.7 60
Content-Container 78.9 75.6 77.1 73.2
Average 66.8 66.3 66.3 64
381
Proceedings of the NAACL HLT Workshop on Semantic Evaluations: Recent Achievements and Future Directions, pages 100?105,
Boulder, Colorado, June 2009. c?2009 Association for Computational Linguistics
SemEval-2010 Task 9: The Interpretation of Noun Compounds
Using Paraphrasing Verbs and Prepositions
Cristina Butnariu
University College Dublin
ioana.butnariu@ucd.ie
Su Nam Kim
University of Melbourne
nkim@csse.unimelb.edu.au
Preslav Nakov
National University of Singapore
nakov@comp.nus.edu.sg
Diarmuid O? Se?aghdha
University of Cambridge
do242@cam.ac.uk
Stan Szpakowicz
University of Ottawa
Polish Academy of Sciences
szpak@site.uottawa.ca
Tony Veale
University College Dublin
tony.veale@ucd.ie
Abstract
We present a brief overview of the main
challenges in understanding the semantics of
noun compounds and consider some known
methods. We introduce a new task to be
part of SemEval-2010: the interpretation of
noun compounds using paraphrasing verbs
and prepositions. The task is meant to provide
a standard testbed for future research on noun
compound semantics. It should also promote
paraphrase-based approaches to the problem,
which can benefit many NLP applications.
1 Introduction
Noun compounds (NCs) ? sequences of two or more
nouns acting as a single noun,1 e.g., colon cancer
tumor suppressor protein ? are abundant in English
and pose a major challenge to the automatic anal-
ysis of written text. Baldwin and Tanaka (2004)
calculated that 3.9% and 2.6% of the tokens in
the Reuters corpus and the British National Corpus
(BNC), respectively, are part of a noun compound.
Compounding is also an extremely productive pro-
cess in English. The frequency spectrum of com-
pound types follows a Zipfian or power-law distribu-
tion (O? Se?aghdha, 2008), so in practice many com-
pound tokens encountered belong to a ?long tail?
of low-frequency types. For example, over half of
the two-noun NC types in the BNC occur just once
(Lapata and Lascarides, 2003). Even for relatively
frequent NCs that occur ten or more times in the
BNC, static English dictionaries give only 27% cov-
erage (Tanaka and Baldwin, 2003). Taken together,
1We follow the definition in (Downing, 1977).
the factors of high frequency and high productiv-
ity mean that achieving robust NC interpretation is
an important goal for broad-coverage semantic pro-
cessing. NCs provide a concise means of evoking a
relationship between two or more nouns, and natu-
ral language processing (NLP) systems that do not
try to recover these implicit relations from NCs are
effectively discarding valuable semantic informa-
tion. Broad coverage should therefore be achieved
by post-hoc interpretation rather than pre-hoc enu-
meration, since it is impossible to build a lexicon of
all NCs likely to be encountered.
The challenges presented by NCs and their se-
mantics have generated significant ongoing interest
in NC interpretation in the NLP community. Repre-
sentative publications include (Butnariu and Veale,
2008; Girju, 2007; Kim and Baldwin, 2006; Nakov,
2008b; Nastase and Szpakowicz, 2003; O? Se?aghdha
and Copestake, 2007). Applications that have been
suggested include Question Answering, Machine
Translation, Information Retrieval and Information
Extraction. For example, a question-answering sys-
tem may need to determine whether headaches in-
duced by caffeine withdrawal is a good paraphrase
for caffeine headaches when answering questions
about the causes of headaches, while an information
extraction system may need to decide whether caf-
feine withdrawal headache and caffeine headache
refer to the same concept when used in the same
document. Similarly, a machine translation system
facing the unknown NC WTO Geneva headquarters
might benefit from the ability to paraphrase it as
Geneva headquarters of the WTO or as WTO head-
quarters located in Geneva. Given a query like can-
100
cer treatment, an information retrieval system could
use suitable paraphrasing verbs like relieve and pre-
vent for page ranking and query refinement.
In this paper, we introduce a new task, which will
be part of the SemEval-2010 competition: NC inter-
pretation using paraphrasing verbs and prepositions.
The task is intended to provide a standard testbed
for future research on noun compound semantics.
We also hope that it will promote paraphrase-based
approaches to the problem, which can benefit many
NLP applications.
The remainder of the paper is organized as fol-
lows: Section 2 presents a brief overview of the
existing approaches to NC semantic interpretation
and introduces the one we will adopt for SemEval-
2010 Task 9; Section 3 provides a general descrip-
tion of the task, the data collection, and the evalua-
tion methodology; Section 4 offers a conclusion.
2 Models of Relational Semantics in NCs
2.1 Inventory-Based Semantics
The prevalent view in theoretical and computational
linguistics holds that the semantic relations that im-
plicitly link the nouns of an NC can be adequately
enumerated via a small inventory of abstract re-
lational categories. In this view, mountain hut,
field mouse and village feast all express ?location
in space?, while the relation implicit in history book
and nativity play can be characterized as ?topicality?
or ?aboutness?. A sample of some of the most influ-
ential relation inventories appears in Table 1.
Levi (1978) proposes that complex nominals ?
a general concept grouping together nominal com-
pounds (e.g., peanut butter), nominalizations (e.g.,
dream analysis) and non-predicative noun phrases
(e.g., electric shock) ? are derived through the com-
plementary processes of recoverable predicate dele-
tion and nominalization; each process is associated
with its own inventory of semantic categories. Table
1 lists the categories for the former.
Warren (1978) posits a hierarchical classifica-
tion scheme derived from a large-scale corpus study
of NCs. The top-level relations in her hierar-
chy are listed in Table 1, while the next level
subdivides CONSTITUTE into SOURCE-RESULT,
RESULT-SOURCE and COPULA; COPULA is then
further subdivided at two additional levels.
In computational linguistics, popular invento-
ries of semantic relations have been proposed by
Nastase and Szpakowicz (2003) and Girju et al
(2005), among others. The former groups 30 fine-
grained relations into five coarse-grained super-
categories, while the latter is a flat list of 21 re-
lations. Both schemes are intended to be suit-
able for broad-coverage analysis of text. For spe-
cialized applications, however, it is often useful
to use domain-specific relations. For example,
Rosario and Hearst (2001) propose 18 abstract rela-
tions for interpreting NCs in biomedical text, e.g.,
DEFECT, MATERIAL, PERSON AFFILIATED,
ATTRIBUTE OF CLINICAL STUDY.
Inventory-based analyses offer significant advan-
tages. Abstract relations such as ?location? and ?pos-
session? capture valuable generalizations about NC
semantics in a parsimonious framework. Unlike
paraphrase-based analyses (Section 2.2), they are
not tied to specific lexical items, which may them-
selves be semantically ambiguous. They also lend
themselves particularly well to automatic interpreta-
tion methods based on multi-class classification.
On the other hand, relation inventories have been
criticized on a number of fronts, most influentially
by Downing (1977). She argues that the great vari-
ety of NC relations makes listing them all impos-
sible; creative NCs like plate length (?what your
hair is when it drags in your food?) are intuitively
compositional, but cannot be assigned to any stan-
dard inventory category. A second criticism is that
restricted inventories are too impoverished a repre-
sentation scheme for NC semantics, e.g., headache
pills and sleeping pills would both be analyzed as
FOR in Levi?s classification, but express very differ-
ent (indeed, contrary) relationships. Downing writes
(p. 826): ?These interpretations are at best reducible
to underlying relationships. . . , but only with the loss
of much of the semantic material considered by sub-
jects to be relevant or essential to the definitions.?
A further drawback associated with sets of abstract
relations is that it is difficult to identify the ?correct?
inventory or to decide whether one proposed classi-
fication scheme should be favored over another.
2.2 Interpretation Using Verbal Paraphrases
An alternative approach to NC interpretation asso-
ciates each compound with an explanatory para-
101
Author(s) Relation Inventory
Levi (1978) CAUSE, HAVE, MAKE, USE, BE, IN, FOR, FROM, ABOUT
Warren (1978) POSSESSION, LOCATION, PURPOSE, ACTIVITY-ACTOR, RESEMBLANCE, CONSTITUTE
Nastase and CAUSALITY (cause, effect, detraction, purpose),
Szpakowicz PARTICIPANT (agent, beneficiary, instrument, object property,
(2003) object, part, possessor, property, product, source, whole, stative),
QUALITY (container, content, equative, material, measure, topic, type),
SPATIAL (direction, location at, location from, location),
TEMPORALITY (frequency, time at, time through)
Girju et al (2005) POSSESSION, ATTRIBUTE-HOLDER, AGENT, TEMPORAL, PART-WHOLE, IS-A, CAUSE,
MAKE/PRODUCE, INSTRUMENT, LOCATION/SPACE, PURPOSE, SOURCE, TOPIC, MANNER,
MEANS, THEME, ACCOMPANIMENT, EXPERIENCER, RECIPIENT, MEASURE, RESULT
Lauer (1995) OF, FOR, IN, AT, ON, FROM, WITH, ABOUT
Table 1: Previously proposed inventories of semantic relations for noun compound interpretation. The first two come
from linguistic theories; the rest have been proposed in computational linguistics.
phrase. Thus, cheese knife and kitchen knife can be
expanded as a knife for cutting cheese and a knife
used in a kitchen, respectively. In the paraphrase-
based paradigm, semantic relations need not come
from a small set; it is possible to have many sub-
tle distinctions afforded by the vocabulary of the
paraphrasing language (in our case, English). This
paradigm avoids the problems of coverage and rep-
resentational poverty, which Downing (1977) ob-
served in inventory-based approaches. It also re-
flects cognitive-linguistic theories of NC semantics,
in which compounds are held to express underlying
event frames and whose constituents are held to de-
note event participants (Ryder, 1994).
Lauer (1995) associates NC semantics with
prepositional paraphrases. As Lauer only consid-
ers a handful of prepositions (about, at, for,
from, in, of, on, with), his model is es-
sentially inventory-based. On the other hand, noun-
preposition co-occurrences can easily be identified
in a corpus, so an automatic interpretation can be
implemented through simple unsupervised methods.
The disadvantage of this approach is the absence of a
one-to-one mapping from prepositions to meanings;
prepositions can be ambiguous (of indicates many
different relations) or synonymous (at, in and on
all express ?location?). This concern arises with all
paraphrasing models, but it is exacerbated by the re-
stricted nature of prepositions. Furthermore, many
NCs cannot be paraphrased adequately with prepo-
sitions, e.g., woman driver, honey bee.
A richer, more flexible paraphrasing model is af-
forded by the use of verbs. In such a model, a honey
bee is a bee that produces honey, a sleeping pill
is a pill that induces sleeping and a headache pill
is a pill that relieves headaches. In some previous
computational work on NC interpretation, manually
constructed dictionaries provided typical activities
or functions associated with nouns (Finin, 1980; Is-
abelle, 1984; Johnston and Busa, 1996). It is, how-
ever, impractical to build large structured lexicons
for broad-coverage systems; these methods can only
be applied to specialized domains. On the other
hand, we expect that the ready availability of large
text corpora should facilitate the automatic mining
of rich paraphrase information.
The SemEval-2010 task we present here builds on
the work of Nakov (Nakov and Hearst, 2006; Nakov,
2007; Nakov, 2008b), where NCs are paraphrased
by combinations of verbs and prepositions. Given
the problem of synonymy, we do not provide a sin-
gle correct paraphrase for a given NC but a prob-
ability distribution over a range of candidates. For
example, highly probable paraphrases for chocolate
bar are bar made of chocolate and bar that tastes
like chocolate, while bar that eats chocolate is very
unlikely. As described in Section 3.3, a set of gold-
standard paraphrase distributions can be constructed
by collating responses from a large number of hu-
man subjects.
In this framework, the task of interpretation be-
comes one of identifying the most likely paraphrases
for an NC. Nakov (2008b) and Butnariu and Veale
(2008) have demonstrated that paraphrasing infor-
mation can be collected from corpora in an un-
supervised fashion; we expect that participants in
102
SemEval-2010 Task 9 will further develop suitable
techniques for this problem. Paraphrases of this kind
have been shown to be useful in applications such as
machine translation (Nakov, 2008a) and as an inter-
mediate step in inventory-based classification of ab-
stract relations (Kim and Baldwin, 2006; Nakov and
Hearst, 2008). Progress in paraphrasing is therefore
likely to have follow-on benefits in many areas.
3 Task Description
The description of the task we present below is pre-
liminary. We invite the interested reader to visit the
official Website of SemEval-2010 Task 9, where up-
to-date information will be published; there is also a
discussion group and a mailing list.2
3.1 Preliminary Study
In a preliminary study, we asked 25-30 human sub-
jects to paraphrase 250 noun-noun compounds us-
ing suitable paraphrasing verbs. This is the Levi-
250 dataset (Levi, 1978); see (Nakov, 2008b) for de-
tails.3 The most popular paraphrases tend to be quite
apt, while some less frequent choices are question-
able. For example, for chocolate bar we obtained
the following paraphrases (the number of subjects
who proposed each one is shown in parentheses):
contain (17); be made of (16); be made
from (10); taste like (7); be composed
of (7); consist of (5); be (3); have (2);
smell of (2); be manufactured from (2);
be formed from (2); melt into (2); serve
(1); sell (1); incorporate (1); be made with
(1); be comprised of (1); be constituted
by (1); be solidified from (1); be flavored
with (1); store (1); be flavored with (1); be
created from (1); taste of (1)
3.2 Objective
We propose a task in which participating systems
must estimate the quality of paraphrases for a test
set of NCs. A list of verb/preposition paraphrases
will be provided for each NC, and for each list a
participating system will be asked to provide aptness
2Please follow the Task #9 link at the SemEval-2010 home-
page http://semeval2.fbk.eu
3This dataset is available from http://sourceforge.
net/projects/multiword/
scores that correlate well (in terms of frequency dis-
tribution) with the human judgments collated from
our test subjects.
3.3 Datasets
Trial/Development Data. As trial/development
data, we will release the previously collected para-
phrase sets for the Levi-250 dataset (after further
review and cleaning). This dataset consists of 250
noun-noun compounds, each paraphrased by 25-30
human subjects (Nakov, 2008b).
Test Data. The test data will consist of approx-
imately 300 NCs, each accompanied by a set of
paraphrasing verbs and prepositions. Following the
methodology of Nakov (2008b), we will use the
Amazon Mechanical Turk Web service4 to recruit
human subjects. This service offers an inexpensive
way to recruit subjects for tasks that require human
intelligence, and provides an API which allows a
computer program to easily run tasks and collate
the responses from human subjects. The Mechanical
Turk is becoming a popular means to elicit and col-
lect linguistic intuitions for NLP research; see Snow
et al (2008) for an overview and a discussion of is-
sues that arise.
We intend to recruit 100 annotators for each NC,
and we will require each annotator to paraphrase
at least five NCs. Annotators will be given clear
instructions and will be asked to produce one or
more paraphrases for a given NC. To help us filter
out subjects with an insufficient grasp of English or
an insufficient interest in the task, annotators will
be asked to complete a short and simple multiple-
choice pretest on NC comprehension before pro-
ceeding to the paraphrasing step.
Post-processing. We will manually check the
trial/development data and the test data. Depending
on the quality of the paraphrases, we may decide to
drop the least frequent verbs.
License. All data will be released under the Cre-
ative Commons Attribution 3.0 Unported license5.
3.4 Evaluation
Single-NC Scores. For each NC, we will compare
human scores (our gold standard) with those pro-
posed by each participating system. We have con-
4http://www.mturk.com
5http://creativecommons.org/licenses/by/3.0/
103
sidered three scores: (1) Pearson?s correlation, (2)
cosine similarity, and (3) Spearman?s rank correla-
tion.
Pearson?s correlation coefficient is a standard
measure of the correlation strength between two dis-
tributions; it can be calculated as follows:
? = E(XY ) ? E(X)E(Y )?
E(X2) ? [E(X)]2?E(Y 2) ? [E(Y )]2
(1)
where X = (x1, . . . , xn) and Y = (y1, . . . , yn) are
vectors of numerical scores for each paraphrase pro-
vided by the humans and the competing systems, re-
spectively, n is the number of paraphrases to score,
and E(X) is the expectation of X .
Cosine correlation coefficient is another popu-
lar alternative and was used by Nakov and Hearst
(2008); it can be seen as an uncentered version of
Pearson?s correlation coefficient:
? = X.Y?X??Y ? (2)
Spearman?s rank correlation coefficient is suit-
able for comparing rankings of sets of items; it is
a special case of Pearson?s correlation, derived by
considering rank indices (1,2,. . . ) as item scores . It
is defined as follows:
? = n
?xiyi ? (?xi)(? yi)?
n?x2i ? (
?xi)2
?
n? y2i ? (
? yi)2
(3)
One problem with using Spearman?s rank coef-
ficient for the current task is the assumption that
swapping any two ranks has the same effect. The
often-skewed nature of paraphrase frequency distri-
butions means that swapping some ranks is intu-
itively less ?wrong? than swapping others. Consider,
for example, the following list of human-proposed
paraphrasing verbs for child actor, which is given in
Nakov (2007):
be (22); look like (4); portray (3); start as
(1); include (1); play (1); have (1); involve
(1); act like (1); star as (1); work as (1);
mimic (1); pass as (1); resemble (1); be
classified as (1); substitute for (1); qualify
as (1); act as (1)
Clearly, a system that swaps the positions for
be (22) and look like (4) for child actor will
have made a significant error, while swapping con-
tain (17) and be made of (16) for chocolate bar (see
Section 3.1) would be less inappropriate. However,
Spearman?s coefficient treats both alterations iden-
tically since it only looks at ranks; thus, we do not
plan to use it for official evaluation, though it may
be useful for post-hoc analysis.
Final Score. A participating system?s final score
will be the average of the scores it achieves over all
test examples.
Scoring Tool. We will provide an automatic eval-
uation tool that participants can use when train-
ing/tuning/testing their systems. We will use the
same tool for the official evaluation.
4 Conclusion
We have presented a noun compound paraphrasing
task that will run as part of SemEval-2010. The goal
of the task is to promote and explore the feasibility
of paraphrase-based methods for compound inter-
pretation. We believe paraphrasing holds some key
advantages over more traditional inventory-based
approaches, such as the ability of paraphrases to rep-
resent fine-grained and overlapping meanings, and
the utility of the resulting paraphrases for other ap-
plications such as Question Answering, Information
Extraction/Retrieval and Machine Translation.
The proposed paraphrasing task is predicated on
two important assumptions: first, that paraphrasing
via a combination of verbs and prepositions pro-
vides a powerful framework for representing and in-
terpreting the meaning of compositional nonlexical-
ized noun compounds; and second, that humans can
agree amongst themselves about what constitutes a
good paraphrase for any given NC. As researchers in
this area and as proponents of this task, we believe
that both assumptions are valid, but if the analysis
of the task were to raise doubts about either assump-
tion (e.g., by showing poor agreement amongst hu-
man annotators), then this in itself would be a mean-
ingful and successful output of the task. As such,
we anticipate that the task and its associated dataset
will inspire further research, both on the theory and
development of paraphrase-based compound inter-
pretation and on its practical applications.
104
References
Timothy Baldwin and Takaaki Tanaka. 2004. Transla-
tion by machine of compound nominals: Getting it
right. In Proceedings of the ACL 2004 Workshop on
Multiword Expressions: Integrating Processing, pages
24?31.
Cristina Butnariu and Tony Veale. 2008. A concept-
centered approach to noun-compound interpretation.
In Proceedings of the 22nd International Conference
on Computational Linguistics (COLING 2008), pages
81?88.
Pamela Downing. 1977. On the creation and use of En-
glish compound nouns. Language, 53(4):810?842.
Timothy Finin. 1980. The Semantic Interpretation of
Compound Nominals. Ph.D. Dissertation, University
of Illinois, Urbana, Illinois.
Roxana Girju, Dan Moldovan, Marta Tatu, and Daniel
Antohe. 2005. On the semantics of noun compounds.
Journal of Computer Speech and Language - Special
Issue on Multiword Expressions, 4(19):479?496.
Roxana Girju. 2007. Improving the interpretation of
noun phrases with cross-linguistic information. In
Proceedings of the 45th Annual Meeting of the Associ-
ation of Computational Linguistics (ACL 2007), pages
568?575.
Pierre Isabelle. 1984. Another look at nominal com-
pounds. In Proceedings of the 10th International Con-
ference on Computational Linguistics, pages 509?516.
Michael Johnston and Frederica Busa. 1996. Qualia
structure and the compositional interpretation of com-
pounds. In Proceedings of the ACL 1996 Workshop on
Breadth and Depth of Semantic Lexicons, pages 77?
88.
Su Nam Kim and Timothy Baldwin. 2006. Interpret-
ing semantic relations in noun compounds via verb
semantics. In Proceedings of the 45th Annual Meet-
ing of the Association of Computational Linguistics
(COLING/ACL 2006) Main Conference Poster Ses-
sions, pages 491?498.
Mirella Lapata and Alex Lascarides. 2003. Detecting
novel compounds: the role of distributional evidence.
In Proceedings of the 10th conference of the European
chapter of the Association for Computational Linguis-
tics (EACL 2003), pages 235?242.
Mark Lauer. 1995. Designing Statistical Language
Learners: Experiments on Noun Compounds. Ph.D.
thesis, Dept. of Computing, Macquarie University,
Australia.
Judith Levi. 1978. The Syntax and Semantics of Complex
Nominals. Academic Press, New York.
Preslav Nakov andMarti A. Hearst. 2006. Using verbs to
characterize noun-noun relations. In LNCS vol. 4183:
Proceedings of the 12th international conference on
Artificial Intelligence: Methodology, Systems and Ap-
plications (AIMSA 2006), pages 233?244. Springer.
Preslav Nakov and Marti A. Hearst. 2008. Solving re-
lational similarity problems using the web as a cor-
pus. In Proceedings of the 46th Annual Meeting of the
Association of Computational Linguistics (ACL 2008),
pages 452?460.
Preslav Nakov. 2007. Using the Web as an Implicit
Training Set: Application to Noun Compound Syntax
and Semantics. Ph.D. thesis, EECS Department, Uni-
versity of California, Berkeley, UCB/EECS-2007-173.
Preslav Nakov. 2008a. Improved statistical machine
translation using monolingual paraphrases. In Pro-
ceedings of the 18th European Conference on Artificial
Intelligence (ECAI?2008), pages 338?342.
Preslav Nakov. 2008b. Noun compound interpretation
using paraphrasing verbs: Feasibility study. In LNAI
vol. 5253: Proceedings of the 13th international con-
ference on Artificial Intelligence: Methodology, Sys-
tems and Applications (AIMSA 2008), pages 103?117.
Springer.
Vivi Nastase and Stan Szpakowicz. 2003. Exploring
noun-modifier semantic relations. In Proceedings of
the 5th International Workshop on Computational Se-
mantics, pages 285?301.
Diarmuid O? Se?aghdha and Ann Copestake. 2007. Co-
occurrence contexts for noun compound interpreta-
tion. In Proceedings of the Workshop on A Broader
Perspective on Multiword Expressions, pages 57?64.
Diarmuid O? Se?aghdha. 2008. Learning Compound Noun
Semantics. Ph.D. thesis, University of Cambridge.
Barbara Rosario and Marti Hearst. 2001. Classify-
ing the semantic relations in noun compounds via a
domain-specific lexical hierarchy. In Proceedings of
the 2001 Conference on Empirical Methods in Natural
Language Processing (EMNLP 2005), pages 82?90.
Mary Ellen Ryder. 1994. Ordered Chaos: The Interpre-
tation of English Noun-Noun Compounds. University
of California Press, Berkeley, CA.
Rion Snow, Brendan O?Connor, Daniel Jurafsky, and An-
drew Ng. 2008. Cheap and fast ? but is it good?
evaluating non-expert annotations for natural language
tasks. In Proceedings of the 2008 Conference on
Empirical Methods in Natural Language Processing
(EMNLP 2008), pages 254?263.
Takaaki Tanaka and Timothy Baldwin. 2003. Noun-
noun compound machine translation: a feasibility
study on shallow processing. In Proceedings of the
ACL 2003 workshop on Multiword expressions, pages
17?24.
Beatrice Warren. 1978. Semantic patterns of noun-noun
compounds. In Gothenburg Studies in English 41,
Goteburg, Acta Universtatis Gothoburgensis.
105
Proceedings of the 5th International Workshop on Semantic Evaluation, ACL 2010, pages 39?44,
Uppsala, Sweden, 15-16 July 2010.
c?2010 Association for Computational Linguistics
SemEval-2010 Task 9: The Interpretation of Noun Compounds
Using Paraphrasing Verbs and Prepositions
Cristina Butnariu
University College Dublin
ioana.butnariu@ucd.ie
Su Nam Kim
University of Melbourne
nkim@csse.unimelb.edu.au
Preslav Nakov
National University of Singapore
nakov@comp.nus.edu.sg
Diarmuid
?
O S
?
eaghdha
University of Cambridge
do242@cam.ac.uk
Stan Szpakowicz
University of Ottawa
Polish Academy of Sciences
szpak@site.uottawa.ca
Tony Veale
University College Dublin
tony.veale@ucd.ie
Abstract
Previous research has shown that the mean-
ing of many noun-noun compounds N
1
N
2
can be approximated reasonably well by
paraphrasing clauses of the form ?N
2
that
. . . N
1
?, where ?. . . ? stands for a verb
with or without a preposition. For exam-
ple, malaria mosquito is a ?mosquito that
carries malaria?. Evaluating the quality of
such paraphrases is the theme of Task 9 at
SemEval-2010. This paper describes some
background, the task definition, the process
of data collection and the task results. We
also venture a few general conclusions be-
fore the participating teams present their
systems at the SemEval-2010 workshop.
There were 5 teams who submitted 7 sys-
tems.
1 Introduction
Noun compounds (NCs) are sequences of two or
more nouns that act as a single noun,1 e.g., stem
cell, stem cell research, stem cell research organi-
zation, etc. Lapata and Lascarides (2003) observe
that NCs pose syntactic and semantic challenges for
three basic reasons: (1) the compounding process
is extremely productive in English; (2) the seman-
tic relation between the head and the modifier is
implicit; (3) the interpretation can be influenced by
contextual and pragmatic factors. Corpus studies
have shown that while NCs are very common in
English, their frequency distribution follows a Zip-
fian or power-law distribution and the majority of
NCs encountered will be rare types (Tanaka and
Baldwin, 2003; Lapata and Lascarides, 2003; Bald-
win and Tanaka, 2004; ?O S?eaghdha, 2008). As a
consequence, Natural Language Processing (NLP)
1
We follow the definition in (Downing, 1977).
applications cannot afford either to ignore NCs or
to assume that they can be handled by relying on a
dictionary or other static resource.
Trouble with lexical resources for NCs notwith-
standing, NC semantics plays a central role in com-
plex knowledge discovery and applications, includ-
ing but not limited to Question Answering (QA),
Machine Translation (MT), and Information Re-
trieval (IR). For example, knowing the (implicit)
semantic relation between the NC components can
help rank and refine queries in QA and IR, or select
promising translation pairs in MT (Nakov, 2008a).
Thus, robust semantic interpretation of NCs should
be of much help in broad-coverage semantic pro-
cessing.
Proposed approaches to modelling NC seman-
tics have used semantic similarity (Nastase and Sz-
pakowicz, 2003; Moldovan et al, 2004; Kim and
Baldwin, 2005; Nastase and Szpakowicz, 2006;
Girju, 2007; ?O S?eaghdha and Copestake, 2007)
and paraphrasing (Vanderwende, 1994; Kim and
Baldwin, 2006; Butnariu and Veale, 2008; Nakov
and Hearst, 2008). The former body of work seeks
to measure the similarity between known and un-
seen NCs by considering various features, usually
context-related. In contrast, the latter group uses
verb semantics to interpret NCs directly, e.g., olive
oil as ?oil that is extracted from olive(s)?, drug
death as ?death that is caused by drug(s)?, flu shot
as a ?shot that prevents flu?.
The growing popularity ? and expected direct
utility ? of paraphrase-based NC semantics has
encouraged us to propose an evaluation exercise
for the 2010 edition of SemEval. This paper gives
a bird?s-eye view of the task. Section 2 presents
its objective, data, data collection, and evaluation
method. Section 3 lists the participating teams.
Section 4 shows the results and our analysis. In
Section 5, we sum up our experience so far.
39
2 Task Description
2.1 The Objective
For the purpose of the task, we focused on two-
word NCs which are modifier-head pairs of nouns,
such as apple pie or malaria mosquito. There are
several ways to ?attack? the paraphrase-based se-
mantics of such NCs.
We have proposed a rather simple problem: as-
sume that many paraphrases can be found ? perhaps
via clever Web search ? but their relevance is up in
the air. Given sufficient training data, we seek to es-
timate the quality of candidate paraphrases in a test
set. Each NC in the training set comes with a long
list of verbs in the infinitive (often with a prepo-
sition) which may paraphrase the NC adequately.
Examples of apt paraphrasing verbs: olive oil ?
be extracted from, drug death ? be caused by, flu
shot ? prevent. These lists have been constructed
from human-proposed paraphrases. For the train-
ing data, we also provide the participants with a
quality score for each paraphrase, which is a simple
count of the number of human subjects who pro-
posed that paraphrase. At test time, given a noun
compound and a list of paraphrasing verbs, a partic-
ipating system needs to produce aptness scores that
correlate well (in terms of relative ranking) with
the held out human judgments. There may be a
diverse range of paraphrases for a given compound,
some of them in fact might be inappropriate, but
it can be expected that the distribution over para-
phrases estimated from a large number of subjects
will indeed be representative of the compound?s
meaning.
2.2 The Datasets
Following Nakov (2008b), we took advantage of
the Amazon Mechanical Turk2 (MTurk) to acquire
paraphrasing verbs from human annotators. The
service offers inexpensive access to subjects for
tasks which require human intelligence. Its API
allows a computer program to run tasks easily and
collate the subjects? responses. MTurk is becoming
a popular means of eliciting and collecting linguis-
tic intuitions for NLP research; see Snow et al
(2008) for an overview and a further discussion.
Even though we recruited human subjects,
whom we required to take a qualification test,3
2
www.mturk.com
3We soon realized that we also had to offer a version of
our assignments without a qualification test (at a lower pay
rate) since very few people were willing to take a test. Overall,
data collection was time-consuming since many
annotators did not follow the instructions. We had
to monitor their progress and to send them timely
messages, pointing out mistakes. Although the
MTurk service allows task owners to accept or re-
ject individual submissions, rejection was the last
resort since it has the triply unpleasant effect of
(1) denying the worker her fee, (2) negatively af-
fecting her rating, and (3) lowering our rating as
a requester. We thus chose to try and educate our
workers ?on the fly?. Even so, we ended up with
many examples which we had to correct manu-
ally by labor-intensive post-processing. The flaws
were not different from those already described by
Nakov (2008b). Post-editing was also necessary to
lemmatize the paraphrasing verbs systematically.
Trial Data. At the end of August 2009, we
released as trial data the previously collected para-
phrase sets (Nakov, 2008b) for the Levi-250 dataset
(after further review and cleaning). This dataset
consisted of 250 noun-noun compounds form (Levi,
1978), each paraphrased by 25-30 MTurk workers
(without a qualification test).
Training Data. The training dataset was an ex-
tension of the trial dataset. It consisted of the same
250 noun-noun compounds, but the number of an-
notators per compound increased significantly. We
aimed to recruit at least 30 additional MTurk work-
ers per compound; for some compounds we man-
aged to get many more. For example, when we
added the paraphrasing verbs from the trial dataset
to the newly collected verbs, we had 131 different
workers for neighborhood bars, compared to just
50 for tear gas. On the average, we had 72.7 work-
ers per compound. Each worker was instructed
to try to produce at least three paraphrasing verbs,
so we ended up with 191.8 paraphrasing verbs per
compound, 84.6 of them being unique. See Table 1
for more details.
Test Data. The test dataset consisted of 388
noun compounds collected from two data sources:
(1) the Nastase and Szpakowicz (2003) dataset;
and (2) the Lauer (1995) dataset. The former
contains 328 noun-noun compounds (there are
also a number of adjective-noun and adverb-noun
pairs), while the latter contains 266 noun-noun
compounds. Since these datasets overlap between
themselves and with the training dataset, we had
to exclude some examples. In the end, we had 388
we found little difference in the quality of work of subjects
recruited with and without the test.
40
Training: 250 NCs Testing: 388 NCs All: 638 NCs
Total Min/Max/Avg Total Min/Max/Avg Total Min/Max/Avg
MTurk workers 28,199 50/131/72.7 17,067 57/96/68.3 45,266 50/131/71.0
Verb types 32,832 25/173/84.6 17,730 41/133/70.9 50,562 25/173/79.3
Verb tokens 74,407 92/462/191.8 46,247 129/291/185.0 120,654 92/462/189.1
Table 1: Statistics about the the training/test datasets. Shown are the total number of verbs proposed as
well as the minimum, maximum and average number of paraphrasing verb types/tokens per compound.
unique noun-noun compounds for testing, distinct
from those used for training. We aimed for 100
human workers per testing NC, but we could only
get 68.3, with a minimum of 57 and a maximum of
96; there were 185.0 paraphrasing verbs per com-
pound, 70.9 of them being unique, which is close
to what we had for the training data.
Data format. We distribute the training data as
a raw text file. Each line has the following tab-
separated format:
NC paraphrase frequency
where NC is a noun-noun compound (e.g., ap-
ple cake, flu virus), paraphrase is a human-
proposed paraphrasing verb optionally followed
by a preposition, and frequency is the number
of annotators who proposed that paraphrase. Here
is an illustrative extract from the training dataset:
flu virus cause 38
flu virus spread 13
flu virus create 6
flu virus give 5
flu virus produce 5
...
flu virus be made up of 1
flu virus be observed in 1
flu virus exacerbate 1
The test file has a similar format, except that the
frequency is not included and the paraphrases for
each noun compound appear in random order:
...
chest pain originate
chest pain start in
chest pain descend in
chest pain be in
...
License. All datasets are released under the Cre-
ative Commons Attribution 3.0 Unported license.4
4
creativecommons.org/licenses/by/3.0
2.3 Evaluation
All evaluation was performed by computing an ap-
propriate measure of similarity/correlation between
system predictions and the compiled judgements of
the human annotators. We did it on a compound-by-
compound basis and averaged over all compounds
in the test dataset. Section 4 shows results for three
measures: Spearman rank correlation, Pearson cor-
relation, and cosine similarity.
Spearman Rank Correlation (?) was adopted
as the official evaluation measure for the competi-
tion. As a rank correlation statistic, it does not use
the numerical values of the predictions or human
judgements, only their relative ordering encoded
as integer ranks. For a sample of n items ranked
by two methods x and y, the rank correlation ? is
calculated as follows:
? =
n
?
x
i
y
i
? (
?
x
i
)(
?
y
i
)
?
n
?
x
2
i
? (
?
x
i
)
2
?
n
?
y
2
i
? (
?
y
i
)
2
(1)
where x
i
, y
i
are the ranks given by x and y to the
ith item, respectively. The value of ? ranges be-
tween -1.0 (total negative correlation) and 1.0 (total
positive correlation).
Pearson Correlation (r) is a standard measure
of correlation strength between real-valued vari-
ables. The formula is the same as (1), but with
x
i
, y
i
taking real values rather than rank values;
just like ?, r?s values fall between -1.0 and 1.0.
Cosine similarity is frequently used in NLP to
compare numerical vectors:
cos =
?
n
i
x
i
y
i
?
?
n
i
x
2
i
?
n
i
y
2
i
(2)
For non-negative data, the cosine similarity takes
values between 0.0 and 1.0. Pearson?s r can be
viewed as a version of the cosine similarity which
performs centering on x and y.
Baseline: To help interpret these evaluation mea-
sures, we implemented a simple baseline. A dis-
tribution over the paraphrases was estimated by
41
System Institution Team Description
NC-INTERP International Institute of
Information Technology,
Hyderabad
Prashant
Mathur
Unsupervised model using verb-argument frequen-
cies from parsed Web snippets and WordNet
smoothing
UCAM University of Cambridge Clemens Hepp-
ner
Unsupervised model using verb-argument frequen-
cies from the British National Corpus
UCD-GOGGLE-I University College
Dublin
Guofu Li Unsupervised probabilistic model using pattern fre-
quencies estimated from the Google N-Gram corpus
UCD-GOGGLE-II Paraphrase ranking model learned from training
data
UCD-GOGGLE-III Combination of UCD-GOGGLE-I and UCD-
GOGGLE-II
UCD-PN University College
Dublin
Paul Nulty Scoring according to the probability of a paraphrase
appearing in the same set as other paraphrases pro-
vided
UVT-MEPHISTO Tilburg University Sander
Wubben
Supervised memory-based ranker using features
from Google N-Gram Corpus and WordNet
Table 2: Teams participating in SemEval-2010 Task 9
summing the frequencies for all compounds in the
training dataset, and the paraphrases for the test ex-
amples were scored according to this distribution.
Note that this baseline entirely ignores the identity
of the nouns in the compound.
3 Participants
The task attracted five teams, one of which (UCD-
GOGGLE) submitted three runs. The participants
are listed in Table 2 along with brief system de-
scriptions; for more details please see the teams?
own description papers.
4 Results and Discussion
The task results appear in Table 3. In an evaluation
by Spearman?s ? (the official ranking measure),
the winning system was UVT-MEPHISTO, which
scored 0.450. UVT also achieved the top Pear-
son?s r score. UCD-PN is the top-scoring system
according to the cosine measure. One participant
submitted part of his results after the official dead-
line, which is marked by an asterisk.
The participants used a variety of information
sources and estimation methods. UVT-MEPHISTO
is a supervised system that uses frequency informa-
tion from the Google N-Gram Corpus and features
from WordNet (Fellbaum, 1998) to rank candidate
paraphrases. On the other hand, UCD-PN uses
no external resources and no supervised training,
yet came within 0.009 of UVT-MEPHISTO in the
official evaluation. The basic idea of UCD-PN ?
that one can predict the plausibility of a paraphrase
simply by knowing which other paraphrases have
been given for that compound regardless of their
frequency ? is clearly a powerful one. Unlike the
other systems, UCD-PN used information about the
test examples (not their ranks, of course) for model
estimation; this has similarities to ?transductive?
methods for semi-supervised learning. However,
post-hoc analysis shows that UCD-PN would have
preserved its rank if it had estimated its model on
the training data only. On the other hand, if the task
had been designed differently ? by asking systems
to propose paraphrases from the set of all possi-
ble verb/preposition combinations ? then we would
not expect UCD-PN?s approach to work as well as
models that use corpus information.
The other systems are comparable to UVT-
MEPHISTO in that they use corpus frequencies
to evaluate paraphrases and apply some kind of
semantic smoothing to handle sparsity. How-
ever, UCD-GOGGLE-I, UCAM and NC-INTERP
are unsupervised systems. UCAM uses the 100-
million word BNC corpus, while the other systems
use Web-scale resources; this has presumably ex-
acerbated sparsity issues and contributed to a rela-
tively poor performance.
The hybrid approach exemplified by UCD-
GOGGLE-III combines the predictions of a sys-
tem that models paraphrase correlations and one
that learns from corpus frequencies and thus at-
tains better performance. Given that the two top-
scoring systems can also be characterized as using
these two distinct information sources, it is natu-
ral to consider combining these systems. Simply
normalizing (to unit sum) and averaging the two
sets of prediction values for each compound does
42
Rank System Supervised? Hybrid? Spearman ? Pearson r Cosine
1 UVT-MEPHISTO yes no 0.450 0.411 0.635
2 UCD-PN no no 0.441 0.361 0.669
3 UCD-GOGGLE-III yes yes 0.432 0.395 0.652
4 UCD-GOGGLE-II yes no 0.418 0.375 0.660
5 UCD-GOGGLE-I no no 0.380 0.252 0.629
6 UCAM no no 0.267 0.219 0.374
7 NC-INTERP* no no 0.186 0.070 0.466
Baseline yes no 0.425 0.344 0.524
Combining UVT and UCD-PN yes yes 0.472 0.431 0.685
Table 3: Evaluation results for SemEval-2010 Task 9 (* denotes a late submission).
indeed give better scores: Spearman ? = 0.472,
r = 0.431, Cosine = 0.685.
The baseline from Section 2.3 turns out to be
very strong. Evaluating with Spearman?s ?, only
three systems outperform it. It is less competitive
on the other evaluation measures though. This
suggests that global paraphrase frequencies may
be useful for telling sensible paraphrases from bad
ones, but will not do for quantifying the plausibility
of a paraphrase for a given noun compound.
5 Conclusion
Given that it is a newly-proposed task, this initial
experiment in paraphrasing noun compounds has
been a moderate success. The participation rate
has been sufficient for the purposes of comparing
and contrasting different approaches to the role
of paraphrases in the interpretation of noun-noun
compounds. We have seen a variety of approaches
applied to the same dataset, and we have been able
to compare the performance of pure approaches to
hybrid approaches, and of supervised approaches
to unsupervised approaches. The results reported
here are also encouraging, though clearly there is
considerable room for improvement.
This task has established a high baseline for sys-
tems to beat. We can take heart from the fact that
the best performance is apparently obtained from a
combination of corpus-derived usage features and
dictionary-derived linguistic knowledge. Although
clever but simple approaches can do quite well on
such a task, it is encouraging to note that the best
results await those who employ the most robust
and the most informed treatments of NCs and their
paraphrases. Despite a good start, this is a chal-
lenge that remains resolutely open. We expect that
the dataset created for the task will be a valuable
resource for future research.
Acknowledgements
This work is partially supported by grants from
Amazon and from the Bulgarian National Science
Foundation (D002-111/15.12.2008 ? SmartBook).
References
Timothy Baldwin and Takaaki Tanaka. 2004. Trans-
lation by Machine of Compound Nominals: Getting
it Right. In Proceedings of the ACL-04 Workshop
on Multiword Expressions: Integrating Processing,
pages 24?31, Barcelona, Spain.
Cristina Butnariu and Tony Veale. 2008. A Concept-
Centered Approach to Noun-Compound Interpreta-
tion. In Proceedings of the 22nd International Con-
ference on Computational Linguistics (COLING-
08), pages 81?88, Manchester, UK.
Pamela Downing. 1977. On the creation and use of En-
glish compound nouns. Language, 53(4):810?842.
Christiane Fellbaum, editor. 1998. WordNet: an elec-
tronic lexical database. MIT Press.
Roxana Girju. 2007. Improving the Interpretation
of Noun Phrases with Cross-linguistic Information.
In Proceedings of the 45th Annual Meeting of the
Association of Computational Linguistics (ACL-07),
pages 568?575, Prague, Czech Republic.
Su Nam Kim and Timothy Baldwin. 2005. Automatic
interpretation of noun compounds using WordNet
similarity. In Proceedings of the 2nd International
Joint Conference on Natural Language Processing
(IJCNLP-05), pages 945?956, Jeju Island, South Ko-
rea.
Su Nam Kim and Timothy Baldwin. 2006. Inter-
preting Semantic Relations in Noun Compounds via
Verb Semantics. In Proceedings of the COLING-
ACL-06 Main Conference Poster Sessions, pages
491?498, Sydney, Australia.
Mirella Lapata and Alex Lascarides. 2003. Detect-
ing novel compounds: The role of distributional evi-
dence. In Proceedings of the 10th Conference of the
43
European Chapter of the Association for Computa-
tional Linguistics (EACL-03), pages 235?242, Bu-
dapest, Hungary.
Mark Lauer. 1995. Designing Statistical Language
Learners: Experiments on Noun Compounds. Ph.D.
thesis, Macquarie University.
Judith Levi. 1978. The Syntax and Semantics of Com-
plex Nominals. Academic Press, New York, NY.
DanMoldovan, Adriana Badulescu, Marta Tatu, Daniel
Antohe, and Roxana Girju. 2004. Models for the Se-
mantic Classification of Noun Phrases. In Proceed-
ings of the HLT-NAACL-04 Workshop on Computa-
tional Lexical Semantics, pages 60?67, Boston, MA.
Preslav Nakov and Marti A. Hearst. 2008. Solving
Relational Similarity Problems Using the Web as a
Corpus. In Proceedings of the 46th Annual Meet-
ing of the Association of Computational Linguistics
(ACL-08), pages 452?460, Columbus, OH.
Preslav Nakov. 2008a. Improved Statistical Machine
Translation Using Monolingual Paraphrases. In Pro-
ceedings of the 18th European Conference on Artifi-
cial Intelligence (ECAI-08), pages 338?342, Patras,
Greece.
Preslav Nakov. 2008b. Noun Compound Interpreta-
tion Using Paraphrasing Verbs: Feasibility Study.
In Proceedings of the 13th International Confer-
ence on Artificial Intelligence: Methodology, Sys-
tems and Applications (AIMSA-08), pages 103?117,
Varna, Bulgaria.
Vivi Nastase and Stan Szpakowicz. 2003. Exploring
noun-modifier semantic relations. In Proceedings
of the 5th International Workshop on Computational
Semantics (IWCS-03), pages 285?301, Tilburg, The
Netherlands.
Vivi Nastase and Stan Szpakowicz. 2006. Matching
syntactic-semantic graphs for semantic relation as-
signment. In Proceedings of the 1st Workshop on
Graph Based Methods for Natural Language Pro-
cessing (TextGraphs-06), pages 81?88, New York,
NY.
Diarmuid
?
O S?eaghdha and Ann Copestake. 2007. Co-
occurrence Contexts for Noun Compound Interpre-
tation. In Proceedings of the ACL-07 Workshop
on A Broader Perspective on Multiword Expressions
(MWE-07), pages 57?64, Prague, Czech Republic.
Diarmuid
?
O S?eaghdha. 2008. Learning Compound
Noun Semantics. Ph.D. thesis, University of Cam-
bridge.
Rion Snow, Brendan O?Connor, Daniel Jurafsky, and
Andrew Ng. 2008. Cheap and Fast ? But is it Good?
Evaluating Non-Expert Annotations for Natural Lan-
guage Tasks. In Proceedings of the 2008 Conference
on Empirical Methods in Natural Language Process-
ing (EMNLP-08), pages 254?263, Honolulu, HI.
Takaaki Tanaka and Timothy Baldwin. 2003. Noun-
noun compound machine translation: A feasibility
study on shallow processing. In Proceedings of the
ACL-03 Workshop on Multiword Expressions (MWE-
03), pages 17?24, Sapporo, Japan.
Lucy Vanderwende. 1994. Algorithm for Automatic
Interpretation of Noun Sequences. In Proceedings
of the 15th International Conference on Compu-
tational Linguistics (COLING-94), pages 782?788,
Kyoto, Japan.
44
