On Combining Language Models :
Oracle Approach
Kadri Hacioglu and Wayne Ward
Center for Spoken Language Research
University of Colorado at Boulder
fhacioglu,whwg@cslr.colorado.edu
ABSTRACT
In this paper, we address the problem of combining several lan-
guage models (LMs). We find that simple interpolation methods,
like log-linear and linear interpolation, improve the performance
but fall short of the performance of an oracle. The oracle knows the
reference word string and selects the word string with the best per-
formance (typically, word or semantic error rate) from a list of word
strings, where each word string has been obtained by using a dif-
ferent LM. Actually, the oracle acts like a dynamic combiner with
hard decisions using the reference. We provide experimental results
that clearly show the need for a dynamic language model combina-
tion to improve the performance further. We suggest a method that
mimics the behavior of the oracle using a neural network or a de-
cision tree. The method amounts to tagging LMs with confidence
measures and picking the best hypothesis corresponding to the LM
with the best confidence.
1. INTRODUCTION
Statistical language models (LMs) are essential in speech recog-
nition and understanding systems for high word and semantic ac-
curacy, not to mention robustness and portability. Several language
models have been proposed and studied during the past two decades
[8]. Although it has turned out to be a rather difficult task to beat
the (almost) standard class/word n-grams (typically n = 2 or 3),
there has been a great deal of interest in grammar based language
models [1]. A promising approach for limited domain applications
is the use of semantically motivated phrase level stochastic context
free grammars (SCFGs) to parse a sentence into a sequence of se-
mantic tags which are further modeled using n-grams [2, 9, 10, 3].
The main motivation behind the grammar based LMs is the inabil-
ity of n-grams to model longer-distance constraints in a language.
With the advent of fairly fast computers and efficient parsing and
search schemes several researchers have focused on incorporating
relatively complex language models into speech recognition and
understanding systems at different levels. For example, in [3], we
The work is supported by DARPA through SPAWAR under grant
#N66001-00-2-8906.
report a significant perplexity improvement with a moderate in-
crease in word/semantic accuracy, at N -best list (rescoring) level,
using a dialog-context dependent, semantically motivated grammar
based language model.
Statistical language modeling is a ?learning from data? problem.
The generic steps to be followed for language modeling are
 preparation of training data
 selection of a model type
 specification of the model structure
 estimation of model parameters
The training data should consist of large amounts of text, which
is hardly satisfied in new applications. In those cases, complex
models fit to the training data. On the other hand, simple models
can not capture the actual structure. In the Bayes? (sequence) de-
cision framework of speech recognition/understanding we heavily
constrain the model structure to come up with a tractable and prac-
tical LM. For instance, in a class/word n-gram LM the dependency
of a word is often restricted to the class that it belongs and the de-
pendency of a class is limited to n-1 previous classes. The estima-
tion of the model parameters, which are commonly the probabili-
ties, is another important issue in language modeling. Besides data
sparseness, the estimation algorithms (e.g. EM algorithm) might be
responsible for the estimated probabilities to be far from optimal.
The aforementioned problems of learning have different effects
on different LM types. Therefore, it is wise to design LMs based on
different paradigms and combine them in some optimal sense. The
simplest combination method is the so called linear interpolation
[4]. Recently, the linear interpolation in the logarithmic domain
has been investigated in [6]. Perplexity results on a couple of tasks
have shown that the log-linear interpolation is better than the linear
interpolation. Theoretically, a far more powerful method for LM
combination is the maximum entropy approach [7]. However, it
has not been widely used in practice, since it is computationally
demanding.
In this research, we consider two LMs:
 class-based 3-gram LM (baseline).
 dialog dependent semantic grammar based 3-gram LM [3].
After N-best list rescoring experiments with linear and log-linear
interpolation, we realized that the performance in terms of word
and semantic accuracies fall considerably short of the performance
of an oracle. We explain the set-up for the oracle experiment and
point out that the oracle is a dynamic LM combiner. To fill the
performance gap, we suggest a method that can mimic the oracle.
concept 
         G S
C
W
dialog goal
generator
generator
sequence
word
speech
generator
waveform
phoneme
sequence
generator
P
A
dialog context
Figure 1: A speech production model
The paper is organized as follows. Section 2 presents the lan-
guage models considered in this study. In Section 3, we briefly
explain combining of LMs using linear and log-linear interpola-
tion. Section 4 explains the set up for the oracle experiment. Ex-
perimental results are reported in Section 5. The future work and
conclusions are given in the last section.
2. LANGUAGE MODELS
In language modeling, the goal is to find the probability distribu-
tion of word sequences, i.e. P (W ), where W = w
1
; w
2
:    ; w
L
.
We first describe a model for sentence generation in a dialog [5]
on which our grammar LM is based. The model is illustrated in
Figure 1. Here, the user has a specific goal that does not change
throughout the dialog. According to the goal and the dialog con-
text the user first picks a set of concepts with respective values and
then use phrase generators associated with concepts to generate the
word sequence. The word sequence is next mapped into a sequence
of phones and converted into a speech signal by the user?s vocal ap-
paratus which we finally observe as a sequence of acoustic feature
vectors.
Assuming that
 the dialog context S is given,
 W is independent of S but the concept sequence C, i.e.
P (W=C; S) = P (W=C),
 (W,C) pair is unique (possible with either Viterbi approxima-
tion or unambigious association between C and W),
one can easily show that P (W ) is given by
P (W ) = P (W=C)P (C=S) (1)
In (1) we identify two models:
 Concept model: P (C=S)
 Syntactic model : P (W=C)
<s> I WANT TO FLY FROM MIAMI FLORIDA TO SYDNEY AUS-
TRALIA ON OCTOBER FIFTH </s>
<s> [i want] [depart loc] [arrive loc] [date] </s>
<s> I DON?T TO FLY FROM MIAMI FLORIDA TO SYDNEY
AFTER AREA ON OCTOBER FIFTH </s>
<s> [Pronoun] [Contraction] [depart loc] [arrive loc] [after] [Noun] [date]
</s>
Figure 2: Examples of parsing into concepts and filler classes
The concept model is conditioned on the dialog context. Al-
though there are several ways to define a dialog context, we select
the last question prompted by the system as the dialog context. It is
simple and yet strongly predictive and constraining.
The concepts are classes of phrases with the same meaning. Put
differently, a concept class is a set of all phrases that may be used
to express that concept (e.g. [i want], [arrive loc]). Those concept
classes are augmented with single word, multiple word and a small
number of broad (and unambigious) part of speech (POS) classes.
In cases where the parser fails, we break the phrase into a sequence
of words and tag them using this set of ?filler? classes. Two exam-
ples in Figure 2 clearly illustrate the scheme.
The structure of the concept sequences is captured by an n-gram
LM. We train a seperate language model for each dialog context.
Given the context S and C = c
0
c
1
   c
K
; c
K+1
, the concept se-
quence probabilities are calculated as (for n = 3)
P (C=S) = P (c
1
= < s >; S)P (c
2
= < s >; c
1
; S)
K+1
Y
k=3
P (c
k
=c
k 2
; c
k 1
; S)
where c
0
and c
K+1
are for the sentence-begin and sentence-end
symbols, respectively.
Each concept class is written as a CFG and compiled into a
stochastic recursive transition network (SRTN). The production rules
define complete paths beginning from the start-node through the
end-node in these nets. The probability of a complete path tra-
versed through one or more SRTNs initiated by the top-level SRTN
associated with the concept is the probability of the phrase given
that concept. This probability is calculated as the multiplication of
all arc probabilities that defines the path. That is,
P (W=C) =
Q
K
i=1
P (s
i
=c
i
)
=
Q
K
i=1
Q
M
i
j=1
P (r
j
=c
i
)
where s
i
is a substring in W = w
1
; w
2
::w
L
= s
1
; ::s
2
; s
K
(K 
L) and r
1
; r
2
; :::r
M
i
are the production rules that construct s
i
. The
concept and rule sequences are assumed to be unique in the above
equations. The parser uses heuristics to comply with this assump-
tion.
SCFG and n-gram probabilities are learned from a text corpus
by simple counting and smoothing. Our semantic grammars have a
low degree of ambiguity and therefore do not require computation-
ally intensive stochastic training and parsing techniques.
The class based LM can be considered as a very special case
of our grammar based model. Concepts (or classes) are restricted
to those that represent a list of semantically similar words, like
[city name] , [day of week], [month day] and so forth. So, instead
of rule probabilities we have given the class the word probabilities,
P (w
i
=c
j
). For simplicity, each word belongs to at most one class.
reference
N-best list
grammar based
LM
class based
LM
dialog context
f   f   g  c  I
S
W
W
g  
c  
best Woracle
training data
Figure 3: The set up for oracle experiments
3. LINEAR AND LOG-LINEAR INTERPO-
LATION
Assuming that we have M language models, P
i
(W ); i = 1; 2;    ;M ,
the combined LM obtained using the linear interpolation (at sen-
tence level) is given by
P (W ) =
M
X
i=1

i
P
i
(W ) (2)
where 
i
are positive interpolation weights that sum up to unity.
The log-linear interpolation suggests an LM, again at sentence
level, given by
P (W ) =
1
Z()
M
Y
i=1
P
i
(W )

i (3)
where Z() is the normalization factor and it is a function of the
interpolation weights. The linearity in logarithmic domain is obvi-
ous if we take the logarithm of both sides. In the sequel, we omit
the normalization term, as its computation is very expensive. We
hope that its impact on the performance is not significant. Yet, it
prevents us from reporting perplexity results.
4. THE ORACLE APPROACH
The set-up for oracle experiments is illustrated in Figure 3. The
purpose of this set-up is twofold. First, we use it to evaluate the or-
acle performance. Second, we use it to prepare data for the training
of a stochastic decision model. For the sake of simplicity, we show
the set-up for two LMs and do experiments accordingly. Nonethe-
less, the set-up can be extended to an arbitrary number of LMs.
The language models are used for N-best list rescoring. The
N-best list is generated by a speech recognizer using a relatively
simpler LM (here, a class-based trigram LM) . The framework for
N-best list rescoring is the following MAP decision:
W

= argmax p
A
P (W=C
W
)P (C
W
=S) (4)
W 2 L
N
where p
A
is the acoustic probability from the first pass, C
W
is the
unique concept sequence associated with W , and L
N
denotes the
N-best list. Each rescoring module supplies the oracle with their
N-best list
grammar based
LM
class based
LM
dialog context
S
best W
I
Ig  
c  
W c  
W g  
f   c  
f   g  select  maxneural network
Figure 4: The LM combining system based on the oracle ap-
proach.
best hypothesis after rescoring. The oracle compares each hypoth-
esis to the reference and pick the one with the best word (or seman-
tic) accuracy.
For training purposes, we create the input feature vector by aug-
menting features from each rescoring module (f
g
; f
c
) and the dia-
log context (S). The output vector is the LM indicator I from the
oracle. The element that corresponds to the LM with the best final
hypothesis is unity and the rest are zeros. After training the oracle
combiner (here, we assume a neural network), we set our system
as shown in Figure 4. The input to the neural network (NN) is the
augmented feature vector. The output of the NN is the LM indica-
tor probably with fuzzy values. So, we first pick the max output,
and then, we select and output the respective word string.
5. EXPERIMENTAL RESULTS
The models were developed and tested in the context of the CU
Communicator dialog system which is used for telephone-based
flight, hotel and rental car reservations [11]. The text corpus was
divided into two parts as training and test sets with 15220 and 1220
sentences, respectively. The test set was further divided into two
parts. Each part, in turn, was used to optimize language and in-
terpolation weights to be used for the other part in a ?jacknife
paradigm?. The results were reported as the average of the two
results. The average sentence length of the corpus was 4 words
(end-of-sentence was treated as a word). We identified 20 dialog
contexts and labeled each sentence with the associated dialog con-
text.
We trained a dialog independent (DI) class based LM and dia-
log dependent (DD) grammar based LM. In all LMs n is set to 3.
It must be noted that the DI class-based LM served as the LM of
the baseline system with 921 unigrams including 19 classes. The
total number of the distinct words in the lexicon was 1681. The
grammar-based LM had 199 concept and filler classes that com-
pletely cover the lexicon. In rescoring experiments we set the N-
best list size to 10. We think that the choice of N = 10 is a reson-
able tradeoff between performance and complexity.
The perplexity results are presented in Table 1. The perplexity
of the grammar-based LM is 36.8% better than the baseline class-
based LM.
We did experiments using 10-best lists from the baseline recog-
nizer. We first determined the best possible performance in WER
Table 1: Perplexity results
LM Perplexity
DI class 3-gram 22.0
DD SCFG 3-gram 13.9
offered by 10-best lists. This is done by picking the hypothesis
with the lowest WER from each list. This gives an upperbound for
the performance gain possible from rescoring 10-best lists . The
rescoring results in terms of absolute and relative improvements in
WER and semantic error rate (SER) along with the best possible
improvement are reported in Table 2. It should be noted that the
optimizations are made using WER. The slight drop in SER with
interpolation might be due to that. Actually this is good for text
transcription but not for a dialog system. We believe that the re-
sults will reverse if we replace the optimization using WER with
the optimization using SER.
Table 2: The WER and SER results of the 10-best list rescoring
with different LMs: the baseline WER is 25.9% and SER is
23.7%
Method WER SER
Class based LM alone 0.0% 0.0%
Grammar based LM alone 1.4(5.4)% 1.4(5.9)%
Linear interpolation 1.6(6.2)% 1.3(5.5)%
Log-linear interpolation 1.7(6.6)% 1.2(5.1)%
Oracle 3.0(11.6)% 2.7(11.4) %
Best 6.4(24.1)% 5.5(23.2)%
The performance gap between the oracle and interpolation meth-
ods promotes the system in Figure 4. We expect that, based on the
universal approximation theory, a neural network with consistent
features, sufficiently large training data and proper training would
approximate fairly well the behavior of the oracle. On the other
hand, the performance gap between the oracle and the best possi-
ble performance from 10-best lists suggests the use of more than
two language models and dynamic combination with the acoustic
model.
6. CONCLUSIONS
We have presented our recent work on language model combin-
ing. We have shown that although a simple interpolation of LMs
improves the performance, it fails to reach the performance of an
oracle. We have proposed a method for LM combination that mim-
ics the behavior of the oracle. Although our work is not complete
without a neural network that mimics the oracle, we argue that
the universal approximation theory ensures the success of such a
method. However, extensive experiments are required to reach the
goal with the main focus on the selection of features. At the mo-
ment, the number of concepts, the number of filler classes and the
number of 3-gram hits in a sentence (all normalized by the length
of the sentence) and the behavior of n-grams in a context are the
features that we consider to use. Also, it has been observed that the
performance of the oracle is still far from the best possible perfor-
mance. This is partly due to the very small number of LMs used
in the rescoring, partly due to the oracle?s hard decision combining
strategy and partly due to the static combination with the acous-
tic model. The work is in progress towards the goal of filling the
performance gap.
7. REFERENCES
[1] J. K. Baker. Trainable grammars for speech recognition. In
Speech Communications for th 97th Meeting of the
Acoustical Society of America, pages 31?35, June 1979.
[2] J. Gillett and W. Ward. A language model combining
trigrams and stochastic context-free grammars. In 5-th
International Conference on Spoken Language Processing,
pages 2319?2322, Sydney, Australia, 1998.
[3] K. Hacioglu and W. Ward. Dialog-context dependent
language models combining n-grams and stochastic
context-free grammars. In submitted toInternational
Conference of Acoustics, Speech, and Signal Processing,
Salt-Lake, Utah,, 2001.
[4] F. Jelinek and R. Mercer. Interpolated estimation of markov
source parameters from sparse data. Pattern Recognition in
Practice, 23:381, 1980.
[5] A. Keller, B. Rueber, F. Seide, and B. Tran. PADIS - an
automatic telephone switchboard and directory information
system. Speech Communication, 23:95?111, 1997.
[6] D. Klakow. Log-linear interpolation of language models. In
5-th International Conference on Spoken Language
Processing, pages 1695?1699, Sydney, Australia, 1998.
[7] R. Rosenfeld. A maximum entropy approach to adaptive
language modeling. Computer Speech and Language,
(10):187?228, 1996.
[8] R. Rosenfeld. Two decades of statistical language modeling:
Where do we go from here? Proceedings of the IEEE,
88(8):1270?1278, August 2000.
[9] B. Souvignier, A. Keller, B. Rueber, H.Schramm, and
F. Seide. The thoughtful elephant: Strategies for spoken
dialog systems. IEEE Transactions on Speech and Audio
Processing, 8(1):51?62, January 2000.
[10] Y. Wang, M. Mahajan, and X.Huang. A unified context-free
grammar and n-gram model for spoken language processing.
In International Conference of Acoustics, Speech, and Signal
Processing, pages 1639?1642, Istanbul, Turkey, 2000.
[11] W. Ward and B. Pellom. The CU communicator system. In
IEEE Workshop on Automatic Speech Recognition and
Understanding, Keystone, Colorado, 1999.
University of Colorado Dialog Systems for 
Travel and Navigation 
B. Pellom, W. Ward, J. Hansen, R. Cole, K. Hacioglu, J. Zhang, X. Yu, S. Pradhan 
Center for Spoken Language Research, University of Colorado 
Boulder, Colorado 80303, USA 
{pellom, whw, jhlh, cole, hacioglu, zjp, xiu, spradhan}@cslr.colorado.edu 
 
ABSTRACT 
This paper presents recent improvements in the development of 
the University of Colorado ?CU Communicator? and ?CU-
Move? spoken dialog systems. First, we describe the CU 
Communicator system that integrates speech recognition, 
synthesis and natural language understanding technologies using 
the DARPA Hub Architecture. Users are able to converse with an 
automated travel agent over the phone to retrieve up-to-date 
travel information such as flight schedules, pricing, along with 
hotel and rental car availability.  The CU Communicator has 
been under development since April of 1999 and represents our 
test-bed system for developing robust human-computer 
interactions where reusability and dialogue system portability 
serve as two main goals of our work.  Next, we describe our more 
recent work on the CU Move dialog system for in-vehicle route 
planning and guidance.  This work is in joint collaboration with 
HRL and is sponsored as part of the DARPA Communicator 
program.  Specifically, we will provide an overview of the task, 
describe the data collection environment for in-vehicle systems 
development, and describe our initial dialog system constructed 
for route planning. 
1. CU COMMUNICATOR 
1.1 Overview  
The Travel Planning Task 
The CU Communicator system [1,2] is a Hub compliant 
implementation of the DARPA Communicator task [3].  The 
system combines continuous speech recognition, natural 
language understanding and flexible dialogue control to enable 
natural conversational interaction by telephone callers to access 
information from the Internet pertaining to airline flights, hotels 
and rental cars.  Specifically, users can describe a desired airline 
flight itinerary to the Communicator and use natural dialog to 
negotiate a flight plan.  Users can also inquire about hotel 
availability and pricing as well as obtain rental car reservation 
information.   
System Overview 
The dialog system is composed of a Hub and several servers as 
shown in Fig. 1.  The Hub is used as a centralized message router 
through which servers can communicate with one another [4].  
Frames containing keys and values are emitted by each server, 
routed by the hub, and received by a secondary server based on 
rules defined in a ?Hub script?.   
 
 
 
 
 
 
 
 
 
Figure 1.  Block diagram of the functional components that 
comprise the CU Communicator system1. 
1.2 Audio Server 
The audio server is responsible for answering the incoming call, 
playing prompts and recording user input.  Currently, our system 
uses the MIT/MITRE audio server that was provided to DARPA 
Communicator program participants.  The telephony hardware 
consists of an external serial modem device that connects to the 
microphone input and speaker output terminals on the host 
computer.  The record process is pipelined to the speech 
recognition server and the play process is pipelined the text-to-
speech server.  This audio server does not support barge-in. 
Recently we have developed a new audio server that supports 
barge-in using the Dialogic hardware platform.  The new audio 
server implements a Fast Normalized Least-Mean-Square (LMS) 
algorithm for software-based echo cancellation.  During 
operation, the echo from the system speech is actively cancelled 
from the recorded audio to allow the user to cut through while 
                                                          
1
 This work was supported by DARPA through SPAWAR under 
Grant No. N66001-002-8906.  The ?CU Move? system is 
supported in part through a joint collaboration with HRL 
Laboratories. 
 
 
 
Language
Generator
Language
enerator
Hub
Speech 
Recognizer
Speech 
Recognizer
Speech 
Synthesizer
Speech 
Synthesizer
Semantic
Parser
Se antic
Parser
Dialogue
Manager
Dialogue
anager
Data Base / 
Backend
Data Base / 
Backend
Confidence
Server
Confidence
Server
Audio ServerAudio Server
www
the system is speaking.  The new audio server operates in the 
Linux environment and is currently being field-tested at CSLR.  
Because the server implements software-based echo cancellation, 
it can work on virtually any low-cost Dialogic hardware 
platform.  This server will be made available to the research 
community as a resource in the near future. 
1.3 Speech Recognizer 
We are currently using the Carnegie Mellon University Sphinx-II 
system [5] in our speech recognition server. This is a semi-
continuous Hidden Markov Model recognizer with a class 
trigram language model. The recognition server receives the 
input vectors from the audio server. The recognition server 
produces a word lattice from which a single best hypothesis is 
picked and sent to the hub for processing by the dialog manager. 
Acoustic Modeling 
During dialog interaction with the user, the audio server sends 
the acoustic samples to three Sphinx-II speech recognizers.  
While the language model is the same for each decoder, the 
acoustic models consist of (i) speaker independent analog 
telephone, (ii) female adapted analog telephone, and (iii) cellular 
telephone adapted acoustic model sets.   Each decoder outputs a 
word string hypothesis along with a word-sequence probability 
for the best path.  An intermediate server is used to examine each 
hypothesis and pass the most likely word string onto the natural 
language understanding module.   
Language Modeling 
The Communicator system is designed for end users to get up-to-
date worldwide air travel, hotel and rental car information via the 
telephone. In the task there are word lists for countries, cities, 
states, airlines, etc.  To train a robust language model, names are 
clustered into different classes. An utterance with class tagging is 
shown in Fig.2.  In this example, city, hour_number, and am_pm 
are class names. 
Figure 2.  Examples of class-based and grammar-based 
language modeling  
Each commonly used word takes one class. The probability of 
word Wi given class Ci is estimated from training corpora. After 
the corpora are correctly tagged, a back-off class-based trigram 
language model can be computed from the tagged corpora.  We 
use the CMU-Cambridge Statistical Language Modeling Toolkit 
to compute our language models. 
More recently, we have developed a dialog context dependent 
language model (LM) combining stochastic context free 
grammars (SCFGs) and n-grams [6,7].  Based on a spoken 
language production model in which a user picks a set of 
concepts with respective values and constructs word sequences 
using phrase generators associated with each concept in 
accordance with the dialog context, this LM computes the 
probability of a word, P(W), as 
 
         P(W) = P(W/C) P(C/S)          (1) 
 
where W is the sequence of words, C is the sequence of concepts 
and S is the dialog context. Here, the assumptions are (i) S is 
given, (ii) W is independent of S but C, and (iii) W and C 
associations are unambiguous. This formulation can be 
considered as a general extension of the standard class word 
based statistical language model as seen in Fig. 2. 
 
The first term in (1) is modeled by SCFGs, one for each concept. 
The concepts are classes of phrases with the same meaning. Each 
SCFG is compiled into a stochastic recursive transition network 
(STRN). Our grammar is a semantic grammar since the 
nonterminals correspond to semantic concepts instead of 
syntactic constituents. The set of task specific concepts is 
augmented with a single word, multiple word and a small number 
of broad but unambigious part of speech (POS) classes to 
account for the phrases that are not covered by the grammar. 
These classes are considered as "filler" concepts within a unified 
framework. The second term in (1) is modeled as a pool of 
concept n-gram LMs. That is, we have a separate LM for each 
dialog context. At the moment, the dialog context is selected as 
the last question prompted by the system, as it is very simple and 
yet strongly predictive and constraining. SCFG and n-gram 
probabilities are learned by simple counting and smoothing. Our 
semantic grammars have a low degree of ambiguity and therefore 
do not require computationally intensive stochastic training and 
parsing techniques. 
 
Experimental results with N-best list rescoring were found 
promising (5-6% relative improvement in WER).  In addition, we 
have shown that a dynamic combining of our new LM and the 
standard class word n-gram (the LM currently in use in our 
system) should result in further improvements. At the present, we 
are interfacing the grammar LM to the speech recognizer using a 
word graph. 
1.4 Confidence Server 
Our prior work on confidence assessment has considered 
detection and rejection of word-level speech recognition errors 
and out-of-domain phrases using language model features [8].  
More recently [9], we have considered detection and rejection of 
misrecognized units at the concept level.  Because concepts are 
used to update the state of the dialog system, we believe that 
concept level confidence is vitally important to ensuring a 
graceful human-computer interaction.  Our current work on 
concept error detection has considered language model features 
(e.g., LM back-off behavior, language model score) as well as 
acoustic features from the speech recognizer (e.g., normalized 
acoustic score, lattice density, phone perplexity).  Confidence 
Original Utterance 
I want to go from Boston to Portland around nine a_m 
Class-Tagged Utterance 
I want to go from [city:Boston] to [city:Portland] 
around [hour_number:nine] [am_pm:a_m] 
Concept-Tagged Utterance 
[I_want: I want to go] [depart_loc: from Boston] 
[arrive_loc: to Portland] [time:around nine a_m] 
features are combined to compute word-level, concept-level, and 
utterance-level confidence scores.  
1.5 Language Understanding 
We use a modified version of the Phoenix [10] parser to map the 
speech recognizer output onto a sequence of semantic frames. A 
Phoenix frame is a named set of slots, where the slots represent 
related pieces of information. Each slot has an associated 
context-free semantic grammar that specifies word string patterns 
that can fill the slot. The grammars are compiled into Recursive 
Transition Networks, which are matched against the recognizer 
output to fill slots. Each filled slot contains a semantic parse tree 
with the slot name as root.  
Phoenix has been modified to also produce an extracted 
representation of the parse that maps directly onto the task 
concept structures. For example, the utterance  
?I want to go from Boston to Denver Tuesday morning?  
would produce the extracted parse: 
Flight_Constraint: Depart_Location.City.Boston 
Flight_Constraint: Arrive_Location.City.Denver 
Flight Constraints:[Date_Time].[Date].[Day_Name].tuesday 
                             [Time_Range].[Period_Of_Day].morning 
1.6 Dialog Management 
The Dialogue Manager controls the system?s interaction with the 
user and the application server. It is responsible for deciding 
what action the system will take at each step in the interaction. 
The Dialogue Manager has several functions. It resolves 
ambiguities in the current interpretation; Estimates confidence in 
the extracted information; Clarifies the interpretation with the 
user if required; Integrates new input with the dialogue context; 
Builds database queries (SQL); Sends information to NL 
generation for presentation to user; and prompts the user for 
missing information. 
We have developed a flexible, event driven dialogue manager in 
which the current context of the system is used to decide what to 
do next. The system does not use a dialogue network or a 
dialogue script, rather a general engine operates on the semantic 
representations and the current context to control the interaction 
flow.  The Dialogue Manager receives the extracted parse. It then 
integrates the parse into the current context. Context consists of a 
set of frames and a set of global variables. As new extracted 
information arrives, it is put into the context frames and 
sometimes used to set global variables. The system provides a 
general-purpose library of routines for manipulating frames. 
This ?event driven? architecture functions similar to a production 
system. An incoming parse causes a set of actions to fire which 
modify the current context. After the parse has been integrated 
into the current context, the DM examines the context to decide 
what action to take next. The DM attempts the following actions 
in the order listed: 
? Clarify if necessary  
? Sign off if all done  
? Retrieve data and present to user  
? Prompt user for required information  
The rules for deciding what to prompt for next are very 
straightforward. The frame in focus is set to be the frame 
produced in response to the user, or to the last system prompt.  
? If there are unfilled required slots in the focus frame, then 
prompt for the highest priority unfilled slot in the frame. 
? If there are no unfilled required slots in the focus frame, 
then prompt for the highest priority missing piece of 
information in the context.  
Our mechanism does not have separate ?user initiative? and 
?system initiative? modes. If the system has enough information 
to act on, then it does it. If it needs information, then it asks for 
it. The system does not require that the user respond to the 
prompt. The user can respond with anything and the system will 
parse the utterance and set the focus to the resulting frame. This 
allows the user to drive the dialog, but doesn?t require it. The 
system prompts are organized locally, at the frame level. The 
dialog manager or user puts a frame in focus, and the system tries 
to fill it. This representation is easy to author, there is no separate 
dialog control specification required. It is also robust in that it 
has a simple control that has no state to lose track of. 
An additional benefit of Dialog Manager mechanism is that it is 
very largely declarative. Most of the work done by a developer 
will be the creation of frames, forms and grammars. The system 
developer creates a task file that specifies the system ontology 
and templates for communicating about nodes in the hierarchy. 
The templates are filled in from the values in the frames to 
generate output in the desired language. This is the way we 
currently generate SQL queries and user prompts. An example 
task frame specification is: 
Frame:Air 
 [Depart_Loc]+ 
    Prompt: "where are you departing from" 
    [City_Name]* 
 Confirm: "You are departing from $([City_Name]).  
    Is that correct?" 
 Sql: "dep_$[leg_num] in (select airport_code from 
 airport_codes where city like '!%' $(and state_province 
like '[Depart_Loc].[State]' ) )" 
    [Airport_Code]* 
 
This example defines a frame with name Air and slot 
[Depart_Loc]. The child nodes of Depart_Loc are are 
[City_Name] and [Airport_Code]. The ?+? after [Depart_Loc] 
indicates that it is a mandatory field. The Prompt string is the 
template for prompting for the node information. The ?*? after 
[City_Name] and [Airport_Code] indicate that if either of them is 
filled, the parent node [Depart_Loc] is filled. The Confirm string 
is a template to prompt the user to confirm the values. The SQL 
string is the template to use the value in an SQL query to the 
database. 
The system will prompt for all mandatory nodes that have 
prompts. Users may specify information in any order, but the 
system will prompt for whatever information is missing until the 
frame is complete.   
1.7 Database & Internet Interface  
The back-end interface consists of an SQL database and domain-
specific Perl scripts for accessing information from the Internet.  
During operation, database requests are transmitted by the Dialog 
Manager to the database server via a formatted frame. 
The back-end consists of a static and dynamic information 
component.  Static tables contain data such as conversions 
between 3-letter airport codes and the city, state, and country of 
the airport (e.g., BOS for Boston Massachusetts).  There are over 
8000 airports in our database, 200 hotel chains, and 50 car rental 
companies.  The dynamic information content consists of 
database tables for car, hotel, and airline flights.   
When a database request is received, the Dialog Manager?s SQL 
command is used to select records in local memory.  If no 
records are found to match, the back-end can submit an HTTP-
based request for the information via the Internet.  Records 
returned from the Internet are then inserted as rows into the local 
SQL database and the SQL statement is once again applied.   
1.8 Language Generation 
The language generation module uses templates to generate text 
based on dialog speech acts.  Example dialog acts include 
?prompt? for prompting the user for needed information, 
?summarize? for summarization of flights, hotels, and rental cars, 
and ?clarify? for clarifying information such as departure and 
arrival cities that share the same name. 
1.9 Text-to-Speech Synthesis 
For audio output, we have developed a domain-dependent 
concatenative speech synthesizer.  Our concatenative synthesizer 
can adjoin units ranging from phonemes, to words, to phrases 
and sentences.   For domain modeling, we use a voice talent to 
record entire task-dependent utterances  (e.g., ?What are your 
travel plans??) as well as short phrases with carefully determined 
break points (e.g., ?United flight?, ?ten?, ?thirty two?, ?departs 
Anchorage at?).    Each utterance is orthographically transcribed 
and phonetically aligned using a HMM-based recognizer.   Our 
research efforts for data collection are currently focused on 
methods for reducing the audible distortion at segment 
boundaries, optimization schemes for prompt generation, as well 
as tools for rapidly correcting boundary misalignments.  In 
general, we find that some degree of hand-correction is always 
required in order to reduce distortions at concatenation points. 
During synthesis, the text is automatically divided into individual 
sentences that are then synthesized and pipelined to the audio 
server.  A text-to-phoneme conversion is applied using a 
phonetic dictionary.  Words that do not appear in the phonetic 
dictionary are automatically pronounced using a multi-layer 
perceptron based pronunciation module.  Here, a 5-letter context 
is extracted from the word to be pronounced.  The letter input is 
fed through the MLP and a phonetic symbol (or possibly epsilon) 
is output by the network.  By sliding the context window, we can 
extract the phonetic pronunciation of the word.   The MLP is 
trained using letter-context and symbol output pairs from a large 
phonetic dictionary. 
The selection of units to concatenate is determined using a hybrid 
search algorithm that operates at the word or phoneme level.  
During synthesis, sections of word-level text that have been 
recorded are automatically concatenated.  Unrecorded words or 
word sequences are synthesized using a Viterbi beam search 
across all available phonetic units.  The cost function includes 
information regarding phonetic context, pitch, duration, and 
signal amplitude.  Audio segments making up the best-path are 
then concatenated to generate the final sentence waveform.   
2. DATA COLLECTION & EVALUATION 
2.1 Data Collection Efforts 
Local Collection Effort 
The Center for Spoken Language Research maintains a dialup 
Communicator system for data collection1. Users wishing to use 
the dialogue system can register at our web site [1] and receive a 
PIN code and system telephone number. To date, our system has 
fielded over 1750 calls totaling over 25,000 utterances from 
nearly 400 registered users.  
NIST Multi-Site Data Collection 
During the months of June and July of 2000, The National 
Institute of Standards (NIST) conducted a multi-site data 
collection effort for the nine DARPA Communicator 
participants.  Participating sites included: AT&T, IBM, BBN, 
SRI, CMU, Colorado, MIT, Lucent, and MITRE.  In this data 
collection, a pool of potential users was selected from various 
parts of the United States by a market research firm.  The 
selected subjects were native speakers of American English who 
were possible frequent travelers.  Users were asked to perform 
nine tasks.  The first seven tasks consisted of fixed scenarios for 
one-way and round-trip flights both within and outside of the 
United States. The final two tasks consisted of users making 
open-ended business or vacation.   
2.2 System Evaluation 
Task Completion 
A total of 72 calls from NIST participants were received by the 
CU Communicator system.  Of these, 44 callers were female and 
28 were male.  Each scenario was inspected by hand and 
compared against the scenario provided by NIST to the subject. 
For the two open-ended tasks, judgment was made based on what 
the user asked for with that of the data provided to the user. In 
total, 53/72 (73.6%) of the tasks were completed successfully.   
A detailed error analysis can be found in [11]. 
Word Error Rate Analysis 
A total of 1327 utterances were recorded from the 72 NIST calls.  
Of these, 1264 contained user speech.  At the time of the June 
2000 NIST evaluation, the CU Communicator system did not 
implement voice-based barge-in.  We noticed that one source of 
error was due to users who spoke before the recording process 
was started.  Even though a tone was presented to the user to 
signify the time to speak, 6.9% of the utterances contained 
instances in which the user spoke before the tone.  Since all users 
were exposed to several other Communicator systems that 
                                                          
2
 The system can be accessed toll-free at 1-866-735-5189 
employed voice barge-in, there may be some effect from 
exposure to those systems. Table 3 summarizes the word error 
rates for the system utilizing the June 2000 NIST data as the test 
set.  Overall, the system had a word error rate (WER) of 26.0% 
when parallel gender-dependent decoders were utilized. Since 
June of 2000, we have collected an additional 15,000 task-
dependent utterances.  With the extra data, we were able to 
remove our dependence on the CMU Communicator training 
data [12].  When the language model was reestimated and 
language model weights reoptimized using only CU 
Communicator data, the WER dropped from 26.0% to 22.5%.  
This amounts to a 13.5% relative reduction in WER. 
Table 1: CU Communicator Word Error Rates for (A) 
Speaker Independent acoustic models and June 2000 
language model, (B) Gender-dependent parallel recognizers 
with June 2000 Language Model, and (C) Language Model 
retrained in December 2000. 
June 2000 NIST Evaluation Data, 1264 
utterances, 72 speakers 
Word Error 
Rate 
(A) Speaker Indep. HMMs (LM#1) 29.8% 
(B) Gender Dependent HMMs (LM#1) 26.0% 
(C) Gender Dependent HMMs (LM#2)  22.5% 
 
Core Metrics 
Sites in the DARPA Communicator program agreed to log a 
common set of metrics for their systems. The proposed set of 
metrics was: Task Completion, Time to Completion, Turns to 
Completion, User Words/Turn, System Words/Turn, User 
Concepts/Turn, Concept Efficiency, State of Itinerary, Error 
Messages, Help Messages, Response Latency, User Words to 
Completion, System Words to Completion, User Repeats, System 
Repeats/Reprompts, Word Error, Mean Length of System 
Utterance, and Mean Length of System Turn. 
Table 2: Dialogue system evaluation metrics 
Item Min Mean Max 
Time to Completion (secs) 120.9 260.3 537.2 
Total Turns to Completion 23 37.6 61 
Response Latency (secs) 1.5 1.9 2.4 
User Words to Task End 19 39.4 105 
System Words to End 173 331.9 914 
Number of Reprompts 0 2.4 15 
 
Table 2 summarizes results obtained from metrics derived 
automatically from the logged timing markers for the calls in 
which the user completed the task assigned to them.  The average 
time to task completion is 260.  During this period there are an 
average of 19 user turns and 19 computer turns (37.6 average 
total turns).  The average response latency was 1.86 seconds.  
The response latency also includes the time required to access the 
data live from the Internet travel information provider. 
3. CU MOVE 
3.1 Task Overview 
The ?CU Move? system represents our work towards achieving 
graceful human-computer interaction in automobile 
environments.  Initially, we have considered the task of vehicle 
route planning and navigation.  As our work progresses, we will 
expand our dialog system to new tasks such as information 
retrieval and summarization and multimedia access. 
The problem of voice dialog within vehicle environments offers 
some important speech research challenges. Speech recognition 
in car environments is in general fragile, with word-error-rates 
(WER) ranging from 30-65% depending on driving conditions. 
These changing environmental conditions include speaker 
changes (task stress, emotion, Lombard effect, etc.) as well as the 
acoustic environment (road/wind noise from windows, air 
conditioning, engine noise, exterior traffic, etc.).   
In developing the CU-Move system [13,14], there are a number 
of research challenges that must be overcome to achieve reliable 
and natural voice interaction within the car environment. Since 
the speaker is performing a task (driving the vehicle), the driver 
will experience a measured level of user task stress and therefore 
this should be included in the speaker-modeling phase. Previous 
studies have clearly shown that the effects of speaker stress and 
Lombard effect can cause speech recognition systems to fail 
rapidly. In addition, microphone type and placement for in-
vehicle speech collection can impact the level of acoustic 
background noise and speech recognition performance.    
3.2 Signal Processing  
Our research for robust recognition in automobile environments 
is concentrated on development of an intelligent microphone 
array.  Here, we employ a Gaussian Mixture Model (GMM) 
based environmental classification scheme to characterize the 
noise conditions in the automobile.  By integrating an 
environmental classification system into the microphone array 
design, decisions can be made as to how best to utilize a noise-
adaptive frequency-partitioned iterative enhancement algorithm 
[15,16] or model-based adaptation algorithms [17,18] during 
decoding to optimize speech recognition accuracy on the beam-
formed signal. 
3.3 Data Collection 
A five-channel microphone array was constructed using Knowles 
microphones and a multi-channel data recorder housing built 
(Fostex) for in-vehicle data collection. An additional reference 
microphone is situated behind the driver?s seat.  Fig. 3 shows the 
constructed microphone array and data recorder housing. 
      
Figure 3: Microphone array and reference microphone (left), 
Fostex multi-channel data recorder (right). 
As part of the CU-Move system formulation, a two phase data 
collection plan has been initiated. Phase I focuses on collecting 
acoustic noise and probe speech from a variety of cars and 
driving conditions. Phase II focuses on a extensive speaker 
collection across multiple U.S. sites. A total of eight vehicles 
have been selected for acoustic noise analysis. These include the 
following: a compact car, minivan, cargo van, sport utility 
vehicle (SUV), compact and full size trucks, sports car, full size 
luxury car.  A fixed 10 mile route through Boulder, CO was used 
for Phase I data collection. The route consisted of city (25 & 
45mph) and highway driving (45 & 65mph). The route included 
stop-and-go traffic, and prescribed locations where 
driver/passenger windows, turn signals, wiper blades, air 
conditioning were operated. Each data collection run per car 
lasted approximately 35-45 minutes.  A detailed acoustic analysis 
of Phase I data can be found in [13]. Our plan is to begin Phase 
II speech/dialogue data collection during spring 2001, which will 
include (i) phonetically balanced utterances, (ii) task-specific 
vocabularies, (iii) natural extemporaneous speech, and (iv) 
human-to-human and Wizard-of-Oz (WOZ) interaction with CU-
Communicator and CU-Move dialog systems. 
3.4 Prototype Dialog System 
Finally, we have developed a prototype dialog system for data 
collection in the car environment.  The dialog system is based on 
the MIT Galaxy-II Hub architecture with base system 
components derived from the CU Communicator system [1].  
Users interacting with the dialog system can enter their origin 
and destination address by voice. Currently, 1107 street names 
for Boulder, CO area are modeled.  The system can resolve street 
addresses by business name via interaction with an Internet 
telephone book.  This allows users to ask more natural route 
queries (e.g., ?I need an auto repair shop?, or ?I need to get to the 
Boulder Marriott?).  The dialog system automatically retrieves 
the driving instructions from the Internet using an online WWW 
route direction provider.  Once downloaded, the driving 
directions are queried locally from an SQL database.  During 
interaction, users mark their location on the route by providing 
spoken odometer readings.  Odometer readings are needed since 
GPS information has not yet been integrated into the prototype 
dialog system. Given the odometer reading of the vehicle as an 
estimate of position, route information such as turn descriptions, 
distances, and summaries can be queried during travel (e.g., 
"What's my next turn", "How far is it", etc.).  
The prototype system uses the CMU Sphinx-II speech recognizer 
with cellular telephone acoustic models along with the Phoenix 
Parser [10] for semantic parsing.  The dialog manager is mixed-
initiative and event driven.  For route guidance, the natural 
language generator formats the driving instructions before 
presentation to the user by the text-to-speech server.   For 
example, the direction,  "Park Ave W. becomes 22nd St." is 
reformatted to, "Park Avenue West becomes Twenty Second  
Street".  Here, knowledge of the task-domain can be used to 
significantly improve the quality of the output text.   For speech 
synthesis, we have developed a Hub-compliant server that 
interfaces to the AT&T NextGen speech synthesizer.   
3.5 Future Work 
We have developed a Hub compliant server that interfaces a 
Garmin GPS-III global positioning device to a mobile computer 
via a serial port link.  The GPS server reports vehicle velocity in 
the X,Y,Z directions as well as real-time updates of  vehicle 
position in latitude and longitude.  HRL Laboratories has 
developed a route server that interfaces to a major navigation 
content provider.  The HRL route server can take GPS 
coordinates as inputs and can describe route maneuvers in terms 
of GPS coordinates.  In the near-term, we will interface our GPS 
server to the HRL route server in order to provide real-time 
updating of vehicle position.  This will eliminate the need for 
periodic location update by the user and also will allow for more 
interesting dialogs to be established (e.g., the computer might 
proactively tell the user about upcoming points of interest, etc.). 
 
4. REFERENCES 
[1] http://communicator.colorado.edu 
[2] W. Ward, B. Pellom, "The CU Communicator System," IEEE 
Workshop on Automatic Speech Recognition and Understanding, 
Keystone Colorado, December, 1999. 
[3] http://fofoca.mitre.org 
[4] Seneff, S., Hurley, E., Lau, R., Pao, C., Schmid, P., Zue,  V., 
?Galaxy-II: A Reference Architecture for Conversational System 
Development,? Proc. ICSLP, Sydney Australia, Vol. 3, pp. 931-
934, 1998. 
[5] Ravishankar, M.K., ?Efficient Algorithms for Speech 
Recognition?. Unpublished Dissertation CMU-CS-96-138, 
Carnegie Mellon University, 1996 
[6] K. Hacioglu, W. Ward, "Dialog-Context Dependent Language 
Modeling Using N-Grams and Stochastic Context-Free Grammars", 
Proc. IEEE ICASSP, Salt Lake City, May 2001. 
[7] K. Hacioglu, W. Ward, "Combining Language Models : Oracle 
Approach", Proc. Human Language Technology Conference, San 
Diego, March 2001. 
[8] R. San-Segundo, B. Pellom, W. Ward, J. M. Pardo, "Confidence 
Measures for Dialogue Management in the CU Communicator 
System," Proc. IEEE ICASSP, Istanbul Turkey, June 2000. 
[9] R. San-Segundo, B. Pellom, K. Hacioglu, W. Ward, J.M. Pardo, 
"Confidence Measures for Dialogue Systems," Proc. IEEE ICASSP, 
Salt Lake City, May 2001.  
[10] Ward, W., ?Extracting Information From Spontaneous Speech?, 
Proc. ICSLP, September 1994. 
[11] B. Pellom, W. Ward, S. Pradhan, "The CU Communicator: An 
Architecture for Dialogue Systems", Proc. ICSLP, Beijing China, 
November 2000. 
[12] Eskenazi,  M., Rudnicky, A., Gregory, K., Constantinides, P.,  
Brennan, R., Bennett, K., Allen, J., ?Data Collection and 
Processing in the Carnegie Mellon Communicator,?   Proc. 
Eurospeech-99, Budapest, Hungary. 
[13] J.H.L. Hansen, J. Plucienkowski, S. Gallant, B.L. Pellom, W. Ward, 
"CU-Move: Robust Speech Processing for In-Vehicle Speech 
Systems," Proc. ICSLP, vol. 1, pp. 524-527, Beijing, China, Oct. 
2000. 
[14] http://cumove.colorado.edu/ 
[15] J.H.L. Hansen, M.A. Clements, ?Constrained Iterative Speech 
Enhancement with Application to Speech Recognition,? IEEE 
Trans. Signal Proc., 39(4):795-805, 1991. 
[16] B. Pellom, J.H.L. Hansen, ?An Improved Constrained Iterative 
Speech Enhancement Algorithm for Colored Noise Environments," 
IEEE Trans. Speech & Audio Proc., 6(6):573-79, 1998. 
[17] R. Sarikaya, J.H.L. Hansen, "Improved Jacobian Adaptation for 
Fast Acoustic Model Adaptation in Noisy Speech Recognition," 
Proc. ICSLP, vol. 3, pp. 702-705, Beijing, China, Oct. 2000. 
[18] R. Sarikaya, J.H.L. Hansen, "PCA-PMC: A novel use of a priori 
knowledge for fast model combination," Proc. ICASSP, vol. II, pp. 
1113-1116, Istanbul, Turkey, June 2000. 
Towards Robust Semantic Role Labeling
Sameer S. Pradhan?
BBN Technologies
Wayne Ward??
University of Colorado
James H. Martin?
University of Colorado
Most semantic role labeling (SRL) research has been focused on training and evaluating on
the same corpus. This strategy, although appropriate for initiating research, can lead to over-
training to the particular corpus. This article describes the operation of ASSERT, a state-of-the
art SRL system, and analyzes the robustness of the system when trained on one genre of data
and used to label a different genre. As a starting point, results are first presented for training
and testing the system on the PropBank corpus, which is annotated Wall Street Journal (WSJ)
data. Experiments are then presented to evaluate the portability of the system to another source of
data. These experiments are based on comparisons of performance using PropBanked WSJ data
and PropBanked Brown Corpus data. The results indicate that whereas syntactic parses and
argument identification transfer relatively well to a new corpus, argument classification does
not. An analysis of the reasons for this is presented and these generally point to the nature of the
more lexical/semantic features dominating the classification task where more general structural
features are dominant in the argument identification task.
1. Introduction
Automatic, accurate, and wide-coverage techniques that can annotate naturally oc-
curring text with semantic structure can play a key role in NLP applications such as
information extraction (Harabagiu, Bejan, and Morarescu 2005), question answering
(Narayanan and Harabagiu 2004), and summarization. Semantic role labeling (SRL) is
one method for producing such semantic structure. When presented with a sentence,
a semantic role labeler should, for each predicate in the sentence, first identify and
then label its semantic arguments. This process entails identifying groups of words
in a sentence that represent these semantic arguments and assigning specific labels to
them. In the bulk of recent work, this problem has been cast as a problem in supervised
machine learning. Using these techniques with hand-corrected syntactic parses, it has
? Department of Speech and Language Processing, 10 Moulton Street, Room 2/245, Cambridge, MA 02138.
E-mail: sameer@cemantix.org.
?? The Center for Spoken Language Research, Campus Box 594, Boulder, CO 80309.
E-mail: whw@colorado.edu.
? The Center for Spoken Language Research, Campus Box 594, Boulder, CO 80309.
E-mail: martin@colorado.edu.
Submission received: 15 July 2006; revised submission received: 3 May 2007; accepted for publication:
19 June 2007.
? 2008 Association for Computational Linguistics
Computational Linguistics Volume 34, Number 2
been possible to achieve accuracies within the range of human inter-annotator agree-
ment. More recent approaches have involved using improved features such as n-best
parses (Koomen et al 2005; Toutanova, Haghighi, and Manning 2005); exploiting argu-
ment interdependence (Jiang, Li, and Ng 2005); using information from fundamentally
different, and complementary syntactic, views (Pradhan, Ward et al 2005); combining
hypotheses from different labeling systems using inference (Ma`rquez et al 2005); as well
as applying novel learning paradigms (Punyakanok et al 2005; Toutanova, Haghighi,
and Manning 2005; Moschitti 2006) that try to capture more sequence and contextual
information. Some have also tried to jointly decode the syntactic and semantic structures
(Yi and Palmer 2005; Musillo and Merlo 2006). This problem has also been the subject
of two CoNLL shared tasks (Carreras and Ma`rquez 2004; Carreras and Ma`rquez 2005).
Although all of these systems perform quite well on the standard test data, they show
significant performance degradation when applied to test data drawn from a genre
different from the data on which the system was trained. The focus of this article is
to present results from an examination into the primary causes of the lack of portability
across genres of data.
To set the stage for these experiments we first describe the operation of ASSERT, our
state-of-the art SRL system. Results are presented for training and testing the system on
the PropBank corpus, which is annotatedWall Street Journal (WSJ) data.
Experiments are then presented to assess the portability of the system to another
genre of data. These experiments are based on comparisons of performance using
PropBanked WSJ data and PropBanked Brown corpus data. The results indicate that
whereas syntactic parses and identification of the argument bearing nodes transfer
relatively well to a new corpus, role classification does not. Analysis of the reasons for
this generally point to the nature of the more lexical/semantic features dominating the
classification task, as opposed to the more structural features that are relied upon for
identifying which constituents are associated with arguments.
2. Semantic Annotation and Corpora
In this article, we report on the task of reproducing the semantic labeling scheme used
by the PropBank corpus (Palmer, Gildea, and Kingsbury 2005). PropBank is a 300k-word
corpus in which predicate argument relations are marked for almost all occurrences
of non-copula verbs in the WSJ part of the Penn Treebank (Marcus, Santorini, and
Marcinkiewicz 1993). PropBank uses predicate independent labels that are sequential
from ARG0 to ARG5, where ARG0 is the PROTO-AGENT (usually the subject of a tran-
sitive verb) and ARG1 is the PROTO-PATIENT (usually its direct object). In addition to
these core arguments, additional adjunctive arguments, referred to as ARGMs, are also
marked. Some examples are ARGM-LOC, for locatives, and ARGM-TMP, for temporals.
Table 1 shows the argument labels associated with the predicate operate in PropBank.
Following is an example structure extracted from the PropBank corpus. The syntax
tree representation along with the argument labels is shown in Figure 1.
[ARG0 It] [predicate operates] [ARG1 stores] [ARGM?LOC mostly in Iowa and Nebraska].
The PropBank annotation scheme assumes that a semantic argument of a predicate
aligns with one or more nodes in the hand-corrected Treebank parses. Although most
frequently the arguments are identified by one node in the tree, there can be cases where
the arguments are discontinuous and more than one node is required to identify parts
of the arguments.
290
Pradhan, Ward, and Martin Towards Robust Semantic Role Labeling
Table 1
Argument labels associated with the predicate operate (sense: work) in the PropBank corpus.
Tag Description
ARG0 Agent, operator
ARG1 Thing operated
ARG2 Explicit patient (thing operated on)
ARG3 Explicit argument
ARG4 Explicit instrument
Treebank trees can also have trace nodes which refer to another node in the tree, but
do not have any words associated with them. These can also be marked as arguments.
As traces are typically not reproduced by current automatic parsers, we decided not
to consider them in our experiments?whether or not they represent arguments of a
predicate. None of the previous work has attempted to recover such trace arguments.
PropBank also contains arguments that are coreferential.
We treat discontinuous and coreferential arguments in accordance to the CoNLL
shared task on semantic role labeling. The first part of a discontinuous argument is
labeled as it is, and the second part of the argument is labeled with a prefix ?C-?
appended to it. All coreferential arguments are labeled with a prefix ?R-? appended.
We follow the standard convention of using Section 02 to Section 21 as the training
set, Section 00 as the development set, and Section 23 as the test set. The training set
comprises about 90,000 predicates instantiating about 250,000 arguments and the test
set comprises about 5,000 predicates instantiating about 12,000 arguments.
3. Task Description
In ASSERT, the task of semantic role labeling is implemented by assigning role labels to
constituents of a syntactic parse. Parts of the overall process can be analyzed as three
different tasks as introduced by Gildea and Jurafsky (2002):
1. Argument Identification?This is the process of identifying parsed
constituents in the sentence that represent semantic arguments of
Figure 1
Syntax tree for a sentence illustrating the PropBank tags.
291
Computational Linguistics Volume 34, Number 2
Figure 2
Syntax tree for a sentence illustrating the PropBank arguments.
a given predicate. Each node in a parse tree can be classified (with
respect to a given predicate) as either one that represents a semantic
argument (i.e., a NON-NULL node) or one that does not represent
any semantic argument (i.e., a NULL node).
2. Argument Classification?Given constituents known to represent
arguments of a predicate, this process assigns the appropriate
argument labels to them.
3. Argument Identification and Classification?A combination of the two tasks.
For example, in the tree shown in Figure 2, the node IN that dominates for is a
NULL node because it does not correspond to a semantic argument. The node NP
that dominates about 20 minutes is a NON-NULL node, because it does correspond to
a semantic argument?ARGM-TMP.
4. ASSERT (Automatic Statistical SEmantic Role Tagger)
4.1 System Architecture
ASSERT
1 produces a separate set of semantic role labels for each candidate predicate in
a sentence. Because PropBank only annotates arguments for non-copula/non-auxiliary
verbs, those are also the predicates considered by ASSERT. ASSERT performs constituent-
based role assignment. The basic inputs are a sentence and a syntactic parse of the
sentence. For each constituent in the parse tree, the system extracts a set of features
and uses a classifier to assign a label to the constituent. The set of labels used are the
PropBank argument labels plus NULL, which means no argument is assigned to that
constituent for the predicate under consideration.
Support vector machines (SVMs) (Burges 1998; Vapnik 1998) have been shown to
perform well on text classification tasks, where data is represented in a high dimen-
sional space using sparse feature vectors (Joachims 1998; Kudo and Matsumoto 2000;
Lodhi et al 2002). We formulate the problem as a multi-class classification problem
using an SVM classifier. We employ a ONE vs ALL (OVA) approach to train n classifiers
for a multi-class problem. The classifiers are trained to discriminate between examples
1 www.cemantix.org/assert.
292
Pradhan, Ward, and Martin Towards Robust Semantic Role Labeling
of each class, and those belonging to all other classes combined. During testing, the
classifier scores on an example are combined to predict its class label.
ASSERT was developed using TinySVM2 along with YamCha3 (Kudo and
Matsumoto 2000, 2001) as the SVM training and classification software. The system
uses a polynomial kernel with degree 2; the cost per unit violation of the margin, C = 1;
and, tolerance of the termination criterion, e = 0.001. SVMs output distances from the
classification hyperplane, not probabilities. These distances may not be comparable
across classifiers, especially if different features are used to train each binary classifier.
These raw SVM scores are converted to probabilities by fitting to a sigmoid function as
done by Platt (2000).
The architecture just described has the drawback that each argument classification
is made independently, without considering other arguments assigned to the same
predicate. This ignores a potentially important source of information: that a predicate is
likely to instantiate a certain set of arguments. To represent this information, a backed-
off trigram model is trained for the argument sequences. In this model, the predicate is
considered as an argument and is part of the sequence. This model represents not only
what arguments a predicate is likely to take, but also the probability of a given sequence
of arguments. During the classification process the system generates an argument
lattice using the n-best hypotheses for each node in the syntax tree. A Viterbi search
through the lattice uses the probabilities assigned by the sigmoid as the observation
probabilities, along with the argument sequence language model probabilities, to find
the maximum likelihood path such that each node is either assigned a value belonging
to the PropBank arguments, or NULL. The search is also constrained so that no two
nodes that overlap are both assigned NON-NULL labels.
4.2 Features
The feature set used in ASSERT is a combination of features described in Gildea and
Jurafsky (2002) as well as those introduced in Pradhan et al (2004), Surdeanu et al
(2003), and the syntactic-frame feature proposed in (Xue and Palmer 2004). Following is
the list of features used.
4.2.1 Predicate. This is the predicate whose arguments are being identified. The surface
form as well as the lemma are added as features.
4.2.2 Path. The syntactic path through the parse tree from the parse constituent to the
predicate being classified.
For example, in Figure 3, the path from ARG0 (The lawyers) to the predicate went is
represented with the string NP?S?VP?VBD. ? and ? represent upward and downward
movement in the tree, respectively.
4.2.3 Phrase Type. Syntactic category (NP, PP, etc.) of the constituent.
4.2.4 Position.Whether the constituent is before or after the predicate.
2 www.chasen.org/~taku/software/TinySVM/.
3 www.chasen.org/~taku/software/YamCha/.
293
Computational Linguistics Volume 34, Number 2
Figure 3
Illustration of path NP?S?VP?VBD.
4.2.5 Voice. Whether the predicate is realized as an active or passive construction. A
set of hand-written tgrep expressions operating on the syntax tree is used to identify
passives.
4.2.6 SubCategorization. This is the phrase structure rule expanding the predicate?s parent
node in the parse tree. For example, in Figure 3, the subcategorization for the predicate
?went? is VP?VBD-PP-NP.
4.2.7 Predicate Cluster. The distance function used for clustering is based on the intuition
that verbs with similar semantics will tend to have similar direct objects. For example,
verbs such as eat, devour, and savor will tend to all occur with direct objects describing
food. The clustering algorithm uses a database of verb?direct-object relations extracted
by Lin (1998). The verbs were clustered into 64 classes using the probabilistic co-
occurrence model of Hofmann and Puzicha (1998). We then use the verb class of the
current predicate as a feature.
4.2.8 Head Word. Syntactic head of the constituent.
4.2.9 Head Word POS. Part of speech of the head word.
4.2.10 Named Entities in Constituents. Binary features for seven named entities
(PERSON, ORGANIZATION, LOCATION, PERCENT, MONEY, TIME, DATE) tagged by
IdentiFinder (Bikel, Schwartz, and Weischedel 1999).
4.2.11 Path Generalizations.
1. Partial Path?Path from the constituent to the lowest common ancestor
of the predicate and the constituent.
2. Clause-based path variations?Position of the clause node (S, SBAR)
seems to be an important feature in argument identification (Hacioglu
et al 2004). Therefore we experimented with four clause-based path
feature variations.
(a) Replacing all the nodes in a path other than clause nodes with an
asterisk. For example, the path NP?S?VP?SBAR?NP?VP?VBD
becomes NP?S?*S?*?*?VBD.
294
Pradhan, Ward, and Martin Towards Robust Semantic Role Labeling
(b) Retaining only the clause nodes in the path, which for the given
example would produce NP?S?S?VBD.
(c) Adding a binary feature that indicates whether the constituent is in
the same clause as the predicate.
(d) Collapsing the nodes between S nodes, which gives
NP?S?NP?VP?VBD.
3. Path n-grams?This feature decomposes a path into a series of trigrams.
For example, the path NP?S?VP?SBAR?NP?VP?VBD becomes: NP?S?VP,
S?VP?SBAR, VP?SBAR?NP, SBAR?NP?VP, and so on. Shorter paths were
padded with nulls.
4. Single character phrase tags?Each phrase category is clustered to a
category defined by the first character of the phrase label.
4.2.12 Predicate Context. We added the predicate context to capture predicate sense
variations. Two words before and two words after were added as features. The POS
of the words were also added as features.
4.2.13 Punctuation. Punctuation plays an particularly important role for some adjunctive
arguments, so punctuation on the left and right of the constituent are included as
features. The absence of punctuation in either location was indicated with a NULL
feature value.
4.2.14 Head Word of PP. Many adjunctive arguments, such as temporals and locatives,
occur as prepositional phrases in a sentence, and it is often the case that the head words
of those phrases, which are prepositions, are not very discriminative; for example, in
the city and in a few minutes both share the same head word in and neither contain a
named entity, but the former is ARGM-LOC, whereas the latter is ARGM-TMP. The head
word of the first noun phrase inside the prepositional phrase is used for this feature.
Preposition information is represented by appending it to the phrase type, for example,
?PP-in? instead of ?PP.?
4.2.15 First and Last Word/POS in Constituent. The first and last words in a constituent
along with their parts of speech.
4.2.16 Ordinal Constituent Position. In order to avoid false positives where constituents
far away from the predicate are spuriously identified as arguments, we added this
feature which is a concatenation of the constituent type and its ordinal position from
the predicate.
4.2.17 Constituent Tree Distance. This is a more fine-grained way of specifying the already
present position feature. This is the number of constituents that are encountered in the
path from the predicate to the constituent under consideration.
4.2.18 Constituent Relative Features. These are nine features representing the phrase type,
head word, and head word part of speech of the parent, and left and right siblings of
the constituent.
4.2.19 Temporal Cue Words. There are several temporal cue words that are not captured
by the named entity tagger and were added as binary features indicating their presence.
295
Computational Linguistics Volume 34, Number 2
The BOW toolkit was used to identify words and bigrams that had highest average
mutual information with the ARGM-TMP argument class.
4.2.20 Syntactic Frame. Sometimes there are multiple children under a constituent having
the same phrase type, and one or both of them represent arguments of the predicate. In
such situations, the path feature is not very good at discriminating between them, and
the position feature is also not very useful. To overcome this limitation, Xue and Palmer
(2004) proposed a feature which they call the syntactic frame. For example, if the sub-
categorization for the predicate is VP?VBD-NP-NP, then the syntactic frame feature
for the first NP in the sequence would be, ?vbd NP np,? and for the second it would be
?vbd np NP.?
4.3 Performance
Table 2 illustrates the performance of the system using Treebank parses and using parses
produced by a Charniak parser (Automatic). Precision (P), Recall (R), and F-scores are
given for the identification and combined tasks, and Classification Accuracy (A) for the
classification task. Classification performance using Charniak parses is only 1% absolute
worse than when using Treebank parses. On the other hand, argument identification
performance using Charniak parses is 10.9% absolute worse. About half of the ID errors
are due to missing constituents in the Charniak parse. Techniques to address the issue
of constituents missing from the syntactic parse tree are reported in Pradhan, Ward
et al (2005).
4.4 Feature Salience
In Pradhan, Hacioglu et al (2005) we reported on a series of experiments to show the
relative importance of features to the Identification task and the Classification task.
The data show that different features are more salient for each of the two tasks. For
the Identification task, the most salient features are the Path and Partial Path. The
Predicate was not particularly salient. For Classification, the most salient features are
Head Word, First Word, and Last Word of a constituent as well as the Predicate itself.
For Classification, the Path and Phrase Type features were not very salient.
A reasonable conclusion is that structural features dominate the Identification task,
whereas more specific lexical or semantic features are important for Classification. As
Table 2
Performance of ASSERT on WSJ test set (Section 23) using correct Treebank parses as well as
Charniak parses.
Parse Task P (%) R (%) F A (%)
Treebank Id. 97.5 96.1 96.8
Class. ? ? ? 93.0
Id. + Class. 91.8 90.5 91.2
Automatic Id. 87.8 84.1 85.9
Class. ? ? ? 92.0
Id. + Class. 81.7 78.4 80.0
296
Pradhan, Ward, and Martin Towards Robust Semantic Role Labeling
we?ll see later, this pattern has critical implications for the portability of these features
across genres.
5. Robustness to Genre of Data
Most work on SRL systems has been focused on improving the labeling performance
on a test set belonging to the same genre of text as the training set. Both the Treebank on
which the syntactic parser is trained, and the PropBank on which the SRL systems are
trained represent articles from the year 1989 of theWall Street Journal. Improvements to
the system may reflect tuning to the specific data set rather than real progress. For this
technology to be widely accepted it is critical that it perform reasonably well on text
with styles different from the training data. The availability of PropBank annotation
for another corpus of a very different style than WSJ makes it possible to evaluate
the portability of SRL techniques, and to understand some of the factors affecting
performance.
5.1 The Brown Corpus
The Brown Corpus is a standard corpus of American English that consists of about one
million words of English text printed in the calendar year 1961 (Kuc?era and Francis
1967). The corpus contains about 500 samples of 2,000+ words each. The motivation
for creating this corpus was to create a heterogeneous sample of English text useful for
comparative language studies. Table 3 lists the sections in the Brown corpus.
5.2 Semantic Annotation
Release 3 of the Penn Treebank contains hand-corrected syntactic trees from a subset
of the Brown Corpus (sections F, G, K, L, M, N, P, and R). Sections belonging to the
newswire genre were not included because a considerable amount of similar material
was already available from the WSJ portion of the Treebank. Palmer, Gildea, and
Kingsbury (2005) annotated a significant portion of the Treebanked Brown corpus
Table 3
List of sections in the Brown corpus.
A. Press reportage
B. Press editorial
C. Press reviews (theater, books, music, and dance)
D. Religion
E. Skills and hobbies
F. Popular lore
G. Belles lettres, biography, memoirs, etc.
H. Miscellaneous
J. Learned
K. General fiction
L. Mystery and detective fiction
M. Science fiction
N. Adventure and Western fiction
P. Romance and love story
R. Humor
297
Computational Linguistics Volume 34, Number 2
with PropBank roles. The PropBanking philosophy is the same as described earlier.
In all, about 17,500 predicates are tagged with their semantic arguments. For these
experiments we use the release of the Brown PropBank dated September 2005.
Table 4 shows the number of predicates that have been tagged for each section:
6. Robustness Experiments
In this section, we present a series of experiments comparing the performance of ASSERT
on the WSJ corpus to performance on the Brown corpus. The intent is to understand
how well the algorithms and features transfer to other sources and to understand the
nature of any problems.
6.1 Cross-Genre Testing
The first experiment evaluates the performance of the system when it is trained on
annotated data from one genre of text (WSJ) and is used to label a test set from a different
genre (the Brown corpus). The ASSERT system described earlier, trained on WSJ Sec-
tions 02?21, was used to label arguments for the PropBanked portion of the Brown
corpus. As before, the Charniak parser was used to generate the syntax parse trees.
Table 5 shows the F-score for Identification and combined Identification and Classi-
fication for each of the eight different text genres as well as the overall performance
on Brown. As can be seen, there is a significant degradation across all the various
sections of Brown. In addition, although there is a noticeable drop in performance for
the Identification task, the bulk of the degradation comes in the combined task.
The following are among the likely factors contributing to this performance
degradation:
1. Syntactic parsing errors?The semantic role labeler is completely
dependent on the quality of the syntactic parses; missing, mislabeled,
and misplaced constituents will all lead to errors. Because the syntactic
parser used to generate the parse trees is heavily lexicalized, the genre
difference will have an impact on the accuracy of the parses, and the
features extracted from them.
2. The Brown corpus may in fact be fundamentally more difficult than the
WSJ. There are many potential sources for this kind of difficulty. Among
Table 4
Number of predicates that have been tagged in the PropBanked portion of the Brown corpus.
Section Total Propositions Total Lemmas
F 926 321
G 777 302
K 8,231 1,476
L 5,546 1,118
M 167 107
N 863 269
P 788 252
R 224 140
298
Pradhan, Ward, and Martin Towards Robust Semantic Role Labeling
Table 5
Performance on the entire PropBanked Brown corpus when ASSERT is trained on WSJ.
Train Test Id. F Id. + Class F
WSJ WSJ (Section 23) 85.9 80.0
WSJ Brown (Popular lore) 77.2 64.9
WSJ Brown (Biography, memoirs) 77.1 61.1
WSJ Brown (General fiction) 78.9 64.9
WSJ Brown (Detective fiction) 82.9 67.1
WSJ Brown (Science fiction) 83.8 64.5
WSJ Brown (Adventure) 82.5 65.5
WSJ Brown (Romance and love story) 81.2 63.9
WSJ Brown (Humor) 78.8 62.5
WSJ Brown (All) 81.2 63.9
Table 6
Deleted/missing argument-bearing constituents in Charniak parses of the WSJ test set
(Section 23) and the entire PropBanked Brown corpus.
Total Misses %
WSJ (Section 23) 13,612 851 6.2
Brown (Popular lore) 2,280 219 9.6
Brown (Biography, memoirs) 2,180 209 9.6
Brown (General fiction) 21,611 1,770 8.2
Brown (Detective fiction) 14,740 1,105 7.5
Brown (Science fiction) 405 23 5.7
Brown (Adventure) 2,144 169 7.9
Brown (Romance and love story) 1,928 136 7.1
Brown (Humor) 592 61 10.3
Brown (All) 45,880 3,692 8.1
the most obvious sources are a greater diversity in the range of use of
predicates and headwords in the Brown domain. That is, the lexical
features may be more varied in terms of predicate senses and raw
number of predicates. More consistent usage of predicates and
headwords in the WSJ may allow very specific features to be trained
in WSJ that will not be as well trained or as salient in Brown.
The following discussion explores each of these possibilities in turn.
Table 6 shows the percentage of argument-bearing nodes deleted from the syntactic
parse leading to an Identification error. The syntactic parser deletes 6.2% of the argu-
ment bearing nodes in the tree when it is trained and tested on WSJ. When tested on
Brown, this number increases to 8.1%, a relative increase of 30%. This effect goes some
way toward explaining the decrease in Identification performance, but does not explain
the large degradation in combined task performance.
The effect of errors from the syntactic parse can be removed by using the correct
syntactic trees from the Treebanks for both corpora. This permits an analysis of other
299
Computational Linguistics Volume 34, Number 2
factors affecting the performance difference. For this experiment, we evaluated per-
formance for all combinations of training and testing on WSJ and Brown. A test set
for the Brown corpus was generated by selecting every tenth sentence in the corpus.
The development set used by Bacchiani et al (2006) was withheld for future parameter
tuning. No parameter tuning was done for these experiments. The parameters used
for the data reported in Table 2 were used for all subsequent tests reported in this
article. This procedure results in a training set for Brown that contains approximately
14k predicates. In order to have training sets comparable in size for the two corpora,
stratified sampling was used to create a WSJ training set of the same size as the Brown
training set. Section 23 of WSJ is still used as the test set for that corpus.
Table 7 shows the results of this experiment. Rows 2 and 4 show the conditions
when the system is trained on the 14k predicate WSJ training. Testing on Brown vs. WSJ
results in a modest reduction in F-score from 95.3 to 93.0 for argument identification.
Although there is some reduction in Identification performance in the absence of errors
in the syntactic parse tree, the effect is not large. However, argument classification
shows a large drop in accuracy from 86.1% to 72.9%. These data reiterate the point that
syntactic parse errors are not the major factor accounting for the reduction in performance
for Brown.
The next point to note is the effect of varying the amount of training data for WSJ
for testing results on WSJ and Brown. The first row of Table 7 shows the performance
when ASSERT is trained on the full WSJ training set of Sections 2?21 (90k predicates).
The second row shows performance when it is trained on the reduced set of 14k pred-
icates. Whereas the F1 score for Identification dropped by 1.5 percentage points (from
96.8% to 95.3%) the Classification rate dropped by 6.9% percent absolute. Classification
seemingly requires considerable more data before its performance begins to asymptote.
Table 7
Performance when ASSERT is trained using correct Treebank parses, and is used to classify test
set from either the same genre or another. For each data set, the number of examples used for
training are shown in parentheses.
SRL Train SRL Test Task P (%) R (%) F A (%)
WSJ WSJ Id. 97.5 96.1 96.8
(90k) (5k) Class. 93.0
Id. + Class. 91.8 90.5 91.2
WSJ WSJ Id. 96.3 94.4 95.3
(14k) (5k) Class. 86.1
Id. + Class. 84.4 79.8 82.0
BROWN BROWN Id. 95.7 94.9 95.2
(14k) (1.6k) Class. 80.1
Id. + Class. 79.9 77.0 78.4
WSJ BROWN Id. 94.6 91.5 93.0
(14k) (1.6k) Class. 72.9
Id. + Class. 72.1 67.2 69.6
BROWN WSJ Id. 94.9 93.8 94.3
(14k) (5k) Class. 78.3
Id. + Class. 76.6 73.3 74.9
300
Pradhan, Ward, and Martin Towards Robust Semantic Role Labeling
Finally, row 3 shows the performance for training and testing on Brown. The
performance of argument Identification is essentially the same as when training and
testing on WSJ. However, argument Classification is 6 percentage points worse (80.1%
vs. 86.1%) when training and testing on Brown than when training and testing on WSJ.
This pattern is consistent with our third hypothesis given previously: Brown may be an
intrinsically harder corpus for this task.
Some possible causes for this difficulty are:
1. More unique predicates or head words than are seen in the WSJ set, so
there is less training data for each;
2. More predicate sense ambiguity in Brown;
3. Less consistent relations between predicates and head words;
4. A greater preponderance of difficult semantic roles in Brown;
5. Relatively fewer examples of predictive features such as named entities.
The remainder of this section explores each of these possibilities in turn.
In order to test the importance of predicate sense in this process, we added oracle
predicate sense information as a feature in ASSERT. Because only about 60% of the
PropBanked Brown corpus was tagged with predicate sense information, these results
are not directly comparable to the one reported in the earlier tables. In this case, both the
Brown training and test sets are subsets of the earlier ones, with about 10k predicates
in training and 1k in testing. For comparison, we used the same size WSJ training
data. Table 8 shows the performance when trained on WSJ and Brown, and tested on
Brown, with andwithout predicate sense information, and for both Treebank parses and
Charniak parses. We find that there is a small increase in the combined identification
and classification performance when trained on Brown and tested on Brown.
One reason for this could simply be the raw number of instances that are seen in
the training data. Because we know that Predicate and Head Word are two particularly
salient features for classification, the percentages of a combination of these features in
the Brown test set that are seen in both the training sets should be informative. This
information is shown in Table 9. In order to get a cross-corpus statistic, we also present
the same numbers on the WSJ test set.
Table 8
Performance on Brown test, using Brown and WSJ training sets, with and without oracle
predicate sense information when using Treebank parses.
Id. Id. + Class.
Train Predicate Sense P % R % F P % R % F
Brown
(10k) ? 95.6 95.4 95.5 78.6 76.2 77.4?
95.7 95.7 95.7 81.1 77.1 79.0
WSJ
(10k) ? 93.4 91.7 92.5 71.1 65.8 68.4?
93.3 91.8 92.5 71.3 66.1 68.6
301
Computational Linguistics Volume 34, Number 2
Table 9
Features seen in training for various test sets.
Test? WSJ Brown
Features T seen t seen T seen t seen
Corpora ? (%) (%) (%) (%)
WSJ Predicate Lemma (P) 76 94 65 80
Predicate Sense (S) 79 93 64 78
Head Word (HW) 61 87 49 76
P+HW 19 31 13 17
Brown Predicate Lemma (P) 64 85 86 94
Predicate Sense (S) 29 35 91 96
Head Word (HW) 37 63 68 87
P+HW 10 17 27 33
T = types; t = tokens.
It can be seen that for both theWSJ and Brown corpus test sets, the number of predi-
cate lemmas as well as the particular senses seen in the respective test sets is quite high.
However, a cross comparison shows that there is about a 15% drop in coverage from
WSJ/WSJ to WSJ/Brown. It is also interesting to note that for WSJ, the drop in coverage
for predicate lemmas is almost the same as that for individual predicate senses. This fur-
ther confirms the hypothesis thatWSJ has a more homogeneous collection of predicates.
When we compare the drop in coverage for Brown/Brown vs. WSJ/Brown, we find
about the same drop in coverage for predicate lemmas, but a much more significant
drop for the senses. This variation in senses in Brown is probably the reason that adding
sense information helps more for the Brown test set. In the WSJ case, the addition of
word sense as a feature does not add much information, and so the numbers are not
much different than for the baseline. Similarly, we can see that percentage of headwords
seen across the two genres also drop significantly, and they are much lower to begin
with. Finding the coverage for the predicate lemma and head word combination is still
worse, and this is not even considering the sense. Therefore, data sparseness is another
potential reason that the importance of the predicate sense feature does not reflect in the
performance numbers.
As noted earlier, another possible source of difficulty for Brown may be the distri-
bution of PropBank arguments in this corpus. Table 10 shows the classification perfor-
mance for each argument, for each of the four configurations (train on Brown orWSJ and
test on WSJ or Brown). Among the two most frequent arguments?ARG0 and ARG1?
ARG1 seems to be affected the most. When the training and test sets are from the same
genre, the performance on ARG0 is slightly worse on the Brown test set. ARG1 on the
other hand is about 5% worse on both precision and recall, when trained and tested on
Brown. For core-arguments ARG2?5 which are highly predicate sense dependent, there
is a much larger performance drop.
Finally, another possible reason for the drop in performance is the distribution of
named entities in the corpus. Table 11 shows the frequency of occurrence of name
entities in 10k WSJ and Brown training sets. It can be seen that number of organizations
talked about in Brown is much smaller than in WSJ, and there are more person names.
Also, monetary amounts which frequently fill the ARG3 and ARG4 slots are also much
more infrequent in Brown, and so is the incidence of percentages. This would definitely
have some impact on the usability of these features in the learned models.
302
Pradhan, Ward, and Martin Towards Robust Semantic Role Labeling
7. Effect of Improved Syntactic Parses
Practical natural language processing systems will always use errorful automatic
parses, and so it would be interesting to find out how much syntactic parser errors hin-
der performance on the task of semantic role labeling. Fortunately, recent improvements
to the Charniak parser provided an opportunity to test this hypothesis. We use the latest
version of the Charniak parser that does n-best re-ranking (Charniak and Johnson 2005)
and the model that is self-trained using the North American News corpus (NANC).
This version adaptsmuch better to the Brown corpus (McClosky, Charniak, and Johnson
Table 10
Classification accuracy for each argument type in the WSJ (W) and Brown (B) test sets.
W?W B?B B?W W?B
Number in Number in P R P R P R P R
Argument WSJ Test Brown Test (%) (%) (%) (%) (%) (%) (%) (%)
ARG0 3,149 1,122 91.1 96.8 90.4 92.8 83.4 92.2 87.4 93.3
ARG1 4,264 1,375 90.2 92.0 85.0 88.5 78.7 79.7 83.4 89.0
ARG2 796 312 73.3 66.6 65.9 60.6 49.7 56.4 59.5 48.1
ARG3 128 25 74.3 40.6 71.4 20.0 30.8 16.0 28.6 4.7
ARG4 72 20 89.1 68.1 57.1 60.0 16.7 5.0 61.1 15.3
C-ARG0 2 4 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0
C-ARG1 165 34 91.5 64.8 80.0 35.3 64.7 32.4 82.1 19.4
R-ARG0 189 45 83.1 93.7 82.7 95.6 62.5 88.9 76.8 77.2
R-ARG1 122 44 77.8 63.1 91.7 75.0 64.5 45.5 54.5 59.8
ARGM-ADV 435 290 78.0 66.0 67.6 64.8 74.7 44.8 49.9 71.0
ARGM-CAU 65 15 82.5 72.3 80.0 53.3 62.5 66.7 86.0 56.9
ARGM-DIR 72 114 57.1 50.0 71.0 62.3 46.6 36.0 39.7 43.1
ARGM-DIS 270 65 87.6 86.7 81.0 72.3 54.1 70.8 89.6 64.1
ARGM-EXT 31 10 83.3 48.4 0.0 0.0 0.0 0.0 33.3 3.2
ARGM-LOC 317 147 73.8 80.8 60.8 70.7 52.6 48.3 60.6 65.6
ARGM-MNR 305 144 56.1 59.0 64.5 63.2 42.6 55.6 51.4 48.9
ARGM-MOD 454 129 99.6 100.0 100.0 100.0 100.0 99.2 99.6 100.0
ARGM-NEG 201 85 100.0 99.5 97.7 98.8 100.0 85.9 94.8 99.5
ARGM-PNC 99 43 60.4 58.6 66.7 55.8 54.8 39.5 52.8 57.6
ARGM-PRD 5 8 0.0 0.0 33.3 12.5 0.0 0.0 0.0 0.0
ARGM-TMP 978 280 85.4 90.4 84.8 85.4 71.3 83.6 82.2 76.0
W?B = ASSERT trained on B and used to classify W test set.
Table 11
Distribution of the named entities in a 10k data fromWSJ and Brown corpora.
Name Entity WSJ Brown
PERSON 1,274 2,037
ORGANIZATION 2,373 455
LOCATION 1,206 555
MONEY 831 32
DATE 710 136
PERCENT 457 5
TIME 9 21
303
Computational Linguistics Volume 34, Number 2
Table 12
Performance for different versions of the Charniak parser used in the experiments.
Train Test F
WSJ WSJ 91.0
WSJ Brown 85.2
Brown Brown 88.4
WSJ+NANC Brown 87.9
2006a, 2006b). We also use another model that is trained on the Brown corpus itself. The
performance of these parsers is shown in Table 12.
We describe the results of the following five experiments:
1. ASSERT is trained on features extracted from automatically generated
parses of the PropBanked WSJ sentences. The syntactic parser (Charniak
parser) is itself trained on the WSJ training sections of the Treebank. This
is used to classify Section 23 of WSJ.
2. ASSERT is trained on features extracted from automatically generated
parses of the PropBanked WSJ sentences. The syntactic parser (Charniak
parser) is itself trained on the WSJ training sections of the Treebank. This
is used to classify the Brown test set.
3. ASSERT is trained on features extracted from automatically generated
parses of the PropBanked Brown corpus sentences. The syntactic parser
is trained using the WSJ portion of the Treebank. This is used to classify
the Brown test set.
4. ASSERT is trained on features extracted from automatically generated
parses of the PropBanked Brown corpus sentences. The syntactic parser
is trained using the Brown training portion of the Treebank. This is used
to classify the Brown test set.
5. ASSERT is trained on features extracted from automatically generated
parses of the PropBanked Brown corpus sentences. The syntactic parser
is the version that is self-trained using 2,500,000 sentences from NANC,
and where the starting version is trained only on WSJ data (McClosky,
Charniak, and Johnson 2006b). This is used to classify the Brown test set.
The same training and test sets used for the systems in Table 7 are used in this
experiment. Table 13 shows the results. For simplicity of discussion we have labeled the
five conditions as A, B, C, D, and E. Comparing conditions B and C shows that when the
features used to train ASSERT are extracted using a syntactic parser that is trained onWSJ
it performs at almost the same level on the task of identification, regardless of whether
it is trained on the PropBanked Brown corpus or the PropBanked WSJ corpus. This,
however, is about 5?6 F-score points lower than when all the three (the syntactic parser
training set, ASSERT training set, and ASSERT test set) are from the same genre?WSJ or
Brown, as seen in A and D. For the combined task, the gap between the performance
for conditions B and C is about 10 F-score points apart (59.1 vs. 69.8). Looking at the
argument classification accuracies, we see that using ASSERT trained on WSJ to test
Brown sentences results in a 12-point drop in F-score. Using ASSERT trained on Brown
304
Pradhan, Ward, and Martin Towards Robust Semantic Role Labeling
Table 13
Performance on WSJ and Brown test sets when ASSERT is trained on features extracted from
automatically generated syntactic parses.
Setup Parser Train SRL Train SRL Test Task P (%) R (%) F A (%)
A. WSJ WSJ WSJ Id. 87.3 84.8 86.0
(40k ? sec:00?21) (14k) (5k) Class. 84.1
Id. + Class. 77.5 69.7 73.4
B. WSJ WSJ Brown Id. 81.7 78.3 79.9
(40k ? sec:00?21) (14k) (1.6k) Class. 72.1
Id. + Class. 63.7 55.1 59.1
C. WSJ Brown Brown Id. 81.7 78.3 80.0
(40k ? sec:00?21) (14k) (1.6k) Class. 79.2
Id. + Class. 78.2 63.2 69.8
D. Brown Brown Brown Id. 87.6 82.3 84.8
(20k) (14k) (1.6k) Class. 78.9
Id. + Class. 77.4 62.1 68.9
E. WSJ+NANC Brown Brown Id. 87.7 82.5 85.0
(2,500k) (14k) (1.6k) Class. 79.9
Id. + Class. 77.2 64.4 70.0
H. WSJ+NANC Brown WSJ Id. 88.2 78.2 82.8
(2,500k) (14k) (5k) Class. 76.9
Id. + Class. 75.4 51.6 61.2
using the WSJ-trained syntactic parser reduces accuracy by about 5 F-score points.
When ASSERT is trained on Brown using a syntactic parser also trained on Brown, we
get a quite similar classification performance, which is again about 5 points lower than
what we get using all WSJ data. Finally, looking at conditions C and D we find that
the difference in performance on the combined task of identification and classification
using the Brown corpus for training ASSERT is very close (69.8 vs. 68.9) even though
the syntactic parser used in C has a performance that is about 3.2 points worse than
that used in D. This indicates that better parse structure is less important than lexical
semantic coverage for obtaining better performance on the Brown corpus.
8. Adapting to a New Genre
One possible way to ameliorate the effects of domain specificity is to incrementally
add small amounts of data from a new domain to the already available out-of-domain
training data. In the following experiments we explore this possibility by slowly adding
data from the Brown corpus to a fixed amount of WSJ data.
One section of the Brown corpus?section K?has about 8,200 predicates anno-
tated. Therefore, we will take six different scenarios?two in which we will use correct
Treebank parses, and the four others in which we will use automatically generated
parses using the variations used before. All training sets start with the same number
of examples as that of the Brown training set. The part of this section used as a test set
for the CoNLL 2005 shared task was used as the test set for these experiments. This test
set contains 804 predicates in 426 sentences of Brown section K.
305
Computational Linguistics Volume 34, Number 2
Table 14 shows the results. In all six settings, the performance on the task of
identification and classification improves gradually until about 5,625 examples of sec-
tion K, which is about 75% of the total added, above which it adds very little. Even
when the syntactic parser is trained on WSJ and the SRL is trained on WSJ, adding
7,500 instances of this new genre achieves almost the same performance as when all
three are from the same genre (67.2 vs. 69.9). For the task of argument identification, the
incremental addition of data from the new genre shows only minimal improvement.
The system that uses a self-trained syntactic parser performs slightly better than other
Table 14
Effect of incrementally adding data from a new genre.
Id. Id. + Class
Parser Train SRL Train P (%) R (%) F P (%) R (%) F
WSJ WSJ (14k) (Treebank parses)
(Treebank +0 examples from K 96.2 91.9 94.0 74.1 66.5 70.1
parses) +1,875 examples from K 96.1 92.9 94.5 77.6 71.3 74.3
+3,750 examples from K 96.3 94.2 95.1 79.1 74.1 76.5
+5,625 examples from K 96.4 94.8 95.6 80.4 76.1 78.1
+7,500 examples from K 96.4 95.2 95.8 80.2 76.1 78.1
Brown Brown (14k) (Treebank parses)
(Treebank +0 examples from K 96.1 94.2 95.1 77.1 73.0 75.0
parses) +1,875 examples from K 96.1 95.4 95.7 78.8 75.1 76.9
+3,750 examples from K 96.3 94.6 95.3 80.4 76.9 78.6
+5,625 examples from K 96.2 94.8 95.5 80.4 77.2 78.7
+7,500 examples from K 96.3 95.1 95.7 81.2 78.1 79.6
WSJ WSJ (14k)
(40k) +0 examples from K 83.1 78.8 80.9 65.2 55.7 60.1
+1,875 examples from K 83.4 79.3 81.3 68.9 57.5 62.7
+3,750 examples from K 83.9 79.1 81.4 71.8 59.3 64.9
+5,625 examples from K 84.5 79.5 81.9 74.3 61.3 67.2
+7,500 examples from K 84.8 79.4 82.0 74.8 61.0 67.2
WSJ Brown (14k)
(40k) +0 examples from K 85.7 77.2 81.2 74.4 57.0 64.5
+1,875 examples from K 85.7 77.6 81.4 75.1 58.7 65.9
+3,750 examples from K 85.6 78.1 81.7 76.1 59.6 66.9
+5,625 examples from K 85.7 78.5 81.9 76.9 60.5 67.7
+7,500 examples from K 85.9 78.1 81.7 76.8 59.8 67.2
Brown Brown (14k)
(20k) +0 examples from K 87.6 80.6 83.9 76.0 59.2 66.5
+1,875 examples from K 87.4 81.2 84.1 76.1 60.0 67.1
+3,750 examples from K 87.5 81.6 84.4 77.7 62.4 69.2
+5,625 examples from K 87.5 82.0 84.6 78.2 63.5 70.1
+7,500 examples from K 87.3 82.1 84.6 78.2 63.2 69.9
WSJ+NANC Brown (14k)
(2,500k) +0 examples from K 89.1 81.7 85.2 74.4 60.1 66.5
+1,875 examples from K 88.6 82.2 85.2 76.2 62.3 68.5
+3,750 examples from K 88.3 82.6 85.3 76.8 63.6 69.6
+5,625 examples from K 88.3 82.4 85.2 77.7 63.8 70.0
+7,500 examples from K 88.9 82.9 85.8 78.2 64.9 70.9
306
Pradhan, Ward, and Martin Towards Robust Semantic Role Labeling
versions that use automatically generated syntactic parses. The improvement on the
identification performance is almost exclusively due to recall. The precision numbers
are almost unaffected, except when the labeler is trained on WSJ PropBank data.
9. Conclusions
In this article, we have presented results from a state-of-the-art Semantic Role Labeling
system trained on PropBankWSJ data and then used to label test sets from both theWSJ
corpus and the Brown corpus. The system?s performance on the Brown test set exhibited
a large drop compared to the WSJ test set. An analysis of these results revealed that the
subtask of Identification, determining which constituents of a syntax tree are arguments
of a predicate, is responsible for only a relatively small part of the drop in performance.
The Classification task, assigning labels to constituents known to be arguments, is where
the major performance loss occurs.
Several possible factors were examined to determine their effect on this perfor-
mance difference:
 The syntactic parser was trained on WSJ. It was shown that errors in the
syntactic parse are not a large factor in the overall performance difference.
The syntactic parser does not show a large degradation in performance
when run on Brown. Even more telling, there is still a large drop in
performance when training and testing using Treebank parses.
When the system was trained and tested on Brown, the performance was still
significantly worse than training and testing on WSJ, even when the amount of training
data is controlled for. Training and testing on Brown showed performance intermediate
between training and testing on WSJ and training on WSJ and testing on Brown. This
leads to our final hypothesis.
 The Brown corpus is in some sense fundamentally more difficult for this
problem. The most obvious reason for this is that it represents a more
heterogeneous source than the WSJ. Among the likely manifestations of
this is that predicates tend to have a single dominating sense in WSJ and
are more polysemous in Brown. Data was presented using gold-standard
word sense information for the predicates for training and testing Brown.
Adding predicate sense information has a large effect for some predicates,
but over the whole Brown test set has only a small effect. Fewer predicates
and headwords could allow very specific modeling of high frequency
predicates, and predicate?headword relations do have a large effect on
overall performance.
The initial experiment is a case of training on homogeneous data and testing on
different data. The more homogeneous training data allows the system to rely heavily
on specific features and relations in the data. It is usually the case that training on a
more heterogeneous data set does not give quite as high performance on test data from
the same corpus as more homogeneous data, but the heterogeneous data ports better to
other corpora. This is seen when training on Brown compared to WSJ. The observation
that the Identification task ports well while the classification task does not is consistent
with this explanation. For the Identification task, structural features such as path and
307
Computational Linguistics Volume 34, Number 2
partial path tend to be the most salient while the Classification task relies more heavily
on lexical/semantic features such as specific predicate-head word combinations.
The question now is what to do about this. Two possibilities are:
 Less homogeneous corpora?Rather than using many examples drawn
from one source, fewer examples could be drawn from many sources. This
would reduce the likelihood of learning idiosyncratic senses and argument
structures for predicates.
 Less specific features?Features, and the values they take on, should be
designed to reduce the likelihood of learning idiosyncratic aspects of the
training domain. Examples of this might include the use of more general
named entity classes, and the use of abstractions over specific headwords
and predicates rather than the words themselves.
Both of these manipulations would, in all likelihood, reduce performance on both
the training data and on test sets of the same genre as the training data. But they
would be more likely to lead to better generalization across genres. Training on very
homogeneous training sets and testing on similar test sets gives amisleading impression
of the performance of a system.
Acknowledgments
We are extremely grateful to Martha Palmer
for providing us with the PropBanked
Brown corpus, and to David McClosky for
providing us with hypotheses on the Brown
test set as well as a cross-validated version
of the Brown training data for the various
models reported in his work reported at
HLT 2006.
This research was partially supported by
the ARDA AQUAINT program via contract
OCG4423B and by the NSF via grants
IS-9978025 and ITR/HCI 0086132. Computer
time was provided by NSF ARI Grant
CDA-9601817, NSF MRI Grant CNS-0420873,
NASA AIST grant NAG2-1646, DOE SciDAC
grant DE-FG02-04ER63870, NSF sponsorship
of the National Center for Atmospheric
Research, and a grant from the IBM Shared
University Research (SUR) program.
References
Bacchiani, Michiel, Michael Riley, Brian
Roark, and Richard Sproat. 2006. MAP
adaptation of stochastic grammars.
Computer Speech and Language, 20(1):41?68.
Bikel, Daniel M., Richard Schwartz, and
Ralph M. Weischedel. 1999. An algorithm
that learns what?s in a name.Machine
Learning, 34:211?231.
Burges, Christopher J. C. 1998. A tutorial
on support vector machines for pattern
recognition. Data Mining and Knowledge
Discovery, 2(2):121?167.
Carreras, Xavier and Llu??s Ma`rquez. 2004.
Introduction to the CoNLL-2004 shared
task: Semantic role labeling. In Proceedings
of the Eighth Conference on Computational
Natural Language Learning (CoNLL),
pages 89?97, Boston, MA.
Carreras, Xavier and Llu??s Ma`rquez. 2005.
Introduction to the CoNLL-2005 shared
task: Semantic role labeling. In Proceedings
of the Ninth Conference on Computational
Natural Language Learning (CoNLL),
pages 152?164, Ann Arbor, MI.
Charniak, Eugene and Mark Johnson. 2005.
Coarse-to-fine n-best parsing and maxent
discriminative reranking. In Proceedings of
the 43rd Annual Meeting of the Association
for Computational Linguistics (ACL),
Ann Arbor, MI.
Gildea, Daniel and Daniel Jurafsky. 2002.
Automatic labeling of semantic roles.
Computational Linguistics, 28(3):245?288.
Hacioglu, Kadri, Sameer Pradhan, Wayne
Ward, James Martin, and Daniel Jurafsky.
2004. Semantic role labeling by tagging
syntactic chunks. In Proceedings of the
Eighth Conference on Computational
Natural Language Learning (CoNLL),
Boston, MA.
Harabagiu, Sanda, Cosmin Adrian Bejan,
and Paul Morarescu. 2005. Shallow
semantics for relation extraction. In
Proceedings of the Nineteenth International
Joint Conference on Artificial Intelligence
(IJCAI), pages 1061?1067, Edinburgh,
Scotland.
308
Pradhan, Ward, and Martin Towards Robust Semantic Role Labeling
Hofmann, Thomas and Jan Puzicha. 1998.
Statistical models for co-occurrence
data. Memo, Massachusetts Institute
of Technology Artificial Intelligence
Laboratory, Cambridge, MA.
Jiang, Zheng Ping, Jia Li, and Hwee Tou Ng.
2005. Semantic argument classification
exploiting argument interdependence. In
Proceedings of the Nineteenth International
Joint Conference on Artificial Intelligence
(IJCAI), pages 1067?1072, Edinburgh,
Scotland.
Joachims, Thorsten. 1998. Text categorization
with support vector machines: Learning
with many relevant features. In Proceedings
of the European Conference on Machine
Learning (ECML), pages 137?142,
Chemnitz, Germany.
Koomen, Peter, Vasin Punyakanok, Dan
Roth, and Wen-tau Yih. 2005. Generalized
inference with multiple semantic role
labeling systems. In Proceedings of the
Ninth Conference on Computational Natural
Language Learning (CoNLL), pages 181?184,
Ann Arbor, MI.
Kuc?era, Henry and W. Nelson Francis. 1967.
Computational Analysis of Present-day
American English. Brown University Press,
Providence, RI.
Kudo, Taku and Yuji Matsumoto. 2000.
Use of support vector learning for chunk
identification. In Proceedings of the Fourth
Conference on Computational Natural
Language Learning (CoNLL), pages 142?144,
Lisbon, Portugal.
Kudo, Taku and Yuji Matsumoto. 2001.
Chunking with support vector machines.
In Proceedings of the Second Meeting of the
North American Chapter of the Association
for Computational Linguistics (NAACL),
Pittsburgh, PA.
Lin, Dekang. 1998. Automatic retrieval
and clustering of similar words. In
Proceedings of the Seventeenth International
Conference on Computational Linguistics
and Thirty Sixth Annual Meeting of the
Association of Computational Linguistics
(COLING/ACL), pages 768?774, Montreal,
Canada.
Lodhi, Huma, Craig Saunders, John
Shawe-Taylor, Nello Cristianini, and
Chris Watkins. 2002. Text classification
using string kernels. Journal of Machine
Learning Research, 2(Feb):419?444.
Marcus, Mitchell P., Beatrice Santorini, and
Mary Ann Marcinkiewicz. 1993. Building
a large annotated corpus of English: The
Penn treebank. Computational Linguistics,
19(2):313?330.
Ma`rquez, Llu??s, Mihai Surdeanu, Pere
Comas, and Jordi Turmo. 2005. A robust
combination strategy for semantic role
labeling. In Proceedings of the Human
Language Technology Conference and
Conference on Empirical Methods in Natural
Language Processing (HLT/EMNLP),
pages 644?651, Vancouver, British
Columbia.
McClosky, David, Eugene Charniak,
and Mark Johnson. 2006a. Effective
self-training for parsing. In Proceedings
of the Human Language Technology
Conference/North American Chapter
of the Association of Computational
Linguistics (HLT/NAACL), pages 152?159,
New York, NY.
McClosky, David, Eugene Charniak, and
Mark Johnson. 2006b. Reranking and
self-training for parser adaptation. In
Proceedings of the Twenty First International
Conference on Computational Linguistics
and Forty Fourth Annual Meeting of the
Association for Computational Linguistics
(COLING/ACL), pages 337?344, Sydney,
Australia.
Moschitti, Alessandro. 2006. Syntactic
kernels for natural language learning:
The semantic role labeling case. In
Proceedings of the Human Language
Technology Conference/North American
Chapter of the Association of Computational
Linguistics (HLT/NAACL), pages 97?100,
New York, NY.
Musillo, Gabriele and Paola Merlo. 2006.
Accurate parsing of the proposition bank.
In Proceedings of the Human Language
Technology Conference/North American
Chapter of the Association of Computational
Linguistics (HLT/NAACL), pages 101?104,
New York, NY.
Narayanan, Srini and Sanda Harabagiu.
2004. Question answering based on
semantic structures. In Proceedings of the
International Conference on Computational
Linguistics (COLING), pages 693?701,
Geneva, Switzerland.
Palmer, Martha, Daniel Gildea, and Paul
Kingsbury. 2005. The Proposition Bank:
An annotated corpus of semantic roles.
Computational Linguistics, 31(1):71?106.
Platt, John. 2000. Probabilities for support
vector machines. In A. Smola, P. Bartlett,
B. Scholkopf, and D. Schuurmans, editors,
Advances in Large Margin Classifiers. MIT
Press, Cambridge, MA, pages 61?74.
Pradhan, Sameer, Kadri Hacioglu, Valerie
Krugler, Wayne Ward, James Martin,
and Dan Jurafsky. 2005. Support vector
309
Computational Linguistics Volume 34, Number 2
learning for semantic argument classification.
Machine Learning Journal, 60(1):11?39.
Pradhan, Sameer, Wayne Ward, Kadri
Hacioglu, James Martin, and Dan Jurafsky.
2004. Shallow semantic parsing using
support vector machines. In Proceedings
of the Human Language Technology Conference/
North American Chapter of the Association of
Computational Linguistics (HLT/NAACL),
pages 233?240, Boston, MA.
Pradhan, Sameer, Wayne Ward, Kadri
Hacioglu, James Martin, and Dan Jurafsky.
2005. Semantic role labeling using different
syntactic views. In Proceedings of the
Forty-Third Annual Meeting of the
Association for Computational Linguistics
(ACL), pages 581?588, Ann Arbor, MI.
Punyakanok, Vasin, Dan Roth, Wen tau Yih,
and Dav Zimak. 2005. Learning and
inference over constrained output. In
Proceedings of the Nineteenth International
Joint Conference on Artificial Intelligence
(IJCAI), pages 1117?1123, Edinburgh,
Scotland.
Surdeanu, Mihai, Sanda Harabagiu,
John Williams, and Paul Aarseth. 2003.
Using predicate-argument structures for
information extraction. In Proceedings of the
Forty-First Annual Meeting of the Association
for Computational Linguistics (ACL),
pages 8?15, Sapporo, Japan.
Toutanova, Kristina, Aria Haghighi, and
Christopher Manning. 2005. Joint learning
improves semantic role labeling. In
Proceedings of the Forty-Third Annual
Meeting of the Association for Computational
Linguistics (ACL), pages 589?596,
Ann Arbor, MI.
Vapnik, Vladimir. 1998. Statistical Learning
Theory. Wiley, New York.
Xue, Nianwen and Martha Palmer. 2004.
Calibrating features for semantic role
labeling. In Proceedings of the Conference
on Empirical Methods in Natural Language
Processing (EMNLP), pages 88?94,
Barcelona, Spain.
Yi, Szu-ting and Martha Palmer. 2005. The
integration of syntactic parsing and
semantic role labeling. In Proceedings of the
Ninth Conference on Computational Natural
Language Learning (CoNLL), pages 237?240,
Ann Arbor, MI.
310






Shallow Semantic Parsing using Support Vector Machines?
Sameer Pradhan, Wayne Ward,
Kadri Hacioglu, James H. Martin
Center for Spoken Language Research,
University of Colorado, Boulder, CO 80303
{spradhan,whw,hacioglu,martin}@cslr.colorado.edu
Dan Jurafsky
Department of Linguistics
Stanford University
Stanford, CA 94305
jurafsky@stanford.edu
Abstract
In this paper, we propose a machine learning al-
gorithm for shallow semantic parsing, extend-
ing the work of Gildea and Jurafsky (2002),
Surdeanu et al (2003) and others. Our al-
gorithm is based on Support Vector Machines
which we show give an improvement in perfor-
mance over earlier classifiers. We show perfor-
mance improvements through a number of new
features and measure their ability to general-
ize to a new test set drawn from the AQUAINT
corpus.
1 Introduction
Automatic, accurate and wide-coverage techniques that
can annotate naturally occurring text with semantic argu-
ment structure can play a key role in NLP applications
such as Information Extraction, Question Answering and
Summarization. Shallow semantic parsing ? the process
of assigning a simple WHO did WHAT to WHOM, WHEN,
WHERE, WHY, HOW, etc. structure to sentences in text,
is the process of producing such a markup. When pre-
sented with a sentence, a parser should, for each predicate
in the sentence, identify and label the predicate?s seman-
tic arguments. This process entails identifying groups of
words in a sentence that represent these semantic argu-
ments and assigning specific labels to them.
In recent work, a number of researchers have cast this
problem as a tagging problem and have applied vari-
ous supervised machine learning techniques to it (Gildea
and Jurafsky (2000, 2002); Blaheta and Charniak (2000);
Gildea and Palmer (2002); Surdeanu et al (2003); Gildea
and Hockenmaier (2003); Chen and Rambow (2003);
Fleischman and Hovy (2003); Hacioglu and Ward (2003);
Thompson et al (2003); Pradhan et al (2003)). In this
?This research was partially supported by the ARDA
AQUAINT program via contract OCG4423B and by the NSF
via grant IS-9978025
paper, we report on a series of experiments exploring this
approach.
For the initial experiments, we adopted the approach
described by Gildea and Jurafsky (2002) (G&J) and eval-
uated a series of modifications to improve its perfor-
mance. In the experiments reported here, we first re-
placed their statistical classification algorithm with one
that uses Support Vector Machines and then added to the
existing feature set. We evaluate results using both hand-
corrected TreeBank syntactic parses, and actual parses
from the Charniak parser.
2 Semantic Annotation and Corpora
We will be reporting on results using PropBank1 (Kings-
bury et al, 2002), a 300k-word corpus in which predi-
cate argument relations are marked for part of the verbs
in the Wall Street Journal (WSJ) part of the Penn Tree-
Bank (Marcus et al, 1994). The arguments of a verb are
labeled ARG0 to ARG5, where ARG0 is the PROTO-
AGENT (usually the subject of a transitive verb) ARG1
is the PROTO-PATIENT (usually its direct object), etc.
PropBank attempts to treat semantically related verbs
consistently. In addition to these CORE ARGUMENTS,
additional ADJUNCTIVE ARGUMENTS, referred to as
ARGMs are also marked. Some examples are ARGM-
LOC, for locatives, and ARGM-TMP, for temporals. Fig-
ure 1 shows the syntax tree representation along with the
argument labels for an example structure extracted from
the PropBank corpus.
Most of the experiments in this paper, unless speci-
fied otherwise, are performed on the July 2002 release
of PropBank. A larger, cleaner, completely adjudicated
version of PropBank was made available in Feb 2004.
We will also report some final best performance numbers
on this corpus. PropBank was constructed by assigning
semantic arguments to constituents of the hand-corrected
TreeBank parses. The data comprise several sections of
the WSJ, and we follow the standard convention of using
1http://www.cis.upenn.edu/?ace/
Section-23 data as the test set. Section-02 to Section-
21 were used for training. In the July 2002 release, the
training set comprises about 51,000 sentences, instantiat-
ing about 132,000 arguments, and the test set comprises
2,700 sentences instantiating about 7,000 arguments. The
Feb 2004 release training set comprises about 85,000 sen-
tences instantiating about 250,000 arguments and the test
set comprises 5,000 sentences instantiating about 12,000
arguments.
[ARG0 He] [predicate talked] for [ARGM?TMP about
20 minutes].
S
hhhh
((((
NP
PRP
He
ARG0
VP
hhhh
((((
VBD
talked
predicate
PP
hhh
(((
IN
for
NULL
NP
hhhhh
(((((
about 20 minutes
ARGM ? TMP
Figure 1: Syntax tree for a sentence illustrating the Prop-
Bank tags.
3 Problem Description
The problem of shallow semantic parsing can be viewed
as three different tasks.
Argument Identification ? This is the process of identi-
fying parsed constituents in the sentence that represent
semantic arguments of a given predicate.
Argument Classification ? Given constituents known to
represent arguments of a predicate, assign the appropri-
ate argument labels to them.
Argument Identification and Classification ? A combina-
tion of the above two tasks.
Each node in the parse tree can be classified as either
one that represents a semantic argument (i.e., a NON-
NULL node) or one that does not represent any seman-
tic argument (i.e., a NULL node). The NON-NULL nodes
can then be further classified into the set of argument la-
bels. For example, in the tree of Figure 1, the node IN
that encompasses ?for? is a NULL node because it does
not correspond to a semantic argument. The node NP
that encompasses ?about 20 minutes? is a NON-NULL
node, since it does correspond to a semantic argument
? ARGM-TMP.
4 Baseline Features
Our baseline system uses the same set of features in-
troduced by G&J. Some of the features, viz., predicate,
voice and verb sub-categorization are shared by all the
nodes in the tree. All the others change with the con-
stituent under consideration.
? Predicate ? The predicate itself is used as a feature.
? Path ? The syntactic path through the parse tree
from the parse constituent to the predicate being
classified. For example, in Figure 1, the path from
ARG0 ? ?He? to the predicate talked, is represented
with the string NP?S?VP?VBD. ? and ? represent
upward and downward movement in the tree respec-
tively.
? Phrase Type ? This is the syntactic category (NP,
PP, S, etc.) of the phrase/constituent corresponding
to the semantic argument.
? Position ? This is a binary feature identifying
whether the phrase is before or after the predicate.
? Voice ? Whether the predicate is realized as an ac-
tive or passive construction.
? Head Word ? The syntactic head of the phrase. This
is calculated using a head word table described by
(Magerman, 1994) and modified by (Collins, 1999,
Appendix. A).
? Sub-categorization ? This is the phrase struc-
ture rule expanding the predicate?s parent node
in the parse tree. For example, in Figure 1, the
sub-categorization for the predicate talked is
VP?VBD-PP.
5 Classifier and Implementation
We formulate the parsing problem as a multi-class clas-
sification problem and use a Support Vector Machine
(SVM) classifier (Hacioglu et al, 2003; Pradhan et al
2003). Since SVMs are binary classifiers, we have to con-
vert the multi-class problem into a number of binary-class
problems. We use the ONE vs ALL (OVA) formalism,
which involves training n binary classifiers for a n-class
problem.
Since the training time taken by SVMs scales exponen-
tially with the number of examples, and about 80% of the
nodes in a syntactic tree have NULL argument labels, we
found it efficient to divide the training process into two
stages, while maintaining the same accuracy:
1. Filter out the nodes that have a very high probabil-
ity of being NULL. A binary NULL vs NON-NULL
classifier is trained on the entire dataset. A sigmoid
function is fitted to the raw scores to convert the
scores to probabilities as described by (Platt, 2000).
2. The remaining training data is used to train OVA
classifiers, one of which is the NULL-NON-NULL
classifier.
With this strategy only one classifier (NULL vs NON-
NULL) has to be trained on all of the data. The remaining
OVA classifiers are trained on the nodes passed by the
filter (approximately 20% of the total), resulting in a con-
siderable savings in training time.
In the testing stage, we do not perform any filtering
of NULL nodes. All the nodes are classified directly
as NULL or one of the arguments using the classifier
trained in step 2 above. We observe no significant per-
formance improvement even if we filter the most likely
NULL nodes in a first pass.
For our experiments, we used TinySVM2 along with
YamCha3 (Kudo and Matsumoto, 2000)
(Kudo and Matsumoto, 2001) as the SVM training and
test software. The system uses a polynomial kernel with
degree 2; the cost per unit violation of the margin, C=1;
and, tolerance of the termination criterion, e=0.001.
6 Baseline System Performance
Table 1 shows the baseline performance numbers on the
three tasks mentioned earlier; these results are based on
syntactic features computed from hand-corrected Tree-
Bank (hence LDC hand-corrected) parses.
For the argument identification and the combined iden-
tification and classification tasks, we report the precision
(P), recall (R) and the F14 scores, and for the argument
classification task we report the classification accuracy
(A). This test set and all test sets, unless noted otherwise
are Section-23 of PropBank.
Classes Task P R F1 A
(%) (%) (%)
ALL Id. 90.9 89.8 90.4
ARGs Classification - - - 87.9
Id. + Classification 83.3 78.5 80.8
CORE Id. 94.7 90.1 92.3
ARGs Classification - - - 91.4
Id. + Classification 88.4 84.1 86.2
Table 1: Baseline performance on all three tasks using
hand-corrected parses.
7 System Improvements
7.1 Disallowing Overlaps
The system as described above might label two con-
stituents NON-NULL even if they overlap in words. This
is a problem since overlapping arguments are not allowed
in PropBank. Among the overlapping constituents we re-
tain the one for which the SVM has the highest confi-
dence, and label the others NULL. The probabilities ob-
tained by applying the sigmoid function to the raw SVM
scores are used as the measure of confidence. Table 2
shows the performance of the parser on the task of iden-
tifying and labeling semantic arguments using the hand-
corrected parses. On all the system improvements, we
perform a ?2 test of significance at p = 0.05, and all the
2http://cl.aist-nara.ac.jp/?talus-Au/software/TinySVM/
3http://cl.aist-nara.ac.jp/?taku-Au/software/yamcha/
4F1 = 2PRP+R
significant improvements are marked with an ?. In this
system, the overlap-removal decisions are taken indepen-
dently of each other.
P R F1
(%) (%)
Baseline 83.3 78.5 80.8
No Overlaps 85.4 78.1 ?81.6
Table 2: Improvements on the task of argument identi-
fication and classification after disallowing overlapping
constituents.
7.2 New Features
We tested several new features. Two were obtained from
the literature ? named entities in constituents and head
word part of speech. Other are novel features.
1. Named Entities in Constituents ? Following
Surdeanu et al (2003), we tagged 7 named en-
tities (PERSON, ORGANIZATION, LOCATION,
PERCENT, MONEY, TIME, DATE) using Identi-
Finder (Bikel et al, 1999) and added them as 7
binary features.
2. Head Word POS ? Surdeanu et al (2003) showed
that using the part of speech (POS) of the head word
gave a significant performance boost to their system.
Following that, we experimented with the addition
of this feature to our system.
3. Verb Clustering ? Since our training data is rel-
atively limited, any real world test set will con-
tain predicates that have not been seen in training.
In these cases, we can benefit from some informa-
tion about the predicate by using predicate clus-
ter as a feature. The verbs were clustered into 64
classes using the probabilistic co-occurrence model
of Hofmann and Puzicha (1998). The clustering al-
gorithm uses a database of verb-direct-object rela-
tions extracted by Lin (1998). We then use the verb
class of the current predicate as a feature.
4. Partial Path ? For the argument identification task,
path is the most salient feature. However, it is also
the most data sparse feature. To overcome this prob-
lem, we tried generalizing the path by adding a new
feature that contains only the part of the path from
the constituent to the lowest common ancestor of the
predicate and the constituent, which we call ?Partial-
Path?.
5. Verb Sense Information ? The arguments that a
predicate can take depend on the word sense of the
predicate. Each predicate tagged in the PropBank
corpus is assigned a separate set of arguments de-
pending on the sense in which it is used. Table 3
illustrates the argument sets for the predicate talk.
Depending on the sense of the predicate talk, either
ARG1 or ARG2 can identify the hearer. Absence of
this information can be potentially confusing to the
learning mechanism.
Talk sense 1: speak sense 2: persuade/dissuade
Tag Description Tag Description
ARG0 Talker ARG0 Talker
ARG1 Subject ARG1 Talked to
ARG2 Hearer ARG2 Secondary action
Table 3: Argument labels associated with the two senses
of predicate talk in PropBank corpus.
We added the oracle sense information extracted
from PropBank, to our features by treating each
sense of a predicate as a distinct predicate.
6. Head Word of Prepositional Phrases ? Many ad-
junctive arguments, such as temporals and locatives,
occur as prepositional phrases in a sentence, and
it is often the case that the head words of those
phrases, which are always prepositions, are not very
discriminative, eg., ?in the city?, ?in a few minutes?,
both share the same head word ?in? and neither
contain a named entity, but the former is ARGM-
LOC, whereas the latter is ARGM-TMP. Therefore,
we tried replacing the head word of a prepositional
phrase, with that of the first noun phrase inside the
prepositional phrase. We retained the preposition in-
formation by appending it to the phrase type, eg.,
?PP-in? instead of ?PP?.
7. First and Last Word/POS in Constituent ? Some
arguments tend to contain discriminative first and
last words so we tried using them along with their
part of speech as four new features.
8. Ordinal constituent position ? In order to avoid
false positives of the type where constituents far
away from the predicate are spuriously identified as
arguments, we added this feature which is a concate-
nation of the constituent type and its ordinal position
from the predicate.
9. Constituent tree distance ? This is a finer way of
specifying the already present position feature.
10. Constituent relative features ? These are nine fea-
tures representing the phrase type, head word and
head word part of speech of the parent, and left and
right siblings of the constituent in focus. These were
added on the intuition that encoding the tree context
this way might add robustness and improve general-
ization.
11. Temporal cue words ? There are several temporal
cue words that are not captured by the named entity
tagger and were considered for addition as a binary
feature indicating their presence.
12. Dynamic class context ? In the task of argument
classification, these are dynamic features that repre-
sent the hypotheses of at most previous two nodes
belonging to the same tree as the node being classi-
fied.
8 Feature Performance
Table 4 shows the effect each feature has on the ar-
gument classification and argument identification tasks,
when added individually to the baseline. Addition of
named entities improves the F1 score for adjunctive ar-
guments ARGM-LOC from 59% to ?68% and ARGM-
TMP from 78.8% to ?83.4%. But, since these arguments
are small in number compared to the core arguments, the
overall accuracy does not show a significant improve-
ment. We found that adding this feature to the NULL vs
NON-NULL classifier degraded its performance. It also
shows the contribution of replacing the head word and the
head word POS separately in the feature where the head
of a prepositional phrase is replaced by the head word
of the noun phrase inside it. Apparently, a combination
of relative features seem to have a significant improve-
ment on either or both the classification and identification
tasks, and so do the first and last words in the constituent.
Features Class ARGUMENT ID
Acc.
P R F1
Baseline 87.9 93.7 88.9 91.3
+ Named entities 88.1 - - -
+ Head POS ?88.6 94.4 90.1 ?92.2
+ Verb cluster 88.1 94.1 89.0 91.5
+ Partial path 88.2 93.3 88.9 91.1
+ Verb sense 88.1 93.7 89.5 91.5
+ Noun head PP (only POS) ?88.6 94.4 90.0 ?92.2
+ Noun head PP (only head) ?89.8 94.0 89.4 91.7
+ Noun head PP (both) ?89.9 94.7 90.5 ?92.6
+ First word in constituent ?89.0 94.4 91.1 ?92.7
+ Last word in constituent ?89.4 93.8 89.4 91.6
+ First POS in constituent 88.4 94.4 90.6 ?92.5
+ Last POS in constituent 88.3 93.6 89.1 91.3
+ Ordinal const. pos. concat. 87.7 93.7 89.2 91.4
+ Const. tree distance 88.0 93.7 89.5 91.5
+ Parent constituent 87.9 94.2 90.2 ?92.2
+ Parent head 85.8 94.2 90.5 ?92.3
+ Parent head POS ?88.5 94.3 90.3 ?92.3
+ Right sibling constituent 87.9 94.0 89.9 91.9
+ Right sibling head 87.9 94.4 89.9 ?92.1
+ Right sibling head POS 88.1 94.1 89.9 92.0
+ Left sibling constituent ?88.6 93.6 89.6 91.6
+ Left sibling head 86.9 93.9 86.1 89.9
+ Left sibling head POS ?88.8 93.5 89.3 91.4
+ Temporal cue words ?88.6 - - -
+ Dynamic class context 88.4 - - -
Table 4: Effect of each feature on the argument identifi-
cation and classification tasks when added to the baseline
system.
We tried two other ways of generalizing the head word:
i) adding the head word cluster as a feature, and ii) replac-
ing the head word with a named entity if it belonged to
any of the seven named entities mentioned earlier. Nei-
ther method showed any improvement. We also tried gen-
eralizing the path feature by i) compressing sequences of
identical labels, and ii) removing the direction in the path,
but none showed any improvement on the baseline.
8.1 Argument Sequence Information
In order to improve the performance of their statistical ar-
gument tagger, G&J used the fact that a predicate is likely
to instantiate a certain set of arguments. We use a similar
strategy, with some additional constraints: i) argument
ordering information is retained, and ii) the predicate is
considered as an argument and is part of the sequence.
We achieve this by training a trigram language model on
the argument sequences, so unlike G&J, we can also es-
timate the probability of argument sets not seen in the
training data. We first convert the raw SVM scores to
probabilities using a sigmoid function. Then, for each
sentence being parsed, we generate an argument lattice
using the n-best hypotheses for each node in the syn-
tax tree. We then perform a Viterbi search through the
lattice using the probabilities assigned by the sigmoid
as the observation probabilities, along with the language
model probabilities, to find the maximum likelihood path
through the lattice, such that each node is either assigned
a value belonging to the PROPBANK ARGUMENTs, or
NULL.
CORE ARGs/ P R F1
Hand-corrected parses (%) (%)
Baseline w/o overlaps 90.0 86.1 88.0
Common predicate 90.8 86.3 88.5
Specific predicate lemma 90.5 87.4 ?88.9
Table 5: Improvements on the task of argument identifi-
cation and tagging after performing a search through the
argument lattice.
The search is constrained in such a way that no two
NON-NULL nodes overlap with each other. To simplify
the search, we allowed only NULL assignments to nodes
having a NULL likelihood above a threshold. While train-
ing the language model, we can either use the actual pred-
icate to estimate the transition probabilities in and out
of the predicate, or we can perform a joint estimation
over all the predicates. We implemented both cases con-
sidering two best hypotheses, which always includes a
NULL (we add NULL to the list if it is not among the
top two). On performing the search, we found that the
overall performance improvement was not much differ-
ent than that obtained by resolving overlaps as mentioned
earlier. However, we found that there was an improve-
ment in the CORE ARGUMENT accuracy on the combined
task of identifying and assigning semantic arguments,
given hand-corrected parses, whereas the accuracy of the
ADJUNCTIVE ARGUMENTS slightly deteriorated. This
seems to be logical considering the fact that the ADJUNC-
TIVE ARGUMENTS are not linguistically constrained in
any way as to their position in the sequence of argu-
ments, or even the quantity. We therefore decided to
use this strategy only for the CORE ARGUMENTS. Al-
though, there was an increase in F1 score when the lan-
guage model probabilities were jointly estimated over all
the predicates, this improvement is not statistically signif-
icant. However, estimating the same using specific predi-
cate lemmas, showed a significant improvement in accu-
racy. The performance improvement is shown in Table 5.
9 Best System Performance
The best system is trained by first filtering the most
likely nulls using the best NULL vs NON-NULL classi-
fier trained using all the features whose argument identi-
fication F1 score is marked in bold in Table 4, and then
training a ONE vs ALL classifier using the data remain-
ing after performing the filtering and using the features
that contribute positively to the classification task ? ones
whose accuracies are marked in bold in Table 4. Table 6
shows the performance of this system.
Classes Task Hand-corrected parses
P R F1 A
(%) (%) (%)
ALL Id. 95.2 92.5 93.8
ARGs Classification - - - 91.0
Id. + Classification 88.9 84.6 86.7
CORE Id. 96.2 93.0 94.6
ARGs Classification - - - 93.9
Id. + Classification 90.5 87.4 88.9
Table 6: Best system performance on all tasks using
hand-corrected parses.
10 Using Automatic Parses
Thus far, we have reported results using hand-corrected
parses. In real-word applications, the system will have
to extract features from an automatically generated
parse. To evaluate this scenario, we used the Charniak
parser (Chaniak, 2001) to generate parses for PropBank
training and test data. We lemmatized the predicate using
the XTAG morphology database5 (Daniel et al, 1992).
Table 7 shows the performance degradation when
automatically generated parses are used.
11 Using Latest PropBank Data
Owing to the Feb 2004 release of much more and com-
pletely adjudicated PropBank data, we have a chance to
5ftp://ftp.cis.upenn.edu/pub/xtag/morph-1.5/morph-
1.5.tar.gz
Classes Task Automatic parses
P R F1 A
(%) (%) (%)
ALL Id. 89.3 82.9 86.0
ARGs Classification - - - 90.0
Id. + Classification 84.0 75.3 79.4
CORE Id. 92.0 83.3 87.4
ARGs Classification - - - 90.5
Id. + Classification 86.4 78.4 82.2
Table 7: Performance degradation when using automatic
parses instead of hand-corrected ones.
report our performance numbers on this data set. Table 8
shows the same information as in previous Tables 6 and
7, but generated using the new data. Owing to time limi-
tations, we could not get the results on the argument iden-
tification task and the combined argument identification
and classification task using automatic parses.
ALL ARGs Task P R F1 A
(%) (%) (%)
HAND Id. 96.2 95.8 96.0
Classification - - - 93.0
Id. + Classification 89.9 89.0 89.4
AUTOMATIC Classification - - - 90.1
Table 8: Best system performance on all tasks using
hand-corrected parses using the latest PropBank data.
12 Feature Analysis
In analyzing the performance of the system, it is useful
to estimate the relative contribution of the various feature
sets used. Table 9 shows the argument classification ac-
curacies for combinations of features on the training and
test data, using hand-corrected parses, for all PropBank
arguments.
Features Accuracy
(%)
All 91.0
All except Path 90.8
All except Phrase Type 90.8
All except HW and HW -POS 90.7
All except All Phrases ?83.6
All except Predicate ?82.4
All except HW and FW and LW -POS ?75.1
Path, Predicate 74.4
Path, Phrase Type 47.2
Head Word 37.7
Path 28.0
Table 9: Performance of various feature combinations on
the task of argument classification.
In the upper part of Table 9 we see the degradation in
performance by leaving out one feature or a feature fam-
ily at a time. After the addition of all the new features,
it is the case that removal of no individual feature except
predicate degrades the classification performance signifi-
cantly, as there are some other features that provide com-
plimentary information. However, removal of predicate
information hurts performance significantly, so does the
removal of a family of features, eg., all phrase types, or
the head word (HW), first word (FW) and last word (LW)
information. The lower part of the table shows the per-
formance of some feature combinations by themselves.
Table 10 shows the feature salience on the task of ar-
gument identification. One important observation we can
make here is that the path feature is the most salient fea-
ture in the task of argument identification, whereas it is
the least salient in the task of argument classification. We
could not provide the numbers for argument identifica-
tion performance upon removal of the path feature since
that made the SVM training prohibitively slow, indicating
that the SVM had a very hard time separating the NULL
class from the NON-NULL class.
Features P R F1
(%) (%)
All 95.2 92.5 93.8
All except HW 95.1 92.3 93.7
All except Predicate 94.5 91.9 93.2
Table 10: Performance of various feature combinations
on the task of argument identification
13 Comparing Performance with Other
Systems
We compare our system against 4 other shallow semantic
parsers in the literature. In comparing systems, results are
reported for all the three types of tasks mentioned earlier.
13.1 Description of the Systems
The Gildea and Palmer (G&P) System.
The Gildea and Palmer (2002) system uses the same
features and the same classification mechanism used by
G&J. These results are reported on the December 2001
release of PropBank.
The Surdeanu et al System.
Surdeanu et al (2003) report results on two systems
using a decision tree classifier. One that uses exactly the
same features as the G&J system. We call this ?Surdeanu
System I.? They then show improved performance of an-
other system ? ?Surdeanu System II,? which uses some
additional features. These results are are reported on the
July 2002 release of PropBank.
The Gildea and Hockenmaier (G&H) System
The Gildea and Hockenmaier (2003) system uses fea-
tures extracted from Combinatory Categorial Grammar
(CCG) corresponding to the features that were used by
G&J and G&P systems. CCG is a form of dependency
grammar and is hoped to capture long distance relation-
ships better than a phrase structure grammar. The fea-
tures are combined using the same algorithm as in G&J
and G&P. They use a slightly newer ? November 2002 re-
lease of PropBank. We will refer to this as ?G&H System
I?.
The Chen and Rambow (C&R) System
Chen and Rambow report on two different systems,
also using a decision tree classifier. The first ?C&R Sys-
tem I? uses surface syntactic features much like the G&P
system. The second ?C&R System II? uses additional
syntactic and semantic representations that are extracted
from a Tree Adjoining Grammar (TAG) ? another gram-
mar formalism that better captures the syntactic proper-
ties of natural languages.
Classifier Accuracy
(%)
SVM 88
Decision Tree (Surdeanu et al, 2003) 79
Gildea and Palmer (2002) 77
Table 11: Argument classification using same features
but different classifiers.
13.2 Comparing Classifiers
Since two systems, in addition to ours, report results us-
ing the same set of features on the same data, we can
directly assess the influence of the classifiers. G&P sys-
tem estimates the posterior probabilities using several dif-
ferent feature sets and interpolate the estimates, while
Surdeanu et al (2003) use a decision tree classifier. Ta-
ble 11 shows a comparison between the three systems for
the task of argument classification.
13.3 Argument Identification (NULL vs NON-NULL)
Table 12 compares the results of the task of identify-
ing the parse constituents that represent semantic argu-
ments. As expected, the performance degrades consider-
ably when we extract features from an automatic parse as
opposed to a hand-corrected parse. This indicates that the
syntactic parser performance directly influences the argu-
ment boundary identification performance. This could be
attributed to the fact that the two features, viz., Path and
Head Word that have been seen to be good discriminators
of the semantically salient nodes in the syntax tree, are
derived from the syntax tree.
Classes System Hand Automatic
P R F1 P R F1
ALL SVM 95 92 94 89 83 86
ARGs Surdeanu System II - - 89 - - -
Surdeanu System I 85 84 85 - - -
Table 12: Argument identification
13.4 Argument Classification
Table 13 compares the argument classification accuracies
of various systems, and at various levels of classification
granularity, and parse accuracy. It can be seen that the
SVM System performs significantly better than all the
other systems on all PropBank arguments.
Classes System Hand Automatic
Accuracy Accuracy
ALL SVM 91 90
ARGs G&P 77 74
Surdeanu System II 84 -
Surdeanu System I 79 -
CORE SVM 93.9 90.5
ARGs C&R System II 93.5 -
C&R System I 92.4 -
Table 13: Argument classification
13.5 Argument Identification and Classification
Table 14 shows the results for the task where the system
first identifies candidate argument boundaries and then
labels them with the most likely argument. This is the
hardest of the three tasks outlined earlier. SVM does a
very good job of generalizing in both stages of process-
ing.
Classes System Hand Automatic
P R F1 P R F1
ALL SVM 89 85 87 84 75 79
ARGs G&H System I 76 68 72 71 63 67
G&P 71 64 67 58 50 54
CORE SVM System 90 87 89 86 78 82
ARGs G&H System I 82 79 80 76 73 75
C&R System II - - - 65 75 70
Table 14: Identification and classification
14 Generalization to a New Text Source
Thus far, in all experiments our unseen test data was
selected from the same source as the training data.
In order to see how well the features generalize to
texts drawn from a similar source, we used the classifier
trained on PropBank training data to test data drawn from
the AQUAINT corpus (LDC, 2002). We annotated 400
sentences from the AQUAINT corpus with PropBank
arguments. This is a collection of text from the New
York Times Inc., Associated Press Inc., and Xinhua
News Service (PropBank by comparison is drawn from
Wall Street Journal). The results are shown in Table 15.
Task P R F1 A
(%) (%) (%)
ALL Id. 75.8 71.4 73.5 -
ARGs Classification - - - 83.8
Id. + Classification 65.2 61.5 63.3 -
CORE Id. 88.4 74.4 80.8 -
ARGs Classification - - - 84.0
Id. + Classification 75.2 63.3 68.7 -
Table 15: Performance on the AQUAINT test set.
There is a significant drop in the precision and recall
numbers for the AQUAINT test set (compared to the pre-
cision and recall numbers for the PropBank test set which
were 84% and 75% respectively). One possible reason
for the drop in performance is relative coverage of the
features on the two test sets. The head word, path and
predicate features all have a large number of possible val-
ues and could contribute to lower coverage when moving
from one domain to another. Also, being more specific
they might not transfer well across domains.
Features Arguments non-Arguments
(%) (%)
Predicate, Path 87.60 2.91
Predicate, Head Word 48.90 26.55
Cluster, Path 96.31 4.99
Cluster, Head Word 83.85 60.14
Path 99.13 15.15
Head Word 93.02 90.59
Table 16: Feature Coverage on PropBank test set using
parser trained on PropBank training set.
Features Arguments non-Arguments
(%) (%)
Predicate, Path 62.11 4.66
Predicate, Head Word 30.26 17.41
Cluster, Path 87.19 10.68
Cluster, Head Word 65.82 45.43
Path 96.50 29.26
Head Word 84.65 83.54
Table 17: Coverage of features on AQUAINT test set us-
ing parser trained on PropBank training set.
Table 16 shows the coverage for features on the hand-
corrected PropBank test set. The tables show feature
coverage for constituents that were Arguments and con-
stituents that were NULL. About 99% of the predicates in
the AQUAINT test set were seen in the PropBank train-
ing set. Table 17 shows coverage for the same features on
the AQUAINT test set. We believe that the drop in cover-
age of the more predictive feature combinations explains
part of the drop in performance.
15 Conclusions
We have described an algorithm which significantly im-
proves the state-of-the-art in shallow semantic parsing.
Like previous work, our parser is based on a supervised
machine learning approach. Key aspects of our results
include significant improvement via an SVM classifier,
improvement from new features and a series of analytic
experiments on the contributions of the features. Adding
features that are generalizations of the more specific fea-
tures seemed to help. These features were named enti-
ties, head word part of speech and verb clusters. We also
analyzed the transferability of the features to a new text
source.
We would like to thank Ralph Weischedel and Scott Miller of
BBN Inc. for letting us use their named entity tagger ? Iden-
tiFinder; Martha Palmer for providing us with the PropBank
data, Valerie Krugler for tagging the AQUAINT test set with
PropBank arguments, and all the anonymous reviewers for their
helpful comments.
References
[Bikel et al1999] Daniel M. Bikel, Richard Schwartz, and Ralph M. Weischedel.
1999. An algorithm that learns what?s in a name. Machine Learning, 34:211?
231.
[Blaheta and Charniak2000] Don Blaheta and Eugene Charniak. 2000. Assigning
function tags to parsed text. In NAACL, pages 234?240.
[Chaniak2001] Eugene Chaniak. 2001. Immediate-head parsing for language
models. In ACL-01.
[Chen and Rambow2003] John Chen and Owen Rambow. 2003. Use of deep
linguistics features for the recognition and labeling of semantic arguments.
EMNLP-03.
[Collins1999] Michael John Collins. 1999. Head-driven Statistical Models
for Natural Language Parsing. Ph.D. thesis, University of Pennsylvania,
Philadelphia.
[Daniel et al1992] K. Daniel, Y. Schabes, M. Zaidel, and D. Egedi. 1992. A freely
available wide coverage morphological analyzer for English. In COLING-92.
[Fleischman and Hovy2003] Michael Fleischman and Eduard Hovy. 2003. A
maximum entropy approach to framenet tagging. In HLT-03.
[Gildea and Hockenmaier2003] Dan Gildea and Julia Hockenmaier. 2003. Identi-
fying semantic roles using combinatory categorial grammar. In EMNLP-03.
[Gildea and Jurafsky2000] Daniel Gildea and Daniel Jurafsky. 2000. Automatic
labeling of semantic roles. In ACL-00, pages 512?520.
[Gildea and Jurafsky2002] Daniel Gildea and Daniel Jurafsky. 2002. Automatic
labeling of semantic roles. Computational Linguistics, 28(3):245?288.
[Gildea and Palmer2002] Daniel Gildea and Martha Palmer. 2002. The necessity
of syntactic parsing for predicate argument recognition. In ACL-02.
[Hacioglu and Ward2003] Kadri Hacioglu and Wayne Ward. 2003. Target word
detection and semantic role chunking using support vector machines. In HLT-
03.
[Hacioglu et al2003] Kadri Hacioglu, Sameer Pradhan, Wayne Ward, James Mar-
tin, and Dan Jurafsky. 2003. Shallow semantic parsing using support vector
machines. Technical Report TR-CSLR-2003-1, Center for Spoken Language
Research, Boulder, Colorado.
[Hofmann and Puzicha1998] Thomas Hofmann and Jan Puzicha. 1998. Statistical
models for co-occurrence data. Memo, MIT AI Laboratory.
[Kingsbury et al2002] Paul Kingsbury, Martha Palmer, and Mitch Marcus. 2002.
Adding semantic annotation to the Penn Treebank. In HLT-02.
[Kudo and Matsumoto2000] Taku Kudo and Yuji Matsumoto. 2000. Use of sup-
port vector learning for chunk identification. In CoNLL-00.
[Kudo and Matsumoto2001] Taku Kudo and Yuji Matsumoto. 2001. Chunking
with support vector machines. In NAACL-01.
[LDC2002] LDC. 2002. The AQUAINT Corpus of English News Text, Catalog
no. LDC2002t31.
[Lin1998] Dekang Lin. 1998. Automatic retrieval and clustering of similar words.
In COLING-98.
[Magerman1994] David Magerman. 1994. Natural Language Parsing as Statisti-
cal Pattern Recognition. Ph.D. thesis, Stanford University, CA.
[Marcus et al1994] Mitchell Marcus, Grace Kim, Mary Ann Marcinkiewicz,
Robert MacIntyre, Ann Bies, Mark Ferguson, Karen Katz, and Britta Schas-
berger. 1994. The Penn TreeBank: Annotating predicate argument structure.
[Platt2000] John Platt. 2000. Probabilities for support vector machines. In
A. Smola, P. Bartlett, B. Scolkopf, and D. Schuurmans, editors, Advances in
Large Margin Classifiers. MIT press.
[Pradhan et al2003] Sameer Pradhan, Kadri Hacioglu, Wayne Ward, James Mar-
tin, and Dan Jurafsky. 2003. Semantic role parsing: Adding semantic struc-
ture to unstructured text. In ICDM-03.
[Surdeanu et al2003] Mihai Surdeanu, Sanda Harabagiu, John Williams, and Paul
Aarseth. 2003. Using predicate-argument structures for information extrac-
tion. In ACL-03.
[Thompson et al2003] Cynthia A. Thompson, Roger Levy, and Christopher D.
Manning. 2003. A generative model for semantic role labeling. In ECML-03.
Parsing Arguments of Nominalizations in English and Chinese?
Sameer Pradhan, Honglin Sun,
Wayne Ward, James H. Martin
Center for Spoken Language Research,
University of Colorado, Boulder, CO 80303
{spradhan,sunh,whw,martin}@cslr.colorado.edu
Dan Jurafsky
Department of Linguistics
Stanford University
Stanford, CA 94305
jurafsky@stanford.edu
Abstract
In this paper, we use a machine learning frame-
work for semantic argument parsing, and apply
it to the task of parsing arguments of eventive
nominalizations in the FrameNet database. We
create a baseline system using a subset of fea-
tures introduced by Gildea and Jurafsky (2002),
which are directly applicable to nominal pred-
icates. We then investigate new features which
are designed to capture the novelties in nom-
inal argument structure and show a significant
performance improvement using these new fea-
tures. We also investigate the parsing perfor-
mance of nominalizations in Chinese and com-
pare the salience of the features for the two lan-
guages.
1 Introduction
The field of NLP had seen a resurgence of research in
shallow semantic analysis. The bulk of this recent work
views semantic analysis as a tagging, or labeling prob-
lem, and has applied various supervised machine learn-
ing techniques to it (Gildea and Jurafsky (2000, 2002);
Gildea and Palmer (2002); Surdeanu et al (2003); Ha-
cioglu and Ward (2003); Thompson et al (2003); Prad-
han et al (2003)). Note that, while all of these systems
are limited to the analysis of verbal predicates, many un-
derlying semantic relations are expressed via nouns, ad-
jectives, and prepositions. This paper presents a prelimi-
nary investigation into the semantic parsing of eventive
nominalizations (Grimshaw, 1990) in English and Chi-
nese.
2 Semantic Annotation and Corpora
For our experiments, we use the FrameNet database
(Baker et al, 1998) which contains frame-specific se-
?This research was partially supported by the ARDA
AQUAINT program via contract OCG4423B and by the NSF
via grant IS-9978025
mantic annotation of a number of predicates in English.
Predicates are grouped by the semantic frame that they
instantiate, depending on the sense of their usage, and
their arguments assume one of the frame elements or
roles specific to that frame. The predicate can be a verb,
noun, adjective, prepositional phrase, etc. FrameNet
contains about 500 different frame types and about 700
distinct frame elements. The following example illus-
trates the general idea. Here, the predicate ?complain?
instantiates a ?Statement? frame once as a nominal
predicate and once as a verbal predicate.
Did [Speaker she] make an official [Predicate:nominal com-
plaint] [Addressee to you] [Topic about the attack.]
[Message?Justice has not been done?] [Speaker he]
[Predicate:verbal complained.]
Nominal predicates in FrameNet include ultra-nominals
(Barker and Dowty, 1992), nominals and nominal-
izations. For the purposes of this study, a human analyst
went through the nominal predicates in FrameNet and
selected those that were identified as nominalizations
in NOMLEX (Macleod et al, 1998). Out of those,
the analyst then selected ones that were eventive
nominalizations.
These data comprise 7,333 annotated sentences, with
11,284 roles. There are 105 frames with about 190 dis-
tinct frame role1 types. A stratified sampling over predi-
cates was performed to select 80% of this data for train-
ing, 10% for development and another 10% for testing.
For the Chinese semantic parsing experiments, we se-
lected 22 nominalizations from the Penn Chinese Tree-
bank and tagged all the sentences containing these predi-
cates with PropBank (Kingsbury and Palmer, 2002) style
arguments ? ARG0, ARG1, etc. These consisted of 630
sentences. These are then split into two parts: 503 (80%)
for training and 127 (20%) for testing.
1We will use the terms role and arguments interchangeably
3 Baseline System
The primary assumption in our system is that a seman-
tic argument aligns with some syntactic constituent. The
goal is to identify and label constituents in a syntactic
tree that represent valid semantic arguments of a given
predicate. Unlike PropBank, there are no hand-corrected
parses available for the sentences in FrameNet, so we
cannot quantify the possible mis-alignment of the nomi-
nal arguments with syntactic constituents. The arguments
that do not align with any constituent are simply missed
by the current system.
3.1 Features We created a baseline system using
all and only those features introduced by Gildea and
Jurafsky that are directly applicable to nominal pred-
icates. Most of the features are extracted from the
syntactic parse of a sentence. We used the Charniak
parser (Chaniak, 2001) to parse the sentences in order to
perform feature extraction. The features are listed below:
Predicate ? The predicate lemma is used as a feature.
Path ? The syntactic path through the parse tree from the
parse constituent being classified to the predicate.
Constituent type ? This is the syntactic category (NP, PP,
S, etc.) of the constituent corresponding to the semantic
argument.
Position ? This is a binary feature identifying whether
the constituent is before or after the predicate.
Head word ? The syntactic head of the constituent.
3.2 Classifier and Implementation We formulate the
parsing problem as a multi-class classification problem
and use a Support Vector Machine (SVM) classifier in the
ONE vs ALL (OVA) formalism, which involves training
n classifiers for a n-class problem ? including the NULL
class. We use TinySVM2 along with YamCha3 (Kudo
and Matsumoto (2000, 2001)) as the SVM training and
test software.
3.3 Performance We evaluate our system on three
tasks: i) Argument Identification: Identifying parse con-
stituents that represent arguments of a given predicate, ii)
Argument Classification: Labeling the constituents that
are known to represent arguments with the most likely
roles, and iii) Argument Identification and Classification:
Finding constituents that represent arguments of a pred-
icate, and labeling them with the most likely roles. The
baseline performance on the three tasks is shown in Ta-
ble 1.
4 New Features
To improve the baseline performance we investigated ad-
ditional features that would provide useful information in
identifying arguments of nominalizations. Following is a
2http://cl.aist-nara.ac.jp/?talus-Au/software/TinySVM/
3http://cl.aist-nara.ac.jp/?taku-Au/software/yamcha/
Task P R F?=1 A
(%) (%) (%)
Id. 81.7 65.7 72.8
Classification - - - 70.9
Id. + Classification 65.7 42.1 51.4
Table 1: Baseline performance on all three tasks.
description of each feature along with an intuitive justifi-
cation. Some of these features are not instantiated for a
particular constituent. In those cases, the respective fea-
ture values are set to ?UNK?.
1. Frame ? The frame instantiated by the particular sense
of the predicate in a sentence. This is an oracle feature.
2. Selected words/POS in constituent ? Nominal predi-
cates tend to assign arguments, most commonly through
postnominal of-complements, possessive prenominal
modifiers, etc. We added the values of the first and last
word in the constituent as two separate features. Another
two features represent the part of speech of these words.
3. Ordinal constituent position ? Arguments of nouns
tend to be located closer to the predicate than those
for verbs. This feature captures the ordinal position
of a particular constituent to the left or right of the
predicate on a left or right tree traversal, eg., first PP
from the predicate, second NP from the predicate, etc.
This feature along with the position will encode the
before/after information for the constituent.
4. Constituent tree distance ? Another way of quan-
tifying the position of the constituent is to identify its
index in the list of constituents that are encountered
during linear traversal of the tree from the predicate to
the constituent.
5. Intervening verb features ? Support verbs play an
important role in realizing the arguments of nominal
predicates. We use three classes of intervening verbs:
i) auxiliary verbs ? ones with part of speech AUX, ii)
light verbs ? a small set of known light verbs: took, take,
make, made, give, gave, went and go, and iii) other verbs
? with part of speech VBx. We added three features for
each: i) a binary feature indicating the presence of the
verb in between the predicate and the constituent ii) the
actual word as a feature, and iii) the path through the
tree from the constituent to the verb, as the subject of
intervening verbs sometimes tend to be arguments of
nominalizations. The following example could explain
the intuition behind this feature:
[Speaker Leapor] makes general [Predicate assertions] [Topic
about marriage]
6. Predicate NP expansion rule ? This is the noun
equivalent of the verb sub-categorization feature used by
Gildea and Jurafsky (2002). This is the expansion rule
instantiated by the parser, for the lowermost NP in the
tree, encompassing the predicate. This would tend to
cluster NPs with a similar internal structure and would
thus help finding argumentive modifiers.
7. Noun head of prepositional phrase constituents
? Instead of using the standard head word rule for
prepositional phrases, we use the head word of the first
NP inside the PP as the head of the PP and replace the
constituent type PP with PP-<preposition>.
8. Constituent sibling features ? These are six features
representing the constituent type, head word and part of
speech of the head word of the left and right siblings
of the constituent in consideration. These are used
to capture arguments represented by the modifiers of
nominalizations.
9. Partial-path from constituent to predicate ? This
is the path from the constituent to the lowest common
parent of the constituent and the predicate. This is used
to generalize the path statistics.
10. Is predicate plural ? A binary feature indicating
whether the predicate is singular or plural as they tend to
have different argument selection properties.
11. Genitives in constituent ? This is a binary feature
which is true if there is a genitive word (one with the part
of speech POS, PRP, PRP$ or WP$) in the constituent,
as these tend to be markers for nominal arguments as in
[Speaker Burma ?s] [Phenomenon oil] [Predicate search] hits
virgin forests
12. Constituent parent features ? Same as the sibling
features, except that that these are extracted from the
constituent?s parent.
13. Verb dominating predicate ? The head word of the
first VP ancestor of the predicate.
14. Named Entities in Constituent ? As in Surdeanu
et al (2003), this is represented as seven binary fea-
tures extracted after tagging the sentence with BBN?s
IdentiFinder (Bikel et al, 1999) named entity tagger.
5 Feature Analysis and Best System
Performance
5.1 English For the task of argument identification,
features 2, 3, 4, 5 (the verb itself, path to light-verb and
presence of a light verb), 6, 7, 9, 10 an 13 contributed pos-
itively to the performance. The Frame feature degrades
performance significantly. This could be just an artifact
of the data sparsity. We trained a new classifier using all
the features that contributed positively to the performance
and the F?=1 score increased from the baseline of 72.8%
to 76.3% (?2; p < 0.05).
For the task of argument classification, adding the
Frame feature to the baseline features, provided the most
significant improvement, increasing the classification
accuracy from 70.9% to 79.0% (?2; p < 0.05). All
other features added one-by-one to the baseline did
not bring any significant improvement to the baseline,
which might again be owing to the comparatively small
training and test data sizes. All the features together
produced a classification accuracy of 80.9%. Since the
Frame feature is an oracle, we were interested in finding
out what all the other features combined contributed.
We ran an experiment with all features, except Frame,
added to the baseline, and this produced an accuracy of
73.1%, which however, is not a statistically significant
improvement over the baseline of 70.9%.
For the task of argument identification and classifi-
cation, features 8 and 11 (right sibling head word part
of speech) hurt performance. We trained a classifier
using all the features that contributed positively to the
performance and the resulting system had an improved
F?=1 score of 56.5% compared to the baseline of 51.4%
(?2; p < 0.05).
We found that a significant subset of features that con-
tribute marginally to the classification performance, hurt
the identification task. Therefore, we decided to perform
a two-step process in which we use the set of features that
gave optimum performance for the argument identifica-
tion task and identify all likely argument nodes. Then, for
those nodes, we use all the available features and classify
them into one of the possible classes. This ?two-pass?
system performs slightly better than the ?one-pass? men-
tioned earlier. Again, we performed the second pass of
classification with and without the Frame feature.
Table 2 shows the improved performance numbers.
Task P R F?=1 A
(%) (%) (%)
Id. 83.8 70.0 76.3
Classification (w/o Frame) - - - 73.1
Classification (with Frame) - - - 80.9
Id. + Classification 69.4 47.6 56.5
(one-pass, w/o Frame)
Id. + Classification 62.2 53.1 57.3
(two-pass, w/o Frame)
Id. + Classification 69.4 59.2 63.9
(two-pass, with Frame)
Table 2: Best performance on all three tasks.
5.2 Chinese For the Chinese task, we use the one-pass
algorithm as used for English. A baseline system was
created using the same features as used for English (Sec-
tion 3). We evaluate this system on just the combined task
of argument identification and classification. The base-
line performance is shown in Table 3.
To improve the system?s performance over the base-
line, we added all the features discussed in Section 4, ex-
cept features Frame ? as the data was labeled in a Prop-
Bank fashion, there are no frames involved as in Frame-
Net; Plurals and Genitives ? as they are not realized the
same way morphologically in Chinese, and Named En-
tities ? owing to the unavailability of a Chinese Named
Entity tagger. We found that of these features, 2, 3, 4, 6, 7
and 13 hurt the performance when added to the baseline,
but the other features helped to some degree, although
not significantly. The improved performance is shown in
Table 3
Features P R F?=1
(%) (%)
Baseline 86.2 32.2 46.9
Baseline 83.9 44.1 57.8
+ more features
Table 3: Parsing performance for Chinese on the com-
bined task of identifying and classifying semantic argu-
ments.
An interesting linguistic phenomenon was observed
which explains part of the reason why recall for Chinese
argument parsing is so low. In Chinese, arguments
which are internal to the NP which encompasses the
nominalized predicate, tend to be multi-word, and are
not associated with any node in the parse tree. These
violates our basic assumption of the arguments aligning
with parse tree constituents, and are guaranteed to be
missed. In the case of English however, these tend to be
single word arguments which are represented by a leaf
in the parse tree and stand a chance of getting classified
correctly.
6 Conclusion
In this paper we investigated the task of identifying and
classifying arguments of eventive nominalizations in
FrameNet. The best system generates an F1 score of
57.3% on the combined task of argument identification
and classification using automatically extracted features
on a test set of about 700 sentences using a classifier
trained on about 6,000 sentences.
As noted earlier, the bulk of past research in this area
has focused on verbal predicates. Two notable exceptions
to this include the work of (Hull and Gomez, 1996) ? a
rule based system for identifying the semantic arguments
of nominal predicates, and the work of (Lapata, 2002)
on interpreting the relation between the head of a nom-
inalized compound and its modifier noun. Unfortunately,
meaningful comparisons to these efforts are difficult due
to differing evaluation metrics.
We would like to thank Ralph Weischedel and Scott Miller of
BBN Inc. for letting us use BBN?s named entity tagger ? Iden-
tiFinder; Ashley Thornton for identifying the sentences from
FrameNet with predicates that are eventive nominalizations.
References
[Baker et al1998] Collin F. Baker, Charles J. Fillmore, and
John B. Lowe. 1998. The Berkeley FrameNet project. In
COLING/ACL-98, pages 86?90, Montreal.
[Barker and Dowty1992] Chris Barker and David Dowty. 1992.
Non-verbal thematic proto-roles. In NELS-23, Amy Schafer,
ed., GSLA, Amherst, pages 49?62.
[Bikel et al1999] Daniel M. Bikel, Richard Schwartz, and
Ralph M. Weischedel. 1999. An algorithm that learns what?s
in a name. Machine Learning, 34:211?231.
[Chaniak2001] Eugene Chaniak. 2001. Immediate-head pars-
ing for language models. In ACL, Toulouse, France.
[Gildea and Jurafsky2000] Daniel Gildea and Daniel Jurafsky.
2000. Automatic labeling of semantic roles. In ACL, pages
512?520, Hong Kong, October.
[Gildea and Jurafsky2002] Daniel Gildea and Daniel Jurafsky.
2002. Automatic labeling of semantic roles. Computational
Linguistics, 28(3):245?288.
[Gildea and Palmer2002] Daniel Gildea and Martha Palmer.
2002. The necessity of syntactic parsing for predicate ar-
gument recognition. In ACL, PA.
[Grimshaw1990] Jane Grimshaw. 1990. Argument Structure.
The MIT Press, US.
[Hacioglu and Ward2003] Kadri Hacioglu and Wayne Ward.
2003. Target word detection and semantic role chunking us-
ing support vector machines. In HLT, Edmonton, Canada.
[Hull and Gomez1996] Richard D. Hull and Fernando Gomez.
1996. Semantic interpretation of nominalizations. In AAAI
Conference, Oregon, pages 1062?1068.
[Kingsbury and Palmer2002] Paul Kingsbury and Martha Pal-
mer. 2002. From Treebank to PropBank. In LREC-2002,
Las Palmas, Canary Islands, Spain.
[Kudo and Matsumoto2000] Taku Kudo and Yuji Matsumoto.
2000. Use of support vector learning for chunk identifica-
tion. In CoNLL-2000, pages 142?144.
[Kudo and Matsumoto2001] Taku Kudo and Yuji Matsumoto.
2001. Chunking with support vector machines. In NAACL.
[Lapata2002] Maria Lapata. 2002. The disambiguation of nom-
inalizations. Computational Linguistics, 28(3):357?388.
[Macleod et al1998] C. Macleod, R. Grishman, A. Meyers,
L. Barrett, and R. Reeves. 1998. Nomlex: A lexicon of
nominalizations.
[Pradhan et al2003] Sameer Pradhan, Kadri Hacioglu, Wayne
Ward, James Martin, and Dan Jurafsky. 2003. Semantic role
parsing: Adding semantic structure to unstructured text. In
ICDM, Melbourne, Florida.
[Surdeanu et al2003] Mihai Surdeanu, Sanda Harabagiu, John
Williams, and Paul Aarseth. 2003. Using predicate-
argument structures for information extraction. In ACL, Sap-
poro, Japan.
[Thompson et al2003] Cynthia A. Thompson, Roger Levy, and
Christopher D. Manning. 2003. A generative model for se-
mantic role labeling. In ECML.
Proceedings of NAACL HLT 2007, pages 556?563,
Rochester, NY, April 2007. c?2007 Association for Computational Linguistics
Towards Robust Semantic Role Labeling
Sameer Pradhan
BBN Technologies
Cambridge, MA 02138
pradhan@bbn.com
Wayne Ward, James H. Martin
University of Colorado
Boulder, CO 80303
{whw,martin}@colorado.edu
Abstract
Most research on semantic role labeling
(SRL) has been focused on training and
evaluating on the same corpus in order
to develop the technology. This strategy,
while appropriate for initiating research,
can lead to over-training to the particular
corpus. The work presented in this pa-
per focuses on analyzing the robustness
of an SRL system when trained on one
genre of data and used to label a different
genre. Our state-of-the-art semantic role
labeling system, while performing well on
WSJ test data, shows significant perfor-
mance degradation when applied to data
from the Brown corpus. We present a se-
ries of experiments designed to investigate
the source of this lack of portability. These
experiments are based on comparisons of
performance using PropBanked WSJ data
and PropBanked Brown corpus data. Our
results indicate that while syntactic parses
and argument identification port relatively
well to a new genre, argument classifica-
tion does not. Our analysis of the reasons
for this is presented and generally point
to the nature of the more lexical/semantic
features dominating the classification task
and general structural features dominating
the argument identification task.
1 Introduction
Automatic, accurate and wide-coverage techniques
that can annotate naturally occurring text with se-
mantic argument structure play a key role in NLP
applications such as Information Extraction (Sur-
deanu et al, 2003; Harabagiu et al, 2005), Question
Answering (Narayanan and Harabagiu, 2004) and
Machine Translation (Boas, 2002; Chen and Fung,
2004). Semantic Role Labeling (SRL) is the pro-
cess of producing such a markup. When presented
with a sentence, a parser should, for each predicate
in the sentence, identify and label the predicate?s se-
mantic arguments. In recent work, a number of re-
searchers have cast this problem as a tagging prob-
lem and have applied various supervised machine
learning techniques to it. On the Wall Street Jour-
nal (WSJ) data, using correct syntactic parses, it is
possible to achieve accuracies rivaling human inter-
annotator agreement. However, the performance gap
widens when information derived from automatic
syntactic parses is used.
So far, most of the work on SRL systems has been
focused on improving the labeling performance on a
test set belonging to the same genre of text as the
training set. Both the Treebank on which the syntac-
tic parser is trained and the PropBank on which the
SRL systems are trained represent articles from the
year 1989 of the WSJ. While all these systems per-
form quite well on the WSJ test data, they show sig-
nificant performance degradation (approximately 10
point drop in F-score) when applied to label test data
that is different than the genre that WSJ represents
(Pradhan et al, 2004; Carreras and Ma`rquez, 2005).
556
Surprisingly, it does not matter much whether the
data is from another newswire, or a completely dif-
ferent type of text ? as in the Brown corpus. These
results indicate that the systems are being over-fit to
the specific genre of text. Many performance im-
provements on the WSJ PropBank corpus may re-
flect tuning to the corpus. For the technology to
be widely accepted and useful, it must be robust
to change in genre of the data. Until recently, data
tagged with similar semantic argument structure was
not available for multiple genres of text. Recently,
Palmer et al, (2005), have PropBanked a significant
portion of the Treebanked Brown corpus which en-
ables us to perform experiments to analyze the rea-
sons behind the performance degradation, and sug-
gest potential solutions.
2 Semantic Annotation and Corpora
In the PropBank1 corpus (Palmer et al, 2005), pred-
icate argument relations are marked for the verbs
in the text. PropBank was constructed by assign-
ing semantic arguments to constituents of the hand-
corrected Treebank parses. The arguments of a verb
are labeled ARG0 to ARG5, where ARG0 is the
PROTO-AGENT (usually the subject of a transitive
verb) ARG1 is the PROTO-PATIENT (usually its di-
rect object), etc. In addition to these CORE ARGU-
MENTS, 16 additional ADJUNCTIVE ARGUMENTS,
referred to as ARGMs are also marked.
More recently the PropBanking effort has been
extended to encompass multiple corpora. In this
study we use PropBanked versions of the Wall Street
Journal (WSJ) part of the Penn Treebank (Marcus et
al., 1994) and part of the Brown portion of the Penn
Treebank.
The WSJ PropBank data comprise 24 sections
of the WSJ, each section representing about 100
documents. PropBank release 1.0 contains about
114,000 predicates instantiating about 250,000 argu-
ments and covering about 3,200 verb lemmas. Sec-
tion 23, which is a standard test set and a test set
in some of our experiments, comprises 5,400 predi-
cates instantiating about 12,000 arguments.
The Brown corpus is a Standard Corpus of Ameri-
can English that consists of about one million words
of English text printed in the calendar year 1961
1http://www.cis.upenn.edu/?ace/
(Kuc?era and Francis, 1967). The corpus contains
about 500 samples of 2000+ words each. The idea
behind creating this corpus was to create a hetero-
geneous sample of English text so that it would be
useful for comparative language studies.
The Release 3 of the Penn Treebank contains the
hand parsed syntactic trees of a subset of the Brown
Corpus ? sections F, G, K, L, M, N, P and R. Palmer
et al, (2005) have recently PropBanked a signifi-
cant portion of this Treebanked Brown corpus. In
all, about 17,500 predicates are tagged with their se-
mantic arguments. For these experiments we used a
limited release of PropBank dated September 2005.
A small portion of the predicates ? about 8,000 have
also been tagged with frame sense information.
3 SRL System Description
We formulate the labeling task as a classification
problem as initiated by Gildea and Jurafsky (2002)
and use Support Vector Machine (SVM) classi-
fiers (2005). We use TinySVM2 along with Yam-
Cha3 (Kudo and Matsumoto, 2000) (Kudo and Mat-
sumoto, 2001) as the SVM training and classifica-
tion software. The system uses a polynomial kernel
with degree 2; the cost per unit violation of the mar-
gin, C=1; and, tolerance of the termination criterion,
e=0.001. More details of this system can be found
in Pradhan et al, (2005). The performance of this
system on section 23 of the WSJ when trained on
sections 02-21 is shown in Table 1
ALL ARGs Task P R F A
(%) (%) (%)
TREEBANK Id. 97.5 96.1 96.8
Class. - - - 93.0
Id. + Class. 91.8 90.5 91.2
AUTOMATIC Id. 86.9 84.2 85.5
Class. - - - 92.0
Id. + Class. 82.1 77.9 79.9
Table 1: Performance of the SRL system on WSJ
The performance of the SRL system is reported
on three different tasks, all of which are with respect
to a particular predicate: i) argument identification
(ID), is the task of identifying the set of words (here,
parse constituents) that represent a semantic role; ii)
argument classification (Class.), is the task of clas-
sifying parse constituents known to represent some
2http://cl.aist-nara.ac.jp/?talus-Au/software/TinySVM/
3http://cl.aist-nara.ac.jp/?taku-Au/software/yamcha/
557
semantic role into one of the many semantic role
types; and iii) argument identification and classifi-
cation (ID + Class.), which involves both the iden-
tification of the parse constituents that represent se-
mantic roles of the predicate and their classification
into the respective semantic roles. As usual, argu-
ment classification is measured as percent accuracy
(A), whereas ID and ID + Class. are measured in
terms of precision (P), recall (R) and F-score (F)
? the harmonic mean of P and R. The first three
rows of Table 1 report performance for the system
that uses hand-corrected Treebank parses, and the
next three report performance for the SRL system
that uses automatically generated ? Charniak parser
? parses, both during training and testing.
4 Robustness Experiments
This section describes experiments that we per-
formed using the PropBanked Brown corpus in an
attempt to analyze the factors affecting the portabil-
ity of SRL systems.
4.1 How does the SRL system trained on WSJ
perform on Brown?
In order to test the robustness of the SRL system,
we used a system trained on the PropBanked WSJ
corpus to label data from the Brown corpus. We use
the entire PropBanked Brown corpus (about 17,500
predicates) as a test set for this experiment and use
the SRL system trained on WSJ sections 02-21 to
tag its arguments.
Table 2 shows the performance for training and
testing on WSJ, and for training on WSJ and testing
on Brown. There is a significant reduction in per-
formance when the system trained on WSJ is used
to label data from the Brown corpus. The degrada-
tion in the Identification task is small compared to
that of the combined Identification and Classifica-
tion task. A number of factors could be responsible
for the loss of performance. It is possible that the
SRL models are tuned to the particular vocabulary
and sense structure associated with the training data.
Also, since the syntactic parser that is used for gen-
erating the syntax parse trees (Charniak) is heavily
lexicalized and is trained on WSJ, it could have de-
creased accuracy on the Brown data resulting in re-
duced accuracy for Semantic Role Labeling. Since
the SRL algorithm walks the syntax tree classifying
each node, if no constituent node is present that cor-
responds to the correct argument, the system cannot
produce a correct labeling for the argument.
Train Test Id. Id. + Class
F F
WSJ WSJ 85.5 79.9
WSJ Brown 82.4 65.1
Table 2: Performance of the SRL system on Brown.
In order to check the extent to which constituent
nodes representing semantic arguments were deleted
from the syntax tree due to parser error, we gener-
ated the performance numbers which are shown in
Table 3. These numbers are for top one parse for the
Charniak parser, and represent not all parser errors,
but deletion of argument bearing constituent nodes.
Total Misses %
PropBank 12000 800 6.7
Brown 45880 3692 8.1
Table 3: Constituent deletions in WSJ and Brown.
The parser misses 6.7% of the argument-bearing
nodes in the PropBank test set and about 8.1% in
the Brown corpus. This indicates that the errors in
syntactic parsing account for a fairly small amount
of the argument deletions and probably do not con-
tributing significantly to the increased SRL error
rate. Obviously, just the presence of a argument-
bearing constituent does not necessarily guarantee
the correctness of the structural connections be-
tween itself and the predicate.
4.2 Identification vs Classification Performance
Different features tend to dominate in the identifi-
cation task vs the classification task. For example,
the path feature (representing the path in the syntax
tree from the argument to the predicate) is the sin-
gle most salient feature for the ID task and is not
very important in the classification task. In the next
experiment we look at cross genre performance of
the ID and Classification tasks. We used gold stan-
dard syntactic trees from the Treebank so there are
no errors in generating the syntactic structure. In
addition to training on the WSJ and testing on WSJ
and Brown, we trained the SRL system on a Brown
training set and tested it on a test set alo from the
Brown corpus. In generating the Brown training and
558
SRL SRL Task P R F A
Train Test (%) (%) (%)
WSJ WSJ Id. 97.5 96.1 96.8
(104k) (5k) Class. 93.0
Id. + Class. 91.8 90.5 91.2
WSJ WSJ Id. 96.3 94.4 95.3
(14k) (5k) Class. 86.1
Id. + Class. 84.4 79.8 82.0
BROWN BROWN Id. 95.7 94.9 95.2
(14k) (1.6k) Class. 80.1
Id. + Class. 79.9 77.0 78.4
WSJ BROWN Id. 94.2 91.4 92.7
(14k) (1.6k) Class. 72.0
Id. + Class. 71.8 65.8 68.6
Table 4: Performance of the SRL system using correct Treebank parses.
test sets, we used stratified sampling, which is often
used by the syntactic parsing community (Gildea,
2001). The test set was generated by selecting ev-
ery 10th sentence in the Brown Corpus. We also
held out the development set used by Bacchiani et
al., (2006) to tune system parameters in the future.
This procedure resulted in a training set of approxi-
mately 14,000 predicates and a test set of about 1600
predicates. We did not perform any parameter tun-
ing for any of the following experiments, and used
the parameter settings from the best performing ver-
sion of the SRL system as reported in Table1. We
compare the performance on this test set with that
obtained when the SRL system is trained using WSJ
sections 02-21 and use section 23 for testing. For
a more balanced comparison, we retrained the SRL
system on the same amount of data as used for train-
ing on Brown, and tested it on section 23. As usual,
trace information, and function tag information from
the Treebank is stripped out.
Table 4 shows the results. There is a fairly small
difference in argument Identification performance
when the SRL system is trained on 14,000 predi-
cates vs 104,000 predicates from the WSJ (F-score
95.3 vs 96.8). However, there is a considerable drop
in Classification accuracy (86.1% vs 93.0%). When
the SRL system is trained and tested on Brown data,
the argument Identification performance is not sig-
nificantly different than that for the system trained
and tested on WSJ data (F-score 95.2 vs 95.3). The
drop in argument Classification accuracy is much
more severe (86.1% vs 80.1%).
This same trend between ID and Classification is
even more pronounced when training on WSJ and
testing on Brown. For a system trained on WSJ,
there is a fairly small drop in performance of the
ID task when tested on Brown vs tested on WSJ (F-
score 92.7 vs 95.3). However, in this same condi-
tion, the Classification task has a very large drop in
performance (72.0% vs 86.1%).
So argument ID is not very sensitive to amount
of training data in a corpus, or to the genre of the
corpus, and ports well from WSJ to Brown. This ex-
periment supports the belief that there is no signifi-
cant drop in the task of identifying the right syntactic
constituents that are arguments ? and this is intuitive
since previous experiments have shown that the task
of argument identification is more dependent on the
structural features ? one such feature being the path
in the syntax tree.
Argument Classification seems to be the problem.
It requires more training data within the WSJ corpus,
does not perform as well when trained and tested on
Brown as it does for WSJ and does not port well
from WSJ to Brown. This suggests that the features
it uses are being over-fit to the training data and are
more idiosyncratic to a given dataset. In particular,
the predicate whose arguments are being identified,
and the head word of the syntactic constituent being
classified are both important features in the task of
argument classification.
As a generalization, the features used by the Iden-
tification task reflect structure and port well. The
features used by the Classification task reflect spe-
cific lexical usage and semantics, and tend to require
more training data and are more subject to over-
fitting. Even when training and testing on Brown,
Classification accuracy is considerably worse than
559
training and testing on WSJ (with comparable train-
ing set size). It is probably the case that the predi-
cates and head words in a homogeneous corpus such
as the WSJ are used more consistently, and tend to
have single dominant word senses. The Brown cor-
pus probably has much more variety in its lexical
usage and word senses.
4.3 How sensitive is semantic argument
prediction to the syntactic correctness
across genre?
This experiment examines the same cross-genre ef-
fects as the last experiment, but uses automatically
generated syntactic parses rather than gold standard
ones.
For this experiment, we used the same amount of
training data from WSJ as available in the Brown
training set ? that is about 14,000 predicates. The
examples from WSJ were selected randomly. The
Brown test set is the same as used in the previous
experiment, and the WSJ test set is the entire section
23.
Recently there have been some improvements to
the Charniak parser, use n-best re-ranking as re-
ported in (Charniak and Johnson, 2005) and self-
training and re-ranking using data from the North
American News corpus (NANC) and adapts much
better to the Brown corpus (McClosky et al, 2006a;
McClosky et al, 2006b). The performance of these
parsers as reported in the respective literature are
shown in Table 6 shows the performance (as re-
ported in the literature) of the Charniak parser: when
trained and tested on WSJ, when trained on WSJ and
tested on Brown, When trained and tested on Brown,
and when trained on WSJ and adapted with NANC.
Train Test F
WSJ WSJ 91.0
WSJ Brown 85.2
Brown Brown 88.4
WSJ+NANC Brown 87.9
Table 6: Charniak parser performance.
We describe the results of Semantic Role Label-
ing under the following five conditions:
1. The SRL system is trained on features ex-
tracted from automatically generated parses of
the PropBanked WSJ sentences. The syntactic
parser ? Charniak parser ? is itself trained on
the WSJ training sections of the Treebank. This
is used for Semantic Role Labeling of section-
23 of WSJ.
2. The SRL system is trained on features ex-
tracted from automatically generated parses of
the PropBanked WSJ sentences. The syntac-
tic parser ? Charniak parser ? is itself trained
on the WSJ training sections of the Treebank.
This is used to classify the Brown test set.
3. The SRL system is trained on features ex-
tracted from automatically generated parses of
the PropBanked Brown corpus sentences. The
syntactic parser is trained using the WSJ por-
tion of the Treebank. This is used to classify
the Brown test set.
4. The SRL system is trained on features ex-
tracted from automatically generated parses of
the PropBanked Brown corpus sentences. The
syntactic parser is trained using the Brown
training portion of the Treebank. This is used
to classify the Brown test set.
5. The SRL system is trained on features ex-
tracted from automatically generated parses of
the PropBanked Brown corpus sentences. The
syntactic parser is the version that is self-
trained using 2,500,000 sentences from NANC,
and where the starting version is trained only
on WSJ data (McClosky et al, 2006b). This is
used to classify the Brown test set.
Table 5 shows the results. For simplicity of dis-
cussion we have tagged the five conditions as 1.,
2., 3., 4., and 5. Comparing conditions 2. and 3.
shows that when the features used to train the SRL
system are extracted using a syntactic parser that is
trained on WSJ it performs at almost the same level
on the task of Identification, regardless of whether
it is trained on the PropBanked Brown corpus or
the PropBanked WSJ corpus. This, however, is sig-
nificantly lower than when all the three ? the syn-
tactic parser training set, the SRL system training
set, and the SRL system test set, are from the same
genre (6 F-score points lower than condition 1, and
5 points lower than conditions 4 and 5). In case of
the combined task, the gap between the performance
for conditions 2 and 3 is about 10 points in F-score
(59.1 vs 69.8). Looking at the argument classifica-
tion accuracies, we see that using the SRL system
560
Setup Parser SRL SRL Task P R F A
Train Train Test (%) (%) (%)
1. WSJ WSJ WSJ Id. 87.3 84.8 86.0
(40k ? sec:00-21) (14k) (5k) Class. 84.1
Id. + Class. 77.5 69.7 73.4
2. WSJ WSJ Brown Id. 81.7 78.3 79.9
(40k ? sec:00-21) (14k) (1.6k) Class. 72.1
Id. + Class. 63.7 55.1 59.1
3. WSJ Brown Brown Id. 81.7 78.3 80.0
(40k ? sec:00-21) (14k) (1.6k) Class. 79.2
Id. + Class. 78.2 63.2 69.8
4. Brown Brown Brown Id. 87.6 82.3 84.8
(20k) (14k) (1.6k) Class. 78.9
Id. + Class. 77.4 62.1 68.9
5. WSJ+NANC Brown Brown Id. 87.7 82.5 85.0
(2,500k) (14k) (1.6k) Class. 79.9
Id. + Class. 77.2 64.4 70.0
Table 5: Performance on WSJ and Brown using automatic syntactic parses
trained on WSJ to test Brown sentences give a 12
point drop in F-score (84.1 vs 72.1). Using the SRL
system trained on Brown using WSJ trained syntac-
tic parser shows a drop in accuracy by about 5 F-
score points (84.1 to 79.2). When the SRL system is
trained on Brown using syntactic parser also trained
on Brown, we get a quite similar classification per-
formance, which is again about 5 points lower than
what we get using all WSJ data. This shows lexical
semantic features might be very important to get a
better argument classification on Brown corpus.
4.4 How much data is required to adapt to a
new genre?
We would like to know how much data from a new
genre we need to annotate and add to the training
data of an existing corpus to adapt the system such
that it gives the same level of performance as when
it is trained on the new genre.
One section of the Brown corpus ? section CK
has about 8,200 predicates annotated. We use six
different conditions ? two in which we use correct
Treebank parses, and the four others in which we
use automatically generated parses using the varia-
tions described before. All training sets start with
the same number of examples as in the Brown train-
ing set. The part of this section used as a test set for
the CoNLL 2005 shared task is used as the test set
here. It contains a total of about 800 predicates.
Table 7 shows a comparison of these conditions.
In all the six conditions, the performance on the task
of Identification and Classification improves gradu-
ally until about 5625 examples of section CK which
is about 75% of the total added, above which they
improve very little. In fact, even 50% of the new
data accounts for 90% of the performance differ-
ence. Even when the syntactic parser is trained on
WSJ and the SRL is trained on WSJ, adding 7,500
instances of the new genres allows it to achieve al-
most the same performance as when all three are
from the same genre (67.2 vs 69.9). Numbers for ar-
gument identification aren?t shown because adding
more data does not have any statistically signifi-
cant impact on its performance. The system that
uses self-trained syntactic parser seems to perform
slightly better than the rest of the versions that use
automatically generated syntactic parses. The preci-
sion numbers are almost unaffected ? except when
the labeler is trained on WSJ PropBank data.
4.5 How much does verb sense information
contribute?
In order to find out how important the verb sense
information is in the process of genre transfer, we
used the subset of PropBanked Brown corpus that
was tagged with verb sense information, ran an ex-
periment similar to that of Experiment 1. We used
the oracle sense information and correct syntactic in-
formation for this experiment.
Table 8 shows the results of this experiment.
There is about 1 point F-score increase on using
oracle sense information on the overall data. We
looked at predicates that had high perplexity in both
the training and test sets, and whose sense distribu-
561
Parser SRL Id. + Class Parser SRL Id. + Class
P R F P R F
Train Train (%) (%) (%) (%)
WSJ WSJ (14k) WSJ Brown (14k)
(Treebank parses) (Treebank parses)
+0 ex. from CK 74.1 66.5 70.1 (40k) +0 ex. from CK 74.4 57.0 64.5
+1875 ex. from CK 77.6 71.3 74.3 +1875 ex. from CK 75.1 58.7 65.9
+3750 ex. from CK 79.1 74.1 76.5 +3750 ex. from CK 76.1 59.6 66.9
+5625 ex. from CK 80.4 76.1 78.1 +5625 ex. from CK 76.9 60.5 67.7
+7500 ex. from CK 80.2 76.1 78.1 +7500 ex. from CK 76.8 59.8 67.2
Brown Brown (14k) Brown Brown (14k)
(Treebank parses) (Treebank parses)
+0 ex. from CK 77.1 73.0 75.0 (20k) +0 ex. from CK 76.0 59.2 66.5
+1875 ex. from CK 78.8 75.1 76.9 +1875 ex. from CK 76.1 60.0 67.1
+3750 ex. from CK 80.4 76.9 78.6 +3750 ex. from CK 77.7 62.4 69.2
+5625 ex. from CK 80.4 77.2 78.7 +5625 ex. from CK 78.2 63.5 70.1
+7500 ex. from CK 81.2 78.1 79.6 +7500 ex. from CK 78.2 63.2 69.9
WSJ WSJ (14k) WSJ+NANC Brown (14k)
(40k) +0 ex. from CK 65.2 55.7 60.1 (2,500k) +0 ex. from CK 74.4 60.1 66.5
+1875 ex. from CK 68.9 57.5 62.7 +1875 ex. from CK 76.2 62.3 68.5
+3750 ex. from CK 71.8 59.3 64.9 +3750 ex. from CK 76.8 63.6 69.6
+5625 ex. from CK 74.3 61.3 67.2 +5625 ex. from CK 77.7 63.8 70.0
+7500 ex. from CK 74.8 61.0 67.2 +7500 ex. from CK 78.2 64.9 70.9
Table 7: Effect of incrementally adding data from a new genre
Train Test Without Sense With Sense
Id. Id.
F F
WSJ Brown (All) 69.1 69.9
WSJ Brown (predicate: go) 46.9 48.9
Table 8: Influence of verb sense feature.
tion was different. One such predicate is ?go?. The
improvement on classifying the arguments of this
predicate was about 2 points (46.9 to 48.9), which
suggests that verb sense is more important when the
sense structure of the test corpus is more ambiguous
and is different from the training. Here we used ora-
cle verb sense information, but one can train a clas-
sifier as done by Girju et al, (2005) which achieves
a disambiguation accuracy in the 80s for within the
WSJ corpus.
5 Conclusions
Our experimental results on robustness to change in
genre can be summarized as follows:
? There is a significant drop in performance when
training and testing on different corpora ? for
both Treebank and Charniak parses
? In this process the classification task is more
disrupted than the identification task.
? There is a performance drop in classification
even when training and testing on Brown (com-
pared to training and testing on WSJ)
? The syntactic parser error is not a large part of
the degradation for the case of automatically
generated parses.
An error analysis leads us to believe that some
reasons for this behavior could be: i) lexical us-
ages that are specific to WSJ, ii) variation in sub-
categorization across corpora, iii) variation in word
sense distribution and iv) changes in topics and enti-
ties. Training and testing on the same corpora tends
to give a high weight to very specific semantic fea-
tures. Two possibilities remedies could be: i) using
less homogeneous corpora and ii) less specific fea-
tures, for eg., proper names are replaced with the
name entities that they represent. This way the sys-
tem could be forced to use the more general features.
Both of these manipulations would most likely re-
duce performance on the training set, and on test
sets of the same genre as the training data. But they
would be likely to generalize better.
6 Acknowledgments
We are extremely grateful to Martha Palmer for pro-
viding us with the PropBanked Brown corpus, and
to David McClosky for providing us with hypothe-
ses on the Brown test set as well as a cross-validated
562
version of the Brown training data for the various
models reported in his work reported at HLT 2006.
This research was partially supported by
the ARDA AQUAINT program via contract
OCG4423B and by the NSF via grants IS-9978025
and ITR/HCI 0086132.
References
Michiel Bacchiani, Michael Riley, Brian Roark, and
Richard Sproat. 2006. MAP adaptation of stochas-
tic grammars. Computer Speech and Language,
20(1):41?68.
Hans Boas. 2002. Bilingual framenet dictionaries for
machine translation. In Proceedings of LREC-2002.
Xavier Carreras and Llu??s Ma`rquez. 2005. Introduction
to the CoNLL-2005 shared task: Semantic role label-
ing. In Proceedings of CoNLL-2005, pages 152?164,
Ann Arbor, MI.
Eugene Charniak and Mark Johnson. 2005. Coarse-to-
fine n-best parsing and maxent discriminative rerank-
ing. In Proceedings of ACL-2005, pages 173?180,
Ann Arbor, MI.
Benfeng Chen and Pascale Fung. 2004. Automatic con-
struction of an english-chinese bilingual framenet. In
Proceedings of the HLT/NAACL-2004, Boston, MA.
Daniel Gildea and Daniel Jurafsky. 2002. Automatic la-
beling of semantic roles. Computational Linguistics,
28(3):245?288.
Daniel Gildea. 2001. Corpus variation and parser perfor-
mance. In In Proceedings of EMNLP-2001.
R. Girju, D. Roth, and M. Sammons. 2005. Token-
level disambiguation of verbnet classes. In Proceed-
ings of the Interdisciplinary Workshop on the Identifi-
cation and Representation of Verb Features and Verb
Classes, K. Erk, A. Melinger, and S. Schulte im Walde
(eds.).
Sanda Harabagiu, Cosmin Adrian Bejan, and
Paul Morarescu. 2005. Shallow semantics for relation
extraction. In IJCAI-2005, pages 1061?1067, Edin-
burgh, Scotland.
Henry Kuc?era and W. Nelson Francis. 1967. Com-
putational analysis of present-day American English.
Brown University Press, Providence, RI.
Taku Kudo and Yuji Matsumoto. 2000. Use of support
vector learning for chunk identification. In Proceed-
ings of CoNLL-2000 and LLL-2000, pages 142?144.
Taku Kudo and Yuji Matsumoto. 2001. Chunking
with support vector machines. In Proceedings of the
NAACL-2001.
Mitchell Marcus, Grace Kim, Mary Ann Marcinkiewicz,
Robert MacIntyre, Ann Bies, Mark Ferguson, Karen
Katz, and Britta Schasberger. 1994. The Penn Tree-
bank: Annotating predicate argument structure.
David McClosky, Eugene Charniak, and Mark Johnson.
2006a. Effective self-training for parsing. In Proceed-
ings of HLT/NAACL-2006, pages 152?159, New York
City, USA. Association for Computational Linguistics.
David McClosky, Eugene Charniak, and Mark Johnson.
2006b. Rerankinng and self-training for parser adapta-
tion. In Proceedings of COLING/ACL-2006, Sydney,
Australia.
Srini Narayanan and Sanda Harabagiu. 2004. Question
answering based on semantic structures. In Proceed-
ings of COLING-2004), Geneva, Switzerland.
Martha Palmer, Daniel Gildea, and Paul Kingsbury.
2005. The proposition bank: An annotated corpus of
semantic roles. Computational Linguistics, 31(1):71?
106.
Sameer Pradhan, Wayne Ward, Kadri Hacioglu, James
Martin, and Dan Jurafsky. 2004. Shallow semantic
parsing using support vector machines. In Proceed-
ings of HLT/NAACL-2004, Boston, MA.
Sameer Pradhan, Wayne Ward, Kadri Hacioglu, James
Martin, and Dan Jurafsky. 2005. Semantic role label-
ing using different syntactic views. In Proceedings of
ACL-2005, Ann Arbor, MI.
Mihai Surdeanu, Sanda Harabagiu, John Williams, and
Paul Aarseth. 2003. Using predicate-argument struc-
tures for information extraction. In Proceedings of
ACL-2003, Sapporo, Japan.
563
Proceedings of the 43rd Annual Meeting of the ACL, pages 581?588,
Ann Arbor, June 2005. c?2005 Association for Computational Linguistics
Semantic Role Labeling Using Different Syntactic Views?
Sameer Pradhan, Wayne Ward,
Kadri Hacioglu, James H. Martin
Center for Spoken Language Research,
University of Colorado,
Boulder, CO 80303
{spradhan,whw,hacioglu,martin}@cslr.colorado.edu
Daniel Jurafsky
Department of Linguistics,
Stanford University,
Stanford, CA 94305
jurafsky@stanford.edu
Abstract
Semantic role labeling is the process of
annotating the predicate-argument struc-
ture in text with semantic labels. In this
paper we present a state-of-the-art base-
line semantic role labeling system based
on Support Vector Machine classifiers.
We show improvements on this system
by: i) adding new features including fea-
tures extracted from dependency parses,
ii) performing feature selection and cali-
bration and iii) combining parses obtained
from semantic parsers trained using dif-
ferent syntactic views. Error analysis of
the baseline system showed that approx-
imately half of the argument identifica-
tion errors resulted from parse errors in
which there was no syntactic constituent
that aligned with the correct argument. In
order to address this problem, we com-
bined semantic parses from a Minipar syn-
tactic parse and from a chunked syntac-
tic representation with our original base-
line system which was based on Charniak
parses. All of the reported techniques re-
sulted in performance improvements.
1 Introduction
Semantic Role Labeling is the process of annotat-
ing the predicate-argument structure in text with se-
?This research was partially supported by the ARDA
AQUAINT program via contract OCG4423B and by the NSF
via grants IS-9978025 and ITR/HCI 0086132
mantic labels (Gildea and Jurafsky, 2000; Gildea
and Jurafsky, 2002; Gildea and Palmer, 2002; Sur-
deanu et al, 2003; Hacioglu and Ward, 2003; Chen
and Rambow, 2003; Gildea and Hockenmaier, 2003;
Pradhan et al, 2004; Hacioglu, 2004). The architec-
ture underlying all of these systems introduces two
distinct sub-problems: the identification of syntactic
constituents that are semantic roles for a given pred-
icate, and the labeling of the those constituents with
the correct semantic role.
A detailed error analysis of our baseline system
indicates that the identification problem poses a sig-
nificant bottleneck to improving overall system per-
formance. The baseline system?s accuracy on the
task of labeling nodes known to represent semantic
arguments is 90%. On the other hand, the system?s
performance on the identification task is quite a bit
lower, achieving only 80% recall with 86% preci-
sion. There are two sources of these identification
errors: i) failures by the system to identify all and
only those constituents that correspond to semantic
roles, when those constituents are present in the syn-
tactic analysis, and ii) failures by the syntactic ana-
lyzer to provide the constituents that align with cor-
rect arguments. The work we present here is tailored
to address these two sources of error in the identifi-
cation problem.
The remainder of this paper is organized as fol-
lows. We first describe a baseline system based on
the best published techniques. We then report on
two sets of experiments using techniques that im-
prove performance on the problem of finding argu-
ments when they are present in the syntactic analy-
sis. In the first set of experiments we explore new
581
features, including features extracted from a parser
that provides a different syntactic view ? a Combi-
natory Categorial Grammar (CCG) parser (Hocken-
maier and Steedman, 2002). In the second set of
experiments, we explore approaches to identify opti-
mal subsets of features for each argument class, and
to calibrate the classifier probabilities.
We then report on experiments that address the
problem of arguments missing from a given syn-
tactic analysis. We investigate ways to combine
hypotheses generated from semantic role taggers
trained using different syntactic views ? one trained
using the Charniak parser (Charniak, 2000), another
on a rule-based dependency parser ? Minipar (Lin,
1998), and a third based on a flat, shallow syntactic
chunk representation (Hacioglu, 2004a). We show
that these three views complement each other to im-
prove performance.
2 Baseline System
For our experiments, we use Feb 2004 release of
PropBank1 (Kingsbury and Palmer, 2002; Palmer
et al, 2005), a corpus in which predicate argument
relations are marked for verbs in the Wall Street
Journal (WSJ) part of the Penn TreeBank (Marcus
et al, 1994). PropBank was constructed by as-
signing semantic arguments to constituents of hand-
corrected TreeBank parses. Arguments of a verb
are labeled ARG0 to ARG5, where ARG0 is the
PROTO-AGENT, ARG1 is the PROTO-PATIENT, etc.
In addition to these CORE ARGUMENTS, additional
ADJUNCTIVE ARGUMENTS, referred to as ARGMs
are also marked. Some examples are ARGM-LOC,
for locatives; ARGM-TMP, for temporals; ARGM-
MNR, for manner, etc. Figure 1 shows a syntax tree
along with the argument labels for an example ex-
tracted from PropBank. We use Sections 02-21 for
training, Section 00 for development and Section 23
for testing.
We formulate the semantic labeling problem as
a multi-class classification problem using Support
Vector Machine (SVM) classifier (Hacioglu et al,
2003; Pradhan et al, 2003; Pradhan et al, 2004)
TinySVM2 along with YamCha3 (Kudo and Mat-
1http://www.cis.upenn.edu/?ace/
2http://chasen.org/?taku/software/TinySVM/
3http://chasen.org/?taku/software/yamcha/
S
hhhh
((((
NP
hhhh
((((
The acquisition
ARG1
VP
```
   
VBD
was
NULL
VP
XXX
VBN
completed
predicate
PP
```
   
in September
ARGM?TMP
[ARG1 The acquisition] was [predicate completed] [ARGM?TMP in September].
Figure 1: Syntax tree for a sentence illustrating the
PropBank tags.
sumoto, 2000; Kudo and Matsumoto, 2001) are used
to implement the system. Using what is known as
the ONE VS ALL classification strategy, n binary
classifiers are trained, where n is number of seman-
tic classes including a NULL class.
The baseline feature set is a combination of fea-
tures introduced by Gildea and Jurafsky (2002) and
ones proposed in Pradhan et al, (2004), Surdeanu et
al., (2003) and the syntactic-frame feature proposed
in (Xue and Palmer, 2004). Table 1 lists the features
used.
PREDICATE LEMMA
PATH: Path from the constituent to the predicate in the parse tree.
POSITION: Whether the constituent is before or after the predicate.
VOICE
PREDICATE SUB-CATEGORIZATION
PREDICATE CLUSTER
HEAD WORD: Head word of the constituent.
HEAD WORD POS: POS of the head word
NAMED ENTITIES IN CONSTITUENTS: 7 named entities as 7 binary features.
PARTIAL PATH: Path from the constituent to the lowest common ancestor
of the predicate and the constituent.
VERB SENSE INFORMATION: Oracle verb sense information from PropBank
HEAD WORD OF PP: Head of PP replaced by head word of NP inside it,
and PP replaced by PP-preposition
FIRST AND LAST WORD/POS IN CONSTITUENT
ORDINAL CONSTITUENT POSITION
CONSTITUENT TREE DISTANCE
CONSTITUENT RELATIVE FEATURES: Nine features representing
the phrase type, head word and head word part of speech of the
parent, and left and right siblings of the constituent.
TEMPORAL CUE WORDS
DYNAMIC CLASS CONTEXT
SYNTACTIC FRAME
CONTENT WORD FEATURES: Content word, its POS and named entities
in the content word
Table 1: Features used in the Baseline system
As described in (Pradhan et al, 2004), we post-
process the n-best hypotheses using a trigram lan-
guage model of the argument sequence.
We analyze the performance on three tasks:
? Argument Identification ? This is the pro-
cess of identifying the parsed constituents in
the sentence that represent semantic arguments
of a given predicate.
582
? Argument Classification ? Given constituents
known to represent arguments of a predicate,
assign the appropriate argument labels to them.
? Argument Identification and Classification ?
A combination of the above two tasks.
ALL ARGs Task P R F1 A
(%) (%) (%)
HAND Id. 96.2 95.8 96.0
Classification - - - 93.0
Id. + Classification 89.9 89.0 89.4
AUTOMATIC Id. 86.8 80.0 83.3
Classification - - - 90.1
Id. + Classification 80.9 76.8 78.8
Table 2: Baseline system performance on all tasks
using hand-corrected parses and automatic parses on
PropBank data.
Table 2 shows the performance of the system us-
ing the hand corrected, TreeBank parses (HAND)
and using parses produced by a Charniak parser
(AUTOMATIC). Precision (P), Recall (R) and F1
scores are given for the identification and combined
tasks, and Classification Accuracy (A) for the clas-
sification task.
Classification performance using Charniak parses
is about 3% absolute worse than when using Tree-
Bank parses. On the other hand, argument identifi-
cation performance using Charniak parses is about
12.7% absolute worse. Half of these errors ? about
7% are due to missing constituents, and the other
half ? about 6% are due to mis-classifications.
Motivated by this severe degradation in argument
identification performance for automatic parses, we
examined a number of techniques for improving
argument identification. We made a number of
changes to the system which resulted in improved
performance. The changes fell into three categories:
i) new features, ii) feature selection and calibration,
and iii) combining parses from different syntactic
representations.
3 Additional Features
3.1 CCG Parse Features
While the Path feature has been identified to be very
important for the argument identification task, it is
one of the most sparse features and may be diffi-
cult to train or generalize (Pradhan et al, 2004; Xue
and Palmer, 2004). A dependency grammar should
generate shorter paths from the predicate to depen-
dent words in the sentence, and could be a more
robust complement to the phrase structure grammar
paths extracted from the Charniak parse tree. Gildea
and Hockenmaier (2003) report that using features
extracted from a Combinatory Categorial Grammar
(CCG) representation improves semantic labeling
performance on core arguments. We evaluated fea-
tures from a CCG parser combined with our baseline
feature set. We used three features that were intro-
duced by Gildea and Hockenmaier (2003):
? Phrase type ? This is the category of the max-
imal projection between the two words ? the
predicate and the dependent word.
? Categorial Path ? This is a feature formed by
concatenating the following three values: i) cat-
egory to which the dependent word belongs, ii)
the direction of dependence and iii) the slot in
the category filled by the dependent word.
? Tree Path ? This is the categorial analogue of
the path feature in the Charniak parse based
system, which traces the path from the depen-
dent word to the predicate through the binary
CCG tree.
Parallel to the hand-corrected TreeBank parses,
we also had access to correct CCG parses derived
from the TreeBank (Hockenmaier and Steedman,
2002a). We performed two sets of experiments.
One using the correct CCG parses, and the other us-
ing parses obtained using StatCCG4 parser (Hocken-
maier and Steedman, 2002). We incorporated these
features in the systems based on hand-corrected
TreeBank parses and Charniak parses respectively.
For each constituent in the Charniak parse tree, if
there was a dependency between the head word of
the constituent and the predicate, then the corre-
sponding CCG features for those words were added
to the features for that constituent. Table 3 shows the
performance of the system when these features were
added. The corresponding baseline performances
are mentioned in parentheses.
3.2 Other Features
We added several other features to the system. Po-
sition of the clause node (S, SBAR) seems to be
4Many thanks to Julia Hockenmaier for providing us with
the CCG bank as well as the StatCCG parser.
583
ALL ARGs Task P R F1
(%) (%)
HAND Id. 97.5 (96.2) 96.1 (95.8) 96.8 (96.0)
Id. + Class. 91.8 (89.9) 90.5 (89.0) 91.2 (89.4)
AUTOMATIC Id. 87.1 (86.8) 80.7 (80.0) 83.8 (83.3)
Id. + Class. 81.5 (80.9) 77.2 (76.8) 79.3 (78.8)
Table 3: Performance improvement upon adding
CCG features to the Baseline system.
an important feature in argument identification (Ha-
cioglu et al, 2004) therefore we experimented with
four clause-based path feature variations. We added
the predicate context to capture predicate sense vari-
ations. For some adjunctive arguments, punctuation
plays an important role, so we added some punctu-
ation features. All the new features are shown in
Table 4
CLAUSE-BASED PATH VARIATIONS:
I. Replacing all the nodes in a path other than clause nodes with an ?*?.
For example, the path NP?S?VP?SBAR?NP?VP?VBD
becomes NP?S?*S?*?*?VBD
II. Retaining only the clause nodes in the path, which for the above
example would produce NP?S?S?VBD,
III. Adding a binary feature that indicates whether the constituent
is in the same clause as the predicate,
IV. collapsing the nodes between S nodes which gives NP?S?NP?VP?VBD.
PATH N-GRAMS: This feature decomposes a path into a series of trigrams.
For example, the path NP?S?VP?SBAR?NP?VP?VBD becomes:
NP?S?VP, S?VP?SBAR, VP?SBAR?NP, SBAR?NP?VP, etc. We
used the first ten trigrams as ten features. Shorter paths were padded
with nulls.
SINGLE CHARACTER PHRASE TAGS: Each phrase category is clustered
to a category defined by the first character of the phrase label.
PREDICATE CONTEXT: Two words and two word POS around the
predicate and including the predicate were added as ten new features.
PUNCTUATION: Punctuation before and after the constituent were
added as two new features.
FEATURE CONTEXT: Features for argument bearing constituents
were added as features to the constituent being classified.
Table 4: Other Features
4 Feature Selection and Calibration
In the baseline system, we used the same set of fea-
tures for all the n binary ONE VS ALL classifiers.
Error analysis showed that some features specifi-
cally suited for one argument class, for example,
core arguments, tend to hurt performance on some
adjunctive arguments. Therefore, we thought that
selecting subsets of features for each argument class
might improve performance. To achieve this, we
performed a simple feature selection procedure. For
each argument, we started with the set of features in-
troduced by (Gildea and Jurafsky, 2002). We pruned
this set by training classifiers after leaving out one
feature at a time and checking its performance on
a development set. We used the ?2 significance
while making pruning decisions. Following that, we
added each of the other features one at a time to the
pruned baseline set of features and selected ones that
showed significantly improved performance. Since
the feature selection experiments were computation-
ally intensive, we performed them using 10k training
examples.
SVMs output distances not probabilities. These
distances may not be comparable across classifiers,
especially if different features are used to train each
binary classifier. In the baseline system, we used the
algorithm described by Platt (Platt, 2000) to convert
the SVM scores into probabilities by fitting to a sig-
moid. When all classifiers used the same set of fea-
tures, fitting all scores to a single sigmoid was found
to give the best performance. Since different fea-
ture sets are now used by the classifiers, we trained
a separate sigmoid for each classifier.
Raw Scores Probabilities
After lattice-rescoring
Uncalibrated Calibrated
(%) (%) (%)
Same Feat. same sigmoid 74.7 74.7 75.4
Selected Feat. diff. sigmoids 75.4 75.1 76.2
Table 5: Performance improvement on selecting fea-
tures per argument and calibrating the probabilities
on 10k training data.
Foster and Stine (2004) show that the pool-
adjacent-violators (PAV) algorithm (Barlow et al,
1972) provides a better method for converting raw
classifier scores to probabilities when Platt?s algo-
rithm fails. The probabilities resulting from either
conversions may not be properly calibrated. So, we
binned the probabilities and trained a warping func-
tion to calibrate them. For each argument classifier,
we used both the methods for converting raw SVM
scores into probabilities and calibrated them using
a development set. Then, we visually inspected
the calibrated plots for each classifier and chose the
method that showed better calibration as the calibra-
tion procedure for that classifier. Plots of the pre-
dicted probabilities versus true probabilities for the
ARGM-TMP VS ALL classifier, before and after cal-
ibration are shown in Figure 2. The performance im-
provement over a classifier that is trained using all
the features for all the classes is shown in Table 5.
Table 6 shows the performance of the system af-
ter adding the CCG features, additional features ex-
584
0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
Predicted Probability
Tr
ue
 P
ro
ba
bi
lity
Before Calibration
0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
Predicted Probability
Tr
ue
 P
ro
ba
bi
lity
After Calibration
Figure 2: Plots showing true probabilities versus predicted probabilities before and after calibration on the
test set for ARGM-TMP.
tracted from the Charniak parse tree, and performing
feature selection and calibration. Numbers in paren-
theses are the corresponding baseline performances.
TASK P R F1 A
(%) (%) (%)
Id. 86.9 (86.8) 84.2 (80.0) 85.5 (83.3)
Class. - - - 92.0 (90.1)
Id. + Class. 82.1 (80.9) 77.9 (76.8) 79.9 (78.8)
Table 6: Best system performance on all tasks using
automatically generated syntactic parses.
5 Alternative Syntactic Views
Adding new features can improve performance
when the syntactic representation being used for
classification contains the correct constituents. Ad-
ditional features can?t recover from the situation
where the parse tree being used for classification
doesn?t contain the correct constituent representing
an argument. Such parse errors account for about
7% absolute of the errors (or, about half of 12.7%)
for the Charniak parse based system. To address
these errors, we added two additional parse repre-
sentations: i) Minipar dependency parser, and ii)
chunking parser (Hacioglu et al, 2004). The hope is
that these parsers will produce different errors than
the Charniak parser since they represent different
syntactic views. The Charniak parser is trained on
the Penn TreeBank corpus. Minipar is a rule based
dependency parser. The chunking parser is trained
on PropBank and produces a flat syntactic represen-
tation that is very different from the full parse tree
produced by Charniak. A combination of the three
different parses could produce better results than any
single one.
5.1 Minipar-based Semantic Labeler
Minipar (Lin, 1998; Lin and Pantel, 2001) is a rule-
based dependency parser. It outputs dependencies
between a word called head and another called mod-
ifier. Each word can modify at most one word. The
dependency relationships form a dependency tree.
The set of words under each node in Minipar?s
dependency tree form a contiguous segment in the
original sentence and correspond to the constituent
in a constituent tree. We formulate the semantic la-
beling problem in the same way as in a constituent
structure parse, except we classify the nodes that
represent head words of constituents. A similar for-
mulation using dependency trees derived from Tree-
Bank was reported in Hacioglu (Hacioglu, 2004).
In that experiment, the dependency trees were de-
rived from hand-corrected TreeBank trees using
head word rules. Here, an SVM is trained to as-
sign PropBank argument labels to nodes in Minipar
dependency trees using the following features:
Table 8 shows the performance of the Minipar-
based semantic parser.
Minipar performance on the PropBank corpus is
substantially worse than the Charniak based system.
This is understandable from the fact that Minipar
is not designed to produce constituents that would
exactly match the constituent segmentation used in
TreeBank. In the test set, about 37% of the argu-
585
PREDICATE LEMMA
HEAD WORD: The word representing the node in the dependency tree.
HEAD WORD POS: Part of speech of the head word.
POS PATH: This is the path from the predicate to the head word through
the dependency tree connecting the part of speech of each node in the tree.
DEPENDENCY PATH: Each word that is connected to the head
word has a particular dependency relationship to the word. These
are represented as labels on the arc between the words. This
feature is the dependencies along the path that connects two words.
VOICE
POSITION
Table 7: Features used in the Baseline system using
Minipar parses.
Task P R F1
(%) (%)
Id. 73.5 43.8 54.6
Id. + Classification 66.2 36.7 47.2
Table 8: Baseline system performance on all tasks
using Minipar parses.
ments do not have corresponding constituents that
match its boundaries. In experiments reported by
Hacioglu (Hacioglu, 2004), a mismatch of about
8% was introduced in the transformation from hand-
corrected constituent trees to dependency trees. Us-
ing an errorful automatically generated tree, a still
higher mismatch would be expected. In case of
the CCG parses, as reported by Gildea and Hock-
enmaier (2003), the mismatch was about 23%. A
more realistic way to score the performance is to
score tags assigned to head words of constituents,
rather than considering the exact boundaries of the
constituents as reported by Gildea and Hocken-
maier (2003). The results for this system are shown
in Table 9.
Task P R F1
(%) (%)
CHARNIAK Id. 92.2 87.5 89.8
Id. + Classification 85.9 81.6 83.7
MINIPAR Id. 83.3 61.1 70.5
Id. + Classification 72.9 53.5 61.7
Table 9: Head-word based performance using Char-
niak and Minipar parses.
5.2 Chunk-based Semantic Labeler
Hacioglu has previously described a chunk based se-
mantic labeling method (Hacioglu et al, 2004). This
system uses SVM classifiers to first chunk input text
into flat chunks or base phrases, each labeled with
a syntactic tag. A second SVM is trained to assign
semantic labels to the chunks. The system is trained
on the PropBank training data.
WORDS
PREDICATE LEMMAS
PART OF SPEECH TAGS
BP POSITIONS: The position of a token in a BP using the IOB2
representation (e.g. B-NP, I-NP, O, etc.)
CLAUSE TAGS: The tags that mark token positions in a sentence
with respect to clauses.
NAMED ENTITIES: The IOB tags of named entities.
TOKEN POSITION: The position of the phrase with respect to
the predicate. It has three values as ?before?, ?after? and ?-? (for
the predicate)
PATH: It defines a flat path between the token and the predicate
CLAUSE BRACKET PATTERNS
CLAUSE POSITION: A binary feature that identifies whether the
token is inside or outside the clause containing the predicate
HEADWORD SUFFIXES: suffixes of headwords of length 2, 3 and 4.
DISTANCE: Distance of the token from the predicate as a number
of base phrases, and the distance as the number of VP chunks.
LENGTH: the number of words in a token.
PREDICATE POS TAG: the part of speech category of the predicate
PREDICATE FREQUENCY: Frequent or rare using a threshold of 3.
PREDICATE BP CONTEXT: The chain of BPs centered at the predicate
within a window of size -2/+2.
PREDICATE POS CONTEXT: POS tags of words immediately preceding
and following the predicate.
PREDICATE ARGUMENT FRAMES: Left and right core argument patterns
around the predicate.
NUMBER OF PREDICATES: This is the number of predicates in
the sentence.
Table 10: Features used by chunk based classifier.
Table 10 lists the features used by this classifier.
For each token (base phrase) to be tagged, a set of
features is created from a fixed size context that sur-
rounds each token. In addition to the above features,
it also uses previous semantic tags that have already
been assigned to the tokens contained in the linguis-
tic context. A 5-token sliding window is used for the
context.
P R F1
(%) (%)
Id. and Classification 72.6 66.9 69.6
Table 11: Semantic chunker performance on the
combined task of Id. and classification.
SVMs were trained for begin (B) and inside (I)
classes of all arguments and outside (O) class for a
total of 78 one-vs-all classifiers. Again, TinySVM5
along with YamCha6 (Kudo and Matsumoto, 2000;
Kudo and Matsumoto, 2001) are used as the SVM
training and test software.
Table 11 presents the system performances on the
PropBank test set for the chunk-based system.
5http://chasen.org/?taku/software/TinySVM/
6http://chasen.org/?taku/software/yamcha/
586
6 Combining Semantic Labelers
We combined the semantic parses as follows: i)
scores for arguments were converted to calibrated
probabilities, and arguments with scores below a
threshold value were deleted. Separate thresholds
were used for each parser. ii) For the remaining ar-
guments, the more probable ones among overlap-
ping ones were selected. In the chunked system,
an argument could consist of a sequence of chunks.
The probability assigned to the begin tag of an ar-
gument was used as the probability of the sequence
of chunks forming an argument. Table 12 shows
the performance improvement after the combina-
tion. Again, numbers in parentheses are respective
baseline performances.
TASK P R F1
(%) (%)
Id. 85.9 (86.8) 88.3 (80.0) 87.1 (83.3)
Id. + Class. 81.3 (80.9) 80.7 (76.8) 81.0 (78.8)
Table 12: Constituent-based best system perfor-
mance on argument identification and argument
identification and classification tasks after combin-
ing all three semantic parses.
The main contribution of combining both the
Minipar based and the Charniak-based parsers was
significantly improved performance on ARG1 in ad-
dition to slight improvements to some other argu-
ments. Table 13 shows the effect on selected argu-
ments on sentences that were altered during the the
combination of Charniak-based and Chunk-based
parses.
Number of Propositions 107
Percentage of perfect props before combination 0.00
Percentage of perfect props after combination 45.95
Before After
P R F1 P R F1
(%) (%) (%) (%)
Overall 94.8 53.4 68.3 80.9 73.8 77.2
ARG0 96.0 85.7 90.5 92.5 89.2 90.9
ARG1 71.4 13.5 22.7 59.4 59.4 59.4
ARG2 100.0 20.0 33.3 50.0 20.0 28.5
ARGM-DIS 100.0 40.0 57.1 100.0 100.0 100.0
Table 13: Performance improvement on parses
changed during pair-wise Charniak and Chunk com-
bination.
A marked increase in number of propositions for
which all the arguments were identified correctly
from 0% to about 46% can be seen. Relatively few
predicates, 107 out of 4500, were affected by this
combination.
To give an idea of what the potential improve-
ments of the combinations could be, we performed
an oracle experiment for a combined system that
tags head words instead of exact constituents as we
did in case of Minipar-based and Charniak-based se-
mantic parser earlier. In case of chunks, first word in
prepositional base phrases was selected as the head
word, and for all other chunks, the last word was se-
lected to be the head word. If the correct argument
was found present in either the Charniak, Minipar or
Chunk hypotheses then that was selected. The re-
sults for this are shown in Table 14. It can be seen
that the head word based performance almost ap-
proaches the constituent based performance reported
on the hand-corrected parses in Table 3 and there
seems to be considerable scope for improvement.
Task P R F1
(%) (%)
C Id. 92.2 87.5 89.8
Id. + Classification 85.9 81.6 83.7
C+M Id. 98.4 90.6 94.3
Id. + Classification 93.1 86.0 89.4
C+CH Id. 98.9 88.8 93.6
Id. + Classification 92.5 83.3 87.7
C+M+CH Id. 99.2 92.5 95.7
Id. + Classification 94.6 88.4 91.5
Table 14: Performance improvement on head word
based scoring after oracle combination. Charniak
(C), Minipar (M) and Chunker (CH).
Table 15 shows the performance improvement in
the actual system for pairwise combination of the
parsers and one using all three.
Task P R F1
(%) (%)
C Id. 92.2 87.5 89.8
Id. + Classification 85.9 81.6 83.7
C+M Id. 91.7 89.9 90.8
Id. + Classification 85.0 83.9 84.5
C+CH Id. 91.5 91.1 91.3
Id. + Classification 84.9 84.3 84.7
C+M+CH Id. 91.5 91.9 91.7
Id. + Classification 85.1 85.5 85.2
Table 15: Performance improvement on head word
based scoring after combination. Charniak (C),
Minipar (M) and Chunker (CH).
587
7 Conclusions
We described a state-of-the-art baseline semantic
role labeling system based on Support Vector Ma-
chine classifiers. Experiments were conducted to
evaluate three types of improvements to the sys-
tem: i) adding new features including features ex-
tracted from a Combinatory Categorial Grammar
parse, ii) performing feature selection and calibra-
tion and iii) combining parses obtained from seman-
tic parsers trained using different syntactic views.
We combined semantic parses from a Minipar syn-
tactic parse and from a chunked syntactic repre-
sentation with our original baseline system which
was based on Charniak parses. The belief was that
semantic parses based on different syntactic views
would make different errors and that the combina-
tion would be complimentary. A simple combina-
tion of these representations did lead to improved
performance.
8 Acknowledgements
This research was partially supported by the ARDA
AQUAINT program via contract OCG4423B and
by the NSF via grants IS-9978025 and ITR/HCI
0086132. Computer time was provided by NSF
ARI Grant #CDA-9601817, NSF MRI Grant #CNS-
0420873, NASA AIST grant #NAG2-1646, DOE
SciDAC grant #DE-FG02-04ER63870, NSF spon-
sorship of the National Center for Atmospheric Re-
search, and a grant from the IBM Shared University
Research (SUR) program.
We would like to thank Ralph Weischedel and
Scott Miller of BBN Inc. for letting us use their
named entity tagger ? IdentiFinder; Martha Palmer
for providing us with the PropBank data; Dan Gildea
and Julia Hockenmaier for providing the gold stan-
dard CCG parser information, and all the anony-
mous reviewers for their helpful comments.
References
R. E. Barlow, D. J. Bartholomew, J. M. Bremmer, and H. D. Brunk. 1972. Statis-
tical Inference under Order Restrictions. Wiley, New York.
Eugene Charniak. 2000. A maximum-entropy-inspired parser. In Proceedings of
NAACL, pages 132?139, Seattle, Washington.
John Chen and Owen Rambow. 2003. Use of deep linguistics features for
the recognition and labeling of semantic arguments. In Proceedings of the
EMNLP, Sapporo, Japan.
Dean P. Foster and Robert A. Stine. 2004. Variable selection in data mining:
building a predictive model for bankruptcy. Journal of American Statistical
Association, 99, pages 303?313.
Dan Gildea and Julia Hockenmaier. 2003. Identifying semantic roles using com-
binatory categorial grammar. In Proceedings of the EMNLP, Sapporo, Japan.
Daniel Gildea and Daniel Jurafsky. 2000. Automatic labeling of semantic roles.
In Proceedings of ACL, pages 512?520, Hong Kong, October.
Daniel Gildea and Daniel Jurafsky. 2002. Automatic labeling of semantic roles.
Computational Linguistics, 28(3):245?288.
Daniel Gildea and Martha Palmer. 2002. The necessity of syntactic parsing for
predicate argument recognition. In Proceedings of ACL, Philadelphia, PA.
Kadri Hacioglu. 2004. Semantic role labeling using dependency trees. In Pro-
ceedings of COLING, Geneva, Switzerland.
Kadri Hacioglu and Wayne Ward. 2003. Target word detection and semantic role
chunking using support vector machines. In Proceedings of HLT/NAACL,
Edmonton, Canada.
Kadri Hacioglu, Sameer Pradhan, Wayne Ward, James Martin, and Dan Jurafsky.
2003. Shallow semantic parsing using support vector machines. Technical
Report TR-CSLR-2003-1, Center for Spoken Language Research, Boulder,
Colorado.
Kadri Hacioglu, Sameer Pradhan, Wayne Ward, James Martin, and Daniel Juraf-
sky. 2004. Semantic role labeling by tagging syntactic chunks. In Proceed-
ings of CoNLL-2004, Shared Task ? Semantic Role Labeling.
Kadri Hacioglu. 2004a. A lightweight semantic chunking model based on tag-
ging. In Proceedings of HLT/NAACL, Boston, MA.
Julia Hockenmaier and Mark Steedman. 2002. Generative models for statistical
parsing with combinatory grammars. In Proceedings of the ACL, pages 335?
342.
Julia Hockenmaier and Mark Steedman. 2002a. Acquiring compact lexicalized
grammars from a cleaner treebank. In Proceedings of the 3rd International
Conference on Language Resources and Evaluation (LREC-2002), Las Pal-
mas, Canary Islands, Spain.
Paul Kingsbury and Martha Palmer. 2002. From Treebank to PropBank. In
Proceedings of LREC, Las Palmas, Canary Islands, Spain.
Taku Kudo and Yuji Matsumoto. 2000. Use of support vector learning for chunk
identification. In Proceedings of CoNLL and LLL, pages 142?144.
Taku Kudo and Yuji Matsumoto. 2001. Chunking with support vector machines.
In Proceedings of the NAACL.
Dekang Lin and Patrick Pantel. 2001. Discovery of inference rules for question
answering. Natural Language Engineering, 7(4):343?360.
Dekang Lin. 1998. Dependency-based evaluation of MINIPAR. In In Workshop
on the Evaluation of Parsing Systems, Granada, Spain.
Mitchell Marcus, Grace Kim, Mary Ann Marcinkiewicz, Robert MacIntyre, Ann
Bies, Mark Ferguson, Karen Katz, and Britta Schasberger. 1994. The Penn
Treebank: Annotating predicate argument structure.
Martha Palmer, Dan Gildea, and Paul Kingsbury. 2005. The proposition bank:
An annotated corpus of semantic roles. To appear Computational Linguistics.
John Platt. 2000. Probabilities for support vector machines. In A. Smola,
P. Bartlett, B. Scholkopf, and D. Schuurmans, editors, Advances in Large
Margin Classifiers. MIT press, Cambridge, MA.
Sameer Pradhan, Kadri Hacioglu, Wayne Ward, James Martin, and Dan Jurafsky.
2003. Semantic role parsing: Adding semantic structure to unstructured text.
In Proceedings of ICDM, Melbourne, Florida.
Sameer Pradhan, Wayne Ward, Kadri Hacioglu, James Martin, and Dan Jurafsky.
2004. Shallow semantic parsing using support vector machines. In Proceed-
ings of HLT/NAACL, Boston, MA.
Mihai Surdeanu, Sanda Harabagiu, John Williams, and Paul Aarseth. 2003. Us-
ing predicate-argument structures for information extraction. In Proceedings
of ACL, Sapporo, Japan.
Nianwen Xue and Martha Palmer. 2004. Calibrating features for semantic role
labeling. In Proceedings of EMNLP, Barcelona, Spain.
588
Semantic Role Labeling by Tagging Syntactic Chunks?
Kadri Hacioglu1, Sameer Pradhan1, Wayne Ward1, James H. Martin1, Daniel Jurafsky2
1University of Colorado at Boulder, 2Stanford University
{hacioglu,spradhan,whw}@cslr.colorado.edu, martin@cs.colorado.edu, jurafsky@stanford.edu
Abstract
In this paper, we present a semantic role la-
beler (or chunker) that groups syntactic chunks
(i.e. base phrases) into the arguments of a pred-
icate. This is accomplished by casting the se-
mantic labeling as the classification of syntactic
chunks (e.g. NP-chunk, PP-chunk) into one of
several classes such as the beginning of an ar-
gument (B-ARG), inside an argument (I-ARG)
and outside an argument (O). This amounts to
tagging syntactic chunks with semantic labels
using the IOB representation. The chunker is
realized using support vector machines as one-
versus-all classifiers. We describe the represen-
tation of data and information used to accom-
plish the task. We participate in the ?closed
challenge? of the CoNLL-2004 shared task and
report results on both development and test
sets.
1 Introduction
In semantic role labeling the goal is to group sequences
of words together and classify them by using semantic la-
bels. For meaning representation the predicate-argument
structure that exists in most languages is used. In this
structure a word (most frequently a verb) is specified as
a predicate, and a number of word groups are considered
as arguments accompanying the word (or predicate).
In this paper, we select support vector machines
(SVMs) (Vapnik, 1995; Burges, 1998) to implement
the semantic role classifiers, due to their ability to han-
dle an extremely large number of (overlapping) features
with quite strong generalization properties. Support vec-
tor machines for semantic role chunking were first used
?This research was partially supported by the ARDA
AQUAINT program via contract OCG4423B and by the NSF
via grant IIS-9978025
in (Hacioglu and Ward, 2003) as word-by-word (W-by-
W) classifiers. The system was then applied to the
constituent-by-constituent (C-by-C) classification in (Ha-
cioglu et al, 2003). In (Pradhan et al, 2003; Prad-
han et al, 2004), several extensions to the basic system
have been proposed, extensively studied and systemati-
cally compared to other systems. In this paper, we imple-
ment a system that classifies syntactic chunks (i.e. base
phrases) instead of words or the constituents derived from
syntactic trees. This system is referred to as the phrase-
by-phrase (P-by-P) semantic role classifier. We partici-
pate in the ?closed challenge? of the CoNLL-2004 shared
task and report results on both development and test sets.
A detailed description of the task, data and related work
can be found in (Carreras and Ma`rquez, 2004).
2 System Description
2.1 Data Representation
In this paper, we change the representation of the original
data as follows:
? Bracketed representation of roles is converted into
IOB2 representation (Ramhsaw and Marcus, 1995;
Sang and Veenstra, 1995)
? Word tokens are collapsed into base phrase (BP) to-
kens.
Since the semantic annotation in the PropBank corpus
does not have any embedded structure there is no loss of
information in the first change. However, this results in
a simpler representation with a reduced set of tagging la-
bels. In the second change, it is possible to miss some
information in cases where the semantic chunks do not
align with the sequence of BPs. However, in Section 3.2
we show that the loss in performance due to the misalign-
ment is much less than the gain in performance that can
be achieved by the change in representation.
from
million
251.2
$
to
declined VBD
CD
NN
IN
CD
$
TO
Sales
%
CD
10
$
278.7
$
CD
NNS
*A2)
*
*
*A4)
*
*
million CD I?NP
O
*
*
B?NP
B?VP
B?NP
I?NP
B?PP
B?NP
I?NP
I?NP
B?PP
B?NP
I?NP
*
(S*
*
*
*
*
*
*
*
*
*
decline
?
?
?
?
?
?
?
?
?
?
?
?
*A3)
(A3*
(A4*
(A2*
(V*V)
(A1*A1)
*S)
O
NP
VP
NP
PP
NP
PP
NP
Sales
declined
NNS
VBD
% NN
TOto
million
from
million
B?NP
CD
CD
O
I?NP
B?PP
I?NP
B?PP
I?NP
?
?
?
?
?
?
?
*
*
*
*
*
* B?V
B?A2
B?A1
B?A3
B?A4
O
O
O
B?VP
IN
*S)
(S*
(b)(a)
. .
. .
decline
Figure 1: Illustration of change in data representation; (a) original word-by-word data representation (b) phrase-by-
phrase data representation used in this paper. Words are collapsed into base phrase types retaining only headwords
with their respective features. Bracketed representation of semantic role labels is converted into IOB2 representation.
See text for details.
The new representation is illustrated in Figure 1 along
with the original representation. Comparing both we note
the following differences and advantages in the new rep-
resentation:
? BPs are being classified instead of words.
? Only the BP headwords (rightmost words) are re-
tained as word information.
? The number of tagging steps is smaller.
? A fixed context spans a larger segment of a sentence.
Therefore, the P-by-P semantic role chunker classifies
larger units, ignores some of the words, uses a relatively
larger context for a given window size and performs the
labeling faster.
2.2 Features
The following features, which we refer to as the base fea-
tures, are provided in the shared task data for each sen-
tence;
? Words
? Predicate lemmas
? Part of Speech tags
? BP Positions: The position of a token in a BP using
the IOB2 representation (e.g. B-NP, I-NP, O etc.)
? Clause tags: The tags that mark token positions in a
sentence with respect to clauses. (e.g *S)*S) marks
a position that two clauses end)
? Named entities: The IOB tags of named entities.
There are four categories; LOC, ORG, PERSON
and MISC.
Using available information we have created the fol-
lowing token level features:
? Token Position: The position of the phrase with re-
spect to the predicate. It has three values as ?be-
fore?, ?after? and ?-? for the predicate.
? Path: It defines a flat path between the token and
the predicate as a chain of base phrases. At both
ends, the chain is terminated with the POS tags of
the predicate and the headword of the token.
? Clause bracket patterns: We use two patterns of
clauses for each token. One is the clause bracket
chain between the token and the predicate, and the
other is from the token to sentence begin or end de-
pending on token?s position with respect to the pred-
icate.
? Clause Position: a binary feature that indicates the
token is inside or outside of the clause which con-
tains the predicate
? Headword suffixes: suffixes of headwords of length
2, 3 and 4.
? Distance: we have two notions of distance; the first
is the distance of the token from the predicate as a
number of base phrases, and the second is the same
distance as the number of VP chunks.
? Length: the number of words in a token.
We also use some sentence level features:
? Predicate POS tag: the part of speech category of
the predicate
? Predicate Frequency; this is a feature which indi-
cates whether the predicate is frequent or rare with
respect to the training set. The threshold on the
counts is currently set to 3.
? Predicate BP Context : The chain of BPs centered
at the predicate within a window of size -2/+2.
? Predicate POS Context : The POS tags of the
words that immediately precede and follow the pred-
icate. The POS tag of a preposition is replaced with
the preposition itself.
? Predicate Argument Frames: We used the left and
right patterns of the core arguments (A0 through A5)
for each predicate . We used the three most frequent
argument frames for both sides depending on the po-
sition of the token in focus with respect to the pred-
icate. (e.g. raise has A0 and A1 AO (A0 being the
most frequent) as its left argument frames, and A1,
A1 A2 and A2 as the three most frequent right argu-
ment frames)
? Number of predicates: This is the number of pred-
icates in the sentence.
For each token (base phrase) to be tagged, a set of or-
dered features is created from a fixed size context that
surrounds each token. In addition to the above features,
we also use previous semantic IOB tags that have already
been assigned to the tokens contained in the context. A
5-token sliding window is used for the context. A greedy
left-to-right tagging is performed.
All of the above features are designed to implicitly cap-
ture the patterns of sentence constructs with respect to
different word/predicate usages and senses. We acknowl-
edge that they significantly overlap and extensive exper-
iments are required to determine the impact of each fea-
ture on the performance.
2.3 Classifier
All SVM classifiers were realized using TinySVM1 with
a polynomial kernel of degree 2 and the general purpose
SVM based chunker YamCha 2. SVMs were trained for
begin (B) and inside (I) classes of all arguments and one
outside (O) class for a total of 78 one-vs-all classifiers
(some arguments do not have an I-tag).
1http://cl.aist-nara.ac.jp/taku-ku/software/TinySVM
2http://cl.aist-nara.ac.jp/taku-ku/software/yamcha
Table 1: Comparison of W-by-W and P-by-P methods.
Both systems use the base features provided (i.e. no fea-
ture engineering is done). Results are on dev set.
Method Precision Recall F?=1
P-by-P 69.04% 54.68% 61.02
W-by-W 68.34% 45.16% 54.39
Table 2: Number of sentences and unique training exam-
ples in each method.
Method Sentences Training Examples
P-by-P 19K 347K
W-by-W 19K 534K
3 Experimental Results
3.1 Data and Evaluation Metrics
The data provided for the shared task is a part of the
February 2004 release of the PropBank corpus. It con-
sists of sections from the Wall Street Journal part of the
Penn Treebank. All experiments were carried out using
Sections 15-18 for training Section-20 for development
and Section-21 for testing. The results were evaluated for
precision, recall and F?=1 numbers using the srl-eval.pl
script provided by the shared task organizers.
3.2 W-by-W and P-by-P Experiments
In these experiments we used only the base features to
compare the two approaches. Table 1 illustrates the over-
all performance on the dev set. Although both systems
were trained using the same number of sentences, the ac-
tual number of training examples in each case were quite
different. Those numbers are presented in Table 2. It is
clear that P-by-P method uses much less data for the same
number of sentences. Despite this we particularly note a
considerable improvement in recall. Actually, the data
reduction was not without a cost. Some arguments have
been missed as they do not align with the base phrase
chunks due to inconsistencies in semantic annotation and
due to errors in automatic base phrase chunking. The per-
centage of this misalignment was around 2.5% (over the
dev set). We observed that nearly 45% of the mismatches
were for the ?outside? chunks. Therefore, sequences of
words with outside tags were not collapsed.
3.3 Best System Results
In these experiments all of the features described earlier
were used with the P-by-P system. Table 3 presents our
best system performance on the development set. Ad-
ditional features have improved the performance from
61.02 to 71.72. The performance of the same system on
the test set is similarly illustrated in Table 4.
Table 3: System results on development set.
Precision Recall F?=1
Overall 74.17% 69.42% 71.72
A0 82.86% 78.50% 80.62
A1 72.82% 73.97% 73.39
A2 60.16% 56.18% 58.10
A3 59.66% 47.65% 52.99
A4 83.21% 74.15% 78.42
A5 100.00% 75.00% 85.71
AM-ADV 52.52% 41.48% 46.35
AM-CAU 61.11% 41.51% 49.44
AM-DIR 47.37% 15.00% 22.78
AM-DIS 76.47% 76.47% 76.47
AM-EXT 74.07% 40.82% 52.63
AM-LOC 51.21% 46.09% 48.51
AM-MNR 51.04% 36.83% 42.78
AM-MOD 99.47% 95.63% 97.51
AM-NEG 99.20% 94.66% 96.88
AM-PNC 70.00% 28.00% 40.00
AM-PRD 0.00% 0.00% 0.00
AM-REC 0.00% 0.00% 0.00
AM-TMP 69.33% 58.37% 63.38
R-A0 91.55% 80.25% 85.53
R-A1 72.46% 67.57% 69.93
R-A2 100.00% 52.94% 69.23
R-AM-LOC 100.00% 25.00% 40.00
R-AM-TMP 0.00% 0.00% 0.00
V 99.05% 99.05% 99.05
4 Conclusions
We have described a semantic role chunker using SVMs.
The chunking method has been based on a chunked sen-
tence structure at both syntactic and semantic levels. We
have jointly performed semantic chunk segmentation and
labeling using a set of one-vs-all SVM classifiers on a
phrase-by-phrase basis. It has been argued that the new
representation has several advantages as compared to the
original representation. It yields a semantic role labeler
that classifies larger units, exploits relatively larger con-
text, uses less data (possibly, redundant and noisy data
are filtered out), runs faster and performs better.
References
Xavier Carreras and Llu??s Ma`rquez. 2004. Introduction
to the CoNLL-2004 Shared Task: Semantic Role La-
beling in the same volume of Proc. of CoNLL?2004
Shared Task.
Christopher J. C. Burges. 1997. A Tutorial on Support
Vector Machines for Pattern Recognition. Data Min-
ing and Knowledge Discovery, 2(2), pages 1-47.
Kadri Hacioglu and Wayne Ward. 2003. Target word
Table 4: System results on test set.
Precision Recall F?=1
Overall 72.43% 66.77% 69.49
A0 82.93% 79.88% 81.37
A1 71.92% 71.33% 71.63
A2 49.37% 49.30% 49.33
A3 57.50% 46.00% 51.11
A4 87.10% 54.00% 66.67
A5 0.00% 0.00% 0.00
AM-ADV 53.36% 38.76% 44.91
AM-CAU 57.89% 22.45% 32.35
AM-DIR 37.84% 28.00% 32.18
AM-DIS 66.83% 62.44% 64.56
AM-EXT 70.00% 50.00% 58.33
AM-LOC 46.63% 36.40% 40.89
AM-MNR 50.31% 31.76% 38.94
AM-MOD 98.12% 92.88% 95.43
AM-NEG 91.11% 96.85% 93.89
AM-PNC 52.00% 15.29% 23.64
AM-PRD 0.00% 0.00% 0.00
AM-TMP 64.57% 50.74% 56.82
R-A0 90.21% 81.13% 85.43
R-A1 83.02% 62.86% 71.54
R-A2 100.00% 33.33% 50.00
R-A3 0.00% 0.00% 0.00
R-AM-LOC 0.00% 0.00% 0.00
R-AM-MNR 0.00% 0.00% 0.00
R-AM-PNC 0.00% 0.00% 0.00
R-AM-TMP 60.00% 21.43% 31.58
V 98.46% 98.46% 98.46
Detection and Semantic Role Chunking Using Support
Vector Machines. Proc. of the HLT-NAACL-03.
Kadri Hacioglu, Sameer Pradhan, Wayne Ward, James
Martin, and Dan Jurafsky. 2003. Shallow Semantic
Parsing Using Support Vector Machines. CSLR Tech.
Report, CSLR-TR-2003-1.
Sameer Pradhan, Kadri Hacioglu, Wayne Ward, James
Martin, and Dan Jurafsky. 2003. Semantic Role Pars-
ing: Adding Semantic Structure to Unstructured Text.
Proc. of Int. Conf. on Data Mining (ICDM03).
Sameer Pradhan, Kadri Hacioglu, Wayne Ward, James
Martin, and Dan Jurafsky. 2004. Support Vector
Learning for Semantic Argument Classification. to ap-
pear in Journal of Machine Learning.
Lance E. Ramhsaw and Mitchell P. Marcus. 1995.
Text Chunking Using Transformation Based Learning.
Proc. of the 3rd ACL Workshop on Very Large Cor-
pora, pages 82-94.
Erik F. T. J. Sang, John Veenstra. 1999. Representing
Text Chunks. Proc. of EACL?99, pages 173-179.
Vladamir Vapnik 1995. The Nature of Statistical Learn-
ing Theory. Springer Verlag, New York, USA.
Proceedings of the 9th Conference on Computational Natural Language Learning (CoNLL),
pages 217?220, Ann Arbor, June 2005. c?2005 Association for Computational Linguistics
Semantic Role Chunking Combining Complementary Syntactic Views
Sameer Pradhan, Kadri Hacioglu, Wayne Ward, James H. Martin and Daniel Jurafsky?
Center for Spoken Language Research, University of Colorado, Boulder, CO 80303
?Department of Linguistics, Stanford University, Stanford, CA 94305
{spradhan,hacioglu,whw,martin}@cslr.colorado.edu, jurafsky@stanford.edu
Abstract
This paper describes a semantic role la-
beling system that uses features derived
from different syntactic views, and com-
bines them within a phrase-based chunk-
ing paradigm. For an input sentence, syn-
tactic constituent structure parses are gen-
erated by a Charniak parser and a Collins
parser. Semantic role labels are assigned
to the constituents of each parse using
Support Vector Machine classifiers. The
resulting semantic role labels are con-
verted to an IOB representation. These
IOB representations are used as additional
features, along with flat syntactic chunks,
by a chunking SVM classifier that pro-
duces the final SRL output. This strategy
for combining features from three differ-
ent syntactic views gives a significant im-
provement in performance over roles pro-
duced by using any one of the syntactic
views individually.
1 Introduction
The task of Semantic Role Labeling (SRL) involves
tagging groups of words in a sentence with the se-
mantic roles that they play with respect to a particu-
lar predicate in that sentence. Our approach is to use
supervised machine learning classifiers to produce
the role labels based on features extracted from the
input. This approach is neutral to the particular set
of labels used, and will learn to tag input according
to the annotated data that it is trained on. The task
reported on here is to produce PropBank (Kingsbury
and Palmer, 2002) labels, given the features pro-
vided for the CoNLL-2005 closed task (Carreras and
Ma`rquez, 2005).
We have previously reported on using SVM clas-
sifiers for semantic role labeling. In this work, we
formulate the semantic labeling problem as a multi-
class classification problem using Support Vector
Machine (SVM) classifiers. Some of these systems
use features based on syntactic constituents pro-
duced by a Charniak parser (Pradhan et al, 2003;
Pradhan et al, 2004) and others use only a flat syn-
tactic representation produced by a syntactic chun-
ker (Hacioglu et al, 2003; Hacioglu and Ward,
2003; Hacioglu, 2004; Hacioglu et al, 2004). The
latter approach lacks the information provided by
the hierarchical syntactic structure, and the former
imposes a limitation that the possible candidate roles
should be one of the nodes already present in the
syntax tree. We found that, while the chunk based
systems are very efficient and robust, the systems
that use features based on full syntactic parses are
generally more accurate. Analysis of the source
of errors for the parse constituent based systems
showed that incorrect parses were a major source
of error. The syntactic parser did not produce any
constituent that corresponded to the correct segmen-
tation for the semantic argument. In Pradhan et al
(2005), we reported on a first attempt to overcome
this problem by combining semantic role labels pro-
duced from different syntactic parses. The hope is
that the syntactic parsers will make different errors,
and that combining their outputs will improve on
217
either system alone. This initial attempt used fea-
tures from a Charniak parser, a Minipar parser and a
chunk based parser. It did show some improvement
from the combination, but the method for combin-
ing the information was heuristic and sub-optimal.
In this paper, we report on what we believe is an im-
proved framework for combining information from
different syntactic views. Our goal is to preserve the
robustness and flexibility of the segmentation of the
phrase-based chunker, but to take advantage of fea-
tures from full syntactic parses. We also want to
combine features from different syntactic parses to
gain additional robustness. To this end, we use fea-
tures generated from a Charniak parser and a Collins
parser, as supplied for the CoNLL-2005 closed task.
2 System Description
We again formulate the semantic labeling problem
as a multi-class classification problem using Sup-
port Vector Machine (SVM) classifiers. TinySVM1
along with YamCha2 (Kudo and Matsumoto, 2000;
Kudo and Matsumoto, 2001) are used to implement
the system. Using what is known as the ONE VS
ALL classification strategy, n binary classifiers are
trained, where n is number of semantic classes in-
cluding a NULL class.
The general framework is to train separate seman-
tic role labeling systems for each of the parse tree
views, and then to use the role arguments output by
these systems as additional features in a semantic
role classifier using a flat syntactic view. The con-
stituent based classifiers walk a syntactic parse tree
and classify each node as NULL (no role) or as one
of the set of semantic roles. Chunk based systems
classify each base phrase as being the B(eginning)
of a semantic role, I(nside) a semantic role, or
O(utside) any semantic role (ie. NULL). This
is referred to as an IOB representation (Ramshaw
and Marcus, 1995). The constituent level roles are
mapped to the IOB representation used by the chun-
ker. The IOB tags are then used as features for a
separate base-phase semantic role labeler (chunker),
in addition to the standard set of features used by
the chunker. An n-fold cross-validation paradigm
is used to train the constituent based role classifiers
1http://chasen.org/?taku/software/TinySVM/
2http://chasen.org/?taku/software/yamcha/
and the chunk based classifier.
For the system reported here, two full syntactic
parsers were used, a Charniak parser and a Collins
parser. Features were extracted by first generating
the Collins and Charniak syntax trees from the word-
by-word decomposed trees in the CoNLL data. The
chunking system for combining all features was
trained using a 4-fold paradigm. In each fold, sepa-
rate SVM classifiers were trained for the Collins and
Charniak parses using 75% of the training data. That
is, one system assigned role labels to the nodes in
Charniak based trees and a separate system assigned
roles to nodes in Collins based trees. The other 25%
of the training data was then labeled by each of the
systems. Iterating this process 4 times created the
training set for the chunker. After the chunker was
trained, the Charniak and Collins based semantic la-
belers were then retrained using all of the training
data.
Two pieces of the system have problems scaling
to large training sets ? the final chunk based clas-
sifier and the NULL VS NON-NULL classifier for
the parse tree syntactic views. Two techniques were
used to reduce the amount of training data ? active
sampling and NULL filtering. The active sampling
process was performed as follows. We first train
a system using 10k seed examples from the train-
ing set. We then labeled an additional block of data
using this system. Any sentences containing an er-
ror were added to the seed training set. The sys-
tem was retrained and the procedure repeated until
there were no misclassified sentences remaining in
the training data. The set of examples produced by
this procedure was used to train the final NULL VS
NON-NULL classifier. The same procedure was car-
ried out for the chunking system. After both these
were trained, we tagged the training data using them
and removed all most likely NULLs from the data.
Table 1 lists the features used in the constituent
based systems. They are a combination of features
introduced by Gildea and Jurafsky (2002), ones pro-
posed in Pradhan et al (2004), Surdeanu et al
(2003) and the syntactic-frame feature proposed in
(Xue and Palmer, 2004). These features are ex-
tracted from the parse tree being labeled. In addition
to the features extracted from the parse tree being
labeled, five features were extracted from the other
parse tree (phrase, head word, head word POS, path
218
PREDICATE LEMMA
PATH: Path from the constituent to the predicate in the parse tree.
POSITION: Whether the constituent is before or after the predicate.
PREDICATE SUB-CATEGORIZATION
HEAD WORD: Head word of the constituent.
HEAD WORD POS: POS of the head word
NAMED ENTITIES IN CONSTITUENTS: Person, Organization, Location
and Miscellaneous.
PARTIAL PATH: Path from the constituent to the lowest common ancestor
of the predicate and the constituent.
HEAD WORD OF PP: Head of PP replaced by head word of NP inside it,
and PP replaced by PP-preposition
FIRST AND LAST WORD/POS IN CONSTITUENT
ORDINAL CONSTITUENT POSITION
CONSTITUENT TREE DISTANCE
CONSTITUENT RELATIVE FEATURES: Nine features representing
the phrase type, head word and head word part of speech of the
parent, and left and right siblings of the constituent.
SYNTACTIC FRAME
CONTENT WORD FEATURES: Content word, its POS and named entities
in the content word
CLAUSE-BASED PATH VARIATIONS:
I. Replacing all the nodes in a path other than clause nodes with an ?*?.
For example, the path NP?S?VP?SBAR?NP?VP?VBD
becomes NP?S?*S?*?*?VBD
II. Retaining only the clause nodes in the path, which for the above
example would produce NP?S?S?VBD,
III. Adding a binary feature that indicates whether the constituent
is in the same clause as the predicate,
IV. collapsing the nodes between S nodes which gives NP?S?NP?VP?VBD.
PATH N-GRAMS: This feature decomposes a path into a series of trigrams.
For example, the path NP?S?VP?SBAR?NP?VP?VBD becomes:
NP?S?VP, S?VP?SBAR, VP?SBAR?NP, SBAR?NP?VP, etc. We
used the first ten trigrams as ten features. Shorter paths were padded
with nulls.
SINGLE CHARACTER PHRASE TAGS: Each phrase category is clustered
to a category defined by the first character of the phrase label.
PREDICATE CONTEXT: Two words and two word POS around the
predicate and including the predicate were added as ten new features.
PUNCTUATION: Punctuation before and after the constituent were
added as two new features.
FEATURE CONTEXT: Features for argument bearing constituents
were added as features to the constituent being classified.
Table 1: Features used by the constituent-based sys-
tem
and predicate sub-categorization). So for example,
when assigning labels to constituents in a Charniak
parse, all of the features in Table 1 were extracted
from the Charniak tree, and in addition phrase, head
word, head word POS, path and sub-categorization
were extracted from the Collins tree. We have pre-
viously determined that using different sets of fea-
tures for each argument (role) achieves better results
than using the same set of features for all argument
classes. A simple feature selection was implemented
by adding features one by one to an initial set of
features and selecting those that contribute signifi-
cantly to the performance. As described in Pradhan
et al (2004), we post-process lattices of n-best de-
cision using a trigram language model of argument
sequences.
Table 2 lists the features used by the chunker.
These are the same set of features that were used
in the CoNLL-2004 semantic role labeling task by
Hacioglu, et al (2004) with the addition of the two
semantic argument (IOB) features. For each token
(base phrase) to be tagged, a set of features is created
from a fixed size context that surrounds each token.
In addition to the features in Table 2, it also uses pre-
vious semantic tags that have already been assigned
to the tokens contained in the linguistic context. A
5-token sliding window is used for the context.
SVMs were trained for begin (B) and inside (I)
classes of all arguments and an outside (O) class.
WORDS
PREDICATE LEMMAS
PART OF SPEECH TAGS
BP POSITIONS: The position of a token in a BP using the IOB2
representation (e.g. B-NP, I-NP, O, etc.)
CLAUSE TAGS: The tags that mark token positions in a sentence
with respect to clauses.
NAMED ENTITIES: The IOB tags of named entities.
TOKEN POSITION: The position of the phrase with respect to
the predicate. It has three values as ?before?, ?after? and ?-? (for
the predicate)
PATH: It defines a flat path between the token and the predicate
HIERARCHICAL PATH: Since we have the syntax tree for the sentences,
we also use the hierarchical path from the phrase being classified to the
base phrase containing the predicate.
CLAUSE BRACKET PATTERNS
CLAUSE POSITION: A binary feature that identifies whether the
token is inside or outside the clause containing the predicate
HEADWORD SUFFIXES: suffixes of headwords of length 2, 3 and 4.
DISTANCE: Distance of the token from the predicate as a number
of base phrases, and the distance as the number of VP chunks.
LENGTH: the number of words in a token.
PREDICATE POS TAG: the part of speech category of the predicate
PREDICATE FREQUENCY: Frequent or rare using a threshold of 3.
PREDICATE BP CONTEXT: The chain of BPs centered at the predicate
within a window of size -2/+2.
PREDICATE POS CONTEXT: POS tags of words immediately preceding
and following the predicate.
PREDICATE ARGUMENT FRAMES: Left and right core argument patterns
around the predicate.
DYNAMIC CLASS CONTEXT: Hypotheses generated for two preceeding
phrases.
NUMBER OF PREDICATES: This is the number of predicates in
the sentence.
CHARNIAK-BASED SEMANTIC IOB TAG: This is the IOB tag generated
using the tagger trained on Charniak trees
COLLINS-BASED SEMANTIC IOB TAG: This is the IOB tag generated
using the tagger trained on Collins? trees
Table 2: Features used by phrase-based chunker.
3 Experimental Results
Table 3 shows the results obtained on the WSJ de-
velopment set (Section 24), the WSJ test set (Section
23) and the Brown test set (Section ck/01-03)
4 Acknowledgments
This research was partially supported by the ARDA
AQUAINT program via contract OCG4423B and
by the NSF via grants IS-9978025 and ITR/HCI
219
Precision Recall F?=1
Development 80.90% 75.38% 78.04
Test WSJ 81.97% 73.27% 77.37
Test Brown 73.73% 61.51% 67.07
Test WSJ+Brown 80.93% 71.69% 76.03
Test WSJ Precision Recall F?=1
Overall 81.97% 73.27% 77.37
A0 91.39% 82.23% 86.57
A1 79.80% 76.23% 77.97
A2 68.61% 62.61% 65.47
A3 73.95% 50.87% 60.27
A4 78.65% 68.63% 73.30
A5 75.00% 60.00% 66.67
AM-ADV 61.64% 46.05% 52.71
AM-CAU 76.19% 43.84% 55.65
AM-DIR 53.33% 37.65% 44.14
AM-DIS 80.56% 63.44% 70.98
AM-EXT 100.00% 46.88% 63.83
AM-LOC 64.48% 51.52% 57.27
AM-MNR 62.90% 45.35% 52.70
AM-MOD 98.64% 92.38% 95.41
AM-NEG 98.21% 95.65% 96.92
AM-PNC 56.67% 44.35% 49.76
AM-PRD 0.00% 0.00% 0.00
AM-REC 0.00% 0.00% 0.00
AM-TMP 83.37% 71.94% 77.23
R-A0 94.29% 88.39% 91.24
R-A1 85.93% 74.36% 79.73
R-A2 100.00% 37.50% 54.55
R-A3 0.00% 0.00% 0.00
R-A4 0.00% 0.00% 0.00
R-AM-ADV 0.00% 0.00% 0.00
R-AM-CAU 0.00% 0.00% 0.00
R-AM-EXT 0.00% 0.00% 0.00
R-AM-LOC 90.00% 42.86% 58.06
R-AM-MNR 66.67% 33.33% 44.44
R-AM-TMP 75.00% 40.38% 52.50
V 98.86% 98.86% 98.86
Table 3: Overall results (top) and detailed results on
the WSJ test (bottom).
0086132. Computer time was provided by NSF
ARI Grant #CDA-9601817, NSF MRI Grant #CNS-
0420873, NASA AIST grant #NAG2-1646, DOE
SciDAC grant #DE-FG02-04ER63870, NSF spon-
sorship of the National Center for Atmospheric Re-
search, and a grant from the IBM Shared University
Research (SUR) program.
Special thanks to Matthew Woitaszek, Theron Vo-
ran and the other administrative team of the Hemi-
sphere and Occam Beowulf clusters. Without these
the training would never be possible.
References
Xavier Carreras and Llu??s Ma`rquez. 2005. n Introduction to the CoNLL-2005
Shared Task: Semantic Role Labeling. In Proceedings of CoNLL-2005.
Daniel Gildea and Daniel Jurafsky. 2002. Automatic labeling of semantic roles.
Computational Linguistics, 28(3):245?288.
Kadri Hacioglu and Wayne Ward. 2003. Target word detection and semantic
role chunking using support vector machines. In Proceedings of the Human
Language Technology Conference, Edmonton, Canada.
Kadri Hacioglu, Sameer Pradhan, Wayne Ward, James Martin, and Dan Jurafsky.
2003. Shallow semantic parsing using support vector machines. Technical
Report TR-CSLR-2003-1, Center for Spoken Language Research, Boulder,
Colorado.
Kadri Hacioglu, Sameer Pradhan, Wayne Ward, James Martin, and Daniel Ju-
rafsky. 2004. Semantic role labeling by tagging syntactic chunks. In Pro-
ceedings of the 8th Conference on CoNLL-2004, Shared Task ? Semantic Role
Labeling.
Kadri Hacioglu. 2004. A lightweight semantic chunking model based on tagging.
In Proceedings of the Human Language Technology Conference /North Amer-
ican chapter of the Association of Computational Linguistics (HLT/NAACL),
Boston, MA.
Paul Kingsbury and Martha Palmer. 2002. From Treebank to PropBank. In
Proceedings of the 3rd International Conference on Language Resources and
Evaluation (LREC-2002), Las Palmas, Canary Islands, Spain.
Taku Kudo and Yuji Matsumoto. 2000. Use of support vector learning for chunk
identification. In Proceedings of the 4th Conference on CoNLL-2000 and
LLL-2000, pages 142?144.
Taku Kudo and Yuji Matsumoto. 2001. Chunking with support vector machines.
In Proceedings of the 2nd Meeting of the North American Chapter of the As-
sociation for Computational Linguistics (NAACL-2001).
Sameer Pradhan, Kadri Hacioglu, Wayne Ward, James Martin, and Dan Jurafsky.
2003. Semantic role parsing: Adding semantic structure to unstructured text.
In Proceedings of the International Conference on Data Mining (ICDM 2003),
Melbourne, Florida.
Sameer Pradhan, Wayne Ward, Kadri Hacioglu, James Martin, and Dan Jurafsky.
2004. Shallow semantic parsing using support vector machines. In Proceed-
ings of the Human Language Technology Conference/North American chapter
of the Association of Computational Linguistics (HLT/NAACL), Boston, MA.
Sameer Pradhan, Wayne Ward, Kadri Hacioglu, James Martin, and Dan Jurafsky.
2005. Semantic role labeling using different syntactic views. In Proceedings
of the Association for Computational Linguistics 43rd annual meeting (ACL-
2005), Ann Arbor, MI.
L. A. Ramshaw and M. P. Marcus. 1995. Text chunking using transformation-
based learning. In Proceedings of the Third Annual Workshop on Very Large
Corpora, pages 82?94. ACL.
Mihai Surdeanu, Sanda Harabagiu, John Williams, and Paul Aarseth. 2003. Us-
ing predicate-argument structures for information extraction. In Proceedings
of the 41st Annual Meeting of the Association for Computational Linguistics,
Sapporo, Japan.
Nianwen Xue and Martha Palmer. 2004. Calibrating features for semantic role
labeling. In Proceedings of the Conference on Empirical Methods in Natural
Language Processing, Barcelona, Spain.
220
Proceedings of the Workshop on Textual Entailment and Paraphrasing, pages 28?35,
Prague, June 2007. c?2007 Association for Computational Linguistics
A Corpus of Fine-Grained Entailment Relations
Rodney D. Nielsen and Wayne Ward
Center for Spoken Language Research
Institute of Cognitive Science
Department of Computer Science
University of Colorado, Boulder
Rodney.Nielsen, Wayne.Ward@Colorado.edu
Abstract
This paper describes on-going efforts to an-
notate  a  corpus  of  almost  16000  answer 
pairs with an estimated 69000 fine-grained 
entailment relationships.  We illustrate the 
need for  more detailed classification than 
currently  exists  and  describe  our  corpus 
and annotation scheme.  We discuss early 
statistical  analysis showing substantial  in-
ter-annotator  agreement  even  at  the  fine-
grained level.  The corpus described here, 
which is  the only one providing such de-
tailed annotations,  will  be made available 
as a public resource later this year (2007). 
This is expected to enable application de-
velopment that is currently not practical.
1 Introduction
Determining whether the propositions in one text 
fragment are entailed by those in another fragment 
is important to numerous NLP applications. Con-
sider an intelligent tutoring system (ITS), where it 
is  critical  for  the  tutor  to  assess  which  specific 
facets of  the desired or reference answer are en-
tailed by the student?s answer. Truly effective in-
teraction and pedagogy is only possible if the auto-
mated tutor can assess this entailment at a relative-
ly fine level of detail (c.f. Jordan et al, 2004).
The PASCAL Recognizing Textual Entailment 
(RTE) challenge (Dagan et al, 2005) has brought 
the issue of textual entailment before a broad com-
munity of researchers in a task independent fash-
ion. This task requires systems to make simple yes-
no judgments as to whether a human reading a text 
t of  one  or  more  full  sentences  would  typically 
consider a second, hypothesis, text  h (usually one 
full sentence) to most likely be true.  This paper 
discusses some of the extensions necessary to this 
scheme in order to satisfy the requirements of an 
ITS and provides a preliminary report on our ef-
forts  to  produce  an  annotated  corpus  applying 
some of  these  additions  to  children?s  answers  to 
science questions.  
We first  provide a  brief  overview of the RTE 
challenge  task  and  a  synopsis  of  answer  assess-
ment  technology  within  existing  ITSs  and  large 
scale  assessment  applications.   We  then  detail 
some of the types of changes required in order to 
facilitate more effective pedagogy.  We provide a 
report on our work in this direction and describe a 
corpus we are annotating with fine-grained entail-
ment information.  Finally, we discuss future direc-
tion and the relevance of this annotation scheme to 
other applications such as question answering.
2 Prior Work
2.1 RTE Challenge Task
Example 1 shows a typical  t-h pair from the RTE 
challenge.  The task is to determine whether typi-
cally a reader would say that  h is most likely true 
having read t.  The system output is a simple yes or 
no decision about this entailment ? in this example, 
the decision is no ? and that is similarly the extent 
to which training data is annotated.  There is no in-
dication of whether some facets of, the potentially 
quite long, h are addressed (as they are in this case) 
in  t or conversely, which facets are not discussed 
or are explicitly contradicted.
(1) <t>At an international disas-ter conference in Kobe, Japan, the 
28
U.N. humanitarian chief said the United Nations should take the lead in creating a tsunami early-warning system in the Indian Ocean.</t><h>Nations affected by the Asian tsunami disaster have agreed the UN should begin work on an early warning system in the Indian Ocean.</h>
However, in the third RTE challenge, there is an 
optional pilot task1 that begins to address some of 
these issues.  Specifically, they have extended the 
task  by  including  an  unknown label,  where  h is 
neither entailed nor contradicted, and have request-
ed justification for decisions.  The form that these 
justifications  will  take  has  been  left  up  to  the 
groups  participating,  but  could  conceivably  pro-
vide some of the information about which specific 
facets of the hypothesis are entailed, contradicted 
and unaddressed.
2.2 Existing Answer Assessment Technology
Effective ITSs exist in the laboratory producing 
learning  gains  in  high-school,  college,  and  adult 
subjects through text-based dialog interaction (e.g., 
Graesser et al, 2001; Koedinger et al, 1997; Peters 
et al, 2004, VanLehn et al, 2005).  However, most 
ITSs today provide only a shallow assessment of 
the learner?s comprehension (e.g., a correct versus 
incorrect decision).  Many ITS researchers are stri-
ving  to  provide  more  refined  learner  feedback 
(Aleven et al, 2001; Graesser et al, 2001; Jordan 
et al, 2004; Peters et al, 2004; Roll et al, 2005; 
Ros? et al, 2003).  However, they are developing 
very  domain-dependent  approaches,  requiring  a 
significant investment in hand-crafted logic repre-
sentations,  parsers,  knowledge-based  ontologies, 
and  or  dialog  control  mechanisms.   Simply  put, 
these domain-dependent techniques will not scale 
to the task of developing general purpose ITSs and 
will  never enable the long-term goal of effective 
unconstrained interaction with learners or the peda-
gogy that requires it.
There is also a small, but growing, body of re-
search in the area of scoring free-text responses to 
short answer questions (e.g., Callear et al,  2001; 
Leacock,  2004;  Mitchell  et  al.,  2003;  Pullman, 
2005; Sukkarieh, 2005).  Shaw (2004) and Whit-
tington (1999) provide reviews of some of these 
approaches.  Most of the systems that have been 
implemented and tested are based on Information 
1 http://nlp.stanford.edu/RTE3-pilot/
Extraction  (IE)  techniques  (Cowie  &  Lehnert, 
1996).  They hand-craft a large number of pattern 
rules, directed at detecting the propositions in com-
mon  correct  and  incorrect  answers.   In  general, 
short-answer  free-text  response  scoring  systems 
are designed for large scale assessment tasks, such 
as those associated with the tests administered by 
ETS.  Therefore,  they are  not  designed with the 
goal  of  accommodating  dynamically  generated, 
previously unseen questions.  Similarly, these sys-
tems do not provide feedback regarding the specif-
ic aspects of answers that are correct or incorrect; 
they merely provide a raw score for each question. 
As with the  related work directed specifically  at 
ITSs, these approaches all require in the range of 
100-500 example student answers for each planned 
test question to assist in the creation of IE patterns 
or to train a machine learning algorithm used with-
in some component of their solution.
3 The Necessity of Finer-grained Analysis
Imagine that you are an elementary school science 
tutor and that rather than having access to the stu-
dent?s full response to your questions, you are sim-
ply  given  the  information  that  their  answer  was 
correct or incorrect,  a yes or no entailment deci-
sion.  Assuming the student?s answer was not cor-
rect, what question do you ask next?  What follow 
up question or action is most likely to lead to better 
understanding on the part  of  the child?  Clearly, 
this is a far from ideal scenario, but it is roughly 
the situation within which many ITSs exist today.
In order to optimize learning gains in the tutor-
ing environment, there are myriad issues the tutor 
must  understand  regarding  the  semantics  of  the 
student?s  response.   Here,  we  focus  strictly  on 
drawing inferences regarding the student?s under-
standing  of  the  low-level  concepts  and  relation-
ships or facets of the reference answer.  I use the 
word facet throughout this paper to generically re-
fer to some part of  a text?s meaning.   The most 
common  type  of  answer  facet  discussed  is  the 
meaning  associated  with  a  pair  of  related  words 
and the relation that connects them.
Rather than have a single yes or no entailment 
decision for the reference answer as a whole, (i.e., 
does the student understand the reference answer 
in its entirety or is there some unspecified part of it 
that  we  are  unsure  whether  the  student  under-
stands),  we  instead  break  the  reference  answer 
29
down into what we consider to be its lowest level 
compositional  facets.   This  roughly  translates  to 
the set of triples composed of labeled dependencies 
in  a  dependency  parse  of  the  reference  answer.2 
The following illustrates  how a  simple  reference 
answer (2) is decomposed into the answer facets 
(2a-d)  derived  from  its  dependency  parse  and 
(2a?-d?) provide a gloss of each facet?s meaning. 
As can be seen in 2b and 2c, the dependencies are 
augmented by thematic roles (Kipper et al, 2000) 
(e.g.,  Agent,  Theme,  Cause,  Instrument?)  pro-
duced  by  a  semantic  role  labeling  system  (c.f., 
Gildea and Jurafsky,  2002).   The facets  also  in-
clude  those  semantic  role  relations  that  are  not 
derivable from a typical dependency tree.  For ex-
ample, in the sentence ?As it freezes the water will  
expand and crack the glass?, water is not a modifi-
er of crack in the dependency tree, but it does play 
the role of Agent in a shallow semantic parse.
(2) A long string produces a low pitch.
(2a) NMod(string, long)
(2b) Agent(produces, string)
(2c) Product(produces, pitch)
(2d) NMod(pitch, low)
(2a?) There is a long string.
(2b?) The string is producing some-
thing.
(2c?) A pitch is being produced.
(2d?) The pitch is low.
Breaking the reference answer down into low-
level  facets  provides  the  tutor?s  dialog  manager 
with a much finer-grained assessment of the stu-
dent?s response, but a simple yes or no entailment 
at  the  facet  level  still  lacks semantic  expressive-
ness with regard to the relation between the studen-
t?s answer and the facet in question.  Did the stu-
dent contradict the facet?  Did they express a relat-
ed  concept  that  indicates  a  misconception?   Did 
they leave the facet unaddressed?  Can you assume 
that they understand the facet even though they did 
not express it, since it was part of the information 
given in the question?  It is clear that, in addition to 
2 The goal of most English dependency parsers is to pro-
duce a single projective tree structure for each sentence, 
where each node represents a word in the sentence, each 
link represents a functional category relation, usually la-
beled, between a governor (head) and a subordinate 
(modifier), and each node has a single governor (c.f., 
Nivre and Scholz, 2004).
breaking  the  reference  answer  into  fine-grained 
facets, it is also necessary to break the annotation 
into finer levels in order to specify more clearly the 
relationship between the student?s answer and the 
reference answer aspect.
There  are  many  other  issues  that  the  system 
must know to achieve near optimal tutoring, some 
of which are mentioned later in the discussion sec-
tion, but these two ? breaking the reference answer 
into fine-grained facets and utilizing more expres-
sive annotation labels ? are the emphasis of this ef-
fort.
4 Current Annotation Efforts
This section describes our current efforts in anno-
tating  a  corpus  of  answers  to  science  questions 
from elementary school students. 
4.1 Corpus
Lacking data from a real tutoring situation, we ac-
quired data gathered from 3rd-6th grade students in 
schools utilizing the Full  Option Science System 
(FOSS).  Assessment is a major FOSS research fo-
cus,  of  which  the  Assessing  Science  Knowledge 
project  is  a  key component.3  The FOSS project 
has developed sixteen science teaching and learn-
ing modules targeted at  grades 3-6,  as shown in 
Table 1.   The ASK project created assessments for 
each of these modules, including multiple choice, 
fill  in  the  blank,  free  response,  and  somewhat 
lengthy  experimental  design  questions.   We  re-
viewed these questions and selected about 290 free 
response questions that were in line with the objec-
tives  of  this  research project,  specifically  we se-
lected questions whose expected responses ranged 
in length from moderately short verb phrases to a 
few sentences, that could be assessed objectively, 
and that were not too open ended.  Table 2 shows a 
3 ?FOSS is a research-based science program for grades 
K?8 developed at the Lawrence Hall of Science, Uni-
versity of California at Berkeley with support from the 
National Science Foundation and published by Delta 
Education.  FOSS is also an ongoing research project 
dedicated to improving the learning and teaching of sci-
ence.?
Assessing Science Knowledge (ASK) is ?designed to 
define, field test, and validate effective assessment tools 
and techniques to be used by grade 3?6 classroom 
teachers to assess, guide, and confirm student learning 
in science.?
http://www.lawrencehallofscience.org/foss/
30
Grade Life Science Physical Science and 
Technology
Earth and Space 
Science
Scientific Reasoning 
and Technology
3-4 HB: Human Body
ST: Structure of Life 
ME: Magnetism & Electricity
PS: Physics of Sound 
WA: Water
EM: Earth Materials 
II: Ideas & Inventions
MS: Measurement 
5-6 FN: Food & Nutrition
EV: Environments
LP: Levers & Pulleys
MX: Mixtures & Solutions
SE: Solar Energy
LF: Landforms
MD: Models & Designs
VB: Variables
1Table 1 FOSS / ASK Learning and Assessment Modules by Area and Grade
HB Q: Dancers need to be able to point their feet. The tibialis is the major muscle on the front of the leg 
and the gastrocnemius is the major muscle on the back of the leg. Describe how the muscles in the 
front and back of the leg work together to make the dancer?s foot point.
R: The muscle in the back of the leg (the gastrocnemius) contracts and the muscle in the front of the 
leg (the tibialis) relaxes to make the foot point.
A: The back muscle and the front muscle stretch to help each other pull up the foot.
ST Q: Why is it important to have more than one shelter in a crayfish habitat with several crayfish?
R: Crayfish are territorial and will protect their territory. The shelters give them places to hide from 
other crayfish. [Crayfish prefer the dark and the shelters provide darkness.]
A: So all the crayfish have room to hide and so they do not fight over them.
ME Q: Lee has an object he wants to test to see if it is an insulator or a conductor. He is going to use the 
circuit you see in the picture. Explain how he can use the circuit to test the object.
R: He should put one of the loose wires on one part of the object and the other loose wire on another 
part of the object (and see if it completes the circuit).
A: You can touch one wire on one end and the other on the other side to see if it will run or not.
PS Q: Kate said: ?An object has to move to produce sound.?  Do you agree with her?   Why or why not?
R: Agree. Vibrations are movements and vibrations produce sound.
A: I agree with Kate because if you talk in a tube it produce sound in a long tone.  And it vibrations 
and make sound.
WA Q: Anna spilled half of her cup of water on the kitchen floor. The other half was still in the cup. When 
she came back hours later, all of the water on the floor had evaporated but most of the water in the 
cup was still there. (Anna knew that no one had wiped up the water on the floor.)  Explain to Anna 
why the water on the floor had all evaporated but most of the water in the cup had not.
R: The water on the floor had a much larger surface area than the water in the cup.
A: Well Anna, in science, I learned that when water is in a more open are, then water evaporates faster. 
So, since tile and floor don't have any boundaries or wall covering the outside, the water on the 
floor evaporated faster, but since the water in the cup has boundaries, the water in the cup didn't 
evaporate as fast.
EM Q: You can tell if a rock contains calcite by putting it into a cold acid (like vinegar). 
Describe what you would observe if you did the acid test on a rock that contains this substance.
R: Many tiny bubbles will rise from the calcite when it comes into contact with cold acid.
A: You would observe if it was fizzing because calcite has a strong reaction to vinegar.
Table 2 Sample Qs from FOSS-ASK with their reference (R) and an example student answer (A).
few questions that are representative of those se-
lected for inclusion in the corpus, along with their 
reference answers and an example student answer 
for  each.   Questions  without  at  least  one  verb 
phrase were rejected because they were assumed to 
be  more  trivial  and  less  interesting  from the  re-
search  perspective.   Examples  of  such  questions 
along with their reference answers and an example 
student response include: Q:  Besides air, what (if  
anything)  can  sound  travel  through? Reference 
Answer: Sound can also travel through liquids and 
solids.  (Also  other  gases.) Student  Answer:  A 
screen door.  Q: Name a property of the sound of a  
fire engine?s siren. Reference Answer:  The sound 
is very loud. OR The sound changes in pitch. Stu-
dent Answer: Annoying.  An example of a free re-
sponse item that was dropped because it was too 
open ended is: Design an investigation to find out  
a plant?s range of tolerance for number of hours of  
sunlight per day. You can use drawings to help ex-
plain your design.
We generated a corpus from a random sample of 
the kids? handwritten responses to these questions. 
The only special transcription instructions were to 
31
fix spelling errors (since these would be irrelevant 
in a spoken dialog environment), but not grammat-
ical errors (which would still be relevant), and to 
skip blank answers and non-answers similar in na-
ture to I don?t know (since these are not particular-
ly interesting from the research perspective).
Three modules were designated as  the test  set 
(Environments, Human Body, and Water) and the 
remaining 13 modules  will  be  used for  develop-
ment and training of  classification systems.   We 
judged the three test set modules to be representa-
tive of the entire corpus in terms of difficulty and 
appropriateness for the types of questions that met 
our research interests.  We transcribed the respons-
es of approximately 40 randomly selected students 
for each question in the training set and 100 ran-
domly selected students for  each question in  the 
test set.  In order to maximize the diversity of lan-
guage and knowledge represented by the training 
and test datasets, random selection of students was 
performed at the question level  rather than using 
the same students? answers for all of the questions 
in a given module.  However, in total there were 
only about 200 children that participated in any in-
dividual  science  module  assessment,  so  there  is 
still  moderate  overlap  in  the  students  from  one 
question to another within a given module.  On the 
other hand, each assessment module was given to a 
different group of kids, so there is no overlap in 
students  between modules.   There  are  almost  60 
questions and 5700 student answers in the test set, 
comprising approximately 20% of all of the ques-
tions utilized and 36% of the total number of tran-
scribed student responses.  In total, including test 
and training datasets, there are nearly 16000 stu-
dent responses.
4.2 Annotation
The answer assessment annotation described in this 
paper is intended to be a step toward specifying the 
detailed semantic understanding of a student?s an-
swer that is required for an ITS to interact effec-
tively with a learner.  With that goal in mind, anno-
tators were asked to consider and annotate accord-
ing to what they would want to know about the stu-
dent?s answer if they were the tutor (but a tutor that 
for some reason could not understand the unstruc-
tured text of the student?s answer).  The key excep-
tion here is that we are only annotating a student?s 
answer in terms of whether or not it accurately and 
completely  addresses  the  facets  of  the  reference 
(desired or correct) answer.  So, if the student also 
discusses concepts not addressed in the reference 
answer, we will not annotate those points regard-
less of their quality or accuracy.
Each reference answer in the corpus is decom-
posed into its constituent facets.  Then each student 
answer is annotated relative to the facets in the cor-
responding reference answer.  As described earlier, 
the reference answer facets are roughly extracted 
from the relations in a syntactic dependency parse 
(c.f.,  Nivre and Scholz,  2004) and a shallow se-
mantic parse (Gildea and Jurafsky, 2002).  These 
are modified slightly to either eliminate most func-
tion words or incorporate them into the relation la-
bels (c.f., Lin and Pantel, 2001).  Example 3 illus-
trates the decomposition of one of the reference an-
swers  into  its  constituent  parts  along  with  their 
glosses.
(3) The string is tighter, so the 
pitch is higher.
(3a)  Is(string, tighter)
(3a?) The string is tighter.
(3b)  Is(pitch, higher)
(3b?) The pitch is higher.
(3c)  Cause(3b, 3a)
(3c?) 3b is caused by 3a
The annotation  tool  lists  the  reference  answer 
facets that students are expected to address.  Both a 
formal  relational  representation  and  an  English-
like gloss of the facet are displayed in a table, one 
row per facet.  The annotator?s job is to label each 
of those facets to indicate the extent to which the 
student addressed it.  We settled on the eight anno-
tation  labels  noted  in  Table  3.   Descriptions  of 
where each annotation label applies and some of 
the most common annotation issues were detailed 
with  several  examples  in  the  guidelines  and  are 
only very briefly summarized in the remainder of 
this subsection.
Example 4 shows a student answer correspond-
ing to  the  reference answer  in  example  3,  along 
with its initial annotation in 4a-c and its final anno-
tation in 4a?-c?.  It is assumed that the student un-
derstands that the pitch is higher (facet 4b), since 
this  is  given in the question (? Write a note to  
David to tell him why the pitch gets higher rather  
than lower) and similarly it is assumed that the stu-
dent will be explaining what has the causal effect 
of producing this higher pitch (facet 4c).  There-
fore, these facets are initialized to Assumed by the 
32
system.  Since the student does not contradict the 
fact that the string is tighter (the string can be both 
longer and tighter),  we do not label this facet  as 
Contradicted.  If the student?s response did not 
mention anything about either the string or tight-
ness,  we  would  annotate  facet  4a  as  Unad-
dressed.   However,  the  student  did  discuss  a 
property of the string, the string is long, producing 
the  facet  Is(string, long).   This  parallels  the 
reference answer facet Is(string, tighter) with 
the exception of a different argument to the Is re-
lation, resulting in the annotation Diff-Arg.  This 
indicates to the tutor that the student expressed a 
related concept, but one which neither implies that 
they understand the reference answer facet nor that 
they explicitly hold a contradictory belief.  Often, 
this indicates that the student has a misconception. 
For example, when asked about an effect on pitch, 
many students say things like the pitch gets louder, 
rather than higher or lower, which implies a mis-
conception involving their understanding of pitch 
and volume.  In this case, the Diff-Arg label can 
help focus the tutor on correcting this misconcep-
tion.  Facet 4c expressing the causal relation be-
tween 4a and 4b is labeled  Expressed, since the 
student did express a causal relation between the 
concepts aligned with 4a and 4c.  The tutor then 
knows that the student was on track in regard to at-
tempting to express the desired causal relation and 
the tutor need only deal with the fact that the cause 
given was incorrect.  
Table 3 Facet Annotation Labels
(4) David this is why because you 
don't listen to your teacher. If the 
string is long, the pitch will be 
high.
(4a) Is(string, tighter), ---
(4b) Is(pitch, higher), Assumed
(4c) Cause(4b, 4a), Assumed
(4a?) Is(string, tighter), Diff-Arg
(4b?) Is(pitch, higher), Expressed
(4c?) Cause(4b, 4a), Expressed
The  Self-Contra annotation is used in cases 
like the response in example 5, where the student 
simultaneously expresses the contradictory notions 
that the string is tighter and that there is less ten-
sion.
(5) The string is tighter, so there is 
less tension so the pitch gets higher.
(5a) Is(string, tighter), Self-Contra
There is no compelling reason from the perspec-
tive of the automated tutoring system to differenti-
ate  between  Expressed and  Inferred facets, 
since in either case the tutor can assume that the 
student understands the concepts involved.  How-
ever,  from  the  systems  development  perspective 
there are three primary reasons for differentiating 
between these facets and similarly between facets 
that are contradicted by inference versus more ex-
plicit expression.  The first reason is that most sta-
tistical  machine  learning  systems  today  cannot 
hope to detect very many pragmatic inferences and 
including these in the training data is likely to con-
fuse the algorithm resulting in worse performance. 
Having separate labels allows one to remove the 
more  difficult  inferences  from the  training  data, 
thus eliminating this problem.  The second ratio-
nale is that systems hoping to handle both types of 
inference might more easily learn to discriminate 
between these opposing classifications if the class-
es are distinguished (for algorithms where this is 
not the case, the classes can easily be combined au-
tomatically).  Similarly, this allows the possibility 
of training separate classifiers to handle the differ-
ent forms of inference.  The third reason for sepa-
rate labels is  that it  facilitates system evaluation, 
including  the  comparison  of  various  techniques 
and the effect of individual features.
Example 6 illustrates an example  of  a student 
answer with the label Inferred.  In this case, the 
decision  requires  pragmatic  inferences,  applying 
the Gricean maxims of Relation, be relevant ? why 
Expressed: Any facet directly expressed or inferred 
by simple reasoning
Inferred: Facets inferred by pragmatics or nontrivial 
logical reasoning
Contra-Expr: Facets directly contradicted by nega-
tion, antonymous expressions and their paraphrases
Contra-Infr:  Facets  contradicted  by  pragmatics  or 
complex reasoning
Self-Contra:  Facets  that  are  both contradicted and 
implied (self contradictions)
Diff-Arg: The core relation is expressed, but it has a 
different modifier or argument
Assumed:  The  system assigns  this  label,  which  is 
changed if any of the above labels apply
Unaddressed: Facets that are not addressed at all by 
the student?s answer
33
would the student  mention vibrations  if  they did 
not  know they were  a  form of  movement  ?  and 
Quantity, do not make your contribution more in-
formative than is required (Grice, 1975).
(6) Q: Kate said: ?An object has to move to produce sound.? Do you agree with her? Why or why not?Ref Ans: ?Agree. Vibrations are move-ments and vibrations produce sound.?Student Answer: Yes because it has to vibrate to make sounds.
(6b) Is(vibration, movement), Inferred
Annotators are primarily students of Education 
and Linguistics and require moderate training on 
the annotation task.  The annotated reference an-
swers are stored in a stand-off markup in xml files, 
including an annotated element for each reference 
answer facet.
4.3 Inter-Annotator Agreement Results
The results reported here are preliminary, based on 
the first two annotators, and must be viewed under 
the light that we have not yet completed annotator 
training.   We  report  results  under  three  label 
groupings: (1)  All-Labels,  where all  labels are 
left  separate,  (2)  Tutor-Labels,  where  Ex-
pressed,  Inferred and  Assumed are combined 
as are  Contra-Expr and  Contra-Infr,  and (3) 
Yes-No, which is a two-way division, Expressed, 
Inferred and Assumed versus all other labels.  
Agreement  on  Tutor-Labels indicates  the 
benefit to the tutor, since it is relatively unimpor-
tant to differentiate between the types of inference 
required  in  determining  that  the  student  under-
stands a reference answer facet (or has contradict-
ed it).  We evaluated mid-training inter-annotator 
agreement  on  a  random selection  of  15  answers 
from each of 14 Physics of Sound questions, total-
ing 210 answers  and 915 total  facet  annotations. 
Mid-training agreement on the  Tutor-Labels is 
87.4%, with a Kappa statistic of 0.717 correspond-
ing with substantial agreement (Cohen, 1960).  In-
ter-annotator  agreement  at  mid-training  is  81.1% 
on All-Labels and 90.1% on the binary Yes-No 
decision.  These also have Kappa statistics in the 
range of substantial agreement.  
The distribution of the 915 annotations is shown 
in Table 4.  It is somewhat surprising that this sci-
ence module had so few contradictions, just 2.7% 
of all annotations, particularly given that many of 
the questions seem more likely to draw contradic-
tions than unaddressed facets (e.g., many ask about 
the effect on pitch and volume, typically eliciting 
one of two possible responses).  An analysis of the 
inter-annotator confusion matrix indicates that the 
most probable disagreement is between Inferred 
and  Unaddressed.   The second most likely dis-
agreement is  between  Assumed and  Expressed. 
In discussing disagreements, the annotators almost 
always  agree  quickly,  reinforcing  our  belief  that 
we will increase agreement significantly with addi-
tional training.
Label Count % Count %
Expressed 348 38.0
Inferred 51 5.6
Assumed 258 28.2
657 71.8
Contra-Expr 21 2.3
Contra-Infr 4 0.4 25 2.7
Self-Contra 1 0.1 1 0.1
Diff-Arg 33 3.6 33 3.6
Unaddressed 199 21.7 199 21.7
Table 4 Distribution of classifications (915 facets)
5 Discussion and Future Work
The goal of our fine-grained classification is to en-
able  more  effective  tutoring  dialog  management. 
The  additional  labels  facilitate  understanding  the 
type  of  mismatch  between  the  reference  answer 
and the student?s answer.  Breaking the reference 
answer down into low-level facets enables the tutor 
to provide feedback relevant specifically to the ap-
propriate  facet  of  the  reference  answer.   In  the 
question answering domain, this facet-based classi-
fication would allow systems to accumulate entail-
ing  evidence  from  a  variety  of  corroborating 
sources and incorporate answer details that might 
not be found in any single sentence.  In other appli-
cations outside  of  the tutoring domain,  this  fine-
grained classification can also facilitate more di-
rected user feedback.  For example, both the addi-
tional classifications and the break down of facets 
can be used to justify system decisions, which is 
the stated goal of the pilot task at the third RTE 
challenge.  
The corpus described in this paper, which will 
be released later this year (2007), represents a sub-
stantial contribution to the entailment community, 
including an estimated 69000 facet entailment an-
notations.  By contrast, three years of RTE chal-
lenge  data  comprise  fewer  than  4600 entailment 
34
annotations.   More  importantly,  this  is  the  only 
corpus that provides entailment information at the 
fine-grained  level  described  in  this  paper.   This 
will enable application development that was not 
practical previously.
Future work includes training machine learning 
algorithms to perform the classifications described 
in this paper.  We also plan to annotate other as-
pects of the students? understanding that are not di-
rect  inferences  of  reference  answer  knowledge. 
Consider example (4), in addition to the issues al-
ready annotated, the student  contradicts  a law of 
physics  that  they  have  surely  encountered  else-
where  in  the  text,  specifically that  longer strings 
produce lower, not higher, pitches.  Under the cur-
rent annotation scheme this is not annotated, since 
it does not pertain directly to the reference answer 
which has to do with the effect of string tension. 
In other annotation plans, it would be very useful 
for training learning algorithms if we provide an 
indication of which student answer facets played a 
role in making the inferences classified.
Initial  inter-annotator  agreement  results  look 
promising, obtaining substantial agreement accord-
ing to the Kappa statistic.  We will continue to re-
fine our annotation guidelines and provide further 
training in order to push the agreement higher on 
all classifications.  
Acknowledgement
We would like to thank Martha Palmer for valu-
able advice on this annotation effort.
References
Aleven V, Popescu O, & Koedinger K. (2001) A tutorial 
dialogue system with knowledge-based understanding 
and classification of student explanations.  IJCAI WS 
knowledge & reasoning in practical dialogue systems
Callear, D., Jerrams-Smith, J., and Soh, V. (2001). CAA 
of short non-MCQ answers. In 5th Intl CAA.
Cohen J. (1960). A coefficient of agreement for nominal 
scales. Educational & Psych Measurement. 20:37-46.
Cowie,  J.,  Lehnert,  W.G.  (1996).  Information  Extrac-
tion. In Communications of the ACM, 39(1), 80-91.
Dagan,  Ido,  Glickman,  Oren,  and Magnini,  Bernardo. 
(2005).  The  PASCAL  Recognizing  Textual  Entail-
ment Challenge.  In 1st RTE Challenge Workshop.
Gildea, D. & Jurafsky, D. (2002). Automatic labeling of 
semantic roles. Computational Linguistics, 28:3, 245?288.
Graesser, A.C., Hu, X., Susarla, S., Harter, D., Person, 
N.K., Louwerse, M., and Olde, B. (2001). AutoTutor: 
An  Intelligent  Tutor  and  Conversational  Tutoring 
Scaffold. In 10th ICAI in Education, 47-49.
Grice,  H.  Paul.  (1975).  Logic  and  conversation.  In  P 
Cole and J Morgan, editors,  Syntax and Semantics,  
Vol 3, Speech Acts, 43?58. Academic Press.
Jordan, P. W., Makatchev, M., & VanLehn, K. (2004). 
Combining  competing  language  understanding  ap-
proaches in an intelligent tutoring system. In 7th ITS.
Kipper, K, Dang, H, & Palmer, M. (2000). Class-Based 
Construction of a Verb Lexicon. AAAI 17th NCAI
Koedinger,  K.R.,  Anderson,  J.R.,  Hadley,  W.H.  & 
Mark,  M.A.  (1997).   Intelligent  tutoring  goes  to 
school in the big city.  Intl Jrnl of AI in Ed, 8, 30-43.
Leacock,  Claudia.  (2004).  Scoring free-response auto-
matically: A case study of a large-scale Assessment. 
Examens, 1(3).
Lin, Dekang and Pantel, Patrick. (2001). Discovery of 
inference rules for Question Answering. In  Natural 
Language Engineering, 7(4):343-360.
Mitchell,  T.  Aldridge, N.,  and Broomhead, P.  (2003). 
Computerized marking of  short-answer  free-text  re-
sponses. In 29th IAEA.
Nivre, J. and Scholz, M. (2004). Deterministic Depen-
dency Parsing of English Text. In Proc COLING.
Peters,  S,  Bratt,  E.O.,  Clark,  B.,  Pon-Barry,  H.  and 
Schultz, K. (2004). Intelligent  Systems for Training 
Damage Control Assistants. In Proc. of ITSE.
Pulman S.G. & Sukkarieh J.Z. (2005). Automatic Short 
Answer Marking. ACL WS Bldg Ed Apps using NLP.
Roll, I, Baker, R, Aleven, V, McLaren, B, & Koedinger, 
K. (2005).  Modeling Students? Metacognitive Errors 
in Two Intelligent Tutoring Systems.  In UM 379?388
Ros?, P. Roque, A., Bhembe, D. & VanLehn, K. (2003). 
A hybrid text classification approach for analysis of 
student essays. In Bldg Ed Apps using NLP
Shaw, Stuart. (2004). Automated writing assessment: a 
review of four conceptual models. In Research Notes,  
Cambridge ESOL.  Downloaded Aug 10, 2005 from 
http://www.cambridgeesol.org/rs_notes/rs_nts17.pdf
Sukkarieh, J. & Pulman, S. (2005). Information extrac-
tion and machine learning: Auto-marking short free 
text responses to science questions. In Proc of AIED.
VanLehn,  K.,  Lynch,  C.,  Schulze,  K.  Shapiro,  J.  A., 
Shelby, R., Taylor, L., Treacy, D., Weinstein, A., & 
Wintersgill,  M.  (2005).  The Andes physics tutoring 
system: Five years of evaluations.  In 12th ICAI in Ed
Whittington,  D.,  Hunt,  H.  (1999).  Approaches  to  the 
Computerised  Assessment  of  Free-Text  Responses. 
Third ICAA.
35
Proceedings of ACL-08: HLT, Short Papers (Companion Volume), pages 241?244,
Columbus, Ohio, USA, June 2008. c?2008 Association for Computational Linguistics
Extracting a Representation from Text for Semantic Analysis  Rodney D. Nielsen1,2, Wayne Ward1,2, James H. Martin1, and Martha Palmer1 1 Center for Computational Language and Education Research, University of Colorado, Boulder 2 Boulder Language Technologies, 2960 Center Green Ct., Boulder, CO 80301 Rodney.Nielsen, Wayne.Ward, James.Martin, Martha.Palmer@Colorado.edu       Abstract We present a novel fine-grained semantic rep-resentation of text and an approach to con-structing it. This representation is largely extractable by today?s technologies and facili-tates more detailed semantic analysis. We dis-cuss the requirements driving the representation, suggest how it might be of value in the automated tutoring domain, and provide evidence of its validity. 1 Introduction This paper presents a new semantic representation intended to allow more detailed assessment of stu-dent responses to questions from an intelligent tu-toring system (ITS). Assessment within current ITSs generally provides little more than an indica-tion that the student?s response expressed the target knowledge or it did not. Furthermore, virtually all ITSs are developed in a very domain-specific way, with each new question requiring the handcrafting of new semantic extraction frames, parsers, logic representations, or knowledge-based ontologies (c.f., Jordan et al, 2004). This is also true of re-search in the area of scoring constructed response questions (e.g., Leacock, 2004). The goal of the representation described here is to facilitate domain-independent assessment of student responses to questions in the context of a known reference answer and to perform this as-sessment at a level of detail that will enable more effective ITS dialog. We have two key criteria for this representation: 1) it must be at a level that fa-cilitates detailed assessment of the learner?s under-standing, indicating exactly where and in what manner the answer did not meet expectations and 
2) the representation and assessment should be learnable by an automated system ? they should not require the handcrafting of domain-specific representations of any kind.  Rather than have a single expressed versus un-expressed assessment of the reference answer as a whole, we instead break the reference answer down into what we consider to be approximately its lowest level compositional facets. This roughly translates to the set of triples composed of labeled (typed) dependencies in a dependency parse of the reference answer. Breaking the reference answer down into fine-grained facets permits a more fo-cused assessment of the student?s response, but a simple yes or no entailment at the facet level still lacks semantic expressiveness with regard to the relation between the student?s answer and the facet in question, (e.g., did the student contradict the facet or completely fail to address it?) Therefore, it is also necessary to break the annotation labels into finer levels in order to specify more clearly the relationship between the student?s answer and the reference answer facet. The emphasis of this paper is on this fine-grained facet-based representation ? considerations in defining it, the process of extract-ing it, and the benefit of using it. 2 Representing the Target Knowledge We acquired grade 3-6 responses to 287 questions from the Assessing Science Knowledge (ASK) project (Lawrence Hall of Science, 2006). The re-sponses, which range in length from moderately short verb phrases to several sentences, cover all 16 diverse Full Option Science System teaching and learning modules spanning life science, physi-cal science, earth and space science, scientific rea-soning, and technology. We generated a corpus by transcribing a random sample (approx. 15400) of the students? handwritten responses. 
241
2.1 Knowledge Representation The ASK assessments included a reference answer for each constructed response question. These ref-erence answers were manually decomposed into fine-grained facets, roughly extracted from the re-lations in a syntactic dependency parse and a shal-low semantic parse. The decomposition is based closely on these well-established frameworks, since the representations have been shown to be learnable by automatic systems (c.f., Gildea and Jurafsky, 2002; Nivre et al, 2006). Figure 1 illustrates the process of deriving the constituent facets that comprise the representation of the final reference answer. We begin by deter-mining the dependency parse following the style of MaltParser (Nivre et al, 2006). This dependency parse was then modified in several ways. The ra-tionale for the modifications, which we elaborate below, is to increase the semantic content of facets. These more expressive facets are used later to gen-erate features for the assessment classification task. These types of modifications to the parser output address known limitations of current statistical parser outputs, and are reminiscent of the modifi-cations advocated by Briscoe and Carroll for more effective parser evaluation, (Briscoe, et. al, 2002). Example 1 illustrates the reference answer facets derived from the final dependencies in Figure 1, along with their glosses.  Figure 1. Reference answer representation revisions (1) The brass ring would not stick to the nail because the ring is not iron. (1a)  NMod(ring, brass)  (1a?) The ring is brass. (1b)  Theme_not(stick, ring) (1b?) The ring does not stick. (1c)  Destination_to_not(stick, nail) (1c?) Something does not stick to the nail. (1d)  Be_not(ring, iron) (1d?) The ring is not iron. (1e)  Cause_because(1b-c, 1d) (1e?) 1b and 1c are caused by 1d. Various linguistic theories take a different stance on what term should be the governor in a 
number of phrase types, particularly noun phrases. In this regard, the manual parses here varied from the style of MaltParser by raising lexical items to governor status when they contextually carried more significant semantics. In our example, the verb stick is made the governor of would, whose modifiers are reattached to stick. Similarly, the noun phrases the pattern of pigments and the bunch of leaves typically result in identical dependency parses. However, the word pattern is considered the governor of pigments; whereas, conversely the word leaves is treated as the governor of bunch because it carries more semantics. Then, terms that were not crucial to the student answer, frequently auxiliary verbs, were removed (e.g., the modal would and determiners in our example). Next, we incorporate prepositions into the de-pendency type labels following (Lin and Pantel, 2001). This results in the two dependencies vmod(stick, to) and pmod(to, nail), each of which carries little semantic value over its key lexical item, stick and nail, being combined into the sin-gle, more expressive dependency vmod_to(stick, nail), ultimately vmod is replaced with destination, as described below. Likewise, the dependencies connected by because are consolidated and be-cause is integrated into the new dependency type.  Next, copulas and a few similar verbs are also incorporated into the dependency types. The verb?s predicate is reattached to its subject, which be-comes the governor, and the dependency is labeled with the verb?s root. In our example, the two se-mantically impoverished dependencies sub(is, ring) and prd(is, iron) are combined to form the more meaningful dependency be(ring, iron). Then terms of negation are similarly incorporated into the dependency types. Finally, wherever a shallow semantic parse would identify a predicate argument structure, we used the thematic role labels in VerbNet (Kipper et al, 2000) between the predicate and the argu-ment?s headword, rather than the MaltParser de-pendency tags. This also involved adding new structural dependencies that a typical dependency parser would not generate. For example, in the sen-tence As it freezes the water will expand and crack the glass, typically the dependency between crack and its subject water is not generated since it would lead to a non-projective tree, but it does play the role of Agent in a semantic parse. In a small number of instances, these labels were also at-
242
tached to noun modifiers, most notably the Loca-tion label. For example, given the reference answer fragment The water on the floor had a much larger surface area, one of the facets extracted was Loca-tion_on(water, floor). We refer to facets that express relations between higher-level propositions as inter-propositional facets. An example of such a facet is (1e) above, connecting the proposition the brass ring did not stick to the nail to the proposition the ring is not iron. In addition to specifying the headwords of inter-propositional facets (stick and is, in 1e), we also note up to two key facets from each of the propositions that the relation is connecting (b, c, and d in example 1). Reference answer facets that are assumed to be understood by the learner a pri-ori, (e.g., because they are part of the question), are also annotated to indicate this.  There were a total of 2878 reference answer fac-ets, resulting in a mean of 10 facets per answer (median 8). Facets that were assumed to be under-stood a priori by students accounted for 33% of all facets and inter-propositional facets accounted for 11%. The results of automated annotation of stu-dent answers (section 3) focus on the facets that are not assumed to be understood a priori (67% of all facets); of these, 12% are inter-propositional.  A total of 36 different facet relation types were utilized. The majority, 21, are VerbNet thematic roles. Direction, Manner, and Purpose are Prop-Bank adjunctive argument labels (Palmer et al, 2005). Quantifier, Means, Cause-to-Know and copulas were added to the preceding roles. Finally, anything that did not fit into the above categories retained its dependency parse type: VMod (Verb Modifier), NMod (Noun Modifier), AMod (Adjec-tive or Adverb Modifier), and Root (Root was used when a single word in the answer, typically yes, no, agree, disagree, A-D, etc., stood alone without a significant relation to the remainder of the refer-ence answer; this occurred only 21 times, account-ing for fewer than 1% of the reference answer facets). The seven highest frequency relations are NMod, Theme, Cause, Be, Patient, AMod, and Location, which together account for 70% of the reference answer facet relations 2.2 Student Answer Annotation For each student answer, we annotated each reference answer facet to indicate whether and how 
the student addressed that facet. We settled on the five annotation categories in Table 1. These labels and the annotation process are detailed in (Nielsen et al, 2008b).  Understood: Reference answer facets directly ex-pressed or whose understanding is inferred Contradiction: Reference answer facets contradicted by negation, antonymous expressions, pragmatics, etc. Self-Contra: Reference answer facets that are both con-tradicted and implied (self contradictions) Diff-Arg: Reference answer facets whose core relation is expressed, but it has a different modifier or argument Unaddressed: Reference answer facets that are not ad-dressed at all by the student?s answer Table 1. Facet Annotation Labels 3 Automated Classification As partial validation of this knowledge representa-tion, we present results of an automatic assessment of our student answers. We start with the hand generated reference answer facets. We generate automatic parses for the reference answers and the student answers and automatically modify these parses to match our desired representation. Then for each reference answer facet, we extract features indicative of the student?s understanding of that facet. Finally, we train a machine learning classi-fier on training data and use it to classify unseen test examples, assigning a Table 1 label for each reference answer facet. We used a variety of linguistic features that as-sess the facets? similarity via lexical entailment probabilities following (Glickman et al, 2005), part of speech tags and lexical stem matches. They include information extracted from modified de-pendency parses such as relevant relation types and path edit distances. Revised dependency parses are used to align the terms and facet-level information for feature extraction. Remaining details can be found in (Nielsen et al, 2008a) and are not central to the semantic representation focus of this paper. Current classification accuracy, assigning a Table 1 label to each reference answer facet to indicate the student?s expressed understanding, is 79% within domain (assessing unseen answers to ques-tions associated with the training data) and 69% out of domain (assessing answers to questions re-garding entirely different science subjects). These results are 26% and 15% over the majority class baselines, respectively, and 21% and 6% over lexi-
243
cal entailment baselines based on Glickman et al (2005). 4 Discussion and Future Work Analyzing the results of reference facet extraction, there are many interesting open linguistic issues in this area. This includes the need for a more sophisticated treatment of adjectives, conjunctions, plurals and quantifiers, all of which are known to be beyond the abilities of state of the art parsers. Analyzing the dependency parses of 51 of the student answers, about 24% had errors that could easily lead to problems in assessment. Over half of these errors resulted from inopportune sentence segmentation due to run-on student sentences con-joined by and (e.g., the parse of a shorter string makes a higher pitch and a longer string makes a lower pitch, errantly conjoined a higher pitch and a longer string as the subject of makes a lower pitch, leaving a shorter string makes without an object). We are working on approaches to mitigate this problem.  In the long term, when the ITS generates its own questions and reference answers, the system will have to construct its own reference answer facets. The automatic construction of reference answer facets must deal with all of the issues described in this paper and is a significant area of future research. Other key areas of future research involve integrating the representation described here into an ITS and evaluating its impact. 5 Conclusion We presented a novel fine-grained semantic repre-sentation and evaluated it in the context of auto-mated tutoring. A significant contribution of this representation is that it will facilitate more precise tutor feedback, targeted to the specific facet of the reference answer and pertaining to the specific level of understanding expressed by the student. This representation could also be useful in areas such as question answering or document summari-zation, where a series of entailed facets could be composed to form a full answer or summary. The representation?s validity is partially demon-strated in the ability of annotators to reliably anno-tate inferences at this facet level, achieving substantial agreement (86%, Kappa=0.72) and by promising results in automatic assessment of stu-
dent answers at this facet level (up to 26% over baseline), particularly given that, in addition to the manual reference answer facet representation, an automatically extracted approximation of the rep-resentation was a key factor in the features utilized by the classifier.  The domain independent approach described here enables systems that can easily scale up to new content and learning environments, avoiding the need for lesson planners or technologists to create extensive new rules or classifiers for each new question the system must handle. This is an obligatory first step to the long-term goal of creat-ing ITSs that can truly engage children in natural unrestricted dialog, such as is required to perform high quality student directed Socratic tutoring. Acknowledgments This work was partially funded by Award Number 0551723 from the National Science Foundation. References  Briscoe, E., Carroll, J., Graham, J., and Copestake, A. 2002. Relational evaluation schemes. In Proc. of the Beyond PARSEVAL Workshop at LREC. Gildea, D. and Jurafsky, D. 2002. Automatic labeling of semantic roles. Computational Linguistics. Glickman, O, Dagan, I, and Koppel, M. 2005. Web Based Probabilistic Textual Entailment. In Proc RTE. Jordan, P, Makatchev, M, VanLehn, K. 2004. Combin-ing competing language understanding approaches in an intelligent tutoring system. In Proc ITS. Kipper, K, Dang, H, and Palmer, M. 2000. Class-Based Construction of a Verb Lexicon. In Proc. AAAI. Lawrence Hall of Science 2006. Assessing Science Knowledge (ASK), UC Berkeley, NSF-0242510 Leacock, C. 2004. Scoring free-response automatically: A case study of a large-scale Assessment. Examens. Lin, D & Pantel, P. 2001. Discovery of inference rules for Question Answering. In Natl. Lang. Engineering. Nielsen, R, Ward, W, and Martin, JH. 2008a. Learning to Assess Low-level Conceptual Understanding. In Proc. FLAIRS. Nielsen, R, Ward, W, Martin, JH and Palmer, P. 2008b. Annotating Students? Understanding of Science Con-cepts. In Proc. LREC. Nivre, J, Hall, J, Nilsson, J, Eryigit, G and Marinov, S. 2006. Labeled Pseudo-Projective Dependency Pars-ing with Support Vector Machines. In Proc. CoNLL. Palmer, M, Gildea, D, & Kingsbury, P. 2005. The proposition bank: An annotated corpus of semantic roles. In Computational Linguistics. 
244
Proceedings of the Third ACL Workshop on Innovative Use of NLP for Building Educational Applications, pages 10?18,
Columbus, Ohio, USA, June 2008. c?2008 Association for Computational Linguistics
Classification Errors in a Domain-Independent Assessment System   Rodney D. Nielsen1,2, Wayne Ward1,2 and James H. Martin1 1 Center for Computational Language and Education Research, University of Colorado, Boulder 2 Boulder Language Technologies, 2960 Center Green Ct., Boulder, CO 80301 Rodney.Nielsen, Wayne.Ward, James.Martin@Colorado.edu       Abstract We present a domain-independent technique for assessing learners? constructed responses.  The system exceeds the accuracy of the ma-jority class baseline by 15.4% and a lexical baseline by 5.9%.  The emphasis of this paper is to provide an error analysis of performance, describing the types of errors committed, their frequency, and some issues in their resolution.   1 Introduction Assessment within state of the art Intelligent Tu-toring Systems (ITSs) generally provides little more than an indication that the student?s response expressed the target knowledge or it did not. There is no indication of exactly what facets of the con-cept a student contradicted or failed to express. Furthermore, virtually all ITSs are developed in a very domain-specific way, with each new question requiring the handcrafting of new semantic extrac-tion frames, parsers, logic representations, or knowledge-based ontologies (c.f., Graesser et al, 2001; Jordan et al, 2004; Peters et al, 2004; Roll et al, 2005; VanLehn et al, 2005). This is also true of research in the area of scoring constructed re-sponse questions (e.g., Callear et al, 2001; Lea-cock, 2004; Mitchell et al, 2002; Pulman and Sukkarieh, 2005). The present paper analyzes the errors of a system that was designed to address these limitations.   Rather than have a single expressed versus not-expressed assessment of the reference answer as a whole, we instead break the reference answer down into what we consider to be approximately 
its lowest level compositional facets. This roughly translates to the set of triples composed of labeled (typed) dependencies in a dependency parse of the reference answer. Breaking the reference answer down into fine-grained facets permits a more fo-cused assessment of the student?s response, but a simple yes or no entailment at the facet level still lacks semantic expressiveness with regard to the relation between the student?s answer and the facet in question, (e.g., did the student contradict the facet or just fail to address it?).  Therefore, it is also necessary to break the annotation labels into finer levels in order to specify more clearly the relationship between the student?s answer and the reference answer facet.   In this paper, we present an error analysis of our system, detailing the most frequent types of errors encountered in our implementation of a domain-independent ITS assessment component and dis-cuss plans for correcting or mitigating some of the errors.  The system expects constructed responses of a phrase to a few sentences, but does not rely on technology developed specifically for the domain or subject matter being tutored ? without changes, it should handle history as easily as science.  We first briefly describe the corpus used, the knowl-edge representation, and the annotation.  In section 3, we describe our assessment system.  Then we present the error analysis and discussion. 2 Assessing Student Answers 2.1 Corpus We acquired grade 3-6 responses to 287 questions from the Assessing Science Knowledge (ASK) project (Lawrence Hall of Science, 2006). The re-sponses, which range in length from moderately 
10
short verb phrases to several sentences, cover all 16 diverse teaching and learning modules, span-ning life science, physical science, earth and space science, scientific reasoning, and technology. We generated a corpus by transcribing a random sample (approx. 15400) of the students? handwritten responses. 2.2 Knowledge Representation The ASK assessments included a reference an-swer for each of their constructed response ques-tions.  We decomposed these reference answers into low-level facets, roughly extracted from the relations in a syntactic dependency parse and a shallow semantic parse. However, we use the word facet to refer to any fine-grained component of the reference answer semantics. The decomposition is based closely on these well-established frame-works, since the representations have been shown to be learnable by automatic systems (c.f., Gildea and Jurafsky, 2002; Nivre et al, 2006). These fac-ets are the basis for assessing learner answers. See (Nielsen et al, 2008b) for details on extracting the facets; here we simply sketch the makeup of the final assessed reference answer facets.   Example 1 presents a reference answer from the Magnetism and Electricity module and illustrates the facets derived from its dependency parse (shown in Figure 1), along with their glosses.  These facets represent the fine-grained knowledge the student is expected to address in their response.  (1) The brass ring would not stick to the nail be-cause the ring is not iron. (1a)  NMod(ring, brass)  (1a?) The ring is brass. (1b)  Theme_not(stick, ring) (1b?) The ring does not stick. (1c)  Destination_to_not(stick, nail) (1c?) Something does not stick to the nail. (1d)  Be_not(ring, iron) (1d?) The ring is not iron. (1e)  Cause_because(1b-c, 1d) (1e?) 1b and 1c are caused by 1d.  Figure 1. Reference answer representation revisions 
Typical facets, as in (1a), are derived directly from a dependency parse, in this case retaining its dependency type label, NMod (noun modifier).  Other facets, such as (1b-e), are the result of com-bining multiple dependencies, VMod(stick, to) and PMod(to, nail) in the case of (1c). When the head of the dependency is a verb, as in (1b,c), we use Thematic Roles from VerbNet (Kipper et al, 2000) and adjuncts from PropBank (Palmer et al, 2005) to label the facet relation.  Some copulas and simi-lar verbs were themselves used as facet relations, as in (1d).  Dependencies involving determiners and many modals, such as would, in ex. 1, are dis-carded and negations, such as not, are incorporated into the associated facets. We refer to facets that express relations between higher-level propositions as inter-propositional facets.  An example of such a facet is (1e) above, connecting the proposition the brass ring did not stick to the nail to the proposition the ring is not iron.  In addition to specifying the headwords of inter-propositional facets (stick and is, in 1e), we also note up to two key facets from each of the propositions that the relation is connecting (b, c, and d in ex. 1).  Reference answer facets that are assumed to be understood by the learner a priori, (generally because they are part of the information given in the question), are also annotated to indi-cate this. There were a total of 2878 reference answer fac-ets, with a mean of 10 facets per reference answer (median of 8).  Facets that were assumed to be un-derstood a priori by students accounted for 33% of all facets and inter-propositional facets accounted for 11%.  The experiments in automated annotation of student answers (section 3) focus on the facets that are not assumed to be understood a priori (67% of all facets); of these, 12% are inter-propositional. 2.3 Annotating Student Understanding After defining the reference answer facets, we annotated each student answer to indicate whether and how they addressed each reference answer facet. We settled on the annotation labels in Table 1. For a given student answer, one label is assigned for each facet in the associated reference answer.  These labels and the annotation process are de-tailed in (Nielsen et al, 2008a).  
11
Assumed: Reference answer facets that are assumed to be understood a priori based on the question Expressed: Any reference answer facet directly ex-pressed or inferred by simple reasoning Inferred: Reference answer facets whose understanding is inferred by pragmatics or nontrivial logical reasoning Contra-Expr: Reference answer facets directly contra-dicted by negation, antonymous expressions, and their paraphrases Contra-Infr: Reference answer facets contradicted by pragmatics or complex reasoning Self-Contra: Reference answer facets that are both con-tradicted and implied (self contradictions) Diff-Arg: Reference answer facets whose core relation is expressed, but it has a different modifier or argument Unaddressed: Reference answer facets that are not ad-dressed at all by the student?s answer Table 1. Facet Annotation Labels Example 2 shows a fragment of a question and associated reference answer broken down into its constituent facets with an indication of whether the facet is assumed to be understood a priori.  A cor-responding student answer is shown in (3) along with its final annotation in 2a?-c?.  It is assumed that the student understands that the pitch is higher (facet 2b), since this is given in the question and similarly it is assumed that the student will be ex-plaining what has the causal effect of producing this higher pitch (facet 2c).  Therefore, unless the student explicitly addresses these facets they are labeled Assumed.  The student phrase the string is long is aligned with reference answer facet 2a, since they are both expressing a property of the string, but since the phrase neither contradicts nor indicates an understanding of the facet, the facet is labeled Diff-Arg, 2a?.  The causal facet 2c? is la-beled Expressed, since the student expresses a causal relation and the cause and effect are each properly aligned.  In this way, the automated tutor will know the student is on track in attempting to address the cause and it can focus on remediating the student?s understanding of that cause. (2) Question: ... Write a note to David to tell him why the pitch gets higher rather than lower. Reference Answer: The string is tighter, so the pitch is higher... (2a) Be(string, tighter), --- (2b) Be(pitch, higher), Assumed (2c) Cause(2b, 2a), Assumed 
(3) David this is why because you don't listen to your teacher. If the string is long, the pitch will be high. (2a?) Be(string, tighter), Diff-Arg (2b?) Be(pitch, higher), Expressed (2c?) Cause(2b?, 2a?), Expressed A tutor will treat the labels Expressed, Inferred and Assumed all as Understood by the student and similarly Contra-Expr and Contra-Infr are com-bined as Contradicted.  These labels are kept sepa-rate in the annotation to facilitate training different systems to detect these different inference relation-ships, as well as to allow evaluation at that level.  The consolidated set of labels, comprised of Un-derstood, Contradicted, Self-Contra, Diff-Arg and Unaddressed, are referred to as the Tutor Labels. 3 Automated Classification A high level description of the assessment proce-dure is as follows. We start with the hand gener-ated reference answer facets. We generate automatic parses for the reference answers and the student answers and automatically modify these parses to match our desired representation. Then for each reference answer facet, we extract features indicative of the student?s understanding of that facet. Finally, we train a machine learning classi-fier on our training data and use it to classify un-seen test examples, assigning a Tutor Label (described in the preceding paragraph), for each reference answer facet.  3.1 Preprocessing and Representation Many of the features utilized by the machine learn-ing algorithm here are based on document co-occurrence counts.  We use three publicly available corpora (English Gigaword, The Reuters corpus, and Tipster) totaling 7.4M articles and 2.6B terms.  These corpora are all drawn from the news do-main, making them less than ideal sources for as-sessing student?s answers to science questions. We utilized these corpora to generate term relatedness statistics primarily because they comprised a read-ily available large body of text.  They were in-dexed and searched using Lucene, a publicly available Information Retrieval tool.   Before extracting features, we automatically generate dependency parses of the reference an-swers and student answers using MaltParser (Nivre 
12
et al, 2006).  These parses are then automatically modified in a way similar to the manual revisions made when extracting the reference answer facets, as sketched in section 2.2.  We reattach auxiliary verbs and their modifiers to the associated regular verbs.  We incorporate prepositions and copulas into the dependency relation labels, and similarly append negation terms onto the associated depend-ency relations.  These modifications, all made automatically, increase the likelihood that terms carrying significant semantic content are joined by dependencies that are utilized in feature extraction.  In the present work, we did not make use of a the-matic role labeler.   3.2 Machine Learning Features & Approach We investigated a variety of linguistic features and chose to utilize the features summarized in Table 2, informed by training set cross validation results.  The features assess the facets? lexical similarity via lexical entailment probabilities following (Glick-man et al, 2005), part of speech (POS) tags, and lexical stem matches.  They include syntactic in-formation extracted from the modified dependency parses such as relevant relation types and path edit distances.  Remaining features include information about polarity among other things.  The revised dependency parses described earlier are used in aligning the terms and facet-level information for feature extraction, as indicated in the feature de-scriptions.  The data was split into a training set and three test sets.  The first test set, Unseen Modules, con-sists of all the data from three of the 16 science modules, providing a domain-independent test set.  The second, Unseen Questions, consists of all the student answers associated with 22 randomly se-lected questions from the 233 questions in the re-maining 13 modules, providing a question-independent test set.  The third test set, Unseen Answers, was created by randomly assigning all of the facets from approximately 6% of the remaining learner answers to a test set with the remainder comprising the training set. In the present work, we utilize only the facets that were not assumed to be understood a priori. This selection resulted in a total of 54,967 training examples, 30,514 examples in the Unseen Modules test set, 6,699 in the Un-seen Questions test set and 3,159 examples in the Unseen Answers test set. 
Lexical Features Gov/Mod_MLE: The lexical entailment probabilities (LEPs) for the reference answer facet governor (Gov; e.g., string in 2a) and modifier (Mod; e.g., tighter in 2a) following (Glickman et al, 2005; c.f., Turney, 2001). The LEP of a reference answer word w is defined as: (1) ,  where v is a word in the student answer, nv is the # of docs (see section 3.1) containing v, and nw,v is the # of docs where w & v cooccur. {Ex. 2a: the LEPs for string?string and tension? tighter, respectively}? Gov/Mod_Match: True if the Gov (Mod) stem has an exact match in learner answer. {Ex. 2a: True for Gov: string, and (False for Mod: no stem match for tighter)}? Subordinate_MLEs: The lexical entailment probabili-ties for the primary constituent facets? Govs and Mods when the facet represents a relation between higher-level propositions (see inter-propositional facet defini-tion in section 2.2). {Ex. 2c: the LEPs for pitch?pitch, up?higher, string?string, and tension?tighter}? Syntactic Features Gov/Mod_POS: POS tags for the facet?s Gov and (Mod). {Ex. 2a: NN for string and (JJR for tighter)}? Facet/AlignedDep_Reltn: The labels of the facet and aligned learner answer dependency ? alignments were based on co-occurrence MLEs as with words, (i.e., they estimate the likelihood of seeing the reference answer dependency in a document given it contains the learner answer dependency ? replace words with dependencies in equation 1 above). {Ex. 2a: Be is the facet label and Have is the aligned student answer dependency}? Dep_Path_Edit_Dist: The edit distance between the dependency path connecting the facet?s Gov and Mod (not necessarily a single step due to parser errors) and the path connecting the aligned terms in the learner an-swer. Paths include the dependency relations generated in our modified parse with their attached prepositions, negations, etc, the direction of each dependency, and the POS tags of the terms on the path. The calculation ap-plies heuristics to judge the similarity of each part of the path (e.g., dropping a subject had a much higher cost than dropping an adjective).  Alignment for this feature was made based on which set of terms in an N-best list (N=5 in the present experiments) for the Gov and Mod resulted in the smallest edit distance.  The N-best list was generated based on the lexical entailment values (see Gov/Mod_MLE). {Ex. 2b: Distance(up:VMod> went:V<pitch:Subject, pitch:Be>higher)}? Other Features Consistent_Negation: True if the facet and aligned student dependency path had the same number of nega-tions. {Ex. 2a: True: neither one have a negation}? RA_CW_cnt: The number of content words (non-function words) in the reference answer. {Ex. 2: 5 = count(string, tighter, so, pitch & higher)}? ? Examples within {} braces are based on reference answer Ex. 2 and the learner answer:  The pitch went up because the string has more tension Table 2. Machine Learning Features 
13
We evaluated several machine learning algo-rithms (rules, trees, boosting, ensembles and an svm) and C4.5 (Quinlan, 1993) achieved the best results in cross validation on the training data.  Therefore, we used it to obtain all of the results presented here.  A number of classifiers performed comparably and Random Forests outperformed C4.5 with a previous feature set and subset of data.  A thorough analysis of the impact of the classifier chosen has not been completed at this time. 3.3 System Results Given a student answer, we generate a separate Tutor Label (described at the end of section 2.3) for each associated reference answer facet to indi-cate the level of understanding expressed in the student?s answer (similar to giving multiple marks on a test).  Table 3 shows the classifier?s Tutor La-bel accuracy over all reference answer facets in cross validation on the training set as well as on each of our test sets.  The columns first show two simpler baselines, the accuracy of a classifier that always chooses the most frequent class in the train-ing set ? Unaddressed, and the accuracy based on a lexical decision that chooses Understood if both the governing term and the modifier are present in the learner?s answer and outputs Unaddressed oth-erwise, (we also tried placing a threshold on the product of the governor and modifier lexical en-tailment probabilities following Glickman et al (2005), who achieved the best results in the first RTE challenge, but this gave virtually the same results as the word matching baseline).  The col-umn labeled Table 2 Features presents the results of our classifier. (Reduced Training is described in the Discussion section, which follows.)  Majority Label Lexical Baseline Table 2 Features Reduced Training Training Set CV 54.6 59.7 77.1  Unseen Answers 51.1 56.1 75.5  Unseen Questions 58.4 63.4 61.7 66.5 Unseen Modules 53.4 62.9 61.4 68.8 Table 3. Classifier Accuracy 4 Discussion and Error Analysis 4.1 Results Discussion The accuracy achieved, assessing learner answers within this new representation framework, repre-
sent an improvement of 24.4%, 3.3%, and 8.0% over the majority class baseline for Unseen An-swers, Questions, and Modules respectively.  Ac-curacy on Unseen Answers is also 19.4% better than the lexical baseline. However, this simple baseline outperformed the classifier on the other two test sets.  It seemed probable that the decision tree over fit the data due to bias in the data itself; specifically, since many of the students? answers are very similar, there are likely to be large clusters of identical feature-class pairings, which could re-sult in classifier decisions that do not generalize as well to other questions or domains.  This bias is not problematic when the test data is very similar to the training data, as is the case for our Unseen Answers test set, but would negatively affect per-formance on less similar data, such as our Unseen Questions and Modules.   To test this hypothesis, we reduced the size of our training set to about 8,000 randomly selected examples, which would result in fewer of these dense clusters, and retrained the classifier.  The result for Unseen Questions, shown in the Reduced Training column, was an improvement of 4.8%.  Given this promising improvement, we attempted to find the optimal training set size through cross-validation on the training data.  Specifically, we iterated over the science modules holding one module out, training on the other 12 and testing on the held out module. We analyzed the learning curve varying the number of randomly selected examples per facet.  We found the optimal accu-racy for training set cross-validation by averaging the results over all the modules and then trained a classifier on that number of random examples per facet in the training set and tested on the Unseen Modules test set.  The result was an increase in accuracy of 7.4% over training on the full training set.  In future work, we will investigate other more principled techniques to avoid this type of over-fitting, which we believe is somewhat atypical. 4.2 Error Analysis In order to focus future work on the areas most likely to benefit the system, an error analysis was performed based on the results of 13-fold cross-validation on the training data (one fold per science module). In other words, 13 C4.5 decision tree classifiers were built, one for each science module in the training set; each classifier was trained, 
14
utilizing the feature set shown in Table 2, on all of the data from 12 science modules and then tested on the data in the remaining, held-out module. This effectively simulates the Unseen Modules test condition. To our knowledge, no prior work has analyzed the assessment errors of such a domain-independent ITS. Several randomly selected examples were analyzed to look for patterns in the types of errors the system makes.  However, only specific categories of data were considered.  Specifically, only the subsets of errors that were most likely to lead to short-term system improvements were considered.  This included only examples where all of the annotators agreed on the annotation, since if the annotation was difficult for humans, it would probably be harder to construct features that would allow the machine learning algorithm to correct its error.  Second, only Expressed and Unaddressed facets were considered, since Inferred facets represent the more challenging judgments, typically based on pragmatic inferences.  Contradictions were excluded since there was almost no attempt to handle these in the present system.  Third, only facets that were not inter-propositional were considered, since the inter-propositional facets are more complicated to process and only represent 12% of the non-Assumed data. We discuss Expressed facets in the next section of the paper and Unaddressed in the following section. 4.3 Errors in Expressed Facets Without examining each example relative to the decision tree that classified it, it is not possible to know exactly what caused the errors.  The analysis here simply indicates what factors are involved in inferring whether the reference answer facets were understood and what relationships exist between the student answer and the reference answer facet.  We analyzed 100 random examples of errors where annotators considered the facet Expressed and the system labeled it Unaddressed, but the analysis only considered one example for any given reference answer facet.  Out of these 100 examples, only one looked as if it was probably incorrectly annotated.  We group the potential error factors seen in the data, listed in order of frequency, according to issues associated with paraphrases, logical inference, pragmatics, and 
preprocessing errors.  In the following paragraphs, these groups are broken down for a more fine-grained analysis.  In over half of the errors considered, there were two or more of these fine-grained factors involved. Paraphrase issues, taken broadly, are subdivided into three main categories: coreference resolution, lexical substitution, syntactic alternation and phrase-based paraphrases. Our results in this area are in line with (Bar-Haim et al, 2005), who considered which inference factors are involved in proving textual entailment. Three coreference resolution factors combined are involved in nearly 30% of the errors.  Students use on average 1.1 pronouns per answer and, more importantly, the pronouns tend to refer to key entities or concepts in the question and reference answer.  A pronoun was used in 15 of the errors (3 personal pronouns ? she, 11 uses of it, and 1 use of one).  It might be possible to correct many of these errors by simply aligning the pronouns to essentially all possible nouns in the reference answer and then choosing the single alignment that gives the learner the most credit. In 6 errors, the student referred to a concept by another term (e.g., substituting stuff for pieces). In another 6 errors, the student used one of the terms in a noun phrase from either the question or reference answer to refer to a concept where the reference answer facet included the other term as its modifier or vice versa. For example, one reference answer was looking for NMod(particles, clay) and Be(particles, light) and the student said Because clay is the lightest, which should have resulted in an Understood classification for the second facet (one could argue that there is an important distinction between the answers, but requiring elementary school students to answer at this level of specificity could result in an overwhelming number of interactions to clarify understanding). As a group, the simple lexical substitution categories (synonymy, hypernymy, hyponymy, meronymy, derivational changes, and other lexical paraphrases) appear more often in errors than any of the other factors with around 35 occurrences.  Roughly half of these relationships should be detectable using broad coverage lexical resources.  For example, substituting tiny for small, CO2 for gas, put for place, pen for ink and push for carry (WordNet entailment).  However, many of these lexical paraphrases are not necessarily associated 
15
in lexical resources such as WordNet.  For example, in the substitution of put the pennies for distribute the pennies, these terms are only connected at the top of the WordNet hierarchy at the Synset (move, displace).  Similarly, WordNet appears not to have any connection at all between have and contain. VerbNet alo does not show a relation between either pair of words. Concept definitions account for an additional 14 issues that could potentially be addressed by lexical resources such as WordNet. Vanderwende et al (2005) found that 34% of the Recognizing Textual Entailment Challenge test data could be handled by recognizing simple syntactic variations.  However, while syntactic variation is certainly common in the kids? data, it did not appear to be the primary factor in any of the system errors.  Most of the remaining paraphrase errors were classified as involving phrase-based paraphrases.  Examples here include ...it will heat up faster versus it got hotter faster and in the middle versus halfway between.  Six related errors essentially involved negation of an antonym, (e.g., substituting not a lot for little and no one has the same fingerprint for everyone has a different print).  Paraphrase recognition is an area that we intend to invest significant time in future research (c.f., Lin and Pantel, 2001; Dolan et al, 2004).  This research should also reduce the error rate on lexical paraphrases. The next most common issues after paraphrases were deep or logical reasoning and then pragmatics.  These two factors were involved in nearly 40% of the errors.  Examples of logical inference include recognizing that two cups have the same amount of water given the following student response, no, cup 1 would be a plastic cup 25 ml water and cup 2 paper cup 25 ml and 10 g sugar, and that two sounds must be very different in the case that ?it is easy to discriminate? Examples of pragmatic issues include recognizing that saying Because the vibrations implies that a rubber band is vibrating given the question context, and that the earth in the response ?the fulcrum is too close to the earth should be considered to be the load referred to in its reference answer. It is interesting that these are all examples that three annotators unanimously considered to be Expressed versus Inferred facets.  Finally, the remaining errors were largely the result of preprocessing issues.  At least two errors 
would be eliminated by simple data normalization (3?three and g?grams). Semantic role labeling has the potential to provide the classifier with information that would clearly indicate the relationships between the student and the reference answer, but there was only one error in which this came to mind as an important factor and it was not due to the role labels themselves, but because MaltParser labels only a single head. Specifically, in the sentence She could sit by the clothes and check every hour if one is dry or not, the pronoun She is attached as the subject of could sit, but check is left without a subject.   In previous work, analyzing the dependency parses of fifty one of the student answers, many had what were believed to be minor errors, 31% had significant errors, and 24% had errors that looked like they could easily lead to problems for the answer assessment classifier. Over half of the more serious dependency parse errors resulted from inopportune sentence segmentation due to run-on student sentences conjoined by and. To overcome these issues, the text could be parsed once using the original sentence segmentation and then again with alternative segmentations under conditions to be determined by further dependency parser error analysis.  One partial approach could be to split sentences when two noun phrases are conjoined and they occur between two verbs, as is the case in the preceding example, where the alternative segmentation results in correct parses. Then the system could choose the parse that is most consistent with the reference answer. While we believe improving the parser output will result in higher accuracy by the assessment classifier, there was little evidence to support this in the small number of parses examined in the assessment error analysis.  We only checked the parses when the dependency path features looked wrong and it was somewhat surprising that the classifier made an error (for example, when there were simple lexical substitutions involving very similar words) ? this was the case for only about 10-15 examples. Only two of these classification errors were associated with parser errors. However, better parses should lead to more reliable (less noisy) features, which in turn will allow the machine learning algorithm to more easily recognize which features are the most predictive. It should be emphasized that over half of the errors in Expressed facets involved more than one 
16
of the fine-grained factors discussed here. For example, to recognize the child understands a tree is blocking the sunlight based on the answer There is a shadow there because the sun is behind it and light cannot go through solid objects. Note, I think that question was kind of dumb, requires resolving it to the tree and the solid object mentioned to the tree, and then recognizing that light cannot go through [the tree] entails the tree blocks the light. 4.4 Errors in Unaddressed Facets Unlike the errors in Expressed facets, a number of the examples here appeared to be questionable annotations. For example, given the student answer fragment You could take a couple of cardboard houses and? 1 with thick glazed insulation?, all three annotators suggested they could not infer the student meant the insulation should be installed in one of the houses. Given the student answer Because the darker the color the faster it will heat up, the annotators did not infer that the student believed the sheeting chosen was the darkest color.  One of the biggest sources of errors in Unaddressed facets is the result of ignoring the context of words. For example, consider the question When you make an electromagnet, why does the core have to be iron or steel? and its reference answer Iron is the only common metal that can become a temporary magnet. Steel is made from iron. Then, given the student answer It has to be iron or steel because it has to pick up the washers, the system classified the facet Material_from(made, iron) as Understood based on the text has to be iron, but ignores the context, specifically, that this should be associated with the production of steel, Product(made, steel). Similarly, the student answer You could wrap the insulated wire to the iron nail and attach the battery and switch leads to the classification of Understood for a facet indicating to touch the nail to a permanent magnet to turn it into a temporary magnet, but wrapping the wire to the nail should have been aligned to a different method of making a temporary magnet. Many of the errors in Unaddressed facets appear to be the result of antonyms having very similar statistical co-occurrence patterns. Examples of errors here include confusing closer with greater distance and absorbs energy with reflects energy. 
However, both of these also may be annotation errors that should have been labeled Contra-Expr. The biggest source of error is simply classifying a number of facets as Understood if there is partial lexical similarity and perhaps syntactic similarity as in the case of accepting the balls are different in place of different girls. However, there are also a few cases where it is unclear why the decision was made, as in an example where the system apparently trusted that the student understood a complicated electrical circuit based on the student answer we learned it in class. The processes and the more informative features described in the preceding section describing errors in Expressed facets should allow the learning algorithm to focus on less noisy features and avoid many of the errors described in this section. However, additional features will need to be added to ensure appropriate lexical and phrasal alignment, which should also provide a significant benefit here. Future plans include training an alignment classifier separate from the assessment classifier.  5 Conclusion To our knowledge, this is the first work to success-fully assess constructed-response answers from elementary school students.  We achieved promis-ing results, 24.4% and 15.4% over the majority class baselines for Unseen Answers and Unseen Modules, respectively.  The annotated corpus asso-ciated with this work will be made available as a public resource for other researches working on educational assessment applications or other tex-tual entailment applications. The focus of this paper was to provide an error analysis of the domain-independent (Unseen Mod-ules) assessment condition.  We discussed the common types of issues involved in errors and their frequency when assessing young students? understanding of the fine-grained facets of refer-ence answers.  This domain-independent assess-ment will facilitate quicker adaptation of tutoring systems (or general test assessment systems) to new topics, avoiding the need for a significant ef-fort in hand-crafting new system components.   It is also a necessary prerequisite to enabling unre-stricted dialogue in tutoring systems.  
17
Acknowledgements We would like to thank the anonymous reviewers, whose comments improved the final paper.  This work was partially funded by Award Number 0551723 from the National Science Foundation. References  Bar-Haim, R., Szpektor, I. and Glickman, O. 2005. Definition and Analysis of Intermediate Entailment Levels. In Proc. Workshop on Empirical Modeling of Semantic Equivalence and Entailment. Callear, D., Jerrams-Smith, J., and Soh, V. 2001. CAA of short non-MCQ answers. In Proc. of the 5th Inter-national CAA conference, Loughborough. Dolan, W.B., Quirk, C, and Brockett, C. 2004. Unsu-pervised Construction of Large Paraphrase Corpora: Exploiting Massively Parallel News Sources. Pro-ceedings of COLING 2004, Geneva, Switzerland. Gildea, D. and Jurafsky, D. 2002. Automatic labeling of semantic roles. Computational Linguistics, 28:3, 245?288. Glickman, O. and Dagan, WE., and Koppel, M. 2005. Web Based Probabilistic Textual Entailment. In Pro-ceedings of the PASCAL Recognizing Textual En-tailment Challenge Workshop. Graesser, A.C., Hu, X., Susarla, S., Harter, D., Person, N.K., Louwerse, M., Olde, B., and the Tutoring Re-search Group. 2001. AutoTutor: An Intelligent Tutor and Conversational Tutoring Scaffold. In Proceed-ings for the 10th International Conference of Artifi-cial Intelligence in Education San Antonio, TX, 47-49. Jordan, P.W., Makatchev, M., and VanLehn, K. 2004. Combining competing language understanding ap-proaches in an intelligent tutoring system. In J. C. Lester, R. M. Vicari, and F. Paraguacu, Eds.), 7th Conference on Intelligent Tutoring Systems, 346-357. Springer-Verlag Berlin Heidelberg. Kipper, K., Dang, H.T., and Palmer, M. 2000. Class-Based Construction of a Verb Lexicon. AAAI Seven-teenth National Conference on Artificial Intelligence, Austin, TX. Lawrence Hall of Science 2006. Assessing Science Knowledge (ASK), University of California at Ber-keley, NSF-0242510 Leacock, C. 2004. Scoring free-response automatically: A case study of a large-scale Assessment. Examens, 1(3). Lin, D. and Pantel, P. 2001. Discovery of inference rules for Question Answering. In Natural Language Engineering, 7(4):343-360. Mitchell, T., Russell, T., Broomhead, P. and Aldridge, N. 2002. Towards Robust Computerized Marking of Free-Text Responses. In Proc. of 6th International 
Computer Aided Assessment Conference, Loughbor-ough.  Nielsen, R., Ward, W., Martin, J. and Palmer, M. 2008a. Annotating Students? Understanding of Science Con-cepts. In Proc. LREC. Nielsen, R., Ward, W., Martin, J. and Palmer, M. 2008b. Extracting a Representation from Text for Semantic Analysis. In Proc. ACL-HLT. Nivre, J. and Scholz, M. 2004. Deterministic Depend-ency Parsing of English Text. In Proceedings of COLING, Geneva, Switzerland, August 23-27. Nivre, J., Hall, J., Nilsson, J., Eryigit, G. and Marinov, S. 2006. Labeled Pseudo-Projective Dependency Parsing with Support Vector Machines. In Proceed-ings of the Tenth Conference on Computational Natural Language Learning (CoNLL). Palmer, M., Gildea, D., and Kingsbury, P. 2005. The proposition bank: An annotated corpus of semantic roles. In Computational Linguistics. Peters, S., Bratt, E.O., Clark, B., Pon-Barry, H., and Schultz, K. 2004. Intelligent Systems for Training Damage Control Assistants. In Proc. of Inter-service/Industry Training, Simulation, and Education Conference. Pulman, S.G. and Sukkarieh, J.Z. 2005. Automatic Short Answer Marking. In Proc. of the 2nd Workshop on Building Educational Applications Using NLP, ACL. Quinlan, J.R. 1993. C4.5: Programs for Machine Learn-ing. Morgan Kaufmann. Roll, WE., Baker, R.S., Aleven, V., McLaren, B.M., and Koedinger, K.R. 2005. Modeling Students? Metacog-nitive Errors in Two Intelligent Tutoring Systems. In L. Ardissono, P. Brna, and A. Mitrovic (Eds.), User Modeling, 379?388. Turney, P.D. 2001. Mining the web for synonyms: PMI-IR versus LSA on TOEFL. In Proceedings of the Twelfth European Conference on Machine Learning (ECML-2001), 491?502. Vanderwende, L., Coughlin, D. and Dolan, WB. 2005. What Syntax can Contribute in the Entailment Task. In Proc. of the PASCAL Workshop for Recognizing Textual Entailment. VanLehn, K., Lynch, C., Schulze, K. Shapiro, J. A., Shelby, R., Taylor, L., Treacy, D., Weinstein, A., and Wintersgill, M. 2005. The Andes physics tutoring system: Five years of evaluations. In G. McCalla and C. K. Looi (Eds.), Proceedings of the 12th Interna-tional Conference on Artificial Intelligence in Educa-tion. Amsterdam: IOS Press.  
18
DISCUSS: A dialogue move taxonomy layered over semantic
representations
Lee Becker1 Wayne H. Ward1,2 Sarel van Vuuren1 Martha Palmer1
{lee.becker, martha.palmer, sarel.vanvuuren}@colorado.edu,
wward@bltek.com
1University of Colorado at Boulder, 2Boulder Language Technologies
Abstract
In this paper we describe DISCUSS, a dialogue move taxonomy layered over semantic represen-
tations. We designed this scheme to enable development of computational models of tutorial dia-
logues and to provide an intermediate representation suitable for question and tutorial act generation.
As such, DISCUSS captures semantic and pragmatic elements across four dimensions: Dialogue Act,
Rhetorical Form, Predicate Type, Semantic Roles. Together these dimensions provide a summary of
an utterance?s propositional content and how it may change the underlying information state of the
conversation. This taxonomy builds on previous work in both general dialogue act taxonomies as
well as work in tutorial act and tutorial question categorization. The types and values found within
our taxonomy are based on preliminary observations and on-going annotation from our corpus of
multimodal tutorial dialogues for elementary school science education.
1 Introduction
Past successes with conversational Intelligent Tutoring Systems (ITS) (Graesser et al, 2001), have helped
to demonstrate the efficacy of computer-led, tutorial dialogue. However, ITS will not reach their full
potential until they can overcome current limitations in spoken dialogue technologies. Producing systems
capable of leading open-ended, Socratic-style tutorials will likely require more sophisticated models to
automate analysis and generation of dialogue. A well defined tutorial dialogue annotation scheme can
serve as a stepping stone towards these goals. Such a scheme should account for differences in tutoring
style and question scaffolding techniques and should capture the subtle distinctions between different
question types. To do this, requires a representation that connects a turn?s communicative and rhetorical
functions to its underlying semantic content.
While efforts such as DAMSL (Core and Allen, 1997) and DIT++ (Bunt, 2009) have helped to make
dialogue act annotation more uniform and applicable to a wider audience, and while tutoring-specific
initiatives (Tsovaltzi and Karagjosova, 2004; Buckley and Wolska, 2008) have helped to bring dialogue
acts to tutorial dialogue, the move granularity in these schemas is too coarse to capture the differences
in tutorial questioning styles exhibited in our corpus of Socratic-style tutorial dialogues. Conversely,
question type categories (Graesser and Person, 1994; Nielsen et al, 2008) have been designed with
education in mind, but they largely ignore how the student and tutor may work together to construct
meaning. The DISCOUNT scheme?s (Pilkington, 1999) combination of dialogue acts and rhetorical
functions enabled it to better capture tutoring moves, but its omission of shallow semantics prevents it
from capturing how content influences behavior.
Our long-term goals of automatic dialogue characterization, tutorial move prediction and question
generation led us to design our own dialogue representation called DISCUSS (Dialogue Scheme for
Unifying Speech and Semantics). Design of this dialogue move taxonomy was based on preliminary
observations from our corpus of tutorial dialogues, and was influenced by the aforementioned research.
We hope that undertaking this ambitious endeavor to capture not only a turn?s pragmatic interpretation,
310
but also its rhetorical and semantic functions will enable us to better model the complexity of open-ended,
tutorial dialogue.
The remainder of the this paper is organized as follows. In the next section we describe our tutorial
dialogue setting and our data. Section 3 discusses the organization of the DISCUSS annotation scheme.
Section 4 briefly explains the current status of our annotation. Lastly section 5 outlines our future plans
and conclusions.
2 Tutorial Dialogue Setting and Data
My Science Tutor (MyST) (Ward et al, 2010) is a conversational virtual tutor designed to improve
science learning and understanding for students in grades 3-5. Students using MyST investigate and
discuss science through natural spoken dialogues and multimedia interactions with a virtual tutor named
Marni. The MyST dialogue design and tutoring style is based on a pedagogy called Questioning the
Author (QtA) (Beck et al, 1996), wherein the teacher facilitates discovery by challenging students with
open-ended questions and by directly keying in on ideas expressed in the student?s language.
To gather data for MyST system coverage and dialogue analysis, we ran Wizard-of-Oz (WoZ) exper-
iments that allowed a human tutor to be inserted into the interaction loop. Project tutors trained in QtA
served as Wizards and were responsible for accepting and overriding system actions. Over the past three
years we have accumulated over five-hundred, 15-minute WoZ sessions across four modules Magnetism
and Electricity, Measurement, Variables, and Water, each with 16 lessons. Student speech from these
sessions was professionally transcribed at the word level.
3 The DISCUSS Annotation Scheme
The Dialogue Scheme for Unifying Speech and Semantics (DISCUSS) is a multifaceted dialogue move
taxonomy intended to capture both the pragmatic and semantic interpretations of an utterance. A DIS-
CUSS move is a tuple composed of values from four dimensions: Dialogue Act, Rhetorical Form, Pred-
icate Type, and Semantic Roles. Together these dimensions convey the communicative action, surface
form, and meaning of an utterance independent of the original utterance text.
We designed DISCUSS to serve as an intermediate representation that will enable future work in
dialogue session characterization, dialogue strategy optimization, and automatic question generation. To
facilitate these goals, we have endeavored to create a taxonomy that is both descriptive and curriculum-
independent while allowing for expansion as necessary. A complete listing of all the DISCUSS moves
and dimensions can be found in our forthcoming technical report.
In the following subsection we will describe the different DISCUSS move categories. Descriptions
of the Semantic Role and Predicate Type are found in the subsection about semantic dimensions, while
discussion about the dialogue act and rhetorical form has been placed in the pragmatic dimensions
subsection. Throughout the rest of this paper we denote DISCUSS tuples using the following notation:
Dialogue Act/Rhetorical Form/Predicate Type ?Semantic Role?.
3.1 Move Categories
DISCUSS moves are dictated by the dialogue act dimension and may belong to one of three broad cate-
gories: Dialogue Control, Information Exchange, and Attention Management. Dialogue Control moves
are largely concerned with maintaining and enabling the flow of information. This includes dialogue
acts such as Acknowledge, Open, Close, Repeat, and RequestRepeat. The Information Exchange moves
relay content (often lesson-specific) between speakers using moves such as Assert, Ask, Answer, Mark,
Revoice. For tutorial dialogue the bulk of student-tutor interactions reside in this category. Lastly, At-
tention Management moves indicate how a speaker exercises initiative over other speakers or topics.
Dialogue acts found in the attention category are Focus, Defer, Elicit, and Direct.
311
3.2 Semantic Dimensions
The semantic dimensions define the objects, events, properties and relations contained within an utter-
ance. The semantic roles at the lowest level of the DISCUSS hierarchy directly capture the propositional
entities. Predicate Types summarize the interactions between all of the semantic roles found within an
utterance.
Semantic Roles: The MyST system models a lesson?s key concepts as propositions which are real-
ized as semantic frames. For MyST natural language understanding, these frames serve as the top-level
nodes for a manually written semantic grammar used by the Phoenix parser (Ward, 1994). Two example
concepts/frames and Phoenix parses are shown below. Although these semantic frames form the basis
of MyST dialogues, for DISCUSS annotation we sought a more domain-independent representation that
would generalize across a wide range of subjects. We began with VerbNet (Schuler, 2005) for defining
our set of semantic roles because of its intuitive balance between descriptiveness and portability. While
we used a majority of the labels as is, we found that the definition of some roles needed to be modified
or extended to properly cover our set of concepts. For example, many concepts that express proportion-
ality relationships can not be easily represented using predicate argument structure, and are more easily
decomposed into cause and effect roles. We also added the catch-all keyword label to reflect terms that
may relate to the proposition, but are not part of the core representation.
For our annotation project, rather than manually tagging all of the utterances with VerbNet labels, we
created a mapping layer between the Phoenix frame roles and the VerbNet roles. The table below shows
two frames along with their role mappings. We envision that in future projects, the hand-tuned semantic
grammars could be replaced with a statistically trained semantic role labeler.
Frame: BatteryFunction Frame: MagnetsAttract
Description: The DCell is the source of elec-
tricity.
Description: Magnets attract to certain ob-
jects.
?Instrument?: [Battery] ?Instrument?: [Magnet]
?Predicate?: [Source] ?Predicate?: [Attract]
?Theme?: [Electricity] ?Theme?: [Object]
Predicate Type: Simply knowing an utterance?s propositional content is insufficient for inferring
what was stated. Consider the two exchanges shown in the table below. The mixture of semantic roles
in both students? responses are identical. Additionally, we can not differentiate between the exchanges
based solely on dialogue act or rhetorical form. We need additional information to know the first scenario
seeks to elicit discussion about observations while the second scenario focuses on procedures. One can
also imagine such information would be useful for identifying communication breakdowns. For example,
responding with a description of a procedure to a request about a process may indicate that the student
did not understand the question or that the student is unwilling or unable to address the question.
T12: Tell me about what?s going on here in this picture.
Ask/Describe/Observation
S13: The wires connect the battery and the light bulb and then then light bulb lights up.
Answer/Describe/Observation
?Instrument?.wires ?Predicate?.connect ?Theme1?.battery ?Theme2?.light bulb ?Effect?.bulb
lights up
T7: Tell me about how you got the bulb to light up.
Ask/Describe/Procedure
S8: To make the light go we connected the wires to the battery and the bulb.
Answer/Describe/Procedure
?Effect?.light go ?Predicate?.connected ?Instrument?.wires ?Theme1?.battery ?Theme2?.bulb
To address this need, we created the Predicate Type based partly on the rhetorical predicates used in
the DISCOUNT (Pilkington, 1999) scheme. While DISCOUNT included discourse relations in the set
of predicate types, we restrict predicate types to those that encapsulate or summarize the collection of
semantic roles in an utterance. Example predicate types include procedure, observation and purpose. A
complete list of predicate types can be found in our forthcoming technical report.
312
3.3 Pragmatic Dimensions
The pragmatic dimensions are composed of the dialogue act dimension and the rhetorical form dimen-
sion. The dialogue act expresses the communicative function of a move and is the most general dimen-
sion in DISCUSS. The rhetorical form expresses attributes of the utterance?s surface realization and can
be thought of as refining the intent of the coarser dialogue act.
Dialogue Act: The dialogue act dimension is the top-level dimension in DISCUSS with the values
of all other dimensions depending on the value of this dimension. Like with the majority of dialogue
act taxonomies, DISCUSS dialogue acts have a grounding in speech act theory with a focus on what
action the utterance performs. While most of the dialogue acts in the Dialogue Control and Informa-
tion Exchange move categories have direct corollaries to those found in other taxonomies like DIT++ or
DAMSL, we needed to supplement them with two frequently used Questioning the Author discussion
moves: marking and revoicing. In marking, the tutor highlights parts of the student?s language to em-
phasize important points and to steer the conversation towards key concepts. Revoicing serves a similar
purpose, but instead of highlighting, the tutor rephrases student speech to clarify ideas they may have
been struggling with. Examples of these acts are shown below.
S5: that when you stick a magnet to a rusty nail and then you stick it to a paper clip it sticks
Answer/Describe/Process
T6: I think I heard you say something about magnets sticking or attracting. Tell me more about that.
Mark/None/None, Ask/Elaborate/Process
S33: well when you scrub the the paperclip to the magnet the paperclip is starting to be a magnet
Answer/Describe/Process
T34: very good, so if the magnet gets close to the paperclip it picks it up
Feedback/Positive/None, Revoice/None/None
Dialogue acts in the Attention Management move category also reflect many of the actions regularly
seen in tutorial dialogue. Focus and Defer acts are often used to move to or away from lesson-specific
topics. In our corpus Direct is typically used to give instructions related to the multimedia (e.g. ?Click
on the box? or ?Look at this animation.?).
Rhetorical Form: The DISCUSS Rhetorical Form dimension provides another mechanism for dif-
ferentiating between utterances with identical semantic content. While the dialogue act dimension is
useful for providing an utterance?s pragmatic interpretation and for determining what sequences are li-
censed, by itself it provides no indication of how a speaker is advancing the topic under discussion.
Additional information is needed to create an utterance?s surface form. Consider the two transactions
in the table below. The semantic parses in both scenarios would be identical, however the tutor?s ques-
tions and the resulting student response serve very different functions. In the first, the tutor is asking
for a description and in the second, identification. Selection of the DISCUSS rhetorical forms found in
the Information Exchange move category were inspired by the sixteen top-level tags used in Rhetori-
cal Structure Theory (RST) (Mann and Thompson, 1988). While RST uses a rhetorical relation to link
clauses and to show the development of an argument, DISCUSS uses the rhetorical form to refine the
dialogue act. A sequence of dialogue acts paired with rhetorical forms can show progressions in the
dialogue and tutoring process such as a shift from open-ended to directed questioning.
T1: Can you tell which one is the battery? T1: Can you describe what is going on with the battery?
Ask/Describe/Visual Ask/Identify/None
S2: The battery is putting out electricity. S2: The battery is the one putting out the electricity.
Answer/Describe/Process Answer/Identify/None
4 Annotation Status
We are still in the early stages of this ambitious annotation project. We currently have approximately
60 transcripts singly-annotated with DISCUSS moves. Each of these transcripts represents roughly 15
minutes of conversation and 50 turns on average. The DISCUSS taxonomy is a work in progress. Though
313
we have created the tags for each dimension based on a wide body of prior research and on preliminary
studies of our transcripts, we expect that future analysis of our annotation reliability and consistency will
likely lead us to add, modify, and combine tags. We anticipate that DISCUSS?s multidimensional nature
will likely raise issues for inter-annotator reliability, and the ability to add multiple tags per turn will
further complicate the process of evaluating agreement.
5 Future Work and Conclusions
We plan to use our corpus of DISCUSS annotated tutorial dialogues to build dialogue models for a variety
of applications including assessment of tutorial quality and dialogue move prediction. This annotation
will allow us to investigate what features of tutorial dialogue correlate with increased learning gains and
what types of questions encourage greater student interaction. Data-driven dialogue characterization will
also allow us to explore how tutorial tactics vary across domains and tutors. We envision this work as an
important first step towards automatic question generation.
In this paper we introduced the DISCUSS dialogue move taxonomy. This scheme overlays dialogue
act and rhetorical annotation over semantic representations. We believe this combination of pragmatic
interpretations and semantic representations provide an intermediate representation rich enough to an-
alyze the interactions in a complex task-oriented domain like tutorial dialogue. Furthermore, we think
DISCUSS moves can succinctly summarize the actions of a speaker?s turn, while still providing suffi-
cient information for natural language generation of dialogue moves.
Acknowledgments This work was supported by grants from the NSF (DRL-0733322, DRL-0733323) and the IES (R3053070434).
Any findings, recommendations, or conclusions are those of the author and do not necessarily represent the views of NSF or
IES.
References
Beck, I. L., M. G. McKeown, J. Worthy, C. A. Sandora, and L. Kucan (1996). Questioning the author: A year-long classroom
implementation to engage students with text. The Elementary School Journal 96(4), 387?416.
Buckley, M. and M. Wolska (2008). A classification of dialogue actions in tutorial dialogue. In Proc. COLING, pp. 73?80.
ACL.
Bunt, H. (2009). The dit++ taxonomy for functional dialogue markup. In Proc. EDAML 2009.
Core, M. and J. Allen (1997). Coding dialogs with the damsl annotation scheme. In AAAI Fall Symposium on Comm. Action in
Humans and Machines, pp. 28?35.
Graesser, A., X. Hu, S. Susarla, D. Harter, N. Person, M. Louwerse, B. Olde, and the Tutoring Research Group (2001).
Autotutor: An intelligent tutor and conversational tutoring scaffold. In Proc. AIED?01, pp. 47?49.
Graesser, A. and N. Person (1994). Question asking during tutoring. American Educational Research Journal 31, 104?137.
Mann, W. C. and S. A. Thompson (1988). Rhetorical structure theory: Toward a functional theory of text organization. Text 8(3),
243?281.
Nielsen, R. D., J. Buckingham, G. Knoll, B. Marsh, and L. Palen (2008, September). A taxonomy of questions for question
generation. In Proc. WS on the Question Generation STEC.
Pilkington, R. M. (1999). Analysing educational discourse: The discount scheme. Technical Report 99/2, Computer Based
Learning Unit, University of Leeds.
Schuler, K. K. (2005). VerbNet: A broad-coverage, comprehensive verb lexicon. Ph. D. thesis, University of Pennsylvania.
Tsovaltzi, D. and E. Karagjosova (2004). A view on dialogue move taxonomies for tutorial dialogues. In Proc. SIGDial, pp.
35?38. ACL.
Ward, W. (1994). Extracting information from spontaneous speech. In Proc. ICSLP.
Ward, W., R. Cole, D. Bolanos, C. Buchenroth-Martin, E. Svirsky, S. Van Vuuren, T. Weston, J. Zheng, and L. Becker (2010).
My science tutor: A conversational multi-media virtual tutor for elementary school science. ACM TSLP: Special Issue on
Speech and Language Processing of Children?s Speech for Child-machine Interaction Applications.
314
The 7th Workshop on the Innovative Use of NLP for Building Educational Applications, pages 1?11,
Montre?al, Canada, June 3-8, 2012. c?2012 Association for Computational Linguistics
Question Ranking and Selection in Tutorial Dialogues
Lee Beckera and Martha Palmer 2a and Sarel van Vuuren 3a and Wayne Ward 4a,b
aThe Center for Computational Language and Education Research (CLEAR)
University of Colorado Boulder
bBoulder Language Technologies
{lee.becker,martha.palmer,sarel.vanvuuren}@colorado.edu
wward@bltek.com
Abstract
A key challenge for dialogue-based intelligent
tutoring systems lies in selecting follow-up
questions that are not only context relevant
but also encourage self-expression and stimu-
late learning. This paper presents an approach
to ranking candidate questions for a given di-
alogue context and introduces an evaluation
framework for this task. We learn to rank us-
ing judgments collected from expert human
tutors, and we show that adding features de-
rived from a rich, multi-layer dialogue act
representation improves system performance
over baseline lexical and syntactic features to
a level in agreement with the judges. The ex-
perimental results highlight the important fac-
tors in modeling the questioning process. This
work provides a framework for future work
in automatic question generation and it rep-
resents a step toward the larger goal of di-
rectly learning tutorial dialogue policies di-
rectly from human examples.
1 Introduction
Socratic tutoring styles place an emphasis on elicit-
ing information from the learner to help them build
their own connections to the material. The role of a
tutor in a Socratic dialogue is to scaffold the material
and present questions that ultimately lead the student
to an ?A-ha!? moment. Numerous studies have il-
lustrated the effectiveness of Socratic-style tutoring
(VanLehn et al, 2007; Rose et al, 2001; Collins and
Stevens, 1982); consequently recreating the behav-
ior on a computer has long been a goal of research
in Intelligent Tutoring Systems (ITS). Recent suc-
cesses have shown the efficacy of conversational ITS
(Graesser et al, 2005; Litman and Silliman, 2004;
Ward et al, 2011b), however these systems are still
not as effective as human tutors, and much improve-
ment is needed before they can truly claim to be So-
cratic. Furthermore, development and tuning of tu-
torial dialogue behavior requires significant human
effort.
While our overarching goal is to improve ITS
by automatically learning tutorial dialogue strategies
directly from expert tutor behavior, we focus on the
crucial subtask of selecting follow-up questions. Al-
though asking questions is only a subset of the over-
all tutoring process, it is still a complex process that
requires understanding of the dialogue state, the stu-
dent?s ability, and the learning goals.
This work frames question selection as a task of
scoring and ranking candidate questions for a spe-
cific point in the tutorial dialogue. Since dialogue
is a dynamic process with multiple correct possibil-
ities, we do not restrict ourselves only to the moves
and questions found in a corpus of transcripts. In-
stead we posit ?What if we had a fully automatic
question generation system?? and subsequently use
candidate questions hand-authored for each dialogue
context. To explore the mechanisms involved in
ranking follow-up questions against one other, we
pair these questions with judgments of quality from
expert human tutors and extract surface form and
dialogue-based features to train machine learning
classification models to rank the appropriateness of
questions for specific points in a dialogue.
Our results show promise with our best question
1
ranking models exhibiting performance on par with
expert human tutors. Furthermore these experiments
demonstrate the utility and importance of rich dia-
logue move annotation for modeling decision mak-
ing in conversation and tutoring.
2 Background and Related Works
Learning tutorial dialogue policies from corpora is
a growing area of research in natural language pro-
cessing and intelligent tutoring systems. Past studies
have made use of hidden Markov models (Boyer et
al., 2009a) and reinforcement learning (Chi et al,
2010; Chi et al, 2009; Chi et al, 2008) to discover
tutoring strategies. However, these approaches are
typically optimized to maximize learning gains, and
are not necessarily focused on replicating human tu-
tor behavior. Other work has explored specific fac-
tors in questioning such as when to ask ?why? ques-
tions (Rose et al, 2003), provide hints (Tsovaltzi
and Matheson, 2001), or insert discourse markers
(Kim et al, 2000).
There is also an expanding body of work that ap-
plies ranking algorithms toward the task of ques-
tion generation (QG) using approaches such as over-
generation-and-ranking (Heilman and Smith, 2010),
language model ranking (Yao, 2010), and heuristics-
based ranking (Agarwal and Mannem, 2011). While
the focus of these efforts centers on issues of gram-
maticality, fluency, and content selection for auto-
matic creation of standalone questions, we move to
the higher level task of choosing context appropri-
ate questions. Our work merges aspects of these
QG approaches with the sentence planning tradi-
tion from natural language generation (Walker et al,
2001; Rambow et al, 2001). In sentence planning
the goal is to select lexico-structural resources that
encode communicative action. Rather than select-
ing representations, we use them directly as part of
the feature space for learning functions to rank the
questions? actual surface form realization. To our
knowledge there has been no research in ranking the
quality and suitability of questions within a tutorial
dialogue context.
Because questioning tactics depend heavily on the
curriculum and choice of pedagogy, we ground our
investigations within the context of the My Science
Tutor (MyST) intelligent tutoring system (Ward et
al., 2011b), a conversational virtual tutor designed
to improve science learning and understanding for
students in grades 3-5 (ages 8-11). Students using
MyST investigate and discuss science through nat-
ural spoken dialogues and multimedia interactions
with a virtual tutor named Marni. The MyST dia-
logue design and tutoring style is based on a ped-
agogy called Questioning the Author (QtA) (Beck
et al, 1996) which emphasizes open-ended ques-
tions and keying in on student language to promote
self-explanation of concepts, and its curriculum is
based on the Full Option Science System (FOSS) 1
a proven system for inquiry based learning.
3 Data Collection
3.1 MyST Logfiles and Transcripts
For these experiments, we use MyST transcripts col-
lected in a Wizard-of-Oz (WoZ) condition with a hu-
man tutor inserted into the interaction loop. Project
tutors trained in both QtA and in the tutorial sub-
ject matter served as the wizards. During a ses-
sion tutors were responsible for accepting, overrid-
ing, and/or authoring system actions. Tutor wizards
were also responsible for setting the current dialogue
frame to indicate which of the learning goals was
currently in focus. Students talked to MyST via mi-
crophone while MyST communicates using Text-to-
Speech (TTS) in the WoZ setting. A typical MyST
session revolves around a single FOSS lesson and
lasts approximately 15 minutes. To obtain a dia-
logue transcript, tutor moves are taken directly from
the system logfile, while student speech is manu-
ally transcribed from audio. In addition to the di-
alogue text, MyST records additional information
such as timestamps and the current dialogue frame
(i.e. learning goal). In total we make use of tran-
scripts from 122 WoZ dialogues covering 10 units
on magnetism and electricity and 2 in measurement
and standards.
3.2 Dialogue Annotation
Lesson-independent analysis of dialogue requires
a level of abstraction that reduces a dialogue to
its underlying actions and intentions. To address
this need we use the Dialogue Schema Unifying
Speech and Semantics (DISCUSS) (Becker et al,
1http://www.fossweb.com
2
2011), a multidimensional dialogue move taxon-
omy that captures both the pragmatic and seman-
tic interpretation of an utterance. Instead of us-
ing one label, a DISCUSS move is a tuple com-
posed of three dimensions: Dialogue Act, Rhetor-
ical Form, Predicate Type. Together these labels
account for the action, function, and content of an
utterance. This scheme draws from past work in
task-oriented dialogue acts (Bunt, 2009; Core and
Allen, 1997), tutorial act taxonomies (Pilkington,
1999; Tsovaltzi and Karagjosova, 2004; Buckley
and Wolska, 2008; Boyer et al, 2009b) discourse
relations (Mann and Thompson, 1986) and question
taxonomies (Graesser and Person, 1994; Nielsen et
al., 2008).
Dialogue Act (22 tags): The dialogue act dimen-
sion is the top-level dimension in DISCUSS, and its
values govern the possible values for the other di-
mensions. Though the DISCUSS dialogue act layer
seeks to replicate the learnings from other well-
established taxonomies like DIT++ (Bunt, 2009) or
DAMSL (Core and Allen, 1997) wherever possible,
the QtA style of pedagogy driving our tutoring ses-
sions dictated the addition of two tutorial specific
acts: marking and revoicing. A mark act highlights
key words from the student?s speech to draw atten-
tion to a particular term or concept. Like with mark-
ing, revoicing keys in on student language, but in-
stead of highlighting specific words, a revoice act
will summarize or refine the student?s language to
bring clarity to a concept.
Rhetorical Form (22 tags): Although the dia-
logue act is useful for identifying the speaker?s in-
tent, it gives no indication of how the speaker is ad-
vancing the conversation. The rhetorical form re-
fines the dialogue act by providing a link to its sur-
face form realization. Consider the questions ?What
is the battery doing?? and ?Which one is the bat-
tery??. They would both be labeled with Ask dia-
logue acts, but they elicit two very different kinds
of responses. The former, which elicits some form
of description, would be labeled with a Describe
rhetorical form, while the latter is seeking to Iden-
tify an object. Similarly an Assert act from a tutor
could be coupled with a Describe rhetorical form to
introduce new information or with a Recap to recon-
vey a major point.
Predicate Type (19 tags): Beyond knowing the
Reliability Metric DA RF PT
Cohen?s Kappa 0.75 0.72 0.63
Exact Agreement 0.80 0.66 0.56
Partial Agreement 0.89 0.77 0.68
Table 1: Inter-annotator agreement for DISCUSS types
(DA=Dialogue Act, RF=Rhetorical Form, PT=Predicate
Type)
propositional content of an utterance, it is useful to
know how the entities and predicates in a response
relate to one another. A student may mention several
keywords that are semantically similar to the learn-
ing goals, but it is important for a tutor to recognize
whether the student?s language provides a deeper de-
scription of some phenomena or if it is simply a su-
perficial observation. The Predicate Type aims to
categorize the semantic relationships a student may
talk about; whether it is a Procedure, a Function, a
Causal Relation, or some other predicate type.
3.2.1 Annotation
All transcripts used in this experiment have been
annotated with DISCUSS labels at the turn level. A
reliability study using 15% of the transcripts was
conducted to assess inter-rater agreement of DIS-
CUSS tagging. This consisted of 18 doubly anno-
tated transcripts comprised of 828 dialogue utter-
ances.
To assess inter-rater reliability we use Cohen?s
Kappa (?) (Carletta, 1996). Because DISCUSS per-
mits multiple labels per instance, we compute a ?
value for each label and provide a mean for each
DISCUSS dimension. To get an additional sense of
agreement, we use two other metrics: exact agree-
ment and partial agreement. For each of these met-
rics, we treat each annotators? annotations as a per
class bag-of-labels. For exact agreement, each an-
notators? set of labels must match exactly to receive
credit. Partial agreement is defined as the number
of intersecting labels divided by the total number
of unique labels. Together these statistics help to
bound the reliability of the DISCUSS annotation.
Table 1 lists all three metrics broken down by DIS-
CUSS dimension. The ? values show fair agreement
for the dialogue act and rhetorical form dimensions,
whereas the predicate type shows more moderate
agreement. This difference reflects the relative diffi-
3
culty in labeling each dimension, and the agreement
as a whole illustrates the open-endedness of the task.
3.3 Question Authoring
While the long-term plan for this work is to inte-
grate fully automatic question generation into a tu-
toring system, for this study we opted to use manu-
ally authored questions. This allows us to remain
focused on learning to identify context appropri-
ate questions rather than confounding our experi-
ments with issues of question grammaticality and
well-formedness. Even though using multiple au-
thors would provide greater diversity of questions,
to avoid repeated effort and to maintain consistency
in authoring we trained a single question author
in both the FOSS material and MyST QtA tech-
niques. Although he was free to author any ques-
tion he found appropriate, our guidelines primar-
ily emphasized authoring by making permutations
aligned with DISCUSS dimensions while also per-
mitting the author to incorporate changes in word-
ing, learning-goal content, and tutoring tactics. For
example, we taught him to consider how QtA moves
such as Revoicing, Marking, or Recapping could al-
ter otherwise similar questions. To minimize the risk
of rater bias, we explicitly told our author to avoid
using positive feedback expressions such as ?Good
job!? or ?Great!?. Table 2 illustrates how the com-
binations of DISCUSS labels, QtA tactics, and dia-
logue context drives the question generation process.
To simulate the conditions available to both the
human WoZ and computer MyST tutors, the author
was presented with the entire dialogue history pre-
ceding the decision point, the current dialogue frame
(learning goal), and any visuals that may be on-
screen. Question authoring contexts were manually
selected to capture points where students provided
responses to tutor questions. This eliminated the
need to account for other dialogue behavior such as
greetings, closings, or meta-behavior, and allowed
us to focus on follow-up style questions. Because
these question authoring contexts came from actual
tutorial dialogues, we also extracted the original turn
provided by the tutor, and we filtered out turns that
did not contain questions related to the lesson con-
tent. Our corpus has 205 question authoring contexts
comprised of 1025 manually authored questions and
131 questions extracted from the original transcript
yielding 1156 questions in total.
3.4 Ratings Collection
To rate questions, we enlisted the help of four tu-
tors who had previously served as project tutors and
wizards. The raters were presented with much of
the same information used during question author-
ing. The interface included the entire dialogue his-
tory preceding the question decision point and a list
of up to 6 candidate questions (5 manually authored,
1 taken from the original transcript if applicable). To
give a more complete tutoring context, raters also
had access to the lessons? learning goals and the in-
teractive visuals used by MyST.
Previous studies in rating questions (Becker et al,
2009) have found poor inter-rater agreement when
rating questions in isolation. To decrease the task?s
difficulty we instead ask raters to simultaneously
score all candidate questions. Because we did not
want to bias raters, we did not specify specific cri-
teria for question quality. Instead we instructed the
raters to consider the question?s role in assisting stu-
dent understanding of the learning goals and to think
about factors such as tutorial pacing, context appro-
priateness, and content. Scores were collected us-
ing an ordinal 10-point scale ranging from 1 (low-
est/worst) to 10 (highest/best).
Each set of questions was rated by at least three
tutors, and rater assignments were selected to ensure
raters never score questions from sessions they tu-
tored themselves. In total we collected ratings for
1156 question representing a total of 205 question
contexts distributed across 30 transcripts.
3.4.1 Rater Agreement
Because these judgments are subjective, a key
challenge in this work centers on understanding to
what degree the tutors agree with one another. Since
our goal is to rank questions and not to score ques-
tions, we convert each tutors scores for a given con-
text into a rank-ordered list. To compute inter-
rater agreement in ranking, we use Kendall?s-Tau
(? ) rank correlation coefficient. This measure is a
non-parametric statistic that quantifies the similarity
in orderings of data, and it is closely tied to AUC,
the area under the receiver operating characteristics
(ROC) curve. Though Kendall?s-? can vary from -1
to 1, its value is highly task dependent, and it is typ-
4
. . .
T: Tell me more about what is happening with the electricity in a complete circuit.
S: Well the battery sends all the electricity in a circuit to the motor so the motor starts to go.
Candidate Question Frame Element DISCUSS
Q1 Roll over the switch and then in your own
words, tell me again what a complete or
closed circuit is all about.
Same Same Direct/Task/Visual
Ask/Describe/Configuration
Q2 How is this circuit setup? Is it open or closed? Same Same Ask/Select/Configuration
Q3 To summarize, a closed circuit allows the
electricity to flow and the motor to spin. Now
in this circuit, we have a new component. The
switch. What is the switch all about?
Diff Diff Assert/Recap/Proposition
Direct/Task/Visual
Ask/Describe/Function
Q4 You said something about the motor spinning
in a complete circuit. Tell me more about that.
Same Same Revoice/None/None
Ask/Elaborate/CausalRelation
Table 2: Example dialogue context snippet and a collection of candidate questions. The frame, element, and DISCUSS
columns show how the questions vary from one another.
ically lower when the range of possible choices is
narrow as it is in this task. To get a single score we
average ? values across all sets of questions (con-
texts) and all pairs of raters. The mean value for all
pairs of raters and contexts is ? = 0.1478. The inter-
rater statistics are shown in table 3. While inter-rater
agreement is fairly modest, we do see lots of vari-
ation between different pairs of tutors. Addition-
ally, we found that a pair of raters agreed on the top
rated question 33% of the time. This suggests that
despite their common training and experience, the
raters may be using different criteria in rating.
To assess the tutors? internal consistency, we had
each tutor re-rate 60 sets of questions approximately
two months after their first trial, and we computed
self-agreement Kendall?s-? values using the method
above. These statistics are listed in the bottom row
of table 3. In contrast with the inter-rater agreement,
self-agreement is much more consistent giving fur-
ther evidence for a difference in criteria. Together
self and inter-rater agreement help bound expected
system performance in ranking.
4 Automatic Ranking
Because we are more interested in learning to pre-
dict which questions are more suitable for a given
tutoring scenario than we are in assigning specific
scores to questions, we approach the task of ques-
tion selection as a ranking task. To create a gold-
rater A rater B rater C rater D
rater A X 0.2590 0.1418 0.0075
rater B 0.2590 X 0.1217 0.2370
rater C 0.1418 0.1217 X 0.0540
rater D 0.0075 0.2370 0.0540 X
mean 0.1361 0.2059 0.1058 0.0995
self 0.4802 0.4022 0.2327 0.3531
Table 3: Inter-rater rank agreement (Kendall?s-? ). The
bottom row is the self-agreement for contexts they rated
in two separate trials.
standard for training and evaluation we first need to
convert the collective ratings for a set of questions
into a rank-ordered list. While the most straight-
forward way to make this conversion is to average
the ratings for each item, this approach assumes all
raters operate on the same scale. Furthermore, a sin-
gle score does not account for how a question re-
lates to other candidate questions. Instead we create
a single rank-order by tabulating pairwise wins for
all pairs of questions qi, qj , (i 6= j) within a given
dialogue context C. If rating(qi) > rating(qj),
questions qi receives a win. This is summed across
all raters for the context. The question(s) with the
most wins has rank 1. Questions with an equal num-
ber of wins are considered tied and are given the av-
erage ranking of their ordinal positions. For exam-
ple if two questions are tied for second place, they
5
are each assigned a ranking of 2.5.
Using this rank-ordering we then train a pairwise
classifier to learn a preferences function (Cohen et
al., 1998) that determines if one question has a bet-
ter rank than another. For each question qi within a
contextC, we construct a vector of features ?i. For a
pair of questions qi and qj , we then create a new vec-
tor using the difference of features: ?(qi, qj , C) =
?i ? ?j . For training, if rank(qi) < rank(qj), the
classification is positive otherwise it is negative. To
account for the possibility of ties, and to make the
difference measure appear symmetric, we train both
combinations (qi, qj) and (qj , qi). During decoding,
we run the trained classifier on all pairs and tabulate
wins using the approach described above.
For our experiments we train pairwise classi-
fiers using Mallet?s Maximum Entropy (McCallum,
2002) and SVMLight?s Support Vector Machines
models (Joachims, 1999). We also use SVMRank
(Joachims, 1999), which performs the same max-
imum margin separation as SVMLight, but uses
Kendall?s-? as a loss function to optimize for rank
ordering. We run SVMRank with a linear kernel
and model parameters of c = 2.0 and  = 0.0156.
For MaxEnt, we use Mallet?s default model param-
eters. Training and evaluation are carried out us-
ing 10-fold cross validation (3 transcripts per fold,
approximately 7 dialogue contexts per transcript).
Folds are partitioned by FOSS unit, to ensure train-
ing and evaluation are on different lessons. To ex-
plore the impact of DISCUSS representations on this
question ranking task, we train and evaluate models
by incrementally adding additional information ex-
tracted from the DISCUSS annotation.
4.1 Features
When designing features for this task, we wanted to
capture the factors that may play a role in the tutor?s
decision making process during question selection.
When rating, scorers may consider factors such as
the question?s surface form, lesson relevance, con-
textual relevance. The subsections below detail the
motivations and intuitions behind these factors.
4.1.1 Surface Form Features
When presented with a list of questions, a rater
likely bases the decision on his or her initial reaction
to the questions? wording. In some cases, wording
may supercede any other decisions regarding edu-
cational value or dialogue cohesiveness. Question
verbosity is captured by the number of words in the
question feature. Analysis of rater comments also
suggested that preferences are often tied to the ques-
tion?s form and structure. A rough measure of form
comes from the Wh-word features to mark the pres-
ence of the following question words: who, what,
why, where, when, which, and how. Additionally we
use the bag-of-part-of-speech-tags (POS) features to
provide another aspect of the question?s structure.
4.1.2 Lexical Similarity Features
Past work (Ward et al, 2011a) has shown that en-
trainment, the process of automatic alignment be-
tween dialogue partners, is a useful predictor of
learning and is a key factor in facilitating a success-
ful conversation. For question selection, we hypoth-
esize that successful tutors ask questions that dis-
play some degree of semantic entrainment with stu-
dent utterances. In MyST-based tutoring, dialogue
actions are driven by the goal of eliciting student re-
sponses that address the learning goals for the les-
son. Consequently, choosing an appropriate ques-
tion may depend on how closely student responses
align with the learning goals. To model both en-
trainment and lexical similarity we extract features
for unigram and bigram overlap of words, word-
lemmas, and part-of-speech tags between the pairs
below.
? The candidate question and the student?s last
utterance
? The candidate question and the last tutor?s ut-
terance
? The candidate question and the text of the cur-
rent learning goal
? The candidate question and the text of the other
learning goals
Example learning goals for a lesson on circuits are
provided in table 4. The current learning goal is sim-
ply the learning goal in focus at the point of question
asking according to the MyST logfile. Other learn-
ing goals are all other goals for the lesson. Using
the example from the table, if goal 2 is the current
learning goal, then goals 1 and 3 are the other goals.
6
Goal 1: Wires carry electricity and can connect
components
Goal 2: Bulb receives electricity and transforms
electricity into heat
Goal 3: A circuit provides a pathway for energy
to flow
Table 4: Example learning goals
4.1.3 DISCUSS Features
The lexical and surface form features provide
some cues about the content of the question, but
they do not account for the action or intent in tutor-
ing. The DISCUSS annotation allows us to bridge
between the question?s semantics and pragmatically
and focus on what differentiates one question from
another. Basic DISCUSS features include bags of
Dialogue Acts (DA), Rhetorical Forms (RF), and
Predicate types (PT) found in the question?s DIS-
CUSS annotation. We capture the question?s dia-
logue cohesiveness with binary features indicating
whether or not the question?s RF and PT match those
found in the previous student and tutor turns.
4.1.4 Contextualized DISCUSS Features
In tutoring, follow-up questions are licensed by
the questions that precede them. For example a tutor
may be less likely to ask how an object functions un-
til after the object has first been identified by the stu-
dent. Along a different dimension, a tutor?s line of
questioning may change to match a student?s under-
standing of the material. Struggling students may re-
quire additional opportunities to explain themselves,
while advanced students may benefit more from a
more rapid pace of instruction.
We model the conditional relevance of moves
by computing dialogue act transition probabilities
from our corpus of DISCUSS annotated tutorial di-
alogues. Although DISCUSS allows multiple tags
per dialogue turn, we simplify probability calcula-
tions by treating each DISCUSS tuple as a separate
event, and tallying all pairs of turn-turn labels. A
DISCUSS tuple consists of a Dialogue Act (DA),
Rhetorical Form (RF), and Predicate Type (PT),
and we use different subsets of the tuple to com-
pute the transition probabilities listed in equations 1-
3. All probabilities are computed using Laplace-
smoothing. When extracting features, we sum the
log of the probabilities for each DISCUSS label
present in the question.
MyST models dialogue as a sequence of seman-
tic frames which correspond to specific learning
goals. For natural language understanding, MyST
uses Phoenix semantic grammars (Ward, 1994) to
identify which elements within these frames have
been filled. To account for student progress in ques-
tion asking, we compute the conditional probabil-
ity of a DISCUSS label given the percentage of el-
ements filled in the current dialogue frame (equa-
tion 4). This progress percentage is discretized into
bins of 0-25%, 25-50%, 50-75%, and 75-100%.
p(DA,RF, PTquestion|DA,RF, PTstud. turn) (1)
p(DA,RFquestion|DA,RFstudent turn) (2)
p(PTquestion|PTstudent turn) (3)
p(DA,RF, PTques.|% elements filled) (4)
4.2 Evaluation
To evaluate our systems? performance in ranking,
we use two measures commonly used in information
retrieval: the Mean Kendall?s-? measure described
in section 3.4.1 and Mean Reciprocal Rank (MRR).
MRR is the average of the multiplicative inverse of
the rank of the highest ranking question across all
contexts. To account for ties we use the Tau-b vari-
ant of Kendall?s-? , and for MRR we compute re-
ciprocal rank by averaging the system rankings for
all of the questions tied for first. To obtain a gold-
standard ranking for comparison, we combine indi-
vidual raters? ratings using the approached described
in section 4.
5 Results and Discussion
We trained several models to investigate how differ-
ent feature classes influence overall performance in
ranking. The results for these experiments are listed
in Table 5. Because we found comparable perfor-
mance between MaxEnt and SVMLight, we only
report results for MaxEnt and SVMRank models.
In addition to MRR and Kendall?s-? , we list the
number of concordances and discordances in pair-
wise classification to give the reader another sense
of the accuracy associated with rank agreement.
Random Baseline: On average, assigning ran-
dom ranks will yield mean ?=0 and MRR=0.408.
7
Model Features Mean Num. Num. Pairwise MRR
Kendall?s-? Concord. Discord. Accuracy
MaxEnt CONTEXT+DA+PT+MATCH+POS- 0.211 1560 974 0.616 0.516
SVMRank CONTEXT+DA+PT+MATCH+POS- 0.190 1725 1154 0.599 0.555
MaxEnt CONTEXT+DA+RF+PT+MATCH+POS- 0.185 1529 1014 0.601 0.512
MaxEnt DA+RF+PT+MATCH+POS- 0.179 1510 1009 0.599 0.503
MaxEnt DA+RF+PT+MATCH+ 0.163 1506 1044 0.591 0.485
MaxEnt DA+RF+PT+ 0.147 1500 1075 0.583 0.480
MaxEnt DA+RF+ 0.130 1458 1082 0.574 0.476
MaxEnt DA+ 0.120 1417 1076 0.568 0.458
SVMRank Baseline 0.108 1601 1278 0.556 0.473
MaxEnt Baseline 0.105 1410 1115 0.558 0.448
Table 5: System scores by feature set and and machine learning model. Presence or absence of specific features is
denoted with a ?+? or ?-? otherwise the label refers to a set of features. The Baseline features consist of the Surface Form
and Lexical Similarity features described in sections 4.1.1 and 4.1.2. POS are the bag-of-POS surface form features.
DA, RF, and PT refer to the DISCUSS presence features for the Dialogue Act, Rhetorical Form, and Predicate Type
dimensions described in section 4.1.3. MATCH refers specifically to the RF and PT match features. CONTEXT
refers to the Contextualized DISCUSS features described in section 4.1.4. The best scores for each column appear in
boldface.
-1.0 -0.8 -0.6 -0.4 -0.2 -0.0 0.2 0.4 0.6 0.8 1.00
10
20
30
40
50
Freq
uen
cy
?mean=0.211
-1.0 -0.8 -0.6 -0.4 -0.2 -0.0 0.2 0.4 0.6 0.8 1.0Kendall's Tau(?) Range
0
10
20
30
40
50
Freq
uen
cy
?mean=0.105
Figure 1: Distribution of per-context Kendall?s-? values
for the top-scoring system (top), and the baseline system
(bottom).
Baseline System: Our baseline system used all
of the surface form and lexical similarity features
described above. This set of features achieves the
highest rank agreement (? = 0.105) using max-
imum entropy and the highest MRR (0.473) with
SVMRank . This improvement over the random
baseline suggests there is a correlation between a
question?s ranking and its surface form.
DISCUSS System: Table 5 shows system per-
formance steadily improves as additional DISCUSS
features are included in the model. When us-
1 2 3 4 5 6 70.00
0.05
0.10
0.15
0.20
0.25
0.30
0.35
0.40
p(R
ank
)
1/MRR=1.80
1 2 3 4 5 6 7Mean System Rank
0.00
0.05
0.10
0.15
0.20
0.25
0.30
0.35
0.40
p(R
ank
)
1/MRR=2.11
Figure 2: Distribution of per-context system ranks for the
highest rated question for the top-scoring system (top),
and the baseline system (bottom). These ranks are the
inverse of the reciprocal rank used to calculate MRR.
ing DISCUSS features, removing the part-of-speech
features gives an additional bump in performance
suggesting that there is an overlap in information
between DISCUSS representations and POS tags.
Finally, adding contextualized DISCUSS features
pushes our ranking models to their highest level
of agreement with ? = 0.211 using MaxEnt and
MRR=0.555 using SVMRank . Inspection of the
MRR values shows that without taking into account
the possibility of ties the baseline system selects
8
the top-ranked question in 44/205 (21.4%) contexts.
While the system with the best MRR score, correctly
chooses the top-ranked question in 71/205 (34.6%)
contexts ? a rate comparable to how often a pair of
raters agreed on the number-one item (33.4%).
Application of the Wilcoxon signed-rank test
shows the DISCUSS system exhibits statistically
significant improvement over the baseline system in
its distribution of Kendall?s-? values (n = 205, z =
7350, p < 0.001) and distribution of reciprocal
ranks (n = 205, z = 3739, p < 0.001). Figures 1
and 2 give visual confirmation of this improvement,
and highlight the overall reduction in negative ? val-
ues as well as the greater-than-50% increase in like-
lihood of selecting the best question first.
To get another perspective on system perfor-
mance, we evaluated our human raters on the gold-
standard rankings from the subset of questions used
for assessing internal agreement. This yielded a
mean ? between 0.2589 and 0.3619. If we remove
ratings so that the gold-standard does not include the
rater under evaluation, tutor performance drops to
a range of 0.1523 to 0.2432, which is roughly cen-
tered around the agreement exhibited by our best-
performing system.
Looking at the impact of learning algorithms
we see that SVMRank tends to perform better on
MRR while the pairwise maximum entropy mod-
els yield higher ? ?s. One possible explanation for
this discrepancy may stem from the ranking algo-
rithms? different treatment of ties. The pairwise
model permits ties, whereas the scores produced by
SVMRank produce a strict order. Without ties, it is
difficult to exactly match the raters? orderings which
had numerous ties, which can in turn produce an
overall higher number of concordances and discor-
dances than the pairwise classification model.
6 Conclusions and Future Work
We have introduced a framework for learning and
evaluating models for ranking and selecting ques-
tions for a given point in a tutorial dialogue. Fur-
thermore these experiments show that it is feasible
to learn this behavior by coupling predefined ques-
tions with ratings from trained tutors. Supplement-
ing our baseline surface form and lexical similarity
features with additional features extracted from the
dialogue context and DISCUSS dialogue act anno-
tation improves system performance in ranking to a
level on par with expert human tutors. These results
illustrate how question asking depends not only on
the form of the question but also on the underlying
dialogue action, function and content.
In the near future we plan to train models on indi-
vidual tutors to investigate which factors drive in-
dividual preferences in question asking. We also
plan to characterize system performance using auto-
matically labeled DISCUSS annotation. Lastly, we
feel these results provide a natural starting point to
explore automatic generation of questions from the
DISCUSS dialogue move representation.
Acknowledgments
This work was supported by grants from the
NSF (DRL-0733322, DRL-0733323), the IES
(R3053070434) and the DARPA GALE program
(Contract No. HR0011-06-C-0022, a supplement
for VerbNet attached to the subcontract from the
BBN-AGILE Team). Any findings, recommenda-
tions, or conclusions are those of the author and do
not necessarily represent the views of NSF, IES, or
DARPA.
References
Manish Agarwal and Prashanth Mannem. 2011. Auto-
matic gap-fill question generation from text books au-
tomatic gap-fill question generation from text books
automatic gap-fill questions from text books. In Pro-
ceedings of the Sixth Workshop on Innovative Use of
NLP for Building Educational Applications.
I. L. Beck, M. G. McKeown, J. Worthy, C. A. San-
dora, and L. Kucan. 1996. Questioning the au-
thor: A year-long classroom implementation to engage
students with text. The Elementary School Journal,
96(4):387?416.
L. Becker, R. D. Nielsen, and W. Ward. 2009. What a
pilot study says about running a question generation
challenge. In Proceedings of the Second Workshop on
Question Generation, Brighton, England, July.
L. Becker, W. Ward, S. van Vuuren, and M. Palmer. 2011.
Discuss: A dialogue move taxonomy layered over se-
mantic representations. In In Proceedings of the In-
ternational Conference on Computational Semantics
(IWCS) 2011, Oxford, England, January 12-14.
K.E. Boyer, E.Y. Ha, M. Wallis, R. Phillips, M.A. Vouk,
and J.C. Lester. 2009a. Discovering tutorial dialogue
9
strategies with hidden markov models. In Proceed-
ings of the 14th International Conference on Artificial
Intelligence in Education (AIED ?09), pages 141?148,
Brighton, U.K.
K.E. Boyer, W.J. Lahti, R. Phillips, M. D. Wallis, M. A.
Vouk, and J. C. Lester. 2009b. An empirically derived
question taxonomy for task-oriented tutorial dialogue.
In Proceedings of the Second Workshop on Question
Generation, pages 9?16, Brighton, U.K.
M. Buckley and M. Wolska. 2008. A classification of
dialogue actions in tutorial dialogue. In Proceedings
of COLING 2008, pages 73?80. ACL.
H. C. Bunt. 2009. The DIT++ taxonomy for functional
dialogue markup. In Proc. EDAML 2009.
Jean Carletta. 1996. Assessing agreement on classifi-
cation tasks: The kappa statistic. Computational Lin-
guistics, 22(2):pp. 249?254.
M. Chi, P. Jordan, K. VanLehn, and M. Hall. 2008. Re-
inforcement learning-based feature selection for devel-
oping pedagogically effective tutorial dialogue tactics.
In Ryan S. Baker, Tiffany Barnes, and Joseph Becker,
editors, Proceedings of the 1st International Confer-
ence on Educational Data Mining, pages pp258?265.
M. Chi, P. W. Jordan, K. VanLehn, and D. J. Litman.
2009. To elicit or to tell: Does it matter? In Artifi-
cial Intelligence in Education, pages 197?204.
M. Chi, K. VanLehn, and D. Litman. 2010. Do micro-
level tutorial decisions matter: Applying reinforce-
ment learning to induce do micro-level tutorial deci-
sions matter. In Vincent Aleven, Judy Kay, and Jack
Mostow, editors, Preceedings of the 10th Internation
Confernce on Intelligent Tutoring Systems (ITS 2010).
William W. Cohen, Robert E. Schapire, and Yoram
Singer. 1998. Learning to order things. In Advances
in Neural Information Processing Systems 10 (NIPS
1998).
A. Collins and A. Stevens. 1982. Goals and methods for
inquiry teachers. Advances in Instructional Psychol-
ogy, 2.
M. G. Core and J.F. Allen. 1997. Coding dialogs with the
DAMSL annotation scheme. In AAAI Fall Symposium,
pages 28?35.
A.C. Graesser and N.K. Person. 1994. Question ask-
ing during tutoring. American Educational Research
Journal, 31:104?137.
A.C. Graesser, P. Chipman, B.C Haynes, and A. Olney.
2005. Autotutor: An intelligent tutoring system with
mixed-initiative dialogue. IEEE Transactions in Edu-
cation, 48:612?618.
M. Heilman and N. A. Smith. 2010. Good question! sta-
tistical ranking for question generation. In Proceed-
ings of NAACL/HLT 2010.
T. Joachims. 1999. Making large-scale svm learning
practical. In B. Scho?lkopf, C. Burges, and A. Smola,
editors, Advances in Kernel Methods - Support Vector
Learning. MIT-Press.
J.H. Kim, M. Glass, R. Freedman, and M.W. Evens.
2000. Learning the use of discourse markers in tuto-
rial dialogue learning the use of discourse markers in
tutorial dialogue. In Proceedings of the 22nd Annual
Conference of the Cognitive Science Society.
D. Litman and S. Silliman. 2004. Itspoke: An intel-
ligent tutoring spoken dialogue system. In Compan-
ion Proceedings of the Human Language Technology
Conference: 4th Meeting of the North American Chap-
ter of the Association for Computational Linguistics
(HLT/NAACL).
W.C. Mann and S.A Thompson. 1986. Rhetorical struc-
ture theory: Description and construction of text struc-
tures. In In Proceedings of the Third International
Workshop on Text Generation, August.
A. K. McCallum, 2002. MALLET: A Machine Learning
for Language Toolkit. http://mallet.cs.umass.edu.
R. D. Nielsen, J. Buckingham, G. Knoll, B. Marsh, and
L. Palen. 2008. A taxonomy of questions for ques-
tion generation. In Proceedings of the Workshop on
the Question Generation Shared Task and Evaluation
Challenge, September.
R.M. Pilkington. 1999. Analysing educational dis-
course: The discount scheme. Technical Report 99/2,
Computer Based Learning Unit, University of Leeds.
Owen Rambow, Monica Rogati, and Marilyn A. Walker.
2001. Evaluating a trainable sentence planner for a
spoken dialogue system evaluating a trainable sen-
tence planner for a spoken dialogue system. In Pro-
ceedings of the 39th Annual Meeting of the Association
for Computational Linguistics (ACL 2001).
C.P. Rose, P. Jordan, M. Ringenberg, S. Siler, K. Van-
Lehn, and A. Weinstein. 2001. A comparative evalu-
ation of socratic versus didactic tutoring. In Proceed-
ings of Cognitive Sciences Society.
C.P. Rose, D. Bhembe, S. Siler, R. Srivastava, and
K. VanLehn. 2003. The role of why questions in ef-
fective human tutoring. In Proceedings of Artificial
Intelligence in Education (AIED 2003).
D. Tsovaltzi and E. Karagjosova. 2004. A view on dia-
logue move taxonomies for tutorial dialogues. In Pro-
ceedings of SIGDIAL 2004, pages 35?38. ACL.
D. Tsovaltzi and C. Matheson. 2001. Formalising hint-
ing in tutorial dialogues. In In EDILOG: 6th workshop
on the semantics and pragmatics of dialogue, pages
185?192.
K. VanLehn, A.C. Graesser, G.T. Jackson, P. Jordan,
A. Olney, and C.P. Rose. 2007. When are tutorial
dialogues more effective than reading? Cognitive Sci-
ence, 31(1):3?62.
10
Marilyn A. Walker, Owen Rambow, and Monica Rogati.
2001. SPOT: A trainable sentence planner. In Pro-
ceedings of the North American Meeting of the Asso-
ciation for Computational Linguistics (NAACL).
A. Ward, D. Litman, and M. Eskenazi. 2011a. Predict-
ing change in student motivation by measuring cohe-
sion between predicting change in student motivation
by measuring cohesion between tutor and student. In
Proceedings of the Sixth Workshop on Innovative Use
of NLP for Building Educational Applications, pages
136?141.
W. Ward, R. Cole, D. Bolan?os, C. Buchenroth-Martin,
E. Svirsky, S. van Vuuren, T. Weston, J. Zheng, and
L. Becker. 2011b. My science tutor: A conversa-
tional multi-media virtual tutor for elementary school
science. ACM Transactions on Speech and Language
Processing (TSLP), 7(4), August.
W. Ward. 1994. Extracting information from sponta-
neous speech. In Proceedings of the International
Conference on Speech and Language Processing (IC-
SLP).
Xuchen Yao. 2010. Question generation with minimal
recursion semantics. Master?s thesis, Saarland Uni-
versity.
11
