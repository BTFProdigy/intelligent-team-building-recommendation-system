Maytag: A multi-staged approach to identifying
complex events in textual data
Conrad Chang, Lisa Ferro, John Gibson, Janet Hitzeman, Suzi Lubar, Justin Palmer,
Sean Munson, Marc Vilain, and Benjamin Wellner
The MITRE Corporation
202 Burlington Rd.
Bedford, MA 01730 USA
contact: mbv@mitre.org (Vilain)
Abstract
We present a novel application of NLP
and text mining to the analysis of finan-
cial documents. In particular, we de-
scribe an implemented prototype, May-
tag, which combines information extrac-
tion and subject classification tools in an
interactive exploratory framework. We
present experimental results on their per-
formance, as tailored to the financial do-
main, and some forward-looking exten-
sions to the approach that enables users
to specify classifications on the fly.
1 Introduction
Our goal is to support the discovery of complex
events in text. By complex events, we mean
events that might be structured out of multiple
occurrences of other events, or that might occur
over a span of time. In financial analysis, the
domain that concerns us here, an example of
what we mean is the problem of understanding
corporate acquisition practices. To gauge a
company?s modus operandi in acquiring other
companies, it isn?t enough to know just that an
acquisition occurred, but it may also be impor-
tant to understand the degree to which it was
debt-leveraged, or whether it was performed
through reciprocal stock exchanges.
In other words, complex events are often
composed of multiple facets beyond the basic
event itself. One of our concerns is therefore to
enable end users to access complex events
through a combination of their possible facets.
Another key characteristic of rich domains
like financial analysis, is that facts and events are
subject to interpretation in context. To a finan-
cial analyst, it makes a difference whether a
multi-million-dollar loss occurs in the context of
recurring operations (a potentially chronic prob-
lem), or in the context of a one-time event, such
as a merger or layoff. A second concern is thus
to enable end users to interpret facts and events
through automated context assessment.
The route we have taken towards this end is to
model the domain of corporate finance through
an interactive suite of language processing tools.
Maytag, our prototype, makes the following
novel contribution. Rather than trying to model
complex events monolithically, we provide a
range of multi-purpose information extraction
and text classification methods, and allow the
end user to combine these interactively. Think
of it as Boolean queries where the query terms
are not keywords but extracted facts, events, en-
tities, and contextual text classifications.
2 The Maytag prototype
Figure 1, below, shows the Maytag prototype
in action. In this instance, the user is browsing a
particular document in the collection, the 2003
securities filings for 3M Corporation. The user
has imposed a context of interpretation by select-
ing the ?Legal matters? subject code, which
causes the browser to only retrieve those portions
of the document that were statistically identified
as pertaining to law suits. The user has also se-
lected retrieval based on extracted facts, in this
case monetary expenses greater than $10 million.
This in turn causes the browser to further restrict
retrieval to those portions of the document that
contain the appropriate linguistic expressions,
e.g., ?$73 million pre-tax charge.?
As the figure shows, the granularity of these
operations in our browser is that of the para-
graph, which strikes a reasonable compromise
between providing enough context to interpret
retrieval results, but not too much. It is also ef-
131
fective at enabling combination of query terms.
Whereas the original document contains 5161
paragraphs, the number of these that were tagged
with the ?Legal matters? code is 27, or .5 percent
of the overall document. Likewise, the query for
expenses greater than $10 million restricts the
return set to 26 paragraphs (.5 percent). The
conjunction of both queries yields a common
intersection of only 4 paragraphs, thus precisely
targeting .07 percent of the overall document.
Under the hood, Maytag consists of both an
on-line component and an off-line one. The on-
line part is a web-based GUI that is connected to
a relational database via CGI scripts (html,
JavaScript, and Python). The off-line part of the
system hosts the bulk of the linguistic and statis-
tical processing that creates document meta-data:
name tagging, relationship extraction, subject
identification, and the like. These processes are
applied to documents entering the text collection,
and the results are stored as meta-data tables.
The tables link the results of the off-line process-
ing to the paragraphs in which they were found,
thereby supporting the kind of extraction- and
classification-based retrieval shown in Figure 1.
3 Extraction in Maytag
As is common practice, Maytag approaches
extraction in stages. We begin with atomic
named entities, and then detect structured
entities, relationships, and events. To do so, we
rely on both rule-based and statistical means.
3.1 Named entities
In Maytag, we currently extract named entities
with a tried-but-true rule-based tagger based on
the legacy Alembic system (Vilain, 1999). Al-
though we?ve also developed more modern sta-
tistical methods (Burger et al 1999, Wellner &
Vilain, 2006), we do not currently have adequate
amounts of hand-marked financial data to train
these systems. We therefore found it more con-
venient to adapt the Alembic name tagger by
manual hill climbing. Because this tagger was
originally designed for a similar newswire task,
we were able to make the port using relatively
small amounts of training data. We relied on two
100+ page-long Securities filings (singly anno-
tated), one for training, and the other for test, on
which we achieve an accuracy of F=94.
We found several characteristics of our finan-
cial data to be especially challenging. The first is
the widespread presence of company name look-
alikes, by which we mean phrases like ?Health
Care Markets? or ?Business Services? that may
look like company names, but in fact denote
business segments or the like. To circumvent
this, we had to explicitly model non-names, in
effect creating a business segment tagger that
captures company name look-alikes and prevents
them from being tagged as companies.
Another challenging characteristic of these fi-
nancial reports is their length, commonly reach-
ing hundreds of pages. This poses a quandary
Figure 1: The Maytag interface
132
for the way we handle discourse effects. As with
most name taggers, we keep a ?found names? list
to compensate for the fact that a name may not
be clearly identified throughout the entire span of
the input text. This list allows the tagger to
propagate a name from clear identifying contexts
to non-identified occurrences elsewhere in the
discourse. In newswire, this strategy boosts re-
call at very little cost to precision, but the sheer
length of financial reports creates a dispropor-
tionate opportunity for found name lists to intro-
duce precision errors, and then propagate them.
3.2 Structured entities, relations, and events
Another way in which financial writing differs
from general news stories is the prevalence of
what we?ve called structured entities, i.e., name-
like entities that have key structural attributes.
The most common of these relate to money. In
financial writing, one doesn?t simply talk of
money: one talks of a loss, gain or expense, of
the business purpose associated therewith, and of
the time period in which it is incurred. Consider:
Worldwide expenses for environmental
compliance [were] $163 million in 2003.
To capture such cases as this, we?ve defined a
repertoire of structured entities. Fine-grained
distinctions about money are encoded as color of
money entities, with such attributes as their color
(in this case, an operating expense), time stamp,
and so forth. We also have structured entities for
expressions of stock shares, assets, and debt.
Finally, we?ve included a number of constructs
that are more properly understood as relations
(job title) or events (acquisitions).
3.3 Statistical training
Because we had no existing methods to address
financial events or relations, we took this oppor-
tunity to develop a trainable approach. Recent
work has begun to address relation and event
extraction through trainable means, chiefly SVM
classification (Zelenko et al 2003, Zhou et al
2005). The approach we?ve used here is classi-
fier-based as well, but relies on maximum en-
tropy modeling instead.
Most trainable approaches to event extraction
are entity-anchored: given a pair of relevant enti-
ties (e.g., a pair of companies), the object of the
endeavor is to identify the relation that holds be-
tween them (e.g., acquisition or subsidiary). We
turn this around: starting with the head of the
relation, we try to find the entities that fill its
constituent roles. This is, unavoidably, a
strongly lexicalized approach. To detect an
event such as a merger or acquisition, we start
from indicative head words, e.g., ?acquire,?
?purchases,? ?acquisition,? and the like.
The process proceeds in two stages. Once
we?ve scanned a text to find instances of our in-
dicator heads, we classify the heads to determine
whether their embedding sentence represents a
valid instance of the target concept. In the case
of acquisitions, this filtering stage eliminates
such non-acquisitions as the use of the word
?purchases? in ?the company purchases raw ma-
terials.? If a head passes this filter, we find the
fillers of its constituent roles through a second
classification stage
The role stage uses a shallow parser to chunk
the sentence, and considers the nominal chunks
and named entities as candidate role fillers. For
acquisition events, for example, these roles in-
clude the object of the acquisition, the buying
agent, the bought assets, the date of acquisition,
and so forth (a total of six roles). E.g.
In the fourth quarter of 2000 (WHEN), 3M
[AGENT] also acquired the multi-layer inte-
grated circuit packaging line [ASSETS] of
W.L. Gore and Associates [OBJECT].
The maximum entropy role classifier relies on
a range of feature types: the semantic type of the
phrase (for named entities), the phrase vocabu-
lary, the distance to the target head, and local
context (words and phrases).
Our initial evaluation of this approach has
given us encouraging first results. Based on a
hand-annotated corpus of acquisition events,
we?ve measured filtering performance at F=79,
and role assignment at F=84 for the critical case
of the object role. A more recent round of ex-
periments has produced considerably higher per-
formance, which we will report on later this year.
4 Subject Classification
Financial events with similar descriptions can
mean different things depending on where these
events appear in a document or in what context
they appear. We attempt to extract this important
contextual information using text classification
methods. We also use text classification methods
to help users to more quickly focus on an area
where interesting transactions exist in an interac-
tive environment. Specifically, we classify each
paragraph in our document collection into one of
several interested financial areas. Examples in-
clude: Accounting Rule Change, Acquisitions
and Mergers, Debt, Derivatives, Legal, etc.
133
4.1 Experiments
In our experiments, we picked 3 corporate an-
nual reports as the training and test document set.
Paragraphs from these 3 documents, which are
from 50 to 150 pages long, were annotated with
the types of financial transactions they are most
related to. Paragraphs that did not fall into a
category of interest were classified as ?other?.
The annotated paragraphs were divided into ran-
dom 4x4 test/training splits for this test. The
?other? category, due to its size, was sub-
sampled to the size of the next-largest category.
As in the work of Nigam et al(2002) or Lodhi
et al(2002), we performed a series of experi-
ments using maximum entropy and support vec-
tor machines. Besides including the words that
appeared in the paragraphs as features, we also
experimented with adding named entity expres-
sions (money, date, location, and organization),
removal of stop words, and stemming. In gen-
eral, each of these variations resulted in little dif-
ference compared with the baseline features con-
sisting of only the words in the paragraphs.
Overall results ranged from F-measures of 70-75
for more frequent categories down to above 30-
40 for categories appearing less frequently.
4.2 Online Learning
We have embedded our text classification
method into an online learning framework that
allows users to select text segments, specify
categories for those segments and subsequently
receive automatically classified paragraphs simi-
lar to those already identified. The highest con-
fidence paragraphs, as determined by the classi-
fier, are presented to the user for verification and
possible re-classification.
Figure 1, at the start of this paper, shows the
way this is implemented in the Maytag interface.
Checkboxes labeled pos and neg are provided
next to each displayed paragraph: by selecting
one or the other of these checkboxes, users indi-
cate whether the paragraph is to be treated as a
positive or a negative example of the category
they are elaborating. In our preliminary studies,
we were able to achieve the peak performance
(the highest F1 score) within the first 20 training
examples using 4 different categories.
5 Discussion and future work
The ability to combine a range of analytic
processing tools, and the ability to explore their
results interactively are the backbone of our ap-
proach. In this paper, we?ve covered the frame-
work of our Maytag prototype, and have looked
under its hood at our extraction and classification
methods, especially as they apply to financial
texts. Much new work is in the offing.
Many experiments are in progress now to as-
sess performance on other text types (financial
news), and to pin down performance on a wider
range of events, relations, and structured entities.
Another question we would like to address is
how best to manage the interaction between clas-
sification and extraction: a mutual feedback
process may well exist here.
We are also concerned with supporting finan-
cial analysis across multiple documents. This
has implications in the area of cross-document
coreference, and is also leading us to investigate
visual ways to define queries that go beyond the
paragraph and span many texts over many years.
Finally, we are hoping to conduct user studies
to validate our fundamental assumption. Indeed,
this work presupposes that interactive application
of multi-purpose classification and extraction
techniques can model complex events as well as
monolithic extraction tools ? laMUC.
Acknowledgements
This research was performed under a MITRE
Corporation sponsored research project.
References
Zhou, G., Su J., Zhang, J., and Zhang, M. 2005. Ex-
ploring various knowledge in relation extraction.
Proc. of the 43rd ACL Conf, Ann Arbor, MI.
Nigam, K., Lafferty, J., and McCallum, A. 1999. Us-
ing maximum entropy for text classification. Proc.
of IJCAI ?99 Workshop on Information Filtering.
Lodhi, H., Saunders, C., Shawe-Taylor, J., Cristianini,
and N., Watkins, C. 2002. Text classification using
string kernels. Journal of Machine Learning Re-
search, Vol. 2, pp. 419-444.
Vilain, M. and Day, D. 1996. Finite-state Phrase Pars-
ing by Rule Sequences, Proc. of COLING-96.
Vilain, M. 1999. Inferential information extraction.
In Pazienza, M.T. & Basili, R., Information Ex-
traction. Springer Verlag.
Wellner, B., and Vilain, M. (2006) Leveraging ma-
chine readable dictionaries in discriminative se-
quence models. Proc. of LREC 2006 (to appear).
Zelenko D., Aone C. and Richardella. 2003. Kernel
methods for relation extraction. Journal of Ma-
chine Learning Research. pp1083-1106.
134
Proceedings of NAACL HLT 2007, Companion Volume, pages 181?184,
Rochester, NY, April 2007. c?2007 Association for Computational Linguistics
Entity Extraction is a Boring Solved Problem ? or is it? 
Marc Vilain The MITRE Corporation Burlington Rd Bedford MA 01730 USA mbv@mitre.org 
Jennifer Su The MITRE Corporation and Cornell University Ithaca NY 14853 USA jfs29@cornell.edu 
Suzi Lubar The MITRE Corporation Burlington Rd Bedford MA 01730 USA slubar@mitre.org  
Abstract This paper presents empirical results that contradict the prevailing opinion that en-tity extraction is a boring solved problem.  In particular, we consider data sets that resemble familiar MUC/ACE data, and re-port surprisingly poor performance for both commercial and research systems.  We then give an error analysis that sug-gests research challenges for entity ex-traction that are neither boring nor solved. 
1 Background Entity extraction or named entity recognition, as it is sometimes called, is a known and familiar prob-lem.  Named entity (NE) tagging has been the sub-ject of numerous shared-task evaluations, including the seminal MUC 6, MUC 7 and MET evaluations, the CoNLL shared task, the SIGHAN bake-offs, and the ACE evaluations.  With this track record, and with commercial vendors now selling named-entity tagging for a fee, many naturally consider entity extraction to be an essentially solved problem.  The present paper challenges this view. The main issue, as we see it, is transfer: NE tag-gers developed for a specific corpus tend not to perform well on other data sets.  Kosseim and Poibeau (2001), for one, show that the informal language of email or speech transcriptions befud-dles taggers built for journalistic text.  Minkov et al(2005) further explore the systematic differences between journalistic and informal texts, training separate taggers for each text source of interest. Because named entity taggers are so strongly based on surface features, it isn?t surprising to ob-
serve poor tagger transfer across texts with signifi-cantly different styles or with unrelated content.  In this paper, we report on the more surprising result that transfer issues arise even for texts with closely aligned content or closely aligned styles. In particular, we consider a range of primarily business-related texts that are, on the face of it, close in style and/or substance to the journalistic stories in existing NE data sets, MUC 6 in particular.  We thus would have expected these texts to sup-port good transfer performance from taggers con-figured to the MUC task.  Instead, we found the same kinds of performance drops as Kosseim and Poibeau had noted for informal texts.  Our aim here is to shed light on the how and why of this. 2 Scope of the present study We begin with a disclaimer.  Our goal is not so much to present new technical solutions to NE rec-ognition, as to draw attention to those aspects of the problem that remain unsolved.  We cover two main thrusts: (i) a black-box evaluation of several NE taggers (commercial and research systems); and (ii) an error analysis of system performance. 2.1 Evaluation data Our evaluation data set contains three distinct sec-tions.  The largest component consists of publicly-available financial reports filed with the Securities and Exchange Commission (SEC), in particular the 2003 forms 10-K filed by eight Fortune 500 com-panies.  These corporate annual reports share the same subject matter as much business news: sales, profits, acquisitions, business strategies and the like.  They take, however, a more technical slant and are rich in accounting jargon.  They are also longer, ranging in our study from 22 to 54 pages. 
181
Preliminary exploration with our own MUC 6 tagger showed these SEC filings to be particularly hard to tag.  Because their sheer length and techni-cal emphasis seemed implicated in this poor per-formance, we assembled a second corpus of forty Web-hosted business stories from such news pro-viders as MS-NBC, CNN Money, and Motley Fool.  These stories focus on the same eight companies as our 10-K data set, but are shorter and less techni-cal, thus allowing us to isolate length and techni-cality as factors in tagging business texts. The final portion of our test set consists of ten news stories that were selected to closely match the kind of data used in past MUC evaluations.  They were drawn from the New York Times (NYT) and Wall Street Journal (WSJ) on-line editions, and fo-cus on current events, thus providing one more comparable dimension of evaluation.1 2.2 Evaluated systems Five systems participated in our study, represent-ing a range of commercial tools and research pro-totypes.  Two of these are state-of-the-art hand-built systems based on rule/pattern interpreters.  Two are open-source statistical systems, one based on HMMs, and the other on CRFs; both were trained on the MUC 6 data set.  The final system is our own legacy MUC-style tagger, noted as Ariel in Table 1.  Except as noted below, all the systems were run out of the box, with no adaptation to the data. License and privacy concerns prevent us from identifying all the systems; instead this paper re-ports most results anonymously, using the names of Disney heroines as system pseudonyms.  We have, however exposed the identity of our own system out of fairness, as it benefited somewhat from earlier tuning to SEC forms 10-K. 2.3 Evaluation method We attempted to replicate the procedure used in the MUC evaluations, extending it only as required by                                                 1 We will make the non-copyrighted part of our corpus (the 10-Ks) available to other researchers. 
the characteristics of the taggers.  The test data were formatted as in MUC 6, and where SGML markup ran afoul of system I/O characteristics, we remapped the data manually, resolving, e.g., cross-ing tags that may have strayed into the output. To provide scores that could be compared with the MUC evaluations, we created MUC6-compliant answer keys (Sundheim, 1995), and remapped sys-tem output to this standard.  We removed system responses that were considered non-taggable in MUC (e.g., URLs) and conflated fine-grained dis-tinctions not made in MUC (e.g., remapping coun-try tags to location).  Scores were assessed with the venerable MUC scorer, which provides partial credit for system responses that match the key in type but not extent, or vice-versa.  The scorer also provides a full error analysis, separately character-izing each error in a system response. 3 Findings Table 2, overleaf, presents our overall findings, aggregated across the three primary entity types: person, organization, and location (the ENAMEX types in the MUC standard).  We generally did not measure the MUC TIMEX (dates, times) and NUMEX types (moneys, percents) because: (i) neither of the statistical systems generate them; (ii) those systems that do generate them tend to do well; (iii) they are overwhelmingly more frequent in the SEC data than in news, thus skewing results.  For completeness? sake, however, Table 2 does provide all-entity news scores in parentheses for those systems that happened to generate the full set of MUC-6 entities. Turning now to actual performance measure-ments, Table 2 does not present an especially pretty picture.  Aside from two systems? runs on the MUC-like current events, all the scores are sub-stantially below those obtained by competitive MUC systems, which typically reached F scores in the mid-90s, with a high of F=96 at MUC-6. SEC.  The worst performances were turned in for SEC filings, as shown in the first block of rows in Table 2.  While precision is generally poor, re-call is even worse.  One reason for this is the very frequent rightwards shortenings of company names (e.g., from 3M Corporation to the Corporation), in contrast to the leftwards shortening (e.g., 3M) fa-vored in news texts.  Ariel had been tuned to tag all these cases, but the other systems only tagged a scattershot fraction.  To isolate the contribution of 
Pocahontas Rule-based Belle Rule-based Jasmine Statistical, HMM, MUC-trained Mulan Statistical, CRF, MUC-trained Ariel Rule-based, 10-K tuning Table 1: system pseudonyms. 
182
these cases to system recall error, we recalculated the scores by making the cases optional.  The scorer removes missing optional responses from the recall denominator, and as expected recall im-proved; see the second block in Table 2.  Business news.  The most consistent perform-ance across systems was achieved with business news, with scores ranging in F=69-80.  This is a huge improvement over the gaping F=36-75 range we saw with SEC filings (F=43-75 with optional short names).  This confirms that length and finan-cial jargon are implicated in the poor performance on forms 10-K.  Nonetheless, these improved scores are still 15-20 points lower than the better MUC scores.  Is business language just hard to tag? MUC-like news.  Our attempt to replicate the MUC evaluation data yields an equivocal answer.  Two systems (Pocahontas and Ariel) achieved MUC6-level scores; it may not be coincidental that both are next-generation versions of systems that participated at MUC.  Of the other systems, MUC-trained Mulan also showed substantial improve-ment going from business news to current events. While it is good news that three of the systems that were explicitly trained on MUC (manually or statistically) did well on MUC-like data, it is disqui-eting to see how poorly this training generalized to other news texts. 4 Factors affecting performance A finer analysis of our three data sets helps trian-gulate the factors leading to the systematic per-formance differences shown in Table 2. Prevalence of organizations.  One factor espe-cially stands out: as Table 3 shows, organizations 
are twice as prevalent in the business sources as in the MUC-like data.  As organization scores gener-ally trail scores for persons and locations (Table 4), this partly explains why business texts are hard. Kinds of organizations.  But that does not ex-plain it all.  The profiles in Figure 1 show that cur-rent events favor government/quasi-government names (e.g.,?Congress,? ?Hamas?).  They are less linguistically productive than the corporate and quasi-corporate names in business texts, and so are more amenable to being explicitly listed in name gazetteers.  Florian et al(2003) note the effective-ness of gazetteers for tagging the CoNLL corpus. Editorial standards.  Our business news data reflect a growing portion of Web-hosted texts that relax the journalistic editorial rules of traditional news sources such as the NYT or WSJ.  For in-stance, our data show the same frequent omission of corporate designators (e.g. ?inc.?) that Kosseim noted in informal text.  Whereas news sources of record will generally mention a company?s desig-nator at least once in a story, our business data fre-quently fail to do so at all, thus removing a key name-tagging cue.  By tracing the Ariel rule base, we found that the absence of any designator was implicated in 81% of the system?s recall error for organization names. Length.  Name taggers often overcome this kind of missing evidence by second-passing a text, propagating name mentions identified in the first 
 Pocahontas Belle Jasmine Mulan Ariel R=58 R=28 R=50 R=50 R=71 P=65 P=52 P=43 P=56 P=79 SEC filings F=61.1 F=36.4 F=42.7 F=52.6 F=74.5 R=71 R=36 R=55 R=60 R=71 P=65 P=52 P=40 P=56 P=79 SEC filings, ?the Corp.? optional F=68.0 F=42.8 F=46.2 F=57.9 F=74.7 R=80 (82) R=64 (69) R=76 R=65 R=71 (75) P=80 (79) P=86 (83) P=63 P=74 P=74 (75) Business news F=80.1 (81) F=73.5 (75) F=69.1 F=69.2 F=72.3 (75) R=94 (94) R=59 (63) R=79 R=79 R=89 (91) P=94 (93) P=82 (80) P=70 P=92 P=91 (92) Current events (MUC-like) F=94.3 (94) F=68.5 (71) F=74.5 F=84.9 F=90.4 (92) Table 2: aggregated extraction scores, ENAMEX only, unless parenthesized (in parens = all entities). 
 SEC Business MUC Org 70% 65% 29% Per 9% 23% 35% Loc 21% 12% 36% Table 3: Relative distribution of entity types 
183
pass to matching but undetected mentions (Mik-heev, 1999).  This strategy runs foul, though, when the first pass produces precision errors, as these too can get propagated.  Document length is implicated in this through the greater cumulative likelihood of making an error on the first pass and of finding a mention that matches the error on the second pass. Quasi-names and non-names.  A final factor that especially afflicts the Forms 10-K is the simi-larity of names and non-names.  Non-taggable product names (?AMD Athlon?) often look like le-gitimate subsidiaries, while valid operating divi-sions (?Health Care?) are often hard to distinguish from generic designations of market segments.  5 Implications for further research. What surprised us most in conducting this study was to find so obvious a transfer gap among what appear to be very similar text sources.  We were also surprised by the involvement in this of relaxed editorial standards around seeming trivia (like the keyword ?inc.?)  This suggests, for one, that cur-rent techniques remain too dependent on skin-deep word co-occurrence features.  It also suggests that the editorially pristine news texts used in so much NE research may be atypically easy to tag. While name-tagging programs may struggle with editorially informal texts, the absence of sur-
face contextual cues poses no noticeable challenge to human readers.  What cues are left, and there are many, are semantic in nature: predicate-argument structure, selectional restrictions, organization of the lexicon, etc.  Recent efforts to create common propositional banks and lexical ontologies may thus have much to offer.  Indeed, current research in these areas is just beginning to trickle down to the name-tagging problem (Mohit & Hwa, 2005). Another key issue is ensuring tagging coher-ency at the whole-document level.  This might help alleviate the kind of error propagation with dual-pass strategies that particularly afflicts long docu-ments.  Recent applications of statistical co-reference models are beginning to show promise (Finkel et al 2005; Ji & Grishman, 2005). Lastly, we can see this whole study as a particu-lar challenge case for transfer learning, and indeed such work as Sutton and McCallum?s (2005) has looked at the name-tagging task from a transfer learning standpoint. It may thus be that today?s exciting emerging work in ?unsolved? areas ? semantics, reference, and learning ? could come to play a key role in what is sometimes maligned as yesterday?s boring solved problem. References Finkel J R, Grenager T, Manning C (2005). Incorporat-ing non-local information into information extraction systems by Gibbs sampling, Proc ACL, Ann Arbor. Florian R, Ittycheriah A, Jing H, Zhang T (2003). Named entity recognition through classifier combina-tion, Proc CoNLL, Edmonton. Ji H, Grishman R (2005). Improving name tagging by reference resolution and relation detection, Proc ACL. Kosseim L, Poibeau T (2001). Extraction de noms pro-pres ? partir de textes vari?s.  Proc. TALN, Toulouse. Mikheev A, Moens M, Grover C (1999). Named entity recognition without gazetteers.  Proc. EACL, Bergen. Minkov E, Wang R, Cohen W (2005). Extracting per-sonal names from email. Proc HLT/EMNLP, Vancouver. Mohit B, Hwa R (2005) Syntax-based semi-supervised named entity tagging.  Proc ACL, Ann Arbor MI. Sundheim B (1995), ed. Proc MUC-6, Columbia MD. Sutton C, McCallum A (2005). Composition of CRFs for transfer learning, Proc HLT/EMNLP, Vancouver. 
 Poca. Belle Jasm. Mul. Ariel S org F=62 F=10 F=46 F=53 F=83   opt F=74 F=14 F=52 F=61 F=83 S per F=75 F=65 F=49 F=64 F=60 S loc F=79 F=77 F=49 F=74 F=78 B org F=77 F=72 F=70 F=63 F=66 B. per F=90 F=85 F=70 F=69 F=79 B. loc F=78 F=76 F=59 F=75 F=73 M org F=90 F=58 F=48 F=80 F=80 M per F=99 F=90 F=84 F=81 F=92 M loc F=98 F=81 F=74 F=89 F=95 Table 4: Type subcores (S=SEC, B=biz., M=MUC) 
184
