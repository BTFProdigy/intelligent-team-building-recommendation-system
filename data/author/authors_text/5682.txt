Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics, pages 377?386,
Avignon, France, April 23 - 27 2012. c?2012 Association for Computational Linguistics
Inferring Selectional Preferences from Part-Of-Speech N-grams 
 
 
Hyeju Jang and Jack Mostow 
Project LISTEN (www.cs.cmu.edu/~listen), School of Computer Science 
Carnegie Mellon University 
Pittsburgh, PA 15213, USA 
hyejuj@cs.cmu.edu, mostow@cs.cmu.edu 
 
 
 
 
 
 
Abstract 
We present the PONG method to compute 
selectional preferences using part-of-speech 
(POS) N-grams.  From a corpus labeled with 
grammatical dependencies, PONG learns the 
distribution of word relations for each POS 
N-gram.  From the much larger but unlabeled 
Google N-grams corpus, PONG learns the 
distribution of POS N-grams for a given pair 
of words.  We derive the probability that one 
word has a given grammatical relation to the 
other. PONG estimates this probability by 
combining both distributions, whether or not 
either word occurs in the labeled corpus.  
PONG achieves higher average precision on 
16 relations than a state-of-the-art baseline in 
a pseudo-disambiguation task, but lower 
coverage and recall. 
1 Introduction 
Selectional preferences specify plausible fillers 
for the arguments of a predicate, e.g., celebrate.  
Can you celebrate a birthday?  Sure.  Can you 
celebrate a pencil?  Arguably yes:  Today the 
Acme Pencil Factory celebrated its one-billionth 
pencil.  However, such a contrived example is 
unnatural because unlike birthday, pencil lacks a 
strong association with celebrate.  How can we 
compute the degree to which birthday or pencil 
is a plausible and typical object of celebrate? 
Formally, we are interested in computing the 
probability Pr(r | t, R), where (as Table 1 
specifies), t is a target word such as celebrate, r 
is a word possibly related to it, such as birthday 
or pencil, and R is a possible relation between 
them, whether a semantic role such as the agent 
of an action, or a grammatical dependency such 
as the object of a verb.  We call t the ?target? 
because originally it referred to a vocabulary 
word targeted for instruction, and r its ?relative.? 
 
Notation Description 
R a relation between words 
t a target word 
r, r' possible relatives of t 
g a word N-gram 
gi and gj i
th and jth words of g 
p the POS N-gram of g 
 
Table 1:  Notation used throughout this paper 
 
Previous work on selectional preferences has 
used them primarily for natural language analytic 
tasks such as word sense disambiguation (Resnik, 
1997),  dependency parsing (Zhou et al 2011), 
and semantic role labeling (Gildea and Jurafsky, 
2002).  However, selectional preferences can 
also apply to natural language generation tasks 
such as sentence generation and question 
generation.  For generation tasks, choosing the 
right word to express a specified argument of a 
relation requires knowing its connotations ? that 
is, its selectional preferences.  Therefore, it is 
useful to know selectional preferences for many 
different relations.  Such knowledge could have 
many uses.  In education, they could help teach 
word connotations.  In machine learning they 
could help computers learn languages.  In 
machine translation, they could help generate 
more natural wording. 
This paper introduces a method named PONG 
(for Part-Of-Speech N-Grams) to compute 
selectional preferences for many different 
relations by combining part-of-speech 
information and Google N-grams.  PONG 
achieves higher precision on a pseudo-
377
disambiguation task than the best previous model 
(Erk et al 2010), but lower coverage. 
The paper is organized as follows.  Section 2 
describes the relations for which we compute 
selectional preferences.  Section 3 describes 
PONG.  Section 4 evaluates PONG.  Section 5 
relates PONG to prior work.  Section 6 concludes.   
2 Relations Used 
Selectional preferences characterize constraints 
on the arguments of predicates.  Selectional 
preferences for semantic roles (such as agent and 
patient) are generally more informative than for 
grammatical dependencies (such as subject and 
object).  For example, consider these 
semantically equivalent but grammatically 
distinct sentences: 
Pat opened the door. 
The door was opened by Pat.   
In both sentences the agent of opened, namely 
Pat, must be capable of opening something ? an 
informative constraint on Pat.  In contrast, 
knowing that the grammatical subject of opened 
is Pat in the first sentence and the door in the 
second sentence tells us only that they are nouns. 
Despite this limitation, selectional preferences 
for grammatical dependencies are still useful, for 
a number of reasons.  First, in practice they 
approximate semantic role labels.  For instance, 
typically the grammatical subject of opened is its 
agent.  Second, grammatical dependencies can be 
extracted by parsers, which tend to be more 
accurate than current semantic role labelers.  
Third, the number of different grammatical 
dependencies is large enough to capture diverse 
relations, but not so large as to have sparse data 
for individual relations.  Thus in this paper, we 
use grammatical dependencies as relations. 
A parse tree determines the basic grammatical 
dependencies between the words in a sentence.  
For instance, in the parse of Pat opened the door, 
the verb opened has Pat as its subject and door 
as its object, and door has the as its determiner.  
Besides these basic dependencies, we use two 
additional types of dependencies. 
Composing two basic dependencies yields a 
collapsed dependency (de Marneffe and Manning, 
2008).  For example, consider this sentence: 
The airplane flies in the sky. 
Here sky is the prepositional object of in, which 
is the head of a prepositional phrase attached to 
flies.  Composing these two dependencies yields 
the collapsed dependency prep_in between flies 
and sky, which captures an important semantic 
relation between these two content words:  sky is 
the location where flies occurs.  Other function 
words yield different collapsed dependencies.  
For example, consider these two sentences: 
The airplane flies over the ocean. 
The airplane flies and lands. 
Collapsed dependencies for the first sentence 
include prep_over between flies and ocean, 
which characterizes their relative vertical 
position, and conj_and between flies and lands, 
which links two actions that an airplane can 
perform.  As these examples illustrate, collapsing 
dependencies involving prepositions and 
conjunctions can yield informative dependencies 
between content words. 
Besides collapsed dependencies, PONG infers 
inverse dependencies.  Inverse selectional 
preferences are selectional preferences of 
arguments for their predicates, such as a 
preference of a subject or object for its verb.  
They capture semantic regularities such as the set 
of verbs that an agent can perform, which tend to 
outnumber the possible agents for a verb (Erk et 
al., 2010). 
3 Method 
To compute selectional preferences, PONG 
combines information from a limited corpus 
labeled with the grammatical dependencies 
described in Section 2, and a much larger 
unlabeled corpus.  The key idea is to abstract 
word sequences labeled with grammatical 
relations into POS N-grams, in order to learn a 
mapping from POS N-grams to those relations.  
For instance, PONG abstracts the parsed 
sentence Pat opened the door as NN VB DT NN, 
with the first and last NN as the subject and 
object of the VB.  To estimate the distribution of 
POS N-grams containing particular target and 
relative words, PONG POS-tags Google N-
grams (Franz and Brants, 2006). 
Section 3.1 derives PONG?s probabilistic 
model for combining information from labeled 
and unlabeled corpora.  Section 3.2 and Section 
3.3 describe how PONG estimates probabilities 
from each corpus.  Section 3.4 discusses a 
sparseness problem revealed during probability 
estimation, and how we address it in PONG. 
3.1 Probabilistic model 
We quantify the selectional preference for a 
relative r to instantiate a relation R of a target t as 
the probability Pr(r | t, R), estimated as follows.  
By the definition of conditional probability: 
378
Pr( , , )Pr( | , ) Pr( , )
r t Rr t R t R
 
We care only about the relative probability of 
different r for fixed t and R, so we rewrite it as:   
Pr( , , )r t R  
We use the chain rule: 
Pr( | , ) Pr( | ) Pr( )R r t r t t 
and notice that t is held constant: 
Pr( | , ) Pr( | )R r t r t 
We estimate the second factor as follows:
 
Pr( , ) freq( , )Pr( | ) Pr( ) freq( )
t r t rr t t t
 
We calculate the denominator freq(t) as the 
number of  N-grams in the Google N-gram 
corpus that contain t, and the numerator freq(t, r) 
as the number of N-grams containing both t and r. 
To estimate the factor Pr(R | r, t) directly from 
a corpus of text labeled with grammatical 
relations, it would be trivial to count how often a 
word r bears relation R to target word t.  
However, the results would be limited to the 
words in the corpus, and many relation 
frequencies would be estimated sparsely or 
missing altogether; t or r might not even occur. 
Instead, we abstract each word in the corpus as 
its part-of-speech (POS) label.  Thus we abstract 
The big boy ate meat as DT JJ NN VB NN.  We 
call this sequence of POS tags a POS N-gram.  
We use POS N-grams to predict word relations.  
For instance, we predict that in any word 
sequence with this POS N-gram, the JJ will 
modify (amod) the first NN, and the second NN 
will be the direct object (dobj) of the VB.   
This prediction is not 100% reliable.  For 
example, the initial 5-gram of The big boy ate 
meat pie has the same POS 5-gram as before.  
However, the dobj of its VB (ate) is not the 
second NN (meat), but the subsequent NN (pie).  
Thus POS N-grams predict word relations only 
in a probabilistic sense. 
To transform Pr(R | r, t) into a form we can 
estimate, we first apply the definition of 
conditional probability: 
 Pr( , , )Pr( | , ) Pr( , )
R t rR t r t r 
To estimate the numerator Pr(R, t, r), we first 
marginalize over the POS N-gram p: 
 Pr( , , , )  Pr( , )p
R t r p
t r
 
We expand the numerator using the chain rule: 
 Pr( | , , ) Pr( | , ) Pr( , )
Pr( , )p
R t r p p t r t r
t r
 
Cancelling the common factor yields: 
 
Pr( | , , ) Pr( | , )
p
R p t r p t r
 
We approximate the first term Pr(R | p, t, r) as 
Pr(R | p), based on the simplifying assumption 
that R is conditionally independent of t and r, 
given p.  In other words, we assume that given a 
POS N-gram, the target and relative words t and 
r give no additional information about the 
probability of a relation.  However, their 
respective positions i and j in the POS N-gram p 
matter, so we condition the probability on them: 
 Pr( | , , ) Pr( | , , )R p t r R p i j 
Summing over their possible positions, we get 
Pr( | , )
Pr( | , , ) Pr( | , )i j
p i j
R r t
R p i j p t g r g
 
As Figure 1 shows, we estimate Pr(R | p, i, j) by 
abstracting the labeled corpus into POS N-grams. 
We estimate Pr(p | t = gi, r = gj) based on the 
frequency of partially lexicalized POS N-grams 
like DT JJ:red NN:hat VB NN among Google N-
grams with t and r in the specified positions. 
Sections 3.2 and 3.3 describe how we estimate 
Pr(R | p, i, j) and Pr(p | t = gi, r = gj), respectively.  
Note that PONG estimates relative rather than 
absolute probabilities.  Therefore it cannot (and 
does not) compare them against a fixed threshold 
to make decisions about selectional preferences.  
3.2 Mapping POS N-grams to relations 
To estimate Pr(R | p, i, j), we use the Penn 
Treebank Wall Street Journal (WSJ) corpus, 
which is labeled with grammatical relations 
using the Stanford dependency parser (Klein and 
Manning, 2003).   
To estimate the probability Pr(R | p, i, j) of a 
relation R between a target at position i and a 
relative at position j in a POS N-gram p, we 
compute what fraction of the word N-grams g 
with POS N-gram p have relation R between 
some target t and relative r at positions i and j: 
Pr( | , , )
freq( . .POS( ) relation( , ) )
freq( . .POS( ) relation( , ))
i j
i j
R p i j
g s t g p g g R
g s t g p g g 
3.3 Estimating POS N-gram distributions 
Given a target and relative, we need to estimate 
their distribution of POS N-grams and positions. 
379
 
Figure 1:  Overview of PONG.   
From the labeled corpus, PONG extracts abstract mappings from POS N-grams to relations. 
From the unlabeled corpus, PONG estimates POS N-gram probability given a target and relative. 
 
A labeled corpus is too sparse for this purpose, 
so we use the much larger unlabeled Google N-
grams corpus (Franz and Brants, 2006). 
The probability that an N-gram with target t at 
position i and relative r at position j will have the 
POS N-gram p is: 
Pr( | , )
freq( . .POS( ) , , ))
freq( . . )
i j
i j
i j
p t g r g
g s t g p g t g r
g s t g t g r
  
To compute this ratio, we first use a well-
indexed table to efficiently retrieve all N-grams 
with words t and r at the specified positions.  We 
then obtain their POS N-grams from the Stanford 
POS tagger (Toutanova et al 2003), and count 
how many of them have the POS N-gram p. 
3.4 Reducing POS N-gram sparseness 
We abstract word N-grams into POS N-grams to 
address the sparseness of the labeled corpus, but 
even the POS N-grams can be sparse.  For n=5, 
the rarer ones occur too sparsely (if at all) in our 
labeled corpus to estimate their frequency. 
To address this issue, we use a coarser POS 
tag set than the Penn Treebank POS tag set.  As 
Table 2 shows, we merge tags for adjectives, 
nouns, adverbs, and verbs into four coarser tags.   
Coarse Original  
ADJ JJ, JJR, JJS 
ADVERB RB, RBR, RBS 
NOUN NN, NNS, NNP, NNPS 
VERB VB, VBD, VBG, VBN, VBP, VBZ 
Table 2:  Coarser POS tag set used in PONG 
To gauge the impact of the coarser POS tags, 
we calculated Pr(r | t, R) for 76 test instances 
used in an earlier unpublished study by Liu Liu, 
a former Project LISTEN graduate student.  Each 
instance consists of two randomly chosen words 
in the WSJ corpus labeled with a grammatical 
relation.  Coarse POS tags increased coverage of 
this pilot set ? that is, the fraction of instances for 
which PONG computes a probability ? from 69% 
to 92%. 
Using the universal tag set (Petrov et al 2011) 
as an even coarser tag set is an interesting future 
direction, especially for other languages.  Its 
smaller size (12 tags vs. our 23) should reduce 
data sparseness, but increase the risk of over-
generalization. 
4 Evaluation 
To evaluate PONG, we use a standard pseudo-
disambiguation task, detailed in Section 4.1.  
Section 4.2 describes our test set.  Section 4.3 
lists the metrics we evaluate on this test set.  
Section 4.4 describes the baselines we compare 
PONG against on these metrics, and Section 4.5 
describes the relations we compare them on.  
Section 4.6 reports our results.  Section 4.7 
analyzes sources of error. 
4.1 Evaluation task 
The pseudo-disambiguation task (Gale et al 
1992; Schutze, 1992) is as follows:  given a 
target word t, a relation R, a relative r, and a 
random distracter r', prefer either r or r', 
whichever is likelier to have relation R to word t. 
This evaluation does not use a threshold:  just 
prefer whichever word is likelier according to the 
model being evaluated.  If the model assigns only 
one of the words a probability, prefer it, based on 
the assumption that the unknown probability of 
the other word is lower.  If the model assigns the 
same probability to both words, or no probability 
to either word, do not prefer either word. 
380
4.2 Test set 
As a source of evaluation data, we used the 
British National Corpus (BNC).  As a common 
test corpus for all the methods we evaluated, we 
selected one half of BNC by sorting filenames 
alphabetically and using the odd-numbered files.  
We used the other half of BNC as a training 
corpus for the baseline methods we compared 
PONG to. 
A test set for the pseudo-disambiguation task 
task consists of tuples of the form (R, t, r, r').  To 
construct a test set, we adapted the process used 
by Rooth et al(1999) and Erk et al(2010). 
First, we chose 100 (R, t) pairs for each 
relation R at random from the test corpus. Rooth 
et al(1999) and Erk et al(2010) chose such 
pairs from a training corpus to ensure that it 
contained the target t.  In contrast, choosing pairs 
from an unseen test corpus includes target words 
whether or not they occur in the training corpus. 
To obtain a sample stratified by frequency, 
rather than skewed heavily toward high-
frequency pairs, Erk et al(2010) drew (R, t) 
pairs from each of five frequency bands in the 
entire British National Corpus (BNC):  50-100 
occurrences; 101-200; 201-500; 500-1000; and 
more than 1000.  However, we use only half of 
BNC as our test corpus, so to obtain a 
comparable test set, we drew 20 (R, t) pairs from 
each of the corresponding frequency bands in 
that half:  26-50 occurrences; 51-100; 101-250; 
251-500; and more than 500. 
For each chosen (R, t) pair, we drew a separate 
(R, t, r) triple from each of six frequency bands:  
1-25 occurrences; 26-50; 51-100; 101-250; 251-
500; and more than 500.  We necessarily omitted 
frequency bands that contained no such triples.  
We filtered out triples where r did not have the 
most frequent part of speech for the relation R.  
For example, this filter would exclude the triple 
(dobj, celebrate, the) because a direct object is 
most frequently a noun, but the is a determiner. 
Then, like Erk et al(2010), we paired the 
relative r in each (R, t, r) triple with a distracter r' 
with the same (most frequent) part of speech as 
the relative r, yielding the test tuple (R, t, r, r'). 
Rooth et al(1999) restricted distracter 
candidates to words with between 30 and 3,000 
occurrences in BNC; accordingly, we chose only 
distracters with between 15 and 1,500 
occurrences in our test corpus.  We selected r' 
from these candidates randomly, with probability 
proportional to their frequency in the test corpus.  
Like Rooth et al(1999), we excluded as 
distracters any actual relatives, i.e. candidates r' 
where the test corpus contained the triple (R, t, r').  
Table 3 shows the resulting number of (R, t, r, r') 
test tuples for each relation. 
 
Relation R # tuples for R # tuples for RT 
advmod 121 131 
amod 162 128 
conj_and 155 151 
dobj 145 167 
nn 173 158 
nsubj  97 124 
prep_of 144 153 
xcomp 139 140 
Table 3:  Test set size for each relation 
4.3 Metrics 
We report four evaluation metrics:  precision, 
coverage, recall, and F-score.  Precision (called 
?accuracy? in some papers on selectional 
preferences) is the percentage of all covered 
tuples where the original relative r is preferred.  
Coverage is the percentage of tuples for which 
the model prefers r to r' or vice versa.  Recall is 
the percentage of all tuples where the original 
relative is preferred, i.e., precision times 
coverage.  F-score is the harmonic mean of 
precision and recall. 
4.4 Baselines 
We compare PONG to two baseline methods.   
EPP is a state-of-the-art model for which Erk 
et al(2010) reported better performance than 
both Resnik?s (1996) WordNet model and 
Rooth?s (1999) EM clustering model.  EPP 
computes selectional preferences using 
distributional similarity, based on the assumption 
that relatives are likely to appear in the same 
contexts as relatives seen in the training corpus.  
EPP computes the similarity of a potential 
relative?s vector space representation to relatives 
in the training corpus. 
EPP has various options for its vector space 
representation, similarity measure, weighting  
scheme, generalization space, and whether to use 
PCA.  In re-implementing EPP, we chose the 
options that performed best according to Erk et al
(2010), with one exception.  To save work, we 
chose not to use PCA, which Erk et al(2010) 
described as performing only slightly better in 
the dependency-based space. 
381
Relation Target Relative Description 
advmod verb adverb Adverbial modifier 
amod noun adjective Adjective modifier 
conj_and noun noun Conjunction with ?and? 
dobj verb noun Direct object 
nn noun noun Noun compound modifier 
nsubj verb noun Nominal subject 
prep_of noun noun Prepositional modifier 
xcomp verb verb Open clausal complement 
 
Table 4: Relations tested in the pseudo-disambiguation experiment.   
Relation names and descriptions are from de Marneffe and Manning (2008) except for prep_of.   
Target and relative POS are the most frequent POS pairs for the relations in our labeled WSJ corpus. 
 
Relation 
Precision (%) Coverage (%) Recall (%) F-score (%) 
PONG EPP DEP PONG EPP DEP PONG EPP DEP PONG EPP DEP 
advmod 78.7 - 98.6 72.1 - 69.2 56.7 - 68.3 65.9 - 80.7 
advmodT 89.0 71.0 97.4 69.5 100 59.5 61.8 71.0 58.0 73.0 71.0 72.7 
amod 78.8 - 99.0 90.1 - 61.1 71.0 - 60.5 74.7 - 75.1 
amodT 84.1 74.0 97.3 83.6 99.2 57.0 70.3 73.4 55.5 76.6 73.7 70.6 
conj_and 77.2 74.2 100 73.6 100 52.3 56.8 74.2 52.3 65.4 74.2 68.6 
conj_andT 80.5 70.2 97.3 74.8 100 49.7 60.3 70.2 48.3 68.9 70.2 64.6 
dobj 87.2 80.0 97.7 80.7 100 60.0 70.3 80.0 58.6 77.9 80.0 73.3 
dobjT 89.6 80.2 98.1 92.2 100 64.1 82.6 80.2 62.9 86.0 80.2 76.6 
nn 86.7 73.8 97.2 95.3 99.4 63.0 82.7 73.4 61.3 84.6 73.6 75.2 
nnT 83.8 79.7 99.0 93.7 100 60.8 78.5 79.7 60.1 81.0 79.7 74.8 
nsubj 76.1 77.3 100 69.1 100 42.3 52.6 77.3 42.3 62.2 77.3 59.4 
nsubjT 78.5 66.9 95.0 86.3 100 48.4 67.7 66.9 46.0 72.7 66.9 62.0 
prep_of 88.4 77.8 98.4 84.0 100 44.4 74.3 77.8 43.8 80.3 77.8 60.6 
prep_ofT 79.2 76.5 97.4 81.7 100 50.3 64.7 76.5 49.0 71.2 76.5 65.2 
xcomp 84.0 61.9 95.3 85.6 100 61.2 71.9 61.9 58.3 77.5 61.9 72.3 
xcompT 86.4 78.6 98.9 89.3 100 63.6 77.1 78.6 62.9 81.5 78.6 76.9 
average 83.0 74.4 97.9 82.6 99.9 56.7 68.7 74.4 55.5 75.0 74.4 70.5 
 
Table 5:  Coverage, Precision, Recall, and F-score for various relations; RT is the inverse of relation R. 
PONG uses POS N-grams, EPP uses distributional similarity, and DEP uses dependency parses. 
 
To score a potential relative r0, EPP uses this 
formula:
,
, 0 0
arg ( , ) ,
( )( ) ( , )R tR t
r Seen s R t R t
wt rSelpref r sim r rZ 
Here sim(r0, r) is the nGCM similarity defined 
below between vector space representations of r0 
and a relative r seen in the training data: 
 
2
1
2
1
'
( , ') exp( ( ) )
'
i i
i
n
b b
nGCM
i
n
b
i
a a
sim a a
a a
where a a
 
The weight function wtr,t(a) is analogous to 
inverse document frequency in Information 
Retrieval. 
DEP, our second baseline method, runs the 
Stanford dependency parser to label the training 
corpus with grammatical relations, and uses their 
frequencies to predict selectional preferences.  
To do the pseudo-disambiguation task, DEP 
compares the frequencies of (R, t, r) and (R, t, r'). 
4.5 Relations tested 
To test PONG, EPP, and DEP, we chose the 
most frequent eight relations between content 
words in the WSJ corpus, which occur over 
10,000 times and are described in Table 4.  We 
also tested their inverse relations.  However, EPP 
does not compute selectional preferences for 
adjective and adverb as relatives.  For this reason, 
we did not test EPP on advmod and amod 
relations with adverbs and adjectives as relatives. 
382
4.6 Experimental results 
Table 5 displays results for all 16 relations.  To 
compute statistical significance conservatively in 
comparing methods, we used paired t-tests with 
N = 16 relations. 
PONG?s precision was significantly better 
than EPP (p<0.001) but worse than DEP 
(p<0.0001).  Still, PONG?s high precision 
validates its underlying assumption that POS N-
grams strongly predict grammatical 
dependencies. 
On coverage and recall, EPP beat PONG, 
which beat DEP (p<0.0001).  PONG?s F-score 
was higher, but not significantly, than EPP?s 
(p>0.5) or DEP?s (p>0.02). 
4.7 Error analysis 
In the pseudo-disambiguation task of choosing 
which of two words is related to a target, PONG 
makes errors of coverage (preferring neither 
word) and precision (preferring the wrong word). 
Coverage errors, which occurred 17.4% of the 
time on average, arose only when PONG failed 
to estimate a probability for either word.  PONG 
fails to score a potential relative r of a target t 
with a specified relation R if the labeled corpus 
has no POS N-grams that (a) map to R, (b) 
contain the POS of t and r, and (c) match Google 
word N-grams with t and r at those positions.  
Every relation has at least one POS N-gram that 
maps to it, so condition (a) never fails.  PONG 
uses the most frequent POS of t and r, and we 
believe that condition (b) never fails.  However, 
condition (c) can and does fail when t and r do 
not co-occur in any Google N-grams, at least that 
match a POS N-gram that can map to relation R.  
For example, oversee and diet do not co-occur in 
any Google N-grams, so PONG cannot score diet 
as a potential dobj of oversee. 
Precision errors, which occur 17% of the time 
on average, arose when (a) PONG scored the 
distracter but failed to score the true relative, or 
(b) scored them both but preferred the distracter.  
Case (a) accounted for 44.62% of the errors on 
the covered test tuples. 
One likely cause of errors in case (b) is over-
generalization when PONG abstracts a word N-
gram labeled with a relation by mapping its POS 
N-gram to that relation.  In particular, the coarse 
POS tag set may discard too much information.  
Another likely cause of errors is probabilities 
estimated poorly due to sparse data.   The 
probability of a relation for a POS N-gram rare in 
the training corpus is likely to be inaccurate.  So 
is the probability of a POS N-gram for rare co-
occurrences of a target and relative in Google 
word N-grams.  Using a smaller tag set may 
reduce the sparse data problem but increase the 
risk of over-generalization. 
5 Relation to Prior Work 
In predicting selectional preferences, a key 
issue is generalization.  Our DEP baseline simply 
counts co-occurrences of target and relative 
words in a corpus to predict selectional 
preferences, but only for words seen in the 
corpus.  Prior work, summarized in  
Table 6, has therefore tried to infer the similarity 
of unseen relatives to seen relatives. To illustrate, 
consider the problem of inducing that the direct 
objects of celebrate tend to be days or events. 
Resnik (1996) combined WordNet with a 
labeled corpus to model the probability that 
relatives of a predicate belong to a particular 
conceptual class.  This method could notice, for 
example, that the direct objects of celebrate tend 
to belong to the conceptual class event.  Thus it 
could prefer anniversary or occasion as the 
object of celebrate even if unseen in its training 
corpus.  However, this method depends strongly 
on the WordNet taxonomy. 
Rather than use linguistic resources such as 
WordNet, Rooth et al(1999) and Wald et al
(2008) induced semantically annotated 
subcategorization frames from unlabeled corpora. 
They modeled semantic classes as hidden 
variables, which they estimated using EM-based 
clustering.  Ritter (2010) computed selectional 
preferences by using unsupervised topic models 
such as LinkLDA, which infers semantic classes 
of words automatically instead of requiring a pre-
defined set of classes as input. 
The contexts in which a linguistic unit occurs 
provide information about its meaning.  Erk 
(2007) and Erk et al(2010) modeled the 
contexts of a word as the distribution of words  
that co-occur with it.  They calculated the 
semantic similarity of two words as the similarity 
of their context distributions according to various 
measures.  Erk et al(2010) reported the state-of-
the-art method we used as our EPP baseline. 
In contrast to prior work that explored various 
solutions to the generalization problem, we don?t 
so much solve this problem as circumvent it.  
Instead of generalizing from a training corpus 
directly to unseen words, PONG abstracts a word 
N-gram to a POS N-gram and maps it to the 
relations that the word N-gram is labeled with. 
383
 
Table 6:  Comparison with prior methods to compute selectional preferences 
 
To compute selectional preferences, whether the 
words are in the training corpus or not, PONG 
applies these abstract mappings to word N-grams 
in the much larger Google N-grams corpus. 
Some prior work on selectional preferences 
has used POS N-grams and a large unlabeled 
corpus.  The most closely related work we found 
was by Gormley et al(2011).  They used 
patterns in POS N-grams to generate test data for 
their selectional preferences model, but not to 
infer preferences.  Zhou et al(2011) identified 
selectional preferences of one word for another 
Reference Relation to 
target 
Lexical 
resource 
Primary  corpus 
(labeled) & 
information 
used 
Generalization  
corpus 
(unlabeled) & 
information used 
Method 
Resnik, 
1996 
Verb-object 
Verb-subject 
Adjective-noun 
Modifier-head 
Head-modifier 
Senses in 
WordNet 
noun 
taxonomy 
Target, relative, 
and relation in a 
parsed, partially 
sense-tagged 
corpus (Brown 
corpus) 
none Information 
theoretic 
model 
Rooth et 
al., 1999 
Verb-object 
Verb-subject 
none Target, relative, 
and relation in a 
parsed corpus 
(parsed BNC) 
none EM-based 
clustering 
Ritter, 
2010 
Verb-subject 
Verb-object 
Subject-verb-
object 
none Subject-verb-
object tuples 
from 500 million 
web-pages 
none LDA model 
Erk, 2007 Predicate and 
Semantic roles 
none Target, relative, 
and relation in a 
semantic role 
labeled corpus 
(FrameNet) 
Words and their 
relations in a 
parsed corpus 
(BNC) 
Similarity 
model based 
on word co-
occurrence  
Erk et al 
2010 
SYN option:  
Verb-subject 
Verb-object, and 
their inverse 
relations 
SEM option:  
verb and 
semantic roles 
that have nouns 
as their headword 
in a primary 
corpus, and their 
inverse relations 
none Target, relative, 
and relation in 
SYN   option:  a  
parsed corpus 
(parsed BNC) 
SEM   option:  a 
semantic role 
labeled corpus 
(FrameNet) 
Two options: 
 
WORDSPACE:  
an unlabeled 
corpus (BNC) 
 
DEPSPACE:  
Words and their 
subject and object 
relations in a 
parsed corpus 
(parsed BNC) 
Similarity 
model using 
vector space 
representation 
of words 
Zhou et 
al., 2011 
Any (relations 
not distinguished) 
none Counts of words 
in Web or 
Google N-gram 
none PMI 
(Pointwise 
Mutual 
Information) 
This paper All grammatical 
dependencies in a 
parsed corpus, 
and their inverse 
relations 
none POS N-gram 
distribution for 
relations in 
parsed WSJ 
corpus 
POS N-gram 
distribution for 
target and relative 
in Google N-gram 
Combine both 
POS N-gram 
distributions 
384
by using Pointwise Mutual Information (PMI) 
(Fano, 1961) to check whether they co-occur 
more frequently in a large corpus than predicted 
by their unigram frequencies.  However, their 
method did not distinguish among different 
relations. 
6 Conclusion 
This paper describes, derives, and evaluates 
PONG, a novel probabilistic model of selectional 
preferences.  PONG uses a labeled corpus to map 
POS N-grams to grammatical relations.  It 
combines this mapping with probabilities 
estimated from a much larger POS-tagged but 
unlabeled Google N-grams corpus. 
We tested PONG on the eight most common 
relations in the WSJ corpus, and their inverses ? 
more relations than evaluated in prior work.  
Compared to the state-of-the-art EPP baseline 
(Erk et al 2010), PONG averaged higher 
precision but lower coverage and recall.  
Compared to the DEP baseline, PONG averaged 
lower precision but higher coverage and recall.  
All these differences were substantial (p < 0.001). 
Compared to both baselines, PONG?s average F-
score was higher, though not significantly. 
Some directions for future work include:  First, 
improve PONG by incorporating models of 
lexical similarity explored in prior work.  Second, 
use the universal tag set to extend PONG to other 
languages, or to perform better in English.  Third, 
in place of grammatical relations, use rich, 
diverse semantic roles, while avoiding sparsity.  
Finally, use selectional preferences to teach word 
connotations by using various relations to 
generate example sentences or useful questions. 
Acknowledgments 
The research reported here was supported by the 
Institute of Education Sciences, U.S. Department 
of Education, through Grant R305A080157.  The 
opinions expressed are those of the authors and 
do not necessarily represent the views of the 
Institute or the U.S. Department of Education.  
We thank the helpful reviewers and Katrin Erk 
for her generous assistance. 
References 
de Marneffe, M.-C. and Manning, C.D. 2008. 
Stanford Typed Dependencies Manual. 
http://nlp.stanford.edu/software/dependencies_man
ual.pdf, Stanford University, Stanford, CA. 
Erk, K. 2007. A Simple, Similarity-Based Model for 
Selectional Preferences. In Proceedings of the 45th 
Annual Meeting of the Association of 
Computational Linguistics, Prague, Czech 
Republic, June, 2007, 216-223. 
Erk, K., Pad?, S. and Pad?, U. 2010. A Flexible, 
Corpus-Driven Model of Regular and Inverse 
Selectional Preferences. Computational Linguistics 
36(4), 723-763. 
Fano, R. 1961. Transmission  O F   Information:  A  
Statistical  Theory  of  Communications. MIT 
Press, Cambridge, MA. 
Franz, A. and Brants, T. 2006. All Our N-Gram Are 
Belong to You. 
Gale, W.A., Church, K.W. and Yarowsky, D. 1992. 
Work on Statistical Methods for Word Sense 
Disambiguation. In Proceedings of the AAAI Fall 
Symposium on Probabilistic Approaches to Natural 
Language, Cambridge, MA, October 23?25, 1992, 
54-60. 
Gildea, D. and Jurafsky, D. 2002. Automatic Labeling 
of Semantic Roles. Computational Linguistics 
28(3), 245-288. 
Gormley, M.R., Dredze, M., Durme, B.V. and Eisner, 
J. 2011. Shared Components Topic Models with 
Application to Selectional Preference, NIPS 
Workshop on Learning Semantics Sierra Nevada, 
Spain. 
im Walde, S.S., Hying, C., Scheible, C. and Schmid, 
H. 2008. Combining Em Training and the Mdl 
Principle for an Automatic Verb Classification 
Incorporating Selectional Preferences. In 
Proceedings of the 46th Annual Meeting of the 
Association for Computational Linguistics, 
Columbus, OH,  2008, 496-504. 
Klein, D. and Manning, C.D. 2003. Accurate 
Unlexicalized Parsing. In Proceedings of the 41st 
Annual Meeting of the Association for 
Computational Linguistics, Sapporo, Japan, July 7-
12, 2003, E.W. HINRICHS and D. ROTH, Eds. 
Petrov, S., Das, D. and McDonald, R.T. 2011. A 
Universal Part-of-Speech Tagset. ArXiv 
1104.2086. 
Resnik, P. 1996. Selectional Constraints: An 
Information-Theoretic Model and Its 
Computational Realization. Cognition 61, 127-159. 
Resnik, P. 1997. Selectional Preference and Sense 
Disambiguation. In ACL SIGLEX Workshop on 
385
Tagging Text with Lexical Semantics: Why, What, 
and How, Washington, DC, April 4-5, 1997, 52-57. 
Ritter, A., Mausam and Etzioni, O. 2010. A Latent 
Dirichlet Allocation Method for Selectional 
Preferences. In Proceedings of the 48th Annual 
Meeting of the Association for Computational 
Linguistics, Uppsala, Sweden,  2010, 424-434. 
Rooth, M., Riezler, S., Prescher, D., Carroll, G. and 
Beil, F. 1999. Inducing a Semantically Annotated 
Lexicon Via Em-Based Clustering. In Proceedings 
of the 37th Annual Meeting of the Association for 
Computational Linguistics on Computational 
Linguistics, College Park, MD,  1999, Association 
for Computational Linguistics, 104-111. 
Schutze, H. 1992. Context Space. In Proceedings of 
the AAAI Fall Symposium on Intelligent 
Probabilistic Approaches to Natural Language, 
Cambridge, MA,  1992, 113-120. 
Toutanova, K., Klein, D., Manning, C. and Singer, Y. 
2003. Feature-Rich Part-of-Speech Tagging with a 
Cyclic Dependency Network. In Proceedings of the 
Human Language Technology Conference and 
Annual Meeting of the North American Chapter of 
the Association for Computational Linguistics 
(HLT-NAACL), Edmonton, Canada,  2003, 252?
259. 
Zhou, G., Zhao, J., Liu, K. and Cai, L. 2011. 
Exploiting Web-Derived Selectional Preference to 
Improve Statistical Dependency Parsing. In 
Proceedings of the 49th Annual Meeting of the 
Association for Computational Linguistics, 
Portland, OR,  2011, 1556?1565. 
 
 
386
2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 382?385,
Montre?al, Canada, June 3-8, 2012. c?2012 Association for Computational Linguistics
Towards Using EEG to Improve ASR Accuracy
Yun-Nung Chen, Kai-Min Chang, and Jack Mostow
Project LISTEN (http://www.cs.cmu.edu/?listen)
School of Computer Science, Carnegie Mellon University
5000 Forbes Avenue, Pittsburgh, PA 15213-3891, USA
{yvchen,kkchang,mostow}@cs.cmu.edu
Abstract
We report on a pilot experiment to improve the per-
formance of an automatic speech recognizer (ASR)
by using a single-channel EEG signal to classify the
speaker?s mental state as reading easy or hard text.
We use a previously published method (Mostow et
al., 2011) to train the EEG classifier. We use its prob-
abilistic output to control weighted interpolation of
separate language models for easy and difficult read-
ing. The EEG-adapted ASR achieves higher accu-
racy than two baselines. We analyze how its perfor-
mance depends on EEG classification accuracy. This
pilot result is a step towards improving ASR more
generally by using EEG to distinguish mental states.
1 Introduction
Humans use speech to communicate what?s on their
mind. However, until now, automatic speech recogniz-
ers (ASR) and dialogue systems have had no direct way
to take into account what is going on in a speaker?s
mind. Some work has attempted to infer cognitive
states from volume and speaking rate to adapt language
modeling (Ward and Vega, 2009) or from query click
logs (Hakkani-Tu?r et al, 2011) to detect domains. A
new way to address this limitation is to infer mental states
from electroencephalogram (EEG) signals.
EEG is a voltage signal that can be measured on the
surface of the scalp, arising from large areas of coordi-
nated neural activity. This neural activity varies as a func-
tion of development, mental state, and cognitive activity,
and EEG can measurably detect such variation.
Recently, a few companies have scaled back medical
grade EEG technology to create portable EEG headsets
that are commercially available and simple to use. The
NeuroSky MindSetTM (2009), for example, is an audio
headset equipped with a single-channel EEG sensor. It
measures the voltage between an electrode that rests on
the forehead and electrodes in contact with the ear. Un-
like the multi-channel electrode nets worn in labs, the
sensor requires no gel or saline for recording, and re-
quires no expertise to wear. Even with the limitations
of recording from only a single sensor and working with
untrained users, Furthermore, Mostow et al(2011) used
its output signal to distinguish easy from difficult reading,
achieving above-chance accuracy. Here we build on that
work by using the output of such classifiers to adapt lan-
guage models for ASR and thereby improve recognition
accuracy.
The most similar work is Jou and Schultz?s (2008) use
of electromyographic (EMG) signals generated by human
articulatory muscles in producing speech. They showed
that augmenting acoustic features with these EMG fea-
tures can achieve rudimentary silent speech detection.
Pasley et al (2012) used electrocorticographic (ECoG)
recordings from nonprimary auditory cortex in the human
superior temporal gyrus to reconstruct acoustic informa-
tion in speech sounds. Our work differs from these efforts
in that we use a consumer-grade single-channel EEG sen-
sor measuring frontal lobe activities, and that we use the
detected mental state just to help improve ASR perfor-
mance rather than to dictate or reconstruct speech, which
are much harder tasks.
Section 2 describes how to use machine learning to dis-
tinguish mental states associated with easy and difficult
readings. Section 3 describes how we use EEG classifier
output to adapt ASR language models. Section 4 uses an
oracle simulation to show how increasing EEG classifier
accuracy will affect ASR accuracy. Section 5 concludes.
2 Mental State Classification Using EEG
We use training and testing data from Mostow et al?s
(2011) experiment, which presented text passages, one
sentence at a time, to 10 adults and 11 nine- to ten-year-
olds wearing a Neurosky MindsetTM (2009). They read
three easy and three difficult texts aloud, in alternating
382
order. The ?easy? passages were from texts classified by
the Common Core Standards1 at the K-1 level. The ?diffi-
cult? passages were from practice materials for the Grad-
uate Record Exam2 and the ACE GED test3. Across the
reading conditions, passages ranged from 62 to 83 words
long. Although instructed to read the text aloud, the read-
ers (especially children) did not always read correctly or
follow the displayed sentences.
Following Mostow et al (2011), we trained binary lo-
gistic regression classifiers to estimate the probability that
an EEG signal is associated with reading an easy (or diffi-
cult) sentence. As features for logistic regression we used
the streams of values logged by the MindSet:
1. The raw EEG signal, sampled at 512 Hz
2. A filtered version of the raw signal, also sampled at
512 Hz, which is raw signal smoothed over a win-
dow of 2 seconds
3. Proprietary ?attention? and ?meditation? measures,
reported at 1 Hz
4. A power spectrum of 1Hz bands from 1-256 Hz, re-
ported at 8 Hz
5. An indicator of signal quality, reported at 1 Hz
Head movement or system instability led to missing or
poor-quality EEG data for some utterances, which we ex-
cluded in order to focus on utterances with clear acous-
tic and EEG signals. The features for each utterance
consisted of measures 1-4, averaged over the utterance,
excluding the 15% of observations where measure 5 re-
ported poor signals. After filtering, the data includes 269
utterances from adults and 243 utterances from children,
where 327 utterances are for the easy passages and 185
utterances are for the difficult passages. To balance the
classes, we used the undersampling method for training.
We trained a reader-specific classifier on each reader?s
data from all but one text passage, tested it on each
sentence in the held-out passage, performed this proce-
dure for each passage, and averaged the results to cross-
validate accuracy within readers. We computed classifi-
cation accuracy as the percentage of utterances classified
correctly. Classification accuracy for adults?, children?s,
and total oral reading was 71.49%, 58.74%, and 65.45%
respectively. A one-tailed t-test, with classification accu-
racy on an utterance as the random variable, showed that
EEG classification was significantly better than chance.
3 Language Model Adaptation for ASR
Traditional ASR decodes a word sequence W ? from the
acoustic model and language model as below:
1http://www.corestandards.org
2http://majortests.com/gre/reading comprehension.php
3http://college.cengage.com:80/devenglish/resources/reading
ace/students
W ? = argmaxW P (W | A) (1)
= argmaxW
P (A | W ) ? P (W )
P (A)
To incorporate EEG, we include mental state N as an ad-
ditional observation in the decoding procedure:
W ? = argmaxW P (W | A,N) (2)
= argmaxW
P (A | W ) ? P (W | N)
P (A)
The six passages use a vocabulary of 430 distinct
words. To evaluate the impact on ASR accuracy of us-
ing EEG to adapt language models, we needed acoustic
models appropriate for the speakers. For adult speech, we
used the US English HUB4 Acoustic Model from CMU
Sphinx. For children?s speech, we used Project LISTEN?s
acoustic models trained on children?s oral reading.
We used separate trigram language models (with bi-
gram and unigram backoff) for easy and difficult text ?
EasyLM, trained on the three easy passages, and Diffi-
cultLM, trained on the three difficult passages. Both lan-
guage models used the same lexicon, consisting of the
430 words in all six target passages. All experiments used
the same ASR parameter values.
As a gold standard, all utterances were manually tran-
scribed by a native English speaker. To measure ASR per-
formance, we computed Word Accuracy (WACC) as the
number of words recognized correctly minus insertions
divided by number of words in the reference transcripts
for each reader, and averaged them.
Then we can adapt the language model to estimate
P (W | N) using mental state information. Using the
EEG classifier described in Section 2, we adapted the lan-
guage model separately for each utterance, using three
types of language model adaptation: hard selection, soft
selection, and combination with ASR output.
3.1 Hard Selection of Language Models
Given the probabilistic estimate that a given utterance
was easy or difficult (SEasy(N) and SDifficult(N)), hard se-
lection simply picks EasyLM if the utterance was likelier
to be easy, or DifficultLM otherwise:
PHard(W | N) = IC(N) ? PEasy(W ) (3)
+ (1 ? IC(N)) ? PDiff(W ).
Here IC(N) = 1 if SEasy(N) > SDifficult(N), and
PEasy(W ) and PDiff(W ) are the probability of word W in
EasyLM and DifficultLM, respectively. For comparison,
the Random Pick baseline randomly picks either EasyLM
or DifficultLM:
383
WACC
Adult Child
Easy Difficult All Easy Difficult All
(a) Baseline 1: Random Pick 54.5 51.2 53.8 32.8 14.7 30.6
(b) EEG-based: Hard Selection 57.6 49.4 52.7 36.4 17.0 32.8
(c) Baseline 2: Equal Weight 63.2 59.9 56.5 37.3 19.5 33.4
(d) EEG-based: Soft Selection w/o smoothing 57.2 48.8 52.4 35.8 17.2 32.5
(e) EEG-based: Soft Selection w/ smoothing 66.0 62.3 64.2 39.8 22.7 36.2
(f) Baseline 3: Weight from ASR (? = 0) 63.8 60.6 61.5 39.2 20.0 35.0
(g) Weight from ASR and EEG (? = 0.5) 64.5 63.4 63.5 39.2 21.9 36.0
Table 1: ASR performance of proposed approaches using EEG-based classification of mental states.
PRandom(W ) = IR ? PEasy(W ) (4)
+ (1 ? IRandom) ? PDiff(W ).
Here IR is randomly set to 0 or 1.
3.2 Soft Selection of Language Models
Mental state classification based on EEG is imperfect,
and using only the corresponding language model (Ea-
syLM or DifficultLM) to decode the target utterance is li-
able to perform worse when the classifier is wrong. Thus,
we use the classifier?s probabilistic estimate that the ut-
terance is easy (or difficult) as interpolation weights to
linearly combine EasyLM and DifficultLM:
PSoft(W | N) = wEasy(N) ? PEasy(W ) (5)
+ wDiff(N) ? PDiff(W ).
Here wEasy(N) and wDiff(N) are from classifier?s output.
wEasy(N) = SEasy(N), wDiff(N) = SDiff(N) (6)
Additionally, we can adjust the range of weights by
smoothing the probability outputted by the EEG classi-
fier:
wEasy(N) =
? + SEasy(N)
2? + 1
, (7)
wDiff(N) =
? + SDiff(N)
2? + 1
Here SEasy(N) (or SDiff(N)) is the classifier?s probabilis-
tic estimate that the sentence is easy (or difficult) and
? is the smoothing weight, which we set to 0.5. Af-
ter smoothing the probabilities, wEasy(N) and wDiff(N)
each lie within the interval [0.25, 0.75], and wEasy(N) +
wDiff(N) = 1. That is, Soft Selection with smoothing in-
terpolates the two language models, but assigns a weight
of at least 0.25 to each one to reduce the impact of EEG
classifier errors. Notice that ? = 0 is equivalent to EEG
Soft Selection without smoothing.
For comparison, the Equal Weight baseline interpo-
lates EasyLM and DifficultLM with equal weights:
PEqual(W ) = 0.5 ? PEasy(W ) + 0.5 ? PDiff(W ) (8)
3.3 Combination with ASR Output
Given the ASR results from the Equal Weight baseline,
we can derive S?Easy(N) as:
S?Easy(N) = ? ? SEasy(N) (9)
+ (1 ? ?) ?
PEasy(W0)
PEasy(W0) + PDiff(W0)
Here we can estimate S?Easy(N) based on the classifier?s
output and the probability of the recognized words W0 in
EasyLM. We can derive S?Diff(N) in the same way. Then
we can use (5) and (7) to re-decode the utterances by us-
ing S?Easy(N) and S
?
Diff(N). Here ? is a linear interpola-
tion weight, where we set to 0.5 to give equal weights to
ASR output and EEG. For comparison, the ASR baseline
uses weights from only the ASR results, where ? = 0.
Notice that the case of ? = 1 is equivalent to EEG Soft
Selection with smoothing.
3.4 Results of Proposed Approaches
Table 1 shows the performance of our proposed ap-
proaches and the corresponding baselines as measured by
WACC. According to one-tailed t-tests with word accu-
racy of an utterance as the random variable, the results
in boldface are significantly better tgan their respective
baselines (p ? 0.05).
Hard Selection (row b) outperforms the Random Pick
baseline (row a). Soft Selection without smoothing (row
d) has similar performance as Hard Selection because the
classifier often outputs probability estimates that are ei-
ther 1 or 0. However, Soft Selection with smoothing (row
e) outperforms the Equal Weight baseline (row c). The
Weight from ASR baseline (row f) is better than the other
baselines. Weight from ASR and EEG (row g) can fur-
ther improve performance, but it?s not better than Soft
Selection with smoothing (row e) - evidence that EEG
gives good estimation for choosing language models. In
short, Table 1 shows that using EEG to choose between
EasyLM and DifficultLM achieves higher ASR accuracy
than the baselines that do not use EEG.
Comparing the first two baselines, the Equal Weight
baseline (row c) outperforms the Random Pick baseline
384
0 
10 
20 
30 
40 
50 
60 
70 
0 10 20 30 40 50 60 70 80 90 100 
Easy utt. 
Difficult utt. 
All utt. 
Predicted WACC (%) 
Simulated Accuracy of Classification (%) 
(a) Adult 
0 
10 
20 
30 
40 
50 
60 
70 
0 10 20 30 40 50 60 70 80 90 100 
Easy utt. 
Difficult utt. 
All utt. 
Predicted WACC (%) 
Simulated Accuracy of Classification (%) 
(b) Child  
Figure 1: The simulated accuracy graphs plot the predicted ASR word accuracy against the level of EEG classification accuracy
simulated by an oracle.
(row a) in every column, because the loss in ASR accu-
racy from picking the wrong language model outweighs
the improvement from picking the right one. Similarly,
EEG-based Soft Selection with smoothing (row e) out-
performs EEG-based Hard Selection (row b) in every col-
umn because the interpolated language model is more
robust to EEG classification error. The third base-line,
Weight from ASR (row f) depends solely on ASR results
to estimate weights; it performs better than other base-
lines, but not as well as EEG-based Soft Selection with
smoothing (row e). That is, using EEG alone can weight
the two language models better than ASR alone.
4 Oracle Simulation
To explore the relationship between EEG classifier ac-
curacy and the effect of EEG-based adaptation on ASR
accuracy, we simulate different classification accuracies
and used Hard Selection to predict the resulting ASR ac-
curacy by selecting between the ASR output from Ea-
syLM and DifficultLM according to the simulated clas-
sifier accuracy. We use the resulting Word Accuracy to
predict ASR performance at that level of EEG classifier
accuracy.
Figure 1 plots predicted ASR WACC against simulated
EEG classification accuracy. As expected, the predicted
ASR accuracy increases as EEG classification accuracy
increases, for both groups (adults and children) and both
levels of difficulty (easy and difficult). However, Figure
1a and 1b shows that WACC was much lower for children
than for adults, especially on difficult utterances, where
even 100% simulated EEG classifier accuracy achieves
barely 20% WACC. One explanation is that on difficult
sentences, children produced reading mistakes and/ or
off-task speech. In contrast, adults read better and stayed
on task. Not only is predicted ASR accuracy higher on
adults? reading, it improves substantially as simulated
EEG classifier accuracy increases.
5 Conclusion
This paper shows that classifying EEG signals from an in-
expensive single-channel device can help adapt language
models to significantly improve ASR performance. An
interpolated language model smoothed to compensate for
classification errors yielded the best performance. ASR
performance depended on the accuracy of mental state
classification. Future work includes improving EEG clas-
sification accuracy, detecting other relevant mental states,
such as emotion, and improving ASR by using word-level
EEG classification. A neurologically-informed ASR may
better capture what people intend to communicate, and
augment acoustic input with non-verbal cues to ASR or
dialogue systems.
Acknowledgements
This work was supported by the Institute of Education
Sciences, U.S. Department of Education, through Grant
R305A080628 to Carnegie Mellon University. Any opin-
ions, findings, and conclusions or recommendations ex-
pressed in this publication are those of the authors and do
not necessarily reflect the views or official policies, either
expressed or implied of the Institute or the U.S. Depart-
ment of Education. We thank the students, educators, and
LISTENers who helped create our data, and the reviewers
for their helpful comments.
References
Hakkani-Tn?r, D., Tur, G., Heck, L., and Shriberg, E. 2011.
Bootstrapping domain detection using query click logs for
new domains Proceedings of InterSpeech, 709-712.
Jou, S.-C. S. and Schultz, T.. 2008. Ears: Electromyograpical
Automatic Recognition of Speech. Proceedings of Biosig-
nals, 3-12.
Mostow, J., Chang, K.-M., and Nelson, J. 2011. Toward Ex-
ploiting EEG Input in a Reading Tutor. Proceedings of the
15th International Conference on Artificial Intelligence in
Education, 230-237.
NeuroSky 2009. NeuroSky?s SenseTM Meters and Detection of
Mental State: Neurisky, Inc.
Pasley, B. N. and et al 2012. Reconstructing speech from au-
ditory cortex. PLos Biology, 10(1), 1-13.
Ward, N. G. and Vega, A. 2009. Towards the use of cognitive
states in language modeling. Proceedings of ASRU, 323-326.
385
Proceedings of the Sixth Workshop on Innovative Use of NLP for Building Educational Applications, pages 105?110,
Portland, Oregon, 24 June 2011. c?2011 Association for Computational Linguistics
Generating Example Contexts to Illustrate a Target Word Sense 
 
 
Jack Mostow Weisi Duan 
Carnegie Mellon University Carnegie Mellon University 
RI-NSH 4103, 5000 Forbes Avenue Language Technologies Institute 
Pittsburgh, PA 15213-3890, USA Pittsburgh, PA 15213-3890, USA 
mostow@cs.cmu.edu wduan@cs.cmu.edu 
 
  
Abstract 
Learning a vocabulary word requires seeing it 
in multiple informative contexts.  We describe 
a system to generate such contexts for a given 
word sense.  Rather than attempt to do word 
sense disambiguation on example contexts al-
ready generated or selected from a corpus, we 
compile information about the word sense into 
the context generation process.  To evaluate the 
sense-appropriateness of the generated contexts 
compared to WordNet examples, three human 
judges chose which word sense(s) fit each ex-
ample, blind to its source and intended sense.  
On average, one judge rated the generated ex-
amples as sense-appropriate, compared to two 
judges for the WordNet examples.  Although 
the system?s precision was only half of Word-
Net?s, its recall was actually higher than 
WordNet?s, thanks to covering many senses for 
which WordNet lacks examples. 
 
1 Introduction 
Learning word meaning from example contexts is 
an important aspect of vocabulary learning. Con-
texts give clues to semantics but also convey many 
other lexical aspects, such as parts of speech, mor-
phology, and pragmatics, which help enrich a per-
son?s word knowledge base (Jenkins 1984; Nagy et 
al. 1985; Schatz 1986; Herman et al 1987; Nagy 
et al 1987; Schwanenflugel et al 1997; Kuhn and 
Stahl 1998; Fukkink et al 2001). Accordingly, one 
key issue in vocabulary instruction is how to find 
or create good example contexts to help children 
learn a particular sense of a word. Hand-vetting 
automatically generated contexts can be easier than 
hand-crafting them from scratch (Mitkov et al 
2006; Liu et al 2009). 
This paper describes what we believe is the first 
system to generate example contexts for a given 
target sense of a polysemous word.  Liu et al 
(2009) characterized good contexts for helping 
children learn vocabulary and generated them for a 
target part of speech, but not a given word sense.  
Pino and Eskenazi  (2009) addressed the polysemy 
issue, but in a system for selecting contexts rather 
than for generating them.  Generation can supply 
more contexts for a given purpose, e.g. teaching 
children, than WordNet or a fixed corpus contains. 
Section 2 describes a method to generate sense-
targeted contexts. Section 3 compares them to 
WordNet examples.  Section 4 concludes. 
2 Approach 
An obvious way to generate sense-targeted con-
texts is to generate contexts containing the target 
word, and use Word Sense Disambiguation (WSD) 
to select the ones that use the target word sense.  
However, without taking the target word sense into 
account, the generation process may not output any 
contexts that use it. Instead, we model word senses 
as topics and incorporate their sense indicators into 
the generation process ? words that imply a unique 
word sense when they co-occur with a target word.   
For example, retreat can mean ?a place of pri-
vacy; a place affording peace and quiet.?  Indica-
tors for this sense, in decreasing order of Pr(word | 
topic for target sense), include retreat, yoga, place, 
retreats, day, home, center, church, spiritual, life, 
city, time, lake, year, room, prayer, years, school, 
dog, park, beautiful, area, and stay.  Generated 
contexts include ?retreat in this bustling city?. 
Another sense of retreat (as defined in Word-
Net) is ?(military) a signal to begin a withdrawal 
105
from a dangerous position,? for which indicators 
include states, war, united, american, military, 
flag, president, world, bush, state, Israel, Iraq, in-
ternational, national, policy, forces, foreign, na-
tion, administration, power, security, iran, force, 
and Russia.  Generated contexts include ?military 
leaders believe that retreat?. 
We decompose our approach into two phases, 
summarized in Figure 1.  Section 2.1 describes the 
Sense Indicator Extraction phase, which obtains 
indicators for each WordNet synset of the target 
word.  Section 2.2 describes the Context Genera-
tion phase, which generates contexts that contain 
the target word and indicators for the target sense. 
 
Figure 1: overall work flow diagram 
2.1 Sense Indicator Extraction 
Kulkarni and Pedersen (2005) and Duan and Yates 
(2010) performed Sense Indicator Extraction, but 
the indicators they extracted are not sense targeted.  
Content words in the definition and examples for 
each sense are often good indicators for that sense, 
but we found that on their own they did poorly. 
One reason is that such indicators sometimes co-
occur with a different sense.  But the main reason 
is that there are so few of them that the word sense 
often appears without any of them.  Thus we need 
more (and if possible better) sense indicators. 
 To obtain sense-targeted indicators for a target 
word, we first assemble a corpus by issuing a 
Google query for each synset of the target word.  
The query lists the target word and all content 
words in the synset?s WordNet definition and ex-
amples, and specifies a limit of 200 hits.  The re-
sulting corpus contains a few hundred documents. 
To extract sense indicators from the corpus for a 
word, we adapt Latent Dirichlet Allocation (LDA) 
(Blei et al 2003).  LDA takes as input a corpus of 
documents and an integer k, and outputs k latent 
topics, each represented as a probability distribu-
tion over the corpus vocabulary.  For k, we use the 
number of word senses.  To bias LDA to learn top-
ics corresponding to the word senses, we use the 
content words in their WordNet definitions and 
examples as seed words. 
After learning these topics and filtering out stop 
words, we pick the 30 highest-probability words 
for each topic as indicators for the corresponding 
word sense, filtering out any words that also indi-
cate other senses. We create a corpus for each tar-
get word and run LDA on it. 
Having outlined the extraction process, we now 
explain in more detail how we learn the topics; the 
mathematically faint-hearted may skip to Section 
2.2.  Formally, given corpus   with  documents, 
let   be the number of topics, and let    and    be 
the parameters of the document and topic distribu-
tions respectively.  LDA assumes this generative 
process for each document    for a corpus  : 
1. Choose            where           
2. Choose            where           
3. For each word      in    where   
       ,   is the number of words in    
(a) Choose a topic                       
(b) Choose a topic                     
where        
In classical LDA, all   ?s are the same. We al-
low them to be different in order to use the seed 
words as high confidence indicators of target 
senses to bias the hyper-parameters of their docu-
ment distributions. 
For inference, we use Gibbs Sampling (Steyvers 
and Griffiths 2006) with transition probability  
                            
              
               
 
              
                  
 
Here        denotes the topic assignments to all 
other words in the corpus except     ;            
is the number of times word   is assigned to topic 
  in the whole corpus;          is the number of 
words assigned to topic   in the entire corpus; 
106
          is the count of tokens assigned to topic 
  in document   ; and    and       are the hyper-
parameters on     and      respectively in the two 
Dirichlet distributions. 
For each document    that contains seed words 
of some synset, we bias    toward the topic   for 
that synset by making      larger; specifically, we 
set each      to 10 times the average value of   .  
This bias causes more words     in    to be as-
signed to topic   because the words of    are likely 
to be relevant to  . These assignments then influ-
ence the topic distribution of   so as to make      
likelier to be assigned to   in any document     , 
and thus shift the document distribution in      
towards  . By this time we are back to the start of 
the loop where the document distribution of      
is biased to  .  Thus this procedure can discover 
more sense indicators for each sense. 
Our method is a variant of Labeled LDA (L-
LDA) (Ramage 2009), which allows only labels 
for each document as topics.  In contrast, our va-
riant allows all topics for each document, because 
it may use more than one sense of the target word.  
Allowing other senses provides additional flexibili-
ty to discover appropriate sense indicators. 
The LDA method we use to obtain sense indica-
tors fits naturally into the framework of bootstrap-
ping WSD (Yarowsky 1995; Mihalcea 2002; 
Martinez et al 2008; Duan and Yates 2010), in 
which seeds are given for each target word, and the 
goal is to disambiguate the target word by boot-
strapping good sense indicators that can identify 
the sense.  In contrast to WSD, our goal is to gen-
erate contexts for each sense of the target word.  
2.2 Context Generation 
To generate sense-targeted contexts, we extend the 
VEGEMATIC context generation system (Liu et 
al. 2009). VEGEMATIC generates contexts for a 
given target word using the Google N-gram cor-
pus.  Starting with a 5-gram that contains the target 
word, VEGEMATIC extends it by concatenating 
additional 5-grams that overlap by 4 words on the 
left or right. 
To satisfy various constraints on good contexts 
for learning the meaning of a word, VEGEMATIC 
uses various heuristic filters.  For example, to gen-
erate contexts likely to be informative about the 
word meaning, VEGEMATIC prefers 5-grams that 
contain words related to the target word, i.e., that 
occur more often in its presence.  However, this 
criterion is not specific to a particular target sense. 
To make VEGEMATIC sense-targeted, we 
modify this heuristic to prefer 5-grams that contain 
sense indicators.  We assign the generated contexts 
to the senses whose sense indicators they contain. 
We discard contexts that contain sense indicators 
for more than one sense. 
3 Experiments and Evaluation 
To evaluate our method, we picked 8 target words 
from a list of polysemous vocabulary words used 
in many domains and hence important for children 
to learn (Beck et al 2002).  Four of them are 
nouns:  advantage (with 3 synsets), content (7), 
force (10), and retreat (7). Four are verbs:  dash 
(6), decline (7), direct (13), and reduce (20).  Some 
of these words can have other parts of speech, but 
we exclude those senses, leaving 73 senses in total. 
We use their definitions from WordNet because 
it is a widely used, comprehensive sense inventory. 
Some alternative sense inventories might be un-
suitable. For instance, children?s dictionaries may 
lack WordNet?s rare senses or hypernym relations. 
We generated contexts for these 73 word senses 
as described in Section 2, typically 3 examples for 
each word sense.  To reduce the evaluation burden 
on our human judges, we chose just one context for 
each word sense, and for words with more than 10 
senses we chose a random sample of them.  To 
avoid unconscious bias, we chose random contexts 
rather than the best ones, which a human would 
likelier pick if vetting the generated contexts by 
hand.  For comparison, we also evaluated WordNet 
examples (23 in total) where available. 
We gave three native English-speaking college-
educated judges the examples to evaluate indepen-
dently, blind to their intended sense.  They filled in 
a table for each target word.  The left column listed 
the examples (both generated and WordNet) in 
random order, one per row.  The top row gave the 
WordNet definition of each synset, one per col-
umn.  Judges were told:  For each example, put a 
1 in the column for the sense that best fits how 
the example uses the target word.  If more than 
one sense fits, rank them 1, 2, etc.  Use the last 
two columns only to say that none of the senses 
fit, or you can't tell, and why.  (Only 10 such cas-
es arose.) 
We measured inter-rater reliability at two levels. 
107
At the fine-grained level, we measured how well 
the judges agreed on which one sense fit the exam-
ple best.  The value of Fleiss? Kappa (Shrout and 
Fleiss 1979) was 42%, considered moderate.  At 
the coarse-grained level, we measured how well 
judges agreed on which sense(s) fit at all.  Here 
Fleiss? Kappa was 48%, also considered moderate. 
We evaluated the examples on three criteria. 
Yield is the percentage of intended senses for 
which we generate at least one example ? whether 
it fits or not.  For the 73 synsets, this percentage is 
92%.  Moreover, we typically generate 3 examples 
for a word sense.  In comparison, only 34% of the 
synsets have even a single example in WordNet. 
 (Fine-grained) precision is the percentage of 
examples that the intended sense fits best accord-
ing to the judges. Human judges often disagree, so 
we prorate this percentage by the percentage of 
judges who chose the intended sense as the best fit.  
The result is algebraically equivalent to computing 
precision separately according to each judge, and 
then averaging the results.  Precision for generated 
examples was 36% for those 23 synsets and 27% 
for all 67 synsets with generated examples.  Al-
though we expected WordNet to be a gold stan-
dard, its precision for the 23 synsets having 
examples was 52% ? far less than 100%. 
This low precision suggests that the WordNet 
contexts to illustrate different senses were often 
not informative enough for the judges to distin-
guish them from all the other senses.  For example, 
the WordNet example reduce one?s standard of 
living is attached to the sense ?lessen and make 
more modest.?  However, this sense is hard to dis-
tinguish from ?lower in grade or rank or force 
somebody into an undignified situation.? In fact, 
two judges did not choose the first sense, and one 
of them chose the second sense as the best fit.   
Coarse-grained precision is similar, but based on 
how often the intended sense fits the example at 
all, whether or not it fits best.  Coarse-grained pre-
cision was 67% for the 23 WordNet examples, 
40% for the examples generated for those 23 syn-
sets, and 33% for all 67 generated examples. 
Coarse-grained precision is important because 
fine-grained semantic distinctions do not matter in 
illustrating a core sense of a word.  The problem of 
how to cluster fine-grained senses into coarse 
senses is hard, especially if consensus is required 
(Navigli et al 2007). Rather than attempt to identi-
fy a single definitive partition of a target word?s 
synsets into coarse senses, we implicitly define a 
coarse sense as the subset of synsets rated by a 
judge as fitting a given example.  Thus the cluster-
ing into coarse senses is not only judge-specific but 
example-specific:   different, possibly overlapping 
sets of synsets may fit different examples. 
Recall is the percentage of synsets that fit their 
generated examples. Algebraically it is the product 
of precision and yield.  Fine-grained recall was 
25% for the generated examples, compared to only 
18% for the WordNet examples. Coarse-grained 
recall was 30% for the generated examples, com-
pared to 23% for the WordNet examples. 
Figure 2 shows how yield, inter-rater agreement, 
and coarse and fine precision for the 8 target words 
vary with their number of synsets.  With so few 
words, this analysis is suggestive, not conclusive. 
We plot all four metrics on the same [0,1] scale to 
save space, but only the last two metrics have di-
rectly comparable values,  However, it is still mea-
ningful to compare how they vary.  Precision and 
inter-rater reliability generally appear to decrease 
with the number of senses.  As polysemy increases, 
the judges have more ways to disagree with each 
other and with our program.  Yield is mostly high, 
but might be lower for words with many senses, 
due to deficient document corpora for rare senses.   
  
Figure 2: Effects of increasing polysemy 
Table 1 compares the generated and WordNet 
examples on various measures.  It compares preci-
sion on the same 23 senses that have WordNet ex-
amples.  It compares recall on all 73 senses.  It 
compares Kappa on the 23 WordNet examples and 
the sample of generated examples the judges rated.   
Number of target word synsets 
0%
10%
20%
30%
40%
50%
60%
70%
80%
90%
100%
0 5 10 15 20
Yield
Fleiss' 
Kappa
Precision 
(Coarse)
Precision 
(Fine)
108
 
Generated WordNet 
Yield 92% 34% 
Senses with examples 67 23 
Avg. words in context 5.91 7.87 
Precision 
(same 23) 
    Fine 36% 52% 
Coarse 40% 67% 
Recall 
Fine  25% 18% 
Coarse 30% 23% 
Fleiss? 
Kappa 
Fine 0.43 0.39 
Coarse 0.48 0.49 
Table 1: Generated examples vs. WordNet 
Errors occur when 1) the corpus is missing a 
word sense; 2) LDA fails to find good sense indi-
cators; or 3) Context Generation fails to generate a 
sense-appropriate context. 
Our method succeeds when (1) the target sense 
occurs in the corpus, (2) LDA finds good indica-
tors for it, and (3) Context Generation uses them to 
construct a sense-appropriate context.  For exam-
ple, the first sense of advantage is ?the quality of 
having a superior or more favorable position,? for 
which we obtain the sense indicators support, work, 
time, order, life, knowledge, mind, media, human, 
market, experience, nature, make, social, informa-
tion, child, individual, cost, people, power, good, 
land, strategy, and company, and generate (among 
others) the context ?knowledge gave him an ad-
vantage?. 
Errors occur when any of these 3 steps fails.  
Step 1 fails for the sense ?reduce in scope while 
retaining essential elements? of reduce because it 
is so general that no good example exists in the 
corpus for it.  Step 2 fails for the sense of force in 
?the force of his eloquence easily persuaded them? 
because its sense indicators are men, made, great, 
page, man, time, general, day, found, side, called, 
and house.  None of these words are precise 
enough to convey the sense.  Step 3 fails for the 
sense of advantage as ?(tennis) first point scored 
after deuce,? with sense indicators point, game, 
player, tennis, set, score, points, ball, court, ser-
vice, serve, called, win, side, players, play, team, 
games, match, wins, won, net, deuce, line, oppo-
nent, and turn.  This list looks suitably tennis-
related.  However, the generated context ?the 
player has an advantage? fits the first sense of 
advantage; here the indicator player for the tennis 
sense is misleading. 
4 Contributions and Limitations 
This paper presents what we believe is the first 
system for generating sense-appropriate contexts to 
illustrate different word senses even if they have 
the same part of speech.  We define the problem of 
generating sense-targeted contexts for vocabulary 
learning, factor it into Sense Indicator Extraction 
and Context Generation, and compare the resulting 
contexts to WordNet in yield, precision, and recall 
according to human judges who decided, given 
definitions of all senses, which one(s) fit each con-
text, without knowing its source or intended sense.  
This test is much more stringent than just deciding 
whether a given word sense fits a given context. 
There are other possible baselines to compare 
against, such as Google snippets. However, Google 
snippets fare poorly on criteria for teaching child-
ren vocabulary (Liu et al under revision).  Another 
shortcoming of this alternative is the inefficiency 
of retrieving all contexts containing the target word 
and filtering out the unsuitable ones.  Instead, we 
compile constraints on suitability into a generator 
that constructs only contexts that satisfy them.  
Moreover, in contrast to retrieve-and-filter, our 
constructive method (concatenation of overlapping 
Google 5-grams) can generate novel contexts. 
There is ample room for future improvement. 
We specify word senses as WordNet synsets rather 
than as coarser-grain dictionary word senses more 
natural for educators.  Our methods for target word 
document corpus construction, Sense Indicator 
Extraction, and Context Generation are all fallible.  
On average, 1 of 3 human judges rated the result-
ing contexts as sense-appropriate, half as many as 
for WordNet examples.  However, thanks to high 
yield, their recall surpassed the percentage of syn-
sets with WordNet examples.  The ultimate crite-
rion for evaluating them will be their value in 
tutorial interventions to help students learn vocabu-
lary. 
Acknowledgments 
This work was supported by the Institute of Educa-
tion Sciences, U.S. Department of Education, 
through Grant R305A080157 to Carnegie Mellon 
University. The opinions expressed are those of the 
authors and do not necessarily represent the views 
of the Institute or the U.S. Department of Educa-
tion.  We thank the reviewers and our judges. 
109
References  
Isabel L.  Beck, Margaret G. Mckeown and Linda 
Kucan. 2002. Bringing Words to Life:  Robust 
Vocabulary Instruction. NY, Guilford. 
David Blei, Andrew Ng and Michael Jordan. 2003. 
Latent Dirichlet alocation. Journal of Machine 
Learning Research 3: 993?1022. 
Weisi Duan and Alexander Yates. 2010. Extracting 
Glosses to Disambiguate Word Senses. Human 
Language Technologies: The 2010 Annual 
Conference of the North American Chapter of the 
Association for Computational Linguistics, Los 
Angeles. 
Ruben G. Fukkink, Henk Blok and Kees De Glopper. 
2001. Deriving word meaning from written context: 
A multicomponential skill. Language Learning 51(3): 
477-496. 
Patricia A. Herman, Richard C. Anderson, P. David 
Pearson and William E. Nagy. 1987. Incidental 
acquisition of word meaning from expositions with 
varied text features. Reading Research Quarterly 
22(3): 263-284. 
Joseph R. Jenkins, Marcy  Stein and Katherine Wysocki. 
1984. Learning vocabulary through reading. 
American Educational Research Journal 21: 767-787. 
Melanie R. Kuhn and Steven A. Stahl. 1998. Teaching 
children to learn word meaning from context: A 
synthesis and some questions. Journal of Literacy 
Research 30(1): 119-138. 
Anagha Kulkarni and Ted Pedersen. 2005. Name 
discrimination and email clustering using 
unsupervised clustering and labeling of similar 
contexts. Proceedings of the Second Indian 
International Conference on Artificial Intelligence, 
Pune, India. 
Liu Liu, Jack Mostow and Greg Aist. 2009. Automated 
Generation of Example Contexts for Helping 
Children Learn Vocabulary. Second ISCA Workshop 
on Speech and Language Technology in Education 
(SLaTE), Wroxall Abbey Estate, Warwickshire, 
England. 
Liu Liu, Jack Mostow and Gregory S. Aist. under 
revision. Generating Example Contexts to Help 
Children Learn Word Meaning. Journal of Natural 
Language Engineering. 
David Martinez, Oier Lopez de Lacalle and Eneko 
Agirre. 2008. On the use of automatically acquired 
examples for all-nouns word sense disambiguation. 
Journal of Artificial Intelligence Research 33: 79--
107. 
Rada Mihalcea. 2002. Bootstrapping large sense tagged 
corpora. Proceedings of the 3rd International 
Conference on Languages Resources and Evaluations 
LREC 2002, Las Palmas, Spain. 
R. Uslan Mitkov, Le An Ha and Nikiforos Karamanis. 
2006. A computer-aided environment for generating 
multiple choice test items. Natural Language 
Engineering 12(2): 177-194. 
William E. Nagy, Richard C. Anderson and Patricia A. 
Herman. 1987. Learning Word Meanings from 
Context during Normal Reading. American 
Educational Research Journal 24(2): 237-270. 
William E. Nagy, Patricia A. Herman and Richard C. 
Anderson. 1985. Learning words from context. 
Reading Research Quarterly 20(2): 233-253. 
Roberto Navigli, Kenneth C. Litkowski and Orin 
Hargraves. 2007. Semeval-2007 task 07: Coarse-
grained English all-words task. Proceedings of the 
4th International Workshop on Semantic Evaluations, 
Association for Computational Linguistics: 30-35. 
Juan Pino and Maxine Eskenazi. 2009. An Application 
of Latent Semantic Analysis to Word Sense 
Discrimination for Words with Related and Unrelated 
Meanings. The 4th Workshop on Innovative Use of 
NLP for Building Educational Applications, 
NAACL-HLT 2009 Workshops, Boulder, CO, USA. 
Daniel Ramage, David Hall, Ramesh Nallapati, and 
Christopher D. Manning. 2009. Labeled LDA: A 
supervised topic model for credit attribution in multi-
labeled corpora. Proceedings of the 2009 Conference 
on Empirical Methods in Natural Language 
Processing, Association for Computational 
Linguistics. 
Elinore K. Schatz and R. Scott Baldwin. 1986. Context 
clues are unreliable predictors of word meanings. 
Reading Research Quarterly 21: 439-453. 
Paula J. Schwanenflugel, Steven A. Stahl and Elisabeth 
L. Mcfalls. 1997. Partial Word Knowledge and 
Vocabulary Growth during Reading Comprehension. 
Journal of Literacy Research 29(4): 531-553. 
Patrick E. Shrout and Joseph L. Fleiss. 1979. Intraclass 
correlations: Uses in assessing rater reliability. 
Psychological Bulletin 86(2): 420-428. 
Mark Steyvers and Tom Griffiths. 2006. Probabilistic 
topic models. Latent Semantic Analysis: A Road to 
Meaning. T. Landauer, D. McNamara, S. Dennis and 
W. Kintsch. Hillsdale, NJ, Laurence Erlbaum. 
David Yarowsky. 1995. Unsupervised WSD rivaling 
supervised methods. Proceedings of the 33rd Annual 
Meeting of the Association for Computational 
Linguistics, Massachusetts Institute of Technology, 
Cambridge, MA. 
  
 
 
110
Proceedings of the SIGDIAL 2011: the 12th Annual Meeting of the Special Interest Group on Discourse and Dialogue, pages 8?17,
Portland, Oregon, June 17-18, 2011. c?2011 Association for Computational Linguistics
Which System Differences Matter?
Using `1/`2 Regularization to Compare Dialogue Systems
Jose? P. Gonza?lez-Brenes and Jack Mostow
Project LISTEN
Language Technologies Institute
School of Computer Science
Carnegie Mellon University
Pittsburgh, PA 15213, USA
{joseg,mostow}@cs.cmu.edu
Abstract
We investigate how to jointly explain the per-
formance and behavioral differences of two
spoken dialogue systems. The Join Evalu-
ation and Differences Identification (JEDI),
finds differences between systems relevant
to performance by formulating the problem
as a multi-task feature selection question.
JEDI provides evidence on the usefulness of
a recent method, `1/`p-regularized regres-
sion (Obozinski et al, 2007). We evaluate
against manually annotated success criteria
from real users interacting with five different
spoken user interfaces that give bus schedule
information.
1 Introduction
This paper addresses the problem of how to deter-
mine which differences between two versions of a
system affect their behavior. Researchers in Spo-
ken Dialogue Systems (SDSs) can be perplexed as to
which of the differences between alternative systems
affect performance metrics (Bacchiani et al, 2008).
For example, when testing on real users at differ-
ent periods of time, the variance of the performance
metrics might be higher than the difference between
systems, causing (i) significantly different scores in
identical systems deployed at different times, and
(ii) the same score on different systems (Gonza?lez-
Brenes et al, 2009).
We approach the problem of finding which system
differences matter by describing dialogues as feature
vectors constructed from the logs of dialogs gener-
ated by the SDSs interacting with real users. Hence,
we aim to identify features that jointly characterize
the system differences and the performance of the
SDS being evaluated. These features should be able
to (i) predict a performance metric and (ii) distin-
guish between the two SDS being evaluated.
The main contribution of this paper is a novel al-
gorithm for detecting differences between two sys-
tems that can explain performance. Additionally, we
provide details on how to implement state-of-the-art
multi-task learning for SDSs.
The rest of this manuscript is organized as fol-
lows. Section 2 reviews multi-task feature selection.
Section 3 describes two algorithms to find which
system differences matter. Section 4 describes the
specific SDS used to illustrate our algorithms. Sec-
tion 5 presents some experimental results. Section 6
reviews related prior work. Section 7 presents some
concluding remarks and future work. Appendix A
provides implementation details of the multi-task
learning approach we used.
2 Feature Selection
In this section we describe how we use regression to
perform feature selection. Feature selection meth-
ods construct and select subsets of features in order
to build a good predictor. We focus our attention on
feature selection methods that use complexity (regu-
larization) penalties, because of their recent theoret-
ical and experimental success (Yuan and Lin, 2006;
Park and Hastie, 2007). We provide a more rigorous
description of how to implement this formulation as
an optimization problem in Appendix A.
We use labels to encode the output we want to
predict. For example, if our performance metric is
binary, we label successful dialogues with a +1, and
unsuccesful dialogues with a ?1. Given a training
set consisting of labeled dialogues, we want to learn
8
a model that assigns a label to unseen dialogues. We
follow an approach called empirical risk minimiza-
tion (Obozinski et al, 2007), that aims to minimize
the error of fitting the training data, while penalizing
the complexity of the model:
Minimize Model loss + ? Complexity (1)
Here the hyper-parameter ? controls the trade-off
between a better fit to the training data (with a higher
risk of over-fitting it), and a simpler model, with
fewer features selected (and less predictive power).
We now review the two components of risk mini-
mization, model loss and complexity penalty.
2.1 Model Loss
We model probabilistically the loss of our model
against the real-life phenomenon studied. Given a
dialogue x, with correct label l, its loss using a
model ? is:
loss?(y?, x) ? P (y = l|x; reality)? P (y? = l|x;?)
(2)
Here y? is the predicted value of the event y. Since l
is the true label, P (y = l|x; reality) = 1. To get the
overall loss of the model, we aggregate over the pre-
diction loss of each of the dialogues in the training
set by summing their individual loss calculated with
Equation 2. Let X = {x(1), x(2), . . . x(n)} be the n
dialogues in the training set. Then the overall loss of
model ? is:
loss?(y
(1), x(1)) + ? ? ?+ loss?(y
(n), x(n))
Since we use discrete labels, we use a logistic
function to model their probability. Let x1, . . . xk
be the k features extracted from dialogue x. Then
the logistic regression model is:
P (y? = +1|x;?) =
1
Z
exp(?1x1 + ? ? ?+ ?kxk)
Here ?1...?k are the parameters of the model, and
Z simply normalizes P to ensure that P is a valid
probability function (the range of P should be 0 to
1):
Z = 1 + exp(?1x1 + ? ? ?+ ?kxk)
Multi-task learning solves related regression
problems at the same time using a shared representa-
tion. We now describe the risk-minimization formu-
lation for multi-task learning. Let ym be the value
of the performance metric. Let ys be the label of the
system that generated the dialogue. The individual
dialogue loss of using models ?m and ?s is:
loss?m(y?
m, x) + loss?s(y?
s, x)
2.2 Complexity Penalties
We consider a feature xi to be selected into the
model if its regression coefficient ?i is non-zero.
Complexity penalties encourage selecting only a few
features. We review several commonly used penal-
ties (Zou and Hastie, 2005):
? `2 Penalty. Under some circumstances `2
penalties perform better than other types of
penalties (Zou and Hastie, 2005). The `2
penalty for a model ? is:
||?||`2 ?
?
(?1)2 + ? ? ?+ (?k)2
? `1 Penalty. An `1 penalty induces sparsity by
setting many parameters of the model ? to ex-
actly zero (Tibshirani, 1996).
||?||`1 ? |?1|+ ? ? ?+ |?k|
? `1/`2 Penalty. Yuan and Lin (2006) proposed
a group penalty for penalizing groups of fea-
tures simultaneously. Previous work has shown
that grouping features between tasks encour-
ages features to be used either by all tasks or
by none (Turlach et al, 2005; Obozinski et
al., 2007; Lounici et al, 2009; Puniyani et al,
2010). Our `1/`2 penalty is:
?
?
?
?
?
(?m1 )
2 + (?s1)
2
?
?
?
?+ ...+
?
?
?
?
?
(?mk )
2 + (?sk)
2
?
?
?
?
3 Finding Features that Predict
Performance and System Differences
We find system differences that are predictive of
SDS performance, relying on:
? Describing dialogues as feature vectors. The
behavior of the systems must be describable
by features extracted from the logs of the sys-
tems. A discussion of feature engineering for
dialogue systems is found in (Gonza?lez-Brenes
and Mostow, 2011).
9
? Finding system differences. The features of a
classifier that distinguishes between SDSs, can
be used to identify their differences (Gonza?lez-
Brenes et al, 2009). When comparing two
SDSs, we label the baseline system with ?1,
and the alternate version with +1.
? Modeling performance. Although our ap-
proach does not depend on a specific perfor-
mance metric, in this paper we use dialogue
success, a binary indicator that triggers that
the user?s query was answered by the SDS.
Task completion is cheaper to compute than di-
alogue success, as it does not require a man-
ual human labeled reference, but we consider
that dialogue success is a more accurate metric.
Task completion is used in commercial applica-
tions (Bacchiani et al, 2008), and has been ex-
tensively studied in the literature (Walker et al,
2001; Walker et al, 2002; Hajdinjak and Mi-
helic, 2006; Levin and Pieraccini, 2006; Mo?ller
et al, 2007; Mo?ller et al, 2008; Schmitt et
al., 2010). We encode success of dialogues by
manually annotating them with a binary vari-
able that distinguishes if the user query is ful-
filled by the SDS.
We now present two algorithms to find what dif-
ferences matter between systems. We introduce Se-
rial EvaluatioN Analysis (SERENA) as a scaffold
for the Join Evaluation and Differences Identifica-
tion (JEDI) algorithm.
3.1 SERENA algorithm
The input to SERENA is a collection of log files
created by two different SDSs and two functions that
represent the correct label for the regression tasks.
In our case these functions should return binary la-
bels (+1,?1): one task distinguishes between suc-
cessful and unsuccessful dialogues, and the other
task distinguishes a baseline from an alternative SDS
version. SERENA?s objective is to select features
from one task, and use them to predict the other task.
For example, SERENA selects features that predict
differences between versions, and uses them to pre-
dict performance.
Algorithm 1 provides the pseudo-code for SER-
ENA. Line 1 builds the training set X from parsing
the logs of the SDSs. Lines 2 and 3 create the output
Algorithm 1 SERENA algorithm
Require: Logs1, Logs2 are the collections of SDS
logs of two systems. task1, task2 are func-
tions that return the value of a performance met-
ric, and which system is being evaluated (?1 if
is the baseline, +1 otherwise).
1: X? extract features(Log1,Log2)
2: yt1 ?
[
task1(Logs1)
task1(Logs2)
]
3: yt2 ?
[
task2(Logs1)
task2(Logs2)
]
4: // Select features that explain both tasks:
5: for ? = {0.1, 0.2, . . . } do
6: ?t1 ? regression`1(X,y
t1 , ?)
7: // Get feature weights:
8: X? ? X; where xk|?xk ? X?, ?
t1
k 6= 0
9: ?? ? regression`2(X
?,yt2 , ?c)
10: end for
11: return ??
variables y for the regression tasks. Line 6 returns
the most predictive features using `1 regularization
as described in Section 2. Line 8 builds a new train-
ing set, removing the features that were not selected
in line 6. Line 9 builds the final coefficients by fitting
a `2-regularized model using a constant ?c. We cal-
culate the coefficients using an `2 penalty, because
it has a better fit to the data (Zou and Hastie, 2005).
Moreover, by using the same penalty, we control for
the idiosyncrasies different penalties have in param-
eter learning. In the experiments described in Sec-
tion 5, all of our experiments are reported fitting a
`2-regularized models.
SERENA is not conmutative with regards to the
order of the tasks: selecting the features that predict
performance and using them to predict system dif-
ferences is not the same as the reverse. More impor-
tantly, SERENA only searches in one of the tasks at
a time. We are interested in finding the features that
explain both tasks simultaneously. In the next sub-
section we describe JEDI which makes use of recent
advances in multi-task feature selection in order to
find the features for both tasks at the same time.
3.2 JEDI algorithm
Algorithm 2 provides the pseudo-code for JEDI.
JEDI uses multi-task regression to find the fea-
tures that affect performance and system differences
10
Algorithm 2 JEDI algorithm
Require: Logs1, Logs2 are the collections of SDS
logs of two systems. task1, task2 are func-
tions that return the value of a performance met-
ric, and which system is being evaluated (?1 if
is the baseline, +1 otherwise).
1: X? extract features(Log1,Log2)
2: yt1 ?
[
task1(Logs1)
task1(Logs2)
]
3: yt2 ?
[
task2(Logs1)
task2(Logs2)
]
4: // Select features that explain both tasks:
5: for ? = {0.1, 0.2, . . . } do
6: ?t1?t2 ? regression`1/`2(X, ~y
t1 , ~yt2 , ?)
7: // Get feature weights:
8: X? ? X; where xk|?xk ? X?, ?
t1
k 6= 0
9: ?? ? regression`2(X
?,yt2 , ?c)
10: end for
11: return ??
jointly. To ensure a sparse feature set across the
tasks, it applies a `1/`2 penalty. In this way, a feature
is either selected to predict performance and system
differences, or it is not selected at all. In the next
section we review a concrete example of a SDS in
which we apply SERENA and JEDI.
4 Case Study
We use data collected from Let?s Go!, an experi-
mentary SDS available for research purposes1 that
provides bus planning information to the Pittsburgh
East End community (Raux et al, 2006). Figure 1
shows a sample dialogue.
We assembled three disjoint datasets to have di-
alogues just before and after what we considered
important architectural differences of Let?s Go?s im-
plementation. By going through the change log, we
realize that the magnitude of the differences varies
widely. Table 1 describes the date the systems were
deployed, the description of the change in the SDS,
and the number of dialogues in each set. From the
86 dialogues of ?Set A?, half are a random sam-
ple of the logs before the acoustic model was re-
trained (Log1), and the other half is a sample af-
ter the acoustic model was retrained (Log2). In the
case of the 84 dialogues of ?Set B?, half of the logs
1http://www.dialrc.org/
System: Welcome to the CMU Let?s Go!
Bus Information System. How
may I help you?
User: I?d like to go to Forbes and Mur-
ray.
System: To Forbes and Murray. Where
are you leaving from?
User: Carnegie Mellon.
System: From Carnegie Mellon. There
are five buses running between
Carnegie Mellon and Forbes and
Murray: the 501, the 59U, the
61A, the 61B, and the 61C.
What else can I do for you?
User: When is the next one?
System: The next 61C going to Forbes
and Murray will leave Carnegie
Mellon at 5:13 PM.
Figure 1: Example of a Let?s Go dialogue, from (Raux et
al., 2006)
use an older version of the Olympus SDS infrastruc-
ture (Bohus et al, 2007), and the other half uses
Olympus II. Since each system was deployed in a
different period of time, we want to corroborate that
we are modeling the differences among systems, and
not seasonal. Hence, for control conditions, we also
chose a data set that contained no major change to
the system or to other conditions (Set C).
Sets were built by randomly sampling from the
collection of logs. They have the same number of di-
alogues from each SDS version (baseline/alternate).
Each dialogue was manually annotated to indicate
whether the user?s query was fulfilled, and we re-
moved from our analysis the two dialogues that were
only partially fulfilled. The number of successful di-
alogues is different from the number of unsuccessful
dialogues.
We created a script to extract features from the log
files of Let?s Go!. The script has an explicit list of
features to extract from the event logs, such as the
words that were identified by the Automatic Speech
Recognizer. Although this script is dependent on our
specific log format, it should be a simple program-
ming task to adapt it to a different dialogue system,
provided its logs are comprehensive enough. The
11
Table 1: Dataset Description
Set Size Description Date
A 86
Baseline 8/05 10/05
New acoustic model 12/05 2/05
B 86
Baseline 8/06 10/06
New SDS architecture 6/07 7/07
C 84
Baseline 10/07 11/07
No change 11/07 12/07
script performs the standard transformation of cen-
tering feature values as z-scores with mean zero and
standard deviation one.
Table 2 summarizes the properties we are inter-
ested to model. Dialogue properties are the features
that summarize the behavior of the whole dialogue,
and turn properties work at a finer-grain. We encode
turn properties into features in the following way:
? Global average. Turn properties are averaged
over the entire dialogue.
? Beginning window. Turn properties are aver-
aged across an initial window. Based on pre-
liminary experiments, we defined the window
as the first 5 turns.
? State. We relied on the fact that SDSs are of-
ten engineered as finite state automata (Bohus
et al, 2007). Properties are averaged across the
states that belong to a specific dialogue state
(for example, asking departure place). Because
we are interested in early identification of dif-
ferences, we restricted state features to be in-
side the beginning window.
5 Evaluation
We assess the performance of our algorithms by
evaluating the classification accuracy using the fea-
tures selected. To facilitate assessment of SDS, we
only consider models that select up to 15 features.
Figure 2 reports mean classification accuracy using
five-fold cross-validation. Its first column describes
how well the features selected perform on detecting
system differences, and the second column describes
how well they predict task success as a performance
metric. We compare JEDI and SERENA against the
following approaches:
Table 2: Features
Dialogue Properties
# of re-prompted turns
# of turns
Mean Dialogue length
is evening?, is weekend?, 0-23 hour
Turn Properties
Occurrences of word w
# of parse errors
# of unrecognized words
# of words
# of repeated words
# of unique words
Turn length
Words per minute
Failed prompts (number and percentage)
Mean Utterance Length
Barge-in (in seconds)
Machine-user pause (in seconds)
User-machine pause (in seconds)
Amplitude (power) statistics
? Majority classifier baseline. A classifier that
always selects the majority class (datasets B
and C are not balanced in the number of suc-
cessful dialogues).
? Same Task Classifier We report the classifica-
tion accuracy of the model trained and tested
on the same task. Features are selected using
an `1 penalty, and the coefficients are estimated
with `2-regularized logistic regression. For ex-
ample, in the column of the left, SERENA uses
the most predictive features of system differ-
ences to predict success, while the same task
classifier uses them to predict system differ-
ences. The same task classifier does not answer
?which system differences matter?, it is just an
interesting benchmark.
We used a one-sample t-test to check for sta-
tistically significant differences against the classifi-
cation accuracy of the majority classifier baseline.
We used a paired-sample t-test to check for sig-
nificant differences in classification accuracy be-
tween classifiers. Paired samples have the same ?
hyper-parameter, which was described in the risk-
12
0 5 10 15
0.4
0.5
0.6
0.7
0.8
0.9
1  System Differences
# of features
 
Da
tas
et 
A
Cla
ssi
fica
tion
 Ac
c.
0 5 10 15
0.4
0.5
0.6
0.7
0.8
0.9
1 Dialogue Success
# of features
Cla
ssi
fica
tion
 Ac
c.
0 5 10 15
0.4
0.5
0.6
0.7
0.8
0.9
1
# of features
 
Da
tas
et 
B
Cla
ssi
fica
tion
 Ac
c.
0 5 10 15
0.4
0.5
0.6
0.7
0.8
0.9
1
# of features
Cla
ssi
fica
tion
 Ac
c.
0 5 10 15
0.4
0.5
0.6
0.7
0.8
0.9
1
# of features
 
Da
tas
et 
C
Cla
ssi
fica
tion
 Ac
c.
 
 
0 5 10 15
0.4
0.5
0.6
0.7
0.8
0.9
1
# of features
Cla
ssi
fica
tion
 Ac
c.
 
 
Majority SERENA JEDI Same Task Classifier
Figure 2: Classification accuracy of different feature se-
lection techniques
minimization formulation explained in Section 2.
This hyper-parameter is related to the number of fea-
tures selected ? as ? increases, the number of fea-
tures selected decreases. We use 5% as the signif-
icance level at which to reject the null hypothesis.
When checking for statistical differences, we tested
on the range of ?s computed2.
First we investigate the performance of the sim-
pler algorithm SERENA. For Dataset A, SERENA
does not yield significant differences over the ma-
jority classifier baseline. For Dataset B, SERENA
is significantly better than the majority classifier in
predicting system differences, but is significantly
worse for predicting success. This means that the or-
der in which we choose the tasks in SERENA affects
its performance. SERENA performs significantly
worse in the Control Set C. We conclude that SER-
2? = {100, 30, 25, 20, 19, 18, . . . , 1, 0.5, 0.25, 0.1}
Table 3: Features selected in Dataset A
Feature Suc. Diff. JEDI
System-user pause 5 5
Weekend night? 3
% of failed prompts 4
?Forbes St.? word 5 3
User?s max. power 5
Table 4: Features selected in Dataset B
Feature Suc. Diff. JEDI
% of failed prompts 5 4
User?s power std.dev. 5
Weekend night? 3
Unrecognized word 5
Words/min. 4
User-system pause 5
Turn length 5 5
ENA is not very reliable in predicting which system
differences matter.
We now discuss how well JEDI is able to fill-in for
the deficiencies of SERENA. As an ?upper-bound?,
we will compare it to a classifier trained and tested
in the same task. This classifier significantly dom-
inates over the majority baseline, even for the the
Control Set C, where there were no changes in the
SDS. This suggests that the classifier might be pick-
ing up on seasonal differences. For Set A, JEDI per-
forms significantly better than the majority classi-
fier and than SERENA. For Set B, there are no sig-
nificant differences between the upper-bound clas-
sifier and JEDI when predicting for changes in the
SDS. Again, JEDI dominates over SERENA and the
majority baseline. For the Control Set C, JEDI is
not statistically different from the majority baseline.
This is the expected behavior, since the difference in
performance cannot be explained by the differences
between the SDS. We hypothesize that the classifi-
cation accuracy of JEDI could be used as a distance
function between SDS: The closer the accuracy of
distinguishing SDS is to 50%, the more similar the
SDSs are. Conversely, when JEDI is able to classify
system differences closer to 100%, it is because the
SDSs are more different.
Tables 3 and 4 describe the features selected for
Sets A and B respectively. The numbers indicate
13
in how many folds the feature was selected by JEDI
and by classifiers trained to predict Success and SDS
differences using five-fold cross validation. The ?
used is selected to contain the closest to five features
(ties are resolved randomly). We only report fea-
tures that appeared in at least three folds. In Dataset
A we see that time of day is selected to predict di-
alogue success. Anecdotally, we have noticed that
many users during weekend nights appear to be in-
toxicated when calling the system. JEDI does not
select ?is weekend night? as a feature, because it
has little predictive power to detect system differ-
ences. In Dataset A, JEDI selects a speech recogni-
tion feature (the token ?Forbes St? was recognized),
and an end-pointing feature. Since in Dataset A, the
difference between systems correspond to a differ-
ent acoustic model, these features make sense intu-
itively. In Dataset B, JEDI detected that the features
most predictive with system differences and success
are percentage of failed prompts and the length of
the turn. The models for both systems make sense
after the fact. However, neither model was known
beforehand, nor did we know which of many fea-
tures considered would turn out to be informative.
Anecdotally, the documentation of the history of
changes of Let?s Go! is maintained manually. Some-
times, because of human error, this history is incom-
plete. The ability of JEDI to identify system differ-
ences has been able to help completing the history
of changes (Gonza?lez-Brenes et al, 2009).
6 Relation to Prior Work
The scientific literature offers several performance
metrics to assess SDS performance (Polifroni et al,
1992; Danieli and Gerbino, 1995; Bacchiani et al,
2008; Suendermann et al, 2010). SDS are eval-
uated using different objective and subjective met-
rics. Examples of objective metrics are the mean
number of turns in the dialogue, and dialogue suc-
cess. Subjective evaluations study measure satisfac-
tion through controlled user studies. Ai et al (2007)
studied the differences in using assessment metrics
with real users and paid users.
PARADISE, a notable example of a SDS subjec-
tive evaluation, finds linear predictors of a satisfac-
tion score using automatic and hand-labeled features
(Hajdinjak and Mihelic, 2006; Walker et al, 2001),
or only automatic features (Hastie et al, 2002). Sat-
isfaction scores are calibrated using surveys in con-
trolled experiments (Mo?ller et al, 2007; Mo?ller et
al., 2008). Alternatively, Eckert et al (1998) pro-
posed simulated users to evaluate SDSs. Their per-
formance metric has to be tuned with a subjective
evaluation as well, in which they refer to the PAR-
ADISE methodology. Our approach does not re-
quire user surveys to be calibrated. Moreover, it
would be feasible to adapt JEDI to regress to PAR-
ADISE, or other performance metrics. Our work ex-
tends previous studies that define performance met-
rics, in proposing an algorithm that finds how system
differences are related to performance.
7 Conclusions and Future Work
We have presented JEDI, a novel algorithm that finds
features describing system differences relevant to a
success metric. This is a novel, automated ?glass
box? assessment in the sense of linking changes in
overall performance to specific behavioral changes.
JEDI is an application of feature selection using reg-
ularized regression.
We have presented empirical evidence suggesting
that JEDI?s use of multi-task feature selection per-
forms better than single-task feature selection. Fu-
ture work could extend JEDI to quantify the vari-
ability in performance explained by the differences
found. Common techniques in econometrics, such
as the Seemingly Unrelated Regressions (SUR) for-
mulation (Zellner, 1962), may prove useful for this.
In our approach we used a single binary evalu-
ation criterion. By using a different loss function,
JEDI can be extended to allow continuous-valued
metrics. Moreover, previous work has argued that
evaluating SDSs should not be based on just a sin-
gle criterion (Paek, 2001). JEDI?s multi-task for-
mulation can be extended to include more than one
performance criterion at the same time, and may
prove helpful to understand trade-offs among differ-
ent evaluation criteria.
A Implementation Details of Feature
Selection
In this appendix we review how to set-up multi-task
feature selection as an optimization problem.
14
A.1 `1-Regularized Regression for Single-Task
Feature Selection
We first review using regression with `1 regulariza-
tion for single-task feature selection. Given a train-
ing set represented by X, denoting a n ? k matrix,
where n is the number of dialogues, and k is the
number of features extracted for each dialogue, we
want to find the coefficients of the parameter vector
~?, that can predict the output variables described in
the vector ~y of length n.
For this, we find the parameter vector that mini-
mizes the loss function J , penalized by a regulariza-
tion term (Tibshirani, 1996):
argmin
~?
J(X, ~?, ~y) + ?||~?||`1 (3)
In the case of binary classification, outputs are bi-
nary (any given y = ?1). A commonly used loss
function J is the Logistic Loss:
Jlog(x, ?, y) ?
1
1 + ey(x??)
(4)
The `p-norm of a vector ~? is defined as:
||~?||`p ?
(
k?
i=1
|?i|
p)1/p
The `?-norm is defined as ||~?||`? ?
max(?1, ?2, . . . , ?k).
The regularization term ||~?||`1 in Equation 3 con-
trols model complexity: The higher the value of the
hyper-parameter ?, the smaller number of features
selected. Conversely, the smaller the value of ?,
the better the fit to the training data, with higher
risk of over-fitting it. Thus, Equation 3 jointly per-
forms feature selection and parameter estimation; it
induces sparsity by setting many coefficients of ~?
to zero (Tibshirani, 1996). Features with non-zero
coefficients are considered the features selected.
A.2 `1-Regularized Regression for Multi-Task
Feature Selection
`1 regularization can be used to learn a classifier for
each of T prediction task independently. In our case
we are interested in only two prediction tasks: ver-
sion and success. We will index tasks with super-
script t, and we define Xt as the n ? k training
data for task t, used to predict the output variable ~yt.
Learning each model separately yields the following
optimization problem (Obozinski et al, 2007):
argmin
~?
t
T?
t=1
J(Xt, ~?
t
, ~yt) + ?||~?
t
||`1 (5)
Solving this problem leads to individual sparsity in
each task (each ~?
t
has many zeros), but the model
does not enforce a common subset of features for
all of the related output variables simultaneously
(Turlach et al, 2005). In the next subsection we
study how to achieve global sparsity across tasks.
A.3 `1/`p-Regularized Regression for
Multi-task Feature Selection
Although `1-regularization is very successful at se-
lecting individual features, it does not perform ad-
equately when a group of features should enter
or leave the model simultaneously (Yuan and Lin,
2006). Group LASSO (Yuan and Lin, 2006), which
relies on `1/`p-regularization to overcome this lim-
itation, by allowing groups of feature entering or
leaving the model simultaneously. `1/`p regular-
ization has been studied for multi-task learning by
grouping each of the k features across the T learning
tasks (Turlach et al, 2005; Obozinski et al, 2007;
Lounici et al, 2009; Puniyani et al, 2010).
Let us define B as a n? T matrix, whose tth col-
umn is the parameter vector for the task t. For ex-
ample, since we have two tasks B = [~?
t=1
, ~?
t=2
].
Let ~?g denote the g
th row of B. In the context of
multi-task learning, the `1/`p-norm of a matrix B is
defined as (Obozinski et al, 2007; Puniyani et al,
2010):
||B||`1/`p ?
k?
g=1
||~?g||`p (6)
Multi-task feature selection with `1/`p regular-
ization is formulated as (Obozinski et al, 2007;
Puniyani et al, 2010):
argmin
B
T?
t=1
J(Xt, ~?
t
, ~yt) + ?||B||`1/`2 (7)
When T = 1, the multi-task problem of Equation 7
reduces to the single-task problem of Equation 5.
15
A.4 Optimization procedure
Puniyani et al (2010) describe that finding the pa-
rameter coefficients B of Equation 7 can be achieved
more easily by transforming the problem into an
equivalent single-task multivariate regression. We
follow their procedure to create ~yg, ~?g and Xg:
1. Concatenate the vectors ~yt?s into a single vec-
tor ~yg of length n ? T . In our case, since we
have only two tasks (T = 2), we get the vector
~yg =
[ ~yt=1
~yt=2
]
.
2. Similarly, we concatenate the ~?
t
?s into a k? T
vector ~?g, in our case ~?g =
[ ~?
t=1
~?
t=2
]
.
3. Build a (n ? T )? (k ? T ) block-diagonal matrix
Xg, where Xt?s are placed along the diagonal,
and the rest of the elements are set to zero. In
our case since we only have two tasks this is
Xg =
[
Xt=1 ?
? Xt=2
]
, where each ? denotes a
n ? k zero-matrix. The expanded notation of
Xg is:
Xg ?
?
?
?
?
?
?
?
?
?
?
?
?
xt=1
(1)
1 ... x
t=1(1)
k 0 ... 0
...
...
...
...
xt=1
(n)
1 ... x
t=1(n)
k 0 ... 0
0 ... 0 xt=2
(1)
1 ... x
t=2(1)
k
...
...
...
...
0 ... 0 xt=2
(n)
1 ... x
t=2(n)
k
?
?
?
?
?
?
?
?
?
?
?
?
Thus, the multi-task learning problem from Equa-
tion 7 is equivalent to (Yuan and Lin, 2006; Puniyani
et al, 2010):
argmin
B
J(Xg, ~?g, ~yg) + ?||B||`1/`2 (8)
In this work we solve this optimization problem us-
ing an existing3 implementation of Block Coordi-
nate Descent (Schmidt et al, 2008) that solves re-
gression problems with a `1/`p penalty.
Acknowledgments
This work was supported by the Institute of Ed-
ucation Sciences, U.S. Department of Education,
3Source code: http://www.cs.ubc.ca/?murphyk/
Software/L1CRF/
through Grant R305A080628 to Carnegie Mellon
University. The opinions expressed are those of the
authors and do not necessarily represent the views of
the Institute or U.S. Department of Education. We
thank the educators, students, and LISTENers who
helped generate, collect, and analyze our data, and
the reviewers for their helpful comments. The first
author was partially supported by the Costa Rican
Ministry of Science and Technology (MICIT).
References
H. Ai, A. Raux, D. Bohus, M. Eskenazi, and D. Lit-
man. 2007. Comparing spoken dialog corpora col-
lected with recruited subjects versus real users. In
Proc. of the 8th SIGdial workshop on Discourse and
Dialogue.
M. Bacchiani, F. Beaufays, J. Schalkwyk, M. Schuster,
and B. Strope. 2008. Deploying GOOG-411: Early
lessons in data, measurement, and testing. In IEEE
International Conference on Acoustics, Speech and
Signal Processing, 2008. ICASSP 2008, pages 5260?
5263.
D. Bohus, A. Raux, T. Harris, M. Eskenazi, and A. Rud-
nicky. 2007. Olympus: an open-source framework
for conversational spoken language interface research.
In HLT-NAACL 2007 Workshop on Bridging the Gap:
Academic and Industrial Research in Dialog Technol-
ogy.
M. Danieli and E. Gerbino. 1995. Metrics for evaluat-
ing dialogue strategies in a spoken language system.
In Proceedings of the 1995 AAAI Spring Symposium
on Empirical Methods in Discourse Interpretation and
Generation, pages 34?39.
W. Eckert, E. Levin, and R. Pieraccini. 1998. Automatic
evaluation of spoken dialogue systems. TWLT13: For-
mal semantics and pragmatics of dialogue, pages 99?
110.
J. P. Gonza?lez-Brenes and J. Mostow. 2011. Classify-
ing dialogue in high-dimensional space. Transactions
of Speech and Language Processing; Special Issue on
Machine Learning for Robust and Adaptive Spoken
Dialogue Systems. In press.
J. P. Gonza?lez-Brenes, A. W. Black, and M. Eskenazi.
2009. Describing Spoken Dialogue Systems Differ-
ences. In International Workshop on Spoken Dialogue
Systems, Irsee, Germany. Springer?Verlat.
M. Hajdinjak and F. Mihelic. 2006. The PARADISE
evaluation framework: Issues and findings. Computa-
tional Linguistics, 32(2):263?272.
H. W. Hastie, R. Prasad, and M. Walker. 2002. Auto-
matic evaluation: Using a date dialogue act tagger for
16
user satisfaction and task completion prediction. In In
LREC 2002, pages 641?648.
E. Levin and R. Pieraccini. 2006. Value-based opti-
mal decision for dialog systems. In Spoken Language
Technology Workshop, 2006. IEEE, pages 198 ?201.
K. Lounici, A.B. Tsybakov, M. Pontil, and van de Geer.
2009. Taking advantage of sparsity in multi-task learn-
ing. In Conference on Learning Theory, volume 1050,
page 9, Montreal, Quebec.
S. Mo?ller, P. Smeele, H. Boland, and J. Krebber. 2007.
Evaluating spoken dialogue systems according to de-
facto standards: A case study. Computer Speech and
Language, 21(1):26 ? 53.
S. Mo?ller, K.P. Engelbrecht, and R. Schleicher. 2008.
Predicting the quality and usability of spoken dialogue
services. Speech Communication, 50(8-9):730?744.
G. Obozinski, B. Taskar, and M.I. Jordan. 2007. Multi-
task feature selection. In The Workshop of Struc-
tural Knowledge Transfer for Machine Learning in the
23rd International Conference on Machine Learning
(ICML), Pittsburgh, PA.
T. Paek. 2001. Empirical methods for evaluating dia-
log systems. In ACL 2001 Workshop on Evaluation
Methodologies for Language and Dialogue systems,
pages 3?10.
M.Y. Park and T. Hastie. 2007. L1-regularization path al-
gorithm for generalized linear models. Journal of the
Royal Statistical Society: Series B (Statistical Method-
ology), 69(19):659?677.
J. Polifroni, L. Hirschman, S. Seneff, and V. Zue. 1992.
Experiments in evaluating interactive spoken language
systems. In Proceedings of the workshop on Speech
and Natural Language, pages 28?33. Association for
Computational Linguistics.
K. Puniyani, S. Kim, and E.P. Xing. 2010. Multi-
population GWA mapping via multi-task regularized
regression. Bioinformatics, 26(12):208.
A. Raux, D. Bohus, B. Langner, A.W. Black, and M. Es-
kenazi. 2006. Doing research on a deployed spoken
dialogue system: one year of Let?s Go! experience. In
Ninth International Conference on Spoken Language
Processing. ISCA.
M. Schmidt, K. Murphy, G. Fung, and R. Rosales. 2008.
Structure learning in random fields for heart motion
abnormality detection. In Computer Vision and Pat-
tern Recognition, 2008. CVPR 2008. IEEE Conference
on, pages 1 ?8.
A. Schmitt, M. Scholz, W. Minker, J. Liscombe, and
D. Suendermann. 2010. Is it possible to predict task
completion in automated troubleshooters? In INTER-
SPEECH, pages 94?97.
D. Suendermann, J. Liscombe, R. Pieraccini, and
K. Evanini. 2010. ?How am I Doing??: A new frame-
work to effectively measure the performance of auto-
mated customer care contact centers. In A. Neustein,
editor, Advances in Speech Recognition: Mobile Envi-
ronments, Call Centers, and Clinics, pages 155?180.
Springer.
R. Tibshirani. 1996. Regression shrinkage and selection
via the lasso. Journal of the Royal Statistical Society.
Series B (Methodological), 58(1):267?288.
B.A. Turlach, W.N. Venables, and S.J. Wright. 2005.
Simultaneous variable selection. Technometrics,
47(3):349?363.
M. Walker, C. Kamm, and D. Litman. 2001. Towards de-
veloping general models of usability with PARADISE.
Natural Language Engineering, 6(3):363?377.
M. A. Walker, I. Langkilde-Geary, H. W. Hastie,
J. Wright, and A. Gorin. 2002. Automatically train-
ing a problematic dialogue predictor for a spoken di-
alogue system. Journal of Artificial Intelligence Re-
search, 16:293?319.
M. Yuan and Y. Lin. 2006. Model selection and esti-
mation in regression with grouped variables. Journal
of the Royal Statistical Society: Series B (Statistical
Methodology), 68(1):49?67.
A. Zellner. 1962. An efficient method of estimating
seemingly unrelated regressions and tests for aggrega-
tion bias. Journal of the American Statistical Associa-
tion, 57(298):pp. 348?368.
H. Zou and T. Hastie. 2005. Regularization and vari-
able selection via the Elastic Net. Journal of the Royal
Statistical Society, 67:301?320.
17
The 7th Workshop on the Innovative Use of NLP for Building Educational Applications, pages 136?146,
Montre?al, Canada, June 3-8, 2012. c?2012 Association for Computational Linguistics
Generating Diagnostic Multiple Choice Comprehension Cloze Questions 
 
 
Jack Mostow and Hyeju Jang 
Project LISTEN (www.cs.cmu.edu/~listen) 
Carnegie Mellon University 
RI-NSH 4103, 5000 Forbes Avenue 
Pittsburgh, PA 15213-3890, USA 
mostow@cs.cmu.edu, hyejuj@cs.cmu.edu 
 
 
 
 
 
 
Abstract 
This paper describes and evaluates DQGen, 
which automatically generates multiple choice 
cloze questions to test a child?s comprehen-
sion while reading a given text.  Unlike previ-
ous methods, it generates different types of 
distracters designed to diagnose different 
types of comprehension failure, and tests 
comprehension not only of an individual sen-
tence but of the context that precedes it.  We 
evaluate the quality of the overall questions 
and the individual distracters, according to 8 
human judges blind to the correct answers and 
intended distracter types.  The results, errors, 
and judges? comments reveal limitations and 
suggest how to address some of them. 
1 Introduction 
This paper presents an automated method to check 
a reader?s comprehension of a given text while 
reading it, and to diagnose comprehension failures.  
In contrast to testing reading comprehension skill, 
for which there are published tests with well-
established psychometric properties (e.g., 
Wiederholt & Bryant, 1992; Woodcock, 1998), 
testing comprehension during reading of a given 
text requires generating a test for that specific text. 
A widely used solution is to replace some of the 
words with blanks for the student to fill, typically 
by selecting from multiple candidates.  Such mul-
tiple choice fill-in-the-blank questions are called 
cloze questions.  They are trivial to score because 
the correct answer is simply the original text word. 
Cloze questions test the ability to decide which 
word is consistent with the surrounding context.  
Thus it taps the comprehension processes that 
judge various types of consistency, such as syntac-
tic, semantic, and inter-sentential. 
In a nutshell, these processes successively en-
code sentences, integrate them into an overall rep-
resentation of meaning, notice gaps and 
inconsistencies, and repair them (see, e.g., Kintsch, 
1993, 2005; van den Broek, Everson, Virtue, Sung, 
& Tzeng, 2002).  The reader?s resulting situation 
model represents ?the content or microworld that 
the text is about? (Graesser & Bertus, 1998). 
In this paper, we introduce DQGen (Diagnostic 
Question Generator), a system that uses natural 
language processing to generate diagnostic cloze 
questions that check the comprehension of some-
one reading a given text.  DQGen differs from pre-
vious methods for generating cloze questions in 
that it is designed to minimize disruption to the 
reading process, and to diagnose different types of 
comprehension failure. 
The intended application context that motivated 
the development of DQGen is an automated read-
ing tutor that listens to children read aloud and 
helps them build their oral reading fluency 
(Mostow, 2008).  Periodic comprehension checks 
should deter children from reading as fast as they 
can and ignoring what the text means.  When the 
child answers incorrectly, the wrong answers 
should provide clues to why they are wrong. 
136
The rest of this article is organized as follows.  
Section 2 describes the generated questions.  Sec-
tion 3 describes how DQGen generates distracters.  
Section 4 reports a pilot evaluation of it.  Section 5 
analyzes errors.  Section 6 relates DQGen to prior 
work.  Section 7 concludes. 
2 Form of Generated Cloze Questions 
Generating cloze questions requires deciding:   
1. Which sentences to make cloze questions?   
2. Which words to delete from them? 
3. How many distracters to provide for them? 
4. What types of distracters? 
To illustrate the results of DQGen?s decisions, 
Figure 1 shows one of the better questions it gen-
erated: 
 
 
Figure 1.  An example of a generated question 
 
The four decisions enumerated above involve 
tradeoffs among preserving the flow of reading, 
encouraging comprehension, and assessing it accu-
rately.  As this example illustrates, DQGen inserts 
cloze questions as comprehension checks at the 
end of paragraphs, where there are natural breaks, 
in order to minimize disruption to the flow of read-
ing.  If the last sentence is shorter than four words 
or DQGen fails to find an acceptable distracter of 
each type, it simply leaves the last sentence un-
changed rather than turn it into a bad cloze ques-
tion. 
DQGen deletes the last word of the sentence, in 
order to allow normal reading up till that point and 
thereby minimize disruption to the flow of reading.  
Deleting a word earlier in the sentence would force 
the reader to skip the deleted word and read ahead 
to answer the cloze question.  Indeed, a review of  
of comprehension assessments (Pearson & Hamm, 
2005) indicates that end-of-sentence multiple 
choice cloze questions are widely used:  ?Delete 
words at the end of sentences and provide a set of 
choices from which examinees are to pick the best 
answer (this tack is employed in several standard-
ized tests, including the Stanford Diagnostic Read-
ing Test and the Degrees of Reading Power).? 
The number of distracters involves a tradeoff.  
On the one hand, the more distracters, the less 
chance of lucky guesses, and the more types of 
distracters possible.  On the other hand, offering 
more distracters lengthens the disruption to the 
flow of reading and raises the cognitive load on the 
reader to remember the paragraph when reading 
the distracters.  As a compromise, DQGen adds 
three distracters, for a total of four choices to pre-
sent in randomized order ? typical for multiple 
choice questions on educational tests for children. 
DQGen uses three types of distracters.  Each 
type of distracter indicates a different type of com-
prehension failure when chosen incorrectly by the 
reader as the answer.  By aggregating children?s 
performance over questions with these same three 
types of distracters, we hope not only to test their 
comprehension, but to profile the difficulties en-
countered by a given child or posed by a given text. 
2.1 Ungrammatical distracters 
The first and presumably easiest type of distracter 
renders the completed sentence ungrammatical.  
Syntactic processing is part of comprehension but 
not necessarily well-developed in children. Analy-
sis of children's responses to 69,000 multiple cloze 
questions automatically generated, presented, and 
scored by the Reading Tutor (Mostow et al, 2004) 
found that children?s performance decreased as the 
number of distracters with the same part of speech 
as the correct answer increased.  However, this 
effect was weaker for lower-level readers, indicat-
ing less sensitivity to syntax (Hensler & Beck, 
2006).  Choosing an ungrammatical distracter indi-
cates failure to detect a syntactic inconsistency.  
The ungrammatical distracter, e.g., are in Figure 1, 
has a different part of speech (POS) than the cor-
rect answer germs.  
2.2 Nonsensical distracters 
The second type of distracter makes the completed 
sentence grammatical but nonsensical.  Choosing a 
nonsensical distracter indicates failure to detect a 
local semantic inconsistency with the rest of the 
sentence.  The nonsensical distracter has the same  
Some of those cells patrol your body.  They are 
hungry, and they eat germs! Some stop the 
trouble germs make.  Others make antibodies.  
They stick to germs.  That helps your body find 
and kill _____  . 
a) are  
b) intestines  
c) terrorists  
d) germs 
 
137
   Ungrammatical Nonsensical Plausible 
Source of candidates 
Other words in 
paragraph 
List of words at 
grade level up to 4 
Matching Google N-grams 
Same as correct answer? No No No (94.96%) 
Related to words earlier in paragraph? ? ? No (lowest score) 
Related to words earlier in sentence? ? ? Yes (55.77%) 
Contains a space? ?No No (100%) ?No 
Frequent enough for children to know? ?Yes ?Yes Yes (96.15%) 
Passes grammar checker? No (65.48%) Yes (52.62%) Yes (92.31%)* 
Same POS as answer? ?No Yes (26.67%) ? 
Matches a Google N-gram? No (95.83%) No (91.67%) ?Yes 
 
Table 1.  Sources and constraints for each distracter type, in order tested (with % satisfied in pilot data).   
Constraints guaranteed to be satisfied or violated without explicit testing are marked ?Yes or ?No. 
* We added this test after the pilot evaluation because Google N-grams aren?t always grammatical.
POS as the correct answer, but plugging it into the 
sentence forms a context not found in the Google 
N-grams corpus. For example, the nonsensical dis-
tracter in Figure 1 is intestines.    
2.3 Plausible distracters 
The third and hardest type of distracter makes the 
completed sentence meaningful in isolation but 
inconsistent with the preceding global context.  
This type of distracter is essential in testing inter-
sentential processing, i.e. ?understanding that 
reaches across sentences in a passage,? because  
otherwise ?an individual's ability to fill in cloze 
blanks does not depend on passage context? ? a 
frequent criticism of cloze questions (Pearson & 
Hamm, 2005).  A plausible distracter has the same 
POS as the correct answer, like a nonsensical dis-
tracter, but the sentence it forms when plugged into 
the blank sounds reasonable ? in isolation.  That is, 
it ends with an N-gram that occurs in the Google 
N-grams corpus.   However, it doesn?t make sense 
in the context of the preceding sentences, because 
the distracter is unrelated to the words in the pre-
ceding sentences.  For example, terrorists in Fig-
ure 1 is a plausible distracter. 
3 Generating and Filtering Distracters 
DQGen uses generate-and-test to construct each 
type of distracter:  it chooses randomly from a 
source of candidates and backtracks if the chosen 
candidate violates a constraint on that type of dis-
tracter.  If none of the candidates that satisfy a con-
straint survive subsequent tests, DQGen drops the 
constraint and considers candidates that violate it. 
The source and constraints vary by distracter type 
(ungrammatical, nonsensical, plausible).  Table 1 
summarizes the tests and the order they are applied.  
Sections 3.1-3.3 discuss them in further detail.  
3.1 Lexical constraints on distracters 
Three constraints apply at the word level. 
No spaces: We constrain all three types of dis-
tracters to be words rather than phrases.  This con-
straint is guaranteed for paragraph words and 
Google N-grams, DQGen?s respective sources of 
ungrammatical and plausible distracters.  However, 
our source of nonsensical distracters is a table 
(Biemiller, 2009) that specifies the grade level not 
only of words but also of some phrases, such as 
barbeque sauce, which DQGen therefore filters out.  
Table 2 shows an excerpt from the table used. 
 
Word Meaning Level ? 
barbecue sauce flavored sauce for meat 2  
intestines guts  4  
intimate close, friendly 10  
intimate a close friend 10  
Table 2. Excerpt from Biemiller's (2009) table 
 
Distinct: DQGen explicitly excludes the correct 
answer as a distracter.  Other constraints on differ-
ent types of distracters are mutually exclusive with 
138
each other.  Consequently, no answer choice can 
appear twice. 
Familiar: Distracters must be familiar to chil-
dren.  DQGen satisfies this constraint for ungram-
matical and nonsensical distracters by choosing 
them from the paragraph and a grade-leveled word 
list (Biemiller, 2009), respectively.  These sources 
suffice to provide candidates, but they are not 
comprehensive enough to test candidates from an-
other source, such as the Google N-grams used to 
generate plausible distracters.  To exclude words 
likely to be unfamiliar to children, DQGen filters 
out candidates whose unigram frequency falls be-
low 5,000,000.  We tuned this threshold by infor-
mal trial and error; higher thresholds proved too 
stringent to allow any distracters from the limited 
source of candidate plausible distracters. 
3.2 Constraints on completed sentences  
Three constraints pertain to making completed sen-
tences sensible or not. 
Grammatical: As Table 1 shows, all three types 
of distracters involve grammaticality constraints.  
Ungrammatical distracters must make the complet-
ed sentence ungrammatical, e.g., That helps your 
body find and kill are.  In contrast, nonsensical and 
plausible distracters must make the completed sen-
tence grammatical, e.g., That helps your body find 
and kill terrorists.   
To check grammaticality of a completed sen-
tence, we use the Link Grammar Parser (Sleator & 
Temperley, 1993), a syntactic dependency parser, 
as a grammar checker.  As a grammar checker, the 
Link Grammar Parser usually accepts grammatical 
sentences and rejects ungrammatical ones, perhaps 
because sentences in children?s text tend to be 
short.  However, it sometimes fails to accept a 
grammatical sentence, as the last row of Table 3 
illustrates. 
 
sentence grammatically parser 
The germs hide in food or 
people 
correct accepted 
The germs hide in food or 
world 
incorrect rejected 
So keep dirty hands away 
from cuts and your face. 
correct rejected 
Table 3. Examples of grammar checking by parser 
 
Part of speech: More than one POS may make 
a distracter grammatical.  DQGen uses the Stan-
ford POS Tagger (Toutanova, Klein, Manning, & 
Singer, 2003) to tag the correct answer and a can-
didate nonsensical distracter when used to com-
plete the sentence, and requires them to have the 
same POS.  This test is superfluous for ungram-
matical distracters and unnecessary for plausible 
distracters. 
Google N-gram: As a heuristic test of whether a 
completed sentence is plausible, we check whether 
its ending occurs in the Google N-grams corpus 
(Brants & Franz, 2006), which means that it ap-
pears at least 40 times on the Web.  For ungram-
matical and nonsensical distracters, the last 4 
words of the completed sentence must not occur in 
this corpus.  For plausible distracters, the last 4 
words followed by ?.? must occur.  To enforce this 
constraint, DQGen?s source of candidate plausible 
distracters consists of Google 5-grams of the form 
W X Y __ .   Here W, X, and Y are the words pre-
ceding the correct answer in the original sentence, 
e.g., find and kill.  If there are fewer than 5 such 5-
grams, DQGen allows 4-grams of the form X Y __ ., 
e.g. and kill __. 
3.3 Relevance to context 
Two constraints on distracters concern context. 
Irrelevant to words earlier in paragraph: A 
plausible distracter should not be too plausible, so 
DQGen tries to ensure that it is unrelated to the 
earlier sentences and hence unlikely to make sense 
in context.  We measure the relatedness of a dis-
tracter to words in the earlier sentences by how 
often it co-occurs with them when used as in the 
last sentence.  DQGen therefore first pairs the can-
didate distracter, e.g. terrorists, with the last con-
tent word preceding the blank, e.g., kill in That 
helps your body find and kill ____.   It then esti-
mates the probability of these two words (kill and 
terrorists) co-occurring with the words in the earli-
er sentences of the paragraph, using a Na?ve Bayes 
formula to score their relevance to that context: 
1
Pr( , | ) Pr( , ) Pr( | , )
n
i
i
c k w c k w c k
=
? ??  
The formula omits Pr( )w
?
because it?s the same for 
all candidate plausible distracters for a given ques-
tion.  Here c is a candidate distracter (e.g., terror-
ists), k is the last content word before the blank 
(e.g., kill), w
?
is a vector of the n content words 
earlier in the paragraph, and wi is the i
th such word.   
139
 
Figure 2.  Prompt for the pilot user test 
 
DQGen scores Pr( | , )iw c k  based on how often 
word wi co-occurs with words c and k in the same 
30-word window in the British National Corpus 
(BNC). 
The purpose of a plausible distracter is to detect 
failures of intersentential comprehension processes 
that monitor global consistency.  As a heuristic to 
violate global consistency, DQGen picks distract-
ers with the lowest relevance scores. 
Relevant to words earlier in sentence: A plau-
sible distracter should be relevant to the words ear-
lier in the sentence.  To score local relevance, 
DQGen uses a Na?ve Bayes formula similar to its 
formula for global relevance: 
1
Pr( | ) Pr( ) Pr( | )
n
i
i
c w c w c
=
? ??  
Here, c is a candidate distracter, w
?
is a vector of 
the n content words earlier in the sentence, and wi 
is the ith such word.  DQGen estimates Pr( | )iw c  in 
the same way as before, but omits k because n is so 
much smaller for the sentence than for the para-
graph context preceding it.  DQGen averages these 
local coherence scores over the candidates, and 
allows only candidates whose local coherence 
scores are above the mean.   
4 Pilot Study 
How good are the generated questions? To evalu-
ate DQGen, we asked human judges to score them.  
Section 4.1 explains how we evaluated questions, 
Section 4.2 reports inter-rater reliability, and Sec-
tion 4.3 presents results. 
4.1 Methodology 
For the evaluation, we used DQGen to insert sam-
ple questions in an informational text for children, 
The Germs, which explains the concept of germs 
and their danger.  Of the 18 paragraphs in this text, 
we rejected one because it was only two sentences 
long, and DQGen rejected another because the last 
sentence failed the grammar checker.  For each of 
the other 16 paragraphs, DQGen generated a cloze 
question with ungrammatical and nonsensical dis-
tracters, but it found plausible distracters for only 
13 of the questions, which we evaluated as follows. 
We recruited eight human judges, members of 
our research team but unfamiliar with DQGen.  We 
asked them to evaluate each question at two levels, 
using the form illustrated in Figure 2.  
At the high level, we evaluated the overall quali-
ty of each question by asking judges to rate it as 
140
Good, OK, or Bad.  We report the percentage of 
generated questions rated by human judges as ac-
ceptable, defined as Good or OK.  We used a 3-
point scale rather than a finer-grained scale both to 
get higher inter-rater reliability, and because we 
were interested more in how many of the questions 
were acceptable than in precise ratings of quality.   
At the low level, we evaluated how often 
DQGen generated the intended type of distracter.  
We asked the judges to categorize each of the mul-
tiple choices (correct answer plus 3 distracters) as 
Ungrammatical, Nonsensical but grammatical, 
Meaningful but incorrect given the preceding text, 
or Correct.  To avoid biasing their responses, we 
did not tell them that each question was supposed 
to have one choice in each category. 
To elicit additional feedback, the form invited 
judges to comment on the questions and distracters. 
4.2 Inter-rater reliability 
It is important to measure inter-rater reliability 
among human judges, especially on experimenter-
designed measures such as the form we used. 
The overall quality ratings involved ranked data 
from more than two judges, so to measure their 
inter-rater reliability we used Kendall?s Coefficient 
of Concordance (Kendall & Smith, 1939).  KCC 
for overall quality was .40 on a scale from 0 to 1.  
This low value reflects the considerable variation 
between the judges, whose average ratings of over-
all quality ranged from 1.3 to 2.6. 
Categorization of each answer choice involved 
unranked data from more than two judges, so we 
used Fleiss? Kappa (Shrout & Fleiss, 1979) to 
measure its inter-rater reliability.  Kappa was .58;   
a value of .4-.6 is considered moderate, .6-.8 sub-
stantial, and .8-1 outstanding (Landis & Koch, 
1977).  Figure 3 shows the Kappa values for each 
label by the judges. 
 
 
Figure 3.  Fleiss's Kappa for inter-rater reliability 
of each type of choice 
The low values of inter-rater reliability measures 
revealed the raters? lack of consensus, presumably 
due to differing interpretations of the instructions.  
For instance, one judge commented that instruction 
for rating the overall quality did not indicate 
whether a good question requires reading the pre-
ceding text.  Another issue was missing and multi-
ple categorical responses.   
Evidently we need to specify our rating criteria 
more clearly, both for overall quality and for indi-
vidual components, especially nonsensical and 
plausible distracters.  A worked-out example might 
help judges understand each type better, but must 
avoid phrasing biased toward how DQGen works. 
4.3 Results 
We computed average ratings of overall quality 
and agreement with the intended category of each 
answer choice.   
We averaged all the ratings of overall quality af-
ter converting Bad, OK, and Good ratings into 1, 2, 
and 3, respectively.  Overall quality ratings aver-
aged 2.04, which corresponds to OK.  For agree-
ment of judges with the intended category of each 
answer choice, Cohen?s Kappa was .60.  Note that 
in contrast to Section 4.2, where we used Kappa to 
measure inter-rater reliability, i.e., how well the 
judges agreed with each other on overall question 
quality, here we use Kappa to measure distracter 
quality, i.e., how well the judges agreed with 
DQGen on the intended type of answer choices. 
Individual judges ranged from 63% to 79% 
agreement with the intended answer (Cohen's 
Kappa .51 to .72).  As Figure 4 shows, agreement 
was stronger for correct answers and ungrammati-
cal distracters than for nonsensical and plausible 
distracters.  On average, judges rated 94% of the 
correct answers as correct and agreed with 
DQGen?s intended distracter type for 91% of the 
ungrammatical distracters, 63% of the nonsensical 
distracters, and only 32% of the plausible distract-
ers.  Apparently correct answers are obviously 
right and ungrammatical answers are obviously 
wrong, but nonsensical and plausible distracters 
are harder to classify. 
5 Analysis of errors 
We now discuss issues revealed by errors and 
judges? comments, and how to address them. 
141
 Figure 4.  Cohen?s Kappa for agreement with the 
intended type of each choice 
5.1 Dependence on preceding text 
The judges? most frequent comment about the 
quality of a question was that answering it did not 
require reading the preceding text.  The judges rat-
ed only 32% of the intended plausible distracters as 
plausible.  Evidently we need to identify further 
constraints on plausible distracters.  We may also 
need to identify constraints on sentences where 
plausible distracters exist for the correct answer. 
5.2 Idioms 
Answer choices, whether correct answers or dis-
tracters, are problematic when they form idioms 
such as twisted in knots or make do.  For instance, 
one pilot cloze question ended with twisted in ____, 
where the correct answer was knots.  Another 
question ended with get your body to make ____, 
with do as a supposedly ungrammatical distracter. 
Idioms pose multiple problems, although we 
found only two cases in our small pilot study.  First, 
we want to test comprehension of the paragraph, 
not just knowledge of specific idioms.  Second, the 
word that completes an idiom can be far likelier 
than any other choice, making it too easy to guess 
based solely on local context, whether correct or 
not.  Third, because idioms have non-componential 
semantics, the missing word is liable to be seman-
tically unrelated to other sentence words, causing 
DQGen to badly underestimate its local relevance. 
Detecting idioms automatically is a research 
problem in its own right (Li, Roth, & Sporleder, 
2010; Li & Sporleder, 2009).  We might be able to 
recognize idioms by using the fact that its N-gram 
frequency is much higher than expected based on 
the frequency of its individual words.  A simpler 
approach is to consult a dictionary of common 
phrases.  Either approach would require extension 
to handle parameterized idioms such as a chip on 
[someone?s] shoulder, or non-contiguous forms 
such as Actions do in fact speak louder than words. 
5.3 Lexical issues for distracters 
The pilot study exposed a number of issues affect-
ing the suitability of words as distracters. 
Same-root words 
DQGen ensures that answer choices are distinct.  
However, one question included two forms of the 
same word as choices, namely throats as the cor-
rect answer and throat as a plausible distracter.  
We need to ensure that answer choices are not only 
distinct but dissimilar, unless we want questions 
that focus on minor differences between them. 
Common verbs and modal verbs 
One judge commented that we might want to avoid 
common verbs as distracters, such as any form of 
be, do, have, and get, and modal verbs, such as can, 
cannot, and will, lest children notice that they are 
seldom the correct answer, and therefore eliminate 
them without considering them.  Accordingly, we 
plan to filter out common verbs and modal verbs. 
Word difficulty 
The same judge considered some words too diffi-
cult for children, such as gauge and roast.  Actual-
ly, Biemiller (2009) rates noun senses of these 
words at grade 2, but the verb sense of gauge as 
estimate at grade 10.  These examples illustrate a 
limitation of DQGen?s methods to pick familiar 
words as distracters.  It picks ungrammatical dis-
tracters from the words in the paragraph, nonsensi-
cal distracters from Biemiller?s word list, and 
plausible distracters from Google N-grams, filtered 
by unigram frequency to avoid rare words.  In all 
three cases, DQGen constrains words rather than 
word senses. 
A more sophisticated approach would determine 
a distracter?s word sense, or at least POS, when 
used to complete the sentence, and rate the famili-
arity of its specific sense or POS.  Tagging the dis-
tracter POS is easier than determining its word 
sense(s) when inserted in the sentence.  Rating the 
familiarity of different word senses would require 
either a grade-leveled list of them like Biemiller?s 
(2009), or a resource with information about the 
frequency of different word senses or POS. 
142
6 Relation to Prior Work 
How does this research relate to previous work? 
There has been considerable research on automatic 
generation of multiple choice cloze questions to 
test vocabulary, grammar, and comprehension.  
Although these types of questions differ in purpose, 
they have much in common when it comes to gen-
erating them automatically.  
6.1 Vocabulary and grammar cloze questions 
A multiple choice cloze question to test vocabulary 
and grammar is constructed from a sentence se-
lected from a corpus by deleting part of it (typical-
ly the target vocabulary word) and selecting 
distracters for it. 
Selecting distracters with the same POS and ap-
proximate frequency as the answer word is a com-
mon strategy (Brown, Frishkoff, & Eskenazi, 
2005; Coniam, 1997; Liu, Wang, & Gao, 2005). 
Besides matching the correct answer?s POS and 
frequency, Liu et al (2005) added a culture-
dependent strategy for generating distracters:  
choose English words with semantically similar 
translations in the learner?s native language to the 
translation of the answer word. 
Correia et al (2010) generated vocabulary ques-
tions for Portuguese with three types of distracters.  
One type of distracter had the same POS and word 
level as the target word, based on its unigram fre-
quency in Portuguese textbooks used in different 
grades.  A second type had the lowest Levenshtein 
distance to the target out of all words with its POS.  
A third type was misspellings of the target word 
using a table of common spelling mistakes.  Al-
dabe et al (2007) also included students? common 
mistakes as candidate distracters.  
Some work also used semantic similarity 
between a distracter and the answer word to choose 
distracters.  Pino et al (2008) selected distracters 
that made the completed sentence grammatical and 
tended to co-occur with the words in the sentence, 
but were semantically distant from the target word 
as measured by WordNet.  In constrast, Smith et al 
(2008) looked for distracters semantically similar 
to the answer word based on distributional simi-
larity. In addition, Sumita et al (2005) used a the-
saurus for the same purpose, and then consulted 
the web to filter out plausible distracters.  
Aldabe et al (2009) considered context in a 
question sentence when choosing distracters.  They 
used an n-gram language model to predict the 
probability of occurrence of a distracter with its 
preceding words. 
Gates et al (2011) generated phrase-type dis-
tracters, unlike other work.  They generated ques-
tions from a dictionary definition of the target 
vocabulary word.  Rather than delete the target 
word, they parsed the definition, deleted a phrase 
from it, and chose distracters with the same syntac-
tic phrase type from definitions of other words, 
filtered to exclude synonyms of the target word. 
6.2 Comprehension cloze questions 
In contrast to vocabulary and grammar questions 
constructed from isolated sentences, DQGen?s 
comprehension questions are for (and inserted into) 
connected text. 
The most closely related work was by Mostow 
et al (2004).  Their Reading Tutor dynamically 
generated multiple choice cloze questions to test 
children?s comprehension of randomly chosen sen-
tences while reading a story.  It randomly chose an 
approximate level of difficulty (?sight?, ?easy?, 
?hard?, and ?defined?) for which word to delete 
from the sentence, and which words to choose ran-
domly from the same story as distracters. 
Goto et al (2010) also generated questions from 
texts.  They used a training corpus of existing cloze 
questions to learn how to select sentences to turn 
into cloze questions, words to delete, and types of 
distracters distinguished by their relation to the 
answer word:  inflectional (e.g., ask ? asked); der-
ivational (e.g., work ? worker); orthographic (e.g., 
circulation ? circumcision); and semantic (e.g., 
synonyms and antonyms). 
Aldabe et al (2010) generated questions for 
learners? assessment in the science domain.  To 
generate distracters, they measured semantic simi-
larity by using Latent Semantic Analysis (LSA) 
and additional information such as semantic rela-
tionships between words.  Experts discarded dis-
tracters that could form a correct answer. 
DQGen differs from prior work on generating 
cloze questions for vocabulary and comprehension 
in two key respects.  First, each question it gener-
ates has multiple types of distracters designed to 
detect different types of comprehension failure.  
Second, to generate plausible distracters it consid-
ers their relation not only to the clozed sentence 
but to the entire paragraph that contains it. 
143
7 Conclusion 
We conclude by summarizing contributions, limi-
tations, and future work. 
7.1 Contributions 
This paper describes a method for generating mul-
tiple choice cloze questions to test students? com-
prehension while reading.  Unlike previous 
methods, some of which also generate multiple 
types of distracters, DQGen?s distracter types are 
diagnostic.  It generates ungrammatical, nonsensi-
cal, and plausible distracters in order to detect fail-
ures of syntactic, semantic, and intersentential 
processing, respectively.  Unlike prior methods, 
which test comprehension only of individual sen-
tences, DQGen?s plausible distracters take their 
preceding context into account. 
We observed that candidate plausible distracters 
with high relevance scores tend to be surprisingly 
sensible answers ? even though the formula 
doesn?t ?know? the correct answer or even the un-
grammatical and nonsensical distracters.  That is, 
grammaticality, N-grams, and a simple relevance 
measure often suffice to produce intelligent an-
swers to a cloze question despite their shallow rep-
resentation of the meaning of the paragraph ? that 
is, without really understanding it.  This finding is 
surprising insofar as one would expect good per-
formance on such questions to require a deep rep-
resentation such as the situation model constructed 
by human readers. 
7.2 Limitations 
Besides describing DQGen?s design and imple-
mentation, we report on an evaluation of 13 gener-
ated questions by eight human judges blind to 
correct answer and intended distracter type.  On 
average they rated overall question quality OK, but 
with a wide range from the least to most favorable 
judge.  They agreed well with DGQen in classify-
ing answers as ungrammatical or correct, but not as 
nonsensical or plausible.  They criticized many 
questions as answerable without reading the text. 
7.3 Future work 
Our analysis of errors and judges? comments re-
vealed several limitations and suggested ways to 
address some of them.  In addition to identifying 
further constraints on plausible distracters, we need 
to identify constraints on good sentences to turn 
into end-of-paragraph cloze questions, beyond just 
the ability to generate a distracter of each type.  
One criterion is reliability:  how well does perfor-
mance on a question correlate with performance on 
other questions about the same text?  Another cri-
terion is informativeness:  what do wrong answers 
reveal about comprehension? 
Besides improving DQGen, we need to test it on 
more stories (both narrative fiction and informa-
tional text) and readers (especially children, our 
target population) to expose additional problems 
and avoid overfitting their solutions. 
One possible use of DQGen is machine-assisted 
generation of comprehension questions, or more 
precisely, human-assisted machine generation, for 
example with the human vetting or selecting 
among candidate questions generated automatical-
ly, thereby reducing the amount of human effort 
currently required to compose comprehension 
questions, and producing them more systematically. 
Success in getting DQGen to produce cloze 
questions on a large scale would have useful appli-
cations.  Periodic comprehension checks should 
deter children from reading as fast as they can and 
ignoring what the text means.  Diagnostic feedback 
based on incorrect answers should shed light on the 
nature of their comprehension failures and may be 
valuable as feedback to teachers or as guidance to 
the reading tutor. 
Another use for large numbers of automatically 
generated cloze questions is to develop methods to 
monitor reading comprehension unobtrusively.  
Student responses to cloze questions could provide 
automated labels for data collected while they read 
the preceding text.   Such data could include oral 
reading (Zhang, Mostow, & Beck, 2007) or even 
EEG (Mostow, Chang, & Nelson, 2011).  Models 
trained and tested on the labeled data could esti-
mate reading comprehension based on unlabeled 
data ? that is, without interrupting to ask questions. 
Acknowledgements 
The research reported here was supported by the Insti-
tute of Education Sciences, U.S. Department of Educa-
tion, through Grant R305A080157.  The opinions 
expressed are those of the authors and do not necessari-
ly represent the views of the Institute or the U.S. De-
partment of Education.  We thank our colleagues who 
judged the generated questions, and the reviewers for 
their helpful comments. 
144
References  
Aldabe, I., & Maritxalar, M. (2010). Automatic 
Distractor Generation for Domain Specific Texts 
Advances in Natural Language Processing. Paper 
presented at the The 7th International Conference on 
NLP, Reykjavk, Iceland. 
Aldabe, I., Maritxalar, M., & Martinez, E. (2007). 
Evaluating and Improving Distractor-Generating 
Heuristics. Paper presented at the The Workshop on 
NLP for Educational Resources. In conjuction with 
RANLP07. 
Aldabe, I., Maritxalar, M., & Mitkov, R. (2009). A 
Study on the Automatic Selection of Candidate 
Sentences and Distractors. Paper presented at the 
Proceedings of the 14th International Conference on 
Artificial Intelligence in Education (AIED2009), 
Brighton, UK. 
Biemiller, A. (2009). Words Worth Teaching:  Closing 
the Vocabulary Gap. Columbus, OH: SRA/McGraw-
Hill. 
Brants, T., & Franz, A. (2006). Web IT 5-gram Version 
1. Philadelpha: Linguistic Data Consortium. 
Brown, J. C., Frishkoff, G. A., & Eskenazi, M. (2005). 
Automatic Question Generation for Vocabulary 
Assessment. Paper presented at the Proceedings of 
Human Language Technology Conference and 
Conference on Empirical Methods in Natural 
Language Processing (HLT/EMNLP), Vancouver.  
Coniam, D. (1997). A preliminary inquiry into using 
corpus word frequency data in the automatic 
generation of English language cloze tests. CALICO 
Journal, 14(2-4), 15-33.  
Correia, R., Baptista, J., Mamede, N., Trancoso, I., & 
Eskenazi, M. (2010, September 22-24). Automatic 
generation of cloze question distractors. Paper 
presented at the Proceedings of the Interspeech 2010 
Satellite Workshop on Second Language Studies: 
Acquisition, Learning, Education and Technology, 
Waseda University, Tokyo, Japan. 
Gates, D., Aist, G., Mostow, J., Mckeown, M., & Bey, J. 
(2011, November 4-6). How to Generate Cloze 
Questions from Definitions: a Syntactic Approach. 
Paper presented at the Proceedings of the AAAI 
Symposium on Question Generation, Arlington, VA. 
Goto, T., Kojiri, T., Watanabe, T., Iwata, T., & Yamada, 
T. (2010). Automatic Generation System of Multiple-
Choice Cloze Questions and its Evaluation. 
Knowledge Management & E-Learning: An 
International Journal (KM&EL), 2(3).  
Graesser, A. C., & Bertus, E. L. (1998). The 
Construction of Causal Inferences While Reading 
Expository Texts on Science and Technology. 
Scientific Studies of Reading, 2(3), 247-269.  
Hensler, B. S., & Beck, J. (2006, June 26-30). Better 
student assessing by finding difficulty factors in a 
fully automated comprehension measure [Best Paper 
nominee]. Paper presented at the Proceedings of the 
8th International Conference on Intelligent Tutoring 
Systems, Jhongli, Taiwan. 
Kendall, M. G., & Smith, B. B. (1939). The Problem of 
m Rankings. The Annals of Mathematical Statistics, 
10(3), 275-287.  
Kintsch, W. (1993). Information Accretion and 
Reduction in Text Processing: Inferences. Discourse 
Processes, 16(1-2), 193-202.  
Kintsch, W. (2005). An Overview of Top-Down and 
Bottom-Up Effects in Comprehension: The CI 
Perspective. Discourse Processes A Multidisciplinary 
Journal, 39(2&3), 125-128.  
Landis, J. R., & Koch, G. G. (1977). The measurement 
of observer agreement for categorical data. 
Biometrics, 33(1), 159-174.  
Li, L., Roth, B., & Sporleder, C. (2010). Topic models 
for word sense disambiguation and token-based 
idiom detection. Paper presented at the Proceedings 
of the 48th Annual Meeting of the Association for 
Computational Linguistics, Uppsala, Sweden.  
Li, L., & Sporleder, C. (2009). Classifier combination 
for contextual idiom detection without labelled data. 
Paper presented at the Proceedings of the 2009 
Conference on Empirical Methods in Natural 
Language Processing, Singapore.  
Liu, C.-L., Wang, C.-H., & Gao, Z.-M. (2005). Using 
Lexical Constraints to Enhance the Quality of 
Computer-Generated Multiple-Choice Cloze Items. 
Computational Linguistics and Chinese Language 
Processing, 10(3), 303-328.  
Liu, C.-L., Wang, C.-H., Gao, Z.-M., & Huang, S.-M. 
(2005). Applications of lexical information for 
algorithmically composing multiple-choice cloze 
items. Paper presented at the Proceedings of the 
second workshop on Building Educational 
Applications Using NLP, Ann Arbor, Michigan.  
Mostow, J. (2008). Experience from a Reading Tutor 
that listens:  Evaluation purposes, excuses, and 
methods. In C. K. Kinzer & L. Verhoeven (Eds.), 
Interactive literacy education:  facilitating literacy 
environments through technology (pp. 117-148). New 
York: Lawrence Erlbaum Associates, Taylor & 
Francis Group. 
Mostow, J., Beck, J. E., Bey, J., Cuneo, A., Sison, J., 
Tobin, B., & Valeri, J. (2004). Using automated 
questions to assess reading comprehension, 
vocabulary, and effects of tutorial interventions. 
Technology, Instruction, Cognition and Learning, 
2(1-2), 97-134.  
Mostow, J., Chang, K.-m., & Nelson, J. (2011, June 28 - 
July 2). Toward Exploiting EEG Input in a Reading 
Tutor [Best Paper Nominee]. Paper presented at the 
Proceedings of the 15th International Conference on 
Artificial Intelligence in Education, Auckland, NZ. 
145
Pearson, P. D., & Hamm, D. N. (2005). The history of 
reading comprehension assessment. S. G. Paris & S. 
A. Stahl (Eds.), Children's reading comprehension 
and assessment, 13-69.  
Pino, J., Heilman, M., & Eskenazi, M. (2008). A 
selection strategy to improve cloze question quality. 
Paper presented at the Proceedings of the Workshop 
on Intelligent Tutoring Systems for Ill-Defined 
Domains. 9th International Conference on Intelligent 
Tutoring Systems, Montreal, Canada. 
Shrout, P. E., & Fleiss, J. L. (1979). Intraclass 
correlations: Uses in assessing rater reliability. 
Psychological Bulletin, 86(2), 420-428.  
Sleator, D. D. K., & Temperley, D. (1993, August 10-
13). Parsing English with a link grammar. Paper 
presented at the Third International Workshop on 
Parsing Technologies, Tilburg, NL, and Durbuy, 
Belgium. 
Smith, S., Sommers, S., & Kilgarriff, A. (2008). 
Learning words right with the Sketch Engine and 
WebBootCat: Automatic cloze generation from 
corpora and the web. Paper presented at the 
Proceedings of the 25th International Conference of 
English Teaching and Learning & 2008 International 
Conference on English Instruction  and Assessment, 
Lisbon, Portugal.  
Sumita, E., Sugaya, F., & Yamamoto, S. (2005). 
Measuring non-native speakers' proficiency of 
English by using a test with automatically-generated 
fill-in-the-blank questions. Paper presented at the 
Proceedings of the second workshop on Building 
Educational Applications Using NLP, Ann Arbor, 
Michigan.  
Toutanova, K., Klein, D., Manning, C., & Singer, Y. 
(2003). Feature-rich part-ofspeech tagging with a 
cyclic dependency network. Paper presented at the 
HLT-NAACL, Edmonton, Canada.  
van den Broek, P., Everson, M., Virtue, S., Sung, Y., & 
Tzeng, Y. (2002). Comprehension and memory of 
science texts: Inferential processes and the 
construction of a mental representation. In J. L. J. 
Otero, & A. C. Graesser (Ed.), The psychology of 
science text comprehension. Mahwah, NJ: Erlbaum. 
Wiederholt, J. L., & Bryant, B. R. (1992). Gray Oral 
Reading Tests (3rd ed.). Austin, TX: Pro-Ed. 
Woodcock, R. W. (1998). Woodcock Reading Mastery 
Tests - Revised (WRMT-R/NU). Circle Pines, 
Minnesota: American Guidance Service. 
Zhang, X., Mostow, J., & Beck, J. E. (2007, July 9-13). 
Can a computer listen for fluctuations in reading 
comprehension? Paper presented at the Proceedings 
of the 13th International Conference on Artificial 
Intelligence in Education, Marina del Rey, CA. 
 
 
146
