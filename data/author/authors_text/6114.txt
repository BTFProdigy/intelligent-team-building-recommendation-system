A Support System for Revising Titles
to Stimulate the Lay Reader?s Interest in Technical Achievements
Yasuko Senda??, Yasusi Sinohara?, and Manabu Okumura?
?System Engineering Research Laboratory,
Central Research Institute of Electric Power Industry, Tokyo Japan
?Department of Computational Intelligence and Systems Science,
Tokyo Institute of Technology, Tokyo Japan
?Precision and Intelligence Laboratory, Tokyo Institute of Technology, Tokyo Japan
Abstract
When we write a report or an explanation
on a newly-developed technology for readers
including laypersons, it is very important to
compose a title that can stimulate their in-
terest in the technology. However, it is dif-
ficult for inexperienced authors to come up
with an appealing title.
In this research, we developed a support sys-
tem for revising titles. We call it ?title revi-
sion wizard?. The wizard provides a guid-
ance on revising draft title to compose a title
meeting three key points, and support tools
for coming up with and elaborating on com-
prehensible or appealing phrases.
In order to test the effect of our title revision
wizard, we conducted a questionnaire sur-
vey on the effect of the titles with or with-
out using the wizard on the interest of lay
readers. The survey showed that the wiz-
ard is effective and helpful for the authors
who cannot compose appealing titles for lay
readers by themselves.
1 Introduction
When we read a document, we usually read its
title first, and then we read the body text only
if the title catches our interest. Therefore, when
we write a report or an explanation on a newly-
developed technology intended for readers in-
cluding laypersons, it is very important to com-
pose a title that will stimulate their interest in
the technology. However, technical specialists
are not necessarily good at composing appeal-
ing titles, because it isn?t clear what sort of titles
will stimulate the interest of lay readers in the
technology.
In the field of NLP and linguistics, there are
few researches which help the specialists com-
pose appealing titles for lay readers. Several
researches have been reported on title genera-
tion (Jin and Hauptmann, 2000) (Berger and
Mittal, 2000) and readability of texts (Minel et
al., 1997) (Hartley and Sydes, 1997) (Inui et al,
2003). However, the researches on title gener-
ation focus on generating a very compact sum-
mary of the document rather than composing
an appealing title. The previous researches on
readability mainly see it as comprehensibility
rather than interestingness.
In this regard, our previous study (Senda and
Sinohara, 2002) clarified what sort of content
and wording in titles are effective in stimulat-
ing lay readers? interest in the technology by an
analysis of a parallel corpus of Japanese techni-
cal paper titles and Japanese newspapers head-
lines. The study categorized the effective con-
tent and wording of the titles into the following
three key points.
Key Point 1 (for Wording) Instead of tech-
nical terms, use synonymous plain terms
even where the plain term is not synony-
mous with the technical term in a precise
sense.
Key Point 2 (for Content) Describe what
the technology is for, rather than what
the technology does.
Key Point 3 (for Content) Describe the ad-
vantages of the technology, rather than
the method of realizing the technology.
Our next goal is to enable inexperienced au-
thors to compose a title according to these key
points. To this end, we developed a support
system for revising titles. We call it ?title revi-
sion wizard?. The wizard provides a guidance
on revising draft title to compose a title meet-
ing the key points, and a few support tools for
coming up with and elaborating on appealing
phrases. In this paper, we report on the title
revision wizard, and a questionnaire survey on
the effect of titles composed with and without
using the wizard on the interest of lay readers.
2 Method for Revising Titles
Intended for Lay Readers
It is difficult for inexperienced authors to change
their ?specialist-centered mind-set? and come
5VGR+PRWVUQOGRJTCUGUKPVQVJGVKVNGVGORNCVG
5VGR4GXKUGVJGRJTCUGUKP1$2UNQVU
5VGR4GXKUGVJGRJTCUGUKP122UNQVU
5VGR5GNGEVVJGVKVNGHTQORQUUKDNGVKVNGUIGPGTCVGFKPVJGCDQXGUVGRU
/GVJQFVQ5JQTVGPVJG4CFKQCEVKXG*CNHNKHGD[/GVCNNKE(WGN($4
=1DNKICVQT[2CTV
1$2? =1RVKQPCN2CTV
122?
/GVJQFVQ5JQTVGPVJG&WTCVKQPQH4CFKCVKQP
/GVJQFVQ5JQTVGPVJG5VQTCIG2GTKQFQH4CFKQCEVKXG9CUVGD[$WTPQWV
6GORNCVGQH1$2HQT-G[2QKPV
%JCPIGVGEJPKECNVGTOUVQU[PQP[OQWURNCKPVGTOU
/GVJQFVQ5JQTVGPVJG5VQTCIG2GTKQFQH4CFKQCEVKXG9CUVG
'ZRNCKPYJCVVJGVGEJPQNQI[KUHQTTCVJGTVJCPYJCVVJGVGEJPQNQI[FQGU
6GORNCVGQH1$2122HQT-G[2QKPV
%JCPIGVGEJPKECNVGTOUVQU[PQP[OQWURNCKPVGTOU
'ZRNCKPVJGCFXCPVCIGUQHVJGVGEJPQNQI[TCVJGTVJCPVJGOGVJQFQHTGCNK\KPIVJGVGEJPQNQI[
/GVJQFVQ5JQTVGPVJG5VQTCIG2GTKQFQH4CFKQCEVKXG9CUVGD[
6KVNG
6KVNG
6KVNG
6KVNG
6GORNCVGQH1$2HQT-G[2QKPV
6GORNCVGQH1$2122HQT-G[2QKPV

&TCHV8GTUKQP
9JKVGITQWPFKUUNQVHQTKPRWVVKPICPFTGXKUKPI
2CTCNNGN%QTRWUQH
6KVNGUCPF*GCFNKPGU
6GEJPKECN6GTOU
%JGEMGT
/GPW1RVKQPUQH
VJG2JTCUGU7UGFKP
0GYURCRGT*GCFNKPG
5WRRQTV6QQNU2TQXKFGF
KPVJG9K\CTF
Figure 1: Steps in the Procedure for Revising draft title Using the Wizard
up with appealing titles for lay readers even
when they know the three key points.
In our title revision wizard, therefore, the au-
thors first input draft title into the title tem-
plate, and then compose candidates of title by
revising the phrases of the draft title according
to the wizard?s guidance with the help of the
support tools provided.
The steps in the wizard?s procedure are il-
lustrated in Figure 1 1 . In the following, we
explain the steps with reference to Figure 1.
2.1 Inputting Some Phrases into the
Title Template
In step 1, the user inputs draft title into the
title template displayed in the wizard window.
The title template consists of an ?OBligatory
Part? (OB-P) and an ?OPtional Part? (OP-P).
The OB-P phrase describes what the technology
does, and OP-P phrase describes the method
used to implement the technology or the advan-
tages of the technology.
2.2 Revising the Obligatory Phrases
In step 2, the wizard presents only OB-P to the
user, and hides OP-P. The user revises only the
OB-P phrase according to the two key points.
First, the user changes technical terms to the
synonymous plain terms according to key point
1The sample titles presented in Figure 1 are trans-
lated into English from original Japanese titles.
1. In the template 2-1 in step 2, the technical
term ?the Radioactive Half-life? is changed to
the plainer term ?the Duration of Radiation?
from this viewpoint.
Secondly, the user changes the phrase in the
OB-P slots to describe the purpose of the tech-
nology rather than what the technology does,
according to key point 2. In the template 2-2
in step 2, the phrase ?Shorten the Radioactive
Half-life? is changed to the phrase ?Shorten the
Storage Period of Radioactive Waste? from this
view point.
These revised OB-P phrases without optional
phrase are recorded as candidates of ?simple?
title for future selection in step 4. At the end
of step 2, for future revision in step 3, the user
selects one title that he/she deems the better
one from these candidates of title.
2.3 Revising the Optional Phrases
In step 3, the wizard presents new title combin-
ing OB-P phrase (selected at the end of step 2)
and the OP-P phrase (inputted in step 1) as a
draft title. The user revises only the OP-P draft
phrase according to the two key points.
First, the user changes technical terms to syn-
onymous plain terms according to key point 1.
In the template 3-1 in step 3, the technical term
?Metallic Fuel FBR? in the slot is changed to
the plainer term ?Burnout? from this viewpoint.
Secondly, the user changes the phrase in the
slot to describe the advantages of the technology
rather than the method of realizing the technol-
ogy according to key point 3. In the template
3-2 in step 3, the OP-P phrase is changed to the
phrase ?by 1/10000? from this viewpoint.
The title combining the title selected in step
2 and each phrase revised at step 3 are recorded
as candidates of title for future selection in step
4. Before next step, the user can return to step
2 to select another OB-P phrase and revise OP-
P phrase attached to the OB-P phrase in step
3 again if he/she likes.
2.4 Select the Title from Title
Candidates
In step 4, the user selects one title from the
candidates of title composed in the above steps.
2.5 An Example of the Wizard Window
Figure 2 is a screenshot of the Wizard. This
screenshot shows the window in step 2 described
in section 2.2. The Japanese text in upper pane
of the window is the explanation of key point
1. The template for the OB-P Phrase for key
point 1 is displayed at the bottom of the win-
dow. The buttons in the center of the pane are
for accessing the support tools for coming up
with and elaborating the input phrase. Detail
of the support tools will be given in the next
section.
Title revision wizard is implemented in PHP
(Hypertext Preprocessor). Users can access the
wizard using a web browser such as Internet Ex-
plorer.
3 Support Tools provided in the
Wizard
In this section, we explain the three support
tools provided in the title revision wizard.
3.1 Database of Paper Titles and
Newspapers Headlines for Related
Technologies
It is difficult for inexperienced authors to come
up with a phrase meeting the three key points.
We therefore prepared a parallel corpus of titles
and headlines for related technologies in order
to provide clues for coming up with an appropri-
ate phrase. The database consists of about 420
titles and 440 headlines. They were categorized
into 150 groups on technology that has been de-
veloped in a research institute, and covers sci-
ence and technology in general. The database
is (b) in Figure 1.
Figure 3 shows an example of the database
window. The upper pane in the window shows
the phrases in titles and the headlines describ-
ing related technologies. The lower part of the
window presents search boxes for menu-based
retrieval and keyword-based retrieval. The pull-
down menu options are organized by technical
fields. Users can search the clues for revising the
title of the draft version from these search boxes
as well as by scrolling through the window.
The role of this parallel corpus is basi-
cally the same as the one of ?Example-Based
Translation Aid (EBTA)? (Sato, 1992) (Furu-
gori and Takeda, 1993) (Kumano and Tanaka,
1998). EBTA researches have shown that par-
allel translation examples (pairs of source text
and its translation equivalent) are very helpful
for translators to translate the similar text be-
cause parallel translation examples give them
useful clues for translation. From the viewpoint
that paper titles and newspapers headlines for
related technologies are also regarded as paral-
lel translation examples (pairs of text for spe-
cialists and its translation equivalent for lay
readers) describing newly-developed technolo-
gies, the our database is expected to be helpful
for the user of the wizard.
3.2 Technical Terms Checker
The author should use comprehensible term for
lay readers in order to compose the titles meet-
ing the key point 1. In order to avoid incompre-
hensible terms, it is important to identify diffi-
culty level of a term that authors want to use in
his/her title. We, therefore, prepared ?techni-
cal term checker? estimating the difficulty level
of a term.
It has been reported that human recognition
level of a term correlates with its frequency of
appearance (Homes and Solomon, 1951). From
that standpoint, it is considered that frequency
of a term on the academic website represents
the recognition level of a term for the special-
ists of the field, and that frequency of the term
on the general website represents the one for
laypersons of the field. On the basis of this con-
cept, our ?technical term checker? estimates the
difficulty level of a technical term on the basis
of the frequencies on the academic and general
website, and then inform the users about the re-
sult on three level (?plain?, ?may be difficult?,
?difficult?).
5W
RR
QT
V
6Q
QN
U
HQ
T
KP
RW
VV
KP
I
6K
VN
G
6G
OR
NC
VG
Figure 2: Screenshot of Title Revision Wizard
Search Boxes for keyword retrieval and menu retrieval organized by technical fields 
Technical Field The OB-P Phrases in Paper Titles The OB-P Phrases in Newspaper Headlines
Figure 3: Parallel Corpus of Titles and Headlines for Related Technology
3.3 Menu Options of the Phrases Used
in Newspapers Headlines
In order to meet key point 3, the authors have to
consider how to represent the advantages of the
technologies from various viewpoints, and come
up with the phrase that can concisely represent
the advantages. However, it is considered that
those who have a detailed knowledge of a techni-
cal field are unaccustomed to represent the ad-
vantages of the technologies in a title because
previous research showed that most of technical
paper titles in Japanese does not included such
a phrase (Senda and Sinohara, 2002).
We therefore prepared the pull-down menu
options of the phrases representing technical ad-
vantages used in newspapers headlines. The
menu options presents about 70 phrases catego-
rized by the way to describe the advantages of
technology. For example, in our menu options,
the following phrases are listed by the following
group: high density, high concentration, high
accuracy, . . . , low price, low cost, low pollution,
. . . , long-lived, long distance, . . . , short time,
short duration, . . . , ; etc 2 .
Figure 4 shows the menu options installed at
the title template for key point 3. This menu
options can help the users come up with or elab-
orate on the phrases by offering various expres-
sions and viewpoints.
4 Effect of Title Revision Wizard
In order to test the effect of the title revision
wizard, we conducted an experiment which had
17 technical researchers revise their titles with
and without using our wizard. We then con-
ducted a questionnaire survey on the effect of
the titles on the interest of lay readers.
4.1 Outline of the Experiment
We conducted the experiment according to the
following procedures.
Experiment 1 (Ex 1)
1. We showed each subject his technical pa-
per title (published after 2000) on his
own developed technology. We asked him
to imagine himself writing an explanation
of the same technology for lay readers.
2. We asked each subject to compose three
candidates of title for the explanation by
his own effort (that is, without our wiz-
ard) within the 20-minute time limit.
Experiment 2 (Ex 2)
1. After Ex 1, we input original technical
paper title into first title template of our
title revision wizard (for draft title), and
presented each subject with the wizard.
2. We asked each subject to compose candi-
dates of titles for the same document us-
ing the wizard and select thee titles from
the candidates within the 20-minute time
limit.
The research fields of subjects were physics,
electrical engineering, material science and me-
teorology. There was no intermission between
Ex 1 and 2.
2These sample phrases are translated into English
from original Japanese phrases.
4.2 Outline of the Questionnaire
Survey
In the questionnaire, each respondent was
shown seven titles per one technology that each
subject developed. They consisted of an origi-
nal technical paper title, three titles composed
at Ex 1, and three titles composed at Ex 2. Each
respondent was asked to select three-most inter-
esting titles from these 7 titles per one technol-
ogy.
Respondents to the questionnaire were 108
persons and general monitors of an Internet re-
search firm. We asked them to answer our ques-
tionnaire on a Web page which was prepared for
this survey.
This means, of course, that the results of the
questionnaire only contain responses from peo-
ple who have some skill at using the Internet,
rather than the public at large, because all re-
spondents were able to use E-mail and access
the web page. However, at least, the result
helps to write document on a Web page which
explains new technology for lay readers.
Moreover, the result is also an exercise in
reaching the general lay readers over the Inter-
net, and the use of the Internet is expected to
be increasingly important in reaching the gen-
eral public.
4.3 Analysis of Experiment and
Questionnaire Survey
Figure 5 shows the (average) share of the vote
which the respondents select three-most inter-
esting titles from each author?s seven titles (by
method of composing). Figure 5 indicates that:
 The average share of the vote of the titles
revised using the wizard are distributed
within the highest range.
 The average share of the vote of the titles
revised without using the wizard are dis-
tributed within the second highest range,
 The share of the vote of original techni-
cal paper titles are distributed within the
third highest range,
We checked if there is a significant difference
between the (average) share of the vote of orig-
inal technical paper titles and the titles revised
without using the wizard by t-test. As the re-
sult, we confirmed that there is a significant dif-
ference between these titles (the significant level
is 1%). We also checked if there is a significant
difference between the share of the vote of the
titles revised with and without using the wizard
Pull down menu of the phrases describing the advantage of the technology
Figure 4: Pull-down Menu Options Organizing the Phrases Used in Newspaper Head-
line
???????????????????????????????????
?????????????????????????????????????????????????????????????
??
?
??
??
??
??
??
??
??
??? ??? ??? ??? ??? ???
???????????????????????????
???????????????????????????????
?
?
?
?
?
?
?
?
?
?
???????
???????????????????????????
????????????????????
Figure 5: Histogram of Share of the Vote
which the Respondents Select Three-
most Interesting Titles from Each Au-
thor?s 7 Titles (by Method of Composing)
by t-test. As the result, We also confirmed that
there is a significant difference between these
titles (the significant level is 5%).
These results show that the titles composed
using the wizard could stimulate the lay readers?
interest the most among each subject?s seven
titles. In other words, the authors can stably
compose the more appealing titles for lay read-
ers with using the wizard than without using
the wizard.
If an author could compose effective titles by
himself, he might not take an advantage from
the wizard. We, therefore, focused only on
seven subjects who could not compose the ti-
tles stimulating a majority of the respondents?
interest without the wizard, and analyze the ef-
? ? ? ? ? ? ?
??
???
???
???
???
???
???
???
???
????????????????????????
????????????????
??
??
??
??
??
??
??
??
??
??
??
??
??
??????????????
Figure 6: Average Share of the Vote of
the Titles Composed with and without
Using Wizard
fect of their titles on the respondents? interest
in more detail.
We, then, compared the average share of the
vote for the titles which each of the seven au-
thor revised with and without using the wizard.
figure 6 shows that all the subjects can stably
compose the more appealing titles with using
the wizard than without using the wizard. We
checked if there is a significant difference be-
tween the share of the vote of the titles revised
with and without using the wizard by t-test. As
the result, We also confirmed that there is a sig-
nificant difference between these titles (the sig-
nificant level is 1%).
As the result of the above analysis, we con-
firmed that the title revision wizard can help
the users who cannot compose effective titles
by their own efforts.
5 Conclusion and Future Work
We emphasized that, in order to stimulate the
lay reader?s interest in newly-developed technol-
ogy, it is very important to compose an appeal-
ing title, however it is difficult for inexperienced
authors to come up with an appealing title.
In this research, we developed a title revi-
sion wizard that provides a guidance on revising
draft title and a few support tools for coming
up with and elaborating on appealing phrases.
Moreover, we verified the effect of the title re-
vision wizard. As a result, the titles composed
using the wizard can stimulate the lay reader?s
interest more than the titles composed without
using the wizard. In particular, our title revi-
sion wizard can help the users who cannot com-
pose effective titles by their own effort.
In future work, we will analyze the difference
of the expression of the titles composed with
and without using the wizard, and investigate
what sort expression is effective to lay readers.
Acknowledgements
The authors would like to express our grati-
tude to Ms. Tomoko Tsuchiya, Ms. Motoko
Kosugi, and Ms. Tomoko Mitamura of Central
Research Institute of Electric Power Industry
for their valuable advices for our questionnaire
survey, and Mr. Masahito Tanaka of Denryoku
Computing Center, Ltd. for his valuable sup-
ports to implement our title revision wizard. In
addition, the authors would like to express our
gratitude to anonymous reviewers for their sug-
gestions to improve our paper.
References
Adam L. Berger and Vibhu O. Mittal. 2000.
Ocelot: A system for summarizing web pages.
In Proc. of the 23rd Annual International
ACM-SIGIR Conference on Research and
Development in Information Retrieval, pages
144?151, Athens, Greece.
Teiji Furugori and Akiko Takeda. 1993. An
example-based system of writing english sen-
tences for japanese english users. Literary
and Linguistics Computing, 8(2):85?90.
James Hartley and Matthew Sydes. 1997. Are
structured abstracts easier to read than tradi-
tional ones? Journal of Research in Reading,
20(2):122?136.
Davis H. Homes and Richard L. Solomon. 1951.
Visual duration threshold as a function of
word-probability. Journal of Experimental
Psychology, 41:401?410.
Kentaro Inui, Atsushi Fujita, Tetsuro Taka-
hashi, Ryu Iida, and Tomoyo Iwakura.
2003. Text simplification for reading assis-
tance: A project note. In Proc. of The Sec-
ond International Workshop on Paraphras-
ing: Paraphrase Acquisition and Applications
(IWP2003), pages 9?16, Sapporo, Japan.
Rong Jin and Alex G. Hauptmann. 2000. Ti-
tle generation for spoken broadcast news us-
ing a training corpus. In Proc. of the 6th In-
ternational Conference on Spoken Language
Processing (ICSLP), pages 680?683, Beijing,
China.
Tadashi Kumano and Hideki Tanaka. 1998.
Translation examples browser: Japanese to
english translation aid for news articles. In
Proc. of NLP+IA 98/TAL+AI 98, pages 96?
102, Moncton, Canada.
Jean-Luc Minel, Sylvaine Nugier, and Gerald
Piat. 1997. How to appreciate the quality
of automatic text summarization? examples
of fan and mluce potocols and their results
on seraphin. intelligent scalable text summa-
rization. In Proc. of 35th Annual Meeting of
the ACL Workshop Intelligent Scalable Text
Summarization, pages 25?30, Madrid, Spain.
Satoshi Sato. 1992. Ctm: An example-based
translation aid system. In Proc. of COLING
1992, pages 23?28, Nantes, France.
Yasuko Senda and Yasusi Sinohara. 2002.
Analysis of titles and readers for title gen-
eration centered on the readers. In Proc. of
COLING 2002, pages 421?424, Taipei, Tai-
wan.
Corpus and Evaluation Measures for Multiple Document Summarization
with Multiple Sources
Tsutomu HIRAO
NTT Communication Science Laboratories
hirao@cslab.kecl.ntt.co.jp
Takahiro FUKUSIMA
Otemon Gakuin University
fukusima@res.otemon.ac.jp
Manabu OKUMURA
Tokyo Institute of Technology
oku@pi.titech.ac.jp
Chikashi NOBATA
Communication Research Laboratories
nova@crl.go.jp
Hidetsugu NANBA
Hiroshima City University
nanba@its.hiroshima-cu.ac.jp
Abstract
In this paper, we introduce a large-scale test collec-
tion for multiple document summarization, the Text
Summarization Challenge 3 (TSC3) corpus. We
detail the corpus construction and evaluation mea-
sures. The significant feature of the corpus is that it
annotates not only the important sentences in a doc-
ument set, but also those among them that have the
same content. Moreover, we define new evaluation
metrics taking redundancy into account and discuss
the effectiveness of redundancy minimization.
1 Introduction
It has been said that we have too much informa-
tion on our hands, forcing us to read through a great
number of documents and extract relevant informa-
tion from them. With a view to coping with this situ-
ation, research on automatic text summarization has
attracted a lot of attention recently and there have
been many studies in this field. There is a particular
need to establish methods for the automatic sum-
marization of multiple documents rather than single
documents.
There have been several evaluation workshops
on text summarization. In 1998, TIPSTER SUM-
MAC (Mani et al, 2002) took place and the Doc-
ument Understanding Conference (DUC)1 has been
held annually since 2001. DUC has included multi-
ple document summarization among its tasks since
the first conference. The Text Summarization Chal-
lenge (TSC)2 has been held once in one and a half
years as part of the NTCIR (NII-NACSIS Test Col-
lection for IR Systems) project since 2001. Multiple
document summarization was included for the first
time as one of the tasks at TSC2 (in 2002) (Okumura
et al, 2003). Multiple document summarization is
now a central issue for text summarization research.
1http://duc.nist.gov
2http://www.lr.pi.titech.ac.jp/tsc
In this paper, we detail the corpus construction
and evaluation measures used at the Text Summa-
rization Challenge 3 (TSC3 hereafter), where multi-
ple document summarization is the main issue. We
also report the results of a preliminary experiment
on simple multiple document summarization sys-
tems.
2 TSC3 Corpus
2.1 Guidelines for Corpus Construction
Multiple document summarization from multiple
sources, i.e., several newspapers concerned with the
same topic but with different publishers, is more dif-
ficult than single document summarization since it
must deal with more text (in terms of numbers of
characters and sentences). Moreover, it is peculiar
to multiple document summarization that the sum-
marization system must decide how much redun-
dant information should be deleted3.
In a single document, there will be few sentences
with the same content. In contrast, in multiple doc-
uments with multiple sources, there will be many
sentences that convey the same content with differ-
ent words and phrases, or even identical sentences.
Thus, a text summarization system needs to recog-
nize such redundant sentences and reduce the redun-
dancy in the output summary.
However, we have no way of measuring the ef-
fectiveness of such redundancy in the corpora for
DUC and TSC2. Key data in TSC2 was given as
abstracts (free summaries) whose number of char-
acters was less than a fixed number and, thus, it
is difficult to use for repeated or automatic evalu-
ation, and for the extraction of important sentences.
Moreover, in DUC, where most of the key data were
abstracts whose number of words was less than a
3It is true that we need other important techniques such as
those for maintaining the consistency of words and phrases that
refer to the same object, and for making the results more read-
able; however, they are not included here.
fixed number, the situation was the same as TSC2.
At DUC 2002, extracts (important sentences) were
used, and this allowed us to evaluate sentence ex-
traction. However, it is not possible to measure the
effectiveness of redundant sentences reduction since
the corpus was not annotated to show sentence with
same content. In addition, this is the same even if
we use the SummBank corpus (Radev et al, 2003).
In any case, because many of the current summa-
rization systems for multiple documents are based
on sentence extraction, we believe these corpora to
be unsuitable as sets of documents for evaluation.
On this basis, in TSC3, we assumed that the pro-
cess of multiple document summarization consists
of the following three steps, and we produce a cor-
pus for the evaluation of the system at each of the
three steps4.
Step 1 Extract important sentences from a given set
of documents
Step 2 Minimize redundant sentences from the re-
sult of Step 1
Step 3 Rewrite the result of Step 2 to reduce the
size of the summary to the specified number of
characters or less.
We have annotated not only the important sen-
tences in the document set, but also those among
them that have the same content. These are the cor-
pora for steps 1 and 2. We have prepared human-
produced free summaries (abstracts) for step 3.
In TSC3, since we have key data (a set of cor-
rect important sentences) for steps 1 and 2, we con-
ducted automatic evaluation using a scoring pro-
gram. We adopted an intrinsic evaluation by human
judges for step 3, which is currently under evalu-
ation. We provide details of the extracts prepared
for steps 1 and 2 and their evaluation measures in
the following sections. We do not report the overall
evaluation results for TSC3.
2.2 Data Preparation for Sentence Extraction
We begin with guidelines for annotating important
sentences (extracts). We think that there are two
kinds of extract.
1. A set of sentences that human annotators
judge as being important in a document set
(Fukusima and Okumura, 2001; Zechner,
1996; Paice, 1990).
4This is based on general ideas of a summarization system
and is not intended to impose any conditions on a summariza-
tion system.
Mainichi articles
Yomiuri articles
abstract
(a)
(b)
(c)
(d)
Doc. x
Doc. y
Figure 1: An example of an abstract and its sources.
2. A set of sentences that are suitable as a source
for producing an abstract, i.e., a set of sen-
tences in the original documents that corre-
spond to the sentences in the abstracts(Kupiec
et al, 1995; Teufel and Moens, 1997; Marcu,
1999; Jing and McKeown, 1999).
When we consider how summaries are produced,
it seems more natural to identify important seg-
ments in the document set and then produce sum-
maries by combining and rephrasing such informa-
tion than to select important sentences and revise
them as summaries. Therefore, we believe that sec-
ond type of extract is superior and thus we prepared
the extracts in that way.
However, as stated in the previous section, with
multiple document summarization, there may be
more than one sentence with the same content, and
thus we may have more than one set of sentences
in the original document that corresponds to a given
sentence in the abstract; that is to say, there may be
more than one key datum for a given sentence in the
abstract5.
we have two sets of sentences that correspond to
sentence   in the abstract.
(1)  of document  , or
(2) a combination of  and  of document 	
This means that  alone is able to produce   , and
  can also be produced by combining   and   (Fig-
ure 1).
We marked all the sentences in the original doc-
uments that were suitable sources for producing the
sentences of the abstract, and this made it possible
for us to determine whether or not a summariza-
tion system deleted redundant sentences correctly
at Step 2. If the system outputs the sentences in
the original documents that are annotated as cor-
responding to the same sentence in the abstract, it
5We use ?set of sentences? since we often find that more
than one sentence corresponds to a sentence in the abstract.
Table 1: Important Sentence Data.
Sentence ID of Abstract Set of Corresponding Sentences
1 
 

2 
Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational
Natural Language Learning, pp. 600?609, Prague, June 2007. c?2007 Association for Computational Linguistics
Japanese Dependency Analysis Using the Ancestor-Descendant Relation
Akihiro Tamura?? Hiroya Takamura?? Manabu Okumura??
? Common Platform Software Research Laboratories NEC Corporation
a-tamura@ah.jp.nec.com
?? Precision and Intelligence Laboratory, Tokyo Institute of Technology, Japan
{takamura,oku}@pi.titech.ac.jp
Abstract
We propose a novel method for Japanese de-
pendency analysis, which is usually reduced
to the construction of a dependency tree. In
deterministic approaches to this task, depen-
dency trees are constructed by series of ac-
tions of attaching a bunsetsu chunk to one of
the nodes in the tree being constructed. Con-
ventional techniques select the node based
on whether the new bunsetsu chunk and each
node in the trees are in a parent-child rela-
tion or not. However, tree structures include
relations between two nodes other than the
parent-child relation. Therefore, we use
ancestor-descendant relations in addition to
parent-child relations, so that the added re-
dundancy helps errors be corrected. Ex-
perimental results show that the proposed
method achieves higher accuracy.
1 Introduction
Japanese dependency analysis has been recognized
as one of the basic techniques in Japanese process-
ing. A number of techniques have been proposed
for years. Japanese dependency is usually repre-
sented by the relation between phrasal units called
?bunsetsu? chunks, which are the smallest meaning-
ful sequences consisting of an independent word and
accompanying words (e.g., a noun and a particle).
Hereafter, a ?chunk? means a bunsetsu chunk in this
paper. The relation between two chunks has a di-
?Akihiro Tamura belonged to Tokyo Institute of Technology
when this work was done.
Figure 1: Example of a dependency tree
rection from the modifier to the modifiee. All de-
pendencies in a sentence are represented by a de-
pendency tree, where a node indicates a chunk, and
node B is the parent of node A when chunk B is the
modifiee of chunk A. Figure 1 shows an example of
a dependency tree. The task of Japanese dependency
analysis is to find the modifiee for each chunk in a
sentence. The task is usually regarded as construc-
tion of a dependency tree.
In primitive approaches, the probabilities of de-
pendencies are given by manually constructed rules
and the modifiee of each chunk is determined. How-
ever, those rule-based approaches have problems in
coverage and consistency. Therefore, a number of
statistical techniques using machine learning algo-
rithms have recently been proposed. In most con-
ventional statistical techniques, the probabilities of
dependencies between two chunks are learned in the
learning phase, and then the modifiee of each chunk
is determined using the learned models in the anal-
ysis phase. In terms of dependency trees, the parent
node of each node is determined based on the likeli-
ness of parent-child relations between two nodes.
We here take notice of the characteristics of de-
pendencies which cannot be captured well only by
600
the parent-child relation. Consider, for example,
Figure 1. In Figure 1, ID 3(pizza-and) and ID
4(salad-accusative) are in a parallel structure. In the
structure, node 4 is a child of node 5(ate), but node
3 is not a child of 5, although 3 and 4 are both foods
and should share a tendency of being subcategorized
by the verb ?eat?. A number of conventional models
use the pair of 3(pizza-and) and 5(ate) as a nega-
tive instance because 3 does not modify 5. Conse-
quently, those models cannot learn and use the sub-
categorization preference of verbs well in the paral-
lel structures.
We focus on ancestor-descendant relations to
compensate for the weakness. Two nodes are in the
ancestor-descendant relation when one of the two
nodes is included in the path from the root node to
the other node. The upper node of the two nodes
is called an ?ancestor node? and the lower node a
?descendant node?. When the ancestor-descendant
relation is used, both of the above two instances
for nodes 3 and 4 can be considered as positive in-
stances. Therefore, it is expected that the ancestor-
descendant relation helps the algorithm capture the
characteristics that cannot be captured well by the
parent-child relation.
We aim to improve the performance of Japanese
dependency analysis by taking the ancestor-
descendant relation into account. In exploiting
ancestor-descendant information, it came to us that
redundant information is effectively utilized in a
coding problem in communications (Mackay, 2003).
Therefore, we propose a method in which the prob-
lem of determining the modifiee of a chunk is re-
garded as a kind of a coding problem: dependency is
expressed as a sequence of values, each of which de-
notes whether a parent-child relation or an ancestor-
descendant relation holds between two chunks.
In Section 2, we present the related work. In Sec-
tion 3, we explain our method. In Section 4, we de-
scribe our experiments and their results, where we
show the effectiveness of the proposed method. In
Section 5, we discuss the results of the experiments.
Finally, we describe the summary of this paper and
the future work in Section 6.
2 Conventional Statistical Methods for
Japanese Dependency Analysis
First, we describe general formulation of the
probability model for dependency analysis. We
denote a sequence of chunks, ?b1, b2, ..., bm?,
by B, and a sequence of dependency pat-
terns, ?Dep(1), Dep(2), ..., Dep(m)?, by D, where
Dep(i) = j means that bi modifies bj . Given the se-
quence B of chunks as an input, dependency analy-
sis is defined as the problem of finding the sequence
D of the dependency patterns that maximizes the
conditional probability P (D | B). A number of
the conventional methods assume that dependency
probabilities are independent of each other and ap-
proximate P (D | B) with
?m?1
i=1 P (Dep(i) | B).
P (Dep(i) | B) is estimated using machine learn-
ing algorithms. For example, Haruno et al (1999)
used Decision Trees, Sekine (2000) used Maximum
Entropy Models, Kudo and Matsumoto (2000) used
Support Vector Machines.
Another notable method is Cascaded Chunking
Model by Kudo and Matsumoto (2002). In their
model, a sentence is parsed by series of the fol-
lowing processes: whether or not the current chunk
modifies the following chunk is estimated, and if it
is so, the two chunks are merged together. Sassano
(2004) parsed a sentence efficiently using a stack.
The stack controls the modifier being analyzed.
These conventional methods determine the mod-
ifiee of each chunk based on the likeliness of de-
pendencies between two chunks (in terms of depen-
dency tree, the likeliness of parent-child relations
between two nodes). The difference between the
conventional methods and the proposed method is
that the proposed method determines the modifiees
based on the likeliness of ancestor-descendant re-
lations in addition to parent-child relations, while
the conventional methods tried to capture charac-
teristics that cannot be captured by parent-child re-
lations, by adding ad-hoc features such as features
of ?the chunk modified by the candidate modifiee?
to features of the candidate modifiee and the mod-
ifier. However, these methods do not deal with
ancestor-descendant relations between two chunks
directly, while our method uses that information di-
rectly. In Section 5, we empirically show that our
method uses the ancestor-descendant relation more
601
effectively than the conventional ones and explain
that our method is justifiable in terms of a coding
problem.
3 Proposed Method
The methods explained in this section construct a
dependency tree by series of actions of attaching
a node to one of the nodes in the trees being con-
structed. Hence, when the parent node of a certain
node is being determined, it is required that the par-
ent node should already be included in the tree being
constructed. To satisfy the requirement, we note the
characteristic of Japanese dependencies: dependen-
cies are directed from left to right. (i.e., the par-
ent node is closer to the end of a sentence than its
child node). Therefore, our methods analyze a sen-
tence backwards as in Sekine (2000) and Kudo and
Matsumoto (2000). Consider, for example, Figure
1. First, our methods determine the parent node of
ID 4(salad-accusative), and then that of ID 3(pizza-
and) is determined. Next, the parent node of ID 2(at
lunchtime), and finally, that of ID 1(he-nominative)
is determined and dependencies in a sentence are
identified. Please note that our methods are applica-
ble only to dependency structures of languages that
have a consistent head-direction like Japanese.
We explain three methods that are different in
the information used in determining the modifiee of
each chunk. In Section 3.1, we explain PARENT
METHOD and ANCESTOR METHOD, which de-
termine the modifiee of each chunk based on the
likeliness of only one type of the relation. PARENT
METHOD uses the parent-child relation, which is
used in conventional Japanese dependency analy-
sis. ANCESTOR METHOD is novel in that it
uses the ancestor-descendant relation which has not
been used in the existing methods. In Section
3.2, we explain our method, PARENT-ANCESTOR
METHOD, which determines the modifiees based
on the likeliness of both ancestor-descendant and
parent-child relations.
When the modifiee is determined using the
ancestor-descendant relation, it is necessary to take
into account the relations with every node in the tree.
Consider, for example, the case that the modifiee
of ID 1(he-nominative) is determined in Figure 1.
When using the parent-child relation, the modifiee
can be determined based only on the relation be-
tween ID 1 and 5. On the other hand, when using the
ancestor-descendant relation, the modifiee cannot be
determined based only on the relation between ID
1 and 5. This is because if one of ID 2, 3 and 4
is the modifiee of ID 1, the relation between ID 1
and 5 is ancestor-descendant. ID 5 is determined
as the modifiee of ID 1 only after the relations with
each node of ID 2, 3 and 4 are recognized not to
be ancestor-descendant. An elegant way to use the
ancestor-descendant relation, which we propose in
this paper, is to represent a dependency as a code-
word where each bit indicates the relation with a
node in the tree, and determine the modifiee based
on the relations with every node in the tree (for de-
tails to the next section).
3.1 Methods with a single relation: PARENT
METHOD and ANCESTOR METHOD
Figure 2 shows the pseudo code of the algo-
rithm to construct a dependency tree using PAR-
ENT METHOD or ANCESTOR METHOD. As
mentioned above, the two methods analyze a sen-
tence backwards. We should note that node1 to
noden in the algorithm respectively correspond to
the last chunk to the first chunk of a sentence.
MODEL PARENT(nodei,nodej) indicates the pre-
diction whether nodej is the parent of nodei or
not, which is the output of the learned model.
MODEL ANCESTOR(nodei,nodej) indicates the
prediction whether nodej is the ancestor of nodei or
not. String output indicates the sequence of the i?
1 predictions stored in step 3. The codeword denoted
by string[k] is the binary sequence given to the ac-
tion that nodei is attached to nodek. Parent[nodei]
indicates the node to which nodei is attached, and
Dis indicates a distance function. Thus, our method
predicts the correct actions by measuring the dis-
tance between the codeword string[k] and the pre-
dicted binary (later extended to real-valued) se-
quences string output. In other words, our method
selects the action that is the closest to the outputs of
the learned model.
Both models are learned from dependency trees
given as training data as shown in Figure 3. Each
relation is learned from ordered pairs of two nodes
in the trees. However, our algorithm in Figure 2
targets at dependencies directed from left to right.
602
1:for i = 1, 2, ..., n do
2: for j = 1, 2, ..., i ? 1 do
3: result parent[j]=MODEL PARENT(nodei,nodej)(in case of PARENT and PARENT-ANCESTOR METHOD)
3: result ancestor[j]=MODEL ANCESTOR(nodei,nodej)(in case of ANCESTOR and PARENT-ANCESTOR METHOD)
4: end
5: Parent[nodei]=argmink Dis(string[k], string output)
6:end
Figure 2: Pseudo code of PARENT, ANCESTOR,
and PARENT-ANCESTOR METHODS
Figure 3: Example of training instances
Therefore, the instances with a right-to-left depen-
dency are excluded from the training data. For ex-
ample, the instance with node4 being the candi-
date parent (or ancestor) of node1 is excluded in
Figure 3. MODEL PARENT uses ordered pairs
of a parent node and a child node as positive in-
stances and the other ordered pairs as negative in-
stances. MODEL ANCESTOR uses ordered pairs
of an ancestor node and a descendant node as
positive instances and the other ordered pairs as
negative instances. From the above description
and Figure 3, the number of training instances
used in learning MODEL PARENT is the same
as the number of training instances used in learn-
ing MODEL ANCESTOR. However, the number of
positive instances in learning MODEL ANCESTOR
is larger than in learning MODEL PARENT be-
cause the set of parent-child relations is a subset of
ancestor-descendant relations.
As mentioned above, the two methods analyze a
sentence backwards. We should note that node1 to
noden in the algorithm respectively correspond to
the last chunk to the first chunk of a sentence.
Next, we illustrate the process of determining the
parent node of a certain node nodem(with Figures 4
and 5). Hereafter, nodem is called a target node.
The parent node is determined based on the like-
liness of a relation; the parent-child and ancestor-
descendant relation are used in PARENT METHOD
and ANCESTOR METHOD respectively.
Our methods regard a dependency between the
target node and its parent node as a set of relations
between the target node and each node in the tree.
Each relation corresponds to one bit, which becomes
1 if the relation holds, ?1 otherwise. For example,
a sequence (?1,?1,?1, 1) represents that the par-
ent of node5 is node4 in PARENT METHOD (Fig-
ure 4), since the relation holds only between nodes
4 and 5.
First, the learned model judges whether the tar-
get node and each node in the current tree are in
a certain relation or not; PARENT METHOD uses
MODEL PARENT as the learned model and AN-
CESTOR METHOD uses MODEL ANCESTOR.
The sequence of the m?1 predictions by the learned
model is stored in string output.
The codeword string[k] is the binary (?1 or 1)
sequence that is to be output when the target node
is attached to the nodek. In Figures 4 and 5, the
set of string[k] (for node5) is in the dashed square.
For example, string[2] in ANCESTOR METHOD
(Figure 5) is (1, 1,?1,?1) since nodes 1 and 2 are
the ancestor of node5 if node5 is attached to node2.
Next, among the set of string[k], the codeword
that is the closest to the string output is selected.
The target node is then attached to the node cor-
responding to the selected codeword. In Figure 4,
the string[4], (?1,?1,?1, 1), is selected and then
node5 is attached to node4.
Japanese dependencies have the non-crossing
constraint: dependencies do not cross one another.
To satisfy the constraint, we remove the nodes that
will break the non-crossing constraint from the can-
didates of a parent node in step 5 of the algorithm.
PARENT METHOD differs from conventional
methods such as Sekine (2000) or Kudo and Mat-
sumoto (2000), in the process of determining the
parent node. These conventional methods select the
node given by argmaxjP (nodej | nodei) as the
parent node of nodei, setting the beam width to 1.
However, their processes are essentially the same as
the process in PARENT METHOD.
603
Figure 4: Analysis example using PARENT
METHOD
Figure 5: Analysis example using ANCESTOR
METHOD
3.2 Proposed method: PARENT-ANCESTOR
METHOD
The proposed method determines the parent node of
a target node based on the likeliness of ancestor-
descendant relations in addition to parent-child
relations. The use of ancestor-descendant rela-
tions makes it possible to capture the character-
istics which cannot be captured by parent-child
relations alone. The pseudo code of the pro-
posed method, PARENT-ANCESTOR METHOD,
is shown in Figure 2. MODEL PARENT and
MODEL ANCESTOR are learned as described in
Section 3.1. String output is the concatenation
of the predictions by both MODEL PARENT and
MODEL ANCESTOR. In addition, string[k] is
provided based not only on parent-child relations but
also on ancestor-descendant relations. An analysis
example using PARENT-ANCESTOR METHOD is
shown in Figure 6.
Figure 6: Analysis example using PARENT-
ANCESTOR METHOD
4 Experiment
4.1 Experimental settings
We used Kyoto University text corpus (Version
2.0) (Kurohashi and Nagao, 1997) for training and
test data. The articles on January 1st through 8th
(7,958 sentences) were used as training data, and the
articles on January 9th (1,246 sentences) as test data.
The dataset is the same as in leading works (Sekine,
2000; Kudo and Matsumoto, 2000; Kudo and Mat-
sumoto, 2002; Sassano, 2004).
We used SVMs as the algorithm of learning and
analyzing the relations between nodes. We used the
third degree polynomial kernel function and set the
soft margin parameter C to 1, which is exactly the
same setting as in Kudo and Matsumoto (2002). We
can obtain the real-valued score in step 3 of the al-
gorithm, which is the output of the separating func-
tion. The score can be regarded as likeliness of the
two nodes being in the parent-child (or the ancestor-
descendant). Therefore, we used the sequence of
the outputs of SVMs as string output, instead of
converting the scores into binary values indicating
whether a certain relation holds or not.
Two feature sets are used: static features and dy-
namic features. The static features used in the ex-
periments are shown in Table 1. The features are the
same as those used in Kudo and Matsumoto (2002).
In Table 1, HeadWord means the rightmost con-
tent word in the chunk whose part-of-speech is not
a functional category. FunctionalWord means the
604
Table 1: Static features used in experiments
Head Word (surface-form, POS, POS-subcategory,
inflection-type, inflection-form), Functional Word (
Modifier / surface-form, POS, POS-subcategory, inflection-type,
Modifiee inflection-form), brackets, quotation-marks,
punctuation-marks, position in sentence (beginning, end)
Between two distance (1,2-5,6-), case-particles, brackets,
chunks quotation-marks, punctuation-parks
Figure 7: Dynamic features
rightmost functional word or the inflectional form of
the rightmost predicate if there is no functional word
in the chunk.
Next, we explain the dynamic features used in
the experiments. Three types of dynamic features
were used in Kudo and Matsumoto (2002): (A)
the chunks modifying the current candidate modi-
fiee, (B) the chunk modified by the current candidate
modifiee, and (C) the chunks modifying the current
candidate modifier. The type C is not available in the
proposed method because the proposed method an-
alyzes a sentence backwards unlike Kudo and Mat-
sumoto (2002). Therefore, we did not use the type
C. We used the type A? and B? which are recursive
expansion of type A and B as the dynamic features
(Figure 7). The form of functional words or inflec-
tion was used as a type A? feature and POS and POS-
subcategory of HeadWord as a type B? feature.
4.2 Experimental results
In this section, we show the effectiveness of the pro-
posed method. First, we compare the three methods
described in Section 3: PARENT METHOD, AN-
CESTOR METHOD, and PARENT-ANCESTOR
METHOD. The results are shown in Table 2. Here,
dependency accuracy is the percentage of correct
dependencies (correct parent-child relations in trees
in test data), and sentence accuracy is the percent-
age of the sentences in which all the modifiees are
determined correctly (correctly constructed trees in
test data).
Table 2 shows that PARENT-ANCESTOR
METHOD is more accurate than the other two
Table 2: Result of dependency analysis using meth-
ods described in Section 3
Method Dependency SentenceAccuracy Accuracy
PARENT 88.95% 44.87%
ANCESTOR 87.64% 43.74%
PARENT-ANCESTOR 89.54% 47.38%
Table 3: Comparison to conventional methods
Feature Method Dependency SentenceAccuracy Accuracy
Only Proposed method 88.88% 46.33%
static Kudo and Matsumoto (2002) 88.71% 45.19%
Static + Proposed method 89.43% 47.94%
Dynamic A,B Kudo and Matsumoto (2002) 89.19% 46.64%
Original
Proposed method 89.54% 47.38%
Sekine (2000) 87.20% 40.76%
Kudo and Matsumoto (2000) 89.09% 46.17%
Kudo and Matsumoto (2002) 89.29% 47.53%
Sassano (2004) 89.56% 48.35%
w/o Rich Sassano (2004) 89.19% 47.05%w/o Conj 89.41% 47.86%
methods. In other words, the accuracy of depen-
dency analysis improves by utilizing the redundant
information. The improvement is statistically sig-
nificant in the sign-test with 1% significance-level.
Next, we compare the proposed method with
conventional methods. We compare the proposed
method particularly with Kudo and Matsumoto
(2002) with the same feature set. The reasons are
that Cascaded Chunking Model proposed in Kudo
and Matsumoto (2002) is used in a popular Japanese
dependency analyzer, CaboCha 1, and the compari-
son can highlight the effectiveness of our approach
because we can experiment under the same condi-
tions (e.g., dataset, feature set, learning algorithm).
A summary of the comparison is shown in Table 3.
Table 3 shows that the proposed method
outperforms conventional methods except Sas-
sano (2004)2, while Sassano (2004) used richer fea-
tures which are not used in the proposed method,
such as features for conjunctive structures based on
Kurohashi and Nagao (1994), features concerning
the leftmost content word in the candidate modi-
fiee. The comparison of the proposed method with
Sassano (2004)?s method without the features of
1http://chasen.org/?taku/software/
cabocha/
2We have not tested the improvement statistically because
we do not have access to the conventional methods.
605
Table 4: Accuracy of dependency analysis on paral-
lel structures
Parallel structures Other thanparallel structures
PARENT 74.18% 91.21%
ANCESTOR 73.24% 90.01%
PARENT-ANCESTOR 76.29% 91.63%
conjunctive structures (w/o Conj) and without the
richer features derived from the words in chunks
(w/o Rich) suggests that the proposed method is bet-
ter than or comparable to Sassano (2004)?s method.
5 Discussion
5.1 Performance on parallel structures
As mentioned in Section 1, the ancestor-descendant
relation is supposed to help to capture parallel struc-
tures. In this section, we discuss the performance of
dependency analysis on parallel structures. Parallel
structures such as those of nouns (e.g., Tom and Ken
eat hamburgers.) and those of verbs (e.g., Tom eats
hamburgers and drinks water.), are marked in Kyoto
University text corpus. We investigate the accuracy
of dependency analysis on parallel structures using
the information.
Table 4 shows that the accuracy on parallel struc-
tures improves by adding the ancestor-descendant
relation. The improvement is statistically significant
in the sign-test with 1% significance-level. Table 4
also shows that error reduction rate on parallel struc-
tures by adding the ancestor-descendant relation is
8.3% and the rate on the others is 4.7%. These show
that the ancestor-descendant relation work well es-
pecially for parallel structures.
In Table 4, the accuracy on parallel structures
using PARENT METHOD is slightly better than
that using ANCESTOR METHOD, while the dif-
ference is not statistically significant in the sign-
test. It shows that the parent-child relation is also
necessary for capturing the characteristics of paral-
lel structures. Consider the following two instances
in Figure 1 as an example: the ordered pair of ID
3(pizza-and) and ID 5(ate), and the ordered pair of
ID 4(salad-accusative) and ID 5. In ANCESTOR
METHOD, both instances are positive instances. On
the other hand, only the ordered pair of ID 4 and
ID 5 is a positive instance in PARENT METHOD.
Table 5: Comparison between usages of the
ancestor-descendant relation
Dependency Sentence
Accuracy Accuracy
Feature 88.57% 44.71%
Model 88.88% 46.33%
Hence, PARENT METHOD can learn appropriate
case-particles in a modifier of a verb. For exam-
ple, the particle which means ?and? does not mod-
ify verbs. However, it is difficult for ANCESTOR
METHOD to learn the characteristic. Therefore,
both parent-child and ancestor-descendant relations
are necessary for capturing parallel structures.
5.2 Discussion on usages of the
ancestor-descendant relation
In the proposed method, MODEL ANCESTOR,
which judges whether the relation between two
nodes is ancestor-descendant or not, is prepared,
and the information on the ancestor-descendant re-
lation is directly utilized. On the other hand,
conventional methods add the features regarding
the ancestor or descendant chunk to capture the
ancestor-descendant relation. In this section, we
empirically show that the proposed method utilizes
the information on the ancestor-descendant rela-
tion more effectively than conventional methods.
The results in the previous sections could not show
the effectiveness because MODEL PARENT and
MODEL ANCESTOR in the proposed method use
the features regarding the ancestor-descendant rela-
tion.
Table 5 shows the result of dependency analy-
sis using two types of usages of the information
on the ancestor-descendant relation. ?Feature? indi-
cates the conventional usage and ?Model? indicates
our usage. Please note that MODEL PARENT and
MODEL ANCESTOR used in ?Model? do not use
the features regarding the ancestor-descendant rela-
tion. Table 5 shows that our usage is more effec-
tive than the conventional usage. This is because
our usage takes advantage of redundancy in terms
of a coding problem as described in the next sec-
tion. Moreover, the learned features through the pro-
posed method would include more information than
606
ad-hoc features that were manually added.
5.3 Proposed method in terms of a coding
problem
In a coding problem, redundancy is effectively uti-
lized so that information can be transmitted more
properly (Mackay, 2003). This idea is the same as
the main point of the proposed method. In this sec-
tion, we discuss the proposed method in terms of a
coding problem.
In a coding problem, when encoding information,
the redundant bits are attached so that the added re-
dundancy helps errors be corrected. Moreover, the
following fact is known (Mackay, 2003):
the error-correcting ability is higher when the dis-
tances between the codewords are longer. (1)
For example, consider the following three types
of encodings: (A) two events are encoded respec-
tively into the codewords ?1 and 1 (the simplest
encoding), (B) into the codewords (?1,?1, 1) and
(1, 1, 1) (hamming distance:2), and (C) into the
codewords (?1,?1,?1) and (1, 1, 1) (hamming
distance:3). Please note that the hamming distance is
defined as the number of bits that differ between two
codewords. In (A), the correct information is not
transmitted if a one-bit error occurs. In (B), if an er-
ror occurs in the third bit, the error can be corrected
by assuming that the original codeword is closest
to the received codeword. In (C), any one-bit error
can be corrected. Thus, (B) has the higher error-
correcting ability than (A), and (C) has the higher
error-correcting ability than (B).
We explain the problem of determining the par-
ent node of a target node in the proposed method in
terms of the coding theory. A sequence of numbers
corresponds to a codeword. It is assumed that the
codeword which expresses the correct parent node
of the target node is transmitted. The codeword is
transmitted through the learned model through chan-
nels to the receiver. The receiver infers the parent
node from the received sequence (string output) in
consideration of the codewords that can be transmit-
ted (string[k]). Therefore, error-correcting ability,
the ability of correcting the errors in predictions in
step 3, is dependent on the distances between the
codewords (string[k]).
The codewords in PARENT-ANCESTOR
METHOD are the concatenation of the bits based on
both parent-child relations and ancestor-descendant
relations. Consequently, the distances between
codewords in PARENT-ANCESTOR METHOD are
longer than those in PARENT METHOD or AN-
CESTOR METHOD. From (1), the error-correcting
ability is expected to be higher. In terms of a coding
problem, the proposed method exploits the essence
of (1), and utilizes ancestor-descendant relations
effectively.
We assume that every bit added as redundancy is
correctly transmitted for the above-mentioned dis-
cussion. However, some of these added bits may be
transmitted wrongly in the proposed method. In that
case, the added redundancy may not help errors be
corrected than cause an error. In the experiments of
dependency analysis, the advantage prevails against
the disadvantage because accuracy of each bit of the
codeword is 94.5%, which is high value.
Discussion on applicability of existing codes
A number of approaches use Error Correcting
Output Coding (ECOC) (Dietterich and Bakiri,
1995; Ghani, 2000) for solving multiclass classifica-
tion problems as a coding problem. The approaches
assign a unique n-bit codeword to each class, and
then n classifiers are trained to predict each bit. The
predicted class is the one whose codeword is clos-
est to the codeword produced by the classifiers. The
codewords in these approaches are designed to be
well-separated from one another and have sufficient
error-correcting ability (e.g., BCH code).
However, these existing codewords are not ap-
plicable to the proposed method. In the proposed
method, we have two models respectively derived
from the parent-child and ancestor-descendant rela-
tion, which can be interpreted in terms of both lin-
guistic aspects and tree structures. If we use ECOC,
however, pairs of nodes are divided into positive and
negative instances arbitrarily. Since this division
lacks linguistic or structural meaning, training in-
stances will lose consistency and any proper model
will not be obtained. Moreover, we have to prepare
different models for each stage in tree construction,
because the length of the codewords vary according
to the number of nodes in the current tree.
607
Table 6: Result of dependency analysis using vari-
ous distance functions
Distance Method Dependency SentenceFunction Accuracy Accuracy
Hamming
PARENT(n) 85.05% 35.35%
PARENT(f) 85.48% 39.87%
ANCESTOR(n) 87.54% 43.42%
ANCESTOR(f) 86.97% 43.18%
Proposed method(n) 88.36% 43.74%
Proposed method(f) 88.45% 44.79%
PARENT 88.95% 44.87%
Cosine / ANCESTOR 87.64% 43.74%
Euclidean Proposed method 89.54% 47.38%
Manhattan
PARENT(n) 88.74% 44.63%
PARENT(f) 88.90% 44.79%
ANCESTOR 87.64% 43.74%
Proposed method 89.24% 46.89%
5.4 Influence of distance functions
In this section, we compare the performance of de-
pendency analysis with various distance functions:
hamming distance, euclidean distance, cosine dis-
tance, and manhattan distance. These distance func-
tions between sequences X=?x1 x2 ... xn? and
Y =?y1 y2 ... yn? are defined as follows:
? Ham(X,Y ) =
?n
i=1(1 ? ?(xi, yi)),
? Euc(X,Y ) =
??n
i=1(xi ? yi)2,
? Cos(X,Y ) = 1 ?
?n
i=1 xi?yi??n
i=1 x
2
i
??n
i=1 y
2
i
,
? Man(X,Y ) =
?n
i=1 | xi ? yi |.
In the hamming distance, string output is con-
verted to a binary sequence with their elements be-
ing of ?1 or 1. The cosine distance is equivalent to
the Euclidean distance under the condition that the
absolute value of every component of string[k] is
1.
The results of dependency analysis using these
distance functions are shown in Table 6. In Table
6, ?(n)? means that the nearest chunk in a sentence
is selected as the modifiee in order to break a tie,
which happens when the number of sequences satis-
fying the condition in step 5 is two or more, while
?(f)? means that the furthest chunk is selected. If the
results in case of (n) and (f) are the same, (n) and (f)
are omitted and only one result is shown.
Table 6 shows that the proposed method out-
performs PARENT METHOD and ANCESTOR
METHOD in any distance functions. It means that
the effectiveness of the proposed method does not
depend on distance functions. The result using the
hamming distance is much worse than using the
other distance functions. It means that using the
scores output by SVMs as the likeliness of a certain
relation improves the accuracy. The results of (n)
and (f) in the hamming distance are different. It is
because the hamming distances are always positive
integers and ties are more likely to happen. Table
6 also shows that the result of the cosine or the eu-
clidean distance is better than that of the manhattan
distance.
6 Conclusions
We proposed a novel method for Japanese depen-
dency analysis, which determines the modifiee of
each chunk based on the likeliness not only of
the parent-child relation but also of the ancestor-
descendant relation in a dependency tree. The
ancestor-descendant relation makes it possible to
capture the parallel structures in more depth. In
terms of a coding theory, the proposed method
boosts error-correcting ability by adding the redun-
dant bits based on ancestor-descendant relations and
increasing the distance between two codewords. Ex-
perimental results showed the effectiveness of the
proposed method. In addition, the results showed
that the proposed method outperforms conventional
methods.
Future work includes the following. In this pa-
per, we use the features proposed in Kudo and Mat-
sumoto (2002). By extracting new features that are
more suitable for the ancestor-descendant relation,
we can further improve our method. The features
used by Sassano (2004) are promising as well. We
are also planning to apply the proposed method to
other tasks which need to construct tree structures.
For example, (zero-) anaphora resolution is consid-
ered as a good candidate task for application.
References
Thomas G. Dietterich and Ghulum Bakiri. 1995. Solving
Multiclass Learning Problems via Error-Correcting
Output Codes. Journal of Artificial Intelligence Re-
search, 2:263?286.
Rayid Ghani. 2000. Using Error-Correcting Codes For
608
Text Classification. In Proc. of ICML-2000, pages
303?310.
Masahiko Haruno, Satoshi Shirai, and Yoshifumi
Ooyama. 1999. Using Decision Trees to Construct
a Practical Parser. Machine Learning, 34:131?149.
Taku Kudo and Yuji Matsumoto. 2000. Japanese Depen-
dency Analysis Based on Support Vector Machines. In
Proc. of EMNLP/VLC 2000, pages 18?25.
Taku Kudo and Yuji Matsumoto. 2002. Japanese Depen-
dency Analysis using Cascaded Chunking. In Proc. of
CoNLL 2002, pages 63?69.
Sadao Kurohashi and Makoto Nagao. 1994. A syntactic
analysis method of long Japanese sentences based on
the detection of conjunctive structures. Computational
Linguistics, 20(4):507?534.
Sadao Kurohashi and Makoto Nagao. 1997. Kyoto Uni-
versity text corpus project. In Proc. of ANLP, pages
115?118, Japan.
David J. C. Mackay. 2003. Information Theory, Infer-
ence, and Learning Algorithms. Cambridge Univer-
sity Press.
Manabu Sassano. 2004. Linear-Time Dependency Anal-
ysis for Japanese. In Proc. of COLING 2004, pages
8?14.
Satoshi Sekine. 2000. Japanese dependency analysis us-
ing a deterministic finite state transducer. In Proc. of
COLING 2000, pages 761?767.
609
91
92
93
94
Latent Variable Models for Semantic Orientations of Phrases
Hiroya Takamura
Precision and Intelligence Laboratory
Tokyo Institute of Technology
takamura@pi.titech.ac.jp
Takashi Inui
Japan Society of the Promotion of Science
tinui@lr.pi.titech.ac.jp
Manabu Okumura
Precision and Intelligence Laboratory
Tokyo Institute of Technology
oku@pi.titech.ac.jp
Abstract
We propose models for semantic orienta-
tions of phrases as well as classification
methods based on the models. Although
each phrase consists of multiple words, the
semantic orientation of the phrase is not a
mere sum of the orientations of the com-
ponent words. Some words can invert the
orientation. In order to capture the prop-
erty of such phrases, we introduce latent
variables into the models. Through exper-
iments, we show that the proposed latent
variable models work well in the classifi-
cation of semantic orientations of phrases
and achieved nearly 82% classification ac-
curacy.
1 Introduction
Technology for affect analysis of texts has recently
gained attention in both academic and industrial
areas. It can be applied to, for example, a survey
of new products or a questionnaire analysis. Au-
tomatic sentiment analysis enables a fast and com-
prehensive investigation.
The most fundamental step for sentiment anal-
ysis is to acquire the semantic orientations of
words: desirable or undesirable (positive or neg-
ative). For example, the word ?beautiful? is pos-
itive, while the word ?dirty? is negative. Many
researchers have developed several methods for
this purpose and obtained good results (Hatzi-
vassiloglou and McKeown, 1997; Turney and
Littman, 2003; Kamps et al, 2004; Takamura
et al, 2005; Kobayashi et al, 2001). One of
the next problems to be solved is to acquire se-
mantic orientations of phrases, or multi-term ex-
pressions. No computational model for semanti-
cally oriented phrases has been proposed so far al-
though some researchers have used techniques de-
veloped for single words. The purpose of this pa-
per is to propose computational models for phrases
with semantic orientations as well as classification
methods based on the models. Indeed the seman-
tic orientations of phrases depend on context just
as the semantic orientations of words do, but we
would like to obtain the most basic orientations of
phrases. We believe that we can use the obtained
basic orientations of phrases for affect analysis of
higher linguistic units such as sentences and doc-
uments.
The semantic orientation of a phrase is not a
mere sum of its component words. Semantic
orientations can emerge out of combinations of
non-oriented words. For example, ?light laptop-
computer? is positively oriented although neither
?light? nor ?laptop-computer? has a positive ori-
entation. Besides, some words can invert the ori-
entation of a neighboring word, such as ?low?
in ?low risk?, where the negative orientation of
?risk? is inverted to a ?positive? by the adjective
?low?. This kind of non-compositional operation
has to be incorporated into the model. We focus
on ?noun+adjective? in this paper, since this type
of phrase contains most of interesting properties
of phrases, such as emergence or inversion of se-
mantic orientations.
In order to capture the properties of semantic
orientations of phrases, we introduce latent vari-
ables into the models, where one random variable
corresponds to nouns and another random vari-
able corresponds to adjectives. The words that
are similar in terms of semantic orientations, such
as ?risk? and ?mortality? (i.e., the positive ori-
entation emerges when they are ?low?), make a
cluster in these models. Our method is language-
201
independent in the sense that it uses only cooccur-
rence data of words and semantic orientations.
2 Related Work
We briefly explain related work from two view-
points: the classification of word pairs and the
identification of semantic orientation.
2.1 Classification of Word Pairs
Torisawa (2001) used a probabilistic model to
identify the appropriate case for a pair of words
constituting a noun and a verb with the case of
the noun-verb pair unknown. Their model is the
same as Probabilistic Latent Semantic Indexing
(PLSI) (Hofmann, 2001), which is a generative
probability model of two random variables. Tori-
sawa?s method is similar to ours in that a latent
variable model is used for word pairs. How-
ever, Torisawa?s objective is different from ours.
In addition, we used not the original PLSI, but
its expanded version, which is more suitable for
this task of semantic orientation classification of
phrases.
Fujita et al (2004) addressed the task of the de-
tection of incorrect case assignment in automat-
ically paraphrased sentences. They reduced the
task to a problem of classifying pairs of a verb
and a noun with a case into correct or incorrect.
They first obtained a latent semantic space with
PLSI and adopted the nearest-neighbors method,
in which they used latent variables as features. Fu-
jita et al?s method is different from ours, and also
from Torisawa?s, in that a probabilistic model is
used for feature extraction.
2.2 Identification of Semantic Orientations
The semantic orientation classification of words
has been pursued by several researchers (Hatzi-
vassiloglou and McKeown, 1997; Turney and
Littman, 2003; Kamps et al, 2004; Takamura et
al., 2005). However, no computational model for
semantically oriented phrases has been proposed
to date although research for a similar purpose has
been proposed.
Some researchers used sequences of words as
features in document classification according to
semantic orientation. Pang et al (2002) used bi-
grams. Matsumoto et al (2005) used sequential
patterns and tree patterns. Although such patterns
were proved to be effective in document classi-
fication, the semantic orientations of the patterns
themselves are not considered.
Suzuki et al (2006) used the Expectation-
Maximization algorithm and the naive bayes clas-
sifier to incorporate the unlabeled data in the clas-
sification of 3-term evaluative expressions. They
focused on the utilization of context information
such as neighboring words and emoticons. Tur-
ney (2002) applied an internet-based technique to
the semantic orientation classification of phrases,
which had originally been developed for word sen-
timent classification. In their method, the num-
ber of hits returned by a search-engine, with a
query consisting of a phrase and a seed word (e.g.,
?phrase NEAR good?) is used to determine the
orientation. Baron and Hirst (2004) extracted col-
locations with Xtract (Smadja, 1993) and classi-
fied the collocations using the orientations of the
words in the neighboring sentences. Their method
is similar to Turney?s in the sense that cooccur-
rence with seed words is used. The three methods
above are based on context information. In con-
trast, our method exploits the internal structure of
the semantic orientations of phrases.
Inui (2004) introduced an attribute plus/minus
for each word and proposed several rules that
determine the semantic orientations of phrases
on the basis of the plus/minus attribute val-
ues and the positive/negative attribute values of
the component words. For example, a rule
[negative+minus=positive] determines ?low (mi-
nus) risk (negative)? to be positive. Wilson et
al. (2005) worked on phrase-level semantic orien-
tations. They introduced a polarity shifter, which
is almost equivalent to the plus/minus attribute
above. They manually created the list of polarity
shifters. The method that we propose in this paper
is an automatic version of Inui?s or Wilson et al?s
idea, in the sense that the method automatically
creates word clusters and their polarity shifters.
3 Latent Variable Models for Semantic
Orientations of Phrases
As mentioned in the Introduction, the semantic
orientation of a phrase is not a mere sum of its
component words. If we know that ?low risk? is
positive, and that ?risk? and ?mortality?, in some
sense, belong to the same semantic cluster, we can
infer that ?low mortality? is also positive. There-
fore, we propose to use latent variable models to
extract such latent semantic clusters and to real-
ize an accurate classification of phrases (we focus
202
N Z
A
N
A C
N Z
A C
N Z
A C
N Z
A C
(a) (b) (c) (d) (e)
Figure 1: Graphical representations:(a) PLSI, (b) naive bayes, (c) 3-PLSI, (d) triangle, (e) U-shaped;
Each node indicates a random variable. Arrows indicate statistical dependency between variables. N , A,
Z and C respectively correspond to nouns, adjectives, latent clusters and semantic orientations.
on two-term phrases in this paper). The models
adopted in this paper are also used for collabora-
tive filtering by Hofmann (2004).
With these models, the nouns (e.g., ?risk? and
?mortality?) that become positive by reducing
their degree or amount would make a cluster. On
the other hand, the adjectives or verbs (e.g., ?re-
duce? and ?decrease?) that are related to reduction
would also make a cluster.
Figure 1 shows graphical representations of sta-
tistical dependencies of models with a latent vari-
able. N , A, Z and C respectively correspond to
nouns, adjectives, latent clusters and semantic ori-
entations. Figure 1-(a) is the PLSI model, which
cannot be used in this task due to the absence of
a variable for semantic orientations. Figure 1-(b)
is the naive bayes model, in which nouns and ad-
jectives are statistically independent of each other
given the semantic orientation. Figure 1-(c) is,
what we call, the 3-PLSI model, which is the 3-
observable variable version of the PLSI. We call
Figure 1-(d) the triangle model, since three of its
four variables make a triangle. We call Figure 1-
(e) the U-shaped model. In the triangle model and
the U-shaped model, adjectives directly influence
semantic orientations (rating categories) through
the probability P (c|az). While nouns and adjec-
tives are associated with the same set of clusters Z
in the 3-PLSI and the triangle models, only nouns
are clustered in the U-shaped model.
In the following, we construct a probability
model for the semantic orientations of phrases us-
ing each model of (b) to (e) in Figure 1. We ex-
plain in detail the triangle model and the U-shaped
model, which we will propose to use for this task.
3.1 Triangle Model
Suppose that a set D of tuples of noun n, adjective
a (predicate, generally) and the rating c is given :
D = {(n1, a1, c1), ? ? ? , (n|D|, a|D|, c|D|)}, (1)
where c ? {?1, 0, 1}, for example. This can be
easily expanded to the case of c ? {1, ? ? ? , 5}. Our
purpose is to predict the rating c for unknown pairs
of n and a.
According to Figure 1-(d), the generative prob-
ability of n, a, c, z is the following :
P (nacz) = P (z|n)P (a|z)P (c|az)P (n). (2)
Remember that for the original PLSI model,
P (naz) = P (z|n)P (a|z)P (n).
We use the Expectation-Maximization (EM) al-
gorithm (Dempster et al, 1977) to estimate the pa-
rameters of the model. According to the theory of
the EM algorithm, we can increase the likelihood
of the model with latent variables by iteratively in-
creasing the Q-function. The Q-function (i.e., the
expected log-likelihood of the joint probability of
complete data with respect to the conditional pos-
terior of the latent variable) is expressed as :
Q(?) =
?
nac
fnac
?
z
P? (z|nac) log P (nazc|?), (3)
where ? denotes the set of the new parameters.
fnac denotes the frequency of a tuple n, a, c in the
data. P? represents the posterior computed using
the current parameters.
The E-step (expectation step) corresponds to
simple posterior computation :
P? (z|nac) = P (z|n)P (a|z)P (c|az)?
z P (z|n)P (a|z)P (c|az)
. (4)
For derivation of update rules in the M-step (max-
imization step), we use a simple Lagrange method
for this optimization problem with constraints :
?z, ?n P (n|z) = 1, ?z,
?
a P (a|z) = 1, and
?a, z, ?c P (c|az) = 1. We obtain the following
update rules :
P (z|n) =
?
ac fnacP? (z|nac)
?
ac fnac
, (5)
203
P (y|z) =
?
nc fnacP? (z|nac)
?
nac fnacP? (z|nac)
, (6)
P (c|az) =
?
n fnacP? (z|nac)
?
nc fnacP? (z|nac)
. (7)
These steps are iteratively computed until conver-
gence. If the difference of the values of Q-function
before and after an iteration becomes smaller than
a threshold, we regard it as converged.
For classification of an unknown pair n, a, we
compare the values of
P (c|na) =
?
z P (z|n)P (a|z)P (c|az)
?
cz P (z|n)P (a|z)P (c|az)
. (8)
Then the rating category c that maximize P (c|na)
is selected.
3.2 U-shaped Model
We suppose that the conditional probability of c
and z given n and a is expressed as :
P (cz|na) = P (c|az)P (z|n). (9)
We compute parameters above using the EM al-
gorithm with the Q-function :
Q(?) =
?
nac
fnac
?
z
P? (z|nac) log P (cz|na, ?).(10)
We obtain the following update rules :
E step
P? (z|nac) = P (c|az)P (z|n)?
z P (c|az)P (z|n)
, (11)
M step
P (c|az) =
?
n fnacP? (z|nac)
?
nc fnacP? (z|nac)
, (12)
P (z|n) =
?
ac fnacP? (z|nac)
?
ac fnac
. (13)
For classification, we use the formula :
P (c|na) =
?
z
P (c|az)P (z|n). (14)
3.3 Other Models for Comparison
We will also test the 3-PLSI model corresponding
to Figure 1-(c).
In addition to the latent models, we test a base-
line classifier, which uses the posterior probabil-
ity :
P (c|na) ? P (n|c)P (a|c)P (c). (15)
This baseline model is equivalent to the 2-term
naive bayes classifier (Mitchell, 1997). The graph-
ical representation of the naive bayes model is (b)
in Figure 1. The parameters are estimated as :
P (n|c) = 1 + fnc|N | + fc
, (16)
P (a|c) = 1 + fac|A| + fc
, (17)
where |N | and |A| are the numbers of the words
for n and a, respectively.
Thus, we have four different models : naive
bayes (baseline), 3-PLSI, triangle, and U-shaped.
3.4 Discussions on the EM computation, the
Models and the Task
In the actual EM computation, we use the tem-
pered EM (Hofmann, 2001) instead of the stan-
dard EM explained above, because the tempered
EM can avoid an inaccurate estimation of the
model caused by ?over-confidence? in computing
the posterior probabilities. The tempered EM can
be realized by a slight modification to the E-step,
which results in a new E-step :
P? (z|nac) =
(
P (c|az)P (z|n)
)?
?
z
(
P (c|az)P (z|n)
)? , (18)
for the U-shaped model, where ? is a positive
hyper-parameter, called the inverse temperature.
The new E-steps for the other models are similarly
expressed.
Now we have two hyper-parameters : inverse
temperature ?, and the number of possible val-
ues M of latent variables. We determine the
values of these hyper-parameters by splitting the
given training dataset into two datasets (the tempo-
rary training dataset 90% and the held-out dataset
10%), and by obtaining the classification accuracy
for the held-out dataset, which is yielded by the
classifier with the temporary training dataset.
We should also note that Z (or any variable)
should not have incoming arrows simultaneously
from N and A, because the model with such ar-
rows has P (z|na), which usually requires an ex-
cessively large memory.
To work with numerical scales of the rating
variable (i.e., the difference between c = ?1 and
c = 1 should be larger than that of c = ?1
and c = 0), Hofmann (2004) used also a Gaus-
sian distribution for P (c|az) in collaborative filter-
ing. However, we do not employ a Gaussian, be-
cause in our dataset, the number of rating classes is
204
only 3, which is so small that a Gaussian distribu-
tion cannot be a good approximation of the actual
probability density function. We conducted pre-
liminary experiments with the model with Gaus-
sians, but failed to obtain good results. For other
datasets with more classes, Gaussians might be a
good model for P (c|az).
The task we address in this paper is somewhat
similar to the trigram prediction task, in the sense
that both are classification tasks given two words.
However, we should note the difference between
these two tasks. In our task, the actual answer
given two specific words are fixed as illustrated
by the fact ?high+salary? is always positive, while
the answer for the trigram prediction task is ran-
domly distributed. We are therefore interested in
the semantic orientations of unseen pairs of words,
while the main purpose of the trigram prediction
is accurately estimate the probability of (possibly
seen) word sequences.
In the proposed models, only the words that ap-
peared in the training dataset can be classified. An
attempt to deal with the unseen words is an in-
teresting task. For example, we could extend our
models to semi-supervised models by regarding C
as a partially observable variable. We could also
use distributional similarity of words (e.g., based
on window-size cooccurrence) to find an observed
word that is most similar to the given unseen word.
However, such methods would not work for the
semantic orientation classification, because those
methods are designed for simple cooccurrence and
cannot distinguish ?survival-rate? from ?infection-
rate?. In fact, the similarity-based method men-
tioned above failed to work efficiently in our pre-
liminary experiments. To solve the problem of un-
seen words, we would have to use other linguistic
resources such as a thesaurus or a dictionary.
4 Experiments
4.1 Experimental Settings
We extracted pairs of a noun (subject) and an ad-
jective (predicate), from Mainichi newspaper ar-
ticles (1995) written in Japanese, and annotated
the pairs with semantic orientation tags : positive,
neutral or negative. We thus obtained the labeled
dataset consisting of 12066 pair instances (7416
different pairs). The dataset contains 4459 neg-
ative instances, 4252 neutral instances, and 3355
positive instances. The number of distinct nouns is
4770 and the number of distinct adjectives is 384.
To check the inter-annotator agreement between
two annotators, we calculated ? statistics, which
was 0.640. This value is allowable, but not quite
high. However, positive-negative disagreement is
observed for only 0.7% of the data. In other words,
this statistics means that the task of extracting neu-
tral examples, which has hardly been explored, is
intrinsically difficult.
We employ 10-fold cross-validation to obtain
the average value of the classification accuracy.
We split the dataset such that there is no overlap-
ping pair (i.e., any pair in the training dataset does
not appear in the test dataset).
If either of the two words in a pair in the test
dataset does not appear in the training dataset, we
excluded the pair from the test dataset since the
problem of unknown words is not in the scope of
this research. Therefore, we evaluate the pairs that
are not in the training dataset, but whose compo-
nent words appear in the training dataset.
In addition to the original dataset, which we call
the standard dataset, we prepared another dataset
in order to examine the power of the latent variable
model. The new dataset, which we call the hard
dataset, consists only of examples with 17 difficult
adjectives such as ?high?, ?low?, ?large?, ?small?,
?heavy?, and ?light?. 1 The semantic orientations
of pairs including these difficult words often shift
depending on the noun they modify. Thus, the
hard dataset is a subset of the standard dataset. The
size of the hard dataset is 4787. Please note that
the hard dataset is used only as a test dataset. For
training, we always use the standard dataset in our
experiments.
We performed experiments with all the values
of ? in {0.1, 0.2, ? ? ? , 1.0} and with all the values
of M in {10, 30, 50, 70, 100, 200, 300, 500}, and
predicted the best values of the hyper-parameters
with the held-out method in Section 3.4.
4.2 Results
The classification accuracies of the four methods
with ? and M predicted by the held-out method
are shown in Table 1. Please note that the naive
bayes method is irrelevant of ? and M . The table
shows that the triangle model and the U-shaped
1The complete list of the 17 Japanese adjectives with their
English counterparts are : takai (high), hikui (low), ookii
(large), chiisai (small), omoi (heavy), karui (light), tsuyoi
(strong), yowai (weak), ooi (many), sukunai (few/little), nai
(no), sugoi (terrific), hageshii (terrific), hukai (deep), asai
(shallow), nagai (long), mizikai (short).
205
Table 1: Accuracies with predicted ? and M
standard hard
accuracy ? M accuracy ? M
Naive Bayes 73.40 ? ? 65.93 ? ?
3-PLSI 67.02 0.73 91.7 60.51 0.80 87.4
Triangle model 81.39 0.60 174.0 77.95 0.60 191.0
U-shaped model 81.94 0.64 60.0 75.86 0.65 48.3
model achieved high accuracies and outperformed
the naive bayes method. This result suggests that
we succeeded in capturing the internal structure
of semantically oriented phrases by way of latent
variables. The more complex structure of the tri-
angle model resulted in the accuracy that is higher
than that of the U-shaped model.
The performance of the 3-PLSI method is even
worse than the baseline method. This result shows
that we should use a model in which adjectives can
directly influence the rating category.
Figures 2, 3, 4 show cross-validated accuracy
values for various values of ?, respectively yielded
by the 3-PLSI model, the triangle model and the
U-shaped model with different numbers M of pos-
sible states for the latent variable. As the figures
show, the classification performance is sensitive to
the value of ?. M = 100 and M = 300 are mostly
better than M = 10. However, this is a tradeoff
between classification performance and training
time, since large values of M demand heavy com-
putation. In that sense, the U-shaped model is use-
ful in many practical situations, since it achieved a
good accuracy even with a relatively small M .
To observe the overall tendency of errors, we
show the contingency table of classification by the
U-shaped model with the predicted values of hy-
perparameters, in Table 2. As this table shows,
most of the errors are caused by the difficulty of
classifying neutral examples. Only 2.26% of the
errors are mix-ups of the positive orientation and
the negative orientation.
We next investigate the causes of errors by ob-
serving those mix-ups of the positive orientation
and the negative orientation.
One type of frequent errors is illustrated by the
pair ?food (?s price) is high?, in which the word
?price? is omitted in the actual example 2. As in
this expression, the attribute (price, in this case) of
an example is sometimes omitted or not correctly
2This kind of ellipsis often occurs in Japanese.
 62
 64
 66
 68
 70
 72
 74
 76
 78
 80
 82
 84
 0.1  0.2  0.3  0.4  0.5  0.6  0.7  0.8  0.9  1
Ac
cu
ra
cy
 (%
)
beta
M=300
M=100
M=10
Figure 2: 3-PLSI model with standard dataset
 62
 64
 66
 68
 70
 72
 74
 76
 78
 80
 82
 84
 0.1  0.2  0.3  0.4  0.5  0.6  0.7  0.8  0.9  1
Ac
cu
ra
cy
 (%
)
beta
M=300
M=100
M=10
Figure 3: Triangle model with standard dataset
 62
 64
 66
 68
 70
 72
 74
 76
 78
 80
 82
 84
 0.1  0.2  0.3  0.4  0.5  0.6  0.7  0.8  0.9  1
Ac
cu
ra
cy
 (%
)
beta
M=300
M=100
M=10
Figure 4: U-shaped model with standard dataset
206
Table 2: Contingency table of classification result by the U-shaped model
U-shaped model
positive neutral negative sum
positive 1856 281 69 2206
Gold standard neutral 202 2021 394 2617
negative 102 321 2335 2758
sum 2160 2623 2798 7581
identified. To tackle these examples, we will need
methods for correctly identifying attributes and
objects. Some researchers are starting to work on
this problem (e.g., Popescu and Etzioni (2005)).
We succeeded in addressing the data-sparseness
problem by introducing a latent variable. How-
ever, this problem still causes some errors. Pre-
cise statistics cannot be obtained for infrequent
words. This problem will be solved by incorporat-
ing other resources such as thesaurus or a dictio-
nary, or combining our method with other methods
using external wider contexts (Suzuki et al, 2006;
Turney, 2002; Baron and Hirst, 2004).
4.3 Examples of Obtained Clusters
Next, we qualitatively evaluate the proposed meth-
ods. For several clusters z, we extract the words
that occur more than twice in the whole dataset
and are in top 50 according to P (z|n). The model
used here as an example is the U-shaped model.
The experimental settings are ? = 0.6 and M =
60. Although some elements of clusters are com-
posed of multiple words in English, the original
Japanese counterparts are single words.
Cluster 1 trouble, objection, disease, complaint, anx-
iety, anamnesis, relapse
Cluster 2 risk, mortality, infection rate, onset rate
Cluster 3 bond, opinion, love, meaning, longing, will
Cluster 4 vote, application, topic, supporter
Cluster 5 abuse, deterioration, shock, impact, burden
Cluster 6 deterioration, discrimination, load, abuse
Cluster 7 relative importance, degree of influence,
number, weight, sense of belonging, wave,
reputation
These obtained clusters match our intuition. For
example, in cluster 2 are the nouns that are neg-
ative when combined with ?high?, and positive
when combined with ?low?. In fact, the posterior
probabilities of semantic orientations for cluster 2
are as follows :
P (negative|high, cluster 2) = 0.995,
P (positive|low, cluster 2) = 0.973.
With conventional clustering methods based on
the cooccurrence of two words, cluster 2 would
include the words resulting in the opposite orien-
tation, such as ?success rate?. We succeeded in
obtaining the clusters that are suitable for our task,
by incorporating the new variable c for semantic
orientation in the EM computation.
5 Conclusion
We proposed models for phrases with semantic
orientations as well as a classification method
based on the models. We introduced a latent vari-
able into the models to capture the properties of
phrases. Through experiments, we showed that
the proposed latent variable models work well
in the classification of semantic orientations of
phrases and achieved nearly 82% classification ac-
curacy. We should also note that our method is
language-independent although evaluation was on
a Japanese dataset.
We plan next to adopt a semi-supervised learn-
ing method in order to correctly classify phrases
with infrequent words, as mentioned in Sec-
tion 4.2. We would also like to extend our method
to 3- or more term phrases. We can also use the
obtained latent variables as features for another
classifier, as Fujita et al (2004) used latent vari-
ables of PLSI for the k-nearest neighbors method.
One important and promising task would be the
use of semantic orientations of words for phrase
level classification.
References
Faye Baron and Graeme Hirst. 2004. Collocations
as cues to semantic orientation. In AAAI Spring
Symposium on Exploring Attitude and Affect in Text:
Theories and Applications.
Arthur P. Dempster, Nan M. Laird, and Donald B. Ru-
bin. 1977. Maximum likelihood from incomplete
data via the EM algorithm. Journal of the Royal Sta-
tistical Society Series B, 39(1):1?38.
207
Atsushi Fujita, Kentaro Inui, and Yuji Matsumoto.
2004. Detection of incorrect case assignments in au-
tomatically generated paraphrases of Japanese sen-
tences. In Proceedings of the 1st International Joint
Conference on Natural Language Processing (IJC-
NLP), pages 14?21.
Vasileios Hatzivassiloglou and Kathleen R. McKeown.
1997. Predicting the semantic orientation of ad-
jectives. In Proceedings of the Thirty-Fifth Annual
Meeting of the Association for Computational Lin-
guistics and the Eighth Conference of the European
Chapter of the Association for Computational Lin-
guistics, pages 174?181.
Thomas Hofmann. 2001. Unsupervised learning
by probabilistic latent semantic analysis. Machine
Learning, 42:177?196.
Thomas Hofmann. 2004. Latent semantic models for
collaborative filtering. ACM Transactions on Infor-
mation Systems, 22:89?115.
Takashi Inui. 2004. Acquiring Causal Knowledge from
Text Using Connective Markers. Ph.D. thesis, Grad-
uate School of Information Science, Nara Institute
of Science and Technology.
Jaap Kamps, Maarten Marx, Robert J. Mokken, and
Maarten de Rijke. 2004. Using wordnet to measure
semantic orientation of adjectives. In Proceedings
of the 4th International Conference on Language
Resources and Evaluation (LREC 2004), volume IV,
pages 1115?1118.
Nozomi Kobayashi, Takashi Inui, and Kentaro Inui.
2001. Dictionary-based acquisition of the lexical
knowledge for p/n analysis (in Japanese). In Pro-
ceedings of Japanese Society for Artificial Intelli-
gence, SLUD-33, pages 45?50.
Mainichi. 1995. Mainichi Shimbun CD-ROM version.
Shotaro Matsumoto, Hiroya Takamura, and Manabu
Okumura. 2005. Sentiment classification using
word sub-sequences and dependency sub-trees. In
Proceedings of the 9th Pacific-Asia Conference on
Knowledge Discovery and Data Mining (PAKDD-
05), pages 301?310.
Tom M. Mitchell. 1997. Machine Learning. McGraw
Hill.
Bo Pang, Lillian Lee, and Shivakumar Vaithyanathan.
2002. Thumbs up? sentiment classification using
machine learning techniques. In Proceedings of the
Conference on Empirical Methods in Natural Lan-
guage Processing (EMNLP?02), pages 79?86.
Ana-Maria Popescu and Oren Etzioni. 2005. Ex-
tracting product features and opinions from re-
views. In Proceedings of joint conference on Hu-
man Language Technology / Conference on Em-
pirical Methods in Natural Language Processing
(HLT/EMNLP?05), pages 339?346.
Frank Z. Smadja. 1993. Retrieving collocations
from text: Xtract. Computational Linguistics,,
19(1):143?177.
Yasuhiro Suzuki, Hiroya Takamura, and Manabu Oku-
mura. 2006. Application of semi-supervised learn-
ing to evaluative expression classification. In Pro-
ceedings of the 7th International Conference on In-
telligent Text Processing and Computational Lin-
guistics (CICLing-06), pages 502?513.
Hiroya Takamura, Takashi Inui, and Manabu Okumura.
2005. Extracting semantic orientations of words us-
ing spin model. In Proceedings 43rd Annual Meet-
ing of the Association for Computational Linguistics
(ACL?05), pages 133?140.
Kentaro Torisawa. 2001. An unsuperveised method
for canonicalization of Japanese postpositions. In
Proceedings of the 6th Natural Language Process-
ing Pacific Rim Symposium (NLPRS 2001), pages
211?218.
Peter D. Turney and Michael L. Littman. 2003. Mea-
suring praise and criticism: Inference of semantic
orientation from association. ACM Transactions on
Information Systems, 21(4):315?346.
Peter D. Turney. 2002. Thumbs up or thumbs down?
semantic orientation applied to unsupervised clas-
sification of reviews. In Proceedings 40th Annual
Meeting of the Association for Computational Lin-
guistics (ACL?02), pages 417?424.
Theresa Wilson, Janyce Wiebe, and Paul Hoffmann.
2005. Recognizing contextual polarity in phrase-
level sentiment analysis. In Proceedings of joint
conference on Human Language Technology / Con-
ference on Empirical Methods in Natural Language
Processing (HLT/EMNLP?05), pages 347?354.
208
Proceedings of the 12th Conference of the European Chapter of the ACL, pages 781?789,
Athens, Greece, 30 March ? 3 April 2009. c?2009 Association for Computational Linguistics
Text Summarization Model
based on Maximum Coverage Problem and its Variant
Hiroya Takamura and Manabu Okumura
Precision and Intelligence Laboratory, Tokyo Institute of Technology
4259 Nagatsuta Midori-ku Yokohama, 226-8503
takamura@pi.titech.ac.jp oku@pi.titech.ac.jp
Abstract
We discuss text summarization in terms of
maximum coverage problem and its vari-
ant. We explore some decoding algorithms
including the ones never used in this sum-
marization formulation, such as a greedy
algorithm with performance guarantee, a
randomized algorithm, and a branch-and-
bound method. On the basis of the results
of comparative experiments, we also aug-
ment the summarization model so that it
takes into account the relevance to the doc-
ument cluster. Through experiments, we
showed that the augmented model is su-
perior to the best-performing method of
DUC?04 on ROUGE-1 without stopwords.
1 Introduction
Automatic text summarization is one of the tasks
that have long been studied in natural language
processing. This task is to create a summary, or
a short and concise document that describes the
content of a given set of documents (Mani, 2001).
One well-known approach to text summariza-
tion is the extractive method, which selects some
linguistic units (e.g., sentences) from given doc-
uments in order to generate a summary. The ex-
tractive method has an advantage that the gram-
maticality is guaranteed at least at the level of the
linguistic units. Since the actual generation of
linguistic expressions has not achieved the level
of the practical use, we focus on the extractive
method in this paper, especially the method based
on the sentence extraction. Most of the extractive
summarization methods rely on sequentially solv-
ing binary classification problems of determining
whether each sentence should be selected or not.
In such sequential methods, however, the view-
point regarding whether the summary is good as
a whole, is not taken into consideration, although
a summary conveys information as a whole.
We represent text summarization as an opti-
mization problem and attempt to globally solve
the problem. In particular, we represent text sum-
marization as a maximum coverage problem with
knapsack constraint (MCKP). One of the advan-
tages of this representation is that MCKP can di-
rectly model whether each concept in the given
documents is covered by the summary or not,
and can dispense with rather counter-intuitive ap-
proaches such as giving penalty to each pair of two
similar sentences. By formally apprehending the
target problem, we can use a lot of knowledge and
techniques developed in the combinatorial mathe-
matics, and also analyse results more precisely. In
fact, on the basis of the results of the experiments,
we augmented the summarization model.
The contributions of this paper are as follows.
We are not the first to represent text summarization
as MCKP. However, no researchers have exploited
the decoding algorithms for solving MCKP in
the summarization task. We conduct compre-
hensive comparative experiments of those algo-
rithms. Specifically, we test the greedy algorithm,
the greedy algorithm with performance guarantee,
the stack decoding, the linear relaxation problem
with randomized decoding, and the branch-and-
bound method. On the basis of the experimental
results, we then propose an augmented model that
takes into account the relevance to the document
cluster. We empirically show that the augmented
model is superior to the best-performing method
of DUC?04 on ROUGE-1 without stopwords.
2 Related Work
Carbonell and Goldstein (2000) used sequential
sentence selection in combination with maximal
marginal relevance (MMR), which gives penalty
to sentences that are similar to the already se-
lected sentences. Schiffman et al?s method (2002)
is also based on sequential sentence selection.
Radev et al (2004), in their method MEAD, used
a clustering technique to find the centroid, that
781
is, the words with high relevance to the topic
of the document cluster. They used the centroid
to rank sentences, together with the MMR-like
redundancy score. Both relevance and redun-
dancy are taken into consideration, but no global
viewpoint is given. In CLASSY, which is the
best-performing method in DUC?04, Conroy et
al. (2004) scored sentences with the sum of tf-idf
scores of words. They also incorporated sentence
compression based on syntactic or heuristic rules.
McDonald (2007) formulated text summariza-
tion as a knapsack problem and obtained the
global solution and its approximate solutions. Its
relation to our method will be discussed in Sec-
tion 6.1. Filatova and Hatzivassiloglou (2004) first
formulated text summarization as MCKP. Their
decoding method is a greedy one and will be em-
pirically compared with other decoding methods
in this paper. Yih et al (2007) used a slightly-
modified stack decoding. The optimization prob-
lem they solved was the MCKP with the last sen-
tence truncation. Their stack decoding is one of
the decoding methods discussed in this paper. Ye
et al (2007) is another example of coverage-based
methods. Shen et al (2007) regarded summariza-
tion as a sequential labelling task and solved it
with Conditional Random Fields. Although the
model is globally optimized in terms of likelihood,
the coverage of concepts is not taken into account.
3 Modeling text summarization
In this paper, we focus on the extractive summa-
rization, which generates a summary by select-
ing linguistic units (e.g., sentences) in given doc-
uments. There are two types of summarization
tasks: single-document summarization and multi-
document summarization. While single-document
summarization is to generate a summary from a
single document, multi-document summarization
is to generate a summary frommultiple documents
regarding one topic. Such a set of multiple docu-
ments is called a document cluster. The method
proposed in this paper is applicable to both tasks.
In both tasks, documents are split into several lin-
guistic units D = {s1, ? ? ? , s|D|} in preprocess-
ing. We will select some linguistic units from D to
generate a summary. Among other linguistic units
that can be used in the method, we use sentences
so that the grammaticality at the sentence level is
going to be guaranteed.
We introduce conceptual units (Filatova and
Hatzivassiloglou, 2004), which compose the
meaning of a sentence. Sentence si is represented
by a set of conceptual units {ei1, ? ? ? , ei|si|}. For
example, the sentence ?The man bought a book
and read it? could be regarded as consisting of two
conceptual units ?the man bought a book? and ?the
man read the book?. It is not easy, however, to
determine the appropriate granularity of concep-
tual units. A simple way would be to regard the
above sentence as consisting of four conceptual
units ?man?, ?book?, ?buy?, and ?read?. There
is some work on the definition of conceptual units.
Hovy et al (2006) proposed to use basic elements,
which are dependency subtrees obtained by trim-
ming dependency trees. Although basic elements
were proposed for evaluation of summaries, they
can probably be used also for summary genera-
tion. However, such novel units have not proved
to be useful for summary generation. Since we fo-
cus more on algorithms and models in this paper,
we simply use words as conceptual units.
The goal of text summarization is to cover as
many conceptual units as possible using only a
small number of sentences. In other words, the
goal is to find a subset S(? D) that covers as
many conceptual units as possible. In the follow-
ing, we introduce models for that purpose. We
think of the situation that the summary length must
be at most K (cardinality constraint) and the sum-
mary length is measured by the number of words
or bytes in the summary.
Let xi denote a variable which is 1 if sentence
si is selected, otherwise 0, aij denote a constant
which is 1 if sentence si contains word ej , oth-
erwise 0. We regard word ej as covered when at
least one sentence containing ej is selected as part
of the summary. That is, word ej is covered if and
only if
?
i aijxi ? 1. Now our objective is to find
the binary assignment on xi with the best coverage
such that the summary length is at most K:
max. |{j|
?
i aijxi ? 1}|
s.t.
?
i cixi ? K; ?i, xi ? {0, 1},
where ci is the cost of selecting si, i.e., the number
of words or bytes in si.
For convenience, we rewrite the problem above:
max.
?
j zj
s.t.
?
i cixi ? K; ?j,
?
i aijxi ? zj ;
?i, xi ? {0, 1}; ?j, zj ? {0, 1},
782
where zj is 1 when ej is covered, 0 otherwise. No-
tice that this new problem is equivalent to the pre-
vious one.
Since not all the words are equally important,
we introduce weights wj on words ej . Then the
objective is restated as maximizing the weighted
sum
?
j wjzj such that the summary length is at
most K. This problem is called maximum cov-
erage problem with knapsack constraint (MCKP),
which is an NP-hard problem (Khuller et al,
1999). We should note that MCKP is different
from a knapsack problem. MCKP merely has a
constraint of knapsack form. Filatova and Hatzi-
vassiloglou (2004) pointed out that text summa-
rization can be formalized by MCKP.
The performance of the method depends on how
to represent words and which words to use. We
represent words with their stems. We use only
the words that are content words (nouns, verbs,
or adjectives) and not in the stopword list used in
ROUGE (Lin, 2004).
The weights wj of words are also an impor-
tant factor of good performance. We tested two
weighting schemes proposed by Yih et al (2007).
The first one is interpolated weights, which are in-
terpolated values of the generative word probabil-
ity in the entire document and that in the beginning
part of the document (namely, the first 100 words).
Each probability is estimated with the maximum
likelihood principle. The second one is trained
weights. These values are estimated by the logis-
tic regression trained on data instances, which are
labeled 1 if the word appears in a summary in the
training dataset, 0 otherwise. The feature set for
the logistic regression includes the frequency of
the word in the document cluster and the position
of the word instance and others.
4 Algorithms for solving MCKP
We explain how to solve MCKP. We first explain
the greedy algorithm applied to text summariza-
tion by Filatova and Hatzivassiloglou (2004). We
then introduce a greedy algorithm with perfor-
mance guarantee. This algorithm has never been
applied to text summarization. We next explain the
stack decoding used by Yih et al (2007). We then
introduce an approximate method based on linear
relaxation and a randomized algorithm, followed
by the branch-and-bound method, which provides
the exact solution.
Although the algorithms used in this paper
themselves are not novel, this work is the first
to apply the greedy algorithm with performance
guarantee, the randomized algorithm, and the
branch-and-bound to solve the MCKP and auto-
matically create a summary. In addition, we con-
duct a comparative study on summarization algo-
rithms including the above.
There are some other well-known methods for
similar problems (e.g., the method of conditional
probability (Hromkovic?, 2003)). A pipage ap-
proach (Ageev and Sviridenko, 2004) has been
proposed for MCKP, but we do not use this algo-
rithm, since it requires costly partial enumeration
and solutions to many linear relaxation problems.
As in the previous section, D denotes the set of
sentences {s1, ? ? ? , s|D|}, and S denotes a subset
of D and thus represents a summary.
4.1 Greedy algorithm
Filatova and Hatzivassiloglou (2004) used a
greedy algorithm. In this section, Wl denotes the
sum of the weights of the words covered by sen-
tence sl. W ?l denotes the sum of the weights of the
words covered by sl, but not by current summary
S. This algorithm sequentially selects sentence sl
with the largest W ?l .
Greedy Algorithm
U ? D, S ? ?
while U 6= ?
si ? argmaxsl?U W ?l
if ci +
?
sl?S cl ? K then insert si into S
delete si in U
end while
output S.
This algorithm has performance guarantee
when the problem has a unit cost (i.e., when each
sentence has the same length), but no performance
guarantee for the general case where costs can
have different values.
4.2 Greedy algorithm with performance
guarantee
We describe a greedy algorithm with performance
guarantee proposed by Khuller et al (1999), which
proves to achieve an approximation factor of (1 ?
1/e)/2 for MCKP. This algorithm sequentially se-
lects sentence sl with the largest ratio W ?l /cl. Af-
ter the sequential selection, the set of the selected
sentences is compared with the single-sentence
summary that has the largest value of the objec-
tive function. The larger of the two is going to
783
be the output of this new greedy algorithm. Here
score(S) is
?
j wjzj , the value of the objective
function for summary S.
Greedy Algorithm with Performance Guarantee
U ? D, S ? ?
while U 6= ?
si ? argmaxsl?U W ?l /cl
if ci +
?
sl?S cl ? K then insert si into S
delete si in U
end while
st ? argmaxsl Wl
if score(S) ? Wt, output S,
otherwise, output {st}.
They also proposed an algorithm with a better per-
formance guarantee, which is not used in this pa-
per because it is costly due to its partial enumera-
tion.
4.3 Stack decoding
Stack decoding is a decoding method proposed by
Jelinek (1969). This algorithm requires K priority
queues, k-th of which is the queue for summaries
of length k. The objective function value is used
for the priority measure. A new solution (sum-
mary) is generated by adding a sentence to a cur-
rent solution in k-th queue and inserted into a suc-
ceeding queue.1 The ?pop? operation in stack de-
coding pops the candidate summary with the least
priority in the queue. By restricting the size of
each queue to a certain constant stacksize, we can
obtain an approximate solution within a practical
computational time.
Stack Decoding
for k = 0 to K ? 1
for each S ? queues[k]
for each sl ? D
insert sl into S
insert S into queues[k + cl]
pop if queue-size exceeds the stacksize
end for
end for
end for
return the best solution in queues[K]
4.4 Randomized algorithm
Khuller et al (2006) proposed a randomized al-
gorithm (Hromkovic?, 2003) for MCKP. In this al-
gorithm, a relaxation linear problem is generated
by replacing the integer constraints xi ? {0, 1}
1We should be aware that stack in a strict data-structure
sense is not used in the algorithm.
and zj ? {0, 1} with linear constraints xi ? [0, 1]
and zj ? [0, 1]. The optimal solution x?i to the re-
laxation problem is regarded as the probability of
sentence si being selected as a part of summary:
x?i = P (xi = 1). The algorithm randomly se-
lects sentence si with probability x?i , in order to
generate a summary. It has been proved that the
expected length of each randomly-generated sum-
mary is upper-bounded by K, and the expected
value of the objective function is at least the op-
timal value multiplied by (1?1/e) (Khuller et al,
2006). This random generation of a summary is it-
erated many times, and the summaries that are not
longer than K are stored as candidate summaries.
Among those many candidate summaries, the one
with the highest value of the objective function is
going to be the output by this algorithm.
4.5 Branch-and-bound method
The branch-and-bound method (Hromkovic?,
2003) is an efficient method for finding the exact
solutions to integer problems. Since MCKP is an
NP-hard problem, it cannot generally be solved in
polynomial time under a reasonable assumption
that NP 6=P. However, if the size of the problem
is limited, sometimes we can obtain the exact
solution within a practical time by means of the
branch-and-bound method.
4.6 Weakly-constrained algorithms
In evaluation with ROUGE (Lin, 2004), sum-
maries are truncated to a target length K. Yih et
al. (2007) used a stack decoding with a slight mod-
ification, which allows the last sentence in a sum-
mary to be truncated to a target length. Let us call
this modified algorithm the weakly-constrained
stack decoding. The weakly-constrained stack de-
coding can be implemented simply by replacing
queues[k + cl] with queues[min(k + cl,K)]. We
can also think of weakly-constrained versions of
the greedy and randomized algorithms introduced
before.
In this paper, we do not adopt weakly-
constrained algorithms, because although an ad-
vantage of the extractive summarization is the
guaranteed grammaticality at the sentence level,
the summaries with a truncated sentence will relin-
quish this advantage. We mentioned the weakly-
constrained algorithms in order to explain the re-
lation between the proposed model and the model
proposed by Yih et al (2007).
784
5 Experiments and Discussion
5.1 Experimental Setting
We conducted experiments on the dataset of
DUC?04 (2004) with settings of task 2, which is
a multi-document summarization task. 50 docu-
ment clusters, each of which consists of 10 doc-
uments, are given. One summary is to be gen-
erated for each cluster. Following the most rel-
evant previous method (Yih et al, 2007), we set
the target length to 100 words. DUC?03 (2003)
dataset was used as the training dataset for trained
weights. All the documents were segmented
into sentences using a script distributed by DUC.
Words are stemmed by Porter?s stemmer (Porter,
1980). ROUGE version 1.5.5 (Lin, 2004) was
used for evaluation.2 Among others, we focus
on ROUGE-1 in the discussion of the result, be-
cause ROUGE-1 has proved to have strong corre-
lation with human annotation (Lin, 2004; Lin and
Hovy, 2003). Wilcoxon signed rank test for paired
samples with significance level 0.05 was used for
the significance test of the difference in ROUGE-
1. The simplex method and the branch-and-bound
method implemented in GLPK (Makhorin, 2006)
were used to solve respectively linear and integer
programming problems.
The methods that are compared here are the
greedy algorithm (greedy), the greedy algorithm
with performance guarantee (g-greedy), the ran-
domized algorithm (rand), the stack decoding
(stack), and the branch-and-bound method (exact).
5.2 Results
The experimental results are shown in Tables 1
and 2. The columns 1, 2, and SU4 in the ta-
bles respectively refer to ROUGE-1, ROUGE-2,
and ROUGE-SU4. In addition, rand100k refers to
the randomized algorithmwith 100,000 randomly-
generated solution candidates, and stack30 refers
to stack with the stacksize being 30. The right-
most column (?time?) shows the average computa-
tional time required for generating a summary for
a document cluster.
Both with interpolated (Table 1) and trained
weights (Table 2), g-greedy significantly outper-
formed greedy. With interpolated weights, there
was no significant difference between exact and
g-greedy, and between exact and stack30. With
trained weights, there was no significant differ-
2With options -n 4 -m -2 4 -u -f A -p 0.5 -l 100 -t 0 -d -s.
Table 1: ROUGE of MCKP with interpolated
weights. Underlined ROUGE-1 scores are signif-
icantly different from the score of exact. Compu-
tational time was measured in seconds.
ROUGE time
1 2 SU4 (sec)
greedy 0.283 0.083 0.123 <0.01
g-greedy 0.294 0.080 0.121 0.01
rand100k 0.300 0.079 0.119 1.88
stack30 0.304 0.078 0.120 4.53
exact 0.305 0.081 0.121 4.04
Table 2: ROUGE of MCKP with trained weights.
Underlined ROUGE-1 scores are significantly dif-
ferent from the score of exact. Computational time
was measured in seconds.
ROUGE time
1 2 SU4 (sec)
greedy 0.283 0.080 0.121 < 0.01
g-greedy 0.310 0.077 0.118 0.01
rand100k 0.299 0.077 0.117 1.93
stack30 0.309 0.080 0.120 4.23
exact 0.307 0.078 0.119 4.56
ence between exact and the other algorithms ex-
cept for greedy and rand100k. The result sug-
gests that approximate fast algorithms can yield
results comparable to the exact method in terms of
ROUGE-1 score. We will later discuss the results
in terms of objective function values and search
errors in Table 4.
We should notice that stack outperformed ex-
act with interpolated weights. To examine this
counter-intuitive point, we changed the stack-
size of stack with interpolated weights (inter) and
trained weights (train) from 10 to 100 and ob-
tained Table 3. This table shows that the ROUGE-
1 value does not increase as the stacksize does;
ROUGE-1 for stack with interpolated weights
does not change much with the stacksize, and the
peak of ROUGE-1 for trained weights is at the
stacksize of 20. Since stack with a larger stack-
size selects a solution from a larger number of so-
lution candidates, this result is counter-intuitive in
the sense that non-global decoding by stack has a
favorable effect.
We also counted the number of the document
clusters for which an approximate algorithm with
interpolated weights yielded the same solution as
785
Table 3: ROUGE of stack with various stacksizes
size 10 20 30 50 100
inter 0.304 0.304 0.304 0.304 0.303
train 0.308 0.310 0.309 0.308 0.307
Table 4: Search errors of MCKP with interpolated
weights
solution same search error
ROUGE (=) = ? ?
greedy 0 1 35 14
g-greedy 0 5 26 19
rand100k 6 5 25 14
stack30 16 11 8 11
exact (?same solution? column in Table 4). If
the approximate algorithm failed to yield the ex-
act solution (?search error? column), we checked
whether the search error made ROUGE score
unchanged (?=? column), decreased (??? col-
umn), or increased (??? column) compared with
ROUGE score of exact. Table 4 shows that (i)
stack30 is a better optimizer than other approx-
imate algorithms, (ii) when the search error oc-
curs, stack30 increases ROUGE-1 more often than
it decreases ROUGE-1 compared with exact in
spite of stack30?s inaccurate solution, (iii) ap-
proximate algorithms sometimes achieved better
ROUGE scores. We observed similar phenomena
for trained weights, though we skip the details due
to space limitation.
These observations on stacksize and search er-
rors suggest that there exists another maximization
problem that is more suitable to summarization.
We should attempt to find the more suitable maxi-
mization problem and solve it using some existing
optimization and approximation techniques.
6 Augmentation of the model
On the basis of the experimental results in the pre-
vious section, we augment our text summarization
model. We first examine the current model more
carefully. As mentioned before, we used words
as conceptual units because defining those units
is hard and still under development by many re-
searchers. Suppose here that a more suitable unit
has more detailed information, such as ?A did B
to C?. Then the event ?A did D to E? is a com-
pletely different unit from ?A did B to C?. How-
ever, when words are used as conceptual units, the
two events have a redundant part ?A?. It can hap-
pen that a document is concise as a summary, but
redundant on word level. By being to some extent
redundant on the word level, a summary can have
sentences that are more relevant to the document
cluster, as both of the sentences above are relevant
to the document cluster if the document cluster is
about ?A?. A summary with high cohesion and co-
herence would have redundancy to some extent. In
this section, we will use this conjecture to augment
our model.
6.1 Augmented summarization model
The objective function of MCKP consists of only
one term that corresponds to coverage. We add
another term
?
i(
?
j wjaij)xi that corresponds
to relevance to the topic of the document clus-
ter. We represent the relevance of sentence si by
the sum of the weights of words in the sentence
(
?
j wjaij). We take the summation of the rele-
vance values of the selected sentences:
max. (1? ?)
?
j wjzj + ?
?
i(
?
j wjaij)xi
s.t.
?
i cixi ? K; ?j,
?
i aijxi ? zj ;
?i, xi ? {0, 1}; ?j, zj ? {0, 1},
where ? is a constant. We call this model MCKP-
Rel, because the relevance to the document cluster
is taken into account.
We discuss the relation to the model proposed
by McDonald (2007), whose objective function
consists of a relevance term and a negative re-
dundancy term. We believe that MCKP-Rel is
more intuitive and suitable for summarization, be-
cause coverage in McDonald (2007) is measured
by subtracting the redundancy represented with
the sum of similarities between two sentences,
while MCKP-Rel focuses directly on coverage.
Suppose sentence s1 contains conceptual units A
and B, s2 contains A, and s3 contains B. The
proposed coverage-based methods can capture the
fact that s1 has the same information as {s2, s3},
while similarity-based methods only learn that s1
is somewhat similar to each of s2 and s3. We
also empirically showed that our method outper-
forms McDonald (2007)?s method in experiments
on DUC?02, where our method achieved 0.354
ROUGE-1 score with interpolated weights and
0.359 with trained weights when the optimal ? is
given, while McDonald (2007)?s method yielded
at most 0.348. However, this very point can also
786
Table 5: ROUGE-1 of MCKP-Rel with inter-
polated weights. The values in the parentheses
are the corresponding values of ? predicted using
DUC?03 as development data. Underlined are the
values that are significantly different from the cor-
responding values of MCKP.
interpolated trained
greedy 0.287 (0.1) 0.288 (0.8)
g-greedy 0.307 (0.3) 0.320 (0.4)
rand100k 0.310 (0.1) 0.316 (0.5)
stack30 0.324 (0.1) 0.327 (0.3)
exact 0.320 (0.3) 0.329 (0.5)
exactopt 0.327 (0.2) 0.329 (0.5)
be a drawback of our method, since our method
premises that a sentence is represented as a set
of conceptual units. Similarity-based methods are
free from such a premise. Taking advantages of
both models is left for future work.
The decoding algorithms introduced before are
also applicable toMCKP-Rel, becauseMCKP-Rel
can be reduced to MCKP by adding, for each sen-
tence si, a dummy conceptual unit which exists
only in si and has the weight
?
j wjaij .
6.2 Experiments of the augmented model
We ran greedy, g-greedy, rand100k, stack30
and exact to solve MCKP-Rel. We experimented
on DUC?04 with the same experimental setting as
the previous ones.
6.2.1 Experiments with the predicted ?
We determined the value of ? for each method us-
ing DUC?03 as development data. Specifically, we
conducted experiments on DUC?03 with different
? (? {0.0, 0.1, ? ? ? , 1.0}) and simply selected the
one with the highest ROUGE-1 value.
The results with these predicted ? are shown
in Table 5. Only ROUGE-1 values are shown.
Method exactopt is exact with the optimal ?, and
can be regarded as the upperbound of MCKP-Rel.
To evaluate the appropriateness of models without
regard to search quality, we first focused on exact
and found that MCKP-Rel outperformed MCKP
with exact. This means that MCKP-Rel model
is superior to MCKP model. Among the algo-
rithms, stack30 and exact performed well. All
methods except for greedy yielded significantly
better ROUGE values compared with the corre-
sponding results in Tables 1 and 2.
Figures 1 and 2 show ROUGE-1 for different
values of ?. The leftmost part (? = 0.0) cor-
responds to MCKP. We can see from the figures,
that MCKP-Rel at the best ? always outperforms
MCKP, and that MCKP-Rel tends to degrade for
very large ?. This means that excessive weight on
relevance has an adversative effect on performance
and therefore the coverage is important.
 0.28
 0.29
 0.3
 0.31
 0.32
 0.33
 0.34
 0  0.2  0.4  0.6  0.8  1
RO
UG
E-1
lambda
exact
stack30
rand100kg-greedygreedy
Figure 1: MCKP-Rel with interpolated weights
 0.28
 0.29
 0.3
 0.31
 0.32
 0.33
 0.34
 0  0.2  0.4  0.6  0.8  1
RO
UG
E-1
lambda
exact
stack30
rand100kg-greedygreedy
Figure 2: MCKP-Rel with trained weights
6.2.2 Experiments with the optimal ?
In the experiments above, we found that ? =
0.2 is the optimal value for exact with interpo-
lated weights. We suppose that this ? gives the
best model, and examined search errors as we
did in Section 5.2. We obtained Table 6, which
shows that search errors in MCKP-Rel counter-
intuitively increase (?) ROUGE-1 score less of-
ten than MCKP did in Table 4. This was the case
also for trained weights. This result suggests that
MCKP-Rel is more suitable to text summariza-
tion than MCKP is. However, exact with trained
weights at the optimal ?(= 0.4) in Figure 2 was
outperformed by stack30. It suggests that there is
still room for future improvement in the model.
787
Table 6: Search errors of MCKP-Rel with interpo-
lated weights (? = 0.2).
solution same search error
ROUGE (=) = ? ?
greedy 0 2 42 6
g-greedy 1 0 34 15
rand100k 3 6 33 8
stack30 14 13 14 10
6.2.3 Comparison with DUC results
In Section 6.2.1, we empirically showed that
the augmented model MCKP-Rel is better than
MCKP, whose optimization problem is used also
in one of the state-of-the-art methods by Yih et
al. (2007). It would also be beneficial to read-
ers to directly compare our method with DUC
results. For that purpose, we conducted experi-
ments with the cardinality constraint of DUC?04,
i.e., each summary should be 665 bytes long or
shorter. Other settings remained unchanged. We
compared the MCKP-Rel with peer65 (Conroy et
al., 2004) of DUC?04, which performed best in
terms of ROUGE-1 in the competition. Tables 7
and 8 are the ROUGE-1 scores, respectively eval-
uated without and with stopwords. The latter is the
official evaluation measure of DUC?04.
Table 7: ROUGE-1 of MCKP-Rel with byte con-
straints, evaluated without stopwords. Underlined
are the values significantly different from peer65.
interpolated train
greedy 0.289 (0.1) 0.284 (0.8)
g-greedy 0.297 (0.4) 0.323 (0.3)
rand100k 0.315 (0.2) 0.308 (0.4)
stack30 0.324 (0.2) 0.323 (0.3)
exact 0.325 (0.3) 0.326 (0.5)
exactopt 0.325 (0.3) 0.329 (0.4)
peer65 0.309
In Table 7, MCKP-Rel with stack30 and exact
yielded significantly better ROUGE-1 scores than
peer65. Although stack30 and exact yielded
greater ROUGE-1 scores than peer65 also in Ta-
ble 8, the difference was not significant. Only
greedy was significantly worse than peer65.3 One
3We actually succeeded in greatly improving the
ROUGE-1 value of MCKP-Rel evaluated with stopwords by
using all the words including stopwords as conceptual units.
However, we ignore those results in this paper, because it
Table 8: ROUGE-1 of MCKP-Rel with byte con-
straints, evaluated with stopwords. Underlined are
the values significantly different from peer65.
interpolated train
greedy 0.374 (0.1) 0.377 (0.4)
g-greedy 0.371 (0.0) 0.385 (0.2)
rand100k 0.373 (0.2) 0.366 (0.3)
stack30 0.384 (0.1) 0.386 (0.3)
exact 0.383 (0.3) 0.384 (0.4)
exactopt 0.385 (0.1) 0.384 (0.4)
peer65 0.382
possible explanation on the difference between Ta-
ble 7 and Table 8 is that peer65 would probably be
tuned to the evaluation with stopwords, since it is
the official setting of DUC?04.
From these results, we can conclude that the
MCKP-Rel is at least comparable to the best-
performing method, if we choose a powerful de-
coding method, such as stack and exact.
7 Conclusion
We regarded text summarization as MCKP. We
applied some algorithms to solve the MCKP and
conducted comparative experiments. We con-
ducted comparative experiments. We also aug-
mented our model to MCKP-Rel, which takes into
consideration the relevance to the document clus-
ter and performs well.
For future work, we will try other conceptual
units such as basic elements (Hovy et al, 2006)
proposed for summary evaluation. We also plan to
include compressed sentences into the set of can-
didate sentences to be selected as done by Yih et
al. (2007). We also plan to design other decod-
ing algorithms for text summarization (e.g., pipage
approach (Ageev and Sviridenko, 2004)). As dis-
cussed in Section 6.2, integration with similarity-
based models is worth consideration. We will in-
corporate techniques for arranging sentences into
an appropriate order, while the current work con-
cerns only selection. Deshpande et al (2007) pro-
posed a selection and ordering technique, which is
applicable only to the unit cost case such as selec-
tion and ordering of words for title generation. We
plan to refine their model so that it can be applied
to general text summarization.
just trickily uses non-content words to increase the evalua-
tion measure, disregarding the actual quality of summaries.
788
References
Alexander A. Ageev and Maxim Sviridenko. 2004. Pi-
page rounding: A new method of constructing algo-
rithms with proven performance guarantee. Journal
of Combinatorial Optimization, 8(3):307?328.
JohnM. Conroy, Judith D. Schlesinger, John Goldstein,
and Dianne P. O?Leary. 2004. Left-brain/right-brain
multi-document summarization. In Proceedings of
the Document Understanding Conference (DUC).
Pawan Deshpande, Regina Barzilay, and David Karger.
2007. Randomized decoding for selection-and-
ordering problems. In Proceedings of the Human
Language Technologies Conference and the North
American Chapter of the Association for Compu-
tational Linguistics Annual Meeting (HLT/NAACL),
pages 444?451.
DUC. 2003. Document Understanding Conference. In
HLT/NAACL Workshop on Text Summarization.
DUC. 2004. Document Understanding Conference. In
HLT/NAACL Workshop on Text Summarization.
Elena Filatova and Vasileios Hatzivassiloglou. 2004.
A formal model for information selection in multi-
sentence text extraction. In Proceedings of the
20th International Conference on Computational
Linguistics (COLING), pages 397?403.
Jade Goldstein, Vibhu Mittal, Jaime Carbonell, and
Mark Kantrowitz. 2000. Multi-document summa-
rization by sentence extraction. In Proceedings of
ANLP/NAACL Workshop on Automatic Summariza-
tion, pages 40?48.
Eduard Hovy, Chin-Yew Lin, Liang Zhou, and Ju-
nichi Fukumoto. 2006. Automated summarization
evaluation with basic elements. In Proceedings of
the Fifth International Conference on Language Re-
sources and Evaluation (LREC).
Juraj Hromkovic?. 2003. Algorithmics for Hard Prob-
lems. Springer.
Frederick Jelinek. 1969. Fast sequential decoding al-
gorithm using a stack. IBM Journal of Research and
Development, 13:675?685.
Samir Khuller, Anna Moss, and Joseph S. Naor. 1999.
The budgeted maximum coverage problem. Infor-
mation Processing Letters, 70(1):39?45.
Samir Khuller, Louiqa Raschid, and Yao Wu. 2006.
LP randomized rounding for maximum coverage
problem and minimum set cover with threshold
problem. Technical Report CS-TR-4805, The Uni-
versity of Maryland.
Chin-Yew Lin and Eduard Hovy. 2003. Auto-
matic evaluation of summaries using n-gram co-
occurrence statistics. In Proceedings of the 2003
Conference of the North American Chapter of the
Association for Computational Linguistics on Hu-
man Language Technology (HLT-NAACL?03), pages
71?78.
Chin-Yew Lin. 2004. ROUGE: a package for auto-
matic evaluation of summaries. In Proceedings of
the Workshop on Text Summarization Branches Out,
pages 74?81.
Andrew Makhorin, 2006. Reference Manual of GNU
Linear Programming Kit, version 4.9.
Inderjeet Mani. 2001. Automatic Summarization.
John Benjamins Publisher.
Ryan McDonald. 2007. A study of global inference al-
gorithms in multi-document summarization. In Pro-
ceedings of the 29th European Conference on Infor-
mation Retrieval (ECIR), pages 557?564.
Martin F. Porter. 1980. An algorithm for suffix strip-
ping. Program, 14(3):130?137.
Dragomir R. Radev, Hongyan Jing, Ma?gorzata Stys?,
and Daniel Tam. 2004. Centroid-based summariza-
tion of multiple documents. Information Processing
Management, 40(6):919?938.
Barry Schiffman, Ani Nenkova, and Kathleen McKe-
own. 2002. Experiments in multidocument sum-
marization. In Proceedings of the Second Interna-
tional Conference on Human Language Technology
Research, pages 52?58.
Dou Shen, Jian-Tao Sun, Hua Li, Qiang Yang, and
Zheng Chen. 2007. Document summarization us-
ing conditional random fields. In Proceedings of the
20th International Joint Conference on Artificial In-
telligence (IJCAI), pages 2862?2867.
Shiren Ye, Tat-Seng Chua, Min-Yen Kan, and Long
Qiu. 2007. Document concept lattice for text un-
derstanding and summarization. Information Pro-
cessing and Management, 43(6):1643?1662.
Wen-Tau Yih, Joshua Goodman, Lucy Vanderwende,
and Hisami Suzuki. 2007. Multi-document summa-
rization by maximizing informative content-words.
In Proceedings of the 20th International Joint Con-
ference on Artificial Intelligence (IJCAI), pages
1776?1782.
789
Proceedings of Human Language Technology Conference and Conference on Empirical Methods in Natural Language
Processing (HLT/EMNLP), pages 145?152, Vancouver, October 2005. c?2005 Association for Computational Linguistics
Kernel-based Approach for Automatic Evaluation of Natural Language
Generation Technologies: Application to Automatic Summarization
Tsutomu Hirao
NTT Communication Science Labs.
NTT Corp.
hirao@cslab.kecl.ntt.co.jp
Manabu Okumura
Precision and Intelligence Labs.
Tokyo Institute of Technology
oku@pi.titech.ac.jp
Hideki Isozaki
NTT Communication Science Labs.
NTT Corp.
isozaki@cslab.kecl.ntt.co.jp
Abstract
In order to promote the study of auto-
matic summarization and translation, we
need an accurate automatic evaluation
method that is close to human evalua-
tion. In this paper, we present an eval-
uation method that is based on convolu-
tion kernels that measure the similarities
between texts considering their substruc-
tures. We conducted an experiment us-
ing automatic summarization evaluation
data developed for Text Summarization
Challenge 3 (TSC-3). A comparison with
conventional techniques shows that our
method correlates more closely with hu-
man evaluations and is more robust.
1 Introduction
Automatic summarization, machine translation, and
paraphrasing have attracted much attention recently.
These tasks include text-to-text language genera-
tion. Evaluation workshops are held in the U.S.
and Japan, e.g., the Document Understanding Con-
ference (DUC)1, NIST Machine Translation Evalu-
ation2 as part of the TIDES project, the Text Sum-
marization Challenge (TSC)3 of the NTCIR project,
and the International Workshop on Spoken Lan-
guage Translation (IWSLT)4.
These evaluation workshops employ human eval-
uations, which are essential in terms of achieving
1http://duc.nist.gov
2http://www.nist.gov/speech/tests/mt/
3http://www.lr.titech.ac.jp/tsc
4http://www.slt.atr.co.jp/IWSLT2004
high quality evaluations results. However, human
evaluations require a huge effort and the cost is con-
siderable. Moreover, we cannot automatically eval-
uate a new system even if we use the corpora built
for these workshops, and we cannot conduct re-
evaluation experiments.
To cope with this situation, there is a particular
need to establish a high quality automatic evalua-
tion method. Once this is done, we can expect great
progress to be made on natural language generation.
In this paper, we propose a novel automatic
evaluation method for natural language generation
technologies. Our method is based on the Ex-
tended String Subsequence Kernel (ESK) (Hirao
et al, 2004b) which is a kind of convolution ker-
nel (Collins and Duffy, 2001). ESK allows us to
calculate the similarities between a pair of texts tak-
ing account of word sequences, their word sense se-
quences and their combinations.
We conducted an experimental evaluation using
automatic summarization evaluation data developed
for TSC-3 (Hirao et al, 2004a). The results of the
comparison with ROUGE-N (Lin and Hovy, 2003;
Lin, 2004a; Lin, 2004b), ROUGE-S(U) (Lin, 2004b;
Lin and Och, 2004) and ROUGE-L (Lin, 2004a;
Lin, 2004b) show that our method correlates more
closely with human evaluations and is more robust.
2 Related Work
Automatic evaluation methods for automatic sum-
marization and machine translation are grouped into
two classes. One is the longest common subse-
quence (LCS) based approach (Hori et al, 2003;
Lin, 2004a; Lin, 2004b; Lin and Och, 2004). The
other is the N-gram based approach (Papineni et al,
145
Table 1: Components of vectors corresponding to S1 and S2. Bold subsequences are common to S1 and S2.
 
subsequence S1 S2   subsequence S1 S2   subsequence S1 S2
Becoming 1 1 Becoming-is 



astronaut-DREAM 0 

DREAM 1 1 Becoming-my  astronaut-ambition 0 

SPACEMAN 1 1 SPACEMAN-DREAM 

astronaut-is 0 1
a 1 0 SPACEMAN-ambition 0 

astronaut-my 0 
ambition 0 1 SPACEMAN-dream   0 cosmonaut-DREAM   0
1
an 0 1 SPACEMAN-great 

0 cosmonaut-dream   0
astronaut 0 1 SPACEMAN-is 1 1 cosmonaut-great 

0
cosmonaut 1 0 SPACEMAN-my  cosmonaut-is 1 0
dream 1 0 a-DREAM 

0 cosmonaut-my  0
great 1 0 a-SPACEMAN 1 0 great-DREAM 1 0
is 1 1 2 a-cosmonaut 1 0 2 great-dream 1 0
my 1 1 a-dream 

0 is-DREAM 


Becoming-DREAM 

a-great   0 is-ambition 0 
Becoming-SPACEMAN  a-is  0 is-dream 

0
Becoming-a 1 0 a-my 

0 is-great  0
Becoming-ambition 0 

an-DREAM 0   is-my 1 1
2 Becoming-an 0 1 an-SPACEMAN 0 1 my-DREAM  1
Becoming-astronaut 0  an-ambition 0   my-ambition 0 1
Becoming-cosmonaut  0 an-astronaut 0 1 my-dream  0
Becoming-dream  0 an-is 0  my-great 1 0
Becoming-great 

0 an-my 0 

2002; Lin and Hovy, 2003; Lin, 2004a; Lin, 2004b;
Soricut and Brill, 2004).
Hori et. al (2003) proposed an automatic eval-
uation method for speech summarization based on
word recognition accuracy. They reported that their
method is superior to BLEU (Papineni et al, 2002)
in terms of the correlation between human assess-
ment and automatic evaluation. Lin (2004a; 2004b)
and Lin and Och (2004) proposed an LCS-based au-
tomatic evaluation measure called ROUGE-L. They
applied ROUGE-L to the evaluation of summariza-
tion and machine translation. The results showed
that the LCS-based measure is comparable to N-
gram-based automatic evaluation methods. How-
ever, these methods tend to be strongly influenced
by word order.
Various N-gram-based methods have been pro-
posed since BLEU, which is now widely used for the
evaluation of machine translation. Lin et al (2003)
proposed a recall-oriented measure, ROUGE-N,
whereas BLEU is precision-oriented. They reported
that ROUGE-N performed well as regards automatic
summarization. In particular, ROUGE-1, i.e., uni-
gram matching, provides the best correlation with
human evaluation. Soricut et. al (2004) proposed
a unified measure. They integrated a precision-
oriented measure with a recall-oriented measure by
using an extension of the harmonic mean formula. It
performs well in evaluations of machine translation,
automatic summarization, and question answering.
However, N-gram based methods have a critical
problem; they cannot consider co-occurrences with
gaps, although the LCS-based method can deal with
them. Therefore, Lin and Och (2004) introduced
skip-bigram statistics for the evaluation of machine
translation. However, they did not consider longer
skip-n-grams such as skip-trigrams. Moreover, their
method does not distinguish between bigrams and
skip-bigrams.
3 Kernel-based Automatic Evaluation
The above N-gram-based methods correlated
closely with human evaluations. However, we
think some skip-n-grams (n 	
 ) are useful. In this
paper, we employ the Extended String Subsequence
Kernel (ESK), which considers both n-grams and
skip-n-grams. In addition, the ESK allows us to add
word senses to each word. The use of word senses
enables flexible matching even when paraphrasing
is used.
The ESK is a kind of convolution kernel (Collins
and Duffy, 2001). Convolution kernels have recently
attracted attention as a novel similarity measure in
natural language processing.
3.1 ESK
The ESK is an extension of the String Subsequence
Kernel (SSK) (Lodhi et al, 2002) and the Word Se-
quence Kernel (WSK) (Cancedda et al, 2003).
The ESK receives two node sequences as inputs
146
and maps each of them into a high-dimensional vec-
tor space. The kernel?s value is simply the inner
product of the two vectors in the vector space. In
order to discount long-skip-n-grams, the decay pa-
rameter  is introduced.
We explain the computation of the ESK?s value
whose inputs are the sentences (S1 and S2) shown
below. In the example, word senses are shown in
braces.
S1 Becoming a cosmonaut:  SPACEMAN  is my great
dream:  DREAM 
S2 Becoming an astronaut:  SPACEMAN  is my ambi-
tion:  DREAM 
In this case, ?cosmonaut? and ?astronaut? share
the same sense  SPACEMAN  and ?ambition? and
?dream? also share the same sense  DREAM  . We
can use WordNet for English and Goitaikei (Ikehara
et al, 1997) for Japanese.
Table 1 shows the subsequences derived from S1
and S2 and its weights. Note that the subsequence
length is two or less. From the table, there are fif-
teen subsequences5 that are common to S1 and S2.
Therefore, ffCorpus-Based Analysis of Japanese Relative
Clause Constructions
Takeshi Abekawa1 and Manabu Okumura2
1 Interdisciplinary Graduate School of Science and Engineering,
Tokyo Institute of Technology, Japan
abekawa@lr.pi.titech.ac.jp
2 Precision and Intelligence Laboratory,
Tokyo Institute of Technology, Japan
oku@pi.titech.ac.jp
Abstract. Japanese relative clause constructions (RCC?s) are defined as
being the NP?s of structure ?S NP?, noting the lack of a relative pronoun
or any other explicit form of noun-clause demarcation. Japanese relative
clause modification should be classified into at least two major semantic
types: case-slot gapping and head restrictive. However, these types for
relative clause modification cannot apparently be distinguished. In this
paper we propose a method of identifying a RCC?s type with a machine
learning technique. The features used in our approach are not only rep-
resenting RCC?s characteristics, but also automatically obtained from
large corpora. The results we obtained from evaluation revealed that our
method outperformed the traditional case frame-based method, and the
features that we presented were effective in identifying RCC?s types.
1 Introduction
Japanese relative clause constructions (RCC?s) are defined as being the NP?s of
structure ?S NP?, noting the lack of a relative pronoun or any other explicit form
of noun-clause demarcation[1]. Japanese relative clause constructions should be
classified into at least two major semantic types: case-slot gapping and head
restrictive. However, these types for relative clause constructions cannot appar-
ently be distinguished.
Given the types of Japanese relative clause constructions and a corpus of
Japanese relative clause construction instances, we present a machine learning
based approach to classifying RCC?s. We present a set of lexical and semantic
features that characterize RCC?s, and integrate them as a classifier to determine
RCC types. We use decision tree learning as the machine learning algorithm.
Distinguishing case-slot gapping and head restrictive relative clauses, or re-
solving the semantic relationship between the relative clause and its head noun
has several application domains, such as machine translation from Japanese[5].
It also has a place in text understanding tasks, such as splitting a long sentence
into multiple shorter sentences, and removing less important clauses to shorten
a sentence[6].
R. Dale et al (Eds.): IJCNLP 2005, LNAI 3651, pp. 46?57, 2005.
c
? Springer-Verlag Berlin Heidelberg 2005
Corpus-Based Analysis of Japanese Relative Clause Constructions 47
Previously, relative clauses had been analyzed with rule-based methods that
utilized case frames[5,2]. Using hand-crafted rules and knowledge creates several
problems: the high cost of constructing them, and lower scalability and coverage.
Recently, due to the availability of very large corpora, corpus-based and ma-
chine learning-based approaches have been actively investigated[7]. Cooccurrence
information between nouns and verbs can be calculated from the syntactically
parsed corpus, and this information can be used preferentially instead of hand-
crafted case frames to determine whether a noun can be the filler of a case-slot
of a verb[7,11].
However, merely using the cooccurrence information between nouns and
verbs instead of case frames cannot provide a good solution to the analysis of
Japanese relative clauses. Clauses with high occurrence probability of the main
verb and the head noun can sometimes be head restrictive. Moreover, just be-
cause the head noun can be the filler of a case-slot of the verb does not always
mean that the clause as case-slot gapping. We have to rely on several differ-
ent clues in order to realize accurate classification. Therefore, in this paper we
present eight features are effective in classifying case-slot gapping and head re-
strictive relative clauses. Most of the features can be automatically acquired by
statistically analyzing a corpus as explained in section 4.
In section 2 we first describe the nature of Japanese RCC?s, and in section
3 we outline previous work on the analysis of Japanese relative clauses. In sec-
tion 4 we explain the features that we present in this paper, and in section 5
we explain the machine learning-based classifier, which uses the features in sec-
tion 4. In section 6 we describe the evaluation of the system and discuss the
experimental results.
2 Japanese Relative Clause Constructions
Japanese relative clause constructions have the structure ?S NP?, and constitute
a noun phrase as a whole. We will term the modifying S the ?relative clause?, the
modified NP the ?head NP?, and the overall NP a ?relative clause construction?
or RCC[2]. Example RCCs are:
RCC should be classified into at least two major semantic types: case-slot gap-
ping and head restrictive. With case-slot gapping RCC?s (also called ?inner?
(a)???? ?? ?
saury grill man
?the mani who ?i grills a saury?
(b)??? ????? ??
everyone know information
?the informationi which everyone knows ?i?
(c)???? ?? ??
saury grill smell
?the smell of saury grilled?
48 T. Abekawa and M. Okumura
relative clauses[14]), the head NP can be considered to have been gapped from a
case slot subcategorized by the main verb of the relative clause. Head restrictive
RCC?s (also called ?outer? relative clause[14]) occur when the relative clause mod-
ifies the head NP. In (a), the head NP ?? (man) can be the subject of the main verb
of the relative clause, and in (b), the head NP ?? (information) can be object of
the main verb. These RCC type are ?inner? relative clauses. In (c) the head NP ??
(smell) cannot fill the gap in the relative clause, and RCC type is ?outer?.
The inherent difficulty in determining the type of RCC derives from the fact
that these two types of RCC are syntactically identical. Even if the relative clause
has case-slot gapping, the type of that clause is not always ?inner?, because in
Japanese the main verb of the relative clause has often zero pronoun. We thus
have to disambiguate the individual RCC instances.
3 Related Work
Previous work on analyzing Japanese relative clauses has used case frames as
useful information. They have first tried to find the case frame for the main verb
of the relative clause and embedded the nouns in the clause into its case-slots.
The head noun is then tried to be embedded into the remaining case-slot in the
case frame. To determine whether a relative clause instance is ?outer? clause,
they have beforehand constructed a dictionary of the nouns that can be modi-
fied by ?outer? clause, such as ??(purpose), or ??(opinion). In one approach[5],
the instance is determined to be ?outer? clause, if the head noun is included
in the dictionary, regardless of the main verb of the relative clause. In another
approach[12], the instance is determined to be ?outer?, if the head noun cannot
be embedded into a case-slot and the head noun is included in the dictionary.
Recently, cooccurrence information between verbs and nouns has been used
in analysis. Kawahara and Kurohashi[7] automatically extracted case frames
from very large corpora, and used the case frames to analyze Japanese relative
clauses. However, they judged the instances as ?outer? clauses, only if case-slot
filling did not succeed.
Murata[11] presented a statistical method of classifying whether the relative
clause is an ?inner? or an ?outer? clause. However this method cannot correctly
classify ?outer? relative clause which had high cooccurrence probability of the
main verbs and the head nouns.
4 Feature Set to Classify RCC Type
In this section, we present eight features that can be considered to be effective
in classifying ?inner? and ?outer? relative clauses.
1. Degree of possibility where the head noun can be modified by the
?outer? relative clause (degree of ?outerness?).
In Japanese, there are two ways of modification between verbs and nouns: nouns
modify verbs by filling a case-slot (noun ? verb), and verbs modify nouns in
Corpus-Based Analysis of Japanese Relative Clause Constructions 49
Table 1. Comparison of the number of cooccurring verbs
relative clauses case-slots
noun freq. verb No. freq. verb No.
(intent) 8,732 941 14,216 677
(fact) 5,454 1,448 7,301 754
(preparation) 2,268 428 2,720 74
(people) 6,681 1,367 10,026 1,998
(city) 1,172 449 3,688 857
(television) 2,740 707 30,627 2,228
relative clauses (verb ? noun). Some pairs of a verb and a noun can cooccur
only in RCC, and cannot cooccur by filling a case-slot of the verb. For example,
Therefore, we can measure the likelihood that the noun will be modified by
?outer? relative clauses, by calculating the difference in the frequency distribution
of verbs cooccurring in relative clauses against the frequency distribution of verbs
cooccurring in the case-slot relation (If the difference is larger, the probability
that the noun can be modified by the ?outer? relative clause becomes larger).
We calculate the likelihood as J(Pk(v|n), Pm(v|n)), the Jensen-Shannon dis-
tance between the cooccurrence probability where nouns fill the case-slots of
verbs(Pk(v|n)) and the cooccurrence probability where verbs cooccur with nouns
in relative clauses(Pm(v|n)). Given two probability distributions p,q, the Jensen-
Shannon distance is defined by the following formula[9]:
J(p, q) =
1
2
[
D(p||p + q
2
) + D(q||p + q
2
)
]
. (1)
D(p||q) is the Kullback-Leibler distance and defined by the following formula[3]:
D(p||q) =
?
i
pi log
pi
qi
. (2)
noun ???? (preparation) and verb ???? (run) can cooccur with each other as
the main verb of a relative clause and its head noun, as in ?????? (prepara-
tion for running), though the noun cannot fill any case-slots of the verb, as in *?
?????? (*preparation runs). For nouns, some verbs only cooccur in relative
clauses, and a number of such verbs tend to be modified by ?outer? clauses.
Table 1 shows the occurrence frequency of sample nouns and the number of
their cooccurring verbs in the relative clauses or in the case-slot relations. For
nouns that do not tend to be modified by ?outer? clauses, such as ????(people),
???? (city), and ?????(television), the ratio between the frequency and the
number of verbs is almost the same between the relative clause and case-slot
cases. On the contrary, for nouns that tend to be modified by ?outer? clauses,
such as ????(intent), ???? (fact), and ????(preparation), the number of
verbs is much bigger in relation to clause cases, although the frequency is smaller.
The reason may be, as previously explained, that some verbs cooccur with the
nouns that tend to be modified by the ?outer? clause only in relative clause
constructions.
50 T. Abekawa and M. Okumura
Table 2. ?outerness? of example nouns
We use the Jensen-Shannon distance rather than the Kullback-Leibler distance,
because the former is symmetric and has stability in various sizes of probability
distribution experimentally. Pk(v|n) and Pm(v|n) are calculated as follows:
Pk(v|n) =
fk(n, v)
fk(n)
, (3)
Pm(v|n) =
fm(n, v)
fm(n)
, (4)
where fk(n, v) is the cooccurrence frequency where noun n fills a case-slots of
verb v, and fk(n) is the frequency of the noun that occurs in the case-slot
of verbs. Similarly, fm(n, v) and fm(n) are the frequencies for relative clause
constructions. Table 2 shows the ?outerness? of sample nouns. The values of the
nouns that are often modified by ?outer? clauses are higher than those of the
nouns which tend to be modified by ?inner? clauses.
2. Cooccurrence information between head noun and main verb of
relative clause.
For a relative clause instance to be an ?inner? clause, the head noun has to fill
a case-slot of the main verb of the relative clause. Consider the following two
examples:
Whether a noun can fill a case-slot of a verb has been traditionally determined
using case frames. However, we use the cooccurrence information between the
head noun and the main verb. In this paper, the cooccurrence between nouns and
verbs is measured by mutual information. Taking into account the information
on case particles, mutual information is calculated with the following formula:
I(n, k; v) = log
p(n, k, v)
p(n, k)p(v)
, (5)
noun ?? ?? ?? ?? ?? ???
(intent) (fact) (preparation) (people) (city) (television)
J(Pk, Pm) 0.546 0.360 0.616 0.160 0.155 0.159
(a) ???? ?
resonate sound
?the soundi that ?i resonates?
(b)???? ?
destruct sound
?the destruction sound?
In (a), ??? (sound) can be the subject (??? case) of the main verb ??????
(resonate). On the contrary, in (b) ??? cannot fill any case-slots of the main
verb ?????? (destruct) and can be considered to be modified by the ?outer?
relative clause. Therefore, if the head noun can fill a case-slot of the main verb,
the relation can be more plausibly assessed as ?inner?.
Corpus-Based Analysis of Japanese Relative Clause Constructions 51
3. Which case-slots are already filled for main verb by nouns in relative
clause.
As previously explained, if the head noun can fill the case-slot of the main verb of
the relative clause, the RCC instance can be judged as an ?inner? clause. However,
if the case-slot that the head noun can fill is already filled by the noun in the
relative clause, and hence unavailable for case-slot gapping, the rule cannot be
applied. Consider, for example, the following two cases:
Taking the situation into account, if a noun in the relative clause fills a case-
slot of the main verb, the mutual information for the case-slot is set to a very
small value Mmin.
4. Whether the head noun is modified by modifiers other than the
relative clause (other modifier).
Previous work on analyzing Japanese relative clauses has taking into account
only the head noun, and has not taking into account modifiers other than the
relative clause. Consider the following two examples:
where p(n, k) is the probability that noun n will cooccur with case particle k
and p(n, k, v) is the cooccurrence probability for noun n, case particle k and verb
v, and p(v) is the occurrence probability for verb v. The following seven case
particles were taken into account: (???,???,???,???,???,???, and ????).
This is because only these case-slots can, in fact, be gapped to the head noun to
construct the relative clause.
(a)????? ?
hear story
?the storyi that (someone) heard ?i?
(b)??? ????? ?
Japanese comic story hear story
?the story that (someone) heard a Japanese comic story?
In (a), since ??? (story) can fill the object (??? case) case-slot of the main verb
???? (hear), the relation can be judged as ?inner?. However, in (b), since the
object (??? case) case-slot of the main verb ???? is already filled by the noun
???? (Japanese comic story), and ??? cannot fill any case-slot, the instance
is judged as ?outer?.
(a)?? ?? ??
him talk purpose
?the purpose that (someone) talk (something) to him?
?the purposei that (someone) talk ?i to him?
(b)?? ?? ??? ??
him talk trip purpose
?the purpose of the trip i that (I) talk ?i to him?
(a) has two interpretations. The first interpretation is that ???? (purpose) do
not fill the remaining case-slots of the main verb ???? (talk) and can be con-
52 T. Abekawa and M. Okumura
If the head noun is modified by modifiers other than the relative clause, such
as adjectives, compound nouns, and ?AB?(B of A), the relative clause type tends
to be ?inner?. The function of ?outer? relative clause describes the content of the
head noun. If the head noun is modified by a modifier, the relative clause need
not describe it. Therefore, the type of relative clause tends to be ?inner?.
To implement the idea, we define a feature ?other modifier?. If the head
noun is modified by any modifiers other than the relative clause, its value is 1,
otherwise, 03.
5. Whether head noun tends to be modified
As for the nouns which tend to be modified by ?outer? relative clauses, the relative
clauses describe the content of the head nouns. It is difficult to understand their
meaning without any modification. Therefore we calculate the percentage to
what degree nouns are modified by any modifier in large corpora. Table 3 shows
the percentage for example nouns.
Table 3. Percentage of modification
3 In the experiment, we use syntactic annotated corpus. Therefore, other modifier
elements are already identified.
sidered to be modified by the ?outer? relative clause. The second interpretation
is that ???? can be the direct object(??? case) of the main verb ???? and
can be considered to be modified by the ?inner? relative clause. On the contrary,
(b) has only the interpretation of ?inner?.
?? ?? ??? ? Average of
(intention) (field) (television) (he) all nouns
0.983 0.973 0.287 0.155 0.460
The percentages of nouns ????(intention) and ????(field), which tend to
be modified by ?outer? relative clause, are close to 1, that is to say, such nouns
must have any modification. We consider, the higher this percentage, the higher
the possibility that the noun is modified by ?outer? relative clause.
6. Percentage where ????? is inserted between relative clause and
head nouns
????? is a function expression that is sometimes inserted between relative
clauses and head nouns. Table 4 shows the percentage where ????? cooccurs
with example nouns.
?? ? ?? ?? Average of
(opinion) (rumor) (place) (people) all nouns
0.335 0.246 0.007 0.008 0.007
Table 4. The percentage of ????? cooccurring with noun
Corpus-Based Analysis of Japanese Relative Clause Constructions 53
7. Whether head noun tends to be modified by past-tensed relative
clauses(tense information)
Table 5. Tense of main verb and distribution of inner/outer
To implement this idea, we first calculated deviations in the distribution of
tense for the relative clauses. The percentage of past-tense main verbs in all
relative clauses, Rpast, and the average for all the nouns were calculated. Table
6 shows the results for sample nouns.
Table 6. Percentage of past-tense main verbs
4 In Japanese, there are just two tense surface markers: present and past. Therefore,
future tense is indicated by the present tense on the surface.
The percentages of nouns ????(opinion) and ???(rumor), which tend to
be modified by ?outer? relative clause, are higher than the average. We consider,
the higher this percentage, the higher possibility that the noun is modified by
?outer? relative clause.
Some nouns tend to be modified by past-tense relative clauses, and others
tend to be modified by those in the present tense. Consider, for example, the
following two nouns: ????(plan) and ???? (memory). Both nouns are con-
sidered to imply the concept of time (future or past) 4 .
?? ??
(plan) (memory)
tense inner outer inner outer
present 6 89 12 0
past 5 0 5 83
For each of the two nouns ????(plan) and ????(memory), we examined
100 relative clause instances that had the noun as the head noun (Table 5).If
the head noun implies the concept of time, the tense of the main verb of the
relative clause tends to coincide with this concept. Furthermore, note that the
tense of the main verb of ?outer? relative clauses is the same as the time concept
of the head noun. From this, if the noun tends to be modified by a specific-tense
relative clause, the relative clause tends to be ?outer?, and if the tense of the
main verb contradicts the time concept of the head noun (tense of frequently
modified relative clauses), the relative clause should be determined as ?inner?.
?? ?? ?? ?? Average of
(plan) (memory) (place) (people) all nouns
0.032 0.958 0.333 0.422 0.322
For a head noun which does not imply the concept of time (???? (place) and
????(people)), the percentage is near average. On the contrary, ????(plan)
and ????(memory) which imply the concept of time have an extreme value.
54 T. Abekawa and M. Okumura
Taking into account the actual tense of the relative clause instances, we
calculated the following score:
Vpast
{
Rpast ? AV Gpast in case of present tense
AV Gpast ? Rpast in case of past tense
(6)
For a head noun not implying the concept of time, in either tense of the main
verb, the score is rather low, and a decision on inner/outer might not be af-
fected by the score. For a head noun implying the concept of time, the ab-
solute value of the score is rather large, and if the tense of the main verb is
the same as the time concept, the score becomes negative; otherwise the score
becomes positive.
8. Whether main verb has a sense of ?exclusion?
The last feature is for identifying exceptional ?outer? relative clause. Consider
the following two examples:
5 Machine Learning Based Classifier for RCC Type
We integrated the eight features in described the last section and used the ma-
chine learning approach to determine the RCC type. We used C5.0[13] as the
machine learning algorithm.
C5.0 is a decision-tree based classification system that has been used in nat-
ural language processing, such as text classification, chunking, text summariza-
tion, and ellipsis resolution[10]. C5.0 takes a set of training instances with a
feature vector and correct type as input, and induces a classifier which charac-
terizes the given feature space.
Since we use only eight features, we think even the state of the art machine
learning method like SVM would yield almost the same accuracy as decision-tree.
Furthermore decision-tree are more easily interpreted by human than SVMs.
(a)??? ?? ?????
Japan except Asian countries
?Asian countries except Japan?
(b)???? ??? ??
injured people except passenger
?the passenger except injured people?
These examples are ?outer? relative clauses, and this RCC type is identified by
the main verb which has sense of exclusion. There are a few verbs which indicate
the RCC type by itself. Therefore, we defined a feature ?excluding verb?. If the
main verb contains a character ??? (which has sense of exclusion), the feature
is set to 1, otherwise, 0.
Corpus-Based Analysis of Japanese Relative Clause Constructions 55
6 Evaluation
6.1 Experiment
Cooccurrence and other statistical information used in this work were calculated
from the corpus of a collection of twenty-two years of newspaper articles. The
corpus was parsed with KNP[8], which is a rule-based Japanese syntactic parser.
The cooccurrence information we obtained was as follows: the number of fk(n, v)
was about 60.8 million, and the number of fm(n, v) was about 12.4 million.
The data used in the evaluation was a set of RCC instances randomly ex-
tracted from the EDR corpus[4] which had syntactically analyzed. Then, a label,
whether the relative clause is ?inner? or ?outer?, was manually annotated. The
statistics on the data are shown in Table 7. Evaluation with C5.0 was carried
out by way of 5-fold cross validation.
Table 7. Statistics on evaluation data
Total Inner Outer
749 580 169
Table 8. Experimental results
Inner Outer
accuracy precision recall precision recall
Baseline 0.774 0.774 1.000 - -
Cooccurrence information only 0.787 0.836 0.906 0.520 0.366
Case frame 0.830 0.868 0.921 0.657 0.521
Our approach 0.902 0.931 0.942 0.794 0.762
Fig. 1. Generated decision tree
...excluding verb = 1: outer(exceptinal type) (22/2)
:.excluding verb = 0:
:..outerness <= 0.212: inner (444/6)
outerness > 0.212:
:..other modifier = 1: inner (84/17)
other modifier = 0:
:..cooccurrence("?" case) > -9.10: inner (28/4)
cooccurrence("?" case) <= -9.10:
:..percentage of "???" > 0.027: outer (105/14)
percentage of "???" <= 0.027:
:..percentage of modified <= 0.735: inner (25/2)
percentage of modified > 0.735:
:..cooccurrence("?" case) <= -13.1:outer (31/5)
cooccurrence("?" case) > -13.1:inner (10/2)
56 T. Abekawa and M. Okumura
The baseline we used determines all instances as ?inner? relative clauses. We
also compared our approach with the traditional method with case frames, and
a method that uses only cooccurrence information (features 2 and 3 in section 4.
An evaluation measure is an accuracy, which is defined as the number of correctly
identified RCCs divided by the number of all RCCs. And for inner/outer relative
clauses, precision and recall are calculated.
Precision =
#number of correctly identified relative clauses
#number of inner/outer attempted by system
Recall =
#number of correctly identified relative clauses
#number of inner/outer relative clauses
The results are shown in Table 8. The generated decision tree from all instances
is shown in Figure 1. The last values on each line, for example ?22/2? and ?444/6?,
indicated ?number of applied examples / number of misclassification?.
6.2 Discussion
Accuracy of our approach is higher than that of the traditional approach. Our
approach works well especially for identifying ?outer? relative clause. Further-
more, using only cooccurrence information could not yield better performance
for ?outer? relative clause. Therefore, we conclude that the features in our ap-
proach can effectively identify the ?outer? relative clause.
Figure 1 shows that the most contributive feature except ?excluding verb?
is the degree of ?outerness?. This feature can classify many instances with high
accuracy (98.6%=438/444). If the degree of ?outerness? is smaller than certain
threshold, RCC type is ?inner? with high probability.
The second contributing feature is the ?other modifier?. If modifiers other
than the relative clause exist, RCC type is ?inner?. However, the accuracy of this
feature is not so good compared with other features.
We unfortunately could not find the ?tense information? in our decision tree.
We consider the reason to be that nouns which imply the concept of time are
very few, and there might be no instances which contain them.
7 Conclusions
In this paper, we presented eight lexical and semantic features that characterized
RCC, and we integrated them using machine learning approach to determine the
RCC type.
Evaluation proved that our approach outperformed the traditional case
frame-based method, and the features that we presented were effective in classi-
fying types into ?inner? and ?outer? relative clauses.
After identification of ?inner? clauses, case identification will be necessary for
semantic analysis. This will be considered in future work.
Corpus-Based Analysis of Japanese Relative Clause Constructions 57
References
1. Baldwin, T., Tokunaga, T. and Tanaka, H.: The parameter-based analysis of
Japanese relative clause constructions. In IPSJ SIGNote on Natural Language 134-
8 (1999) 55-62
2. Baldwin, T.: Making Sense of Japanese Relative Clause Constructions. In Proceed-
ings of the Second Workshop on Text Meaning and Interpretation (2004) 49-56.
3. Dagan, I., Lee, L. and Pereira, F.: Similarity-based models of word cooccurrence
probabilities. Machine Learning 34 (1999) 65-81
4. EDR.: EDR electronic dictionary technical guide. Technical Report TR045,
Japanese Electronic Dictionary Research Institute Ltd (1995)
5. Ikehara, S., Shirai, S., Yokoo, A. and Nakaiwa, H.: Toward an MT system with-
out pre-editing effect of new methods in ALT-J/E . In Proceedings of the Third
Machine Translation Summit (1991)
6. Ishizako, T., Kataoka, A., Masuyama, S., Yamamoto, K. and Nakagawa, S.: Re-
duction of overlapping expressions using dependency relations. Natural Language
Processing 7(4) (2000) 119-142. (in Japanese)
7. Kawahara, D. and Kurohashi, S.: Fertilization of case frame dictionary for robust
Japanese case analysis. In Proceedings of the 19th International Conference on
Computational Linguistics (2002) 425-431
8. Kurohashi, S. and Nagao, M.: Kn parser: Japanese dependency/case structure ana-
lyzer. In Proceeding of the International Workshop on Sharable Natural Language
Resources (1994) 48-55
9. Lin, J.: Divergence measures based on the shannon entropy. IEEE TRANSAC-
TIONS ON INFORMATION THEORY. 37(1) (1991) 145-151
10. Manning, C. and Schutze, H.: Foundations of Statistical Natural Language Pro-
cessing. MIT Press (1999)
11. Murata, M.: Extraction of negative examples based on positive examples automatic
detection of mis-spelled Japanese expressions and relative clauses that do not have
case relations with their heads . In IPSJ SIGNote on Natural Language 144-15
(2001) 105-112. (in Japanese)
12. Narita, H.: Parsing Japanese clauses modifying nominals. In IPSJ SIGNote on
Natural Language 99-11 (1994) 79-86. (in Japanese)
13. Quinlan, J.: C4.5: Programs for Machine Learning. Morgan Kaufmann (1993)
14. Teramura, H.: Rentai-shuushoku no shintakusu to imi. No.1-4. Nihongo Nihon-
bunka 4-7 (1975-1978) (in Japanese)
Classification of Multiple-Sentence Questions
Akihiro Tamura, Hiroya Takamura, and Manabu Okumura
Precision and Intelligence Laboratory,
Tokyo Institute of Technology, Japan
aki@lr.pi.titech.ac.jp
{takamura, oku}@pi.titech.ac.jp
Abstract. Conventional QA systems cannot answer to the questions
composed of two or more sentences. Therefore, we aim to construct a
QA system that can answer such multiple-sentence questions. As the first
stage, we propose a method for classifying multiple-sentence questions
into question types. Specifically, we first extract the core sentence from
a given question text. We use the core sentence and its question focus in
question classification. The result of experiments shows that the proposed
method improves F-measure by 8.8% and accuracy by 4.4%.
1 Introduction
Question-Answering (QA) systems are useful in that QA systems return the
answer itself, while most information retrieval systems return documents that
may contain the answer.
QA systems have been evaluated at TREC QA-Track1 in U.S. and QAC
(Question & Answering Challenge)2 in Japan. In these workshops, the inputs
to systems are only single-sentence questions, which are defined as the ques-
tions composed of one sentence. On the other hand, on the web there are a
lot of multiple-sentence questions (e.g., answer bank3, AskAnOwner4), which
are defined as the questions composed of two or more sentences: For example,
?My computer reboots as soon as it gets started. OS is Windows XP. Is there
any homepage that tells why it happens??. For conventional QA systems, these
questions are not expected and existing techniques are not applicable or work
poorly to these questions. Therefore, constructing QA systems that can handle
multiple-sentence questions is desirable.
An usual QA system is composed of three components: question process-
ing, document retrieval, and answer extraction. In question processing, a given
question is analyzed, and its question type is determined. This process is called
?question classification?. Depending on the question type, the process in the an-
swer extraction component usually changes. Consequently, the accuracy and the
efficiency of answer extraction depend on the accuracy of question classification.
1 http://trec.nist.gov/tracks.htm
2 http://www.nlp.is.ritsumei.ac.jp/qac/
3 http://www.theanswerbank.co.uk/
4 http://www.askanowner.com/
R. Dale et al (Eds.): IJCNLP 2005, LNAI 3651, pp. 426?437, 2005.
c
? Springer-Verlag Berlin Heidelberg 2005
Classification of Multiple-Sentence Questions 427
Therefore, as a first step towards developing a QA system that can han-
dle multiple-sentence questions, we propose a method for classifying multiple-
sentence questions. Specifically, in this work, we treat only questions which re-
quire one answer. For example, if the question ?The icon to return to desktop has
been deleted. Please tell me how to recover it.? is given, we would like ?WAY?
to be selected as the question type. We thus introduce core sentence extraction
component, which extracts the most important sentence for question classifica-
tion. This is because there are unnecessary sentences for question classification
in a multiple-sentence question, and we hope noisy features should be eliminated
before question classification with the component. If a multiple-sentence question
is given, we first extract the most important sentence for question classification
and then classify the question using the only information in the sentence.
In Section 2, we present the related work. In Section 3, we explain our pro-
posed method. In Section 4, we describe our experiments and results, where we
can confirm the effectiveness of the proposed method. Finally, in Section 5, we
describe the summary of this paper and the future work.
2 Related Work
This section presents some existing methods for question classification. The
methods are roughly divided into two groups: the ones based on hand-crafted
rules and the ones based on machine learning. The system ?SAIQA? [1], Xu et al
[2] used hand-crafted rules for question classification. However, methods based
on pattern matching have the following two drawbacks: high cost of making rules
or patterns by hand and low coverage.
Machine learning can be considered to solve these problems. Li et al [3] used
SNoW for question classification. The SNoW is a multi-class classifier that is
specifically tailored for learning in the presence of a very large number of fea-
tures. Zukerman et al [4] used decision tree. Ittycheriah et al [5] used maximum
entropy. Suzuki [6] used Support Vector Machines (SVMs). Suzuki [6] compared
question classification using machine learning methods (decision tree, maximum
entropy, SVM) with a rule-based method. The result showed that the accuracy
of question classification with SVM is the highest of all. According to Suzuki [6],
a lot of information is needed to improve the accuracy of question classification
and SVM is suitable for question classification, because SVM can classify ques-
tions with high accuracy even when the dimension of the feature space is large.
Moreover, Zhang et al [7] compared question classification with five machine
learning algorithms and showed that SVM outperforms the other four methods
as Suzuki [6] showed. Therefore, we also use SVM in classifying questions, as we
will explain later.
However, please note that we treat not only usual single-sentence questions,
but also multiple-sentence questions. Furthermore, our work differs from previous
work in that we treat real data on the web, not artificial data prepared for the
QA task. From these points, the results in this paper cannot be compared with
the ones in the previous work.
428 A.Tamura, H. Takamura, and M. Okumura
3 Two-Step Approach to Multiple-Sentence Question
Classification
This section describes our method for classifying multiple-sentence questions.
We first explain the entire flow of our question classification. Figure 1 shows the
proposed method.
question
preprocessing
core sentence 
extraction component 
question classification
component
a core sentence
single-sentence 
question
multiple-sentence 
question
question type
peculiar process to 
multiple-sentence questions
Fig. 1. The entire flow of question classification
An input question consisting of possibly multiple sentences is first prepro-
cessed. Parentheses parts are excluded in order to avoid errors in syntactic pars-
ing. The question is divided into sentences by punctuation marks.
The next process changes depending on whether the given question is a single-
sentence question or a multiple-sentence question. If the question consists of a
single sentence, the question is sent directly to question classification component.
If the question consists of multiple sentences, the question is sent to core sentence
extraction component. In the component, a core sentence, which is defined as
the most important sentence for question classification, is extracted. Then, the
core sentence is sent to the question classification component and the question is
classified using the information in the core sentence. In Figure 1, ?core sentence
extraction? is peculiar to multiple-sentence questions.
3.1 Core Sentence Extraction
When a multiple-sentence question is given, the core sentence of the question is
extracted. For example, if the question ?I have studied the US history. Therefore,
I am looking for the web page that tells me what day Independence Day is.? is
given, the sentence ?Therefore, I am looking for the web page that tells me what
day Independence Day is.? is extracted as the core sentence.
With the core sentence extraction, we can eliminate noisy information before
question classification. In the above example, the occurrence of the sentence
Classification of Multiple-Sentence Questions 429
?I have studied the US history.? would be a misleading information in terms of
question classification.
Here, we have based our work on the following assumption: a multiple-
sentence question can be classified using only the core sentence. Please note
that we treat only questions which require one answer.
We explain the method for extracting a core sentence. Suppose we have a
classifier, which returns Score(Si) for each sentence Si of Question. Question is
the set of sentences composing a given question. Score(Si) indicates the likeliness
of Si being the core sentence. The sentence with the largest value is selected as
the core sentence:
Core sentence = argmaxSi?QuestionScore(Si). (1)
We then extract features for constructing a classifier which returns Score(Si).
We use the information on the words as features. Only the features from the
target sentence would not be enough for accurate classification. This issue is
exemplified by the following questions (core sentences are underlined).
? Question 1:
Please advise a medication effective for hay fever. I want to relieve my
headache and stuffy nose. Especially my headache is severe.
? Question 2:
I want to relieve my headache and stuffy nose. Especially my head-
ache is severe.
While the sentence ?I want to relieve my headache and stuffy nose.? written in
bold-faced type is the core sentence in Question 2, the sentence is not suitable as
the core sentence in Question 1. These examples show that the target sentence
alone is sometimes not a sufficient evidence for core sentence extraction.
Thus, in classification of a sentence, we use its preceding and following sen-
tences. For that purpose, we introduce a notion of window size. ?Window size is
n? means ?the preceding n sentences and the following n sentences in addition to
the target sentence are used to make a feature vector?. For example, if window
size is 0, we use only the target sentence. If window size is ?, we use all the
sentences in the question.
We use SVM as a classifier. We regard the functional distance from the
separating hyperplane (i.e., the output of the separating function) as Score(Si).
Word unigrams and word bigrams of the target sentence and the sentences in
the window are used as features. A word in the target sentence and the same
word in the other sentences are regarded as two different features.
3.2 Question Classification
As discussed in Section 2, we use SVM in the classification of questions. We use
five sets of features: word unigrams, word bigrams, semantic categories of nouns,
question focuses, and semantic categories of question focuses. The semantic cat-
egories are obtained from a thesaurus (e.g., SHOP, STATION, CITY).
430 A.Tamura, H. Takamura, and M. Okumura
?Question focus? is the word that determines the answer class of the ques-
tion. The notion of question focus was described by Moldovan et al [8]. For
instance, in the question ?What country is ???, the question focus is ?coun-
try?. In many researches, question focuses are extracted with hand-crafted rules.
However, since we treat all kinds of questions including the questions which are
not in an interrogative form, such as ?Please teach me ?? and ?I don?t know ??,
it is difficult to manually create a comprehensive set of rules. Therefore, in this
paper, we automatically find the question focus in a core sentence according to
the following steps :
step 1 find the phrase5 including the last verb of the sentence or the phrase
with ??? at the end.
step 2 find the phrase that modifies the phrase found in step 1.
step 3 output the nouns and the unknown words in the phrase found in step 2.
The output of this procedure is regarded as a question focus. Although this
procedure itself is specific to Japanese, we suppose that we can extract question
focus for other languages with a similar simple procedure.
4 Experiments
We designed experiments to confirm the effectiveness of the proposed method.
In the experiments, we use data in Japanese. We use a package for SVM
computation, TinySVM 6, and a Japanese morphological analyzer, ChaSen 7 for
word segmentation of Japanese text. We use CaboCha 8 to obtain dependency
relations, when a question focus is extracted from a question. Semantic categories
are obtained from a thesaurus ?Goitaikei? [9].
4.1 Experimental Settings
We collect questions from two Japanese Q&A sites: hatena9 and
Yahoo!tiebukuro10. 2000 questions are extracted from each site and experimental
data consist of 4000 questions in total. A Q&A site is the site where a user puts a
question on the site and other users answer the question on the site. Such Q&A
sites include many multiple-sentence questions in various forms. Therefore, those
questions are appropriate for our experiments where non-artificial questions are
required.
Here, we manually exclude the following three kinds of questions from the
dataset: questions whose answers are only Yes or No, questions which require two
5 Phrase here is actually Japanese bunsetsu phrase, which is the smallest meaningful
sequence consisting of an independent word and accompanying words.
6 http://chasen.org/?taku/software/TinySVM/
7 http://chasen.naist.jp/hiki/ChaSen/
8 http://chasen.org/?taku/software/cabocha/
9 http://www.hatena.ne.jp/
10 http://knowledge.yahoo.co.jp/
Classification of Multiple-Sentence Questions 431
Table 1. The types and the distribution of 2376 questions
Nominal Answer Non-nominal Answer
Question Type Number Question Type Number
PERSON 64 REASON 132
PRODUCT 238 WAY 500
FACILITY 139 DEFINITION 73
LOCATION 393 DESCRIPTION 228
TIME 108 OPINION 173
NUMBER 53 OTHERS (TEXT) 131
OTHERS (NOUN) 144
1139 1237
TOTAL 2376
or more answers, and questions which are not actually questions. This deletion
left us 2376 questions. The question types that we used and their numbers are
shown in Table 111. Question types requiring nominal answers are determined
referring to the categories used by Sasaki et al [1].
Of the 2376 questions, 818 are single-sentence questions and 1558 are
multiple-sentence questions. The average number of sentences in a multiple-
sentence question is 3.49. Therefore, the task of core sentence extraction in our
setting is to decide a core sentence from 3.49 sentences on the average. As an eval-
uation measure for core sentence extraction, we use accuracy, which is defined
as the number of multiple-sentence questions whose core sentence is correctly
identified over the number of all the multiple-sentence questions. To calculate
the accuracy, correct core sentence of the 2376 questions is manually tagged in
the preparation of the experiments.
As an evaluation measure for question classification, we use F-measure, which
is defined as 2?Recall?Precision / (Recall+Precision). As another evaluation
measure for question classification, we use also accuracy, which is defined as the
number of questions whose type is correctly classified over the number of the
questions. All experimental results are obtained with two-fold cross-validation.
4.2 Core Sentence Extraction
We conduct experiments of core sentence extraction with four different window
sizes (0, 1, 2, and ?) and three different feature sets (unigram, bigram, and
unigram+bigram). Table 2 shows the result.
As this result shows, we obtained a high accuracy, more than 90% for this
task. The accuracy is so good that we can use this result for the succeeding task
of question classification, which is our main target. This result also shows that
large widow sizes are better for core sentence extraction. This shows that good
clues for core sentence extraction are scattered all over the question.
11 Although Sasaki et al [1] includes ORGANIZATION in question types, ORGA-
NIZATION is integrated into OTHERS (NOUN) in our work because the size of
ORGANIZATION is small.
432 A.Tamura, H. Takamura, and M. Okumura
Table 2. Accuracy of core sentence extraction with different window sizes and features
Window Size\ Features Unigram Bigram Unigram+Bigram
0 1350/1558= 0.866 1378/1558= 0.884 1385/1558= 0.889
1 1357/1558= 0.871 1386/1558= 0.890 1396/1558= 0.896
2 1364/1558= 0.875 1397/1558= 0.897 1405/1558= 0.902
? 1376/1558= 0.883 1407/1558= 0.903 1416/1558= 0.909
Table 3. Accuracy of core sentence extraction with simple methodologies
Methodology Accuracy
First Sentence 743/1558= 0.477
Last Sentence 471/1558= 0.302
Interrogative Sentence 1077/1558= 0.691
The result in Table 2 also shows that unigram+bigram features are most
effective for any window size in core sentence extraction.
To confirm the validity of our proposed method, we extract core sentences
with three simple methodologies, which respectively extract one of the following
sentences as the core sentence : (1) the first sentence, (2) the last sentence,
and (3) the last interrogative sentence (or the first sentence). Table 3 shows the
result. The result shows that such simple methodologies would not work in core
sentence extraction.
4.3 Question Classification: The Effectiveness of Core Sentence
Extraction
We conduct experiments to examine whether the core sentence extraction is
effective for question classification or not. For that purpose, we construct the
following three models:
Plain question. The given question is the input of question classification com-
ponent without core sentence extraction process.
Predicted core sentence. The core sentence extracted by the proposed
method in Section 3.1 is the input of question classification component. The
accuracy of core sentence extraction process is 90.9% as mentioned in Sec-
tion 4.2.
Correct core sentence. The correct core sentence tagged by hand is the input
of question classification component. This case corresponds to the case when
the accuracy of core sentence extraction process is 100%.
Word unigrams, word bigrams, and semantic categories of nouns are used as
features. The features concerning question focus cannot be used for the plain
question model, because the method for identifying the question focus requires
that the input be one sentence. Therefore, in order to clarify the effectiveness of
core sentence extraction itself, through fair comparison we do not use question
focus for each of the three models in these experiments.
Classification of Multiple-Sentence Questions 433
Table 4. F-measure and Accuracy of the three models for question classification
Model Plain Question Predicted Core Sentence Correct Core Sentence
Accuracy Of
Core Sentence Extraction ? 0.909 1.000
PERSON 0.462 0.434 0.505
PRODUCT 0.381 0.467 0.480
FACILITY 0.584 0.569 0.586
LOCATION 0.758 0.780 0.824
TIME 0.340 0.508 0.524
NUMBER 0.262 0.442 0.421
OTHERS (NOUN) 0.049 0.144 0.145
REASON 0.280 0.539 0.579
WAY 0.756 0.778 0.798
DEFINITION 0.643 0.624 0.656
DESCRIPTION 0.296 0.315 0.317
OPINION 0.591 0.675 0.659
OTHERS (TEXT) 0.090 0.179 0.186
Average 0.423 0.496 0.514
Accuracy 0.617 0.621 0.652
Table 4 shows the result. For most question types, the proposed method
with a predicted core sentence improves F-measure. This result shows that the
core sentence extraction is effective in question classification. We can still expect
some more improvement of performance, by boosting accuracy of core sentence
extraction.
In order to further clarify the importance of core sentence extraction, we
examine the accuracy for the questions whose core sentences are not correctly
extracted. Of 142 such questions, 54 questions are correctly classified. In short,
the accuracy is 38% and very low. Therefore, we can claim that without accurate
core sentence extraction, accurate question classification is quite hard.
4.4 Question Classification: More Detailed Investigation of Features
Here we investigate the effectiveness of each set of features and the influence
of the preceding and the following sentences of the core sentence. After that,
we conduct concluding experiments. In the first two experiments of this section,
we use only the correct core sentence tagged by hand as the input of question
classification.
The Effectiveness of Each Feature Set
First, to examine which feature set is effective in question classification, we
exclude a feature set one by one from the five feature sets described in Section
3.2 and conduct experiments of question classification. Please note that the five
feature sets can be used unlike the last experiment (Table 4), because the input
of question classification is one sentence.
434 A.Tamura, H. Takamura, and M. Okumura
Table 5. Experiments with each feature set being excluded. Here ?sem. noun? means
semantic categories of nouns. ?sem. qf? means semantic categories of question focuses.
Excluded Feature Set
All Unigram Bigram Sem. noun Qf Sem. Qf
PERSON 0.574 0.571 0.620 0.536 0.505 0.505
(-0.003) (+0.046) (-0.038) (-0.069) (-0.069)
PRODUCT 0.506 0.489 0.579 0.483 0.512 0.502
(-0.017) (+0.073) (-0.023) (+0.006) (-0.004)
FACILITY 0.612 0.599 0.642 0.549 0.615 0.576
(-0.013) (+0.03) (-0.063) (+0.003) (-0.036)
LOCATION 0.832 0.826 0.841 0.844 0.825 0.833
(-0.006) (+0.009) (+0.012) (-0.007) (+0.001)
TIME 0.475 0.506 0.548 0.420 0.502 0.517
(+0.031) (+0.073) (-0.055) (+0.027) (+0.042)
NUMBER 0.442 0.362 0.475 0.440 0.466 0.413
(-0.080) (+0.033) (-0.002) (+0.024) (-0.029)
OTHERS (NOUN) 0.210 0.182 0.267 0.204 0.198 0.156
(-0.028) (+0.057) (-0.006) (-0.012) (-0.054)
REASON 0.564 0.349 0.622 0.603 0.576 0.582
(-0.215) (+0.058) (+0.039) (+0.012) (+0.018)
WAY 0.817 0.803 0.787 0.820 0.817 0.807
(-0.014) (-0.030) (+0.003) (?0.000) (-0.010)
DEFINITION 0.652 0.659 0.603 0.640 0.647 0.633
(+0.007) (-0.049) (-0.012) (-0.005) (-0.019)
DESCRIPTION 0.355 0.308 0.355 0.363 0.357 0.334
(-0.047) (?0.000) (+0.008) (+0.002) (-0.021)
OPINION 0.696 0.670 0.650 0.703 0.676 0.685
(-0.026) (-0.046) (+0.007) (-0.020) (-0.011)
OTHERS (TEXT) 0.183 0.176 0.179 0.154 0.190 0.198
(-0.007) (-0.004) (-0.029) (+0.007) (+0.015)
Average 0.532 0.500 0.551 0.520 0.530 0.518
(-0.032) (+0.019) (-0.012) (-0.002) (-0.014)
Accuracy 0.674 0.632 0.638 0.668 0.661 0.661
Table 5 shows the result. The numbers in parentheses are differences of
F-measure compared with its original value. The decrease of F-measure suggests
the effectiveness of the excluded feature set.
We first discuss the difference of F-measure values in Table 5, by taking
PRODUCT and WAY as examples. The F-measure of PRODUCT is much
smaller than that of WAY. This difference is due to whether characteristic ex-
pressions are present in the type or not. In WAY, words and phrases such as
?method? and ?How do I - ?? are often used. Such words and phrases work as
good clues for classification. However, there is no such characteristic expressions
for PRODUCT. Although there is a frequently-used expression ?What is [noun] -
??, this expression is often used also in other types such as LOCATION and FA-
CILITY. We have to rely on currently-unavailable world knowledge of whether
the noun is a product name or not. This is the reason of the low F-measure for
PRODUCT.
We next discuss the difference of effective feature sets according to question
types. We again take PRODUCT and WAY as examples. The most effective
Classification of Multiple-Sentence Questions 435
Table 6. Experiments with different window sizes
Window Size
0 1 2 ?
PERSON 0.574 0.558 0.565 0.570
PRODUCT 0.506 0.449 0.441 0.419
FACILITY 0.612 0.607 0.596 0.578
LOCATION 0.832 0.827 0.817 0.815
TIME 0.475 0.312 0.288 0.302
NUMBER 0.442 0.322 0.296 0.311
OTHERS (NOUN) 0.210 0.123 0.120 0.050
REASON 0.564 0.486 0.472 0.439
WAY 0.817 0.808 0.809 0.792
DEFINITION 0.652 0.658 0.658 0.641
DESCRIPTION 0.355 0.358 0.357 0.340
OPINION 0.696 0.670 0.658 0.635
OTHERS (TEXT) 0.183 0.140 0.129 0.133
Average 0.532 0.486 0.477 0.463
Accuracy 0.674 0.656 0.658 0.653
feature set is semantic categories of nouns for ?PRODUCT? and bigrams for
?WAY?. Since whether a noun is a product name or not is important for PROD-
UCT as discussed before, semantic categories of nouns are crucial to PRODUCT.
On the other hand, important clues for WAY are phrases such as ?How do I?.
Therefore, bigrams are crucial to WAY.
Finally, we discuss the effectiveness of a question focus. The result in Table
5 shows that the F-measure does not change so much even if question focuses or
their semantic categories are excluded. This is because both question focuses and
their semantic categories are redundantly put in the feature sets. By comparing
Tables 4 and 5, we can confirm that question focuses improve question classifi-
cation performance (F-measure increases from 0.514 to 0.532). Please note again
that question focuses are not used in Table 4 for fair comparison.
The Influence of Window Size
Next, we clarify the influence of window size. As in core sentence extraction,
?Window size is n? means that ?the preceding n sentences and the following
n sentences in addition to the core sentence are used to make a feature vec-
tor?. We construct four models with different window sizes (0, 1, 2, and ?)
and compare their experimental results. In this experiment, we use five sets of
features and correct core sentence as the input of question classification like the
last experiment (Table 5).
Table 6 shows the result of the experiment. The result in Table 6 shows that
the model with the core sentence alone is best. Therefore, the sentences other
than the core sentence are considered to be noisy for classification and would
not contain effective information for question classification. This result suggests
that the assumption (a multiple-sentence question can be classified using only
the core sentence) described in Section 3.1 be correct.
436 A.Tamura, H. Takamura, and M. Okumura
Table 7. The result of concluding experiments
Plain Question The Proposed Method
core sentence extraction No Yes
feature sets unigram, bigram unigram,bigram,qf
sem. noun sem. noun,sem. qf
PERSON 0.462 0.492
PRODUCT 0.381 0.504
FACILITY 0.584 0.575
LOCATION 0.758 0.792
TIME 0.340 0.495
NUMBER 0.262 0.456
OTHERS (NOUN) 0.049 0.189
REASON 0.280 0.537
WAY 0.756 0.789
DEFINITION 0.643 0.626
DESCRIPTION 0.296 0.321
OPINION 0.591 0.677
OTHERS (TEXT) 0.090 0.189
Average 0.423 0.511
Accuracy 0.617 0.661
Concluding Experiments
We have so far shown that core sentence extraction and question focuses work
well for question classification. In this section, we conduct concluding experi-
ments which show that our method significantly improves the classification per-
formance. In the discussion on effective features, we used correct core sentences.
Here we use predicted core sentences.
The result is shown in Table 7. For comparison, we add to this table the
values of F-measure in Table 4, which correspond to plain question (i.e., without
core sentence extraction). The result shows that F-measure of most categories
increase, except for FACILITY and DEFINITION. From comparison of ?All?
in Table 5 with Table 7, the reason of decrease would be the low accuracies of
core sentence extraction for these categories. As shown in this table, in conclu-
sion, we obtained 8.8% increase of average F-measure of all and 4.4% increase of
accuracy, which is statistically significant in the sign-test with 1% significance-
level.
Someone may consider that the type of multiple-sentence questions can be
identified by ?one-step? approach without core sentence extraction. In a word,
the question type of each sentence in the given multiple-sentence question is
first identified by a classifier, and then the type of the sentence for which the
classifier outputs the largest score is selected as the type of the given question.
The classifier?s output indicates the likeliness of being the question type of a
given question. Therefore, we compared the proposed model with this model
in the preliminary experiment. The accuracy of question classification with the
proposed model is 66.1% (1570/2376), and that of the one-step approach is
61.7% (1467/2376). This result shows that our two-step approach is effective for
classification of multiple-sentence questions.
Classification of Multiple-Sentence Questions 437
5 Conclusions
In this paper, we proposed a method for identifying the types of multiple-
sentence questions. In our method, the core sentence is first extracted from a
given multiple-sentence question and then used for question classification.
We obtained accuracy of 90.9% in core sentence extraction and empirically
showed that larger window sizes are more effective in core sentence extraction.
We also showed that the extracted core sentences and the question focuses are
good for question classification. Core sentence extraction is quite important also
in the sense that question focuses could not be introduced without core sentences.
With the proposed method, we obtained the 8.8% increase of F-measure and
4.4% increase of accuracy.
Future work includes the following. The question focuses extracted in the
proposed method include nouns which might not be appropriate for question
classification. Therefore, we regard the improvement on the question focus detec-
tion as future work. To construct a QA system that can handle multiple-sentence
question, we are also planning to work on the other components: document re-
trieval, answer extraction.
References
1. Yutaka Sasaki, Hideki Isozaki, Tsutomu Hirao, Koji Kokuryou, and Eisaku Maeda:
NTT?s QA Systems for NTCIR QAC-1. Working Notes, NTCIR Workshop 3, Tokyo,
pp. 63?70, 2002.
2. Jinxi Xu, Ana Licuanan, and Ralph M.Weischedel: TREC 2003 QA at BBN: An-
swering Definitional Questions. TREC 2003, pp. 98?106, 2003.
3. Xin Li and Dan Roth: Learning Question Classifiers. COLING 2002, Taipei, Taiwan,
pp. 556?562, 2002.
4. Ingrid Zukerman and Eric Horvitz: Using Machine Learning Techniques to Interpret
WH-questions. ACL 2001, Toulouse, France, pp. 547?554, 2001.
5. Abraham Ittycheriah, Martin Franz, Wei-Jing Zhu, and Adwait Ratnaparkhi: Ques-
tion Answering Using Maximum Entropy Components. NAACL 2001, pp. 33?39,
2001.
6. Jun Suzuki: Kernels for Structured Data in Natural Language Processing, Doctor
Thesis, Nara Institute of Science and Technology, 2005.
7. Dell Zhang and Wee Sun Lee: Question Classification using Support Vector Ma-
chines. SIGIR, Toronto, Canada, pp. 26?32, 2003.
8. Dan Moldovan, Sanda Harabagiu, Marius Pasca, Rada Mihalcea, Richard Goodrum,
Roxana Girju, and Vasile Rus: Lasso: A Tool for Surfing the Answer Net. TREC-8,
pp. 175?184, 1999.
9. Satoru Ikehara, Masahiro Miyazaki, Satoshi Shirai, Akio Yokoo, Hiromi Nakaiwa,
Kentaro Ogura, Yoshifumi Oyama, and Yoshihiko Hayashi, editors: The Semantic
System, volume 1 of Goi-Taikei ? A Japanese Lexicon. Iwanami Shoten, 1997 (in
Japanese).
Identifying Cross-Document Relations between Sentences
Yasunari Miyabe ?? Hiroya Takamura ? Manabu Okumura ?
?Interdisciplinary Graduate School of Science and Engineering,
Tokyo Institute of Technology, Japan
?Precision and Intelligence Laboratory,
Tokyo Institute of Technology, Japan
miyabe@lr.pi.titech.ac.jp, {takamura,oku}@pi.titech.ac.jp
Abstract
A pair of sentences in different newspaper
articles on an event can have one of sev-
eral relations. Of these, we have focused on
two, i.e., equivalence and transition. Equiv-
alence is the relation between two sentences
that have the same information on an event.
Transition is the relation between two sen-
tences that have the same information except
for values of numeric attributes. We pro-
pose methods of identifying these relations.
We first split a dataset consisting of pairs
of sentences into clusters according to their
similarities, and then construct a classifier
for each cluster to identify equivalence re-
lations. We also adopt a ?coarse-to-fine? ap-
proach. We further propose using the identi-
fied equivalence relations to address the task
of identifying transition relations.
1 Introduction
A document generally consists of semantic units
called sentences and various relations hold between
them. The analysis of the structure of a document by
identifying the relations between sentences is called
discourse analysis.
The discourse structure of one document has
been the target of the traditional discourse anal-
ysis (Marcu, 2000; Marcu and Echihabi, 2002;
Yokoyama et al, 2003), based on rhetorical struc-
ture theory (RST) (Mann and Thompson, 1987).
?Yasunari Miyabe currently works at Toshiba Solutions Cor-
poration.
Inspired by RST, Radev (2000) proposed the
cross-document structure theory (CST) for multi-
document analysis, such as multi-document summa-
rization, and topic detection and tracking. CST takes
the structure of a set of related documents into ac-
count. Radev defined relations that hold between
sentences across the documents on an event (e.g., an
earthquake or a traffic accident).
Radev presented a taxonomy of cross-document
relations, consisting of 24 types. In Japanese, Etoh
et al (2005) redefined 14 CST types based on
Radev?s taxonomy. For example, a pair of sentences
with an ?equivalence relation? (EQ) has the same
information on an event. EQ can be considered to
correspond to the identity and equivalence relations
in Radev?s taxonomy. A sentence pair with a ?tran-
sition relation? (TR) contains the same numeric at-
tributes with different values. TR roughly corre-
sponds to the follow-up and fulfilment relations in
Radev?s taxonomy. We will provide examples of
CST relations:
1. ABC telephone company announced on the 9th
that the number of users of its mobile-phone
service had reached one million. Users can ac-
cess the Internet, reserve train tickets, as well
as make phone calls through this service.
2. ABC said on the 18th that the number of
users of its mobile-phone service had reached
1,500,000. This service includes Internet ac-
cess, and enables train-ticket reservations and
telephone calls.
The pair of the first sentence in 1 and the first sen-
tence in 2 is in TR, because the number of users
141
has changed from one million to 1.5 millions, while
other things remain unchanged. The pair of the sec-
ond sentence in 1 and the second sentence in 2 is
in EQ, because these two sentences have the same
information.
Identification of CST relations has attracted more
attention since the study of multi-document dis-
course emerged. Identified CST types are helpful
in various applications such as multi-document sum-
marization and information extraction. For example,
EQ is useful for detecting and eliminating redundant
information in multi-document summarization. TR
can be used to visualize time-series trends.
We focus on the two relations EQ and TR in the
Japanese CST taxonomy, and present methods for
their identification. For the identification of EQ
pairs, we first split a dataset consisting of sentence
pairs into clusters according to their similarities, and
then construct a classifier for each cluster. In addi-
tion, we adopt a coarse-to-fine approach, in which a
more general (coarse) class is first identified before
the target fine class (EQ). For the identification of TR
pairs, we use variable noun phrases (VNPs), which
are defined as noun phrases representing a variable
with a number as its value (e.g., stock prices, and
population).
2 Related Work
Hatzivassiloglou et al (1999; 2001) proposed a
method based on supervised machine learning to
identify whether two paragraphs contain similar in-
formation. However, we found it was difficult to
accurately identify EQ pairs between two sentences
simply by using similarities as features. Zhang et
al. (2003) presented a method of classifying CST
relations between sentence pairs. However, their
method used the same features for every type of
CST, resulting in low recall and precision. We thus
select better features for each CST type, and for each
cluster of EQ.
The EQ identification task is apparently related to
Textual Entailment task (Dagan et al, 2005). Entail-
ment is asymmetrical while EQ is symmetrical, in
the sense that if a sentence entails and is entailed by
another sentence, then this sentence pair is in EQ.
However in the EQ identification, we usually need
to find EQ pairs from an extremely biased dataset of
sentence pairs, most of which have no relation at all.
3 Identification of EQ pairs
This section explains a method of identifying EQ
pairs. We regarded the identification of a CST re-
lation as a standard binary classification task. Given
a pair of sentences that are from two different but
related documents, we determine whether the pair
is in EQ or not. We use Support Vector Machines
(SVMs) (Vapnik, 1998) as a supervised classifier.
Please note that one instance consists of a pair of two
sentences. Therefore, a similarity value between two
sentences is only given to one instance, not two.
3.1 Clusterwise Classification
Although some pairs in EQ have quite high similar-
ity values, others do not. Simultaneously using both
of these two types of pairs for training will adversely
affect the accuracy of classification. Therefore, we
propose splitting the dataset first according to sim-
ilarities of pairs, and then constructing a classifier
for each cluster (sub-dataset). We call this method
clusterwise classification.
We use the following similarity in the cosine mea-
sure between two sentences (s1, s2):
cos(s1, s2) = u1 ? u2/|u1||u2|, (1)
where u1 and u2 denote the frequency vectors of
content words (nouns, verbs, adjectives) for respec-
tive s1 and s2. The distribution of the sentence pairs
according to the cosine measure is summarized in
Table 1. From the table, we can see a large dif-
ference in distributions of EQ and no-relation pairs.
This difference suggests that the clusterwise classi-
fication approach is reasonable.
We split the dataset into three clusters: high-
similarity cluster, intermediate-similarity cluster,
and low-similarity cluster. Intuitively, we ex-
pected that a pair in the high-similarity cluster
would have many common bigrams, that a pair in
the intermediate-similarity cluster would have many
common unigrams but few common bigrams, and
that a pair in the low-similarity cluster would have
few common unigrams or bigrams.
3.2 Two-Stage Identification Method
The number of sentence pairs in EQ in the
intermediate- or low-similarity clusters is much
142
Table 1: The distribution of sentence pairs according to the cosine measure (NO indicates pairs with no
relation. The pairs with other relations are not on the table due to the space limitation)
cos (0.0, 0.1] (0.1, 0.2] (0.2, 0.3] (0.3, 0.4] (0.4, 0.5] (0.5, 0.6] (0.6, 0.7] (0.7, 0.8] (0.8, 0.9] (0.9, 1.0]
EQ 12 13 21 25 37 61 73 61 69 426
summary 5 5 25 19 22 13 16 6 6 0
refinement 3 4 15 11 12 15 6 6 3 2
NO 194938 162221 68283 28152 11306 4214 1379 460 178 455
Figure 1: Method of identifying EQ pairs
smaller than the total number of sentence pairs as
shown in Table 1. These two clusters also contain
many pairs that belong to a ?summary? and a ?re-
finement? relation, which are very much akin to EQ.
This may cause difficulties in identifying EQ pairs.
We gave a generic name, GEN(general)-EQ, to
the union of EQ, ?summary?, and ?refinement? re-
lations. For pairs in the intermediate- or low-
similarity clusters, we propose a two-stage method
using GEN-EQ on the basis of the above observa-
tions, which first identifies GEN-EQ pairs between
sentences, and then identifies EQ pairs from GEN-
EQ pairs.
This two-stage method can be regarded as a
coarse-to-fine approach (Vanderburg and Rosenfeld,
1977; Rosenfeld and Vanderbrug, 1977), which first
identifies a coarse class and then finds the target fine
class. We used the coarse-to-fine approach on top of
the clusterwise classification method as in Fig. 1.
There are by far less EQ pairs than pairs without
relation. This coarse-to-fine approach will reduce
this bias, since GEN-EQ pairs outnumber EQ pairs.
3.3 Features for identifying EQ pairs
Instances (i.e., pairs of sentences) are represented as
binary vectors. Numeric features ranging from 0.0
to 1.0 are discretized and represented by 10 binary
features (e.g., a feature value of 0.65 is transformed
into the vector 0000001000). Let us first explain ba-
sic features used in all clusters. We will then explain
other features that are specific to a cluster.
3.3.1 Basic features
1. Cosine similarity measures: We use unigram, bi-
gram, trigram, bunsetsu-chunk1 similarities at all the
sentence levels, and unigram similarities at the para-
graph and the document levels. These similarities
are calculated by replacing u1 and u2 in Eq. (1) with
the frequency vectors of each sentence level.
2. Normalized lengths of sentences: Given an in-
stance of sentence pair s1 and s2, we can define fea-
tures normL(s1) and normL(s2), which represent
(normalized) lengths of sentences, as:
normL(s) = len(s)/EventMax(s), (2)
where len(s) is the number of characters in
s. EventMax(s) is maxs??event(s) len(s?), where
event(s) is the set of sentences in the event that
doc(s) describes. doc(s) is the document contain-
ing s.
3. Difference in publication dates: This feature de-
pends on the interval between the publication dates
of doc(s1) and doc(s2) and is defined as:
DateDiff(s1, s2) = 1 ?
|Date(s1) ? Date(s2)|
EventSpan(s1, s2)
, (3)
where Date(s) is the publication date of an arti-
cle containing s, and EventSpan(s1, s2) is the time
span of the event, i.e., the difference between the
publication dates for the first and the last articles that
are on the same event. For example, if doc(s1) is
published on 1/15/99 and doc(s2) on 1/17/99, and
if the time span of the event ranges from 1/1/99 to
1/21/99, then the feature value is 1-2/20 = 0.9.
1Bunsetsu-chunks are Japanese phrasal units usually con-
sisting of a pair of a noun phrase and a case marker.
143
4. Positions of sentences in documents (Edmund-
son, 1969): This feature is defined as
Posit(s) = lenBef(s)/len(doc(s)), (4)
where lenBef(s) is the number of characters be-
fore s in the document, and len(doc(s)) is the total
number of characters in doc(s).
5. Semantic similarities: This feature is measured by
Eq. (1) with u1 and u2 being the frequency vectors
of semantic classes of nouns, verbs, and adjectives.
We used the semantic classes in a Japanese thesaurus
called ?Goi-taikei? (Ikehara et al, 1997).
6. Conjunction (Yokoyama et al, 2003): Each of 55
conjunctions corresponds to one feature. If a con-
junction appears at the beginning of the sentence,
the feature value is 1, otherwise 0.
7. Expressions at the end of sentences: Yokoyama
et al (2003) created rules that map sentence endings
to their functions. Each function corresponds to a
feature. If a function appears in the sentence, the
value of the feature for the function is 1, otherwise 0.
Functions of sentence endings are past, present, as-
sertion, existence, conjecture, interrogation, judge-
ment, possibility, reason, request, description, duty,
opinion, continuation, causation, hearsay, and mode.
8. Named entity: This feature represents sim-
ilarities measured through named entities in the
sentences. Its value is measured by Eq. (1)
with u1 and u2 being the frequency vectors of the
named entities. We used the named-entity chun-
ker bar2. The types of named entities are ARTI-
FACT?DATE?ORGANIZATION?MONEY?LO-
CATION?TIME?PERCENT?and PERSON.
9. Types of named entities with particle: This fea-
ture represents the occurrence of types of named en-
tities accompanied by a case marker (particle). We
used 11 different case markers.
3.3.2 Additional features to identify fine class
We will next explain additional features used in
identifying EQ pairs from GEN-EQ pairs.
1. Numbers of words (morphemes) and phrases:
These features represent the closeness of the num-
bers of words and bunsetsu-chunks in the two sen-
tences. This feature is defined as:
2http://chasen.naist.jp/?masayu-a/p/bar/
NumW (s1, s2) = 1 ?
|frqW (s1) ? frqW (s2)|
max(frqW (s1), frqW (s2))
, (5)
where frqW (s) indicates the number of words in
s. Similarly, NumP (s1, s2) is obtained by replac-
ing frqW in Eq. (5) with frqP , where frqP (s)
indicates the number of phrases in s.
2. Head verb: There are three features of this kind.
The first indicates whether the two sentences have
the same head verb or not. The second indicates
whether the two sentences have a semantically sim-
ilar head verb or not. If the two verbs have the
same semantic class in a thesaurus, they are re-
garded as being semantically similar. The last in-
dicates whether both sentences have a verb or not.
The head verbs are extracted using rules proposed
by Hatayama (2001).
3. Salient words: This feature indicates whether the
salient words of the two sentences are the same or
not. We approximate the salient word with the ga-
or the wa-case word that appears first.
4. Numeric expressions and units (Nanba et al,
2005): The first feature indicates whether the two
sentences share a numeric expression or not. The
second feature is similarly defined for numeric units.
4 Experiments on identifying EQ pairs
We used the Text Summarization Challenge (TSC) 2
and 3 corpora (Okumura et al, 2003) and the Work-
shop on Multimodal Summarization for Trend Infor-
mation (Must) corpus (Kato et al, 2005). These two
corpora contained 115 sets of related news articles
(10 documents per set on average) on various events.
A document contained 9.9 sentences on average.
Etoh et al (2005) annotated these two corpora with
CST types. There were 471,586 pairs of sentences
and 798 pairs of these had EQ. We conducted the
experiments with 10-fold cross-validation (i.e., ap-
proximately 425,000 pairs on average, out of which
approximately 700 pairs are in EQ, are in the train-
ing dataset for each fold). The average, maximum,
and minimum lengths of the sentences in the whole
datset are shown in Table 2. We used precision,
recall, and F-measure as evaluation measures. We
used a Japanese morphological analyzer ChaSen3 to
3http://chasen.naist.jp/hiki/Chasen/
144
Table 2: Average, max, min lengths of the sentences
in the dataset
average max min
# of words 33.27 458 1
# of characters 111.22 1107 2
extract parts-of-speech. and a dependency analyzer
CaboCha4 to extract bunsetsu-chunks.
4.1 Estimation of threshold
We split the set of sentence pairs into clusters ac-
cording to their similarities in identifying EQ pairs
as explained. We used 10-fold cross validation again
within the training data (i.e., the approximately
425,000 pairs above are split into a temporary train-
ing dataset and a temporary test dataset 10 times) to
estimate the threshold to split the set, to select the
best feature set, and to determine the degree of the
polynomial kernel function and the value for soft-
margin parameter C in SVMs. No training instances
are used in the estimation of these parameters.
4.1.1 Threshold between high- and
intermediate-similarity clusters
We will first explain how to estimate the threshold
between high- and intermediate-similarity clusters.
We expected that a pair in high-similarity cluster
would have many common bigrams, and that a pair
in intermediate-similarity cluster would have many
common unigrams but few common bigrams. We
therefore assumed that bigram similarity would be
ineffective in intermediate-similarity cluster.
We determined the threshold in the following way
for each fold of cross-validation. We decreased the
threshold by 0.01 from 1.0. We carried out 10-fold
cross-validation within the training data, excluding
one of the 14 features (6 cosine similarities and other
basic features) for each value of the threshold. If
the exclusion of a feature type deteriorates both av-
erage precision and recall obtained by the cross-
validation within the training data, we call it ineffec-
tive. We set the threshold to the minimum value for
which bigram similarity is not ineffective. We obtain
a threshold value for each fold of cross-validation.
The average value of threshold was 0.87.
4http://chasen.naist.jp/?taku/software/cabocha/
Table 3: Ineffective feature types for each threshold
threshold ineffective features
0.90 particle, bunsetsu-chunk similarity, semantic similarity
0.89
semantic similarity, expression at end of sentences,
bigram similarity, particle
0.88 bigram similarity
0.87
difference in publication dates, similarity between documents,
expression at end of sentences, number of tokens,
bigram similarity, similarity between paragraphs,
positions of sentences, particle
0.86 particle, similarity between documents, bigram similarity
Table 4: F-measure calculated by cross-validation
within the training data for each threshold in
?intermediate-similarity cluster?
threshold precision recall F-measure
0.60 49,71 14.95 22.99
0.59 52.92 15.05 23.44
0.58 55.08 16.64 25.56
0.57 52.81 16.93 25.64
0.56 49.15 14.45 22.34
0.55 51.51 14.84 23.04
0.54 51.89 15.21 23.52
0.53 54.59 13.61 21.78
As an example, we show the table of obtained
ineffective feature types for one fold of cross-
validation (Table 3). The threshold was set to 0.90
in this fold.
4.1.2 Threshold between intermediate- and
low-similarity clusters
We will next explain how to estimate the threshold
between intermediate- and low-similarity clusters.
There are numerous no-relation pairs in low-
similarity pairs. We expected that this imbalance
would adversely affect classification. We therefore
simply attemted to exclude low-similarity pairs. We
decreased the threshold by 0.01 from the threshold
between high- and intermediate-similarity clusters.
We chose a value that yielded the best average F-
measure calculated by the cross-validation within
the training data. The average value of the thresh-
old was 0.57. Table 4 is an example of thresholds
and F-measures for one fold.
4.2 Results of identifying EQ pairs
The results of EQ identification are shown in Ta-
ble 5. We tested the following models:
Bow-cos: This is the simplest baseline we used. We represented
sentences with bag-of-words model. Instances with the cosine
similarity in Eq. (1) larger than a threshold were classified as
EQ. The threshold that yielded the best F-measure in the test
145
Table 5: Results of identifying EQ pairs
precision recall F-measure
Bow-cos 87.29 57.35 69.22
basic features
Clusterwise 81.98 59.40 68.88
Non-Clusterwise 86.10 59.49 70.36
ClusterC2F 94.96 62.27 75.22
with additional features
Clusterwise 80.93 59.74 68.63
Non-Clusterwise 86.11 60.16 70.84
ClusterC2F 94.99 62.65 75.50
Table 6: Results with basic features
Results for ?high-similarity cluster?
precision recall F-measure
Clusterwise 94.23 96.83 95.51
Non-clusterwise 95.51 96.29 95.90
ClusterC2F 94.23 96.83 95.51
Results for ?intermediate-similarity cluster?
Clusterwise 42.77 23.03 29.94
Non-clusterwise 53.46 25.31 34.36
ClusterC2F 100.00 36.29 53.25
data was chosen.
Non-Clusterwise: This is a supervised method without the
clusterwise approach. One classifier was constructed regard-
less of the similarity of the instance. We used the second degree
polynomial kernel. Soft margin parameter C was set to 0.01.
Clusterwise: This is a clusterwise method without the coarse-
to-fine approach. The second degree polynomial kernel was
used. Soft margin parameter C was set to 0.1 for high-similarity
cluster and 0.01 for the other clusters.
ClusterC2F: This is our model, which integrates clusterwise
classification with the coarse-to-fine approach (Figure 1).
Table 5 shows that ClusterC2F yielded the best
F-measure regardless of presence of additional fea-
tures. The difference between ClusterC2F and the
others was statistically significant in the Wilcoxon
signed rank sum test with 5% significance level.
4.3 Results for each cluster
We examined the results for each cluster. The re-
sults with basic features are summarized in Table 6
and those with basic features plus additional fea-
tures are in Table 7. The tables show that there
are no significant differences among the models
for high-similarity cluster. However, there are sig-
nificant differences for intermediate-similarity clus-
ter. We thus concluded that the proposed model
(ClusterC2F) works especially well in intermediate-
similarity cluster.
Table 7: Results with additional features
Results for ?high-similarity cluster?
precision recall F-measure
Clusterwise 94.23 96.83 95.51
Non-clusterwise 95.70 96.76 96.23
ClusterC2F 94.23 96.83 95.51
Results for ?intermediate-similarity cluster?
Clusterwise 39.77 22.93 29.09
Non-clusterwise 55.61 26.81 36.18
ClusterC2F 100.00 38.06 55.13
5 Identification of TR pairs
We regarded the identification of the relations be-
tween sentences as binary classification, whether a
pair of sentences is classified into TR or not. We
used SVMs (Vapnik, 1998).
The sentence pairs in TR have the same numeric
attributes with different values, as mentioned in In-
troduction. Therefore, VNPs will be good clues for
the identification.
5.1 Extraction of VNPs
We extract VNPs in the following way.
1. Search for noun phrases that have numeric ex-
pressions (we call them numeric phrases).
2. Search for the phrases that the numeric phrases
depend on (we call them predicate phrases).
3. Search for the noun phrases that depend on the
predicate phrases.
4. Extract the noun phrases that depend on the
noun phrases found in step 3, except for date expres-
sions. Both the extracted noun phrases and the noun
phrases found in step 3 were regarded as VNPs.
In the example in Introduction, ?one million? and
?1,500,000? are numeric phrases, and ?had reached?
is a predicate phrase. Then, ?the number of users of
its mobile-phone service? is a VNP.
5.2 Features for identifying TR pairs
We used some features used in EQ identification:
sentence-level uni-, bi-, tirgrams, and bunsetsu-
chunk unigrams, normalized lengths of sentences,
difference in publication dates, position of sentences
in documents, semantic similarities, conjunctions,
expressions at the end of sentences, and named enti-
ties. In addition, we use the following features.
1. Similarities through VNPs: The cosine similarity
of the frequency vectors of nouns in the VNPs in s1
146
and s2 is used. If there are more than one VNP, the
largest cosine similarity is chosen.
2. Similarities through bigrams and trigrams in
VNPs: These features are defined similarly to the
previous feature, but each VNP is represented by the
frequency vector of word bi- and trigrams.
3. Similarities of noun phrases in nominative case:
Instances in TR often have similar subjects. A noun
phrase containing a ga-, wa-, or mo-case is regarded
as the subject phrase of a sentence. The similarity is
calculated by Eq. (1) with the frequency vectors of
nouns in the phrase.
4. Changes in value of numeric attributes: This fea-
ture is 1 if the values of the numeric phrases in the
two sentences are different, otherwise 0.
5. Presence of numerical units: If a numerical unit
is present in both sentences, the value of the feature
is 1, otherwise 0.
6. Expressions that mean changes in value: In-
stances in TR often contain those expressions, such
as ?reduce? and ?increase? (Nanba et al, 2005). We
have three features for each of these expressions.
The first feature is 1 if both sentences have the ex-
pression, otherwise 0. The second is 1 if s1 has the
expression, otherwise 0. The third is 1 if s2 has the
expression, otherwise 0.
7. Predicates: We define one feature for a predicate.
The value of this feature is 1 if the predicate appears
in the two sentences, otherwise 0.
8. Reporter: This feature represents who is report-
ing the incident. This feature is represented by the
cosine similarity between the frequency vectors of
nouns in phrases respectively expressing reporters in
s1 and s2. The subjects of verbs such as ?report? and
?announce? are regarded as phrases of the reporter.
5.3 Use of EQ
A pair of sentences in TR often has a high degree
of similarity. Such pairs are likely to be confused
with pairs in EQ. We used the identified EQ pairs for
the identification of TR in order to circumvent this
confusion. Pairs classified as EQ with our method
were excluded from candidates for TR.
Table 8: Results of identifying TR pairs
precision recall F-measure
Bow-cos 27.44 41.26 32.96
NANBA 19.85 45.96 27.73
WithoutEq 42.41 47.06 44.61
WithEq 43.13 48.51 45.67
WithEqActual 43.06 48.55 45.64
6 Experiments on identifying TR pairs
Most experimental settings are the same as in the ex-
periments of EQ identification. Sentence pairs with-
out numeric expressions were excluded in advance
and 55,547 pairs were left. This exclusion process
does not degrade recall at all, because TR pairs by
definition contain numberic expressions.
We used precision, recall and F-measure for eval-
uation. We employed 10-fold cross validation.
6.1 Results of identifying TR pairs
The results of the experiments are summarized in
Table 8. We compared four following models with
ours. A linear kernel was used in SVMs and soft
margin parameter C was set to 1.0 for all models:
Bow-cos (baseline): We calculated the similarity through
VPNs. If the similarity was larger than a threshold and the two
sentences had the same expressions meaning changes in value
and had different values, then this pair was classified as TR. The
threshold was set to 0.7, which yielded the best F-measure in the
test data.
NANBA (Nanba et al, 2005): If the unigram cosine similarity
between the two sentences was larger than a threshold and the
two sentences had expressions meaning changes in value, then
this pair was classified as TR. The value of the threshold was set
to 0.42, which yielded the best F-measure in the test data.
WithEq (Our method): This model uses the identified EQ
pairs.
WithoutEq: This model uses no information on EQ.
WithEqActual: This model uses the actual EQ pairs given by
oracle.
The results in Table 8 show that bow-cos is better
than NANBA in F-measure. This result suggests that
focusing on VNPs is more effective than a simple
bag-of-words approach.
WithEq and WithEqActual were better than With-
outEq. This suggests that we successfully excluded
EQ pairs, which are TR look-alikes. WithEq and
WithEqActual yielded almost the same F-measure.
This means that our EQ identifier was good enough
147
to improve the identification of TR pairs.
7 Conclusion
We proposed methods for identifying EQ and TR
pairs in different newspaper articles on an event.
We empirically demonstrated that the methods work
well in this task.
Although we focused on resolving a bias in the
dataset, we can expect that the classification perfor-
mance will improve by making use of methods de-
veloped in different but related tasks such as Textual
Entailment recognition on top of our method.
References
Ido Dagan, Oren Glickman, and Bernardo Magnini.
2005. The pascal recognising textual entailment chal-
lenge. In Proceedings of the PASCAL Challenges
Workshop on Recognising Textual Entailment, pages
177?190.
Harold Edmundson. 1969. New methods in automatic
extracting. Journal of ACM, 16(2):246?285.
Junji Etoh and Manabu Okumura. 2005. Making
cross-document relationship between sentences cor-
pus. In Proceedings of the Eleventh Annual Meeting
of the Association for Natural Language Processing
(in Japanese), pages 482?485.
Mamiko Hatayama, Yoshihiro Matsuo, and Satoshi Shi-
rai. 2001. Summarizing newspaper articles using ex-
tracted information and functional words. In 6th Natu-
ral Language Processing Pacific Rim Symposium (NL-
PRS2001), pages 593?600.
Vasileios Hatzivassiloglou, Judith L. Klavans, and
Eleazar Eskin. 1999. Detecting text similarity over
short passages: Exploring linguistic feature combi-
nations via machine learning. In Proceedings of the
Empirical Methods for Natural Language Processing,
pages 203?212.
Vasileios Hatzivassiloglou, Judith L. Klavans, Melissa L.
Holcombe, Regina Barzilay, Min-Yen Kan, and Kath-
leen R. McKeown. 2001. Simfinder: A flexible clus-
tering tool for summarization. In Proceedings of the
Workshop on Automatic Summarization, pages 41?49.
Satoru Ikehara, Masahiro Miyazaki, Satoshi Shirai, Akio
Yokoo, Hiromi Nakaiwa, Kentaro Ogura, Yoshifumi
Oyama, and Yoshihiko Hayashi. 1997. Goi-Taikei ? A
Japanese Lexicon (in Japanese). Iwanami Shoten.
Tsuneaki Kato, Mitsunori Matsushita, and Noriko
Kando. 2005. Must:a workshop on multimodal sum-
marization for trend information. In Proceedings of
the NTCIR-5 Workshop Meeting, pages 556?563.
William Mann and Sandra Thompson. 1987. Rhetorical
structure theory: Description and construction of text
structures. In Gerard Kempen, editor, Natural Lan-
guage Generation: New Results in Artificial Intelli-
gence, Psychology, and Linguistics, pages 85?96. Ni-
jhoff, Dordrecht.
Daniel Marcu and Abdessamad Echihabi. 2002. An
unsupervised approach to recognizing discourse rela-
tions. In Proceedings of the 40th Annual Meeting of
the Association for Computational Linguistics, pages
368?375.
Daniel Marcu. 2000. The rhetorical parsing of un-
restricted texts a surface-based approach. Computa-
tional Linguistics, 26(3):395?448.
Hidetsugu Nanba, Yoshinobu Kunimasa, Shiho
Fukushima, Teruaki Aizawa, and Manabu Oku-
mura. 2005. Extraction and visualization of trend
information based on the cross-document structure.
In Information Processing Society of Japan, Special
Interest Group on Natural Language Processing
(IPSJ-SIGNL), NL-168 (in Japanese), pages 67?74.
Manabu Okumura, Takahiro Fukushima, and Hidetsugu
Nanba. 2003. Text summarization challenge 2 -
text summarization evaluation at ntcir workshop 3.
In HLT-NAACL 2003 Workshop: Text Summarization
(DUC03), pages 49?56.
Dragomir Radev. 2000. A common theory of infor-
mation fusion from multiple text sources, step one:
Cross-document structure. In Proceedings of the 1st
ACL SIGDIAL Workshop on Discourse and Dialogue,
pages 74?83.
Azriel Rosenfeld and Gorden Vanderbrug. 1977.
Coarse-fine template matching. IEEE transactions
Systems, Man, and Cybernetics, 7:104?107.
Gorden Vanderburg and Azriel Rosenfeld. 1977. Two-
stage template matching. IEEE transactions on com-
puters, 26(4):384?393.
Vladimir Vapnik. 1998. Statistical Learning Theory.
John Wiley, New York.
Kenji Yokoyama, Hidetsugu Nanba, and Manabu Oku-
mura. 2003. Discourse analysis using support vector
machine. In Information Processing Society of Japan,
Special Interest Group on Natural Language Process-
ing (IPSJ-SIGNL), 2003-NL-155 (in Japanese), pages
193?200.
Zhu Zhang, Jahna Otterbacher, and Dragomir R.Radev.
2003. Learning cross-document structural relation-
ships using boosting. In Proceedings of the 12th Inter-
national Conference on Information and Knowledge
Management, pages 124?130.
148
Learning to Shift the Polarity of Words for Sentiment Classification
Daisuke Ikeda? Hiroya Takamura? Lev-Arie Ratinov?? Manabu Okumura?
?Department of Computational Intelligence and Systems Science, Tokyo Institute of Technology
ikeda@lr.pi.titech.ac.jp
??Department of Computer Science, University of Illinois at Urbana-Champaign
ratinov2@uiuc.edu
?Precision and Intelligence Laboratory, Tokyo Institute of Technology
{takamura,oku}@pi.titech.ac.jp
Abstract
We propose a machine learning based
method of sentiment classification of sen-
tences using word-level polarity. The polari-
ties of words in a sentence are not always the
same as that of the sentence, because there
can be polarity-shifters such as negation ex-
pressions. The proposed method models
the polarity-shifters. Our model can be
trained in two different ways: word-wise and
sentence-wise learning. In sentence-wise
learning, the model can be trained so that the
prediction of sentence polarities should be
accurate. The model can also be combined
with features used in previous work such
as bag-of-words and n-grams. We empiri-
cally show that our method almost always
improves the performance of sentiment clas-
sification of sentences especially when we
have only small amount of training data.
1 Introduction
Due to the recent popularity of the internet, individ-
uals have been able to provide various information
to the public easily and actively (e.g., by weblogs
or online bulletin boards). The information often in-
cludes opinions or sentiments on a variety of things
such as new products. A huge amount of work has
been devoted to analysis of the information, which
is called sentiment analysis. The sentiment analysis
has been done at different levels including words,
sentences, and documents. Among them, we focus
on the sentiment classification of sentences, the task
to classify sentences into ?positive? or ?negative?,
because this task is fundamental and has a wide ap-
plicability in sentiment analysis. For example, we
can retrieve individuals? opinions that are related to
a product and can find whether they have the positive
attitude to the product.
There has been much work on the identification of
sentiment polarity of words. For instance, ?beauti-
ful? is positively oriented, while ?dirty? is negatively
oriented. We use the term sentiment words to refer
to those words that are listed in a predefined polar-
ity dictionary. Sentiment words are a basic resource
for sentiment analysis and thus believed to have a
great potential for applications. However, it is still
an open problem how we can effectively use sen-
timent words to improve performance of sentiment
classification of sentences or documents.
The simplest way for that purpose would be the
majority voting by the number of positive words and
the number of negative words in the given sentence.
However, the polarities of words in a sentence are
not always the same as that of the sentence, be-
cause there can be polarity-shifters such as nega-
tion expressions. This inconsistency of word-level
polarity and sentence-level polarity often causes er-
rors in classification by the simple majority voting
method. A manual list of polarity-shifters, which
are the words that can shift the sentiment polarity of
another word (e.g., negations), has been suggested.
However, it has limitations due to the diversity of
expressions.
Therefore, we propose a machine learning based
method that models the polarity-shifters. The model
can be trained in two different ways: word-wise
296
and sentence-wise. While the word-wise learn-
ing focuses on the prediction of polarity shifts, the
sentence-wise learning focuses more on the predic-
tion of sentence polarities. The model can also be
combined with features used in previous work such
as bag-of-words, n-grams and dependency trees. We
empirically show that our method almost always im-
proves the performance of sentiment classification
of sentences especially when we have only small
amount of training data.
The rest of the paper is organized as follows. In
Section 2, we briefly present the related work. In
Section 3, we discuss well-known methods that use
word-level polarities and describe our motivation. In
Section 4, we describe our proposed model, how to
train the model, and how to classify sentences using
the model. We present our experiments and results
in Section 5. Finally in Section 6, we conclude our
work and mention possible future work.
2 Related Work
Supervised machine learning methods including
Support Vector Machines (SVM) are often used in
sentiment analysis and shown to be very promising
(Pang et al, 2002; Matsumoto et al, 2005; Kudo and
Matsumoto, 2004; Mullen and Collier, 2004; Ga-
mon, 2004). One of the advantages of these meth-
ods is that a wide variety of features such as depen-
dency trees and sequences of words can easily be in-
corporated (Matsumoto et al, 2005; Kudo and Mat-
sumoto, 2004; Pang et al, 2002). Our attempt in this
paper is not to use the information included in those
substructures of sentences, but to use the word-level
polarities, which is a resource usually at hand. Thus
our work is an instantiation of the idea to use a re-
source on one linguistic layer (e.g., word level) to
the analysis of another layer (sentence level).
There have been some pieces of work which fo-
cus on multiple levels in text. Mao and Lebanon
(2006) proposed a method that captures local senti-
ment flow in documents using isotonic conditional
random fields. Pang and Lee (2004) proposed to
eliminate objective sentences before the sentiment
classification of documents. McDonald et al (2007)
proposed a model for classifying sentences and doc-
uments simultaneously. They experimented with
joint classification of subjectivity for sentence-level,
and sentiment for document-level, and reported that
their model obtained higher accuracy than the stan-
dard document classification model.
Although these pieces of work aim to predict not
sentence-level but document-level sentiments, their
concepts are similar to ours. However, all the above
methods require annotated corpora for all levels,
such as both subjectivity for sentences and senti-
ments for documents, which are fairly expensive to
obtain. Although we also focus on two different lay-
ers, our method does not require such expensive la-
beled data. What we require is just sentence-level
labeled training data and a polarity dictionary of sen-
timent words.
3 Simple Voting by Sentiment Words
One of the simplest ways to classify sentences us-
ing word-level polarities would be a majority voting,
where the occurrences of positive words and those
of negative words in the given sentence are counted
and compared with each other. However, this major-
ity voting method has several weaknesses. First, the
majority voting cannot take into account at all the
phenomenon that the word-level polarity is not al-
ways the same as the polarity of the sentence. Con-
sider the following example:
I have not had any distortion problems
with this phone and am more pleased with
this phone than any I?ve used before.
where negative words are underlined and positive
words are double-underlined. The example sentence
has the positive polarity, though it locally contains
negative words. The majority voting would misclas-
sify it because of the two negative words.
This kind of inconsistency between sentence-level
polarity and word-level polarity often occurs and
causes errors in the majority voting. The reason
is that the majority voting cannot take into ac-
count negation expressions or adversative conjunc-
tions, e.g., ?I have not had any ...? in the example
above. Therefore, taking such polarity-shifting into
account is important for classification of sentences
using a polarity dictionary. To circumvent this prob-
lem, Kennedy and Inkpen (2006) and Hu and Liu
(2004) proposed to use a manually-constructed list
of polarity-shifters. However, it has limitations due
to the diversity of expressions.
297
Another weakness of the majority voting is that
it cannot be easily combined with existing methods
that use the n-gram model or tree structures of the
sentence as features. The method we propose here
can easily be combined with existing methods and
show better performance.
4 Word-Level Polarity-Shifting Model
We assume that when the polarity of a word is dif-
ferent from the polarity of the sentence, the polarity
of the word is shifted by its context to adapt to the
polarity of the sentence. Capturing such polarity-
shifts will improve the classification performance of
the majority voting classifier as well as of more so-
phisticated classifiers.
In this paper, we propose a word polarity-shifting
model to capture such phenomena. This model is
a kind of binary classification model which deter-
mines whether the polarity is shifted by its context.
The model assigns a score sshift(x, S) to the senti-
ment word x in the sentence S. If the polarity of x
is shifted in S, sshift(x, S) > 0. If the polarity of x
is not shifted in S, sshift(x, S) ? 0. Let w be a pa-
rameter vector of the model and ? be a pre-defined
feature function. Function sshift is defined as
sshift(x, S) = w ? ?(x, S). (1)
Since this model is a linear discriminative model,
there are well-known algorithms to estimate the pa-
rameters of the model.
Usually, such models are trained with each occur-
rence of words as one instance (word-wise learning).
However, we can train our model more effectively
with each sentence being one instance (sentence-
wise learning). In this section, we describe how to
train our model in two different ways and how to
apply the model to a sentence classification.
4.1 Word-wise Learning
In this learning method, we train the word-level
polarity-shift model with each occurrence of sen-
timent words being an instance. Training exam-
ples are automatically extracted by finding sentiment
words in labeled sentences. In the example of Sec-
tion 3, for instance, both negative words (?distor-
tion? or ?problems?) and a positive word (?pleased?)
appear in a positive sentence. We regard ?distortion?
and ?problems?, whose polarities are different from
that of the sentence, as belonging to the polarity-
shifted class. On the contrary, we regard ?pleased?,
whose polarity is the same as that of the sentence, as
not belonging to polarity-shifted class.
We can use the majority voting by those (possi-
bly polarity-shifted) sentiment words. Specifically,
we first classify each sentiment word in the sentence
according to whether the polarity is shifted or not.
Then we use the majority voting to determine the
polarity of the sentence. If the first classifier classi-
fies a positive word into the ?polarity-shifted? class,
we treat the word as a negative one. We expect that
the majority voting with polarity-shifting will out-
perform the simple majority voting without polarity-
shifting. We actually use the weighted majority vot-
ing, where the polarity-shifting score for each senti-
ment word is used as the weight of the vote by the
word. We expect that the score works as a confi-
dence measure.
We can formulate this method as follows. Here,
N and P are respectively defined as the sets of neg-
ative sentiment words and positive sentiment words.
For instance, x ? N means that x is a negative word.
We also write x ? S to express that the word x oc-
curs in S.
First, let us define two scores, scorep(S) and
scoren(S), for the input sentence S. The scorep(S)
and the scoren(S) respectively represent the num-
ber of votes for S being positive and the number
of votes for S being negative. If scorep(S) >
scoren(S), we regard the sentence S as having the
positive polarity, otherwise negative. We suppose
that the following relations hold for the scores:
scorep(S) =
?
x?P?S
?sshift(x, S) +
?
x?N?S
sshift(x, S), (2)
scoren(S) =
?
x?P?S
sshift(x, S) +
?
x?N?S
?sshift(x, S). (3)
When either a polarity-unchanged positive word
(sshift(x, S) ? 0) or a polarity-shifted negative
word occurs in the sentence S, scorep(S) increases.
We can easily obtain the following relation between
two scores:
scorep(S) = ?scoren(S). (4)
298
Since, according to this relation, scorep(S) >
scoren(S) is equivalent to scorep(S) > 0, we use
only scorep(S) for the rest of this paper.
4.2 Sentence-wise Learning
The equation (2) can be rewritten as
scorep(S) =
?
x?S
sshift(x, S)I(x)
=
?
x?S
w ? ?(x, S)I(x)
= w ?
{
?
x?S
?(x, S)I(x)
}
, (5)
where I(x) is the function defined as follows:
I(x) =
?
?
?
?
?
+1 if x ? N ,
?1 if x ? P ,
0 otherwise.
(6)
This scorep(S) can also be seen as a linear discrimi-
native model and the parameters of the model can be
estimated directly (i.e., without carrying out word-
wise learning). Each labeled sentence in a corpus
can be used as a training instance for the model.
In this method, the model is learned so that the
predictive ability for sentence classification is opti-
mized, instead of the predictive ability for polarity-
shifting. Therefore, this model can remain indeci-
sive on the classification of word instances that have
little contextual evidence about whether polarity-
shifting occurs or not. The model can rely more
heavily on word instances that have much evidence.
In contrast, the word-wise learning trains the
model with all the sentiment words appearing in a
corpus. It is assumed here that all the sentiment
words have relations with the sentence-level polar-
ity, and that we can always find the evidence of the
phenomena that the polarity of a word is different
from that of a sentence. Obviously, this assump-
tion is not always correct. As a result, the word-wise
learning sometimes puts a large weight on a context
word that is irrelevant to the polarity-shifting. This
might degrade the performance of sentence classifi-
cation as well as of polarity-shifting.
4.3 Hybrid Model
Both methods described in Sections 4.1 and 4.2
are to predict the sentence-level polarity only with
the word-level polarity. On the other hand, sev-
eral methods that use another set of features, for ex-
ample, bag-of-words, n-grams or dependency trees,
were proposed for the sentence or document classi-
fication tasks. We propose to combine our method
with existing methods. We refer to it as hybrid
model.
In recent work, discriminative models including
SVM are often used with many different features.
These methods are generally represented as
score?p(X) = w? ? ??(X), (7)
where X indicates the target of classification, for ex-
ample, a sentence or a document. If score?p(X) > 0,
X is classified into the target class. ??(X) is a fea-
ture function. When the method uses the bag-of-
words model, ?? maps X to a vector with each ele-
ment corresponding to a word.
Here, we define new score function scorecomb(S)
as a linear combination of scorep(S), the score
function of our sentence-wise learning, and
score?p(S), the score function of an existing
method. Using this, we can write the function as
scorecomb(S) = ?scorep(S) + (1 ? ?)score?p(S)
= ?
?
x?S
w ? ?(x, S)I(x) + (1 ? ?)w? ? ??(S)
= wcomb ?
?
?
?
x?S
?(x, S)I(x), (1 ? ?)??(S)
?
. (8)
Note that ?? indicates the concatenation of two vec-
tors, wcomb is defined as ?w, w?? and ? is a param-
eter which controls the influence of the word-level
polarity-shifting model. This model is also a dis-
criminative model and we can estimate the param-
eters with a variety of algorithms including SVMs.
We can incorporate additional information like bag-
of-words or dependency trees by ??(S).
4.4 Discussions on the Proposed Model
Features such as n-grams or dependency trees can
also capture some negations or polarity-shifters. For
example, although ?satisfy? is positive, the bigram
model will learn ?not satisfy? as a feature corre-
lated with negative polarity if it appears in the train-
ing data. However, the bigram model cannot gener-
alize the learned knowledge to other features such
299
Table 1: Statistics of the corpus
customer movie
# of Labeled Sentences 1,700 10,662
Available 1,436 9,492
# of Sentiment Words 3,276 26,493
Inconsistent Words 1,076 10,674
as ?not great? or ?not disappoint?. On the other
hand, our polarity-shifter model learns that the word
?not? causes polarity-shifts. Therefore, even if there
was no ?not disappoint? in training data, our model
can determine that ?not disappoint? has correlation
with positive class, because the dictionary contains
?disappoint? as a negative word. For this reason,
the polarity-shifting model can be learned even with
smaller training data.
What we can obtain from the proposed method is
not only a set of polarity-shifters. We can also obtain
the weight vector w, which indicates the strength of
each polarity-shifter and is learned so that the pre-
dictive ability of sentence classification is optimized
especially in the sentence-wise learning. It is impos-
sible to manually determine such weights for numer-
ous features.
It is also worth noting that all the models proposed
in this paper can be represented as a kernel function.
For example, the hybrid model can be seen as the
following kernel:
Kcomb(S1, S2) = ?
?
xi?S1
?
xj?S2
K((xi, S1), (xj , S2))
+(1 ? ?)K ?(S1, S2). (9)
Here, K means the kernel function between
words and K ? means the kernel function be-
tween sentences respectively. In addition,
?
xi
?
xjK((xi, S1), (xj , S2)) can be seen as
an instance of convolution kernels, which was
proposed by Haussler (1999). Convolution kernels
are a general class of kernel functions which are
calculated on the basis of kernels between substruc-
tures of inputs. Our proposed kernel treats sentences
as input, and treats sentiment words as substructures
of sentences. We can use high degree polynomial
kernels as both K which is a kernel between sub-
structures, i.e. sentiment words, of sentences, and
K ? which is a kernel between sentences to make the
classifiers take into consideration the combination
of features.
5 Evaluation
5.1 Datasets
We used two datasets, customer reviews 1 (Hu
and Liu, 2004) and movie reviews 2 (Pang and
Lee, 2005) to evaluate sentiment classification of
sentences. Both of these two datasets are often
used for evaluation in sentiment analysis researches.
The number of examples and other statistics of the
datasets are shown in Table 1.
Our method cannot be applied to sentences which
contain no sentiment words. We therefore elimi-
nated such sentences from the datasets. ?Available?
in Table 1 means the number of examples to which
our method can be applied. ?Sentiment Words?
shows the number of sentiment words that are found
in the given sentences. Please remember that senti-
ment words are defined as those words that are listed
in a predefined polarity dictionary in this paper. ?In-
consistent Words? shows the number of the words
whose polarities conflicted with the polarity of the
sentence.
We performed 5-fold cross-validation and used
the classification accuracy as the evaluation mea-
sure. We extracted sentiment words from General
Inquirer (Stone et al, 1996) and constructed a polar-
ity dictionary. After some preprocessing, the dictio-
nary contains 2,084 positive words and 2,685 nega-
tive words.
5.2 Experimental Settings
We employed the Max Margin Online Learning
Algorithms for parameter estimation of the model
(Crammer et al, 2006; McDonald et al, 2007).
In preliminary experiments, this algorithm yielded
equal or better results compared to SVMs. As the
feature representation, ?(x, S), of polarity-shifting
model, we used the local context of three words
to the left and right of the target sentiment word.
We used the polynomial kernel of degree 2 for
polarity-shifting model and the linear kernel for oth-
1http://www.cs.uic.edu/?liub/FBS/FBS.
html
2http://www.cs.cornell.edu/people/pabo/
movie-review-data/
300
Table 2: Experimental results of the sentence classi-
fication
methods customer movie
Baseline 0.638 0.504
BoW 0.790 0.724
2gram 0.809 0.756
3gram 0.800 0.762
Simple-Voting 0.716 0.624
Negation Voting 0.733 0.658
Word-wise 0.783 0.699
Sentence-wise 0.806 0.718
Hybrid BoW 0.827 0.748
Hybrid 2gram 0.840 0.755
Hybrid 3gram 0.837 0.758
Opt 0.840 0.770
ers, and feature vectors are normalized to 1. In hy-
brid models, the feature vectors,
?
x?S ?(x, S)I(x)
and ??(S) are normalized respectively.
5.3 Comparison of the Methods
We compared the following methods:
? Baseline classifies all sentences as positive.
? BoW uses unigram features. 2gram uses uni-
grams and bigrams. 3gram uses unigrams, bi-
grams, and 3grams.
? Simple-Voting is the most simple majority vot-
ing with word-level polarity (Section 3).
? Negation Voting proposed by Hu and
Liu (2004) is the majority voting that takes
negations into account. As negations, we
employed not, no, yet, never, none, nobody,
nowhere, nothing, and neither, which are taken
from (Polanyi and Zaenen, 2004; Kennedy and
Inkpen, 2006; Hu and Liu, 2004) (Section 3).
? Word-wise was described in Section 4.1.
? Sentence-wise was described in Section 4.2.
? Hybrid BoW, hybrid 2gram, hybrid 3gram
are combinations of sentence-wise model and
respectively BoW, 2gram and 3gram (Section
4.3). We set ? = 0.5.
Table 2 shows the results of these experiments.
Hybrid 3gram, which corresponds to the proposed
method, obtained the best accuracy on customer re-
view dataset. However, on movie review dataset,
the proposed method did not outperform 3gram. In
Section 5.4, we will discuss this result in details.
Comparing word-wise to simple-voting, the accu-
racy increased by about 7 points. This means that
the polarity-shifting model can capture the polarity-
shifts and it is an important factor for sentiment clas-
sification. In addition, we can see the effectiveness
of sentence-wise, by comparing it to word-wise in
accuracy.
?Opt? in Table 2 shows the results of hybrid mod-
els with optimal ? and combination of models. The
optimal results of hybrid models achieved the best
accuracy on both datasets.
We show some dominating polarity-shifters ob-
tained through learning. We obtained many nega-
tions (e.g., no, not, n?t, never), modal verbs (e.g.,
might, would, may), prepositions (e.g., without, de-
spite), comma with a conjunction (e.g., ?, but? as
in ?the case is strong and stylish, but lacks a win-
dow?), and idiomatic expressions (e.g., ?hard resist?
as in ?it is hard to resist?, and ?real snooze?).
5.4 Effect of Training Data Size
When we have a large amount of training data, the n-
gram classifier can learn well whether each n-gram
tends to appear in the positive class or the negative
class. However, when we have only a small amount
of training data, the n-gram classifier cannot capture
such tendency. Therefore the external knowledge,
such as word-level polarity, could be more valuable
information for classification. Thus it is expected
that the sentence-wise model and the hybrid model
will outperform n-gram classifier which does not
take word-level polarity into account, more largely
with few training data.
To verify this conjecture, we conducted experi-
ments by changing the number of the training ex-
amples, i.e., the labeled sentences. We evaluated
three models: sentence-wise, 3gram model and hy-
brid 3gram on both customer review and movie re-
view.
Figures 1 and 2 show the results on customer re-
view and movie review respectively. When the size
of the training data is small, sentence-wise outper-
301
Figure 1: Experimental results on customer review
Figure 2: Experimental results on movie review
forms 3gram on both datasets. We can also see that
the advantage of sentence-wise becomes smaller as
the amount of training data increases, and that the
hybrid 3gram model almost always achieved the best
accuracy among the three models. Similar behaviour
was observed when we ran the same experiments
with 2gram or BoW model. From these results, we
can conclude that, as we expected above, the word-
level polarity is especially effective when we have
only a limited amount of training data, and that the
hybrid model can combine two models effectively.
6 Conclusion
We proposed a model that captures the polarity-
shifting of sentiment words in sentences. We also
presented two different learning methods for the
model and proposed an augmented hybrid classifier
that is based both on the model and on existing clas-
sifiers. We evaluated our method and reported that
the proposed method almost always improved the
accuracy of sentence classification compared with
other simpler methods. The improvement was more
significant when we have only a limited amount of
training data.
For future work, we plan to explore new feature
sets appropriate for our model. The feature sets we
used for evaluation in this paper are not necessar-
ily optimal and we can expect a better performance
by exploring appropriate features. For example, de-
pendency relations between words or appearances of
conjunctions will be useful. The position of a word
in the given sentence is also an important factor in
sentiment analysis (Taboada and Grieve, 2004). Fur-
thermore, we should directly take into account the
fact that some words do not affect the polarity of the
sentence, though the proposed method tackled this
problem indirectly. We cannot avoid this problem
to use word-level polarity more effectively. Lastly,
since we proposed a method for the sentence-level
sentiment prediction, our next step is to extend the
method to the document-level sentiment prediction.
Acknowledgement
This research was supported in part by Overseas Ad-
vanced Educational Research Practice Support Pro-
gram by Ministry of Education, Culture, Sports, Sci-
ence and Technology.
References
Koby Crammer, Ofer Dekel, Joseph Keshet, Shai
Shalev-Shwartz, and Yoram Singer. Online Passive-
Aggressive Algorithms. In Journal of Machine Learn-
ing Research, Vol.7, Mar, pp.551?585, 2006.
Michael Gamon. Sentiment classification on customer
feedback data: noisy data, large feature vectors, and
the role of linguistic analysis. In Proceedings of the
20th International Conference on Computational Lin-
guistics (COLING-2004) , pp.841?847, 2004.
David Haussler. Convolution Kernels on Discrete Struc-
tures, Technical Report UCS-CRL-99-10, University
of California in Santa Cruz, 1999.
Minqing Hu and Bing Liu. Mining Opinion Features
in Customer Reviews. In Proceedings of Nineteeth
National Conference on Artificial Intellgience (AAAI-
2004) , pp.755?560, San Jose, USA, July 2004.
302
Alistair Kennedy and Diana Inkpen. Sentiment Classi-
fication of Movie and Product Reviews Using Con-
textual Valence Shifters. In Workshop on the Analysis
of Formal and Informal Information Exchange during
Negotiations (FINEXIN-2005), 2005.
Taku Kudo and Yuji Matsumoto. A Boosting Algorithm
for Classification of Semi-Structured Text. In Proceed-
ings of the Conference on Empirical Methods in Nat-
ural Language Processing (EMNLP-2004), pp.301?
308, 2004.
Yu Mao and Guy Lebanon. Isotonic Conditional Ran-
dom Fields and Local Sentiment Flow. In Proceedings
of the Newral Information Processing Systems (NIPS-
2006), pp.961?968, 2006.
Shotaro Matsumoto, Hiroya Takamura, and Manabu
Okumura. Sentiment Classification using Word Sub-
Sequences and Dependency Sub-Trees. In Proceed-
ings of the 9th Pacific-Asia International Conference
on Knowledge Discovery and Data Mining (PAKDD-
2005), pp.301?310 , 2005.
Ryan McDonald, Kerry Hannan, Tyler Neylon, Mike
Wells, and Jeff Reynar. Structured Models for Fine-to-
Coarse Sentiment Analysis. In Proceedings of the 45th
Annual Meeting of the Association for Computational
Linguistics (ACL-2007), pp.432?439, 2007.
Tony Mullen and Nigel Collier. Sentiment analysis us-
ing support vector machines with diverse informa-
tion sources. In Proceedings of the Conference on
Empirical Methods in Natural Language Processing
(EMNLP-2004), pp.412?418, 2004.
Bo Pang, Lillian Lee, and Shivakumar Vaithyanathan.
Thumbs up? Sentiment Classification using Machine
Learning Techniques. In Proceedings of the Confer-
ence on Empirical Methods in Natural Language Pro-
cessing (EMNLP-2002), pp.76?86, 2002.
Bo Pang and Lillian Lee. A Sentimental Education:
Sentiment Analysis Using Subjectivity Summarization
Based on Minimum Cuts. In Proceedings of the 42th
Annual Meeting of the Association for Computational
Linguistics (ACL-2004), pp.271?278, 2004.
Bo Pang and Lillian Lee. Seeing stars: Exploiting class
relationships for sentiment categorization with respect
to rating scales. In Proceedings of the 43rd Annual
Meeting of the Association for Computational Linguis-
tics (ACL-2005), pp.115?124, 2005.
Livia Polanyi and Annie Zaenen. Contextual Valence
Shifters. In AAAI Spring Symposium on Exploring At-
titude and Affect in Text: Theories and Applications
(AAAI-EAAT2004), 2004.
Philip J. Stone, Dexter C. Dunphy, Marshall S. Smith,
and Daniel M. Ogilvie. The General Inquirer: A Com-
puter Approach to Content Analysis. The MIT Press,
1996.
Maite Taboada and Jack Grieve. Analyzing Appraisal
Automatically. In AAAI Spring Symposium on Explor-
ing Attitude and Affect in Text: Theories and Applica-
tions (AAAI-EAAT2004), pp.158?161, 2004.
303
Proceedings of NAACL HLT 2007, pages 292?299,
Rochester, NY, April 2007. c?2007 Association for Computational Linguistics
Extracting Semantic Orientations of Phrases from Dictionary
Hiroya Takamura
Precision and Intelligence Laboratory
Tokyo Institute of Technology
takamura@pi.titech.ac.jp
Takashi Inui
Integrated Research Institute
Tokyo Institute of Technology
inui@iri.titech.ac.jp
Manabu Okumura
Precision and Intelligence Laboratory
Tokyo Institute of Technology
oku@pi.titech.ac.jp
Abstract
We propose a method for extracting se-
mantic orientations of phrases (pairs of an
adjective and a noun): positive, negative,
or neutral. Given an adjective, the seman-
tic orientation classification of phrases can
be reduced to the classification of words.
We construct a lexical network by con-
necting similar/related words. In the net-
work, each node has one of the three ori-
entation values and the neighboring nodes
tend to have the same value. We adopt
the Potts model for the probability model
of the lexical network. For each adjec-
tive, we estimate the states of the nodes,
which indicate the semantic orientations
of the adjective-noun pairs. Unlike ex-
isting methods for phrase classification,
the proposed method can classify phrases
consisting of unseen words. We also pro-
pose to use unlabeled data for a seed set of
probability computation. Empirical evalu-
ation shows the effectiveness of the pro-
posed method.
1 Introduction
Technology for affect analysis of texts has recently
gained attention in both academic and industrial ar-
eas. It can be applied to, for example, a survey of
new products or a questionnaire analysis. Automatic
sentiment analysis enables a fast and comprehensive
investigation.
The most fundamental step for sentiment analy-
sis is to acquire the semantic orientations of words:
positive or negative (desirable or undesirable). For
example, the word ?beautiful? is positive, while the
word ?dirty? is negative. Many researchers have de-
veloped several methods for this purpose and ob-
tained good results. One of the next problems to be
solved is to acquire semantic orientations of phrases,
or multi-term expressions, such as ?high+risk? and
?light+laptop-computer?. Indeed the semantic ori-
entations of phrases depend on context just as the se-
mantic orientations of words do, but we would like
to obtain the orientations of phrases as basic units
for sentiment analysis. We believe that we can use
the obtained basic orientations of phrases for affect
analysis of higher linguistic units such as sentences
and documents.
A computational model for the semantic orienta-
tions of phrases has been proposed by Takamura et
al. (2006). However, their method cannot deal with
the words that did not appear in the training data.
The purpose of this paper is to propose a method for
extracting semantic orientations of phrases, which is
applicable also to expressions consisting of unseen
words. In our method, we regard this task as the
noun classification problem for each adjective; the
nouns that become respectively positive (negative,
or neutral) when combined with a given adjective
are distinguished from the other nouns. We create
a lexical network with words being nodes, by con-
necting two words if one of the two appears in the
gloss of the other. In the network, each node has one
of the three orientation values and the neighboring
nodes expectedly tend to have the same value. For
292
example, the gloss of ?cost? is ?a sacrifice, loss, or
penalty? and these words (cost, sacrifice, loss, and
penalty) have the same orientation. To capture this
tendency of the network, we adopt the Potts model
for the probability distribution of the lexical net-
work. For each adjective, we estimate the states of
the nodes, which indicate the semantic orientations
of the adjective-noun pairs. Information from seed
words is diffused to unseen nouns on the network.
We also propose a method for enlarging the seed
set by using the output of an existing method for the
seed words of the probability computation.
Empirical evaluation shows that our method
works well both for seen and unseen nouns, and that
the enlarged seed set significantly improves the clas-
sification performance of the proposed model.
2 Related Work
The semantic orientation classification of words has
been pursued by several researchers. Some of
them used corpora (Hatzivassiloglou and McKeown,
1997; Turney and Littman, 2003), while others used
dictionaries (Kobayashi et al, 2001; Kamps et al,
2004; Takamura et al, 2005; Esuli and Sebastiani,
2005).
Turney (2002) applied an internet-based tech-
nique to the semantic orientation classification of
phrases, which had originally been developed for
word sentiment classification. In their method, the
number of hits returned by a search-engine, with a
query consisting of a phrase and a seed word (e.g.,
?phrase NEAR good?) is used to determine the ori-
entation. Baron and Hirst (2004) extracted colloca-
tions with Xtract (Smadja, 1993) and classified the
collocations using the orientations of the words in
the neighboring sentences. Their method is similar
to Turney?s in the sense that cooccurrence with seed
words is used. In addition to individual seed words,
Kanayama and Nasukawa (2006) used more compli-
cated syntactic patterns that were manually created.
The four methods above are based on context infor-
mation. In contrast, our method exploits the internal
structure of the semantic orientations of phrases.
Wilson et al (2005) worked on phrase-level se-
mantic orientations. They introduced a polarity
shifter. They manually created the list of polarity
shifters. Inui (2004) also proposed a similar idea.
Takamura et al (2006) proposed to use based on
latent variable models for sentiment classification of
noun-adjective pairs. Their model consists of vari-
ables respectively representing nouns, adjectives, se-
mantic orientations, and latent clusters, as well as
the edges between the nodes. The words that are
similar in terms of semantic orientations, such as
?risk? and ?mortality? (i.e., the positive orientation
emerges when they are ?low?), make a cluster in
their model, which can be an automated version of
Inui?s or Wilson et al?s idea above. However, their
method cannot do anything for the words that did not
appear in the labeled training data. In this paper, we
call their method the latent variable method (LVM).
3 Potts Model
If a variable can have more than two values and
there is no ordering relation between the values,
the network comprised of such variables is called
Potts model (Wu, 1982). In this section, we ex-
plain the simplified mathematical model of Potts
model, which is used for our task in Section 4.
The Potts system has been used as a mathematical
model in several applications such as image restora-
tion (Tanaka and Morita, 1996) and rumor transmis-
sion (Liu et al, 2001).
3.1 Introduction to the Potts Model
Suppose a network consisting of nodes and weighted
edges is given. States of nodes are represented by c.
The weight between i and j is represented by wij .
Let H(c) denote an energy function, which indi-
cates a state of the whole network:
H(c) = ??
?
ij
wij?(ci, cj)+?
?
i?L
??(ci, ai), (1)
where ? is a constant called the inverse-temperature,
L is the set of the indices for the observed variables,
ai is the state of each observed variable indexed by i,
and ? is a positive constant representing a weight on
labeled data. Function ? returns 1 if two arguments
are equal to each other, 0 otherwise. The state is
penalized if ci (i ? L) is different from ai. Using
H(c), the probability distribution of the network is
represented as P (c) = exp{?H(c)}/Z, where Z is
a normalization factor.
However, it is computationally difficult to exactly
estimate the state of this network. We resort to a
293
mean-field approximation method that is described
by Nishimori (2001). In the method, P (c) is re-
placed by factorized function ?(c) =
?
i ?i(ci).
Then we can obtain the function with the smallest
value of the variational free energy:
F (c) =
?
c
P (c)H(c)?
?
c
?P (c) logP (c)
= ??
?
i
?
ci
?i(ci)?(ci, ai)
??
?
ij
?
ci,cj
?i(ci)?j(cj)wij?(ci, cj)
?
?
i
?
ci
??i(ci) log ?i(ci). (2)
By minimizing F (c) under the condition that ?i,?
ci ?i(ci) = 1, we obtain the following fixed point
equation for i ? L:
?i(c) =
exp(??(c, ai) + ?
?
j wij?j(c))?
n exp(??(n, ai) + ?
?
j wij?j(n))
. (3)
The fixed point equation for i /? L can be obtained
by removing ??(c, ai) from above.
This fixed point equation is solved by an itera-
tive computation. In the actual implementation, we
represent ?i with a linear combination of the dis-
crete Tchebycheff polynomials (Tanaka and Morita,
1996). Details on the Potts model and its computa-
tion can be found in the literature (Nishimori, 2001).
After the computation, we obtain the function?
i ?i(ci). When the number of classes is 2, the Potts
model in this formulation is equivalent to the mean-
field Ising model (Nishimori, 2001).
3.2 Relation to Other Models
This Potts model with the mean-field approximation
has relation to several other models.
As is often discussed (Mackay, 2003), the min-
imization of the variational free energy (Equa-
tion (2)) is equivalent to the obtaining the factorized
model that is most similar to the maximum likeli-
hood model in terms of the Kullback-Leibler diver-
gence.
The second term of Equation (2) is the entropy
of the factorized function. Hence the optimization
problem to be solved here is a kind of the maxi-
mum entropy model with a penalty term, which cor-
responds to the first term of Equation (2).
We can find a similarity also to the PageRank al-
gorithm (Brin and Page, 1998), which has been ap-
plied also to natural language processing tasks (Mi-
halcea, 2004; Mihalcea, 2005). In the PageRank al-
gorithm, the pagerank score ri is updated as
ri = (1? d) + d
?
j
wijrj , (4)
where d is a constant (0 ? d ? 1). This update
equation consists of the first term corresponding to
random jump from an arbitrary node and the sec-
ond term corresponding to the random walk from the
neighboring node.
Let us derive the first order Taylor expansion of
Equation (3). We use the equation for i /? L and
denote the denominator by Z? , for simplicity. Since
expx ? 1 + x, we obtain
?i(c) =
exp(?
?
j wij?j(c))
Z?
?
1 + ?
?
j wij?j(c)
Z?
= 1Z?
+ ?Z?
?
j
wij?j(c). (5)
Equation (5) clearly has a quite similar form as
Equation (4). Thus, the PageRank algorithm can be
regarded as an approximation of our model. Let us
clarify the difference between the two algorithms.
The PageRank is designed for two-class classifica-
tion, while the Potts model can be used for an arbi-
trary number of classes. In this sense, the PageRank
is an approximated Ising model. The PageRank is
applicable to asymmetric graphs, while the theory
used in this paper is based on symmetric graphs.
4 Potts Model for Phrasal Semantic
Orientations
In this section, we explain our classification method,
which is applicable also to the pairs consisting of an
adjective and an unseen noun.
4.1 Construction of Lexical Networks
We construct a lexical network, which Takamura et
al. (2005) call the gloss network, by linking two
words if one word appears in the gloss of the other
word. Each link belongs to one of two groups:
294
the same-orientation links SL and the different-
orientation links DL.
If a negation word (e.g., nai, for Japanese) follows
a word in the gloss of the other word, the link is a
different-orientation link. Otherwise the links is a
same-orientation link1.
We next set weights W = (wij) to links :
wij =
?
??
??
1?
d(i)d(j)
(lij ? SL)
? 1?
d(i)d(j)
(lij ? DL)
0 otherwise
, (6)
where lij denotes the link between word i and word
j, and d(i) denotes the degree of word i, which
means the number of words linked with word i. Two
words without connections are regarded as being
connected by a link of weight 0.
4.2 Classification of Phrases
Takamura et al (2005) used the Ising model to ex-
tract semantic orientations of words (not phrases).
We extend their idea and use the Potts model to ex-
tract semantic orientations of phrasal expressions.
Given an adjective, the decision remaining to be
made in classification of phrasal expressions con-
cerns nouns. We therefore estimate the state of the
nodes on the lexical network for each adjective. The
nouns paring with the given adjective in the train-
ing data are regarded as seed words, which we call
seen words, while the words that did not appear in
the training data are referred to as unseen words.
We use the mean-field method to estimate the
state of the system. If the probability ?i(c) of a vari-
able being positive (negative, neutral) is the highest
of the three classes, then the word corresponding to
the variable is classified as a positive (negative, neu-
tral) word.
We explain the reason why we use the Potts model
instead of the Ising model. While only two classes
(i.e., positive and negative) can be modeled by the
Ising model, three classes (i.e., positive, negative
and neutral) can be modelled by the Potts model.
For the semantic orientations of words, all the words
are sorted in the order of the average orientation
value, equivalently the probability of the word be-
ing positive. Therefore, even if the neutral class is
1For English data, a negation should precede a word, in or-
der for the corresponding link to be a different-orientation link.
not explicitly incorporated, we can manually deter-
mine two thresholds that define respectively the pos-
itive/neutral and negative/neutral boundaries. For
the semantic orientations of phrasal expressions,
however, it is impractical to manually determine
the thresholds for each of the numerous adjectives.
Therefore, we have to incorporate the neutral class
using the Potts model.
For some adjectives, the semantic orientation is
constant regardless of the nouns. We need not use
the Potts model for those unambiguous adjectives.
We thus propose the following two-step classifica-
tion procedure for a given noun-adjective pair <
n, a >.
1. if the semantic orientation of all the instances
with a in L is c, then classify < n, a > into c.
2. otherwise, use the Potts model.
We can also construct a probability model for
each noun to deal with unseen adjectives. However,
we focus on the unseen nouns in this paper, because
our dataset has many more nouns than adjectives.
4.3 Hyper-parameter Prediction
The performance of the proposed method largely de-
pends on the value of hyper-parameter ?. In order to
make the method more practical, we propose a cri-
terion for determining its value.
Takamura et al (2005) proposed two kinds of cri-
teria. One of the two criteria is an approximated
leave-one-out error rate and can be used only when a
large labeled dataset is available. The other is a no-
tion from statistical physics, that is, magnetization:
m =
?
i
x?i/N. (7)
At a high temperature, variables are randomly ori-
ented (paramagnetic phase, m ? 0). At a low
temperature, most of the variables have the same
direction (ferromagnetic phase, m 6= 0). It is
known that at some intermediate temperature, ferro-
magnetic phase suddenly changes to paramagnetic
phase. This phenomenon is called phase transition.
Slightly before the phase transition, variables are lo-
cally polarized; strongly connected nodes have the
same polarity, but not in a global way. Intuitively,
the state of the lexical network is locally polarized.
295
Therefore, they calculate values of m with several
different values of ? and select the value just before
the phase transition.
Since we cannot expect a large labeled dataset
to be available for each adjective, we use not
the approximated leave-one-out error rate, but the
magnetization-like criterion. However, the magne-
tization above is defined for the Ising model. We
therefore consider that the phase transition has oc-
curred, if a certain class c begins to be favored all
over the system. In practice, when the maximum of
the spatial averages of the approximated probabil-
ities maxc
?
i ?i(c)/N exceeds a threshold during
increasing ?, we consider that the phase transition
has occurred. We select the value of ? slightly be-
fore the phase transition.
4.4 Enlarging Seed Word Set
We usually have only a few seed words for a given
adjective. Enlarging the set of seed words will in-
crease the classification performance. Therefore, we
automatically classify unlabeled pairs by means of
an existing method and use the classified instances
as seeds.
As an existing classifier, we use LVM. Their
model can classify instances that consist of a seen
noun and a seen adjective, but are unseen as a pair.
Although we could classify and use all the nouns
that appeared in the training data (with an adjective
which is different from the given one), we do not
adopt such an alternative, because it will incorporate
even non-collocating pairs such as ?green+idea? into
seeds, resulting in possible degradation of classifi-
cation performance. Therefore, we sample unseen
pairs consisting of a seen noun and a seen adjective
from a corpus, classify the pairs with the latent vari-
able model, and add them to the seed set. The en-
larged seed set consists of pairs used in newspaper
articles and does not include non-collocating pairs.
5 Experiments
5.1 Dataset
We extracted pairs of a noun (subject) and an ad-
jective (predicate), from Mainichi newspaper arti-
cles (1995) written in Japanese, and annotated the
pairs with semantic orientation tags : positive, neu-
tral or negative. We thus obtained the labeled dataset
consisting of 12066 pair instances (7416 different
pairs). The dataset contains 4459 negative instances,
4252 neutral instances, and 3355 positive instances.
The number of distinct nouns is 4770 and the num-
ber of distinct adjectives is 384. To check the inter-
annotator agreement between two annotators, we
calculated ? statistics, which was 0.6402. This value
is allowable, but not quite high. However, positive-
negative disagreement is observed for only 0.7% of
the data. In other words, this statistics means that
the task of extracting neutral examples, which has
hardly been explored, is intrinsically difficult.
We should note that the judgment in annotation
depends on which perspective the annotator takes;
?high+salary? is positive from employee?s perspec-
tive, but negative from employer?s perspective. The
annotators are supposed to take a perspective subjec-
tively. Our attempt is to imitate annotator?s decision.
To construct a classifier that matches the decision of
the average person, we also have to address how to
create an average corpus. We do not pursue this is-
sue because it is out of the scope of the paper.
As unlabeled data, we extracted approximately
65,000 pairs for each iteration of the 10-fold cross-
validation, from the same news source.
The average number of seed nouns for each am-
biguous adjective was respectively 104 in the la-
beled seed set and 264 in the labeled+unlabeled seed
set. Please note that these figures are counted for
only ambiguous adjectives. Usually ambiguous ad-
jectives are more frequent than unambiguous adjec-
tives.
5.2 Experimental Settings
We employ 10-fold cross-validation to obtain the
averaged classification accuracy. We split the data
such that there is no overlapping pair (i.e., any pair
in the training data does not appear in the test data).
Hyperparameter ? was set to 1000, which is very
large since we regard the labels in the seed set is
reliable. For the seed words added by the classifier,
lower ? can be better. Determining a good value for
? is regarded as future work.
Hyperparameter ? is automatically selected from
2Although Kanayama and Nasukawa (2006) that ? for their
dataset similar to ours was 0.83, this value cannot be directly
compared with our value because their dataset includes both in-
dividual words and pairs of words.
296
{0.1, 0.2, ? ? ?, 2.5} for each adjective and each fold
of the cross-validation using the prediction method
described in Section 4.3.
5.3 Results
The results of the classification experiments are
summarized in Table 1.
The proposed method succeeded in classifying,
with approximately 65% in accuracy, those phrases
consisting of an ambiguous adjective and an unseen
noun, which could not be classified with existing
computational models such as LVM.
Incorporation of unlabeled data improves accu-
racy by 15.5 points for pairs consisting of a seen
noun and an ambiguous adjective, and by 3.5 points
for pairs consisting of an unseen noun and an am-
biguous adjective, approximately. The reason why
the former obtained high increase is that pairs with
an ambiguous adjective3 are usually frequent and
likely to be found in the added unlabeled dataset.
If we regard this classification task as binary clas-
sification problems where we are to classify in-
stances into one class or not, we obtain three accu-
racies: 90.76% for positive, 81.75% for neutral, and
86.85% for negative. This results suggests the iden-
tification of neutral instances is relatively difficult.
Next we compare the proposed method with
LVM. The latent variable method is applicable only
to instance pairs consisting of an adjective and a
seen noun. Therefore, we computed the accuracy
for 6586 instances using the latent variable method
and obtained 80.76 %. The corresponding accuracy
by our method was 80.93%. This comparison shows
that our method is better than or at least comparable
to the latent variable method. However, we have to
note that this accuracy of the proposed method was
computed using the unlabeled data classified by the
latent variable method.
5.4 Discussion
There are still 3320 (=12066-8746) word pairs
which could not be classified, because there are no
entries for those words in the dictionary. However,
the main cause of this problem is word segmenta-
3Seen nouns are observed in both the training and the test
datasets because they are frequent. Ambiguous adjectives are
often-used adjectives such as ?large?, ?small?, ?high?, and
?low?.
tion, since many compound nouns and exceedingly-
subdivided morphemes are not in dictionaries. An
appropriate mapping from the words found in cor-
pus to entries of a dictionary will solve this problem.
We found a number of proper nouns, many of which
are not in the dictionary. By estimating a class of a
proper noun and finding the words that matches the
class in the dictionary, we can predict the semantic
orientations of the proper noun based on the orienta-
tions of the found words.
In order to see the overall tendency of errors, we
calculated the confusion matrices both for pairs of
an ambiguous adjective and a seen noun, and for
pairs of an ambiguous adjective and an unseen noun
(Table 2). The proposed method works quite well for
positive/negative classification, though it finds still
some difficulty in correctly classifying neutral in-
stances even after enhanced with the unlabeled data.
In order to qualitatively evaluate the method,
we list several word pairs below. These word
pairs are classified by the Potts model with the la-
beled+unlabeled seed set. All nouns are unseen;
they did not appear in the original training dataset.
Please note again that the actual data is Japanese.
positive instances
noun adjective
cost low
basic price low
loss little
intelligence high
educational background high
contagion not-happening
version new
cafe many
salary high
commission low
negative instances
noun adjective
damage heavy
chance little
terrorist many
trouble many
variation little
capacity small
salary low
disaster many
disappointment big
knowledge little
For example, although both ?salary? and ?com-
mission? are kinds of money, our method captures
297
Table 1: Classification accuracies (%) for various seed sets and test datasets. ?Labeled? seed set corresponds
to the set of manually labeled pairs. ?Labeled+unlabeled? seed set corresponds to the union of ?labeled? seed
set and the set of pairs labeled by LVM. ?Seen nouns? for test are the nouns that appeared in the training
data, while ?unseen nouns? are the nouns that did not appear in the training dataset?. Please note that seen
pairs are excluded from the test data. ?Unambiguous? adjectives corresponds to the pairs with an adjective
which has a unique orientation in the original training dataset, while ?ambiguous? adjectives corresponds to
the pairs with an adjective which has more than one orientation in the original training dataset.
seed\test seen nouns unseen nouns total
labeled 68.24 73.70 69.59
(4494/6586) (1592/2160) (6086/8746)
unambiguous ambiguous unambiguous ambiguous
98.15 61.65 94.85 61.85
(1166/1188) (3328/5398) (736/776) (856/1384)
labeled+unlabeled 80.93 75.88 79.68
(5330/6586) (1639/2160) (6969/8746)
unambiguous ambiguous unambiguous ambiguous
98.15 77.14 94.85 65.25
(1166/1188) (4164/5398) (736/776) (903/1384)
Table 2: Confusion matrices of classification result with labeled+unlabeled seed set
Potts model
seen nouns unseen nouns
positive neutral negative sum positive neutral negative sum
positive 964 254 60 1278 126 84 30 240
Gold standard neutral 198 1656 286 2140 60 427 104 591
negative 39 397 1544 1980 46 157 350 553
sum 1201 2307 1890 5398 232 668 484 1384
the difference between them; ?high salary? is posi-
tive, while ?low (cheap) commission? is also posi-
tive.
6 Conclusion
We proposed a method for extracting semantic ori-
entations of phrases (pairs of an adjective and a
noun). For each adjective, we constructed a Potts
system, which is actually a lexical network extracted
from glosses in a dictionary. We empirically showed
that the proposed method works well in terms of
classification accuracy.
Future work includes the following:
? We assumed that each word has a semantic ori-
entation. However, word senses and subjectiv-
ity have strong interaction (Wiebe and Mihal-
cea, 2006).
? The value of ? must be properly set, because
lower ? can be better for the seed words added
by the classifier,
? To address word-segmentation problem dis-
cussed in Section 5.3, we can utilize the fact
that the heads of compound nouns often inherit
the property determining the semantic orienta-
tion when combined with an adjective.
? The semantic orientations of pairs consisting of
a proper noun will be estimated from the named
entity classes of the proper nouns such as per-
son name and organization.
298
References
Faye Baron and Graeme Hirst. 2004. Collocations as
cues to semantic orientation. In AAAI Spring Sympo-
sium on Exploring Attitude and Affect in Text: Theo-
ries and Applications.
Sergey Brin and Lawrence Page. 1998. The anatomy of
a large-scale hypertextual Web search engine. Com-
puter Networks and ISDN Systems, 30(1?7):107?117.
Andrea Esuli and Fabrizio Sebastiani. 2005. Determin-
ing the semantic orientation of terms through gloss
analysis. In Proceedings of the 14th ACM Inter-
national Conference on Information and Knowledge
Management (CIKM?05), pages 617?624.
Vasileios Hatzivassiloglou and Kathleen R. McKeown.
1997. Predicting the semantic orientation of adjec-
tives. In Proceedings of the 35th Annual Meeting of
the Association for Computational Linguistics and the
8th Conference of the European Chapter of the Asso-
ciation for Computational Linguistics, pages 174?181.
Takashi Inui. 2004. Acquiring Causal Knowledge from
Text Using Connective Markers. Ph.D. thesis, Grad-
uate School of Information Science, Nara Institute of
Science and Technology.
Jaap Kamps, Maarten Marx, Robert J. Mokken, and
Maarten de Rijke. 2004. Using wordnet to measure
semantic orientation of adjectives. In Proceedings
of the 4th International Conference on Language Re-
sources and Evaluation (LREC?04), volume IV, pages
1115?1118.
Hiroshi Kanayama and Tetsuya Nasukawa. 2006. Fully
automatic lexicon expansion for domain-oriented sen-
timent analysis. In Proceedings of the Conference on
Empirical Methods in Natural Language Processing
(EMNLP?06), pages 355?363.
Nozomi Kobayashi, Takashi Inui, and Kentaro Inui.
2001. Dictionary-based acquisition of the lexical
knowledge for p/n analysis (in Japanese). In Pro-
ceedings of Japanese Society for Artificial Intelligence,
SLUD-33, pages 45?50.
Zhongzhu Liu, Jun Luo, and Chenggang Shao. 2001.
Potts model for exaggeration of a simple rumor trans-
mitted by recreant rumormongers. Physical Review E,
64:046134,1?046134,9.
David J. C. Mackay. 2003. Information Theory, Infer-
ence and Learning Algorithms. Cambridge University
Press.
Mainichi. 1995. Mainichi Shimbun CD-ROM version.
Rada Mihalcea. 2004. Graph-based ranking algorithms
for sentence extraction, applied to text summarization.
In The Companion Volume to the Proceedings of the
42nd Annual Meeting of the Association for Computa-
tional Linguistics, (ACL?04), pages 170?173.
Rada Mihalcea. 2005. Unsupervised large-vocabulary
word sense disambiguation with graph-based algo-
rithms for sequence data labeling. In Proceedings of
the Joint Conference on Human Language Technology
/ Empirical Methods in Natural Language Processing
(HLT/EMNLP), pages 411?418.
Hidetoshi Nishimori. 2001. Statistical Physics of Spin
Glasses and Information Processing. Oxford Univer-
sity Press.
Frank Z. Smadja. 1993. Retrieving collocations from
text: Xtract. Computational Linguistics, 19(1):143?
177.
Hiroya Takamura, Takashi Inui, and Manabu Okumura.
2005. Extracting semantic orientations of words using
spin model. In Proceedings of the 43rd Annual Meet-
ing of the Association for Computational Linguistics
(ACL?05), pages 133?140.
Hiroya Takamura, Takashi Inui, and Manabu Okumura.
2006. Latent variable models for semantic orientations
of phrases. In Proceedings of the 11th Conference of
the European Chapter of the Association for Compu-
tational Linguistics (EACL?06).
Kazuyuki Tanaka and Tohru Morita. 1996. Application
of cluster variation method to image restoration prob-
lem. In Theory and Applications of the Cluster Vari-
ation and Path Probability Methods, pages 353?373.
Plenum Press, New York.
Peter D. Turney and Michael L. Littman. 2003. Measur-
ing praise and criticism: Inference of semantic orien-
tation from association. ACM Transactions on Infor-
mation Systems, 21(4):315?346.
Peter D. Turney. 2002. Thumbs up or thumbs down?
semantic orientation applied to unsupervised classifi-
cation of reviews. In Proceedings 40th Annual Meet-
ing of the Association for Computational Linguistics
(ACL?02), pages 417?424.
Janyce M. Wiebe and Rada Mihalcea. 2006. Word sense
and subjectivity. In Proceedings of the 21st Interna-
tional Conference on Computational Linguistics and
the 44th annual meeting of the Association for Compu-
tational Linguistics (COLING-ACL?06), pages 1065?
1072.
Theresa Wilson, Janyce Wiebe, and Paul Hoffmann.
2005. Recognizing contextual polarity in phrase-level
sentiment analysis. In Proceedings of joint confer-
ence on Human Language Technology / Conference on
Empirical Methods in Natural Language Processing
(HLT/EMNLP?05), pages 347?354.
Fa-Yueh Wu. 1982. The potts model. Reviews of Mod-
ern Physics, 54(1):235?268.
299
Proceedings of the 43rd Annual Meeting of the ACL, pages 133?140,
Ann Arbor, June 2005. c?2005 Association for Computational Linguistics
Extracting Semantic Orientations of Words using Spin Model
Hiroya Takamura Takashi Inui Manabu Okumura
Precision and Intelligence Laboratory
Tokyo Institute of Technology
4259 Nagatsuta Midori-ku Yokohama, 226-8503 Japan
{takamura,oku}@pi.titech.ac.jp,
tinui@lr.pi.titech.ac.jp
Abstract
We propose a method for extracting se-
mantic orientations of words: desirable
or undesirable. Regarding semantic ori-
entations as spins of electrons, we use
the mean field approximation to compute
the approximate probability function of
the system instead of the intractable ac-
tual probability function. We also pro-
pose a criterion for parameter selection on
the basis of magnetization. Given only
a small number of seed words, the pro-
posed method extracts semantic orienta-
tions with high accuracy in the exper-
iments on English lexicon. The result
is comparable to the best value ever re-
ported.
1 Introduction
Identification of emotions (including opinions and
attitudes) in text is an important task which has a va-
riety of possible applications. For example, we can
efficiently collect opinions on a new product from
the internet, if opinions in bulletin boards are auto-
matically identified. We will also be able to grasp
people?s attitudes in questionnaire, without actually
reading all the responds.
An important resource in realizing such identifi-
cation tasks is a list of words with semantic orienta-
tion: positive or negative (desirable or undesirable).
Frequent appearance of positive words in a docu-
ment implies that the writer of the document would
have a positive attitude on the topic. The goal of this
paper is to propose a method for automatically cre-
ating such a word list from glosses (i.e., definition
or explanation sentences ) in a dictionary, as well as
from a thesaurus and a corpus. For this purpose, we
use spin model, which is a model for a set of elec-
trons with spins. Just as each electron has a direc-
tion of spin (up or down), each word has a semantic
orientation (positive or negative). We therefore re-
gard words as a set of electrons and apply the mean
field approximation to compute the average orienta-
tion of each word. We also propose a criterion for
parameter selection on the basis of magnetization, a
notion in statistical physics. Magnetization indicates
the global tendency of polarization.
We empirically show that the proposed method
works well even with a small number of seed words.
2 Related Work
Turney and Littman (2003) proposed two algorithms
for extraction of semantic orientations of words. To
calculate the association strength of a word with pos-
itive (negative) seed words, they used the number
of hits returned by a search engine, with a query
consisting of the word and one of seed words (e.g.,
?word NEAR good?, ?word NEAR bad?). They re-
garded the difference of two association strengths as
a measure of semantic orientation. They also pro-
posed to use Latent Semantic Analysis to compute
the association strength with seed words. An em-
pirical evaluation was conducted on 3596 words ex-
tracted from General Inquirer (Stone et al, 1966).
Hatzivassiloglou and McKeown (1997) focused
on conjunctive expressions such as ?simple and
133
well-received? and ?simplistic but well-received?,
where the former pair of words tend to have the same
semantic orientation, and the latter tend to have the
opposite orientation. They first classify each con-
junctive expression into the same-orientation class
or the different-orientation class. They then use the
classified expressions to cluster words into the pos-
itive class and the negative class. The experiments
were conducted with the dataset that they created on
their own. Evaluation was limited to adjectives.
Kobayashi et al (2001) proposed a method for ex-
tracting semantic orientations of words with boot-
strapping. The semantic orientation of a word is
determined on the basis of its gloss, if any of their
52 hand-crafted rules is applicable to the sentence.
Rules are applied iteratively in the bootstrapping
framework. Although Kobayashi et al?s work pro-
vided an accurate investigation on this task and in-
spired our work, it has drawbacks: low recall and
language dependency. They reported that the seman-
tic orientations of only 113 words are extracted with
precision 84.1% (the low recall is due partly to their
large set of seed words (1187 words)). The hand-
crafted rules are only for Japanese.
Kamps et al (2004) constructed a network by
connecting each pair of synonymous words provided
by WordNet (Fellbaum, 1998), and then used the
shortest paths to two seed words ?good? and ?bad?
to obtain the semantic orientation of a word. Limi-
tations of their method are that a synonymy dictio-
nary is required, that antonym relations cannot be
incorporated into the model. Their evaluation is re-
stricted to adjectives. The method proposed by Hu
and Liu (2004) is quite similar to the shortest-path
method. Hu and Liu?s method iteratively determines
the semantic orientations of the words neighboring
any of the seed words and enlarges the seed word
set in a bootstrapping manner.
Subjective words are often semantically oriented.
Wiebe (2000) used a learning method to collect sub-
jective adjectives from corpora. Riloff et al (2003)
focused on the collection of subjective nouns.
We later compare our method with Turney and
Littman?s method and Kamps et al?s method.
The other pieces of research work mentioned
above are related to ours, but their objectives are dif-
ferent from ours.
3 Spin Model and Mean Field
Approximation
We give a brief introduction to the spin model
and the mean field approximation, which are well-
studied subjects both in the statistical mechanics
and the machine learning communities (Geman and
Geman, 1984; Inoue and Carlucci, 2001; Mackay,
2003).
A spin system is an array of N electrons, each of
which has a spin with one of two values ?+1 (up)? or
??1 (down)?. Two electrons next to each other en-
ergetically tend to have the same spin. This model
is called the Ising spin model, or simply the spin
model (Chandler, 1987). The energy function of a
spin system can be represented as
E(x,W ) = ?12
?
ij
wijxixj , (1)
where xi and xj (? x) are spins of electrons i and j,
matrix W = {wij} represents weights between two
electrons.
In a spin system, the variable vector x follows the
Boltzmann distribution :
P (x|W ) = exp(??E(x,W ))Z(W ) , (2)
where Z(W ) = ?x exp(??E(x,W )) is the nor-
malization factor, which is called the partition
function and ? is a constant called the inverse-
temperature. As this distribution function suggests,
a configuration with a higher energy value has a
smaller probability.
Although we have a distribution function, com-
puting various probability values is computationally
difficult. The bottleneck is the evaluation of Z(W ),
since there are 2N configurations of spins in this sys-
tem.
We therefore approximate P (x|W ) with a simple
function Q(x; ?). The set of parameters ? for Q, is
determined such that Q(x; ?) becomes as similar to
P (x|W ) as possible. As a measure for the distance
between P and Q, the variational free energy F is
often used, which is defined as the difference be-
tween the mean energy with respect to Q and the
entropy of Q :
F (?) = ?
?
x
Q(x; ?)E(x;W )
134
?
(
?
?
x
Q(x; ?) logQ(x; ?)
)
. (3)
The parameters ? that minimizes the variational free
energy will be chosen. It has been shown that mini-
mizing F is equivalent to minimizing the Kullback-
Leibler divergence between P and Q (Mackay,
2003).
We next assume that the function Q(x; ?) has the
factorial form :
Q(x; ?) =
?
i
Q(xi; ?i). (4)
Simple substitution and transformation leads us to
the following variational free energy :
F (?) = ??2
?
ij
wij x?ix?j
?
?
i
(
?
?
xi
Q(xi; ?i) logQ(xi; ?i)
)
.
(5)
With the usual method of Lagrange multipliers,
we obtain the mean field equation :
x?i =
?
xi xi exp
(
?xi
?
j wij x?j
)
?
xi exp
(
?xi
?
j wij x?j
) . (6)
This equation is solved by the iterative update rule :
x?newi =
?
xi xi exp
(
?xi
?
j wij x?oldj
)
?
xi exp
(
?xi
?
j wij x?oldj
) . (7)
4 Extraction of Semantic Orientation of
Words with Spin Model
We use the spin model to extract semantic orienta-
tions of words.
Each spin has a direction taking one of two values:
up or down. Two neighboring spins tend to have the
same direction from a energetic reason. Regarding
each word as an electron and its semantic orientation
as the spin of the electron, we construct a lexical net-
work by connecting two words if, for example, one
word appears in the gloss of the other word. Intu-
ition behind this is that if a word is semantically ori-
ented in one direction, then the words in its gloss
tend to be oriented in the same direction.
Using the mean-field method developed in statis-
tical mechanics, we determine the semantic orienta-
tions on the network in a global manner. The global
optimization enables the incorporation of possibly
noisy resources such as glosses and corpora, while
existing simple methods such as the shortest-path
method and the bootstrapping method cannot work
in the presence of such noisy evidences. Those
methods depend on less-noisy data such as a the-
saurus.
4.1 Construction of Lexical Networks
We construct a lexical network by linking two words
if one word appears in the gloss of the other word.
Each link belongs to one of two groups: the same-
orientation links SL and the different-orientation
links DL. If at least one word precedes a nega-
tion word (e.g., not) in the gloss of the other word,
the link is a different-orientation link. Otherwise the
links is a same-orientation link.
We next set weights W = (wij) to links :
wij =
?
???
???
1?
d(i)d(j) (lij ? SL)
? 1?d(i)d(j) (lij ? DL)
0 otherwise
, (8)
where lij denotes the link between word i and word
j, and d(i) denotes the degree of word i, which
means the number of words linked with word i. Two
words without connections are regarded as being
connected by a link of weight 0. We call this net-
work the gloss network (G).
We construct another network, the gloss-
thesaurus network (GT), by linking synonyms,
antonyms and hypernyms, in addition to the the
above linked words. Only antonym links are in DL.
We enhance the gloss-thesaurus network with
cooccurrence information extracted from corpus. As
mentioned in Section 2, Hatzivassiloglou and McK-
eown (1997) used conjunctive expressions in corpus.
Following their method, we connect two adjectives
if the adjectives appear in a conjunctive form in the
corpus. If the adjectives are connected by ?and?, the
link belongs to SL. If they are connected by ?but?,
the link belongs to DL. We call this network the
gloss-thesaurus-corpus network (GTC).
135
4.2 Extraction of Orientations
We suppose that a small number of seed words are
given. In other words, we know beforehand the se-
mantic orientations of those given words. We incor-
porate this small labeled dataset by modifying the
previous update rule.
Instead of ?E(x,W ) in Equation (2), we use the
following function H(?, x,W ) :
H(?, x,W ) = ??2
?
ij
wijxixj + ?
?
i?L
(xi ? ai)2,
(9)
where L is the set of seed words, ai is the orientation
of seed word i, and ? is a positive constant. This
expression means that if xi (i ? L) is different from
ai, the state is penalized.
Using function H , we obtain the new update rule
for xi (i ? L) :
x?newi =
?
xi xi exp
(
?xisoldi ? ?(xi ? ai)2
)
?
xi exp
(
?xisoldi ? ?(xi ? ai)2
) ,
(10)
where soldi =
?
j wij x?oldj . x?oldi and x?newi are the
averages of xi respectively before and after update.
What is discussed here was constructed with the ref-
erence to work by Inoue and Carlucci (2001), in
which they applied the spin glass model to image
restoration.
Initially, the averages of the seed words are set
according to their given orientations. The other av-
erages are set to 0.
When the difference in the value of the variational
free energy is smaller than a threshold before and
after update, we regard computation converged.
The words with high final average values are clas-
sified as positive words. The words with low final
average values are classified as negative words.
4.3 Hyper-parameter Prediction
The performance of the proposed method largely de-
pends on the value of hyper-parameter ?. In order to
make the method more practical, we propose criteria
for determining its value.
When a large labeled dataset is available, we can
obtain a reliable pseudo leave-one-out error rate :
1
|L|
?
i?L
[aix??i], (11)
where [t] is 1 if t is negative, otherwise 0, and x??i is
calculated with the right-hand-side of Equation (6),
where the penalty term ?(x?i?ai)2 in Equation (10)
is ignored. We choose ? that minimizes this value.
However, when a large amount of labeled data is
unavailable, the value of pseudo leave-one-out error
rate is not reliable. In such cases, we use magnetiza-
tion m for hyper-parameter prediction :
m = 1N
?
i
x?i. (12)
At a high temperature, spins are randomly ori-
ented (paramagnetic phase, m ? 0). At a low
temperature, most of the spins have the same di-
rection (ferromagnetic phase, m 6= 0). It is
known that at some intermediate temperature, ferro-
magnetic phase suddenly changes to paramagnetic
phase. This phenomenon is called phase transition.
Slightly before the phase transition, spins are locally
polarized; strongly connected spins have the same
polarity, but not in a global way.
Intuitively, the state of the lexical network is lo-
cally polarized. Therefore, we calculate values of
m with several different values of ? and select the
value just before the phase transition.
4.4 Discussion on the Model
In our model, the semantic orientations of words
are determined according to the averages values of
the spins. Despite the heuristic flavor of this deci-
sion rule, it has a theoretical background related to
maximizer of posterior marginal (MPM) estimation,
or ?finite-temperature decoding? (Iba, 1999; Marro-
quin, 1985). In MPM, the average is the marginal
distribution over xi obtained from the distribution
over x. We should note that the finite-temperature
decoding is quite different from annealing type algo-
rithms or ?zero-temperature decoding?, which cor-
respond to maximum a posteriori (MAP) estima-
tion and also often used in natural language process-
ing (Cowie et al, 1992).
Since the model estimation has been reduced
to simple update calculations, the proposed model
is similar to conventional spreading activation ap-
proaches, which have been applied, for example, to
word sense disambiguation (Veronis and Ide, 1990).
Actually, the proposed model can be regarded as a
spreading activation model with a specific update
136
rule, as long as we are dealing with 2-class model
(2-Ising model).
However, there are some advantages in our mod-
elling. The largest advantage is its theoretical back-
ground. We have an objective function and its ap-
proximation method. We thus have a measure of
goodness in model estimation and can use another
better approximation method, such as Bethe approx-
imation (Tanaka et al, 2003). The theory tells
us which update rule to use. We also have a no-
tion of magnetization, which can be used for hyper-
parameter estimation. We can use a plenty of knowl-
edge, methods and algorithms developed in the field
of statistical mechanics. We can also extend our
model to a multiclass model (Q-Ising model).
Another interesting point is the relation to maxi-
mum entropy model (Berger et al, 1996), which is
popular in the natural language processing commu-
nity. Our model can be obtained by maximizing the
entropy of the probability distribution Q(x) under
constraints regarding the energy function.
5 Experiments
We used glosses, synonyms, antonyms and hyper-
nyms of WordNet (Fellbaum, 1998) to construct an
English lexical network. For part-of-speech tag-
ging and lemmatization of glosses, we used Tree-
Tagger (Schmid, 1994). 35 stopwords (quite fre-
quent words such as ?be? and ?have?) are removed
from the lexical network. Negation words include
33 words. In addition to usual negation words such
as ?not? and ?never?, we include words and phrases
which mean negation in a general sense, such as
?free from? and ?lack of?. The whole network con-
sists of approximately 88,000 words. We collected
804 conjunctive expressions from Wall Street Jour-
nal and Brown corpus as described in Section 4.2.
The labeled dataset used as a gold standard is
General Inquirer lexicon (Stone et al, 1966) as in the
work by Turney and Littman (2003). We extracted
the words tagged with ?Positiv? or ?Negativ?, and
reduced multiple-entry words to single entries. As a
result, we obtained 3596 words (1616 positive words
and 1980 negative words) 1. In the computation of
1Although we preprocessed in the same way as Turney and
Littman, there is a slight difference between their dataset and
our dataset. However, we believe this difference is insignificant.
Table 1: Classification accuracy (%) with various
networks and four different sets of seed words. In
the parentheses, the predicted value of ? is written.
For cv, no value is written for ?, since 10 different
values are obtained.
seeds GTC GT G
cv 90.8 (?) 90.9 (?) 86.9 (?)
14 81.9 (1.0) 80.2 (1.0) 76.2 (1.0)
4 73.8 (0.9) 73.7 (1.0) 65.2 (0.9)
2 74.6 (1.0) 61.8 (1.0) 65.7 (1.0)
accuracy, seed words are eliminated from these 3596
words.
We conducted experiments with different values
of ? from 0.1 to 2.0, with the interval 0.1, and pre-
dicted the best value as explained in Section 4.3. The
threshold of the magnetization for hyper-parameter
estimation is set to 1.0 ? 10?5. That is, the pre-
dicted optimal value of ? is the largest ? whose
corresponding magnetization does not exceeds the
threshold value.
We performed 10-fold cross validation as well as
experiments with fixed seed words. The fixed seed
words are the ones used by Turney and Littman: 14
seed words {good, nice, excellent, positive, fortu-
nate, correct, superior, bad, nasty, poor, negative,
unfortunate, wrong, inferior}; 4 seed words {good,
superior, bad, inferior}; 2 seed words {good, bad}.
5.1 Classification Accuracy
Table 1 shows the accuracy values of semantic ori-
entation classification for four different sets of seed
words and various networks. In the table, cv corre-
sponds to the result of 10-fold cross validation, in
which case we use the pseudo leave-one-out error
for hyper-parameter estimation, while in other cases
we use magnetization.
In most cases, the synonyms and the cooccurrence
information from corpus improve accuracy. The
only exception is the case of 2 seed words, in which
G performs better than GT. One possible reason of
this inversion is that the computation is trapped in a
local optimum, since a small number of seed words
leave a relatively large degree of freedom in the so-
lution space, resulting in more local optimal points.
We compare our results with Turney and
137
Table 2: Actual best classification accuracy (%)
with various networks and four different sets of seed
words. In the parenthesis, the actual best value of ?
is written, except for cv.
seeds GTC GT G
cv 91.5 (?) 91.5 (?) 87.0 (?)
14 81.9 (1.0) 80.2 (1.0) 76.2 (1.0)
4 74.4 (0.6) 74.4 (0.6) 65.3 (0.8)
2 75.2 (0.8) 61.9 (0.8) 67.5 (0.5)
Littman?s results. With 14 seed words, they achieved
61.26% for a small corpus (approx. 1? 107 words),
76.06% for a medium-sized corpus (approx. 2?109
words), 82.84% for a large corpus (approx. 1?1011
words).
Without a corpus nor a thesaurus (but with glosses
in a dictionary), we obtained accuracy that is compa-
rable to Turney and Littman?s with a medium-sized
corpus. When we enhance the lexical network with
corpus and thesaurus, our result is comparable to
Turney and Littman?s with a large corpus.
5.2 Prediction of ?
We examine how accurately our prediction method
for ? works by comparing Table 1 above and Ta-
ble 2 below. Our method predicts good ? quite well
especially for 14 seed words. For small numbers of
seed words, our method using magnetization tends
to predict a little larger value.
We also display the figure of magnetization and
accuracy in Figure 1. We can see that the sharp
change of magnetization occurs at around ? = 1.0
(phrase transition). At almost the same point, the
classification accuracy reaches the peak.
5.3 Precision for the Words with High
Confidence
We next evaluate the proposed method in terms of
precision for the words that are classified with high
confidence. We regard the absolute value of each
average as a confidence measure and evaluate the top
words with the highest absolute values of averages.
The result of this experiment is shown in Figure 2,
for 14 seed words as an example. The top 1000
words achieved more than 92% accuracy. This re-
sult shows that the absolute value of each average
-0.1
 0
 0.1
 0.2
 0.3
 0.4
 0.5
 0.6
 0.7
 0.8
 0.9
 0  1  2  3  4  5  6  7  8  9  10
 40
 45
 50
 55
 60
 65
 70
 75
 80
 85
 90
M
ag
ne
tiz
ati
on
Ac
cu
ra
cy
Beta
magnetization
accuracy
Figure 1: Example of magnetization and classifica-
tion accuracy(14 seed words).
 75
 80
 85
 90
 95
 100
 0  500  1000  1500  2000  2500  3000  3500  4000
Pr
ec
isi
on
Number of selected words
GTC
GT
G
Figure 2: Precision (%) with 14 seed words.
138
Table 3: Precision (%) for selected adjectives.
Comparison between the proposed method and the
shortest-path method.
seeds proposed short. path
14 73.4 (1.0) 70.8
4 71.0 (1.0) 64.9
2 68.2 (1.0) 66.0
Table 4: Precision (%) for adjectives. Comparison
between the proposed method and the bootstrapping
method.
seeds proposed bootstrap
14 83.6 (0.8) 72.8
4 82.3 (0.9) 73.2
2 83.5 (0.7) 71.1
can work as a confidence measure of classification.
5.4 Comparison with other methods
In order to further investigate the model, we conduct
experiments in restricted settings.
We first construct a lexical network using only
synonyms. We compare the spin model with
the shortest-path method proposed by Kamps et
al. (2004) on this network, because the shortest-
path method cannot incorporate negative links of
antonyms. We also restrict the test data to 697 ad-
jectives, which is the number of examples that the
shortest-path method can assign a non-zero orien-
tation value. Since the shortest-path method is de-
signed for 2 seed words, the method is extended
to use the average shortest-path lengths for 4 seed
words and 14 seed words. Table 3 shows the re-
sult. Since the only difference is their algorithms,
we can conclude that the global optimization of the
spin model works well for the semantic orientation
extraction.
We next compare the proposed method with a
simple bootstrapping method proposed by Hu and
Liu (2004). We construct a lexical network using
synonyms and antonyms. We restrict the test data
to 1470 adjectives for comparison of methods. The
result in Table 4 also shows that the global optimiza-
tion of the spin model works well for the semantic
orientation extraction.
We also tested the shortest path method and the
bootstrapping method on GTC and GT, and obtained
low accuracies as expected in the discussion in Sec-
tion 4.
5.5 Error Analysis
We investigated a number of errors and concluded
that there were mainly three types of errors.
One is the ambiguity of word senses. For exam-
ple, one of the glosses of ?costly?is ?entailing great
loss or sacrifice?. The word ?great? here means
?large?, although it usually means ?outstanding? and
is positively oriented.
Another is lack of structural information. For ex-
ample, ?arrogance? means ?overbearing pride evi-
denced by a superior manner toward the weak?. Al-
though ?arrogance? is mistakingly predicted as posi-
tive due to the word ?superior?, what is superior here
is ?manner?.
The last one is idiomatic expressions. For exam-
ple, although ?brag? means ?show off?, neither of
?show? and ?off? has the negative orientation. Id-
iomatic expressions often does not inherit the se-
mantic orientation from or to the words in the gloss.
The current model cannot deal with these types of
errors. We leave their solutions as future work.
6 Conclusion and Future Work
We proposed a method for extracting semantic ori-
entations of words. In the proposed method, we re-
garded semantic orientations as spins of electrons,
and used the mean field approximation to compute
the approximate probability function of the system
instead of the intractable actual probability function.
We succeeded in extracting semantic orientations
with high accuracy, even when only a small number
of seed words are available.
There are a number of directions for future work.
One is the incorporation of syntactic information.
Since the importance of each word consisting a gloss
depends on its syntactic role. syntactic information
in glosses should be useful for classification.
Another is active learning. To decrease the
amount of manual tagging for seed words, an active
learning scheme is desired, in which a small number
of good seed words are automatically selected.
Although our model can easily extended to a
139
multi-state model, the effectiveness of using such a
multi-state model has not been shown yet.
Our model uses only the tendency of having the
same orientation. Therefore we can extract seman-
tic orientations of new words that are not listed in
a dictionary. The validation of such extension will
widen the possibility of application of our method.
Larger corpora such as web data will improve per-
formance. The combination of our method and the
method by Turney and Littman (2003) is promising.
Finally, we believe that the proposed model is ap-
plicable to other tasks in computational linguistics.
References
Adam L. Berger, Stephen Della Pietra, and Vincent
J. Della Pietra. 1996. A maximum entropy approach
to natural language processing. Computational Lin-
guistics, 22(1):39?71.
David Chandler. 1987. Introduction to Modern Statisti-
cal Mechanics. Oxford University Press.
Jim Cowie, Joe Guthrie, and Louise Guthrie. 1992. Lexi-
cal disambiguation using simulated annealing. In Pro-
ceedings of the 14th conference on Computational lin-
guistics, volume 1, pages 359?365.
Christiane Fellbaum. 1998. WordNet: An Electronic
Lexical Database, Language, Speech, and Communi-
cation Series. MIT Press.
Stuart Geman and Donald Geman. 1984. Stochastic re-
laxation, gibbs distributions, and the bayesian restora-
tion of images. IEEE Transactions on Pattern Analysis
and Machine Intelligence, 6:721?741.
Vasileios Hatzivassiloglou and Kathleen R. McKeown.
1997. Predicting the semantic orientation of adjec-
tives. In Proceedings of the Thirty-Fifth Annual Meet-
ing of the Association for Computational Linguistics
and the Eighth Conference of the European Chapter of
the Association for Computational Linguistics, pages
174?181.
Minqing Hu and Bing Liu. 2004. Mining and summa-
rizing customer reviews. In Proceedings of the 2004
ACM SIGKDD international conference on Knowl-
edge discovery and data mining (KDD-2004), pages
168?177.
Yukito Iba. 1999. The nishimori line and bayesian statis-
tics. Journal of Physics A: Mathematical and General,
pages 3875?3888.
Junichi Inoue and Domenico M. Carlucci. 2001. Image
restoration using the q-ising spin glass. Physical Re-
view E, 64:036121?1 ? 036121?18.
Jaap Kamps, Maarten Marx, Robert J. Mokken, and
Maarten de Rijke. 2004. Using wordnet to mea-
sure semantic orientation of adjectives. In Proceed-
ings of the 4th International Conference on Language
Resources and Evaluation (LREC 2004), volume IV,
pages 1115?1118.
Nozomi Kobayashi, Takashi Inui, and Kentaro Inui.
2001. Dictionary-based acquisition of the lexical
knowledge for p/n analysis (in Japanese). In Pro-
ceedings of Japanese Society for Artificial Intelligence,
SLUD-33, pages 45?50.
David J. C. Mackay. 2003. Information Theory, Infer-
ence and Learning Algorithms. Cambridge University
Press.
Jose L. Marroquin. 1985. Optimal bayesian estima-
tors for image segmentation and surface reconstruc-
tion. Technical Report A.I. Memo 839, Massachusetts
Institute of Technology.
Ellen Riloff, Janyce Wiebe, and Theresa Wilson. 2003.
Learning subjective nouns using extraction pattern
bootstrapping. In Proceedings of the Seventh Con-
ference on Natural Language Learning (CoNLL-03),
pages 25?32.
Helmut Schmid. 1994. Probabilistic part-of-speech tag-
ging using decision trees. In Proceedings of Interna-
tional Conference on New Methods in Language Pro-
cessing, pages 44?49.
Philip J. Stone, Dexter C. Dunphy, Marshall S. Smith,
and Daniel M. Ogilvie. 1966. The General Inquirer:
A Computer Approach to Content Analysis. The MIT
Press.
Kazuyuki Tanaka, Junichi Inoue, and Mike Titterington.
2003. Probabilistic image processing by means of the
bethe approximation for the q-ising model. Journal
of Physics A: Mathematical and General, 36:11023?
11035.
Peter D. Turney and Michael L. Littman. 2003. Measur-
ing praise and criticism: Inference of semantic orien-
tation from association. ACM Transactions on Infor-
mation Systems, 21(4):315?346.
Jean Veronis and Nancy M. Ide. 1990. Word sense dis-
ambiguation with very large neural networks extracted
from machine readable dictionaries. In Proceedings
of the 13th Conference on Computational Linguistics,
volume 2, pages 389?394.
Janyce M. Wiebe. 2000. Learning subjective adjec-
tives from corpora. In Proceedings of the 17th Na-
tional Conference on Artificial Intelligence (AAAI-
2000), pages 735?740.
140
Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the ACL, pages 833?840,
Sydney, July 2006. c?2006 Association for Computational Linguistics
Japanese Dependency Parsing Using Co-occurrence Information and a
Combination of Case Elements
Takeshi Abekawa
Graduate School of Education
University of Tokyo
abekawa@p.u-tokyo.ac.jp
Manabu Okumura
Precision and Intelligence Laboratory
Tokyo Institute of Technology
oku@pi.titech.ac.jp
Abstract
In this paper, we present a method that
improves Japanese dependency parsing by
using large-scale statistical information. It
takes into account two kinds of informa-
tion not considered in previous statistical
(machine learning based) parsing meth-
ods: information about dependency rela-
tions among the case elements of a verb,
and information about co-occurrence re-
lations between a verb and its case ele-
ment. This information can be collected
from the results of automatic dependency
parsing of large-scale corpora. The results
of an experiment in which our method was
used to rerank the results obtained using an
existing machine learning based parsing
method showed that our method can im-
prove the accuracy of the results obtained
using the existing method.
1 Introduction
Dependency parsing is a basic technology for pro-
cessing Japanese and has been the subject of much
research. The Japanese dependency structure is
usually represented by the relationship between
phrasal units called bunsetsu, each of which con-
sists of one or more content words that may be
followed by any number of function words. The
dependency between two bunsetsus is direct from
a dependent to its head.
Manually written rules have usually been used
to determine which bunsetsu another bunsetsu
tends to modify, but this method poses problems in
terms of the coverage and consistency of the rules.
The recent availability of larger-scale corpora an-
notated with dependency information has thus re-
sulted in more work on statistical dependency
analysis technologies that use machine learning al-
gorithms (Kudo and Matsumoto, 2002; Sassano,
2004; Uchimoto et al, 1999; Uchimoto et al,
2000).
Work on statistical Japanese dependency analy-
sis has usually assumed that all the dependency re-
lations in a sentence are independent of each other,
and has considered the bunsetsus in a sentence in-
dependently when judging whether or not a pair
of bunsetsus is in a dependency relation. In judg-
ing which bunsetsu a bunsetsu modifies, this type
of work has used as features the information of
two bunsetsus, such as the head words of the two
bunsetsus, and the morphemes at the ends of the
bunsetsus (Uchimoto et al, 1999). It is necessary,
however, to also consider features for the contex-
tual information of the two bunsetsus. One such
feature is the constraint that two case elements
with the same case do not modify a verb.
Statistical Japanese dependency analysis takes
into account syntactic information but tends not to
take into account lexical information, such as co-
occurrence between a case element and a verb.
The recent availability of more corpora has en-
abled much information about dependency rela-
tions to be obtained by using a Japanese depen-
dency analyzer such as KNP (Kurohashi and Na-
gao, 1994) or CaboCha (Kudo and Matsumoto,
2002). Although this information is less accu-
rate than manually annotated information, these
automatic analyzers provide a large amount of
co-occurrence information as well as information
about combinations of multiple cases that tend to
modify a verb.
In this paper, we present a method for improv-
ing the accuracy of Japanese dependency analy-
sis by representing the lexical information of co-
occurrence and dependency relations of multiple
cases as statistical models. We also show the re-
sults of experiments demonstrating the effective-
ness of our method.
833
Keisatsu-de umibe-dehitori-de arui-teiru syonen-wo hogo-shita
(The police/subj) (on the beach)(alone) (was walking) (boy/obj) (had custody)
(The police had custody of the boy who was walking alone on the beach.)
Figure 1: Example of a Japanese sentence, bunsetsu and dependencies
2 Parsing Japanese
The Japanese language is basically an SOV lan-
guage, but word order is relatively free. In English
the syntactic function of each word is represented
by word order, while in Japanese it is represented
by postpositions. For example, one or more post-
positions following a noun play a role similar to
the declension of nouns in German, which indi-
cates grammatical case.
The syntax of a Japanese sentence is analyzed
by using segments, called bunsetsu, that usually
contain one or more content words like a noun,
verb, or adjective, and zero or more function
words like a particle (case marker) or verb/noun
suffix. By defining a bunsetsu in this manner, we
can analyze a sentence in a way similar to that used
when analyzing the grammatical roles of words in
inflected languages like German.
Japanese dependencies have the following char-
acteristics:
? Each bunsetsu except the rightmost one has
only one head.
? Each head bunsetsu is always placed to the
right of (i.e. after) its modifier.
? Dependencies do not cross one another.
Statistical Japanese dependency analyzers
(Kudo and Matsumoto, 2005; Kudo and Mat-
sumoto, 2002; Sassano, 2004; Uchimoto et al,
1999; Uchimoto et al, 2000) automatically learn
the likelihood of dependencies from a tagged
corpus and calculate the best dependencies for an
input sentence. These likelihoods are learned by
considering the features of bunsetsus such as their
character strings, parts of speech, and inflection
types, as well as information between bunsetsus
such as punctuation and the distance between
bunsetsus. The weight of given features is learned
from a training corpus by calculating the weights
from the frequencies of the features in the training
data.
3 Japanese dependency analysis taking
account of co-occurrence information
and a combination of multiple cases
One constraint in Japanese is that multiple nouns
of the same case do not modify a verb. Previ-
ous work on Japanese dependency analysis has as-
sumed that all the dependency relations are inde-
pendent of one another. It is therefore necessary
to also consider such a constraint as a feature for
contextual information. Uchimoto et al, for ex-
ample, used as such a feature whether a particu-
lar type of bunsetsu is between two bunsetsus in a
dependency relation (Uchimoto et al, 1999), and
Sassano used information about what is just be-
fore and after the modifying bunsetsu and modi-
fyee bunsetsu (Sassano, 2004).
In the artificial example shown in Figure 1, it
is natural to consider that ?keisatsu-de? will mod-
ify ?hogo-shita?. Statistical Japanese dependency
analyzers (Uchimoto et al, 2000; Kudo and Mat-
sumoto, 2002), however, will output the result
where ?keisatsu-de? modifies ?arui-teiru?. This is
because in sentences without internal punctuation
a noun tends to modify the nearest verb, and these
analyzers do not take into account a combination
of multiple cases.
Another kind of information useful in depen-
dency analysis is the co-occurrence of a noun and
a verb, which indicates to what degree the noun
tends to modify the verb. In the above example,
the possible modifyees of ?keisatsu-de? are ?arui-
teiru? and ?hogo-shita?. Taking into account in-
formation about the co-occurrence of ?keisatsu-
de? and ?arui-teiru? and of ?keisatsu-de? and
?hogo-shita? makes it obvious that ?keisatsu-de?
is more likely to modify ?hogo-shita?.
834
In summary, we think that statistical Japanese
dependency analysis needs to take into account
at least two more kinds of information: the de-
pendency relation between multiple cases where
multiple nouns of the same case do not modify a
verb, and the co-occurrence of nouns and verbs.
One way to use such information in statistical de-
pendency analysis is to directly use it as features.
However, Kehler et al pointed out that this does
not make the analysis more accurate (Kehler et al,
2004). This paper therefore presents a model that
uses the co-occurrence information separately and
reranks the analysis candidates generated by the
existing machine learning model.
4 Our proposed model
We first introduce the notation for the explanation
of the dependency structure T :
m(T ) : the number of verbs in T
vi(T ) : the i-th verb in T
ci(T ) : the number of case elements that mod-
ify the i-th verb in T
esi(T ) : the set of case elements that modify the
i-th verb in T
rsi(T ) : the set of particles in the set of case el-
ements that modify the i-th verb in T
nsi(T ) : the set of nouns in the set of case ele-
ments that modify the i-th verb in T
ri,j(T ) : the j-th particle that modifies the i-th
verb in T
ni,j(T ) : the j-th noun that modifies the i-th verb
in T
We defined case element as a pair of a noun
and following particles. For the dependency
structure we assume the conditional probability
P (esi(T )|vi(T )) that the set of case elements
esi(T ) depends on the vi(T ), and assume the set
of case elements esi(T ) is composed of the set of
noun nsi(T ) and particles rsi(T ).
P (esi(T )|vi(T ))
def= P (rsi(T ), nsi(T )|vi(T )) (1)
= P (rsi(T )|vi(T )) ?
P (nsi(T )|rsi(T ), vi(T )) (2)
' P (rsi(T )|vi(T )) ?
ci(T )
?
j=1
P (ni,j(T)|rsi(T),vi(T)) (3)
' P (rsi(T )|vi(T )) ?
ci(T )
?
j=1
P (ni,j(T)|ri,j(T),vi(T)) (4)
In the transformation from Equation (2) to Equa-
tion (3), we assume that the set of noun nsi(T ) is
independent of the verb vi(T ). And in the trans-
formation from Equation (3) to Equation (4), we
assume that the noun ni,j(T ) is dependent on only
its following particle ri,j(T ).
Now we assume the dependency structure T of
the whole sentence is composed of only the depen-
dency relation between case elements and verbs,
and propose the sentence probability defined by
Equation (5).
P (T ) =
m(T )
?
i=1
P (rsi(T )|vi(T )) ?
ci(T )
?
j=1
P (ni,j(T )|ri,j(T ), vi(T )) (5)
We call P (rsi(T )|vi(T )) the co-occurrence prob-
ability of the particle set and the verb, and we
call P (ni,j(T )|ri,j(T ), vi(T )) the co-occurrence
probability of the case element set and the verb.
In the actual dependency analysis, we try to se-
lect the dependency structure T? that maximizes
the Equation (5) from the possible parses T for the
inputted sentence:
T? = argmax
T
m(T )
?
i=1
P (rsi(T )|vi(T )) ?
ci(T )
?
j=1
P (ni,j(T )|ri,j(T ), vi(T )). (6)
The proposed model is inspired by the semantic
role labeling method (Gildea and Jurafsky, 2002),
which uses the frame element group in place of the
particle set.
It differs from the previous parsing models in
that we take into account the dependency relations
among particles in the set of case elements that
modify a verb. This information can constrain the
combination of particles (cases) among bunsetsus
that modify a verb. Assuming the independence
among particles, we can rewrite Equation (5) as
P (T ) =
m(T )
?
i=1
ci(T )
?
j=1
P (ni,j(T ), ri,j(T )|vi(T )). (7)
4.1 Syntactic property of a verb
In Japanese, the ?ha? case that indicates a topic
tends to modify the main verb in a sentence and
tends not to modify a verb in a relative clause. The
835
verb: ?aru-ku? verb: ?hogo-suru?
case elements particle set case elements particle set
a keisatsu-de umibe-de hitori-de { de,de,de } syonen-wo {wo}
b umibe-de hitori-de {de,de} keisatsu-de syonen-wo {de,wo}
c hitori-de {de} keisatsu-de umibe-de syonen-wo {de,de,wo}
d {none} keisatsu-de umibe-de hitori-de syonen-wo { de,de,de,wo }
Table 1: Analytical process of the example sentence
co-occurrence probability of the particle set there-
fore tends to be different for verbs with different
syntactic properties.
Like (Shirai, 1998), to take into account the re-
liance of the co-occurrence probability of the par-
ticle set on the syntactic property of a verb, instead
of using P (rsi(T )|vi(T )) in Equation (5), we use
P (rsi(T )|syni(T ), vi(T )), where syni(T ) is the
syntactic property of the i-th verb in T and takes
one of the following three values:
?verb? when v modifies another verb
?noun? when v modifies a noun
?main? when v modifies nothing (when it is at the
end of the sentence, and is the main verb)
4.2 Illustration of model application
Here, we illustrate the process of applying our pro-
posed model to the example sentence in Figure 1,
for which there are four possible combinations of
dependency relations. The bunsetsu combinations
and corresponding sets of particles are listed in Ta-
ble 1. In the analytical process, we calculate for
all the combinations the co-occurrence probability
of the case element set (bunsetsu set) and the co-
occurrence probability of the particle set, and we
select the T? that maximizes the probability.
Some of the co-occurrence probabilities of the
particle sets for the verbs ?aru-ku? and ?hogo-
suru? in the sentence are listed in Table 2. How to
estimate these probabilities is described in section
5.3. Basically, the larger the number of particles,
the lower the probability is. As you can see in the
comparison between {de, wo} and {de, de}, the
probability becomes lower when multiple same
cases are included. Therefore, the probability can
reflect the constraint that multiple case elements
of the same particle tend not to modify a verb.
5 Experiments
We evaluated the effectiveness of our model ex-
perimentally. Since our model treats only the de-
rsi P (rsi|noun, v1) P (rsi|main, v2)
v1 = ?aru-ku? v2 = ?hogo-suru?
{none} 0.29 0.35
{wo} 0.30 0.24
{ga} 0.056 0.072
{ni} 0.040 0.041
{de} 0.032 0.033
{ha} 0.035 0.041
{de, wo} 0.022 0.018
{de, de} 0.00038 0.00038
{de, de, wo} 0.00022 0.00018
{de, de, de} 0.0000019 0.0000018
{de, de, de, wo} 0.00000085 0.00000070
Table 2: Example of the co-occurrence probabili-
ties of particle sets
pendency relations between a noun and a verb, we
cannot determine all the dependency relations in
a sentence. We therefore use one of the currently
available dependency analyzers to generate an or-
dered list of n-best possible parses for the sentence
and then use our proposed model to rerank them
and select the best parse.
5.1 Dependency analyzer for outputting
n-best parses
We generated the n-best parses by using the ?pos-
terior context model? (Uchimoto et al, 2000). The
features we used were those in (Uchimoto et al,
1999) and their combinations. We also added our
original features and their combinations, with ref-
erence to (Sassano, 2004; Kudo and Matsumoto,
2002), but we removed the features that had a fre-
quency of less than 30 in our training data. The
total number of features is thus 105,608.
5.2 Reranking method
Because our model considers only the dependency
relations between a noun and a verb, and thus
cannot determine all the dependency relations in
a sentence, we restricted the possible parses for
836
reranking as illustrated in Figure 2. The possi-
ble parses for reranking were the first-ranked parse
and those of the next-best parses in which the
verb to modify was different from that in the first-
ranked one. For example, parses 1 and 3 in Figure
2 are the only candidates for reranking. In our ex-
periments, n is set to 50.
The score we used for reranking the parses was
the product of the probability of the posterior con-
text model and the probability of our proposed
model:
score = Pcontext(T )? ? P (T ), (8)
where Pcontext(T ) is the probability of the poste-
rior context model. The ? here is a parameter with
which we can adjust the balance of the two proba-
bilities, and is fixed to the best value by consider-
ing development data (different from the training
data)1.
Reranking
Candidate 1
Candidate 2
Candidate 3
Candidate 4
: Case element : Verb
Candidate
Candidate
Figure 2: Selection of possible parses for reranking
Many methods for reranking the parsing of En-
glish sentences have been proposed (Charniak and
Johnson, 2005; Collins and Koo, 2005; Hender-
son and Titov, 2005), all of which are discrimina-
tive methods which learn the difference between
the best parse and next-best parses. While our
reranking model using generation probability is
quite simple, we can easily verify our hypothesis
that the two proposed probabilities have an effect
on improving the parsing accuracy. We can also
verify that the parsing accuracy improves by using
imprecise information obtained from an automati-
cally parsed corpus.
Klein and Manning proposed a generative
model in which syntactic (PCFG) and semantic
(lexical dependency) structures are scored with
separate models (Klein and Manning, 2002), but
1In our experiments, ? is set to 2.0 using development
data.
they do not take into account the combination of
dependencies. Shirai et al also proposed a statis-
tical model of Japanese language which integrates
lexical association statistics with syntactic prefer-
ence (Shirai et al, 1998). Our proposed model dif-
fers from their method in that it explicitly uses the
combination of multiple cases.
5.3 Estimation of co-occurrence probability
We estimated the co-occurrence probability of the
particle set and the co-occurrence probability of
the case element set used in our model by analyz-
ing a large-scale corpus. We collected a 30-year
newspaper corpus2, applied the morphological an-
alyzer JUMAN (Kurohashi and Nagao, 1998b),
and then applied the dependency analyzer with
a posterior context model3. To ensure that we
collected reliable co-occurrence information, we
removed the information for the bunsetsus with
punctuation4.
Like (Torisawa, 2001), we estimated the co-
occurrence probability P (?n, r, v?) of the case
element set (noun n, particle r, and verb v)
by using probabilistic latent semantic indexing
(PLSI) (Hofmann, 1999)5. If ?n, r, v? is the
co-occurrence of n and ?r, v?, we can calculate
P (?n, r, v?) by using the following equation:
P (?n, r, v?) =
?
z?Z
P (n|z)P (?r, v?|z)P (z), (9)
where z indicates a latent semantic class of co-
occurrence (hidden class). Probabilistic parame-
ters P (n|z), P (?r, v?|z), and P (z) in Equation (9)
can be estimated by using the EM algorithm. In
our experiments, the dimension of the hidden class
z was set to 300. As a result, the collected ?n, r, v?
total 102,581,924 pairs. The number of n and v is
57,315 and 15,098, respectively.
The particles for which the co-occurrence prob-
ability was estimated were the set of case particles,
the ?ha? case particle, and a class of ?fukujoshi?
213 years? worth of articles from the Mainichi Shimbun,
14 years? worth from the Yomiuri Shimbun, and 3 years?
worth from the Asahi Shimbun.
3We used the following package for calculation of
Maximum Entropy:
http://homepages.inf.ed.ac.uk/s0450736/maxent toolkit.html.
4The result of dependency analysis with a posterior con-
text model for the Kyodai Corpus showed that the accuracy
for the bunsetsu without punctuation is 90.6%, while the ac-
curacy is only 76.4% for those with punctuation.
5We used the following package for calculation of PLSI:
http://chasen.org/?taku/software/plsi/.
837
Bunsetsu accuracy Sentence accuracy
Whole data Context model 90.95% (73,390/80,695) 54.40% (5,052/9,287)
Our model 91.21% (73,603/80,695) 55.17% (5,124/9,287)
Only for reranked sentences Context model 90.72% (68,971/76,026) 48,33% (3,813/7,889)
Our model 91.00% (69,184/76,026) 49.25% (3,885/7,889)
Only for case elements Context model 91.80% (28,849/31,427) ?
Our model 92.47% (29,062/31,427) ?
Table 3: Accuracy before/after reranking
particles. Therefore, the total number of particles
was 10.
We also estimated the co-occurrence probability
of the particle set P (rs|syn, v) by using PLSI. We
regarded the triple ?rs, syn, v? (the co-occurrence
of particle set rs, verb v, and the syntactic prop-
erty syn) as the co-occurrence of rs and ?syn, v?.
The dimension of the hidden class was 100. The
total number of ?rs, syn, v? pairs was 1,016,508,
v was 18,423, and rs was 1,490. The particle set
should be treated not as a non-ordered set but as
an occurrence ordered set. However, we think cor-
rect probability estimation using an occurrence or-
dered set is difficult, because it gives rise to an ex-
plosion in the number of combination,
5.4 Experimental environment
The evaluation data we used was Kyodai Cor-
pus 3.0, a corpus manually annotated with depen-
dency relations (Kurohashi and Nagao, 1998a).
The statistics of the data are as follows:
? Training data: 24,263 sentences, 234,474
bunsetsus
? Development data: 4,833 sentences, 47,580
bunsetsus
? Test data: 9,287 sentences, 89,982 bunsetsus
The test data contained 31,427 case elements, and
28,801 verbs.
The evaluation measures we used were bunsetsu
accuracy (the percentage of bunsetsu for which the
correct modifyee was identified) and sentence ac-
curacy (the percentage of sentences for which the
correct dependency structure was identified).
5.5 Experimental results
5.5.1 Evaluation of our model
Our first experiment evaluated the effectiveness
of reranking with our proposed model. Bunsetsu
Our reranking model
correct incorrect
Context model correct 73,119 271
incorrect 484 6,821
Table 4: 2 ? 2 contingency table of the number of
correct bunsetsu (posterior context model ? our
model)
and sentence accuracies before and after rerank-
ing, for the entire set of test data as well as for
only those sentences whose parse was actually
reranked, are listed in Table 3.
The results showed that the accuracy could be
improved by using our proposed model to rerank
the results obtained with the posterior context
model. McNemar testing showed that the null hy-
pothesis that there is no difference between the ac-
curacy of the results obtained with the posterior
context model and those obtained with our model
could be rejected with a p value < 0.01. The
difference in accuracy is therefore significant.
5.5.2 Comparing variant models
We next experimentally compare the following
variations of the proposed model:
(a) one in which the case element set is assumed
to be independent [Equation (7)]
(b) one using the co-occurrence probability of
the particle set, P (rs|syn, v), in our model
(c) one using only the co-occurrence probability
of the case element, P (n|r, v), in our model
(d) one not taking into account the syntactic
property of a verb (i,e. a model in which
the co-occurrence probability is defined as
P (r|v), without the syntactic property syn)
(e) one in which the co-occurrence probability of
the case element, P (n|r, v), is simply added
838
Bunsetsu Sentence
accuracy accuracy
Context model 90.95% 54.40%
Our model 91.21% 55.17%
model (a) 91.12% 54.90%
model (b) 91.10% 54.69%
model (c) 91.11% 54.91%
model (d) 91.15% 54.82%
model (e) 90.96% 54.33%
model (f) 89.50% 48.33%
Kudo et al2005 91.37% 56.00%
Table 5: Comparison of various models
to a feature set used in the posterior context
model
(f) one using only our proposed probabilities
without the probability of the posterior con-
text model
The accuracies obtained with each of these
models are listed in Table 5, from which we can
conclude that it is effective to take into account the
dependency between case elements because model
(a) is less accurate than our model.
Since the accuracy of model (d) is comparable
to that of our model, we can conclude that the con-
sideration of the syntactic property of a verb does
not necessarily improve dependency analysis.
The accuracy of model (e), which uses the co-
occurrence probability of the case element set as
features in the posterior context model, is compa-
rable to that of the posterior context model. This
result is similar to the one obtained by (Kehler et
al., 2004), where the task was anaphora resolution.
Although we think the co-occurrence probability
is useful information for dependency analysis, this
result shows that simply adding it as a feature does
not improve the accuracy.
5.5.3 Changing the amount of training data
Changing the size of the training data set, we
investigated whether the degree of accuracy im-
provement due to reranking depends on the accu-
racy of the existing dependency analyzer.
Figure 3 shows that the accuracy improvement
is constant even if the accuracy of the dependency
analyzer is varied.
5.6 Discussion
The score used in reranking is the product of the
probability of the posterior context model and the
 0.894
 0.896
 0.898
 0.9
 0.902
 0.904
 0.906
 0.908
 0.91
 0.912
 0.914
 4000  6000  8000  10000  12000  14000  16000  18000  20000  22000  24000  26000No. of training sentences
Buns
etsu 
acc
ura
cy
posterior context modelproposed model
Figure 3: Bunsetsu accuracy when the size of the
training data is changed
probability of our proposed model. The results in
Table 5 show that the parsing accuracy of model
(f), which uses only the probabilities obtained with
our proposed model, is quite low. We think the
reason for this is that our two co-occurrence prob-
abilities cannot take account of syntactic proper-
ties, such as punctuation and the distance between
two bunsetsus, which improve dependency analy-
sis.
Furthermore, when the sentence has multiple
verbs and case elements, the constraint of our pro-
posed model tends to distribute case elements to
each verb equally. To investigate such bias, we
calculated the variance of the number of case ele-
ments per verb.
Table 6 shows that the variance for our proposed
model (Equation [5]) is the lowest, and this model
distributes case elements to each verb equally. The
variance of the posterior context model is higher
than that of the test data, probably because the
syntactic constraint in this model affects parsing
too much. Therefore the variance of the reranking
model (Equation [8]), which is the combination
of our proposed model and the posterior context
model, is close to that of the test data.
The best parse which uses this data set is (Kudo
and Matsumoto, 2005), and their parsing accuracy
is 91.37%. The features and the parsing method
used by their model are almost equal to the poste-
rior context model, but they use a different method
of probability estimation. If their model could
generate n-best parsing and attach some kind of
score to each parse tree, we would combine their
model in place of the posterior context model.
At the stage of incorporating the proposed ap-
proach to a parser, the consistency with other pos-
839
context model test data Equation [8] Equation [5]
variance (?2) 0.724 0.702 0.696 0.666
*The average number of elements per verb is 1.078.
Table 6: The variance of the number of elements per verb
sible methods that deal with other relations should
be taken into account. This will be one of our fu-
ture tasks.
6 Conclusion
We presented a method of improving Japanese de-
pendency parsing by using large-scale statistical
information. Our method takes into account two
types of information, not considered in previous
statistical (machine learning based) parsing meth-
ods. One is information about the dependency re-
lations among the case elements of a verb, and the
other is information about co-occurrence relations
between a verb and its case element. Experimen-
tal results showed that our method can improve the
accuracy of the existing method.
References
Eugene Charniak and Mark Johnson. 2005. Coarse-
to-fine n-best parsing and maxent discriminative
reranking. In Proceedings of the 43rd Annual Meet-
ing of the ACL, pages 173?180.
Michael Collins and Terry Koo. 2005. Discriminative
reranking for natural language parsing. Computa-
tional Linguistics, 31(1):25?69.
Daniel Gildea and Daniel Jurafsky. 2002. Automatic
labeling of semantic roles. Computational Linguis-
tics, 28(3):245?288.
James Henderson and Ivan Titov. 2005. Data-defined
kernels for parse reranking derived from probabilis-
tic models. In Proceedings of the 43rd Annual Meet-
ing of the ACL, pages 181?188.
Thomas Hofmann. 1999. Probabilistic latent semantic
indexing. In Proceedings of the 22nd Annual Inter-
national SIGIR Conference on Research and Devel-
opment in Information Retrieval, pages 50?57.
Andrew Kehler, Douglas Appelt, Lara Taylor, and
Aleksandr Simma. 2004. The (non)utility of
predicate-argument frequencies for pronoun inter-
pretation. In Proceedings of the HLT/NAACL 2004,
pages 289?296.
Dan Klein and Christopher D. Manning. 2002. Fast
exact inference with a factored model for natural
language parsing. In Advances in Neural Informa-
tion Processing Systems 15 (NIPS 2002), pages 3?
10.
Taku Kudo and Yuji Matsumoto. 2002. Japanese
dependency analysis using cascaded chunking. In
CoNLL 2002: Proceedings of the 6th Conference on
Natural Language Learning 2002 (COLING 2002
Post-Conference Workshops), pages 63?69.
Taku Kudo and Yuji Matsumoto. 2005. Japanese de-
pendency parsing using relative preference of depen-
dency. Transactions of Information Processing So-
ciety of Japan, 46(4):1082?1092. (in Japanese).
Sadao Kurohashi andMakoto Nagao. 1994. Kn parser:
Japanese dependency/case structure analyzer. In
Proceedings of the Workshop on Sharable Natural
Language Resources, pages 48?55.
Sadao Kurohashi and Makoto Nagao. 1998a. Building
a Japanese parsed corpus while improving the pars-
ing system. In Proceedings of the 1st International
Conference on Language Resources and Evaluation,
pages 719?724.
Sadao Kurohashi andMakoto Nagao. 1998b. Japanese
Morphological Analysis System JUMAN version
3.5. Department of Informatics, Kyoto University.
(in Japanese).
Manabu Sassano. 2004. Linear-time dependency anal-
ysis for Japanese. In Proceedings of the COLING
2004, pages 8?14.
Kiyoaki Shirai, Kentaro Inui, Takenobu Tokunaga, and
Hozumi Tanaka. 1998. An empirical evaluation on
statistical parsing of Japanese sentences using lexi-
cal association statistics. In Proceedings of the 3rd
Conference on EMNLP, pages 80?87.
Kiyoaki Shirai. 1998. The integrated natural language
processing using statistical information. Technical
Report TR98?0004, Department of Computer Sci-
ence, Tokyo Institute of Technology. (in Japanese).
Kentaro Torisawa. 2001. An unsupervised method for
canonicalization of Japanese postpositions. In Pro-
ceedings of the 6th Natural Language Processing
Pacific Rim Symposium (NLPRS), pages 211?218.
Kiyotaka Uchimoto, Satoshi Sekine, and Hitoshi Isa-
hara. 1999. Japanese dependency structure analy-
sis based on maximum entropy models. Transac-
tions of Information Processing Society of Japan,
40(9):3397?3407. (in Japanese).
Kiyotaka Uchimoto, Masaki Murata, Satoshi Sekine,
and Hitoshi Isahara. 2000. Dependency model
using posterior context. In Proceedings of the
Sixth International Workshop on Parsing Technol-
ogy (IWPT2000), pages 321?322.
840
Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the ACL, pages 1153?1160,
Sydney, July 2006. c?2006 Association for Computational Linguistics
Time Period Identification of Events in Text 
 
 
Taichi Noro? Takashi Inui?? Hiroya Takamura? Manabu Okumura?
?Interdisciplinary Graduate School of Science and Engineering 
Tokyo Institute of Technology 
4259 Nagatsuta-cho, Midori-ku, Yokohama, Kanagawa, Japan 
??Japan Society for the Promotion of Science 
?Precision and Intelligence Laboratory, Tokyo Institute of Technology 
{norot, tinui}@lr.pi.titech.ac.jp,{takamura, oku}@pi.titech.ac.jp 
 
  
 
Abstract 
This study aims at identifying when an 
event written in text occurs. In particular, 
we classify a sentence for an event into 
four time-slots; morning, daytime, eve-
ning, and night. To realize our goal, we 
focus on expressions associated with 
time-slot (time-associated words). How-
ever, listing up all the time-associated 
words is impractical, because there are 
numerous time-associated expressions. 
We therefore use a semi-supervised 
learning method, the Na?ve Bayes classi-
fier backed up with the Expectation 
Maximization algorithm, in order to it-
eratively extract time-associated words 
while improving the classifier. We also 
propose to use Support Vector Machines 
to filter out noisy instances that indicates 
no specific time period. As a result of ex-
periments, the proposed method achieved 
0.864 of accuracy and outperformed 
other methods. 
1 Introduction 
In recent years, the spread of the internet has ac-
celerated. The documents on the internet have 
increased their importance as targets of business 
marketing. Such circumstances have evoked 
many studies on information extraction from text 
especially on the internet, such as sentiment 
analysis and extraction of location information. 
In this paper, we focus on the extraction of tem-
poral information. Many authors of documents 
on the web often write about events in their daily 
life. Identifying when the events occur provides 
us valuable information. For example, we can 
use temporal information as a new axis in the 
information retrieval. From time-annotated text, 
companies can figure out when customers use 
their products. We can explore activities of users 
for marketing researches, such as ?What do 
people eat in the morning??, ?What do people 
spend money for in daytime?? 
Most of previous work on temporal processing 
of events in text dealt with only newswire text. In 
those researches, it is assumed that temporal ex-
pressions indicating the time-period of events are 
often explicitly written in text. Some examples of 
explicit temporal expressions are as follows: ?on 
March 23?, ?at 7 p.m.?. 
However, other types of text including web 
diaries and blogs contain few explicit temporal 
expressions. Therefore one cannot acquire suffi-
cient temporal information using existing meth-
ods. Although dealing with such text as web dia-
ries and blogs is a hard problem, those types of 
text are excellent information sources due to 
their overwhelmingly huge amount. 
In this paper, we propose a method for estimat-
ing occurrence time of events expressed in in-
formal text. In particular, we classify sentences 
in text into one of four time-slots; morning, day-
time, evening, and night. To realize our goal, we 
focus on expressions associated with time-slot 
(hereafter, called time-associated words), such as 
?commute (morning)?, ?nap (daytime)? and 
?cocktail (night)?. Explicit temporal expressions 
have more certain information than the time-
associated words. However, these expressions 
are rare in usual text. On the other hand, al-
though the time-associated words provide us 
only indirect information for estimating occur-
rence time of events, these words frequently ap-
pear in usual text. Actually, Figure 2 (we will 
discuss the graph in Section 5.2, again) shows 
the number of sentences including explicit tem-
1153
poral expressions and time-associated words re-
spectively in text. The numbers are obtained 
from a corpus we used in this paper. We can fig-
ure out that there are much more time-associated 
words than explicit temporal expressions in blog 
text. In other words, we can deal with wide cov-
erage of sentences in informal text by our 
method with time-associated words. 
However, listing up all the time-associated 
words is impractical, because there are numerous 
time-associated expressions. Therefore, we use a 
semi-supervised method with a small amount of 
labeled data and a large amount of unlabeled data, 
because to prepare a large quantity of labeled 
data is costly, while unlabeled data is easy to ob-
tain. Specifically, we adopt the Na?ve Bayes 
classifier backed up with the Expectation Maxi-
mization (EM) algorithm (Dempster et al, 1977) 
for semi-supervised learning. In addition, we 
propose to use Support Vector Machines to filter 
out noisy sentences that degrade the performance 
of the semi-supervised method. 
In our experiments using blog data, we ob-
tained 0.864 of accuracy, and we have shown 
effectiveness of the proposed method. 
This paper is organized as follows. In Section 
2 we briefly describe related work. In Section 3 
we describe the details of our corpus. The pro-
posed method is presented in Section 4. In Sec-
tion 5, we describe experimental results and dis-
cussions. We conclude the paper in Section 6. 
 
2 Related Work 
The task of time period identification is new 
and has not been explored much to date. 
Setzer et al (2001) and Mani et al (2000) 
aimed at annotating newswire text for analyzing 
temporal information. However, these previous 
work are different from ours, because these work 
only dealt with newswire text including a lot of 
explicit temporal expressions. 
Tsuchiya et al (2005) pursued a similar goal 
as ours. They manually prepared a dictionary 
with temporal information. They use the hand-
crafted dictionary and some inference rules to 
determine the time periods of events. In contrast, 
we do not resort to such a hand-crafted material, 
which requires much labor and cost. Our method 
automatically acquires temporal information 
from actual data of people's activities (blog). 
Henceforth, we can get temporal information 
associated with your daily life that would be not 
existed in a dictionary. 
3 Corpus 
In this section, we describe a corpus made from 
blog entries. The corpus is used for training and 
test data of machine learning methods mentioned 
in Section 4. 
The blog entries we used are collected by the 
method of Nanno et al (2004). All the entries are 
written in Japanese. All the entries are split into 
sentences automatically by some heuristic rules. 
In the next section, we are going to explain 
?time-slot? tag added at every sentence. 
3.1 Time-Slot Tag 
The ?time-slot? tag represents when an event 
occurs in five classes; ?morning?, ?daytime?, 
?evening?, ?night?, and ?time-unknown?. ?Time-
unknown? means that there is no temporal in-
formation. We set the criteria of time-slot tags as 
follows. 
Morning: 04:00--10:59 
from early morning till before noon, breakfast 
Daytime: 11:00--15:59 
from noon till before dusk, lunch 
Evening: 16:00--17:59 
from dusk till before sunset 
Night: 18:00--03:59 
from sunset till dawn, dinner 
Note that above criteria are just interpreted as 
rough standards. We think time-slot recognized 
by authors is more important. For example, in a 
case of ?about 3 o'clock this morning? we judge 
the case as ?morning? (not ?night?) with the ex-
pression written by the author ?this morning?. 
To annotate sentences in text, we used two dif-
ferent clues. One is the explicit temporal expres-
sions or time-associated words included in the 
sentence to be judged. The other is contextual 
information around the sentences to be judged. 
The examples corresponding to the former case 
are as follows: 
 
Example 1 
a. I went to post office by bicycle in the morning. 
b. I had spaghetti at restaurant at noon. 
c. I cooked stew as dinner on that day. 
 
Suppose that the two sentences in Example 2 
appear successively in a document. In this case, 
we first judge the first sentence as morning. Next, 
we judge the second sentence as morning by con-
textual information (i.e., the preceding sentence 
is judged as morning), although we cannot know 
the time period just from the content of the sec-
ond sentence itself. 
1154
4.2 Na?ve Bayes Classifier Example 2 
1. I went to X by bicycle in the morning. In this section, we describe multinomial model 
that is a kind of Na?ve Bayes classifiers. 2. I went to a shop on the way back from X. 
A generative probability of example x  given a 
category  has the form: c
3.2 Corpus Statistics 
We manually annotated the corpus. The number 
of the blog entries is 7,413. The number of sen-
tences is 70,775. Of 70,775, the number of sen-
tences representing any events1 is 14,220. The 
frequency distribution of time-slot tags is shown 
in Table 1. We can figure out that the number of 
time-unknown sentences is much larger than the 
other sentences from this table. This bias would 
affect our classification process. Therefore, we 
propose a method for tackling the problem. 
 
( ) ( ) ( ) ( )( )?= w
xwN
xwN
cwP
xxPcxP
,
|
!,|
,
?  (1) 
where ( )xP  denotes the probability that a sen-
tence of length x  occurs,  denotes the 
number of occurrences of w  in text 
( xwN , )
x . The oc-
currence of a sentence is modeled as a set of tri-
als, in which a word is drawn from the whole 
vocabulary.  
In time-slot classification, the x  is correspond 
to each sentence, the c  is correspond to one of 
time-slots in {morning, daytime, evening, night}. 
Features are words in the sentence. A detailed 
description of features will be described in Sec-
tion 4.5. 
morning 711 
daytime 599 
evening 207 
night 1,035 
time-unknown 11,668 
Total 14,220 
4.3 Incorporation of Unlabeled Data with 
the EM Algorithm 
 
Table 1: The numbers of time-slot tags. 
 The EM algorithm (Dempster et al, 1977) is a 
method to estimate a model that has the maximal 
likelihood of the data when some variables can-
not be observed (these variables are called latent 
variables). Nigam et al (2000) proposed a com-
bination of the Na?ve Bayes classifiers and the 
EM algorithm. 
4 Proposed Method 
4.1 Basic Idea 
Suppose, for example, ?breakfast? is a strong 
clue for the morning class, i.e. the word is a 
time-associated word of morning. Thereby we 
can classify the sentence ?I have cereal for 
breakfast.? into the morning class. Then ?cereal? 
will be a time-associated word of morning. 
Therefore we can use ?cereal? as a clue of time-
slot classification. By iterating this process, we 
can obtain a lot of time-associated words with 
bootstrapping method, improving sentence clas-
sification performance at the same time. 
Ignoring the unrelated factors of Eq. (1), we 
obtain 
 
( ) ( ) ( )??
w
xwNcwPcxP ,|,| ,?  (2) 
( ) ( ) ( ) ( )???
w
xwN
c
cwPcPxP .|| ,?  (3) 
We express model parameters as ? . 
If we regard c  as a latent variable and intro-
duce a Dirichlet distribution as the prior distribu-
tion for the parameters, the Q-function (i.e., the 
expected log-likelihood) of this model is defined 
as: 
To realize the bootstrapping method, we use 
the EM algorithm. This algorithm has a theoreti-
cal base of likelihood maximization of incom-
plete data and can enhance supervised learning 
methods. We specifically adopted the combina-
tion of the Na?ve Bayes classifier and the EM 
algorithm. This combination has been proven to 
be effective in the text classification (Nigam et 
al., 2000). 
 ( ) ( )( ) ( )
( ) ( ) ( ) ,|log
,|log|
, ???
????
?
?+=
?
??
?
w
xwN
Dx c
cwPcP
cxPPQ ????
 (4) 
where ( ) ( ) ( )( )( )? ? ??? c w cwPcPP 11 | ??? . ?  is a 
user given parameter and D  is the set of exam-
ples used for model estimation. 
 
                                                 
1 The aim of this study is time-slot classification of 
events. Therefore we treat only sentences expressing 
an event. 
We obtain the next EM equation from this Q-
function: 
1155
 
Figure 1: The flow of 2-step classification. 
 
 
E-step: 
( ) ( ) ( )( ) ( ),,|| ,||,| ?= c cxPcP
cxPcP
xcP ??
???  (5) 
M-step: 
( ) ( ) ( )( ) ,1 ,|1 DC xcPcP Dx +?+?= ? ?? ??  (6) 
( )
( ) ( ) ( )
( ) ( ) ( ) ,,,|1 ,,|1
|
? ?
?
?
?
+?
+?=
w Dx
Dx
xwNxcPW
xwNxcP
cwP
??
??
 (7) 
where C  denotes the number of categories, W  
denotes the number of features variety. For la-
beled example x , Eq. (5) is not used. Instead, ( )?,| xcP  is set as 1.0 if c  is the category of x , 
otherwise 0. 
Instead of the usual EM algorithm, we use the 
tempered EM algorithm (Hofmann, 2001). This 
algorithm allows coordinating complexity of the 
model. We can realize this algorithm by substi-
tuting the next equation for Eq. (5) at E-step: 
 
( ) ( ) ( ){ }( ) ( ){ } ,,|| ,||,| ?= c cxPcP
cxPcP
xcP ?
?
??
???  (8) 
where ?  denotes a hyper parameter for coordi-
nating complexity of the model, and it is positive 
value. By decreasing this hyper-parameter ? , we 
can reduce the influence of intermediate classifi-
cation results if those results are unreliable. 
Too much influence by unlabeled data some-
times deteriorates the model estimation. There-
fore, we introduce a new hyper-parameter 
( 10 ?? )??  which acts as weight on unlabeled 
data. We exchange the second term in the right-
hand-side of Eq. (4) for the next equation: 
( ) ( ) ( ) ( )
( ) ( ) ( ) ( ) ,|log,|
|log,|
,
,
? ??
? ??
?
?
???
????
?+
???
????
?
u
l
Dx w
xwN
c
Dx w
xwN
c
cwPcPxcP
cwPcPxcP
??
?
 
where lD  denotes labeled data, uD  denotes 
unlabeled data. We can reduce the influence of 
unlabeled data by decreasing the value of ? . 
We derived new update rules from this new Q-
function. The EM computation stops when the 
difference in values of the Q-function is smaller 
than a threshold. 
4.4 Class Imbalance Problem 
We have two problems with respect to ?time-
unknown? tag.  
The first problem is the class imbalance prob-
lem (Japkowicz 2000). The number of time-
unknown time-slot sentences is much larger than 
that of the other sentences as shown in Table 1. 
There are more than ten times as many time-
unknown time-slot sentences as the other sen-
tences.  
Second, there are no time-associated words in 
the sentences categorized into ?time-unknown?. 
Thus the feature distribution of time-unknown 
time-slot sentences is remarkably different from 
the others. It would be expected that they ad-
versely affect proposed method. 
There have been some methodologies in order 
to solve the class imbalance problem, such as 
Zhang and Mani (2003), Fan et al (1999) and 
Abe et al (2004). However, in our case, we have 
to resolve the latter problem in addition to the 
class imbalance problem. To deal with two prob-
lems above simultaneously and precisely, we 
develop a cascaded classification procedure. 
SVM 
NB + EM 
Step 2 
Time-Slot 
Classifier 
time-slot = time-unknown 
time-slot = morning, daytime, evening, night 
time-slot = morning 
time-slot = daytime 
time-slot = morning, daytime, evening, night, time-unknown Step1 
Time-Unknown 
Filter 
time-slot = night 
time-slot = evening 
1156
4.5 Time-Slot Classification Method 
It?s desirable to treat only ?time-known? sen-
tences at NB+EM process to avoid the above-
mentioned problems. We prepare another classi-
fier for filtering time-unknown sentences before 
NB+EM process for that purpose. Thus, we pro-
pose a classification method in 2 steps (Method 
A). The flow of the 2-step classification is shown 
in Figure 1. In this figure, ovals represent classi-
fiers, and arrows represent flow of data. 
The first classifier (hereafter, ?time-unknown? 
filter) classifies sentences into two classes; 
?time-unknown? and ?time-known?. The ?time-
known? class is a coarse class consisting of four 
time-slots (morning, daytime, evening, and 
night). We use Support Vector Machines as a 
classifier. The features we used are all words 
included in the sentence to be classified.  
The second classifier (time-slot classifier) 
classifies ?time-known? sentences into four 
classes. We use Na?ve Bayes classifier backed up 
with the Expectation Maximization (EM) algo-
rithm mentioned in Section 4.3.  
The features for the time-slot classifier are 
words, whose part of speech is noun or verb. The 
set of these features are called NORMAL in the 
rest of this paper. In addition, we use information 
from the previous and the following sentences in 
the blog entry. The words included in such sen-
tences are also used as features. The set of these 
features are called CONTEXT. The features in 
CONTEXT would be effective for estimating 
time-slot of the sentences as mentioned in Ex-
ample2 in Section 3.1. 
We also use a simple classifier (Method B) for 
comparison. The Method B classifies all time-
slots (morning ~ night, time-unknown) sentences 
at just one step. We use Na?ve Bayes classifier 
backed up with the Expectation Maximization 
(EM) algorithm at this learning. The features are 
words (whose part-of-speech is noun or verb) 
included in the sentence to be classified. 
 
5 Experimental Results and Discussion 
5.1 Time-Slot Classifier with Time-
Associated Words 
5.1.1 Time-Unknown Filter 
We used 11.668 positive (time-unknown) sam-
ples and 2,552 negative (morning ~ night) sam-
ples. We conducted a classification experiment 
by Support Vector Machines with 10-fold cross 
validation. We used TinySVM2 software pack-
age for implementation. The soft margin parame-
ter is automatically estimated by 10-fold cross 
validation with training data. The result is shown 
in Table 2. 
 
Table 2 clarified that the ?time-unknown? fil-
ter achieved good performance; F-measure of 
0.899. In addition, since we obtained a high re-
call (0.969), many of the noisy sentences will be 
filtered out at this step and the classifier of the 
second step is likely to perform well. 
 
Accuracy 0.878 
Precision 0.838 
Recall 0.969 
F-measure 0.899 
 
Table 2: Classification result of  
the time-unknown filter. 
 
5.1.2 Time-Slot Classification 
In step 2, we used ?time-known? sentences clas-
sified by the unknown filter as test data. We con-
ducted a classification experiment by Na?ve 
Bayes classifier + the EM algorithm with 10-fold 
cross validation. For unlabeled data, we used 
64,782 sentences, which have no intersection 
with the labeled data. The parameters, ?  and ? , 
are automatically estimated by 10-fold cross 
validation with training data. The result is shown 
in Table 3. 
 
Accuracy Method 
NORMAL CONTEXT
Explicit 0.109 
Baseline 0.406 
NB 0.567 0.464 
NB + EM 0.673 0.670 
Table 3: The result of time-slot classifier. 
                                                 
2 http://www.chasen.org/~taku/software/TinySVM 
1157
 
 
 
 
 
 
 
 
 
 
Table 4: Confusion matrix of output. 
 
 morning daytime evening night 
rank word p(c|w) word p(c|w) word p(c|w) word p(c|w)
1 this morning 0.729 noon 0.728 evening 0.750 last night 0.702 
2 morning 0.673 early after noon 0.674 sunset 0.557 night 0.689 
3 breakfast 0.659 afternoon 0.667 academy 0.448 fireworks 0.688 
4 early morning 0.656 daytime 0.655 dusk 0.430 dinner 0.684 
5 before noon 0.617 lunch 0.653 Hills 0.429 go to bed 0.664 
6 compacted snow 0.603 lunch 0.636 run on 0.429 night 0.641 
7 commute 0.561 lunch break 0.629 directions 0.429 bow 0.634 
8 --- 0.541 lunch 0.607 pinecone 0.429 overtime 0.606 
9 parade 0.540 noon 0.567 priest 0.428 year-end party 0.603 
10 wake up 0.520 butterfly 0.558 sand beach 0.428 dinner 0.574 
11 leave harbor 0.504 Chinese food 0.554 --- 0.413 beach 0.572 
12 rise late 0.504 forenoon 0.541 Omori 0.413 cocktail 0.570 
13 cargo work 0.504 breast-feeding 0.536 fan 0.413 me 0.562 
14 alarm clock 0.497 nap 0.521 Haneda 0.412 Tomoyuki 0.560 
15 --- 0.494 diaper 0.511 preview 0.402 return home 0.557 
16 sunglow 0.490 Japanese food 0.502 cloud 0.396 close 0.555 
17 wheel 0.479 star festival 0.502 Dominus 0.392 stay up late 0.551 
18 wake up 0.477 hot noodle 0.502 slip 0.392 tonight 0.549 
19 perm 0.474 pharmacy 0.477 tasting 0.391 night 0.534 
20 morning paper 0.470 noodle 0.476 nest 0.386 every night 0.521 
Table 5: Time-associated words examples. 
 
In Table 3, ?Explicit? indicates the result by a 
simple classifier based on regular expressions 3  
including explicit temporal expressions. The 
baseline method classifies all sentences into 
night because the number of night sentences is 
the largest. The ?CONTEXT? column shows the 
results obtained by classifiers learned with the 
features in CONTEXT in addition to the features 
                                                 
3 For example, we classify sentences matching follow-
ing regular expressions into morning class: 
[(gozen)(gozen-no)(asa) (asa-no)(am)(AM)(am-
no)(AM-no)][456789(10)] ji, [(04)(05)(06)(07)(08) 
(09)]ji, [(04)(05)(06)(07) (08) (09)]:[0-9]{2,2}, 
[456789(10)][(am)(AM)]. 
??gozen?, ?gozen?no? means before noon. ?asa?, 
?asa-no? means morning. ?ji? means o?clock.? 
in NORMAL. The accuracy of the Explicit 
method is lower than the baseline. This means 
existing methods based on explicit temporal ex-
pressions cannot work well in blog text. The ac-
curacy of the method 'NB' exceeds that of the 
baseline by 16%. Furthermore, the accuracy of 
the proposed method 'NB+EM' exceeds that of 
the 'NB' by 11%. Thus, we figure out that using 
unlabeled data improves the performance of our 
time-slot classification.  
In this experiment, unfortunately, CONTEXT 
only deteriorated the accuracy. The time-slot tags 
of the sentences preceding or following the target 
sentence may still provide information to im-
prove the accuracy. Thus, we tried a sequential 
tagging method for sentences, in which tags are 
output of time-slot classifier 
 morning daytime evening night time-unknown 
sum 
morning 332 14 1 37 327 711 
daytime 30 212 1 44 312 599 
evening 4 5 70 18 110 207 
night 21 19 4 382 609 1035 
tim
e-
sl
ot
 ta
g 
time-unknown 85 66 13 203 11301 11668 
sum 472 316 89 684 12659 14220 
1158
predicted in the order of their occurrence. The 
predicted tags are used as features in the predic-
tion of the next tag. This type of sequential tag-
ging method regard as a chunking procedure 
(Kudo and Matsumoto, 2000) at sentence level. 
We conducted time-slot (five classes) classifica-
tion experiment, and tried forward tagging and 
backward tagging, with several window sizes. 
We used YamCha4, the multi-purpose text chun-
ker using Support Vector Machines, as an ex-
perimental tool. However, any tagging direction 
and window sizes did not improve the perform-
ance of classification. Although a chunking 
method has possibility of correctly classifying a 
sequence of text units, it can be adversely biased 
by the preceding or the following tag. The sen-
tences in blog used in our experiments would not 
have a very clear tendency in order of tags. This 
is why the chunking-method failed to improve 
the performance in this task. We would like to 
try other bias-free methods such as Conditional 
Random Fields (Lafferty et al, 2001) for future 
work. 
5.1.3 2-step Classification 
Finally, we show an accuracy of the 2-step clas-
sifier (Method A) and compare it with those of 
other classifiers in Table 6. The accuracies are 
calculated with the equation: 
 
. 
 
In Table 6, the baseline method classifies all 
sentences into time-unknown because the num-
ber of time-unknown sentences is the largest. 
Accuracy of Method A (proposed method) is 
higher than that of Method B (4.1% over). These 
results show that time-unknown sentences ad-
versely affect the classifier learning, and 2-step 
classification is an effective method. 
Table 4 shows the confusion matrix corre-
sponding to the Method A (NORMAL). From 
this table, we can see Method A works well for 
classification of morning, daytime, evening, and 
night, but has some difficulty in 
 
                                                 
4 http://www.chasen.org/~taku/software/YamCha 
Table 6: Comparison of the methods for five 
class classification 
 
 
Figure 2: Change of # sentences that have time-
associated words: ?Explicit? indicates the num-
ber of sentences including explicit temporal ex-
pressions, ?NE-TIME? indicates the number of 
sentences including NE-TIME tag. 
 
classification of time-unknown. The 11.7% of 
samples were wrongly classified into ?night? or 
?unknown?. 
We briefly describe an error analysis. We 
found that our classifier tends to wrongly classify 
samples in which two or more events are written 
in a sentence. The followings are examples: 
 
Example 3 
a. I attended a party last night, and I got back 
on the first train in this morning because the 
party was running over. 
b. I bought a cake this morning, and ate it after 
the dinner. 
5.2 Examples of Time-Associated Words 
Table 5 shows some time-associated words ob-
tained by the proposed method. The words are 
sorted in the descending order of the value of ( )wcP | . Although some consist of two or three 
words, their original forms in Japanese consist of 
one word. There are some expressions appearing 
more than once, such as ?dinner?. Actually these 
expressions have different forms in Japanese. 
Meaningless (non-word) strings caused by mor-
Method Conclusive accuracy
Explicit 0.833 
Baseline 0.821 
Method A (NORMAL) 0.864 
Method A (CONTEXT) 0.862 
Method B 0.823 
0
1000
2000
3000
4000
5000
1 10 20 30 40 50 60 70 80 90 100
# time-associated words (N-best)
# 
se
nt
en
ce
s 
in
cl
ud
in
g 
ti
m
e-
as
so
ci
at
ed
 w
or
ds
   Explicit 
NE-TIME 
# time-unknown sentences correctly classi-
fied by the time-unknown filter 
# known sentences correctly classi-
fied by the time-slot classifier + 
# sentences with a time-slot tag value 
1159
phological analysis error are presented as the 
symbol ?---?. We obtained a lot of interesting 
time-associated words, such as ?commute (morn-
ing)?, ?fireworks (night)?, and ?cocktail (night)?. 
Most words obtained are significantly different 
from explicit temporal expressions and NE-
TIME expressions. 
Figure 2 shows the number of sentences in-
cluding time-associated words in blog text. The 
horizontal axis represents the number of time-
associated words. We sort the words in the de-
scending order of  and selected the top N 
words. The vertical axis represents the number of 
sentences including any N-best time-associated 
words. We also show the number of sentences 
including explicit temporal expressions, and the 
number of sentences including NE-TIME tag 
(Sekine and Isahara, 1999) for comparison. The 
set of explicit temporal expressions was ex-
tracted by the method described in Section 5.1.2. 
We used a Japanese linguistic analyzer ?Cabo-
Cha
( wcP | )
                                                
5 ? to obtain NE-TIME information. From 
this graph, we can confirm that the number of 
target sentences of our proposed method is larger 
than that of existing methods. 
 
6 Conclusion 
In our study, we proposed a method for identify-
ing when an event in text occurs. We succeeded 
in using a semi-supervised method, the Na?ve 
Bayes Classifier enhanced by the EM algorithm, 
with a small amount of labeled data and a large 
amount of unlabeled data. In order to avoid the 
class imbalance problem, we used a 2-step classi-
fier, which first filters out time-unknown sen-
tences and then classifies the remaining sen-
tences into one of 4 classes. The proposed 
method outperformed the simple 1-step method. 
We obtained 86.4% of accuracy that exceeds the 
existing method and the baseline method. 
 
References 
Naoki Abe, Bianca Zadrozny, John Langford. 2004. 
An Iterative Method for Multi-class Cost-sensitive 
Learning. In Proc. of the 10th. ACM SIGKDD, 
pp.3?11. 
Arthur P. Dempster, Nan M. laird, and Donald B. 
Rubin. 1977. Maximum likelihood from incom-
plete data via the EM algorithm. Journal of the 
 
5 http://chasen.org/~taku/software/cabocha/ 
Royal Statistical Society Series B, Vol. 39, No. 1, 
pp.1?38. 
Wei Fan, Salvatore J. Stolfo, Junxin Zhang, Philip K. 
Chan. 1999. AdaCost: Misclassification Cost-
sensitive Boosting. In Proc. of ICML, pp.97?105. 
Thomas Hofmann. 2001. Unsupervised learning by 
probabilistic latent semantic analysis. Machine 
Learning, 42:177?196. 
Nathalie Japkowicz. 2000. Learning from Imbalanced 
Data Sets: A Comparison of Various Strategies. In 
Proc. of the AAAI Workshop on Learning from Im-
balanced Data Sets, pp.10 ?15. 
Taku Kudo, Yuji Matsumoto. 2000. Use of Support 
Vector Learning for Chunking Identification, In 
Proc of the 4th CoNLL, pp.142?144. 
John Lafferty, Andrew McCallum, and Fernando 
Pereira. 2001. Conditional random fields: Probabil-
istic models for segmenting and labeling sequence 
data, In Proc. of ICML, pp.282?289. 
Inderjeet Mani, George Wilson 2000. Robust Tempo-
ral Processing of News. In Proc. of the 38th ACL, 
pp.69?76. 
Tomoyuki Nanno, Yasuhiro Suzuki, Toshiaki Fujiki, 
Manabu Okumura. 2004. Automatically Collecting 
and Monitoring Japanese Weblogs. Journal for 
Japanese Society for Artificial Intelligence ?
Vol.19, No.6, pp.511?520. (in Japanese) 
Kamal Nigam, Andrew McCallum, Sebastian Thrun, 
and Tom Mitchell. 2000. Text classification from 
labeled and unlabeled documents using EM. Ma-
chine Learning, Vol. 39, No.2/3, pp.103?134. 
Satoshi Sekine, Hitoshi Isahara. 1999. IREX project 
overview. Proceedings of the IREX Workshop. 
Andrea Setzer, Robert Gaizauskas. 2001.  A Pilot 
Study on Annotating Temporal Relations in Text. 
In Proc. of the ACL-2001 Workshop on Temporal 
and Spatial Information Processing, Toulose, 
France, July, pp.88?95. 
Seiji Tsuchiya, Hirokazu Watabe, Tsukasa Kawaoka. 
2005. Evaluation of a Time Judgement Technique 
Based on an Association Mechanism. IPSG SIG 
Technical Reports?2005-NL-168, pp.113?118. (in 
Japanese) 
Jianping Zhang, Inderjeet Mani. 2003. kNN Approach 
to Unbalanced Data Distributions: A Case Study 
involving Information Extraction. In Proc. of 
ICML Workshop on Learning from Imbalanced 
Datasets II., pp.42?48. 
1160
Proceedings of the COLING/ACL 2006 Main Conference Poster Sessions, pages 9?16,
Sydney, July 2006. c?2006 Association for Computational Linguistics
A Rote Extractor with Edit Distance-based Generalisation and
Multi-corpora Precision Calculation
Enrique Alfonseca12 Pablo Castells1 Manabu Okumura2 Maria Ruiz-Casado12
1Computer Science Deptartment 2Precision and Intelligence Laboratory
Univ. Auto?noma de Madrid Tokyo Institute of Technology
Enrique.Alfonseca@uam.es enrique@lr.pi.titech.ac.jp
Pablo.Castells@uam.es oku@pi.titech.ac.jp
Maria.Ruiz@uam.es maria@lr.pi.titech.ac.jp
Abstract
In this paper, we describe a rote extrac-
tor that learns patterns for finding seman-
tic relationships in unrestricted text, with
new procedures for pattern generalization
and scoring. These include the use of part-
of-speech tags to guide the generalization,
Named Entity categories inside the pat-
terns, an edit-distance-based pattern gen-
eralization algorithm, and a pattern accu-
racy calculation procedure based on eval-
uating the patterns on several test corpora.
In an evaluation with 14 entities, the sys-
tem attains a precision higher than 50% for
half of the relationships considered.
1 Introduction
Recently, there is an increasing interest in auto-
matically extracting structured information from
large corpora and, in particular, from the Web
(Craven et al, 1999). Because of the difficulty of
collecting annotated data, several procedures have
been described that can be trained on unannotated
textual corpora (Riloff and Schmelzenbach, 1998;
Soderland, 1999; Mann and Yarowsky, 2005).
An interesting approach is that of rote extrac-
tors (Brin, 1998; Agichtein and Gravano, 2000;
Ravichandran and Hovy, 2002), which look for
textual contexts that happen to convey a certain re-
lationship between two concepts.
In this paper, we describe some contributions
to the training of Rote extractors, including a pro-
cedure for generalizing the patterns, and a more
complex way of calculating their accuracy. We
first introduce the general structure of a rote ex-
tractor and its limitations. Next, we describe the
proposed modifications (Sections 2, 3 and 4) and
the evaluation performed (Section 5).
1.1 Rote extractors
According to the traditional definition of rote ex-
tractors (Mann and Yarowsky, 2005), they esti-
mate the probability of a relationship r(p, q) given
the surrounding contextA1pA2qA3. This is calcu-
lated, with a training corpus T , as the number of
times that two related elements r(x, y) from T ap-
pear with that same context A1xA2yA3, divided
by the total number of times that x appears in that
context together with any other word:
P (r(p, q)|A1pA2qA3) =
P
x,yr c(A1xA2yA3)
P
x,z c(A1xA2zA3)
(1)
x is called the hook, and y the target. In order
to train a Rote extractor from the web, this proce-
dure is usually followed (Ravichandran and Hovy,
2002):
1. Select a pair of related elements to be used
as seed. For instance, (Dickens,1812) for the
relationship birth year.
2. Submit the query Dickens AND 1812 to a
search engine, and download a number of
documents to build the training corpus.
3. Keep all the sentences containing both ele-
ments.
4. Extract the set of contexts between them and
identify repeated patterns. This may just be
the m characters to the left or to the right,
(Brin, 1998), the longest common substring
of several contexts (Agichtein and Gravano,
2000), or all substrings obtained with a suf-
fix tree constructor (Ravichandran and Hovy,
2002).
5. Download a separate corpus, called hook cor-
pus, containing just the hook (in the example,
Dickens).
6. Apply the previous patterns to the hook cor-
pus, calculate the precision of each pattern
9
in the following way: the number of times it
identifies a target related to the hook divided
by the total number of times the pattern ap-
pears.
7. Repeat the procedure for other examples of
the same relationship.
To illustrate this process, let us suppose that we
want to learn patterns to identify birth years. We
may start with the pair (Dickens, 1812). From the
downloaded corpus, we extract sentences such as
Dickens was born in 1812
Dickens (1812 - 1870) was an English writer
Dickens (1812 - 1870) wrote Oliver Twist
The system identifies that the contexts of the last
two sentences are very similar and chooses their
longest common substring to produce the follow-
ing patterns:
<hook> was born in <target>
<hook> ( <target> - 1870 )
In order to measure the precision of the ex-
tracted patterns, a new corpus is downloaded us-
ing the hook Dickens as the only query word, and
the system looks for appearances of the patterns
in the corpus. For every occurrence in which the
hook of the relationship is Dickens, if the target
is 1812 it will be deemed correct, and otherwise
it will be deemed incorrect (e.g. in Dickens was
born in Portsmouth).
1.2 Limitations and new proposal
We have identified the following limitations in this
algorithm: firstly, to our knowledge, no Rote ex-
tractor allows for the insertion of wildcards (e.g.
*) in the extracted patterns. Ravichandran and
Hovy (2002) have noted that this might be dan-
gerous if the wildcard matches unrestrictedly in-
correct sentences. However, we believe that the
precision estimation that is performed at the last
step of the algorithm, using the hook corpus, may
be used to rule out the dangerous wildcards while
keeping the useful ones.
Secondly, we believe that the procedure for cal-
culating the precision of the patterns may be some-
what unreliable in a few cases. For instance,
Ravichandran and Hovy (2002) report the follow-
ing patterns for the relationships Inventor, Discov-
erer and Location:
Relation Prec. Pattern
Inventor 1.0 <target> ?s <hook> and
Inventor 1.0 that <target> ?s <hook>
Discoverer 0.91 of <target> ?s <hook>
Location 1.0 <target> ?s <hook>
In this case, it can be seen that the same pattern
(the genitive construction) may be used to indi-
cate several different relationships, apart from the
most common use indicating possession. How-
ever, they all receive very high precision values.
The reason is that the patterns are only evaluated
for the same hook for which they were extracted.
Let us suppose that we obtain the pattern for Loca-
tion using the pairs (New York, Chrysler Building).
The genitive construction can be extracted from
the context New York?s Chrysler Building. After-
ward, when evaluating it, only sentences contain-
ing <target>?s Chrysler Building are taken into
account, which makes it unlikely that the pattern
is expressing a relationship other than Location,
so the pattern will receive a high precision value.
For our purposes, however, we need to collect
patterns for several relations such as writer-book,
painter-picture, director-film, actor-film, and we
want to make sure that the obtained patterns are
only applicable to the desired relationship. Pat-
terns like <target> ?s <hook> are very likely to
be applicable to all of these relationships at the
same time, so we would like to be able to discard
them automatically.
Hence, we propose the following improvements
for a Rote extractor:
? A new pattern generalization procedure that
allows the inclusion of wildcards in the pat-
terns.
? The combination with Named Entity recogni-
tion, so people, locations, organizations and
dates are replaced by their entity type in the
patterns, in order to increase their degree of
generality. This is in line with Mann and
Yarowsky (2003)?s modification, consisting
in replacing all numbers in the patterns with
the symbol ####.
? A new precision calculation procedure, in a
way that the patterns obtained for a given re-
lationship are evaluated on the corpus for dif-
ferent relationships, in order to improve the
detection of over-general patterns.
2 Proposed pattern generalization
procedure
To begin with, for every appearance of a pair of
concepts, we extract a context around them. Next,
those contexts are generalized to obtain the parts
that are shared by several of them. The procedure
is detailed in the following subsections.
10
Birth year:
BOS/BOS <hook> (/( <target> -/- number/entity )/) EOS/EOS
BOS/BOS <hook> (/( <target> -/- number/entity )/) British/JJ writer/NN
BOS/BOS <hook> was/VBD born/VBN on/IN the/DT first/JJ of/IN time expr/entity ,/, <target> ,/, at/IN location/entity ,/, of/IN
BOS/BOS <hook> (/( <target> -/- )/) a/DT web/NN guide/NN
Birth place:
BOS/BOS <hook> was/VBD born/VBN in/IN <target> ,/, in/IN central/JJ location/entity ,/,
BOS/BOS <hook> was/VBD born/VBN in/IN <target> date/entity and/CC moved/VBD to/TO location/entity
BOS/BOS Artist/NN :/, <hook> -/- <target> ,/, location/entity (/( number/entity -/-
BOS/BOS <hook> ,/, born/VBN in/IN <target> on/IN date/entity ,/, worked/VBN as/IN
Author-book:
BOS/BOS <hook> author/NN of/IN <target> EOS/EOS
BOS/BOS Odysseus/NNP :/, Based/VBN on/IN <target> ,/, <hook> ?s/POS epic/NN from/IN Greek/JJ mythology/NN
BOS/BOS Background/NN on/IN <target> by/IN <hook> EOS/EOS
did/VBD the/DT circumstances/NNS in/IN which/WDT <hook> wrote/VBD "/?? <target> "/?? in/IN number/entity ,/, and/CC
Capital-country:
BOS/BOS <hook> is/VBZ the/DT capital/NN of/IN <target> location/entity ,/, location/entity correct/JJ time/NN
BOS/BOS The/DT harbor/NN in/IN <hook> ,/, the/DT capital/NN of/IN <target> ,/, is/VBZ number/entity of/IN location/entity
BOS/BOS <hook> ,/, <target> EOS/EOS
BOS/BOS <hook> ,/, <target> -/- organization/entity EOS/EOS
Figure 1: Example patterns extracted from the training corpus for each several kinds of relationships.
2.1 Context extraction procedure
After selecting the sentences for each pair of re-
lated words in the training set, these are pro-
cessed with a part-of-speech tagger and a module
for Named Entity Recognition and Classification
(NERC) that annotates people, organizations, lo-
cations, dates, relative temporal expressions and
numbers. Afterward, a context around the two
words in the pair is extracted, including (a) at most
five words to the left of the first word; (b) all the
words in between the pair words; (c) at most five
words to the right of the second word. The context
never jumps over sentence boundaries, which are
marked with the symbols BOS (Beginning of sen-
tence) and EOS (End of sentence). The two related
concepts are marked as <hook> and <target>.
Figure 1 shows several example contexts extracted
for the relationships birth year, birth place, writer-
book and capital-country.
Furthermore, for each of the entities in the re-
lationship, the system also stores in a separate file
the way in which they are annotated in the training
corpus: the sequences of part-of-speech tags of ev-
ery appearance, and the entity type (if marked as
such). So, for instance, typical PoS sequences for
names of authors are ?NNP?1 (surname) and ?NNP
NNP? (first name and surname). A typical entity
kind for an author is person.
2.2 Generalization pseudocode
In order to identify the portions in common be-
tween the patterns, and to generalize them, we ap-
ply the following pseudocode (Ruiz-Casado et al,
in press):
1All the PoS examples in this paper are done with Penn
Treebank labels (Marcus et al, 1993).
1. Store all the patterns in a set P .
2. Initialize a setR as an empty set.
3. While P is not empty,
(a) For each possible pair of patterns, cal-
culate the distance between them (de-
scribed in Section 2.3).
(b) Take the two patterns with the smallest
distance, pi and pj .
(c) Remove them from P , and add them to
R.
(d) Obtain the generalization of both, pg
(Section 2.4).
(e) If pg does not have a wildcard adjacent
to the hook or the target, add it to P .
4. ReturnR
At the end, R contains all the initial patterns
and those obtained while generalizing the previous
ones. The motivation for step (e) is that, if a pat-
tern contains a wildcard adjacent to either the hook
or the target, it will be impossible to know where
it starts or ends. For instance, when applying the
pattern <hook> wrote * <target> to a text, the
wildcard prevents the system from guessing where
the title of the book starts.
2.3 Edit distance calculation
So as to calculate the similarity between two pat-
terns, a slightly modified version of the dynamic
programming algorithm for edit-distance calcula-
tion (Wagner and Fischer, 1974) is used. The dis-
tance between two patterns A and B is defined as
the minimum number of changes (insertion, addi-
tion or replacement) that have to be done to the
first one in order to obtain the second one.
The calculation is carried on by filling a ma-
trix M, as shown in Figure 2 (left). At the same
11
A: wrote the well known novel
B: wrote the classic novel
M 0 1 2 3 4 D 0 1 2 3 4
0 0 1 2 3 4 0 I I I I
1 1 0 1 2 3 1 R E I I I
2 2 1 0 1 2 2 R R E I I
3 3 2 1 1 2 3 R R R U I
4 4 3 2 2 2 4 R R R R U
5 5 4 3 3 2 5 R R R R E
Figure 2: Example of the edit distance algorithm. A and B are two word patterns;M is the matrix in which the edit distance
is calculated, and D is the matrix indicating the choice that produced the minimal distance for each cell inM.
time that we calculate the edit distance matrix, it
is possible to fill in another matrix D, in which we
record which of the choices was selected at each
step: insertion, deletion, replacement or no edi-
tion. This will be used later to obtain the gener-
alized pattern. We have used the following four
characters:
? I means that it is necessary to insert a token
in the first pattern to obtain the second one.
? R means that it is necessary to remove a to-
ken.
? E means that the corresponding tokens are
equal, so no edition is required.
? U means that the corresponding tokens are
unequal, so a replacement has to be done.
Figure 2 shows an example for two patterns,
A and B, containing respectively 5 and 4 to-
kens. M(5, 4) has the value 2, indicating the dis-
tance between the two complete patterns. For in-
stance, the two editions would be replacing well
by classic and removing known.
2.4 Obtaining the generalized pattern
After calculating the edit distance between two
patterns A and B, we can use matrix D to obtain
a generalized pattern, which should maintain the
common tokens shared by them. The procedure
used is the following:
? Every time there is an insertion or a deletion,
the generalized pattern will contain a wild-
card, indicating that there may be anything in
between.
? Every time there is replacement, the general-
ized pattern will contain a disjunction of both
tokens.
? Finally, in the positions where there is no edit
operation, the token that is shared between
the two patterns is left unchanged.
The patterns in the example will produced the
generalized pattern
Wrote the well known novel
Wrote the classic novel
???????????
Wrote the well|classic * novel
The generalization of these two patterns pro-
duces one that can match a wide variety of sen-
tences, so we should always take care in order not
to over-generalize.
2.5 Considering part-of-speech tags and
Named Entities
If we consider the result in the previous example,
we can see that the disjunction has been made be-
tween an adverb and an adjective, while the other
adjective has been deleted. A more natural result,
with the same number of editing operations as the
previous one, would have been to delete the adverb
to obtain the following generalization:
Wrote the well known novel
Wrote the classic novel
???????????
Wrote the * known|classic novel
This is done taking into account part-of-speech
tags in the generalization process. In this way, the
edit distance has been modified so that a replace-
ment operation can only be done between words of
the same part-of-speech.2 Furthermore, replace-
ments are given an edit distance of 0. This favors
the choice of replacements with respect to dele-
tions and insertions. To illustrate this point, the
distance between known|classic/JJ and old/JJ
2Note that, although our tagger produces the very detailed
PennTreebank labels, we consider that all nouns (NN, NNS,
NNP and NNPS) belong to the same part-of-speech class, and
the same for adjectives, verbs and adverbs.
12
Hook Birth Death Birth place Author of Director of Capital of
Charles Dickens 1812 1870 Portsmouth
{Oliver Twist,
The Pickwick Papers,
Nicholas Nickleby,
David Copperfield...}
None None
Woody Allen 1935 None Brooklin None
{Bananas,
Annie Hall,
Manhattan, ... }
None
Luanda None None None None None Angola
Table 1: Example rows in the input table for the system.
will be set to 0, because both tokens are adjectives.
In other words, the d function is redefined as:
d(A[i], B[j]) =
(
0 if PoS(A[i]) = PoS(B[j])
1 otherwise
(2)
Note that all the entities identified by the NERC
module will appear with a PoS tag of entity,
so it is possible to have a disjunction such as
location|organization/entity in a general-
ized pattern (See Figure 1).
3 Proposed pattern scoring procedure
As indicated above, if we measure the precision of
the patterns using a hook corpus-based approach,
the score may be inadvertently increased because
they are only evaluated using the same terms with
which they were extracted. The approach pro-
posed herein is to take advantage of the fact that
we are obtaining patterns for several relationships.
Thus, the hook corpora for some of the patterns
can be used also to identify errors done by other
patterns.
The input of the system now is not just a list
of related pairs, but a table including several rela-
tionships for the same entities. We may consider
it as mini-biographies as in Mann and Yarowsky
(2005)?s system. Table 1 shows a few rows in the
input table for the system. The cells for which
no data is provided have a default value of None,
which means that anything extracted for that cell
will be considered as incorrect.
Although this table can be written by hand, in
our experiments we have chosen to build it auto-
matically from the lists of related pairs. The sys-
tem receives the seed pairs for the relationships,
and mixes the information from all of them in a
single table. In this way, if Dickens appears in
the seed list for the birth year, death year, birth
place and writer-book relationships, four of the
cells in its row will be filled in with values, and
all the rest will be set to None. This is probably a
very strict evaluation, because, for all the cells for
which there was no value in any of the lists, any re-
sult obtained will be judged as incorrect. However,
the advantage is that we can study the behavior of
the system working with incomplete data.
The new procedure for calculating the patterns?
precisions is as follows:
1. For every relationship, and for every hook,
collect a hook corpus from the Internet.
2. Apply the patterns to all the hook corpora
collected. Whenever a pattern extracts a re-
lationship from a sentence,
? If the table does not contain a row for
the hook, ignore the result.
? If the extracted target appears in the cor-
responding cell in the table, consider it
correct.
? If that cell contained different values, or
None, consider it incorrect.
For instance, the pattern <target> ?s <hook>
extracted for director-film may find, in the Dick-
ens corpus, book titles. Because these titles do not
appear in the table as films directed by Dickens,
the pattern will be considered to have a low accu-
racy.
In this step, every pattern that did not apply at
least three times in the test corpora was discarded.
4 Pattern application
Finally, given a set of patterns for a particular
relation, the procedure for obtaining new pairs is
straightforward:
1. For any of the patterns,
2. For each sentence in the test corpus,
(a) Look for the left-hand-side context in
the sentence.
(b) Look for the middle context.
(c) Look for the right-hand-side context.
(d) Take the words in between, and check
that either the sequence of part-of-
speech tags or the entity type had been
13
Applied Prec. Pattern
3 1.0 BOS/BOS On/IN time expr/entity TARGET HOOK was/VBD baptized|born/VBN EOS/EOS
15 1.0 "/?? HOOK (/( TARGET -/-
4 1.0 ,/, TARGET ,/, */* Eugne|philosopher|playwright|poet/NNP HOOK earned|was/VBD */* at|in/IN
23 1.0 -|--/- HOOK (/( TARGET -/-
12 1.0 AND|and|or/CC HOOK (/( TARGET -/-
48 1.0 By|about|after|by|for|in|of|with/IN HOOK TARGET -/-
4 1.0 On|of|on/IN TARGET ,/, HOOK emigrated|faced|graduated|grew|has|perjured|settled|was/VBD
12 1.0 BOS/BOS HOOK TARGET -|--/-
49 1.0 ABOUT|ALFRED|Amy|Audre|Authors|BY| (...) |teacher|writer/NNPS HOOK (/( TARGET -|--/-
7 1.0 BOS/BOS HOOK (/( born/VBN TARGET )/)
3 1.0 BOS/BOS HOOK ,/, */* ,/, TARGET ,/,
13 1.0 BOS/BOS HOOK ,|:/, TARGET -/-
132 0.98 BOS/BOS HOOK (/( TARGET -|--/-
18 0.94 By|Of|about|as|between|by|for|from|of|on|with/IN HOOK (/( TARGET -/-
33 0.91 BOS/BOS HOOK ,|:/, */* (/( TARGET -|--/-
10 0.9 BOS/BOS HOOK ,|:/, */* ,|:/, TARGET -/-
3 0.67 ,|:|;/, TARGET ,|:/, */* Birth|son/NN of/IN */* General|playwright/NNP HOOK ,|;/,
210 0.63 ,|:|;/, HOOK (/( TARGET -|--/-
7 0.29 (/( HOOK TARGET )/)
Table 3: Patterns for the relationship birth year.
.
Relation Seeds Extr. Gener. Filt.
Actor-film 133 480 519 10
Writer-book 836 3858 4847 171
Birth-year 492 2520 3220 19
Birth-place 68 681 762 5
Country-capital 36 932 1075 161
Country-president 56 1260 1463 119
Death-year 492 2540 3219 16
Director-film 1530 3126 3668 121
Painter-picture 44 487 542 69
Player-team 110 2903 3514 195
Table 2: Number of seed pairs for each relation,
and number of unique patterns after the extraction
and the generalization step, and after calculating
their accuracy and filtering those that did not apply
3 times on the test corpus.
seen in the training corpus for that role
(hook or target). If so, output the rela-
tionship.
5 Evaluation and results
The procedure has been tested with 10 different
relationships. For each pair in each seed list, a
corpus with 500 documents has been collected us-
ing Google, from which the patterns are extracted.
Table 2 shows the number of patterns obtained. It
is interesting to see that for some relations, such as
birth-year or birth-place, more than one thousand
patterns have been reduced to a few. Table 3 shows
the patterns obtained for the relationship birth-
year. It can also be seen that some of the patterns
with good precision contain the wildcard *, which
helped extract the correct birth year in roughly 50
occasions. Specially of interest is the last pattern,
(/( HOOK TARGET )/)
which resulted in an accuracy of 0.29 with the pro-
Relation Precision Incl. prec. Applied
Actor-film 0% 76.84% 95
Writer-book 6.25% 28.13% 32
Birth-year 79.67% 79.67% 477
Birth-place 14.56% 14.56% 103
Country-capital 72.43% 72.43% 599
Country-president 81.40% 81.40% 43
Death-year 96.71% 96.71% 152
Director-film 43.40% 84.91% 53
Painter-picture - - 0
Player-team 52.50% 52.50% 120
Table 4: Precision, inclusion precision and num-
ber of times that a pattern extracted information,
when applied to a test corpus.
cedure here indicated, but which would have ob-
tained an accuracy of 0.54 using the traditional
hook corpus approach. This is because in other
test corpora (e.g. in the one containing soccer
players and clubs) it is more frequent to find the
name of a person followed by a number that is not
his/her birth year, while that did not happen so of-
ten in the birth year test corpus.
For evaluating the patterns, a new test corpus
has been collected for fourteen entities not present
in the training corpora, again using Google. The
chosen entities are Robert de Niro and Natalie
Wood (actors), Isaac Asimov and Alfred Bester
(writers), Montevideo and Yaounde (capitals),
Gloria Macapagal Arroyo and Hosni Mubarak
(country presidents), Bernardo Bertolucci and
Federico Fellini (directors), Peter Paul Rubens and
Paul Gauguin (painters), and Jens Lehmann and
Thierry Henry (soccer players). Table 4 shows the
results obtained for each relationship.
We have observed that, for those relationships
in which the target does not belong to a Named
14
Entity type, it is common for the patterns to extract
additional words together with the right target. For
example, rather than extracting The Last Emperor,
the patterns may extract this title together with
its rating or its length, the title between quotes,
or phrases such as The classic The Last Emperor.
In the second column in the table, we measured
the percentage of times that a correct answer ap-
pears inside the extracted target, so these examples
would be considered correct. We call this metric
inclusion precision.
5.1 Comparison to related approaches
Although the above results are not comparable to
Mann and Yarowsky (2005), as the corpora used
are different, in most cases the precision is equal
or higher to that reported there. On the other hand,
we have rerun Ravichandran and Hovy (2002)?s
algorithm on our corpus. In order to assure a
fair comparison, their algorithm has been slightly
modified so it also takes into account the part-of-
speech sequences and entity types while extract-
ing the hooks and the targets during the rule ap-
plication. So, for instance, the relationship birth
date is only extracted between a hook tagged as
a person and a target tagged as either a date or
a number. The results are shown in Table 5. As
can be seen, our procedure seems to perform bet-
ter for all of the relations except birth place. It
is interesting to note that, as could be expected,
for those targets for which there is no entity type
defined (films, books and pictures), Ravichandran
and Hovy (2002)?s extracts many errors, because
it is not possible to apply the Named Entity Rec-
ognizer to clean up the results, and the accuracy
remains below 10%. On the other hand, that trend
does not seem to affect our system, which had
very poor results for painter-picture, but reason-
ably good for actor-film.
Other interesting case is that of birth places.
A manual observation of our generalized patterns
shows that they often contain disjunctions of verbs
such as that in (1), that detects not just the birth
place but also places where the person lived. In
this case, Ravichandran and Hovy (2002)?s pat-
terns resulted more precise as they do not contain
disjunctions or wildcards.
(1) HOOK ,/, returned|travelled|born/VBN
to|in/IN TARGET
It is interesting that, among the three relation-
ships with the smaller number of extracted pat-
terns, one of them did not produce any result, and
Ravichandran
Relation Our approach and Hovy?s
Actor-film 76.84% 1.71%
Writer-book 28.13% 8.55%
Birth-year 79.67% 49.49%
Birth-place 14.56% 88.66%
Country-capital 72.43% 24.79%
Country-president 81.40% 16.13%
Death-year 96.71% 35.35%
Director-film 84.91% 1.01%
Painter-picture - 0.85%
Player-team 52.50% 44.44%
Table 5: Inclusion precision on the same test cor-
pus for our approach and Ravichandran and Hovy
(2002)?s.
the two others attained a low precision. Therefore,
it should be possible to improve the performance
of the system if, while training, we augment the
training corpora until the number of extracted pat-
terns exceeds a given threshold.
6 Related work
Extracting information using Machine Learning
algorithms has received much attention since the
nineties, mainly motivated by the Message Un-
derstanding Conferences (MUC6, 1995; MUC7,
1998). From the mid-nineties, there are systems
that learn extraction patterns from partially an-
notated and unannotated data (Huffman, 1995;
Riloff, 1996; Riloff and Schmelzenbach, 1998;
Soderland, 1999).
Generalizing textual patterns (both manually
and automatically) for the identification of re-
lationships has been proposed since the early
nineties (Hearst, 1992), and it has been applied
to extending ontologies with hyperonymy and
holonymy relationships (Kietz et al, 2000; Cimi-
ano et al, 2004; Berland and Charniak, 1999),
with overall precision varying between 0.39 and
0.68. Finkelstein-Landau and Morin (1999) learn
patterns for company merging relationships with
exceedingly good accuracies (between 0.72 and
0.93).
Rote extraction systems from the web have
the advantage that the training corpora can be
collected easily and automatically. Several
similar approaches have been proposed (Brin,
1998; Agichtein and Gravano, 2000; Ravichan-
dran and Hovy, 2002), with various applications:
Question-Answering (Ravichandran and Hovy,
2002), multi-document Named Entity Corefer-
ence (Mann and Yarowsky, 2003), and generating
15
biographical information (Mann and Yarowsky,
2005).
7 Conclusions and future work
We have described here a new procedure for build-
ing a rote extractor from the web. Compared to
other similar approaches, it addresses several is-
sues: (a) it is able to generate generalized patterns
containing wildcards; (b) it makes use of PoS and
Named Entity tags during the generalization pro-
cess; and (c) several relationships are learned and
evaluated at the same time, in order to test each
one on the test corpora built for the others. The re-
sults, measured in terms of precision and inclusion
precision are very good in most of the cases.
Our system needs an input table, which may
seem more complicated to compile that the list of
related pairs used by previous approaches, but we
have seen that the table can be built automatically
from the lists, with no extra work. In any case,
the time to build the table is significantly smaller
than the time needed to write the extraction pat-
terns manually.
Concerning future work, we are currently trying
to improve the estimation of the patterns accuracy
for the pruning step. We also plan to apply the ob-
tained patterns in a system for automatically gen-
erating biographical knowledge bases from vari-
ous web corpora.
References
E. Agichtein and L. Gravano. 2000. Snowball: Ex-
tracting relations from large plain-text collections.
In Proceedings of ICDL, pages 85?94.
M. Berland and E. Charniak. 1999. Finding parts in
very large corpora. In Proceedings of ACL-99.
S. Brin. 1998. Extracting patterns and relations from
the World Wide Web. In Proceedings of the WebDB
Workshop at the 6th International Conference on Ex-
tending Database Technology, EDBT?98.
P. Cimiano, S. Handschuh, and S. Staab. 2004. To-
wards the self-annotating web. In Proceedings of the
13th World Wide Web Conference, pages 462?471.
M. Craven, D. DiPasquo, D. Freitag, A. McCallum,
T. Mitchell, K. Nigam, and S. Slattery. 1999. Learn-
ing to construct knowledge bases from the world
wide web. Artificial Intelligence, 118(1?2):69?113.
M. Finkelstein-Landau and E. Morin. 1999. Extracting
semantic relationships between terms: supervised
vs. unsupervised methods. In Workshop on Ontolo-
gial Engineering on the Global Info. Infrastructure.
M. Hearst. 1992. Automatic acquisition of hyponyms
from large text corpora. In COLING-92.
S. Huffman. 1995. Learning information extraction
patterns from examples. In IJCAI-95 Workshop on
New Approaches to Learning for NLP.
J. Kietz, A. Maedche, and R. Volz. 2000. A method
for semi-automatic ontology acquisition from a cor-
porate intranet. In Workshop ?Ontologies and text?.
G. S. Mann and D. Yarowsky. 2003. Unsupervised
personal name disambiguation. In CoNLL-2003.
G. S. Mann and D. Yarowsky. 2005. Multi-field in-
formation extraction and cross-document fusion. In
ACL 2005.
M. Marcus, B. Santorini, and M.A. Marcinkiewicz.
1993. Building a large annotated corpus of En-
glish: the Penn Treebank. Computational Linguis-
tics, 19(2):313?330.
MUC6. 1995. Proceedings of the 6th Message Under-
standing Conference (MUC-6). Morgan Kaufman.
MUC7. 1998. Proceedings of the 7th Message Under-
standing Conference (MUC-7). Morgan Kaufman.
D. Ravichandran and E. Hovy. 2002. Learning surface
text patterns for a question answering system. In
Proceedings of ACL-2002, pages 41?47.
E. Riloff and M. Schmelzenbach. 1998. An empirical
approach to conceptual case frame acquisition. In
Proceedings of WVLC, pages 49?56.
E. Riloff. 1996. Automatically generating extraction
patterns from untagged text. In AAAI.
M. Ruiz-Casado, E. Alfonseca, and P. Castells. in
press. Automatising the learning of lexical pat-
terns: an application to the enrichment of wordnet
by extracting semantic relationships from wikipedia.
Data and Knowledge Engineering.
S. Soderland. 1999. Learning information extraction
rules for semi-structured and free text. Machine
Learning, 34(1?3):233?272.
R. Wagner and M. Fischer. 1974. The string-to-
string correction problem. Journal of Association
for Computing Machinery, 21.
16
Proceedings of the COLING/ACL 2006 Main Conference Poster Sessions, pages 603?610,
Sydney, July 2006. c?2006 Association for Computational Linguistics
An Automatic Method for Summary Evaluation  
Using Multiple Evaluation Results by a Manual Method 
 
Hidetsugu Nanba 
Faculty of Information Sciences, 
Hiroshima City University 
3-4-1 Ozuka, Hiroshima, 731-3194 Japan 
nanba@its.hiroshima-cu.ac.jp 
Manabu Okumura 
Precision and Intelligence Laboratory, 
Tokyo Institute of Technology 
4259 Nagatsuta, Yokohama, 226-8503 Japan
oku@pi.titech.ac.jp 
 
  
 
Abstract 
To solve a problem of how to evaluate 
computer-produced summaries, a number 
of automatic and manual methods have 
been proposed. Manual methods evaluate 
summaries correctly, because humans 
evaluate them, but are costly. On the 
other hand, automatic methods, which 
use evaluation tools or programs, are low 
cost, although these methods cannot 
evaluate summaries as accurately as 
manual methods. In this paper, we 
investigate an automatic evaluation 
method that can reduce the errors of 
traditional automatic methods by using 
several evaluation results obtained 
manually. We conducted some 
experiments using the data of the Text 
Summarization Challenge 2 (TSC-2). A 
comparison with conventional automatic 
methods shows that our method 
outperforms other methods usually used. 
1 Introduction 
Recently, the evaluation of computer-produced 
summaries has become recognized as one of the 
problem areas that must be addressed in the field 
of automatic summarization. To solve this 
problem, a number of automatic (Donaway et al, 
2000, Hirao et al, 2005, Lin et al, 2003, Lin, 
2004, Hori et al, 2003) and manual methods 
(Nenkova et al, 2004, Teufel et al, 2004) have 
been proposed. Manual methods evaluate 
summaries correctly, because humans evaluate 
them, but are costly. On the other hand, 
automatic methods, which use evaluation tools or 
programs, are low cost, although these methods 
cannot evaluate summaries as accurately as 
manual methods. In this paper, we investigate an 
automatic method that can reduce the errors of 
traditional automatic methods by using several 
evaluation results obtained manually. Unlike 
other automatic methods, our method estimates 
manual evaluation scores. Therefore, our method 
makes it possible to compare a new system with 
other systems that have been evaluated manually. 
There are two research studies related to our 
work (Kazawa et al, 2003, Yasuda et al, 2003). 
Kazawa et al (2003) proposed an automatic 
evaluation method using multiple evaluation 
results from a manual method. In the field of 
machine translation, Yasuda et al (2003) 
proposed an automatic method that gives an 
evaluation result of a translation system as a 
score for the Test of English for International 
Communication (TOEIC). Although the 
effectiveness of both methods was confirmed 
experimentally, further discussion of four points, 
which we describe in Section 3, is necessary for 
a more accurate summary evaluation. In this 
paper, we address three of these points based on 
Kazawa?s and Yasuda?s methods. We also 
investigate whether these methods can 
outperform other automatic methods. 
The remainder of this paper is organized as 
follows. Section 2 describes related work. 
Section 3 describes our method. To investigate 
the effectiveness of our method, we conducted 
some examinations and Section 4 reports on 
these. We present some conclusions in Section 5. 
2 Related Work 
Generally, similar summaries are considered to 
obtain similar evaluation results. If there is a set 
of summaries (pooled summaries) produced from 
a document (or multiple documents) and if these 
are evaluated manually, then we can estimate a 
manual evaluation score for any summary to be 
evaluated with the evaluation results for those 
pooled summaries. Based on this idea, Kazawa et 
603
al. (2003) proposed an automatic method using 
multiple evaluation results from a manual 
method. First, n summaries for each document, m, 
were prepared. A summarization system 
generated summaries from m documents. Here, 
we represent the ith summary for the jth document 
and its evaluation score as xij and yij, respectively. 
The system was evaluated using Equation 1. 
??
= =
+=
m
i
ijij
n
j
j bxxSimywxscr
1 1
),()(  (1) 
The evaluation score of summary x was 
obtained by summing parameter b for all the 
subscores calculated for each pooled summary, 
xij. A subscore was obtained by multiplying a 
parameter wj, by the evaluation score yij, and the 
similarity between x and xij. 
In the field of machine translation, there is 
another related study. Yasuda et al (2003) 
proposed an automatic method that gives an 
evaluation result of a translation system as a 
score for TOEIC. They prepared 29 human 
subjects, whose TOEIC scores were from 300s to 
800s, and asked them to translate 23 Japanese 
conversations into English. They also generated 
translations using a system for each conversation. 
Then, they evaluated both translations using an 
automatic method, and obtained WH, which 
indicated the ratio of system translations that 
were superior to human translations. Yasuda et al 
calculated WH for each subject and plotted the 
values along with their corresponding TOEIC 
scores to produce a regression line. Finally, they 
defined a point where the regression line crossed 
WH = 0.5 to provide the TOEIC score for the 
system. 
Though, the effectiveness of Kazawa?s and 
Yasuda?s methods were confirmed 
experimentally, further discussions of four points, 
which we describe in the next section, are 
necessary for a more accurate summary 
evaluation. 
3 Investigation of an Automatic Method 
using Multiple Manual Evaluation 
Results 
3.1 Overview of Our Evaluation Method 
and Essential Points to be Discussed 
We investigate an automatic method using 
multiple evaluation results by a manual method 
based on Kazawa?s and Yasuda?s method. The 
procedure of our evaluation method is shown as 
follows; 
 
(Step 1) Prepare summaries and their 
evaluation results by a manual method 
 
 
(Step 2) Calculate the similarities between a 
summary to be evaluated and the pooled 
summaries 
 
 
(Step 3) Combine manual scores of pooled 
summaries in proportion to their similarities 
to the summary to be evaluated 
 
For each step, we need to discuss the following 
points. 
(Step 1) 
1. How many summaries, and what type 
(variety) of summaries should be prepared? 
Kazawa et al prepared 6 summaries for 
each document, and Yasuda et al prepared 
29 translations for each conversation. 
However, they did not examine about the 
number and the type of pooled summaries 
required to the evaluation. 
(Step 2) 
2. Which measure is better for calculating the 
similarities between a summary to be 
evaluated and the pooled summaries? 
Kazawa et al used Equation 2 to calculate 
similarities. 
|)||,min(|
||
),(
xx
xx
xxSim
ij
ij
ij
?=  (2) 
where xxij ?  indicates the number of 
discourse units1 that appear in both xij and x, 
and | x | represents the number of words in x. 
However, there are many other measures 
that can be used to calculate the topical 
similarities between two documents (or 
passages). 
As well as Yasuda?s method does, using 
WH is another way to calculate similarities 
between a summary to be evaluated and 
pooled summaries indirectly. Yasuda et al 
(2003) tested DP matching (Su et al, 1992), 
BLEU (Papineni et al, 2002), and NIST2, 
for the calculation of WH. However there are 
many other measures for summary 
evaluation. 
                                                 
1 Rhetorical Structure Theory Discourse Treebank. 
www.ldc.upenn.edu/Catalog/CatalogEntry.jsp?catalog
Id=LDC2002T07 Linguistic Data Consortium.  
2 http://www.nist.gov/speech/tests/mt/mt2001/resource/ 
604
3. How many summaries should be used to 
calculate the score of a summary to be 
evaluated? Kazawa et al used all the pooled 
summaries but this does not ensure the best 
performance of their evaluation method. 
(Step 3) 
4. How to combine the manual scores of the 
pooled summaries? Kazawa et al calculated 
the score of a summary as a weighted linear 
sum of the manual scores. Applying 
regression analysis (Yasuda et al, 2003) is 
another method of combining several 
manual scores. 
3.2 Three Points Addressed in Our Study 
We address the second, third and fourth points in 
Section 3.1. 
 
(Point 2) A measure for calculating 
similarities between a summary to be 
evaluated and pooled summaries: 
There are many measures that can calculate the 
topical similarities between two documents (or 
passages). We tested several measures, such as 
ROUGE (Lin, 2004) and the cosine distance. We 
describe these measures in detail in Section 4.2. 
 
(Point 3) The number of summaries used to 
calculate the score of a summary to be 
evaluated: 
We use summaries whose similarities to a 
summary to be evaluated are higher than a 
threshold value.  
 
(Point 4) Combination of manual scores: 
We used both Kazawa?s and Yasuda?s methods. 
4 Experiments 
4.1 Experimental Methods 
To investigate the three points described in 
Section 3.2, we conducted the following four 
experiments. 
 
z Exp-1: We examined Points 2 and 3 based 
on Kazawa?s method. We tested threshold 
values from 0 to 1 at 0.005 intervals. We 
also tested several similarity measures, such 
as cosine distance and 11 kinds of ROUGE.  
z Exp-2: In order to investigate whether the 
evaluation based on Kazawa?s method can 
outperform other automatic methods, we 
compared the evaluation with other 
automatic methods. In this experiment, we 
used the similarity measure, which obtain 
the best performance in Exp-1. 
z Exp-3: We also examined Point 2 based on 
Yasuda?s method. As a similarity measure, 
we tested cosine distance and 11 kinds of 
ROUGE. Then, we examined Point 4 by 
comparing the result of Yasuda?s method 
with that of Kazawa?s.  
z Exp-4: In the same way as Exp-2, we 
compared the evaluation with other 
automatic methods, which we describe in 
the next section, to investigate whether the 
evaluation based on Yasuda?s method can 
outperform other automatic methods.  
4.2 Automatic Evaluation Methods Used in 
the Experiments 
In the following, we show the automatic 
evaluation methods used in our experiments.  
 
Content-based evaluation (Donaway et al, 
2000) 
This measure evaluates summaries by comparing 
their content words with those of the human-
produced extracts. The score of the content-
based measure is obtained by computing the 
similarity between the term vector using tf*idf 
weighting of a computer-produced summary and 
the term vector of a human-produced summary 
by cosine distance. 
 
ROUGE-N (Lin, 2004) 
This measure compares n-grams of two 
summaries, and counts the number of matches. 
The measure is defined by Equation 3. 
? ?
? ?
? ?
? ?=?
RS Sgram
N
RS Sgram
Nmatch
N
N
gramCount
gramCount
NROUGE
)(
)(
 (3) 
where Count(gramN) is the number of an N-gram 
and Countmatch(gramN) denotes the number of n-
gram co-occurrences in two summaries. 
 
ROUGE-L (Lin, 2004) 
This measure evaluates summaries by longest 
common subsequence (LCS) defined by 
Equation 4. 
m
CrLCS
LROUGE
u
ii
i?
=
?
=?
),(
 (4) 
where LCSU(ri,C) is the LCS score of the union?s 
longest common subsequence between reference 
sentences ri and the summary to be evaluated, 
and m is the number of words contained in a 
reference summary. 
605
ROUGE-S (Lin, 2004) 
Skip-bigram is any pair of words in their 
sentence order, allowing for arbitrary gaps. 
ROUGE-S measures the overlap of skip-bigrams 
in a candidate summary and a reference 
summary. Several variations of ROUGE-S are 
possible by limiting the maximum skip distance 
between the two in-order words that are allowed 
to form a skip-bigram. In the following, 
ROUGE-SN denotes ROUGE-S with maximum 
skip distance N. 
 
ROUGE-SU (Lin, 2004) 
This measure is an extension of ROUGE-S; it 
adds a unigram as a counting unit. In the 
following, ROUGE-SUN denotes ROUGE-SU 
with maximum skip distance N. 
 
4.3 Evaluation Methods 
In the following, we elaborate on the evaluation 
methods for each experiment. 
 
Exp-1: An experiment for Points 2 and 3 
based on Kazawa?s method 
We evaluated Kazawa?s method from the 
viewpoint of ?Gap?. Differing from other 
automatic methods, the method uses multiple 
manual evaluation results and estimates the 
manual scores of the summaries to be evaluated 
or the summarization systems. We therefore 
evaluated the automatic methods using Gap, 
which manually indicates the difference between 
the scores from a manual method and each 
automatic method that estimates the scores. First, 
an arbitrary summary is selected from the 10 
summaries in a dataset, which we describe in 
Section 4.4, and an evaluation score is calculated 
by Kazawa?s method using the other nine 
summaries. The score is compared with a manual 
score of the summary by Gap, which is defined 
by Equation 5. 
nm
yxscr
Gap
m
k
n
l
klkl
?
?
=
??
= =1 1
|)('|
 (5) 
where xkl is the kth system?s lth summary, and ykl 
is the score from a manual evaluation method for 
the kth system?s lth summary. To distinguish our 
evaluation function from Kazawa?s, we denote it 
as scr?(x). As a similarity measure in scr?(x), we 
tested ROUGE and the cosine distance. 
We also tested the coverage of the automatic 
method. The method cannot calculate scores if 
there are no similar summaries above a given 
threshold value. Therefore, we checked the 
coverage of the method, which is defined by 
Equation 6. 
summariesgivenofnumberThe
methodthebyevaluated
summariesofnumberThe
Coverage =  (6) 
Exp-2: Comparison of Kazawa?s method with 
other automatic methods 
Traditionally, automatic methods have been 
evaluated by ?Ranking?. This means that 
summarization systems are ranked based on the 
results of the automatic and manual methods. 
Then, the effectiveness of the automatic method 
is evaluated by the number of matches between 
both rankings using Spearman?s rank correlation 
coefficient and Pearson?s rank correlation 
coefficient (Lin et al, 2003, Lin, 2004, Hirao et 
al., 2005). However, we did not use both 
correlation coefficients, because evaluation 
scores are not always calculated by a Kazawa-
based method, which we described in Exp-1. 
Therefore, we ranked the summaries instead of 
the summarization systems. Two arbitrary 
summaries from the 10 summaries in a dataset 
were selected and ranked by Kazawa?s method. 
Then, Kazawa?s method was evaluated using 
?Precision,? which calculates the percentage of 
cases where the order of the manual method of 
the two summaries matches the order of their 
ranks calculated by Kazawa?s method. The two 
summaries were also ranked by ROUGE and by 
cosine distance, and both Precision values were 
calculated. Finally, the Precision value of 
Kazawa?s method was compared with those of 
ROUGE and cosine distance. 
Exp-3: An experiment for Point 2 based on 
Yasuda?s method 
An arbitrary system was selected from the 10 
systems, and Yasuda?s method estimated its 
manual score from the other nine systems. 
Yasuda?s method was evaluated by Gap, which 
is defined by Equation 7. 
m
yxs
Gap
m
k
kk?
=
?
= 1
|)(|
 (7) 
where xk is the kth system, s(xk) is a score of xk by 
Yasuda?s method, and yk is the manual score for 
the kth system. Yasuda et al (2003) tested DP 
matching (Su et al, 1992), BLEU (Papineni et al, 
2002), and NIST3, as automatic methods used in 
their evaluation. Instead of those methods, we 
                                                 
3 http://www.nist.gov/speech/tests/mt/mt2001/resource/ 
606
tested ROUGE and cosine distance, both of 
which have been used for summary evaluation. 
If a score by Yasuda?s method exceeds the 
range of the manual score, the score is modified 
to be within the range. In our experiments, we 
used evaluation by revision (Fukushima et al, 
2002) as the manual evaluation method. The 
range of the score of this method is between zero 
and 0.5. If the score is less than zero, it is 
changed to zero and if greater than 0.5 it is 
changed to 0.5. 
Exp-4: Comparison of Yasuda?s method and 
other automatic methods 
In the same way as for the evaluation of 
Kazawa?s method in Exp-2, we evaluated 
Yasuda?s method by Precision. Two arbitrary 
summaries from the 10 summaries in a dataset 
were selected, and ranked by Yasuda?s method. 
Then, Yasuda?s method was evaluated using 
Precision. Two summaries were also ranked by 
ROUGE and by cosine distance and both 
Precision values were calculated. Finally, the 
Precision value of Yasuda?s method was 
compared with those of ROUGE and cosine 
distance. 
4.4 The Data Used in Our Experiments 
We used the TSC-2 data (Fukushima et al, 
2002) in our examinations. The data consisted of 
human-produced extracts (denoted as ?PART?), 
human-produced abstracts (denoted as ?FREE?), 
computer-produced summaries (eight systems 
and a baseline system using the lead method 
(denoted as ?LEAD?)) 4 , and their evaluation 
results by two manual methods. All the 
summaries were derived from 30 newspaper 
articles, written in Japanese, and were extracted 
from the Mainichi newspaper database for the 
years 1998 and 1999. Two tasks were conducted 
in TSC-2, and we used the data from a single 
document summarization task. In this task, 
participants were asked to produce summaries in 
plain text in the ratios of 20% and 40%.  
Summaries were evaluated using a ranking 
evaluation method and the revision method 
evaluation. In our experiments, we used the 
results of evaluation from the revision method. 
This method evaluates summaries by measuring 
the degree to which computer-produced 
summaries are revised. The judges read the 
                                                 
4 In Exp-2 and 4, we evaluated ?PART?, ?LEAD?, 
and eight systems (candidate summaries) by 
automatic methods using ?FREE? as the reference 
summaries. 
original texts and revised the computer-produced 
summaries in terms of their content and 
readability. The human revisions were made with 
only three editing operations (insertion, deletion, 
replacement). The degree of the human revision, 
called the ?edit distance,? is computed from the 
number of revised characters divided by the 
number of characters in the original summary. If 
the summary?s quality was so low that a revision 
of more than half of the original summary was 
required, the judges stopped the revision and a 
score of 0.5 was given. 
The effectiveness of evaluation by the revision 
method was confirmed in our previous work 
(Nanba et al, 2004). We compared evaluation by 
revision with ranking evaluation. We also tested 
other automatic methods: content-based 
evaluation, BLEU (Papineni et al, 2001) and 
ROUGE-1 (Lin, 2004), and compared their 
results with that of evaluation by revision as 
reference. As a result, we found that evaluation 
by revision is effective for recognizing slight 
differences between computer-produced 
summaries. 
4.5 Experimental Results and Discussion 
Exp-1: An experiment for Points 2 and 3 
based on Kazawa?s method 
To address Points 2 and 3, we evaluated 
summaries by the method based on Kazawa?s 
method using 12 measures, described in Section 
4.4, as measures to calculate topical similarities 
between summaries, and compared these 
measures by Gap. The experimental results for 
summarization ratios of 40% and 20% are 
shown in Tables 1 and 2, respectively. Tables 
show the Gap values of 12 measures for each 
Coverage value from 0.2 to 1.0 at 0.1 intervals. 
Average values of Gap for each measure are also 
shown in these tables. As can be seen from 
Tables 1 and 2, the larger the threshold value, 
the smaller the value of Gap. From the result, we 
can conclude for Point 3 that more accurate 
evaluation is possible when we use similar 
pooled summaries (Point 2). However, the 
number of summaries that can be evaluated by 
this method was limited when the threshold 
value was large.  
Of the 12 measures, unigram-based methods, 
such as cosine distance and ROUGE-1, produced 
good results. However, there were no significant 
differences between measures except for when 
ROUGE-L was used. 
607
 Table 1 Comparison of Gap values for several measures 
(ratio: 40%) 
 
Coverage 
Measure 
1.0 0.9 0.8 0.7 0.6 0.5 0.4 0.3 0.2 Average
R-1 0.080 0.070 0.067 0.057 0.064 0.062 0.058 0.045 0.041 0.062
R-2 0.082 0.074 0.070 0.070 0.069 0.063 0.059 0.051 0.042 0.065
R-3 0.083 0.074 0.075 0.071 0.069 0.063 0.059 0.051 0.045 0.066
R-4 0.085 0.078 0.076 0.073 0.069 0.064 0.060 0.051 0.043 0.067
R-L 0.102 0.100 0.097 0.094 0.091 0.090 0.089 0.082 0.078 0.091
R-S 0.083 0.077 0.073 0.073 0.069 0.067 0.064 0.060 0.045 0.068
R-S4 0.083 0.072 0.071 0.069 0.066 0.066 0.060 0.054 0.044 0.065
R-S9 0.083 0.075 0.069 0.070 0.067 0.066 0.066 0.057 0.046 0.067
R-SU 0.083 0.077 0.070 0.071 0.069 0.068 0.064 0.057 0.043 0.067
R-SU4 0.082 0.073 0.069 0.069 0.065 0.068 0.063 0.051 0.043 0.065
R-SU9 0.083 0.074 0.070 0.068 0.066 0.067 0.066 0.054 0.046 0.066
Cosine 0.081 0.074 0.065 0.062 0.059 0.056 0.057 0.039 0.043 0.059
Threshold Small                                                                                Large 
 Table 2 Comparison of Gap values for several measures 
(ratio: 20%) 
 
Coverage 
Measure 
1.0 0.9 0.8 0.7 0.6 0.5 0.4 0.3 0.2 Average
R-1 0.129 0.104 0.102 0.976 0.090 0.089 0.089 0.083 0.082 0.096
R-2 0.132 0.115 0.107 0.109 0.096 0.093 0.079 0.081 0.082 0.099
R-3 0.132 0.115 0.116 0.111 0.102 0.092 0.080 0.078 0.079 0.101
R-4 0.134 0.121 0.121 0.112 0.103 0.090 0.080 0.080 0.078 0.102
R-L 0.140 0.135 0.134 0.125 0.117 0.110 0.105 0.769 0.060 0.111
R-S 0.130 0.119 0.113 0.106 0.098 0.099 0.089 0.089 0.087 0.103
R-S4 0.130 0.114 0.109 0.105 0.102 0.092 0.085 0.088 0.085 0.101
R-S9 0.130 0.119 0.113 0.105 0.095 0.097 0.095 0.085 0.084 0.103
R-SU 0.130 0.118 0.109 0.109 0.097 0.098 0.088 0.089 0.079 0.102
R-SU4 0.130 0.111 0.107 0.106 0.100 0.090 0.086 0.084 0.087 0.100
R-SU9 0.130 0.116 0.108 0.105 0.096 0.090 0.085 0.085 0.082 0.099
Cosine 0.128 0.106 0.102 0.094 0.091 0.090 0.079 0.080 0.057 0.092
Threshold Small                                                                                Large 
Exp-2: Comparison of Kazawa?s method with 
other automatic methods (Point 2) 
In Exp-1, cosine distance outperformed the other 
11 measures. We therefore used cosine distance 
in Kazawa?s method in Exp-2. We ranked 
summaries by Kazawa?s method, ROUGE and 
cosine distance, calculated using Precision.  
The results of the evaluation by Precision for 
summarization ratios of 40% and 20% are shown 
in Figures 1 and 2, respectively. We plotted the 
Precision value of Kazawa?s method by changing 
the threshold value from 0 to 1 at 0.05 intervals. 
We also plotted the Precision values of ROUGE-
2 as dotted lines. ROUGE-2 was superior to the 
other 11 measures in terms of Ranking. The X 
and Y axes in Figures 1 and 2 show the threshold 
value of Kazawa?s method and the Precision 
values, respectively. From the result shown in 
Figure 1, we found that Kazawa?s method 
outperformed ROUGE-2, when the threshold 
value was greater than 0.968. The Coverage 
value of this point was 0.203. In Figure 2, the 
Precision curve of Kazawa?s method crossed the 
dotted line at a threshold value of 0.890. The 
Coverage value of this point was 0.405. 
To improve these Coverage values, we need to 
prepare more summaries and their manual 
evaluation results, because the Coverage is 
critically dependent on the number and variety of 
pooled summaries. This is exactly the first point 
in Section 3.1, which we do not address in this 
paper. We will investigate this point as the next 
step in our future work. 
608
00.2
0.4
0.6
0.8
1
0 0.2 0.4 0.6 0.8 1
threshold value
pr
ec
is
io
n
Kazawa's method R-2
Figure 1 Comparison of Kazawa?s method and 
ROUGE-2 (ratio: 40%) 
 
0
0.2
0.4
0.6
0.8
1
0 0.2 0.4 0.6 0.8 1
threshold value
pr
ec
is
io
n
Kazawa's method R-2
Figure 2 Comparison of Kazawa?s method and 
ROUGE-2 (ratio: 20%) 
 
Exp-3: An experiment for Point 3 based on 
Yasuda?s method 
For Point 2 in Section 3.2, we also examined 
Yasuda?s method. The experimental result by 
Gap is shown in Table 3. When the ratio is 20%, 
ROUGE-SU4 is the best. The N-gram and the 
skip-bigram are both useful when the 
summarization ratio is low. 
For Point 4, we compared the result by 
Yasuda?s method (Table 3) with that of 
Kazawa?s method (in Tables 1 and 2). Yasuda?s 
method could accurately estimate manual scores. 
In particular, the Gap values of 0.023 by 
ROUGE-2 and by ROUGE-3 are smaller than 
those produced by Kazawa?s method with a 
threshold value of 0.9 (Tables 1 and 2). This 
indicates that regression analysis used in 
Yasuda?s method is superior to that used in 
Kazawa?s method. 
 
Table 3 Gap between the manual method and 
Yasuda?s method 
Ratio  
20% 40% 
Average 
Cosine 0.037 0.031 0.035
R-1 0.033 0.022 0.028
R-2 0.028 0.023 0.025
R-3 0.028 0.023 0.025
R-4 0.036 0.024 0.030
R-L 0.040 0.038 0.039
R-S(?) 0.051 0.060 0.055
R-S4 0.025 0.040 0.033
R-S9 0.042 0.052 0.047
R-SU(?) 0.027 0.055 0.041
R-SU4 0.022 0.037 0.029
R-SU9 0.023 0.048 0.036
 
Exp-4: Comparison of Yasuda?s method with 
other automatic methods 
We also evaluated Yasuda?s method by 
comparison with other automatic methods in 
terms of Ranking. We evaluated 10 systems by 
Yasuda?s method with ROUGE-3, which 
produced the best results in Exp-3. We also 
evaluated the systems by ROUGE and cosine 
distance, and compared the results. The results 
are shown in Table 4. 
 
Table 4 Comparison between Yasuda?s method and 
automatic methods 
Ratio  
20% 40% 
Average 
Yasuda 0.867 0.844 0.856
Cosine 0.844 0.800 0.822
R-1 0.822 0.778 0.800
R-2 0.844 0.800 0.822
R-3 0.822 0.800 0.811
R-4 0.822 0.844 0.833
R-L 0.822 0.800 0.811
R-S(?) 0.667 0.689 0.678
R-S4 0.800 0.756 0.778
R-S9 0.733 0.689 0.711
R-SU(?) 0.711 0.711 0.711
R-SU4 0.800 0.822 0.811
R-SU9 0.756 0.711 0.733
 
As can be seen from Table 4, Yasuda?s method 
produced the best results for the ratios of 20% 
and 40%. Of the automatic methods compared, 
ROUGE-4 was the best. 
609
As evaluation scores by Yasuda?s method 
were calculated based on ROUGE-3, there were 
no striking differences between Yasuda?s method 
and the others except for the integration process 
of evaluation scores for each summary. Yasuda?s 
method uses a regression analysis, whereas the 
other methods average the scores for each 
summary. Yasuda?s method using ROUGE-3 
outperformed the original ROUGE-3 for both 
ratios, 20% and 40%. 
5 Conclusions 
We have investigated an automatic method that 
uses several evaluation results from a manual 
method based on Kazawa?s and Yasuda?s 
methods. From the experimental results based on 
Kazawa?s method, we found that limiting the 
number of pooled summaries could produce 
better results than using all the pooled summaries. 
However, the number of summaries that can be 
evaluated by this method was limited. To 
improve the Coverage of Kazawa?s method, 
more summaries and their evaluation results are 
required, because the Coverage is critically 
dependent on the number and variety of pooled 
summaries. 
We also investigated an automatic method 
based on Yasuda?s method and found that the 
method using ROUGE-2 and -3 could accurately 
estimate manual scores, and could outperform 
Kazawa?s method and the other automatic 
methods tested. From these results, we can 
conclude that the automatic method performed 
the best when ROUGE-2 or 3 is used as a 
similarity measure, and a regression analysis is 
used for combining manual method. 
References 
Robert L. Donaway, Kevin W. Drummey and Laura 
A. Mather. 2000. A Comparison of Rankings 
Produced by Summarization Evaluation Measures. 
Proceedings of the ANLP/NAACL 2000 
Workshop on Automatic Summarization: 69?78. 
Takahiro Fukushima and Manabu Okumura. 2001. 
Text Summarization Challenge/Text 
Summarization Evaluation at NTCIR Workshop2. 
Proceedings of the Second NTCIR Workshop on 
Research in Chinese and Japanese Text Retrieval 
and Text Summarization: 45?51. 
Takahiro Fukushima, Manabu Okumura and 
Hidetsugu Nanba. 2002. Text Summarization 
Challenge 2/Text Summarization Evaluation at 
NTCIR Workshop3. Working Notes of the 3rd 
NTCIR Workshop Meeting, PART V: 1?7. 
Tsutomu Hirao, Manabu Okumura, and Hideki 
Isozaki. 2005. Kernel-based Approach for 
Automatic Evaluation of Natural Language 
Generation Technologies: Application to 
Automatic Summarization. Proceedings of HLT-
EMNLP 2005: 145?152. 
Chiori Hori, Takaaki Hori, and Sadaoki Furui. 2003. 
Evaluation Methods for Automatic Speech 
Summarization. Proceedings of Eurospeech 2003: 
2825?2828. 
Hideto Kazawa, Thomas Arrigan, Tsutomu Hirao and 
Eisaku Maeda. 2003. An Automatic Evaluation 
Method of Machine-Generated Extracts. IPSJ SIG 
Technical Reports, 2003-NL-158: 25?30. (in 
Japanese). 
Chin-Yew Lin and Eduard Hovy. 2003. Automatic 
Evaluation of Summaries Using N-gram Co-
Occurrence Statistics. Proceedings of the Human 
Language Technology Conference 2003: 71?78. 
Chin-Yew Lin. 2004. ROUGE: A Package for 
Automatic Evaluation of Summaries. Proceedings 
of the ACL-04 Workshop ?Text Summarization 
Branches Out?: 74?81. 
Hidetsugu Nanba and Manabu Okumura. 2004. 
Comparison of Some Automatic and Manual 
Methods for Summary Evaluation Based on the 
Text Summarization Challenge 2. Proceedings of 
the Fourth International Conference on Language 
Resources and Evaluation: 1029?1032. 
Ani Nenkova and Rebecca Passonneau, 2004. 
Evaluating Content Selection in Summarization: 
The Pyramid Method. Proceedings of HLT-NAACL 
2004: 145?152. 
Kishore Papineni, Salim Roukos, Todd Ward and 
Wei-Jing Zhu. 2001. BLEU: A Method for 
Automatic Evaluation of Machine Translation. 
IBM Research Report, RC22176 (W0109-022). 
Keh-Yih Su, Ming-Wen Wu, and Jing-Shin Chang. 
1992. A New Quantitative Quality Measure for 
Machine Translation Systems. Proceedings of the 
14th International Conference on Computational 
Linguistics: 433?439. 
Simone Teufel and Hans van Halteren. 2004. 
Evaluating Information Content by Factoid 
Analysis: Human Annotation and Stability. 
Proceedings of EMNLP 2004: 419?426. 
Kenji Yasuda, Fumiaki Sugaya, Toshiyuki Takezawa, 
Seiichi Yamamoto and Masuzo Yanagida. 2003. 
Applications of Automatic Evaluation Methods to 
Measuring a Capability of Speech Translation 
System. Proceedings of the Tenth Conference of 
the European Chapter of the Association for 
Computational Linguistics: 371?378. 
610
Text Summarization Challenge 2 
Text summarization evaluation at NTCIR Workshop 3 
 
Manabu Okumura 
Tokyo Institute of Technology 
oku@pi.titech.ac.jp 
Takahiro Fukusima  
Otemon Gakuin University  
fukusima@res.otemon.ac.jp 
Hidetsugu Nanba  
Hiroshima City University 
nanba@its.hiroshima-cu.ac.jp 
Abstract 
  We describe the outline of Text Summarization 
Challenge 2 (TSC2 hereafter), a sequel text 
summarization evaluation conducted as one of the tasks 
at the NTCIR Workshop 3.  First, we describe briefly the 
previous evaluation, Text Summarization Challenge 
(TSC1) as introduction to TSC2.   Then we explain 
TSC2 including the participants, the two tasks in TSC2, 
data used, evaluation methods for each task, and brief 
report on the results. 
 
Keywords: automatic text summarization, 
summarization evaluation 
1 
2 
Introduction 
As research on automatic text summarization is being 
a hot topic in NLP, we also see the needs to discuss and 
clarify the issues on how to evaluate text summarization 
systems. SUMMAC in May 1998 as a part of TIPSTER 
(Phase III) project ([1], [2]) and Document 
Understanding Conference (DUC) ([3]) in the United 
States show the need and importance of the evaluation 
for text summarization. 
In Japan, Text Summarization Challenge (TSC1), a 
text summarization evaluation, the first of its kind, was 
conducted in the years of 1999 to 2000 as a part of the 
NTCIR Workshop 2.  It was realized in order for the 
researchers in the field to collect and share text data for 
summarization, and to make clearer the issues of 
evaluation measures for summarization of Japanese 
texts ([4],[5],[6]). TSC1 used newspaper articles and 
had two tasks for a set of single articles with intrinsic 
and extrinsic evaluations.  The first task (task A) was to 
produce summaries (extracts and free summaries) for 
intrinsic evaluations.  We used recall, precision and F-
measure for the evaluation of the extracts, and content-
based as well as subjective methods for the evaluation 
of the free summaries. 
The summarization rates for task A were as follows: 
10, 30, 50% for extracts and 20, 40% for free 
summaries. 
The second task (task B) was to produce summaries 
for information retrieval (relevance judgment) task. The 
measures for evaluation were recall, precision and F-
measure to indicate the accuracy of the task, as well as 
the time to indicate how long it takes to carry out the 
task. 
We also prepared human-produced summaries 
including key data for the evaluation.  In terms of genre, 
we used editorials and business news articles at TSC1?s 
dryrun, and editorials and articles on social issues at the 
formal run evaluation.   
As sharable data, we had summaries for 180 
newspaper articles by spring 2001.  For each article, we 
had the following seven types of summaries: important 
sentences (10, 30, 50%), important parts specified (20, 
40%), and free summaries (20, 40%). 
In comparison, TSC2 uses newspaper articles and 
has two tasks (single- and multi-document 
summarization) for two types of intrinsic evaluations. In 
the following sections, we describe TSC2 in detail.  
Two Tasks in TSC2 and its Schedule 
TSC2 has two tasks.  They are single document 
summarization (task A) and multi-document 
summarization  (task B). 
Task A: We ask the participants to produce 
summaries in plain text to be compared with human-
prepared summaries from single documents.  
Summarization rate is a rate between the number of 
characters in the summary and the total number of 
characters in the original article.  The rates are about 
20% and 40%.  This task is the same as task A-2 in 
TSC1. 
Task B: In this task, more than two (multiple) 
documents are summarized for the task. Given a set of 
documents, which has been gathered for a pre-defined 
topic, the participants produce summaries of the set in 
plain text format. The information that was used to 
produce the document set, such as queries, as well as 
summarization lengths are given to the participants. 
Two summarization lengths are specified, short and 
long summaries for one set of documents. 
The schedule of evaluations at TSC2 was as follows: 
dryrun was conducted in December 2001 and formal run 
was in May 2002.  The final evaluation results were 
reported to the participants by early July 2002. 
3 Data Used for TSC2 
We use newspaper articles from the Mainichi 
newspaper database of 1998, 1999. As key data (human 
prepared summaries), we prepare the following types of 
summaries. 
 
Extract-type summaries:  
We asked captioners who are well experienced in 
summarization to select important sentences from 
each article.  The summarization rates are 10%, 30%, 
and 50%. 
Abstract-type summaries:  
We asked the captioners to summarize the original 
articles in two ways.  The first is to choose important 
parts of the sentences recognized important in 
extract-type summaries (abstract-type type1).  The 
second is to summarize the original articles ?freely? 
without worrying about sentence boundaries, trying 
to obtain the main idea of the articles (abstract-type 
type2).  Both types of abstract-type summaries are 
used for task A.  The summarization rates are 20% 
and 40%. 
 
Both extract-type and abstract-type summaries are 
summaries from single articles. 
 
Summaries from more than two articles: 
Given a set of newspaper articles that has been 
selected based on a certain topic, the captioners 
produced free summaries (short and long summaries) 
for the set.  Topics are various, from kidnapping case 
to Y2K problem. 
4 Evaluation Methods for each task 
We use summaries prepared by human as key data 
for evaluation. The same two intrinsic evaluation 
methods are used for both tasks.  They are evaluation by 
ranking summaries and by measuring the degree of 
revisions.  Here are the details of the two methods. We 
use 30 articles for task A and 30 sets of documents (30 
topics) for task B at formal run evaluation.  
Unfortunately,  due to the limitation of the budget,  only  
an evaluator evaluates a system?s result for an article(or 
a set). 
4.1. Evaluation by ranking 
This is basically the same as the evaluation method 
used for TSC1 task A-2 (subjective evaluation). We ask 
human judges, who are experienced in producing 
summaries, to evaluate and rank the system summaries 
in terms of two points of views. 
1. Content: How much the system summary covers 
the important content of the original article.  
2. Readability: How readable the system summary is. 
The judges are given 4 types of summaries to be 
evaluated and rank them in 1 to 4 scale (1 is the best, 2 
for the second, 3 for the third best, and 4 for the worst). 
For task A, the first two types are human-produced 
abstract-type type1 and type2 summaries.  The third is 
system results, and the fourth is summaries produced by 
lead method. 
For task B, the first is human-produced free 
summaries of the given set of documents, and the 
second is system results.  The third is the results of the 
baseline system based on lead method where the first 
sentence of each document is used.  The fourth is the 
results of the benchmark system using Stein method 
([7]) whose procedure is as follows: 
1. Produce a summary for each document. 
2. Group the summaries into several clusters. The 
number of clusters is adjusted to be less than the 
half of the number of the documents. 
3. Choose the most representative summary as the 
summary of the cluster. 
4. Compute the similarity among the clusters and 
output the representative summaries in such order 
that the similarity of neighboring summaries is 
high. 
4.2. Evaluation by revision 
 It is a newly introduced evaluation method in TSC2 
to evaluate the summaries by measuring the degree of 
revision to system results.  The judges read the original 
documents and revise the system summaries in terms of 
the content and readability.  The revisions are made by 
one of three editing operations (insertion, deletion, 
replacement). The degree of the revision is computed 
based on the number of the operations and the number 
of revised characters. The revisers could be completely 
free in what they did, though they were instructed to do 
minimum revision. 
  As baseline for task A, lead-method results are used. 
As reference for task A, human produced summaries 
(abstract type1 and abstract type 2) are used. And as 
baseline, reference, and benchmark for task B, lead-
method results, human produced summaries that are 
different from the key data, and the results based on the 
Stein method are used respectively. 
  When more than half of the document needs to be 
revised, the judges can ?give up? revising the document. 
5 Participants 
We had 4 participating systems for Task A, and 5 
systems for Task B at dryrun.  We have 8 participating 
systems for Task A and 9 systems for Task B at formal 
run.  As group, we had 8 participating groups, which are 
all Japanese, of universities, governmental research 
institute or companies in Japan.  Table 1 shows the 
breakdown of the groups. 
 
University 6 
Governmental 
research institute  1 
Company 2 
Table 1  Breakdown of Participants 
(Please note that one group consists of a company and a 
university.) 
6 Results 
6.1. Results of Evaluation by ranking 
Table 2 shows the result of evaluation by ranking 
for task A and Table 3 shows the result of evaluation by 
ranking for task B.  Each score is the average of the 
scores for 30 articles for task A, and 30 topics for task B 
at formal run. 
 
System 
No 
Content 
20% 
Read- 
ability 
20% 
Content 
40% 
Read- 
ability
40% 
F0101 2.53 2.87 2.60 2.77 
F0102 2.67 2.97 2.50 2.77 
F0103 2.80 2.93 2.90 2.90 
F0104 2.77 2.73 2.80 2.90 
F0105 2.70 2.73 2.60 2.77 
F0106 2.73 2.57 2.63 2.67 
F0107 2.70 2.60 2.50 2.53 
F0108 2.40 2.83 2.60 2.77 
TF 3.30 3.30 3.20 3.10 
Human 2.33 2.20 2.10 2.03 
Table 2 Ranking evaluation (task A) 
 
In Tables 2 and 3, F01* and F02* are labels for the 
different systems involved, respectively.  In Table 2, 
?TF? indicates a baseline system based on term-
frequency method, and ?Human? indicates human-
produced summaries that are different from the key data 
used in ranking judgement. 
   In Table 3, ?Human? indicates human-produced 
summaries that are different from the key data used in 
ranking judgement. 
 
System No ContentShort 
Read- 
ability 
Short 
Content 
Long 
Read- 
ability 
Long 
F0201 2.70 3.17 2.50 3.23 
F0202 2.73 2.70 2.77 2.93 
F0203 2.60 2.33 2.97 3.03 
F0204 2.63 2.90 2.80 3.03 
F0205 2.53 3.10 2.73 3.30 
F0206 3.20 3.00 3.47 3.30 
F0207 2.40 2.87 2.63 3.27 
F0208 2.93 2.70 2.53 2.80 
F0209 2.83 2.73 2.53 2.87 
Human 2.00 2.17 1.83 2.33 
Table 3 Ranking evaluation (task B) 
 
   In Appendix A, we also show tables giving the 
fraction of time that each system beats the baseline, one 
human summary, or two human summaries for task A.  
In Appendix B,  we show tables giving the fraction of 
time that each system beats the baseline, the benchmark, 
or  human summary for task B. 
 
 
Content
20% 
Read- 
ability 
20% 
Content 
40% 
Read- 
ability 
40% 
Human 
(type 1)
1.58 1.61 1.67 1.69 
Human
(type 2)
1.50 1.57 1.42 1.55 
Baseline
(Lead) 
3.80 3.60 3.83 3.55 
Table 4 Ranking evaluation (task A, human and 
baseline) 
 
 
Content
Short 
Read- 
ability 
Short 
Content 
Long 
Read- 
ability 
Long 
Human 
(type 2)
1.65 2.38 1.82 2.38 
Baseline
(Lead) 
2.80 2.20 2.70 2.22 
Benchmark
(Stein) 
2.48 2.00 2.50 1.99 
Table 5 Ranking evaluation (task B, human, 
baseline, and benchmark) 
 
  In comparison with the system results (Table 2 and 
Table 3), the scores for the human summaries, the 
baseline systems, and the benchmark system(the 
summaries to be compared)  are shown in Table 4 and 
Table 5.  
6.2. Results of Evaluation by revision 
   Table 6 shows the result of evaluation by revision for 
task A at rate 40%, and Table 7 shows the result of 
evaluation by revision for task A at rate 20%.  Table 8 
shows the result of evaluation by revision for task B 
long, and Table 9 shows the result of evaluation by 
revision for task B short. All the tables show the 
evaluation results in terms of average number of 
revisions (editing operations) per document. 
 
Deletion Insertion Replacement 
System 
UIM RD IM RD C RD 
F0101 2.0  0.1  1.5  0.4  0.5  0.7 
F0102 1.6  0.4  1.5  0.4  0.4  0.8 
F0103 2.3  0.2  2.4  0.2  0.4  0.5 
F0104 2.4  0.4  2.7  0.5  0.4  0.5 
F0105 2.0  0.3  1.7  0.1  0.7  0.7 
F0106 2.8  0.2  2.3  0.4  0.3  0.6 
F0107 2.5  0.6  1.8  0.2  0.1  0.5 
F0108 2.0  0.4  2.4  0.1  0.4  0.6 
ld 2.9  0.1  0.7  0.1  0.4  0.1 
free 0.4  0.4  1.2  0.4  0.1  0.3 
part 0.7  0.6  0.9  0.3  0.1  0.4 
edit 0.3 0.1 0.4 0.3 0.1 0.2 
ALL 1.9  0.3  1.8  0.3  0.3  0.5 
Table 6 Evaluation by revision (task A 40%) 
 
   Please note that UIM stands for unimportant, RD for 
readability, IM for important, C for content in Tables 6 
to 9.  They mean the reason for the operations, e.g. 
?unimportant? is for deletion operation due to the part 
judged to be unimportant, and ?content? is for 
replacement operation due to excess and deficiency of 
content. 
  In Table 6 and Table 7, ?ld? means a baseline system 
using lead method, ?free? is free summaries produced by 
human (abstract type 2), and ?part? is human-produced 
(abstract type1) summaries, and these three are baseline 
and reference scores for task A. 
 
 
 
 
 
 
Deletion Insertion Replacement 
System
UIM RD IM RD C RD 
F0101 1.4 0.4 1.3 0.2  0.5  0.3 
F0102 1.2 0.4 1.0  0.0  0.4  0.5 
F0103 0.8 0.1 1.2  0.0  0.2  0.1 
F0104 0.8 0.1 1.2  0.1  0.1  0.2 
F0105 1.2 0.1 0.7  0.0  0.4  0.2 
F0106 2.1 0.2 1.7  0.1  0.1  0.2 
F0107 0.8 0.6 0.9  0.1  0.2  0.1 
F0108 1.4 0.1 1.1  0.1  0.2  0.6 
ld 1.9 0.1 1.3  0.0  0.0  0.0 
free 0.6 0.4 1.1  0.1  0.2  0.1 
part 0.7 0.3 1.1  0.1  0.1  0.2 
edit 0.2 0.1 0.5 0.1 0.2 0.2 
ALL 1.1 0.3 1.1  0.1  0.2  0.3 
Table 7 Evaluation by revision (task A 20%) 
 
Deletion Insertion Replacement 
System
UIM RD IM RD C RD 
F0201 3.8 0.7 7.2 1.4 1.1 0.9 
F0202 5.2 0.6 3.5 0.4 0.7 0.5 
F0203 5.1 0.6 3.8 0.5 0.9 0.6 
F0204 4.2 0.6 3.4 0.7 1.4 0.7 
F0205 8.1 0.6 5.4 1.7 3.0 1.3 
F0206 3.2 0.2 4.7 0.7 0.8 0.6 
F0207 7.0 1.1 4.1 1.1 1.1 1.1 
F0208 4.8 0.7 4.0 0.4 0.8 0.9 
F0209 4.6 0.5 3.9 0.5 0.5 0.5 
human 3.0 0.9 3.4 7.8 1.0 1.2 
ld 5.7 0.9 2.9 0.4 0.7 0.5 
stein 4.0 0.5 2.2 0.3 0.8 0.5 
edit 3.0 1.2 2.9 0.7 0.7 1.1 
ALL 4.9 0.7 4.0 1.3 1.1 0.8 
Table 8 Evaluation by revision (task B long) 
 
In Table 8 and Table 9, ?human? means human-
produced summaries which are different from the key 
data, and ?ld? means a baseline system using lead 
method, ?stein? means a benchmark system using Stein 
method, and these three are baseline,  reference,  and 
benchmark scores for task B. 
To determine the plausibility of the judges?  revision,  
the revised summaries were again evaluated with the 
evaluation methods in section 5.  In Tables 6 to 9, `edit? 
means the evaluation results for the revised summaries. 
We also measure as degree of revision the number of 
revised characters for the three editing operations, and 
the number of documents that are given up revising by 
the judges.  Please look at the detailed data at NTCIR 
Workshop 3 data booklet. 
 Figure 1 indicates how much the scores for content 
and readability vary for the summaries of the same 
summarization rate.  It shows that the readability scores 
tend to be higher than those for content, and it is 
especially clearer for 40% summarization. 
 
Deletion Insertion Replacement 
System UI
M RD IM RD C RD 
F0201 3.5 0.5 4.3 0.8 1.1 0.7 
F0202 3.5 0.4 2.4 0.2 0.7 0.2 
F0203 3.6 0.3 2.8 0.2 0.5 0.4 
F0204 2.7 0.5 2.3 0.2 1.2 0.7 
F0205 5.5 0.4 2.5 0.8 2.0 0.7 
F0206 2.0 0.4 3.4 0.6 0.4 0.4 
F0207 3.5 0.4 2.7 0.3 0.6 0.6 
F0208 2.4 0.5 2.3 0.4 0.2 0.3 
F0209 2.5 0.5 2.2 0.2 0.3 0.4 
human 1.9 0.8 2.4 2.0 0.9 0.7 
ld 2.8 0.7 2.4 0.2 0.5 0.4 
stein 3.0 0.3 1.8 0.2 0.4 0.3 
edit 2.2 0.8 2.5 0.6 1.0 1.2 
ALL 3.1 0.5 2.6 0.5 0.7 0.5 
 
-0.300
-0.200
-0.100
0.000
0.100
0.200
0.300
F0101
F0102
F0103
F0104
F0105
F0106
F0107
F0108
TF Hum
an
C20-C40
R20-R40
Figure 2 Score difference between 20% and 40% 
summarizations (Task A) 
 
 Figure 2 shows the differences in scores for the 
different summarization rates, i.e. 20% and 40% of task 
A.  ?C20-C40? means the score for content 20% minus 
the score for content 40%.  ?R20-R40? ?means the score 
for readability 20% minus the score for readability 40%.  
Table 9 Evaluation by revision (task B short) 
7 Discussion  Figure 2 tells us that the ranking scores for 20% 
summarization tend to be higher than those for 40%, 
and this is true with the baseline system and human 
summaries as well. 7.1. Discussion for Evaluation by ranking 
 Second, consider task B.  Figure 3 shows the 
differences in scores for content and readability for each 
system for task B. ?CS-RS? means the score for content 
short summaries minus the score for readability short 
summaries.   ?CL-RL? is computed in the same way for 
long summaries. 
 We here further look into how the participating 
systems perform by analysing the ranking results in 
terms of differences in scores for content and those for 
readability. 
 First, consider task A. Figure 1 shows the differences 
in scores for content and readability for each system.  
?C20-R20? means the score for content 20% minus the 
score for readability 20%.   ?C40-R40? means the score 
for content 40% minus the score for readability 40%.    
 
-0.800
-0.600
-0.400
-0.200
0.000
0.200
0.400
F0201
F0202
F0203
F0204
F0205
F0206
F0207
F0208
F0209
H
um
an CS-RS
CL-RL
 
 
-0.500
-0.400
-0.300
-0.200
-0.100
0.000
0.100
0.200
F0101
F0102
F0103
F0104
F0105
F0106
F0107
F0108
TF Hum
an C20-R20
C40-R40
Figure 1 Score difference between Content and 
Readability (Task A) 
Figure 3 Score difference between content and 
readability (Task B) 
 
Figure 3 shows, like Figure 1, that the scores for 
readability tend to be higher, thence, the differences are 
in minus values, than those for content for both short 
and long summaries.  In addition, the differences are 
larger than the differences we saw for task A, i.e. in 
Figure 1. 
 Figure 4 shows the differences in scores for the 
different summarization lengths, i.e. short and long 
summaries of task B.  ?CS-CL? means the score for 
content short summaries minus the score for content 
long summaries.  ?RS-RL? means the score for 
readability short summaries minus the score for 
readability long summaries. 
 Figure 4 tells us, unlike Figure2, the scores for short 
summaries tend to be lower than those for long 
summaries.  This tendency is very clear for the 
readability ranking scores.  
Figure 1 and 3 show that when we compare the 
ranking scores for content and readability summaries, 
the readability scores tend to be higher than those for 
content, which means that the evaluation for readability 
is worse than that for content.  Figure 2 and 4 shows 
contradicting tendencies.  Figure 2 indicates that short 
(20%) summaries are higher in ranking scores, i.e. 
worse in evaluation.  However, Figure 4 indicates the 
other way round. 
Intuitively longer summaries can have better 
readability since they have more words to deal with, and 
it is shown in Figure2.  However, it is not the case with 
task B ranking results.  Longer summaries had worse 
scores, especially in readability evaluation.  
 
-0.800
-0.600
-0.400
-0.200
0.000
0.200
0.400
0.600
F0201
F0202
F0203
F0204
F0205
F0206
F0207
F0208
F0209
H
um
an
CS-CL
RS-RL
 
Figure 4 Score difference between different 
summarization lengths (Task B) 
7.2. Discussion for Evaluation by revision 
 To determine the plausibility of the judges?  revision,  
the revised summaries were again evaluated with the 
evaluation methods in section 5.  As Tables 6 to 9 show, 
the degree of the revisions for the revised summaries is 
rather smaller than that for the original ones and is 
almost same as that for human summaries. 
Tables 10 and 11 show the results of evaluation by 
ranking for the revised summaries at task A and B 
respectively. Compared with Tables 2 to 5, Tables 10 
and 11 show that the scores for the revised summaries 
are rather smaller than those for the original ones and 
are almost same as those for human summaries. 
From these results,  the quality of the revised 
summaries is considered as same as that of human 
summaries. 
 
System No Content20% 
Read- 
ability 
20% 
Content 
40% 
Read- 
ability 
40% 
edit 2.37 2.43 2.33 2.33 
Table 10 Ranking evaluation (task A) 
 
System No ContentShort 
Read- 
ability 
Short 
Content 
Long 
Read- 
ability 
Long 
edit 1.93 2.23 2.13 2.50 
Table 11 Ranking evaluation (task B) 
8. Conclusions 
 We have described the outline of the Text 
Summarization Challenge 2.  In addition to the two 
evaluation runs, we held two round-table discussions, 
one right after dryrun, and the other after formal run.  At 
the second round-table discussion, it was pointed out 
that we might need to examine more closely the results 
of evaluation, especially the one by ranking.  
We are now starting the third evaluation (TSC3). 
Please see our web page[4]  for the details of the task.  
References 
[1] Proceedings of The Tipster Text Program Phase III, 
Morgan Kaufmann, 1999. 
[2] Mani, I., et al The TIPSTER SUMMAC Text 
Summarization Evaluation, Technical Report, MTR 
98W0000138,  The MITRE Corp.,  1998. 
[3] http://www-nlpir.nist.gov/projects/duc/. 
[4] http://oku-gw.pi.titech.ac.jp/tsc/index-en.html. 
[5] Takahiro Fukusima and Manabu Okumura, ?Text 
Summarization Challenge ?Text Summarization 
Evaluation at NTCIR Workshop 2?, In Proceedings of 
NTCIR Workshop 2, pp.45-50, 2001. 
[6] Takahiro Fukusima and Manabu Okumura, ?Text 
Summarization Challenge ? Text Summarization 
Evaluation in Japan?, North American Association for 
Computational Linguistics (NAACL2001), Workshop 
on Automatic Summarization, pp.51-59, 2001. 
[7] Gees C. Stein, Tomek Strazalkowski and G. Bowden 
Wise, ?Summarizing Multiple Documents using Text 
Extraction and Interactive Clustering?, Pacific 
Association for Computational Linguistics, pp.200-
208, 1999. 
Appendix A 
 
20%  
readability lead human humans
F101 0.767 0.100 0.033 
F102 0.667 0.100 0.033 
F103 0.667 0.100 0.033 
F104 0.733 0.133 0.067 
F105 0.833 0.233 0.100 
F106 0.867 0.233 0.133 
F107 0.733 0.233 0.200 
F108 0.833 0.067 0.033 
human 0.933 0.467 0.233 
tf 0.267 0.067 0.067 
    
20% 
content lead human humans
F101 0.867 0.200 0.167 
F102 0.900 0.200 0.100 
F103 0.800 0.067 0.033 
F104 0.767 0.067 0.033 
F105 0.933 0.200 0.067 
F106 0.900 0.200 0.100 
F107 0.800 0.167 0.133 
F108 1.000 0.267 0.167 
human 1.000 0.400 0.233 
tf 0.233 0.000 0.000 
    
40% 
readability lead human humans
F101 0.833 0.233 0.033 
F102 0.700 0.133 0.100 
F103 0.800 0.100 0.067 
F104 0.800 0.133 0.033 
F105 0.767 0.200 0.167 
F106 0.800 0.167 0.100 
F107 0.767 0.200 0.167 
F108 0.833 0.100 0.067 
human 0.867 0.467 0.300 
tf 0.400 0.100 0.100 
    
40% 
content lead human humans
F101 0.967 0.167 0.100 
F102 0.900 0.267 0.200 
F103 0.800 0.100 0.033 
F104 0.900 0.133 0.067 
F105 0.867 0.200 0.167 
F106 0.967 0.200 0.100 
F107 0.933 0.233 0.167 
F108 1.000 0.167 0.100 
human 0.967 0.300 0.267 
tf 0.367 0.000 0.000 
Appendix B 
 
short  
readability lead stein human 
F201 0.233 0.167 0.333 
F202 0.333 0.267 0.367 
F203 0.367 0.333 0.533 
F204 0.300 0.233 0.300 
F205 0.200 0.233 0.267 
F206 0.267 0.233 0.233 
F207 0.200 0.267 0.400 
F208 0.367 0.300 0.233 
F209 0.433 0.167 0.433 
human 0.667 0.600 0.533 
    
short  
content lead stein human 
F201 0.533 0.400 0.267 
F202 0.433 0.333 0.200 
F203 0.500 0.500 0.100 
F204 0.433 0.400 0.200 
F205 0.500 0.533 0.233 
F206 0.300 0.200 0.100 
F207 0.633 0.633 0.233 
F208 0.400 0.333 0.133 
F209 0.433 0.267 0.167 
human 0.700 0.700 0.467 
    
long  
readability lead stein human 
F201 0.167 0.167 0.267 
F202 0.367 0.333 0.300 
F203 0.300 0.267 0.367 
F204 0.233 0.267 0.333 
F205 0.300 0.100 0.233 
F206 0.133 0.100 0.233 
F207 0.200 0.233 0.200 
F208 0.333 0.300 0.333 
F209 0.267 0.300 0.367 
human 0.567 0.533 0.467 
    
long  
content lead stein human 
F201 0.500 0.500 0.400 
F202 0.533 0.300 0.167 
F203 0.433 0.300 0.100 
F204 0.333 0.400 0.233 
F205 0.567 0.367 0.300 
F206 0.200 0.067 0.167 
F207 0.567 0.500 0.233 
F208 0.433 0.533 0.200 
F209 0.567 0.533 0.267 
human 0.733 0.700 0.567 
 
Patent Claim Processing for Readability
- Structure Analysis and Term Explanation -
Akihiro SHINMORI
Department of
Computational
Intelligence and
Systems Sciences,
Tokyo Institute of
Technology, and
INTEC Web and
Genome Informatics Co.
shinmori@isl.intec.co.jp
Manabu OKUMURA
Precision and
Intelligence
Laboratory,
Tokyo Institute of
Technology
oku@pi.titech.ac.jp
Yuzo MARUKAWA
Japan Science and
Technology Corp., and
National Institute of
Informatics
maru@nii.ac.jp
Makoto IWAYAMA
Precision and
Intelligence
Laboratory,
Tokyo Institute of
Technology, and
Hitachi, Ltd.
iwayama@pi.titech.ac.jp
Abstract
Patent corpus processing should be cen-
tered around patent claim processing be-
cause claims are the most important part
in patent specifications. It is common that
claims written in Japanese are described in
one sentence with peculiar style and word-
ing and are difficult to understand for ordi-
nary people. The peculiarity is caused by
structural complexity of the sentences and
many difficult terms used in the descrip-
tion. We have already proposed a frame-
work to represent the structure of patent
claims and a method to automatically an-
alyze it. We are currently investigating a
method to clarify terms in patent claims
and to find the explanatory portions from
the detailed description part of the patent
specifications. Through both approaches,
we believe we can improve readability of
patent claims.
1 Introduction
The importance of intellectual property, specifically
patent, is being recognized more than ever. In the
academia, patent is being considered as the core
component for technology transfer to industry. With
the upsurge of business method patents and software
patents, more and more business persons are con-
cerned about patent.
Patent is described in patent specification which is
a kind of legal documents. The most important part
of patent specification is where the claims are writ-
ten, because ?the claims specify the boundaries of
the legal monopoly created by the patent? (Burgun-
der, 1995). Therefore, we believe that patent corpus
processing should be centered around patent claim
processing.
It is common that Japanese patent claims are de-
scribed in one sentence with peculiar style and word-
ing and that they are difficult to read and under-
stand for ordinary people. After surveying related
literature and investigating NTCIR3 patent collec-
tion (Iwayama et al, 2003), we found the difficulty
has two aspects: structural difficulty and term diffi-
culty.
In this paper, we first present the characteristics
of patent claims. Next, we present our work on the
structure analysis of patent claims. Third, we intro-
duce our on-going research on term explanation for
patent claims.
2 Characteristics of Patent Claim
Typical Japanese patent claims taken from two
patents are shown in Figure 1 and 2.
In general, Japanese sentences are inserted with
the touten ??? or ??? (comma) and end with the
kuten ??? or ??? (period) . The touten plays a
role of segmenting the sentence for disambiguating
the meaning and for improving readability. Accord-
ing to the literature (Maekawa, 1995), the average
length of Japanese sentences is 55.85 characters in
newspaper articles on politics and 75.37 characters
on social affairs articles.
The claims of Figure 1 and 2 are both written
in one sentence. Though they are appropriately in-
?????????????????????
??????????????????????
?????????????????????
???? ?????? ??????????
?????????????????????
????????????????????
??????? ????????? ????
?????????????????????
?????????????????????
?????????????????????
?????????????????????
?????????????????????
?????????
Figure 1: A sample Japanese patent claim
(Publication Number=10-011111)
??????????????????????
????????????????<nl>
?????????????????????
?????????????????????
?????????????????????
??????????????????
Figure 2: A sample Japanese patent claim con-
taining a newline (Publication Number=10-146993)
(Note: <nl> means a newline.)
serted with the touten ???, they are unusually long
with the length of 295 characters and 119 charac-
ters. It is definitely true that most Japanese who are
not accustomed to reading patent claims have diffi-
culty in reading them. In fact, according to (Kasuya,
1999), Japanese patent attorneys themselves recog-
nize that Japanese patent claims are difficult to read.
The salient characteristics of Japanese patent
claims from the viewpoint of readability are as fol-
lows:
1. The length of sentence is long.
2. The structure of description is complex.
3. There are several terms which are difficult to
understand or requires explanation for under-
standing.
To examine the first point, we extracted all of the
first claims of the sample data (59,968 patents) in the
NTCIR3 patent collection, and calculated the aver-
age sentence length. We found that it is 242 char-
acters and confirmed that Japanese patent claims are
unusually long.
With regard to the second point, we surveyed
several books and articles written for patent appli-
cants to explain how to draft patent claims(Kasai,
1999; Kasuya, 1999) and how to translate patent
claims(Lise, 2002).
Based on the survey, we classify the description
style into the following three. [Note: In the follow-
ing explanation, Japanese phrases are followed by
their literal expression in [] and their English trans-
lation in (). ]
Process sequence style As in ?...? [shi](does), ...
? [shi](does), ... ?? [shita] (and does)...??
the sequence of processes is described?Mainly
used in method inventions.
Element enumeration style As in ?...? [to](and),
... ? [to](and), ... ????? [to kara
naru](comprising), ...?, the set of element is de-
scribed. Mainly used in product inventions.
Jepson-like style As in ?...???? [ni oite](in), ...
?????? [wo tokuchou to suru](be charac-
terized by), ...?, the description consists of the
first half part and the last half part. In the first
half part, either the known or the precondition
part is described. In the last half part, either the
new or the main part is described 1.
These patterns are not mutually exclusive. For ex-
ample, the first half part of the Jepson-like style may
be written in the process sequence style or in the el-
ement enumeration style.
With regard to the third point, Figure 1 contains
the term ?????????(an actuator) and Figure
2 contains the term ???????(sticky ink) which
require explanation for understanding.
Because of these characteristics, the well-known
Japanese parser KNP (Kurohashi, 2000) incorrectly
analyze or cannot process most of the Japanese
patent claims.
KNP?s dependency analysis works by detecting
parallel structure utilizing thesaurus and dynamic
programming, but it does not work well for patent
1Note that the term ?Jepson claim? is rigidly defined and
used in Europe or in the USA to describe the kind of claims
in which the known part and the new part are clearly sepa-
rated. In Japan, that is not common and the separation is more
vague(Lise, 2002). That?s why we name this as ?Jepson-like
style?.
Table 1: Relations for Japanese patent claims
Type Relation Explanation Example
Multi- PROCEDURE Process Sequence [???][???][???]X
Nuclear Style [Note: The above means ?X which [does?,]
[does?,] [and does?].?]
Multi- COMPONENT Element Enumeration [???][???][??]?
Nuclear Style [Note: The above means ?[?,] [?,] [and?].?]
Mono- ELABORATION S elaborates N. [X? Y??][Z? A]
Nuclear [Note: The above means ?[A of Z] [which Y X].?]
Mono- FEATURE Characterization [X??? Y][??????]
Nuclear [Note: The above means ?[characterized
by] [Y which is X].?].
Mono- PRECONDITION Jepson-like Style [X?????][Y?? Z]
Nuclear [Note: The above means ?[In X,] [Z which Y].?.
Mono- COMPOSE Composition [????????][????]X
Nuclear [Note: The above means ?X [composed of] [?,
?, and?].?].
claims because they often include ?chain expres-
sions? in which one concept is first defined and next
another concept is defined using the first. For the
claim in Figure 1, although ???????? (a load
detection method), ??????????? (a fre-
quency transfer device no.1), ?????????
?? (a frequency transfer device no.2), ??????
(a modulation method), and ???????? (an os-
cillation generation method) need to be recognized
as parallel, it cannot be recognized due to the exis-
tence of the expressions designated by the underline.
3 Structure Analysis of Patent Claims
3.1 Background
To improve readability of Japanese patent claims,
we claim that the structure of description needs to
be presented in a readable way. To do so, the struc-
ture needs to be analyzed first.
Japanese patent claims are described in such a
way that multiple sentences are coerced into one
sentence(Kasuya, 1999). In other words, a claim
is composed of multiple sentences that have some
kind of relationships with each other. Therefore, we
decided to apply the RST (Rhetorical Structure The-
ory) (Mann, 1999) that was proposed to analyze dis-
course structure composed of multiple sentences.
RST was proposed in the 1980?s and has been
successfully applied to automatic summarization
(Marcu, 2000), automatic layout (John Bateman,
2000), and so on. A Tcl/Tk-based interactive tool
(OD?onnel, 1997) was developed to support to man-
ually edit and to visually show the structure.
3.2 Framework
For the structure analysis of Japanese patent claims,
we defined six relations as in Table 1. Two of them
are multi-nuclear where composing elements are
equally important. Four of them are mono-nuclear
where one element is nucleus, the other is satellite,
and the nucleus is more important than the satellite.
In the ?Example? column of Table 1, the regions en-
closed with ?[? and ?]? are segments or spans and
the underlined ones are nuclei.
Given the patent claims in Figure 1 and Figure
2, we can analyze their structure and present them
visually by using RSTTool (OD?onnel, 1997) as in
Figure 3 and Figure 4 2.
3.3 Cue-phrase-based Approach
In designing the algorithm, we took a similar ap-
proach to (Marcu, 2000). We collected cue phrases
that can be used for segmenting long claims and es-
tablishing relations among segments or spans.
2Because RSTTool is written in Tcl/Tk and Tcl/Tk is an in-
ternationalized language, we did not have to localize it to dis-
play Japanese characters.
Figure 3: A result of structure analysis of patent claim in Figure 1 (using RSTTool v2.7)
Figure 4: A result of structure analysis of patent claim in Figure 2 (using RSTTool v2.7)
Table 2: Description pattern just before the newlines
in claims in which newline are explicitly inserted
No Pattern Ratio
1 (Noun|Symbol)? (?|?) 46.1%
[Note: ??? means ?and?.]
2 (Verb-Cont-Form| 17.5%
AuxVerb-Cont-Form)(?|?)
3 (Noun|Symbol)???? (?|?) 16.4%
[Note: ?????? means ?in?.]
4 (Noun|Symbol)???? (?|?) 7.2%
[Note: ?????? means ?in?.]
Cue phrases were first collected manually by
reading patent claims. Then we found that about half
of the claims are inserted with newlines at seemingly
segment boundaries as in Figure 2.
We investigated all of the extracted first claims
of the sample data and 48.5% of them are newline-
inserted claims. It seems that the drafters of patent
claims explicitly inserted those newlines for read-
ability for themselves. We checked the description
pattern of the last three morphemes just before each
newline of those claims. The result is shown in Ta-
ble 2. In Table 2, ?Verb-Cont-Form? means ???
???? (verb in continuous form) and ?AuxVerb-
Cont-Form? means ???????? (auxiliary verb
in continuous form). Note that the description pat-
terns are expressed in the regular expression notation
of Perl.
Summarizing the above, we came up with the
cue phrases in Table 3. In Table 3, ?Verb-Basic-
Form? means ??????? (verb in basic form)
and ?AuxVerb-Basic-Form? means ????????
(auxiliary verb in basic form).
3.4 Algorithm and Implementation
We designed an algorithm for analyzing structure
of independent claims3. Although patent claims are
written in natural language, it?s not written in a free
form and is restricted in a sense that there are de-
scription styles established in the community. So,
we designed an algorithm composed of a lexical an-
alyzer and a parser as in the formal language proces-
sors.
3Independent claims are claims which do not refer to any
other claims.
First, the input claim is analyzed with the morpho-
logical analyzer ?chasen? (Matsumoto et al, 2002).
Because some patent claims explicitly contain new-
lines as in Figure 2, we use the ?-j? option setting
the sentence delimiter as ????? in ?.chasenrc?.
Next, the output from chasen is analyzed with the
lexical analyzer. The main point of our algorithm
is the context-dependent behavior of the lexical ana-
lyzer as follows:
? The lexical analyzer outputs two types of to-
ken: cue phrase token and morpheme token.
? Outputting morpheme tokens is done depend-
ing on some contextual conditions to avoid am-
biguities in the parsing.
? For other morphemes whose context did not
satisfy the above conditions, an anonymous
morpheme token (WORD) is output.
Next, the output from the lexical analyzer is pro-
cessed with the parser generated from a context-free
grammar (CFG) by using Bison (Donnelly and Stall-
man, 1995)-compatible parser generator. The CFG
we designed for Japanese patent claim consists of 57
rules, 11 terminals, and 19 non-terminals.
Finally, a structure tree is constructed in the form
of ?.rs2? file used in RSTTool v2.7. By using RST-
Tool, the output is visually displayed as in Figure 3
and Figure 4.
3.5 Evaluation
The evaluation was done by using the first claims 4
of 59,956 patents extracted from the NTCIR3 patent
data collection.
The NTCIR3 patent data collection consists of
697,262 patents opened to public in 1998 and in
1999. For the analysis, the collection of cue phrases,
and the creation of the CFG, we used patents in
1998. For the evaluation, we used patents in 1999.
We checked the IPC (International Patent Classi-
fication) code of 59,956 patents and confirmed that
the distribution is similar to the one of all opened
patents in 1999 disclosed by JPO (Japan Patent Of-
fice).
The evaluation was done in the following points:
4First claims are always independent claims.
Table 3: Cue phrases which can be used to analyze patent claims
Token Name Cue Phrase Gloss
JEPSON CUE ? (? |?)?? (?|?) [ni oite] (in)
???? (?|?) [de atte] (in)
???? (?|?) [ni atari] (in)
?? (?)?? (?|?) [ni atari] (in)
FEATURE CUE ???? (?? |??)(?|?)? [wo tokuchou to
(shita|suru)]
(characterized by)
COMPOSE CUE ????????? (? |? |???) (?|?)? [wo tousaishite kousei
sare (ta|ru|teiru)]
(comprising)
? (?|?)?(? |? |??)? (? |? |???) (?|?)? [wo sonae (ta|ru|teiru)]
(comprising)
? (?|?)??? (?? |?? |???? |????) [wo gubi (shita|suru|
(?|?)? shiteiru|shitenaru)]
(comprising)
(? |??)???? (? |???) (?|?)? [(de|kara) kousei sare
(ta|teiru)]
(comprising)
? (?|?)?? (?? |??) (?|?)? [wo yuu (suru|shita)]
(comprising)
? (?|?)??? (?? |??) (?|?)? [wo hougan (suru|shita)]
(comprising)
? (?|?)?? (? |??) (?|?)? [wo fuku (mu|nda)]
(comprising)
?? (?|?)?(?? |??? |?????) (?|?)? [kara (naru|natta
|natteiru)]
(comprising)
?? (?|?)?(?? |??? |?????) (?|?)? [kara (naru|natta
|natteiru)]
(comprising)
? (?|?)??? (? |???) (?|?)? [wo mouke (ta|teiru)]
(comprising)
? (?|?)??? (?? |?? |????) (?|?)? [wo soubi (suru|shita
|shiteiru)]
(comprising)
NOUN The sequence of ?(Noun|Symbol)? (?|?)?
POSTP TO
PUNCT TOUTEN
VERB RENYOU The sequence of
PUNCT TOUTEN ?(Verb-Cont-Form|AuxVerb-Cont-Form)(?|?)?
which exist before
?(Verb-Basic-Form|AuxVerb-Basic-Form)
(Noun|Symbol)?
Accept Ratio The ratio of claims accepted by the
parser generated by the CFG.
Processing Speed The time required to process one
claim.
Accuracy The accuracy of the analysis result eval-
uated indirectly and directly.
The accept ratio was more than 99.77%. The pro-
cessing speed was 0.30 second per each claim (eval-
uated on a Linux PC using Pentium III 1GHz and
512MB memory). So, it is almost real-time.
3.5.1 Indirect Evaluation on Accuracy
By specifying a command-line switch, our pro-
gram can be run without utilizing the originally in-
serted newlines. The newline insertion positions can
be predicted by the result of structure analysis and
some heuristics. So, indirect evaluation was done by
comparing the newline insertion positions between
the originally newline-inserted claims and the auto-
matically newline-inserted claims utilizing the result
of structure analysis. The recall(R), the precision(P),
and the F-measure(F) are calculated by the follow-
ings, where c is the number of correctly-inserted
newlines, n is the number of newlines in the orig-
inal claim, and i is the number of inserted newlines.
R =
c
n
(1)
P =
c
i
(2)
F =
2 ? R ? P
R + P
(3)
The baseline was set in that the newlines are in-
serted mechanically at the end of every sequence
of ?(NOUN|SYMBOL)(?|?)? and ?(Verb-Cont-
Form|AuxVerb-Cont-Form)(?|?)?.
Note that newlines are sometimes inserted at the
positions that are not segment boundaries in the
meaning of RST. For example, it is often the case
that at the end of ???? (a postpositional particle
representing the subject), newlines are inserted. So,
our newline-insertion prediction algorithm has the
inherent upper limit whose recall is 0.873.
The result is shown in Table 4.
Table 4: Evaluation result (Indirect)
Index Baseline Newline Upper
Insertion Limit
utilizing
RST
Recall(R) 0.478 0.674 0.8736
Precision(P) 0.374 0.663 N/A
F-measure 0.420 0.669 N/A
Table 5: Evaluation result (Direct)
Category Count Percentage
(Except
?No judgment?)
Correct 76 80.85%
Partially Correct 11 11.70%
Incorrect 7 7.45%
No judgment 6 -
3.5.2 Direct Evaluation on Accuracy
The direct evaluation on accuracy was done by us-
ing randomly selected 100 claims extracted. All of
these claims are the first claims. Again, we checked
the distribution of IPC and confirmed it?s similar to
the one of all opened patents in 1999 disclosed by
JPO.
The 100 claims were analyzed by our program
and the visually-displayed outputs like Figure 3 and
4 were presented to a subject who had some expe-
rience in reading patent specifications. The subject
evaluated the result by the following criteria:
? when the claim is in the Jepson-like style,
whether that is correctly recognized.
? when the claim is in the Jepson-like style,
whether the structure is correctly analyzed for
the first half part and for the last half part.
? when the claim is not in the Jepson-like style,
whether the structure is correctly analyzed for
the whole.
The result is shown in Table 5.
3.6 Application to Patent Claim Paraphrase
Once the structure of patent claims are analyzed, we
can apply the result to paraphrase patent claims.
To do so, the following actions are incorporated
into the lexical analyzer and the parser.
? The lexical analyzer deletes the words ????
(the), ??? (the), and ???? (the).
? For the parser, new actions are added which re-
locates the ?noun group? located at the end to
the front. Same thing for the ?noun group? lo-
cated just before JEPSON CUE for the Jepson-
like style claims.
? For the process sequence style, the lexical an-
alyzer conjugates verbs and adverbs from their
continuous form to basic form and replaces the
touten ?(?|?)? with the kuten ???.
? For the element enumeration style, the lexical
analyzer converts those cue phrases such as ??
????(consist of) and ?????? (include)
to their ??????(?teiru? form) plus ??? and
deletes ?? (?|?)? (and) at the end of each el-
ement.
? The lexical analyzer converts ????(thing)
just before ????????(characterized by)
to ????(the following).
? For the Jepson-like style, the parser separates
the first-half part and the last-half part by in-
serting a newline.
By doing the above processing, long patent claim
sentences are divided into multiple sentences. But
as there are cases where some of the generated sen-
tences are still too long, those sentences longer than
the threshold length (75 characters) are recursively
processed.
An example of paraphrase is shown in Figure 5.
We believe that paraphrasing can not only im-
prove readability of patent claims but also can work
effectively as a preprocessing for machine transla-
tion 5.
5In fact, there are several commercial machine translation
software which does special preprocessing for patent claims be-
fore translating from Japanese to English.
?????????????????????
???????????
?????????????????????
??????????:
?????????????????????
??????
?????????????????????
????????????????
?????????????????????
?????????????????
?????????????????????
?????????????????????
????????????
?????????????????????
????????
Figure 5: A sample paraphrase for Figure 1
4 Term Explanation for Patent Claims
4.1 Background and Motivation
Once the structure of patent claims are analyzed
and presented visually, next hurdle for readability is
terms.
There are many novel terms used in patent claim
description. They can be classified into the follow-
ing categories:
Terms specific to the invention Patent drafters of-
ten assign unique names to the invention, its
elements, and its processes for their identifica-
tion.
Terms specific to the domain The patent law re-
quires patents should be written so that those
who have ordinary knowledge in the domain
can understand and perform the invention. So,
technical terms that are established in the do-
main are often used. Additionally, there exist
?patent jargons? which are created by combin-
ing two kanji characters such as ???? (put
and insert) and ???? (put into the hall)(Kasai,
1999). They are first created by some patent
drafters for the sake of brevity and have been
widely used in the community. So, they are
terms specific to the inventions of the domain.
Those who do not have enough knowledge in
the domain or those who are not accustomed to
reading patent specifications have difficulty in
understanding them.
Giving appropriate explanations for these terms
would help to improve readability of patent claims.
4.2 Approach
First of all, it is necessary to recognize terms to be
explained. There are many research issues in term
extraction in general, but for our purpose we use
the following morphological pattern to extract terms
from patent claims:
(Prefix)*(Noun|Unknown-Words|Symbol
|Verb-Cont-Form|Verb-Compound-With-
Indeclinable-Word)+
By using the above pattern, we can extract such
terms as ?????????? (method to blow heat
wind), ????? (read value), and ???? (liquid
drop) which contain verbs.
Second, by using the result of structure analysis,
we can infer the categories of the terms as follows:
? If the term appears at the end of the claim or
just before the JEPSON CUE in the Jepson-
like style, or just before ??? (and) in the el-
ement enumeration style, it is a term specific
to the invention. For example, ???????
?????? (an operational virtual oscillation
generating device) and ????????(a load
detection method) in Figure 1 are terms specific
to this invention.
? If the term appears in the middle of the first half
in the Jepson-like style, it can be a term specific
to the domain. For example, ???????
??(an actuator) in Figure 1 is a technical term
in the domain.
? If the term is a two-kanji character and is not
listed in the ordinary dictionaries, it can be a
patent jargon.
Finally, by looking at the detailed description of
the invention or related inventions, we can back up
the above inference as follows:
? The terms specific to the invention should be
described after the ?means to solve the prob-
lem? section in the detailed description of the
invention.
? The terms specific to the domain are widely
used in the inventions of the domain. So, it is
highly possible that they occur frequently in the
related inventions. We can consider the collec-
tion of search result as the related inventions.
? Some of the technical terms specific to the do-
main are described in the ?prior art? section of
the detailed description of the invention or re-
lated inventions in the domain.
For those technical terms specific to the domain,
explanatory portions such as the following can be
found:
 ?...???????????????????????
?????????????????...?
(... driving the oil pressure cylinder (or the actuator) at
the speed of ...)
 ?...??????????...?
(... the spout (or the orifice) ...)
 ?...????????????????????...?
(... blowing out ink preliminarily (namely, purging ink)
...?
 ?...????????????????????...?
(... ink of the hot-melt type (or solid ink) ...
As can be seen in the above, explanatory por-
tions can be found by using cue phrases such as ?
?? and ???, ???? (?in the following?), and ???
?? (?or? or ?namely?).
4.3 Sample Scenario
From the patent claim in Figure 2, we find many
terms that are candidates for explanation such as ??
???? (time measurement), ???????? (the
method to measure time), ?????? (measurement
result), ??????? (sticky ink), ???????
?? (removal of sticky ink), ???????????
(removal processing of sticky ink), ???????
???? (the method to remove sticky ink).
Among the above terms, ???????? (the
method to measure time) and ?????????
?? (the method to remove sticky ink) are terms spe-
cific to the invention because they are judged as the
elements by structure analysis.
By searching the detailed description, we can find
the explanatory portion for ??????? (sticky
ink) as follows.
 ?...???????????????????????
????...?
(... the ink of increased stickiness (in the following, we
call it as ?sticky ink? ...)
4.4 Further Analysis and Experimentation
We continue to analyze the NTCIR3 patent data col-
lection, specifically ?Patolis Test Collection? which
is a test collection for patent retrieval consisting of
a set of query and search result. We use each search
result as ?related inventions? and analyze them to
collect cue phrases for finding explanatory portions
for technical terms specific to the domain.
5 Related Work
A NLP research for patent claim is already reported
in (Kameda, 1995). It is directed toward dependency
analysis of patent claims. Although it is proposed to
support ?analytic reading? of patent claims, the eval-
uation result for large-scale real patent data is not
reported. Our approach is different from (Kameda,
1995) in that the top-level structure is analyzed.
In (Sheremetyeva and Nirenburg, 1996), a re-
search on a system for authoring patent claims us-
ing NLP and knowledge engineering technique is re-
ported.
6 Concluding Remarks
We have presented a framework to represent the
structure of patent claims and a method to automat-
ically analyze it. The evaluation result suggest that
our approach is robust and practical.
We are currently investigating a method to clar-
ify terms in patent claims and to find the explana-
tory portions from the detailed description part of
the patent specifications.
It is not only a step toward improving readability,
but it can also lead to more challenging task of auto-
matic patent map generation(Study group on patent
map, 1990).
Acknowledgements
The NTCIR3 patent data collection was used in our research.
References
Lee B. Burgunder. 1995. Legal Aspects of Managing
Technology. South Western.
Charles Donnelly and Richard Stallman, 1995. Bison:
The YACC-compatible Parser Generator, Version 1.25.
Makoto Iwayama, Atsushi Fujii, Akihiko Takano, and
Noriko Kando. 2003. Overview of patent retrieval
task at ntcir-3. In The Third NTCIR Workshop on Re-
search in Information Retrieval, Automatic Text Sum-
marization and Question Answering. National Institute
of Informatics.
Jorg Kleinz Klaus Reichenberger John Bateman,
Thomas Kamps. 2000. Toward constructive text,
diagram, and layout generation for information pre-
sentation. Computational Linguistics, 27(3):409?449.
Masayuki Kameda. 1995. Support functions for reading
japanese text. In IPSJ SIGNotes Natural Language,
number 110. Information Processing Society of Japan.
(in Japanese).
Yasuji Kasai. 1999. Manual for Drafting Patent Claims.
Kougyo Chosakai. (in Japanese).
Youji Kasuya. 1999. On the description style of patent
claims and the techniques to draft them. Patent, 52(2).
(in Japanese).
Sadao Kurohashi. 2000. KNP - japanese parsing for real.
IPSJ MAGAZINE, 41(11). (in Japanese).
William Lise. 2002. An investigation of ter-
minology and syntax in japanese and us patents
and the implications for the patent translator.
http://www.lise.jp/patsur.html.
Mamoru Maekawa. 1995. Science of Sentences.
Iwanama. (in Japanese).
Bill Mann. 1999. An introduction
to rhetorical structure theory (RST).
http://www.sil.org/ mannb/rst/rintro99.htm.
Daniel Marcu. 2000. The Theory and Practice of Dis-
course Parsing and Summarization. MIT Press.
Yuji Matsumoto, Akira Kitauchi, Tatsuo Yamashita,
Yoshitaka Hirano, Hiroshi Matsuda, Kazuma Takaoka,
and Masayuki Asahara, 2002. Morphological Analy-
sis System ChaSen version 2.2.9 Manual. Nara Insti-
tute of Science and Technology.
Michael OD?onnel. 1997. RST-Tool: An RST analysis
tool. In The 6th European Workshop on Natural Lan-
guage Generation.
Svelana Sheremetyeva and Sergey Nirenburg. 1996.
Knowledge elicitation for authoring patent claims.
IEEE Computer, 57?63.
Study group on patent map, editor. 1990. Patent Map
and Information Strategy. Japan Institute of Invention
and Innovation. (in Japanese).
Proceedings of the Workshop on Frontiers in Corpus Annotation II: Pie in the Sky, pages 37?44,
Ann Arbor, June 2005. c?2005 Association for Computational Linguistics
Investigating the Characteristics of Causal Relations in Japanese Text
Takashi Inui and Manabu Okumura
Precision and Intelligence Laboratory
Tokyo Institute of Technology
4259, Nagatsuta, Midori-ku, Yokohama, 226-8503, Japan
tinui@lr.pi.titech.ac.jp, oku@pi.titech.ac.jp
Abstract
We investigated of the characteristics of
in-text causal relations. We designed
causal relation tags. With our designed
tag set, three annotators annotated 750
Japanese newspaper articles. Then, using
the annotated corpus, we investigated the
causal relation instances from some view-
points. Our quantitative study shows that
what amount of causal relation instances
are present, where these relation instances
are present, and which types of linguistic
expressions are used for expressing these
relation instances in text.
1 Introduction
For many applications of natural language tech-
niques such as question-answering systems and di-
alogue systems, acquiring knowledge about causal
relations is one central issue. In recent researches,
some automatic acquisition methods for causal
knowledge have been proposed (Girju, 2003; Sato et
al., 1999; Inui, 2004). They have used as knowledge
resources a large amount of electric text documents:
newspaper articles and Web documents.
To realize their knowledge acquisition methods
accurately and efficiently, it is important to know-
ing the characteristics of presence of in-text causal
relations. However, while the acquisition methods
have been improved by some researches, the char-
acteristics of presence of in-text causal relations are
still unclear: we have no empirical study about what
amount of causal relation instances exist in text and
where in text causal relation instances tend to ap-
pear.
In this work, aiming to resolve the above issues,
we create a corpus annotated with causal relation
information which is useful for investigating what
amount of causal relation instances are present and
where these instances are present in text. Given
some Japanese newspaper articles, we add our de-
signed causal relation tags to the text segments. Af-
ter creating the annotated corpus, we investigate the
causal relation instances from three viewpoints: (i)
cue phrase markers, (ii) part-of-speech information,
and (iii) positions in sentences.
There are some pieces of previous work on anal-
ysis of in-text causal relations. However, although
causal relation instances appear in several different
ways, just a few forms have been treated in the pre-
vious studies: the verb phrase form with cue phrase
markers such as in (1a) has been mainly treated. In
contrast, we add our causal relation tags to several
types of linguistic expressions with wide coverage to
realize further analyses from above three points. Ac-
tually, we treat not only linguistic expressions with
explicit cues such as in (1a) , but also those with-
out explicit cues, i.e. implicit, as in (1b) , those
formed by noun phrases as in (1c), and those formed
between sentences as in (1d) .
(1) a.   -   -  	 
 -   - 
heavy rain-NOM fall-PAST because river-NOM rise-PAST
(explicit)
b.   -   -  
 -   - 
heavy rain-NOM fall-PUNC river-NOM rise-PAST
(implicit)
c.   -  
 -   - 
heavy rain-because of river-NOM rise-PAST
(noun phrase)
37
d.   -   -  -  
 -   - 
heavy rain-NOM fall-PAST-PUNC river-NOM rise-PAST
(between sentences)
We apply new criteria for judging whether a lin-
guistic expression includes a causal relation or not.
Generally, it is hard to define rigorously the notion
of causal relation. Therefore, in previous studies,
there have been no standard common criteria for
judging causal relations. Researchers have resorted
to annotators? subjective judgements. Our criteria
are represented in the form of linguistic templates
which the annotators apply in making their judge-
ments (see Section 3.2).
In Section 2, we will outline several previous
research efforts on in-text causal relations. In
Section 3 to Section 6, we will describe the details
of the design of our causal relation tags and the an-
notation workflow. In Section 7, using the annotated
corpus, we will then discuss the results for the inves-
tigation of characteristics of in-text causal relations.
2 Related work
Liu (2004) analyzed the differences of usages of
some Japanese connectives marking causal rela-
tions. The results are useful for accounting for an
appropriate connective for each context within the
documents. However Liu conducted no quantitative
studies.
Marcu (1997) investigated the frequency distri-
bution of English connectives including ?because?
and ?since? for implementation of rhetorical pars-
ing. However, although Marcu?s study was quanti-
tative one, Marcu treated only explicit linguistic ex-
pressions with connectives. In the Timebank corpus
(Pustejovsky et al, 2003), the causal relation infor-
mation is included. However, the information is op-
tional for implicit linguistic expressions.
Although both explicit expressions and implicit
expressions are treated in the Penn Discourse Tree-
bank (PDTB) corpus (Miltsakaki et al, 2004), no
information on causal relations is contained in this
corpus.
Altenberg (1984) investigated the frequency dis-
tribution of causal relation instances from some
viewpoints such as document style and the syntac-
tic form in English dialog data. Nishizawa (1997)
also conducted a similar work using Japanese dialog
data. Some parts of their viewpoints are overlapping
with ours. However, while their studies focused on
dialog data, our target is text documents. In fact, Al-
tenberg treated also English text documents. How-
ever, our focus in this work is Japanese.
3 Annotated information
3.1 Causal relation tags
We use three tags head, mod, and causal rel to rep-
resent the basic causal relation information. Our an-
notation scheme for events is similar to that of the
PropBank (Palmer et al, 2005). An event is re-
garded as consisting of a head element and some
modifiers. The tags head and mod are used to repre-
sent an event which forms one part of the two events
held in a causal relation. The tag causal rel is used
to represent a causal relation between two annotated
events.
Figure 1 shows an example of attaching the causal
relation information to the sentence (2a), in which a
causal relation is held between two events indicated
(2b) and (2c) . Hereafter, we denote the former
(cause) part of event as e1 and the latter (effect) part
of event as e2.
(2) a. ffProceedings of the 2nd Workshop on Ontology Learning and Population, pages 49?56,
Sydney, July 2006. c?2006 Association for Computational Linguistics
Towards Large-scale Non-taxonomic Relation Extraction: Estimating the
Precision of Rote Extractors?
Enrique Alfonseca?? Maria Ruiz-Casado??
?Precision and Intelligence Laboratory
Tokyo Institute of Techonology
enrique@lr.pi.titech.ac.jp
oku@pi.titech.ac.jp
Manabu Okumura? Pablo Castells?
?Computer Science Department
Universidad Autonoma de Madrid
enrique.alfonseca@uam.es
maria.ruiz@uam.es
pablo.castells@uam.es
Abstract
In this paper, we describe a rote extrac-
tor that learns patterns for finding seman-
tic relations in unrestricted text, with new
procedures for pattern generalisation and
scoring. An improved method for estimat-
ing the precision of the extracted patterns
is presented. We show that our method ap-
proximates the precision values as evalu-
ated by hand much better than the proce-
dure traditionally used in rote extractors.
1 Introduction
With the large growth of the information stored in
the web, it is necessary to have available automatic
or semi-automatic tools so as to be able to process
all this web content. Therefore, a large effort has
been invested in developing automatic or semi-
automatic techniques for locating and annotating
patterns and implicit information from the web,
a task known as Web Mining. In the particular
case of web content mining, the aim is automati-
cally mining data from textual web documents that
can be represented with machine-readable seman-
tic formalisms such as ontologies and semantic-
web languages.
Recently, there is an increasing interest in au-
tomatically extracting structured information from
large corpora and, in particular, from the Web
(Craven et al, 1999). Because of the character-
istics of the web, it is necessary to develop effi-
cient algorithms able to learn from unannotated
data (Riloff and Schmelzenbach, 1998; Soderland,
1999; Mann and Yarowsky, 2005). New types of
web content such as blogs and wikis, are also a
?This work has been sponsored by MEC, project number
TIN-2005-06885.
source of textual information that contain an un-
derlying structure from which specialist systems
can benefit.
Consequently, rote extractors (Brin, 1998;
Agichtein and Gravano, 2000; Ravichandran and
Hovy, 2002) have been identified as an appropri-
ate method to look for textual contexts that happen
to convey a certain relation between two concepts.
In this paper, we describe a new procedure for es-
timating the precision of the patterns learnt by a
rote extractor, and how it compares to previous ap-
proaches. The solution proposed opens new pos-
sibilities for improving the precision of the gener-
ated patterns, as described below.
This paper is structured as follows: Section 2
describe related work; Section 3 and 4 describe the
proposed procedure and its evaluation, and Sec-
tion 5 presents the conclusions and future work.
2 Related work
Extracting information using Machine Learning
algorithms has received much attention since
the nineties, mainly motivated by the Message
Understanding Conferences. From the mid-
nineties, there are systems that learn extraction
patterns from partially annotated and unannotated
data (Huffman, 1995; Riloff, 1996; Riloff and
Schmelzenbach, 1998; Soderland, 1999).
Generalising textual patterns (both manually
and automatically) for the identification of rela-
tions has been proposed since the early nineties
(Hearst, 1992), and it has been applied to extend-
ing ontologies with hyperonymy and holonymy re-
lations (Morin and Jacquemin, 1999; Kietz et al,
2000; Cimiano et al, 2004; Berland and Char-
niak, 1999). Finkelstein-Landau andMorin (1999)
learn patterns for company merging relations with
exceedingly good accuracies. Recently, kernel
49
methods are also becoming widely used for rela-
tion extraction (Bunescu and Mooney, 2005; Zhao
and Grishman, 2005).
Concerning rote extractors from the web, they
have the advantage that the training corpora can
be collected easily and automatically, so they
are useful in discovering many different relations
from text. Several similar approaches have been
proposed (Brin, 1998; Agichtein and Gravano,
2000; Ravichandran and Hovy, 2002), with vari-
ous applications: Question-Answering (Ravichan-
dran and Hovy, 2002), multi-document Named
Entity Coreference (Mann and Yarowsky, 2003),
and generating biographical information (Mann
and Yarowsky, 2005). Szpektor et al (2004) ap-
plies a similar, with no seed lists, to extract auto-
matically entailment relationships between verbs,
and Etzioni et al (2005) report very good results
extracting Named Entities and relationships from
the web.
2.1 Rote extractors
Rote extractors (Mann and Yarowsky, 2005) es-
timate the probability of a relation r(p, q) given
the surrounding context A1pA2qA3. This is cal-
culated, with a training corpus T , as the number
of times that two related elements r(x, y) from T
appear with that same contextA1xA2yA3, divided
by the total number of times that x appears in that
context together with any other word:
P (r(p, q)|A1pA2qA3) =
P
x,yr c(A1xA2yA3)
P
x,z c(A1xA2zA3)
(1)
x is called the hook, and y the target. In order to
train a Rote extractor from the web, this procedure
is mostly used (Ravichandran and Hovy, 2002):
1. Select a pair of related elements to be used
as seed. For instance, (Dickens,1812) for the
relation birth year.
2. Submit the query Dickens AND 1812 to a
search engine, and download a number of
documents to build the training corpus.
3. Keep all the sentences containing both ele-
ments.
4. Extract the set of contexts between them and
identify repeated patterns. This may just be
the m characters to the left or to the right
(Brin, 1998), the longest common substring
of several contexts (Agichtein and Gravano,
2000), or all substrings obtained with a suf-
fix tree constructor (Ravichandran and Hovy,
2002).
5. Download a separate corpus, called hook cor-
pus, containing just the hook (in the example,
Dickens).
6. Apply the previous patterns to the hook cor-
pus, calculate the precision of each pattern
in the following way: the number of times it
identifies a target related to the hook divided
by the total number of times the pattern ap-
pears.
7. Repeat the procedure for other examples of
the same relation.
To illustrate this process, let us suppose that we
want to learn patterns to identify birth years. We
may start with the pair (Dickens, 1812). From the
downloaded corpus, we extract sentences such as
Dickens was born in 1812
Dickens (1812 - 1870) was an English writer
Dickens (1812 - 1870) wrote Oliver Twist
The system identifies that the contexts of the last
two sentences are very similar and chooses their
longest common substring to produce the follow-
ing patterns:
<hook> was born in <target>
<hook> ( <target> - 1870 )
The rote extractor needs to estimate automati-
cally the precision of the extracted patterns, in or-
der to keep the best ones. So as to measure these
precision values, a hook corpus is now down-
loaded using the hook Dickens as the only query
word, and the system looks for appearances of the
patterns in this corpus. For every occurrence in
which the hook of the relation is Dickens, if the
target is 1812 it will be deemed correct, and oth-
erwise it will be deemed incorrect (e.g. in Dickens
was born in Portsmouth).
3 Our proposal
3.1 Motivation
In a rote extractor as described above, we believe
that the procedure for calculating the precision of
the patterns may be unreliable in some cases. For
example, the following patterns are reported by
Ravichandran and Hovy (2002) for identifying the
relations Inventor, Discoverer and Location:
Relation Prec. Pattern
Inventor 1.0 <target> ?s <hook> and
Inventor 1.0 that <target> ?s <hook>
Discoverer 0.91 of <target> ?s <hook>
Location 1.0 <target> ?s <hook>
In the particular application in which they are
used (relation extraction for Question Answering),
they are useful because there is initially a ques-
tion to be answered that indicates whether we are
50
looking for an invention, a discovery or a location.
However, if we want to apply them to unrestricted
relation extraction, we have the problem that the
same pattern, the genitive construction, represents
all these relations, apart from the most common
use indicating possession.
If patterns like these are so ambiguous, then
why do they receive so high a precision estimate?
One reason is that the patterns are only evalu-
ated for the same hook for which they were ex-
tracted. To illustrate this with an example, let
us suppose that we obtain a pattern for the rela-
tion located-at using the pairs (New York, Chrysler
Building). The genitive construction can be ex-
tracted from the context New York?s Chrysler
Building. Afterwards, when estimating the pre-
cision of this pattern, only sentences containing
<target>?s Chrysler Building are taken into ac-
count. Because of this, most of the pairs extracted
by this pattern may extract the target New York,
apart from a few that extract the name of the ar-
chitect that built it, van Allen. Thus we can expect
that the genitive pattern will receive a high preci-
sion estimate as a located-at pattern.
For our purposes, however, we want to collect
patterns for several relations such as writer-book,
painter-picture, director-film, actor-film, and we
want to make sure that the obtained patterns are
only applicable to the desired relation. Patterns
like <target> ?s <hook> are very likely to be ap-
plicable to all of these relations at the same time,
so we would like to be able to discard them auto-
matically by assigning them a low precision.
3.2 Suggested improvements
Therefore, we propose the following three im-
provements to this procedure:
1. Collecting not only a hook corpus but also a
target corpus should help in calculating the
precision. In the example of the Chrysler
building, we have seen that in most cases
that we look for the pattern ?s Chrysler build-
ing the previous words are New York, and
so the pattern is considered accurate. How-
ever, if we look for the pattern New York?s,
we shall surely find it followed by many dif-
ferent terms representing different relations,
and the precision estimate will decrease.
2. Testing the patterns obtained for one relation
using the hook and target corpora collected
for other relations. For instance, if the geni-
tive construction has been extracted as a pos-
sible pattern for the writer-book relation, and
we apply it to a corpus about painters, the rote
extractor can detect that it also extracts pairs
with painters and paintings, so that particular
pattern will not be very precise for that rela-
tion.
3. Many of the pairs extracted by the patterns
in the hook corpora were not evaluated at all
when the hook in the extracted pair was not
present in the seed lists. To overcome this,
we propose to use the web to check whether
the extracted pair might be correct, as shown
below.
3.3 Algorithm
In our implementation, the rote extractor starts
with a table containing some information about the
relations for which we want to learn patterns. This
procedure needs a little more information than just
the seed list, which is provided as a table in the
format displayed in Table 1. The data provided for
each relation is the following: (a) The name of the
relation, used for naming the output files contain-
ing the patterns; (b) the name of the file contain-
ing the seed list; (c) the cardinality of the relation.
For instance, given that many people can be born
on the same year, but for every person there is just
one birth year, the cardinality of the relation birth
year is n:1; (d) the restrictions on the hook and
the target. These can be of the following three cat-
egories: unrestricted, if the pattern can extract any
sequence of words as hook or target of the relation,
Entity, if the pattern can extract as hook or target
only things of the same entity type as the words
in the seed list (as annotated by the NERC mod-
ule), or PoS, if the pattern can extract as hook or
target any sequence of words whose sequence of
PoS labels was seen in the training corpus; and (e)
a sequence of queries that could be used to check,
using the web, whether an extracted pair is correct
or not.
We assume that the system has used the seed list
to extract and generalise a set of patterns for each
of the relations using training corpora (Ravichan-
dran and Hovy, 2002; Alfonseca et al, 2006a).
Our procedure for calculating the patterns? preci-
sions is as follows:
1. For every relation,
(a) For every hook, collect a hook corpus
from the web.
51
Relation name Seed-list Cardinality Hook-type Target-type Web queries
birth year birth-date.txt n:1 entity entity $1 was born in $2
death year death-date.txt n:1 entity entity $1 died in $2
birth place birth-place.txt n:1 entity entity $1 was born in $2
country-capital country-capital.txt 1:1 entity entity $2 is the capital of $1
author-book author-book.txt n:n entity unrestricted $1 is the author of $2
director-film director-film.txt 1:n entity unrestricted $1 directed $2, $2 directed by $1
Table 1: Example rows in the input table for the system.
(b) For every target, collect a target corpus
from the web.
2. For every relation r,
(a) For every pattern P , collected during
training, apply it to every hook and tar-
get corpora to extract a set of pairs.
For every pair p = (ph, pt),
? If it appears in the seed list of r, con-
sider it correct.
? If it appears in the seed list of other
relation, consider it incorrect.
? If the hook ph appears in the seed list
of r with a different target, and the
cardinality is 1:1 or n:1, consider it
incorrect.
? If the target pt appears in r?s seed list
with a different hook, and the cardi-
nality is 1:1 or 1:n, incorrect.
? Otherwise, the seed list does not
provide enough information to eval-
uate p, so we perform a test on the
web. For every query provided for r,
the system replaces $1 with ph and
$2 with pt, and sends the query to
Google. The pair is deemed correct
if and only if there is at least one an-
swer.
The precision of P is estimated as the
number of extracted pairs that are sup-
posedly correct divided by the total
number of pairs extracted.
In this step, every pattern that did not apply at
least twice in the hook and target corpora is also
discarded.
3.4 Example
After collecting and generalising patterns for
the relation director-film, we apply each pat-
tern to the hook and target corpora collected
for every relation. Let us suppose that we
want to estimate the precision of the pattern
<target> ?s <hook>
and we apply it to the hook and the target cor-
pora for this relation and for author-book. Pos-
sible pairs extracted are (Woody Allen, Bananas),
(Woody Allen, Without Fears), (Charles Dickens,
A Christmas Carol). Only the first one is correct.
The rote extractor proceeds as follows:
? The first pair appears in the seed list, so it is
considered correct.
? Although Woody Allen appears as hook in the
seed list andWithout Fears does not appear as
target, the second pair is still not considered
incorrect because the directed-by relation has
n:n cardinality.
? The third pair appears in the seed list for
writer-book, so it is directly marked as incor-
rect.
? Finally, because still the system has not made
a decision about the second pair, it queries
Google with the sequences
Woody Allen directed Without Fears
Without Fears directed by Woody Allen
Because neither of those queries provide any
answer, it is considered incorrect.
In this way, it can be expected that the patterns
that are equally applicable to several relations,
such as writer-book, director-film or painter-
picture will attain a low precision because they
will extract many incorrect relations from the cor-
pora corresponding to the other relations.
4 Experiment and results
4.1 Rote extractor settings
The initial steps of the rote extractor follows the
general approach: downloading a training cor-
pus using the seed list and extracting patterns.
The training corpora are processed with a part-
of-speech tagger and a module for Named Entity
Recognition and Classification (NERC) that anno-
tates people, organisations, locations, dates, rela-
tive temporal expressions and numbers (Alfonseca
et al, 2006b), so this information can be included
in the patterns. Furthermore, for each of the terms
in a pair in the training corpora, the system also
52
Birth year:
BOS/BOS <hook> (/( <target> -/- number/entity )/) EOS/EOS
BOS/BOS <hook> (/( <target> -/- number/entity )/) British/JJ writer/NN
BOS/BOS <hook> was/VBD born/VBN on/IN the/DT first/JJ of/IN time expr/entity ,/, <target> ,/, at/IN location/entity ,/, of/IN
BOS/BOS <hook> (/( <target> -/- )/) a/DT web/NN guide/NN
Birth place:
BOS/BOS <hook> was/VBD born/VBN in/IN <target> ,/, in/IN central/JJ location/entity ,/,
BOS/BOS <hook> was/VBD born/VBN in/IN <target> date/entity and/CC moved/VBD to/TO location/entity
BOS/BOS Artist/NN :/, <hook> -/- <target> ,/, location/entity (/( number/entity -/-
BOS/BOS <hook> ,/, born/VBN in/IN <target> on/IN date/entity ,/, worked/VBN as/IN
Author-book:
BOS/BOS <hook> author/NN of/IN <target> EOS/EOS
BOS/BOS Odysseus/NNP :/, Based/VBN on/IN <target> ,/, <hook> ?s/POS epic/NN from/IN Greek/JJ mythology/NN
BOS/BOS Background/NN on/IN <target> by/IN <hook> EOS/EOS
did/VBD the/DT circumstances/NNS in/IN which/WDT <hook> wrote/VBD "/?? <target> "/?? in/IN number/entity ,/, and/CC
Capital-country:
BOS/BOS <hook> is/VBZ the/DT capital/NN of/IN <target> location/entity ,/, location/entity correct/JJ time/NN
BOS/BOS The/DT harbor/NN in/IN <hook> ,/, the/DT capital/NN of/IN <target> ,/, is/VBZ number/entity of/IN location/entity
BOS/BOS <hook> ,/, <target> EOS/EOS
BOS/BOS <hook> ,/, <target> -/- organization/entity EOS/EOS
Figure 1: Example patterns extracted from the training corpus for each several kinds of relations.
stores in a separate file the way in which they are
annotated in the training corpus: the sequences of
part-of-speech tags of every appearance, and the
entity type (if marked as such). So, for instance,
typical PoS sequences for names of authors are
?NNP?1 (surname) and ?NNP NNP? (first name
and surname). A typical entity kind for an author
is person.
In the case that a pair from the seed list is found
in a sentence, a context around the two words in
the pair is extracted, including (a) at most five
words to the left of the first word; (b) all the
words in between the pair words; (c) at most five
words to the right of the second word. The context
never jumps over sentence boundaries, which are
marked with the symbols BOS (Beginning of sen-
tence) and EOS (End of sentence). The two related
concepts are marked as <hook> and <target>.
Figure 1 shows several example contexts extracted
for the relations birth year, birth place, writer-
book and country-capital city.
The approach followed for the generalisation
is the one described by (Alfonseca et al, 2006a;
Ruiz-Casado et al, in press), which has a few
modifications with respect to Ravichandran and
Hovy (2002)?s, such as the use of the wildcard * to
represent any sequence of words, and the addition
of part-of-speech and Named Entity labels to the
patterns.
The input table has been built with the fol-
lowing nineteen relations: birth year, death year,
birth place, death place, author?book, actor?
film, director?film, painter?painting, Employee?
organisation, chief of state, soccer player?team,
1All the PoS examples in this paper are done with Penn
Treebank labels.
Relation Seeds Extr. Gener. Filt.
Birth year 244 2374 4748 30
Death year 216 2178 4356 14
Birth place 169 764 1528 28
Death place 76 295 590 6
Author-book 198 8297 16594 283
Actor-film 49 739 1478 3
Director-film 85 6933 13866 200
Painter-painting 92 597 1194 15
Employee-organisation 62 1667 3334 6
Chief of state 55 1989 3978 8
Soccer player-team 194 4259 8518 39
Soccer team-city 185 180 360 0
Soccer team-manager 43 994 1988 9
Country/region-capital city 222 4533 9066 107
Country/region-area 226 762 1524 2
Country/region-population 288 318 636 3
Country-bordering country 157 6828 13656 240
Country-inhabitant 228 2711 5422 17
Country-continent 197 1606 3212 21
Table 2: Number of seed pairs for each relation,
and number of unique patterns in each step.
soccer team-city, soccer team-manager, country
or region?capital city, country or region?area,
country or region?population, country?bordering
country, country-name of inhabitant (e.g. Spain-
Spaniard), and country-continent. The time re-
quired to build the table and the seed lists was less
than one person-day, as some of the seed lists were
directly collected from web pages.
For each step, the following settings have been
set:
? The size of the training corpus has been set
to 50 documents for each pair in the original
seed lists. Given that the typical sizes of the
lists collected are between 50 and 300 pairs,
this means that several thousand documents
are downloaded for each relation.
? Before the generalisation step, the rote ex-
tractor discards those patterns in which the
hook and the target are too far away to each
other, because they are usually difficult to
generalise. The maximum allowed distance
53
No. Pattern Applied Prec1 Prec2 Real
1
Biography|Hymns|Infography|Life|Love|POETRY|Poetry|Quotations|
Search|Sketch|Woolf|charts|genius|kindness|poets/NN */*
OF|Of|about|by|for|from|like|of/IN <hook> (/( <target> -/-
6 1.00 1.00 1.00
2 "/?? <hook> (/( <target> -/- 4 1.00 1.00 1.00
3
[BOS]/[BOS] <hook> was/VBD born/VBN about|around|in/IN <target>
B.C.|B.C.E|BC/NNP at|in/IN
3 1.00 1.00 1.00
4
[BOS]/[BOS] <hook> was/VBD born/VBN about|around|in/IN <target>
B.C.|B.C.E|BC/NNP at|in/IN location/entity
3 1.00 1.00 1.00
5
[BOS]/[BOS] <hook> was/VBD born/VBN around/IN <target> B.C.E/NNP at/IN
location/entity ,/, a/DT
3 1.00 1.00 1.00
6
[BOS]/[BOS] <hook> was/VBD born/VBN around|in/IN <target> B.C.|B.C.E/NNP
at|in/IN location/entity ,/,
3 1.00 1.00 1.00
7
[BOS]/[BOS] */* ATTRIBUTION|Artist|Author|Authors|Composer|Details|
Email|Extractions|Myth|PAL|Person|Quotes|Title|Topic/NNP :/, <hook> (/(
<target> -/-
3 1.00 1.00 1.00
8
classical/JJ playwrights/NNS of/IN organisation/entity ,/, <hook> was/VBD
born/VBN near/IN location/entity in/IN <target> BCE/NNP ,/, in/IN the/DT
village/NN
3 1.00 1.00 1.00
9 [BOS]/[BOS] <hook> (/( <target> -/- )/) 2 1.00 1.00 1.00
10 [BOS]/[BOS] <hook> (/( <target> -|--/- )/) 2 1.00 1.00 1.00
11 [BOS]/[BOS] <hook> (/( <target> person/entity BC/NNP ;/, Greek/NNP :/, 2 1.00 1.00 1.00
12
ACCESS|AND|Alice|Author|Authors|BY|Biography|CARL|Dame|Don|ELIZABETH|
(...)|web|writer|writerMuriel|years/NNP <hook> (/( <target> -|- -/-
8 0.75 1.00
13 -/- <hook> (/( <target> -/- 3 0.67 1.00 0.67
14 -|--/- <hook> (/( <target> -/- 3 0.67 1.00 0.67
15 [BOS]/[BOS] <hook> (/( <target> -/- 60 0.62 1.00 0.81
16 [BOS]/[BOS] <hook> (/( <target> -/- */* )/) 60 0.62 1.00 0.81
17 [BOS]/[BOS] <hook> (/( <target> -|--/- 60 0.62 1.00 0.81
18 ,|:/, <hook> (/( <target> -/- 32 0.41 0.67 0.28
19 [BOS]/[BOS] <hook> ,/, */* (/( <target> -|--/- 15 0.40 1.00 0.67
20 ,|:|;/, <hook> (/( <target> -|--/- 34 0.38 0.67 0.29
21
AND|Alice|Authors|Biography|Dame|Don|ELIZABETH|Email|Fiction|Frances|
GEORGE|Home|I.|Introduction|Jean|L|Neben|PAL|PAULA|Percy|Playwrights|
Poets|Sir|Stanisaw|Stanislaw|W.|WILLIAM|feedback|history|writer/NNP <hook>
(/( <target> -/-
3 0.33 n/a 0.67
22 AND|Frances|Percy|Sir/NNP <hook> (/( <target> -/- 3 0.33 n/a 0.67
23
Alice|Authors|Biography|Dame|Don|ELIZABETH|Email|Fiction|Frances|
GEORGE|Home|I.|Introduction|Jean|L|Neben|PAL|PAULA|Percy|Playwrights|
Poets|Sir|Stanisaw|Stanislaw|W.|WILLIAM|feedback|history|writer/NN <hook>
(/( <target> -/-
3 0.33 n/a 0.67
24 [BOS]/[BOS] <hook> ,|:/, */* ,|:/, <target> -/- 7 0.28 0.67 0.43
25 [BOS]/[BOS] <hook> ,|:/, <target> -/- 36 0.19 1.00 0.11
26 [BOS]/[BOS] <hook> ,/, */* (/( <target> )/) 20 0.15 0.33 0.10
27 [BOS]/[BOS] <target> <hook> ,/, 18 0.00 n/a 0.00
28 In|On|on/IN <target> ,/, <hook> grew|was/VBD 17 0.00 0.00 0.00
29 In|On|on/IN <target> ,/, <hook> grew|was|went/VBD 17 0.00 0.00 0.00
30
[BOS]/[BOS] <hook> ,/, */* DE|SARAH|VON|dramatist|novelist|
playwright|poet/NNP (/( <target> -/-
3 0.00 n/a 1.0
TOTAL 436 0.46 0.84 0.54
Table 3: Patterns for the relation birth year, results extracted by each, precision estimated with this
procedure and with the traditional hook corpus approach, and precision evaluated by hand).
between them has been set to 8 words.
? At each step, the two most similar patterns
are generalised, and their generalisation is
added to the set of patterns. No pattern is dis-
carded at this step. This process stops when
all the patterns resulting from the generalisa-
tion of existing ones contain wildcards adja-
cent to either the hook or the target.
? For the precision estimation, for each pair in
the seed lists, 50 documents are collected for
the hook and other 50 for the target. Because
of time constraints, and given that the total
size of the hook and the target corpora ex-
ceeds 100,000 documents, for each pattern a
sample of 250 documents is randomly cho-
sen and the patterns are applied to it. This
sample is built randomly but with the fol-
lowing constraints: there should be an equal
amount of documents selected from the cor-
pora from each relationship; and there should
be an equal amount of documents from hook
corpora and from target corpora.
4.2 Output obtained
Table 2 shows the number of patterns obtained for
each relation. Note that the generalisation proce-
dure applied produces new (generalised) patterns
to the set of original patterns, but no original pat-
tern is removed, so they all are evaluated; this is
why the set of patterns increases after the gener-
alisation. The filtering criterion was to keep the
patterns that applied at least twice on the test cor-
pus.
It is interesting to see that for most relations the
reduction of the pruning is very drastic. This is
because of two reasons: Firstly, most patterns are
far too specific, as they include up to 5 words at
each side of the hook and the target, and all the
words in between. Only those patterns that have
generalised very much, substituting large portions
with wildcards or disjunctions are likely to apply
to the sentences in the hook and target corpora.
54
Secondly, the samples of the hook and target cor-
pora used are too small for some of the relations
to apply, so few patterns apply more than twice.
Note that, for some relations, the output of the
generalisation step contains less patterns that the
output of the initial extraction step: that is due to
the fact that the patterns in which the hook and
the target are not nearby were removed in between
these two steps.
Concerning the precision estimates, a full eval-
uation is provided for the birth-year relation. Ta-
ble 3 shows in detail the thirty patterns obtained.
It can also be seen that some of the patterns with
good precision contain the wildcard *. For in-
stance, the first pattern indicates that the presence
of any of the words biography, poetry, etc. any-
where in a sentence before a person name and a
date or number between parenthesis is a strong in-
dication that the target is a birth year.
The last columns in the table indicate the num-
ber of times that each rule applied in the hook and
target corpora, and the precision of the rule in each
of the following cases:
? As estimated by the complete program
(Prec1).
? As estimated by the traditional hook cor-
pus approach (Prec2). Here, cardinality is
not taken into account, patterns are evaluated
only on the hook corpora from the same rela-
tion, and those pairs whose hook is not in the
seed list are ignored.
? The real precision of the rule (real). In or-
der to obtain this metric, two different an-
notators evaluated the pairs applied indepen-
dently, and the precision was estimated from
the pairs in which they agreed (there was a
96.29% agreement, Kappa=0.926).
As can be seen, in most of the cases our procedure
produces lower precision estimates.
If we calculate the total precision of all the rules
altogether, shown in the last row of the table, we
can see that, without the modifications, the whole
set of rules would be considered to have a total
precision of 0.84, while that estimate decreases
sharply to 0.46 when they are used. This value
is nearer the precision of 0.54 evaluated by hand.
Although it may seem surprising that the precision
estimated by the new procedure is even lower than
the real precision of the patterns, as measured by
hand, that is due to the fact that the web queries
consider unknown pairs as incorrect unless they
Relation Prec1 Prec2 Real
Birth year 0.46 [0.41,0.51] 0.84 [0.81,0.87] 0.54 [0.49,0.59]
Death year 0.29 [0.24,0.34] 0.55 [0.41,0.69] 0.38 [0.31,0.44]
Birth place 0.65 [0.62,0.69] 0.36 [0.29,0.43] 0.84 [0.79,0.89]
Death place 0.82 [0.73,0.91] 1.00 [1.00,1.00] 0.96 [0.93,0.99]
Author-book 0.07 [0.07,0.07] 0.26 [0.19,0.33] 0.03 [0.00,0.05]
Actor-film 0.07 [0.01,0.13] 1.00 [1.00,1.00] 0.02 [0.00,0.03]
Director-film 0.03 [0.03,0.03] 0.26 [0.18,0.34] 0.01 [0.00,0.01]
Painter-painting 0.10 [0.07,0.12] 0.35 [0.23,0.47] 0.17 [0.12,0.22]
Employee-organisation 0.31 [0.22,0.40] 1.00 [1.00,1.00] 0.33 [0.26,0.40]
Chief of state 0.00 [0.00,0.00] - 0.00 [0.00,0.00]
Soccer player-team 0.07 [0.06,0.08] 1.00 [1.00,1.00] 0.08 [0.04,0.12]
Soccer team-city - - -
Soccer team-manager 0.61 [0.53,0.69] 1.00 [1.00,1.00] 0.83 [0.77,0.88]
Country/region-capital city 0.12 [0.11,0.13] 0.23 [0.22,0.24] 0.12 [0.07,0.16]
Country/region-area 0.09 [0.00,0.19] 1.00 [1.00,1.00] 0.06 [0.02,0.09]
Country/region-population 1.00 [1.00,1.00] 1.00 [1.00,1.00] 1.00 [1.00,1.00]
Country-bordering country 0.17 [0.17,0.17] 1.00 [1.00,1.00] 0.15 [0.10,0.20]
Country-inhabitant 0.01 [0.00,0.01] 0.80 [0.67,0.93] 0.01 [0.00,0.01]
Country-continent 0.16 [0.14,0.18] 0.07 [0.04,0.10] 0.00 [0.00,0.01]
Table 4: Precision estimates for the whole set of
extracted pairs by all rules and all relations.
appear in the web exactly in the format of the
query in the input table. Specially for not very
well-known people, we cannot expect that all of
them will appear in the web following the pattern
?X was born in date?, so the web estimates tend
to be over-conservative.
Table 4 shows the precision estimates for every
pair extracted with all the rules using both proce-
dures, with 0.95 confidence intervals. The real
precision has been estimating by sampling ran-
domly 200 pairs and evaluating them by hand, as
explained above for the birth year relation. As can
be observed, out of the 19 relations, the precision
estimate of the whole set of rules for 11 of them
is not statistically dissimilar to the real precision,
while that only holds for two relationships using
the previous approach.
Please note as well that the precisions indicated
in the table refer to all the pairs extracted by all the
rules, some of which are very precise, but some of
which are very imprecise. If the rules are to be
applied in an annotation system, only those with
a high precision estimate would be used, and ex-
pectedly much better overall results would be ob-
tained.
5 Conclusions and future work
We have described here a new procedure for es-
timating the precision of the patterns learnt by a
rote extractor that learns from the web. Compared
to other similar approaches, it has the following
improvements:
? For each pair (hook,target) in the seed list, a
target corpora is also collected (apart from
the hook corpora), and the evaluation is per-
formed using corpora from several relations.
55
This has been observed to improve the esti-
mate of the rule?s precision, given that the
evaluation pairs not only refer to the elements
in the seed list.
? The cardinality of the relations is taken into
consideration in the estimation process using
the seed list. This is important, for instance,
to be able to estimate the precision in n:n re-
lations like author-work, given that we can-
not assume that the only books written by
someone are those in the seed list.
? For those pairs that cannot be evaluated using
the seed list, a simple query to the Google
search engine is employed.
The precisions estimated with this procedure
are significantly lower than the precisions obtained
with the usual hook corpus approach, specially for
ambiguous patterns, and much near the precision
estimate when evaluated by hand.
Concerning future work, we plan to estimate the
precision of the patterns using the whole hook and
target corpora, rather than using a random sample.
A second objective we have in mind is not to throw
away the ambiguous patterns with low precision
(e.g. the possessive construction), but to train a
model so that we can disambiguate which is the
relation they are conveying in each context (Girju
et al, 2003).
References
E. Agichtein and L. Gravano. 2000. Snowball: Ex-
tracting relations from large plain-text collections.
In Proceedings of ICDL, pages 85?94.
E. Alfonseca, P. Castells, M. Okumura, and M. Ruiz-
Casado. 2006a. A rote extractor with edit distance-
based generalisation and multi-corpora precision
calculation. In Poster session of ACL-2006.
E. Alfonseca, A. Moreno-Sandoval, J. M. Guirao, and
M. Ruiz-Casado. 2006b. The wraetlic NLP suite.
In Proceedings of LREC-2006.
M. Berland and E. Charniak. 1999. Finding parts in
very large corpora. In Proceedings of ACL-99.
S. Brin. 1998. Extracting patterns and relations from
the World Wide Web. In Proceedings of the WebDB
Workshop at EDBT?98.
R. Bunescu and R. J. Mooney. 2005. A shortest path
dependency kernel for relation extraction. In Pro-
ceedings of the HLT Conference and EMNLP.
P. Cimiano, S. Handschuh, and S. Staab. 2004. To-
wards the self-annotating web. In Proceedings of the
13th World Wide Web Conference, pages 462?471.
M. Craven, D. DiPasquo, D. Freitag, A. McCallum,
T. Mitchell, K. Nigam, and S. Slattery. 1999. Learn-
ing to construct knowledge bases from the world
wide web. Artificial Intelligence, 118(1?2):69?113.
O. Etzioni, M. Cafarella, D. Downey, A.-M. Popescu,
T. Shaked, S. Soderland, D. S. Weld, and A. Yates.
2005. Unsupervised named entity extraction from
the web: An experimental study. Artificial Intelli-
gence, 165(1):91?134.
M. Finkelstein-Landau and E. Morin. 1999. Extracting
semantic relationships between terms: supervised
vs. unsupervised methods. In Workshop on Ontolo-
gial Engineering on the Global Info. Infrastructure.
R. Girju, A. Badulescu, and D. Moldovan. 2003.
Learning semantic constraints for the automatic dis-
covery of part-whole relations. In HLT-NAACL-03.
M. Hearst. 1992. Automatic acquisition of hyponyms
from large text corpora. In COLING-92.
S. Huffman. 1995. Learning information extraction
patterns from examples. In IJCAI-95 Workshop on
New Approaches to Learning for NLP.
J. Kietz, A. Maedche, and R. Volz. 2000. A method
for semi-automatic ontology acquisition from a cor-
porate intranet. In Workshop ?Ontologies and text?.
G. S. Mann and D. Yarowsky. 2003. Unsupervised
personal name disambiguation. In CoNLL-2003.
G. S. Mann and D. Yarowsky. 2005. Multi-field in-
formation extraction and cross-document fusion. In
Proceedings of ACL 2005.
E. Morin and C. Jacquemin. 1999. Projecting corpus-
based semantic links on a thesaurus. In ACL-99.
D. Ravichandran and E. Hovy. 2002. Learning surface
text patterns for a question answering system. In
Proceedings of ACL-2002, pages 41?47.
E. Riloff and M. Schmelzenbach. 1998. An empirical
approach to conceptual case frame acquisition. In
Proceedings of WVLC, pages 49?56.
E. Riloff. 1996. Automatically generating extraction
patterns from untagged text. In AAAI.
M. Ruiz-Casado, E. Alfonseca, and P. Castells. in
press. Automatising the learning of lexical patterns:
an application to the enrichment of WordNet by ex-
tracting semantic relationships from the Wikipedia.
Data and Knowledge Engineering, in press.
S. Soderland. 1999. Learning information extraction
rules for semi-structured and free text. Machine
Learning, 34(1?3):233?272.
I. Szpektor, H. Tanev, I. Dagan, and B. Coppola. 2004.
Scaling web-based acquisition of entailment rela-
tions. In Proceedings of EMNLP 2004.
S. Zhao and R. Grishman. 2005. Extracting relations
with integrated information using kernel methods.
In Proceedings of ACL-2005.
56
Proceedings of the 4th International Workshop on Semantic Evaluations (SemEval-2007), pages 318?321,
Prague, June 2007. c?2007 Association for Computational Linguistics
TITPI: Web People Search Task
Using Semi-Supervised Clustering Approach
Kazunari Sugiyama
Precision and Intelligence Laboratory
Tokyo Institute of Technology
4259 Nagatsuta, Midori, Yokohama,
Kanagawa 226-8503, Japan
sugiyama@lr.pi.titech.ac.jp
Manabu Okumura
Precision and Intelligence Laboratory
Tokyo Institute of Technology
4259 Nagatsuta, Midori, Yokohama,
Kanagawa 226-8503, Japan
oku@pi.titech.ac.jp
Abstract
Most of the previous works that disam-
biguate personal names in Web search re-
sults employ agglomerative clustering ap-
proaches. However, these approaches tend
to generate clusters that contain a single el-
ement depending on a certain criterion of
merging similar clusters. In contrast to such
previous works, we have adopted a semi-
supervised clustering approach to integrate
similar documents into a labeled document.
Moreover, our proposed approach is char-
acterized by controlling the fluctuation of
the centroid of a cluster in order to generate
more accurate clusters.
1 Introduction
Personal names are often submitted to search en-
gines as query keywords, as described in a report1
indicating that about 10% of the English queries
from the search engine ALLTheWeb2 contain per-
sonal names. However, in response to a personal
name query, search engines return a long list of
search results containing that contains Web pages
about several namesakes. For example, when a
user submits a personal name like ?William Cohen?
as a query to the search engine Google3, the re-
turned results represent more than one person named
?William Cohen.? In the results, a computer sci-
ence professor, an American politician, a surgeon,
1http://tap.stanford.edu/PeopleSearch.pdf
2http://www.alltheweb.com/
3http://www.google.com/
and others are not classified into separate clusters
but mixed together.
Most of the previous works on disambiguating
personal names in Web search results employ sev-
eral kinds of agglomerative clustering approach as
described in Section 2. However, in these ap-
proaches, a lot of clusters that contain only one el-
ement tend to be generated, depending on a certain
criterion for merging similar clusters. In addition,
in person search results from the World Wide Web
(WWW), we can often observe that a small num-
ber of entities have a lot of search-result Web pages,
while others have only one or two. In light of these
facts, if a labeled Web page that describes a person
is introduced, clustering for personal name disam-
biguation would be much more accurate. In the fol-
lowing, we refer to such a labeled Web page as the
?seed page.? Then, in order to disambiguate per-
sonal names in Web search results, we introduce
semi-supervised clustering that uses the seed page
to aid the clustering of unlabeled search-result Web
pages. Our semi-supervised clustering approach is
characterized by controlling the fluctuation of the
centroid of a cluster.
2 Related Work
(Mann and Yarowsky, 2003) first extract biographi-
cal information, such as birthdates, birthplaces, oc-
cupations, and so on. Then, for each document,
they generate a feature vector composed of the ex-
tracted biographical information, proper nouns, and
the TF-IDF score computed from the documents in
the search results. Finally, using this feature vec-
tor, they disambiguate personal names by generating
clusters based on a bottom-up centroid agglomera-
318
tive clustering algorithm. (Wan et al, 2005) employ
an approach similar to that of (Mann and Yarowsky,
2003), and have developed a system called Web-
Hawk.
(Pedersen et al, 2005) recently proposed a
method for discriminating names by clustering the
instances of a given name into groups. They ex-
tract the context of each instance of an ambiguous
name and generate second-order context vectors us-
ing significant bigrams. The vectors are then clus-
tered such that instances that are similar to each
other are grouped into the same cluster.
(Bekkerman and McCallum, 2005) propose the
following three unsupervised approaches: (1) an
approach based on the hyperlink structures of
Web pages; (2) an approach based on agglomera-
tive/conglomerative double clustering (Bekkerman
et al, 2005); and (3) a hybrid approach combining
the first two.
(Bollegala et al, 2006) first agglomeratively clus-
ter a set of documents and then select key phrases
from the resulting clusters to distinguish different
namesakes. They extract key phrases from the doc-
uments and merge the clusters according to the sim-
ilarity between the extracted phrases.
3 Our Proposed Approach
In this section, we first review the pure agglomera-
tive clustering approach that most of the previous re-
lated works employ and then describe our proposed
semi-supervised clustering approach.
In the following discussion, we denote the feature
vector   of a search-result Web page  in a set of
search results as follows:
 
	
 


 


 
 (1)
where  is the number of distinct terms in the Web
page  , and Producing More tleadable Extracts by Revising Them 
Hidetsugu Nanba (~) and Manabu Okumura  ( f ,  ~) 
~School of Informatioi1 Science 
Japan  Advanced  Ins t i tu te  of Science and : t 'echnology 
++Precision and inte l l igence Laboratory  
Tokyo  Ins t i tu te  of  Techno logy  
nanbaOj  a i s t .  ac .  jp ,  oku?p: .  "c2"cech. ac .  jp  
Abstract 
In this paper, we first experimentally investigated 
the factors that make extracts hard to read. We did 
this by having human subjects try to revise extracts 
to produce more readable ones. We then classified 
the factors into five, most of which are related to 
cohesion, after which we devised revision rnles for 
each factor, and partially implemented a system that 
revises extracts. 
1 In t roduct ion  
The increasing number of on-line texts available has 
resulted in automatic text summarization becom- 
iI,g a major research topic in the NLP community. 
The main approach is to extract important sentences 
fiom the texts, and the main task in this approach 
is: that of evaluating the importance of sentences 
\[MIT, 1999\]. This producing of extracts - that is, 
sets of extracted important sentences - is thought o 
be easy, and has therefore long been the main way 
that texts are summarized. As Paice pointed out, 
however, computer-produced extracts tend to suffer 
from a 'lack of cohesion' \[Paice, 1990\]. For example, 
the antecedents corresponding to anaphors in an ex- 
tract are not always included in the extract. This 
often makes tile extracts hard to read. 
In the work described in this paper, we there- 
fore developed a method for making extracts easier 
to read by revising them. We first experimentally 
investigated the factors that make extracts hard to 
read. We did this by having human subjects try to 
revise extracts to produce more readable ones. We 
then classified the factors into five, most of which 
are related to cohesion \[Halliday et al, 1976\], after 
which we devised revision rules tbr each factor, and 
partially implemented a system that revises extract- 
s. We then evaluated our system by comparing its 
revisions with those produced by human subjects 
and also by comparing the readability judgments of 
human subjects between the revised and original ex-- 
tracts. 
In tile following sections we briefly review relat- 
ed works, describe our investigation of what make 
extracts hard to read, and explain our system for 
revising extracts to make tl~em more readable. Fi- 
nally, we describe our evaluation of the system and 
discuss the results of that evaluation. 
2 Re la ted  Works  
Many investigators have tried to measure the read- 
ability of texts \[Klare, 1963\]. Most of them have e-- 
valuated well-formed texts produced by people, and 
used two measures: percentage of familiar words in 
the texts (word level) and the average length of the 
sentences in the texts (syntactic level). These mea- 
sures, however, do not necessarily reflect the actu- 
al readability of computer-produced xtracts. We 
therefore have to take into account other factors that 
might reduce the readability of extracts. 
One of them could be a lack of cohesion. Italli- 
day and ttasan \[ttalliday et al, 1976\] described five 
kinds of cohesion: reference, substitution, ellipsis, 
conjunction, and lexical cohesion. 
Minel \[Minel et al, 1997\] tried to measure the 
readability of extracts in two ways: by counting the 
number of anaphors in an extract that do not have 
antecedents in the extract, and by counting the num- 
ber of sentences which are not included in an extract 
but closely connected to sentences in the extract. 
We therefore regard kinds of cohesion as impor- 
tant in trying to classify tile factors that make ex- 
tracts less readable in the next section. 
One of the notable previous works dealing with 
ways to produce more cohesive extracts is that 
of Paiee \[Paiee, 1990\]. Mathis presented a frame- 
work in which a pair of short sentences are com- 
bined into one to yield a more readable extract 
\[Mathis et al, 197,3\]. We think, however, that none 
of the previous tudies have adequately investigated 
the factors making extracts hard to read. 
Some investigators have compared human- 
produced abstracts with the original texts and inves- 
tigated how people revise texts to produce abstracts 
1071 
\[Kawahara, 1989, Jing, 1999\]. Revision is thought o 
be done for (at least) the following three purposes: 
(1) to shorten texts, 
(2) to change the style of texts, 
(3) to make texts more readable. 
J ing \[aing, 1999\] is trying to implement a human 
summarizat ion model that includes two revision op- 
erations: reduction (1) and combinat ion (3). Mani 
\[Mani et al, 1999\] proposed a revision system that  
uses three operations: el imination (1), aggregation 
(1), and smoothing (1, 3). Mani showed that his 
system can make extracts more informative with- 
out degrading their readabi l i ty.  The present work, 
however, is concerned not with improving readabi l i -  
ty but with improving the informativeness. 
3 Less Readabi l i ty  of Extracts  
To investigate the revision of extracts exper imental -  
ly, we had 12 graduate students produce extracts 
of 25 newspaper articles from the NIHON KE IZA I  
SHINBUN, the average length of which was 30 sen- 
tences. We then asked them to revise the extracts 
(six subjects per extract) .  
We obtained extracts containing 343 revisions, 
made for any of the three purposes l isted in the last 
section. We selected the revisions for readabi l i ty,  
and classified them into 5 categories, by taking into 
account the categories of cohesion by Hal l iday and 
Hasan\[Hal l iday et al, 1976\]. Table 1 shows the sum 
of the investigation. 
Next, we i l lustrate each category of revisions. In 
the examples, darkened sentences are those that are 
not included in extracts,  but are shown for explana- 
tion. The serial number in the original text is also 
shown at the beginning of sentences a
A)  Lack  o f  con junct ive  express ions /presence  
o f  ext raneous  con junct ive  express ions  
The relation between sentences 15 and 16 is ad- 
versative, because there is a conjunctive 'L. b ' \ [ l  
(However)'  at the beginning of sentence 16. But 
because sentence 15 is not in the extract,  ' L  h 'L  
(However)'  is considered unnecessary and should be 
deleted. Conversely, lack of conjunctive xpression- 
s might cause the relation between sentences to be 
difficult to understand. In such a case, a suitable 
conjunctive expression should be added. For these 
tasks, discourse structure analyzer is required. 
We use the following three tags to show revisions. 
< ,,t,~ > F,~ < I ,aa  >: add a new expression ~.  
< ,~t > B z < In,4 >: delete an expression ~.. 
< , , I ,  ~t  > t% < / r , r  >: replace an expression ~3 with B41 
(The company plans to give women more opportu- 
nity to work by employing fidl-time workers.) 
15. ~KmJUI~TIY, h ~ ~ ,?J<, y.~, J :  -) ~" !cV~.:~?lnm 
(Since there have been no similar cases before, the 
project hat women join is now in a hard situation, 
though the company puts hopes on it.) 
16. <del> L b 'L  </del> \[~{~/.~" _ '_O0-~':12"~e(.~ 4.s'~t~ 
)\ f { J~ iiriI~\] Z~,i#l-: t{lJ 6\[i:~.~  if(Ill ~ \]EYb ~~., 5. 
(<del>However,</del> it is making efforts of ref- 
ormation which will be profitable both for the com- 
pany and the female workers.) 
B)  Syntact i c  complex i ty  
2. (fl:flEf~ij;before revision) 
(It is the first project in telecommunication busi- 
ness, which President Kashio wants to be one of 
the central businesses in the future, and it is also 
the preparation for expanding the business to cel- 
lular phone.) 
$ 
(/f'f iE ~{;aft er revision) 
P~ rE- f~ !:~ ?.k. ~ I\] 6 -I~ ~m ~':~,#:.~  ~ I,: ~-~ ~ ~ f~ ~, -~- 
(It is the first project in telecommunication busi- 
nesses, which President Kashio wants to be one of 
the central business in the future.) 
6. 
(It is also the preparation for expanding the busi- 
ness to cellular phone.) 
Longer sentences tend to have a syntact ical ly  
complex structure \[Klare, 1963\], and a long com- 
pound sentence should generally be divided into two 
simpler sentences. It has also been claimed, however, 
that  short coordinate sentences hould be combined 
\[Mathis et al, 1973\]. 
C)  Redundant  repet i t ion  
00_  b,:~;.~,~l: \]\ ~( rb .  
(The new product 'ECHIGO BEST 100' which 
ECHIGO SEIKA released this April is popular a- 
mong housewives.) 
(<rep The company> ECHIGO SEIKA </rep> 
has been making use of NTT Captain system since 
1987.) 
If subjects of adjacent sentences in an extract  are 
the same, as in the above example, readers might 
think they are redundant.  In such a ease, repeated 
expressions hould be omitted or replaced by pro- 
nouns. In this example, the anaphoric expression 
'\[iiJ ~1: (the eoinpany) '  is used instead of the original 
expression. 
1072 
Table 1: Factors of less readabil ity and their revision methods 
~ factors 
-A-- 
B syntactic omplexity 
C 
lack of conjunctive expressions/ 
presence of extraneous 
conjunctive xpressions 
redundant repetition 
lack of information 
revision methods 
add/delete conjunctive expressions 
combine two sentences; divide a sentence into two 
prononfinalizei onfit expressions; 
add demonstratives 
supplement omitted expressions; 
replace anaphors by antecedents; delete anaphors 
required techniques 
discourse structure 
analysis 
anaphora nd 
ellipsis resolution 
add supplementary information information extraction 
lack of adverbial particles; add/delete adverbial particles 
presence of extraneous 
adverbial particles 
D) Lack  of  in fo rmat ion  
~:' :,.-- Y--25. 
(These are the car maker C\[tRYSLER and the com- 
puter maker COMPAC.) 
(We are now in a vicious circle where the layoffs by 
companies discourage consumptions, which in turn 
results in lower sales.) 
9. <del> ~"q~' ( " .  </del> "; 9 4 2. ~- - t ) '  )<~\[,hJL-C 
(<:del>In such a situation,</del> CHRYSLER has 
done well, because its management strategy exactly 
fits the age of low growth.) 
In this example, the referent of '~ l~- ( "  (in such 
a s ituat ion) '  in sentence 9 is sentence 8, which is 
not in the extract. In such a case, there are two 
ways to revise: to replace the anaphoric expression 
with its antecedent, or to delete the expression. The 
re.vision in the example is the latter one. For the 
task, a method for anaphora nd ellipsis resolution 
is required. 
(Masayoshi Son, CEO of Softhank, is now suffering 
from jet lag.) 
3. <:add> '/ 7 I" ';D q'~ <: /add> ~'~:~t:t-~#' ?\[ig~#' 
ROM ~2 f'l{'~ 12 '~' :~ I- 0'sltJi;Yd. 
(CEO Son <add> of Softbank <:/add> is eager to 
sell softwares using CD-ROM, and he think it is a 
big project for his company.) 
In this second example, since 'CEO Son' appears 
without the name of the company in the extrac- 
t, without any background knowledge, we may not 
u:nderstand what company Mr. Son is the CEO 
of. Therefore, the name of the company 'Softbank'  
should be added as the supplementary information. 
The task requires a method for information extrac- 
tion or at least named entity extraction. 
E) lack  of  adverb ia l  par t i c les /presence  of  
ext raneous  adverb ia l  par t i c les  
'2,6. ~,"\["1~ F - :-. ? t/q,~dl-(-J~atil I l~tl,I,jlq~,'l, tl?D~.?-~z~.,.vb 
(It is a good opportunity opromote the mutual un- 
derstanding between Japan and Vietnam that M- 
r. Do MUOI, a chief secretary of Vietnam, visits 
J t~pan.) 
(From a viewpoint of security, Vietnam will be a 
key country in Asia.) 
30..~':}','fib}d 7J~\[(li~C " <del> 4, </del> ~ ,tl-~.~O~.~e2~l~ 
(Japanese government should consider long-term e- 
conomical support<del>, too </del>.) 
In the above example, there is an adverbial parti- 
cle "5 (, too)' and we can find that sentences 29 and 
30 are paratactical. But, because sentence 29 is not 
in the extract, the particle '-L (, too)' is unnecessary 
and should be deleted. 
4 Rev is ion  System 
Our system uses the Japanese publ ic-domain an- 
alyzers JUMAN \[Kurohashi et al, 1998\] and KNP 
\[t(urohashi, 1998\] morphologically and syntactical ly 
analyze an original newspaper article and its extrac- 
t. It then applies revisions rules to the extract re- 
peatedly, with reference to the original text, unti l  no 
rules can revise the extract further. 
4.1 Revision Rules 
Because tile techniques needed for dealing with all 
the categories of revisions dealt with in the previous 
1073 
section were not available, we devised and imple- 
mented revision rules only for factors (A), (C), and 
(D) in Table 1 by using JPerl. 
a) Delet ion of  con junct ive  xpressions 
We prepared a list of 52 conjunctive expres- 
sions, and made it a rule to delete each of them 
whenever the extract does not include the sentence 
that expression is related. To identify the sen- 
tence related to the sentence by the conjunction 
\[Mann et al, 1986\], the system performs partial dis- 
course structure analysis taking into account all sen- 
tences within three sentences of the one containing 
the conjunctive xpression. 
The implementation of our partial discourse 
structure analyzer was based on Fukumoto's dis- 
course structure analyzer \[Fukumoto, 1990\]. It in- 
fers the relationship between two sentences by refer- 
ring to the conjunctive xpressions, topical words, 
and demonstrative words. 
c) Omission of  redundant  expressions 
If subjects (or topical expressions marked with 
topical postposition 'wa') of adjacent sentences in 
an extract were the same, the repeated expressions 
were considered redundant and were deleted. 
d - l )  Delet ion of  anaphors  
To treat anaphora nd ellipsis successfully, we 
would need a mechanism for anaphora nd ellipsis 
resolution (finding the antecedents and omitted ex- 
pressions). Because we have no such mechanism, 
we implement a rule with ad hoc heuristics: If an 
anaphor appears at the beginning of a sentence in 
an extract, its antecedent must be in the preceding 
sentence. Therefore, if that sentence was not in the 
extract, the anaphor was deleted. 
d-2) Supp lement  of  omi t ted  subjects  
If a subject in a sentence in an extract is omit- 
ted, the revision rule supplements he subject from 
the nearest preceding sentence whose subject is not 
omitted in the original text. This rule is implement- 
ed by using heuristics imilar to the above revision 
rule. 
5 Eva luat ion  o f  Rev is ion  Sys- 
tem 
We evaluated our revision system by comparing its 
revisions with those by human subjects (evaluation 
1), and comparing readability judgments between 
the revised and original extracts (evaluation 2). 
5.1 Eva luat ion  1: compar ing  sys tem 
rev is ions  and  human rev is ions  
Because revision is a subjective task, it was not easy 
to prepare an answer set of revisions to which our 
system's revisions could be compared. The revisions 
that more subjects make, however, can be consid- 
ered more reliable and more likely to be necessary. 
When comparing the revisions made by our system 
with those made by human subjects, we therefore 
took into account the degree of agreement among 
subjects. 
For this evaluation, we used 31 newspaper ar- 
ticles (NIHON KEIZAI SHINBUN) and their ex- 
tracts. They were different from the articles used 
for making rules. Fifteen of extracts are taken fronl 
Nomoto's work \[Nomoto et al, 1997\], and the rest 
were made by our group. The average numbers of 
sentences in the original articles and the extracts 
were 25.2 and 5.1. 
Each extract was revised by five subjects who 
had been instructed to revise the extracts to make 
them more readable and had been shown the 5 ex- 
amples in section 3. As a result, we obtained 167 
revisions in total. The results are listed in Table 2. 
Table 2: The number of revisions 
I \]revision methods I total I 
A add(61)/delete(ll) 
conjunctive xpressions 72 
B combine two sentences(2) 
divide a sentence into two(6) 8 
C pronominalize(5); omit expressions(3) 
add demonstratives(8) 16 
D supplement omitted expressions(lI) 
replace anaphors by antecedents(10) 
delete anaphors(15) 36 
add supplementary information(26) 26 
E delete adverbial particles(4) 
add adverbial particles(5) 9 
167 
We compared our system's revisions with the an- 
swer set comprising revisions that more than two 
subjects made. And we used recall (R) and preci~ 
sion (P) as measures of the system's performances. 
( Numberofsystem'srevisions ) 
matched to the answer 
R= Number of revisions in the answer 
( Number ofsystem'srevisions ) 
matched to the answer 
P= Number of systemfs revisions 
Evaluation results are listed in Table 3. As in 
Table 3, the coverage of our revision rules is rather 
small (about 1/4) in the whole set of revisions in 
Table 2. It is true that the experiment is rather small 
and can be considered as less reliable. Though it is 
less reliable, some of the implemented rules can cover 
most of the necessary revisions by human subjects. 
However, precision should be improved. 
1074 
Table 3: Comt)arison between tile revisions by hu- 
nlan aud our system 
re visionrules J\] I-( I P I 
a(total:ll) 2/2 2/5 
c(total:3) 0/0 0/0 
d-l(total:15) 4/5 4/7 
d-2(total:ll) II 2/4 2/10 
5.2 Evaluation 2: colnparillg human 
readabil ity judgments of original 
and revised extracts 
In the second evaluation, using the same 31 texts 
as in evaluation 1, we asked five human subject- 
s to rank the following four kinds extracts in the 
order of readability: the original extract (without 
revision)(NON-REV), human-revised ones (REV-1 
and REV-2), and the one revised by our system 
(REV-AUTO).  REV-1 and REV-2 were respective- 
ly extracts revised in the cases where more than one 
and more than two subjects agreed to revise. 
We considered ajudgment by tile majority (more 
than two subjects) to be reliable. The results are 
listed in Table 4. The column 'split' in Table 4 indi- 
cates the number of cases where no majority could 
agree. The results show that both REV-1 and REV- 
2 extracts were more readable than NON-REV ex- 
tracts and that REV-2 extracts might be better than 
REV-1 extracts, since the number of 'worse' evalua- 
tions was smaller for REV-2 extracts. 
Table 4: Comparison of readability among original 
extracts and revised ones 
\]\] better same \] worse \] split 
REV-2vs. NON \] 15 1: 2 2 
REV-1 vs. NON 22 7 1 
AUTO vs. NON \[\[ 2 13 \[ 12 \] 0 
In comparing REV-AUTO with NON-REV,  we 
use 27 texts where the readability does not de- 
grade in REV-2, since the readability cannot im- 
prove with revisions by our system in those texts 
where the readability degrades even with human re- 
visions. Even with those texts, however, in ahnost 
half the cases, the readability of the revised extrac- 
t was worse than that of the original extract. The 
main reason is that the revision system supplement- 
ed incorrect subjects. 
6 D iscuss ion  
Although the results of the evaluation are encour- 
aging, they also show that our system needs to be 
improved. We have to impleinent inore revision rules 
to enlarge the coverage of our system. One of the 
most frequent revisions is to add conjunctions(37%). 
We also need to reform our revision rules into more 
thorough implementation. To improve our system, 
we think it is necessary to develop a robust discourse 
structure analyzer, a robust mechanism for anapho- 
ra and ellipsis resolution, and a robust system of 
extracting named entities. They are under develop- 
lllent now. 
7 Conc lus ion  
In this paper we described our investigation of the 
factors that make extracts less readable than they 
should be. We had human subjects revise extracts 
to made them more readable, and we classified the 
factors into five categories. We then devised revision 
rules for three of these factors and iinplemented a
system that uses them to revise extracts. We found 
experimentally that our revision system can improve 
the readability of extracts. 
References  
\[Fukumoto, 1990\] Fukumoto, J. (1990) Context Struc- 
ture Analysis Based on the Writer's Insistence. IPSJ 
SIG Notes, NL-78-15, pp.113-120, in Japanese. 
\[ttalliday et al, 1976\] ttalliday, M.A.K., Hasan,R. 
(1976) Cohesion in English. Longman. 
\[Jing, 1999\] Jing,H. (1999) Sunmmry Generation 
through Intelligent Cutting and Pasting of the Input 
Document. Ph.D. Thesis Proposal, Columbia Univ. 
\[Kawahara, 1989\] Kawahara,H. (1989) Chapter 9, in 
Bunshoukouzou to youyakubun o shosou. Kuroshio- 
shuppan, pp.141-167, in Japanese. 
\[Klare, 1963\] Klare,G.R. (1963} The Measurement of 
Readability. Iowa State University Press. 
\[Kurohashi et al, 1998\] Kurohashi,S., Nagao,M. (1998) 
Japanese Morphological Analysis System JUMAN 
version 3.5. 
\[Kurohashi, 1998\] I(urohashi, S. (1908) Japanese parser 
KNP version 2.0 b6. 
\[Mathis et al, 1973\] 
Mathis,B., I/.ush,J., Young,C. (1973) hnprovement of
Automatic Abstracts by the Use of Structural Analy~ 
sis. JASIS,24(2),pp.lOl-109. 
\[Mani et al, 1999\] Mani,I., Gates,B., Bloedorn,E. 
(1999) Improving Sununaries by Revising Them. the 
37th Annual Meeting of the ACL, pp.558-565. 
\[MIT, 1999\] Mani,I., Maybury,M.T. (1999) Advances in 
Automatic Text Summarization. MIT Press. 
\[Mann et al, 1986\] Mann,W.C., Thompson,S.A. (1986) 
Rhetorical Structure Theory: Description and Con- 
struction of Text Structure. Proe. of the third Inter- 
national Workshop on Text Generation. 
\[Minel et al, 1997\] Minel,J., Nugier,S., Geralcl,P. (1997) 
How to Appreciate the Quality of Automatic Text 
Summarization? Examples of FAN and MLUCE Pro- 
tocols and their Results on SERAPHIN. Irttelligent 
Sealable Text Summarization, Proe. of a Workshop, 
A CL, pp.25-30. 
\[Nomoto et al, 1997\] Nonmto,T., Matsumoto,Y. (1997) 
The Readability of tIuman Coding and Effects on Au- 
tmnatic Abstracting. IPSJ SIG Notes, NL-120-11, 
pp.71-76, in Japanese. 
\[Paice, 19901 Paice,C.D. (1990) Constructing Literature 
Abstracts by Comlmter: Techniques and Prospects. 
Info. Proe. gJ Manage., 26 (1), pp. 171-186. 
1075 
Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 832?842,
MIT, Massachusetts, USA, 9-11 October 2010. c?2010 Association for Computational Linguistics
An Efficient Algorithm for Unsupervised Word Segmentation with
Branching Entropy and MDL
Valentin Zhikov
Interdisciplinary Graduate School
of Science and Engineering
Tokyo Institute of Technology
zhikov@lr.pi.titech.ac.jp
Hiroya Takamura
Precision and Intelligence Laboratory
Tokyo Institute of Technology
takamura@pi.titech.ac.jp
Manabu Okumura
Precision and Intelligence Laboratory
Tokyo Institute of Technology
oku@pi.titech.ac.jp
Abstract
This paper proposes a fast and simple unsuper-
vised word segmentation algorithm that uti-
lizes the local predictability of adjacent char-
acter sequences, while searching for a least-
effort representation of the data. The model
uses branching entropy as a means of con-
straining the hypothesis space, in order to ef-
ficiently obtain a solution that minimizes the
length of a two-part MDL code. An evaluation
with corpora in Japanese, Thai, English, and
the ?CHILDES? corpus for research in lan-
guage development reveals that the algorithm
achieves an accuracy, comparable to that of
the state-of-the-art methods in unsupervised
word segmentation, in a significantly reduced
computational time.
1 Introduction
As an inherent preprocessing step to nearly all NLP
tasks for writing systems without orthographical
marking of word boundaries, such as Japanese and
Chinese, the importance of word segmentation has
lead to the emergence of a micro-genre in NLP fo-
cused exclusively on this problem.
Supervised probabilistic models such as Condi-
tional Random Fields (CRF) (Lafferty et al, 2001)
have a wide application to the morphological anal-
ysis of these languages. However, the development
of the annotated training corpora necessary for their
functioning is a labor-intensive task, which involves
multiple stages of manual tagging. Because of the
scarcity of labeled data, the domain adaptation of
morphological analyzers is also problematic, and
semi-supervised algorithms that address this issue
have also been proposed (e.g. Liang, 2005; Tsuboi
et al, 2008).
Recent advances in unsupervised word segmen-
tation have been promoted by human cognition re-
search, where it is involved in the modeling of the
mechanisms that underlie language acquisition. An-
other motivation to study unsupervised approaches
is their potential to support the domain adaptation of
morphological analyzers through the incorporation
of unannotated training data, thus reducing the de-
pendency on costly manual work. Apart from the
considerable difficulties in discovering reliable cri-
teria for word induction, the practical application
of such approaches is impeded by their prohibitive
computational cost.
In this paper, we address the issue of achiev-
ing high accuracy in a practical computational time
through an efficient method that relies on a combina-
tion of evidences: the local predictability of charac-
ter patterns, and the reduction of effort achieved by
a given representation of the language data. Both of
these criteria are assumed to play a key role in native
language acquisition. The proposed model allows
experimentation in a more realistic setting, where
the learner is able to apply them simultaneously. The
832
method shows a high performance in terms of accu-
racy and speed, can be applied to language samples
of substantial length, and generalizes well to corpora
in different languages.
2 Related Work
The principle of least effort (Zipf, 1949) postulates
that the path of minimum resistance underlies all
human behavior. Recent research has recognized
its importance in the process of language acquisi-
tion (Kit, 2003). Compression-based word induc-
tion models comply to this principle, as they reor-
ganize the data into a more compact representation
while identifying the vocabulary of a text. The min-
imum description length framework (MDL) (Ris-
sanen, 1978) is an appealing means of formalizing
such models, as it provides a robust foundation for
learning and inference, based solely on compres-
sion.
The major problem in MDL-based word segmen-
tation is the lack of standardized search algorithms
for the exponential hypothesis space (Goldwater,
2006). The representative MDL models compare
favorably to the current state-of-the-art models in
terms of accuracy. Brent and Cartwright (1996) car-
ried out an exhaustive search through the possible
segmentations of a limited subset of the data. Yu
(2000) proposed an EM optimization routine, which
achieved a high accuracy, in spite of a lower com-
pression than the gold standard segmentation.
As a solution to the aforementioned issue, the pro-
posed method incorporates the local predictability of
character sequences into the inference process. Nu-
merous studies have shown that local distributional
cues can serve well the purpose of inducing word
boundaries. Behavioral science has confirmed that
infants are sensitive to the transitional probabilities
found in speech (Saffran et al, 1996). The increase
in uncertainty following a given word prefix is a
well studied criterion for morpheme boundary pre-
diction (Harris, 1955). A good deal of research has
been conducted on methods through which such lo-
cal statistics can be applied to the word induction
problem (e.g. Kempe, 1999; Huang and Powers,
2003; Jin and Tanaka-Ishii, 2006). Hutchens and
Adler (1998) noticed that entropic chunking has the
effect of reducing the perplexity of a text.
Most methods for unsupervised word segmenta-
tion based solely on local statistics presume a cer-
tain ? albeit minimum ? level of acquaintance with
the target language. For instance, the model of
Huang and Powers (2003) involves some parame-
ters (Markov chain order, numerous threshold val-
ues) that allow its adaptation to the individuality of
written Chinese. In comparison, the method pro-
posed in this paper generalizes easily to a variety of
languages and domains, and is less dependent on an-
notated development data.
The state-of-the-art in unsupervised word seg-
mentation is represented by Bayesian models. Gold-
water et al (2006) justified the importance of
context as a means of avoiding undersegmentation,
through a method based on hierarchical Dirichlet
processes. Mochihashi et al (2009) proposed ex-
tensions to this method, which included a nested
character model and an optimized inference proce-
dure. Johnson and Goldwater (2009) have proposed
a novel method based on adaptor grammars, whose
accuracy surpasses the aforementioned methods by
a large margin, when appropriate assumptions are
made regarding the structural units of a language.
3 Proposed Method
3.1 Word segmentation with MDL
The proposed two-part code incorporates some ex-
tensions of models presented in related work, aimed
at achieving a more precise estimation of the repre-
sentation length. We first introduce the general two-
part code, which consists of:
? the model, embodied by a codebook, i.e., a lexi-
con of unique word typesM = {w1, ..., w|M |},
? the source text D, obtained through encoding
the corpus using the lexicon.
The total description length amounts to the num-
ber of bits necessary for simultaneous transmission
of the codebook and the source text. Therefore, our
objective is to minimize the combined description
length of both terms:
L(D,M) = L(M) + L(D|M).
The description length of the data given M is cal-
culated using the Shannon-Fano code:
833
L(D|M) = ?
|M |?
j=1
#wj log2 P (wj),
where #wj stands for the frequency of the word wj
in the text.
Different strategies have been proposed in the lit-
erature for the calculation of the codebook cost. A
common technique in segmentation and morphology
induction models is to calculate the product of the
total length in characters of the lexicon and an esti-
mate of the per-character entropy. In this way, both
the probabilities and lengths of words are taken into
consideration. The use of a constant value is an ef-
fective and easily computable approach, but it is far
from precise. For instance, in Yu (2000) the average
entropy per character is measured against the orig-
inal corpus, but this model does not capture the ef-
fects of the word distributions on the observed char-
acter probabilities. For this reason, we propose a
different method: the codebook is modeled as a sep-
arate Markov chain of characters.
A lexicon of characters M ? is defined. The de-
scription length of the lexicon data D? given M ? is
then calculated as:
L(D?|M ?) = ?
|C|?
i=1
#ci log2 P (ci),
where #ci denotes the frequency of a character ci
in the lexicon of hypothesis M . The term L(M ?)
is constant for any choice of hypothesis, as is repre-
sents the character set of a corpus.
The total description length under the proposed
model is thus calculated as:
L(M) + L(D|M) = L(M ?) + L(D?|M ?) + L(D|M) =
?
|C|?
i=1
#ci log2 P (ci)?
|M |?
j=1
#wj log2 P (wj) +O(1).
A rigorous definition should include two addi-
tional terms, L(?|M) and L(??|M ?), which give the
representation cost of the parameters of both mod-
els. The L(?|M) can be calculated as:
L(?|M) =
|M | ? 1
2
? log2 S,
where |M | ? 1 gives the number of parameters (de-
grees of freedom), and S is the size of the dataset
(the total length of the text in characters). The para-
metric complexity term is calculated in the same
way for the lexicon. For a derivation of the above
formula, refer to e.g. Li (1998).
MDL is closely related to Bayesian inference. De-
pending on the choice of a universal code, the two
approaches can overlap, as is the case with the two-
part code discussed in this paper. It can be shown
that the model selection in our method is equiva-
lent to a MAP inference, conducted under the as-
sumption that the prior probability of a model de-
creases exponentially with its length (Goldwater,
2006). Thus, the task that we are trying to accom-
plish is to conduct a focused search through the hy-
pothesis space that will allow us to obtain an approx-
imation of the MAP solution in a reasonable time.
The MDL framework does not provide standard
search algorithms for obtaining the hypotheses that
minimize the description length. In the rest of this
section, we will describe an efficient technique suit-
able for the word segmentation task.
3.2 Obtaining an initial hypothesis
First, a rough initial hypothesis is built by an algo-
rithm that combines the branching entropy and MDL
criteria.
Given a setX , comprising all the characters found
in a text, the entropy of branching at position k of the
text is defined as:
H(Xk|xk?1, ..., xk?n) =
?
?
x?X
P (x|xk?1, ..., xk?n) log2 P (x|xk?1, ..., xk?n),
where xk represents the character found at position
k, and n is the order of the Markov model over char-
acters. For brevity, hereafter we shall denote the ob-
served sequence {xk?1, ..., xk?n} as {xk?1:k?n} .
The above definition is extended to combine the
entropy estimates in the left-to-right and right-to-
left directions, as this factor has reportedly improved
performance figures for models based on branching
entropy (Jin and Tanaka-Ishii, 2006). The estimates
in both directions are summed up, yielding a single
value per position:
834
H ?(Xk;k?1|xk?1:k?n;xk:k+n?1) =
?
?
x?X
P (x|xk?1:k?n) log2 P (x|xk?1:k?n)
?
?
x?X
P (x|xk:k+n?1) log2 P (x|xk:k+n?1).
Suffix arrays are employed during the collection
of frequency statistics. For a character model of or-
der n over a testing corpus of size t and a training
corpus of size m, suffix arrays allow these to be
acquired in O(tn logm) time. Faster implementa-
tions reduce the complexity toO(t(n+logm)). For
further discussion, see Manber and Myers (1991).
During the experiments, we did not use the caching
functionality provided by the suffix array library, but
instead kept the statistics for the current iterative
pass (n-gram order and direction) in a local table.
The chunking technique we adopt is to insert a
boundary when the branching entropy measured in
sequences of length n exceeds a certain threshold
value (H(X|xk?1:k?n) > ?). Both n and ? are fixed.
Within the described framework, the increase in
context length n promotes precision and recall at
first, but causes a performance degradation when the
entropy estimates become unreliable due to the re-
duced frequencies of long strings. High threshold
values produce a combination of high precision and
low recall, while low values result in low precision
and high recall.
Since the F-score curve obtained as decreasing
values are assigned to the threshold is typically uni-
modal as in many applications of MDL, we employ
a bisection search routine for the estimation of the
threshold (Algorithm 1).
All positions of the dataset are sorted by their en-
tropy values. At each iteration, at most two new
hypotheses are built, and their description lengths
are calculated in time linear to the data size. The
computational complexity of the described routine
is O(t log t), where t is the corpus length in charac-
ters.
The order of the Markov chain n used during the
entropy calculation is the only input variable of the
proposed model. Since different values perform the
best across the various languages, the most appro-
priate settings can be obtained with the help of a
small annotated corpus. However, the MDL objec-
tive also enables unsupervised optimization against
Algorithm 1 Generates an initial hypothesis.
thresholds[] := sorted H(Xk) values;
threshold := median of thresholds[];
step := length of thresholds[]/4;
direction := ascending;
minimum := +?;
while step > 0 do
nextThreshold := thresholds[] value one step in last
direction;
DL = calculateDL(nextThreshold);
if DL < minimum then
minimum:= DL; threshold := nextThreshold;
step := step/2; continue;
end if
reverse direction;
nextThreshold := thresholds[] value one step in last
direction;
if DL < minimum then
minimum:= DL; threshold := nextThreshold;
step := step/2; continue;
end if
reverse direction;
step := step/2;
end while
Corpus [1] [2] [3] [4]
CHILDES 394655.52 367711.66 368056.10 405264.53
Kyoto 1.291E+07 1.289E+07 1.398E+07 1.837E+07
Table 1: Length in bits of the solutions proposed by Al-
gorithm 1 with respect to the character n-gram order.
a sufficiently large unlabeled dataset. The order that
minimizes the description length of the data can be
discovered in a few iterations of Algorithm 1 with
increasing values of n, and it typically matches the
optimal value of the parameter (Table 1).
Although an acceptable initial segmentation can
be built using the described approach, it is possible
to obtain higher accuracy with an extended model
that takes into account the statistics of Markov
chains from several orders during the entropy calcu-
lation. This can be done by summing up the entropy
estimates, in the way introduced earlier for combin-
ing the values in both directions:
H ??(Xk;k?1|xk?1:k?n;xk:k+n?1) =
?
nmax?
n=1
(
?
x?X
P (x|xk?1:k?n) log2 P (x|xk?1:k?n)
+
?
x?X
P (x|xk:k+n?1) log2 P (x|xk:k+n?1)),
835
where nmax is the index of the highest order to be
taken into consideration.
3.3 Refining the initial hypothesis
In the second phase of the proposed method, we will
refine the initial hypothesis through the reorganiza-
tion of local co-occurrences which produce redun-
dant description length. We opt for greedy optimiza-
tion, as our primary interest is to further explore the
impact that description length minimization has on
accuracy. Of course, such an approach is unlikely
to obtain global minima, but it is a feasible means of
conducting the optimization process, and guarantees
a certain increase in compression.
Since a preliminary segmentation is available, it
is convenient to proceed by inserting or removing
boundaries in the text, thus splitting or merging the
already discovered tokens. The ranked positions in-
volved in the previous step can be reused here, as
this is a way to bias the search towards areas of
the text where boundaries are more likely to occur.
Boundary insertion should start in regions where the
branching entropy is high, and removal should first
occur in regions where the entropy is close to zero.
A drawback of this approach is that it omits loca-
tions where the gains are not immediately obvious,
as it cannot assess the cumulative gains arising from
the merging or splitting of all occurrences of a cer-
tain pair (Algorithm 2).
A clean-up routine, which compensates for this
shortage, is also implemented (Algorithm 3). It op-
erates directly on the types found in the lexicon pro-
duced by Algorithm 2, and is capable of modify-
ing a large number of occurrences of a given pair
in a single step. The lexicon types are sorted by
their contribution to the total description length of
the corpus. For each word type, splitting or merg-
ing is attempted at every letter, beginning from the
center. The algorithm eliminates unlikely types with
low contribution, which represent mostly noise, and
redistributes their cost among more likely ones. The
design of the merging routine makes it impossible to
produce types longer than the ones already found in
the lexicon, as an exhaustive search would be pro-
hibitive.
The evaluation of each hypothetical change in
the segmentation requires that the description length
of the two-part code is recalculated. In order to
Algorithm 2 Compresses local token co-occurrences.
path[][]:= positions sorted by H(Xk) values;
minimum := DL of model produced at initialization;
repeat
for i = max H(Xk) to min H(Xk) do
pos:= path[i][k];
if no boundary exists at pos then
leftToken := token to the left;
rightToken := token to the right;
longToken := leftToken + rightToken;
calculate DL after splitting;
if DL < minimum then
accept split, update model, update DP vari-
ables;
end if
end if
end for
for i = min H(Xk) to max H(Xk) do
merge leftToken and rightToken into longToken
if DL will decrease (analogous to splitting)
end for
until no change is evident in model
Algorithm 3 A lexicon clean-up procedure.
types[] := lexicon types sorted by cost;
minimum := DL of model produced by Algorithm 2;
repeat
for i = min cost to max cost do
for pos = middle to both ends of types[i] do
longType := types[i];
leftType := sequence from first character to
pos;
rightType:= sequence from pos to last charac-
ter;
calculate DL after splitting longType into left-
Type and rightType;
if DL < minimum then
accept split, update model, update DP vari-
ables;
break out of inner loop;
end if
end for
end for
types[] := lexicon types sorted by cost;
for i = max cost to min cost do
for pos = middle to both ends of types[i] do
merge leftType and rightType into longType if
DL will decrease (analogous to splitting)
break out of inner loop;
end for
end for
until no change is evident in model
836
make this optimization phase computationally fea-
sible, dynamic programming is employed in Algo-
rithms 2 and 3. The approach adopted for the re-
calculation of the source text term L(D|M) is ex-
plained below. The estimation of the lexicon cost is
analogous. The term L(D|M) can be rewritten as:
L(D|M) = ?
|M |?
j=1
#wj log2
#wj
N
=
?
|M |?
j=1
#wj log2 #wj +N log2N = T1 + T2,
where #wj is the frequency of wj in the segmented
corpus, and N =
?|M |
j=1 #wj is the cumulative to-
ken count. In order to calculate the new length, we
keep the values of the terms T1 and T2 obtained at
the last change of the model. Their new values are
computed for each hypothetical split or merge on the
basis of the last values, and the expected description
length is calculated as their sum. If the produced es-
timate is lower, the model is modified and the new
values of T1 and T2 are stored for future use.
In order to maintain precise token counts, Algo-
rithms 2 and 3 recognize the fact that recurring se-
quences (?byebye? etc.) appear in the corpora, and
handle them accordingly. Known boundaries, such
as the sentence boundaries in the CHILDES corpus,
are also taken into consideration.
4 Experimental Settings
We evaluated the proposed model against four
datasets. The first one is the Bernstein-Ratner cor-
pus for language acquisition based on transcripts
from the CHILDES database (Bernstein-Ratner,
1987). It comprises phonetically transcribed utter-
ances of adult speech directed to 13 through 21-
month-old children. We evaluated the performance
of our learner in the cases when the few boundaries
among the individual sentences are available to it
(B), and when it starts from a blank state (N). The
Kyoto University Corpus (Kurohashi and Nagao,
1998) is a standard dataset for Japanese morpho-
logical and dependency structure analysis, which
comprises newspaper articles and editorials from the
Mainichi Shimbun. The BEST corpus for word seg-
mentation and named entity recognition in Thai lan-
guage combines text from a variety of sources in-
Corpus Language Size
(MB)
Chars
(K)
Tokens
(K)
Types
(K)
CHILDES-
B/N
English 0.1 95.8 33.3 1.3
Kyoto Japanese 5.02 1674.9 972.9 39.5
WSJ English 5.22 5220.0 1174.2 49.1
BEST-E Thai 12.64 4360.2 1163.2 26.2
BEST-N Thai 18.37 6422.7 1659.4 36.3
BEST-A Thai 4.59 1619.9 438.7 13.9
BEST-F Thai 16.18 5568.0 1670.8 22.6
Wikipedia Japanese 425.0 169069.3 / /
Asahi Japanese 337.2 112401.1 / /
BEST-All Thai 51.2 17424.0 4371.8 73.4
Table 2: Corpora used during the evaluation. Precise to-
ken and type counts have been omitted for Wikipedia and
Asahi, as no gold standard segmentations are available.
cluding encyclopedias (E), newspaper articles (N),
scientific articles (A), and novels (F). The WSJ sub-
set of the Penn Treebank II Corpus incorporates
selected stories from the Wall Street Journal, year
1989 (Marcus et al, 1994). Both the original text
(O), and a version in which all characters were con-
verted to lower case (L) were used.
The datasets listed above were built by remov-
ing the tags and blank spaces found in the corpora,
and concatenating the remaining text. We added
two more training datasets for Japanese, which were
used in a separate experiment solely for the acqui-
sition of frequency statistics. One of them was
created from 200,000 randomly chosen Wikipedia
articles, stripped from structural elements. The
other one contains text from the year 2005 issues of
Asahi Newspaper. Statistics regarding all described
datasets are presented in Table 2.
One whole corpus is segmented in each experi-
ment, in order to avoid the statement of an extended
model that would allow the separation of training
and test data. This setting is also necessary for the
direct comparison between the proposed model and
other recent methods evaluated against the entire
CHILDES corpus.
We report the obtained precision, recall and F-
score values calculated using boundary, token and
type counts. Precision (P) and recall (R) are defined
as:
P =
#correct units
# output units
, R =
#correct units
#gold standard units
.
Boundary, token and lexicon F-scores, denoted
as B-F and T -F and L-F , are calculated as the
837
Model Corpus & Settings B-Prec B-Rec B-F T-Prec T-Rec T-F DL
(bits)
Ref.DL
(bits)
Time
(ms)
1 CHILDES, ? = 1.2, n = [1-6] 0.8667 0.8898 0.8781 0.6808 0.6990 0.6898 344781.74 1060.2
2a (H?) CHILDES, n = 2 0.7636 0.9109 0.8308 0.5352 0.6384 0.5823 367711.66 300490.52 753.1
2b (H??) CHILDES, nmax = 3 0.8692 0.8865 0.8777 0.6792 0.6927 0.6859 347633.07 885.3
1 Kyoto, ? = 0, n = [1-6] 0.8208 0.8208 0.8208 0.5784 0.5784 0.5784 1.325E+07 54958.8
2a (H?) Kyoto, n = 2 0.8100 0.8621 0.8353 0.5934 0.6316 0.6119 1.289E+07 1.120E+07 22909.7
2b (H??) Kyoto, nmax = 2 0.8024 0.9177 0.8562 0.6093 0.6969 0.6501 1.248+E07 23212.8
Table 3: Comparison of the proposed method (2a, 2b) with the model of Jin and Tanaka-Ishii (2006) (1). Execution
times include the obtaining of frequency statistics, and are represented by averages over 10 runs.
harmonic averages of the corresponding precision
and recall values (F = 2PR/(P + R)). As a
rule, boundary-based evaluation produces the high-
est scores among the three evaluation modes, as it
only considers the correspondence between the pro-
posed and the gold standard boundaries at the indi-
vidual positions of the corpora. Token-based evalua-
tion is more strict ? it accepts a word as correct only
if its beginning and end are identified accurately, and
no additional boundaries lie in between. Lexicon-
based evaluation reflects the extent to which the vo-
cabulary of the original text has been recovered.
It provides another useful perspective for the error
analysis, which in combination with token scores
can give a better idea of the relationship between the
accuracy of induction and item frequency.
The system was implemented in Java, however it
handled the suffix arrays through an external C li-
brary called Sary.1 All experiments were conducted
on a 2 GHz Core2Duo T7200 machine with 2 GB
RAM.
5 Results and Discussion
The scores we obtained using the described instan-
tiations of the branching entropy criterion at the ini-
tialization phase are presented in Table 3, along with
those generated by our own implementation of the
method presented in Jin and Tanaka-Ishii (2006),
where the threshold parameter ? was adjusted man-
ually for optimal performance.
The heuristic of Jin and Tanaka-Ishii takes advan-
tage of the trend that branching entropy decreases
as the observed character sequences become longer;
sudden rises can thus be regarded as an indication of
locations where a boundary is likely to exist. Their
method uses a common value for thresholding the
1http://sary.sourceforge.net
entropy change throughout all n-gram orders, and
combines the boundaries discovered in both direc-
tions in a separate step. These properties of the
method would lead to complications if we tried to
employ it in the first phase of our method (i.e. a step
parameter for iterative adjustment of the threshold
value, rules for combining the boundaries, etc.).
The proposed criterion with an automatically de-
termined threshold value produced slightly worse
results than that of Jin and Tanaka-Ishii at the
CHILDES corpus. However, we found out that our
approach achieves approximately 1% higher score
when the best performing threshold value is selected
from the candidate list. There are two observations
that account for the suboptimal threshold choice by
our algorithm. On one hand, the correspondence
between description length and F-score is not abso-
lutely perfect, and this may pose an obstacle to the
optimization process for relatively small language
samples. Another issue lies in the bisection search
routine, which suggests approximations of the de-
scription length minima. The edge that our method
has on the Kyoto corpus can be attributed to a better
estimation of the optimal treshold value due to the
larger amount of data.
The experimental results obtained at the comple-
tion of Algorithm 3 are summarized in Tables 4 and
5. Presented durations include the obtaining of fre-
quency statistics. The nmax parameter is set to the
value which maximizes the compression during the
initial phase, in order to make the results representa-
tive of the case in which no annotated development
corpora are accessible to the algorithm.
It is evident that after the optimization carried out
in the second phase, the description length is re-
duced to levels significantly lower than the ground
truth. In this aspect, the algorithm outperforms the
EM-based method of Yu (2000).
838
Corpus & Settings B-F T-F L-F Time
(ms)
CHILDES-B, nmax=3 0.9092 0.7542 0.5890 2597.2
CHILDES-N, nmax=3 0.9070 0.7499 0.5578 2949.3
Kyoto, nmax=2 0.8855 0.7131 0.3725 70164.6
BEST-E, nmax=5 0.9081 0.7793 0.3549 738055.0
BEST-N, nmax=5 0.8811 0.7339 0.2807 505327.0
BEST-A, nmax=5 0.9045 0.7632 0.4246 250863.0
BEST-F, nmax=5 0.9343 0.8216 0.4820 305522.0
WSJ-O, nmax=6 0.8405 0.6059 0.3338 658214.0
WSJ-L, nmax=6 0.8515 0.6373 0.3233 582382.0
Table 4: Results obtained after the termination of Algo-
rithm 3.
Corpus & Settings Description
Length (Proposed)
Description
Length (Total)
CHILDES-B, nmax=3 290592.30 300490.52
CHILDES-N, nmax=3 290666.12 300490.52
Kyoto, nmax=2 1.078E+07 1.120E+07
BEST-E, nmax=5 1.180E+07 1.252E+07
BEST-N, nmax=5 1.670E+07 1.809E+07
BEST-A, nmax=5 4438600.32 4711363.62
BEST-F, nmax=5 1.562E+07 1.634E+07
WSJ-O, nmax=6 1.358E+07 1.460E+07
WSJ-L, nmax=6 1.317E+07 1.399E+07
Table 5: Description length - proposed versus reference
segmentation.
We conducted experiments involving various ini-
tialization strategies: scattering boundaries at ran-
dom throughout the text, starting from entirely un-
segmented state, or considering each symbol of the
text to be a separate token. The results obtained
with random initialization confirm the strong rela-
tionship between compression and segmentation ac-
curacy, evident in the increase of token F-score be-
tween the random initialization and the termination
of the algorithm, where description length is lower
(Table 6). They also reveal the importance of the
branching entropy criterion to the generation of hy-
potheses that maximize the evaluation scores and
compression, as well as the role it plays in the re-
duction of computational time.
T-F-Score Description Time
Random Init Refinement Length (ms)
0.0441 (0.25) 0.3833 387603.02 6660.4
0.0713 (0.50) 0.3721 383279.86 4975.1
0.0596 (0.75) 0.2777 412743.67 3753.3
Table 6: Experimental results for CHILDES-N with ran-
domized initialization and search path. The numbers in
brackets represent the seed boundaries/character ratios.
The greedy algorithms fail to suggest any opti-
mizations that improve the compression in the ex-
treme cases when the boundaries/character ratio is
either 0 or 1. When no boundaries are given, split-
ting operations produce unique types with a low
frequency that increase the cost of both parts of
the MDL code, and are rejected. The algorithm
runs slowly, as each evaluation operates on candi-
date strings of enormous length. Similarly, when the
corpus is broken down into single-character tokens,
merging individual pairs does not produce any in-
crease in compression. This could be achieved by an
algorithm that estimates the total effect from merg-
ing all instances of a given pair, but such an algo-
rithm would be computationally infeasible for large
corpora.
Finally, we tried randomizing the search path for
Algorithm 2 after an entropy-guided initialization, to
observe a small deterioration in accuracy in the final
segmentation (less than 1% on average).
Figure 1a illustrates the effect that training data
size has on the accuracy of segmentation for the Ky-
oto corpus. The learning curves are similar through-
out the different corpora. For the CHILDES cor-
pus, which has a rather limited vocabulary, token
F-score above 70% can be achieved for datasets as
small as 5000 characters of training data, provided
that reasonable values are set for the nmax parameter
(we used the values presented in Table 4 throughout
these experiments).
Figure 1b shows the evolution of token F-score by
stage for all corpora. The initialization phase seems
to have the highest contribution to the formation of
the final segmentation, and the refinement phase is
highly dependent on the output it produces. As a
consequence, results improve when a more adequate
language sample is provided during the learning of
local dependencies at initialization. This is evident
in the experiments with the larger unlabeled Thai
and Japanese corpora.
For Japanese language with the setting for the
nmax parameter that maximized compression, we
observed an almost 4% increase in the token F-score
produced at the end of the first phase with the Asahi
corpus as training data. Only a small (less than 1%)
rise was observed in the overall performance. The
quite larger dataset of randomly chosen Wikipedia
articles achieved no improvement. We attributed this
839
Figure 1: a) corpus size / accuracy relationship (Kyoto); b) accuracy levels by phase; c) accuracy levels by phase
with various corpora for frequency statistics (Kyoto); d) accuracy levels by phase with different corpora for frequency
statistics (BEST).
to the higher degree of correspondence between the
domains of the Asahi and Kyoto corpora (Figure 1c).
Experiments with the BEST corpus reveal bet-
ter the influence of domain-specific data on the ac-
curacy of segmentation. Performance deteriorates
significantly when out-of-domain training data is
used. In spite of its size, the assorted composite cor-
pus, in which in-domain and out-of-domain training
data are mixed, produces worse results than the cor-
pora which include only domain-specific data (Fig-
ure 1d).
Finally, a comparison of the proposed method
with Bayesian n-gram models is presented in Ta-
ble 7. Through the increase of compression in the
refinement phase of the algorithm, accuracy is im-
proved by around 3%, and the scores approach those
of the explicit probabilistic models of Goldwater et
al. (2009) and Mochihashi et al (2009). The pro-
posed learner surpasses the other unsupervised word
induction models in terms of processing speed. It
should be noticed that a direct comparison of accu-
racy is not possible with Mochihashi et al (2009),
as they evaluated their system with separate datasets
for training and testing. Furthermore, different seg-
mentation standards exist for Japanese, and there-
fore the ?ground truth? provided by the Kyoto cor-
pus cannot be considered an ideal measure of accu-
racy.
6 Conclusions and Future Work
This paper has presented an efficient algorithm for
unsupervised word induction, which relies on a
combination of evidences. New instantiations of the
branching entropy and MDL criteria have been pro-
posed and evaluated against corpora in different lan-
guages. The MDL-based optimization eliminates
the discretion in the choice of the context length
and threshold parameters, common in segmenta-
tion models based on local statistics. At the same
time, the branching entropy criterion enables a con-
strained search through the hypothesis space, allow-
ing the proposed method to demonstrate a very high
840
Model Corpus T-Prec T-Rec T-F L-Prec L-Rec L-F Time
NPY(3) CHILDES 0.7480 0.7520 0.7500 0.4780 0.5970 0.5310 17 min
NPY(2) CHILDES 0.7480 0.7670 0.7570 0.5730 0.5660 0.5700 17 min
HDP(2) CHILDES 0.7520 0.6960 0.7230 0.6350 0.5520 0.5910 -
Ent-MDL CHILDES 0.7634 0.7453 0.7542 0.6844 0.5170 0.5890 2.60 sec
NPY(2) Kyoto - - 0.6210 - - - -
NPY(3) Kyoto - - 0.6660 - - - -
Ent-MDL Kyoto 0.6912 0.7365 0.7131 0.5908 0.2720 0.3725 70.16 sec
Table 7: Comparison of the proposed method (Ent-MDL) with the methods of Mochihashi et al, 2009 (NPY) and
Goldwater et al, 2009 (HDP).
performance in terms of both accuracy and speed.
Possible improvements of the proposed method
include modeling the dependencies among neigh-
boring tokens, which would allow the evaluation
of the context to be reflected in the cost func-
tion. Mechanisms for stochastic optimization imple-
mented in the place of the greedy algorithms could
provide an additional flexibility of search for such
more complex models. As the proposed approach
provides significant performance improvements, it
could be utilized in the development of more so-
phisticated novel word induction schemes, e.g. en-
semble models trained independently with different
data. Of course, we are also going to explore the
model?s potential in the setting of semi-supervised
morphological analysis.
References
Bernstein-Ratner, Nan 1987. The phonology of parent ?
child speech. Childrens Language, 6:159?174
Brent, Michael R and Timothy A. Cartwright. 1996. Dis-
tributional Regularity and Phonotactic Constraints are
Useful for Segmentation. Cognition 61: 93?125
Goldwater, Sharon. 2006. Nonparametric Bayesian
Models of Lexical Acquisition. Brown University,
Ph.D. Thesis
Goldwater, Sharon, Thomas L. Griffiths and Mark John-
son. 2006. Contextual dependencies in unsupervised
word segmentation. Proceedings of the 21st Interna-
tional Conference on Computational Linguistics and
the 44th annual meeting of the Association for Com-
putational Linguistics, Sydney, 673?680
Goldwater, Sharon, Thomas L. Griffiths and Mark John-
son. 2009. A Bayesian framework for word segmen-
tation: Exploring the effects of context. Cognition,
112:1, 21?54.
Harris, Zellig. 1955. From Phoneme to Morpheme. Lan-
guage, 31(2):190-222.
Huang, Jin H. and David Powers. 2003. Chinese Word
Segmentation Based on Contextual Entropy. Proceed-
ings of 17th Pacific Asia Conference, 152?158
Hutchens, Jason L. and Michael D. Alder. 1998. Finding
structure via compression. Proceedings of the Inter-
national Conference on Computational Natural Lan-
guage Learning, 79?82
Jin, Zhihui and Kumiko Tanaka-Ishii. 2006. Unsuper-
vised Segmentation of Chinese Text by Use of Branch-
ing Entropy. Proceedings of the COLING/ACL on
Main conference poster sessions, 428?435
Johnson, Mark and Sharon Goldwater. 2009. Improving
nonparameteric Bayesian inference: experiments on
unsupervised word segmentation with adaptor gram-
mars. Proceedings of Human Language Technologies:
The 2009 Annual Conference of the North American
Association for Computational Linguistics, 317?325.
Kempe, Andre. 1999. Experiments in Unsupervised
Entropy Based Corpus Segmentation. Proceedings of
CoNLL?99, pp. 371?385
Kit, Chunyu. 2003. How does lexical acquisition begin?
A cognitive perspective. Cognitive Science 1(1): 1?
50.
Kurohashi, Sadao and Makoto Nagao. 1998. Building
a Japanese Parsed Corpus while Improving the Pars-
ing System. Proceedings of the First International
Conference on Language Resources and Evaluation,
Granada, Spain, 719?724
Lafferty, John, Andrew McCallum and Fernando Pereira.
2001. Conditional Random Fields: Probabilistic Mod-
els for Segmenting and Labeling Sequence Data. Pro-
ceedings of the International Conference on Machine
Learning.
Li, Hang. 1998. A Probabilistic Approach to Lexical
Semantic Knowledge Acquisition and Structural Dis-
ambiguation. University of Tokyo, Ph.D. Thesis
Liang, Percy. 2005. Semi-Supervised Learning for Nat-
ural Language. Massachusets Institute of Technology,
Master?s Thesis.
Manber, Udi and Gene Myers. 1991. Suffix arrays: a
new method for on-line string searches. SIAM Journal
on Computing 22:935?948
841
Marcus, Mitchell, Grace Kim, Mary Ann Marcinkiewicz,
Robert MacIntyre, Ann Bies, Mark Ferguson, Karen
Katz and Britta Schasberger. 1994. The Penn Tree-
bank: Annotating Predicate Argument Structure. Hu-
man Language Technology, 114?119
Mochihashi, Daiichi, Takeshi Yamada and Naonori Ueda.
2009. Bayesian unsupervised word segmentation with
nested Pitman-Yor language modeling. Proceedings
of the Joint Conference of the 47th Annual Meeting of
the ACL and the 4th International Joint Conference on
Natural Language Processing of the Asian Federation
of Natural Language Processing, 1: 100?108
Rissanen, Jorma. 1978. Modeling by Shortest Data De-
scription. Aulomatica, 14:465?471.
Saffran, Jenny R., Richard N. Aslin and Elissa L. New-
port. 1996. Statistical learning in 8-month-old infants
Science; 274:1926-1928
Tsuboi, Yuta, Hisashi Kashima., Hiroki Oda, Shinsuke
Mori and Yuji Matsumoto. 2008. Training Condi-
tional Random Fields Using Incomplete Annotations.
Proceedings of the 22nd International Conference on
Computational Linguistics - Volume 1,897?904.
Yu, Hua. 2000. Unsupervised word induction using
MDL criterion. Proceedings of tne International Sym-
posium of Chinese Spoken Language Processing, Bei-
jing.
Zipf, George K. 1949. Human Behavior and the Princi-
ple of Least Effort. Addison-Wesley.
842
Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1213?1223,
Seattle, Washington, USA, 18-21 October 2013. c?2013 Association for Computational Linguistics
Automatic Knowledge Acquisition for Case Alternation
between the Passive and Active Voices in Japanese
Ryohei Sasano1 Daisuke Kawahara2 Sadao Kurohashi2 Manabu Okumura1
1 Precision and Intelligence Laboratory, Tokyo Institute of Technology
2 Graduate School of Informatics, Kyoto University
{sasano,oku}@pi.titech.ac.jp, {dk,kuro}@i.kyoto-u.ac.jp
Abstract
We present a method for automatically acquir-
ing knowledge for case alternation between
the passive and active voices in Japanese. By
leveraging several linguistic constraints on al-
ternation patterns and lexical case frames ob-
tained from a large Web corpus, our method
aligns a case frame in the passive voice to a
corresponding case frame in the active voice
and finds an alignment between their cases.
We then apply the acquired knowledge to a
case alternation task and prove its usefulness.
1 Introduction
Predicate-argument structure analysis is one of the
fundamental techniques for many natural language
applications such as recognition of textual entail-
ment, information retrieval, and machine transla-
tion. In Japanese, the relationship between a pred-
icate and its argument is usually represented by us-
ing case particles1 (Kawahara and Kurohashi, 2006;
Taira et al, 2008; Yoshikawa et al, 2011). However,
since case particles vary depending on the voices,
we have to take case alternation into account to rep-
resent predicate-argument structure. There are thus
two major types of representations: one uses surface
cases, and the other uses normalized-cases for the
base form of predicates. For example, while the Ky-
oto University Text Corpus (Kawahara et al, 2004),
one of the major Japanese corpora that contains an-
notations of predicate-argument structures, adopts
1Japanese is a head-final language. Word order does not
mark syntactic relations. Instead, postpositional case particles
function as case markers.
the former representation, the NAIST Text Corpora
(Iida et al, 2007), another major Japanese corpus,
adopts the latter representation.
Examples (1) and (2) describe the same event in
the passive and active voices, respectively. When
we use surface cases to represent the relationship be-
tween the predicate and its argument in Example (1),
the case of ?? (woman)? is ga2 and the case of ??
(man)? is ni.2 On the other hand, when we use the
normalized-cases for the base form, the case of ??
(woman)? is wo2 and the case of ?? (man)? is ga,
which are the same as the surface cases in the active
voice as in Example (2).
(1) ?? ?? ????????
woman-ga man-ni was pushed down
(A woman was pushed down by a man.)
(2) ?? ?? ???????
man-ga woman-wo pushed down
(A man pushed down a woman.)
Both representations have their own advantages.
Surface case analysis is easier than normalized-case
analysis, especially when we consider omitted ar-
guments, which are also called zero anaphors (Na-
gao and Hasida, 1998). In Japanese, zero anaphora
frequently occurs, and the omitted unnormalized-
case of a zero anaphor is often the same as the
surface case of its antecedent (Sasano and Kuro-
hashi, 2011). Therefore, surface case analysis suits
zero anaphora resolution. On the other hand, when
2Ga, wo, and ni are typical Japanese postpositional case par-
ticles. In most cases, they indicate nominative, accusative, and
dative, respectively.
1213
we focus on the resulting predicate argument struc-
tures, the normalized-case structure is more useful.
Specifically, since a normalized-case structure rep-
resents the same meaning in the same representa-
tion, normalized-case analysis is useful for recog-
nizing textual entailment and information retrieval.
Therefore, we need a system that first analyzes
surface cases and then alternates the surface cases
with normalized-cases. In particular, we focus on
the transformation of the passive voice into the ac-
tive voice in this paper. Passive-to-active voice
transformation in English can be performed system-
atically, which does not depend on lexical infor-
mation in most cases. However, in Japanese, the
method of transformation depends on lexical infor-
mation. For example, while the case particle ni in
Example (1) is alternated with ga in the active voice,
the case particle ni in Example (3) is not alternated in
the active voice as in Example (4) even though both
their predicates are ???????? (be pushed
down).?
(3) ?? ?? ????????
woman-ga sea-ni was pushed down
(A woman was pushed down into the sea.)
(4) ?? ?? ???????
woman-wo sea-ni pushed down
(? pushed down a woman into the sea.)
The ni case in Example (1) indicates agent. On
the other hand, the ni case in Example (3) indicates
direction. To determine the difference is important
for many NLP applications including machine trans-
lation. In fact, Google Translate (GT)3 translates
Examples (1) and (3) as ?Woman was pushed down
in the man? and ?Woman was pushed down in the
sea,? respectively, which may be because GT cannot
distinguish between the roles of ni in Examples (1)
and (3).
(5) ?? ?? ?????
prize-ga man-ni was awarded
(A prize was awarded to a man.)
In example (5), although the ni-case argument
?? (man)? is the same as in Example (1), the case
particle ni indicates recipient and is not alternated
in the active voice. These examples show that case
3http://translate.google.com, accessed 2013-2-20.
alternation between the passive and active voices in
Japanese depends on not only predicates but also ar-
guments, and we have to consider their combina-
tions. Since it is impractical to manually describe
the case alternation rules for all combinations of
predicates and arguments, we have to acquire such
knowledge automatically.
Thus, in this paper, we present a method for ac-
quiring the knowledge for case alternation between
the passive and active voices in Japanese. Our
method leverages several linguistic constraints on al-
ternation patterns and lexical case frames obtained
from a large Web corpus, which are constructed for
each meaning and voice of each predicate.
2 Related Work
Levin (1993) grouped English verbs into classes on
the basis of their shared meaning components and
syntactic behavior, defined in terms of diathesis al-
ternations. Hence, diathesis alternations have been
the topic of interest for a number of researchers
in the field of automatic verb classification, which
aims to induce possible verb frames from corpora
(e.g., McCarthy 2000; Lapata and Brew 2004; Joa-
nis et al 2008; Schulte im Walde et al 2008; Li and
Brew 2008; Sun and Korhonen 2009; Theijssen et al
2012). Baroni and Lenci (2010) used distributional
slot similarity to distinguish between verbs undergo-
ing the causative-inchoative alternations, and verbs
that do not alternate.
There is some work on passive-to-active voice
transformation in Japanese. Baldwin and Tanaka
(2000) empirically identified the range and fre-
quency of basic verb alternation, including active-
passive alternation, in Japanese. They automatically
extracted alternation types by using hand-crafted
case frames but did not evaluate the quality. Kondo
et al (2001) dealt with case alternation between the
passive and active voices as a subtask of paraphras-
ing a simple sentence. They manually introduced
case alternation rules on the basis of verb types and
case patterns and transformed passive sentences into
active sentences.
Murata et al (2006) developed a machine-
learning-based method for Japanese case alterna-
tion. They extracted 3,576 case particles in passive
sentences from the Kyoto University Text Corpus
1214
Case particle Grammatical function
ga nominative
wo accusative
ni dative
de locative, instrumental
kara ablative
no genitive
Table 1: Examples of Japanese postpositional case parti-
cles and their typical grammatical functions.
and tagged their cases in the active voice. Then,
they trained SVM classifiers using the tagged cor-
pus. Their features for training SVM were made
by using several lexical resources such as IPAL
(IPA, 1987), the Japanese thesaurus Bunrui Goi Hyo
(NLRI, 1993), and the output of Kondo et al?s
method.
3 Lexicalized Case Frames
To acquire knowledge for case alternation, we ex-
ploit lexicalized case frames that are automatically
constructed from 6.9 billion Web sentences by using
Kawahara and Kurohashi (2002)?s method. In short,
their method first parses the input sentences, and
then constructs case frames by collecting reliable
modifier-head relations from the resulting parses.
These case frames are constructed for each predi-
cate like PropBank frames (Palmer et al, 2005), for
each meaning of the predicate like FrameNet frames
(Fillmore et al, 2003), and for each voice. However,
neither pseudo-semantic role labels such as Arg1 in
PropBank nor information about frames defined in
FrameNet are included in these case frames. Each
case frame describes surface cases that each predi-
cate has and instances that can fill a case slot, which
is fully lexicalized like the subcategorization lexicon
VALEX (Korhonen et al, 2006).
We list some Japanese postpositional case parti-
cles with their typical grammatical functions in Ta-
ble 1 and show examples of case frames in Table
2.4 Ideally, one case frame is constructed for each
meaning and voice of the target predicate. However,
since Kawahara and Kurohashi?s method is unsuper-
vised, several case frames are actually constructed
4Niyotte in Table 2 is a Japanese functional phrase that in-
dicates agent in this case. We treat niyotte as a case particle in
this paper for the sake of simplicity.
Case Frame: ????????-4 (be pushed down-4)?
{?? (woman):5,? (I):2,? (woman):2, ? ? ? }-ga
{? (sea):229,? (bottom):115,? (pond):51, ? ? ? }-ni
{??(stepmother):2,????(Pegasus):2, ? ? ? }-niyotte
? ? ?
Case Frame: ????????-5 (be pushed down-5)?
{?? (Kyoko):3,?? (manager):1, ? ? ? }-ga
{?? (someone):143,??? (somebody):85, ? ? ? }-ni
{?? (stair):20,? (ship):7,? (cliff):7, ? ? ? }-kara
? ? ?
Case Frame: ??????-2 (push down-2)?
{? (man):14,?? (lion):5,? (tiger):3, ? ? ? }-ga
{?(child):316,??(child):81,?(person):51, ? ? ? }-wo
{? (sea):580,? (ravine):576,? (river):352 ? ? ? }-ni
? ? ?
Case Frame: ??????-4 (push down-4)?
{?? (someone):14,???? (lion):5, ? ? ? }-ga
{? (person):257,? (I):214,? (child):137, ? ? ? }-wo
{? (cliff):53,?? (stair):28, ? ? ? }-kara
? ? ?
Table 2: Examples of case frames for ???????
? (be pushed down)? and ?????? (push down).?
Words in curly braces denote instances that can fill cor-
responding cases and the numbers following these words
denote their frequency in the corpus.
for each meaning and voice. For example, 59 and
eight case frames were respectively constructed for
the predicate in the passive voice ????????
(be pushed down)? and in the active voice ????
?? (push down)? from 6.9 billion Web sentences.
Table 2 shows the 4th and 5th case frames for ???
????? (be pushed down)? and the 2nd and 4th
case frames for ?????? (push down).?
Table 3 shows an example of case frames for
??? (hit),? which includes no-case. Here, the
Japanese postpositional case particle ?no? roughly
corresponds to ?of,? that is, ?X no Y? means ?Y of
X,? and thus no-case is not an argument of the target
predicate. While Kawahara and Kurohashi?s method
basically collects arguments of the target predicate,
the phrase of no-case that modifies the direct object
of the predicate is also collected as no-case. This
is because, as we will show in the next section, this
phrase can be represented as ga-case in the passive
voice.
1215
Case Frame: ???-2 (hit-2)?
{? (man):51,? (fist):30,?? (someone):23, ? ? ? }-ga
{?? (myself):360,? (I):223, ? ? ? }-no
{? (head):5424,? (face):3215, ? ? ? }-wo
{? (fist):316,?? (palm):157,?? (fist):126, ? ? ? }-de
? ? ?
Table 3: An example of case frames for ??? (hit).?
4 Passive-Active Transformation in
Japanese
Morphologically speaking, the passive voice in
Japanese is expressed by using the auxiliary verbs
??? (reru)? and ???? (rareru),? whose past
forms are ??? (reta)? and ???? (rareta),? re-
spectively. For example, the verb in the base form
?????? (tsukiotosu, push down)? is trans-
formed into the past passive form ???????
? (tsukiotosa-reta, was pushed down).? Case al-
ternations accompany passive-active transformation
in Japanese. There are only two case alternations
at most in passive-active transformation. One is the
case represented as ga in the passive voice, and the
other is the case represented as ga in the active voice.
Japanese passive sentences can be classified into
three types in accordance with what is represented
as ga-case in the passive voice: direct passive, in-
direct passive, and possessor passive.
In direct passive sentence, the object of the pred-
icate in the active voice is represented as ga-case.
Examples (1), (3), and (5) are all direct passive sen-
tences. The case that is represented as ga in the ac-
tive voice is usually represented as ni, niyotte, kara,
or de in the passive sentence. In the first sentence of
Examples (6) and (7),5 ga-cases in the active voice
are represented as niyotte and kara, respectively. On
the other hand, ga-case in the passive sentence is al-
ternated with wo or ni as shown with broken lines in
the second sentence of Examples (6) and (7).
(6) P: ???...... ????? ??????
cause-ga..... man-niyotte was identified
(The cause was identified by a man.)
A: ?? ???...... ?????
man-ga cause-wo...... identified
(A man identified the cause.)
5?P? denotes a passive sentence and ?A? denotes the corre-
sponding active sentence in these examples.
(7) P: ??...... ??? ????????
man-ga..... woman-kara was talked to
(A man was talked to by a woman.)
A: ?? ??...... ??????
woman-ga man-ni.... talked to
(A woman talked to a man.)
Indirect passive is also called adversative pas-
sive, in which an indirectly influenced agent is repre-
sented with ga. For example, ?? (I),? the argument
represented with ga in the first sentence of Exam-
ple (8), does not appear in the active voice, i.e. the
second sentence of Example (8). In the case of in-
direct passive, ga-case in the active sentence is al-
ways alternated with ni-case in the passive sentence
as shown with solid lines in Examples (8).
(8) P: ??...... ??? ?????
I-ga..... child-ni was cried
(I?ve got a child crying.)
A: ??? ????(A child cried.)
child-ga cried
Possessor passive is similar to indirect passive in
that the argument represented with ga-case does not
appear as an argument of the predicate in the ac-
tive voice. Therefore, possessor passive is some-
times treated as a kind of indirect passive. How-
ever, in the case of possessor passive, the argument
appears in the active sentence as a possessor of the
direct object. For example, the ga-case argument
?? (woman)? in the passive sentence of Example
(9) does not appear as an argument of the predicate
???? (hit)? in the active sentence but appears in
the phrase that modifies the direct object ?? (head)?
with the case particle no, which indicates that ??
(woman)? is the possessor of ?? (head).?
(9) P: ??...... ?? ?? ?????
woman-ga..... man-ni head-wo was hit
(A woman was hit on the head by a man.)
A:?? ??...... ?? ????
man-ga woman-no..... head-wo hit
(A man hit the head of a woman.)
In conclusion, the number of case alternation pat-
terns accompanying passive-active transformation in
Japanese is limited. Ga-case in the passive voice can
1216
be alternated only with either wo, ni, or no, or does
not appear in the active voice. Ga-case in the active
voice can be represented only by ni, niyotte, kara,
or de in the passive voice. Hence, it is sufficient to
consider only their combinations.
5 Knowledge Acquisition for Case
Alternation
5.1 Task Definition
Our objective is to acquire knowledge for case al-
ternation between the passive and active voices in
Japanese. We leverage lexical case frames obtained
from a large Web corpus by using Kawahara and
Kurohashi (2002)?s method and align cases of a case
frame in the passive voice and cases of a case frame
in the active voice. As described in Section 2, sev-
eral case frames are constructed for each voice of
each predicate. Our task consists of the following
two subtasks:
1. Identify a corresponding case frame in the ac-
tive voice.
2. Find an alignment between cases of case
frames in the passive and active voice.
Figure 1 shows the overview of our task. If a case
frame in the passive voice is input, we identify a cor-
responding case frame in the active voice, and find
an alignment between cases by using the algorithm
described in Section 5.3. In this example, an active
case frame ??????-4 (push down-4)? is iden-
tified as a corresponding case frame for the input
passive case frame ????????-5 (be pushed
down-5)? and ga, ni, and kara-cases in the passive
case frame are aligned to wo, ga, and kara-cases in
the active case frame, respectively.
5.2 Clues for Knowledge Acquisition
We exploit three clues for corresponding case frame
identification and case alignment as follows:
1. Semantic similarity between the instances of
the aligned cases: simSEM .
2. Case distribution similarity between the corre-
sponding case frames: simDIST .
3. Preference of alternation patterns: fPP .
&DVH)UDPH SXVKGRZQ
^DZRUG 	ZRUGV `JD
^
,SHUVRQ`ZR
^ ERWWRPKHOO`QL

&DVH)UDPH SXVKGRZQ
^PDQ OLRQ `JD
^FKLOGFKLOG`ZR
^VHDUDYLQHProceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 153?158,
October 25-29, 2014, Doha, Qatar.
c
?2014 Association for Computational Linguistics
Unsupervised Word Alignment Using Frequency Constraint in Posterior
Regularized EM
Hidetaka Kamigaito
1,2
, Taro Watanabe
2
, Hiroya Takamura
1
, Manabu Okumura
1
1
Tokyo Institute of Technology, Precision and Intelligence Laboratory
4259 Nagatsuta-cho Midori-ku Yokohama, Japan
2
National Institute of Information and Communication Technology
3-5 Hikari-dai, Seika-cho, Soraku-gun, Kyoto, Japan
Abstract
Generative word alignment models, such
as IBM Models, are restricted to one-
to-many alignment, and cannot explicitly
represent many-to-many relationships in
a bilingual text. The problem is par-
tially solved either by introducing heuris-
tics or by agreement constraints such that
two directional word alignments agree
with each other. In this paper, we fo-
cus on the posterior regularization frame-
work (Ganchev et al., 2010) that can force
two directional word alignment models
to agree with each other during train-
ing, and propose new constraints that can
take into account the difference between
function words and content words. Ex-
perimental results on French-to-English
and Japanese-to-English alignment tasks
show statistically significant gains over the
previous posterior regularization baseline.
We also observed gains in Japanese-to-
English translation tasks, which prove the
effectiveness of our methods under gram-
matically different language pairs.
1 Introduction
Word alignment is an important component in sta-
tistical machine translation (SMT). For instance
phrase-based SMT (Koehn et al., 2003) is based
on the concept of phrase pairs that are automat-
ically extracted from bilingual data and rely on
word alignment annotation. Similarly, the model
for hierarchical phrase-based SMT is built from
exhaustively extracted phrases that are, in turn,
heavily reliant on word alignment.
The Generative word alignment models, such as
the IBM Models (Brown et al., 1993) and HMM
(Vogel et al., 1996), are popular methods for au-
tomatically aligning bilingual texts, but are re-
stricted to represent one-to-many correspondence
of each word. To resolve this weakness, vari-
ous symmetrization methods are proposed. Och
and Ney (2003) and Koehn et al. (2003) propose
various heuristic methods to combine two direc-
tional models to represent many-to-many relation-
ships. As an alternative to heuristic methods, fil-
tering methods employ a threshold to control the
trade-off between precision and recall based on
a score estimated from the posterior probabili-
ties from two directional models. Matusov et al.
(2004) proposed arithmetic means of two mod-
els as a score for the filtering, whereas Liang et
al. (2006) reported better results using geometric
means. The joint training method (Liang et al.,
2006) enforces agreement between two directional
models. Posterior regularization (Ganchev et al.,
2010) is an alternative agreement method which
directly encodes agreement during training. DeN-
ero and Macherey (2011) and Chang et al. (2014)
also enforce agreement during decoding.
However, these agreement models do not take
into account the difference in language pairs,
which is crucial for linguistically different lan-
guage pairs, such as Japanese and English: al-
though content words may be aligned with each
other by introducing some agreement constraints,
function words are difficult to align.
We focus on the posterior regularization frame-
work and improve upon the previous work by
proposing new constraint functions that take into
account the difference in languages in terms of
content words and function words. In particular,
we differentiate between content words and func-
tion words by frequency in bilingual data, follow-
ing Setiawan et al. (2007).
Experimental results show that the proposed
methods achieved better alignment qualities on the
French-English Hansard data and the Japanese-
English Kyoto free translation task (KFTT) mea-
sured by AER and F-measure. In translation eval-
uations, we achieved statistically significant gains
153
in BLEU scores in the NTCIR10.
2 Statistical word alignment with
posterior regularization framework
Given a bilingual sentence x = (x
s
,x
t
) where x
s
andx
t
denote a source and target sentence, respec-
tively, the bilingual sentence is aligned by a many-
to-many alignment of y. We represent posterior
probabilities from two directional word alignment
models as
??
p
?
(
??
y |x) and
??
p
?
(
??
y |x) with each ar-
row indicating a particular direction, and use ? to
denote the parameters of the models. For instance,
??
y is a subset of y for the alignment from x
s
to
x
t
under the model of p(x
t
,
??
y |x
s
). In the case of
IBM Model 1, the model is represented as follows:
p(xt,??y |xs) =
?
i
1
|xs|+ 1
p
t
(x
t
i
|xs??
y
i
). (1)
where we define the index of x
t
, x
s
as i, j(1 ?
i ? |x
t
|, 1 ? j ? |x
s
|) and the posterior probabil-
ity for the word pair (x
t
i
, x
s
j
) is defined as follows:
??
p (i, j|x) =
p
t
(x
t
i
|x
s
j
)
?
j
?
p
t
(x
t
i
|x
s
j
?
)
. (2)
Herein, we assume that the posterior probabil-
ity for wrong directional alignment is zero (i.e.,
??
p (
??
y |x) = 0).
1
Given the two directional mod-
els, Ganchev et al. defined a symmetric feature for
each target/source position pair, i, j as follows:
?
i,j
(x,y) =
?
?
?
+1 (
??y ? y) ? (??y
i
= j),
?1 (
??y ? y) ? (??y
j
= i),
0 otherwise.
(3)
The feature assigns 1 for the subset of word align-
ment for
??
y , but assigns ?1 for
??
y . As a result,
if a word pair i, j is aligned with equal posterior
probabilities in two directions, the expectation of
the feature value will be zero. Ganchev et al. de-
fined a joint model that combines two directional
models using arithmetic means:
p
?
(y|x) =
1
2
??
p
?
(y|x) +
1
2
??
p
?
(y|x). (4)
Under the posterior regularization framework, we
instead use q that is derived by maximizing the fol-
lowing posterior probability parametrized by ? for
each bilingual data x as follows (Ganchev et al.,
2010):
q?(y|x) =
??
p
?
(
??y |x) +??p
?
(
??y |x)
2
(5)
?
exp{?? ? ?(x,y)}
Z
1
No alignment is represented by alignment into a special
token ?null?.
=
??
q (
??y |x)
Z??
q
??
p
?
(x) +
??
q (
??y |x)
Z??
q
??
p
?
(x)
2Z
,
Z =
1
2
(
Z
??
q
??
p
?
+
Z
??
q
??
p
?
),
??
q (
??y |x) =
1
Z
??
q
??
p
?
(
??y ,x)exp{?? ? ?(x,y)},
Z
??
q
=
?
??
y
??
p
?
(
??y ,x)exp{?? ? ?(x,y)},
??
q (
??y |x) =
1
Z
??
q
??
p
?
(
??y ,x)exp{?? ? ?(x, y)},
Z
??
q
=
?
??
y
??
p
?
(
??y ,x)exp{?? ? ?(x,y)},
such that E
q? [?i,j(x,y)] = 0. In the E-step of
EM-algorithm, we employ q? instead of p? to ac-
cumulate fractional counts for its use in the M-
step. ? is efficiently estimated by the gradient as-
cent for each bilingual sentence x. Note that pos-
terior regularization is performed during parame-
ter estimation, and not during testing.
3 Posterior Regularization with
Frequency Constraint
The symmetric constraint method represented in
Equation (3) assumes a strong one-to-one rela-
tion for any word, and does not take into account
the divergence in language pairs. For linguisti-
cally different language pairs, such as Japanese-
English, content words may be easily aligned one-
to-one, but function words are not always aligned
together. In addition, Japanese is a pro-drop lan-
guage which can easily violate the symmetric con-
straint when proper nouns in the English side have
to be aligned with a ?null? word. In addition, low
frequency words may cause unreliable estimates
for adjusting the weighing parameters ?.
In order to solve the problem, we improve
Ganchev?s symmetric constraint so that it can con-
sider the difference between content words and
function words in each language. In particular, we
follow the frequency-based idea of Setiawan et al.
(2007) that discriminates content words and func-
tion words by their frequencies. We propose con-
straint features that take into account the differ-
ence between content words and function words,
determined by a frequency threshold.
3.1 Mismatching constraint
First, we propose a mismatching constraint that
penalizes word alignment between content words
and function words by decreasing the correspond-
ing posterior probabilities.
154
The constraint is represented as f2c (function to
content) constraint:
?
f2c
i,j
(x,y) = (6)
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
+1 (
??y ? y) ? (??y
i
= j) ? ((x
t
i
? C
t
? x
s
j
? F
s
)
?(x
t
i
? F
t
? x
s
j
? C
s
)) ? (?
i,j
(x,y) > 0),
0 (
??y ? y) ? (??y
j
= i) ? ((x
t
i
? C
t
? x
s
j
? F
s
)
?(x
t
i
? F
t
? x
s
j
? C
s
)) ? (?
i,j
(x,y) > 0),
0 (
??y ? y) ? (??y
i
= j) ? ((x
t
i
? C
t
? x
s
j
? F
s
)
?(x
t
i
? F
t
? x
s
j
? C
s
)) ? (?
i,j
(x,y) < 0),
?1 (
??y ? y) ? (??y
j
= i) ? ((x
t
i
? C
t
? x
s
j
? F
s
)
?(x
t
i
? F
t
? x
s
j
? C
s
)) ? (?
i,j
(x,y) < 0).
where ?
i,j
(x,y) =
??
p
?
(i, j|x) ?
??
p
?
(i, j|x) is
the difference in the posterior probabilities be-
tween the source-to-target and the target-to-source
alignment. C
s
and C
t
represent content words in
the source sentence and target sentence, respec-
tively. Similarly, F
s
and F
t
are function words
in the source and target sentence, respectively. In-
tuitively, when there exists a mismatch in content
word and function word for a word pair (i, j), the
constraint function returns a non-zero value for
the model with the highest posterior probability.
When coupled with the constraint such that the ex-
pectation of the feature value is zero, the constraint
function decreases the posterior probability of the
highest direction and discourages agreement with
each other.
Note that when this constraint is not fired, we
fall back to the constraint function in Equation (3)
for each word pair.
3.2 Matching constraint
In contrast to the mismatching constraint, our
second constraint function rewards alignment for
function to function word matching, namely f2f.
The f2f constraint function is defined as follows:
?
f2f
i,j
(x,y) = (7)
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
+1 (
??y ? y) ? (??y
i
= j)?
(x
t
i
? F
t
? x
s
j
? F
s
) ? (?
i,j
(x,y) > 0),
0 (
??y ? y) ? (??y
j
= i)?
(x
t
i
? F
t
? x
s
j
? F
s
) ? (?
i,j
(x,y) > 0),
0 (
??y ? y) ? (??y
i
= j)?
(x
t
i
? F
t
? x
s
j
? F
s
) ? (?
i,j
(x,y) < 0),
?1 (
??y ? y) ? (??y
j
= i)?
(x
t
i
? F
t
? x
s
j
? F
s
) ? (?
i,j
(x,y) < 0).
This constraint function returns a non-zero value
for a word pair (i, j) when they are function
words. As a result, the pair of function words
are encouraged to agree with each other, but not
other pairs. The content to content word matching
function c2c can be defined similarly by replac-
ing F
s
and F
t
by C
s
and C
t
, respectively. Like-
wise, the function to content word matching func-
tion f2c is defined by considering the matching
of content words and function words in two lan-
guages. As noted in the mismatch function, when
no constraint is fired, we fall back to Eq (3) for
each word pair.
4 Experiment
4.1 Experimental Setup
The data sets used in our experiments are the
French-English Hansard Corpus, and two data sets
for Japanese-English tasks: the Kyoto free trans-
lation task (KFTT) and NTCIR10. The Hansard
Corpus consists of parallel texts drawn from of-
ficial records of the proceedings of the Canadian
Parliament. The KFTT (Neubig, 2011) is derived
from Japanese Wikipedia articles related to Ky-
oto, which is professionally translated into En-
glish. NTCIR10 comes from patent data employed
in a machine translation shared task (Goto et al.,
2013). The statistics of these data are presented in
Table 1.
Sentences of over 40 words on both source and
target sides are removed for training alignment
models. We used a word alignment toolkit ci-
cada
2
for training the IBM Model 4 with our
proposed methods. Training is bootstrapped from
IBM Model 1, followed by HMM and IBM Model
4. When generating the final bidirectional word
alignment, we use a grow-diag-final heuristic for
the Japanese-English tasks and an intersection
heuristic in the French-English task, judged by
preliminary studies.
Following Bisazza and Federico (2012), we
automatically decide the threshold for word fre-
quency to discriminate between content words and
function words. Specifically, the threshold is de-
termined by the ratio of highly frequent words.
The threshold th is the maximum frequency that
satisfies the following equation:
?
w?(freq(w)>th)
freq(w)
?
w?all
freq(w)
> r. (8)
Here, we empirically set r = 0.5 by preliminary
studies. This method is based on the intuition that
content words and function words exist in a docu-
ment at a constant rate.
4.2 Word alignment evaluation
We measure the impact of our proposed meth-
ods on the quality of word alignment measured
2
https://github.com/tarowatanabe/cicada
155
Table 1: The statistics of the data sets
hansard kftt NTCIR10
French English Japanese English Japanese English
train sentence 1.13M 329.88K 2.02M
word 23.3M 19.8M 6.08M 5.91M 53.4M 49.4M
vocabulary 78.1K 57.3K 114K 138K 114K 183K
dev sentence 1.17K 2K
word 26.8K 24.3K 73K 67.3K
vocabulary 4.51K 4.78K 4.38K 5.04K
test WA sentence 447 582
word 7.76K 7.02K 14.4K 12.6K
vocabulary 1,92K 1.69K 2.57K 2.65K
TR sentence 1.16K 8.6K
word 28.5K 26.7K 334K 310K
vocabulary 4.91K 4.57K 10.4K 12.7K
Figure 1: Precision Recall graph in Hansard
French-English
Figure 2: Precision Recall graph in KFTT
Figure 3: AER in Hansard French-English Figure 4: AER in KFTT
156
Table 2: Results of word alignment evaluation with the heuristics-based method (GDF)
KFTT Hansard (French-English)
method precision recall AER F precision recall AER F
symmetric 0.4595 0.5942 48.18 0.5182 0.7029 0.8816 7.29 0.7822
f2f 0.4633 0.5997 47.73 0.5227 0.7042 0.8851 7.29 0.7844
c2c 0.4606 0.5964 48.02 0.5198 0.7001 0.8816 7.34 0.7804
f2c 0.4630 0.5998 47.74 0.5226 0.7037 0.8871 7.10 0.7848
by AER and F-measure (Och and Ney, 2003).
Since there exists no distinction for sure-possible
alignments in the KFTT data, we use only sure
alignment for our evaluation, both for the French-
English and the Japanese-English tasks. Table 2
summarizes our results.
The baseline method is symmetric constraint
(Ganchev et al., 2010) shown in Table 2. The num-
bers in bold and in italics indicate the best score
and the second best score, respectively. The dif-
ferences between f2f,f2c and baseline in KFTT are
statistically significant at p < 0.05 using the sign-
test, but in hansard corpus, there exist no signifi-
cant differences between the baseline and the pro-
posed methods. In terms of F-measure, it is clear
that the f2f method is the most effective method
in KFTT, and both f2f and f2c methods exceed the
original posterior regularized model of Ganchev et
al. (2010).
We also compared these methods with filtering
methods (Liang et al., 2006), in addition to heuris-
tic methods. We plot precision/recall curves and
AER by varying the threshold between 0.1 and
0.9 with 0.1 increments. From Figures, it can be
seen that our proposed methods are superior to
the baseline in terms of both precision-recall and
AER.
4.3 Translation evaluation
Next, we performed a translation evaluation, mea-
sured by BLEU (Papineni et al., 2002). We
compared the grow-diag-final and filtering method
(Liang et al., 2006) for creating phrase tables.
The threshold for the filtering factor was set to
0.1 which was the best setting in the word align-
ment experiment in section 4.2 under KFTT. From
the English side of the training data, we trained a
word using the 5-gram model with SRILM (Stol-
cke and others, 2002). ?Moses? toolkit was used
as a decoder (Koehn et al., 2007) and the model
parameters were tuned by k-best MIRA (Cherry
and Foster, 2012). In order to avoid tuning insta-
bility, we evaluated the average of five runs (Hop-
kins and May, 2011). The results are summarized
Table 3: Results of translation evaluation
KFTT NTCIR10
GDF Filtered GDF Filtered
symmetric 19.06 19.28 28.3 29.71
f2f 19.15 19.17 28.36 29.74
c2c 19.26 19.02 28.36 29.92
f2c 18.91 19.20 28.36 29.67
in Table 3. Our proposed methods achieved large
gains in NTCIR10 task with the filtered method,
but observed no gain in the KFTT with the filtered
method. In NTCIR10 task with GDF, the gain in
BLEU was smaller than that of KFTT. We cal-
culate p-values and the difference between sym-
metric and c2c (the most effective proposed con-
straint) are lower than 0.05 in kftt with GDF and
NTCIR10 with filtered method. There seems to
be no clear tendency in the improved alignment
qualities and the translation qualities, as shown in
numerous previous studies (Ganchev et al., 2008).
5 Conclusion
In this paper, we proposed new constraint func-
tions under the posterior regularization frame-
work. Our constraint functions introduce a
fine-grained agreement constraint considering the
frequency of words, a assuming that the high
frequency words correspond to function words
whereas the less frequent words may be treated
as content words, based on the previous work of
Setiawan et al. (2007). Experiments on word
alignment tasks showed better alignment quali-
ties measured by F-measure and AER on both the
Hansard task and KFTT. We also observed large
gain in BLEU, 0.2 on average, when compared
with the previous posterior regularization method
under NTCIR10 task.
As our future work, we will investigate more
precise methods for deciding function words and
content words for better alignment and translation
qualities.
157
References
Arianna Bisazza and Marcello Federico. 2012. Cutting
the long tail: Hybrid language models for translation
style adaptation. In Proceedings of the 13th Confer-
ence of the European Chapter of the Association for
Computational Linguistics, pages 439?448. Associ-
ation for Computational Linguistics.
Peter F Brown, Vincent J Della Pietra, Stephen A Della
Pietra, and Robert L Mercer. 1993. The mathemat-
ics of statistical machine translation: Parameter esti-
mation. Computational linguistics, 19(2):263?311.
Yin-Wen Chang, Alexander M. Rush, John DeNero,
and Michael Collins. 2014. A constrained viterbi
relaxation for bidirectional word alignment. In Pro-
ceedings of the 52nd Annual Meeting of the Associa-
tion for Computational Linguistics (Volume 1: Long
Papers), pages 1481?1490, Baltimore, Maryland,
June. Association for Computational Linguistics.
Colin Cherry and George Foster. 2012. Batch tun-
ing strategies for statistical machine translation. In
Proceedings of the 2012 Conference of the North
American Chapter of the Association for Computa-
tional Linguistics: Human Language Technologies,
pages 427?436. Association for Computational Lin-
guistics.
John DeNero and Klaus Macherey. 2011. Model-
based aligner combination using dual decomposi-
tion. In Proceedings of the 49th Annual Meeting of
the Association for Computational Linguistics: Hu-
man Language Technologies, pages 420?429, Port-
land, Oregon, USA, June. Association for Computa-
tional Linguistics.
Kuzman Ganchev, Jo?ao V. Grac?a, and Ben Taskar.
2008. Better alignments = better translations?
In Proceedings of ACL-08: HLT, pages 986?993,
Columbus, Ohio, June. Association for Computa-
tional Linguistics.
Kuzman Ganchev, Joao Grac?a, Jennifer Gillenwater,
and Ben Taskar. 2010. Posterior regularization for
structured latent variable models. The Journal of
Machine Learning Research, 99:2001?2049.
Isao Goto, Ka Po Chow, Bin Lu, Eiichiro Sumita, and
Benjamin K Tsou. 2013. Overview of the patent
machine translation task at the ntcir-10 workshop.
In Proceedings of the 10th NTCIR Workshop Meet-
ing on Evaluation of Information Access Technolo-
gies: Information Retrieval, Question Answering
and Cross-Lingual Information Access, NTCIR-10.
Mark Hopkins and Jonathan May. 2011. Tuning as
ranking. In Proceedings of the 2011 Conference on
Empirical Methods in Natural Language Process-
ing, pages 1352?1362, Edinburgh, Scotland, UK.,
July. Association for Computational Linguistics.
Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical phrase-based translation. In
Proceedings of the 2003 Conference of the North
American Chapter of the Association for Computa-
tional Linguistics on Human Language Technology-
Volume 1, pages 48?54. Association for Computa-
tional Linguistics.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran,
Richard Zens, et al. 2007. Moses: Open source
toolkit for statistical machine translation. In Pro-
ceedings of the 45th Annual Meeting of the ACL
on Interactive Poster and Demonstration Sessions,
pages 177?180. Association for Computational Lin-
guistics.
Percy Liang, Ben Taskar, and Dan Klein. 2006. Align-
ment by agreement. In Proceedings of the Human
Language Technology Conference of the NAACL,
Main Conference, pages 104?111, New York City,
USA, June. Association for Computational Linguis-
tics.
E. Matusov, R. Zens, and H. Ney. 2004. Symmetric
Word Alignments for Statistical Machine Transla-
tion. In Proceedings of COLING 2004, pages 219?
225, Geneva, Switzerland, August 23?27.
Graham Neubig. 2011. The Kyoto free translation
task. http://www.phontron.com/kftt.
Franz Josef Och and Hermann Ney. 2003. A sys-
tematic comparison of various statistical alignment
models. Computational linguistics, 29(1):19?51.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: a method for automatic
evaluation of machine translation. In Proceedings of
the 40th annual meeting on association for compu-
tational linguistics, pages 311?318. Association for
Computational Linguistics.
Hendra Setiawan, Min-Yen Kan, and Haizhou Li.
2007. Ordering phrases with function words. In
Proceedings of the 45th annual meeting on associ-
ation for computational linguistics, pages 712?719.
Association for Computational Linguistics.
Andreas Stolcke et al. 2002. Srilm-an extensible lan-
guage modeling toolkit. In INTERSPEECH.
Stephan Vogel, Hermann Ney, and Christoph Tillmann.
1996. Hmm-based word alignment in statistical
translation. In Proceedings of the 16th conference
on Computational linguistics-Volume 2, pages 836?
841. Association for Computational Linguistics.
158
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:shortpapers, pages 223?229,
Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational Linguistics
Query Snowball: A Co-occurrence-based Approach to Multi-document
Summarization for Question Answering
Hajime Morita1 2 and Tetsuya Sakai1 and Manabu Okumura3
1Microsoft Research Asia, Beijing, China
2Tokyo Institute of Technology, Tokyo, Japan
3Precision and Intelligence Laboratory, Tokyo Institute of Technology, Tokyo, Japan
morita@lr.pi.titech.ac.jp, tetsuyasakai@acm.org,
oku@pi.titech.ac.jp
Abstract
We propose a new method for query-oriented
extractive multi-document summarization. To
enrich the information need representation of
a given query, we build a co-occurrence graph
to obtain words that augment the original
query terms. We then formulate the sum-
marization problem as a Maximum Coverage
Problem with Knapsack Constraints based on
word pairs rather than single words. Our
experiments with the NTCIR ACLIA ques-
tion answering test collections show that our
method achieves a pyramid F3-score of up to
0.313, a 36% improvement over a baseline us-
ing Maximal Marginal Relevance.
1 Introduction
Automatic text summarization aims at reducing the
amount of text the user has to read while preserv-
ing important contents, and has many applications
in this age of digital information overload (Mani,
2001). In particular, query-oriented multi-document
summarization is useful for helping the user satisfy
his information need efficiently by gathering impor-
tant pieces of information from multiple documents.
In this study, we focus on extractive summariza-
tion (Liu and Liu, 2009), in particular, on sentence
selection from a given set of source documents that
contain relevant sentences. One well-known chal-
lenge in selecting sentences relevant to the informa-
tion need is the vocabulary mismatch between the
query (i.e. information need representation) and the
candidate sentences. Hence, to enrich the informa-
tion need representation, we build a co-occurrence
graph to obtain words that augment the original
query terms. We call this method Query Snowball.
Another challenge in sentence selection for
query-oriented multi-document summarization is
how to avoid redundancy so that diverse pieces of
information (i.e. nuggets (Voorhees, 2003)) can be
covered. For penalizing redundancy across sen-
tences, using single words as the basic unit may not
always be appropriate, because different nuggets for
a given information need often have many words
in common. Figure 1 shows an example of this
word overlap problem from the NTCIR-8 ACLIA2
Japanese question answering test collection. Here,
two gold-standard nuggets for the question ?Sen to
Chihiro no Kamikakushi (Spirited Away) is a full-
length animated movie from Japan. The user wants
to know how it was received overseas.? (in English
translation) is shown. Each nugget represents a par-
ticular award that the movie received, and the two
Japanese nugget strings have as many as three words
in common: ??? (review/critic)?, ???? (ani-
mation)? and ?? (award).? Thus, if we use single
words as the basis for penalising redundancy in sen-
tence selection, it would be difficult to cover both of
these nuggets in the summary because of the word
overlaps.
We therefore use word pairs as the basic unit for
computing sentence scores, and then formulate the
summarization problem as a Maximum Cover Prob-
lem with Knapsack Constraints (MCKP) (Filatova
and Hatzivassiloglou, 2004; Takamura and Oku-
mura, 2009a). This problem is an optimization prob-
lem that maximizes the total score of words covered
by a summary under a summary length limit.
223
? Question
Sen to Chihiro no Kamikakushi (Spirited Away) is a full-length
animated movie from Japan. The user wants to know how it
was received overseas.
? Nugget example 1
?????????????
National Board of Review of Motion Pictures Best Animated
Feature
? Nugget example 2
?????????????????
Los Angeles Film Critics Association Award for Best Ani-
mated Film
Figure 1: Question and gold-standard nuggets example in
NTCIR-8 ACLIA2 dataset
We evaluate our proposed method using Japanese
complex question answering test collections from
NTCIR ACLIA?Advanced Cross-lingual Informa-
tion Access task (Mitamura et al, 2008; Mitamura
et al, 2010). However, our method can easily be
extended for handling other languages.
2 Related Work
Much work has been done for generic multi-
document summarization (Takamura and Okumura,
2009a; Takamura and Okumura, 2009b; Celiky-
ilmaz and Hakkani-Tur, 2010; Lin et al, 2010a;
Lin and Bilmes, 2010). Carbonell and Goldstein
(1998) proposed the Maximal Marginal Relevance
(MMR) criteria for non-redundant sentence selec-
tion, which consist of document similarity and re-
dundancy penalty. McDonald (2007) presented
an approximate dynamic programming approach to
maximize the MMR criteria. Yih et al (2007)
formulated the document summarization problem
as an MCKP, and proposed a supervised method.
Whereas, our method is unsupervised. Filatova
and Hatzivassiloglou (2004) also formulated sum-
marization as an MCKP, and they used two types
of concepts in documents: single words and events
(named entity pairs with a verb or a noun). While
their work was for generic summarization, our
method is designed specifically for query-oriented
summarization.
MMR-based methods are also popular for query-
oriented summarization (Jagarlamudi et al, 2005;
Li et al, 2008; Hasegawa et al, 2010; Lin et al,
2010b). Moreover, graph-based methods for sum-
marization and sentence retrieval are popular (Otter-
bacher et al, 2005; Varadarajan and Hristidis, 2006;
Bosma, 2009). Unlike existing graph-based meth-
ods, our method explicitly computes indirect rela-
tionships between the query and words in the docu-
ments to enrich the information need representation.
To this end, our method utilizes within-sentence co-
occurrences of words.
The approach taken by Jagarlamudi et al (2005)
is similar to our proposed method in that it uses word
co-occurrence and dependencies within sentences in
order to measure relevance of words to the query.
However, while their approach measures the generic
relevance of each word based on Hyperspace Ana-
logue to Language (Lund and Burgess, 1996) using
an external corpus, our method measures the rele-
vance of each word within the document contexts,
and the query relevance scores are propagated recur-
sively.
3 Proposed Method
Section 3.1 introduces the Query Snowball (QSB)
method which computes the query relevance score
for each word. Then, Section 3.2 describes how
we formulate the summarization problem based on
word pairs.
3.1 Query Snowball method (QSB)
The basic idea behind QSB is to close the gap
between the query (i.e. information need rep-
resentation) and relevant sentences by enriching
the information need representation based on co-
occurrences. To this end, QSB computes a query
relevance score for each word in the source docu-
ments as described below.
Figure 2 shows the concept of QSB. Here, Q is
the set of query terms (each represented by q), R1
is the set of words (r1) that co-occur with a query
term in the same sentence, andR2 is the set of words
(r2) that co-occur with a word from R1, excluding
those that are already in R1. The imaginary root
node at the center represents the information need,
and we assume that the need is propagated through
this graph, where edges represent within-sentence
co-occurrences. Thus, to compute sentence scores,
we use not only the query terms but also the words
in R1 and R2.
Our first clue for computing a word score is
the query-independent importance of the word.
224
 q 
q 
q 
r1 
r1 
r1 
r1 r1 
r1 
r1 
r2 
r2 
r2 
r2 
r2 
r2 
r2 
r2 
r2 
r2 
R1 
R2 
Q 
root 
r2 
r2 r2 
r2 
r2 
Figure 2: Co-occurrence Graph (Query Snowball)
We represent this base word score by sb(w) =
log(N/ctf (w)) or sb(w) = log(N/n(w)), where
ctf (w) is the total number of occurrences of w
within the corpus and n(w) is the document fre-
quency of w, and N is the total number of docu-
ments in the corpus. We will refer to these two ver-
sions as itf and idf, respectively. Our second clue
is the weight propagated from the center of the co-
occurence graph shown in Figure 1. Below, we de-
scribe how to compute the word scores for words in
R1 and then those for words in R2.
As Figure 2 suggests, the query relevance score
for r1 ? R1 is computed based not only on its base
word score but also on the relationship between r1
and q ? Q. To be more specific, let freq(w,w?)
denote the within-sentence co-occurrence frequency
for words w and w?, and let distance(w,w?) denote
the minimum dependency distance between w and
w?: A dependency distance is the path length be-
tween nodes w and w? within a dependency parse
tree; the minimum dependency distance is the short-
est path length among all dependency parse trees of
source-document sentences in which w and w? co-
occur. Then, the query relevance score for r1 can be
computed as:
sr(r1) =
?
q?Q
sb(r1)
( sb(q)
sumQ
)( freq(q, r1)
distance(q, r1) + 1.0
)
(1)
where sumQ =
?
q?Q sb(q). It can be observed that
the query relevance score sr(r1) reflects the base
word scores of both q and r1, as well as the co-
occurrence frequency freq(q, r1). Moreover, sr(r1)
depends on distance(q, r1), the minimum depen-
dency distance between q and r1, which reflects
the strength of relationship between q and r1. This
quantity is used in one of its denominators in Eq.1
as small values of distance(q, r1) imply a strong re-
lationship between q and r1. The 1.0 in the denom-
inator avoids division by zero.
Similarly, the query relevance score for r2 ? R2
is computed based on the base word score of r2 and
the relationship between r2 and r1 ? R1:
sr(r2) =
?
r1?R1
sb(r2)
( sr(r1)
sumR1
)( freq(r1, r2)
distance(r1, r2) + 1.0
)
(2)
where sumR1 =
?
r1?R1sr(r1).
3.2 Score Maximization Using Word Pairs
Having determined the query relevance score, the
next step is to define the summary score. To this end,
we use word pairs rather than individual words as the
basic unit. This is because word pairs are more in-
formative for discriminating across different pieces
of information than single common words. (Re-
call the example mentioned in Section 1) Thus, the
word pair score is simply defined as: sp(w1, w2) =
sr(w1)sr(w2) and the summary score is computed
as:
fQSBP (S) =
?
{w1,w2|w1 6=w2 and w1,w2?u and u?S}
sp(w1, w2) (3)
where u is a textual unit, which in our case is a
sentence. Our problem then is to select S to maxi-
mize fQSBP (S). The above function based on word
pairs is still submodular, and therefore we can apply
a greedy approximate algorithm with performance
guarantee as proposed in previous work (Khuller
et al, 1999; Takamura and Okumura, 2009a). Let
l(u) denote the length of u. Given a set of source
documents D and a length limit L for a sum-
mary,
Require: D,L
1: W = D,S = ?
2: while W 6= ? do
3: u = argmaxu?W
f(S?{u})?f(S)
l(u)
4: if l(u) +
?
uS?S l(uS) ? L then
5: S = S ? {u}
6: end if
7: W = W/{u}
8: end while
9: umax = argmaxu?D f(u)
10: if f(umax) > f(S) then
11: return umax
12: else return S
13: end if
where f(?) is some score function such as fQSBP .
We call our proposed method QSBP: Query Snow-
ball with Word Pairs.
225
4 Experiments
4.1 Experimental Environment
ACLIA1 ACLIA2
Development Test Test
#of questions 101 100 80*
#of avg. nuggets 5.8 12.8 11.2*
Question types DEFINITION, BIOGRAPHY,RELATIONSHIP, EVENT +WHY
Articles years 1998-2001 2002-2005
Documents Mainichi Newspaper
*After removing the factoid questions.
Table 1: ACLIA dataset statistics
We evaluate our method using Japanese QA test
collections from NTCIR-7 ACLIA1 and NTCIR-
8 ACLIA2 (Mitamura et al, 2008; Mitamura et
al., 2010). The collections contain complex ques-
tions and their answer nuggets with weights. Ta-
ble 1 shows some statistics of the data. We use the
ACLIA1 development data for tuning a parameter
for our baseline as shown in Section 4.2 (whereas
our proposed method is parameter-free), and the
ACLIA1 and ACLIA2 test data for evaluating dif-
ferent methods The results for the ACLIA1 test data
are omitted due to lack of space. As our aim is
to answer complex questions by means of multi-
document summarization, we removed factoid ques-
tions from the ACLIA2 test data.
Although the ACLIA test collections were origi-
nally designed for Japanese QA evaluation, we treat
them as query-oriented summarization test collec-
tions. We use all the candidate documents from
which nuggets were extracted as input to the multi-
document summarizers. That is, in our problem set-
ting, the relevant documents are already given, al-
though the given document sets also occasionally
contain documents that were eventually never used
for nugget extraction (Mitamura et al, 2008; Mita-
mura et al, 2010).
We preprocessed the Japanese documents basi-
cally by automatically detecting sentence bound-
aries based on Japanese punctuation marks, but we
also used regular-expression-based heuristics to de-
tect glossary of terms in articles. As the descrip-
tions of these glossaries are usually very useful for
answering BIOGRAPHY and DEFINITION ques-
tions, we treated each term description (generally
multiple sentences) as a single sentence.
We used Mecab (Kudo et al, 2004) for morpho-
logical analysis, and calculated base word scores
sb(w) using Mainichi articles from 1991 to 2005.
We also used Mecab to convert each word to its base
form and to filter using POS tags to extract content
words. As for dependency parsing for distance com-
putation, we used Cabocha (Kudo and Matsumoto,
2000). We did not use a stop word list or any other
external knowledge.
Following the NTCIR-9 one click access task
setting1, we aimed at generating summaries of
Japanese 500 characters or less. To evaluate the
summaries, we followed the practices at the TAC
summarization tasks (Dang, 2008) and NTCIR
ACLIA tasks, and computed pyramid-based preci-
sion with an allowance parameter of C, recall, F?
(where ? is 1 or 3) scores. The value of C was
determined based on the average nugget length for
each question type of the ACLIA2 collection (Mita-
mura et al, 2010). Precision and recall are computed
based on the nuggets that the summary covered as
well as their weights. The first author of this paper
manually evaluated whether each nugget matches a
summary. The evaluation metrics are formally de-
fined as follows:
precision = min
(C ? (] of matched nuggets)
summary length , 1
)
,
recall = sum of weights over matched nuggetssum of weights over all nuggets ,
F? = (1 + ?
2) ? precision ? recall
?2 ? recision + recall .
4.2 Baseline
MMR is a popular approach in query-oriented sum-
marization. For example, at the TAC 2008 opin-
ion summarization track, a top performer in terms
of pyramid F score used an MMR-based method.
Our own implementation of an MMR-based base-
line uses an existing algorithm to maximize the fol-
lowing summary set score function (Lin and Bilmes,
2010):
fMMR(S) = ?
(
?
u?S
Sim(u, vD) +
?
u?S
Sim(u, vQ)
)
?(1 ? ?)
?
{(ui,uj)|i 6=j and ui,uj?S}
Sim(ui, uj) (4)
where vD is the vector representing the source docu-
ments, vQ is the vector representing the query terms,
Sim is the cosine similarity, and ? is a parameter.
1http://research.microsoft.com/en-us/people/tesakai/1click.aspx
226
Thus, the first term of this function reflects how the
sentences reflect the entire documents; the second
term reflects the relevance of the sentences to the
query; and finally the function penalizes redundant
sentences. We set ? to 0.8 and the scaling factor
used in the algorithm to 0.3 based on a preliminary
experiment with a part of the ACLIA1 development
data. We also tried incorporating sentence position
information (Radev, 2001) to our MMR baseline but
this actually hurt performance in our preliminary ex-
periments.
4.3 Variants of the Proposed Method
To clarify the contributions of each components, the
minimum dependency distance, QSB and the word
pair, we also evaluated the following simplified ver-
sions of QSBP. (We use the itf version by default,
and will refer to the idf version as QSBP(idf). ) To
examine the contribution of using minimum depen-
dency distance, We remove distance(w,w?) from
Eq.1 and Eq.2. We call the method QSBP(nodist).
To examine the contribution of using word pairs for
score maximization (see Section 3.2) on the perfor-
mance of QSBP, we replaced Eq.3 with:
fQSB(S) =
?
{w|w?ui and ui?S}
sr(w) . (5)
To examine the contribution of the QSB relevance
scoring (see Section 3.1) on the performance of
QSBP, we replaced Eq.3 with:
fWP (S) =
?
{w1,w2|w1 6=w2 and w1,w2?ui and ui?S}
sb(w1)sb(w2) . (6)
We will refer to this as WP. Note that this relies only
on base word scores and is query-independent.
4.4 Results
Tables 2 and 3 summarize our results. We used
the two-tailed sign test for testing statistical signif-
icance. Significant improvements over the MMR
baseline are marked with a ? (?=0.05) or a ?
(?=0.01); those over QSBP(nodist) are marked with
a ] (?=0.05) or a ]] (?=0.01); and those over QSB
are marked with a ? (?=0.05) or a ?? (?=0.01); and
those over WP are marked with a ? (?=0.05) or a
?
? (?=0.01). From Table 2, it can be observed that
both QSBP and QSBP(idf) significantly outperforms
QSBP(nodist), QSB, WP and the baseline in terms
of all evaluation metrics. Thus, the minimum depen-
dency distance, Query Snowball and the use of word
pairs all contribute significantly to the performance
of QSBP. Note that we are using the ACLIA data as
summarization test collections and that the official
QA results of ACLIA should not be compared with
ours.
QSBP and QSBP(idf) achieve 0.312 and 0.313 in
F3 score, and the differences between the two are
not statistically significant. Table 3 shows the F3
scores for each question type. It can be observed
that QSBP is the top performer for BIO, DEF and
REL questions on average, while QSBP(idf) is the
top performer for EVENT and WHY questions on
average. It is possible that different word scoring
methods work well for different question types.
Method Precision Recall F1 score F3 score
Baseline 0.076?? 0.370?? 0.116?? 0.231??
QSBP 0.107????? ]] 0.482????? ]] 0.161????? ]] 0.312????? ]]
QSBP(idf) 0.106????? ]] 0.485????? ]] 0.161????? ]] 0.313????? ]]
QSBP(nodist) 0.083??? 0.396?? 0.125?? 0.248??
QSB 0.086??? 0.400?? 0.129??? 0.253???
WP 0.053 0.222 0.080 0.152
Table 2: ACLIA2 test data results
Type BIO DEF REL EVENT WHY
Baseline 0.207? 0.251?? 0.270 0.212 0.213
QSBP 0.315?? 0.329??? 0.401? 0.258??? ]] 0.275?]
QSBP(idf) 0.304??] 0.328??? 0.397? 0.268??? 0.280??
QSBP(nodist) 0.255 0.281?? 0.329 0.196 0.212??
QSB 0.245?? 0.273?? 0.324 0.217 0.215
WP 0.109 0.037 0.235 0.141 0.161
Table 3: F3-scores for each question type (ACLIA2 test)
5 Conclusions and Future work
We proposed the Query Snowball (QSB) method for
query-oriented multi-document summarization. To
enrich the information need representation of a given
query, QSB obtains words that augment the original
query terms from a co-occurrence graph. We then
formulated the summarization problem as an MCKP
based on word pairs rather than single words. Our
method, QSBP, achieves a pyramid F3-score of up
to 0.313 with the ACLIA2 Japanese test collection,
a 36% improvement over a baseline using Maximal
Marginal Relevance.
Moreover, as the principles of QSBP are basically
language independent, we will investigate the effec-
tiveness of QSBP in other languages. Also, we plan
to extend our approach to abstractive summariza-
tion.
227
References
Wauter Bosma. 2009. Contextual salience in query-
based summarization. In Proceedings of the Interna-
tional Conference RANLP-2009, pages 39?44. Asso-
ciation for Computational Linguistics.
Jaime Carbonell and Jade Goldstein. 1998. The use of
mmr, diversity-based reranking for reordering docu-
ments and producing summaries. In Proceedings of
the 21st annual international ACM SIGIR conference
on Research and development in information retrieval,
SIGIR ?98, pages 335?336. Association for Comput-
ing Machinery.
Asli Celikyilmaz and Dilek Hakkani-Tur. 2010. A hy-
brid hierarchical model for multi-document summa-
rization. In Proceedings of the 48th Annual Meeting
of the Association for Computational Linguistics, ACL
?10, pages 815?824. Association for Computational
Linguistics.
Hoa Trang Dang. 2008. Overview of the tac 2008 opin-
ion question answering and summarization tasks. In
Proceedings of Text Analysis Conference.
Elena Filatova and Vasileios Hatzivassiloglou. 2004.
A formal model for information selection in multi-
sentence text extraction. In Proceedings of the 20th in-
ternational conference on Computational Linguistics,
COLING ?04. Association for Computational Linguis-
tics.
Takaaki Hasegawa, Hitoshi Nishikawa, Kenji Imamura,
Genichiro Kikui, and Manabu Okumura. 2010. A
Web Page Summarization for Mobile Phones. Trans-
actions of the Japanese Society for Artificial Intelli-
gence, 25:133?143.
Jagadeesh Jagarlamudi, Prasad Pingali, and Vasudeva
Varma. 2005. A relevance-based language modeling
approach to duc 2005. In Proceedings of Document
Understanding Conferences (along with HLT-EMNLP
2005).
Samir Khuller, Anna Moss, and Joseph S. Naor. 1999.
The budgeted maximum coverage problem. Informa-
tion Processing Letters, 70(1):39?45.
Taku Kudo and Yuji Matsumoto. 2000. Japanese de-
pendency structure analysis based on support vector
machines. In Proceedings of the 2000 Joint SIGDAT
conference on Empirical methods in natural language
processing and very large corpora: held in conjunc-
tion with the 38th Annual Meeting of the Association
for Computational Linguistics, volume 13, pages 18?
25. Association for Computational Linguistics.
Taku Kudo, Kaoru Yamamoto, and Yuji Matsumoto.
2004. Applying conditional random fields to Japanese
morphological analysis. In Proceedings of the Confer-
ence on Emprical Methods in Natural Language Pro-
cessing (EMNLP 2004), volume 2004, pages 230?237.
Wenjie Li, You Ouyang, Yi Hu, and Furu Wei. 2008.
PolyU at TAC 2008. In Proceedings of Text Analysis
Conference.
Hui Lin and Jeff Bilmes. 2010. Multi-document sum-
marization via budgeted maximization of submodular
functions. In Human Language Technologies: The
2010 Annual Conference of the North American Chap-
ter of the Association for Computational Linguistics,
HLT ?10, pages 912?920. Association for Computa-
tional Linguistics.
Hui Lin, Jeff Bilmes, and Shasha Xie. 2010a. Graph-
based submodular selection for extractive summariza-
tion. In Automatic Speech Recognition & Understand-
ing, 2009. ASRU 2009. IEEEWorkshop on, pages 381?
386. IEEE.
Jimmy Lin, Nitin Madnani, and Bonnie J. Dorr. 2010b.
Putting the user in the loop: interactive maximal
marginal relevance for query-focused summarization.
In Human Language Technologies: The 2010 Annual
Conference of the North American Chapter of the
Association for Computational Linguistics, HLT ?10,
pages 305?308. Association for Computational Lin-
guistics.
Fei Liu and Yang Liu. 2009. From extractive to abstrac-
tive meeting summaries: can it be done by sentence
compression? In Proceedings of the ACL-IJCNLP
2009 Conference Short Papers, ACLShort ?09, pages
261?264. Association for Computational Linguistics.
Kevin Lund and Curt Burgess. 1996. Producing
high-dimensional semantic spaces from lexical co-
occurrence. Behavior Research Methods, 28:203?208.
Inderjeet Mani. 2001. Automatic summarization. John
Benjamins Publishing Co.
RyanMcDonald. 2007. A study of global inference algo-
rithms in multi-document summarization. In Proceed-
ings of the 29th European conference on IR research,
ECIR?07, pages 557?564. Springer-Verlag.
Teruko Mitamura, Eric Nyberg, Hideki Shima, Tsuneaki
Kato, Tatsunori Mori, Chin-Yew Lin, Ruihua Song,
Chuan-Jie Lin, Tetsuya Sakai, Donghong Ji, and
Noriko Kando. 2008. Overview of the NTCIR-7
ACLIA tasks: Advanced cross-lingual information ac-
cess. In Proceedings of the 7th NTCIR Workshop.
Teruko Mitamura, Hideki Shima, Tetsuya Sakai, Noriko
Kando, Tatsunori Mori, Koichi Takeda, Chin-Yew Lin,
Ruihua Song, Chuan-Jie Lin, and Cheng-Wei Lee.
2010. Overview of the NTCIR-8 ACLIA tasks: Ad-
vanced cross-lingual information access. In Proceed-
ings of the 8th NTCIR Workshop.
Jahna Otterbacher, Gu?nes? Erkan, and Dragomir R. Radev.
2005. Using random walks for question-focused sen-
tence retrieval. In Proceedings of the conference on
Human Language Technology and Empirical Methods
228
in Natural Language Processing, HLT ?05, pages 915?
922. Association for Computational Linguistics.
Dragomir R. Radev. 2001. Experiments in single and
multidocument summarization using mead. In First
Document Understanding Conference.
Hiroya Takamura and Manabu Okumura. 2009a. Text
summarization model based on maximum coverage
problem and its variant. In Proceedings of the 12th
Conference of the European Chapter of the ACL
(EACL 2009), pages 781?789. Association for Com-
putational Linguistics.
Hiroya Takamura and Manabu Okumura. 2009b. Text
summarization model based on the budgeted median
problem. In Proceeding of the 18th ACM conference
on Information and knowledge management, CIKM
?09, pages 1589?1592. Association for Computing
Machinery.
Ramakrishna Varadarajan and Vagelis Hristidis. 2006.
A system for query-specific document summarization.
In Proceedings of the 15th ACM international con-
ference on Information and knowledge management,
CIKM ?06, pages 622?631. ACM.
Ellen M. Voorhees. 2003. Overview of the TREC
2003 Question Answering Track. In Proceedings of
the Twelfth Text REtrieval Conference (TREC 2003),
pages 54?68.
Wen-tau Yih, Joshua Goodman, Lucy Vanderwende, and
Hisami Suzuki. 2007. Multi-document summariza-
tion by maximizing informative content-words. In
Proceedings of the 20th international joint conference
on Artifical intelligence, pages 1776?1782. Morgan
Kaufmann Publishers Inc.
229
Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 349?353,
Jeju, Republic of Korea, 8-14 July 2012. c?2012 Association for Computational Linguistics
Sentence Compression with Semantic Role Constraints
Katsumasa Yoshikawa
Precision and Intelligence Laboratory,
Tokyo Institute of Technology, Japan
IBM Research-Tokyo, IBM Japan, Ltd.
katsumasay@gmail.com
Ryu Iida
Department of Computer Science,
Tokyo Institute of Technology, Japan
ryu-i@cl.cs.titech.ac.jp
Tsutomu Hirao
NTT Communication Science Laboratories,
NTT Corporation, Japan
hirao.tsutomu@lab.ntt.co.jp
Manabu Okumura
Precision and Intelligence Laboratory,
Tokyo Institute of Technology, Japan
oku@lr.pi.titech.ac.jp
Abstract
For sentence compression, we propose new se-
mantic constraints to directly capture the relations
between a predicate and its arguments, whereas
the existing approaches have focused on relatively
shallow linguistic properties, such as lexical and
syntactic information. These constraints are based
on semantic roles and superior to the constraints
of syntactic dependencies. Our empirical eval-
uation on the Written News Compression Cor-
pus (Clarke and Lapata, 2008) demonstrates that
our system achieves results comparable to other
state-of-the-art techniques.
1 Introduction
Recent work in document summarization do not
only extract sentences but also compress sentences.
Sentence compression enables summarizers to re-
duce the redundancy in sentences and generate in-
formative summaries beyond the extractive summa-
rization systems (Knight and Marcu, 2002). Con-
ventional approaches to sentence compression ex-
ploit various linguistic properties based on lexical
information and syntactic dependencies (McDonald,
2006; Clarke and Lapata, 2008; Cohn and Lapata,
2008; Galanis and Androutsopoulos, 2010).
In contrast, our approach utilizes another property
based on semantic roles (SRs) which improves weak-
nesses of syntactic dependencies. Syntactic depen-
dencies are not sufficient to compress some complex
sentences with coordination, with passive voice, and
with an auxiliary verb. Figure 1 shows an example
with a coordination structure. 1
1This example is from Written News Compression Cor-
pus (http://jamesclarke.net/research/resources).
Figure 1: Semantic Role vs. Dependency Relation
In this example, a SR labeler annotated thatHarari
is an A0 argument of left and an A1 argument of
became. Harari is syntactically dependent on left ?
SBJ(left-2, Harari-1). However, Harari is not depen-
dent on became and we are hence unable to utilize a
dependency relation between Harari and became di-
rectly. SRs allow us to model the relations between
a predicate and its arguments in a direct fashion.
SR constraints are also advantageous in that we
can compress sentences with semantic information.
In Figure 1, became has three arguments, Harari as
A1, businessman as A2, and shortly afterward as
AM-TMP. As shown in this example, shortly after-
word can be omitted (shaded boxes). In general,
modifier arguments like AM-TMP or AM-LOC are
more likely to be reduced than complement cases
like A0-A4. We can implement such properties by
SR constraints.
Liu and Gildea (2010) suggests that SR features
contribute to generating more readable sentence in
machine translation. We expect that SR features also
help our system to improve readability in sentence
compression and summarization.
2 Why are Semantic Roles Useful for Com-
pressing Sentences?
Before describing our system, we show the statis-
tics in terms of predicates, arguments and their rela-
349
Label In Compression / Total Ratio
A0 1454 / 1607 0.905
A1 1916 / 2208 0.868
A2 427 / 490 0.871
AM-TMP 261 / 488 0.535
AM-LOC 134 / 214 0.626
AM-ADV 115 / 213 0.544
AM-DIS 8 / 85 0.094
Table 1: Statistics of Arguments in Compression
tions in the Written News Compression (WNC) Cor-
pus. It has 82 documents (1,629 sentences). We di-
vided them into three: 55 documents are used for
training (1106 sentences); 10 for development (184
sentences); 17 for testing (339 sentences).
Our investigation was held in training data. There
are 3137 verbal predicates and 7852 unique argu-
ments. We performed SR labeling by LTH (Johans-
son and Nugues, 2008), an SR labeler for CoNLL-
2008 shared task. Based on the SR labels annotated
by LTH, we investigated that, for all predicates in
compression, how many their arguments were also
in. Table 1 shows the survival ratio of main argu-
ments in compression. Labels A0, A1, and A2 are
complement case roles and over 85% of them survive
with their predicates. On the other hand, for modifier
arguments (AM-X), survival ratios are down to lower
than 65%. Our SR constraints implement the differ-
ence of survival ratios by SR labels. Note that de-
pendency labels SBJ and OBJ generally correspond
to SR labels A0 and A1, respectively. But their total
numbers are 777 / 919 (SBJ) and 918 / 1211 (OBJ)
and much fewer than A0 and A1 labels. Thus, SR la-
bels can connect much more arguments to their pred-
icates.
3 Approach
This section describes our new approach to sen-
tence compression. In order to introduce rich syn-
tactic and semantic constraints to a sentence com-
pression model, we employ Markov Logic (Richard-
son and Domingos, 2006). Since Markov Logic sup-
ports both soft and hard constraints, we can imple-
ment our SR constraints in simple and direct fash-
ion. Moreover, implementations of learning and
inference methods are already provided in existing
Markov Logic interpreters such as Alchemy 2 and
Markov thebeast. 3 Thus, we can focus our effort
2http://alchemy.cs.washington.edu/
3http://code.google.com/p/thebeast/
on building a set of formulae called Markov Logic
Network (MLN). So, in this section, we describe our
proposed MLN in detail.
3.1 Proposed Markov Logic Network
First, let us define our MLN predicates. We sum-
marize the MLN predicates in Table 2. We have only
one hidden MLN predicate, inComp(i) which mod-
els the decision we need to make: whether a token i
is in compression or not. The other MLN predicates
are called observed which provide features. With our
MLN predicates defined, we can now go on to in-
corporate our intuition about the task using weighted
first-order logic formulae. We define SR constraints
and the other formulae in Sections 3.1.1 and 3.1.2,
respectively.
3.1.1 Semantic Role Constraints
Semantic role labeling generally includes the three
subtasks: predicate identification; argument role la-
beling; sense disambiguation. Our model exploits
the results of predicate identification and argument
role labeling. 4 pred(i) and role(i, j, r) indicate the
results of predicate identification and role labeling,
respectively.
First, the formula describing a local property of a
predicate is
pred(i) ? inComp(i) (1)
which denotes that, if token i is a predicate then i is
in compression. A formula with exact one hidden
predicate is called local formula.
A predicate is not always in compression. The for-
mula reducing some predicates is
pred(i) ? height(i,+n) ? ?inComp(i) (2)
which implies that a predicate i is not in compression
with n height in a dependency tree. Note the + nota-
tion indicates that the MLN contains one instance of
the rule, with a separate weight, for each assignment
of the variables with a plus sign.
As mentioned earlier, our SR constraints model
the difference of the survival rate of role labels in
compression. Such SR constraints are encoded as:
role(i, j, +r) ? inComp(i) ? inComp( j) (3)
role(i, j,+r) ? ?inComp(i) ? ?inComp( j) (4)
which represent that, if a predicate i is (not) in com-
pression, then its argument j is (not) also in with
4Sense information is too sparse because the size of the
WNC Corpus is not big enough.
350
predicate definition
inComp(i) Token i is in compression
pred(i) Token i is a predicate
role(i, j, r) Token i has an argument j with role r
word(i, w) Token i has word w
pos(i, p) Token i has Pos tag p
dep(i, j, d) Token i is dependent on token j with
dependency label d
path(i, j, l) Tokens i and j has syntactic path l
height(i, n) Token i has height n in dependency tree
Table 2: MLN Predicates
role r. These formulae are called global formulae
because they have more than two hidden MLN pred-
icates. With global formulae, our model makes two
decisions at a time. When considering the example
in Figure 1, Formula (3) will be grounded as:
role(9, 1, A0) ? inComp(9) ? inComp(1) (5)
role(9, 7, AM-TMP) ? inComp(9) ? inComp(7). (6)
In fact, Formula (5) gains a higher weight than For-
mula (6) by learning on training data. As a re-
sult, our system gives ?1-Harari? more chance to
survive in compression. We also add some exten-
sions of Formula (3) combined with dep(i, j, +d) and
path(i, j, +l) which enhance SR constraints. Note, all
our SR constraints are ?predicate-driven? (only ?
not ? as in Formula (13)). Because an argument is
usually related to multiple predicates, it is difficult to
model ?argument-driven? formula.
3.1.2 Lexical and Syntactic Features
For lexical and syntactic features, we mainly refer
to the previous work (McDonald, 2006; Clarke and
Lapata, 2008). The first two formulae in this sec-
tion capture the relation of the tokens with their lexi-
cal and syntactic properties. The formula describing
such a local property of a word form is
word(i,+w) ? inComp(i) (7)
which implies that a token i is in compression with a
weight that depends on the word form.
For part-of-speech (POS), we add unigram and bi-
gram features with the formulae,
pos(i, +p) ? inComp(i) (8)
pos(i, +p1) ? pos(i + 1,+p2) ? inComp(i). (9)
POS features are often more reasonable than word
form features to combine with the other properties.
The formula,
pos(i, +p) ? height(i, +n) ? inComp(i). (10)
is a combination of POS features and a height in a
dependency tree.
The next formula combines POS bigram features
with dependency relations.
pos(i,+p1) ? pos( j, +p2) ?
dep(i, j,+d) ? inComp(i). (11)
Moreover, our model includes the following
global formulae,
dep(i, j,+d) ? inComp(i) ? inComp( j) (12)
dep(i, j,+d) ? inComp(i) ? inComp( j) (13)
which enforce the consistencies between head and
modifier tokens. Formula (12) represents that if
we include a head token in compression then its
modifier must also be included. Formula (13) en-
sures that head and modifier words must be simul-
taneously kept in compression or dropped. Though
Clarke and Lapata (2008) implemented these depen-
dency constraints by ILP, we implement them by
soft constraints of MLN. Note that Formula (12) ex-
presses the same properties as Formula (3) replacing
dep(i, j, +d) by role(i, j,+r).
4 Experiment and Result
4.1 Experimental Setup
Our experimental setting follows previous
work (Clarke and Lapata, 2008). As stated in
Section 2, we employed the WNC Corpus. For
preprocessing, we performed POS tagging by
stanford-tagger. 5 and dependency parsing by
MST-parser (McDonald et al, 2005). In addition,
LTH 6 was exploited to perform both dependency
parsing and SR labeling. We implemented our
model by Markov Thebeast with Gurobi optimizer. 7
Our evaluation consists of two types of automatic
evaluations. The first evaluation is dependency based
evaluation same as Riezler et al (2003). We per-
formed dependency parsing on gold data and system
outputs by RASP. 8 Then we calculated precision, re-
call, and F1 for the set of label(head, modi f ier).
In order to demonstrate how well our SR con-
straints keep correct predicate-argument structures
in compression, we propose SRL based evalua-
tion. We performed SR labeling on gold data
5http://nlp.stanford.edu/software/tagger.shtml
6http://nlp.cs.lth.se/software/semantic_
parsing:_propbank_nombank_frames
7http://www.gurobi.com/
8http://www.informatics.susx.ac.uk/research/
groups/nlp/rasp/
351
Original [A0 They] [pred say] [A1 the refugees will enhance productivity and economic growth].
MLN with SRL [A0 They] [pred say] [A1 the refugees will enhance growth].
Gold Standard [A1? the refugees will enhance productivity and growth].
Original [A0 A ?16.1m dam] [AM?MOD will] [pred hold] back [A1 a 2.6-mile-long artificial lake to be
known as the Roadford Reservoir].
MLN with SRL [A0 A dam] will [pred hold] back [A1 a artificial lake to be known as the Roadford Reservoir].
Gold Standard [A0 A ?16.1m dam] [AM?MOD will] [pred hold back [A1 a 2.6-mile-long Roadford Reservoir].
Table 4: Analysis of Errors
Model CompR F1-Dep F1-SRL
McDonald 73.6% 38.4% 49.9%
MLN w/o SRL 68.3% 51.3% 57.2%
MLN with SRL 73.1% 58.9% 64.1%
Gold Standard 73.3% ? ?
Table 3: Results of Sentence Compression
and system outputs by LTH. Then we calculated
precision, recall, and F1 value for the set of
role(predicate, argument).
The training time of our MLN model are approx-
imately 8 minutes on all training data, with 3.1GHz
Intel Core i3 CPU and 4G memory. While the pre-
diction can be done within 20 seconds on the test
data.
4.2 Results
Table 3 shows the results of our compression
models by compression rate (CompR), dependency-
based F1 (F1-Dep), and SRL-based F1 (F1-SRL). In
our experiment, we have three models. McDonald
is a re-implementation of McDonald (2006). Clarke
and Lapata (2008) also re-implemented McDonald?s
model with an ILP solver and experimented it on the
WNC Corpus. 9 MLN with SRL and MLN w/o
SRL are our Markov Logic models with and with-
out SR Constraints, respectively.
Note our three models have no constraint for the
length of compression. Therefore, we think the com-
pression rate of the better system should get closer to
that of human compression. In comparison between
MLNmodels and McDonald, the former models out-
perform the latter model on both F1-Dep and F1-
SRL. Because MLN models have global constraints
and can generate syntactically correct sentences.
Our concern is how a model with SR constraints
is superior to a model without them. MLN with
SRL outperforms MLN without SRL with a 7.6
points margin (F1-Dep). The compression rate of
MLN with SRL goes up to 73.1% and gets close
9Clarke?s re-implementation got 60.1% for CompR and
36.0%pt for F1-Dep
to that of gold standard. SRL-based evaluation also
shows that SR constraints actually help extract cor-
rect predicate-argument structures. These results are
promising to improve readability.
It is difficult to directly compare our results with
those of state-of-the-art systems (Cohn and Lapata,
2009; Clarke and Lapata, 2010; Galanis and An-
droutsopoulos, 2010) since they have different test-
ing sets and the results with different compression
rates. However, though our MLN model with SR
constraints utilizes no large-scale data, it is the only
model which achieves close on 60% in F1-Dep.
4.3 Error Analysis
Table 4 indicates two critical examples which our
SR constraints failed to compress correctly. For the
first example, our model leaves an argument with its
predicate because our SR constraints are ?predicate-
driven?. In addition, ?say? is the main verb in this
sentence and hard to be deleted due to the syntactic
significance.
The second example in Table 4 requires to iden-
tify a coreference relation between artificial lake and
Roadford Reservour. We consider that discourse
constraints (Clarke and Lapata, 2010) help our model
handle these cases. Discourse and coreference infor-
mation enable our model to select important argu-
ments and their predicates.
5 Conclusion
In this paper, we proposed new semantic con-
straints for sentence compression. Our model with
global constraints of semantic roles selected correct
predicate-argument structures and successfully im-
proved performance of sentence compression.
As future work, we will compare our model with
the other state-of-the-art systems. We will also inves-
tigate the correlation between readability and SRL-
based score by manual evaluations. Furthermore, we
would like to combine discourse constraints with SR
constraints.
352
References
James Clarke and Mirella Lapata. 2008. Global infer-
ence for sentence compression: An integer linear pro-
gramming approach. Journal of Artificial Intelligence
Research, 31(1):399?429.
James Clarke and Mirella Lapata. 2010. Discourse con-
straints for document compression. Computational
Linguistics, 36(3):411?441.
Trevor Cohn and Mirella Lapata. 2008. Sentence com-
pression beyond word deletion. In Proceedings of
the 22nd International Conference on Computational
Linguistics-Volume 1, pages 137?144. Association for
Computational Linguistics.
Trevor Cohn and Mirella Lapata. 2009. Sentence com-
pression as tree transduction. Journal of Artificial In-
telligence Research, 34:637?674.
Dimitrios Galanis and Ion Androutsopoulos. 2010. An
extractive supervised two-stage method for sentence
compression. In Human Language Technologies: The
2010 Annual Conference of the North American Chap-
ter of the Association for Computational Linguistics,
HLT ?10, pages 885?893, Stroudsburg, PA, USA. As-
sociation for Computational Linguistics.
Richard Johansson and Pierre Nugues. 2008.
Dependency-based syntactic-semantic analysis
with propbank and nombank. In Proceedings of
the Twelfth Conference on Computational Natural
Language Learning, pages 183?187. Association for
Computational Linguistics.
Kevin Knight and Daniel Marcu. 2002. Summariza-
tion beyond sentence extraction: A probabilistic ap-
proach to sentence compression. Artificial Intelligence,
139(1):91?107.
Ding Liu and Daniel Gildea. 2010. Semantic role fea-
tures for machine translation. In Proceedings of the
23rd International Conference on Computational Lin-
guistics (Coling 2010), pages 716?724, Beijing, China,
August. Coling 2010 Organizing Committee.
RyanMcDonald, Fernando Pereira, Kiril Ribarov, and Jan
Hajic?. 2005. Non-projective dependency parsing us-
ing spanning tree algorithms. In Proceedings of the
conference on Human Language Technology and Em-
pirical Methods in Natural Language Processing, HLT
?05, pages 523?530, Stroudsburg, PA, USA. Associa-
tion for Computational Linguistics.
Ryan McDonald. 2006. Discriminative sentence com-
pression with soft syntactic evidence. In Proceedings
of EACL, pages 297?304.
Matthew Richardson and Pedro Domingos. 2006.
Markov logic networks. Machine Learning, 62(1-
2):107?136.
Stefan Riezler, Tracy H. King, Richard Crouch, and An-
nie Zaenen. 2003. Statistical sentence condensation
using ambiguity packing and stochastic disambigua-
tion methods for lexical-functional grammar. In Pro-
ceedings of the 2003 Conference of the North American
Chapter of the Association for Computational Linguis-
tics on Human Language Technology-Volume 1, pages
118?125. Association for Computational Linguistics.
353
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 841?851,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
Part-of-Speech Induction in Dependency Trees for Statistical Machine
Translation
Akihiro Tamura?,?, Taro Watanabe?, Eiichiro Sumita?,
Hiroya Takamura?, Manabu Okumura?
? National Institute of Information and Communications Technology
{akihiro.tamura, taro.watanabe, eiichiro.sumita}@nict.go.jp
? Precision and Intelligence Laboratory, Tokyo Institute of Technology
{takamura, oku}@pi.titech.ac.jp
Abstract
This paper proposes a nonparametric
Bayesian method for inducing Part-of-
Speech (POS) tags in dependency trees
to improve the performance of statistical
machine translation (SMT). In particular,
we extend the monolingual infinite tree
model (Finkel et al, 2007) to a bilin-
gual scenario: each hidden state (POS tag)
of a source-side dependency tree emits a
source word together with its aligned tar-
get word, either jointly (joint model), or
independently (independent model). Eval-
uations of Japanese-to-English translation
on the NTCIR-9 data show that our in-
duced Japanese POS tags for dependency
trees improve the performance of a forest-
to-string SMT system. Our independent
model gains over 1 point in BLEU by re-
solving the sparseness problem introduced
in the joint model.
1 Introduction
In recent years, syntax-based SMT has made
promising progress by employing either depen-
dency parsing (Lin, 2004; Ding and Palmer, 2005;
Quirk et al, 2005; Shen et al, 2008; Mi and Liu,
2010) or constituency parsing (Huang et al, 2006;
Liu et al, 2006; Galley et al, 2006; Mi and Huang,
2008; Zhang et al, 2008; Cohn and Blunsom,
2009; Liu et al, 2009; Mi and Liu, 2010; Zhang
et al, 2011) on the source side, the target side,
or both. However, dependency parsing, which
is a popular choice for Japanese, can incorporate
only shallow syntactic information, i.e., POS tags,
compared with the richer syntactic phrasal cate-
gories in constituency parsing. Moreover, exist-
ing POS tagsets might not be optimal for SMT
because they are constructed without considering
the language in the other side. Consider the ex-
amples in Figure 1. The Japanese noun ???? in
? ? ?? ?? ? ??
??? ? ??????? ? ???? ??
You can not use the Internet  .
I  pay  usage fees  .
noun particle particlenoun noun verb auxiliary verb
noun particle noun noun verbparticle
[Example 1]
[Example 2]
Japanese POS:
Japanese POS:
Figure 1: Examples of Existing Japanese POS
Tags and Dependency Structures
Example 1 corresponds to the English verb ?use?,
while that in Example 2 corresponds to the English
noun ?usage?. Thus, Japanese nouns act like verbs
in English in one situation, and nouns in English
in another. If we could discriminate POS tags for
two cases, we might improve the performance of a
Japanese-to-English SMT system.
In the face of the above situations, this pa-
per proposes an unsupervised method for inducing
POS tags for SMT, and aims to improve the perfor-
mance of syntax-based SMT by utilizing the in-
duced POS tagset. The proposed method is based
on the infinite tree model proposed by Finkel et
al. (2007), which is a nonparametric Bayesian
method for inducing POS tags from syntactic de-
pendency structures. In this model, hidden states
represent POS tags, the observations they generate
represent the words themselves, and tree structures
represent syntactic dependencies between pairs of
POS tags.
The proposed method builds on this model by
incorporating the aligned words in the other lan-
guage into the observations. We investigate two
types of models: (i) a joint model and (ii) an in-
dependent model. In the joint model, each hid-
den state jointly emits both a source word and its
aligned target word as an observation. The in-
dependent model separately emits words in two
languages from hidden states. By inferring POS
841
tags based on bilingual observations, both mod-
els can induce POS tags by incorporating infor-
mation from the other language. Consider, for ex-
ample, inducing a POS tag for the Japanese word ?
??? in Figure 1. Under a monolingual induction
method (e.g., the infinite tree model), the ????
in Example 1 and 2 would both be assigned the
same POS tag since they share the same observa-
tion. However, our models would assign separate
tags for the two different instances since the ??
?? in Example 1 and Example 2 could be disam-
biguated by encoding the target-side information,
either ?use? or ?usage?, in the observations.
Inference is efficiently carried out by beam sam-
pling (Gael et al, 2008), which combines slice
sampling and dynamic programming. Experi-
ments are carried out on the NTCIR-9 Japanese-
to-English task using a binarized forest-to-string
SMT system with dependency trees as its source
side. Our bilingually-induced tagset signifi-
cantly outperforms the original tagset and the
monolingually-induced tagset. Further, our inde-
pendent model achieves a more than 1 point gain
in BLEU, which resolves the sparseness problem
introduced by the bi-word observations.
2 Related Work
A number of unsupervised methods have been
proposed for inducing POS tags. Early methods
have the problem that the number of possible POS
tags must be provided preliminarily. This limita-
tion has been overcome by automatically adjust-
ing the number of possible POS tags using non-
parametric Bayesian methods (Finkel et al, 2007;
Gael et al, 2009; Blunsom and Cohn, 2011; Sirts
and Aluma?e, 2012). Gael et al (2009) applied
infinite HMM (iHMM) (Beal et al, 2001; Teh
et al, 2006), a nonparametric version of HMM,
to POS induction. Blunsom and Cohn (2011)
used a hierarchical Pitman-Yor process prior to the
transition and emission distribution for sophisti-
cated smoothing. Sirts and Aluma?e (2012) built a
model that combines POS induction and morpho-
logical segmentation into a single learning prob-
lem. Finkel et al (2007) proposed the infinite
tree model, which represents recursive branching
structures over infinite hidden states and induces
POS tags from syntactic dependency structures. In
the following, we overview the infinite tree model,
which is the basis of our proposed model. In par-
ticular, we will describe the independent children
H ?k
?k? z1
z2 z3
x1 x2 x3k=1,?,C
Hk
k
~
),...,(Dirichlet~|? ???pi
Figure 2: A Graphical Representation of the Finite
Tree Model
model (Finkel et al, 2007), where children are
dependent only on their parents, used in our pro-
posed model1.
2.1 Finite Tree Model
We first review the finite tree model, which can
be graphically represented in Figure 2. Let
Tt denote the tree whose root node is t. A
node t has a hidden state zt (the POS tag)
and an observation xt (the word). The prob-
ability of a tree Tt, pT (Tt), is recursively de-
fined: pT (Tt) = p(xt|zt)
?
t??c(t)
p(zt? |zt)pT (Tt?),
where c(t) is the set of the children of t.
Let each hidden state variable have C possible
values indexed by k. For each state k, there is
a parameter ?k which parameterizes the observa-
tion distribution for that state: xt|zt ? F (?zt). ?k
is distributed according to a prior distribution H:
?k ? H .
Transitions between states are governed by
Markov dynamics parameterized by pi, where
?ij = p(zc(t) = j|zt = i) and pik are the transition
probabilities from the parent?s state k. pik is dis-
tributed according to a Dirichlet distribution with
parameter ?: pik|? ? Dirichlet(?, . . . , ?). The
hidden state of each child zt? is distributed accord-
ing to a multinomial distributionpizt specific to the
parent?s state zt: zt? |zt ? Multinomial(pizt).
2.2 Infinite Tree Model
In the infinite tree model, the number of possible
hidden states is potentially infinite. The infinite
model is formed by extending the finite tree model
using a hierarchical Dirichlet process (HDP) (Teh
et al, 2006). The reason for using an HDP rather
1Finkel et al (2007) originally proposed three types of
models: besides the independent children model, the simul-
taneous children model and the markov children model. Al-
though we could apply the other two models, we leave this
for future work.
842
H ?k
?k?0 z1
z2 z3
x1 x2 x3?
? ?
Hk
k
~
),(DP~,|
)(GEM~|
00? ????pi
???
Figure 3: A Graphical Representation of the Infi-
nite Tree Model
than a simple Dirichlet process (DP)2 (Ferguson,
1973) is that we have to introduce coupling across
transitions from different parent?s states. A similar
measure was adopted in iHMM (Beal et al, 2001).
HDP is a set of DPs coupled through a shared
random base measure which is itself drawn from
a DP: each Gk ? DP(?0, G0) with a shared base
measure G0, and G0 ? DP(?,H) with a global
base measure H . From the viewpoint of the stick-
breaking construction3 (Sethuraman, 1994), the
HDP is interpreted as follows: G0 =
??
k?=1
?k???k?
and Gk =
??
k?=1
?kk???k? , where ? ? GEM(?),
pik ? DP(?0,?), and ?k? ? H .
We regard each Gk as two coindexed distribu-
tions: pik, a distribution over the transition prob-
abilities from the parent?s state k, and ?k? , an ob-
servation distribution for the state k?. Then, the
infinite tree model is formally defined as follows:
?|? ? GEM(?),
pik|?0,? ? DP(?0,?),
?k ? H,
zt? |zt ? Multinomial(pizt),
xt|zt ? F (?zt).
Figure 3 shows the graphical representation of the
infinite tree model. The primary difference be-
2DP is a measure on measures. It has two parameters, a
scaling parameter ? and a base measure H: DP (?,H).
3Sethuraman (1994) showed a definition of a measure
G ? DP(?0, G0). First, infinite sequences of i.i.d variables
(??k)?k=1 and (?k)?k=1 are generated: ??k|?0 ? Beta(1, ?0),
?k ? G0. Then, G is defined as: ?k = ??k
?k?1
l=1 (1 ? ??l),
G = ??k=1 ?k??k . If pi is defined by this process, then we
write pi ? GEM(?0).
H ?k
?k?0
?
? ? z1
z2 z3
z4 z5 z6
???
+pay? ??????+fees? ???+usage???+I? ???
Figure 4: An Example of the Joint Model
tween Figure 2 and Figure 3 is whether the number
of copies of the state is finite or not.
3 Bilingual Infinite Tree Model
We propose a bilingual variant of the infinite tree
model, the bilingual infinite tree model, which uti-
lizes information from the other language. Specifi-
cally, the proposed model introduces bilingual ob-
servations by embedding the aligned target words
in the source-side dependency trees. This paper
proposes two types of models that differ in their
processes for generating observations: the joint
model and the independent model.
3.1 Joint Model
The joint model is a simple application of the in-
finite tree model under a bilingual scenario. The
model is formally defined in the same way as in
Section 2.2 and is graphically represented simi-
larly to Figure 3. The only difference from the
infinite tree model is the instances of observations
(xt). Observations in the joint model are the com-
bination of source words and their aligned target
words4, while observations in the monolingual in-
finite tree model represent only source words. For
each source word, all the aligned target words are
copied and sorted in alphabetical order, and then
concatenated into a single observation. Therefore,
a single target word may be emitted multiple times
if the target word is aligned with multiple source
words. Likewise, there may be target words which
may not be emitted by our model, if the target
words are not aligned.
Figure 4 shows the process of generating Exam-
ple 2 in Figure 1 through the joint model, where
aligned words are jointly emitted as observations.
In Figure 4, the POS tag of ???? (z5) generates
4When no target words are aligned, we simply add a
NULL target word.
843
H ?k
?k?0
? ? z1
z2 z3
z4 z5
H? ?'k
?? pay
I?
???
??
? NONE NONEusage
fees z6
?
'~',~
),(DP~,|
)(GEM~|
00 HH kk
k ?? ????pi
???
Figure 5: A Graphical Representation of the Inde-
pendent Model
the string ???+usage? as the observation (x5).
Similarly, the POS tag of ???? in Example 1
would generate the string ???+use?. Hence, this
model can assign different POS tags to the two dif-
ferent instances of the word ????, based on the
different observation distributions in inference.
3.2 Independent Model
The joint model is prone to a data sparseness prob-
lem, since each observation is a combination of a
source word and its aligned target word. Thus, we
propose an independent model, where each hidden
state generates a source word and its aligned target
word separately. For the aligned target side, we in-
troduce an observation variable x?t for each zt and
a parameter ??k for each state k, which parame-
terizes a distinct distribution over the observations
x?t for that state. ??k is distributed according to a
prior distribution H ?. Specifically, the indepen-
dent model is formally defined as follows:
?|? ? GEM(?),
pik|?0,? ? DP(?0,?),
?k ? H, ??k ? H ?,
zt? |zt ? Multinomial(pizt),
xt|zt ? F (?zt), x?t|zt ? F ?(??zt).
When multiple target words are aligned to a single
source word, each aligned word is generated sepa-
rately from observation distribution parameterized
by ??k.
Figure 5 graphs the process of generating Ex-
ample 2 in Figure 1 using the independent model.
x?t and ??k are introduced for aligned target words.
The state of ???? (z5) generates the Japanese
word ???? as x5 and the English word ?usage?
as x?5. Due to this factorization, the independent
model is less subject to the sparseness problem.
3.3 Introduction of Other Factors
We assumed the surface form of aligned target
words as additional observations in previous sec-
tions. Here, we introduce additional factors, i.e.,
the POS of aligned target words, in the observa-
tions. Note that POSs of target words are assigned
by a POS tagger in the target language and are not
inferred in the proposed model.
First, we can simply replace surface forms of
target words with their POSs to overcome the
sparseness problem. Second, we can incorporate
both information from the target language as ob-
servations. In the joint model, two pieces of in-
formation are concatenated into a single observa-
tion. In the independent model, we introduce ob-
servation variables (e.g., x?t and x??t ) and parame-
ters (e.g., ??k and ???k) for each information. Specif-
ically, x?t and ??k are introduced for the surface
form of aligned words, and x??t and ???k for the POS
of aligned words. Consider, for example, Example
1 in Figure 1. The POS tag of ???? generates the
string ???+use+verb? as the observation in the
joint model, while it generates ????, ?use?, and
?verb? independently in the independent model.
3.4 POS Refinement
We have assumed a completely unsupervised way
of inducing POS tags in dependency trees. An-
other realistic scenario is to refine the existing POS
tags (Finkel et al, 2007; Liang et al, 2007) so
that each refined sub-POS tag may reflect the in-
formation from the aligned words while preserv-
ing the handcrafted distinction from original POS
tagset. Major difference is that we introduce sep-
arate transition probabilities pisk and observation
distributions (?sk, ?
?s
k ) for each existing POS tag s.
Then, each node t is constrained to follow the dis-
tributions indicated by the initially assigned POS
tag st, and we use the pair (st, zt) as a state repre-
sentation.
3.5 Inference
In inference, we find the state set that maximizes
the posterior probability of state transitions given
observations (i.e., P (z1:n|x1:n)). However, we
cannot evaluate the probability for all possible
states because the number of states is infinite.
Finkel et al (2007) presented a sampling algo-
rithm for the infinite tree model, which is based on
the Gibbs sampling in the direct assignment rep-
resentation for iHMM (Teh et al, 2006). In the
844
Gibbs sampling, individual hidden state variables
are resampled conditioned on all other variables.
Unfortunately, its convergence is slow in HMM
settings because sequential data is likely to have
a strong correlation between hidden states (Gael
et al, 2008).
We present an inference procedure based on
beam sampling (Gael et al, 2008) for the joint
model and the independent model. Beam sam-
pling limits the number of possible state transi-
tions for each node to a finite number using slice
sampling (Neal, 2003), and then efficiently sam-
ples whole hidden state transitions using dynamic
programming. Beam sampling does not suffer
from slow convergence as in Gibbs sampling by
sampling the whole state variables at once. In ad-
dition, Gael et al (2008) showed that beam sam-
pling is more robust to initialization and hyperpa-
rameter choice than Gibbs sampling.
Specifically, we introduce an auxiliary variable
ut for each node in a dependency tree to limit
the number of possible transitions. Our procedure
alternates between sampling each of the follow-
ing variables: the auxiliary variables u, the state
assignments z, the transition probabilities pi, the
shared DP parameters ?, and the hyperparameters
?0 and ?. We can parallelize procedures in sam-
pling u and z because the slice sampling for u and
the dynamic programing for z are independent for
each sentence. See Gael el al. (2009) for details.
The only difference between inferences in the
joint model and the independent model is in com-
puting the posterior probability of state transi-
tions given observations (e.g., p(z1:n|x1:n) and
p(z1:n|x1:n, x?1:n)) in sampling z. In the follow-
ing, we describe each sampling stage. See Teh et
al., (2006) for details of sampling pi, ?, ?0 and ?.
Sampling u:
Each ut is sampled from the uniform distribu-
tion on [0, ?zd(t)zt ], where d(t) is the parent of
t: ut ? Uniform(0, ?zd(t)zt). Note that ut is a
positive number, since each transition probability
?zd(t)zt is larger than zero.
Sampling z:
Possible values k of zt are divided into the two
sets using ut: a finite set with ?zd(t)k > ut and
an infinite set with ?zd(t)k ? ut. The beam
sampling considers only the former set. Owing
to the truncation of the latter set, we can compute
the posterior probability of a state zt given ob-
servations for all t (t = 1, . . . , T ) using dynamic
programming as follows:
In the joint model, p(zt|x?(t), u?(t)) ?
p(xt|zt) ?
?
zd(t):?zd(t)zt>ut
p(zd(t)|x?(d(t)), u?(d(t))),
and in the independent model,
p(zt|x?(t), x??(t), u?(t)) ? p(xt|zt) ? p(x?t|zt)
?
?
zd(t):?zd(t)zt>ut
p(zd(t)|x?(d(t)), x??(d(t)), u?(d(t))),
where x?(t) (or u?(t)) denotes the set of xt (or ut)
on the path from the root node to the node t in a
tree.
In our experiments, we assume that F (?k)
is Multinomial(?k) and H is Dirichlet(?, . . . , ?),
which is the same in Finkel et al (2007). Un-
der this assumption, the posterior probability of an
observation is as follows: p(xt|zt) =
n?xtk + ?
n??k + N?
,
where n?xk is the number of observations x with
state k, n??k is the number of hidden states whose
values are k, and N is the total number of observa-
tions x. Similarly, p(x?t|zt) =
n?x?tk + ?
?
n??k + N ???
, where
N ? is the total number of observations x?.
When the posterior probability of a state zt
given observations for all t can be computed,
we first sample the state of each leaf node and
then perform backtrack sampling for every other
zt where the zt is sampled given the sample
for zc(t) as follows: p(zt|zc(t), x1:T , u1:T ) ?
p(zt|x?(t), u?(t))
?
t??c(t) p(zt? |zt, ut?).
Sampling pi:
We introduce a count variable nij ? n,
which is the number of observations with
state j whose parent?s state is i. Then,
we sample pi using the Dirichlet distri-
bution: (?k1, . . . , ?kK ,
??
k?=K+1 ?kk?) ?
Dirichlet(nk1 + ?0?1, . . . , nkK +
?0?K , ?0
??
k?=K+1 ?k?), where K is the
number of distinct states in z.
Sampling ?:
We introduce a set of auxiliary variables m, where
mij ? m is the number of elements of pij
corresponding to ?i. The conditional distribu-
tion of each variable is p(mij = m|z,?, ?0) ?
S(nij ,m)(?0?j)m, where S(n,m) are unsigned
Stirling numbers of the first kind5.
5S(0, 0) = S(1, 1) = 1, S(n, 0) = 0 for n > 0,
S(n,m) = 0 for m > n, and S(n + 1,m) = S(n,m ?
1) + nS(n,m) for others.
845
The parameters ? are sampled using the Dirich-
let distribution: (?1, . . . , ?K ,
??
k?=K+1 ?k?) ?
Dirichlet(m?1, . . . ,m?K , ?), where m?k =?K
k?=1 mk?k.
Sampling ?0:
?0 is parameterized by a gamma hyperprior
with hyperparameters ?a and ?b. We introduce
two types of auxiliary variables for each state
(k = 1, . . . ,K), wk ? [0, 1] and vk ? {0, 1}.
The conditional distribution of each wk is
p(wk|?0) ? w?0k (1?wk)n?k?1 and that of each vk
is p(vk|?0) ? (
n?k
?0
)
vk
, where n?k =
?K
k?=1 nk?k.
The conditional distribution of ?0 given wk
and vk (k = 1, . . . ,K) is p(?0|w,v) ?
??a?1+m..?
?K
k=1 vk
0 e??0(?b?
?K
k=1 logwk), where
m?? =
?K
k?=1
?K
k??=1 mk?k?? .
Sampling ?:
? is parameterized by a gamma hyperprior with
hyperparameters ?a and ?b. We introduce an
auxiliary variable ?, whose conditional distribu-
tion is p(?|?) ? ??(1 ? ?)m???1. The con-
ditional distribution of ? given ? is p(?|?) ?
??a?1+Ke??(?b?log?).
4 Experiment
We tested our proposed models under the
NTCIR-9 Japanese-to-English patent translation
task (Goto et al, 2011), consisting of approxi-
mately 3.2 million bilingual sentences. Both the
development data and the test data consist of 2,000
sentences. We also used the NTCIR-7 develop-
ment data consisting of 2,741 sentences for devel-
opment testing purposes.
4.1 Experimental Setup
We evaluated our bilingual infinite tree model
for POS induction using an in-house developed
syntax-based forest-to-string SMT system. In
the training process, the following steps are per-
formed sequentially: preprocessing, inducing a
POS tagset for a source language, training a POS
tagger and a dependency parser, and training a
forest-to-string MT model.
Step 1. Preprocessing
We used the first 10,000 Japanese-English sen-
tence pairs in the NTCIR-9 training data for in-
ducing a POS tagset for Japanese6. The Japanese
sentences were segmented using MeCab7, and the
English sentences were tokenized and POS tagged
using TreeTagger (Schmid, 1994), where 43 and
58 types of POS tags are included in the Japanese
sentences and the English sentences, respectively.
The Japanese POS tags come from the second-
level POS tags in the IPA POS tagset (Asahara and
Matsumoto, 2003) and the English POS tags are
derived from the Penn Treebank. Note that the
Japanese POS tags are used for initialization of
hidden states and the English POS tags are used
as observations emitted by hidden states.
Word-by-word alignments for the sentence
pairs are produced by first running GIZA++ (Och
and Ney, 2003) in both directions and then com-
bining the alignments using the ?grow-diag-final-
and? heuristic (Koehn et al, 2003). Note that we
ran GIZA++ on all of the NTCIR-9 training data
in order to obtain better alignements.
The Japanese sentences are parsed using
CaboCha (Kudo and Matsumoto, 2002), which
generates dependency structures using a phrasal
unit called a bunsetsu8, rather than a word unit as
in English or Chinese dependency parsing. Since
we focus on the word-level POS induction, each
bunsetsu-based dependency tree is converted into
its corresponding word-based dependency tree us-
ing the following heuristic9: first, the last func-
tion word inside each bunsetsu is identified as
the head word10; then, the remaining words are
treated as dependents of the head word in the same
bunsetsu; finally, a bunsetsu-based dependency
structure is transformed to a word-based depen-
dency structure by preserving the head/modifier
relationships of the determined head words.
Step 2. POS Induction
A POS tag for each word in the Japanese sentences
is inferred by our bilingual infinite tree model, ei-
6Due to the high computational cost, we did not use all
the NTCIR-9 training data. We leave scaling up to a larger
dataset for future work.
7http://mecab.googlecode.com/svn/
trunk/mecab/doc/index.html
8A bunsetsu is the smallest meaningful sequence con-
sisting of a content word and accompanying function words
(e.g., a noun and a particle).
9We could use other word-based dependency trees such
as trees by the infinite PCFG model (Liang et al, 2007)
and syntactic-head or semantic-head dependency trees in
Nakazawa and Kurohashi (2012), although it is not our major
focus. We leave this for future work.
10If no function words exist in a bunsetsu, the last content
word is treated as the head word.
846
ther jointly (Joint) or independently (Ind). We
also performed monolingual induction of Finkel et
al. (2007) for comparison (Mono). In each model,
a sequence of sampling u, z, pi, ?, ?0, and ? is
repeated 10,000 times. In sampling ?0 and ?, hy-
perparameters ?a, ?b, ?a, and ?b are set to 2, 1,
1, and 1, respectively, which is the same setting in
Gael et al (2008). In sampling z, parameters ?, ??,
. . ., are set to 0.01. In the experiments, three types
of factors for the aligned English words are com-
pared: surface forms (?s?), POS tags (?P?), and the
combination of both (?s+P?). Further, two types of
inference frameworks are compared: induction
(IND) and refinement (REF ). In both frame-
works, each hidden state zt is first initialized to
the POS tags assigned by MeCab (the IPA POS
tagset), and then each state is updated through
the inference procedure described in Section 3.5.
Note that in REF , the sampling distribution over
zt is constrained to include only states that are a
refinement of the initially assigned POS tag.
Step 3. Training a POS Tagger and a
Dependency Parser
In this step, we train a Japanese dependency parser
from the 10,000 Japanese dependency trees with
the induced POS tags which are derived from Step
2. We employed a transition-based dependency
parser which can jointly learn POS tagging and
dependency parsing (Hatori et al, 2011) under an
incremental framework11. Note that the learned
parser can identify dependencies between words
and attach an induced POS tag for each word.
Step 4. Training a Forest-to-String MT
In this step, we train a forest-to-string MT model
based on the learned dependency parser in Step 3.
We use an in-house developed hypergraph-based
toolkit, cicada, for training and decoding with a
tree-to-string model, which has been successfully
employed in our previous work for system com-
bination (Watanabe and Sumita, 2011) and online
learning (Watanabe, 2012). All the Japanese and
English sentences in the NTCIR-9 training data
are segmented in the same way as in Step 1, and
then each Japanese sentence is parsed by the de-
pendency parser learned in Step 3, which simul-
taneously assigns induced POS tags and word de-
pendencies. Finally, a forest-to-string MT model
is learned with Zhang et al, (2011), which ex-
tracts translation rules by a forest-based variant of
11http://triplet.cc/software/corbit/
IND REF
BS 27.54
Mono 27.66 26.83
Joint[s] 28.00 28.00
Joint[P] 26.36 26.72
Joint[s+P] 27.99 27.82
Ind[s] 28.00 27.93
Ind[P] 28.11 28.63
Ind[s+P] 28.13 28.62
Table 1: Performance on Japanese-to-English
Translation Measured by BLEU (%)
the GHKM algorithm (Mi and Huang, 2008) af-
ter each parse tree is restructured into a binarized
packed forest. Parameters are tuned on the devel-
opment data using xBLEU (Rosti et al, 2011) as
an objective and L-BFGS (Liu and Nocedal, 1989)
as an optimization toolkit, since it is stable and less
prone to randomness, unlike MERT (Och, 2003)
or PRO (Hopkins and May, 2011). The develop-
ment test data is used to set up hyperparameters,
i.e., to terminate tuning iterations.
When translating Japanese sentences, a parse
tree for each sentence is constructed in the same
way as described earlier in this step, and then the
parse trees are translated into English sentences
using the learned forest-to-string MT model.
4.2 Experimental Results
Table 1 shows the performance for the test data
measured by case sensitive BLEU (Papineni et
al., 2002). We also present the performance of
our baseline forest-to-string MT system (BS) us-
ing the original IPA POS tags. In Table 1, num-
bers in bold indicate that the systems outperform
the baselines, BS and Mono. Under the Moses
phrase-based SMT system (Koehn et al, 2007)
with the default settings, we achieved a 26.80%
BLEU score.
Table 1 shows that the proposed systems outper-
form the baseline Mono. The differences between
the performance of Ind[s+P] and Mono are statis-
tically significant in the bootstrap method (Koehn,
2004), with a 1% significance level both in IND
and REF . The results indicate that integrating the
aligned target-side information in POS induction
makes inferred tagsets more suitable for SMT.
Table 1 also shows that the independent model
is more effective for SMT than the joint model.
This means that sparseness is a severe problem in
847
Model IND REF
Joint[s+P] 164 620
Ind[s+P] 102 517
IPA POS tags 42
Table 2: The Number of POS Tags
POS induction when jointly encoding bilingual in-
formation into observations. Additionally, all the
systems using the independent model outperform
BS. The improvements are statistically significant
in the bootstrap method (Koehn, 2004), with a 1%
significance level. The results show that the pro-
posed models can generate more favorable POS
tagsets for SMT than an existing POS tagset.
In Table 1, REF s are at least comparable to, or
better than, INDs except for Mono. This shows
that REF achieves better performance by preserv-
ing the clues from the original POS tagset. How-
ever, REF may suffer sever overfitting problem
for Mono since no bilingual information was in-
corporated. Further, when the full-level IPA POS
tags12 were used in BS, the system achieved a
27.49% BLEU score, which is worse than the re-
sult using the second-level IPA POS tags. This
means that manual refinement without bilingual
information may also cause an overfitting problem
in MT.
5 Discussion
5.1 Comparison to the IPA POS Tagset
Table 2 shows the number of the IPA POS tags
used in the experiments and the POS tags induced
by the proposed models. This table shows that
each induced tagset contains more POS tags than
the IPA POS tagset. In the experimental data,
some of Japanese verbs correspond to genuine En-
glish verbs, some are nominalized, and others cor-
respond to English past participle verbs or present
participle verbs which modify other words. Re-
spective examples are ?I use a card.?, ?Using the
index is faster.?, and ?I explain using an exam-
ple.?, where all the underlined words correspond
to the same Japanese word, ????, whose IPA
POS tag is a verb. Ind[s+P] in REF generated
the POS tagset where the three types are assigned
to separate POS groups.
The Japanese particle ??? is sometimes at-
tached to nouns to give them adverb roles. For
12377 types of full-level IPA POS tags were included in our
experimental data.
Tagging Dependency
IND REF IND REF
Original 90.37 93.62
Mono 90.75 88.04 91.77 91.51
Joint[s] 89.08 86.73 91.55 91.14
Joint[P] 80.54 79.98 91.06 91.29
Joint[s+P] 87.56 84.92 91.31 91.10
Ind[s] 87.62 84.33 92.06 92.58
Ind[P] 90.21 88.50 92.85 93.03
Ind[s+P] 89.57 86.12 92.96 92.78
Table 3: Tagging and Dependency Accuracy (%)
example, ??? (mutual) ??? is translated as
the adverb ?mutually? in English. Other times,
it is attached to words to make them the objects
of verbs. For example, ?? (he) ??????
(give)? is translated as ?give him?. The POS tags
by Ind[s+P] in REF discriminated the two types.
These examples show that the proposed mod-
els can disambiguate POS tags that have different
functions in English, whereas the IPA POS tagset
treats them jointly. Thus, such discrimination im-
proves the performance of a forest-to-string SMT.
5.2 Impact of Tagging and Dependency
Accuracy
The performance of our methods depends not only
on the quality of the induced tag sets but also on
the performance of the dependency parser learned
in Step 3 of Section 4.1. We cannot directly eval-
uate the tagging accuracy of the parser trained
through Step 3 because we do not have any data
with induced POS tags other than the 10,000-
sentence data gained through Step 2. Thus we split
the 10,000 data into the first 9,000 data for train-
ing and the remaining 1,000 for testing, and then
a dependency parser was learned in the same way
as in Step 3.
Table 3 shows the results. Original is the per-
formance of the parser learned from the training
data with the original POS tagset. Note that the de-
pendency accuracies are measured on the automat-
ically parsed dependency trees, not on the syntac-
tically correct gold standard trees. Thus Original
achieved the best dependency accuracy.
In Table 3, the performance for our bilingually-
induced POSs, Joint and Ind, are lower than
Original and Mono. It seems performing pars-
ing and tagging with the bilingually-induced POS
tagset is too difficult when only monolingual in-
848
formation is available to the parser. However, our
bilingually-induced POSs, except for Joint[P ],
with the lower accuracies are more effective for
SMT than the monolingually-induced POSs and
the original POSs, as indicated in Table 1. The
tagging accuracies for Joint[P ] both in IND and
REF are significantly lower than the others, while
the dependency accuracies do not differ signifi-
cantly. The lower tagging accuracies may directly
reflect the lower translation qualities for Joint[P ]
in Table 1.
6 Conclusion
We proposed a novel method for inducing POS
tags for SMT. The proposed method is a non-
parametric Bayesian method, which infers hidden
states (i.e., POS tags) based on observations repre-
senting not only source words themselves but also
aligned target words. Our experiments showed
that a more favorable POS tagset can be induced
by integrating aligned information, and further-
more, the POS tagset generated by the proposed
method is more effective for SMT than an existing
POS tagset (the IPA POS tagset).
Even though we employed word alignment
from GIZA++ with potential errors, large gains
were achieved using our proposed method. We
would like to investigate the influence of align-
ment errors in the future. In addition, we are plan-
ning to prove the effectiveness of our proposed
method for language pairs other than Japanese-to-
English. We are also planning to introduce our
proposed method to other syntax-based SMT, such
as a string-to-tree SMT and a tree-to-tree SMT.
Acknowledgments
We thank Isao Goto for helpful discussions and
anonymous reviewers for valuable comments. We
also thank Jun Hatori for helping us to apply his
software, Corbit, to our induced POS tagsets.
References
Masayuki Asahara and Yuji Matsumoto. 2003.
IPADIC User Manual. Technical report, Japan.
Matthew J. Beal, Zoubin Ghahramani, and Carl E. Ras-
mussen. 2001. The Infinite Hidden Markov Model.
In Advances in Neural Information Processing Sys-
tems, pages 577?584.
Phil Blunsom and Trevor Cohn. 2011. A Hierarchical
Pitman-Yor Process HMM for Unsupervised Part of
Speech Induction. In Proceedings of the 49th An-
nual Meeting of the Association for Computational
Linguistics, pages 865?874.
Trevor Cohn and Phil Blunsom. 2009. A Bayesian
Model of Syntax-Directed Tree to String Grammar
Induction. In Proceedings of the 2009 Conference
on Empirical Methods in Natural Language Pro-
cessing, pages 352?361.
Yuan Ding and Martha Palmer. 2005. Machine Trans-
lation Using Probabilistic Synchronous Dependency
Insertion Grammars. In Proceedings of the 43rd An-
nual Meeting of the Association for Computational
Linguistics, pages 541?548.
Thomas S. Ferguson. 1973. A Bayesian Analysis
of Some Nonparametric Problems. The Annals of
Statistics, 1(2):209?230.
Jenny Rose Finkel, Trond Grenager, and Christo-
pher D. Manning. 2007. The Infinite Tree. In Pro-
ceedings of the 45th Annual Meeting of the Associa-
tion of Computational Linguistics, pages 272?279.
Jurgen Van Gael, Yunus Saatci, Yee Whye Teh, and
Zoubin Ghahramani. 2008. Beam Sampling for
the Infinite Hidden Markov Model. In Proceedings
of the 25th International Conference on Machine
Learning, pages 1088?1095.
Jurgen Van Gael, Andreas Vlachos, and Zoubin
Ghahramani. 2009. The infinite HMM for unsuper-
vised PoS tagging. In Proceedings of the 2009 Con-
ference on Empirical Methods in Natural Language
Processing: Volume 2 - Volume 2, pages 678?687.
Michel Galley, Jonathan Graehl, Kevin Knight, Daniel
Marcu, Steve DeNeefe, Wei Wang, and Ignacio
Thayer. 2006. Scalable Inference and Training
of Context-Rich Syntactic Translation Models. In
Proceedings of the 21st International Conference on
Computational Linguistics and 44th Annual Meet-
ing of the Association for Computational Linguis-
tics, pages 961?968.
Isao Goto, Bin Lu, Ka Po Chow, Eiichiro Sumita, and
Benjamin K. Tsou. 2011. Overview of the Patent
Machine Translation Task at the NTCIR-9 Work-
shop. In Proceedings of the 9th NTCIR Workshop,
pages 559?578.
Jun Hatori, Takuya Matsuzaki, Yusuke Miyao, and
Jun?ichi Tsujii. 2011. Incremental Joint POS Tag-
ging and Dependency Parsing in Chinese. In Pro-
ceedings of 5th International Joint Conference on
Natural Language Processing, pages 1216?1224.
Mark Hopkins and Jonathan May. 2011. Tuning as
Ranking. In Proceedings of the 2011 Conference on
Empirical Methods in Natural Language Process-
ing, pages 1352?1362.
Liang Huang, Kevin Knight, and Aravind Joshi. 2006.
A Syntax-Directed Translator with Extended Do-
main of Locality. In Proceedings of the Workshop on
849
Computationally Hard Problemsand Joint Inference
in Speech and Language Processing, pages 1?8.
Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical Phrase-Based Translation. In Pro-
ceedings of the 2003 Human Language Technology
Conference: North American Chapter of the Associ-
ation for Computational Linguistics, pages 48?54.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran,
Richard Zens, Chris Dyer, Ondrej Bojar, Alexan-
dra Constrantin, and Evan Herbst. 2007. Moses:
Open Source Toolkit for Statistical Machine Trans-
lation. In Proceedings of the 45th Annual Meeting of
the Association for Computational Linguistics on In-
teractive Poster and Demonstration Sessions, pages
177?180.
Philipp Koehn. 2004. Statistical Significance Tests for
Machine Translation Evaluation. In Proceedings of
the 2004 Conference on Empirical Methods in Nat-
ural Language Processing, pages 388?395.
Taku Kudo and Yuji Matsumoto. 2002. Japanese De-
pendency Analysis using Cascaded Chunking. In
Proceedings of the 6th Conference on Natural Lan-
guage Learning, pages 63?69.
Percy Liang, Slav Petrov, Michael I. Jordan, and Dan
Klein. 2007. The Infinite PCFG using Hierarchi-
cal Dirichlet Processes. In Proceedings of the 2007
Joint Conference on Empirical Methods in Natural
Language Processing and Computational Natural
Language Learning, pages 688?697.
Dekang Lin. 2004. A Path-based Transfer Model for
Machine Translation. In Proceedings of the 20th In-
ternational Conference on Computational Linguis-
tics, pages 625?630.
Dong C. Liu and Jorge Nocedal. 1989. On the limited
memory BFGS method for large scale optimization.
Mathematical Programming B, 45(3):503?528.
Yang Liu, Qun Liu, and Shouxun Lin. 2006. Tree-to-
String Alignment Template for Statistical Machine
Translation. In Proceedings of the 21st Interna-
tional Conference on Computational Linguistics and
44th Annual Meeting of the Association for Compu-
tational Linguistics, pages 609?616.
Yang Liu, Yajuan Lu?, and Qun Liu. 2009. Improv-
ing Tree-to-Tree Translation with Packed Forests.
In Proceedings of the 47th Annual Meeting of the
Association for Computational Linguistics and the
4th International Joint Conference on Natural Lan-
guage Processing of the Asian Federation of Natural
Language Processing, pages 558?566.
Haitao Mi and Liang Huang. 2008. Forest-based
Translation Rule Extraction. In Proceedings of the
2008 Conference on Empirical Methods in Natural
Language Processing, pages 206?214.
Haitao Mi and Qun Liu. 2010. Constituency to De-
pendency Translation with Forests. In Proceedings
of the 48th Annual Conference of the Association for
Computational Linguistics, pages 1433?1442.
Toshiaki Nakazawa and Sadao Kurohashi. 2012.
Alignment by Bilingual Generation and Monolin-
gual Derivation. In Proceedings of the 24th Inter-
national Conference on Computational Linguistics,
pages 1963?1978.
Radford M. Neal. 2003. Slice Sampling. Annals of
Statistics, 31:705?767.
Franz Josef Och and Hermann Ney. 2003. A System-
atic Comparison of Various Statistical Alignment
Models. Computational Linguistics, 29:19?51.
Franz Josef Och. 2003. Minimum Error Rate Training
in Statistical Machine Translation. In Proceedings
of the 41st Annual Meeting of the Association for
Computational Linguistics, pages 160?167.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: a Method for Automatic
Evaluation of Machine Translation. In Proceedings
of 40th Annual Meeting of the Association for Com-
putational Linguistics, pages 311?318.
Chris Quirk, Arul Menezes, and Colin Cherry. 2005.
Dependency Treelet Translation: Syntactically In-
formed Phrasal SMT. In Proceedings of the 43rd
Annual Conference of the Association for Computa-
tional Linguistics, pages 271?279.
Antti-Veikko Rosti, Bing Zhang, Spyros Matsoukas,
and Richard Schwartz. 2011. Expected BLEU
Training for Graphs: BBN System Description for
WMT11 System Combination Task. In Proceedings
of the Sixth Workshop on Statistical Machine Trans-
lation, pages 159?165.
Helmut Schmid. 1994. Probabilistic Part-of-Speech
Tagging Using Decision Trees. In Proceedings of
the International Conference on New Methods in
Language Processing, pages 44?49.
Jayaram Sethuraman. 1994. A Constructive Definition
of Dirichlet Priors. Statistica Sinica, 4(2):639?650.
Libin Shen, Jinxi Xu, and Ralph Weischedel. 2008.
A New String-to-Dependency Machine Translation
Algorithm with a Target Dependency Language
Model. In Proceedings of the 46th Annual Confer-
ence of the Association for Computational Linguis-
tics: Human Language Technologies, pages 577?
585.
Kairit Sirts and Tanel Aluma?e. 2012. A Hierarchi-
cal Dirichlet Process Model for Joint Part-of-Speech
and Morphology Induction. In Proceedings of the
2012 Conference of the North American Chapter of
the Association for Computational Linguistics: Hu-
man Language Technologies, pages 407?416.
850
Yee Whye Teh, Michael I. Jordan, Matthew J. Beal,
and David M. Blei. 2006. Hierarchical Dirichlet
Processes. Journal of the American Statistical Asso-
ciation, 101(476):1566?1581.
Taro Watanabe and Eiichiro Sumita. 2011. Machine
Translation System Combination by Confusion For-
est. In Proceedings of the 49th Annual Meeting of
the Association for Computational Linguistics: Hu-
man Language Technologies, pages 1249?1257.
Taro Watanabe. 2012. Optimized Online Rank Learn-
ing for Machine Translation. In Proceedings of the
2012 Conference of the North American Chapter of
the Association for Computational Linguistics: Hu-
man Language Technologies, pages 253?262.
Min Zhang, Hongfei Jiang, Aiti Aw, Haizhou Li,
Chew Lim Tan, and Sheng Li. 2008. A Tree Se-
quence Alignment-based Tree-to-Tree Translation
Model. In Proceedings of the 46th Annual Confer-
ence of the Association for Computational Linguis-
tics: Human Language Technologies, pages 559?
567.
Hao Zhang, Licheng Fang, Peng Xu, and Xiaoyun Wu.
2011. Binarized Forest to String Translation. In
Proceedings of the 49th Annual Meeting of the Asso-
ciation for Computational Linguistics, pages 19?24.
851
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 1023?1032,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
Subtree Extractive Summarization via Submodular Maximization
Hajime Morita
Tokyo Institute of Technology, Japan
morita@lr.pi.titech.ac.jp
Hiroya Takamura
Tokyo Institute of Technology, Japan
takamura@pi.titech.ac.jp
Ryohei Sasano
Tokyo Institute of Technology, Japan
sasano@pi.titech.ac.jp
Manabu Okumura
Tokyo Institute of Technology, Japan
oku@pi.titech.ac.jp
Abstract
This study proposes a text summarization
model that simultaneously performs sen-
tence extraction and compression. We
translate the text summarization task into
a problem of extracting a set of depen-
dency subtrees in the document cluster.
We also encode obligatory case constraints
as must-link dependency constraints in or-
der to guarantee the readability of the gen-
erated summary. In order to handle the
subtree extraction problem, we investigate
a new class of submodular maximization
problem, and a new algorithm that has
the approximation ratio 12(1 ? e?1). Ourexperiments with the NTCIR ACLIA test
collections show that our approach outper-
forms a state-of-the-art algorithm.
1 Introduction
Text summarization is often addressed as a task
of simultaneously performing sentence extraction
and sentence compression (Berg-Kirkpatrick et
al., 2011; Martins and Smith, 2009). Joint mod-
els of sentence extraction and compression have
a great benefit in that they have a large degree of
freedom as far as controlling redundancy goes. In
contrast, conventional two-stage approaches (Za-
jic et al, 2006), which first generate candidate
compressed sentences and then use them to gen-
erate a summary, have less computational com-
plexity than joint models. However, two-stage ap-
proaches are suboptimal for text summarization.
For example, when we compress sentences first,
the compressed sentences may fail to contain im-
portant pieces of information due to the length
limit imposed on each sentence. On the other
hand, when we extract sentences first, an impor-
tant sentence may fail to be selected, simply be-
cause it is long. Enumerating a huge number
of compressed sentences is also infeasible. Joint
models can prune unimportant or redundant de-
scriptions without resorting to enumeration.
Meanwhile, submodular maximization has re-
cently been applied to the text summarization task,
and the methods thereof have performed very well
(Lin and Bilmes, 2010; Lin and Bilmes, 2011;
Morita et al, 2011). Formalizing summarization
as a submodular maximization problem has an im-
portant benefit inthat the problem can be solved by
using a greedy algorithm with a performance guar-
antee.
We therefore decided to formalize the task of si-
multaneously performing sentence extraction and
compression as a submodular maximization prob-
lem. That is, we extract subsentences for mak-
ing the summary directly from all available sub-
sentences in the documents and not in a stepwise
fashion. However, there is a difficulty with such
a formalization. In the past, the resulting maxi-
mization problem has been often accompanied by
thousands of linear constraints representing logi-
cal relations between words. The existing greedy
algorithm for solving submodular maximization
problems cannot work in the presence of such nu-
merous constraints although monotone and non-
monotone submodular maximization with con-
straints other than budget constraints have been
studied (Lee et al, 2009; Kulik et al, 2009; Gupta
et al, 2010). In this study, we avoid this difficulty
by reducing the task to one of extracting depen-
dency subtrees from sentences in the source doc-
uments. The reduction replaces the difficulty of
numerous linear constraints with another difficulty
wherein two subtrees can share the same word to-
1023
ken when they are selected from the same sen-
tence, and as a result, the cost of the union of the
two subtrees is not always the mere sum of their
costs. We can overcome this difficulty by tackling
a new class of submodular maximization prob-
lem: a budgeted monotone nondecreasing sub-
modular function maximization with a cost func-
tion, where the cost of an extraction unit varies
depending on what other extraction units are se-
lected. By formalizing the subtree extraction prob-
lem as this new maximization problem, we can
treat the constraints regarding the grammaticality
of the compressed sentences in a straightforward
way and use an arbitrary monotone submodular
word score function for words including our word
score function (shown later). We also propose a
new greedy algorithm that solves this new class of
maximization problem with a performance guar-
antee 12(1? e?1).
We evaluated our method on by using it to per-
form query-oriented summarization (Tang et al,
2009). Experimental results show that it is supe-
rior to state-of-the-art methods.
2 Related Work
Submodularity is formally defined as a property of
a set function for a finite universe V . The function
f : 2V ? R maps a subset S ? V to a real value.
If for any S, T ? V , f(S ? T ) + f(S ? T ) ?
f(S)+f(T ), f is called submodular. This defini-
tion is equivalent to that of diminishing returns,
which is well known in the field of economics:
f(S ?{u})? f(S) ? f(T ?{u})? f(T ), where
T ? S ? V and u is an element of V . Di-
minishing returns means that the value of an el-
ement u remains the same or decreases as S be-
comes larger. This property is suitable for sum-
marization purposes, because the gain of adding a
new sentence to a summary that already contains
sufficient information should be small. Therefore,
many studies have formalized text summarization
as a submodular maximization problem (Lin and
Bilmes, 2010; Lin and Bilmes, 2011; Morita et
al., 2011). Their approaches, however, have been
based on sentence extraction. To our knowledge,
there is no study that addresses the joint task of
simultaneously performing compression and ex-
traction through an approximate submodular max-
imization with a performance guarantee.
In the field of constrained maximization prob-
lems, Kulik et al (2009) proposed an algorithm
that solves the submodular maximization problem
under multiple linear constraints with a perfor-
mance guarantee 1? e?1 in polynomial time. Al-
though their approach can represent more flexible
constraints, we cannot use their algorithm to solve
our problem, because their algorithm needs to enu-
merate many combinations of elements. Integer
linear programming (ILP) formulations can repre-
sent such flexible constraints, and they are com-
monly used to model text summarization (McDon-
ald, 2007). Berg-Kirkpatrick et al (2011) formu-
lated a unified task of sentence extraction and sen-
tence compression as an ILP. However, it is hard to
solve large-scale ILP problems exactly in a practi-
cal amount of time.
3 Budgeted Submodular Maximization
with Cost Function
3.1 Problem Definition
Let V be the finite set of all valid subtrees in
the source documents, where valid subtrees are
defined to be the ones that can be regarded as
grammatical sentences. In this paper, we regard
subtrees containing the root node of the sentence
as valid. Accordingly, V denotes a set of all
rooted subtrees in all sentences. A subtree con-
tains a set of elements that are units in a de-
pendency structure (e.g., morphemes, words or
clauses). Let us consider the following problem
of budgeted monotone nondecreasing submodu-
lar function maximization with a cost function:
maxS?V {f(S) : c (S) ? L} , where S is a sum-
mary represented as a set of subtrees, c(?) is the
cost function for the set of subtrees, L is our bud-
get, and the submodular function f(?) scores the
summary quality. The cost function is not always
the sum of the costs of the covered subtrees, but
depends on the set of the covered elements by the
subtrees. Here, we will assume that the generated
summary has to be as long as or shorter than the
given summary length limit, as measured by the
number of characters. This means the cost of a
subtree is the integer number of characters it con-
tains.
V is partitioned into exclusive subsetsB of valid
subtrees, and each subset corresponds to the orig-
inal sentence from which the valid subtrees de-
rived. However, the cost of a union of subtrees
from different sentences is simply the sum of the
costs of subtrees, while the cost of a union of sub-
trees from the same sentence is smaller than the
sum of the costs. Therefore, the problem can be
represented as follows:
1024
max
S?V
{
f(S) :
?
B?B
c (B ? S) ? L
}
. (1)
For example, if we add a subtree t containing
words {wa,wb,wc} to a summary that already
covers words {wa, wb, wd} from the same sen-
tence, the additional cost of t is only c({wc}) be-
cause wa and wb are already covered1.
The problem has two requirements. The first
requirement is that the union of valid subtrees is
also a valid subtree. The second requirement is
that the union of subtrees and a single valid sub-
tree have the same score and the same cost if they
cover the same elements. We will refer to the sin-
gle valid subtree as the equivalent subtree of the
union of subtrees. These requirements enable us
to represent sentence compression as the extrac-
tion of subtrees from a sentence. This is because
the requirements guarantee that the extracted sub-
trees represent a sentence.
3.2 Greedy Algorithm
We propose Algorithm 1 that solves the maximiza-
tion problem (Eq.1). The algorithm is based on
ones proposed by Khuller et al (1999) and Krause
et al (2005). Instead of enumerating all candidate
subtrees, we use a local search to extract the ele-
ment that has the highest gain per cost. In the al-
gorithm, Gi indicates a summary set obtained by
adding element si to Gi?1. U means the set of
subtrees that are not extracted. The algorithm it-
eratively adds to the current summary the element
si that has the largest ratio of the objective func-
tion gain to the additional cost, unless adding it
violates the budget constraint. We set a parame-
ter r that is the scaling factor proposed by Lin and
Bilmes (2010). After the loop, the algorithm com-
pares Gi with the {s?} that has the largest value of
the objective function among all subtrees that are
under the budget, and it outputs the summary can-
didate with the largest value.
Let us analyze the performance guarantee of Al-
gorithm 12.
1Each subset B corresponds to a kind of greedoid con-
straint. V implicitly constrains the model such that it can
only select valid subtrees from a set of nodes and edges.
2Our performance guarantee is lower than that reported
by Lin and Bilmes (2010). However, their proof is er-
roneous. In their proof of Lemma 2, they derive ?u ?
S?\Gi?1, ?u(Gi?1)Cru ?
?vi (Gi?1)
Crvi
, for any i(1 ? i ? |G|),
from line 4 of their Algorithm 1, which selects the densest
element out of all available elements. However, the inequal-
ity does not hold for i, for which element u selected on line
4 is discarded on line 5 of their algorithm. The performance
guarantee of their algorithm is actually the same as ours, since
Algorithm 1 Modified greedy algorithm for budgeted
submodular function maximization with a cost function .
1: G0 ? ?
2: U ? V
3: i? 1
4: while U 6= ? do
5: si ? argmaxs?U f(Gi?1?{s})?f(Gi?1)(c(Gi?1?{s})?c(Gi?1))r
6: if c({si} ?Gi?1) ? L then
7: Gi ? Gi?1 ? {si}
8: i? i + 1
9: end if
10: U ? U\{si}
11: end while
12: s?? argmaxs?V,c(s)?L f({s})
13: return Gf = argmaxS?{{s?},Gi} f(S)
Theorem 1 For a normalized monotone submod-
ular function f(?), Algorithm 1 has a constant
approximation factor when r = 1 as follows:
f(Gf ) ?
(1
2(1? e
?1)
)
f(S?), (2)
where S? is the optimal solution and, Gf is the
solution obtained by Greedy Algorithm 1.
Proof. See appendix.
3.3 Relation with Discrete Optimization
We argue that our optimization problem can be
regarded as an extraction of subtrees rooted at a
given node from a directed graph, instead of from
a tree. Let D be the set of edges of the directed
graph, F be a subset of D that is a subtree. In the
field of combinatorial optimization, a pair (D, F)
is a kind of greedoid: directed branching greedoid
(Schmidt, 1991). A greedoid is a generalization of
the matroid concept. However, while matroids are
often used to represent constraints on submodular
maximization problems (Conforti and Cornue?jols,
1984; Calinescu et al, 2011), greedoids have not
been used for that purpose, in spite of their high
representation ability. To our knowledge, this is
the first study that gives a constant performance
guarantee for the submodular maximization under
greedoid (non-matroid) constraints.
the guarantee 12 (1? e?1) was already proved by Krause andGuestrin (2005). We show a counterexample. Suppose that
V is { e1(density 4:cost 6), e2(density 2:cost 4), e3(density
3:cost 1), e4(density 1:cost 1) }, and cost limit K is 10. The
optimal solution is S? = {e1, e2}. Their algorithm selects
e1, e3, e4 in this order. However the algorithm selects e2 on
line 4 after selecting e3, and it drops e2 on line 5. As a result,
e4 selected by the algorithm does not satisfy the inequality
?u ? S?\Gi?1, ?u(Gi?1)Cru ?
?vi (Gi?1)
Crvi
.
1025
4 Joint Model of Extraction and
Compression
We will formalize the unified task of sentence
compression and extraction as a budgeted mono-
tone nondecreasing submodular function maxi-
mization with a cost function. In this formaliza-
tion, a valid subtree of a sentence represents a
candidate of a compressed sentence. We will re-
fer to all valid subtrees of a given sentence as a
valid set. A valid set corresponds to all candi-
dates of the compression of a sentence. Note that
although we use the valid set in the formaliza-
tion, we do not have to enumerate all the candi-
dates for each sentence. Since, from the require-
ments, the union of valid subtrees is also a valid
subtree in the valid set, the model can extract one
or more subtrees from one sentence, and generate
a compressed sentence by merging those subtrees
to generate an equivalent subtree. Therefore, the
joint model can extract an arbitrarily compressed
sentence as a subtree without enumerating all can-
didates. The joint model can remove the redundant
part as well as the irrelevant part of a sentence, be-
cause the model simultaneously extracts and com-
presses sentences. We can approximately solve the
subtree extraction problem by using Algorithm 1.
On line 5 of the algorithm, the subtree extraction
is performed as a local search that finds maximal
density subtrees from the whole documents. The
maximal density subtree is a subtree that has the
highest score per cost of subtree. We use a cost
function to represent the cost, which indicates the
length of word tokens in the subtree.
In this paper, we address the task of summariza-
tion of Japanese text by means of sentence com-
pression and extraction. In Japanese, syntactic
subtrees that contain the root of the dependency
tree of the original sentence often make gram-
matical sentences. This means that the require-
ments mentioned in Section 3.1 that a union of
valid subtrees is a valid and equivalent tree is of-
ten true for Japanese. The root indicates the pred-
icate of a sentence, and it is syntactically modi-
fied by other prior words. Some modifying words
can be pruned. Therefore, sentence compression
can be represented as edge pruning. The linguis-
tic units we extract are bunsetsu phrases, which
are syntactic chunks often containing a functional
word after one or more content words. We will re-
fer to bunsetsu phrases as phrases for simplicity.
Since Japanese syntactic dependency is generally
defined between two phrases, we use the phrases
as the nodes of subtrees.
In this joint model, we generate a compressed
sentence by extracting an arbitrary subtree from a
dependency tree of a sentence. However, not all
subtrees are always valid. The sentence generated
by a subtree can be unnatural even though the sub-
tree contains the root node of the sentence. To
avoid generating such ungrammatical sentences,
we need to detect and retain the obligatory de-
pendency relations in the dependency tree. We
address this problem by imposing must-link con-
straints if a phrase corresponds to an obligatory
case of the main predicate. We merge obligatory
phrases with the predicate beforehand so that the
merged nodes make a single large node.
Although we focus on Japanese in this pa-
per, our approach can be applied to English and
other languages if certain conditions are satisfied.
First, we need a dependency parser of the lan-
guage in order to represent sentence compression
as dependency tree pruning. Moreover, although,
in Japanese, obligatory cases distinguish which
edges of the dependency tree can be pruned or not,
we need another technique to distinguish them in
other languages. For example we can distinguish
obligatory phrases from optional ones by using se-
mantic role labeling to detect arguments of predi-
cates. The adaptation to other languages is left for
future work.
4.1 Objective Function
We extract subtrees from sentences in order to
solve the query-oriented summarization problem
as a unified one consisting of sentence compres-
sion and extraction. We thus need to allocate a
query relevance score to each node. Off-the-shelf
similarity measures such as the cosine similarity of
bag-of-words vectors with query terms would al-
locate scores to the terms that appear in the query,
but would give no scores to terms that do not ap-
pear in it. With such a similarity, sentence com-
pression extracts nearly only the query terms and
fails to contain important information. Instead,
we used Query SnowBall (QSB) (Morita et al,
2011) to calculate the query relevance score of
each phrase. QSB is a method for query-oriented
summarization, which calculates the similarity be-
tween query terms and each word by using co-
occurrences within the source documents. Al-
though the authors of QSB also provided scores
of word pairs to avoid putting excessive penalties
1026
on word overlaps, we do not score word pairs. The
score function is supermodular as a score function
of subtree extraction3, because the union of two
subtrees can have extra word pairs that are not in-
cluded in either subtree. If the extra pair has a pos-
itive score, the score of the union is greater than
the sum of the score of the subtrees. This violates
the definition of submodularity, and invalidates the
performance guarantee of our algorithms.
We designed our objective function by combin-
ing this relevance score with a penalty for redun-
dancy and too-compressed sentences. Important
words that describe the main topic should occur
multiple times in a good summary. However, ex-
cessive overlap undermines the quality of a sum-
mary, as do irrelevant words. Therefore, the scores
of overlapping words should be lower than thoseof
new words. The behavior can be represented by a
submodular objective function that reduces word
scores depending on those already included in the
summary. Furthermore, a summary consisting of
many too-compressed sentences would lack read-
ability. We thus gives a positive reward to long
sentences. The positive reward leads to a natu-
ral summary being generated with fewer sentences
and indirectly penalizes too short sentences. Our
positive reward for long sentences is represented
as
reward(S) = c(S)? |S|, (3)
where c(S) is the cost of summary S, and |S| is the
number of sentences in S. Since a sentence must
contain more than one character, the reward con-
sistently gives a positive score, and gives a higher
score to a summary that consists of fewer sen-
tences.
Let d be the damping rate, countS(w) be the
number of sentences containing word w in sum-
mary S, words(S) be the set of words included in
summary S, qsb(w) be the query relevance score
of word w, and ? be a parameter that adjusts the
rate of sentence compression. Our score function
for a summary S is as follows:
f(S) =
?
w?words(S)
?
?
?
countS(w)?1?
i=0
qsb(w)di
?
?
?+ ? reward(S).
(4)
An optimization problem with this objective
function cannot be regarded as an ILP problem be-
cause it contains non-linear terms. It is also ad-
3The score is still submodular for the purpose of sentence
extraction.
vantageous that the submodular maximization can
deal with such objective functions. Note that the
objective function is such that it can be calculated
according to the type of word. Due to the na-
ture of the objective function, we can use dynamic
programming to effectively search for the subtree
with the maximal density.
4.2 Local Search for Maximal Density
Subtree
Let us now discuss the local search used on line
5 of Algorithm 1. We will use a fast algorithm to
find the maximal density subtree (MDS) of a given
sentence for each cost in Algorithm 1.
Consider the objective function Eq. 4, We can
ignore the second term of the reward function
while looking for the MDS in a sentence because
the number of sentences is the same for every
MDS in a sentence. That is, the gain function of
adding a subtree to a summary can be represented
as the sum of gains for words:
g(t) =
?
w?t
{gainS(w) + freqt(w)c(w)?},
gainS(w) = qsb(w)dcountS(w),
where freqt(w) is the number of ws in subtree
t, and gainS(w) is the gain of adding the word
w to the summary S. Our algorithm is based on
dynamic programming, and it selects a subtree that
maximizes the gain function per cost.
When the word gain is a constant, the algorithm
proposed by Hsieh et al (2010) can be used to
find the MDS. We extended this algorithm to work
for submodular word gain functions that are not
constant. Note that the gain of a word that oc-
curs only once in the sentence, can be treated as
a constant. In what follows, we will describe an
extended algorithm to find the MDS even if there
is word overlap.
For example, let us describe how to obtain the
MDS in the case of a binary tree. First let us tackle
the case in which the gain is always constant. Let
n be a node in the tree, a and b be child nodes of n,
c(n) be the cost of n, mdsca be the MDS rooted at
a and have cost c. mdsn = {mdsc(n)n , . . . ,mdsLn}
denotes the set of MDSs for each cost and its root
node n. The valid subtrees rooted at n can be ob-
tained by taking unions of n with one or both of
t1 ? mdsa and t2 ? mdsb. mdscn is the union that
has the largest gain over the union with the cost of
c (by enumerating all the unions). The MDS for
1027
the sentence root can be found by calculating each
mdscn from the bottom of the tree to the top.
Next, let us consider the objective function that
returns the sum of values of submodular word gain
functions. When there is no word overlap within
the union, we can obtain mdscn in the same man-
ner as for the constant gain. In contrast, if the
union includes word overlap, the gain is less than
the sum of gains: g(mdscn) ? g(n) + g(mdska) +
g(mdsc?k?c(n)b ), where k and c are variables. Thescore reduction can change the order of the gains
of the union. That is, it is possible that another
union without word overlaps will have a larger
gain. Therefore, the algorithm needs to know
whether each t ? mdsn has the potential to have
word overlaps with other MDSs. Let O be the set
of words that occur twice or more in the sentence
on which the local seach focuses. The algorithm
stores MDS for each o ? O, as well as each cost.
By storing MDS for each o and cost as shown
in Fig. 1, the algorithm can find MDS with the
largest gain over the combinations of subtrees.
Algorithm 2 shows the procedure. In it, t andm
denote subtrees, words(t) returns a set of words
in the subtree, g(t) returns the gain of t, tree(n)
means a tree consisting of node n, and t ?m de-
notes the union of subtrees: t and m. subt in-
dicates a set of current maximal density subtrees
among the combinations calculated before. newt
indicates a set of temporary maximal density sub-
trees for the combinations calculated from line 4
to 8. subt[cost,ws] indicates a element of subt that
has a cost cost and contains a set of words ws.
newt[cost,ws] is defined similarly. Line 1 sets subt
to a set consisting of a subtree that indicates node
n itself. The algorithm calculates maximal den-
sity subtrees within combinations of the root node
n and MDSs rooted at child nodes of n. Line 3
iteratively adds MDSs rooted at a next child node
to the combinations; the algorithm then calculates
MDSs newt between subt and the MDSs of the
child node. The procedure from line 6 to 8 selects
a subtree that has a larger gain from the tempo-
rary maximal subtree and the union of t and m.
The computational complexity of this algorithm is
O(NC2) when there is no word overlap within the
sentence, where C denotes the cost of the whole
sentence, and N denotes the number of nodes in
the sentence. The complexity order is the same
as that of the algorithm of Hsieh et al (2010).
When we treat word overlaps, we need to count
Algorithm 2 Algorithm for finding maximal density
subtree for each cost: MDSs.
Function: MDSs
Require: root node n
1: subt[c(n),words(n)?O] = tree(n)
2: newt = ?
3: for i ? child node of n do
4: for t ?MDSs(i) do
5: for m ? subt do
6: index = [c(t ?m), words(t ?m) ? O]
7: newtindex = argmaxj?{newtindex,t?m} g(j)8: end for
9: end for
10: subt = newt
11: end for
12: return subt
Figure 1: Maximal density subtree extraction. The
right table enumerates the subtrees rooted at w2 in
the left tree for all indices. The number in each
tree node is the score of the word.
all unions of combinations of the stored MDSs.
There are at most (C2|O|) MDSs that the algo-
rithm needs to store at each node. Therefore the
total computational complexity is O(NC222|O|).
Since it is unlikely that a sentence contains many
word tokens of one type, the computational cost
may not be so large in practical situations.
5 Experimental Settings
We evaluate our method on Japanese QA test
collections from NTCIR-7 ACLIA1 and NTCIR-
8 ACLIA2 (Mitamura et al, 2008; Mitamura et
al., 2010). The collections contain questions and
weighted answer nuggets. Our experimental set-
tings followed the settings of (Morita et al, 2011),
except for the maximum summary length. We
generated summaries consisting of 140 Japanese
characters or less, with the question as the query
terms. We did this because our aim is to use our
method in mobile situations. We used ?ACLIA1
test data? to tune the parameters, and evaluated our
method on ?ACLIA2 test? data.
We used JUMAN (Kurohashi and Kawahara,
2009a) for word segmentation and part-of-speech
tagging, and we calculated idf over Mainichi
newspaper articles from 1991 to 2005. For the de-
1028
POURPRE Precision Recall F1 F3
Lin and Bilmes (2011) 0.215 0.126 0.201 0.135 0.174
Subtree extraction (SbE) 0.268 0.238 0.213 0.159 0.190
Sentence extraction (NC) 0.278 0.206 0.215 0.139 0.183
Table 1: Results on ACLIA2 test data.
pendency parsing, we used KNP (Kurohashi and
Kawahara, 2009b). Since KNP internally has a
flag that indicates either an ?obligatory case? or an
?adjacent case?, we regarded dependency relations
flagged by KNP as obligatory in the sentence com-
pression. KNP utilizes Kyoto University?s case
frames (Kawahara and Kurohashi, 2006) as the re-
source for detecting obligatory or adjacent cases.
To evaluate the summaries, we followed the
practices of the TAC summarization tasks (Dang,
2008) and NTCIR ACLIA tasks, and computed
pyramid-based precision with the allowance pa-
rameter, recall, and F? (where ? is 1 or 3)
scores. The allowance parameter was determined
from the average nugget length for each question
type of the ACLIA2 collection (Mitamura et al,
2010). Precision and recall are computed from the
nuggets that the summary covered along with their
weights. One of the authors of this paper man-
ually evaluated whether each nugget matched the
summary. We also used the automatic evaluation
measure, POURPRE (Lin and Demner-Fushman,
2006). POURPRE is based on word matching
of reference nuggets and system outputs. We re-
garded as stopwords the most frequent 100 words
in Mainichi articles from 1991 to 2005 (the doc-
ument frequency was used to measure the fre-
quency). We also set the threshold of nugget
matching as 0.5 and binarized the nugget match-
ing, following the previous study (Mitamura et al,
2010). We tuned the parameters by using POUR-
PRE on the development dataset.
Lin and Bilmes (2011) designed a monotone
submodular function for query-oriented summa-
rization. Their succinct method performed well
in DUC from 2004 to 2007. They proposed a
positive diversity reward function in order to de-
fine a monotone submodular objective function for
generating a non-redundant summary. The diver-
sity reward gives a smaller gain for a biased sum-
mary, because it consists of gains based on three
clusters and calculates a square root score with
respect to each sentence. The reward also con-
tains a score for the similarity of a sentence to
the query, for purposes of query-oriented summa-
Recall Length # of nuggets
Subtree extraction 0.213 11,143 100
Reconstructed (RC) 0.228 13,797 108
Table 2: Effect of sentence compression.
rization. Their objective function also includes a
coverage function based on the similarity wi,j be-
tween sentences. In the coverage function min
function limits the maximum gain ??i?V wi,j ,
which is a small fraction ? of the similarity be-
tween a sentence j and the all source documents.
The objective function is the sum of the positive
reward R and the coverage function L over the
source documents V , as follows:
F(S) = L(S) +
3?
k=1
?kRQ,k(S),
L(S) = ?
i?V
min
??
?
?
j?S
wi,j , ?
?
k?V
wi,k
??
? ,
RQ,k =
?
c?Ck
???? ?
j?S?c
( ?N
?
i?V
wi,j + (1? ?)rj,Q),
where ?, ? and ?k are parameters, and rj,Q repre-
sents the similarity between sentence j and query
Q. We tuned the parameters on the development
dataset. Lin and Bilmes (2011) used three clusters
Ck with different granularities, which were calcu-
lated in advance. We set the granularity to (0.2N ,
0.15N , 0.05N ) according to the settings of them,
where N is the number of sentences in a docu-
ment.
We also regarded as stopwords ???? (tell),?
??? (know),? ?? (what)? and their conjugated
forms, which are excessively common in ques-
tions. For the query expansion in the baseline, we
used Japanese WordNet to obtain synonyms and
hypernyms of query terms.
6 Results
Table 1 summarizes our results. ?Subtree ex-
traction (SbE)? is our method, and ?Sentence ex-
traction (NC)? is a version of our method with-
out compression. The NC has the same objec-
tive function but only extracts sentences. The F1-
measure and F3-measure of our method are 0.159
and 0.190 respectively, while those of the state-of-
1029
the-art baseline are 0.135 and 0.174 respectively.
Unfortunately, since the document set is small, the
difference is not statistically significant. Compar-
ing our method with the one without compression,
we can see that there are improvements in the F1
and F3 scores of the human evaluation, whereas
the POURPRE score of the version of our method
without compression is higher than that of our
method with compression. The compression im-
proved the precision of our method, but slightly
decreased the recall.
For the error analyses, we reconstructed the
original sentences from which our method ex-
tracted the subtrees. Table 2 shows the statistics
of the summaries of SbE and reconstructed sum-
maries (RC). The original sentences covered 108
answer nuggets in total, and 8 of these answer
nuggets were dropped by the sentence compres-
sion. Comparing the results of SbE and RC, we
can see that the sentence compression caused the
recall of SbE to be 7% lower than that of RC.
However, the drop is relatively small in light of
the fact that the sentence compression can discard
19% of the original character length with SbE.
This suggests that the compression can efficiently
prune words while avoiding pruning informative
content.
Since the summary length is short, we can select
only two or three sentences for a summary. As
Morita et al (2011) mentioned, answer nuggets
overlap each other. The baseline objective func-
tion R tends to extract sentences from various
clusters. If the answer nuggets are present in the
same cluster, the objective function does not fit the
situation. However, our methods (SbE and NC)
have a parameter d that can directly adjust overlap
penalty with respect to word importance as well
as query relevance. This may help our methods to
cover similar answer nuggets. In fact, the develop-
ment data resulted in a relatively high parameter d
(0.8) for NC compared with 0.2 for SbE.
7 Conclusions and Future Work
We formalized a query-oriented summarization,
which is a task in which one simultaneously per-
forms sentence compression and extraction, as a
new optimization problem: budgeted monotone
nondecreasing submodular function maximization
with a cost function. We devised an approximate
algorithm to solve the problem in a reasonable
computational time and proved that its approxima-
tion rate is 12(1 ? e?1). Our approach achieved
an F3-measure of 0.19 on the ACLIA2 Japanese
test collection, which is 9.2 % improvement over
a state-of-the-art method using a submodular ob-
jective function.
Since our algorithm requires that the objective
function is the sum of word score functions, our
proposed method has a restriction that we cannot
use an arbitrary monotone submodular function as
the objective function for the summary. Our fu-
ture work will improve the local search algorithm
to remove this restriction. As mentioned before,
we also plan to adapt of our system to other lan-
guages.
Appendix
Here, we analyze the performance guarantee of
Algorithm 1. We use the following notation. S? is
the optimal solution, cu(S) is the residual cost of
subtree u when S is already covered, and i? is the
last step before the algorithm discards a subtree
s ? S? or a part of the subtree s. This is because
the subtree does not belong to either the approxi-
mate solution or the optimal solution. We can re-
move the subtree s? from V without changing the
approximate rate. si is the i-th subtree obtained by
line 5 of Algorithm 1. Gi is the set obtained after
adding subtree si to Gi?1 from the valid set Bi.
Gf is the final solution obtained by Algorithm 1.
f(?) : 2V ? R is a monotone submodular func-
tion.
We assume that there is an equivalent sub-
tree with any union of subtrees in a valid set B:
?t1, t2,?te, te ? {t1, t2}. Note that for any or-
der of the set, the cost or profit of the set is fixed:?
ui?S={u1,...,u|S|} cui(Si?1) = c(S).
Lemma 1 ?X,Y ? V, f(X) ? f(Y ) +?
u?X\Y ?u(Y ), where ?u(S) = f(S ? {u}) ?
f(S).
The inequality can be derived from the definition
of submodularity. 2
Lemma 2 For i = 1, . . . , i?+1, when 0 ? r ? 1,
f(S?)?f(Gi?1)?L
r |S?|1?r
csi (Gi?1)
(f(Gi?1?{si})?f(Gi?1)),
where cu(S)=c(S?{u})?c(S).
Proof. From line 5 of Algorithm 1, we have
?u ? S?\Gi?1,
?u(Gi?1)
cu(Gi?1)r
? ?si(Gi?1)csi(Gi?1)r
.
Let B be a valid set, and union be a func-
tion that returns the union of subtrees. We have
1030
?T ? B, ?b ? B, b = union(T ), because we
have an equivalent tree b ? B for each union
of trees T in a valid set B. That is, for any
set of subtrees, we have an equivalent set of sub-
trees, where bi ? Bi. Without loss of generality,
we can replace the difference set S?\Gi?1 with
a set T ?i?1 = {b0, . . . , b|T ?i?1|} that does not con-tain any two elements extracted from the same
valid set. Thus when 0 ? r ? 1 and 0 ?
i ? i? + 1, ?s?\Gi?1 (Gi?1)cS?\Gi?1 (Gi?1)r =
?T ?i?1 (Gi?1)
cT ?i?1 (Gi?1)
r , and
?bj ? T ?i?1,
?bj (Gi?1)
cbj (Gi?1)r
? ?si (Gi?1)csi (Gi?1)r . Thus,
?T ?i?1 (Gi?1) =
?
u?T ?i?1
?u(Gi?1)
? ?si (Gi?1)csi (Gi?1)r
?
u?T ?i?1
cu(Gi?1)r
? ?si (Gi?1)csi (Gi?1)r |T
?
i?1|
(?
u?T ?i?1
cu(Gi?1)
|T ?i?1|
)r
? ?si (Gi?1)csi (Gi?1)r |T
?
i?1|1?r
(?
u?T ?i?1
cu(?)
)r
? ?si (Gi?1)csi (Gi?1)r |S
?|1?rLr,
where the second inequality is from Ho?lder?s in-
equality. The third inequality uses the submodu-
larity of the cost function,
cu(Gi?1) = c({u} ?Gi?1)? c(Gi?1) ? cu(?)
and the fact that |S?| ? |S?\Gi?1| ? |T ?i?1|, and?
u?T ?i?1 cu(?) = c(T
?
i?1) ? L .
As a result, we have
?s?\Gi?1(Gi?1) = ?T ?i?1(Gi?1)
? ?si(Gi?1)csi(Gi?1)r
|S?|1?rLr.
Let X = S? and Y = Gi?1. Applying Lemma
1 yields
f(S?) ? f(Gi?1) + ?u?S?\Gi?1(Gi?1).
? f(Gi?1) +
?si(Gi?1)
csi(Gi?1)
|S?|1?rLr.
The lemma follows as a result.
Lemma 3 For a normalized monotone submodu-
lar f(?), for i = 1, . . . , i? + 1 and 0 ? r ? 1 and
letting si be the i-th unit added into G and Gi be
the set after adding si, we have
f(Gi) ?
(
1?
i?
k=1
(
1? csk(Gk?1)
r
Lr|S?|1?r
))
f(S?).
Proof. This is proved similarly to Lemma 3 of
(Krause and Guestrin, 2005) using Lemma 2.
Proof of Theorem 1. This is proved similarly to
Theorem 1 of (Krause and Guestrin, 2005) using
Lemma 3.
References
Taylor Berg-Kirkpatrick, Dan Gillick, and Dan Klein.
2011. Jointly learning to extract and compress. In
Proceedings of the 49th Annual Meeting of the Asso-
ciation for Computational Linguistics: Human Lan-
guage Technologies - Volume 1, HLT ?11, pages
481?490, Stroudsburg, PA, USA. Association for
Computational Linguistics.
Calinescu Calinescu, Chandra Chekuri, Martin Pa?l,
and Jan Vondra?k. 2011. Maximizing a monotone
submodular function subject to a matroid constraint.
SIAM Journal on Computing, 40(6):1740?1766.
Michele Conforti and Ge?rard Cornue?jols. 1984. Sub-
modular set functions, matroids and the greedy al-
gorithm: Tight worst-case bounds and some gener-
alizations of the rado-edmonds theorem. Discrete
Applied Mathematics, 7(3):251 ? 274.
Hoa Trang Dang. 2008. Overview of the tac
2008 opinion question answering and summariza-
tion tasks. In Proceedings of Text Analysis Confer-
ence.
Anupam Gupta, Aaron Roth, Grant Schoenebeck, and
Kunal Talwar. 2010. Constrained non-monotone
submodular maximization: offline and secretary
algorithms. In Proceedings of the 6th interna-
tional conference on Internet and network eco-
nomics, WINE?10, pages 246?257, Berlin, Heidel-
berg. Springer-Verlag.
Sun-Yuan Hsieh and Ting-Yu Chou. 2010. The
weight-constrained maximum-density subtree prob-
lem and related problems in trees. The Journal of
Supercomputing, 54(3):366?380, December.
Daisuke Kawahara and Sadao Kurohashi. 2006. A
fully-lexicalized probabilistic model for japanese
syntactic and case structure analysis. In Proceedings
of the main conference on Human Language Tech-
nology Conference of the North American Chap-
ter of the Association of Computational Linguistics,
HLT-NAACL ?06, pages 176?183, Stroudsburg, PA,
USA. Association for Computational Linguistics.
Samir Khuller, Anna Moss, and Joseph S. Naor. 1999.
The budgeted maximum coverage problem. Infor-
mation Processing Letters, 70(1):39?45.
Andreas Krause and Carlos Guestrin. 2005. A
note on the budgeted maximization on submodular
functions. Technical Report CMU-CALD-05-103,
Carnegie Mellon University.
1031
Ariel Kulik, Hadas Shachnai, and Tami Tamir. 2009.
Maximizing submodular set functions subject to
multiple linear constraints. In Proceedings of
the twentieth Annual ACM-SIAM Symposium on
Discrete Algorithms, SODA ?09, pages 545?554,
Philadelphia, PA, USA. Society for Industrial and
Applied Mathematics.
Sadao Kurohashi and Daisuke Kawahara, 2009a.
Japanese Morphological Analysis System JUMAN
6.0 Users Manual. http://nlp.ist.i.
kyoto-u.ac.jp/EN/index.php?JUMAN.
Sadao Kurohashi and Daisuke Kawahara, 2009b. KN
parser (Kurohashi-Nagao parser) 3.0 Users Man-
ual. http://nlp.ist.i.kyoto-u.ac.jp/
EN/index.php?KNP.
Jon Lee, Vahab S. Mirrokni, Viswanath Nagarajan, and
Maxim Sviridenko. 2009. Non-monotone submod-
ular maximization under matroid and knapsack con-
straints. In Proceedings of the 41st annual ACM
symposium on Theory of computing, STOC ?09,
pages 323?332, New York, NY, USA. ACM.
Hui Lin and Jeff Bilmes. 2010. Multi-document sum-
marization via budgeted maximization of submod-
ular functions. In Human Language Technologies:
The 2010 Annual Conference of the North American
Chapter of the Association for Computational Lin-
guistics, HLT ?10, pages 912?920, Stroudsburg, PA,
USA. Association for Computational Linguistics.
Hui Lin and Jeff Bilmes. 2011. A class of submodular
functions for document summarization. In Proceed-
ings of the 49th Annual Meeting of the Association
for Computational Linguistics: Human Language
Technologies - Volume 1, HLT ?11, pages 510?520,
Stroudsburg, PA, USA. Association for Computa-
tional Linguistics.
Jimmy Lin and Dina Demner-Fushman. 2006. Meth-
ods for automatically evaluating answers to com-
plex questions. Information Retrieval, 9(5):565?
587, November.
Andre? F. T. Martins and Noah A. Smith. 2009. Sum-
marization with a joint model for sentence extraction
and compression. In Proceedings of the Workshop
on Integer Linear Programming for Natural Lan-
gauge Processing, ILP ?09, pages 1?9, Stroudsburg,
PA, USA. Association for Computational Linguis-
tics.
Ryan McDonald. 2007. A study of global inference
algorithms in multi-document summarization. In
Proceedings of the 29th European conference on IR
research, ECIR?07, pages 557?564, Berlin, Heidel-
berg. Springer-Verlag.
Teruko Mitamura, Eric Nyberg, Hideki Shima,
Tsuneaki Kato, Tatsunori Mori, Chin-Yew Lin, Rui-
hua Song, Chuan-Jie Lin, Tetsuya Sakai, Donghong
Ji, and Noriko Kando. 2008. Overview of the
NTCIR-7 ACLIA Tasks: Advanced Cross-Lingual
Information Access. In Proceedings of the 7th NT-
CIR Workshop.
Teruko Mitamura, Hideki Shima, Tetsuya Sakai,
Noriko Kando, Tatsunori Mori, Koichi Takeda,
Chin-Yew Lin, Ruihua Song, Chuan-Jie Lin, and
Cheng-Wei Lee. 2010. Overview of the ntcir-8 aclia
tasks: Advanced cross-lingual information access.
In Proceedings of the 8th NTCIR Workshop.
Hajime Morita, Tetsuya Sakai, and Manabu Okumura.
2011. Query snowball: a co-occurrence-based ap-
proach to multi-document summarization for ques-
tion answering. In Proceedings of the 49th Annual
Meeting of the Association for Computational Lin-
guistics: Human Language Technologies: short pa-
pers - Volume 2, HLT ?11, pages 223?229, Strouds-
burg, PA, USA. Association for Computational Lin-
guistics.
Wolfgang Schmidt. 1991. Greedoids and searches in
directed graphs. Discrete Mathmatics, 93(1):75?88,
November.
Jie Tang, Limin Yao, and Dewei Chen. 2009. Multi-
topic based query-oriented summarization. In Pro-
ceedings of 2009 SIAM International Conference
Data Mining (SDM?2009), pages 1147?1158.
David M. Zajic, Bonnie J. Dorr, Jimmy Lin, and
Richard Schwartz. 2006. Sentence compression
as a component of a multi-document summariza-
tion system. In Proceedings of the 2006 Doc-
ument Understanding Conference (DUC 2006) at
NLT/NAACL 2006.
1032
Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 315?320,
Baltimore, Maryland, USA, June 23-25 2014.
c
?2014 Association for Computational Linguistics
Single Document Summarization based on Nested Tree Structure
Yuta Kikuchi
?
Tsutomu Hirao
?
Hiroya Takamura
?
?
Tokyo Institute of technology
4295, Nagatsuta, Midori-ku, Yokohama, 226-8503, Japan
{kikuchi,takamura,oku}@lr.pi.titech.ac.jp
?
NTT Communication Science Laboratories, NTT Corporation
2-4, Hikaridai, Seika-cho, Soraku-gun, Kyoto, 619-0237, Japan
{hirao.tsutomu,nagata.masaaki}@lab.ntt.co.jp
Manabu Okumura
?
Masaaki Nagata
?
Abstract
Many methods of text summarization
combining sentence selection and sen-
tence compression have recently been pro-
posed. Although the dependency between
words has been used in most of these
methods, the dependency between sen-
tences, i.e., rhetorical structures, has not
been exploited in such joint methods. We
used both dependency between words and
dependency between sentences by con-
structing a nested tree, in which nodes
in the document tree representing depen-
dency between sentences were replaced by
a sentence tree representing dependency
between words. We formulated a sum-
marization task as a combinatorial opti-
mization problem, in which the nested
tree was trimmed without losing impor-
tant content in the source document. The
results from an empirical evaluation re-
vealed that our method based on the trim-
ming of the nested tree significantly im-
proved the summarization of texts.
1 Introduction
Extractive summarization is one well-known ap-
proach to text summarization and extractive meth-
ods represent a document (or a set of documents)
as a set of some textual units (e.g., sentences,
clauses, and words) and select their subset as a
summary. Formulating extractive summarization
as a combinational optimization problem greatly
improves the quality of summarization (McDon-
ald, 2007; Filatova and Hatzivassiloglou, 2004;
Takamura and Okumura, 2009). There has re-
cently been increasing attention focused on ap-
proaches that jointly optimize sentence extraction
and sentence compression (Tomita et al, 2009;
Qian and Liu, 2013; Morita et al, 2013; Gillick
and Favre, 2009; Almeida and Martins, 2013;
Berg-Kirkpatrick et al, 2011). We can only ex-
tract important content by trimming redundant
parts from sentences.
However, as these methods did not include the
discourse structures of documents, the generated
summaries lacked coherence. It is important for
generated summaries to have a discourse struc-
ture that is similar to that of the source docu-
ment. Rhetorical Structure Theory (RST) (Mann
and Thompson, 1988) is one way of introduc-
ing the discourse structure of a document to a
summarization task (Marcu, 1998; Daum?e III and
Marcu, 2002; Hirao et al, 2013). Hirao et al
recently transformed RST trees into dependency
trees and used them for single document summa-
rization (Hirao et al, 2013). They formulated the
summarization problem as a tree knapsack prob-
lem with constraints represented by the depen-
dency trees.
We propose a method of summarizing a single
document that utilizes dependency between sen-
tences obtained from rhetorical structures and de-
pendency between words obtained from a depen-
dency parser. We have explained our method with
an example in Figure 1. First, we represent a doc-
ument as a nested tree, which is composed of two
types of tree structures: a document tree and a
sentence tree. The document tree is a tree that has
sentences as nodes and head modifier relationships
between sentences obtained by RST as edges. The
sentence tree is a tree that has words as nodes
and head modifier relationships between words
obtained by the dependency parser as edges. We
can build the nested tree by regarding each node of
the document tree as a sentence tree. Finally, we
formulate the problem of single document sum-
marization as that of combinatorial optimization,
which is based on the trimming of the nested tree.
315
John  was  running  on  a  track  in  the  park.
He  looks very tired. Mike  said  he  is  trainning  for  a  race.
The  race  is  held  on  next  month.
?
  Source document                                   
John was running on a track in the park.
He looks very tired.
Mike said he is training for a race.
The race is held on next month.
  Summary                                              
John was running on a track.
he is training for a race. *
The race is held on next month.
EDU?? ???? ??? ????
0
2
4
6
8
10
12
14
16
EDU
selsection
sentence subtree
selection
sentence
selection
reference 
summary
Nu
m
be
r o
f  
se
lec
ted
 se
nt
en
ce
s 
fro
m
 so
ur
ce
 do
cu
m
en
t
John  was  running  on  a  track  in  the  park.
He  looks very tired. Mike  said  he  is  training  for  a  race.
The  race  is  held  next  month.
?
  Source document                                   
John was running on a track in the park.
He looks very tired.
Mike said he is training for a race.
The race is held next month.
  Summary                                              
John was running on a track.
he is training for a race. *
The race is held next month.
Figure 1: Overview of our method. The source document is represented as a nested tree. Our method
simultaneously selects a rooted document subtree and sentence subtree from each node.
Our method jointly utilizes relations between sen-
tences and relations between words, and extracts
a rooted document subtree from a document tree
whose nodes are arbitrary subtrees of the sentence
tree.
Elementary Discourse Units (EDUs) in RST are
defined as the minimal building blocks of dis-
course. EDUs roughly correspond to clauses.
Most methods of summarization based on RST use
EDUs as extraction textual units. We converted
the rhetorical relations between EDUs to the re-
lations between sentences to build the nested tree
structure. We could thus take into account both
relations between sentences and relations between
words.
2 Related work
Extracting a subtree from the dependency tree of
words is one approach to sentence compression
(Tomita et al, 2009; Qian and Liu, 2013; Morita
et al, 2013; Gillick and Favre, 2009). However,
these studies have only extracted rooted subtrees
from sentences. We allowed our model to extract
a subtree that did not include the root word (See
the sentence with an asterisk ? in Figure 1). The
method of Filippova and Strube (2008) allows the
model to extract non-rooted subtrees in sentence
compression tasks that compress a single sentence
with a given compression ratio. However, it is not
trivial to apply their method to text summariza-
tion because no compression ratio is given to sen-
tences. None of these methods use the discourse
structures of documents.
Daum?e III and Marcu (2002) proposed a noisy-
channel model that used RST. Although their
method generated a well-organized summary, no
optimality of information coverage was guaran-
teed and their method could not accept large texts
because of the high computational cost. In addi-
- The scare over Alar, a growth regulator
- that makes apples redder and crunchier
- but may be carcinogenic,
- made consumers shy away from the Delicious,
- though they were less affected than the McIntosh.
Figure 2: Example of one sentence. Each line cor-
responds to one EDU.
tion, their method required large sets of data to cal-
culate the accurate probability. There have been
some studies that have used discourse structures
locally to optimize the order of selected sentences
(Nishikawa et al, 2010; Christensen et al, 2013).
3 Generating summary from nested tree
3.1 Building Nested Tree with RST
A document in RST is segmented into EDUs and
adjacent EDUs are linked with rhetorical relations
to build an RST-Discourse Tree (RST-DT) that has
a hierarchical structure of the relations. There are
78 types of rhetorical relations between two spans,
and each span has one of two aspects of a nu-
cleus and a satellite. The nucleus is more salient
to the discourse structure, while the other span, the
satellite, represents supporting information. RST-
DT is a tree whose terminal nodes correspond
to EDUs and whose nonterminal nodes indicate
the relations. Hirao et al converted RST-DTs
into dependency-based discourse trees (DEP-DTs)
whose nodes corresponded to EDUs and whose
edges corresponded to the head modifier relation-
ships of EDUs. See Hirao et al for details (Hirao
et al, 2013).
Our model requires sentence-level dependency.
Fortunately we can simply convert DEP-DTs to
obtain dependency trees between sentences. We
specifically merge EDUs that belong to the same
sentence. Each sentence has only one root EDU
that is the parent of all the other EDUs in the sen-
tence. Each root EDU in a sentence has the parent
316
max.
n
?
i
m
i
?
j
w
ij
z
ij
s.t.
?
n
i
?
m
i
j
z
ij
? L; (1)
x
parent(i)
? x
i
; ?i (2)
z
parent(i,j)
? z
ij
+ r
ij
? 0; ?i, j (3)
x
i
? z
ij
; ?i, j (4)
?
m
i
j
z
ij
? min(?, len(i))x
i
; ?i (5)
?
m
i
j
r
ij
= x
i
; ?i (6)
?
j /?R
c
(i)
r
ij
= 0; ?i (7)
r
ij
? z
ij
; ?i, j (8)
r
ij
+ z
parent(i,j)
? 1; ?i, j (9)
r
iroot(i)
= z
iroot(i)
; ?i (10)
?
j?sub(i)
z
ij
? x
i
; ?i (11)
?
j?obj(i)
z
ij
? x
i
; ?i (12)
Figure 3: ILP formulation (x
i
, z
ij
, r
ij
? {0, 1})
EDU in another sentence. Hence, we can deter-
mine the parent-child relations between sentences.
As a result, we obtain a tree that represents the
parent-child relations of sentences, and we can use
it as a document tree. After the document tree is
obtained, we use a dependency parser to obtain the
syntactic dependency trees of sentences. Finally,
we obtain a nested tree.
3.2 ILP formulation
Our method generates a summary by trimming a
nested tree. In particular, we extract a rooted docu-
ment subtree from the document tree, and sentence
subtrees from sentence trees in the document tree.
We formulate our problem of optimization in this
section as that of integer linear programming. Our
model is shown in Figure 3.
Let us denote by w
ij
the term weight of word
ij (word j in sentence i). x
i
is a variable that
is one if sentence i is selected as part of a sum-
mary, and z
ij
is a variable that is one if word ij
is selected as part of a summary. According to the
objective function, the score for the resulting sum-
mary is the sum of the term weights w
ij
that are
included in the summary. We denote by r
ij
the
variable that is one if word ij is selected as a root
of an extracting sentence subtree. Constraint (1)
guarantees that the summary length will be less
than or equal to limit L. Constraints (2) and (3)
are tree constraints for a document tree and sen-
tence trees. r
ij
in Constraint (3) allows the system
to extract non-rooted sentence subtrees, as we pre-
viously mentioned. Function parent(i) returns the
parent of sentence i and function parent(i, j) re-
turns the parent of word ij. Constraint (4) guaran-
tees that words are only selected from a selected
sentence. Constraint (5) guarantees that each se-
lected sentence subtree has at least ? words. Func-
tion len(i) returns the number of words in sentence
i. Constraints (6)-(10) allow the model to extract
subtrees that have an arbitrary root node. Con-
straint (6) guarantees that there is only one root
per selected sentence. We can set the candidate
for the root node of the subtree by using constraint
(7). The R
c
(i) returns a set of the nodes that are
the candidates of the root nodes in sentence i. It
returned the parser?s root node and the verb nodes
in this study. Constraint (8) maintains consistency
between z
ij
and r
ij
. Constraint (9) prevents the
system from selecting the parent node of the root
node. Constraint (10) guarantees that the parser?s
root node will only be selected when the system
extracts a rooted sentence subtree. The root(i) re-
turns the word index of the parser?s root. Con-
straints (11) and (12) guarantee that the selected
sentence subtree has at least one subject and one
object if it has any. The sub(i) and obj(i) return
the word indices whose dependency tag is ?SUB?
and ?OBJ?.
3.3 Additional constraint for grammaticality
We added two types of constraints to our model
to extract a grammatical sentence subtree from a
dependency tree:
z
ik
= z
il
, (13)
?
k?s(i,j)
z
ik
= |s(i, j)|x
i
. (14)
Equation (13) means that words z
ik
and z
il
have
to be selected together, i.e., a word whose depen-
dency tag is PMOD or VC and its parent word, a
negation and its parent word, a word whose de-
pendency tag is SUB or OBJ and its parent verb,
a comparative (JJR) or superlative (JJS) adjective
and its parent word, an article (a/the) and its par-
ent word, and the word ?to? and its parent word.
Equation (14) means that the sequence of words
has to be selected together, i.e., a proper noun se-
quence whose POS tag is PRP$, WP%, or POS
and a possessive word and its parent word and the
words between them. The s(i, j) returns the set of
word indices that are selected together with word
ij.
317
Table 1: ROUGE score of each model. Note that
the top two rows are both our proposals.
ROUGE-1
Sentence subtree 0.354
Rooted sentence subtree 0.352
Sentence selection 0.254
EDU selection (Hirao et al, 2013) 0.321
LEAD
EDU
0.240
LEAD
snt
0.157
4 Experiment
4.1 Experimental Settings
We experimentally evaluated the test collection for
single document summarization contained in the
RST Discourse Treebank (RST-DTB) (Carlson et
al., 2001) distributed by the Linguistic Data Con-
sortium (LDC)
1
. The RST-DTB Corpus includes
385 Wall Street Journal articles with RST anno-
tations, and 30 of these documents also have one
manually prepared reference summary. We set the
length constraint, L, as the number of words in
each reference summary. The average length of
the reference summaries corresponded to approxi-
mately 10% of the length of the source document.
This dataset was first used by Marcu et al for
evaluating a text summarization system (Marcu,
1998). We used ROUGE (Lin, 2004) as an eval-
uation criterion.
We compared our method (sentence subtree)
with that of EDU selection (Hirao et al, 2013).
We examined two other methods, i.e., rooted sen-
tence subtree and sentence selection. These two
are different from our method in the way that they
select a sentence subtree. Rooted sentence subtree
only selects rooted sentence subtrees
2
. Sentence
selection does not trim sentence trees. It simply
selects full sentences from a document tree
3
. We
built all document trees from the RST-DTs that
were annotated in the corpus.
We set the term weight, w
ij
, for our model as:
w
ij
=
log(1 + tf
ij
)
depth(i)
2
, (15)
where tf
ij
is the term frequency of word ij in a
document and depth(i) is the depth of sentence
1
http://www.ldc.upenn.edu/Catalog/CatalogEntry.jsp?
catalogId=LDC2002T07
2
We achieved this by making R
c
(i) only return the
parser?s root node in Figure 7.
3
We achieved this by setting ? to a very large number.
i within the sentence-level DEP-DT that we de-
scribed in Section 3.1. For Constraint (5), we set
? to eight.
4.2 Results and Discussion
4.2.1 Comparing ROUGE scores
We have summarized the Recall-Oriented Under-
study for Gisting Evaluation (ROUGE) scores for
each method in Table 1. The score for sentence
selection is low (0.254). However, introducing
sentence compression to the system greatly im-
proved the ROUGE score (0.354). The score is
also higher than that with EDU selection, which
is a state-of-the-art method. We applied a multi-
ple test by using Holm?s method and found that
our method significantly outperformed EDU se-
lection and sentence selection. The difference be-
tween the sentence subtree and the rooted sentence
subtree methods was fairly small. We therefore
qualitatively analyzed some actual examples that
will be discussed in Section 4.2.2. We also exam-
ined the ROUGE scores of two LEAD
4
methods
with different textual units: EDUs (LEAD
EDU
)
and sentences (LEAD
SNT
). Although LEAD
works well and often obtains high ROUGE scores
for news articles, the scores for LEAD
EDU
and
LEAD
SNT
were very low.
4.2.2 Qualitative Evaluation of Sentence
Subtree Selection
This subsection compares the methods of subtree
selection and rooted subtree selection. Figure 4
has two example sentences for which both meth-
ods selected a subtree as part of a summary. The
{?} indicates the parser?s root word. The [?] indi-
cates the word that the system selected as the root
of the subtree. Subtree selection selected a root in
both examples that differed from the parser?s root.
As we can see, subtree selection only selected im-
portant subtrees that did not include the parser?s
root, e.g., purpose-clauses and that-clauses. This
capability is very effective because we have to
contain important content in summaries within
given length limits, especially when the compres-
sion ratio is high (i.e., the method has to gener-
ate much shorter summaries than the source docu-
ments).
4
LEADmethods simply take the firstK textual units from
a source document until the summary length reaches L.
318
Original sentence : John Kriz, a Moody?s vice president, {said} Boston Safe Deposit?s performance has been
hurt this year by a mismatch in the maturities of its assets and liabilities.
Rooted subtree selection : John Kriz a Moody?s vice president [{said}] Boston Safe Deposit?s performance has been
hurt this year
Subtree selection : Boston Safe Deposit?s performance has [been] hurt this year
Original sentence : Recent surveys by Leo J. Shapiro & Associates, a market research firm in Chicago,
{suggest} that Sears is having a tough time attracting shoppers because it hasn?t yet done
enough to improve service or its selection of merchandise.
Rooted subtree selection : surveys [{suggest}] that Sears is having a time
Subtree selection : Sears [is] having a tough time attracting shoppers
Figure 4: Example sentences and subtrees selected by each method.
Table 2: Average number of words that individual
extracted textual units contained.
Subtree Sentence EDU
15.29 18.96 9.98
4.2.3 Fragmentation of Information
Many studies that have utilized RST have simply
adopted EDUs as textual units (Mann and Thomp-
son, 1988; Daum?e III and Marcu, 2002; Hirao et
al., 2013; Knight and Marcu, 2000). While EDUs
are textual units for RST, they are too fine grained
as textual units for methods of extractive summa-
rization. Therefore, the models have tended to se-
lect small fragments from many sentences to max-
imize objective functions and have led to frag-
mented summaries being generated. Figure 2 has
an example of EDUs. A fragmented summary
is generated when small fragments are selected
from many sentences. Hence, the number of sen-
tences in the source document included in the re-
sulting summary can be an indicator to measure
the fragmentation of information. We counted
the number of sentences in the source document
that each method used to generate a summary
5
.
The average for our method was 4.73 and its me-
dian was four sentences. In contrast, methods
of EDU selection had an average of 5.77 and a
median of five sentences. This meant that our
method generated a summary with a significantly
smaller number of sentences
6
. In other words, our
method relaxed fragmentation without decreasing
the ROUGE score. There are boxplots of the num-
bers of selected sentences in Figure 5. Table 2 lists
the number of words in each textual unit extracted
by each method. It indicates that EDUs are shorter
than the other textual units. Hence, the number of
sentences tends to be large.
5
Note that the number for the EDU method is not equal to
selected textual units because a sentence in the source docu-
ment may contain multiple EDUs.
6
We used the Wilcoxon signed-rank test (p < 0.05).
John  was  running  on  a  track  in  the  park.
He  looks very tired. Mike  said  he  is  trainning  for  a  race.
The  race  is  held  on  next  month.
  Source document                                   John was running on a track in the park.He looks very tired.Mike said he is training for a race.The race is held on next month.
  Summary                                              John was running on a track.he is training for a race.The race is held on next month.
EDU?? ???? ??? ????0
2
4
6
8
10
12
14
16
EDUselsection sentence subtreeselection sentenceselection reference summary
Num
ber o
f  sel
ected
 sent
ence
s 
from
 sour
ce do
cume
nt
Figure 5: Number of sentences that each method
selected.
5 Conclusion
We proposed a method of summarizing a sin-
gle document that included relations between sen-
tences and relations between words. We built a
nested tree and formulated the problem of summa-
rization as that of integer linear programming. Our
method significantly improved the ROUGE score
with significantly fewer sentences than the method
of EDU selection. The results suggest that our
method relaxed the fragmentation of information.
We also discussed the effectiveness of sentence
subtree selection that did not restrict rooted sub-
trees. Although ROUGE scores are widely used
as evaluation metrics for text summarization sys-
tems, they cannot take into consideration linguis-
tic qualities such as human readability. Hence, we
plan to conduct evaluations with people
7
.
We only used the rhetorical structures between
sentences in this study. However, there were also
rhetorical structures between EDUs inside individ-
ual sentences. Hence, utilizing these for sentence
compression has been left for future work. In addi-
tion, we used rhetorical structures that were man-
ually annotated. There have been related studies
on building RST parsers (duVerle and Prendinger,
2009; Hernault et al, 2010) and by using such
parsers, we should be able to apply our model to
other corpora or to multi-document settings.
7
For example, the quality question metric from the Docu-
ment Understanding Conference (DUC).
319
References
Miguel Almeida and Andre Martins. 2013. Fast
and robust compressive summarization with dual de-
composition and multi-task learning. In ACL, pages
196?206, August.
Taylor Berg-Kirkpatrick, Dan Gillick, and Dan Klein.
2011. Jointly learning to extract and compress. In
ACL, pages 481?490, Portland, Oregon, USA, June.
Lynn Carlson, Daniel Marcu, and Mary Ellen
Okurowski. 2001. Building a discourse-tagged cor-
pus in the framework of rhetorical structure theory.
In SIGDIAL, pages 1?10.
Janara Christensen, Mausam, Stephen Soderland, and
Oren Etzioni. 2013. Towards coherent multi-
document summarization. In NAACL:HLT, pages
1163?1173.
Hal Daum?e III and Daniel Marcu. 2002. A noisy-
channel model for document compression. ACL,
pages 449?456.
David duVerle and Helmut Prendinger. 2009. A novel
discourse parser based on support vector machine
classification. In IJCNLP, pages 665?673.
Elena Filatova and Vasileios Hatzivassiloglou. 2004.
A formal model for information selection in multi-
sentence text extraction. In COLING.
Katja Filippova and Michael Strube. 2008. Depen-
dency tree based sentence compression. In INLG,
pages 25?32.
Dan Gillick and Benoit Favre. 2009. A scalable global
model for summarization. In ILP, pages 10?18.
Hugo Hernault, Helmut Prendinger, David duVerle,
and Mitsuru Ishizuka. 2010. Hilda: A discourse
parser using support vector machine classification.
Dialogue & Discourse, 1(3):1?30.
Tsutomu Hirao, Yasuhisa Yoshida, Masaaki Nishino,
Norihito Yasuda, and Masaaki Nagata. 2013.
Single-document summarization as a tree knapsack
problem. In EMNLP, pages 1515?1520.
Kevin Knight and Daniel Marcu. 2000. Statistics-
based summarization - step one: Sentence compres-
sion. In National Conference on Artificial Intelli-
gence (AAAI), pages 703?710.
Chin-Yew Lin. 2004. Rouge: A package for automatic
evaluation of summaries. In Proc. ACL workshop on
Text Summarization Branches Out, pages 74?81.
William C. Mann and Sandra A. Thompson. 1988.
Rhetorical structure theory: Toward a functional the-
ory of text organization. Text, pages 243?281.
Daniel Marcu. 1998. Improving summarization
through rhetorical parsing tuning. In In Proc. of the
6th Workshop on Very Large Corpora, pages 206?
215.
Ryan T. McDonald. 2007. A study of global infer-
ence algorithms in multi-document summarization.
In ECIR, pages 557?564.
Hajime Morita, Ryohei Sasano, Hiroya Takamura, and
Manabu Okumura. 2013. Subtree extractive sum-
marization via submodular maximization. In ACL,
pages 1023?1032.
Hitoshi Nishikawa, Takaaki Hasegawa, Yoshihiro Mat-
suo, and Genichiro Kikui. 2010. Opinion summa-
rization with integer linear programming formula-
tion for sentence extraction and ordering. In COL-
ING, pages 910?918.
Xian Qian and Yang Liu. 2013. Fast joint compres-
sion and summarization via graph cuts. In EMNLP,
pages 1492?1502.
Hiroya Takamura and Manabu Okumura. 2009. Text
summarization model based on the budgeted median
problem. In CIKM, pages 1589?1592.
Kohei Tomita, Hiroya Takamura, and Manabu Oku-
mura. 2009. A new approach of extractive sum-
marization combining sentence selection and com-
pression. IPSJ SIG Notes, pages 13?20.
320
Proceedings of the 5th International Workshop on Semantic Evaluation, ACL 2010, pages 69?74,
Uppsala, Sweden, 15-16 July 2010. c?2010 Association for Computational Linguistics
SemEval-2010 Task: Japanese WSD
Manabu Okumura
Tokyo Institute of Technology
oku@pi.titech.ac.jp
Kiyoaki Shirai
Japan Advanced Institute of Science and Technology
kshirai@jaist.ac.jp
Kanako Komiya
Tokyo University of Agriculture and Technology
kkomiya@cc.tuat.ac.jp
Hikaru Yokono
Tokyo Institute of Technology
yokono@lr.pi.titech.ac.jp
Abstract
An overview of the SemEval-2 Japanese
WSD task is presented. It is a lexical
sample task, and word senses are defined
according to a Japanese dictionary, the
Iwanami Kokugo Jiten. This dictionary
and a training corpus were distributed to
participants. The number of target words
was 50, with 22 nouns, 23 verbs, and 5
adjectives. Fifty instances of each target
word were provided, consisting of a to-
tal of 2,500 instances for the evaluation.
Nine systems from four organizations par-
ticipated in the task.
1 Introduction
This paper reports an overview of the SemEval-
2 Japanese Word Sense Disambiguation (WSD)
task. It can be considered an extension of the
SENSEVAL-2 Japanese monolingual dictionary-
based task (Shirai, 2001), so it is a lexical sam-
ple task. Word senses are defined according to
the Iwanami Kokugo Jiten (Nishio et al, 1994), a
Japanese dictionary published by Iwanami Shoten.
It was distributed to participants as a sense inven-
tory. Our task has the following two new charac-
teristics:
1. All previous Japanese sense-tagged corpora
were from newspaper articles, while sense-
tagged corpora were constructed in English
on balanced corpora, such as Brown corpus
and BNC corpus. The first balanced corpus
of contemporary written Japanese (BCCWJ
corpus) is now being constructed as part of
a national project in Japan (Maekawa, 2008),
and we are now constructing a sense-tagged
corpus based on it. Therefore, the task will
use the first balanced Japanese sense-tagged
corpus.
Because a balanced corpus consists of docu-
ments from multiple genres, the corpus can
be divided into multiple sub-corpora of a
genre. In supervised learning approaches
on word sense disambiguation, because word
sense distribution might vary across different
sub-corpora, we need to take into account the
genres of training and test corpora. There-
fore, word sense disambiguation on a bal-
anced corpus requires tackling a kind of do-
main (genre) adaptation problem (Chang and
Ng, 2006; Agirre and de Lacalle, 2008).
2. In previous WSD tasks, systems have been
required to select a sense from a given set of
senses in a dictionary for a word in one con-
text (an instance). However, the set of senses
in the dictionary is not always complete. New
word senses sometimes appear after the dic-
tionary has been compiled. Therefore, some
instances might have a sense that cannot be
found in the dictionary?s set. The task will
take into account not only the instances that
have a sense in the given set but also the in-
stances that have a sense that cannot be found
in the set. In the latter case, systems should
output that the instances have a sense that is
not in the set.
Training data, a corpus that consists of three
genres (books, newspaper articles, and white pa-
pers) and is manually annotated with sense IDs,
was also distributed to participants. For the evalu-
ation, we distributed a corpus that consists of four
genres (books, newspaper articles, white papers,
and documents from a Q&A site on the WWW)
with marked target words as test data. Participants
were requested to assign one or more sense IDs to
each target word, optionally with associated prob-
abilities. The number of target words was 50, with
22 nouns, 23 verbs, and 5 adjectives. Fifty in-
stances of each target word were provided, con-
69
sisting of a total of 2,500 instances for the evalua-
tion.
In what follows, section two describes the de-
tails of the data used in the Japanese WSD task.
Section three describes the process to construct
the sense tagged data, including the analysis of an
inter-annotator agreement. Section four briefly in-
troduces participating systems and section five de-
scribes their results. Finally, section six concludes
the paper.
2 Data
In the Japanese WSD task, three types of data were
distributed to all participants: a sense inventory,
training data, and test data1.
2.1 Sense Inventory
As described in section one, word senses are
defined according to a Japanese dictionary, the
Iwanami Kokugo Jiten. The number of headwords
and word senses in the Iwanami Kokugo Jiten is
60,321 and 85,870.
As described in the task description of
SENSEVAL-2 Japanese dictionary task (Shirai,
2001), the Iwanami Kokugo Jiten has hierarchi-
cal structures in word sense descriptions. The
Iwanami Kokugo Jiten has at most three hierarchi-
cal layers.
2.2 Training Data
An annotated corpus was distributed as the train-
ing data. It consists of 240 documents of three
genres (books, newspaper articles, and white pa-
pers) from the BCCWJ corpus. The annotated in-
formation in the training data is as follows:
? Morphological information
The document was annotated with morpho-
logical information (word boundaries, a part-
of-speech (POS) tag, a base form, and a read-
ing) for all words. All the morphological in-
formation was automatically annotated using
chasen2 with unidic and was manually post-
edited.
1Due to space limits, we unfortunately cannot present the
statistics of the training and test data, such as the number
of instances in different genres, the number of instances for
a new word sense, and the Jensen Shannon (JS) divergence
(Lin, 1991; Dagan et al, 1997) between the word sense dis-
tributions of two different genres. We hope we will present
them in another paper in the near future.
2http://chasen-legacy.sourceforge.jp/
? Genre code
Each document was assigned a code indicat-
ing its genre from the aforementioned list.
? Word sense IDs
3,437 word types in the data were annotated
for sense IDs, and the data contain 31,611
sense-tagged instances that include 2,500 in-
stances for the 50 target words. Words as-
signed with sense IDs satisfied the following
conditions:
1. The Iwanami Kokugo Jiten gave their
sense description.
2. Their POSs were either a noun, a verb,
or an adjective.
3. They were ambiguous, that is, there
were more than two word senses for
them in the dictionary.
Word sense IDs were manually annotated.
2.3 Test Data
The test data consists of 695 documents of four
genres (books, newspaper articles, white papers,
and documents from a Q&A site on the WWW)
from the BCCWJ corpus, with marked target
words. The documents used for the training and
test data are not mutually exclusive. The num-
ber of overlapping documents between the train-
ing and test data is 185. The instances used for the
evaluation were not provided as the training data3.
The annotated information in the test data is as fol-
lows:
? Morphological information
Similar to the training data, the document
was annotated with morphological informa-
tion (word boundaries, a POS tag, a base
form, and a reading) for all words. All mor-
phological information was automatically an-
notated using chasen with unidic and was
manually post-edited.
? Genre code
As in the training data, each document was
assigned a code indicating its genre from the
aforementioned list.
? Word sense IDs
Word sense IDs were manually annotated for
3The word sense IDs for them were hidden from the par-
ticipants.
70
the target words4.
The number of target words was 50, with 22
nouns, 23 verbs, and 5 adjectives. Fifty instances
of each target word were provided, consisting of a
total of 2,500 instances for the evaluation.
3 Word Sense Tagging
Except for the word sense IDs, the data described
in section two was developed by the National In-
stitute of Japanese Language. However, the word
sense IDs were newly annotated on the data. This
section presents the process of annotating the word
sense IDs, and the analysis of the inter-annotator
agreement.
3.1 Sampling Target Words
When we chose target words, we considered the
following conditions:
? The POSs of target words were either a noun,
a verb, or an adjective.
? We chose words that occurred more than 50
times in the training data.
? The relative ?difficulty? in disambiguating
the sense of words was taken into account.
The difficulty of the word w was defined by
the entropy of the word sense distribution
E(w) in the test data (Kilgarriff and Rosen-
zweig, 2000). Obviously, the higher E(w) is,
the more difficult the WSD for w is.
? The number of instances for a new sense was
also taken into account.
3.2 Manual Annotation
Nine annotators assigned the correct word sense
IDs for the training and test data. All of them had a
certain level of linguistic knowledge. The process
of manual annotation was as follows:
1. An annotator chose a sense ID for each word
separately in accordance with the following
guidelines:
? One sense ID was to be chosen for each
word.
? Sense IDs at any layers in the hierarchi-
cal structures were assignable.
4They were hidden from the participants during the for-
mal run.
? The ?new word sense? tag was to be
chosen only when all sense IDs were not
absolutely applicable.
2. For the instances that had a ?new word sense?
tag, another annotator reexamined carefully
whether those instances really had a new
sense.
Because a fragment of the corpus was tagged by
multiple annotators in a preliminary annotation,
the inter-annotator agreement between the two an-
notators in step 1 was calculated with Kappa statis-
tics. It was 0.678.
4 Evaluation Methodology
The evaluation was returned in the following two
ways:
1. The outputted sense IDs were evaluated, as-
suming the ?new sense? as another sense ID.
The outputted sense IDs were compared to
the given gold standard word senses, and the
usual precision measure for supervised word
sense disambiguation systems was computed
using the scorer. The Iwanami Kokugo Jiten
has three levels for sense IDs, and we used
the middle-level sense in the task. Therefore,
the scoring in the task was ?middle-grained
scoring.?
2. The ability of finding the instances of new
senses was evaluated, assuming the task
as classifying each instance into a ?known
sense? or ?new sense? class. The outputted
sense IDs (same as in 1.) were compared to
the given gold standard word senses, and the
usual accuracy for binary classification was
computed, assuming all sense IDs in the dic-
tionary were in the ?known sense? class.
5 Participating Systems
In the Japanese WSD task, 10 organizations reg-
istered for participation. However, only the nine
systems from four organizations submitted the re-
sults. In what follows, we outline them with the
following description:
1. learning algorithm used,
2. features used,
3. language resources used,
71
4. level of analysis performed in the system,
5. whether and how the difference in the text
genre was taken into account,
6. method to detect new senses of words, if any.
Note that most of the systems used supervised
learning techniques.
? HIT-1
1. Naive Bayes, 2. Word form/POS of the
target word, word form/POS before or after
the target word, content words in the con-
text, classes in a thesaurus for those words in
the context, the text genre, 3. ?Bunrui-Goi-
Hyou?, a Japanese thesaurus (National Insti-
tute of Japanese Language, 1964), 4. Mor-
phological analysis, 5. A genre is included in
the features. 6. Assuming that the posterior
probability has a normal distribution, the sys-
tem judges those instances deviating from the
distribution at the 0.05 significance level as a
new word sense
? JAIST-1
1. Agglomerative clustering, 2. Bag-of-
words in context, etc. 3. None, 4. Mor-
phological analysis, 5. The system does not
merge example sentences in different genre
sub-corpus into a cluster. 6. First, the system
makes clusters of example sentences, then
measures the similarity between a cluster and
a sense in the dictionary, finally regarding the
cluster as a collection of new senses when
the similarity is small. For WSD, the system
chooses the most similar sense for each clus-
ter, then it considers all the instances in the
cluster to have that sense.
? JAIST-2
1. SVM, 2. Word form/POS before or after
the target word, content words in the context,
etc. 3. None, 4. Morphological analysis, 5.
The system was trained with the feature set
where features are distinguished whether or
not they are derived from only one genre sub-
corpus. 6. ?New sense? is treated as one of the
sense classes.
? JAIST-3
The system is an ensemble of JAIST-1 and
JAIST-2. The judgment of a new sense is per-
formed by JAIST-1. The output of JAIST-1 is
chosen when the similarity between a cluster
and a sense in the dictionary is sufficiently
high. Otherwise, the output of JAIST-2 is
used.
? MSS-1,2,3
1. Maximum entropy, 2. Three word
forms/lemmas/POSs before or after the target
word, bigrams, and skip bigrams in the con-
text, bag-of-words in the document, a class
of the document categorized by a topic clas-
sifier, etc. 3. None, 4. None, 5. For each tar-
get word, the system selected the genre and
dictionary examples combinations for train-
ing data, which got the best results in cross-
validation. 6. The system calculated the en-
tropy for each target word given by the Maxi-
mum Entropy Model (MEM). It assumed that
high entropy (when probabilities of classes
are uniformly dispersed) was indicative of a
new sense. The threshold was tuned by using
the words with a new sense tag in the training
data. Three official submissions correspond
to different thresholds.
? RALI-1, RALI-2
1. Naive Bayes, 2. Only the ?writing? of
the words (inside of <mor> tag), 3. The
Mainichi 2005 corpus of NTCIR, parsed with
chasen+unidic, 4. None, 5. Not taken into ac-
count, 6. ?New sense? is only used when it is
evident in the training data
For more details, please refer to their description
papers.
6 Their Results
The evaluation results of all the systems are shown
in tables 1 and 2. ?Baseline? for WSD indicates
the results of the baseline system that used SVM
with the following features:
? Morphological features
Bag-of-words (BOW), Part-of-speech (POS),
and detailed POS classification. We extract
these features from the target word itself and
the two words to the right and left of it.
? Syntactic features
? If the POS of a target word is a noun,
extract the verb in a grammatical depen-
dency relation with the noun.
72
Table 1: Results: Word sense disambiguation
Precision
Baseline 0.7528
HIT-1 0.6612
JAIST-1 0.6864
JAIST-2 0.7476
JAIST-3 0.7208
MSS-1 0.6404
MSS-2 0.6384
MSS-3 0.6604
RALI-1 0.7592
RALI-2 0.7636
Table 2: Results: New sense detection
Accuracy Precision Recall
Baseline 0.9844 - 0
HIT-1 0.9132 0.0297 0.0769
JAIST-1 0.9512 0.0337 0.0769
JAIST-2 0.9872 1 0.1795
JAIST-3 0.9532 0.0851 0.2051
MSS-1 0.9416 0.1409 0.5385
MSS-2 0.9384 0.1338 0.5385
MSS-3 0.9652 0.2333 0.5385
RALI-1 0.9864 0.7778 0.1795
RALI-2 0.9872 0.8182 0.2308
? If the POS of a target word is a verb, ex-
tract the noun in a grammatical depen-
dency relation with the verb.
? Figures in Bunrui-Goi-Hyou
4 and 5 digits regarding the content word to
the right and left of the target word.
The baseline system did not take into account any
information on the text genre. ?Baseline? for new
sense detection (NSD) indicates the results of the
baseline system, which outputs a sense in the dic-
tionary and never outputs the new sense tag. Pre-
cision and recall for NSD are shown just for refer-
ence. Because relatively few instances for a new
word sense were found (39 out of 2500), the task
of the new sense detection was found to be rather
difficult.
Tables 3 and 4 show the results for nouns, verbs,
and adjectives. In our comparison of the base-
line system scores for WSD, the score for nouns
was the biggest, and the score for verbs was the
smallest (table 3). However, the average entropy
of nouns was the second biggest (0.7257), and that
Table 3: Results for each POS (Precision): Word
sense disambiguation
Noun Verb Adjective
Baseline 0.8255 0.6878 0.732
HIT-1 0.7436 0.5739 0.7
JAIST-1 0.7645 0.5957 0.76
JAIST-2 0.84 0.6626 0.732
JAIST-3 0.8236 0.6217 0.724
MSS-1 0.7 0.5504 0.792
MSS-2 0.6991 0.5470 0.792
MSS-3 0.7218 0.5713 0.8
RALI-1 0.8236 0.6965 0.764
RALI-2 0.8127 0.7191 0.752
Table 4: Results for each POS (Accuracy): New
sense detection
Noun Verb Adjective
Baseline 0.97 0.9948 1
HIT-1 0.8881 0.9304 0.944
JAIST-1 0.9518 0.9470 0.968
JAIST-2 0.9764 0.9948 1
JAIST-3 0.9564 0.9470 0.968
MSS-1 0.9355 0.9409 0.972
MSS-2 0.9336 0.9357 0.972
MSS-3 0.96 0.9670 0.98
RALI-1 0.9745 0.9948 1
RALI-2 0.9764 0.9948 1
of verbs was the biggest (1.194)5.
We set up three word classes, D
diff
(E(w) ?
1), D
mid
(0.5 ? E(w) < 1), and D
easy
(E(w) <
0.5). D
diff
, D
mid
, and D
easy
consist of 20, 19
and 11 words, respectively. Tables 5 and 6 show
the results for each word class. The results of
WSD are quite natural in that the higher E(w) is,
the more difficult WSD is, and the more the per-
formance degrades.
7 Conclusion
This paper reported an overview of the SemEval-2
Japanese WSD task. The data used in this task will
be available when you contact the task organizer
and sign a copyright agreement form. We hope
this valuable data helps many researchers improve
their WSD systems.
5The average entropy of adjectives was 0.6326.
73
Table 5: Results for entropy classes (Precision):
Word sense disambiguation
D
easy
D
mid
D
diff
Baseline 0.9418 0.7411 0.66
HIT-1 0.8436 0.6832 0.54
JAIST-1 0.8782 0.7158 0.553
JAIST-2 0.9509 0.7484 0.635
JAIST-3 0.92 0.7368 0.596
MSS-1 0.8291 0.6558 0.522
MSS-2 0.8273 0.6558 0.518
MSS-3 0.8345 0.6905 0.536
RALI-1 0.9455 0.7653 0.651
RALI-2 0.94 0.7558 0.674
Table 6: Results for Entropy classes (Accuracy):
New sense detection
D
easy
D
mid
D
diff
Baseline 1 0.9737 0.986
HIT-1 0.8909 0.9095 0.929
JAIST-1 0.9672 0.9505 0.943
JAIST-2 1 0.9811 0.986
JAIST-3 0.9673 0.9558 0.943
MSS-1 0.9818 0.9221 0.938
MSS-2 0.98 0.9221 0.931
MSS-3 0.9873 0.9611 0.957
RALI-1 1 0.9789 0.986
RALI-2 1 0.9811 0.986
Acknowledgments
We would like to thank all the participants and the
annotators for constructing this sense tagged cor-
pus.
References
Eneko Agirre and Oier Lopez de Lacalle. 2008. On ro-
bustness and domain adaptation using svd for word
sense disambiguation. In Proc. of COLING?08.
Yee Seng Chang and Hwee Tou Ng. 2006. Estimating
class priors in domain adaptation for wsd. In Proc.
of ACL?06.
Ido Dagan, Lillian Lee, and Fernando Pereira. 1997.
Similarity-based methods for word sense disam-
biguation. In Proceedings of the Thirty-Fifth An-
nual Meeting of the Association for Computational
Linguistics and Eighth Conference of the European
Chapter of the Association for Computational Lin-
guistics, pages 56?63.
A. Kilgarriff and J. Rosenzweig. 2000. English sense-
val: Report and results. In Proc. LREC?00.
J. Lin. 1991. Divergence measures based on the shan-
non entropy. IEEE Transactions on Information
Theory, 37(1):145?151.
Kikuo Maekawa. 2008. Balanced corpus of con-
temporary written japanese. In Proceedings of the
6th Workshop on Asian Language Resources (ALR),
pages 101?102.
National Institute of Japanese Language. 1964. Bun-
ruigoihyou. Shuuei Shuppan. In Japanese.
Minoru Nishio, Etsutaro Iwabuchi, and Shizuo Mizu-
tani. 1994. Iwanami Kokugo Jiten Dai Go Han.
Iwanami Publisher. In Japanese.
Kiyoaki Shirai. 2001. Senseval-2 japanese dictionary
task. In Proceedings of SENSEVAL-2: Second Inter-
national Workshop on Evaluating Word Sense Dis-
ambiguation Systems, pages 33?36.
74
Proceedings of the 2nd Workshop on Computational Approaches to Subjectivity and Sentiment Analysis, ACL-HLT 2011, pages 80?86,
24 June, 2011, Portland, Oregon, USA c?2011 Association for Computational Linguistics
Developing Japanese WordNet Affect for Analyzing Emotions 
 
 
Yoshimitsu Torii1       Dipankar Das2       Sivaji Bandyopadhyay2      Manabu Okumura1 
1Precision and Intelligence Laboratory, Tokyo Institute of Technology, Japan 
2Computer Science and Engineering Department, Jadavpur University, India 
torii@lr.pi.titech.ac.jp, dipankar.dipnil2005@gmail.com 
sivaji_cse_ju@yahoo.com, oku@pi.titech.ac.jp 
 
Abstract 
This paper reports the development of Jap-
anese WordNet Affect from the English 
WordNet Affect lists with the help of Eng-
lish SentiWordNet and Japanese WordNet. 
Expanding the available synsets of the 
English WordNet Affect using SentiWord-
Net, we have performed the translation of 
the expanded lists into Japanese based on 
the synsetIDs in the Japanese WordNet. A 
baseline system for emotion analysis of 
Japanese sentences has been developed 
based on the Japanese WordNet Affect. The 
incorporation of morphology improves the 
performance of the system. Overall, the 
system achieves average precision, recall 
and F-scores of 32.76%, 53% and 40.49% 
respectively on 89 sentences of the Japa-
nese judgment corpus and 83.52%, 49.58% 
and 62.22% on 1000 translated Japanese 
sentences of the SemEval 2007 affect sens-
ing test corpus. Different experimental out-
comes and morphological analysis suggest 
that irrespective of the google translation 
error, the performance of the system could 
be improved by enhancing the Japanese 
WordNet Affect in terms of coverage.  
1 Introduction 
Emotion analysis, a recent sub discipline at the 
crossroads of information retrieval (Sood et al, 
2009) and computational linguistics (Wiebe et al, 
2006) is becoming increasingly important from 
application view points of affective computing.  
The majority of subjective analysis methods that 
are related to emotion is based on textual keywords 
spotting that use specific lexical resources. Senti-
WordNet (Baccianella et al, 2010) is a lexical re-
source that assigns positive, negative and objective 
scores to each WordNet synset (Miller, 1995). Sub-
jectivity wordlist (Banea et al, 2008) assigns 
words with the strong or weak subjectivity and 
prior polarities of types positive, negative and neu-
tral.  Affective lexicon (Strapparava and Valitutti, 
2004), one of the most efficient resources of emo-
tion analysis, contains emotion words. To the best 
of our knowledge, these lexical resources have 
been created for English. A recent study shows that 
non-native English speakers support the growing 
use of the Internet1. Hence, there is a demand for 
automatic text analysis tools and linguistic re-
sources for languages other than English.  
In the present task, we have prepared the Japa-
nese WordNet Affect from the already available 
English WordNet Affect (Strapparava and Valitutti, 
2004). Entries in the English WordNet Affect are 
annotated using Ekman?s (1993) six emotional 
categories (joy, fear, anger, sadness, disgust, sur-
prise). The collection of the English WordNet Af-
fect 2 synsets that are used in the present work was 
provided as a resource in the ?Affective Text? 
shared task of SemEval-2007 Workshop.  
The six WordNet Affect lists that were provided 
in the shared task contain only 612 synsets in total 
with 1536 words. The words in each of the six 
emotion lists have been observed to be not more 
than 37.2% of the words present in the correspond-
ing SentiWordNet synsets. Hence, these six lists 
are expanded with the synsets retrieved from the 
                                                        
1 http://www.internetworldstats.com/stats.htm 
2 http://www.cse.unt.edu/~rada/affectivetext/ 
80
English SentiWordNet (Baccianella et al, 2010). 
We assumed that the new sentiment bearing words 
in English SentiWordNet might have some emo-
tional connotation in Japanese even keeping their 
part-of-speech (POS) information unchanged. The 
numbers of entries in the expanded word lists are 
increased by 69.77% and 74.60% at synset and 
word levels respectively. We have mapped the 
synsetID of the WordNet Affect lists with the syn-
setID of the WordNet 3.03. This mapping helps in 
expanding the WordNet Affect lists with the recent 
version of SentiWordNet 3.0 4 as well as translating 
with the Japanese WordNet (Bond et al, 2009). 
Some affect synsets (e.g., 00115193-a huffy, mad, 
sore) are not translated into Japanese as there are 
no equivalent synset in the Japanese WordNet.  
Primarily, we have developed a baseline system 
based on the Japanese WordNet Affect and carried 
out the evaluation on a Japanese judgement corpus 
of 89 sentences. The system achieves the average 
F-score of 36.39% with respect to six emotion 
classes. We have also incorporated an open source 
Japanese morphological analyser 5 . The perform-
ance of the system has been increased by 4.1% in 
average F-score with respect to six emotion classes. 
Scarcity of emotion corpus in Japanese moti-
vated us to apply an open source google translator6 
to build the Japanese emotion corpus from the 
available English SemEval-2007 affect sensing 
corpus. The baseline system based on the Japanese 
WordNet Affect achieves average precision, recall 
and F-score of 83.52%, 49.58% and 62.22% re-
spectively on 1000 translated test sentences. The 
inclusion of morphological processing improves 
the performance of the system. Different experi-
ments have been carried out by selecting different 
ranges of annotated emotion scores. Error analysis 
suggests that though the system performs satisfac-
torily in identifying the sentential emotions based 
on the available words of the Japanese WordNet 
Affect, the system suffers from the translated ver-
sion of the corpus. In addition to that, the Japanese 
WordNet Affect also needs an improvement in 
terms of coverage.  
 The rest of the paper is organized as follows. 
Different developmental phases of the Japanese 
WordNet Affect are described in Section 3. Prepa-
                                                        
3 http://wordnet.princeton.edu/wordnet/download/ 
4 http://sentiwordnet.isti.cnr.it/ 
5 http://mecab.sourceforge.net/ 
6 http://translate.google.com/# 
ration of the translated Japanese corpus, different 
experiments and evaluations based on morphology 
and the annotated emotion scores are elaborated in 
Section 4. Finally Section 5 concludes the paper. 
2 Related Works  
The extraction and annotation of subjective terms 
started with machine learning approaches (Hat-
zivassiloglou and McKeown, 1997). Some well 
known sentiment lexicons have been developed, 
such as subjective adjective list (Baroni and Veg-
naduzzo, 2004), English SentiWordNet (Esuli et. 
al., 2006), Taboada?s adjective list (Voll and 
Taboada, 2007), SubjectivityWord List (Banea et 
al., 2008) etc. Andreevskaia and Bergler (2006) 
present a method for extracting positive or negative 
sentiment bearing adjectives from WordNet using 
the Sentiment Tag Extraction Program (STEP). 
The proposed methods in (Wiebe and Riloff, 2006) 
automatically generate resources for subjectivity 
analysis for a new target language from the avail-
able resources for English. On the other hand, an 
automatically generated and scored sentiment lexi-
con, SentiFul (Neviarouskaya et al, 2009), its 
expansion, morphological modifications and dis-
tinguishing sentiment features also shows the con-
tributory results.   
But, all of the above mentioned resources are in 
English and have been used in coarse grained sen-
timent analysis (e.g., positive, negative or neutral). 
The proposed method in (Takamura et al, 2005) 
extracts semantic orientations from a small number 
of seed words with high accuracy in the experi-
ments on English as well as Japanese lexicons. 
But, it was also aimed for sentiment bearing words. 
Instead of English WordNet Affect (Strapparava 
and Valitutti, 2004), there are a few attempts in 
other languages such as, Russian and Romanian 
(Bobicev et al, 2010), Bengali (Das and Bandyop-
adhyay, 2010) etc. Our present approach is similar 
to some of these approaches but in contrast, we 
have evaluated our Japanese WordNet Affect on the 
SemEval 2007 affect sensing corpus translated into 
Japanese. In recent trends, the application of me-
chanical turk for generating emotion lexicon (Mo-
hammad and Turney, 2010) shows promising 
results. In the present task, we have incorporated 
the open source, available and accessible resources 
to achieve our goals.   
81
3 Developmental Phases  
3.1 WordNet Affect 
The English WordNet Affect, based on Ekman?s six 
emotion types is a small lexical resource compared 
to the complete WordNet but its affective annota-
tion helps in emotion analysis. Some collection of 
WordNet Affect synsets was provided as a resource 
for the shared task of Affective Text in SemEval-
2007. The whole data is provided in six files 
named by the six emotions. Each file contains a list 
of synsets and one synset per line. An example 
synset entry from WordNet Affect is as follows. 
a#00117872 angered  enraged  furious  infuri-
ated  maddened 
The first letter of each line indicates the part of 
speech (POS) and is followed by the affectID. The 
representation was simple and easy for further 
processing. We have retrieved and linked the com-
patible synsetID from the recent version of Word-
Net 3.0 with the affectID of the WordNet Affect 
synsets. We have searched each WordNet Affect 
synset in WordNet 3.0. If a matching WordNet 3.0 
synset is found, the WordNet 3.0 synsetID is 
mapped to the WordNet Affect affectID.  The link-
ing between two synsets of WordNet Affect and 
WordNet 3.0 is shown in Figure 1.  
 
WordNet Affect: 
n#05587878 anger choler ire 
a#02336957 annoyed harassed harried pestered 
vexed 
WordNet:  
07516354-n anger, ire, choler 
02455845-a annoyed harassed harried pestered 
vexed 
Linked Synset ID with Affect ID:  
   n#05587878 ?? 07516354-n anger choler ire  
  a#02336957 ?? 02455845-a annoyed harassed 
harried pestered vexed 
Figure 1: Linking between the synsets of Word-
Net Affect and WordNet 
3.2 Expansion of WordNet Affect using Sen-
tiWordNet 
It has been observed that the WordNet Affect con-
tains fewer number of emotion word entries. The 
six lists provided in the SemEval 2007 shared task 
contain only 612 synsets in total with 1536 words. 
The detail distribution of the emotion words as 
well as the synsets in the six different lists accord-
ing to their POS is shown in Table 1. Hence, we 
have expanded the lists with adequate number of 
emotion words using SentiWordNet before at-
tempting any translation of the lists into Japanese. 
SentiWordNet assigns each synset of WordNet with 
two coarse grained subjective scores such as posi-
tive and negative along with an objective score. 
SentiWordNet contains more number of coarse 
grained emotional words than WordNet Affect. We 
assumed that the translation of the coarse grained 
emotional words into Japanese might contain more 
or less fine-grained emotion words. One example 
entry of the SentiWordNet is shown below. The 
POS of the entry is followed by a synset ID, posi-
tive and negative scores and synsets containing 
sentiment words.   
SentiWordNet:  
a 121184  0.25 0.25 infuri-
ated#a#1 furious#a#2 maddened#a#1 en-
raged#a#1 angered#a#1 
Our aim is to increase the number of emotion 
words in the WordNet Affect using SentiWordNet, 
both of which are developed from the WordNet. 
Hence, each word of the WordNet Affect is re-
placed by the equivalent synsets retrieved from 
SentiWordNet if the synset contains that emotion 
word. The POS information in the WordNet Affect 
is kept unchanged during expansion. A related ex-
ample is shown in Figure 2. The distributions of 
expanded synsets and words for each of the six 
emotion classes based on four different POS types 
(noun N, verb V, adjective Adj. and adverb Adv.) 
are shown in Table 1. But, we have kept the dupli-
cate entries at synset level for identifying the emo-
tion related scores in our future attempts by 
utilizing the already associated positive and nega-
tive scores of SentiWordNet. The percentage of 
entries in the updated word lists are increased by 
69.77 and 74.60 at synset and word levels.  
3.3 Translation of Expanded WordNet Affect 
into Japanese  
We have mapped the affectID of the WordNet Af-
fect to the corresponding synsetID of the WordNet 
3.0. This mapping helps to expand the WordNet 
Affect with the recent version of SentiWordNet 3.0 
as well as translating the expanded lists into Japa-
nese using the Japanese WordNet (Bond et al, 
2009).
82
Emotion 
Classes 
WordNet Affect Synset (S) and Word (W) [After SentiWordNet updating] 
N V Adj Adv 
S W S W S W S W 
Anger 48 [198] 99 [403] 19 [103] 64 [399] 39 [89] 120 [328] 21 [23] 35 [50] 
Disgust 3 [17] 6 [21] 6 [21] 22 [62] 6  [38] 34  [230] 4  [5] 10 [19] 
Fear 23[89] 45 [224] 15  [48] 40 [243] 29  [62] 97  [261] 15 [21] 26 [49] 
Joy 73 [375] 149 [761] 40 [252] 122 [727] 84  [194] 203 [616] 30  [45] 65 [133] 
Sadness 32 [115] 64 [180] 10  [43] 33 [92] 55 [129] 169 [779] 26 [26] 43 [47] 
Surprise 5 [31]    8 [28] 7  [42] 28 [205] 12  [33] 41  [164] 4  [6] 13 [28] 
Table 1: Number of POS based Synsets and Words in six WordNet Affect lists before and after updating 
using SentiWordNet 
 
Linked Affect word:  
n#05587878 ?? 07516354-n anger choler ire  
 
SentiWordNet synsets containing  ?anger?:  
07516354-n anger, ire, choler 
14036539-n angriness, anger 
00758972-n anger, ira, ire, wrath 
01785971-v anger 
01787106-v see_red, anger 
 
SentiWordNet synsets containing  ?choler?:  
07552729-n fretfulness, fussiness, crossness, pe-
tulance, peevishness, irritability, choler 
05406958-n choler, yellow_bile 
 
Expanded Affect word:  
n#05587878?? 07516354-n anger choler ire 
14036539-n angriness anger 00758972-n anger 
ira, ire wrath 01785971-v anger  
? 05406958-n choler 
Figure 2: Expansion of WordNet Affect synset 
using SentiWordNet 
 
As the Japanese WordNet 7  is freely available 
and it is being developed based on the English 
WordNet, the synsets of the expanded lists are au-
tomatically translated into Japanese equivalent 
synsets based on the synsetIDs. The number of 
translated Japanese words and synsets for six affect 
lists are shown in Table 2 and Table 3 respectively. 
The following are some translated samples that 
contain word as well as phrase level translations. 
07510348-n surprise ? ??, ?? 
07503260-n disgust ? ????, ?? 
07532440-n unhappiness, sadness ? ????
?, ??, ???, ????, ????  
                                                        
7 http://nlpwww.nict.go.jp/wn-ja/index.en.html 
07527352-n joy, joyousness, joyfulness ? ??
?, ??, ??????, ??, ????, ??, 
??, ?, ???, ??, ?????? 
 
Emotion 
Classes 
Translated WordNet Affect list 
in Japanese (#Words) 
N V Adj Adv 
Anger 861 501 231 9 
Disgust 49 63 219 10 
Fear 375 235 334 104 
Joy 1959 1831 772 154 
Sadness 533 307 575 39 
Surprise 144 218 204 153 
Table 2: Number of POS based translated word 
entries in six Japanese WordNet Affect lists 
 
Emotion 
Classes 
Japanese WordNet Affect list 
Trans 
(#Syn) 
Non-
Trans 
(#Syn) 
Translated 
Morphemes 
(#W) (#P) 
Anger 254 159 1033 450 
Disgust 57 24 218 97 
Fear 146 74 615 315 
Joy 628 238 2940 1273 
Sadness 216 97 846 519 
Surprise 112 25 456 216 
Table 3: Number of translated (Trans) and non-
translated (Non-Trans) synsets (Syn), words (W) 
and phrases (P) in six Japanese WordNet Affects. 
3.4 Analyzing Translation Errors  
Some SentiWordNet synsets (e.g., 00115193-a huf-
fy, mad, sore) are not translated into Japanese as 
there are no equivalent synset entries in the Japa-
nese WordNet. There were a large number of word 
combinations, collocations and idioms in the Japa-
nese WordNet Affect. These parts of synsets show 
problems during translation and therefore manual 
83
translation is carried out for these types. Some of 
the English synsets (?07517292-n lividity?) were 
not translated into Japanese. But, an equivalent 
gloss of the word ?lividity? that is present in the 
Japanese WordNet is ?a state of fury so great the 
face becomes discolored?. One of the reasons of 
such translation problems may be that no equiva-
lent Japanese word sense is available for such Eng-
lish words. 
4 Evaluation and Analysis 
We have evaluated the lexical coverage of the de-
veloped Japanese WordNet Affect on a small emo-
tional judgment corpus and SemEval 2007 affect 
sensing corpus.  
4.1 Evaluation on Judgment Corpus    
The judgment corpus that is being developed by 
the Japan System Applications Co. Ltd. 8 contains 
only 100 sentences of emotional judgments. But, 
this corpus is not an open source till date. We have 
evaluated our Japanese WordNet Affect based base-
line system on these 100 sentences and the results 
for each of the six emotion classes are shown in 
Table 4. We have also incorporated an open source 
morphological analyzer9 in our baseline system.   
The algorithm is that, if a word in a sentence is 
present in any of the Japanese WordNet Affect lists; 
the sentence is tagged with the emotion label cor-
responding to that affect list. But, if any word is 
not found in any of the six lists, each word of the 
sentence is passed through the morphological 
process to identify its root form which is searched 
through the Japanese WordNet Affect lists again. If 
the root form is found in any of the six Japanese 
WordNet Affect lists, the sentence is tagged accor-
dingly. Otherwise, the sentence is tagged as non-
emotional or neutral. The average F-Score of the 
baseline system has been improved by 4.1% with 
respect to the six emotion classes. Due to the fewer 
number of sentential instances in some emotion 
classes (e.g., joy, sadness, surprise), the perfor-
mance of the system gives poor results even after 
including the morphological knowledge. One of 
the reasons may be the less number of words and 
synset entries in some WordNet Affect lists (e.g., 
fear). Hence, we have aimed to translate the Eng-
                                                        
8 http://www.jsa.co.jp/ 
9 http://mecab.sourceforge.net/ 
lish SemEval 2007 affect sensing corpus into Japa-
nese and evaluate our system on the translated cor-
pus. 
 
Emotion 
Classes  
(#Sentences) 
Judgment Corpus (in %) 
Before Morphology [After Mor-
phology] 
Precision Recall F-Score 
Anger 
 (#32) 
51.61 
[64.29] 
50.00 
[68.12] 
50.79 
[66.14] 
disgust 
 (#18) 
25.00 
[45.00] 
5.56 
[10.56] 
9.09 
[17.10] 
fear (#33) NULL 
joy  
(#3) 
3.45 
[8.08] 
66.67 
[100.00] 
6.56 
[14.95] 
Sadness  (#5) NULL 
surprise  
(#9) 
6.90 
[13.69] 
22.22 
[33.33] 
10.53 
[19.41] 
Table 4: Precision, Recall and F-Scores (in %) 
of the system per emotion class on the Judgment 
corpus by including and excluding morphology. 
4.2 Evaluation on Translated SemEval 2007 
Affect Sensing Corpus    
The English SemEval 2007 affect sensing corpus 
consists of news headlines only. Each of the news 
headlines is tagged with a valence score and scores 
for all the six Ekman?s emotions. The six emotion 
scores for each sentence are in the range of 0 to 
100. We have considered that each sentence is as-
signed a single sentential emotion tag based on the 
maximum emotion score out of six annotated emo-
tion scores. We have used the Google translator 
API 10to translate the 250 and 1000 sentences of 
the trial and test sets of the SemEval 2007 corpus 
respectively. The experiments regarding morphol-
ogy and emotion scores are conducted on the trial 
corpus. We have carried out different experiments 
on 1000 test sentences by selecting different ranges 
of emotion scores. The corresponding experimental 
results are also shown in Table 5. Incorporation of 
morphology improves the performance of the sys-
tem. On the other hand, it is observed that the per-
formance of the system decreases by increasing the 
range of Emotion Scores (ES). The reason may be 
that the numeric distribution of the sentential in-
stances in each of the emotion classes decreases as 
the range in emotion scores increases. 
                                                        
10 http://translate.google.com/# 
 
84
Emotion 
Classes 
Japanese Translated SemEval 2007 Test Corpus (in %) 
Before Morphology [After Morphology] 
Emotion Score (ES) ? 0 Emotion Score (ES) ? 10 
Precision Recall F-Score Precision Recall F-Score 
Anger 61.01[68.75] 18.83[31.16] 28.78[42.88] 44.65[52.08] 25.54[33.32] 32.49[40.35] 
disgust 79.55[85.05] 8.35[16.06] 15.12[27.01] 40.91[41.46] 9.89[18.07] 15.93[24.97] 
Fear 93.42[95.45] 10.26[16.77] 18.49[28.52] 77.63[81.82] 13.32[21.42] 22.74[34.03] 
Joy 69.07[72.68] 57.03[80.30] 62.48[76.29] 53.89[55.61] 56.50[96.22] 55.17[70.40] 
sadness 83.33[84.29] 10.58[19.54] 18.77[31.67] 67.78[69.87] 11.78[19.88] 20.07[30.86] 
surprise 94.94[94.94] 7.84[13.65] 14.48[23.99] 72.15[74.58] 8.25[15.87] 14.81[26.30] 
Emotion Score (ES) ? 30 Emotion Score (ES) ? 50 
Anger 21.38[28.12] 39.08[62.45] 27.64[38.59] 6.92[10.42] 57.89[78.02] 12.36[18.26] 
disgust 2.27[5.04] 3.70[6.72] 2.82[6.15] NIL NIL NIL 
Fear 44.74[56.82] 16.67[28.76] 24.29[38.45] 21.05[29.55] 17.98[31.26] 19.39[30.79] 
Joy 31.48[33.42] 56.86[97.08] 40.52[50.53] 12.04[24.98] 61.32[87.66] 20.12[39.10] 
sadness 37.78[69.86] 15.60[25.31] 22.08[37.22] 13.33[23.07] 12.12[22.57] 12.70[18.71] 
surprise 17.72[20.34] 8.14[18.56] 11.16[20.35] 3.80[8.50] 7.50[12.50] 5.04[10.11] 
Table 6: Precision, Recall and F-Scores (in %) of the system per emotion class on the translated Japanese 
SemEval 2007 test corpus before and after including morphology on different ranges of Emotion Scores. 
4.3 Analysis of Morphology  
Japanese affect lists include words as well as 
phrases. We deal with phrases using Japanese 
morphology tool to find affect words in a sentence 
and substitute an affect word into its original con-
jugated form. One of the main reasons of using a 
morphology tool is to analyze the conjugated form 
and to identify the phrases. For example, the Japa-
nese word for the equivalent English word ?anger? 
is "?? (o ko ru)" but there are other conjugated 
word forms such as "???(o ko tta)" that means 
?angered? and it is used in past tense. Similarly, 
other conjugated form "????? (o ko tte i ta)" 
denotes the past participle form ?have angered? of 
the original word ?anger?. The morphological form 
of its passive sense is "???? (o ko ra re ru)" 
that means ?be angered?. We identify the word 
forms from their corresponding phrases by using 
the morpheme information. For example, the 
phrase "???? (o ko ra re ru)" consists of two 
words, one is ??? (o ko ra) that is in an imper-
fective form and other word is "?? (re ru) which 
is in an original form. The original form of the im-
perfective word ?? (o ko ra) is "?? (o ko 
ru)". It has been found that some of the English 
multi-word phrases have no equivalent Japanese 
phrase available. Only the equivalent Japanese 
words are found in Japanese WordNet. For exam 
 
ple, the following synset contains a multi-word 
phrase ?see-red?. Instead of any equivalent phrases, 
only words are found in Japanese WordNet. 
01787106-v anger, see -red ? ??, ??, ?? 
5 Conclusion 
The present paper describes the preparation of Jap-
anese WordNet Affect containing six types of emo-
tion words in six separate lists. The automatic 
approach of expanding, translating and sense dis-
ambiguation tasks reduces the manual effort. The 
resource is still being updated with more number 
of emotional words to increase the coverage. The 
sense disambiguation task needs to be improved 
further in future by incorporating more number of 
translators and considering their agreement into 
account. In future we will adopt a corpus-driven 
approach for updating the resource with more 
number of emotion words and phrases for extend-
ing the emotion analysis task in Japanese. 
Acknowledgments 
The work reported in this paper is supported by a 
grant from the India-Japan Cooperative Pro-
gramme (DST-JST) 2009 Research project entitled 
?Sentiment Analysis where AI meets Psychology? 
funded by Department of Science and Technology 
(DST), Government of India. 
85
References  
Andreevskaia A. and Bergler Sabine. 2007. CLaC and 
CLaC-NB: Knowledge-based and corpus-based ap-
proaches to sentiment tagging. 4th International 
Workshop on Semantic Evaluations (SemEval-2007), 
pp. 117?120, Prague. 
Baccianella Stefano, Esuli Andrea and Sebas-tiani Fa-
brizio. 2010. SentiWordNet 3.0: An Enhanced Lexi-
cal Re-source for Sentiment Analysis and Opinion 
Mining. In Proceedings of the 7th Conference on 
Language Resources and Evaluation, pp. 2200-2204. 
Banea, Carmen, Mihalcea Rada, Wiebe Janyce. 2008.  
A Bootstrapping Method for Building Subjectivity 
Lexicons for Languages with Scarce Resources. The 
Sixth International Conference on Language Re-
sources and Evaluation (LREC 2008). 
Baroni M. and Vegnaduzzo S. 2004. Identifying subjec-
tive adjectives through web-based mutual informa-
tion. Proceedings of the German Conference on NLP. 
Bobicev Victoria, Maxim Victoria, Prodan Tatiana, 
Burciu Natalia, Anghelus Victoria. 2010. Emotions 
in words: developing a multilingual WordNet-Affect. 
CICLING 2010.  
Bond, Francis, Hitoshi Isahara, Sanae Fujita, Kiyotaka 
Uchimoto, Takayuki Kuribayashi and Kyoko Kanza-
ki. 2009. Enhancing the Japanese WordNet. 7th 
Workshop on Asian Language Resources, ACL-
IJCNLP 2009, Singapore.  
Das Dipankar and Bandyopadhyay Sivaji. 2010. Devel-
oping Bengali WordNet Affect for Analyzing Emo-
tion. 23rd International Conference on the Computer 
Processing of Oriental Languages (ICCPOL-2010), 
pp. 35-40, California, USA. 
Ekman Paul. 1992. An argument for basic emotions, 
Cognition and Emotion, 6(3-4):169-200. 
Esuli, Andrea. and Sebastiani, Fabrizio. 2006. 
SENTIWORDNET: A Publicly Available Lexical 
Resource for Opinion Mining, LREC. 
Hatzivassiloglou V. and McKeown K. R. 1997. Predict-
ing the semantic orientation of adjectives. 35th An-
nual Meeting of the ACL and the 8th Conference of 
the European Chapter of the ACL, pp. 174?181. 
Miller, A. G. 1995. WordNet: a lexical database for 
English. In Communications of the ACM, vol. 38 
(11), November, pp. 39-41. 
Mohammad, S. and Turney, P.D. 2010. Emotions 
evoked by common words and phrases: Using Me-
chanical Turk to create an emotion lexicon. Proceed-
ings of the NAACL-HLT 2010 Workshop on 
Computational Approaches to Analysis and Genera-
tion of Emotion in Text, LA, California, 26-34. 
Neviarouskaya, Alena, Prendinger Helmut, and Ishizuka 
Mitsuru. 2009. SentiFul: Generating a Reliable Lex-
icon for Sentiment Analysis. International Confe-
rence on Affective Computing and Intelligent 
Interaction (ACII'09), IEEE, pp. 363-368. 
Sood S. and Vasserman, L. 2009. ESSE: Exploring 
Mood on the Web. 3rd International AAAI Confe-
rence on Weblogs and Social Media (ICWSM) Data 
Challenge Workshop. 
Strapparava Carlo and Valitutti, A. 2004. Wordnet-
affect: an affective extension of wordnet, In 4th In-
ternational Conference on Language Resources and 
Evaluation, pp. 1083-1086. 
Strapparava Carlo and Mihalcea Rada. 2007. SemEval-
2007 Task 14: Affective Text. 45th Aunual Meeting 
of Association for Computational linguistics. 
Takamura Hiroya, Inui Takashi, Okumura Manabu. 
2005. Extracting Semantic Orientations of Words us-
ing Spin Model. 43rd Annual Meeting of the Associa-
tion for Computational Linguistics, pp.133-140.  
Voll, K. and M. Taboada. 2007. Not All Words are 
Created Equal: Extracting Semantic Orientation as a 
Function of Adjective Relevance. In Proceedings of 
the 20th Australian Joint Conference on Artificial In-
telligence. pp. 337-346, Gold Coast, Australia. 
Wiebe Janyce and Riloff Ellen. 2006. Creating Subjec-
tive and Objective Sentence Classifiers from Unan-
notated Texts. International Conference on 
Intelligent Text Processing and Computational Lin-
guistics, Mexico City, pp. 475?486. 
 
 
 
86
