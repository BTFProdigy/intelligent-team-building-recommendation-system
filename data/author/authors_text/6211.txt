Analysis of Link Grammar on Biomedical Dependency Corpus
Targeted at Protein-Protein Interactions
Sampo Pyysalo, Filip Ginter, Tapio Pahikkala,
Jorma Boberg, Jouni Ja?rvinen, Tapio Salakoski
Turku Centre for Computer Science (TUCS)
and Dept. of computer science, University of Turku
Lemminka?isenkatu 14A
20520 Turku, Finland,
first name.last name@it.utu.fi
Jeppe Koivula
MediCel Ltd.,
Haartmaninkatu 8
00290 Helsinki, Finland,
jeppe.koivula@medicel.com
Abstract
In this paper, we present an evaluation of the
Link Grammar parser on a corpus consisting
of sentences describing protein-protein interac-
tions. We introduce the notion of an interac-
tion subgraph, which is the subgraph of a de-
pendency graph expressing a protein-protein in-
teraction. We measure the performance of the
parser for recovery of dependencies, fully correct
linkages and interaction subgraphs. We analyze
the causes of parser failure and report specific
causes of error, and identify potential modifica-
tions to the grammar to address the identified
issues. We also report and discuss the effect of
an extension to the dictionary of the parser.
1 Introduction
The challenges of processing the vast amounts of
biomedical publications available in databases
such as MEDLINE have recently attracted a
considerable interest in the Natural Language
Processing (NLP) research community. The
task of information extraction, commonly tar-
geting entity relationships, such as protein-
protein interactions, is an often studied prob-
lem to which various NLP methods have been
applied, ranging from keyword-based methods
(see, e.g., Ginter et al (2004)) to full syntactic
analysis as employed, for example, by Craven
and Kumlien (1999), Temkin and Gilder (2003)
and Daraselia et al (2004).
In this paper, we focus on the syntactic anal-
ysis component of an information extraction
system targeted to find protein-protein inter-
actions from the dependency output produced
by the Link Grammar1 (LG) parser of Sleator
and Temperley (1991). Two recent papers study
LG in the context of biomedical NLP. The work
by Szolovits (2003) proposes a fully automated
method to extend the dictionary of the LG
1http://www.link.cs.cmu.edu/link/
parser with the UMLS Specialist2 lexicon, and
Ding et al (2003) perform a basic evaluation
of LG performance on biomedical text. As both
papers suggest, LG will require modifications in
order to provide a correct analysis of grammat-
ical phenomena that are rare in general English
text, but common in biomedical language. Im-
plementing such modifications is a major effort
that requires a careful analysis of the perfor-
mance of the LG parser to identify the most
common causes of parsing failures and to target
modification efforts.
While Szolovits (2003) does not attempt to
evaluate parser performance at all and Ding
et al (2003) provide only an informal evalua-
tion on manually simplified sentences, we focus
on a more formal evaluation of the LG parser.
For the purpose of this study and also for sub-
sequent research of biomedical information ex-
traction with the LG parser, we have developed
a hand-annotated corpus consisting of unmod-
ified sentences from publications. We use this
corpus to evaluate the performance of the LG
parser and to identify problems and potential
improvements to the grammar and parser.
2 Link Grammar and parser
The Link Grammar and its parser represent an
implementation of a dependency-based compu-
tational grammar. The result of LG analysis for
a sentence is a labeled undirected simple graph,
whose nodes represent the words of the sentence
and whose edges and their labels express the
grammatical relationships between the words.
In LG terminology, the graph is called a link-
age, and its edges are called links. The linkage
must be planar (i.e., links must not cross) when
drawn above the words of the sentence, and the
labels of the links must satisfy the linking con-
straints specified for each word in the grammar.
A connected linkage is termed complete.
2http://www.nlm.nih.gov/research/umls/
15
findings suggest that PIP2 binds to proteins such as profilin
Figure 1: Annotation example. The interaction of two proteins, PIP2 and profilin, is stated by
the words binds to. The links joining these words form the interaction subgraph (drawn with solid
lines).
Due to the structural ambiguity of natural
language, several linkages can typically be con-
structed for an input sentence. In such cases,
the LG parser enumerates all linkages allowed
by the grammar. A post-processing step is then
employed to enforce a number of additional con-
straints. The number of linkages for some sen-
tences can be very high, making post-processing
and storage prohibitively expensive. This prob-
lem is addressed in the LG parser by defining
kmax, the maximal number of linkages to be
post-processed. If the parsing algorithm pro-
duces more than kmax linkages, the output is
reduced to kmax linkages by random sampling.
The linkages are then ordered from best to worst
using heuristic goodness criteria.
In order to be usable in practice, a parser is
typically required to provide a partial analysis
of a sentence for which it cannot construct a full
analysis. If the LG parser cannot construct a
complete linkage for a sentence, the connected-
ness requirement is relaxed so that some words
do not belong to the linkage at all. The LG
parser is also time-limited. If the full set of
linkages cannot be constructed in a given time
tmax, the parser enters a panic mode, in which it
performs an efficient but considerably restricted
parse, resulting in reduced performance. The
parameters tmax and kmax set the trade-off be-
tween the qualitative performance and the re-
source efficiency of the parser.
3 Corpus annotation and interaction
subgraphs
To compile a corpus of sentences describing
protein-protein interactions, we first selected
pairs of proteins that are known to interact
from the Database of Interacting Proteins3. We
entered these pairs as search terms into the
PubMed retrieval system. We then split the
publication abstracts returned by the searches
into sentences and included titles. These were
again searched for the protein pairs. This gave
us a set of 1927 sentences that contain the
3http://dip.doe-mbi.ucla.edu/
names of at least two proteins that are known
to interact. A domain expert annotated these
sentences for protein names and for words stat-
ing their interactions. Of these sentences, 1114
described at least one protein-protein interac-
tion.
Thereafter, we performed a dependency anal-
ysis and produced annotation of dependencies.
To minimize the amount of mistakes, each sen-
tence was independently annotated by two an-
notators and differences were then resolved by
discussion. The assigned dependency structure
was produced according to the LG linkage con-
ventions. Link types were not included in the
annotation, and no cycles were introduced in
the dependency graphs. All ambiguities where
the LG parser is capable of at least enumerat-
ing all alternatives (such as prepositional phrase
attachment) were enforced in the annotation.
A random sample consisting of 300 sentences,
including 28 publication titles, has so far been
fully annotated, giving 7098 word-to-word de-
pendencies. This set of sentences is the corpus
we refer to in the following sections.
An information extraction system targeted
at protein-protein interactions and their types
needs to identify three constituents that express
an interaction in a sentence: the proteins in-
volved and the word or phrase that states their
interaction and suggests the type of this inter-
action. To extract this information from a LG
linkage, the links connecting these items must
be recovered correctly by the parser. The fol-
lowing definition formalizes this notion.
Definition 1 (Interaction subgraph) The
interaction subgraph for an interaction between
two proteins A and B in a linkage L is the
minimal connected subgraph of L that contains
A, B, and the word or phrase that states their
interaction.
The recovery of a connected component con-
taining the protein names and the interaction
word is not sufficient: by the definition of a
complete linkage, such a component is always
present. Consequently, the exact set of links
16
that forms the interaction subgraph must be re-
covered.
For each interaction stated in a sentence,
the corpus annotation specifies the proteins in-
volved and the interaction word. The interac-
tion subgraph for each interaction can thus be
extracted automatically from the corpus. Be-
cause the corpus does not contain cyclic depen-
dencies, the interaction subgraphs are unique.
366 interaction subgraphs were identified from
the corpus, one for each described interaction.
The interaction subgraphs can be partially over-
lapping, because a single link can be part of
more than one interaction subgraph. Figure 1
shows an example of an annotated text frag-
ment.
4 Evaluation criteria
We evaluated the performance of the LG parser
according to the following three quantitative cri-
teria:
? Number of dependencies recovered
? Number of fully correct linkages
? Number of interaction subgraphs recovered
The number of recovered dependencies gives an
estimate of the probability that a dependency
will be correctly identified by the LG parser
(this criterion is also employed by, e.g., Collins
et al (1999)). The number of fully correct link-
ages, i.e. linkages where all annotated depen-
dencies are recovered, measures the fraction of
sentences that are parsed without error. How-
ever, a fully correct linkage is not necessary to
extract protein-protein interactions from a sen-
tence; to estimate how many interactions can
potentially be recovered, we measure the num-
ber of interaction subgraphs for which all de-
pendencies were recovered.
For each criterion, we measure the perfor-
mance for the first linkage returned by the
parser. However, the first linkage as ordered
by the heuristics of the LG parser was often not
the best (according to the criteria above) of the
linkages returned by the parser. To separate
the effect of the heuristics from overall LG per-
formance, we identify separately for each of the
three criteria the best linkage among the link-
ages returned by the parser, and we also report
performance for the best linkages.
We further divide the parsed sentences into
three categories: (1) sentences for which the
time tmax for producing a normal parse was ex-
hausted and the parser entered panic mode, (2)
sentences where linkages were sampled because
more than kmax linkages were produced, and
(3) stable sentences for which neither of these
occurred. A full analysis of all linkages that
the grammar allows is only possible for stable
sentences. For sentences in the other two cat-
egories, random effects may affect the results:
sentences for which more than kmax linkages are
produced are subject to randomness in sam-
pling, and sentences where the parser enters
panic mode were always subject to subsequent
sampling in our experiments.
5 Evaluation
To evaluate the ability of the LG parser to pro-
duce correct linkages, we increased the number
of stable sentences by setting the tmax param-
eter to 10 minutes and the kmax parameter to
10000 instead of using the defaults tmax = 30
seconds and kmax = 1000. When parsing the
corpus using these parameters, 28 sentences fell
into the panic category, 61 into the sampled
category, and 211 were stable. The measured
parser performance for the corpus is presented
in Table 1.
While the fraction of sentences that have a
fully correct linkage as the first linkage is quite
low (approximately 7%), for 28% of sentences
the parser is capable of producing a fully cor-
rect linkage. Performance was especially poor
for the publication titles in the corpus. Because
titles are typically fragments not containing a
verb, and LG is designed to model full clauses,
the parser failed to produce a fully correct link-
age for any of the titles.
The performance for recovered interaction
subgraphs is more encouraging, as 25% of the
subgraphs were recovered in the first linkage and
more than half in the best linkage. Yet many in-
teraction subgraphs remain unrecovered by the
parser: the results suggest an upper limit of
approximately 60% to the fraction of protein-
protein interactions that can be recovered from
any linkage produced by the unmodified LG.
In the following sections we further analyze the
reasons why the parser fails to recover all de-
pendencies.
5.1 Panics
No fully correct linkages and very few interac-
tion subgraphs were found in the panic mode.
This effect may be partly due to the complex-
ity of the sentences for which the parser en-
17
Category
Criterion Linkage Stable Sampled Panic Overall
Dependency First linkage 3242 (80.0%) 1376 (74.3%) 569 (52.3%) 5187 (73.1%)
Best linkage 3601 (86.6%) 1576 (85.0%) 620 (57.0%) 5797 (81.7%)
Total 4157 1853 1088 7098
Fully correct First linkage 22 (10.4%) 0 (0.0%) 0 (0.0%) 22 (7.3%)
Best linkage 79 (37.4%) 6 (9.8%) 0 (0.0%) 85 (28.3%)
Total 211 61 28 300
Interaction First linkage 75 (30.5%) 16 (20.2%) 0 (0.0%) 91 (24.9%)
subgraph Best linkage 156 (63.4%) 49 (62.0%) 4 (9.8%) 209 (57.1%)
Total 246 79 41 366
Table 1: Parser performance. The fraction of fulfilled criteria is shown by category (the criteria and
categories are explained in Section 4). The total rows give the number of criteria for each category,
and the overall column gives combined results for all categories.
tered panic mode. The effect of panics can
be better estimated by forcing the parser to
bypass standard parsing and to directly apply
panic options. For the 272 sentences where the
parser did not enter the panic mode, 77% of
dependencies were recovered in the first link-
age. When these sentences were parsed in forced
panic mode, 67% of dependencies were recov-
ered, suggesting that on average parses in panic
mode recover approximately 10% fewer depen-
dencies than in standard parsing mode. Simi-
larly, the number of fully correct first linkages
decreased from 22 to 6 and the number of inter-
action subgraphs recovered in the first linkage
from 91 to 65. These numbers indicate that
panics are a significant cause of error.
Experiments indicate than on a 1GHz ma-
chine approximately 40% of sentences can be
fully parsed in under a second, 80% in under 10
seconds and 90% within 10 minutes; yet approx-
imately 5% of sentences take more than an hour
to fully parse. With tmax set to 10 minutes, the
total parsing time was 165 minutes.
Long parsing times are caused by ambiguous
sentences for which the parser creates thousands
or even millions of alternative linkages. In ad-
dition to simply increasing the time limit, the
fraction of sentences where the parser enters the
panic mode could therefore be reduced by re-
ducing the ambiguity of the sentences, for ex-
ample, by extending the dictionary of the parser
(see Section 7).
5.2 Heuristics
When several linkages are produced for a sen-
tence, the LG parser applies heuristics to or-
der the sentences so that linkages that are more
likely to be correct are presented first. The
heuristics are based on examination and intu-
itions on general English, and may not be opti-
mal for biomedical text. Note in Table 1 that
both for recovered full linkages and interaction
subgraphs, the number of items that were recov-
ered in the best linkage is more than twice the
number recovered in the first linkage, suggesting
that a better ordering heuristic could dramat-
ically improve the performance of the parser.
Such improvements could perhaps be achieved
by tuning the heuristics to the domain or by
adopting a probabilistic ordering model.
6 Failure analysis
A significant fraction of dependencies were not
recovered in any linkage, even in sentences
where resources were not exhausted. In order to
identify reasons for the parser failing to recover
the correct dependencies, we analyze sentences
for which it is certain that the grammar cannot
produce a fully correct linkage. We thus ana-
lyzed the 132 stable sentences for which some
dependencies were not recovered.
For each sentence, we attempt to identify the
reason for the failure of the parser. For each
identified reason, we manually edit the sentence
to remove the source of failure. We repeat this
procedure until the parser is capable of pro-
ducing a correct parse for the sentence. Note
that this implies that also the interaction sub-
graphs in the sentence are correctly recovered,
and therefore the reasons for failures to recover
interaction subgraphs are a subset of the iden-
tified issues. The results of the analysis are
18
Reason for failure Cases
Unknown grammatical structure 72 (34.4%)
Dictionary issue 54 (25.8%)
Unknown word handling 35 (16.7%)
Sentence fragment 27 (12.9%)
Ungrammatical sentence 17 (8.1%)
Other 4 (1.9%)
Table 2: Results of failure analysis
summarized in Table 2. In many of the sen-
tences, more than one reason for parser failure
was found; in total 209 issues were identified in
the 132 sentences. The results are described in
more detail in the following sections.
6.1 Fragments and ungrammatical
sentences
As some of the analyzed sentences were taken
from publication titles, not all of them were
full clauses. To identify further problems when
parsing fragments not containing a verb, the
phrase ?is explained? and required determiners
were added to these fragments, a technique used
also by Ding et al (2003). The completed frag-
ments were then analyzed for potential further
problems.
A number of other ungrammatical sentences
were also encountered. The most common
problem was the omission of determiners, but
some other issues such as missing possessive
markers and errors in agreement (e.g., ?expres-
sions. . . has?) were also encountered.
Ungrammatical sentences pose interesting
challenges for parsing. Because many authors
are not native English speakers, a greater toler-
ance for grammatical mistakes should allow the
parser to identify the intended parse for more
sentences. Similarly, the ability to parse publi-
cation titles would extend the applicability of
the parser; in some cases it may be possible
to extract information concerning the key find-
ings of a publication from the title. However,
while relaxing completeness and correctness re-
quirements, such as mandatory determiners and
subject-predicate agreement, would allow the
parser to create a complete linkage for more sen-
tences, it would also be expected to lead to in-
creased ambiguity for all sentences, and subse-
quent difficulties in identifying the correct link-
age. If the ability to parse titles is considered
important, a potential solution not incurring
this cost would be to develop a separate version
of the grammar for parsing titles.
capping protein and actin genes
capping protein and actin genes
Figure 2: Multiple modifier coordination prob-
lem. Above: correct linkage disallowed by the
LG parser. Below: solution by chaining modi-
fiers.
6.2 Unknown grammatical structures
The method of the LG implementation for pars-
ing coordinations was found to be a frequent
cause of failures. A specific coordination prob-
lem occurs with multiple noun-modifiers: the
parser assumes that coordinated constituents
can be connected to the rest of the sentence
through exactly one word, and the grammar at-
taches all noun-modifiers to the head. Biomed-
ical texts frequently contain phrases that cause
these requirements to conflict: for example, in
the phrase ?capping protein and actin genes?
(where ?capping protein genes? and ?actin
genes? is the intended parse), the parser allows
only one of the words ?capping? and ?protein?
to connect to the word ?genes?, and is thus un-
able to produce the correct linkage (for illustra-
tion, see Figure 2(a)).
This multiple modifier coordination issue
could be addressed by modifying the grammar
to chain modifiers (Figure 2(b)). This alterna-
tive model is adopted by another major depen-
dency grammar, the EngCG-based Connexor
Machinese. The problem could also be ad-
dressed by altering the coordination handling
system in the parser.
Other identified grammatical structures not
known to the parser were number postmodifiers
to nouns (e.g., ?serine 38?), specifiers in paren-
theses (e.g., ?profilin mutant (H119E)?), coor-
dination with the phrase ?but not?, and various
unknown uses of colons and quotes. Single in-
stances of several distinct unknown grammati-
cal structures were also noted (e.g., ?5 to 10?,
?as expected from?, ?most concentrated in?).
Most of these issues can be addressed by local
modifications to the grammar.
6.3 Unknown word handling
The LG parser assigns unknown words to cate-
gories based on morphological or other surface
clues when possible. For remaining unknown
19
words, parses are attempted by assigning the
words to the generic noun, verb and adjective
types in all possible combinations.
Some problems with the unknown word pro-
cessing method were encountered during analy-
sis; for example, the assumption that unknown
capitalized words are proper nouns often caused
failures, especially in sentences beginning with
an unknown word. Similarly, the assumption
that words containing a hyphen behave as ad-
jectives was violated by a number of unknown
verbs (e.g., ?cross-links?).
Another problem that was noted occurred
with lowercase unknown words that should be
treated as proper nouns: because LG does not
allow unknown lowercase words to act as proper
nouns, the parser assigns incorrect structure to
a number of phrases containing words such as
?actin?. Improving unknown word handling re-
quires some modifications to the LG parser.
6.4 Dictionary issues
Cases where the LG dictionary contains a word,
but not in the sense in which it appears in a
sentence, almost always lead to errors. For ex-
ample, the LG dictionary does not contain the
word ?assembly? in the sense ?construction?,
causing the parser to erroneously require a de-
terminer for ?protein assembly?4. A related
frequent problem occurred with proper names
headed by a common noun, where the parser ex-
pects a determiner for such names (e.g., ?myosin
heavy chain?), and fails when one is not present.
These issues are mostly straightforward to ad-
dress in the grammar, but difficult to identify
automatically.
6.5 Biomedical entity names
Many of the causes for parser failure discussed
above are related to the presence of biomed-
ical entity names. While the causes for fail-
ures relating to names can be addressed in the
grammar, the existence of biomedical named en-
tity (NE) recognition systems (for a recent sur-
vey, see, e.g., Bunescu et al (2004)) suggests
an alternative solution: NEs could be identified
in preprocessing, and treated as single (proper
noun) tokens during the parse. During failure
analysis, 59 cases (28% of all cases) were noted
where this procedure would have eliminated the
error, assuming that no errors are made in NE
430 distinct problematic word definitions were iden-
tified, including ?breakdown?, ?composed?, ?factor?,
?half?, ?independent?, ?localized?, ?parallel?, ?pro-
moter?, ?segment?, ?upstream? and ?via?.
recognition. However, the performance of cur-
rent NE recognition systems is not perfect, and
it is not clear what the effect of adopting such
a method would be on parser performance.
7 Dictionary extension
Szolovits (2003) describes an automatic method
for mapping lexical information from one lexi-
con to another, and applies this method to aug-
ment the LG dictionary with terms from the ex-
tensive UMLS Specialist lexicon. The extension
introduces more than 125,000 new words into
the LG dictionary, more than tripling its size.
We evaluated the effect of this dictionary exten-
sion on LG parser performance using the criteria
described above. The fraction of distinct tokens
in the corpus found in the parser dictionary in-
creased from 52% to 72% with the dictionary
extension, representing a significant reduction
in uncertainty. This decrease was coupled with
a 32% reduction in total parsing time.
Because the LG parser is unable to produce
any linkage for sentences where it cannot iden-
tify a verb (even incorrectly), extending the dic-
tionary significantly reduced the ability of LG
to extract dependencies in titles, where the frac-
tion of recovered dependencies fell from the al-
ready low value of 67% to 55%.
For the sentences excluding titles, the benefits
of the dictionary extension were most significant
for sentences that were in the panic category
when using the unextended LG dictionary; 12
of these 28 sentences could be parsed without
panic with the dictionary extension. In the first
linkage of these sentences, the fraction of re-
covered dependencies increased by 8%, and the
fraction of recovered interaction subgraphs in-
creased from zero to 15% with the dictionary
extension.
The overall effect of the dictionary extension
was positive but modest, with no more than
2.5% improvement for either the first or best
linkages for any criterion, despite the threefold
increase in dictionary size. This result agrees
with the failure analysis: most problems can-
not be removed by extending the dictionary and
must instead be addressed by modifications of
the grammar or parser.
8 Conclusion
We have presented an analysis of Link Gram-
mar performance using a custom dependency
corpus targeted at protein-protein interactions.
We introduced the concept of the interaction
20
subgraph and reported parser performance for
three criteria: recovery of dependencies, in-
teraction subgraphs and fully correct linkages.
While LG was able to recover 73% of dependen-
cies in the first linkage, only 7% of sentences had
a fully correct first linkage. However, fully cor-
rect linkages are not required for information ex-
traction, and we found that 25% of interaction
subgraphs were recovered in the first linkage.
Resource exhaustion was found to be a signif-
icant cause of poor performance. Furthermore,
an evaluation of performance in the case when
optimal heuristics for ordering linkages are ap-
plied indicated that the fraction of recovered in-
teraction subgraphs could be more than doubled
(to 57%) by optimal heuristics.
To further analyze the cases where the parser
cannot produce a correct linkage, we carefully
examined the sentences and were able to iden-
tify five problem types. For each identified
case, we discussed potential modifications for
addressing the problems. We also considered
the possibility of using a named entity recogni-
tion system to improve parser performance and
found that 28% of LG failures would be avoided
by a flawless named entity recognition system.
We evaluated the effect of the dictionary ex-
tension proposed by Szolovits (2003), and found
that while it significantly reduced ambiguity
and improved performance for the most ambigu-
ous sentences, overall improvement was only
2.5%. This indicates that extending the dic-
tionary is not sufficient to address the perfor-
mance problems and that modifications to the
grammar and parser are necessary.
The quantitative analysis of LG performance
confirms that, in its current state, LG is not well
suited to the IE task discussed. However, in the
failure analysis we have identified a number of
specific issues and problematic areas for LG in
parsing biomedical publications, and suggested
improvements for adapting the parser to this
domain. The examination and implementation
of these improvements is a natural follow-up of
this study. Our initial experiments suggest that
it is indeed possible to implement general so-
lutions to many of the discussed problems, and
such modifications would be expected to lead to
improved applicability of LG to the biomedical
domain.
9 Acknowledgments
This work has been supported by Tekes, the
Finnish National Technology Agency.
References
Razvan Bunescu, Ruifang Ge, Rohit J. Kate,
Edward M. Marcotte, Raymond J. Mooney,
Arun Kumar Ramani, and Yuk Wah Wong. 2004
(to appear). Comparative experiments on learn-
ing information extractors for proteins and their
interactions. Artificial Intelligence in Medicine.
Special Issue on Summarization and Information
Extraction from Medical Documents.
Michael Collins, Jan Hajic, Lance Ramshaw, and
Christoph Tillmann. 1999. A statistical parser
for Czech. In 37th Annual Meeting of the Associ-
ation for Computational Linguistics, pages 505?
512. Association for Computational Linguistics,
Somerset, New Jersey.
Mark Craven and Johan Kumlien. 1999. Construct-
ing biological knowledge bases by extracting in-
formation from text sources. In T. Lengauer,
R. Schneider, P. Bork, D. Brutlag, J. Glasgow,
H HW Mewes, and Zimmer R., editors, Proceed-
ings of the 7th International Conference on Intel-
ligent Systems in Molecular Biology, pages 77?86.
AAAI Press, Menlo Park, CA.
Nikolai Daraselia, Anton Yuryev, Sergei Egorov,
Svetalana Novichkova, Alexander Nikitin, and
Ilya Mazo. 2004. Extracting human protein in-
teractions from MEDLINE using a full-sentence
parser. Bioinformatics, 20(5):604?611.
Jing Ding, Daniel Berleant, Jun Xu, and Andy W.
Fulmer. 2003. Extracting biochemical interac-
tions from medline using a link grammar parser.
In B. Werner, editor, Proceedings of the 15th
IEEE International Conference on Tools with Ar-
tificial Intelligence, pages 467?471. IEEE Com-
puter Society, Los Alamitos, CA.
Filip Ginter, Tapio Pahikkala, Sampo Pyysalo,
Jorma Boberg, Jouni Ja?rvinen, and Tapio
Salakoski. 2004. Extracting protein-protein inter-
action sentences by applying rough set data anal-
ysis. In S. Tsumoto, R. Slowinski, J. Komorowski,
and J.W. Grzymala-Busse, editors, Lecture Notes
in Computer Science 3066. Springer, Heidelberg.
Daniel D. Sleator and Davy Temperley. 1991. Pars-
ing english with a link grammar. Technical Re-
port CMU-CS-91-196, Department of Computer
Science, Carnegie Mellon University, Pittsburgh,
PA.
Peter Szolovits. 2003. Adding a medical lexicon to
an english parser. In Mark Musen, editor, Pro-
ceedings of the 2003 AMIA Annual Symposium,
pages 639?643. American Medical Informatics As-
sociation, Bethesda, MD.
Joshua M. Temkin and Mark R. Gilder. 2003. Ex-
traction of protein interaction information from
unstructured text using a context-free grammar.
Bioinformatics, 19(16):2046?2053.
21
BioNLP 2008: Current Trends in Biomedical Natural Language Processing, pages 1?9,
Columbus, Ohio, USA, June 2008. c?2008 Association for Computational Linguistics
A Graph Kernel for Protein-Protein Interaction Extraction
Antti Airola, Sampo Pyysalo, Jari Bjo?rne, Tapio Pahikkala, Filip Ginter and Tapio Salakoski
Turku Centre for Computer Science
and Department of IT, University of Turku
Joukahaisenkatu 3-5
20520 Turku, Finland
firstname.lastname@utu.fi
Abstract
In this paper, we propose a graph kernel
based approach for the automated extraction
of protein-protein interactions (PPI) from sci-
entific literature. In contrast to earlier ap-
proaches to PPI extraction, the introduced all-
dependency-paths kernel has the capability
to consider full, general dependency graphs.
We evaluate the proposed method across five
publicly available PPI corpora providing the
most comprehensive evaluation done for a ma-
chine learning based PPI-extraction system.
Our method is shown to achieve state-of-the-
art performance with respect to comparable
evaluations, achieving 56.4 F-score and 84.8
AUC on the AImed corpus. Further, we iden-
tify several pitfalls that can make evaluations
of PPI-extraction systems incomparable, or
even invalid. These include incorrect cross-
validation strategies and problems related to
comparing F-score results achieved on differ-
ent evaluation resources.
1 Introduction
Automated protein-protein interaction (PPI) extrac-
tion from scientific literature is a task of significant
interest in the BioNLP field. The most commonly
addressed problem has been the extraction of binary
interactions, where the system identifies which pro-
tein pairs in a sentence have a biologically relevant
relationship between them. Proposed solutions in-
clude both hand-crafted rule-based systems and ma-
chine learning approaches (see e.g. (Bunescu et al,
2005)). A wide range of results have been reported
for the systems, but as we will show, differences in
evaluation resources, metrics and strategies make di-
rect comparison of these numbers problematic. Fur-
ther, the results gained from the BioCreative II eval-
uation, where the best performing system achieved
a 29% F-score (Hunter et al, 2008), suggest that the
problem of extracting binary protein protein interac-
tions is far from solved.
The public availability of large annotated PPI-
corpora such as AImed (Bunescu et al, 2005),
BioInfer (Pyysalo et al, 2007a) and GENIA (Kim
et al, 2008), provides an opportunity for building
PPI extraction systems automatically using machine
learning. A major challenge is how to supply the
learner with the contextual and syntactic informa-
tion needed to distinguish between interactions and
non-interactions. To address the ambiguity and vari-
ability of the natural language expressions used to
state PPI, several recent studies have focused on
the development, adaptation and application of NLP
tools for the biomedical domain. Many high-quality
domain-specific tools are now freely available, in-
cluding full parsers such as that introduced by Char-
niak and Lease (2005). Additionally, a number
of conversions from phrase structure parses to de-
pendency structures that make the relationships be-
tween words more directly accessible have been in-
troduced. These include conversions into represen-
tations such as the Stanford dependency scheme (de
Marneffe et al, 2006) that are explicitly designed for
information extraction purposes. However, special-
ized feature representations and kernels are required
to make learning from such structures possible.
Approaches such as subsequence kernels
(Bunescu and Mooney, 2006), tree kernels (Zelenko
1
interaction of P1 and P2
prep_of> conj_and>prep_of>
P1 is a P2 binding protein
<nn<nn
<det<cop
<nsubj
P1 fails to bind P2
<nsubj <aux dobj>xcomp>
<xsubj
Figure 1: Stanford dependency parses (?collapsed? rep-
resentation) where the shortest path, shown in bold, ex-
cludes important words.
et al, 2003) and shortest path kernels (Bunescu
and Mooney, 2005) have been proposed and suc-
cessfully used for relation extraction. However,
these methods lack the expressive power to consider
representations derived from general, possibly
cyclic, dependency graph structures, such as those
generated by the Stanford tools. The subsequence
kernel approach does not consider parses at all, and
the shortest path approach is limited to representing
only a single path in the full dependency graph,
which excludes relevant words even in many simple
cases (Figure 1). Tree kernels can represent more
complex structures, but are still restricted to tree
representations.
Lately, in the framework of kernel-based machine
learning methods there has been an increased in-
terest in designing kernel functions for graph data.
Building on the work of Ga?rtner et al (2003),
graph representations tailored for the task of depen-
dency parse ranking were proposed by Pahikkala et
al. (2006b). Though the proposed representations
are not directly applicable to the task of PPI extrac-
tion, they offer insight in how to learn from depen-
dency graphs. We develop a graph kernel approach
for PPI extraction based on these ideas.
We next define a graph representation suitable for
describing potential interactions and introduce a ker-
nel which makes efficient learning from a general,
unrestricted graph representation possible. Then we
provide a short description of the sparse regular-
ized least squares (sparse RLS) kernel-based ma-
chine learning method we use for PPI-extraction.
Further, we rigorously assess our method on five
publicly available PPI corpora, providing the first
broad cross-corpus evaluation with a machine learn-
ing approach to PPI extraction. Finally, we discuss
the effects that different evaluation strategies, choice
of corpus and applied metrics have on measured per-
formance, and conclude.
2 Method
We next present our graph representation, formalize
the notion of graph kernels, and present our learning
method of choice, the sparse RLS.
2.1 Graph encoding of sentence structure
As in most recent work on machine learning for PPI
extraction, we cast the task as learning a decision
function that determines for each unordered candi-
date pair of protein names occurring together in a
sentence whether the two proteins interact. In the
following, we first define the graph representation
used to represent an interaction candidate pair. We
then proceed to derive the kernel used to measure
the similarities of these graphs.
We assume that the input of our learning method
is a dependency parse of a sentence where a pair of
protein names is marked as the candidate interac-
tion for which an extraction decision must be made.
Based on this, we form a weighted, directed graph
that consists of two unconnected subgraphs. One
represents the dependency structure of the sentence,
and the other the linear order of the words (see Fig-
ure 2).
The first subgraph is built from the dependency
analysis. One vertex and an associated set of labels
is created in the graph for each token and for each
dependency. The vertices that represent tokens have
as labels the text and part-of-speech (POS) of the
token. To ensure generalization of the learned ex-
traction model, the labels of vertices that correspond
to protein names are replaced with PROT1, PROT2
or PROT, where PROT1 and PROT2 are the pair of
interest. The vertices that represent dependencies
are labeled with the type of the dependency. The
edges in the subgraph are defined so that each de-
pendency vertex is connected by an incoming edge
from the vertex representing its governor token, and
by an outgoing edge to the vertex representing its de-
2
Figure 2: Graph representation generated from an example sentence. The candidate interaction pair is marked as
PROT1 and PROT2, the third protein is marked as PROT. The shortest path between the proteins is shown in bold. In
the dependency based subgraph all nodes in a shortest path are specialized using a post-tag (IP). In the linear order
subgraph possible tags are (B)efore, (M)iddle, and (A)fter. For the other two candidate pairs in the sentence, graphs
with the same structure but different weights and labels would be generated.
pendent token. The graph thus represents the entire
sentence structure.
It is widely acknowledged that the words between
the candidate entities or connecting them in a syn-
tactic representation are particularly likely to carry
information regarding their relationship; (Bunescu
and Mooney, 2005) formalize this intuition for de-
pendency graphs as the shortest path hypothesis. We
apply this insight in two ways in the graph repre-
sentation: the labels of the nodes on the shortest
undirected paths connecting PROT1 and PROT2 are
differentiated from the labels outside the paths us-
ing a special tag. Further, the edges are assigned
weights; after limited preliminary experiments, we
chose a simple weighting scheme where all edges
on the shortest paths receive a weight of 0.9 and
other edges receive a weight of 0.3. The represen-
tation thus allows us to emphasize the shortest path
without completely disregarding potentially relevant
words outside of the path.
The second subgraph is built from the linear struc-
ture of the sentence. For each token, a second ver-
tex is created and the labels for the vertices are de-
rived from the texts, POS-tags and named entity tag-
ging as above. The labels of each word are special-
ized to denote whether the word appears before, in-
between, or after the protein pair of interest. Each
word node is connected by an edge to its succeed-
ing word, as determined by sentence order the of the
words. Each edge is given the weight 0.9.
2.2 The all-dependency-paths graph kernel
We next formalize the graph representation and
present the all-dependency-paths kernel. This ker-
nel can be considered as a practical instantiation of
the theoretical graph kernel framework introduced
by Ga?rtner et al (2003). Let V be the set of ver-
tices in the graph and L be the set of possible labels
vertices can have. We represent the graph with an
adjacency matrix A ? R|V |?|V |, whose rows and
columns are indexed by the vertices, and [A]i,j con-
tains the weight of the edge connecting vi ? V and
vj ? V if such an edge exists, and zero otherwise.
Further, we represent the labels as a label allocation
matrix L ? R|L|?|V | so that Li,j = 1 if the j-th
vertex has the i-th label and Li,j = 0 otherwise. Be-
cause only a very small fraction of all the possible
labels are ever assigned to any single node, this ma-
trix is extremely sparse.
It is well known that when an adjacency matrix is
multiplied with itself, each element [A2]i,j contains
the summed weight of paths from vertex vi to vertex
vj through one intervening vertex, that is, paths of
length two. Similarly, for any length n, the summed
weights from vi to vj can be determined by calculat-
ing [An]i,j .
Since we are interested not only in paths of one
specific length, it is natural to combine the effect of
paths of different lengths by summing the powers
of the adjacency matrices. We calculate the infinite
sum of the weights of all possible paths connecting
3
the vertices using the Neumann Series, defined as
(I ?A)?1 = I + A + A2 + ... =
??
k=0
Ak
if |A| < 1 where |A| is the spectral radius of A
(Meyer, 2000). From this sum we can form a new
adjacency matrix
W = (I ?A)?1 ? I .
The final adjacency matrix contains the summed
weights of all possible paths connecting the ver-
tices. The identity matrix is subtracted to remove
the paths of length zero, which would correspond to
self-loops.
Next, we present the graph kernel that utilizes the
graph representation defined previously. We define
an instance G representing a candidate interaction
as G = LWLT, where L and W are the label al-
location matrix and the final adjacency matrix cor-
responding to the graph representation of the candi-
date interaction.
Following Ga?rtner et al (2003) the graph kernel
is defined as
k(G?, G??) =
|L|?
i=1
|L|?
j=1
G?i,jG
??
i,j ,
where G? and G?? are two instances formed as de-
fined previously. The features can be thought as
combinations of labels from connected pairs of ver-
tices, with a value that represents the strength of
their connection. In practical implementations, the
full G matrices, which consist mostly of zeroes, are
never explicitly formed. Rather, only the non-zero
elements are stored in memory and used when cal-
culating the kernels.
2.3 Scalable learning with Sparse RLS
RLS is a state-of-the-art kernel-based machine
learning method which has been shown to have
comparable performance to support vector machines
(Rifkin et al, 2003). We choose the sparse version
of the algorithm, also known as subset of regressors,
as it allows us to scale up the method to very large
training set sizes. Sparse RLS also has the property
that it is possible to perform cross-validation and
regularization parameter selection so that their time
complexities are negligible compared to the training
complexity. These efficient methods are analogous
to the ones proposed by Pahikkala et al (2006a) for
the basic RLS regression.
We now briefly present the basic sparse RLS al-
gorithm. Let m denote the training set size and
M = {1, . . . ,m} an index set in which the indices
refer to the examples in the training set. Instead of
allowing functions that can be expressed as a linear
combination over the whole training set, as in the
case of basic RLS regression, we only allow func-
tions of the following restricted type:
f(?) =
?
i?B
aik(?, xi), (1)
where k is the kernel function, xi are training data
points, ai ? R are weights, and the set indexing the
basis vectors B ? M is selected in advance. The co-
efficients ai that determine (1) are obtained by min-
imizing
m?
i=1
(yi ?
?
j?B
ajk(xi, xj))
2 + ?
?
i,j?B
aiajk(xi, xj),
where the first term is the squared loss function, the
second term is the regularizer, and ? ? R+ is a reg-
ularization parameter. Note that all the training in-
stances are used for determining the coefficient vec-
tor. The minimizer is obtained by solving the corre-
sponding system of linear equations, which can be
performed in O(m|B|2) time.
We set the maximum number of basis vectors to
4000 in all experiments in this study. The subset
is selected randomly when the training set size ex-
ceeds this number. Other methods for the selection
of the basis vectors were considered by Rifkin et
al. (2003), who however reported that the random
selection worked as well as the more sophisticated
approaches.
3 Experimental evaluation
We next describe the evaluation resources and met-
rics used, provide a comprehensive evaluation of our
method across five PPI corpora, and compare our re-
sults to earlier work. Further, we discuss the chal-
lenges inherent in providing a valid method evalua-
tion and propose solutions.
4
Statistics Graph Kernel Co-occ.
Corpus #POS. #NEG. P R F ?F AUC ?AUC P F
AIMed 1000 4834 0.529 0.618 0.564 0.050 0.848 0.023 0.178 0.301
BioInfer 1370 8924 0.477 0.599 0.529 0.053 0.849 0.065 0.135 0.237
HPRD50 163 270 0.643 0.658 0.634 0.114 0.797 0.063 0.389 0.554
IEPA 335 482 0.696 0.827 0.751 0.070 0.851 0.051 0.408 0.576
LLL 164 166 0.725 0.872 0.768 0.178 0.834 0.122 0.559 0.703
Table 1: Counts of positive and negative examples in the corpora and (P)recision, (R)ecall (F)-score and AUC for the
graph kernel, with standard deviations provided for F and AUC.
3.1 Corpora and evaluation criteria
We evaluate our method using five publicly avail-
able corpora that contain PPI interaction annotation:
AImed (Bunescu et al, 2005), BioInfer (Pyysalo et
al., 2007a), HPRD50 (Fundel et al, 2007), IEPA
(Ding et al, 2002) and LLL (Ne?dellec, 2005). All
the corpora were processed to a common format us-
ing transformations1 that we have introduced ear-
lier (Pyysalo et al, 2008). We parse these cor-
pora with the Charniak-Lease parser (Charniak and
Lease, 2005), which has been found to perform best
among a number of parsers tested in recent domain
evaluations (Clegg and Shepherd, 2007; Pyysalo et
al., 2007b). The Charniak-Lease phrase structure
parses are transformed into the collapsed Stanford
dependency scheme using the Stanford tools (de
Marneffe et al, 2006). We cast the PPI extraction
task as binary classification, where protein pairs that
are stated to interact are positive examples and other
co-occuring pairs negative. Thus, from each sen-
tence,
(n
2
)
examples are generated, where n is the
number of occurrences of protein names in the sen-
tence. Finally, we form the graph representation de-
scribed earlier for each candidate interaction.
We evaluate the method with 10-fold document-
level cross-validation on all of the corpora. This
guarantees the maximal use of the available data,
and also allows comparison to relevant earlier work.
In particular, on the AImed corpus we apply the ex-
act same 10-fold split that was used by Bunescu et
al. (2006) and Giuliano et al (2006). Performance
is measured according to the following criteria: in-
teractions are considered untyped, undirected pair-
wise relations between specific protein mentions,
that is, if the same protein name occurs multiple
1Available at http://mars.cs.utu.fi/PPICorpora.
times in a sentence, the correct interactions must be
extracted for each occurrence. Further, we do not
consider self-interactions as candidates and remove
them from the corpora prior to evaluation.
The majority of PPI extraction system evaluations
use the balanced F-score measure for quantifying the
performance of the systems. This metric is defined
as F = 2prp+r , where p is precision and r recall. Like-
wise, we provide F-score, precision, and recall val-
ues in our evaluation. It should be noted that F-score
is very sensitive to the underlying positive/negative
pair distribution of the corpus ? a property whose
impact on evaluation is discussed in detail below. As
an alternative to F-score, we also evaluate the per-
formance of our system using the area under the re-
ceiver operating characteristics curve (AUC) mea-
sure (Hanley and McNeil, 1982). AUC has the im-
portant property that it is invariant to the class dis-
tribution of the used dataset. Due to this and other
beneficial properties for comparative evaluation, the
usage of AUC for performance evaluation has been
recently advocated in the machine learning commu-
nity (see e.g. (Bradley, 1997)). Formally, AUC can
be defined as
AUC =
?m+
i=1
?m?
j=1H(xi ? yi)
m+m?
,
where m+ and m? are the numbers of positive
and negative examples, respectively, and x1,...,xm+
are the outputs of the system for the positive, and
y1,...,ym? for the negative examples, and
H(r) =
?
?
?
1, if r > 0
0.5, if r = 0
0, otherwise.
The measure corresponds to the probability that
given a randomly chosen positive and negative ex-
5
ample, the system will be able to correctly disin-
guish which one is which.
3.2 Performance across corpora
The performance of our method on the five corpora
for the various metrics is presented in Table 1. For
reference, we show also the performance of the co-
occurrence (or all-true) baseline, which simply as-
signs each candidate into the interaction class. The
recall of the co-occurrence method is trivially 100%,
and in terms of AUC it has a score of 0.5, the ran-
dom baseline. All the numbers in Table 1 are aver-
ages taken over the ten folds. One should note that
because of the non-linearity of the F-score measure,
the average precision and recall will not produce ex-
actly the average F.
The results hold several interesting findings. First,
we briefly observe that on the AImed corpus, which
has recently been applied in numerous evaluations
(S?tre et al, 2008) and can be seen as an emerging
de facto standard for PPI extraction method evalua-
tion, the method achieves an F-score performance of
56.4%. As we argue in more detail below, this level
of performance is comparable to the state-of-the-art
in machine learning based PPI extraction. For the
other large corpus, BioInfer, F-score performance is
slightly lower.
Second, we observe that the F-score performance
of the method varies strikingly between the differ-
ent corpora, with results on IEPA and LLL approx-
imately 20 percentage units higher than on AImed
and BioInfer, despite the larger size of the latter two.
In our previous work we have observed similar re-
sults with a rule-based extraction method (Pyysalo et
al., 2008). As the first broad cross-corpus evaluation
using a state-of-the-art machine learning method for
PPI extraction, our results support and extend the
key finding that F-score performance results mea-
sured on different corpora cannot, in general, be
meaningfully compared.
The co-occurrence baseline numbers indicate one
reason for the high F-score variance between the
corpora. The F-score metric is not invariant to the
distribution of positive and negative examples: for
example, halving the number of negative test exam-
ples is expected to approximately halve the number
of false positives at a given recall point. Thus, the
greater the fraction of true interactions in a corpus
is, the easier it is to reach high performance in terms
of F-score. This is reflected in co-occurrence re-
sults, which range from 24% to 70% depending on
the class distribution of the corpus.
This is a critical weakness of the F-score metric in
cross-corpus comparisons as, for example, the frac-
tion of true interactions out of all candidates is 50%
on the LLL corpus but only 17% on AImed. By
contrast to the large differences in performance mea-
sured using F-score, we find that for the distribution-
invariant AUC measure the performance for all of
the AImed, BioInfer, IEPA, and LLL corpora falls in
the narrow range of 83-85%. In terms of AUC, per-
formance on the HPRD50 corpus is an outlier, being
approximately three percentage units lower than for
any other corpus. Nevertheless, the results provide a
strong argument in favor of applying the AUC met-
ric instead of, or in addition to, F-score. AUC is also
more stable in terms of variance.
Finally, we note that the similar performance in
terms of AUC for corpora with as widely differing
sizes as LLL and BioInfer indicates that past a rel-
atively modest number of examples, increasing cor-
pus size has a surprisingly small effect on the perfor-
mance of the method. A similar finding can be seen,
for example, in the relatively flat learning curve of
Giuliano et al (2006). While the issue requires fur-
ther investigation, these results suggest that there
may be more value in investing effort in develop-
ing better learning methods as opposed to larger cor-
pora.
3.3 Performance compared to other methods
We next discuss the performance of our method
compared to other methods introduced in the liter-
ature and the challenges of meaningful comparison,
where we identify three major issues.
First, as indicated by the results above, differ-
ences in the makeup of different corpora render
cross-corpus comparisons in terms of F-score es-
sentially meaningless. As F-score is typically the
only metric for which results are reported in the PPI
extraction literature, we are limited to comparing
against results on single corpora. We consider the
AImed and BioInfer evaluations to be the most rele-
vant ones, as these corpora are sufficiently large for
training and reliably testing machine learning meth-
ods. As the present study is, to the best of our knowl-
6
P R F
(Giuliano et al, 2006) 60.9% 57.2% 59.0%
All-dependency-paths graph kernel 52.9% 61.8% 56.4%
(Bunescu and Mooney, 2006) 65.0% 46.4% 54.2%
(S?tre et al, 2008) 64.3% 44.1% 52.0%
(Mitsumori et al, 2006) 54.2% 42.6% 47.7%
(Yakushiji et al, 2005) 33.7% 33.1% 33.4%
Table 2: (P)recision, (R)ecall and (F)-score results for methods evaluated on AImed with the correct cross-validation
methodology.
edge, the first to report machine learning method
performance on BioInfer, we will focus on AImed
in the following comparison.
Second, the cross-validation strategy used in eval-
uation has a large impact on measured performance.
In earlier system evaluations, two major strategies
for defining the splits used in cross-validation can
be observed. The approach used by Bunescu and
Mooney (2006), which we consider the correct one,
is to split the data into folds on level of docu-
ments. This guarantees that all pairs generated from
the same document are always either in the train-
ing set or in the test set. Another approach is to
pool all the generated pairs together, and then ran-
domly split them to folds. To illustrate the signifi-
cance of this choice, consider two interaction candi-
dates extracted from the same sentence, e.g. from
a statement of the form ?P1 and P2 [. . . ] P3?,
where ?[. . . ]? is any statement of interaction or non-
interaction. Due to the near-identity of contexts, a
machine learning method will easily learn to predict
that the label of the pair (P1, P2) should match that
of (P1, P3). However, such ?learning? will clearly
not generalize. This approach must thus be consid-
ered invalid, because allowing pairs generated from
same sentences to appear in different folds leads to
an information leak between the training and test
sets. S?tre et al (2008) observed that adopting the
latter cross-validation strategy on AImed could lead
up to 18 F-score percentage unit overestimation of
performance. For this reason, we will not consider
results listed in the ?False 10-fold cross-validation?
table (2b) of S?tre et al (2008).
With these restrictions in place, we now turn to
comparison with relevant results reported in related
research, summarized in Table 2. We note that
Bunescu and Mooney (2006) only applied evalua-
tion criteria where it is enough to extract only one
occurrence of each mention of an interaction from
each abstract, while the other results shown were
evaluated using the same criteria as applied here.
The former approach can produce higher perfor-
mance: the evaluation of Giuliano et al (2006) in-
cludes both alternatives, and their method achieves
an F-score of 63.9% under the former criterion,
which they term One Answer per Relation in a
given Document (OARD). Our method outperforms
most studies using similar evaluation methodology,
with the exception being the approach of Giuliano
et al (2006). This result is somewhat surprising,
as the method proposed by Giuliano does not ap-
ply any form of parsing but relies instead only on
the sequential order of the words. This brings us
to our third point regarding comparability of meth-
ods. As pointed out by S?tre et al (2008), the
AImed corpus allows remarkably different ?inter-
pretations? regarding the number of interacting and
non-interacting pairs. For example, where we have
identified 1000 interacting and 4834 non-interacting
protein pairs in AImed, in the data used by Giuliano
there are eight more interacting and 200 fewer non-
interacting pairs. The corpus can also be prepro-
cessed in a number of ways. In particular we noticed
that whereas protein names are always blinded in our
data, in the data used by Giuliano protein names are
sometimes partly left visible. As Giuliano has gen-
erously made his method implementation available2,
we were able to test the performance of his system
on the data we used in our experiments. This re-
sulted in an F-score of 52.4%.
Finally, there remains an issue of parameter se-
lection. For sparse RLS the values of the regular-
2Available at http://tcc.itc.it/research/
textec/tools-resources/jsre.html.
7
ization parameter ? and the decision threshold sep-
arating the positive and negative classes must be
chosen, which can be problematic when no sepa-
rate data for choosing them is available. Choos-
ing from several parameter values the ones that give
best results in testing, or picking the best point
from a precision/recall curve when evaluating in
terms of F-score, will lead to an overoptimistic eval-
uation of performance. This issue has often not
been addressed in earlier evaluations that do cross-
validation on a whole corpus. We choose the pa-
rameters by doing further leave-one-document-out
cross-validation within each round of 10-fold-cross-
validation, on the nine folds that constitute the train-
ing set.
As a conclusion, we observe the results achieved
with the all-dependency-paths kernel to be state-of-
the-art level. However, differences in evaluation
strategies and the large variance exhibited in the re-
sults make it impossible to state which of the sys-
tems considered can be expected in general to per-
form best. We encourage future PPI-system evalua-
tions to report AUC and F-score results over mul-
tiple corpora, following clearly defined evaluation
strategies, to bring further clarity to this issue.
4 Conclusions and future work
In this paper we have proposed a graph kernel
approach to extracting protein-protein interactions,
which captures the information in unrestricted de-
pendency graphs to a format that kernel based learn-
ing algorithms can process. The method combines
syntactic analysis with a representation of the lin-
ear order of the sentence, and considers all possi-
ble paths connecting any two vertices in the result-
ing graph. We demonstrate state-of-the art perfor-
mance for the approach. All software developed in
the course of this study is made publicly available at
http://mars.cs.utu.fi/PPICorpora.
We identify a number of issues which make re-
sults achieved with different evaluation strategies
and resources incomparable, or even incorrect. In
our experimental design we consider the problems
related to differences across corpora, the effects dif-
ferent cross-validation strategies have, and how pa-
rameter selection can be done. Our recommendation
is to provide evaluations over different corpora, to
use document-level cross-validation and to always
selected parameters on the training set.
We draw attention to the behaviour of the F-score
metric over corpora with differing pair distributions.
The higher the relative frequency of interacting pairs
is, the higher the performance can be expected to
be. This is noticed both for the graph kernel method
and for the naive co-occurrence baseline. Indeed,
the strategy of just stating that all pairs interact leads
to as high result as 70% F-score on one of the cor-
pora. We consider AUC as an alternative measure
that does not exhibit such behaviour, as it is invari-
ant to the distribution of pairs. The AUC metric is
much more stable across all the corpora, and never
gives better results than random for approaches such
as the naive co-occurrence.
Though we only consider binary interactions in
this work, the graph representations have the prop-
erty that they could be used to represent more com-
plex structures than pairs. The availability of cor-
pora that annotate complex interactions, such as the
full BioInfer and GENIA, makes training a PPI ex-
traction system for extracting complex interactions
an important avenue of future research. However,
how to avoid the combinatorial explosion following
from considering triplets, quartets etc. remains an
open question. Also, the performance of the cur-
rent approaches may need to be yet improved before
extending them to recognize complex interactions.
Acknowledgements
We would like to thank Razvan Bunescu, Claudio
Giuliano and Rune S?tre for their generous assis-
tance in providing us with data, software and infor-
mation about their work on PPI extraction. Further,
we thank CSC, the Finnish IT center for science,
for providing us extensive computational resources.
This work has been supported by the Academy of
Finland and the Finnish Funding Agency for Tech-
nology and Innovation, Tekes.
References
Andrew P. Bradley. 1997. The use of the area under
the ROC curve in the evaluation of machine learning
algorithms. Pattern Recognition, 30(7):1145?1159.
Razvan Bunescu and Raymond Mooney. 2005. A short-
est path dependency kernel for relation extraction. In
Proceedings of HLT/EMNLP?05, pages 724?731.
8
Razvan Bunescu and Raymond Mooney. 2006. Subse-
quence kernels for relation extraction. In Proceedings
of NIPS?05, pages 171?178. MIT Press.
Razvan C. Bunescu, Ruifang Ge, Rohit J. Kate, Ed-
ward M. Marcotte, Raymond J. Mooney, Arun Ku-
mar Ramani, and Yuk Wah Wong. 2005. Compar-
ative experiments on learning information extractors
for proteins and their interactions. Artif Intell Med,
33(2):139?155.
Eugene Charniak and Matthew Lease. 2005. Parsing
biomedical literature. In Proceedings of IJCNLP?05,
pages 58?69.
Andrew Brian Clegg and Adrian Shepherd. 2007.
Benchmarking natural-language parsers for biological
applications using dependency graphs. BMC Bioinfor-
matics, 8(1):24.
Marie-Catherine de Marneffe, Bill MacCartney, and
Christopher D. Manning. 2006. Generating typed de-
pendency parses from phrase structure parses. In Pro-
ceedings of LREC?06, pages 449?454.
J. Ding, D. Berleant, D. Nettleton, and E. Wurtele. 2002.
Mining MEDLINE: abstracts, sentences, or phrases?
In Proceedings of PSB?02, pages 326?337.
Katrin Fundel, Robert Kuffner, and Ralf Zimmer. 2007.
RelEx?Relation extraction using dependency parse
trees. Bioinformatics, 23(3):365?371.
Thomas Ga?rtner, Peter A. Flach, and Stefan Wrobel.
2003. On graph kernels: Hardness results and efficient
alternatives. In COLT?03, pages 129?143. Springer.
Claudio Giuliano, Alberto Lavelli, and Lorenza Romano.
2006. Exploiting shallow linguistic information for re-
lation extraction from biomedical literature. In Pro-
ceedings of EACL?06.
James A. Hanley and B. J. McNeil. 1982. The meaning
and use of the area under a receiver operating charac-
teristic (roc) curve. Radiology, 143(1):29?36.
Lawrence Hunter, Zhiyong Lu, James Firby, William A.
Baumgartner, Helen L Johnson, Philip V. Ogren, and
K. Bretonnel Cohen. 2008. OpenDMAP: An open-
source, ontology-driven concept analysis engine, with
applications to capturing knowledge regarding protein
transport, protein interactions and cell-specific gene
expression. BMC Bioinformatics, 9(78).
Jin-Dong Kim, Tomoko Ohta, and Jun?ichi Tsujii. 2008.
Corpus annotation for mining biomedical events from
literature. BMC Bioinformatics, 9(10).
Carl D. Meyer. 2000. Matrix analysis and applied linear
algebra. Society for Industrial and Applied Mathe-
matics.
Tomohiro Mitsumori, Masaki Murata, Yasushi Fukuda,
Kouichi Doi, and Hirohumi Doi. 2006. Extracting
protein-protein interaction information from biomed-
ical text with svm. IEICE - Trans. Inf. Syst., E89-
D(8):2464?2466.
Claire Ne?dellec. 2005. Learning language in logic -
genic interaction extraction challenge. In Proceedings
of LLL?05.
Tapio Pahikkala, Jorma Boberg, and Tapio Salakoski.
2006a. Fast n-fold cross-validation for regularized
least-squares. In Proceedings of SCAI?06, pages 83?
90.
Tapio Pahikkala, Evgeni Tsivtsivadze, Jorma Boberg, and
Tapio Salakoski. 2006b. Graph kernels versus graph
representations: a case study in parse ranking. In Pro-
ceedings of the ECML/PKDD?06 workshop on Mining
and Learning with Graphs.
Sampo Pyysalo, Filip Ginter, Juho Heimonen, Jari
Bjo?rne, Jorma Boberg, Jouni Ja?rvinen, and Tapio
Salakoski. 2007a. BioInfer: A corpus for information
extraction in the biomedical domain. BMC Bioinfor-
matics, 8(50).
Sampo Pyysalo, Filip Ginter, Veronika Laippala, Ka-
tri Haverinen, Juho Heimonen, and Tapio Salakoski.
2007b. On the unification of syntactic annotations un-
der the stanford dependency scheme: A case study on
BioInfer and GENIA. In Proceedings of BioNLP?07,
pages 25?32.
Sampo Pyysalo, Antti Airola, Juho Heimonen, Jari
Bjo?rne, Filip Ginter, and Tapio Salakoski. 2008.
Comparative analysis of five protein-protein interac-
tion corpora. BMC Bioinformatics, special issue,
9(Suppl 3):S6.
Ryan Rifkin, Gene Yeo, and Tomaso Poggio, 2003. Reg-
ularized Least-squares Classification, volume 190 of
NATO Science Series III: Computer and System Sci-
ences, chapter 7, pages 131?154. IOS Press.
Rune S?tre, Kenji Sagae, and Jun?ichi Tsujii. 2008. Syn-
tactic features for protein-protein interaction extrac-
tion. In Proceedings of LBM?07, volume 319, pages
6.1?6.14.
Akane Yakushiji, Yusuke Miyao, Yuka Tateisi, and
Jun?ichi Tsujii. 2005. Biomedical information ex-
traction with predicate-argument structure patterns. In
Proceedings of SMBM?05, pages 60?69.
Dmitry Zelenko, Chinatsu Aone, and Anthony
Richardella. 2003. Kernel methods for relation
extraction. J. Mach. Learn. Res., 3:1083?1106.
9
Proceedings of the Workshop on BioNLP: Shared Task, pages 10?18,
Boulder, Colorado, June 2009. c?2009 Association for Computational Linguistics
Extracting Complex Biological Events with Rich Graph-Based Feature Sets
Jari Bjo?rne,1 Juho Heimonen,1,2 Filip Ginter,1 Antti Airola,1,2
Tapio Pahikkala1 and Tapio Salakoski1,2
1Department of Information Technology, University of Turku
2Turku Centre for Computer Science (TUCS)
Joukahaisenkatu 3-5, 20520 Turku, Finland
firstname.lastname@utu.fi
Abstract
We describe a system for extracting com-
plex events among genes and proteins from
biomedical literature, developed in context of
the BioNLP?09 Shared Task on Event Extrac-
tion. For each event, its text trigger, class, and
arguments are extracted. In contrast to the pre-
vailing approaches in the domain, events can
be arguments of other events, resulting in a
nested structure that better captures the under-
lying biological statements. We divide the task
into independent steps which we approach as
machine learning problems. We define a wide
array of features and in particular make ex-
tensive use of dependency parse graphs. A
rule-based post-processing step is used to re-
fine the output in accordance with the restric-
tions of the extraction task. In the shared task
evaluation, the system achieved an F-score of
51.95% on the primary task, the best perfor-
mance among the participants.
1 Introduction
In this paper, we present the best-performing system
in the primary task of the BioNLP?09 Shared Task
on Event Extraction (Kim et al, 2009).1 The pur-
pose of this shared task was to competitively eval-
uate information extraction systems targeting com-
plex events in the biomedical domain. Such an eval-
uation helps to establish the relative merits of com-
peting approaches, allowing direct comparability of
results in a controlled setting. The shared task was
1http://www-tsujii.is.s.u-tokyo.ac.jp/
GENIA/SharedTask
the first competitive evaluation of its kind in the
BioNLP field as the extraction of complex events
became possible only recently with the introduction
of corpora containing the necessary annotation: the
GENIA event corpus (Kim et al, 2008a) and the
BioInfer corpus (Pyysalo et al, 2007).
The objective of the primary task (Task 1) was
to detect biologically relevant events such as pro-
tein binding and phosphorylation, given only anno-
tation of named entities. For each event, its class,
trigger expression in the text, and arguments need to
be extracted. The task follows the recent movement
in BioNLP towards the extraction of semantically
typed, complex events the arguments of which can
also be other events. This results in a nested struc-
ture that captures the underlying biological state-
ments more accurately compared to the prevailing
approach of merely detecting binary interactions of
pairs of biological entities.
Our system is characterized by heavy reliance
on efficient, state-of-the-art machine learning tech-
niques and a wide array of features derived from
a full dependency analysis of each sentence. The
system is a pipeline of three major processing steps:
trigger recognition, argument detection and seman-
tic post-processing. By separating trigger recog-
nition from argument detection, we can use meth-
ods familiar from named entity recognition to tag
words as event triggers. Event argument detection
then becomes the task of predicting for each trigger?
trigger or trigger?named entity pair whether it cor-
responds to an actual instantiation of an event argu-
ment. Both steps can thus be approached as classi-
fication tasks. In contrast, semantic post-processing
10
Sentence?splitting
Tokenization
Parsing
Conversion?to?graph?
representation
Trigger?detection
(multi?class?SVM)
System?output
Semantic?post?processing
(rule?based)
Edge?detection
(multi?class?SVM)
Input?data
Figure 1: The main components of the system.
is rule-based, directly implementing argument type
constraints following from the definition of the task.
In the following sections, we present the imple-
mentation of the three stages of our information ex-
traction system in detail, and provide insights into
why we chose the approach we did. We also discuss
alternate directions we followed but that did not im-
prove performance. Finally, we analyze the overall
performance of our system in the shared task as well
as evaluate its components individually.
2 The system description
The overall architecture of the system is shown
in Figure 1. All steps in the system process one
sentence at a time. Since 95% of all annotated
events are fully contained within a single sentence,
this does not incur a large performance penalty but
greatly reduces the size and complexity of the ma-
chine learning problems.
2.1 Graph representation
We represent the extraction target in terms of seman-
tic networks, graphs where the nodes correspond
to named entities and events, and the edges corre-
spond to event arguments. The shared task can then
be viewed as the problem of finding the nodes and
edges of this graph. For instance, nested events are
naturally represented through edges connecting two
event nodes. The graph representation of an exam-
ple sentence is illustrated in Figure 2D.
We have previously used this graph representa-
tion for information extraction (Heimonen et al,
2008; Bjo?rne et al, 2009) as well as for establishing
the connection between events and syntactic depen-
dency parses in the Stanford scheme of de Marneffe
and Manning (2008) (Bjo?rne et al, 2008).
2.2 Trigger detection
We cast trigger detection as a token labeling prob-
lem, that is, each token is assigned to an event class,
or a negative class if it does not belong to a trig-
ger. Triggers are then formed based on the predicted
classes of the individual tokens. Since 92% of all
triggers in the data consist of a single token, adjacent
tokens with the same class prediction form a single
trigger only in case that the resulting string occurs
as a trigger in the training data. An event node is
created for each detected trigger (Figure 2B).
In rare cases, the triggers of events of different
class share a token, thus the token belongs to sev-
eral separate classes. To be able to approach trigger
detection as a multi-class classification task where
each token is given a single prediction, we intro-
duce combined classes as needed. For instance the
class gene expression/positive regulation denotes to-
kens that act as a trigger to two events of the two
respective classes. Note that this implies that the
trigger detection step produces at most one event
node per class for any detected trigger. In the shared
task, however, multiple events of the same class can
share the same trigger. For instance, the trigger in-
volves in Figure 2 corresponds to two separate regu-
lation events. A separate post-processing step is in-
troduced after event argument detection to duplicate
event nodes as necessary (see Section 2.4).
Due to the nature of the GENIA event annota-
tion principles, trigger detection cannot be easily re-
duced to a simple dictionary lookup of trigger ex-
pressions for two main reasons. First, a number of
common textual expressions act as event triggers in
some cases, but not in other cases. For example,
only 28% of the instances of the expression activates
are triggers for a positive regulation event while the
remaining 72% are not triggers for any event. Sec-
ond, a single expression may be associated with var-
ious event classes. For example, the instances of the
token overexpression are evenly distributed among
11
Regulation
Protein
IL-4 gene
Regulation
regulation
Regulation
involves
Protein
NFAT1 and
Protein
NFAT2 .
<Theme <Theme Cause> Cause>
NN NN NN VBZ NN CC .
<nn conj_and><nn dobj><nsubj
NN
Protein
IL-4 gene
Regulation
regulation
Regulation
involves
Protein
NFAT1 and
Protein
NFAT2 .
<Theme <Theme Cause>
Cause><Theme
node duplication
pars
e
T29   Regulation   regulationT30   Regulation   involvesE10   Regulation:T29   Theme:T7E11   Regulation:T30   Theme:E10   Cause:T9E12   Regulation:T30   Theme:E10   Cause:T8
equivalent
D
C
E
T7     Protein   IL-4T8     Protein   NFAT1T9     Protein   NFAT2
Protein
IL-4 gene
Regulation
regulation
Regulation
involves
Protein
NFAT1 and
Protein
NFAT2 .
Protein
IL-4 gene regulation involves
Protein
NFAT1 and
Protein
NFAT2 .
edge detection
trigger recognition
Trai
ning
 Dat
a Pr
epa
ratio
n
B
A
Eve
nt E
xtra
ction
dobj>
Figure 2: An example sentence from Shared Task document 10069428 (simplified). A) Named entities are given.
B) Triggers are detected and corresponding event nodes are created. C) Event argument edges are predicted between
nodes. The result is a sentence-level semantic network. D) One node may denote multiple events of the same class,
therefore nodes are duplicated in the semantic post-processing step. E) The resulting graph can be losslessly trans-
formed into the Shared Task event annotation. Training data for the trigger recognizer includes named entity annotation
(A) and for the edge detector the semantic network with no node duplication (C).
gene expression, positive regulation, and the nega-
tive class. In light of these properties, we address
trigger detection with a multi-class support vector
machine (SVM) classifier that assigns event classes
to individual tokens, one at a time. This is in con-
trast to sequence labeling problems such as named
entity recognition, where a sequential model is typ-
ically employed. The classifier is trained on gold-
standard triggers from the training data and incorpo-
rates a wide array of features capturing the proper-
ties of the token to be classified, both its linear and
dependency context, and the named entities within
the sentence.
Token features include binary tests for capital-
ization, presence of punctuation or numeric charac-
ters, stem using the Porter stemmer (Porter, 1980),
character bigrams and trigrams, and presence of the
token in a gazetteer of known trigger expressions
and their classes, extracted from the training data.
Token features are generated not only for the token
to be classified, but also for tokens in the immediate
linear context and dependency context (tokens that
govern or depend on the token to be classified).
Frequency features include the number of
named entities in the sentence and in a linear win-
dow around the token in question as well as bag-of-
word counts of token texts in the sentence.
Dependency chains up to depth of three are
constructed, starting from the token to be classified.
At each depth, both token features and dependency
type are included, as well as the sequence of depen-
dency types in the chain.
The trigger detector used in the shared task is
in fact a weighted combination of two indepen-
12
dent SVM trigger detectors, both based on the same
multi-class classification principle and somewhat
different feature sets.2 The predictions of the two
trigger detectors are combined as follows. For each
trigger detector and each token, the classifier confi-
dence scores of the top five classes are re-normalized
into the [0, 1] interval. The renormalized confidence
scores of the two detectors are then linearly inter-
polated using a parameter ?, 0 ? ? ? 1, whose
value is set experimentally on the development set,
as discussed below.
Setting the correct precision?recall trade-off in
trigger detection is very important. On one hand,
any trigger left undetected directly implies a false
negative event. On the other hand, the edge detec-
tor is trained on gold standard data where there are
no event nodes without arguments, which creates a
bias toward predicting edges for any event node the
edge detector is presented with. On the develop-
ment set, essentially all predicted event nodes are
given at least one argument edge. We optimize the
precision?recall trade-off explicitly by introducing a
parameter ?, 0 ? ?, that multiplies the classifier
confidence score given to the negative class, that is,
the ?no trigger? class. When ? < 1, the confidence
of the negative class is decreased, thus increasing
the possibility of a given token forming a trigger,
and consequently increasing the recall of the trigger
detector (naturally, at the expense of its precision).
Both trigger detection parameters, the interpola-
tion weight ? and the precision?recall trade-off pa-
rameter ?, are set experimentally using a grid search
to find the globally optimal performance of the en-
tire system on the development set, using the shared
task performance metric. The parameters are thus
not set to optimize the performance of trigger detec-
tion in isolation; they are rather set to optimize the
performance of the whole system.
2.3 Edge detection
After trigger detection, edge detection is used to pre-
dict the edges of the semantic graph, thus extracting
event arguments. Like the trigger detector, the edge
detector is based on a multi-class SVM classifier.
We generate examples for all potential edges, which
2This design should be considered an artifact of the time-
constrained, experiment-driven development of the system
rather than a principled design choice.
 0
 5
 10
 15
 20
 25
 30
 35
 40
 45
 50
0 1 2 3 4 5 6 7 8 9 10 >10
Pro
por
tion
 of 
edg
es [%
]
Edge length
dependency distancelinear distance
Figure 3: The distribution of event argument edge lengths
measured as the number of dependencies on the shortest
dependency path between the edge terminal nodes, con-
trasted with edge lengths measured as the linear token
distance.
are always directed from an event node to another
event node (event nesting) or from an event node to
a named entity node. Each example is then classified
as theme, cause, or a negative denoting the absence
of an edge between the two nodes in the given di-
rection. It should be noted that even though event
nodes often require multiple outgoing edges corre-
sponding to multiple event arguments, all edges are
predicted independently and are not affected by pos-
itive or negative classifications of other edges.
The feature set makes extensive use of syntac-
tic dependencies, in line with many recent stud-
ies in biomedical information extraction (see, e.g.
(Kim et al, 2008b; Miwa et al, 2008; Airola et al,
2008; Van Landeghem et al, 2008; Katrenko and
Adriaans, 2008)). The central concept in generat-
ing features of potential event argument edges is the
shortest undirected path of syntactic dependencies
in the Stanford scheme parse of the sentence which
we assume to accurately capture the relationship ex-
pressed by the edge. In Figure 3, we show that the
distances among event and named entity nodes in
terms of shortest dependency path length are con-
siderably shorter than in terms of their linear order in
the sentence. The end points of the path are the syn-
tactic head tokens of the two named entities or event
triggers. The head tokens are identified using a sim-
ple heuristic. Where multiple shortest paths exist,
all are considered. Most features are built by com-
bining the attributes of multiple tokens (token text,
13
POS tag and entity or event class, such as protein or
binding) or dependencies (type such as subject and
direction relative to surrounding tokens).
N-grams are generated by merging the at-
tributes of 2?4 consecutive tokens. N-grams are also
built for consecutive dependencies. Additional tri-
grams are built for each token and its two flank-
ing dependencies, as well as for each dependency
and its two flanking tokens. These N-grams are de-
fined in the direction of the potential event argument
edge. To take into account the varying directions
of the dependencies, each pair of consecutive tokens
forms an additional bigram defining their governor-
dependent relationship.
Individual component features are defined for
each token and edge in a path based on their
attributes which are also combined with the to-
ken/edge position at either the interior or the end of
the path. Edge attributes are combined with their di-
rection relative to the path.
Semantic node features are built by directly
combining the attributes of the two terminal
event/entity nodes of the potential event argument
edge. These features concatenate both the specific
types of the nodes (e.g. protein or binding) as well
as their categories (event or named entity). Finally,
if the events/entities have the same head token, this
self-loop is explicitly defined as a feature.
Frequency features include the length of the
shortest path as an integer-valued feature as well as
an explicit binary feature for each length. The num-
ber of named entities and event nodes, per type, in
the sentence are defined for each example.
We have used this type of edge detector with a
largely similar feature set previously (Bjo?rne et al,
2009). Also, many of these features are standard
in relation extraction studies (see, e.g., Buyko et al
(2008)).
2.4 Semantic post-processing
The semantic graph produced by the trigger and
edge detection steps is not final. In particular, it
may contain event nodes with an improper combi-
nation of arguments, or no arguments whatsoever.
Additionally, as discussed in Section 2.2, if there are
events of the same class with the same trigger, they
are represented by a single node. Therefore, we in-
troduce a rule-based post-processing step to refine
Figure 4: Example of event duplication. A) All theme?
cause combinations are generated for regulation events.
B) A heuristic is applied to decide how theme arguments
of binding events should be grouped.
the graph, using the restrictions on event argument
types and combinations defined in the shared task.
In Task 1, the allowed argument edges in the
graph are 1) theme from an event to a named en-
tity, 2) theme or cause from a regulation event (or its
subclasses) to an event or a named entity. Edges cor-
responding to invalid arguments are removed. Also,
directed cycles are broken by removing the edge
with the weakest classification confidence score.
After pruning invalid edges, event nodes are du-
plicated so that all events have a valid combination
of arguments. For example, the regulation event in-
volves in Figure 2C has two cause arguments and
therefore represents two distinct events. We thus
duplicate the event node, obtaining one regulation
event for each of the cause arguments (Figure 2D).
Events of type gene expression, transcription,
translation, protein catabolism, localization, and
phosphorylation must have exactly one theme argu-
ment, which makes the duplication process trivial:
duplicate events are created, one for each of the ar-
guments. Regulation events must have one theme
and can additionally have one cause argument. For
these classes we use a heuristic, generating a new
event for each theme?cause combination of outgo-
ing edges (Figure 4A). Binding is the only event
class that can have multiple theme arguments. There
is thus no simple way of determining how multi-
ple outgoing theme edges should be grouped (Fig-
ure 4B). We apply a heuristic that first groups the ar-
guments by their syntactic role, defined here as xthe
first dependency in the shortest path from the event
14
to the argument. It then generates an event for each
pair of arguments that are in different groups. In the
case of only one group, all single-argument events
are generated.
Finally, all events with no arguments as well as
regulation events without a theme argument are iter-
atively removed until no such event remains. The
resulting graph is the output of our event extrac-
tion system and can be losslessly converted into the
shared task format (Figure 2D&E).
2.5 Alternative directions
We now briefly describe some of the alternative di-
rections explored during the system development,
which however did not result in increased perfor-
mance, and were thus not included in the final sys-
tem. Whether the reason was due to the considered
approaches being inadequate for the extraction task,
or simply a result of the tight time constraints en-
forced by the shared task is a question only further
research can shed light on.
For the purpose of dividing the extraction prob-
lem into manageable subproblems, we make strong
independence assumptions. This is particularly the
case in the edge detection phase where each edge
is considered in isolation from other edges, some
of which may actually be associated with the same
event. Similar assumptions are made in the trigger
detection phase, where the classifications of individ-
ual tokens are independent.
A common way to relax independence assump-
tions is to use N -best re-ranking where N most-
likely candidates are re-ranked using global features
that model data dependencies that could not be mod-
elled in the candidate generation step. The best can-
didate with respect to this re-ranked order is then
the final prediction of the system. N -best re-ranking
has been successfully applied for example in statisti-
cal parsing (Charniak and Johnson, 2005). We gen-
erated the ten most likely candidate graphs, as de-
termined by the confidence scores of the individual
edges given by the multi-class SVM. A perfect re-
ranking of these ten candidates would lead to 11.5
percentage point improvement in the overall system
F-score on the development set. While we were un-
able to produce a re-ranker sufficiently accurate to
improve the system performance in the time given,
the large potential gain warrants further research.
In trigger word detection, we experimented with
a structural SVM incorporating Hidden Markov
Model type of sequential dependencies (Altun et al,
2003; Tsochantaridis et al, 2004), which allow con-
ditioning classification decisions on decisions made
for previous tokens as well as with a conditional ran-
dom field (CRF) sequence classifier (Lafferty et al,
2001). Neither of these experiments led to a perfor-
mance gain over the multiclass SVM classifier.
As discussed previously, 4.8% of all annotated
events cross sentence boundaries. This problem
could be approached using coreference resolution
techniques, however, the necessary explicit corefer-
ence annotation to train a coreference resolution sys-
tem is not present in the data. Instead, we attempted
to build a machine-learning based system to detect
cross-sentence event arguments directly, rather than
via their referring expression, but were unable to im-
prove the system performance.
3 Tools and resources
3.1 Multi-class SVM
We use a support vector machine (SVM) multi-class
classifier which has been shown to have state-of-
the-art classification performance (see e.g. (Cram-
mer and Singer, 2002; Tsochantaridis et al, 2004)).
Namely, we use the SVMmulticlass implementa-
tion3 which is one of the fastest multi-class SVM
implementations currently available. Analogously
to the binary SVMs, multi-class SVMs have a reg-
ularization parameter that determines the trade-off
between the training error and the complexity of the
learned concept. We select the value of the parame-
ter on the development set. Multi-class SVMs scale
linearly with respect to both the amount of training
data and the average number of nonzero features per
training example, making them an especially suit-
able learning method for our purposes. They also
provide a real-valued prediction for each example
to be classified which is used as a confidence score
in trigger detection precision?recall trade-off adjust-
ment and event argument edge cycle breaking in se-
mantic post-processing. We use the linear kernel,
the only practical choice to train the classifier with
the large training sets available. For example, the
3http://svmlight.joachims.org/svm_
multiclass.html
15
 0
 10
 20
 30
 40
 50
 60
 70
 80
 0  10  20  30  40  50  60  70  80
Re
cal
l [%
]
Precision [%]
Figure 5: Performance of the 24 systems that participated
in Task 1, together with an F-score contour plot for refer-
ence. Our system is marked with a full circle.
final training data of the edge detector (8932 sen-
tences) consists of 31792 training examples with
295034 unique features. Training with even this
amount of data is computationally feasible, typically
taking less than an hour.
All classifiers used in the system are trained as
follows. First we optimize the regularization param-
eter C by training on the shared task training set and
testing on the shared task development set. We then
re-train the final classifier on the union of the train-
ing and development sets, using the best value of C
in the previous step. The same protocol is followed
for the ? and ? parameters in trigger detection.
3.2 Dependency parses
Both trigger detection and edge prediction rely on
a wide array of features derived from full depen-
dency parses of the sentence. We use the McClosky-
Charniak domain-adapted parser (McClosky and
Charniak, 2008) which is among the best perform-
ing parsers trained on the GENIA Treebank corpus.
The native constituency output of the parser is trans-
formed to the ?collapsed? form of the Stanford de-
pendency scheme (de Marneffe and Manning, 2008)
using the Stanford parser tools.4 The parses were
provided by the shared task organizers.
4 Results and discussion
The final evaluation of the system was performed by
the shared task organizers using a test set whose an-
4http://nlp.stanford.edu/software/
notation was at no point available to the task partici-
pants. By the main criterion of Task 1, approximate
span matching with approximate recursive match-
ing, our system achieved an F-score of 51.95%. Fig-
ure 5 shows the performance of all systems partic-
ipating in Task 1. The per-class results in Table 1
show that regulation events (including positive and
negative regulation) as well as binding events are the
hardest to extract. These classes have F-scores in
the 31?44% range, while the other classes fall into
the 50?78% range. This is not particularly surpris-
ing since binding and regulation are the only classes
in which events can have multiple arguments, which
means that for an event to be detected correctly, the
edge detector often must make several correct pre-
dictions. Additionally, these classes have the lowest
trigger recognition performance on the development
set. It is interesting to note that the per-class perfor-
mance in Table 1 shows no clear correlation between
the number of events of a class and its F-score.
Table 2 shows the performance of the system us-
ing various other evaluation criteria defined in the
shared task. The most interesting of these is the
strict matching criterion, which, in order to consider
an event correctly extracted, requires exact trigger
span as well as all its nested events to be recursively
correct. The performance of the system with respect
to the strict criterion is 47.41% F-score, only 4.5 per-
centage points lower than the relaxed primary mea-
sure. As seen in Table 2, this difference is almost
exclusively due to triggers with incorrect span.
To evaluate the performance impact of each sys-
tem component individually, we report in Table 3
overall system performance on the development set,
obtained by progressively replacing the processing
steps with gold-standard data. The results show that
the errors of the system are almost evenly distributed
between the trigger and edge detectors. For instance,
a perfect trigger detector would decrease the overall
system error of 46.5% by 18.58 percentage points,
a relative decrease of 40%. A perfect edge detec-
tor would, in combination with a perfect trigger de-
tector, lead to system performance of 94.69%. The
improvement that could be gained by further devel-
opment of the semantic post-processing step is thus
limited, indicating that the strict argument combina-
tion restrictions of Task 1 are sufficient to resolve the
majority of post-processing cases.
16
Event Class # R P F
Protein catabolism 14 42.86 66.67 52.17
Phosphorylation 135 80.74 74.66 77.58
Transcription 137 39.42 69.23 50.23
Localization 174 49.43 81.90 61.65
Regulation 291 25.43 38.14 30.52
Binding 347 40.06 49.82 44.41
Negative regulation 379 35.36 43.46 38.99
Gene expression 722 69.81 78.50 73.90
Positive regulation 983 38.76 48.72 43.17
Total 3182 46.73 58.48 51.95
Table 1: Per-class performance in terms of Recall, Preci-
sion, and F-score on the test set (3182 events) using ap-
proximate span and recursive matching, the primary eval-
uation criterion of Task 1.
Matching R P F
Strict 42.65 53.38 47.41
Approx. Span 46.51 58.24 51.72
Approx. Span&Recursive 46.73 58.48 51.95
Table 2: Performance of our system on the test set (3182
events) with respect to other evaluation measures in the
shared task.
5 Conclusions
We have described a system for extracting complex,
typed events from biomedical literature, only assum-
ing named entities as given knowledge. The high
rank achieved in the BioNLP?09 Shared Task com-
petitive evaluation validates the approach taken in
building the system. While the performance is cur-
rently the highest achieved on this data, the F-score
of 51.95% indicates that there remains considerable
room for further development and improvement.
We use a unified graph representation of the data
in which the individual processing steps can be for-
mulated as simple graph transformations: adding or
removing nodes and edges. It is our experience that
such a representation makes handling the data fast,
easy and consistent. The choice of graph representa-
tion is further motivated by the close correlation of
these graphs with dependency parses. As we are go-
ing to explore the interpretation and applications of
these graphs in the future, the graph representation
will likely provide a flexible base to build on.
Dividing the task of event extraction into multi-
ple subtasks that can be approached by well-studied
Trig Edge PP R P F ?F
pred pred pred 51.54 55.62 53.50
GS pred pred 71.66 72.51 72.08 18.58
GS GS pred 97.21 92.30 94.69 22.61
GS GS GS 100.0 100.0 100.0 5.31
Table 3: Effect of the trigger detector (Trig), edge detec-
tor (Edge), and post-processing (PP) on performance on
the development set (1789 events). The ?F column in-
dicates the effect of replacing the predictions (pred) of
a component with the corresponding gold standard data
(GS), i.e. the maximal possible performance gain obtain-
able from further development of that component.
methods proved to be an effective approach in de-
veloping our system. We relied on state-of-the-art
machine learning techniques that scale up to the task
and allow the use of a considerable number of fea-
tures. We also carefully optimized the various pa-
rameters, a vital step when using machine learning
methods, to fine-tune the performance of the system.
In Section 2.5, we discussed alternative directions
pursued during the development of the current sys-
tem, indicating possible future research directions.
To support this future work as well as complement
the description of the system in this paper we intend
to publish our system under an open-source license.
This shared task represents the first competi-
tive evaluation of complex event extraction in the
biomedical domain. The prior research has largely
focused on binary interaction extraction, achieving
after a substantial research effort F-scores of slightly
over 60% (see, e.g., Miwa et al (2008)) on AIMed,
the de facto standard corpus for this task. Even if
a direct comparison of these results is difficult, they
suggest that 52% F-score in complex event extrac-
tion is a non-trivial achievement, especially consid-
ering the more detailed semantics of the extracted
events. Further, complex event extraction is still a
new problem ? relevant corpora having been avail-
able for only a few years.
Acknowledgments
This research was funded by the Academy of Fin-
land. Computational resources were provided by
CSC ? IT Center for Science Ltd. We thank the
shared task organizers for their efforts in data prepa-
ration and system evaluation.
17
References
Antti Airola, Sampo Pyysalo, Jari Bjo?rne, Tapio
Pahikkala, Filip Ginter, and Tapio Salakoski. 2008.
All-paths graph kernel for protein-protein interaction
extraction with evaluation of cross-corpus learning.
BMC Bioinformatics, 9(Suppl 11):S2.
Yasemin Altun, Ioannis Tsochantaridis, and Thomas
Hofmann. 2003. Hidden Markov support vector ma-
chines. In Proceedings of the Twentieth International
Conference on Machine Learning (ICML?03), pages
3?10. AAAI Press.
Jari Bjo?rne, Sampo Pyysalo, Filip Ginter, and Tapio
Salakoski. 2008. How complex are complex
protein-protein interactions? In Proceedings of the
Third International Symposium on Semantic Mining in
Biomedicine (SMBM?08), pages 125?128. TUCS.
Jari Bjo?rne, Filip Ginter, Juho Heimonen, Sampo
Pyysalo, and Tapio Salakoski. 2009. Learning to ex-
tract biological event and relation graphs. In Proceed-
ings of the 17th Nordic Conference on Computational
Linguistics (NODALIDA?09).
Ekaterina Buyko, Elena Beisswanger, and Udo Hahn.
2008. Testing different ACE-style feature sets for
the extraction of gene regulation relations from MED-
LINE abstracts. In Proceedings of the Third Interna-
tional Symposium on Semantic Mining in Biomedicine
(SMBM?08), pages 21?28. TUCS.
Eugene Charniak and Mark Johnson. 2005. Coarse-to-
fine n-best parsing and maxent discriminative rerank-
ing. In Proceedings of the 43rd Annual Meeting of the
Association for Computational Linguistics (ACL?05),
pages 173?180. ACL.
Koby Crammer and Yoram Singer. 2002. On the al-
gorithmic implementation of multiclass kernel-based
vector machines. Journal of Machine Learning Re-
search, 2:265?292.
Marie-Catherine de Marneffe and Christopher Manning.
2008. Stanford typed hierarchies representation. In
Proceedings of the COLING?08 Workshop on Cross-
Framework and Cross-Domain Parser Evaluation,
pages 1?8.
Juho Heimonen, Sampo Pyysalo, Filip Ginter, and Tapio
Salakoski. 2008. Complex-to-pairwise mapping of
biological relationships using a semantic network rep-
resentation. In Proceedings of the Third Interna-
tional Symposium on Semantic Mining in Biomedicine
(SMBM?08), pages 45?52. TUCS.
Sophia Katrenko and Pieter Adriaans. 2008. A local
alignment kernel in the context of NLP. In Proceed-
ings of the 22nd International Conference on Compu-
tational Linguistics (Coling?08).
Jin-Dong Kim, Tomoko Ohta, and Tsujii Jun?ichi. 2008a.
Corpus annotation for mining biomedical events from
literature. BMC Bioinformatics, 9(1):10.
Seonho Kim, Juntae Yoon, and Jihoon Yang. 2008b.
Kernel approaches for genic interaction extraction.
Bioinformatics, 24(1):118?126.
Jin-Dong Kim, Tomoko Ohta, Sampo Pyysalo, Yoshi-
nobu Kano, and Jun?ichi Tsujii. 2009. Overview
of BioNLP?09 shared task on event extraction. In
Proceedings of the NAACL-HLT 2009 Workshop
on Natural Language Processing in Biomedicine
(BioNLP?09). ACL.
John Lafferty, Andrew McCallum, and Fernando Pereira.
2001. Conditional random fields: Probabilistic mod-
els for segmenting and labeling sequence data. In Pro-
ceedings of the 18th International Conference on Ma-
chine Learning (ICML?01), pages 282?289.
David McClosky and Eugene Charniak. 2008. Self-
training for biomedical parsing. In Proceedings of
ACL-08: HLT, Short Papers, pages 101?104. Associa-
tion for Computational Linguistics.
Makoto Miwa, Rune S?tre, Yusuke Miyao, Tomoko
Ohta, and Jun?ichi Tsujii. 2008. Combining
multiple layers of syntactic information for protein-
protein interaction extraction. In Proceedings of the
Third International Symposium on Semantic Mining in
Biomedicine (SMBM?08), pages 101?108. TUCS.
Martin F. Porter. 1980. An algorithm for suffix stripping.
Program, 14(3):130?137.
Sampo Pyysalo, Filip Ginter, Juho Heimonen, Jari
Bjo?rne, Jorma Boberg, Jouni Ja?rvinen, and Tapio
Salakoski. 2007. BioInfer: A corpus for information
extraction in the biomedical domain. BMC Bioinfor-
matics, 8(1):50.
Ioannis Tsochantaridis, Thomas Hofmann, Thorsten
Joachims, and Yasemin Altun. 2004. Support vec-
tor machine learning for interdependent and structured
output spaces. In Proceedings of the Twenty-first Inter-
national Conference on Machine Learning (ICML?04),
pages 104?111. ACM.
Sofie Van Landeghem, Yvan Saeys, Bernard De Baets,
and Yves Van de Peer. 2008. Extracting protein-
protein interactions from text using rich feature vec-
tors and feature selection. In Proceedings of the
Third International Symposium on Semantic Mining in
Biomedicine (SMBM?08), pages 77?84. TUCS.
18
