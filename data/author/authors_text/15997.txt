2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 548?552,
Montre?al, Canada, June 3-8, 2012. c?2012 Association for Computational Linguistics
Ranking-based readability assessment for early primary children?s literature
Yi Ma, Eric Fosler-Lussier
Dept. of Computer Science & Engineering
The Ohio State University
Columbus, OH 43210, USA
may,fosler@cse.ohio-state.edu
Robert Lofthus
Xerox Corporation
Rochester, NY 14604, USA
Robert.Lofthus@xerox.com
Abstract
Determining the reading level of children?s lit-
erature is an important task for providing edu-
cators and parents with an appropriate reading
trajectory through a curriculum. Automating
this process has been a challenge addressed
before in the computational linguistics litera-
ture, with most studies attempting to predict
the particular grade level of a text. However,
guided reading levels developed by educators
operate at a more fine-grained level, with mul-
tiple levels corresponding to each grade. We
find that ranking performs much better than
classification at the fine-grained leveling task,
and that features derived from the visual lay-
out of a book are just as predictive as standard
text features of level; including both sets of
features, we find that we can predict the read-
ing level up to 83% of the time on a small cor-
pus of children?s books.
1 Introduction
Determining the reading level of a text has received
significant attention in the literature, dating back to
simple arithmetic metrics to assess the reading level
based on syllable counts (Flesch, 1948). In the com-
putational linguistics community, several projects
have attempted to determine the grade level of a text
(2nd/3rd/4th/etc). However, the education commu-
nity typically makes finer distinctions in reading lev-
els, with each grade being covered by multiple lev-
els. Moreover, there are multiple scales within the
educational community; for example 1st grade is ap-
proximately covered by levels 3?14 on the Reading
Recovery scale,1 or levels C to H in the Fountas and
Pinnell leveling system.2
For grade-level assessment, classification and
regression approaches have been very promising.
However, it is not clear that an increased number of
classes will allow classification techniques to suc-
ceed with a more fine-grained leveling system. Sim-
ilarly, regression techniques may have problems if
the reading levels are not linearly distributed. In this
work, we investigate a ranking approach to book lev-
eling, and apply this to a fine-grained leveling prob-
lem for Kindergarten through 2nd grade books. The
ranking approach also allows us to be more agnostic
to the particular leveling system: for the vast ma-
jority of pairs of books, different systems will rank
the levels of the books the same way, even if the
exact differences in levels are not the same. Since
most previous work uses classification techniques,
we compare against an SVM multi-class classifier
as well as an SVM regression approach.
What has not received much attention in recent
research is the visual layout of the page. Yet, if one
walks into a bookstore and rummages through the
children?s section, it is very easy to tell the reading
level of a book just by thumbing through the pages.
Visual clues such as the number of text lines per
page, or the area of text boxes relative to the illustra-
tions, or the font size, give instant information to the
reader about the reading level of the book. What is
not clear is if this information is sensitive enough to
deliver a fine-grained assessment of the book. While
1http://www.readingrecovery.org
2http://www.fountasandpinnellleveledbooks.com
548
publishers may have standard guidelines for content
providers on visual layout, these guidelines likely
differ from publisher to publisher and are not avail-
able for the general public. Moreover, in the digi-
tal age teachers are also content providers who do
not have access to these guidelines, so our proposed
ranking system would be very helpful as they cre-
ate reading materials such as worksheets, web pages,
etc.
2 Related Work
Due to the limitations of traditional approaches,
more advanced methods which use statistical lan-
guage processing techniques have been introduced
by recent work in this area (Collins-Thompson and
Callan, 2004; Schwarm and Ostendorf, 2005; Feng
et al, 2010). Collins-Thompson and Callan (2004)
used a smoothed unigram language model to pre-
dict the grade reading levels of web page documents
and short passages. Heilman et al (2007) com-
bined a language modeling approach with grammar-
based features to improve readability assessment for
first and second language texts. Schwarm/Petersen
and Ostendorf (2005; 2009) used a support vector
machine to combine surface features with language
models and parsed features. The datasets used in
these previous related works mostly consist of web
page documents and short passages, or articles from
educational newspapers. Since the datasets used are
text-intensive, many efforts have been made to in-
vestigate text properties at a higher linguistic level,
such as discourse analysis, language modeling, part-
of-speech and parsed-based features. However, to
the best of our knowledge, no prior work attempts to
rank scanned children?s books (in fine-grained read-
ing levels) directly by analyzing the visual layout of
the page.
3 Ranking Book Leveling Algorithm
Our proposed method can be regarded as a modi-
fied version of a standard ranking algorithm, where
we develop a leveling classification by first rank-
ing books, and then assigning the level based on
the ranking output. Given a set of leveled books,
the process to generate a prediction for a new target
book involves the following two steps.
In the first step, we extract features from each
book, and train an off-the-shelf ranking model to
minimize the pairwise error of books. During the
test phase (second step), we rank all of the leveled
training books as well as the new target (test) book
using the trained ranking model. The predicted read-
ing level of the target book then can be inferred from
the reading levels of neighboring leveled books in
the rank-ordered list of books (in our experiment, we
take into account a window of three books above and
below the target book with reading levels weighted
by distance). Intuitively, we can imagine a book-
shelf in which books are sorted by their reading lev-
els. The ranker?s prediction of the reading level of a
target book corresponds to inserting the target book
into the sorted bookshelf.
4 Data Preparation
4.1 Book Selection, Scanning and Markup
We have processed 36 children?s books which range
from reading level A to L (3 books each level). The
golden standard key reading levels of those books
are obtained from Fountas and Pinnell leveled book
list (Fountas and Pinnell, 1996) in which letter A in-
dicates the easiest books to read and letter L iden-
tifies more challenging books; this range covers
roughly Kindergarten through Second Grade. The
set of children?s books covers a large variety of gen-
res, series and publishers.
After seeking permission from the publishers,3
all of the books are scanned and OCRed (Optical
Character Recognized) to create PDF versions of
the book. In order to facilitate the feature extrac-
tion process, we manually annotate each book using
Adobe Acrobat markup drawing tools before con-
verting them into corresponding XML files. The
annotation process consists of two straightforward
steps: first, draw surrounding rectangles around the
location of text content; second, find where the pri-
mary illustration images are and mark them using
rectangle markups. Then the corresponding XML
can be generated directly from Adobe Acrobat with
one click on a customized menu item, which is im-
plemented by using Adobe Acrobat JavaScript API.
3This is perhaps the most time-consuming part of the pro-
cess.
549
# of partitions 1 2 3 4
?1 Accuracy %
SVM Ranker 72.2 69.4 80.6 83.3
SVM Classifier 47.2 61.1 55.6 63.9
SVM Regression 72.2 61.1 58.3 58.3
Flesch-Kincaid 30.6 30.6 30.6 19.4
Spache 27.8 13.9 13.9 11.1
Average leveling error ? standard deviation
SVM Ranker 1.00 ? 0.99 1.03 ? 0.91 0.94 ? 0.83 0.92 ? 0.73
SVM Classifier 2.00 ? 1.60 1.86 ? 1.69 1.78 ? 1.57 1.44 ? 1.23
SVM Regression 1.14 ? 1.13 1.25 ? 1.11 1.33 ? 1.22 1.36 ? 1.22
Flesch-Kincaid 3.03 ? 2.21 3.03 ? 2.29 3.08 ? 2.31 3.31 ? 2.28
Spache 4.06 ? 3.33 4.72 ? 3.27 4.83 ? 3.34 5.19 ? 3.21
Table 1: Per-book (averaged) results for ranking versus classification, reporting accuracy within one level and average
error for different numbers of partitions
4.2 Feature Design
4.2.1 Surface-level Features
We extract a number of purely text-based features
that have typically been used in the education litera-
ture (e.g., (Flesch, 1948)), including:
1. Number of words; 2. Number of letters per
word; 3. Number of sentences; 4. Average sentence
length; 5. Type-token ratio of the text content.
4.2.2 Visually-oriented Features
In this feature set, we include a number of features
that would not be available without looking at the
physical layout of the page; with the annotated PDF
versions of the book we are able to extract:
1. Page count; 2. Number of words per page; 3.
Number of sentences per page; 4. Number of text
lines per page; 5. Number of words per text line;
6. Number of words per annotated text rectangle;
7. Number of text lines per annotated text rectan-
gle; 8. Average ratio of annotated text rectangle area
to page area; 9. Average ratio of annotated image
rectangle area to page area; 10. Average ratio of an-
notated text rectangle area to annotated image rect-
angle area; 11. Average font size.
The OCR process provides some of this informa-
tion automatically; while we have manually anno-
tated rectangles for this study one could theoreti-
cally use the OCR information and vision process-
ing techniques to extract rectangles automatically.
5 Experiments
5.1 Ranking vs. Classification/Regression
In this experiment, we look at whether treating book
leveling as a ranking problem is promising com-
pared to using classification/regression techniques.
Besides taking a whole book as input, we also exper-
iment with partitioning each book uniformly into 2,
3, or 4 parts, treating each sub-book as an indepen-
dent entity. We use a leave-n-out paradigm ? dur-
ing each iteration of the training (iterated through all
books), the system leaves out all n partitions corre-
sponding to one book and then tests on all partitions
corresponding to the held-out book. By averaging
the results for the partitions of the held-out book, we
can obtain its predicted reading level.
For ranking, we use the SVMrank ranker
(Joachims, 2006), which learns a (sparse) weight
vector that minimizes the number of swapped pairs
in the training set. The test book is inserted into the
ordering of the training books by the ranking algo-
rithm, and the level is assigned by averaging the lev-
els of the books above and below the order. To com-
pare the performance of our method with classifiers,
we use both SVMmulticlass classifier (Tsochantaridis
et al, 2004) and SVMlight (with regression learning
option) (Joachims, 1999) to determine the level of
the book directly. All systems are given the same
set of surface text-based and visual-based features
(Sections 4.2.1 and 4.2.2) as input.
550
# of partitions 1 2 3 4
?1 Accuracy %
All Features 72.2 69.4 80.6 83.3
Surface Features 61.1 63.9 58.3 61.1
Visual Features 72.2 72.2 72.2 83.3
Average leveling error ? standard deviation
All Features 1.00 ? 0.99 1.03 ? 0.91 0.94 ? 0.83 0.92 ? 0.73
Surface Features 1.42 ? 1.18 1.28 ? 1.00 1.44 ? 0.91 1.28 ? 1.11
Visual Features 1.03 ? 0.88 0.94 ? 0.86 1.03 ? 0.81 0.89 ? 0.82
Table 2: Per-book (averaged) results for all, surface-only, and visual-only features, reporting accuracy within one level
and average error for different numbers of partitions
We score the systems in two ways: first, we com-
pute the accuracy of the system by claiming it is cor-
rect if the book level is within ?1 of the true level.4
The second scoring method is the absolute error of
number of levels away from the true value, averaged
over all of the books.
As we can observe from Table 1, our ranking
system constantly beats the other two approaches
(the ranker is statistically significantly better than
the classifier at p < 0.05 level ? figures in bold).
One bit of interesting discovery is that SVM regres-
sion needs more data in order to have reliable results,
as the performance is downgraded when the number
of partitions goes up; the ranking approach benefits
from averaging the increasing number of partitions.5
All three methods have the same style of learner
(support vector learning), which suggests that the
performance gain is due to using a ranking crite-
rion in our method. Therefore we believe ranking
is likely a more effective and accurate method than
classification for this task.
One might also wonder how a traditional measure
of reading level (in this case, the Flesch-Kincaid
(Flesch, 1948) and Spache (Spache, 1953) Grade
Level) would hold up for this data. Flesch-Kincaid
and Spache predictions are linearly converted from
calculated grade levels to Fountas-Pinnell levels; all
of the systems utilizing our full feature set outper-
form these two baselines by a significant amount on
both ?1 accuracy and average leveling error.
4Note that this is still rather fine-grained as there are multi-
ple book levels per grade level.
5We only partition the books up to 4 sub-books because the
shortest book we have only contains 4 PDF pages (8 ?book?
pages) and further partitioning the book will lead to sparse data.
5.2 Visual vs. Surface Features
In order to evaluate the benefits of using visual cues
to assess reading levels, we repeat the experiments
using SVMrank based on our proposed ranking book
leveling algorithm with only the visual features or
only surface features.
Table 2 shows that the visual features surprisingly
outperform the surface features (statistically signif-
icant at p < 0.05 level ? figures in bold) and on
some partition levels, visual cues even beat the com-
bination of all features. We note, however, that for
early children?s books, pictures and textual layout
dominate the book content over text. Visual features
can be as useful as traditional surface text-based fea-
tures, but as one moves out of primary literature, we
suspect text features will likely be more effective for
leveling as content becomes more complex.
6 Conclusions
In this paper, we proposed a ranking-based book lev-
eling algorithm to assess reading level for children?s
literature. Our experimental results showed that the
ranking-based approach performs significantly bet-
ter than classification approaches as used in current
literature. The increased number of classes deterio-
rates the performance of classifiers in a fine-grained
leveling system. We also introduced visual features
into readability assessment and have seen consider-
able benefits of using visual cues. Since our target
data are children?s books that contain many illustra-
tions and pictures, it is quite reasonable to utilize vi-
sual content to help predict a more accurate reading
level. Future studies in early childhood readability
need to take visual content into account.
551
References
K. Collins-Thompson and J. Callan. 2004. A language
modeling approach to predicting reading difficulty. In
Proceedings of HLT / NAACL 2004, volume 4, pages
193?200, Boston, USA.
L. Feng, M. Jansche, M. Huenerfauth, and N. Elhadad.
2010. A comparison of features for automatic read-
ability assessment. In Proceedings of the 23rd In-
ternational Conference on Computational Linguistics
(COLING 2010), pages 276?284, Beijing, China. As-
sociation for Computational Linguistics.
R. Flesch. 1948. A new readability yardstick. Journal of
applied psychology, 32(3):221?233.
I. Fountas and G. Pinnell. 1996. Guided Reading:
Good First Teaching for All Children. Heinemann,
Portsmouth, NH.
M. Heilman, K. Collins-Thompson, J. Callan, and M. Es-
kenazi. 2007. Combining lexical and grammatical
features to improve readability measures for first and
second language texts. In Proceedings of NAACL
HLT, pages 460?467.
T. Joachims. 1999. Making large-scale SVM learning
practical. In B. Scho?lkopf, C. Burges, and A. Smola,
editors, Advances in Kernel Methods - Support Vec-
tor Learning, chapter 11, pages 169?184. MIT Press,
Cambridge, MA.
T. Joachims. 2006. Training linear SVMs in linear time.
In Proceedings of the 12th ACM SIGKDD interna-
tional conference on Knowledge discovery and data
mining, pages 217?226. ACM.
S. Petersen and M. Ostendorf. 2009. A machine learn-
ing approach to reading level assessment. Computer
Speech & Language, 23(1):89?106.
S. Schwarm and M. Ostendorf. 2005. Reading level as-
sessment using support vector machines and statistical
language models. In Proceedings of the 43rd Annual
Meeting on Association for Computational Linguis-
tics, pages 523?530. Association for Computational
Linguistics.
G. Spache. 1953. A new readability formula for primary-
grade reading materials. The Elementary School Jour-
nal, 53(7):410?413.
I. Tsochantaridis, T. Hofmann, T. Joachims, and Y. Altun.
2004. Support vector machine learning for interdepen-
dent and structured output spaces. In Proceedings of
the twenty-first international conference on Machine
learning, page 104. ACM.
552
NAACL-HLT 2012 Workshop on Predicting and Improving Text Readability for target reader populations (PITR 2012)., pages 58?64,
Montre?al, Canada, June 7, 2012. c?2012 Association for Computational Linguistics
Comparing human versus automatic feature extraction for fine-grained
elementary readability assessment
Yi Ma, Ritu Singh, Eric Fosler-Lussier
Dept. of Computer Science & Engineering
The Ohio State University
Columbus, OH 43210, USA
may,singhri,fosler@cse.ohio-state.edu
Robert Lofthus
Xerox Corporation
Rochester, NY 14604, USA
Robert.Lofthus@xerox.com
Abstract
Early primary children?s literature poses some
interesting challenges for automated readabil-
ity assessment: for example, teachers often
use fine-grained reading leveling systems for
determining appropriate books for children to
read (many current systems approach read-
ability assessment at a coarser whole grade
level). In previous work (Ma et al, 2012),
we suggested that the fine-grained assess-
ment task can be approached using a ranking
methodology, and incorporating features that
correspond to the visual layout of the page
improves performance. However, the previ-
ous methodology for using ?found? text (e.g.,
scanning in a book from the library) requires
human annotation of the text regions and cor-
rection of the OCR text. In this work, we ask
whether the annotation process can be auto-
mated, and also experiment with richer syntac-
tic features found in the literature that can be
automatically derived from either the human-
corrected or raw OCR text. We find that auto-
mated visual and text feature extraction work
reasonably well and can allow for scaling to
larger datasets, but that in our particular exper-
iments the use of syntactic features adds little
to the performance of the system, contrary to
previous findings.
1 Introduction
Knowing the reading level of a children?s book
is an important task in the educational setting.
Teachers want to have leveling for books in the
school library; parents are trying to select appro-
priate books for their children; writers need guid-
ance while writing for different literacy needs (e.g.
text simplification)?reading level assessment is re-
quired in a variety of contexts. The history of as-
sessing readability using simple arithmetic metrics
dates back to the 1920s when Thorndike (1921) has
measured difficulty of texts by tabulating words ac-
cording to the frequency of their use in general lit-
erature. Most of the traditional readability formulas
were also based on countable features of text, such
as syllable counts (Flesch, 1948).
More advanced machine learning techniques such
as classification and regression have been applied
to the task of reading level prediction (Collins-
Thompson and Callan, 2004; Schwarm and Osten-
dorf, 2005; Petersen and Ostendorf, 2009; Feng et
al., 2010); such works are described in further de-
tail in the next Section 2. In recent work (Ma et al,
2012), we approached the problem of fine-grained
leveling of books, demonstrating that a ranking ap-
proach to predicting reading level outperforms both
classification and regression approaches in that do-
main. A further finding was that visually-oriented
features that consider the visual layout of the page
(e.g. number of text lines per annotated text region,
text region area compared to the whole page area
and font size etc.) play an important role in predict-
ing the reading levels of children?s books in which
pictures and textual layout dominate the book con-
tent over text.
However, the data preparation process in our pre-
vious study involves human intervention?we ask
human annotators to draw rectangle markups around
text region over pages. Moreover, we only use a
very shallow surface level text-based feature set to
58
compare with the visually-oriented features. Hence
in this paper, we assess the effect of using com-
pletely automated annotation processing within the
same framework. We are interested in exploring
how much performance will change by completely
eliminating manual intervention. At the same time,
we have also extended our previous feature set by in-
troducing a richer set of automatically derived text-
based features, proposed by Feng et al (2010),
which capture deeper syntactic complexities of the
text. Unlike our previous work, the major goal of
this paper is not trying to compare different machine
learning techniques used in readability assessment
task, but rather to compare the performance differ-
ences between with and without human labor in-
volved within our previous proposed system frame-
work.
We begin the paper with the description of re-
lated work in Section 2, followed by detailed ex-
planation regarding data preparation and automatic
annotations in Section 3. The extended features will
be covered in Section 4, followed by experimental
analysis in Section 5, in which we will compare the
results between human annotations and automatic
annotations. We will also report the system per-
formance after incorporating the rich text features
(structural features). Conclusions follow in Section
6.
2 Related Work
Since 1920, approximately 200 readability formulas
have been reported in the literature (DuBay, 2004);
statistical language processing techniques have re-
cently entered into the fray for readability assess-
ment. Si and Callan (2001) and Collins-Thompson
and Callan (2004) have demonstrated the use of lan-
guage models is more robust for web documents
and passages. Heilman et al (2007) studied the
impact of grammar-based features combined with
language modeling approach for readability assess-
ment of first and second language texts. They ar-
gued that grammar-based features are more perti-
nent for second language learners than for the first
language readers. Schwarm and Ostendorf (2005)
and Petersen and Ostendorf (2009) both used a sup-
port vector machine to classify texts based on the
reading level. They combined traditional methods
of readability assessment and the features from lan-
guage models and parsers. Aluisio et al (2010)
have developed a tool for text simplification for the
authoring process which addresses lexical and syn-
tactic phenomena to make text readable but their as-
sessment takes place at more coarse levels of liter-
acy instead of finer-grained levels used for children?s
books.
A detailed analysis of various features for auto-
matic readability assessment has been done by Feng
et al (2010). Most of the previous work has used
web page documents, short passages or articles from
educational newspapers as their datasets; typically
the task is to assess reading level at a whole-grade
level. In contrast, early primary children?s literature
is typically leveled in a more fine-grained manner,
and the research question we pursued in our previ-
ous study was to investigate appropriate methods of
predicting what we suspected was a non-linear read-
ing level scale.
Automating the process of readability assessment
is crucial for eventual widespread acceptance. Pre-
vious studies have looked at documents that were
already found in electronic form, such as web texts.
While e-books are certainly on the rise (and would
help automated processing) it is unlikely that paper
books will be completely eliminated from the pri-
mary school classroom soon. Our previous study re-
quired both manual scanning of the books and man-
ual annotation of the books to extract the location
and content of text within the book ? the necessity
of which we evaluate in this study by examining the
effects of errors from the digitization process.
3 Data Preparation and Book Annotation
Our previous study was based on a corpus of 36
scanned children?s books; in this study we have ex-
panded the set to 97 books which range from lev-
els A to N in Fountas and Pinnell Benchmark As-
sessment System 1 (Fountas and Pinnell, 2010); the
Fountas and Pinnell level serves as our gold stan-
dard. The distribution of number of books per read-
ing level is shown in Table 1. Levels A to N,
in increasing difficulty, corresponds to the primary
grade books from roughly kindergarten through
third grade. The collection of children?s books cov-
ers a large diversity of genres, series and publishers.
59
Reading # of Reading # of
Level Books Level Books
A 6 H 7
B 9 I 6
C 5 J 11
D 8 K 6
E 11 L 3
F 10 M 6
G 7 N 2
Table 1: Distribution of books over Fountas and Pinnell
reading levels
Our agreement with the books? publishers only
allows access to physical copies of books rather
than electronic versions; we scan each book into
a PDF version. This situation would be similar to
that of a contemporary classroom teacher who is se-
lecting books from the classroom or school library
for evaluating a child?s literacy progress.1 We then
use Adobe Acrobat to run OCR (Optical Character
Recognition) on the PDF books. Following our pre-
vious work, we first begin our process of annotat-
ing each book using Adobe Acrobat before convert-
ing them into corresponding XML files. Features
for each book are extracted from their correspond-
ing XMLs which contain all the text information and
book layout contents necessary to calculate the fea-
tures. Each book is manually scanned, and then an-
notated in two different ways: we use human anno-
tators (Section 3.1) and a completely automated pro-
cess (Section 3.2). The job of human annotators is
primarily to eliminate the errors made by OCR soft-
ware, as well as correctly identifying text regions on
each page. We encountered three types of typical
OCR errors for the children?s books in our set:
1. False alarms: some small illustration picture
segments (e.g. flower patterns on a little girl?s
pajama or grass growing in bunches on the
ground) are recognized as text.
2. False negatives: this is more likely to occur for
text on irregular background such as white text
1While it is clear that publishers will be moving toward elec-
tronic books which would avoid the process of scanning (and
likely corresponding OCR problems), it is also clear that phys-
ical books and documents will be present in the classroom for
years to come.
OCR Correct Example
output word
1 I 1 ? I
! I ! ? I
[ f [or ? for
O 0 1OO ? 100
nn rm wann ? warm
rn m horne ? home
IT! m aIT! ? am
1n m tilne ? time
n1. m n1.y ? my
1V W 1Ve ? We
vv w vvhen ? when
Table 2: Some common OCR errors
on black background or text overlapped with
illustrations.
3. OCR could misread the text. These are most
common errors. Some examples of this type of
error are shown in Table 2.
The two different annotation processes are explained
in the following Subsections 3.1 and 3.2.
3.1 Human Annotation
Annotators manually draw a rectangular box over
the text region on each page using Adobe Acrobat
markup drawing tools. The annotators also correct
the type 2 and 3 of OCR errors which are mentioned
above. In human annotation process, the false alarm
(type 1) errors are implicitly prevented since the an-
notators will only annotate the regions where text
truly exists on the page (no matter whether the OCR
recognized or not).
3.2 Automatic Annotation
For automatic annotation, we make use of JavaScript
API provided by Adobe Acrobat. The automatic an-
notation tool is implemented as a JavaScript plugin
menu item within Adobe Acrobat. The JavaScript
API can return the position of every single recog-
nized word on the page. Based on the position cues
of each word, we design a simple algorithm to auto-
matically cluster the words into separate groups ac-
cording to certain spatial distance thresholds.2 In-
2A distance threshold of 22 pixels was used in practice.
60
tuitively, one could imagine the words as small
floating soap bubbles on the page?where smaller
bubbles (individual words) which are close enough
will merge together to form bigger bubbles (text re-
gions) automatically. For each detected text region,
a bounding rectangle box annotation is drawn on
the page automatically. Beyond this point, the rest
of the data preparation process is identical to hu-
man annotation, in which the corresponding XMLs
will be generated from the annotated versions of
the PDF books. However, unlike human annota-
tion, automating the annotation process can intro-
duce noise into the data due to uncorrected OCR er-
rors. In correspondence to the three types of OCR
errors, automatic annotation could also draw extra
bounding rectangle boxes on non-text region (where
OCR thinks there is text there but there is not), fails
to draw bounding rectangle boxes on text region
(where OCR should have recognized text there but
it does not) and accepts many mis-recognized non-
word symbols as text content (where OCR misreads
words).
3.3 Generating XMLs From Annotated PDF
Books
This process is also implemented as another
JavaScript plugin menu item within Adobe Acrobat.
The plugin is run on the annotated PDFs and is de-
signed to be agnostic to the annotation types?it will
work on both human-annotated and auto-annotated
versions of PDFs. Once the XMLs for each chil-
dren?s book are generated, we could proceed to the
feature extraction step. The set of features we use in
the experiments are described in the following Sec-
tion 4.
4 Features
For surface-level features and visual features, we
utilize similar features proposed in our previous
study.3 For completeness? sake, we list these two
sets of features as follows in Section 4.1:
3We discard two visual features in both the human and au-
tomatic annotation that require the annotation of the location
of images on the page, as these were features that the Adobe
Acrobat JavaScript API could not directly access.
4.1 Surface-level Features and
Visually-oriented Features
? Surface-level Features
1. Number of words
2. Number of letters per word
3. Number of sentences
4. Average sentence length
5. Type-token ratio of the text content.
? Visually-oriented Features
1. Page count
2. Number of words per page
3. Number of sentences per page
4. Number of text lines per page
5. Number of words per text line
6. Number of words per annotated text rect-
angle
7. Number of text lines per annotated text
rectangle
8. Average ratio of annotated text rectangle
area to page area
9. Average font size
4.2 Structural Features
Since our previous work only uses surface level of
text features, we are interested in investigating the
contribution of high-level structural features to the
current system. Feng et al (2010) found several
parsing-based features and part-of-speech based fea-
tures to be useful. We utilize the Stanford Parser
(Klein and Manning, 2003) to extract the following
features from the XML files based on those used in
(Feng et al, 2010):
? Parsed Syntactic Features for NPs and VPs
1. Number of the NPs/VPs
2. Number of NPs/VPs per sentence
3. Average NP/VP length measured by num-
ber of words
4. Number of non-terminal nodes per parse
tree
5. Number of non-terminal ancestors per
word in NPs/VPs
? POS-based Features
61
1. Fraction of tokens labeled as
noun/preposition
2. Fraction of types labeled as
noun/preposition
3. Number of noun/preposition tokens per
sentence
4. Number of noun/preposition types per
sentence
5 Experiments
In the experiments, we look at how much the perfor-
mance dropped by switching to zero human inputs.
We also investigate the impact of using a richer set
of text-based features. We apply the ranking-based
book leveling algorithm proposed by our previous
study (Ma et al, 2012) and use the SVMrank ranker
(Joachims, 2006) for our experiments. In this sys-
tem, the ranker learns to sort the training books into
leveled order. The unknown test book is inserted
into the ordering of the training books by the trained
ranking model, and the predicted reading level is
calculated by averaging over the levels of the known
books above and below the test book. Following the
previous study, each book is uniformly partitioned
into 4 parts, treating each sub-book as an individ-
ual entity. A leave-n-out procedure is utilized for
evaluation: during each iteration of the training, the
system leaves out all n partitions (sub-books) cor-
responding to one book. In the testing phase, the
trained ranking model tests on all partitions corre-
sponding to the held-out book. We obtain a single
predicted reading level for the held-out book by av-
eraging the results for all its partitions; averaging
produces a more robust result. Two separate experi-
ments are carried out on human-annotated and auto-
annotated PDF books respectively.
We use two metrics to determine quality: first, the
accuracy of the system is computed by claiming it
is correct if the predicted book level is within ?1 of
the true reading level.4 The second scoring metric is
the absolute error of number of levels away from the
key reading level, averaged over all of the books.
4We follow our previous study to use ?1 accuracy evalu-
ation metric in order to generate consistent results and allow
easy comparison. Another thing to notice is that this is still
rather fine-grained since multiple reading levels correspond to
one single grade level.
We report the experiment results on different
combinations of feature sets: surface level features
plus visually-oriented features, surface level features
only, visually-oriented features only, structural fea-
tures only and finally combining all the features to-
gether.
5.1 Human Annotation vs. Automatic
Annotation
As we can observe from Table 3,5 overall the human
annotation gives higher accuracy than automatic an-
notation across different feature sets. The perfor-
mance difference between human annotation and au-
tomatic annotation could be attributed to the OCR
errors (described in Section 3.2) which are intro-
duced in the automatic annotation process. How-
ever, to our surprise, the best performance of human
annotation is not significantly better than automatic
annotation even at p < 0.1 level (figures in bold).6
Only for the experiment using all features does hu-
man annotation outperform the automatic annota-
tion at p < 0.1 level (still not significantly better
at p < 0.05 level, figures with asterisks). There-
fore, we believe that the extra labor involved in the
annotation step could be replaced by the automatic
process without leading to a significant performance
drop. While the process does still require manual
scanning of each book (which can be time consum-
ing depending on the kind of scanner), the automatic
processing can reduce the labor per book from ap-
proximately twenty minutes per book to just a few
seconds.
5.2 Incorporating Structural Features
Our previous study demonstrated that combin-
ing surface features with visual features produces
promising results. As mentioned above, the sec-
ond aim of this study is to see how much benefit
we can get from incorporating high-level structural
features, such as those used in (Feng et al, 2010)
(described in Section 4.2), with the features in our
previous study.
Table 3 shows that for both human and automatic
5In three of the books, the OCR completely failed; thus only
94 books are available for evaluation of the automatic annota-
tion.
6One-tailed Z-test was used with each book taken as an in-
dependent sample.
62
Annotation type Human Automatic
?1 Accuracy %
Surface+Visual features 76.3 70.2
Surface level features 69.1 64.9
Visual features 63.9 58.5
Structural features 63.9 58.5
All features 76.3? 66.0?
Average leveling error ? standard deviation
Surface+Visual features 0.99 ? 0.87 1.16 ? 0.83
Surface level features 1.24 ? 1.05 1.16 ? 0.97
Visual features 1.24 ? 1.00 1.37 ? 0.89
Structural features 1.30 ? 0.89 1.33 ? 0.91
All features 1.05 ? 0.78 1.15 ? 0.90
Table 3: Results on 97 books using human annotations vs. automatic annotations, reporting accuracy within one level
and average error for 4 partitions per book.
annotation under the ?1 accuracy metric, the vi-
sual features and the structural features have the
same performance, whose accuracy are both slightly
lower than that of surface level features. By combin-
ing the surface level features with the visual features,
the system obtains the best performance. How-
ever, by combining all three feature sets, the sys-
tem performance does not change for human annota-
tion whereas it hurts the performance for automatic
annotation?it is likely that the OCR errors existing
in the automatic annotations give rise to erroneous
structural features (e.g. the parser would produce
less robust parses for sentences which have out of
vocabulary words). Overall, we did not observe bet-
ter performance by incorporating structural features.
Using structural features on their own also did not
produce noteworthy results. Although among the
three kinds of features (surface, visual and struc-
tural), structural features have the highest computa-
tional cost, it exhibits no significant improvement to
system results. In the average leveling error metric,
the best performance is again obtained at the com-
bination of surface level features and visual features
for human annotation, whereas the performance re-
mains almost the same after incorporating structural
features for automatic annotation.
6 Conclusion
In this paper, we explore the possibility of reducing
human involvement in the specific task of predicting
reading levels of scanned children?s books by elimi-
nating the need for human annotation. Clearly there
is a trade off between the amount of human labor
involved and the accuracy of the reading level pre-
dicted. Based on the experimental results, we did
not observe significant performance drop by switch-
ing from human annotation to automatic annotation
in the task of predicting reading levels for scanned
children?s books.
We also study the effect of incorporating struc-
tural features into the proposed ranking system. The
experimental results showed that structural features
exhibit no significant effect to the system perfor-
mance. We conclude for the simply structured, short
text that appears in most children?s books, a deep
level analysis of the text properties may be overkill
for the task and produced unsatisfactory results at a
high computational cost for our task.
In the future, we are interested in investigating the
importance of each individual feature as well as ap-
plying various feature selection methods to further
improve the overall performance of the system?in
the hope that making the ranking system more ro-
bust to OCR errors introduced by automatic annota-
tion processing. Another interesting open question
is that how many scanned book pages are needed to
make a good prediction.7 Such analysis would be
very helpful for practical purposes, since a teacher
7We thank an anonymous reviewer of the paper for this sug-
gestion.
63
could just scan few sample pages instead of a full
book for a reliable prediction.
References
S. Aluisio, L. Specia, C. Gasperin, and C. Scarton. 2010.
Readability assessment for text simplification. In Pro-
ceedings of the NAACL HLT 2010 Fifth Workshop on
Innovative Use of NLP for Building Educational Ap-
plications, pages 1?9. Association for Computational
Linguistics.
K. Collins-Thompson and J. Callan. 2004. A language
modeling approach to predicting reading difficulty. In
Proceedings of HLT / NAACL 2004, volume 4, pages
193?200, Boston, USA.
W.H. DuBay. 2004. The principles of readability. Im-
pact Information, pages 1?76.
L. Feng, M. Jansche, M. Huenerfauth, and N. Elhadad.
2010. A comparison of features for automatic read-
ability assessment. In Proceedings of the 23rd In-
ternational Conference on Computational Linguistics
(COLING 2010), pages 276?284, Beijing, China. As-
sociation for Computational Linguistics.
R. Flesch. 1948. A new readability yardstick. Journal of
applied psychology, 32(3):221?233.
I. Fountas and G. Pinnell. 2010. Fountas
and pinnell benchmark assessment system 1.
http://www.heinemann.com/products/E02776.aspx.
M. Heilman, K. Collins-Thompson, J. Callan, and M. Es-
kenazi. 2007. Combining lexical and grammatical
features to improve readability measures for first and
second language texts. In Proceedings of NAACL
HLT, pages 460?467.
T. Joachims. 2006. Training linear SVMs in linear time.
In Proceedings of the 12th ACM SIGKDD interna-
tional conference on Knowledge discovery and data
mining, pages 217?226. ACM.
D. Klein and C. Manning. 2003. Accurate unlexical-
ized parsing. In Proceedings of the 41st Meeting of
the Association for Computational Linguistics, pages
423?430.
Y. Ma, E. Fosler-Lussier, and R. Lofthus. 2012.
Ranking-based readability assessment for early pri-
mary children?s literature. In Proceedings of NAACL
HLT.
S. Petersen and M. Ostendorf. 2009. A machine learn-
ing approach to reading level assessment. Computer
Speech & Language, 23(1):89?106.
S. Schwarm and M. Ostendorf. 2005. Reading level as-
sessment using support vector machines and statistical
language models. In Proceedings of the 43rd Annual
Meeting on Association for Computational Linguis-
tics, pages 523?530. Association for Computational
Linguistics.
L. Si and J. Callan. 2001. A statistical model for scien-
tific readability. In Proceedings of the tenth interna-
tional conference on Information and knowledge man-
agement, pages 574?576. ACM.
E.L. Thorndike. 1921. The teacher?s word book, volume
134. Teachers College, Columbia University New
York.
64
