Proceedings of HLT/EMNLP 2005 Demonstration Abstracts, pages 26?27,
Vancouver, October 2005.
Japanese Speech Understanding Using Grammar Specialization
Manny Rayner, Nikos Chatzichrisafis, Pierrette Bouillon
University of Geneva, TIM/ISSCO
40 bvd du Pont-d?Arve, CH-1211 Geneva 4, Switzerland
mrayner@riacs.edu
{Pierrette.Bouillon,Nikolaos.Chatzichrisafis}@issco.unige.ch
Yukie Nakao, Hitoshi Isahara, Kyoko Kanzaki
National Institute of Information and Communications Technology
3-5 Hikaridai, Seika-cho, Soraku-gun, Kyoto, Japan 619-0289
yukie-n@khn.nict.go.jp, {isahara,kanzaki}@nict.go.jp
Beth Ann Hockey
UCSC/NASA Ames Research Center
Moffet Field, CA 94035
bahockey@riacs.edu
Marianne Santaholma, Marianne Starlander
University of Geneva, TIM/ISSCO
40 bvd du Pont-d?Arve
CH-1211 Geneva 4, Switzerland
Marianne.Santaholma@eti.unige.ch
Marianne.Starlander@eti.unige.ch
The most common speech understanding archi-
tecture for spoken dialogue systems is a combination
of speech recognition based on a class N-gram lan-
guage model, and robust parsing. For many types
of applications, however, grammar-based recogni-
tion can offer concrete advantages. Training a
good class N-gram language model requires sub-
stantial quantities of corpus data, which is gen-
erally not available at the start of a new project.
Head-to-head comparisons of class N-gram/robust
and grammar-based systems also suggest that users
who are familiar with system coverage get better re-
sults from grammar-based architectures (Knight et
al., 2001). As a consequence, deployed spoken dia-
logue systems for real-world applications frequently
use grammar-based methods. This is particularly
the case for speech translation systems. Although
leading research systems like Verbmobil and NE-
SPOLE! (Wahlster, 2000; Lavie et al, 2001) usu-
ally employ complex architectures combining sta-
tistical and rule-based methods, successful practical
examples like Phraselator and S-MINDS (Phrasela-
tor, 2005; Sehda, 2005) are typically phrasal trans-
lators with grammar-based recognizers.
Voice recognition platforms like the Nuance
Toolkit provide CFG-based languages for writing
grammar-based language models (GLMs), but it is
challenging to develop and maintain grammars con-
sisting of large sets of ad hoc phrase-structure rules.
For this reason, there has been considerable inter-
est in developing systems that permit language mod-
els be specified in higher-level formalisms, normally
some kind of unification grammar (UG), and then
compile these grammars down to the low-level plat-
form formalisms. A prominent early example of this
approach is the Gemini system (Moore, 1998).
Gemini raises the level of abstraction signifi-
cantly, but still assumes that the grammars will be
domain-dependent. In the Open Source REGULUS
project (Regulus, 2005; Rayner et al, 2003), we
have taken a further step in the direction of increased
abstraction, and derive all recognizers from a sin-
gle linguistically motivated UG. This derivation pro-
cedure starts with a large, application-independent
UG for a language. An application-specific UG is
then derived using an Explanation Based Learning
(EBL) specialization technique. This corpus-based
specialization process is parameterized by the train-
ing corpus and operationality criteria. The training
corpus, which can be relatively small, consists of ex-
amples of utterances that should be recognized by
the target application. The sentences of the corpus
are parsed using the general grammar, then those
parses are partitioned into phrases based on the op-
erationality criteria. Each phrase defined by the
operationality criteria is flattened, producing rules
of a phrasal grammar for the application domain.
This application-specific UG is then compiled into
26
a CFG, formatted to be compatible with the Nuance
recognition platform. The CFG is compiled into the
runtime recognizer using Nuance tools.
Previously, the REGULUS grammar specialization
programme has only been implemented for English.
In this demo, we will show how we can apply the
same methodology to Japanese. Japanese is struc-
turally a very different language from English, so it
is by no means obvious that methods which work
for English will be applicable in this new context:
in fact, they appear to work very well. We will
demo the grammars and resulting recognizers in the
context of Japanese ? English and Japanese ?
French versions of the Open Source MedSLT medi-
cal speech translation system (Bouillon et al, 2005;
MedSLT, 2005).
The generic problem to be solved when building
any sort of recognition grammar is that syntax alone
is insufficiently constraining; many of the real con-
straints in a given domain and use situation tend to
be semantic and pragmatic in nature. The challenge
is thus to include enough non-syntactic constraints
in the grammar to create a language model that can
support reliable domain-specific speech recognition:
we sketch our solution for Japanese.
The basic structure of our current general
Japanese grammar is as follows. There are four main
groups of rules, covering NP, PP, VP and CLAUSE
structure respectively. The NP and PP rules each as-
sign a sortal type to the head constituent, based on
the domain-specific sortal constraints defined in the
lexicon. VP rules define the complement structure
of each syntactic class of verb, again making use of
the sortal features. There are also rules that allow
a VP to combine with optional adjuncts, and rules
which allow null constituents, in particular null sub-
jects and objects. Finally, clause-level rules form a
clause out of a VP, an optional subject and optional
adjuncts. The sortal features constrain the subject
and the complements combining with a verb, but the
lack of constraints on null constituents and optional
adjuncts still means that the grammar is very loose.
The grammar specialization mechanism flattens the
grammar into a set of much simpler structures, elim-
inating the VP level and only permitting specific pat-
terns of null constituents and adjuncts licenced by
the training corpus.
We will demo several different versions of the
Japanese-input medical speech translation system,
differing with respect to the target language and
the recognition architecture used. In particular, we
will show a) that versions based on the specialized
Japanese grammar offer fast and accurate recogni-
tion on utterances within the intended coverage of
the system (Word Error Rate around 5%, speed un-
der 0.1?RT), b) that versions based on the original
general Japanese grammar are much less accurate
and more than an order of magnitude slower.
References
P. Bouillon, M. Rayner, N. Chatzichrisafis, B.A. Hockey,
M. Santaholma, M. Starlander, Y. Nakao, K. Kanzaki,
and H. Isahara. 2005. A generic multi-lingual open
source platform for limited-domain medical speech
translation. In In Proceedings of the 10th Conference
of the European Association for Machine Translation
(EAMT), Budapest, Hungary.
S. Knight, G. Gorrell, M. Rayner, D. Milward, R. Koel-
ing, and I. Lewin. 2001. Comparing grammar-based
and robust approaches to speech understanding: a case
study. In Proceedings of Eurospeech 2001, pages
1779?1782, Aalborg, Denmark.
A. Lavie, C. Langley, A. Waibel, F. Pianesi, G. Lazzari,
P. Coletti, L. Taddei, and F. Balducci. 2001. Ar-
chitecture and design considerations in NESPOLE!:
a speech translation system for e-commerce applica-
tions. In Proceedings of HLT: Human Language Tech-
nology Conference, San Diego, California.
MedSLT, 2005. http://sourceforge.net/projects/medslt/.
As of 9 June 2005.
R. Moore. 1998. Using natural language knowledge
sources in speech recognition. In Proceedings of the
NATO Advanced Studies Institute.
Phraselator, 2005. http://www.phraselator.com/. As of 9
June 2005.
M. Rayner, B.A. Hockey, and J. Dowding. 2003. An
open source environment for compiling typed unifica-
tion grammars into speech recognisers. In Proceed-
ings of the 10th EACL (demo track), Budapest, Hun-
gary.
Regulus, 2005. http://sourceforge.net/projects/regulus/.
As of 9 June 2005.
Sehda, 2005. http://www.sehda.com/. As of 9 June 2005.
W. Wahlster, editor. 2000. Verbmobil: Foundations of
Speech-to-Speech Translation. Springer.
27
Evaluating Task Performance for a Unidirectional Controlled Language 
Medical Speech Translation System 
 
 
Nikos Chatzichrisafis, Pierrette Bouillon, Manny Rayner, Marianne Santaholma, 
Marianne Starlander 
University of Geneva, TIM/ISSCO 
40 bvd du Pont-d'Arve, CH-1211 Geneva 4, Switzerland 
 
Nikos.Chatzichrisafis@vozZup.com, Pierrette.Bouillon@issco.unige.ch, 
Emmanuel.Rayner@issco.unige.ch, Marianne.Santaholma@eti.unige.ch, 
Marianne.Starlander@eti.unige.ch  
 
Beth Ann Hockey 
UCSC 
NASA Ames Research Center 
Moffett Field, CA 94035 
bahockey@email.arc.nasa.gov 
 
  
Abstract 
We present a task-level evaluation of the 
French to English version of MedSLT, a 
medium-vocabulary unidirectional con-
trolled language medical speech transla-
tion system designed for doctor-patient 
diagnosis interviews. Our main goal was 
to establish task performance levels of 
novice users and compare them to expert 
users. Tests were carried out on eight 
medical students with no previous expo-
sure to the system, with each student us-
ing the system for a total of three 
sessions. By the end of the third session, 
all the students were able to use the sys-
tem confidently, with an average task 
completion time of about 4 minutes. 
1 Introduction 
Medical applications have emerged as one of the 
most promising application areas for spoken lan-
guage translation, but there is still little agreement 
about the question of architectures. There are in 
particular two architectural dimensions which we 
will address: general processing strategy (statistical 
or grammar-based), and top-level translation func-
tionality (unidirectional or bidirectional transla-
tion). Given the current state of the art in 
recognition and machine translation technology, 
what is the most appropriate combination of 
choices along these two dimensions? 
Reflecting current trends, a common approach 
for speech translation systems is the statistical one. 
Statistical translation systems rely on parallel cor-
pora of source and target language texts, from 
which a translation model is trained. However, this 
is not necessarily the best alternative in safety-
critical medical applications. Anecdotally, many 
doctors express reluctance to trust a translation 
device whose output is not readily predictable, and 
most of the speech translation systems which have 
reached the stage of field testing rely on various 
types of grammar-based recognition and rule-based 
translation (Phraselator, 2006; S-MINDS, 2006; 
MedBridge, 2006). Even though statistical systems 
exhibit many desirable properties (purely data-
driven, domain independence), grammar-based 
systems utilizing probabilistic context-free gram-
mar tuning appear to deliver better results when 
training data is sparse (Rayner et al, 2005a). 
One drawback of grammar-based systems is that 
out-of-coverage utterances will be neither recog-
nized nor translated, an objection that critics have 
sometimes painted as decisive. It is by no means 
obvious, however, that restricted coverage is such 
a serious problem. In text processing, work on sev-
eral generations of controlled language systems has 
developed a range of techniques for keeping users 
within the bounds of system coverage (Kittredge, 
2003; Mitamura, 1999). If these techniques work 
for text processing, it is surely not inconceivable 
that variants of them will be equally successful for 
spoken language applications. Users are usually 
able to adapt to a controlled language system given 
enough time. The critical questions are how to 
provide efficient support to guide them towards the 
system's coverage, and how much time they will 
then need before they have acclimatized. 
With regard to top-level translation functional-
ity, the choice is between unidirectional and bidi-
rectional systems. Bidirectional systems are 
certainly possible today1, but the arguments in fa-
vor of them are not as clear-cut as might first ap-
pear. Ceteris paribus, doctors would certainly 
prefer bidirectional systems; in particular, medical 
students are trained to conduct examination dia-
logues using ?open questions? (WH-questions), 
and to avoid leading the patient by asking YN-
questions. 
The problem with a bidirectional system is, 
however, that open questions only really work well 
if the system can reliably handle a broad spectrum 
of replies from the patients, which is over-
optimistic given the current state of the art. In prac-
tice, the system's coverage is always more or less 
restricted, and some experimentation is required 
before the user can understand what language it is 
capable of handling. A doctor, who uses the system 
regularly, will acquire the necessary familiarity. 
The same might be true for a few patients, if spe-
cial circumstances mean that they encounter 
speech translation applications reasonably fre-
quently. Most patients, however, will have had no 
previous exposure to the system, and may be un-
willing to use a type of technology which they 
have trouble understanding.  
A unidirectional system, in which the doctor 
mostly asks YN-questions, will never be ideal. If, 
                                                          
1
 For example, the S-MINDS system (S-MINDS, 2006) 
offers bidirectional translation. 
however, the doctor can become proficient in using 
it, it may still be very much better than the alterna-
tive of no translation assistance at all.  
To summarize, today?s technology definitely 
lets us build unidirectional grammar-based medical 
speech translation systems which work for regular 
users who have had time to adapt to their limita-
tions. While bidirectional systems are possible, the 
case for them is less obvious, since users on the 
patient side may not in practice be able to use them 
effectively. 
In this paper, we will empirically investigate the 
ability of medical students to adapt to the coverage 
of unidirectional spoken language translation sys-
tem. We report a series of experiments, carried out 
using a French to English speech translation sys-
tem, in which medical students with no previous 
experience to the system were asked to use it to 
carry out a series of verbal examinations on sub-
jects who were simulating the symptoms of various 
types of medical conditions. Evaluation will be 
focused on usability. We primarily want to know 
how quickly subjects learn to use the system, and 
how their performance compares to that of expert 
users. 
2 The MedSLT system 
MedSLT (MedSLT, 2005; Bouillon et al, 2005) 
is a unidirectional, grammar-based medical speech 
translation system intended for use in doctor-
patient diagnosis dialogues. The system is built on 
top of Regulus (Regulus, 2006), an Open Source 
platform for developing grammar-based speech 
applications. Regulus supports rapid construction 
of complex grammar-based language models using 
an example-based method (Rayner et al, 2003; 
Rayner et al, 2006), which extracts most of the 
structure of the model from a general linguistically 
motivated resource grammar. Regulus-based rec-
ognizers are reasonably easy to maintain, and 
grammar structure is shared automatically across 
different subdomains. Resource grammars are now 
available for several languages, including English, 
Japanese (Rayner et al, 2005b), French (Bouillon 
et al, 2006) and Spanish. 
MedSLT includes a help module, whose purpose 
is to add robustness to the system and guide the 
user towards the supported coverage. The help 
module uses a second backup recognizer, equipped 
with a statistical language model; it matches the 
results from this second recognizer against a cor-
pus of utterances, which are within system cover-
age and have already been judged to give correct 
translations. In previous studies (Rayner et al, 
2005a; Starlander et al, 2005), we showed that the 
grammar-based recognizer performs much better 
than the statistical one on in-coverage utterances, 
and rather worse on out-of-coverage ones. We also 
found that having the help module available ap-
proximately doubled the speed at which subjects 
learned to use the system, measured as the average 
difference in semantic error rate between the re-
sults for their first quarter-session and their last 
quarter-session. It is also possible to recover from 
recognition errors by selecting one of the displayed 
help sentences; in the cited studies, we found that 
this increased the number of acceptably processed 
utterances by about 10%. 
The version of MedSLT used for the experi-
ments described in the present paper was config-
ured to translate from spoken French into spoken 
English in the headache subdomain. Coverage is 
based on standard headache-related examination 
questions obtained from a doctor, and consists 
mostly of yes/no questions. WH-questions and el-
liptical constructions are also supported. A typical 
short session with MedSLT might be as follows: 
- is the pain in the side of the head? 
- does the pain radiate to the neck? 
- to the jaw? 
- do you usually have headaches in the morn-
ing ?  
The recognizer?s vocabulary is about 1000 sur-
face words; on in-grammar material, Word Error 
Rate is about 8% and semantic error rate (per ut-
terance) about 10% (Bouillon et al, 2006). Both 
the main grammar-based recognizer and the statis-
tical recognizer used by the help system were 
trained from the same corpus of about 975 utter-
ances. Help sentences were also taken from this 
corpus. 
3 Experimental Setup 
In previous work, we have shown how to build a 
robust and extendable speech translation system. 
We have focused on performance metrics defined 
in terms of recognition and translation quality, and 
tested the system on na?ve users without any medi-
cal background (Bouillon et al, 2005; Rayner et 
al., 2005a; Starlander et al, 2005). 
In this paper, our primary goal was rather to fo-
cus on task performance evaluation using plausible 
potential users. The basic methodology used is 
common in evaluating usability in software sys-
tems in general, and spoken language systems in 
particular (Cohen et. al 2000). We defined a simu-
lated situation, where a French-speaking doctor 
was required to carry out a verbal examination of 
an English-speaking patient who claimed to be suf-
fering from a headache, using the MedSLT system 
to translate all their questions. The patients were 
played by members of the development team, who 
had been trained to answer questions consistently 
with the symptoms of different medical conditions 
which could cause headaches. We recruited eight 
native French-speaking medical students to play 
the part of the doctor. All of the students had com-
pleted at least four years of medical school; five of 
them were already familiar with the symptoms of 
different types of headaches, and were experienced 
in real diagnosis situations. 
The experiment was designed to study how well 
users were able to perform the task using the 
MedSLT system. In particular, we wished to de-
termine how quickly they could adapt to the re-
stricted language and limited coverage of the 
system. As a comparison point, representing near-
perfect performance, we also carried out the same 
test on two developers who had been active in im-
plementing the system, and were familiar with its 
coverage. 
Since it seemed reasonable to assume that most 
users would not interact with the system on a daily 
basis, we conducted testing in three sessions, with 
an interval of two days between each session. At 
the beginning of the first session, subjects were 
given a standardized 10-minute introduction to the 
system. This consisted of instruction on how to set 
up the microphone, a detailed description of the 
MedSLT push-to-talk interface, and a video clip 
showing the system in action. At the end of the 
presentation, the subject was given four sample 
sentences to get familiar with the system. 
After the training was completed, subjects were 
asked to play the part of a doctor, and conduct an 
examination through the system. Their task was to 
identify the headache-related condition simulated 
by the ?patient?, out of nine possible conditions. 
Subjects were given definitions of the simulated 
headache types, which included conceptual infor-
mation about location, duration, frequency, onset 
and possible other symptoms the particular type of 
headache might exhibit. 
Subjects were instructed to signal the conclusion 
of their examination when they were sure about the 
type of simulated headache. The time required to 
reach a conclusion was noted in the experiment 
protocols by the experiment supervisor. 
The subjects repeated the same diagnosis task on 
different predetermined sets of simulated condi-
tions during the second and third sessions. The ses-
sions were concluded either when a time limit of 
30 minutes was reached, or when the subject com-
pleted three headache diagnoses. At the end of the 
third session, the subject was asked to fill out a 
questionnaire. 
4 Results 
Performance of a speech translation system is 
best evaluated by looking at system performance 
as a whole, and not separately for each subcompo-
nent in the systems processing pipeline (Rayner et. 
al. 2000, pp. 297-pp. 312). In this paper, we conse-
quently focus our analysis on objective and subjec-
tive usability-oriented measures. 
In Section 4.1, we present objective usability 
measures obtained by analyzing user-system inter-
actions and measuring task performance. In Sec-
tion 4.2, we present subjective usability figures and 
a preliminary analysis of translation quality. 
4.1 Objective Usability Figures 
4.1.1 Analysis of User Interactions 
Most of our analysis is based on data from the 
MedSLT system log, which records all interactions 
between the user and the system. An interaction is 
initiated when the user presses the ?Start Recogni-
tion? button. The system then attempts to recog-
nize what the user says. If it can do so, it next 
attempts to show the user how it has interpreted the 
recognition result, by first translating it into the 
Interlingua, and then translating it back into the 
source language (in this case, French). If the user 
decides that the back-translation is correct, they 
press the ?Translate? button. This results in the 
system attempting to translate the Interlingua rep-
resentation into the target language (in this case, 
English), and speak it using a Text-To-Speech en-
gine. The system also displays a list of ?help sen-
tences?, consisting of examples that are known to 
be within coverage, and which approximately 
match the result of performing recognition with the 
statistical language model. The user has the option 
of choosing a help sentence from the list, using the 
mouse, and submitting this to translation instead.  
We classify each interaction as either ?success-
ful? or ?unsuccessful?. An interaction is defined to 
be unsuccessful if either 
i) the user re-initiates recognition without 
asking the system for a translation, or 
ii) the system fails to produce a correct 
translation or back translation. 
Our definition of ?unsuccessful interaction? in-
cludes instances where users accidentally press the 
wrong button (i.e. ?Start Recognition? instead of 
?Translate?), press the button and then say nothing, 
or press the button and change their minds about 
what they want to ask half way through. We ob-
served all of these behaviors during the tests. 
Interactions where the system produced a trans-
lation were counted as successful, irrespective of 
whether the translation came directly from the 
user?s spoken input or from the help list. In at least 
some examples, we found that when the translation 
came from a help sentence it did not correspond 
directly to the sentence the user had spoken; to our 
surprise, it could even be the case that the help sen-
tence expressed the directly opposite question to 
the one the user had actually asked. This type of 
interaction was usually caused by some deficiency 
in the system, normally bad recognition or missing 
coverage. Our informal observation, however, was 
that, when this kind of thing happened, the user 
perceived the help module positively: it enabled 
them to elicit at least some information from the 
patient, and was less frustrating than being forced 
to ask the question again. 
Table I to Table III show the number of total in-
teractions per session, the proportion of successful 
interactions, and the proportion of interactions 
completed by selecting a sentence from the help 
list. The total number of interactions required to 
complete a session decreased over the three ses-
sions, declining from an average of 98.6 interac-
tions in the first session to 63.4 in the second (36% 
relative) and 53.9 in the third (45% relative). It is 
interesting to note that interactions involving the 
help system did not decrease in frequency, but re-
mained almost constant over the first two sessions 
(15.5% and 14.0%), and were in fact most com-
mon during the third session (21.7%). 
 
Session 1 
Subject Interactions % Successful % Help 
User 1 57 56.1% 0.0% 
User 2 98 52.0% 25.5% 
User 3 91 63.7% 15.4% 
User 4 156 69.9% 10.3% 
User 5 86 64.0% 22.1% 
User 6 134 47.0% 19.4% 
User 7 56 53.6% 5.4% 
User 8 111 63.1% 26.1% 
AVG 98.6 58.7% 15.5% 
Table I Total interaction rounds, percentage of 
successful interactions, and interactions involving 
the help system by subject for the 1st session 
 
Session 2 
Subject Interactions % Successful % Help 
User 1 50 74.0% 2.0% 
User 2 63 55.6% 27.0% 
User 3 34 88.2% 23.5% 
User 4 96 57.3% 17.7% 
User 5 64 65.6% 21.9% 
User 6 93 68.8% 10.8% 
User 7 48 60.4% 4.2% 
User 8 59 79.7% 5.1% 
AVG 63.4 68.7% 14.0% 
Table II Total interaction rounds, percentage of 
successful interactions, and interactions involving 
the help system by subject for the 2nd session 
 
Session 3 
Subject Interactions % Successful % Help 
User 1 33 90.9% 33.3% 
User 2 57 56.1% 22.8% 
User 3 48 72.9% 29.2% 
User 4 67 70.2% 16.4% 
User 5 68 73.5% 27.9% 
User 6 60 70.0% 6.7% 
User 7 41 65.9% 14.6% 
User 8 57 56.1% 22.8% 
AVG 53.9 69.5% 21.7% 
Table III Total interaction rounds, percentage of 
successful interactions, and interactions involving 
the help system by subject for the 3rd session 
In order to establish a performance baseline, we 
also analyzed interaction data for two expert users, 
who performed the same experiment. The expert 
users were two native French-speaking system de-
velopers, which were both familiar with the diag-
nosis domain. Table IV summarizes the results of 
those users. One of our expert users, listed as Ex-
pert 2, is the French grammar developer, and had 
no failed interactions. This confirms that recogni-
tion is very accurate for users who know the cov-
erage. 
 
Session 1 / Expert Users 
Subject Interactions % Successful % Help 
Expert 1 36 77.8% 13.9% 
Expert 2 30 100.0% 3.3% 
AVG 33 88.9% 8.6% 
Table IV Number of interactions, and percentages 
of successful interactions, and interactions 
involving the help component 
 
The expert users were able to complete the ex-
periment using an average of 33 interaction rounds. 
Similar performance levels were achieved by some 
subjects during the second and third session, which 
suggests that it is possible for at least some new 
users to achieve performance close to expert level 
within a few sessions. 
4.1.2 Task Level Performance 
One of the important performance indicators for 
end users is how long it takes to perform a given 
task. During the experiments, the instructors noted 
completion times required to reach a definite diag-
nosis in the experiment log. Table VI shows task 
completion times, categorized by session (col-
umns) and task within the session (rows).  
 Session 1 Session 2 Session 3 
Diagnosis 1 17:00 min 11:00 min 7:54 min 
Diagnosis 2 11:00 min 6:18 min 5:34 min 
Diagnosis 3 7:54 min 4:10 min 4:00 min 
Table V Average time required by subjects to 
complete diagnoses 
 
In the last two sessions, after subjects had ac-
climatized to the system, a diagnosis takes an aver-
age of about four minutes to complete. This 
compares to a three-minute average required to 
complete a diagnosis by our expert users. 
4.1.3 System coverage 
Table VI shows the percentage of in-coverage 
sentences uttered by the users on interactions that 
did not involve invocation of the help component. 
 
 IN-COVERAGE SENTENCES 
Session 1 54.9% 
Session 2 60.7% 
Session 3 64.6% 
Table VI Percentage of in-coverage sentences 
 
This indicates that subjects learn and adapt to 
the system coverage as they use the system more. 
The average proportion of in-coverage utterances 
is 10 percent higher during the third session than 
during the first session. 
4.2 Subjective Usability Measures 
4.2.1 Results of Questionnaire 
After finishing the third session, subjects were 
asked to fill in a short questionnaire, where re-
sponses were on a five-point scale ranging from 1 
(?strongly disagree?) to 5 (?strongly agree?). The 
results are presented in Table VIII. 
 
STATEMENT SCORE 
I quickly learned how to use the system. 4.4 
System response times were generally 
satisfactory. 
4.5 
When the system did not understand me, 
the help system usually showed me an-
other way to ask the question. 
4.6 
When I knew what I could say, the sys-
tem usually recognized me correctly. 
4.3 
I was often unable to ask the questions I 
wanted. 
3.8 
I could ask enough questions that I was 
sure of my diagnosis. 
4.3 
This system is more effective than non-
verbal communication using gestures. 
4.3 
I would use this system again in a simi-
lar situation. 
4.1 
Table VIII Subject responses to questionnaire. 
Scores are on a 5-point scale, averaged over all 
answers. 
 
Answers were in general positive, and most of 
the subjects were clearly very comfortable with the 
system after just an hour and a half of use. Interest-
ingly, even though most of the subjects answered 
?yes? to the question ?I was often unable to ask the 
questions I wanted?, the good performance of the 
help system appeared to compensate adequately for 
missing coverage. 
4.2.2 Translation Performance 
In order to evaluate the translation quality of the 
newly developed French-to-English system, we 
conducted a preliminary performance evaluation, 
similar to the evaluation method described in 
(Bouillon 2005). 
We performed translation judgment in two 
rounds. In the first round, an English-speaking 
judge was asked to categorize target utterances as 
comprehensible or not without looking at corre-
sponding source sentences. 91.1% of the sentences 
were judged as comprehensible. The remaining 
8.9% consisted of sentences where the terminology 
used was not familiar to the judge and of sentences 
where the translation component failed to produce 
a sufficiently good translation. An example sen-
tence is 
- Are the headaches better when you experi-
ence dark room? 
which stems from the French source sentence 
- Vos maux de t?te sont ils soulag?s par obs-
curit?? 
In the second round, English-speaking judges, 
sufficiently fluent in French to understand source 
language utterances, were shown the French source 
utterance, and asked to decide whether the target 
language utterance correctly reflected the meaning 
of the source language utterance. They were also 
asked to judge the style of the target language ut-
terance. Specifically, judges were asked to classify 
sentences as ?BAD? if the meaning of the English 
sentence did not reflect the meaning of the French 
sentence. Sentences were categorized as ?OK? if 
the meaning was transferred correctly and the sen-
tence was comprehensible, but the style of the re-
sulting English sentence was not perfect. Sentences 
were judged as ?GOOD? when they were compre-
hensible, and both meaning and style were consid-
ered to be completely correct. Table VIII 
summarizes results of two judges. 
 
 Good OK Bad  
Judge 1 15.8% 73.80% 10.3% 
Judge 2 46.6% 47.1% 6.3% 
Table VIII Judgments of the quality of the transla-
tions of 546 utterances 
 
It is apparent that translation judging is a highly 
subjective process. When translations were marked 
as ?bad?, the problem most often seemed to be re-
lated to lexical items where it was challenging to 
find an exact correspondence between French and 
English. Two common examples were ?troubles de 
la vision?, which was translated as ?blurred vi-
sion?, and ?faiblesse musculaire?, which was trans-
lated as ?weakness?. It is likely that a more careful 
choice of lexical translation rules would deal with 
at least some of these cases. 
5 Summary 
We have presented a first end-to-end evaluation 
of the MedSLT spoken language translation sys-
tem. The medical students who tested it were all 
able to use the system well, with performance in 
some cases comparable to that of that of system 
developers after only two sessions. At least for the 
fairly simple type of diagnoses covered by our sce-
nario, the system?s performance appeared clearly 
adequate for the task.  
This is particularly encouraging, since the 
French to English version of the system is quite 
new, and has not yet received the level of attention 
required for a clinical system. The robustness 
added by the help system was sufficient to com-
pensate for that, and in most cases, subjects were 
able to find ways to maneuver around coverage 
holes and other problems. It is entirely reasonable 
to hope that performance, which is already fairly 
good, would be substantially better with another 
couple of months of development work. 
In summary, we feel that this study shows that 
the conservative architecture we have chosen 
shows genuine potential for use in medical diagno-
sis situations. Before the end of 2006, we hope to 
have advanced to the stage where we can start ini-
tial trials with real doctors and patients. 
 
 
Acknowledgments 
We would like to thank Agnes Lisowska, Alia 
Rahal, and Nancy Underwood for being impartial 
judges over our system?s results. 
This work was funded by the Swiss National 
Science Foundation. 
References 
P. Bouillon, M. Rayner, N. Chatzichrisafis, B.A. 
Hockey, M. Santaholma, M. Starlander, Y. Nakao, K. 
Kanzaki, and H. Isahara. 2005. A generic multi-
lingual open source platform for limited-domain 
medical speech translation. In Proceedings of the 
10th Conference of the European Association for 
Machine Translation (EAMT), Budapest, Hungary. 
P. Bouillon, M. Rayner, B. Novellas, Y. Nakao, M. San-
taholma, M. Starlander, and N. Chatzichrisafis. 2006. 
Une grammaire multilingue partag?e pour la recon-
naissance et la g?n?ration. In Proceedings of TALN 
2006, Leuwen, Belgium. 
M. Cohen, J. Giangola, and J. Balogh. 2004, Voice User 
Interface Design. Addison Wesley Publishing. 
R. I. Kittredge. 2003. Sublanguages and comtrolled 
languages. In R. Mitkov, editor, The Oxford Hand-
book of Computational Linguistics, pages 430?447. 
Oxford University Press. 
MedBridge, 2006. http://www.medtablet.com/. As of 
15th March 2006. 
MedSLT, 2005. http://sourceforge.net/projects/medslt/. 
As of 15th March 2006. 
T. Mitamura. 1999. Controlled language for multilin-
gual machine translation. In Proceedings of Machine 
Translation Summit VII, Singapore. 
Phraselator, 2006. http://www.phraselator.com. As of 
15 February 2006. 
M. Rayner, B.A. Hockey, and J. Dowding. 2003. An 
open source environment for compiling typed unifi-
cation grammars into speech recognisers. In Pro-
ceedings of the 10th EACL (demo track), Budapest, 
Hungary. 
M. Rayner, N. Chatzichrisafis, P. Bouillon, Y. Nakao, 
H. Isahara, K. Kanzaki, and B.A. Hockey. 2005b. 
Japanese speech understanding using grammar spe-
cialization. In HLT-NAACL 2005: Demo Session, 
Vancouver, British Columbia, Canada. Association 
for Computational Linguistics. 
M. Rayner, P. Bouillon, N. Chatzichrisafis, B.A. 
Hockey, M. Santaholma,M. Starlander, H. Isahara, 
K. Kankazi, and Y. Nakao. 2005a. A methodology for 
comparing grammar-based and robust approaches to 
speech understanding. In Proceedings of the 9th In-
ternational Conference on Spoken Language Process-
ing (ICSLP), Lisboa, Portugal. 
M. Rayner, D. Carter, P. Bouillon, V. Digalakis, and M. 
Wir?n. 2000. The Spoken Language Translator, 
Cambridge University Press.  
M. Rayner, N. Chatzichrisafis, P. Bouillon, Y. Nakao, 
H. Isahara, K. Kanzaki, and B.A. Hockey. 2005b. 
Japanese speech understanding using grammar spe-
cialization. In HLT-NAACL 2005: Demo Session, 
Vancouver, British Columbia, Canada. Association 
for Computational Linguistics. 
M. Rayner, B.A. Hockey, and P. Bouillon. 2006. Put-
ting Linguistics into Speech Recognition: The 
Regulus Grammar Compiler. CSLI Press, Chicago.  
Regulus, 2006. http://sourceforge.net/projects/regulus/. 
As of 15 March 2006. 
S-MINDS, 2006. http://www.sehda.com/. As of 15 
March 2006. 
M. Starlander, P. Bouillon, N. Chatzichrisafis, M. San-
taholma, M. Rayner, B.A. Hockey, H. Isahara, K. 
Kanzaki, and Y. Nakao. 2005. Practicing controlled 
language through a help system integrated into the 
medical speech translation system (MedSLT). In Pro-
ceedings of the MT Summit X, Phuket, Thailand 
 
MedSLT: A Limited-Domain Unidirectional Grammar-Based Medical
Speech Translator
Manny Rayner, Pierrette Bouillon, Nikos Chatzichrisafis, Marianne Santaholma, Marianne Starlander
University of Geneva, TIM/ISSCO, 40 bvd du Pont-d?Arve, CH-1211 Geneva 4, Switzerland
Emmanuel.Rayner@issco.unige.ch
Pierrette.Bouillon@issco.unige.ch, Nikos.Chatzichrisafis@vozZup.com
Marianne.Santaholma@eti.unige.ch, Marianne.Starlander@eti.unige.ch
Beth Ann Hockey
UCSC/NASA Ames Research Center, Moffet Field, CA 94035
bahockey@email.arc.nasa.gov
Yukie Nakao, Hitoshi Isahara, Kyoko Kanzaki
National Institute of Information and Communications Technology
3-5 Hikaridai, Seika-cho, Soraku-gun, Kyoto, Japan 619-0289
yukie-n@khn.nict.go.jp, {isahara,kanzaki}@nict.go.jp
Abstract
MedSLT is a unidirectional medical
speech translation system intended for
use in doctor-patient diagnosis dialogues,
which provides coverage of several differ-
ent language pairs and subdomains. Vo-
cabulary ranges from about 350 to 1000
surface words, depending on the language
and subdomain. We will demo both the
system itself and the development envi-
ronment, which uses a combination of
rule-based and data-driven methods to
construct efficient recognisers, generators
and transfer rule sets from small corpora.
1 Overview
The mainstream in speech translation work is for the
moment statistical, but rule-based systems are still a
very respectable alternative. In particular, nearly all
systems which have actually been deployed are rule-
based. Prominent examples are (Phraselator, 2006;
S-MINDS, 2006; MedBridge, 2006).
MedSLT (MedSLT, 2005; Bouillon et al, 2005)
is a unidirectional medical speech translation system
for use in doctor-patient diagnosis dialogues, which
covers several different language pairs and subdo-
mains. Recognition is performed using grammar-
based language models, and translation uses a rule-
based interlingual framework. The system, includ-
ing the development environment, is built on top of
Regulus (Regulus, 2006), an Open Source platform
for developing grammar-based speech applications,
which in turn sits on top of the Nuance Toolkit.
The demo will show how MedSLT can be used
to carry out non-trivial diagnostic dialogues. In par-
ticular, we will demonstrate how an integrated intel-
ligent help system counteracts the brittleness inher-
ent in rule-based processing, and rapidly leads new
users towards the supported system coverage. We
will also demo the development environment, and
show how grammars and sets of transfer rules can be
efficiently constructed from small corpora of a few
hundred to a thousand examples.
2 The MedSLT system
The MedSLT demonstrator has already been exten-
sively described elsewhere (Bouillon et al, 2005;
Rayner et al, 2005a), so this section will only
present a brief summary. The main components are
a set of speech recognisers for the source languages,
a set of generators for the target languages, a transla-
tion engine, sets of rules for translating to and from
interlingua, a simple discourse engine for dealing
with context-dependent translation, and a top-level
which manages the information flow between the
other modules and the user.
MedSLT also includes an intelligent help mod-
ule, which adds robustness to the system and guides
the user towards the supported coverage. The help
module uses a backup recogniser, equipped with a
statistical language model, and matches the results
from this second recogniser against a corpus of utter-
ances which are within system coverage and trans-
late correctly. In previous studies, we showed that
the grammar-based recogniser performs much bet-
ter than the statistical one on in-coverage utterances,
but worse on out-of-coverage ones. Having the help
system available approximately doubled the speed
at which subjects learned, measured as the average
difference in semantic error rate between the results
for their first quarter-session and their last quarter-
session (Rayner et al, 2005a). It is also possible to
recover from recognition errors by selecting a dis-
played help sentence; this typically increases the
number of acceptably processed utterances by about
10% (Starlander et al, 2005).
We will demo several versions of the system, us-
ing different source languages, target languages and
subdomains. Coverage is based on standard exami-
nation questions obtained from doctors, and consists
mainly of yes/no questions, though there is also sup-
port for WH-questions and elliptical utterances. Ta-
ble 1 gives examples of the coverage in the English-
input headache version, and Table 2 summarises
recognition performance in this domain for the three
main input languages. Differences in the sizes of the
recognition vocabularies are primarily due to differ-
ences in use of inflection. Japanese, with little in-
flectional morphology, has the smallest vocabulary;
French, which inflects most parts of speech, has the
largest.
3 The development environment
Although the MedSLT system is rule-based, we
would, for the usual reasons, prefer to acquire these
rules from corpora using some well-defined method.
There is, however, little or no material available for
most medical speech translation domains, including
ours. As noted in (Probst and Levin, 2002), scarcity
of data generally implies use of some strategy to ob-
tain a carefully structured training corpus. If the cor-
pus is not organised in this way, conflicts between
alternate learned rules occur, and it is hard to in-
Where?
?do you experience the pain in your jaw?
?does the pain spread to the shoulder?
When?
?have you had the pain for more than a month?
?do the headaches ever occur in the morning?
How long?
?does the pain typically last a few minutes?
?does the pain ever last more than two hours?
How often?
?do you get headaches several times a week?
?are the headaches occurring more often?
How?
?is it a stabbing pain?
?is the pain usually severe?
Associated symptoms?
?do you vomit when you get the headaches?
?is the pain accompanied by blurred vision?
Why?
?does bright light make the pain worse?
?do you get headaches when you eat cheese?
What helps?
?does sleep make the pain better?
?does massage help?
Background?
?do you have a history of sinus disease?
?have you had an e c g?
Table 1: Examples of English MedSLT coverage
duce a stable set of rules. As Probst and Levin sug-
gest, one obvious way to attack the problem is to
implement a (formal or informal) elicitation strat-
egy, which biases the informant towards translations
which are consistent with the existing ones. This is
the approach we have adopted in MedSLT.
The Regulus platform, on which MedSLT
is based, supports rapid construction of com-
plex grammar-based language models; it uses an
example-based method driven by small corpora
of disambiguated parsed examples (Rayner et al,
2003; Rayner et al, 2006), which extracts most of
the structure of the model from a general linguis-
tically motivated resource grammar. The result is
a specialised version of the general grammar, tai-
lored to the example corpus, which can then be com-
piled into an efficient recogniser or into a genera-
Language Vocab WER SemER
English 441 6% 18%
French 1025 8% 10%
Japanese 347 4% 4%
Table 2: Recognition performance for English,
French and Japanese headache-domain recognisers.
?Vocab? = number of surface words in source lan-
guage recogniser vocabulary; ?WER? = Word Error
Rate for source language recogniser, on in-coverage
material; ?SemER? = semantic error rate for source
language recogniser, on in-coverage material.
tion module. Regulus-based recognisers and gen-
erators are easy to maintain, and grammar struc-
ture is shared automatically across different subdo-
mains. Resource grammars are available for several
languages, including English, Japanese, French and
Spanish.
Nuance recognisers derived from the resource
grammars produce both a recognition string and a
semantic representation. This representation con-
sists of a list of key/value pairs, optionally including
one level of nesting; the format of interlingua and
target language representations is similar. The for-
malism is sufficiently expressive that a reasonable
range of temporal and causal constructions can be
represented (Rayner et al, 2005b). A typical exam-
ple is shown in Figure 1. A translation rule maps
a list of key/value pairs to a list of key/value pairs,
optionally specifying conditions requiring that other
key/value pairs either be present or absent in the
source representation.
When developing new coverage for a given lan-
guage pair, the developer has two main tasks. First,
they need to add new training examples to the
corpora used to derive the specialised grammars
used for the source and target languages; second,
they must add translation rules to handle the new
key/value pairs. The simple structure of the Med-
SLT representations makes it easy to support semi-
automatic acquisition of both of these types of in-
formation. The basic principle is to attempt to find
the minimal set of new rules that can be added to the
existing set, in order to cover the new corpus exam-
ple; this is done through a short elicitation dialogue
with the developer. We illustrate this with a simple
example.
Suppose we are developing coverage for the En-
glish ? Spanish version of the system, and that
the English corpus sentence ?does the pain occur at
night? fails to translate. The acquisition tool first
notes that processing fails when converting from in-
terlingua to Spanish. The interlingua representation
is
[[utterance_type,ynq],
[pronoun,you],
[state,have_symptom],
[symptom,pain],[tense,present],
[prep,in_time],[time,night]]
Applying Interlingua ? Spanish rules, the result is
[[utterance_type,ynq],
[pronoun,usted],
[state,tener],[symptom,dolor],
[tense,present],
[prep,por_temporal],
failed:[time,night]]
where the tag failed indicates that the element
[time,night] could not be processed. The tool
matches the incomplete transferred representation
against a set of correctly translated examples, and
shows the developer the English and Spanish strings
for the three most similar ones, here
does it appear in the morning
-> tiene el dolor por la man?ana
does the pain appear in the morning
-> tiene el dolor por la man?ana
does the pain come in the morning
-> tiene el dolor por la man?ana
This suggests that a translation for ?does the pain
occur at night? consistent with the existing rules
would be ?tiene el dolor por la noche?. The devel-
oper gives this example to the system, which parses
it using both the general Spanish resource grammar
and the specialised grammar used for generation in
the headache domain. The specialised grammar fails
to produce an analysis, while the resource grammar
produces two analyses,
[[utterance_type,ynq],
[pronoun,usted],
[state,tener],[symptom,dolor],
[[utterance_type,ynq],[pronoun,you],[state,have_symptom],
[tense,present],[symptom,headache],[sc,when],
[[clause,[[utterance_type,dcl],[pronoun,you],
[action,drink],[tense,present],[cause,coffee]]]]
Figure 1: Representation of ?do you get headaches when you drink coffee?
[tense,present],
[prep,por_temporal],
[temporal,noche]]
and
[[utterance_type,dcl],
[pronoun,usted],
[state,tener],[symptom,dolor],
[tense,present],
[prep,por_temporal],
[temporal,noche]]
The first of these corresponds to the YN-question
reading of the sentence (?do you have the pain at
night?), while the second is the declarative reading
(?you have the pain at night?). Since the first (YN-
question) reading matches the Interlingua represen-
tation better, the acquisition tool assumes that it is
the intended one. It can now suggest two pieces of
information to extend the system?s coverage.
First, it adds the YN-question reading of ?tiene
el dolor por la noche? to the corpus used to train
the specialised generation grammar. The piece
of information acquired from this example is that
[temporal,noche] should be realised in this
domain as ?la noche?. Second, it compares the cor-
rect Spanish representation with the incomplete one
produced by the current set of rules, and induces a
new Interlingua to Spanish translation rule. This will
be of the form
[time,night] -> [temporal,noche]
In the demo, we will show how the development
environment makes it possible to quickly add new
coverage to the system, while also checking that old
coverage is not broken.
References
P. Bouillon, M. Rayner, N. Chatzichrisafis, B.A. Hockey,
M. Santaholma, M. Starlander, Y. Nakao, K. Kanzaki,
and H. Isahara. 2005. A generic multi-lingual open
source platform for limited-domain medical speech
translation. In In Proceedings of the 10th Conference
of the European Association for Machine Translation
(EAMT), Budapest, Hungary.
MedBridge, 2006. http://www.medtablet.com/index.html.
As of 15 March 2006.
MedSLT, 2005. http://sourceforge.net/projects/medslt/.
As of 15 March 2005.
Phraselator, 2006. http://www.phraselator.com. As of 15
March 2006.
K. Probst and L. Levin. 2002. Challenges in automatic
elicitation of a controlled bilingual corpus. In Pro-
ceedings of the 9th International Conference on The-
oretical and Methodological Issues in Machine Trans-
lation.
M. Rayner, B.A. Hockey, and J. Dowding. 2003. An
open source environment for compiling typed unifica-
tion grammars into speech recognisers. In Proceed-
ings of the 10th EACL (demo track), Budapest, Hun-
gary.
M. Rayner, P. Bouillon, N. Chatzichrisafis, B.A. Hockey,
M. Santaholma, M. Starlander, H. Isahara, K. Kankazi,
and Y. Nakao. 2005a. A methodology for comparing
grammar-based and robust approaches to speech un-
derstanding. In Proceedings of the 9th International
Conference on Spoken Language Processing (ICSLP),
Lisboa, Portugal.
M. Rayner, P. Bouillon, M. Santaholma, and Y. Nakao.
2005b. Representational and architectural issues in a
limited-domain medical speech translator. In Proceed-
ings of TALN/RECITAL, Dourdan, France.
M. Rayner, B.A. Hockey, and P. Bouillon. 2006. Putting
Linguistics into Speech Recognition: The Regulus
Grammar Compiler. CSLI Press, Chicago.
Regulus, 2006. http://sourceforge.net/projects/regulus/.
As of 15 March 2006.
S-MINDS, 2006. http://www.sehda.com. As of 15
March 2006.
M. Starlander, P. Bouillon, N. Chatzichrisafis, M. Santa-
holma, M. Rayner, B.A. Hockey, H. Isahara, K. Kan-
zaki, and Y. Nakao. 2005. Practicing controlled lan-
guage through a help system integrated into the medi-
cal speech translation system (MedSLT). In Proceed-
ings of the MT Summit X, Phuket, Thailand.
Proceedings of SPEECHGRAM 2007, pages 41?48,
Prague, Czech Republic, June 2007. c?2007 Association for Computational Linguistics
A Bidirectional Grammar-Based Medical Speech Translator
Pierrette Bouillon1, Glenn Flores2, Marianne Starlander1, Nikos Chatzichrisafis1
Marianne Santaholma1, Nikos Tsourakis1, Manny Rayner1,3, Beth Ann Hockey4
1 University of Geneva, TIM/ISSCO, 40 bvd du Pont-d?Arve, CH-1211 Geneva 4, Switzerland
Pierrette.Bouillon@issco.unige.ch
Marianne.Starlander@eti.unige.ch, Nikos.Chatzichrisafis@vozZup.com
Marianne.Santaholma@eti.unige.ch, Nikolaos.Tsourakis@issco.unige.ch
2 Medical College of Wisconsin, 8701 Watertown Plank Road, Milwaukee, WI 53226
gflores@mcw.edu
3 Powerset, Inc., 475 Brannan Street, San Francisco, CA 94107
manny@powerset.com
4 Mail Stop 19-26, UCSC UARC, NASA Ames Research Center, Moffett Field, CA 94035?1000
bahockey@ucsc.edu
Abstract
We describe a bidirectional version of the
grammar-based MedSLT medical speech
system. The system supports simple medi-
cal examination dialogues about throat pain
between an English-speaking physician and
a Spanish-speaking patient. The physician?s
side of the dialogue is assumed to consist
mostly of WH-questions, and the patient?s of
elliptical answers. The paper focusses on the
grammar-based speech processing architec-
ture, the ellipsis resolution mechanism, and
the online help system.
1 Background
There is an urgent need for medical speech trans-
lation systems. The world?s current population
of 6.6 billion speaks more than 6,000 languages
(Graddol, 2004). Language barriers are associated
with a wide variety of deleterious consequences in
healthcare, including impaired health status, a lower
likelihood of having a regular physician, lower rates
of mammograms, pap smears, and other preven-
tive services, non-adherence with medications, a
greater likelihood of a diagnosis of more severe psy-
chopathology and leaving the hospital against med-
ical advice among psychiatric patients, a lower like-
lihood of being given a follow-up appointment af-
ter an emergency department visit, an increased risk
of intubation among children with asthma, a greater
risk of hospital admissions among adults, an in-
creased risk of drug complications, longer medical
visits, higher resource utilization for diagnostic test-
ing, lower patient satisfaction, impaired patient un-
derstanding of diagnoses, medications, and follow-
up, and medical errors and injuries (Flores, 2005;
Flores, 2006). Nevertheless, many patients who
need medical interpreters do not get them. For ex-
ample, in the United States, where 52 million peo-
ple speak a language other than English at home
and 23 million people have limited English profi-
ciency (LEP) (Census, 2007), one study found that
about half of LEP patients presenting to an emer-
gency department were not provided with a medical
interpreter (Baker et al, 1996). There is thus a sub-
stantial gap between the need for and availability of
language services in health care, a gap that could be
bridged through effective medical speech translation
systems.
An ideal system would be able to interpret ac-
curately and flexibly between patients and health
care professionals, using unrestricted language and
a large vocabulary. A system of this kind is, un-
fortunately, beyond the current state of the art.
It is, however, possible, using today?s technol-
ogy, to build speech translation systems for specific
scenarios and language-pairs, which can achieve
acceptable levels of reliability within the bounds
41
of a well-defined controlled language. MedSLT
(Bouillon et al, 2005) is an Open Source system
of this type, which has been under construction at
Geneva University since 2003. The system is built
on top of Regulus (Rayner et al, 2006), an Open
Source platform which supports development of
grammar-based speech-enabled applications. Regu-
lus has also been used to build several other systems,
including NASA?s Clarissa (Rayner et al, 2005b).
The most common architecture for speech trans-
lation today uses statistical methods to perform both
speech recognition and translation, so it is worth
clarifying why we have chosen to use grammar-
based methods. Even though statistical architec-
tures exhibit many desirable properties (purely data-
driven, domain independent), this is not necessar-
ily the best alternative in safety-critical medical ap-
plications. Anecdotally, many physicians express
reluctance to trust a translation device whose out-
put is not readily predictable, and most of the
speech translation systems which have reached the
stage of field testing rely on various types of
grammar-based recognition and rule-based transla-
tion (Phraselator, 2007; Fluential, 2007).
Statistical speech recognisers can achieve impres-
sive levels of accuracy when trained on enough data,
but it is a daunting task to collect training mate-
rial in the requisite quantities (usually, tens of thou-
sands of high-quality utterances) when trying to
build practical systems. Considering that the medi-
cal speech translation applications we are interested
in constructing here need to work for multiple lan-
guages and subdomains, the problem becomes even
more challenging. Our experience is that grammar-
based systems which also incorporate probabilistic
context-free grammar tuning deliver better results
than purely statistical ones when training data are
sparse (Rayner et al, 2005a).
Another common criticism of grammar-based
systems is that out-of-coverage utterances will
neither be recognized nor translated, an objec-
tion that critics have sometimes painted as de-
cisive. It is by no means obvious, however,
that restricted coverage is such a serious prob-
lem. In text processing, work on several gener-
ations of controlled language systems has devel-
oped a range of techniques for keeping users within
the bounds of system coverage (Kittredge, 2003;
Mitamura, 1999), and variants of these methods can
also be adapted for spoken language applications.
Our experiments with MedSLT show that even a
quite simple help system is enough to guide users
quickly towards the intended coverage of a medium-
vocabulary grammar-based speech translation appli-
cation, with most users appearing confident after just
an hour or two of exposure (Starlander et al, 2005;
Chatzichrisafis et al, 2006).
Until recently, the MedSLT system only sup-
ported unidirectional processing in the physician
to patient direction. The assumption was that the
physician would mostly ask yes/no questions, to
which the patient would respond non-verbally, for
example by nodding or shaking their head. A uni-
directional architecture is easier to make habitable
than a bidirectional one. It is reasonable to as-
sume that the physician will use the system regu-
larly enough to learn the coverage, but most patients
will not have used the system before, and it is less
clear that they will be able to acclimatize within the
narrow window at their disposal. These consider-
ations must however be balanced against the fact
that a unidirectional system does not allow for a
patient-centered interaction characterized by mean-
ingful patient-clinician communication or shared de-
cision making. Multiple studies in the medical lit-
erature document that patient-centeredness, effec-
tive patient-clinician communication, and shared de-
cision making are associated with significant im-
provements in patient health outcomes, including
reduced anxiety levels, improved functional sta-
tus, reduced pain, better control of diabetes melli-
tus, blood pressure reduction among hypertensives,
improved adherence, increased patient satisfaction,
and symptom reduction for a variety of conditions
(Stewart, 1995; Michie et al, 2003). A bidirectional
system is considered close to essential from a health-
care perspective, since it appropriately addresses the
key issues of patient centeredness and shared de-
cision making. For these reasons, we have over
the last few months developed a bidirectional ver-
sion of MedSLT, initially focussing on a throat pain
scenario with an English-speaking physician and a
Spanish-speaking patient. The physician uses full
sentences, while the patient answers with short re-
sponses.
One of the strengths of the Regulus approach is
42
that it is very easy to construct parallel versions of
a grammar; generally, all that is required is to vary
the training corpus. (We will have more to say about
this soon). We have exploited these properties of
the platform to create two different configurations
of the bidirectional system, so that we can compare
competing approaches to the problem of accommo-
dating patients unfamiliar with speech technology.
In Version 1 (less restricted), the patient is allowed
to answer using both elliptical utterances and short
sentences, while in Version 2 (more restricted) they
are only permitted to use elliptical utterances. Thus,
for example, if the physician asks the question ?How
long have you had a sore throat??, Version 1 allows
the patient to respond both ?Desde algunos d??as?
(?For several days?) and ?Me ha dolido la garganta
desde algunos d??as? (?I have had a sore throat for
several days?), while Version 2 only allows the first
of these. Both the short and the long versions are
translated uniformly, with the short version resolved
using the context from the preceding question.
In both versions, if the patient finds it too chal-
lenging to use the system to answer WH-questions
directly, it is possible to back off to the earlier di-
alogue architecture in which the physician uses Y-
N questions and the patient responds with simple
yes/no answers, or nonverbally. Continuing the ex-
ample, if the patient is unable to find an appro-
priate way to answer the physician?s question, the
physician could ask ?Have you had a sore throat for
more than three days??; if the patient responds nega-
tively, they could continue with the follow-on ques-
tion ?More than a week??, and so on.
In the rest of the paper, we first describe the
system top-level (Section 2), the way in which
grammar-based processing is used (Section 3), the
ellipsis processing mechanism (Section 4), and the
help system (Section 5). Section 6 presents an ini-
tial evaluation, and the final section concludes.
2 Top-level architecture
The system is operated through the graphical user
interface (GUI) shown in Figures 1 and 2. In
accordance with the basic principles of patient-
centeredness and shared decision-making outlined
in Section 1, the patient and the physician each have
their own headset, use their own mouse, and share
the same view of the screen. This is in sharp contrast
to the majority of the medical speech translation sys-
tems described in the literature (Somers, 2006).
As shown in the screenshots, the main GUI win-
dow is separated into two tabbed panes, marked
?Doctor? and ?Patient?. Initially, the ?Doctor? view
(the one shown in Figure 1) is active. The physician
presses the ?Push to talk? button, and speaks into
the headset microphone. If recognition is success-
ful, the GUI displays four separate results, listed on
the right side of the screen. At the top, immediately
under the heading ?Question?, we can see the actual
words returned by speech recognition. Here, these
words are ?Have you had rapid strep test?. Below,
we have the help pane: this displays similar ques-
tions taken from the help corpus, which are known to
be within system coverage. The pane marked ?Sys-
tem understood? shows a back-translation, produced
by first translating the recognition result into inter-
lingua, and then translating it back into English. In
the present example, this corrects the minor mistake
the recogniser has made, missing the indefinite ar-
ticle ?a?, and confirms that the system has obtained
a correct grammatical analysis and interpretation at
the level of interlingua. At the bottom, we see the
target language translation. The left-hand side of the
screen logs the history of the conversation to date, so
that both sides can refer back to it.
If the physician decides that the system has cor-
rectly understood what they said, they can now press
the ?Play? button. This results in the system produc-
ing a spoken output, using the Vocalizer TTS engine.
Simultaneously with speaking, the GUI shifts to the
?Patient? configuration shown in Figure 2. This dif-
fers from the ?Doctor? configuration in two respects:
all text is in the patient language, and the help pane
presents its suggestions immediately, based on the
preceding physician question. The various process-
ing components used to support these functionalities
are described in the following sections.
3 Grammar-based processing
Grammar-based processing is used for source-
language speech recognition and target-side genera-
tion. (Source-language analysis is part of the recog-
nition process, since grammar-based recognition in-
cludes creating a parse). All of these functionalities
43
Figure 1: Screenshot showing the state of the GUI after the physician has spoken, but before he has pressed
the ?Play? button. The help pane shows similar queries known to be within coverage.
Figure 2: Screenshot showing the state of the GUI after the physician has pressed the ?Play? button. The
help pane shows known valid responses to similar questions.
44
are implemented using the Regulus platform, with
the task-specific grammars compiled out of general
feature grammar resources by the Regulus tools. For
both recognition and generation, the first step is
to extract a domain-specific feature grammar from
the general one, using a version of the Explanation
Based Learning (EBL) algorithm.
The extraction process is driven by a corpus of ex-
amples and a set of ?operationality criteria?, which
define how the rules in the original resource gram-
mar are recombined into domain-specific ones. It is
important to realise that the domain-specific gram-
mar is not merely a subset of the resource grammar;
a typical domain-specific grammar rule is created by
merging two to five resource grammar rules into a
single ?flatter? rule. The result is a feature gram-
mar which is less general than the original one, but
more efficient. For recognition, the grammar is then
processed further into a CFG language model, using
an algorithm which alternates expansion of feature
values and filtering of the partially expanded gram-
mar to remove irrelevant rules. Detailed descrip-
tions of the EBL learning and feature grammar ?
CFG compilation algorithms can be found in Chap-
ters 8 and 10 of (Rayner et al, 2006). Regulus fea-
ture grammars can also be compiled into generators
using a version of the Semantic Head Driven algo-
rithm (Shieber et al, 1990).
The English (physician) side recogniser is com-
piled from the large English resource grammar de-
scribed in Chapter 9 of (Rayner et al, 2006), and
was constructed in the same way as the one de-
scribed in (Rayner et al, 2005a), which was used for
a headache examination task. The operationality cri-
teria are the same, and the only changes are a differ-
ent training corpus and the addition of new entries
to the lexicon. The same resources, with a differ-
ent training corpus, were used to build the English
language generator. It is worth pointing out that, al-
though a uniform method was used to build these
various grammars, the results were all very differ-
ent. For example, the recognition grammar from
(Rayner et al, 2005a) is specialised to cover only
second-person questions (?Do you get headaches
in the mornings??), while the generator grammar
used in the present application covers only first-
person declarative statements (?I visited the doctor
last Monday.?). In terms of structure, each gram-
mar contains several important constructions that the
other lacks. For example, subordinate clauses are
central in the headache domain (?Do the headaches
occur when you are stressed??) but are not present
in the sore throat domain; this is because the stan-
dard headache examination questions mostly focus
on generic conditions, while the sore throat exami-
nation questions only relate to concrete ones. Con-
versely, relative clauses are important in the sore
throat domain (?I have recently been in contact with
someone who has strep throat?), but are not suffi-
ciently important in the headache domain to be cov-
ered there.
On the Spanish (patient) side, there are four
grammars involved. For recognition, we have
two different grammars, corresponding to the two
versions of the system; the grammar for Ver-
sion 2 is essentially a subset of that for Version
1. For generation, there are two separate and
quite different grammars: one is used for trans-
lating the physician?s questions, while the other
produces back-translations of the patient?s ques-
tions. All of these grammars are extracted from
a general shared resource grammar for Romance
languages, which currently combines rules for
French, Spanish and Catalan (Bouillon et al, 2006;
Bouillon et al, to appear 2007b).
One interesting consequence of our methodology
is related to the fact that Spanish is a prodrop lan-
guage, which implies that many sentences are sys-
tematically ambiguous between declarative and Y-N
question readings. For example, ?He consultado un
me?dico? could in principle mean either ?I visited a
doctor? or ?Did I visit a doctor??. When training the
specialised Spanish grammars, it is thus necessary to
specify which readings of the training sentences are
to be used. Continuing the example, if the sentence
occurred in training material for the answer gram-
mar, we would specify that the declarative reading
was the intended one1.
4 Ellipsis processing and contextual
interpretation
In Version 1 of the system, the patient is per-
mitted to answer using elliptical phrases; in Ver-
1The specification can be formulated as a preference that
applies uniformly to all the training examples in a given group.
45
sion 2, she is obliged to do so. Ability to pro-
cess elliptical responses makes it easier to guide the
patient towards the intended coverage of the sys-
tem, without degrading the quality of recognition
(Bouillon et al, to appear 2007a). The downside is
that ellipses are also harder to translate than full sen-
tences. Even in a limited domain like ours, and in a
closely related language-pair, ellipsis can generally
not be translated word for word, and it is necessary
to look at the preceding context if the rules are to
be applied correctly. In examples 1 and 2 below,
the locative phrase ?In your stomach? in the English
source becomes the subject in the Spanish transla-
tion. This implies that the translation of the ellipsis
in the second physician utterance needs to change
syntactic category: ?In your head? (PP) becomes
?La cabeza? (NP).
(1) Doctor: Do you have a pain in your
stomach?
(Trans): Le duele el estomago?
(2) Doctor: In your head?
(Trans): *En la cabeza?
Since examples like this are frequent, our sys-
tem implements a solution in which the patient?s
replies are translated in the context of the preced-
ing utterance. If the patient-side recogniser?s output
is classified as an ellipsis (this can done fairly reli-
ably thanks to use of suitably specialised grammars;
cf. Section 3), we expand the incomplete phrase
into a full sentence structure by adding appropriate
structural elements from the preceding physician-
side question; the expanded semantic structure is the
one which is then translated into interlingual form,
and thence back to the physician-side language.
Since all linguistic representations, including
those of elliptical phrases and their contexts, are rep-
resented as flat attribute-value lists, we are able to
implement the resolution algorithm very simply in
terms of list manipulation. In YN-questions, where
the elliptical answer intuitively adds information to
the question (?Did you visit the doctor??; ?El lunes?
? ?I visited the doctor on Monday?), the repre-
sentations are organised so that resolution mainly
amounts to concatenation of the two lists2. In WH-
questions, where the answer intuitively substitutes
the elliptical answer for the WH-phrase (?What is
2It is also necessary to replace second-person pronouns with
first-person counterparts.
your temperature??; ?Cuarenta grados?? ?My tem-
perature is forty degrees?), resolution substitutes the
representation of the elliptical phrase for that of a
semantically similar element in the question.
The least trivial aspect of this process is provid-
ing a suitable definition of ?semantically similar?.
This is done using a simple example-based method,
in which the grammar developer writes a set of dec-
larations, each of which lists a set of semantically
similar NPs. At compile-time, the grammar is used
to parse each NP, and extract a generalised skele-
ton, in which specific lexical information is stripped
away; at run-time, two NPs are held to be semanti-
cally similar if they can each be unified with skele-
tons in the same equivalence class. This ensures that
the definition of the semantic similarity relation is
stable across most changes to the grammar and lex-
icon. The issues are described in greater detail in
(Bouillon et al, to appear 2007a).
5 Help system
Since the performance of grammar-based speech un-
derstanding is only reliable on in-coverage mate-
rial, systems based on this type of architecture must
necessarily use a controlled language approach, in
which it is assumed that the user is able to learn the
relevant coverage. As previously noted, the Med-
SLT system addresses this problem by incorporat-
ing an online help system (Starlander et al, 2005;
Chatzichrisafis et al, 2006).
On the physician side, the help system offers, af-
ter each recognition event, a list of related ques-
tions; similarly, on the patient side, it provides ex-
amples of known valid answers to the current ques-
tion. In both cases, the help examples are extracted
from a precompiled corpus of question-answer pairs,
which have been judged for correctness by system
developers. The process of selecting the examples
is slightly different on the two sides. For questions
(physician side), the system performs a second par-
allel recognition of the input speech, using a sta-
tistical recogniser. It then compares the recogni-
tion result, using an N-gram based metric, against
the set of known correct in-coverage questions from
the question-answer corpus, to extract the most sim-
ilar ones. For answers (patient side), the help sys-
tem searches the question-answer corpus to find the
46
questions most similar to the current one, and shows
the list of corresponding valid answers, using the
whole list in the case of Version 1 of the system, and
only the subset consisting of elliptical phrases in the
case of Version 2.
6 Evaluation
In previous studies, we have evaluated speech
recognition and speech understanding per-
formance for physician-side questions in
English (Bouillon et al, 2005) and Spanish
(Bouillon et al, to appear 2007b), and investi-
gated the impact on performance of the help system
(Rayner et al, 2005a; Starlander et al, 2005). We
have also carried out recent evaluations designed to
contrast recognition performance on elliptical and
full versions of the same utterance; here, our results
suggest that elliptical forms of (French-language)
MedSLT utterances are slightly easier to recognise
in terms of semantic error rate than full sentential
forms (Bouillon et al, to appear 2007a). Our initial
evaluation studies on the bidirectional system have
focussed on a specific question which has particular
relevance to this new version of MedSLT. Since
we are assuming that the patient will respond
using elliptical utterances, and that these utterances
will be translated in the context of the preceding
physician-side question, how confident can we
be that this context-dependent translation will be
correct?
In order to investigate these issues, we performed
a small data-collection using Version 2 of the sys-
tem, whose results we summarise here. One of the
authors of the paper played the role of an English-
speaking physician, in a simulated medical exam-
ination scenario where the goal was to determine
whether or not the ?patient? was suffering from a
viral throat infection. The six subjects playing the
role of the patient were all native speakers of Span-
ish, and had had no previous exposure to the system,
or indeed any kind of speech technology. They were
given cards describing the symptoms they were sup-
posed to be displaying, on which they were asked
to based their answers. From a total of 92 cor-
rectly recognised patient responses, we obtained 50
yes/no answers and 42 examples of real elliptical ut-
terances. Out of these, 36 were judged to have been
translated completely correctly, and a further 3 were
judged correct in terms of meaning, but less than flu-
ent. Only 3 examples were badly translated: of these
two were caused by problems in a translation rule,
and one by incorrect treatment of ellipsis resolution.
We show representative exchanges below; the last of
these is the one in which ellipsis processing failed to
work correctly.
(3) Doctor: For how long have you
had your sore throat?
Patient: Desde hace ma?s de
una semana
(Trans): I have had a sore
throat for more than one week
(4) Doctor: What were the results?
Patient: Negativo
(Trans): The results were negative
(5) Doctor: Have you seen a doctor
for your sore throat?
Patient: S?? el lunes
(Trans): I visited the doctor
for my sore throat monday
(6) Doctor: Have you been with anyone
recently who has a strep throat?
Patient: Si ma?s de dos semanas
(Trans): I was in contact with someone
more than two weeks recently
who had strep throat
7 Conclusions
We have presented a bidirectional grammar-based
English ? Spanish medical speech translation sys-
tem built using a linguistically motivated archi-
tecture, where all linguistic information is ulti-
mately derived from two resource grammars, one
for each language. We have shown how this en-
ables us to derive the multiple grammars needed,
which differ both with respect to function (recog-
nition/generation) and to domain (physician ques-
tions/patient answers). The system is currently un-
dergoing initial lab testing; we hope to advance to
initial trials on real patients some time towards the
end of the year.
References
[Baker et al1996] D.W. Baker, R.M. Parker, M.V.
Williams, W.C. Coates, and Kathryn Pitkin. 1996.
47
Use and effectiveness of interpreters in an emer-
gency department. Journal of the American Medical
Association, 275:783?8.
[Bouillon et al2005] P. Bouillon, M. Rayner,
N. Chatzichrisafis, B.A. Hockey, M. Santaholma,
M. Starlander, Y. Nakao, K. Kanzaki, and H. Isahara.
2005. A generic multi-lingual open source platform
for limited-domain medical speech translation. In
Proceedings of the 10th Conference of the European
Association for Machine Translation (EAMT), pages
50?58, Budapest, Hungary.
[Bouillon et al2006] P. Bouillon, M. Rayner, B. Novel-
las Vall, Y. Nakao, M. Santaholma, M. Starlander, and
N. Chatzichrisafis. 2006. Une grammaire multilingue
partage?e pour la traduction automatique de la parole.
In Proceedings of TALN 2006, Leuwen, Belgium.
[Bouillon et alto appear 2007a] P. Bouillon, M. Rayner,
M. Santaholma, and M. Starlander. to appear 2007a.
Les ellipses dans un syste`me de traduction automa-
tique de la parole. In Proceedings of TALN 2006,
Toulouse, France.
[Bouillon et alto appear 2007b] P. Bouillon, M. Rayner,
B. Novellas Vall, Y. Nakao, M. Santaholma, M. Star-
lander, and N. Chatzichrisafis. to appear 2007b. Une
grammaire partage?e multi-ta?che pour le traitement de
la parole : application aux langues romanes. Traite-
ment Automatique des Langues.
[Census2007] U.S. Census, 2007. Selected Social Char-
acteristics in the United States: 2005. Data Set: 2005
American Community Survey. Available here.
[Chatzichrisafis et al2006] N. Chatzichrisafis, P. Bouil-
lon, M. Rayner, M. Santaholma, M. Starlander, and
B.A. Hockey. 2006. Evaluating task performance for
a unidirectional controlled language medical speech
translation system. In Proceedings of the HLT-NAACL
International Workshop on Medical Speech Transla-
tion, pages 9?16, New York.
[Flores2005] G. Flores. 2005. The impact of medical in-
terpreter services on the quality of health care: A sys-
tematic review. Medical Care Research and Review,
62:255?299.
[Flores2006] G. Flores. 2006. Language barriers to
health care in the united states. New England Journal
of Medicine, 355:229?231.
[Fluential2007] Fluential, 2007.
http://www.fluentialinc.com. As of 24 March
2007.
[Graddol2004] D. Graddol. 2004. The future of lan-
guage. Science, 303:1329?1331.
[Kittredge2003] R. I. Kittredge. 2003. Sublanguages and
comtrolled languages. In R. Mitkov, editor, The Ox-
ford Handbook of Computational Linguistics, pages
430?447. Oxford University Press.
[Michie et al2003] S. Michie, J. Miles, and J. Weinman.
2003. Patient-centeredness in chronic illness: what is
it and does it matter? Patient Education and Counsel-
ing, 51:197?206.
[Mitamura1999] T. Mitamura. 1999. Controlled lan-
guage for multilingual machine translation. In Pro-
ceedings of Machine Translation Summit VII, Singa-
pore.
[Phraselator2007] Phraselator, 2007.
http://www.voxtec.com/. As of 24 March 2007.
[Rayner et al2005a] M. Rayner, P. Bouillon,
N. Chatzichrisafis, B.A. Hockey, M. Santaholma,
M. Starlander, H. Isahara, K. Kanzaki, and Y. Nakao.
2005a. A methodology for comparing grammar-based
and robust approaches to speech understanding. In
Proceedings of the 9th International Conference
on Spoken Language Processing (ICSLP), pages
1103?1107, Lisboa, Portugal.
[Rayner et al2005b] M. Rayner, B.A. Hockey, J.M. Ren-
ders, N. Chatzichrisafis, and K. Farrell. 2005b. A
voice enabled procedure browser for the International
Space Station. In Proceedings of the 43rd Annual
Meeting of the Association for Computational Linguis-
tics (interactive poster and demo track), Ann Arbor,
MI.
[Rayner et al2006] M. Rayner, B.A. Hockey, and
P. Bouillon. 2006. Putting Linguistics into Speech
Recognition: The Regulus Grammar Compiler. CSLI
Press, Chicago.
[Shieber et al1990] S. Shieber, G. van Noord, F.C.N.
Pereira, and R.C. Moore. 1990. Semantic-head-driven
generation. Computational Linguistics, 16(1).
[Somers2006] H. Somers. 2006. Language engineering
and the path to healthcare: a user-oriented view. In
Proceedings of the HLT-NAACL International Work-
shop on Medical Speech Translation, pages 32?39,
New York.
[Starlander et al2005] M. Starlander, P. Bouillon,
N. Chatzichrisafis, M. Santaholma, M. Rayner, B.A.
Hockey, H. Isahara, K. Kanzaki, and Y. Nakao. 2005.
Practising controlled language through a help system
integrated into the medical speech translation system
(MedSLT). In Proceedings of MT Summit X, Phuket,
Thailand.
[Stewart1995] M.A. Stewart. 1995. Effective physician-
patient communication and health outcomes: a review.
Canadian Medical Association Journal, 152:1423?
1433.
48
Coling 2008: Proceedings of the workshop on Speech Processing for Safety Critical Translation and Pervasive Applications, pages 32?35
Manchester, August 2008
The 2008 MedSLT System
Manny Rayner1, Pierrette Bouillon1, Jane Brotanek2, Glenn Flores2
Sonia Halimi1, Beth Ann Hockey3, Hitoshi Isahara4, Kyoko Kanzaki4
Elisabeth Kron5, Yukie Nakao6, Marianne Santaholma1
Marianne Starlander1, Nikos Tsourakis1
1 University of Geneva, TIM/ISSCO, 40 bvd du Pont-d?Arve, CH-1211 Geneva 4, Switzerland
{Emmanuel.Rayner,Pierrette.Bouillon,Nikolaos.Tsourakis}@issco.unige.ch
{Sonia.Halimi,Marianne.Santaholma,Marianne.Starlander}@eti.unige.ch
2 UT Southwestern Medical Center, Children?s Medical Center of Dallas
{Glenn.Flores,Jane.Brotanek}@utsouthwestern.edu
3 Mail Stop 19-26, UCSC UARC, NASA Ames Research Center, Moffett Field, CA 94035?1000
bahockey@ucsc.edu
4 NICT, 3-5 Hikaridai, Seika-cho, Soraku-gun, Kyoto, Japan 619-0289
{isahara,kanzaki}@nict.go.jp
5 3 St Margarets Road, Cambridge CB3 0LT, England
elisabethkron@yahoo.co.uk
6 University of Nantes, LINA, 2, rue de la Houssinie`re, BP 92208 44322 Nantes Cedex 03
yukie.nakao@univ-nantes.fr
Abstract
MedSLT is a grammar-based medical
speech translation system intended for
use in doctor-patient diagnosis dialogues,
which provides coverage of several dif-
ferent subdomains and multiple language
pairs. Vocabulary ranges from about 350 to
1000 surface words, depending on the lan-
guage and subdomain. We will demo three
different versions of the system: an any-
to-any multilingual version involving the
languages Japanese, English, French and
Arabic, a bidirectional English ? Span-
ish version, and a mobile version run-
ning on a hand-held PDA. We will also
demo the Regulus development environ-
ment, focussing on features which sup-
port rapid prototyping of grammar-based
speech translation systems.
c
? 2008. Licensed under the Creative Commons
Attribution-Noncommercial-Share Alike 3.0 Unported li-
cense (http://creativecommons.org/licenses/by-nc-sa/3.0/).
Some rights reserved.
1 Introduction
MedSLT is a medium-vocabulary grammar-based
medical speech translation system built on top of
the Regulus platform (Rayner et al, 2006). It is
intended for use in doctor-patient diagnosis dia-
logues, and provides coverage of several subdo-
mains and a large number of different language-
pairs. Coverage is based on standard examina-
tion questions obtained from physicians, and fo-
cusses primarily on yes/no questions, though there
is also support for WH-questions and elliptical ut-
terances.
Detailed descriptions of MedSLT can be found
in earlier papers (Bouillon et al, 2005; Bouil-
lon et al, 2008)1. In the rest of this note, we
will briefly sketch several versions of the system
that we intend to demo at the workshop, each of
which displays new features developed over the
last year. Section 2 describes an any-language-to-
any-language multilingual version of the system;
Section 3, a bidirectional English ? Spanish ver-
sion; Section 4, a version running on a mobile PDA
1All MedSLT publications are available on-line
at http://www.issco.unige.ch/projects/
medslt/publications.shtml.
32
platform; and Section 5, the Regulus development
environment.
2 A multilingual version
During the last few months, we have reorganised
the MedSLT translation model in several ways2. In
particular, we give a much more central role to the
interlingua; we now treat this as a language in its
own right, defined by a normal Regulus grammar,
and using a syntax which essentially amounts to
a greatly simplified form of English. Making the
interlingua into another language has made it easy
to enforce tight constraints on well-formedness of
interlingual semantic expressions, since checking
well-formedness now just amounts to performing
generation using the interlingua grammar.
Another major advantage of the scheme is that
it is also possible to systematise multilingual de-
velopment, and only work with translation from
source language to interlingua, and from interlin-
gua to target language; here, the important point
is that the human-readable interlingua surface syn-
tax makes it feasible in practice to evaluate transla-
tion between normal languages and the interlingua.
Development of rules for translation to interlingua
is based on appropriate corpora for each source
language. Development of rules for translating
from interlingua uses a corpus which is formed by
merging together the results of translating each of
the individual source-language corpora into inter-
lingua.
We will demonstrate our new capabilities in
interlingua-based translation, using a version of
the system which translates doctor questions in the
headache domain from any language to any lan-
guage in the set {English, French, Japanese, Ara-
bic}. Table 1 gives examples of the coverage of the
English-input headache-domain version, and Ta-
ble 2 summarises recognition performance in this
domain for the three input languages where we
have so far performed serious evaluations. Differ-
ences in the sizes of the recognition vocabularies
are primarily due to differences in use of inflec-
tion.
3 A bidirectional version
The system from the preceding section is unidi-
rectional; all communication is in the doctor-to-
patient direction, the expectation being that the pa-
2The ideas in the section are described at greater length in
(Bouillon et al, 2008).
Language Vocab WER SemER
English 447 6% 11%
French 1025 8% 10%
Japanese 422 3% 4%
Table 2: Recognition performance for English,
French and Japanese headache-domain recognis-
ers. ?Vocab? = number of surface words in source
language recogniser vocabulary; ?WER? = Word
Error Rate for source language recogniser, on in-
coverage material; ?SemER? = semantic error rate
for source language recogniser, on in-coverage
material.
tient will respond non-verbally. Our second demo,
an early version of which is described in (Bouillon
et al, 2007), supports bidirectional translation for
the sore throat domain, in the English ? Spanish
pair. Here, the English-speaking doctor typically
asks WH-questions, and the Spanish-speaking pa-
tient responds with elliptical utterances, which are
translated as full sentence responses. A short ex-
ample dialogue is shown in Table 3.
Doctor: Where is the pain?
?Do?nde le duele?
Patient: En la garganta.
I experience the pain in my throat.
Doctor: How long have you had a pain
in your throat?
?Desde cua?ndo le duele la garganta?
Patient: Ma?s de tres d??as.
I have experienced the pain in my
throat for more than three days.
Table 3: Short dialogue with bidirectional English
? Spanish version. System translations are in ital-
ics.
4 A mobile platform version
When we have shown MedSLT to medical profes-
sionals, one of the most common complaints has
been that a laptop is not an ideal platform for use
in emergency medical situations. Our third demo
shows an experimental version of the system us-
ing a client/server architecture. The client, which
contains the user interface, runs on a Nokia Linux
N800 Internet Tablet; most of the heavy process-
ing, including in particular speech recognition, is
hosted on the remote server, with the nodes com-
municating over a wireless network. A picture of
33
Where? Is the pain above your eye?
When? Have you had the pain for more than a month?
How long? Does the pain typically last a few minutes?
How often? Do you get headaches several times a week?
How? Is it a stabbing pain?
Associated symptoms? Do you vomit when you get the headaches?
Why? Does bright light make the pain worse?
What helps? Does sleep make the pain better?
Background? Do you have a history of sinus disease?
Table 1: Examples of English MedSLT coverage
the tablet, showing the user interface, is presented
in Figure 1. The sentences appearing under the
back-translation at the top are produced by an on-
line help component, and are intended to guide the
user into the grammar?s coverage (Chatzichrisafis
et al, 2006).
The architecture is described further in
(Tsourakis et al, 2008), which also gives perfor-
mance results for another Regulus applications.
These strongly suggest that recognition perfor-
mance in the client/server environment is no
worse than on a laptop, as long as a comparable
microphone is used.
5 The development environment
Our final demo highlights the new Regulus devel-
opment environment (Kron et al, 2007), which has
over the last few months acquired a large amount
of new functionality designed to facilitate rapid
prototyping of spoken language applications3 . The
developer initially constructs and debugs her com-
ponents (grammar, translation rules etc) in a text
view. As soon as they are consistent, she is able
to compile the source-language grammar into a
recogniser, and combine this with other compo-
nents to run a complete speech translation system
within the development environment. Connections
between components are defined by a simple con-
fig file. Figure 2 shows an example.
References
Bouillon, P., M. Rayner, N. Chatzichrisafis, B.A.
Hockey, M. Santaholma, M. Starlander, Y. Nakao,
K. Kanzaki, and H. Isahara. 2005. A generic multi-
lingual open source platform for limited-domain
medical speech translation. In Proceedings of the
10th Conference of the European Association for
3This work is presented in a paper currently under review.
Machine Translation (EAMT), pages 50?58, Bu-
dapest, Hungary.
Bouillon, P., G. Flores, M. Starlander,
N. Chatzichrisafis, M. Santaholma, N. Tsourakis,
M. Rayner, and B.A. Hockey. 2007. A bidirectional
grammar-based medical speech translator. In Pro-
ceedings of the ACL Workshop on Grammar-based
Approaches to Spoken Language Processing, pages
41?48, Prague, Czech Republic.
Bouillon, P., S. Halimi, Y. Nakao, K. Kanzaki, H. Isa-
hara, N. Tsourakis, M. Starlander, B.A. Hockey, and
M. Rayner. 2008. Developing non-european trans-
lation pairs in a medium-vocabulary medical speech
translation system. In Proceedings of LREC 2008,
Marrakesh, Morocco.
Chatzichrisafis, N., P. Bouillon, M. Rayner, M. Santa-
holma, M. Starlander, and B.A. Hockey. 2006. Eval-
uating task performance for a unidirectional con-
trolled language medical speech translation system.
In Proceedings of the HLT-NAACL International
Workshop on Medical Speech Translation, pages 9?
16, New York.
Kron, E., M. Rayner, P. Bouillon, and M. Santa-
holma. 2007. A development environment for build-
ing grammar-based speech-enabled applications. In
Proceedings of the ACL Workshop on Grammar-
based Approaches to Spoken Language Processing,
pages 49?52, Prague, Czech Republic.
Rayner, M., B.A. Hockey, and P. Bouillon. 2006.
Putting Linguistics into Speech Recognition: The
Regulus Grammar Compiler. CSLI Press, Chicago.
Tsourakis, N., M. Georghescul, P. Bouillon, and
M. Rayner. 2008. Building mobile spoken dialogue
applications using regulus. In Proceedings of LREC
2008, Marrakesh, Morocco.
34
Figure 1: Mobile version of the MedSLT system, running on a Nokia tablet.
Figure 2: Speech to speech translation from the development environment, using a Japanese to Arabic
translator built from MedSLT components. The user presses the Recognise button (top right), speaks in
Japanese, and receives a spoken translation in Arabic together with screen display of various processing
results. The application is defined by a config file which combines a Japanese recogniser and analy-
sis grammar, Japanese to Interlingua and Interlingua to Arabic translation rules, an Arabic generation
grammar, and recorded Arabic wavfiles used to construct a spoken result.
35
Coling 2008: Proceedings of the workshop on Speech Processing for Safety Critical Translation and Pervasive Applications, pages 60?63
Manchester, August 2008
A Small-Vocabulary Shared Task for Medical Speech Translation
Manny Rayner1, Pierrette Bouillon1, Glenn Flores2, Farzad Ehsani3
Marianne Starlander1, Beth Ann Hockey4, Jane Brotanek2, Lukas Biewald5
1 University of Geneva, TIM/ISSCO, 40 bvd du Pont-d?Arve, CH-1211 Geneva 4, Switzerland
{Emmanuel.Rayner,Pierrette.Bouillon}@issco.unige.ch
Marianne.Starlander@eti.unige.ch
2 UT Southwestern Medical Center, Children?s Medical Center of Dallas
{Glenn.Flores,Jane.Brotanek}@utsouthwestern.edu
3 Fluential, Inc, 1153 Bordeaux Drive, Suite 211, Sunnyvale, CA 94089, USA
farzad@fluentialinc.com
4 Mail Stop 19-26, UCSC UARC, NASA Ames Research Center, Moffett Field, CA 94035?1000
bahockey@ucsc.edu
5 Dolores Labs
lukeab@gmail.com
Abstract
We outline a possible small-vocabulary
shared task for the emerging medical
speech translation community. Data would
consist of about 2000 recorded and tran-
scribed utterances collected during an eval-
uation of an English ? Spanish version
of the Open Source MedSLT system; the
vocabulary covered consisted of about 450
words in English, and 250 in Spanish. The
key problem in defining the task is to agree
on a scoring system which is acceptable
both to medical professionals and to the
speech and language community. We sug-
gest a framework for defining and admin-
istering a scoring system of this kind.
1 Introduction
In computer science research, a ?shared task? is a
competition between interested teams, where the
goal is to achieve as good performance as possible
on a well-defined problem that everyone agrees to
work on. The shared task has three main compo-
nents: training data, test data, and an evaluation
metric. Both test and training data are divided
up into sets of items, which are to be processed.
c
? 2008. Licensed under the Creative Commons
Attribution-Noncommercial-Share Alike 3.0 Unported li-
cense (http://creativecommons.org/licenses/by-nc-sa/3.0/).
Some rights reserved.
The evaluation metric defines a score for each pro-
cessed item. Competitors are first given the train-
ing data, which they use to construct and/or train
their systems. They are then evaluated on the test
data, which they have not previously seen.
In many areas of speech and language process-
ing, agreement on a shared task has been a major
step forward. Often, it has in effect created a new
subfield, since it allows objective comparison of
results between different groups. For example, it
is very common at speech conference to have spe-
cial sessions devoted to recognition within a par-
ticular shared task database. In fact, a conference
without at least a couple of such sessions would
be an anomaly. A recent success story in language
processing is the Recognizing Textual Entailment
(RTE) task1. Since its inception in 2004, this has
become extremely popular; the yearly RTE work-
shop now attracts around 40 submissions, and error
rates on the task have more than halved.
Automatic medical speech translation would
clearly benefit from a shared task. As was made
apparent at the initial 2006 workshop in New
York2, nearly every group has both a unique ar-
chitecture and a unique set of data, essentially
making comparisons impossible. In this note, we
will suggest an initial small-vocabulary medical
1http://www.pascal-network.org/
Challenges/RTE/
2http://www.issco.unige.ch/pub/
SLT workshop proceedings book.pdf
60
shared task. The aspect of the task that is hard-
est to define is the evaluation metric, since there
unfortunately appears to be considerable tension
between the preferences of medical professionals
and speech system implementers. Medical profes-
sionals would prefer to carry out a ?deep? evalu-
ation, in terms of possible clinical consequences
following from a mistranslation. System evalua-
tors will on the other hand prefer an evaluation
method that can be carried out quickly, enabling
frequent evaluations of evolving systems. The plan
we will sketch out is intended to be a compromise
between these two opposing positions.
The rest of the note is organised as follows.
Section 2 describes the data we propose to use,
and Section 3 discusses our approach to evaluation
metrics. Section 4 concludes.
2 Data
The data we would use in the task is for the English
? Spanish language pair, and was collected us-
ing two different versions of the MedSLT system3.
In each case, the scenario imagines an English-
speaking doctor conducting a verbal examination
of a Spanish-speaking patient, who was assumed
to be have visited the doctor because they were
displaying symptoms which included a sore throat.
The doctor?s task was to use the translation sys-
tem to determine the likely reason for the patient?s
symptoms.
The two versions of the system differed in
terms of the linguistic coverage offered. The
more restricted version supported a minimal range
of English questions (vocabulary size, about 200
words), and only allowed the patient to respond
using short phrases (vocabulary size, 100 words).
Thus for example the doctor could ask ?How long
have you had a sore throat??, and the patient would
respond Hace dos d??as (?for two days?). The
less restricted version supported a broader range
of doctor questions (vocabulary size, about 450
words), and allowed the patient to respond using
both short phrases and complete sentences (vocab-
ulary size, about 225 words). Thus in response
to ?How long have you had a sore throat??, the
patient could say either Hace dos d??as (?for two
days?) or Tengo dolor en la garganta hace dos d??as
(?I have had a sore throat for two days?).
Data was collected in 64 sessions, carried out
3http://www.issco.unige.ch/projects/
medslt/
over two days in February 2008 at the University
of Texas Medical Center, Dallas. In each session,
the part of the ?doctor? was played by a real physi-
cian, and the part of the ?patient? by a Spanish-
speaking interpreter. This resulted in 1005 En-
glish utterances, and 967 Spanish utterances. All
speech data is available in SPHERE-headed form,
and totals about 90 MB. A master file, organised in
spreadsheet form, lists metadata for each recorded
file. This includes a transcription, a possible valid
translation (verified by a bilingual translator), IDs
for the ?doctor?, the ?patient?, the session and the
system version, and the preceding context. Con-
text is primarily required for short answers, and
consists of the most recent preceding doctor ques-
tion.
3 Evaluation metrics
The job of the evaluation component in the shared
task is to assign a score to each translated utter-
ance. Our basic model will be the usual one for
shared tasks in speech and language. Each pro-
cessed utterance will be assigned to a category;
each category will be associated with a specified
score; the score for a complete testset will the sum
of the scores for all of its utterances. We thus have
three sub-problems: deciding what the categories
are, deciding how to assign a category to a pro-
cessing utterance, and deciding what scores to as-
sociate with each category.
3.1 Defining categories
If the system attempts to translate an utterance,
there are a priori three things that can happen:
it can produce a correct translation, an incorrect
translation, or no translation. Medical speech
translation is a safety-critical problem; a mistrans-
lation may have serious consequences, up to and
including the death of the patient. This implies
that the negative score for an incorrect translation
should be high in comparison to the positive score
for a correct translation. So a naive scoring func-
tion might be ?1 point for a correct translation, 0
points for no translation, ?1000 points for an in-
correct translation.?
However, since the high negative score for a
mistranslation is justified by the possible serious
consequences, not all mistranslations are equal;
some are much more likely than others to result in
clinical consequences. For example, consider the
possible consequences of two different mistrans-
61
lations of the Spanish sentence La penicilina me
da alergias. Ideally, we would like the system to
translate this as ?I am allergic to penicillin?. If it
instead says ?I am allergic to the penicillin?, the
translation is slightly imperfect, but it is hard to see
any important misunderstanding arising as a result.
In contrast, the translation ?I am not allergic to
penicillin?, which might be produced as the result
of a mistake in speech recognition, could have very
serious consequences indeed. (Note in passing that
both errors are single-word insertions). Another
type of result is a nonsensical translation, perhaps
due to an internal system error. For instance, sup-
pose the translation of our sample sentence were
?The allergy penicillin does me?. In this case, it
is not clear what will happen. Most users will
probably dismiss the output as meaningless; a few
might be tempted to try and decipher it, with un-
predictable results.
Examples like these show that it is important for
the scoring metric to differentiate between differ-
ent classes of mistranslations, with the differentia-
tion based on possible clinical consequences of the
error. For similar reasons, it is important to think
about the clinical consequences when the system
produces correct translations, or fails to produce
a translation. For example, when the system cor-
rectly translates ?Hello? as Buenas d??as, there are
not likely to be any clinical consequences, so it is
reasonable to reward it with a lower score than the
one assigned to a clinically contentful utterance.
When no translation is produced, it also seems cor-
rect to distinguish the case where the user was able
recover by a suitably rephrasing the utterance from
the one where they simply gave up. For example,
if the system failed to translate ?How long has this
cough been troubling you??, but correctly handled
the simpler formulation ?How long have you had a
cough??, we would give this a small positive score,
rather than a simple zero.
Summarising, we propose to classify transla-
tions into the following seven categories:
1. Perfect translation, useful clinical conse-
quences.
2. Perfect translation, no useful clinical conse-
quences.
3. Imperfect translation, but not dangerous in
terms of clinical consequences.
4. Imperfect translation, potentially dangerous.
5. Nonsense.
6. No translation produced, but later rephrased
in a way the system handled adequately.
7. No translation produced, but not rephrased in
a way the system handled adequately.
3.2 Assigning utterances to categories
At the moment, medical professionals will only
accept the validity of category assignments made
by trained physicians. In the worst case, it is
clearly true that a layman, even one who has re-
ceived some training, will not be able to determine
whether or not a mistranslation has clinical signif-
icance.
Physician time is, however, a scarce and valu-
able resource, and, as usual, typical case and worst
case may be very different. Particularly for routine
testing during system development, it is clearly not
possible to rely on expert physician assessments.
We consequently suggest a compromise strategy.
We will first carry out an evaluation using medical
experts, in order to establish a gold standard. We
will then repeat this evaluation using non-experts,
and determine how large the differential is in prac-
tice.
We initially intend to experiment with two dif-
ferent groups of non-experts. At Geneva Uni-
versity, we will use students from the School of
Translation. These students will be selected for
competence in English and Spanish, and will re-
ceive a few hours of training on determination of
clinical significance in translation, using guide-
lines developed in collaboration with Glenn Flores
and his colleagues at the UT Southwestern Medi-
cal Center, Texas. Given that the corpus material
is simple and sterotypical, we think that this ap-
proach should yield a useful approximation to ex-
pert judgements.
Although translation students are far cheaper
than doctors, they are still quite expensive, and
evaluation turn-around will be slow. For these rea-
sons, we also propose to investigate the idea of per-
forming evaluations using Amazon?s Mechanical
Turk4. This will be done by Dolores Labs, a new
startup specialising in Turk-based crowdsourcing.
3.3 Scores for categories
We have not yet agreed on exact scores for the
different categories, and this is something that is
4http://www.mturk.com/mturk/welcome
62
probably best decided after mutual discussion at
the workshop. Some basic principles will be evi-
dent from the preceding discussion. The scale will
be normalised so that failure to produce a trans-
lation is counted as zero; potentially dangerous
mistranslations will be associated with a negative
score large in comparison to the positive score for
a useful correct translation. Inability to communi-
cate can certainly be dangerous (this is the point of
having a translation system in the first place), but
mistakenly believing that one has communicated
is usually much worse. As Mark Twain put it: ?It
ain?t what you don?t know that gets you into trou-
ble. It?s what you know for sure that just ain?t so?.
3.4 Discarding uncertain responses
Given that both speech recognition and machine
translation are uncertain technologies, a high
penalty for mistranslations means that systems
which attempt to translate everything may eas-
ily end up with an average negative score - in
other words, they would score worse than a system
which did nothing! For the shared task to be in-
teresting, we must address this problem, and in the
doctor to patient direction there is a natural way
to do so. Since the doctor can reasonably be as-
sumed to be a trained professional who has had
time to learn to operate the system, we can say that
he has the option of aborting any translation where
the machine does not appear to have understood
correctly.
We thus relativise the task with respect to a ?fil-
ter?: for each utterance, we produce both a transla-
tion in the target language, and a ?reference trans-
lation? in the source language, which in some way
gives information about what the machine has un-
derstood. The simplest way to produce this ?ref-
erence translation? is to show the words produced
by speech recognition. When scoring, we evaluate
both translations, and ignore all examples where
the reference translation is evaluated as incorrect.
To go back to the ?penicillin? example, suppose
that Spanish source-language speech recognition
has incorrectly recognised La penicilina me da
alergias as La penicilina no me da alergias. Even
if this produces the seriously incorrect translation
?I am not allergic to penicillin?, we can score it
as a zero rather than a negative, on the grounds
that the speech recognition result already shows
the Spanish-speaking doctor that something has
gone wrong before any translation has happened.
The reference translation may also be produced in
a more elaborate way; a common approach is to
translate back from the target language result into
the source language.
Although the ?filtered? version of the medical
speech translation task makes good sense in the
doctor to patient direction, it is less clear how
meaningful it is in the patient to doctor direction.
Most patients will not have used the system before,
and may be distressed or in pain. It is consequently
less reasonable to expect them to be able to pay at-
tention to the reference translation when using the
system.
4 Summary and conclusions
The preceding notes are intended to form a frame-
work which will serve as a basis for discussion at
the workshop. As already indicated, the key chal-
lenge here is to arrive at metrics which are ac-
ceptable to both the medical and the speech and
language community. This will certainly require
more negotiation. We are however encouraged by
the fact that the proposal, as presented here, has
been developed jointly by representatives of both
communities, and that we appear to be fairly near
agreement. Another important parameter which
we have intentionally left blank is the duration of
the task; we think it will be more productive to de-
termine this based on the schedules of interested
parties.
Realistically, the initial definition of the metric
can hardly be more than a rough guess. Experi-
mentation during the course of the shared task will
probably show that some adjustment will be desir-
able, in order to make it conform more closely to
the requirements of the medical community. If we
do this, we will, in the interests of fairness, score
competing systems using all versions of the metric.
63
