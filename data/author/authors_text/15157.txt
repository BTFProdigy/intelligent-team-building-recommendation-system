Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 844?853,
October 25-29, 2014, Doha, Qatar.
c?2014 Association for Computational Linguistics
Syllable weight encodes mostly the same information for English word
segmentation as dictionary stress
John K Pate Mark Johnson
Centre for Language Technology
Macquarie University
Sydney, NSW, Australia
{john.pate,mark.johnson}@mq.edu.au
Abstract
Stress is a useful cue for English word
segmentation. A wide range of computa-
tional models have found that stress cues
enable a 2-10% improvement in segmen-
tation accuracy, depending on the kind of
model, by using input that has been anno-
tated with stress using a pronouncing dic-
tionary. However, stress is neither invari-
ably produced nor unambiguously iden-
tifiable in real speech. Heavy syllables,
i.e. those with long vowels or syllable
codas, attract stress in English. We de-
vise Adaptor Grammar word segmentation
models that exploit either stress, or sylla-
ble weight, or both, and evaluate the util-
ity of syllable weight as a cue to word
boundaries. Our results suggest that sylla-
ble weight encodes largely the same infor-
mation for word segmentation in English
that annotated dictionary stress does.
1 Introduction
One of the first skills a child must develop in the
course of language acquisition is the ability to seg-
ment speech into words. Stress has long been
recognized as a useful cue for English word seg-
mentation, following the observation that words
in English are predominantly stress-initial (Cutler
and Carter, 1987), together with the result that 9-
month-old English-learning infants prefer stress-
initial stimuli (Jusczyk et al., 1993). A range of
statistical (Doyle and Levy, 2013; Christiansen et
al., 1998; B?orschinger and Johnson, 2014) and
rule-based (Yang, 2004; Lignos and Yang, 2010)
models have used stress information to improve
word segmentation. However, that work uses
stress-marked input prepared by marking vowels
that are listed as stressed in a pronouncing dic-
tionary. This pre-processing step glosses over the
fact that stress identification itself involves a non-
trival learning problem, since stress has many pos-
sible phonetic reflexes and no known invariants
(Campbell and Beckman, 1997; Fry, 1955; Fry,
1958). One known strong correlate of stress in
English is syllable weight: heavy syllables, which
end in a consonant or have a long vowel, at-
tract stress in English. We present experiments
with Bayesian Adaptor Grammars (Johnson et al.,
2007) that suggest syllable weight encodes largely
the same information for word segmentation that
dictionary stress information does.
Specifically, we modify the Adaptor
Grammar word segmentation model of
B?orschinger and Johnson (2014) to compare
the utility of syllable weight and stress cues for
finding word boundaries, both individually and in
combination. We describe how a shortcoming of
Adaptor Grammars prevents us from comparing
stress and weight cues in combination with the full
range of phonotactic cues for word segmentation,
and design two experiments to work around this
limitation. The first experiment uses grammars
that provide parallel analyses for syllable weight
and stress, and learns initial/non-initial phonotac-
tic distinctions. In this first experiment, syllable
weight cues are actually more useful than stress
cues at larger input sizes. The second experiment
focuses on incorporating phonotactic cues for
typical word-final consonant clusters (such as
inflectional morphemes), at the expense of parallel
structures. In this second experiment, weight cues
merely match stress cues at larger input sizes,
and the learning curve for the combined weight-
and-stress grammar follows almost perfectly with
the stress-only grammar. This second experiment
suggests that the advantage of weight over stress
in the first experiment was purely due to poor
modeling of word-final consonant clusters by
the stress-only grammar, not weight per se. All
together, these results indicate that syllable weight
844
is highly redundant with dictionary-based stress
for the purposes of English word segmentation;
in fact, in our experiments, there is no detectable
difference between relying on syllable weight and
relying on dictionary stress.
2 Background
Stress is the perception that some syllables are
more prominent than others, and reflects a com-
plex, language-specific interaction between acous-
tic cues (such as loudness and duration), and
phonological patterns (such as syllable shapes).
The details on how stress is assigned, produced,
and perceived vary greatly across languages.
Three aspects of the English stress system are
relevant for this paper. First, although English
stress can shift in different contexts (Liberman and
Prince, 1977), such as from the first syllable of
?fourteen? in isolation to the second syllable when
followed by a stressed syllable, it is largely stable
across different tokens of a given word. Second,
most words in English end up being stress-initial
on a type and token basis. Third, heavy syllables
(those with a long vowel or a consonant coda) at-
tract stress in English.
There is experimental evidence that English-
learning infants prefer stress-initial words from
around the age of seven months (Jusczyk et al.,
1993; Juszcyk et al., 1999; Jusczyk et al., 1993;
Thiessen and Saffran, 2003). A variety of com-
putational models have subsequently been devel-
oped that take stress-annotated input and use this
regularity to improve segmentation accuracy. The
earliest Simple Recurrent Network (SRN) mod-
eling experiments of Christiansen et al. (1998)
and Christiansen and Curtin (1999) found that
stress improved word segmentation from about
39% to 43% token f-score (see Evaluation). Ryt-
ting et al. (2010) applied the SRN model to prob-
ability distributions over phones obtained from a
speech recognition system, and found that the en-
tropy of the probability distribution over phones,
as a proxy to local hyperarticulation and hence a
stress cue, improved token f-score from about 16%
to 23%. In a deterministic approach using pre-
syllabified input, Yang (2004), with follow-ups in
Lignos and Yang (2010) and Lignos (2011; 2012),
showed that a ?Unique Stress Constraint? (USC),
or assuming each word has at most one stressed
syllable, leads to an improvement of about 2.5%
boundary f-score.
Among explicitly probabilistic models,
Doyle and Levy (2013) incorporated stress into
Goldwater et al.?s (2009) Bigram model. They
did this by modifying the base distribution over
lexical forms to generate not simply phone strings
but a sequence of syllables that may or may
not be stressed. The resulting model can learn
that some sequences of syllables (in particular,
sequences that start with a stressed syllable)
are more likely than others. However, observed
stress improved token f-score by only 1%.
B?orschinger and Johnson (2014) used Adaptor
Grammars (Johnson et al., 2007), a generalization
of Goldwater et al.?s (2009) Bigram model that
will be described shortly, and found a clearer
4-10% advantage in token f-score, depending on
the amount of training data.
Together, the experimental and computational
results suggest that infants in fact pay attention
to stress, and that stress carries useful information
for segmenting words in running speech. How-
ever, stress identification is itself a non-trivial
task, as stress has many highly variable, context-
sensitive, and optional phonetic reflexes. How-
ever, one strong phonological cue in English is
syllable weight: heavy syllables attract stress.
Heavy syllables, in turn, are syllables with a
coda and/or a long vowel, which, in English,
are tense vowels. Turk et al. (1995) replicated
the Jusczyk et al. (1993) finding that English-
learning infants prefer stress-initial stimuli (using
non-words), and then examined how stress inter-
acted with syllable weight. They found that sylla-
ble weight was not a necessary condition to trig-
ger the preference: infants preferred stress-initial
stimuli even if the initial syllable was light. How-
ever, they also found that infants most strongly
preferred stimuli whose first syllable was both
stressed and heavy: infants preferred stress-initial
and heavy-initial stimuli to stress-initial and light-
initial stimuli. This result suggests that infants are
sensitive to syllable weight in determining typical
stress and rythmic patterns in their language.
2.1 Models
We will adopt the Adaptor Grammar framework
used by B?orschinger and Johnson (2014) to ex-
plore the utility of syllable weight as a cue
to word segmentation by way of its covariance
with stress. Adaptor Grammars are Probabilis-
tic Context Free Grammars (PCFGs) with a spe-
845
Syll
Onset
k
Rhyme
Nucleus
?
Coda
ts
(a) Basic syllable.
SyllIF
OnsetI
k
RhymeIF
NucleusI
?
CodaF
ts
(b) Mono-syllable with initial Rhyme.
SyllIF
OnsetI
k
RhymeF
NucleusF
?
CodaF
ts
(c) Mono-syllable with final Rhyme.
Figure 1: Different ways to incorporate phonotactics. It is not possible to capture word-final codas and
word initial rhymes in monosyllabic words with factors the size of a PCFG rule.
cial set of adapted non-terminal nodes. We un-
derline adapted non-terminals (X) to distinguish
them from non-adapted non-terminals (Y). While
a vanilla PCFG can only directly model regular-
ities that are expressed by a single re-write rule,
an Adaptor Grammar model caches entire subtrees
that are rooted at adapted non-terminals. Adaptor
Grammars can thus learn the internal structure of
words, such as syllables, syllable onsets, and syl-
lable rhymes, while still learning entire words as
well.
In Adaptor Grammars, parameters are associ-
ated with PCFG rules. While this has been a useful
factorization in previous work, it makes it difficult
to integrate syllable weight and syllable stress in
a linguistically natural way. A syllable is typically
analyzed as having an optional onset followed by a
rhyme, with the rhyme rewriting to a nucleus (the
vowel) followed by an optional coda, as in Fig-
ure 1a. We expect stress and syllable weight to be
useful primarily because initial syllables tend to be
different from non-initial syllables. However, dis-
tinguishing final from non-final codas should be
useful as well, due to the frequency of suffixes in
English, and the importance of edge phenomena in
phonology more generally (Brent and Cartwright,
1996). These principles come into conflict when
modeling monosyllabic words. If we say that a
monosyllable is an Initial and Final SyllIF, and
has an initial Onset and an initial Rhyme, as in
Figure 1b, then we can learn the initial/non-initial
generalization about stressed or heavy rhymes at
the expense of the generalization about final and
non-final codas. If we say that a monosyllable is
an initial onset with a final rhyme, the reverse oc-
curs: we can learn the final/non-final coda gen-
eralization at the expense of the initial/non-initial
regularities. If we split the symbols further, we?d
generalize even less: we?d essentially have to learn
the initial/non-initial patterns separately for mono-
syllables and polysyllables.
The most direct solution would introduce fac-
tors that are ?smaller? than a single PCFG rule. Es-
sentially, we would compute the score of a PCFG
rule in terms of multiple features of its right-hand
side, rather than a single ?one-hot? feature identi-
fying the expansion. We left this direction for fu-
ture work and instead carried out two experiments
using Adaptor Grammars that were designed to
work around this limitation.
Our first experiment focuses on modeling
the initial/non-initial distinction, leaving the
final/non-final coda distinction unmodeled. The
models in this experiment assume parallel struc-
tures for syllable weight and stress, and focus on
providing the most direct comparison between syl-
lable weight and stress with a strictly initial/non-
initial distinction. This first experiment shows that
observing dictionary stress is better early in learn-
ing, but that modeling syllable weight is better
later in learning. However, it is possible that sylla-
ble weight was more useful because modeling syl-
lable weight involves modeling the characteristics
of codas; the advantage may not have been due to
weight per se but due to having learned something
about the effects of suffixes on final codas.
Our second experiment focuses on modeling
some aspects of final codas at the expense of main-
taining a rigid parallelism in the structures for syl-
lable weight and stress. The models in this exper-
iment split only those symbols that are necessary
to bring stress or weight patterns into the expres-
sive power of the model, and focus on comparing
richer models of syllable weight and stress that
account for inital/internal/final distinctions. This
second experiment shows that observing dictio-
nary stress is better early in learning, and that
modeling syllable weight merely catches up to
846
Sentence ? Collocations3
+
(1)
Collocations3 ? Collocations2
+
(2)
Collocations2 ? Collocation
+
(3)
Collocation ? Word
+
(4)
Figure 2: Three levels of collocation; symbols fol-
lowed by
+
may occur one or more times.
stress without surpassing it. Moreover, a com-
bined stress-and-weight model does no better than
a stress model, suggesting that the weight gram-
mar?s contribution is fully redundant, for the pur-
poses of word segmentation, with the stress obser-
vations.
Together, these experiments suggest that sylla-
ble weight eventually encodes everything about
word segmentation that dictionary stress does, and
that any advantage that syllable weight has over
observing dictionary stress is entirely redundant
with knowledge of word-final codas.
3 Experiments
3.1 Adaptor Grammars
We follow B?orschinger and Johnson (2014) in us-
ing a 3-level collocation Adaptor Grammar, as in-
troduced by Johnson and Goldwater (2009) and
presented in Figure 2, as the backbone for all
models, including the baseline. A 3-level collo-
cation grammar assumes that words are grouped
into collocations of words that tend to appear with
each other, and that the collocations themselves
are grouped into larger collocations, up to three
levels of collocations. This collocational struc-
ture allows the model to capture strong word-
to-word dependencies without having to group
frequently-occuring word sequences into a single,
incorrect, undersegmented ?word? as the unigram
model tends to do (Johnson and Goldwater, 2009)
Word rewrites in different ways in Experiment I
and Experiment II, which will be explained in the
relevant experiment section.
3.2 Experimental Set-up
We applied the same experimental set-up used by
B?orschinger and Johnson (2014), to their dataset,
as described below. To understand how different
modeling assumptions interact with corpus size,
we train on prefixes of each corpus with increas-
ing input size: 100, 200, 500, 1,000, 2,000, 5,000,
and 10,000 utterances. Inference closely fol-
lowed B?orschinger and Johnson (2014) and John-
son and Goldwater (2009). We set our hyperpa-
rameters to encourage onset maximization. The
hyperparameter for syllable nodes to rewrite to
an onset followed by a rhyme was 10, and the
hyperparameter for syllable nodes to rewrite to a
rhyme only was 1. Similarly, the hyperparame-
ter for rhyme nodes to include a coda was 1, and
the hyperparameter for rhyme nodes to exclude
the coda was 10. All other hyperparameters spec-
ified vague priors. We ran eight chains of each
model for 1,000 iterations, collecting 20 samples
with a lag of 10 iterations between samples and a
burn-in of 800 iterations. We used the same batch-
initialization and table-label resampling to encour-
age the model to mix.
After gathering the samples, we used them to
perform a single minimum Bayes risk decoding
of a separate, held-out test set. This test set was
constructed by taking the last 1,000 utterances of
each corpus. We use a common test-set instead
of just evaluating on the training data to ensure
that performance figures are comparable across in-
put sizes; when we see learning curves slope up-
ward, we can be confident that the increase is due
to learning rather than easier evaluation sets.
We measured our models? performance with the
usual token f-score metric (Brent, 1999), the har-
monic mean of how many proposed word tokens
are correct (token precision) and how many of the
actual word tokens are recovered (token recall).
For example, a model may propose ?the in side?
when the true segmentation is ?the inside.? This
segmentation would have a token precision of
1
3
,
since one of three predicted words matches the
true word token (even though the other predicted
words are valid word types), and a token recall of
1
2
, since it correctly recovered one of two words,
yield a token f-score of 0.4.
3.3 Dataset
We evaluated on a dataset drawn from the Alex
portion of the Providence corpus (Demuth et al.,
2006). This dataset contains 17, 948 utterances
with 72, 859 word tokens directed to one child
from the age of 16 months to 41 months. We used
a version of this dataset that contained annota-
tions of primary stress that B?orschinger and John-
son (2014) added to this input using an extended
847
RhymeI ? HeavyRhyme
RhymeI ? LightRhyme
Rhyme ? HeavyRhyme
Rhyme ? LightRhyme
HeavyRhyme ? LongVowel
HeavyRhyme ? Vowel Coda
LightRhyme ? ShortVowel
(a) Weight-sensitive grammar
RhymeI ? RhymeS
RhymeI ? RhymeU
Rhyme ? RhymeS
Rhyme ? RhymeU
RhymeS ? Vowel Stress (Coda)
RhymeU ? Vowel (Coda)
(b) Stress-sensitive grammar
RhymeI ? Vowel (Coda)
Rhyme ? Vowel (Coda)
(c) Baseline grammar
RhymeI ? HeavyRhymeS
RhymeI ? HeavyRhymeU
RhymeI ? LightRhymeS
RhymeI ? LightRhymeU
Rhyme ? HeavyRhymeS
Rhyme ? HeavyRhymeU
Rhyme ? LightRhymeS
Rhyme ? LightRhymeU
HeavyRhymeS ? LongVowel Stress
HeavyRhymeS ? LongVowel Stress Coda
HeavyRhymeU ? LongVowel
HeavyRhymeU ? LongVowel Coda
LightRhymeS ? ShortVowel Stress
LightRhymeU ? ShortVowel
(d) Combined grammar
Figure 3: Experiment I Grammars
version of CMUDict (cmu, 2008).
1
The mean
number of syllables per word token was 1.2, and
only three word tokens had more than five sylla-
bles. Of the 40, 323 word tokens with a stressed
syllable, 27, 258 were monosyllabic. Of the
13, 065 polysyllabic word tokens with a stressed
syllable, 9, 931 were stress-initial. Turning to the
32, 536 word tokens with no stress (i.e., the func-
tion words), all but 23 were monosyllabic (the 23
were primarily contractions, such as ?couldn?t?).
3.4 Experiment I: Parallel Structures
The goal of this first experiment is to provide the
most direct comparison possible between gram-
mars that attend to stress cues and grammars that
attend to syllable weight cues. As these are both
hypothesized to be useful by way of an initial/non-
initial distinction, we defined a word to be an ini-
tial syllable SyllI followed by zero to three sylla-
bles, and syllables to consist of an optional onset
1
This dataset and these Adap-
tor Grammar models are available at:
http://web.science.mq.edu.au/?jpate/stress/
and a rhyme:
Word ? SyllI (Syll)
{0,3}
(5)
SyllI ? (OnsetI) RhymeI (6)
Syll ? (Onset) Rhyme (7)
In the baseline grammar, presented in Figure 3c,
rhymes rewrite to a vowel followed by an optional
consonant coda. Rhymes then rewrite to be heavy
or light in the weight grammar, as in Figure 3a, to
be stressed or unstressed in the stress grammar, as
in Figure 3b. In the combination grammar, rhymes
rewrite to be heavy or light and stressed or un-
stressed, as in Figure 3d. LongVowel and Short-
Vowel both re-write to all vowels. An additional
grammar that restricted them to rewrite to long and
short vowels, respectively, led to virtually identi-
cal performance, suggesting that vowel quantity
can be learned for the purposes of word segmenta-
tion from distributional cues. We will also present
evidence that the model did manage to learn most
of the contrast.
Figure 4 presents learning curves for the gram-
mars in this parallel structured comparison. We
see that observing stress without modeling weight
848
0.6
0.7
0.8
0.9
100 1000 10000
Number of utterances
To
ke
n
 F
?S
co
re
noweight:nostress
noweight:stress
weight:nostress
weight:stress
Figure 4: Learning curves on the Alex corpus for Experiment I grammars with parallel distinctions
between Stressed/Unstressed and Heavy/Light syllable rhymes.
?
?
?
?
?
a?
a?
?
e?
?
i?
o?
?
??
u?
LongVowel ShortVowel Vowel
Vo
we
l
1 10 100 1000 7000
Vowel counts by quantity
Figure 5: Heatmap of learned vowels in the Ex-
periment I weight-only grammar. Each cell cor-
responds to the count of a particular vowel being
analyzed as one of the three vowel types. Diph-
thongs are rarely ShortVowel.
outperforms both the baseline and the weight-only
grammar early in learning. The weight-only gram-
mar rapidly improves in performance at larger
training data sizes, increasing its advantage over
the baseline, while the advantage of the stress-
only grammar slows and appears to disappear at
the largest training data size. At 10,000 utterances,
the improvement of the weight-only grammar over
the stress-only grammar is significant according to
an independent samples t-test (t = 7.2, p < 0.001,
14 degrees of freedom). This pattern suggests that
annotated dictionary stress is easy to take advan-
tage of at low data sizes, but that, with sufficient
data, syllable weight can provide even more in-
formation about word boundaries. The best over-
all performance early in learning is obtained by
the combined grammar, suggesting that syllable
weight and dictionary stress provide information
about word segmentation that is not redundant.
An examination of the final segmentation sug-
gests that the weight grammar has learned that
initial syllables tend to be heavy. Specifically,
across eight runs, 98.1% of RhymeI symbols
rewrote to HeavyRhyme, whereas only 54.5% of
Rhyme symbols (i.e. non-initial rhymes) rewrote
to HeavyRhyme.
849
Model Mean TF Std. Dev.
noweight:nostress 0.830 0.005
noweight:stress 0.831 0.008
weight:nostress 0.861 0.008
weight:stress 0.861 0.008
Table 1: Segmentation Token F-score for Experi-
ment I at 10,000 utterances across eight runs.
We also examined the final segmentation to see
well the model learned the distinction between
long vowels and short vowels. Figure 5 presents a
heatmap, with colors on a log-scale, showing how
many times each vowel label rewrote to each pos-
sible vowel in the (translated to IPA). Although the
quantity generalisations are not perfect, we do see
a general trend where ShortVowel rarely rewrites
to diphthongs.
3.5 Experiment II: Word-final Codas
Experiment I suggested that, under a ba-
sic initial/non-initial distinction, syllable weight
eventually encodes more information about word
boundaries than does dictionary stress. This is
a surprising result, since we initially investigated
syllable weight as a noisy proxy for dictionary
stress. One possible source of the ?extra? advan-
tage that the syllable weight grammar exhibited
has to do with the importance of word-final codas,
which can encode word-final morphemes in En-
glish (Brent and Cartwright, 1996). Even though
the grammars did not explicitly model them, the
weight grammar could implicitly capture a bias for
or against having a coda in non-initial position,
while the stress grammar could not. This is be-
cause most word tokens are one or two syllables,
and only one of the two rhyme types of the weight
grammar included a coda. Thus, the HeavyRhyme
symbol could simultaneously capture the most im-
portant aspects of both stress and coda constraints.
To see if the extra advantage of the syllable
weight grammar can be attributed to the influence
of word-final codas, we formulated a set of gram-
mars that model word-final codas and also can
learn stress and/or syllable weight patterns. These
grammars are more similar in structure to the ones
that B?orschinger and Johnson (2014) used. For the
baseline and weight grammar, we again defined
words to consist of up to four syllables with an ini-
tial SyllI syllable, but this time distinguished final
syllables SyllF in polysyllabic words. The non-
stress grammars use the following rules for pro-
ducing syllables:
Word ? SyllIF (8)
Word ? SyllI (Syll)
{0,2}
SyllF (9)
SyllIF ? (OnsetI) RhymeI (10)
SyllI ? (OnsetI) RhymeI (11)
Syll ? (Onset) Rhyme (12)
SyllF ? (Onset) RhymeF (13)
For the stress grammar, we followed
B?orschinger and Johnson (2014) in distin-
guishing stressed and unstressed syllables, rather
than simply stressed rhymes as in Experiment I,
to allow the model to learn likely stress patterns
at the word level. A word can consist of up to
four syllables, and any syllable and any number
of syllables may be stressed, as in Figure 6a.
The baseline grammar is similar to the previous
one, except it distinguishes word-final codas, as
in Figure 6b. The weight grammar, presented in
Figure 6c, rewrites rhymes to a nucleus followed
by an optional coda and distinguishes nuclei in
open syllables according to their position in the
word. The stress grammar, presented in Figure 6d,
is the all-stress-patterns model (without the unique
stress constraint) B?orschinger and Johnson (2014).
This grammar introduces additional distinctions at
the syllable level to learn likely stress patterns,
and distinguishes final from non-final codas. The
combined model is identical to the stress model,
except Vowel non-terminals in closed and word-
internal syllables are replaced with Nucleus non-
terminals, and Vowel non-terminals in word-inital
(-final) open syllables are replaced with NucleusI
(NucleusF) non-terminals.
To summarize, the stress models distinguish
stressed and unstressed syllables in initial, final,
and internal position. The weight models distin-
guish the vowels of initial open syllables, the vow-
els of final open syllables, and other vowels, al-
lowing them to take advantage of an important cue
from syllable weight for word segmentation: if an
initial vowel is open, it should usually be long.
Figure 7 shows segmentation performance on
the Alex corpus with these more complete models.
While the performance of the weight grammars is
virtually unchanged compared to Figure 4, the two
grammars that do not model syllable weight im-
prove dramatically. This result supports our pro-
posal that much of the advantage of the weight
850
Word ? {SyllUIF|SyllSIF}
Word ? {SyllUI|SyllSI} {SyllU|SyllS}
{0,2}
{SyllUF|SyllSF}
(a) The all-patterns stress model
Rhyme ? Vowel (Coda)
RhymeF ? Vowel (CodaF)
(b) Baseline grammar
RhymeI ? NucleusI
RhymeI ? Nucleus Coda
Rhyme ? Nucleus (Coda)
RhymeF ? NucleusF
RhymeF ? Nucleus CodaF
(c) Weight-sensitive grammar
SyllSIF ? OnsetI RhymeSF
SyllUIF ? OnsetI RhymeUF
SyllSI ? Onset RhymeS
SyllUI ? Onset RhymeU
SyllSF ? Onset RhymeSF
SyllUF ? Onset RhymeUF
RhymeSI ? Vowel Stress (Coda)
RhymeUI ? Vowel (Coda)
RhymeS ? Vowel Stress (Coda)
RhymeU ? Vowel (Coda)
RhymeSF ? Vowel Stress (CodaF)
RhymeUF ? Vowel (CodaF)
(d) Stress-sensitive grammar
Figure 6: Experiment II Grammars.
0.6
0.7
0.8
0.9
100 1000 10000
Number of utterances
To
ke
n
 F
?S
co
re
noweight:nostress
noweight:stress
weight:nostress
weight:stress
Figure 7: Learning curves on the Alex corpus for Experiment II grammars with word-final phonotactics
that exploit Stress and Weight.
851
Model Mean TF Std. Dev.
noweight:nostress 0.846 0.007
noweight:stress 0.880 0.005
weight:nostress 0.865 0.011
weight:stress 0.875 0.005
Table 2: Segmentation Token F-score for Experi-
ment II at 10,000 utterances across eight runs.
grammars over stress in Experiment I was due to
modeling of word-final coda phonotactics.
Table 2 presents token f-score at 10,000 train-
ing utterances averaged across eight runs, along
with the standard deviation in f-score. We see that
the noweight:nostress grammar is several standard
deviations than the grammars that model sylla-
ble weight and/or stress, while the syllable weight
and/or stress grammars exhibit a high degree of
overlap.
4 Conclusion
We have presented computational modeling exper-
iments that suggest that syllable weight (eventu-
ally) encodes nearly everything about word seg-
mentation that dictionary stress does. Indeed,
our experiments did not find a persistent advan-
tage to observing stress over modeling syllable
weight. While it is possible that a different mod-
eling approach might find such a persistent advan-
tage, this advantage could not provide more than
13% absolute F-score. This result suggests that
children may be able to learn and exploit impor-
tant rhythm cues to word boundaries purely on
the basis of segmental input. However, this result
also suggests that annotating input with dictionary
stress has missed important aspects of the role
of stress in word segmentation. As mentioned,
Turk et al. (1995) found that infants preferred ini-
tial light syllables to be stressed. Such a prefer-
ence obviously cannot be learned by attending to
syllable weight alone, so infants who have learned
weight distinctions must also be sensitive to non-
segmental acoustic correlates to stress. There was
no long-term advantage to observing stress in ad-
dition to attending to syllable weight in our mod-
els, however, suggesting that annotated dictionary
stress does not capture the relevant non-segmental
phonetic detail. More modeling is necessary to as-
sess the non-segmental phonetic features that dis-
tinguish stressed light syllables from unstressed
light syllables.
This investigation also highlighted a weakness
of current Adaptor Grammar models: the ?small-
est? factors are the size of one PCFG rule. Allow-
ing further factorizations, perhaps using feature
functions of a rule?s right-hand side, would allow
models to capture finer-grained distinctions with-
out fully splitting the symbols that are involved.
References
Benjamin B?orschinger and Mark Johnson. 2014. Ex-
ploring the role of stress in Bayesian word segmen-
tation using Adaptor Grammars. Transactions of the
ACL, 2:93?104.
Michael R Brent and Timothy A Cartwright. 1996.
Distributional regularity and phonotactic constraints
are useful for segmentation. Cognition, 61:93?125.
Michael Brent. 1999. An efficient, probabilistically
sound algorithm for segmentation and word discov-
ery. Machine Learning, 34:71?105.
Nick Campbell and Mary Beckman. 1997. Stress,
prominence, and spectral tilt. In Proceedings of an
ESCA workshop, pages 67?70, Athens, Greece.
Morten H. Christiansen and Suzanne L Curtin. 1999.
The power of statistical learning: No need for alge-
braic rules. In Proceedings of the 21st annual con-
ference of the Cognitive Science Society.
Morten H. Christiansen, Joseph Allen, and Mark S.
Seidenberg. 1998. Learning to segment speech
using multiple cues: A connectionist model. Lan-
guage and Cognitive Processes, 13:221?268.
2008. The CMU pronouncing dictionary.
http://www.speech.cs.cmu.edu/
cgi-bin/cmudict.
Anne Cutler and David M Carter. 1987. The predom-
inance of strong initial syllables in the English vo-
cabulary. Computer Speech & Language, 2(3):133?
142.
Katherine Demuth, Jennifer Culbertson, and Jennifer
Alter. 2006. Word-minimality, epenthesis, and coda
licensing in the acquisition of English. Language
and Speech, 49:137?174.
Gabriel Doyle and Roger Levy. 2013. Combining mul-
tiple information types in Bayesian word segmenta-
tion. In Proceedings of NAACL 2013, pages 117?
126. Association for Computational Linguistics.
D B Fry. 1955. Duration and intensity as physical cor-
relates of linguistic stress. J. Acoust. Soc. of Am.,
27:765?768.
D B Fry. 1958. Experiments in the perception of stress.
Language and Speech, 1:126?152.
852
Sharon Goldwater, Thomas L. Griffiths, and Mark
Johnson. 2009. A Bayesian framework for word
segmentation: Exploring the effects of context.
Cognition, 112(1):21?54.
Mark Johnson and Sharon Goldwater. 2009. Im-
proving nonparametric Bayesian inference: exper-
iments on unsupervised word segmentation with
adaptor grammars. In Proceedings of Human Lan-
guage Technologies: The 2009 Annual Conference
of the North American Chapter of the Association
for Computational Linguistics, pages 317?325. As-
sociation for Computational Linguistics.
Mark Johnson, Thomas L Griffiths, and Sharon Gold-
water. 2007. Adaptor grammars: A framework for
specifying compositional nonparametric Bayesian
models. In B Schoelkopf, J Platt, and T Hoffmann,
editors, Advances in Neural Information Processing
Systems, volume 19. The MIT Press.
Peter W Jusczyk, Anne Cutler, and Nancy J Redanz.
1993. Infants? preference for the predominant stress
patterns of English words. Child Development,
64(3):675?687.
Peter W Juszcyk, Derek M Houston, and Mary New-
some. 1999. The beginnings of word segmentation
in English-learning infants. Cognitive Psychology,
39(3?4):159?207.
Mark Liberman and Alan Prince. 1977. On stress and
linguistic rhythm. Linguistic Inquiry, 8(2):249?336,
Spring.
Constantine Lignos and Charles Yang. 2010. Reces-
sion segmentation: simpler online word segmenta-
tion using limited resources. In Proceedings of ACL
2010, pages 88?97. Association for Computational
Linguistics.
Constantine Lignos. 2011. Modeling infant word seg-
mentation. In Proceedings of the fifteenth confer-
ence on computational natural language learning,
pages 29?38. Association for Computational Lin-
guistics.
Constantine Lignos. 2012. Infant word segmentation:
An incremental, integrated model. In Proceedings
of the West Coast Conference on Formal Linguistics
30.
C Anton Rytting, Chris Brew, and Eric Fosler-Lussier.
2010. Segmenting words from natural speech: sub-
segmental variation in segmental cues. Journal of
Child Language, 37(3):513?543.
Erik D Thiessen and Jenny R Saffran. 2003. When
cues collide: use of stress and statistical cues to word
boundaries by 7-to-9-month-old infants. Develop-
mental Psychology, 39(4):706?716.
Alice Turk, Peter W Jusczyk, and Louann Gerken.
1995. Do English-learning infants use syllable
weight to determine stress? Language and Speech,
38(2):143?158.
Charles Yang. 2004. Universal grammar, statistics or
both? Trends in Cognitive Science, 8(10):451?456.
853
Transactions of the Association for Computational Linguistics, 1 (2013) 63?74. Action Editor: Brian Roark.
Submitted 9/2012; Published 3/2013. c?2013 Association for Computational Linguistics.
Unsupervised Dependency Parsing with Acoustic Cues
John K Pate??
j.k.pate@sms.ed.ac.uk
Sharon Goldwater?
sgwater@inf.ed.ac.uk
?ILCC, School of Informatics ?Department of Computing
University of Edinburgh Macquarie University
Edinburgh, EH8 9AB, UK Sydney, NSW 2109, Australia
Abstract
Unsupervised parsing is a difficult task that
infants readily perform. Progress has been
made on this task using text-based models, but
few computational approaches have considered
how infants might benefit from acoustic cues.
This paper explores the hypothesis that word
duration can help with learning syntax. We de-
scribe how duration information can be incor-
porated into an unsupervised Bayesian depen-
dency parser whose only other source of infor-
mation is the words themselves (without punc-
tuation or parts of speech). Our results, evalu-
ated on both adult-directed and child-directed
utterances, show that using word duration can
improve parse quality relative to words-only
baselines. These results support the idea that
acoustic cues provide useful evidence about
syntactic structure for language-learning in-
fants, and motivate the use of word duration
cues in NLP tasks with speech.
1 Introduction
Unsupervised learning of syntax is difficult for NLP
systems, yet infants perform this task routinely. Pre-
vious work in NLP has focused on using the implicit
syntactic information available in part-of-speech
(POS) tags (Klein and Manning, 2004), punctuation
(Seginer, 2007; Spitkovsky et al, 2011b; Ponvert et
al., 2011), and syntactic similarities between related
languages (Cohen and Smith, 2009; Cohen et al,
2011). However, these approaches likely use the data
in a very different way from children: neither POS
tags nor punctuation are observed during language
acquisition (although see Spitkovsky et al (2011a)
and Christodoulopoulos et al (2012) for encourag-
ing results using unsupervised POS tags), and many
children learn in a broadly monolingual environment.
This paper explores a possible source of information
that NLP systems typically ignore: word duration, or
the length of time taken to pronounce each word.
There are good reasons to think that word dura-
tion might be useful for learning syntax. First, the
well-established Prosodic Bootstrapping hypothesis
(Gleitman and Wanner, 1982) proposes that infants
use acoustic-prosodic cues (such as word duration)
to help them identify syntactic structure, because
prosodic and syntactic structures sometimes coincide.
More recently, we proposed (Pate and Goldwater,
2011) that infants might use word duration as a di-
rect cue to syntactic structure (i.e., without requir-
ing intermediate prosodic structure), because words
in high-probability syntactic structures tend to be
pronounced more quickly (Gahl and Garnsey, 2004;
Gahl et al, 2006; Tily et al, 2009).
Like most recent work on unsupervised parsing,
we focus on learning syntactic dependencies. Our
work is based on Headden et al (2009)?s Bayesian
version of the Dependency Model with Valence
(DMV) (Klein and Manning, 2004), using interpo-
lated backoff techniques to incorporate multiple infor-
mation sources per token. However, whereas Head-
den et al used words and POS tags as input, we
use words and word duration information, presenting
three variants of their model that use this information
in slightly different ways.1
1By using neither gold-standard nor learned POS tags as
input, our work differs from nearly all previous work on unsuper-
vised dependency parsing. While learned tags might be plausible
63
To our knowledge, this is the first work to incor-
porate acoustic cues into an unsupervised system for
learning full syntactic parses. The methods in this
paper were inspired by our previous approach (Pate
and Goldwater, 2011), which showed that word dura-
tion measurements could improve the performance
of an unsupervised lexicalized syntactic chunker over
a words-only baseline. However, that work was lim-
ited to HMM-like sequence models, tested on adult-
directed speech (ADS) only, and none of the models
outperformed uniform-branching baselines. Here, we
extend our results to full dependency parsing, and
experiment on transcripts of both spontaneous ADS
and child-directed speech (CDS). Our models us-
ing word duration outperform words-only baselines,
along with the Common Cover Link parser of Seginer
(2007), and the Unsupervised Partial Parser of Pon-
vert et al (2011), unsupervised lexicalized parsers
that have obtained state-of-the-art results on standard
newswire treebanks (though their performance here
is worse, as our input lacks punctuation). We also
outperform uniform-branching baselines.
2 Syntax and Word Duration
Before presenting our models and experiments, we
first discuss why word duration might be a useful cue
to syntax. This section reviews the two possible rea-
sons mentioned above: duration as a cue to prosodic
structure, or as a cue to predictability.
2.1 Prosodic Bootstrapping
Prosody is the structure of speech as conveyed by
rhythm and intonation, which are, in turn, conveyed
by such measurable phenomena as variation in fun-
damental frequency, word duration, and spectral tilt.
Prosodic structure is typically analyzed as imposing
a shallow, hierarchical grouping structure on speech,
with the ends of prosodic phrases (constituents) be-
ing cued in part by lengthening the last word of the
phrase (Beckman and Pierrehumbert, 1986).
The Prosodic Bootstrapping hypothesis (Gleit-
man and Wanner, 1982) points out that prosodic
phrases are often also syntactic phrases, and proposes
that language-acquiring infants exploit this correla-
tion. Specifically, if infants can learn about prosodic
phrase structure using word duration (and fundamen-
in a model of language acquisition, gold tags certainly are not.
tal frequency), they may be able to identify syntactic
phrases more easily using word strings and prosodic
trees than using word strings alone.
Several behavioral experiments support the con-
nection between prosody and syntax and the prosodic
bootstrapping hypothesis specifically. For example,
there is evidence that adults use prosodic information
for syntactic disambiguation (Millotte et al, 2007;
Price et al, 1991) and to help in learning the syntax
of an artificial language (Morgan et al, 1987), while
infants can use acoustic-prosodic cues for utterance-
internal clause segmentation (Seidl, 2007).
On the computational side, we are aware of only
our previous HMM-based chunkers (Pate and Gold-
water, 2011), which learned shallow syntax from
words, words and word durations, or words and hand-
annotated prosody. Using these chunkers, we found
that using words plus prosodic annotation worked
better than just words, and words plus word duration
worked even better. While these results are consistent
with the prosodic bootstrapping hypothesis, we sug-
gested that predictability bootstrapping (see below)
might be a more plausible explanation.
Other computational work has combined prosody
with syntax, but only in supervised systems, and typi-
cally using hand-annotated prosodic information. For
example, Huang and Harper (2010) used annotated
prosodic breaks as a kind of punctuation in a su-
pervised PCFG, while prosodic breaks learned in a
semi-supervised way have been used as features for
parse reranking (Kahn et al, 2005) or PCFG state-
splitting (Dreyer and Shafran, 2007). In contrast to
these methods, our approach observes neither parse
trees nor prosodic annotations.
2.2 Predictability Bootstrapping
On the basis of our HMM chunkers, we introduced
the predictability bootstrapping hypothesis (Pate and
Goldwater, 2011): the idea that word durations could
be a useful cue to syntactic structure not (or not only)
because they provide information about prosodic
structure, but because they are a direct cue to syntac-
tic predictability. It is well-established that talkers
tend to pronounce words more quickly when they
are more predictable, as measured by, e.g., word
frequency, n-gram probability, or whether or not the
word has been previously mentioned (Aylett and Turk,
2004; Bell et al, 2009). However, syntactic proba-
64
you threw it right at the basket
Figure 1: Example unlabeled dependency parse.
bility also seems to matter, with studies showing that
verbs tend to be pronounced more quickly when they
are in their preferred syntactic frame?transitive vs.
intransitive or direct object vs. sentential comple-
ment (Gahl and Garnsey, 2004; Gahl et al, 2006;
Tily et al, 2009). While this syntactic evidence is
only for verbs, together with the evidence that effects
of other notions of predictability, it suggests that such
syntactic effects may also be widespread. If so, the
duration of a word could give clues as to whether it
is being used in a high-probability or low-probability
structure, and thus what the correct structure is.
We found that our syntactic chunkers benefited
more from duration information than prosodic an-
notations, providing some preliminary evidence in
favor of predictability bootstrapping, but not ruling
out prosodic bootstrapping. So, we are left with two
plausible mechanisms by which word duration could
help with learning syntax. Slow pronunciations may
cue the end of a prosodic phrase, which is sometimes
also the end of a syntactic phrase. Alternatively, slow
pronunciations may indicate that the hidden syntactic
structure is low probability, facilitating the induc-
tion of a probabilistic grammar. This paper will not
seek to determine which mechanism is useful, instead
taking the presence of two possible mechanisms as
encouraging for the prospect of incorporating word
duration into unsupervised parsing.
3 Models2
As mentioned, we will be incorporating word dura-
tion into unsupervised dependency parsing, produc-
ing analyses like the one in Figure 1. Each arc is
between two words, with the head at the non-arrow
end of the arc, and the dependent at the arrow end.
One word, the root, depends on no word, and all
other words depend on exactly one word. Following
previous work on unsupervised dependency parsing,
we will not label the arcs.
2The implementation of these models is available at
http://github.com/jpate/predictabilityParsing
3.1 Dependency Model with Valence
All of our models are ultimately based on the De-
pendency Model with Valence (DMV) of Klein and
Manning (2004), a generative, probabilistic model
for projective (i.e. no crossing arcs), unlabeled de-
pendency parses, such as the one in Figure 1.
The DMV generates dependency parses using
three probability distributions, which together com-
prise model parameters ?. First, the root of the
sentence is drawn from Proot . Second, we decide
whether to stop generating dependents of the head
h in direction dir ? {left, right} with probability
Pstop(?|h, dir , v), where v is T if h has a dir-ward
dependent and F otherwise. If we decide to stop,
then h takes no more dependents in the direction of
dir. If we don?t stop, we use the third probability
distribution Pchoose(d|h, dir) to determine which de-
pendent d to generate. The second and third step
repeat for each generated word until all words have
stopped generating in both directions.
The DMV was the first unsupervised parsing
model to outperform a uniform-branching baseline
on the Wall Street Journal corpus. It was trained
using EM to obtain a maximum-likelihood estimate
of the parameters ?, and learned from POS tags to
avoid rare events. However, all work on syntactic
predictability effects on word duration has been lexi-
calized (looking at, e.g., the transitivity bias of par-
ticular verbs). In addition, it is unlikely that children
have access to the correct parts of speech when first
learning syntactic structure. Thus, we want a DMV
variant that learns from words rather than POS tags.
We therefore adopt several extensions to the DMV
due to Headden et al (2009), described next.
3.2 The DMV with Backoff
Headden et al (2009) sought to improve the DMV by
incorporating lexical information in addition to POS
tags. However, arcs between particular words are
rare, so they modified the DMV in two ways to deal
with this sparsity. First, they switched from MLE to a
Bayesian approach, estimating a probability distribu-
tion over model parameters ? and dependency trees
T given the training corpus C and a prior distribution
? over models: P (T, ?|C,?).
Headden et al avoided overestimating the proba-
bility of rare events that happen to occur in the train-
65
ing data by picking ? to assign low probability to
models ? which give high probability to rare events.
Accordingly, models that overcommit to rare events
will contribute little to the final average over models.
Specifically, Headden et al use Dirichlet priors, with
? being the Dirichlet hyperparameters.
Headden et al?s second innovation was to adapt in-
terpolated backoff methods from language modeling
with n-grams, where one can estimate the probabil-
ity of word wn given word wn?1 by interpolating
between unigram and bigram probability estimates:
P? (wn|wn?1) = ?P (wn|wn?1) + (1? ?)P (wn)
with ? ? [0, 1]. Ideally, ? should be large whenwn?1
is frequent, and small when wn?1 is rare. Headden et
al. (2009) apply this method to the DMV by backing
off from Choose and Stop distributions that condition
on both head word and POS to distributions that
condition on only the head POS.
In the equation above, ? is a scalar parameter.
However, it actually specifies a probability distri-
bution over the decision to back off (B) or not back
off (?B), and we can use different notation to reflect
this view. Specifically, ?stop(?) and ?choose(?) will
represent our backoff distributions for the Stop and
Choose decision, respectively. Using hp and dp to
represent head and dependent POS tag and hw and
dw to represent head and dependent word, one of the
models Headden et al explored estimates:
P? choose(dp|hw, hp, dir , val) =
?choose(?B|hw, hp, dir)Pchoose(dp|hw, hp, dir)
+?choose(B|hw, hp, dir)Pchoose(dp|hp, dir) (1)
with an analogous backoff for Pstop . We can see
from Equation 1 that P?choose backs off from a dis-
tribution that conditions on hw to a distribution that
marginalizes out hw, and that the extent of backoff
varies across hw; we can use this to back off more
when we have less evidence about hw. This model
only conditions on words; it does not generate them
in the dependents. This means it is actually a condi-
tional, rather than fully generative, model of observed
POS tags and unobserved syntax conditioned on the
observed words.
Since identifying the true posterior distribution
P (T, ?|C,?) is intractable, Headden et al use Mean-
field Variational Bayes (Kurihara and Sato, 2006;
Johnson, 2007), which finds an approximation to the
posterior using an iterative EM-like algorithm. In the
E-step of VBEM, expected countsE(ri) are gathered
for each latent variable using the Inside-Outside algo-
rithm, exactly as in the E-step of traditional EM. The
Maximization step differs from the M-Step of EM in
two ways. First, the expected counts for each value
of the latent variable ri are incremented by the hy-
perparameter ?i. Second, the numerator and denom-
inator are scaled by the function exp(?(?)), which
reduces the probability of rare events. Specifically,
the Pchoose distribution is estimated using expecta-
tions for each arc adp,h,dir from head h to dependent
POS tag dp in direction dir, and the update equation
for Pchoose from iteration n to n+ 1 is:
P?n+1choose(dp|h, dir) =
exp(?(En(adp,h,dir ) + ?dp,h,dir ))
exp(?(
?
c(En(ac,h,dir ) + ?c,h,dir )))
(2)
where h is the head POS tag for the backoff distri-
bution, and the head (word, POS) pair for the no
backoff distribution. The update equation for Pstop
is analogous.
Now consider the update equations for ?choose :
??n+1choose(?B|hw, hp, dir) =
exp(?(??B +
?
c(En(ac,hw,hp,dir ))))
exp(?(?B + ??B +
?
c(En(ac,hw,hp,dir ))))
??n+1choose(B|hw, hp, dir) =
exp(?(?B))
exp(?(?B + ??B +
?
c(En(ac,hw,hp,dir ))))
Only the ?B numerator includes the expected counts,
so as we see hw in direction dir more often, the ?B
numerator will swamp the B numerator. By picking
?B larger than ??B, we can bias our ? distribution to
prefer backing off until we expect at least ?B ? ??B
arcs out of hw with tag hp in the direction of dir.
To obtain good performance, Headden et al re-
placed each word that appeared fewer than 100 times
in the training data with the token ?UNK.? We will
also use such an UNK cutoff.
3.3 DMV with Duration
We explore three models. One is a straightforward
application of the DMV with Backoff to words and
66
(quantized) word duration, and the other two are fully-
generative variants. We also consider using words
and POS tags as input to these models. Backoff mod-
els are given two streams of information, providing
two of word identity, POS tag, or word duration for
each observed token. We call one stream the ?back-
off? stream, and the other the ?extra? stream. Backoff
models learn a probability distribution conditioning
on both streams, backing off to condition on only the
backoff stream.
Our first words and duration model takes the du-
ration as the extra stream and the word identity as
the backoff stream, and, using ha to represent the
acoustic information for the head, defines:
P? choose(dw|hw, ha, dir) =
?choose(?B|hw, ha, dir)Pchoose(dw|hw, ha, dir)
+?choose(B|hw, ha, dir)Pchoose(dw|hw, dir) (3)
with an analogous backoff scheme for Pstop . We will
refer to this conditional model as ?Cond.? in our
experiments. This equation is similar to Equation 1,
except it uses words and duration instead of words
and POS tags, and backs off to, not away from, words.
We back off to the sparse words, rather than the less
sparse duration, because duration provides almost no
information about syntax in isolation.3
Directly modelling the extra stream among the
dependents may allow us to capture selectional re-
strictions in POS and words models, or exploit ef-
fects of syntactic predictability on dependent dura-
tion. We therefore explore variants that generate both
streams in the dependents. First, we examine a model
(?Joint?) that generates them jointly:
P?choose(dw, da|hw, hp, dir) =
?choose(?B|hw, ha, dir)
Pchoose(dw, da|hw, ha, dir)
+?choose(B|hw, ha, dir)
Pchoose(dw, da|hw, dir) (4)
However, this joint model will have a very large state-
space and may suffer from the same data sparsity, so
we also explore a model (?Indep.?) that generates the
3Preliminary dev-set experiments confirmed this intuition, as
models that backed off to word duration performed poorly.
extra and backoff independently:
P?choose(dw, da|hw, hp, dir) =
?choose(?B|hw, ha, dir)
Pchoose backoff (dw|hw, ha, dir)
Pchoose extra(da|hw, ha, dir)
+ ?choose(B|hw, ha, dir)
Pchoose backoff (dw|hw, dir)
Pchoose extra(da|hw, dir) (5)
We also modified the DMV with Backoff to handle
heavily lexicalized models. In Headden et al (2009),
arcs between words that never appear in the same
sentence are given probability mass only by virtue
of the backoff distribution to POS tags, which all
appear in the same sentence at least once. We want to
avoid relying on POS tags, and we also want to use
held-out development and test sets to avoid implicitly
overfitting the data when exploring different model
structures. To this end, we add one extra ?UNK hyper-
parameter to the Dirichlet prior of Pchoose for each
combination of conditioning events. This hyperpa-
rameter reserves probability mass for a head h to take
a word dw as a dependent if h and dw never appeared
together in the training data. The amount of probabil-
ity mass reserved decreases as we see hw more often.
This is implemented in training by adding ?UNK to
the denominator of the Pchoose update equation for
each h and dir. At test time, if a word dw appears
as an unseen dependent for head h, h takes dw as a
dependent with probability:
P? choose(dw|h, dir) = (6)
exp(?(?UNK))
exp(?(?UNK +
?
c(Elast(rc,h,dir ) + ?c,h,dir )))
Here, h may be a word, (word, POS) pair, or (word,
duration) pair. Since this event by definition never
occurs in the training data, ?UNK does not appear in
the numerator during training. 4
Finally, the conditional model ignores the extra
stream in Proot , and the generative models estimate
4Note also that ?UNK is different from a global UNK cutoff,
which is imposed in preprocessing, and so effects every occur-
rence of an an UNK?d word in the model. ?UNK affects only
dependents in Pchoose , and treats a dependent as UNK iff it did
not occur on that particular side of that particular head word in
any sentence. We used both global UNK cutoffs (optimized on
the dev set) and these ?UNK hyperparameters.
67
Train Dev Test
ws
j1
0 Word tokens 42,505 1,765 2,571
Word types 7,804 818 1,134
Sentences 6,007 233 357
sw
bd
nx
t1
0 Word tokens 24,998 2,980 3,052
Word types 2,647 760 767
Sentences 3,998 488 491
br
en
t Word tokens 20,954 2,127 2,206
Word types 1,390 482 488
Sentences 6,249 424 449
Table 1: Statistics for our three corpora.
Proot over both streams jointly and independently,
respectively.
4 Experimental Setup
4.1 Datasets
We evaluate on three datasets: wsj10, sentences of
length 10 or less from the Wall Street Journal por-
tion of the Penn Treebank; swbdnxt10, sentences
of length 10 or less from the Switchboard dataset
of ADS used by Pate and Goldwater (2011); and
brent, part of the Brent corpus of CDS (Brent and
Siskind, 2001). Table 1 presents corpus statistics.
4.1.1 wsj10
We present a new evaluation of the DMV with
Backoff on wsj10, which does not have any acous-
tic information, simply to verify that ?UNK performs
sensibly on a standard corpus. Additionally, Headden
et al (2009) use an intensive initializer that relies on
dozens of random restarts, and so, strictly speaking,
only show that the backoff technology is useful for
good initializations. Our new evaluation will show
that the backoff technology provides a substantial
benefit even for harmonic initialization.
wsj10 was created in the standard way; all punc-
tuation and traces were removed, and sentences con-
taining more than ten tokens were discarded. For
our fully lexicalized version of wsj10, all words
were lowercased, and numbers were replaced with
the token ?NUMBER.?5 Following standard practice,
we used sections 2-21 for training, section 22 for
development, and section 23 for test. wsj10 con-
tains hand-annotated constituency parses, not depen-
dency parses, so we used the standard ?constituency-
5Numbers were treated in this way only in wsj10.
to-dependency? conversion tool of Johansson and
Nugues (2007) to obtain high-quality CoNLL-style
dependency parses.
4.1.2 swbdnxt10
Next, we evaluate on swbdnxt10, which con-
tains all sentences up to length 10 from the same
sections of the swbdnxt version of Switchboard
used by Pate and Goldwater (2011). Short sentences
are usually formulaic discourse responses (e.g. ?oh
ok?), so this dataset alo excludes sentences shorter
than three words. As our models successfully use
word durations, this evaluation provides an important
replication of the basic result from Pate and Goldwa-
ter (2011) with a different kind of syntactic model.
swbdnxt10 has a forced alignment of a
dictionary-based phonetic transcription of each ut-
terance to audio, providing our word duration infor-
mation. As a very simple model of hyper-articulation
and hypo-articulation, we classify a word as in the
longest third duration, shortest third, or middle third.
To minimize effects of word form, this classification
was based on vowel count (counting a diphthong as
one vowel): each word with n vowels is classified as
in the shortest, longest, or middle tercile of duration
among words with n vowels.
Like wsj10, swbdnxt10 is annotated only
with constituency parses, so to provide approximate
?gold-standard? dependencies, we used the same
constituency-to-dependency conversion tool as for
wsj10. We evaluated 200 randomly-selected sen-
tences to check the accuracy of the conversion tool,
which was designed for newspaper text. Excluding
arcs involving words with no clear role in depen-
dency structure (such as ?um?), about 86% of the
arcs were correct. While this rate is uncomfortably
low, it is still much higher than unsupervised depen-
dency parsers typically achieve, and so may provide
a reasonable measure of relative dependency parse
quality among competing systems.
4.1.3 brent
We also evaluated our models on the ?Large Brent?
dataset introduced in Rytting et al (2010), a por-
tion of the Brent corpus of child-directed speech
(Brent and Siskind, 2001). We call this corpus
brent. It consists of utterances from four of the
mothers in Brent and Siskind?s (2001) study, and, like
68
swbdnxt10, has a forced alignment from which we
obtain duration terciles. Rytting et al (2010) used
a 90%/10% train/test partition. We extracted every
ninth utterance from the original training partition to
create a dev set, producing an 80%/10%/10% parti-
tion. We also separated clitics from their base word.
This dataset only has 186 sentences longer than ten
words, with a maximum length of 22 words, so we
discarded only sentences shorter than three words
from the evaluation sets.
The Brent corpus is distributed via CHILDES
(MacWhinney, 2000) with automatic dependency an-
notations. However, these are not hand-corrected,
and rely on a different tokenization of the dataset
than is present on the transcription tier. To produce a
reliable gold-standard,6 we annotated all sentences of
length 2 or greater from the development and test sets
with dependencies drawn from the Stanford Typed
Dependency set (de Marneffe and Manning, 2008)
using the annotation tool used for the Copenhagen
Dependency Treebank (Kromann, 2003).
4.2 Parameters
In all experiments, hyperparameters for Proot , Pstop ,
and Pchoose (and their backed-off distributions, and
including ?UNK) were 1, ?B was 10, and ??B was 1.
VBEM was run on the training set until the data
log-likelihood changed by less than 0.001%, and
then the parameters were held fixed and used to
obtain Viterbi parses for the evaluation sentences.
Finally, we explored different global UNK cutoffs,
replacing each word that appeared less than c times
with the token UNK. We ran each model for each
c ? {0, 1, 25, 50, 100}, and picked the best-scoring
c on the development set for running on the test set
and presentation here. We used a harmonic initializer
similar to the one in Klein and Manning (2004).
4.3 Evaluation
In addition to evaluating the various incarnations of
the DMV with backoff and input types, we compare
to uniform branching baselines, the Common Cover
Link (CCL) parser of Seginer (2007), and the Unsu-
pervised Partial Parser (UPP) of Ponvert et al (2011).
The UPP produces a constituency parse from words
and punctuation using a series of finite-state chun-
6Available at http://homepages.inf.ed.ac.uk/s0930006/brentDep/
kers; we use the best-performing (Probabilistic Right
Linear Grammar) version. The CCL parser produces
a constituency parse using a novel ?Cover Link? rep-
resentation, scoring these links heuristically. Both
CCL and UPP rely on punctuation (though according
to Ponvert et al (2011), UPP less so), which our in-
put is missing. The left-headed ?LH? (right-headed
?RH?) baseline assumes that each word takes the first
word to its right (left) as a dependent, and corre-
sponds to a uniform right-branching (left-branching)
constituency baseline.
We evaluate the output of all models in terms
of both constituency scores and dependency accu-
racy. Our wsj10 and swbdnxt10 corpora are
originally annotated for constituency structure, with
the dependency gold standard derived as described
above, while our brent corpus is originally anno-
tated for dependency structure, with the constituency
gold standard derived by defining a constituent to
span a head and each of its dependents (ignoring
any one-word ?constituents?). As the CCL and
UPP parsers don?t produce dependencies, only con-
stituency scores are provided.
For constituency scores, we present the standard
unlabeled Precision, Recall, and F-measure scores.
For dependency scores, we present Directed attach-
ment accuracy, Undirected attachment accuracy, and
the ?Neutral Edge Detection? (NED) score intro-
duced by Schwartz et al (2011). Directed attachment
accuracy counts an arc as a true positive if it correctly
identifies both a head and a dependent, whereas undi-
rected attachment accuracy ignores arc direction in
counting true positives. NED counts an arc as a true
positive if it would be a true positive under the Undi-
rected attachment score, or if the proposed head is
the gold-standard grandparent of the proposed depen-
dent. This avoids penalizing parses for flipping an
arc, such as making determiners, rather than nouns,
the head of noun phrases.
To assess statistical significance, we carried out
stratified shuffling tests, with 10, 000 random shuf-
fles, for all measures. Tables indicate significance
differences between the backoff models and the most
competitive baseline model on that measure, indi-
cated by an italic score. A star (?) indicates p < 0.05,
and a dagger (?) indicates p < 0.01. To see the di-
rection of a significant difference (i.e. whether the
backoff model is better or worse than the baseline),
69
wsj10 swbdnxt10
Dependency Constituency Dependency Constituency
UNK Dir. Undir. NED P R F UNK Dir. Undir. NED P R F
EM
Wds 25 32.5 52.5 67.0 49.5 48.5 49.0 25 30.6 50.9 66.8 45.4 47.1 46.3
POS ? 46.4 63.8 78.1 59.2 58.1 58.6 ? 53.0 65.0 76.8 52.5 52.9 52.7
VB Wds 25 29.4 52.4 70.5 51.3 52.6 52.0 25 36.1 54.9 72.7 49.0 50.0 49.5POS ? 43.5 61.9 77.3 59.7 57.1 58.4 ? 51.3 62.5 74.3 47.1 46.6 46.8
W
ds+
PO
S Cond. 50 49.9? 66.1? 79.6? 64.2? 61.9? 63.0? 100 45.5? 62.4? 77.8 58.4? 58.9? 58.7?
Joint 50 46.0 63.7 79.0 62.0? 59.1 60.5? 1 49.4? 63.7 79.6? 60.0? 52.9 56.3?
Indep. 25 52.5? 68.0? 83.5? 63.5? 61.5? 62.5? 100 55.7? 65.8 74.6? 61.5? 57.9? 59.6?
LH ? 26.0 55.8 74.3 53.1 69.6 60.3 ? 24.1 50.8 72.7 60.8 82.5 70.0
RH ? 31.2 56.4 61.4 25.8 33.8 29.3 ? 29.2 52.0 57.9 22.2 30.1 25.5
CCL ? ? ? ? 50.8 40.7 45.2 ? ? ? ? 53.6 47.4 50.3
UPP ? ? ? ? 52.8 37.2 43.7 ? ? ? ? 60.0 46.6 52.4
Table 2: Performance on wsj10 and swbdnxt10 for models using words and POS tags only. Bold scores indicate
the best performance of all models and baselines on that measure.
? Significantly different from best non-uniform baseline (italics) by a stratified shuffling test, p < 0.01; ?: p < 0.05.
look to the scores themselves.
5 Results
In all results, when a model sees only one kind of
information, that is expressed by writing out the ab-
breviation for the relevant stream: ?Wds? for words,
?POS? for Part-Of-Speech, ?Dur? for word duration.
For baseline models that see two streams, the abbre-
viations are joined by a ??? symbol (as they treat
input pairs as atoms drawn in the cross-product of the
two streams? vocabulary). For the backoff models,
the abbreviations are joined by a ?+? symbol (as they
combine the information sources with a weighted
sum), with the ?extra? stream name first.
5.1 Results: wsj10
The left half of Table 2 presents results on wsj10.
For the baseline models, the first column with hori-
zontal text indicates the input, while for the backoff
(Wds+POS) models, the first column with horizontal
text indicates whether and how the extra stream is
modeled in dependents (as described in Section 3.3).
The EM model with POS input is largely a repli-
cation of the original DMV, differing in the use of
separate train, dev, and test sets, and possibly the
details of the harmonic initializer. Our replication
achieves an undirected attachment score of 63.8 on
the test set, similar to the score of 64.5 reported by
Klein and Manning (2004) when training and evalu-
ating on all of wsj10. Cohen et al (2008) use the
same train/dev/test partition that we do, and report
a directed attachment score of 45.8, similar to our
directed attachment score of 46.4.
The VB model which learns from POS tags does
not outperform the EM model which learns from POS
tags, suggesting that data sparsity does not hurt the
DMV when using POS tags. As expected, the words-
only models perform much worse than both the POS
input models and the uniform LH baseline. VB does
improve the words-only constituency performance.
The Cond. and Indep. backoff models outperform
the POS-only baseline on all measures, but the Joint
backoff model does not demonstrate a clear advan-
tage over the POS-only baseline on any measure. The
success of the Indep. model indicates that modelling
dependent word identity does provide enough infor-
mation to justify the increase in sparsity. The failure
of the Joint model to provide a further improvement
indicates that the extra information in the full joint
over dependents does not justify the large increase
in parameters. We also see that several models out-
perform the LH baseline on dependencies, but the
advantage is much less in F-Score, underscoring the
loss of information in the conversion of dependen-
cies to constituencies. Finally, all models outperform
CCL and UPP on F-score, emphasizing their reliance
on the punctuation we removed.
70
Dependency Constituency
UNK Dir. Undir. NED P R F
EM
Wds 25 30.6 50.9 66.8 45.4 47.1 46.3
Wds?Dur 25 26.1 46.5 62.0 45.6 48.7 47.1
VB Wds 25 36.4 55.1 73.0 49.1 50.0 49.6Wds?Dur 25 31.8 51.7 71.3 49.2 55.9 52.3
Du
r+W
ds Cond. 25 32.6? 55.1 74.5? 59.1? 71.4? 64.7?
Joint 50 31.8? 51.8? 70.8? 54.4? 60.5? 57.3?
Indep. 50 40.3? 59.1? 76.0? 56.1? 61.7? 58.8?
LH ? 24.1 50.8 72.7 60.8 82.5 70.0
RH ? 29.2 52.0 57.9 22.2 30.1 25.5
CCL ? ? ? ? 53.6 47.4 50.3
UPP ? ? ? ? 60.0 46.6 52.4
l
l
45 50 55 60
45
50
55
60
65
70
75
Switchboard Model Performance
Undirected Attachment Score
Co
nst
itue
ncy
 F?
sco
re
l
l
Wds
WdsxDur
Cond.
Joint
Indep.
LH
Table 3: Performance on swbdnxt10 for models using words and duration. The scatterplot includes a subset of the
information in the table: F-score and undirected attachment accuracy for backoff models and VB and LH baseline.
Bold, italics, and significance annotations as in Table 2.
5.2 Results: swbdnxt10
The right half of Table 2 presents performance fig-
ures on swbdnxt10 for input involving words and
POS tags. As expected, the EM and VB baselines
perform best when learning from gold-standard POS
tags, and we again see no benefit for the VB POS-
only model compared to the EM POS-only model.
The POS-only baselines far outperform the uniform-
attachment baselines on the dependency measures; to
our knowledge this is the first demonstration outside
the newspaper domain that the DMV outperforms a
uniform branching strategy on these measures.
The other comparisons among systems listed in
Table 2 are largely inconclusive. Models do com-
paratively well on either the constituency or depen-
dency evaluation, but not both. The backoff mod-
els outperform the baseline POS-only models in the
constituency evaluation, but underperform or match
those same models in the dependency evaluation.
Conversely, most models outperform the LH base-
line in the dependency evaluation, but not in the
constituency evaluation. There are probably two
causes for the ambiguity in these results. First, the
noise in the dependency gold-standard may have over-
whelmed any advantage from backoff. Second, as we
saw with wsj10, the conversion from dependencies
to constituencies removes information, which may
explain the failure of any model to outperform the
LH baseline in the constituency evaluation.
Table 3 presents performance figures on
swbdnxt10 for input involving words and duration,
including a scatter-plot of Undirected attachment
against constituency F-Score for the interesting
comparisons. In the scatter-plot, models up and
to the right performed better, and we see that the
negative correlation between the dependency and
constituency evaluations persists in words and dura-
tion input. VB substantially outperforms EM in the
baselines, indicating that good smoothing is helpful
when learning from words. Other comparisons
are again ambiguous; the dependency evaluation
is noisy, and backoff models outperform baseline
models on the constituency evaluation but not the
LH baseline. Still, the backoff models outperform
all words-only baselines in constituency score, with
two performing slightly worse in dependency score
and one performing much better. So there is some
evidence that word duration is useful, but we will
find clearer evidence on the brent corpus.
5.3 Results: brent
Table 4 presents results on the brent dataset. VB
is even more effective than in the other datasets for
improving performance among baseline models, lead-
ing to double-digit improvements on some measures.
Moreover, the best dev-set UNK cutoff drops to 1
for all VB models, indicating that, on this dataset,
VB provides good smoothing even in models without
backoff. This difference between datasets is likely
related to differences in vocabulary diversity; the
71
Dependency Constituency
UNK Dir. Undir. NED P R F
EM
Wds 25 36.9 56.3 70.7 52.4 69.5 59.8
Wds?Dur 25 31.3 51.1 66.9 50.7 64.7 56.9
VB Wds 1 51.2 64.2 77.3 63.3 68.1 66.0Wds?Dur 1 47.0 60.5 74.0 66.2 64.9 65.5
Du
r+W
ds Cond. 1 53.1? 65.5? 78.7? 65.4 68.6 67.0?
Joint 1 50.7 63.0 76.3 65.6 65.4? 65.5
Indep. 1 53.2 66.7? 79.6? 61.5? 67.9 64.5
LH ? 28.3 53.6 78.3 47.9 85.6 61.4
RH ? 27.2 48.8 61.1 26.2 46.8 33.6
CCL ? ? ? ? 41.7 58.8 48.8
UPP ? ? ? ? 56.8 63.8 60.1
l
l
50 55 60 65 70
60
62
64
66
68
70
Brent Model Performance
Undirected Attachment Score
Co
nst
itue
ncy
 F?
sco
re
l
l
Wds
WdsxDur
Cond.
Joint
Indep.
LH
Table 4: Performance on brent for models using words and duration. The scatterplot includes a subset of the
information in the table: F-score and undirected attachment accuracy for backoff models and VB and LH baseline.
Bold, italics, and significance annotations as in Table 2.
type:token ratio in the brent training set is about
1:15, compared to 1:5 and 1:9 in the wsj10 and
swbdnxt10 training sets, respectively.
More importantly for our main hypothesis, all
three backoff models using words and duration out-
perform the words-only baselines (including CCL
and UPP) on all dependency measures?the most
accurate measures on this corpus, which has hand-
annotated dependencies?and the Cond. model also
wins on F-score.
6 Conclusion
In this paper, we showed how to use the DMV with
Backoff and two fully-generative variants to explore
the utility of word duration in fully lexicalized un-
supervised dependency parsing. Although other re-
searchers have incorporated features beyond words
and POS tags into DMV-like models (e.g., semantics:
Naseem and Barzilay (2011); morphology: Berg-
Kirkpatrick et al (2009)), we believe this is the first
example based on Headden et al (2009)?s backoff
method. As far as we know, our work is also the first
test of a DMV-based model on transcribed conver-
sational speech and the first to outperform uniform-
branching baselines without using either POS tags or
punctuation in the input. Our results show that fully-
lexicalized models can do well if they are smoothed
properly and exploit multiple cues.
Our experiments also suggest that CDS is espe-
cially easy to learn from. Model performance on
the brent dataset was generally higher than on
swbdnxt10, with a much lower UNK threshold.
This latter point, and the fact that brent has a much
lower word type/token ratio than the other datasets,
suggest that CDS provides more and clearer evidence
about words? syntactic behavior.
Finally, our results provide more evidence, using
a different, more powerful syntactic model than that
of Pate and Goldwater (2011), that word duration
is a useful cue for unsupervised parsing. We found
that several ways of incorporating duration were use-
ful, although the extra sparsity of Joint emissions
was not justified in any of our investigations. Our
results are consistent with both the prosodic and pre-
dictability bootstrapping hypotheses of language ac-
quisition, providing the first computational support
for these using a full syntactic parsing model and
tested on child-directed speech. While our models do
not provide a mechanistic account of how children
might use duration information to help with learning
syntax, they do show that this information is useful
in principle, even without any knowledge of latent
prosodic structure or its relationship to syntax. In ad-
dition, our results suggest it may be useful to explore
using word duration to enrich NLP tasks in speech-
related technologies, such as syntactically-inspired
language models for text-to-speech generation. In
the future, we also hope to investigate why duration
is helpful, designing experiments to tease apart the
role of prosody and predictability in learning syntax.
72
References
Matthew Aylett and Alice Turk. 2004. The smooth signal
redundancy hypothesis: A functional explanation for re-
lationships between redundancy, prosodic prominence,
and duration in spontaneous speech. Language and
Speech, 47(1):31?56.
Mary Beckman and Janet Pierrehumbert. 1986. Intona-
tional structure in Japanese and English. Phonology
Yearbook, 3:255?309.
Alan Bell, Jason M Brenier, Michelle Gregory, Cynthia
Girand, and Dan Jurafsky. 2009. Predictability effects
on durations of content and function words in conver-
sational English. Journal of Memory and Language,
60:92?111.
Taylor Berg-Kirkpatrick, Alexandre Bouchard-Co?te?, John
DeNero, and Dan Klein. 2009. Painless unsupervised
learning with features. In Proceedings of NAACL.
Michael R Brent and Jeffrey M Siskind. 2001. The role
of exposure to isolated words in early vocabulary de-
velopment. Cognition, 81:31?44.
Christos Christodoulopoulos, Sharon Goldwater, and
Mark Steedman. 2012. Turning the pipeline into a loop:
Iterated unsupervised dependency parsing and PoS in-
duction. In Proceedings of the NAACL-HLT Workshop
on the Induction of Linguistic Structure, pages 96?99,
Montre?al, Canada, June. Association for Computational
Linguistics.
Shay B Cohen and Noah A Smith. 2009. Shared lo-
gistic normal distributions for soft parameter tying in
unsupervised grammar induction. In Proceedings of
NAACL.
Shay B Cohen, Kevin Gimpel, and Noah A Smith. 2008.
Logistic normal priors for unsupervised probabilistic
grammar induction. In Advances in Neural Information
Processing Systems 22.
Shay B Cohen, Dipanjan Das, and Noah A Smith. 2011.
Unsupervised structure prediction with non-parallel
multilingual guidance. In Proceedings of EMNLP.
Marie-Catherine de Marneffe and Christopher D Manning.
2008. Stanford typed dependencies manual. Technical
report.
Markus Dreyer and Izhak Shafran. 2007. Exploiting
prosody for PCFGs with latent annotations. In Pro-
ceedings of Interspeech, Antwerp, Belgium, August.
Susanne Gahl and Susan M Garnsey. 2004. Knowledge of
grammar, knowledge of usage: Syntactic probabilities
affect pronunciation variation. Language, 80:748?775.
Susanne Gahl, Susan M Garnsey, Cynthia Fisher, and
Laura Matzen. 2006. ?That sounds unlikely?: Syntac-
tic probabilities affect pronunciation. In Proceedings
of the 27th meeting of the Cognitive Science Society.
Lila Gleitman and Eric Wanner. 1982. Language acqui-
sition: The state of the art. In Eric Wanner and Lila
Gleitman, editors, Language acquisition: The state of
the art, pages 3?48. Cambridge University Press, Cam-
bridge, UK.
Will Headden, Mark Johnson, and David McClosky. 2009.
Improved unsupervised dependency parsing with richer
contexts and smoothing. In Proceedings of NAACL-
HLT.
Zhongqiang Huang and Mary Harper. 2010. Appropri-
ately handled prosodic breaks help PCFG parsing. In
Proceedings of NAACL-HLT, pages 37?45, Los Ange-
les, California, June. Association for Computational
Linguistics.
Richard Johansson and Pierre Nugues. 2007. Extended
constituent-to-dependency conversion for English. In
Proceedings of NODALIDA 2007.
Mark Johnson. 2007. Why doesn?t EM find good HMM
POS-taggers. In Proceedings of EMNLP-CoNLL, pages
296?305.
Jeremy G Kahn, Matthew Lease, Eugene Charniak, Mark
Johnson, and Mari Ostendorf. 2005. Effective use of
prosody in parsing conversational speech. In Proceed-
ings of HLT-EMNLP, pages 233?240.
Dan Klein and Christopher D. Manning. 2004. Corpus-
based induction of syntactic structure: Models of de-
pendency and constituency. In Proceedings of ACL,
pages 479?486.
Matthias Trautner Kromann. 2003. The Danish Depen-
dency Treebank and the DTAG treebank tool. In Pro-
ceedings of the Second Workshop on Treebanks and
Linguistic Theories, pages 217?220.
Kenichi Kurihara and Taisuke Sato. 2006. Variational
Bayesian grammar induction for natural language. In
Proceedings of the International Colloquium on Gram-
matical Inference, pages 84?96.
Brian MacWhinney. 2000. The CHILDES project: Tools
for analyzing talk. Lawrence Erlbaum Associates, Mah-
wah, NJ, third edition.
Se?verine Millotte, Roger Wales, and Anne Christophe.
2007. Phrasal prosody disambiguates syntax. Lan-
guage and Cognitive Processes, 22(6):898?909.
James L Morgan, Richard P Meier, and Elissa L Newport.
1987. Structural packaging in the input to language
learning: contributions of prosodic and morphologi-
cal marking of phrases to the acquisition of language.
Cognitive Psychology, 19:498?550.
Tahira Naseem and Regina Barzilay. 2011. Using seman-
tic cues to learn syntax. In Proceedings of AAAI.
John K Pate and Sharon Goldwater. 2011. Unsupervised
syntactic chunking with acoustic cues: computational
models for prosodic bootstrapping. In Proceedings
of the 2nd ACL workshop on Cognitive Modeling and
Computational Linguistics.
73
Elias Ponvert, Jason Baldridge, and Katrin Erk. 2011.
Simple unsupervised grammar induction from raw text
with cascaded finite state models. In Proceedings of
ACL-HLT.
Patti J Price, Mari Ostendorf, Stefanie Shattuck-Hufnagel,
and Cynthia Fong. 1991. The use of prosody in syntac-
tic disambiguation. In Proceedings of the HLT work-
shop on Speech and Natural Language, pages 372?377,
Morristown, NJ, USA. Association for Computational
Linguistics.
C Anton Rytting, Chris Brew, and Eric Fosler-Lussier.
2010. Segmenting words from natural speech: subseg-
mental variation in segmental cues. Journal of Child
Language, 37(3):513?543.
Roy Schwartz, Omri Abend, Roi Reichart, and Ari Rap-
poport1. 2011. Neutralizing linguistically problematic
annotations in unsupervised dependency parsing evalu-
ation. In Proceedings of the 49th ACL, pages 663?672.
Yoav Seginer. 2007. Fast unsupervised incremental pars-
ing. In Proceedings of ACL.
Amanda Seidl. 2007. Infants? use and weighting of
prosodic cues in clause segmentation. Journal of Mem-
ory and Language, 57(1):24?48.
Valentin I Spitkovsky, Hiyan Alshawi, Angel X Chang,
and Daniel Jurafsky. 2011a. Unsupervised dependency
parsing without gold part-of-speech tags. In Proceed-
ings of EMNLP.
Valentin I Spitkovsky, Hiyan Alshawi, and Daniel Jurafsky.
2011b. Punctuation: Making a point in unsupervised
dependency parsing. In Proceedings of CoNLL.
Harry Tily, Susanne Gahl, Inbal Arnon, Neal Snider,
Anubha Kothari, and Joan Bresnan. 2009. Syntactic
probabilities affect pronunciation variation in sponta-
neous speech. Language and Cognition, 1(2):147?165.
74
Proceedings of the 2nd Workshop on Cognitive Modeling and Computational Linguistics, pages 20?29,
Portland, Oregon, June 2011. c?2011 Association for Computational Linguistics
Unsupervised syntactic chunking with acoustic cues:
computational models for prosodic bootstrapping
John K Pate (j.k.pate@sms.ed.ac.uk)
Sharon Goldwater (sgwater@inf.ed.ac.uk)
School of Informatics, University of Edinburgh
10 Crichton St., Edinburgh EH8 9AB, UK
Abstract
Learning to group words into phrases with-
out supervision is a hard task for NLP sys-
tems, but infants routinely accomplish it. We
hypothesize that infants use acoustic cues to
prosody, which NLP systems typically ignore.
To evaluate the utility of prosodic information
for phrase discovery, we present an HMM-
based unsupervised chunker that learns from
only transcribed words and raw acoustic cor-
relates to prosody. Unlike previous work on
unsupervised parsing and chunking, we use
neither gold standard part-of-speech tags nor
punctuation in the input. Evaluated on the
Switchboard corpus, our model outperforms
several baselines that exploit either lexical or
prosodic information alone, and, despite pro-
ducing a flat structure, performs competitively
with a state-of-the-art unsupervised lexical-
ized parser, with a substantial advantage in
precision. Our results support the hypothesis
that acoustic-prosodic cues provide useful ev-
idence about syntactic phrases for language-
learning infants.
1 Introduction
Young children routinely learn to group words into
phrases, yet computational methods have so far
struggled to accomplish this task without supervi-
sion. Previous work on unsupervised grammar in-
duction has made progress by exploiting information
such as gold-standard part of speech tags (e.g. Klein
and Manning (2004)) or punctuation (e.g. Seginer
(2007)). While this information may be available
in some NLP contexts, our focus here is on the com-
putational problem facing language-learning infants,
who do not have access to either part of speech
tags or punctuation. However, infants do have ac-
cess to certain cues that have not been well explored
by NLP researchers focused on grammar induction
from text. In particular, we consider the cues to syn-
tactic structure that might be available from prosody
(roughly, the structure of speech conveyed through
rhythm and intonation) and its acoustic realization.
The idea that prosody provides important ini-
tial cues for grammar acquisition is known as the
prosodic bootstrapping hypothesis, and is well-
established in the field of language acquisition
(Gleitman and Wanner, 1982). Experimental work
has provided strong support for this hypothesis, for
example by showing that infants begin learning ba-
sic rhythmic properties of their language prenatally
(Mehler et al, 1988) and that 9-month-olds use
prosodic cues to distinguish verb phrases from non-
constituents (Soderstrom et al, 2003). However, as
far as we know, there has so far been no direct com-
putational evaluation of the prosodic bootstrapping
hypothesis. In this paper, we provide the first such
evaluation by exploring the utility of acoustic cues
for unsupervised syntactic chunking, i.e., grouping
words into non-hierarchical syntactic phrases.
Nearly all previous work on unsupervised gram-
mar induction has focused on learning hierarchical
phrase structure (Lari and Young, 1990; Liang et al,
2007) or dependency structure (Klein and Manning,
2004); we are aware of only one previous paper
on unsupervised syntactic chunking (Ponvert et al,
2010). Ponvert et al describe a simple method for
chunking that uses only bigram counts and punctu-
ation; when the chunks are combined using a right-
branching structure, the resulting trees achieve un-
labeled bracketing precision and recall that is com-
petitive with other unsupervised parsers. The sys-
20
tem?s dependence on punctuation renders it inappro-
priate for addressing the questions we are interested
in here, but its good performance reccommends syn-
tactic chunking as a profitable approach to the prob-
lem of grammar induction, especially since chunks
can be learned using much simpler models than are
needed for hierarchical structure.
The models used in this paper are all variants of
HMMs. Our baseline models are standard HMMs
that learn from either lexical or prosodic observa-
tions only; we also consider three types of models
(including a coupled HMM) that incorporate both
lexical and prosodic observations, but vary the de-
gree to which syntactic and prosodic variables are
tied together in the latent structure of the models.
In addition, we compare the use of hand-annotated
prosodic information (ToBI annotations) to the use
of direct acoustic measures (specifically, duration
measures) as the prosodic observations. All of our
models are unsupervised, receiving no bracketing
information during training.
The results of our experiments strongly support
the prosodic bootstrapping hypothesis: we find
that using either ToBI annotations or acoustic mea-
sures in addition to lexical observations (i.e., word
sequences) vastly improves chunking performance
over any source of information alone. Interestingly,
our best results are achieved using a combination
of words and acoustic information as input, rather
than words and ToBI annotations. Our best com-
bined model achieves an F-score of 41% when eval-
uated on the lowest level of syntactic structure in
the Switchboard corpus1, as compared to 25% for
a words-only model and only 3% for an acoustics-
only model. Although the combined model?s score
is still fairly low, additional results suggest that our
corpus of transcribed naturalistic speech is signifi-
cantly more difficult for unsupervised parsing than
the written text that is typically used for training.
Specifically, we find that a state-of the-art unsuper-
vised lexicalized parser, the Common Cover Link
1Since our interest is in child language acquisition, we
would prefer to evaluate our system on data from the CHILDES
database of child-directed speech (MacWhinney, 2000). Unfor-
tunately, there are no corpora in the database that include phrase
structure annotations. We are in the process of annotating a
small evaluation corpus with phrase structure trees, and hope to
use this for evaluation in future work.
(CCL) parser (Seginer, 2007), achieves only 38%
unlabeled bracketing F-score on our corpus, as com-
pared to published results of 76% on WSJ10 (En-
glish) and 59% on Negra10 (German). Interestingly,
we find that when evaluated against full parse trees,
our best chunker achieves an F-score comparable to
that of CCL despite positing only flat structure.
Before describing our models and experiments in
more detail, we first present a brief review of rel-
evant information about prosody and its relation-
ship to syntax, including previous work combining
prosody and syntax in supervised parsing systems.
2 Prosody and syntax
Prosody is a theoretical linguistic concept posit-
ing an abstract organizational structure for speech.2
While it is often closely associated with such mea-
surable phenomena as movement in fundamen-
tal frequency or variation in spectral tilt, these
are merely observable acoustic correlates that pro-
vide evidence of varying quality about the hidden
prosodic structure, which specifies such hidden vari-
ables as contrastive stress or question intonation.
Prosody has been hypothesized to be useful for
learning syntax because it imposes a grouping struc-
ture on word sequences that sometimes coincides
with traditional constituency analyses (Ladd, 1996;
Shattuck-Hufnagel and Turk, 1996). Moreover,
laboratory experiments have shown that adults use
prosody both for syntactic disambiguation (Millotte
et al, 2007; Price et al, 1991) and, crucially, in
learning the syntax of an artificial language (Morgan
et al, 1987). Accordingly, if prosodic structure is
sufficiently prominent in the acoustic signal, and co-
incides often enough with syntactic structure, then it
may provide children with useful information about
how to combine words into phrases.
Although there are several theories of how to rep-
resent and annotate prosodic structure, one of the
most influential is the ToBI (Tones and Break In-
dices) theory (Beckman et al, 2005), which we
will use in some of our experiments. ToBI pro-
poses, among other things, that the prosodic phras-
ing of languages can be represented in terms of se-
quences of break indices indicating the strength of
2Signed languages also exhibit prosodic phenomena, but
they are not addressed here.
21
word boundaries. In Mainstream American English
ToBI, for example, the boundary between a clitic
and its base word (e.g. ?do? and ?n?t? of ?don?t?)
is 0, representing a very weak boundary, while the
boundary following a word at the end of an intona-
tional phrase is 4, indicating a very strong boundary.
Below we examine how useful these break indices
are for identifying syntactic boundaries.
Finally, we note that our work is not the first com-
putational approach to using prosody for identifying
syntactic structure. However, previous work (Gre-
gory et al, 2004; Kahn et al, 2005; Dreyer and
Shafran, 2007; No?th et al, 2000) has focused on
supervised parsing rather than unsupervised chunk-
ing, and also makes different assumptions about
prosody. For example, Gregory et al (2004) assume
that prosody is an acoustically-realized substitute for
punctuation; our own treatment is much less con-
strained. Kahn et al (2005) and Dreyer and Shafran
(2007) use ToBI labels to represent prosodic infor-
mation, whereas we explore both ToBI and direct
acoustic measures. Finally, No?th et al (2000) do not
use ToBI, instead developing a novel prosodic anno-
tation system designed specifically to provide cues
to syntax and for annotation efficiency. However,
their system is supervised and focuses on improving
parse speed rather than accuracy.
3 Models
Following previous work (e.g. Molina and Pla
(2002) Sha and Pereira (2003)), we formulate
chunking as a tagging task. We use Hidden Markov
Models (HMMs) and their variants to perform the
tagging, with carefully specified tags and con-
strained transition distributions to allow us to inter-
pret the results as a bracketing of the input. Specif-
ically, we use four chunk tags: B (?Begin?) and
E (?End?) tags are interpreted as the first and last
words of a chunk, respectively, with I (?Inside?)
corresponding to other words inside a chunk and O
(?Outside?) to all other words. The transition ma-
trices are constrained to afford 0 probability to tran-
sitions that violate these definitions. Additionally,
the initial probabilities are constrained to forbid the
models from starting inside or at the end of a phrase.
We use this four-tag OBIE tagset rather than the
more typical three-tag IOB tagset for two reasons.
First, the OBIE set forces all chunks to be at least
two words long (the shortest chunk allowed is B E).
Imposing this requirement allows us to characterize
the task in concrete terms as ?learning when to group
words together.? Second, as we seek to incorporate
acoustic correlates of prosody into chunking, we ex-
pect edge behavior to merit explicit modeling.3
In the following subsections, we describe the var-
ious models we use. Note that input to all mod-
els is discrete, consisting of words, ToBI annota-
tions, and/or discretized acoustic measures (we de-
scribe these measures and their discretization in Sec-
tion 3.3). See Figure 1 for examples of system input
and output; different models will receive different
combinations of the three kinds of input.
3.1 Baseline Models
Our baseline models are all standard HMMs, with
the graphical structure shown in Figure 2(a). The
first baseline uses lexical information only; the ob-
servation at each time step is the phonetic transcrip-
tion of the current word in the sentence. To han-
dle unseen words at test time, we use an ?UNK.?
token to replace all words in the training and eval-
uation sets that appear less than twice in the train-
ing data. Our second baseline uses prosodic infor-
mation only; the observation at each time step is
the hand-annotated ToBI Break Index for the cur-
rent word, which takes on one of six values: { 0, 1,
2, 3, 4, X, None }.4 Our final baseline uses acous-
tic information only. The observations are one of
six automatically determined clusters in an acoustic
space, as described in Section 3.3.
We trained the HMMs using Baum-Welch, and
used Viterbi for inference.5
3Indeed, when we tried using the IOB tag set in prelimi-
nary experiments, dev-set performance dropped substantially,
supporting this latter intuition.
4The numerical break indices indicate breaks of increas-
ing strength, ?X? represents a break of uncertain strength, and
?None? indicates that the preceding word is outside one of the
fluent prosodic phrases selected for annotation. Additional dis-
tinctions marked by ?-? and ?p? were ignored.
5We actually used the junction tree algorithm from MAL-
LET, which, in the special case of an HMM, reduces to the
Forward-Backward algorithm when using Sum-Product mes-
sages, and to the Viterbi algorithm when using Max-Product
messages. Our extension of MALLET to build junction trees
efficiently for Dynamic Bayes Nets is available online, and is
being prepared for submission to the main MALLET project.
22
(a) Words g.aa dh.ae.t.s dh.ae.t s.aw.n.d.z p.r.ih.t.iy b.ae.d t.ax m.iy
Acoustics 4 4 6 4 5 4 5 6
ToBI 1 2 1 1 1 1 1 3
(b) O O B I I E B E
(c) ( ) ( )
(d) ( ( ) ( ) )
Figure 1: (a) Example input sequences for the three types of input (phonetic word transcriptions, acoustic clusters, and
ToBI break indices). (b) Example output tags. (c) The bracketing corresponding to (b). (d) The flat tree built from (b).
w1
C1
w2
C2
w3
C3
  
// //
(a) Standard HMM (HMM)
w1
C1
d1
w2
C2
d2
w3
C3
d3

OO
 
OO
// //
OO
(b) Two Output HMM (THMM)
w1
C1
P1
d1
w2
C2
P2
d2
w3
C3
P3
d3

OO

OO

OO
%%
99
//
//
%%
99
//
//
(c) Coupled HMM (CHMM)
Figure 2: Graphical structures for our various HMMs. ci nodes are constrained using the OBIE system, pi nodes
are not. wi nodes represent lexical outputs, and di nodes represent acoustic or ToBI outputs. (Rectangular nodes are
observed, circular nodes are hidden).
3.2 Combined Models
As discussed in Section 2, previous theoretical
and experimental work suggests a combined model
which models uncertainty both between prosody and
acoustics, and between prosody and syntax. To mea-
sure the importance of modeling these kinds of un-
certainty, we will evaluate a series of model struc-
tures that gradually divorce acoustic-prosodic cues
from lexical-syntactic cues.
Our first model is the standard HMM from Fig-
ure 2(a), but generates a (word, acoustics) or (word,
ToBI) pair at each time step. This model has the sim-
plest structure, but includes a separate parameter for
every unique (state, word, acoustics) triple, so may
be too unconstrained to learn anything useful.
To reduce the number of parameters, we pro-
pose a second model that assumes independence be-
tween the acoustic and lexical observations, given
the syntactic state. We call this a ?Two-output HMM
(THMM)? and present its graphical structure in Fig-
ure 2(b). It is straightforward to extend Baum-Welch
to accommodate the extra outputs of the THMM.
Finally, we consider a model that explicitly rep-
resents prosodic structure distinctly from syntactic
structure with a second sequence of tags. We use
a Coupled HMM (CHMM) (Nefian et al, 2002),
which models a set of observation sequences us-
ing a set of hidden variable sequences. Figure 2(c)
presents a two-stream Coupled HMM for three time
steps. The model consists of an initial state proba-
bility distribution pis for each stream s, a transition
matrix as for each stream s conditioning the distri-
bution of stream s at time t + 1 on the state of both
streams at time t, and an emission matrix bs for each
stream conditioning the observation of stream s at
time t on the hidden state of stream s at time t.6
Intuitively, the states emitting acoustic measures
operationalize prosodic structure, and the states
emitting words operationalize syntactic structure.
Crucially, Coupled HMMs impose no a priori cor-
respondence between variables of different streams,
allowing our ?syntactic? states to vary freely from
our ?prosodic? states. As two-stream CHMMs
maintain two emission matrices, two transition ma-
6We explored a number of minor variations on this graphical
structure, but preliminary experiments yielded no improvement.
23
trices, and two initial state distributions, they are
more complex than the other combined models, but
more closely embody intuitions inspired by previous
work on the prosody-syntax interface.
Our Coupled HMMs were also trained using EM.
Marginals for the E-step were computed using the
implementation of the junction tree algorithm avail-
able in MALLET (McCallum, 2002; Sutton, 2006).
During test, the Viterbi tag sequence for each model
is obtained by simply replacing the sum-product
messages with max-product messages.
3.3 Acoustic Cues
As explained in Section 2, prosody is an abstract hid-
den structure which only correlates with observable
features of the acoustic signal, and we seek to select
features which are both easy to measure and likely to
correlate strongly with the hidden prosodic phrasal
structure. While there are many possible cues, we
have chosen to use duration cues. These should pro-
vide good evidence about phrases due to the phe-
nomenon of pre-boundary lengthening (e.g. Beck-
man and Edwards (1990), Wightman et al (1992)),
wherein words, and their final rime, lengthen phrase-
finally. This is likely especially useful for English
due to the lack of confounding segmental duration
contrasts (although variation in duration is unpre-
dictably distributed (Klatt, 1976)), but should be
useful in varying degrees for other languages.
We gather five duration measures:
1. Log total word duration: The annotated word
end time minus the annotated word start time.
2. Log onset duration: The duration from the be-
ginning of the word to the end of the first vowel.
3. Log offset duration: The duration from the be-
ginning of the last vowel to the end of the word.
4. Onset proportion consonant: The duration of
the non-vocalic portion of the word onset di-
vided by the total onset duration.
5. Offset proportion consonant: The duration of
the non-vocalic portion of the word offset di-
vided by the total offset duration.
If a word contains no canonical vowels, then the
first and last sonorants are counted as vocalic. If a
Train Dev Test
Words 68,533 7,981 8,746
Sentences 6,420 778 802
Table 1: Data set statistics
word contains no vowels or sonorants, then the on-
set and offset are the entire word and the propor-
tion consonant for both onset and offset is 1 (this
occurred for 186 words in our corpus).
The potential utility of this acoustic space was
verified by visual inspection of the first few PCA
components, which suggested that the position of a
word in this acoustic space correlated with bracket
count. We discretize the raw (i.e. non-PCA) space
with k-means with six initially random centers for
consistency with the number of ToBI break indices.
4 Experiments
4.1 Dataset
All experiments were performed on part of the Nite
XML Toolkit edition of the Switchboard corpus
(Calhoun et al, 2010). Specifically, we gathered all
conversations which have been annotated for syn-
tax, ToBI, and Mississippi State phonetic alignments
(which lack punctuation).7 The syntactic parses,
word sequences, and ToBI break indices were hand-
annotated by trained linguists, while the Mississippi
State phonetic alignments were automatically pro-
duced by a forced alignment of the speech signal
to a pronunciation-dictionary based phone sequence,
providing an estimate of the beginning and end time
of each phone. A small number of annotation er-
rors (in which the beginning and end times of some
phones had been swapped) were corrected by hand.
This corpus has 74 conversations with two sides
each.
We split this corpus into an 80%/10%/10%
train/dev/test 8 partition by dividing the entire cor-
pus into ten-sentence chunks, assigning the first
eight to the training partition, and the ninth and tenth
to the dev and test partitions, respectively. We then
removed all sentences containing only one or two
7We threw out a small number of sentences with annotations
errors, e.g. pointing to missing words.
8The dev set was used to explore different model structures
in preliminary experiments; all reported results are on the test
set.
24
words. Sentences this short have a trivial parse, and
are usually formulaic discourse responses (Bell et
al., 2009), which may influence their prosody. The
final corpus statistics are presented in Table 1.
4.2 Evaluation
We use the Penn Treebank parsed version of Switch-
board for evaluation. This version uses a slightly
different tokenization from the Mississippi State
transcriptions that were used as input to the mod-
els, so we transformed the Penn treebank tokeniza-
tion to agree with the Mississippi State tokeniza-
tion (primarily by concatenating clitics to their base
words?i.e. ?do? and ??nt? into ?don?t??and split-
ting multi-word expressions). We also removed all
gold-standard nodes spanning only Trace or PUNC
(recall that the input to the models did not include
punctuation) and collapsed all unary productions.9
In all evaluations, we convert our models? out-
put tag sequence to a set of matched brackets by in-
serting a left bracket preceding each word tagged B
tag and a right bracket following each word tagged
E. This procedure occasionally results in a sentence
with an unmatched opening bracket. If the un-
matched opening bracket is one word from the end
of the sentence, we delete it, otherwise we insert a
closing bracket at the end of the sentence. Figure 1
shows example input sequences together with exam-
ple output tags and their corresponding bracketings.
Previous work on chunking, most notably the
2000 CONLL shared task (Tjong et al, 2000), has
defined gold standard chunks that are useful for find-
ing grammatical relations but which do not corre-
spond to any particular linguistic notion. It is not
clear that such chunks should play a role in lan-
guage acquisition, so instead we evaluate against tra-
ditional syntactic constituents from Penn Treebank-
style parses in two different ways.
Our first evaluation method compares the output
of the chunkers to what Ponvert et al (2010) call
clumps, which are just syntactic constituents that
span only terminals. We created our clump gold-
standard by taking the parse trees resulting from the
preprocessing described above and deleting nodes
that span a non-terminal. Figure 3 presents an ex-
9As we evaluate unlabeled bracketing precision and recall,
the label of the resulting nodes is irrelevant.
g.aa dh.ae.t.s dh.ae.t
s.aw.n.d.z
p.r.ih.t.iy b.ae.d t.ax m.iy
Figure 3: Example gold-standard with clumps in boxes.
ample gold-standard parse tree with the clumps in
boxes. This evaluation avoids penalizing chunkers
for not positing hierarchical structure, but rewards
chunkers only for finding very low-level structure.
In the interest of making no a priori assumptions
about the kinds of phrases our unsupervised method
recovers, we also evaluate our completely flat, non-
recursive chunks directly against the fully recursive
parses in the treebank. To do so, we turn our chun-
ked utterance into a flat tree by simply putting brack-
ets around the entire utterance as in Figure 1(d).
This evaluation penalizes chunkers for never posit-
ing hierarchical structure, but makes no assumptions
about which kinds of phrases ought to be found.
4.3 Models and training
In all, nine HMM models, two versions of the
CCL parser, and a uniform right-branching baseline
were evaluated. Three of the HMMs were standard
HMMs with chunking constraints on the four hidden
states (as described in Section 3.2) that received as
input either words, ToBI break indices, or word du-
ration cluster information, intended as baselines to
illuminate the utility of each information source in
isolation. We also ran two each of Coupled HMM
and Two-output HMM models that received words
in one observed chain and either ToBI break index or
duration cluster in the other observed chain. In the
CHMM models, chunking constraints were enforced
on the chain generating the words, while variables
generating the duration or ToBI information ranged
over four discrete states with no constraints.10 All
non-zero parameters were initialized approximately
uniformly at random,11 and we ran EM until the log
10We also tried imposing chunking constraints on the second
chain, but dev-set performance dropped slightly.
11In preliminary dev-set experiments, different random ini-
tializations performed within two points of each other.
25
Condition Prec Rec F-sc
B
as
el
in
es
H
M
M
Wds 23.5 39.9 26.3
BI 7.2 4.8 5.8
Ac 4.7 2.5 3.3
C
om
bi
ne
d
M
od
el
s H
M
M Wds+BI 24.4 22.2 23.2
Wds+Ac 20.7 22.7 21.7
T
H
M
M Wds+BI 18.2 19.6 18.9
Wds+Ac 36.1 47.8 41.2
C
H
M
M Wds+BI 25.5 36.3 29.9
Wds+Ac 33.6 48.1 39.5
C
C
L Parser 15.4 41.5 22.4
Clumper 36.8 37.9 37.3
Table 2: Scores for all models, evaluated on clumps. In-
put is words (Wds), break indices (BI), and/or acoustics.
corpus probability changed less than 0.001%, typi-
cally for 50-150 iterations.
The CCL parser was trained on the same word se-
quences provided to our models. We also evaluated
the CCL parser as a clumper (CCL Clumper) by re-
moving internal nodes spanning a non-terminal. The
right-branching baseline was generated by inserting
one opening bracket in front of all but the last word,
and closing all brackets at the end of the sentence.
4.4 Results and Discussion
Table 2 presents results for our flat chunkers evalu-
ated against Ponvert et al (2010)-style clumps. Sev-
eral points are apparent. First, all three HMM base-
lines yield very poor results, especially the prosodic
baselines, whose precision and recall are both be-
low 10%. Although the best combined models
still have relatively low performance, it is markedly
higher than either of the individual baselines, and
also higher than the clumps identified by the CCL
parser. Particularly notable is the fact that lexi-
cal and prosodic information appear to be super-
additive in some cases, yielding combined perfor-
mance that is higher than the sum of the individual
scores. Not all combined models work equally well,
however: the poor performance of the HMM com-
bined model supports our initial hypothesis that it is
over parameterized. Interestingly, our acoustic clus-
ters work better than break indices when combined
with words. Finally, we see that the THMM and
CHMM obtain similar performance using words +
acoustics, suggesting that modeling prosodic struc-
% Covered words
chunk
chunk
uttCondition Words Utts
B
as
el
in
es
H
M
M
Wds 81.9 98.4 3.16 2.82
BI 68.2 68.1 4.95 1.50
Ac 46.3 71.1 4.18 1.21
C
om
bi
ne
d
M
od
el
s H
M
M Wds+BI 79.8 98.3 4.30 2.02
Wds+Ac 83.3 98.5 3.71 2.45
T
H
M
M Wds+BI 84.6 99.0 3.84 2.40
Wds+Ac 68.0 96.1 2.52 2.94
C
H
M
M Wds+BI 83.1 99.0 2.86 3.17
Wds+Ac 76.5 97.6 2.62 3.19
CCL Clumper 48.3 99.9 2.30 2.29
Table 3: % words in a chunk, % utterances with > 0
chunks, and mean chunk length and chunks per utterance.
Condition Prec Rec F-sc
B
as
el
in
es
H
M
M
Wds 48.8(32) 26.3(15) 34.2(20)
BI 52.4(21) 18.5(5) 27.3(8)
Ac 52.5(15) 16.3(3) 24.9(5)
C
om
bi
ne
d
M
od
el
s H
M
M Wds+BI 54.4(32) 23.2(11) 32.5(16)
Wds+Ac 51.0(32) 24.7(13) 33.3(18)
T
H
M
M Wds+BI 55.9(38) 26.8(15) 36.2(21)
Wds+Ac 55.8(41) 31.0(20) 39.9(27)
C
H
M
M Wds+BI 48.4(32) 28.4(17) 35.8(22)
Wds+Ac 54.1(40) 31.9(21) 40.1(28)
C
C
L Parser 38.2(28) 37.6(28) 37.9(28)
Clumper 58.8(42) 27.3(16) 37.3(23)
Table 4: Model performance, evaluated on full trees.
Scores in parentheses were computed after removing the
full sentence bracket, which provides a free true positive.
ture separately from syntactic structure may be un-
necessary (or that the CHMM does so badly).
To provide further intuition into the kinds of
chunks recovered by the different models, we list
some relevant statistics in Table 3. These statis-
tics show that the models using lexical information
identify at least one chunk in virtually all utterances,
with the better models averaging 2-3 chunks per ut-
terance of around 3 words each. In contrast, the
unlexicalized models find longer chunks (4-5 words
each) but far fewer of them, with about 30% of ut-
terances containing none at all.
We turn now to the models? performance on full
parse trees, shown in Table 4. Two different scores
are given for each system: the first includes the
top-level bracketing of the full sentence (which is
26
standard in computing bracketing accuracy, but is a
free true positive), while the second does not (for a
more accurate picture of the system?s performance
on ambiguous brackets). Comparing the second set
of scores to the clumping evaluation, recall is much
lower for all the chunkers; the relatively small in-
crease in precision indicates that the chunkers are
most effective at finding low-level structure. For
both sets of scores, the relative F-scores of the chun-
kers are similar to the clumping evaluation, with
the words + acoustics versions of the THMM and
CHMM scoring best. Not surprisingly, the CCL
parser has much higher recall than the chunkers,
though the best chunkers have much higher preci-
sion. The result is that, using standard Parseval
scoring (first column), the best chunkers outperform
CCL on F-score; even discounting the free sentence-
level bracket (second column) they do about as well.
It is worth noting that, although CCL achieves
state-of-the-art performance on the English WSJ
and German Negra corpora (Seginer (2007) reports
75.9% F-score on WSJ10, for example), its perfor-
mance on our corpus is far lower. In fact, on this cor-
pus the CCL parser (as well as our chunkers) under-
perform a uniform right-branching baseline, which
obtains 42.2% precision and 64.8% recall (including
the top-level bracket), leading to an overall F-score
of 51.1%. This suggests that our corpus is signifi-
cantly more difficult than WSJ, probably due to dis-
fluencies and/or lack of punctuation.12 Moreover,
we stress that the use of a right-branching baseline,
while useful as a measure of overall performance,
is not plausible as a model of language acquisition
since it is highly language-specific.
5 Conclusion
Taken together, our results indicate that a purely
local model that combines lexical and acoustic-
prosodic information in an appropriate way can
identify syntactic phrases far more effectively than
a similar model using either source of information
alone. Our best combined models outperformed
the baseline individual models by a wide margin
when evaluated against the lowest level of syntac-
tic structure, and their performance was compara-
12Including punctuation improves CCL little, possibly be-
cause the punctuation in this corpus is nearly all sentence-final.
ble to CCL, a state-of-the-art unsupervised lexical-
ized parser, when evaluated against full parse trees.
It is disappointing that all of these systems scored
worse than a right-branching baseline, but this result
underscores the major differences between parsing
spoken utterances (even using transcriptions) and
parsing written text (where CCL and other unsu-
pervised parsers were developed and tested). Since
children learning language do not (at least initially)
know the head direction of their language, the right-
branching baseline for English is not available to
them. Thus, combining lexical and acoustic cues
may provide them with initial useful information
about the location of syntactic phrases, as suggested
by the prosodic bootstrapping hypothesis.
Nevertheless, we caution against assuming that
the usefulness of acoustic information must re-
sult from its relation to prosody (especially be-
cause we found that direct acoustic information was
more useful than hand-annotated prosodic labels).
The ?Smooth Signal Hypothesis? (Aylett and Turk,
2004) posits that talkers modulate their communica-
tive effort according to the predictability of their
message in order to achieve efficient communica-
tion, pronouncing more predictable parts of mes-
sages more quickly or less distinctly. If talkers con-
sider syntactic predictability in this process, then
it is possible that acoustic cues help initial gram-
mar learning not by serving as cues to prosody but
by serving as cues to the talker?s syntax-dependent
view of predictability. In this case, it may make
more sense to discuss ?predictability bootstrapping?
rather than ?prosodic bootstrapping.?
Regardless of the underlying reason, we have
shown that acoustic cues can be useful for identi-
fying syntactic structure when used in combination
with lexical information. In order to further substan-
tiate these results, we plan to replicate our experi-
ments on a corpus of child-directed speech, which
we are currently annotating for evaluation purposes.
We also hope to extend our findings to a model that
can identify hierarchical structure, and to analyze
more carefully the reasons for CCL?s poor perfor-
mance on the Switchboard corpus, in hopes of devel-
oping a model that can reach levels of performance
closer to those typical of unsupervised parsers for
written text.
27
References
Matthew Aylett and Alice Turk. 2004. The smooth sig-
nal redundancy hypothesis: A functional explanation
for relationships between redundancy, prosodic promi-
nence, and duration in spontaneous speech. Language
and Speech, 47(1):31 ? 56.
Mary E. Beckman and Jan Edwards. 1990. Lengthen-
ings and shortenings and the nature of prosodic con-
stituency. In J. Kingston and Mary E. Beckman, edi-
tors, Between the grammar and physics of speech: Pa-
pers in laboratory phonology I, pages 152?178. Cam-
bridge: Cambridge University Press.
M Beckman, J Hirschberg, and S Shattuck-Hufnagel.
2005. The original tobi system and the evolution of
the tobi framework. In S.-A. Jun, editor, Prosodic Ty-
pology ? The Phonology of Intonation and Phrasing.
Oxford University Press.
Alan Bell, Jason M. Brenier, Michelle Gregory, Cynthia
Girand, and Dan Jurafsky. 2009. Predictability effects
on durations of content and function words in conver-
sational english. Journal of Memory and Language,
60:92 ? 111.
S Calhoun, J Carletta, J Brenier, N Mayo, D Jurafsky,
M Steedman, and D Beaver. 2010. The nxt-format
switchboard corpus: A rich resource for investigat-
ing the syntax, semantics, pragmatics and prosody
of dialogue. Language Resources and Evaluation,
44(4):387 ? 419.
Markus Dreyer and Izhak Shafran. 2007. Exploiting
prosody for pcfgs with latent annotations. In Proc. of
Interspeech, Antwerp, Belgium, August.
L. Gleitman and E. Wanner. 1982. Language acquisition:
The state of the art. In E. Wanner and L. Gleitman, ed-
itors, Language acquisition: The state of the art, pages
3?48. Cambridge University Press, Cambridge, UK.
Michelle L. Gregory, Mark Johnson, and Eugene Char-
niak. 2004. Sentence-internal prosody does not help
parsing the way punctuation does. In Proceedings
of the North American Association for Computational
Linguistics (NAACL), pages 81?88.
J. G. Kahn, M. Lease, E. Charniak, M. Johnson, and
M. Ostendorf. 2005. Effective use of prosody in pars-
ing conversational speech. In Proc. of HLT/EMNLP-
05.
D H Klatt. 1976. Linguistic uses of segmental durations
in english: Acoustic and perceptual evidence. JASA,
59:1208 ? 1221.
Dan Klein and Christopher D. Manning. 2004. Corpus-
based induction of syntactic structure: Models of de-
pendency and constituency. In Proceedings of the
42nd Annual Meeting of the Association for Compu-
tational Linguistics (ACL 2004), pages 479?486.
Bob Ladd. 1996. Intonational Phonology. Cambridge
University Press.
K Lari and S J Young. 1990. The estimation of stochastic
context-free grammars using the inside-outside algo-
rithm. Computer Speech and Language, 5:237 ? 257.
P. Liang, S. Petrov, M. I. Jordan, and D. Klein. 2007. The
infinite PCFG using hierarchical Dirichlet processes.
In Empirical Methods in Natural Language Process-
ing and Computational Natural Language Learning
(EMNLP/CoNLL).
Brian MacWhinney. 2000. The CHILDES project: Tools
for analyzing talk. Lawrence Erlbaum Associates,
Mahwah, NJ, third edition.
Andrew Kachites McCallum. 2002. Mal-
let: A machine learning for language toolkit.
http://mallet.cs.umass.edu.
Jacques Mehler, Peter Juszcyk, Ghislaine Lambertz,
Nilofar Halsted, Josiane Bertoncini, and Claudine
Amiele-Tison. 1988. A precursor to language acqui-
sition in young infants. Cognition, 29:143 ? 178.
Se?verine Millotte, Roger Wales, and Anne Christophe.
2007. Phrasal prosody disambiguates syntax. Lan-
guage and Cognitive Processes, 22(6):898 ? 909.
Antonio Molina and Feran Pla. 2002. Shallow parsing
using specialized HMMs. Journal of Machine Learn-
ing Research, 2:595 ? 613.
James L. Morgan, Richard P. Meier, and Elissa L. New-
port. 1987. Structural packaging in the input to lan-
guage learning: contributions of prosodic and morpho-
logical marking of phrases to the acquisition of lan-
guage. Cognitive Psychology, 19:498 ? 550.
Ara V. Nefian, Luhong Liang, Xiaobao Pi, Liu Xiaoxi-
ang, Crusoe Moe, and Kevin Murphy. 2002. A cou-
pled hmm for audiovisual speech recognition. In Inter-
national Conference on Acoustics, Speech and Signal
Processing.
Elmer No?th, Anton Batliner, Andreas Kieling, and Ralfe
Kompe. 2000. Verbmobil: The use of prosody in the
linguistic components of a speech understanding sys-
tem. IEEE Transactions on Speech and Audio Pro-
cessing, 8(5).
Elias Ponvert, Jason Baldridge, and Katrin Erk. 2010.
Simple unsupervised identification of low-level con-
stituents. In ICSC.
P J Price, M Ostendorf, S Shattuck-Hufnagel, and
C Fong. 1991. The use of prosody in syntactic dis-
ambiguation. JASA, pages 2956 ? 2970.
Yoav Seginer. 2007. Fast unsupervised incremental pars-
ing. In Proceedings of the Association of Computa-
tional Linguistics.
Fei Sha and Fernando Pereira. 2003. Shallow pars-
ing with conditional random fields. In Proceedings of
HLT-NAACL 03, pages 213?220.
28
Stefanie Shattuck-Hufnagel and Alice E Turk. 1996. A
prosody tutorial for investigators of auditory sentence
processing. Journal of Psycholinguistic Research,
25(2):193 ? 247.
M. Soderstrom, A. Seidl, D. G. K. Nelson, and P. W.
Jusczyk. 2003. The prosodic bootstrapping of
phrases: Evidence from prelinguistic infants. Journal
of Memory and Language, 49:249?267.
Charles Sutton. 2006. Grmm: Graphical models in mal-
let. http://mallet.cs.umass.edu/grmm/.
Erik F. Tjong, Kim Sang, and Sabine Buchholz. 2000.
Introduction to the conll-2000 shared task: Chunking.
In Proceedings of CoNLL-2000 and LLL-2000, Lis-
bon, Portugal.
C W Wightman, S Shattuck-Hufnagel, M. Ostendorf, and
P J Price. 1992. Segmental durations in the vicinity of
prosodic phrase boundaries. Journal of the Acoustical
Society of America, 91(3):1707 ? 1717.
29
