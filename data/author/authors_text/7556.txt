Proceedings of Human Language Technology Conference and Conference on Empirical Methods in Natural Language
Processing (HLT/EMNLP), pages 755?762, Vancouver, October 2005. c?2005 Association for Computational Linguistics
Translating with non-contiguous phrases
Michel Simard, Nicola Cancedda, Bruno Cavestro, Marc Dymetman,
Eric Gaussier, Cyril Goutte, Kenji Yamada
Xerox Research Centre Europe
FirstName.FamilyName@xrce.xerox.com
Philippe Langlais
RALI/DIRO Universite? de Montre?al
felipe@iro.umontreal.ca
Arne Mauser
RWTH Aachen University
arne.mauser@rwth-aachen.de
Abstract
This paper presents a phrase-based statis-
tical machine translation method, based
on non-contiguous phrases, i.e. phrases
with gaps. A method for producing such
phrases from a word-aligned corpora is
proposed. A statistical translation model
is also presented that deals such phrases,
as well as a training method based on the
maximization of translation accuracy, as
measured with the NIST evaluation met-
ric. Translations are produced by means of
a beam-search decoder. Experimental re-
sults are presented, that demonstrate how
the proposed method allows to better gen-
eralize from the training data.
1 Introduction
Possibly the most remarkable evolution of recent
years in statistical machine translation is the step
from word-based models to phrase-based models
(Och et al, 1999; Marcu and Wong, 2002; Yamada
and Knight, 2002; Tillmann and Xia, 2003). While
in traditional word-based statistical models (Brown
et al, 1993) the atomic unit that translation operates
on is the word, phrase-based methods acknowledge
the significant role played in language by multi-
word expressions, thus incorporating in a statistical
framework the insight behind Example-Based Ma-
chine Translation (Somers, 1999).
However, Phrase-based models proposed so far
only deal with multi-word units that are sequences
of contiguous words on both the source and the tar-
get side. We propose here a model designed to deal
with multi-word expressions that need not be con-
tiguous in either or both the source and the target
side.
The rest of this paper is organised as follows. Sec-
tion 2 provides motivations, definition and extrac-
tion procedure for non-contiguous phrases. The log-
linear conditional translation model we adopted is
the object of Section 3; the method used to train
its parameters is described in Section 4. Section 5
briefly describes the decoder. The experiments we
conducted to asses the effectiveness of using non-
contiguous phrases are presented in Section 6.
2 Non-contiguous phrases
Why should it be a good thing to use phrases
composed of possibly non-contiguous sequences of
words? In doing so we expect to improve trans-
lation quality by better accounting for additional
linguistic phenomena as well as by extending the
effect of contextual semantic disambiguation and
example-based translation inherent in phrase-based
MT. An example of a phenomenon best described
using non-contiguous units is provided by English
phrasal verbs. Consider the sentence ?Mary switches
her table lamp off?. Word-based statistical mod-
els would be at odds when selecting the appropri-
ate translation of the verb. If French were the target
language, for instance, corpus evidence would come
from both examples in which ?switch? is translated
as ?allumer? (to switch on) and as ?e?teindre? (to
switch off). If many-to-one word alignments are not
allowed from English to French, as it is usually the
755
2 31
Pierre
Pierre
ne mange pas
does not eat
Figure 1: An example of a complex alignment asso-
ciated with different syntax for negation in English
and French.
case, then the best thing a word-based model could
do in this case would be to align ?off? to the empty
word and hope to select the correct translation from
?switch? only, basically a 50-50 bet. While han-
dling inseparable phrasal verbs such as ?to run out?
correctly, previously proposed phrase-based models
would be helpless in this case. A comparable behav-
ior is displayed by German separable verbs. More-
over, non-contiguous linguistic units are not limited
to verbs. Negation is formed, in French, by inserting
the words ?ne? and ?pas? before and after a verb re-
spectively. So, the sentence ?Pierre ne mange pas?
and its English translation display a complex word-
level alignment (Figure 1) current models cannot ac-
count for.
Flexible idioms, allowing for the insertion of lin-
guistic material, are other phenomena best modeled
with non-contiguous units.
2.1 Definition and library construction
We define a bi-phrase as a pair comprising a source
phrase and a target phrase: b = ?s?, t??. Each of the
source and target phrases is a sequence of words and
gaps (indicated by the symbol ?); each gap acts as
a placeholder for exactly one unspecified word. For
example, w? = w1w2?w3?? w4 is a phrase of length
7, made up of two contiguous words w1 and w2, a
first gap, a third word w3, two consecutive gaps and
a final word w4. To avoid redundancy, phrases may
not begin or end with a gap. If a phrase does not
contain any gaps, we say it is contiguous; otherwise
it is non-contiguous. Likewise, a bi-phrase is said to
be contiguous if both its phrases are contiguous.
The translation of a source sentence s is produced
by combining together bi-phrases so as to cover the
source sentence, and produce a well-formed target-
language sentence (i.e. without gaps). A complete
translation for s can be described as an ordered se-
quence of bi-phrases b1...bK . When piecing together
the final translation, the target-language portion t?1
of the first bi-phrase b1 is first layed down, then each
subsequent t?k is positioned on the first ?free? posi-
tion in the target language sentence, i.e. either the
leftmost gap, or the right end of the sequence. Fig-
ure 2 illustrates this process with an example.
To produce translations, our approach therefore
relies on a collection of bi-phrases, what we call a
bi-phrase library. Such a library is constructed from
a corpus of existing translations, aligned at the word
level.
Two strategies come to mind to produce non-
contiguous bi-phrases for these libraries. The first is
to align the words using a ?standard? word aligne-
ment technique, such as the Refined Method de-
scribed in (Och and Ney, 2003) (the intersection of
two IBM Viterbi alignments, forward and reverse,
enriched with alignments from the union) and then
generate bi-phrases by combining together individ-
ual alignments that co-occur in the same pair of sen-
tences. This is the strategy that is usually adopted in
other phrase-based MT approaches (Zens and Ney,
2003; Och and Ney, 2004). Here, the difference is
that we are not restricted to combinations that pro-
duce strictly contiguous bi-phrases.
The second strategy is to rely on a word-
alignment method that naturally produces many-to-
many alignments between non-contiguous words,
such as the method described in (Goutte et al,
2004). By means of a matrix factorization, this
method produces a parallel partition of the two texts,
seen as sets of word tokens. Each token therefore
belongs to one, and only one, subset within this par-
tition, and corresponding subsets in the source and
target make up what are called cepts. For example,
in Figure 1, these cepts are represented by the circles
numbered 1, 2 and 3; each cept thus connects word
tokens in the source and the target, regardless of po-
sition or contiguity. These cepts naturally constitute
bi-phrases, and can be used directly to produce a bi-
phrase library.
Obviously, the two strategies can be combined,
and it is always possible to produce increasingly
large and complex bi-phrases by combining together
co-occurring bi-phrases, contiguous or not. One
problem with this approach, however, is that the re-
sulting libraries can become very large. With con-
756
danser le tango
to tango
I do not want to tango anymore
I do not want anymore
doI want
Je ne veux plus danser le tango
Je
I
ne plus
veux
wantdo
not anymore
I
source =
bi?phrase 1 =
bi?phrase 2 =
bi?phrase 3 =
bi?phrase 4 =
target =
Figure 2: Combining bi-phrases to produce a translation.
tiguous phrases, the number of bi-phrases that can
be extracted from a single pair of sentences typically
grows quadratically with the size of the sentences;
with non-contiguous phrases, however, this growth
is exponential. As it turns out, the number of avail-
able bi-phrases for the translation of a sentence has
a direct impact on the time required to compute the
translation; we will therefore typically rely on vari-
ous filtering techniques, aimed at keeping only those
bi-phrases that are more likely to be useful. For ex-
ample, we may retain only the most frequently ob-
served bi-phrases, or impose limits on the number of
cepts, the size of gaps, etc.
3 The Model
In statistical machine translation, we are given a
source language input sJ1 = s1...sJ , and seek the
target-language sentence tI1 = t1...tI that is its most
likely translation:
t?I1 = argmaxtI1Pr(t
I
1|s
J
1 ) (1)
Our approach is based on a direct approximation
of the posterior probability Pr(tI1|sJ1 ), using a log-
linear model:
Pr(tI1|s
J
1 ) =
1
ZsJ1
exp
(
M?
m=1
?mhm(t
I
1, s
J
1 )
)
In such a model, the contribution of each feature
function hm is determined by the corresponding
model parameter ?m; ZsJ1 denotes a normalization
constant. This type of model is now quite widely
used for machine translation (Tillmann and Xia,
2003; Zens and Ney, 2003)1.
Additional variables can be introduced in such a
model, so as to account for hidden characteristics,
and the feature functions can be extended accord-
ingly. For example, our model must take into ac-
count the actual set of bi-phrases that was used to
produce this translation:
Pr(tI1, b
K
1 |s
J
1 ) =
1
ZsJ1
exp
(
M?
m=1
?mhm(t
I
1, s
J
1 , b
K
1 )
)
Our model currently relies on seven feature func-
tions, which we describe here.
? The bi-phrase feature function hbp: it rep-
resents the probability of producing tI1 using
some set of bi-phrases, under the assump-
tion that each source phrase produces a target
phrase independently of the others:
hbp(t
I
1, s
J
1 , b
K
1 ) =
K?
k=1
logPr(t?k|s?k) (2)
Individual bi-phrase probabilities Pr(t?k|s?k)
are estimated based on occurrence counts in the
word-aligned training corpus.
? The compositional bi-phrase feature function
hcomp: this is introduced to compensate for
1Recent work from Chiang (Chiang, 2005) addresses simi-
lar concerns to those motivating our work by introducing a Syn-
chronous CFG for bi-phrases. If on one hand SCFGs allow to
better control the order of the material inserted in the gaps, on
the other gap size does not seem to be taken into account, and
phrase dovetailing such as the one involving ?do ?want? and
?not ???anymore? in Fig. 2 is disallowed.
757
hbp?s strong tendency to overestimate the prob-
ability of rare bi-phrases; it is computed as in
equation (2), except that bi-phrase probabilities
are computed based on individual word transla-
tion probabilities, somewhat as in IBM model
1 (Brown et al, 1993):
Pr(t?|s?) =
1
|s?||t?|
?
t?t?
?
s?s?
Pr(t|s)
? The target language feature function htl: this
is based on a N -gram language model of the
target language. As such, it ignores the source
language sentence and the decomposition of
the target into bi-phrases, to focus on the actual
sequence of target-language words produced
by the combination of bi-phrases:
htl(t
I
1, s
J
1 , b
K
1 ) =
I?
i=1
logPr(ti|t
i?1
i?N+1)
? The word-count and bi-phrase count feature
functions hwc and hbc: these control the length
of the translation and the number of bi-phrases
used to produce it:
hwc(tI1, s
J
1 , b
K
1 ) = I hbc(t
I
1, s
J
1 , b
K
1 ) = K
? The reordering feature function
hreord(tI1, s
J
1 , b
K
1 ): it measures the amount of
reordering between bi-phrases of the source
and target sentences.
? the gap count feature function hgc: It takes as
value the total number of gaps (source and tar-
get) within the bi-phrases of bK1 , thus allowing
the model some control over the nature of the
bi-phrases it uses, in terms of the discontigui-
ties they contain.
4 Parameter Estimation
The values of the ? parameters of the log-linear
model can be set so as to optimize a given crite-
rion. For instance, one can maximize the likely-
hood of some set of training sentences. Instead, and
as suggested by Och (2003), we chose to maximize
directly the quality of the translations produced by
the system, as measured with a machine translation
evaluation metric.
Say we have a set of source-language sentences
S. For a given value of ?, we can compute the set of
corresponding target-language translations T . Given
a set of reference (?gold-standard?) translations R
for S and a function E(T,R) which measures the
?error? in T relative to R, then we can formulate the
parameter estimation problem as2:
?? = argmin?E(T,R)
As pointed out by Och, one notable difficulty with
this approach is that, because the computation of T
is based on an argmax operation (see eq. 1), it is not
continuous with regard to ?, and standard gradient-
descent methods cannot be used to solve the opti-
mization. Och proposes two workarounds to this
problem: the first one relies on a direct optimiza-
tion method derived from Powell?s algorithm; the
second introduces a smoothed (continuous) version
of the error function E(T,R) and then relies on a
gradient-based optimization method.
We have opted for this last approach. Och shows
how to implement it when the error function can be
computed as the sum of errors on individual sen-
tences. Unfortunately, this is not the case for such
widely used MT evaluation metrics as BLEU (Pa-
pineni et al, 2002) and NIST (Doddington, 2002).
We show here how it can be done for NIST; a simi-
lar derivation is possible for BLEU.
The NIST evaluation metric computes a weighted
n-gram precision between T and R, multiplied by
a factor B(S, T,R) that penalizes short translations.
It can be formulated as:
B(S, T,R) ?
N?
n=1
?
s?S In(ts, rs)
?
s?S Cn(ts)
(3)
where N is the largest n-gram considered (usually
N = 4), In(ts, rs) is a weighted count of common
n-grams between the target (ts) and reference (rs)
translations of sentence s, and Cn(ts) is the total
number of n-grams in ts.
To derive a version of this formula that is a con-
tinuous function of ?, we will need multiple trans-
lations ts,1, ..., ts,K for each source sentence s. The
general idea is to weight each of these translations
2For the sake of simplicity, we consider a single reference
translation per source sentence, but the argument can easily be
extended to multiple references.
758
by a factor w(?, s, k), proportional to the score
m?(ts,k|s) that ts,k is assigned by the log-linear
model for a given ?:
w(?, s, k) =
[
m?(ts,k|s)
?
k? m?(ts,k? |s)
]?
where ? is the smoothing factor. Thus, in
the smoothed version of the NIST function, the
term In(ts, rs) in equation (3) is replaced by?
k w(?, s, k)In(ts,k, rs), and the term Cn(ts) is
replaced by
?
k w(?, s, k)Cn(ts,k). As for the
brevity penalty factor B(S, T,R), it depends on
the total length of translation T , i.e.
?
s |ts|. In
the smoothed version, this term is replaced by
?
s
?
k w(?, s, k)|ts,k|. Note that, when ? ? ?,
then w(?, s, k) ? 0 for all translations of s, except
the one for which the model gives the highest score,
and so the smooth and normal NIST functions pro-
duce the same value. In practice, we determine some
?good? value for ? by trial and error (5 works fine).
We thus obtain a scoring function for which we
can compute a derivative relative to ?, and which can
be optimized using gradient-based methods. In prac-
tice, we use the OPT++ implementation of a quasi-
Newton optimization (Meza, 1994). As observed by
Och, the smoothed error function is not convex, and
therefore this sort of minimum-error rate training is
quite sensitive to the initialization values for the ?
parameters. Our approach is to use a random set of
initializations for the parameters, perform the opti-
mization for each initialization, and select the model
which gives the overall best performance.
Globally, parameter estimation proceeds along
these steps:
1. Initialize the training set: using random pa-
rameter values ?0, for each source sentence of
some given set of sentences S, we compute
multiple translations. (In practice, we use the
M -best translations produced by our decoder;
see Section 5).
2. Optimize the parameters: using the method de-
scribed above, we find ? that produces the best
smoothed NIST score on the training set.
3. Iterate: we then re-translate the sentences of S
with this new ?, combine the resulting multiple
translations with those already in the training
set, and go back to step 2.
Steps 2 and 3 can be repeated until the smooothed
NIST score does not increase anymore3.
5 Decoder
We implemented a version of the beam-search stack
decoder described in (Koehn, 2003), extended to
cope with non-contiguous phrases. Each transla-
tion is the result of a sequence of decisions, each of
which involves the selection of a bi-phrase and of a
target position. The final result is obtained by com-
bining decisions, as in Figure 2. Hypotheses, cor-
responding to partial translations, are organised in a
sequence of priority stacks, one for each number of
source words covered. Hypotheses are extended by
filling the first available uncovered position in the
target sentence; each extended hypotheses is then
inserted in the stack corresponding to the updated
number of covered source words. Each hypothesis is
assigned a score which is obtained as a combination
of the actual feature function values and of admissi-
ble heuristics, adapted to deal with gaps in phrases,
estimating the future cost for completing a transla-
tion. Each stack undergoes both threshold and his-
togram pruning. Whenever two hypotheses are in-
distinguishable as far as the potential for further ex-
tension is concerned, they are merged and only the
highest-scoring is further extended. Complete trans-
lations are eventually recovered in the ?last? priority
stack, i.e. the one corresponding to the total num-
ber of source words: the best translation is the one
with the highest score, and that does not have any
remaining gaps in the target.
6 Evaluation
We have conducted a number of experiments to eval-
uate the potential of our approach. We were par-
ticularly interested in assessing the impact of non-
contiguous bi-phrases on translation quality, as well
as comparing the different bi-phrase library contruc-
tion strategies evoked in Section 2.1.
3It can be seen that, as the set of possible translations for
S stabilizes, we eventually reach a point where the procedure
converges to a maximum. In practice, however, we can usually
stop much earlier.
759
6.1 Experimental Setting
All our experiments focused exclusively on French
to English translation, and were conducted using the
Aligned Hansards of the 36th Parliament of Canada,
provided by the Natural Language Group of the USC
Information Sciences Institute, and edited by Ulrich
Germann. From this data, we extracted three dis-
tinct subcorpora, which we refer to as the bi-phrase-
building set, the training set and the test set. These
were extracted from the so-called training, test-1
and test-2 portions of the Aligned Hansard, respec-
tively. Because of efficiency issues, we limited our-
selves to source-language sentences of 30 words or
less. More details on the evaluation data is presented
in Table 14.
6.2 Bi-phrase Libraries
From the bi-phrase-building set, we built a number
of libraries. A first family of libraries was based on
a word alignment ?A?, produced using the Refined
method described in (Och and Ney, 2003) (com-
bination of two IBM-Viterbi alignments): we call
these the A libraries. A second family of libraries
was built using alignments ?B? produced with the
method in (Goutte et al, 2004): these are the B li-
braries. The most notable difference between these
two alignments is that B contains ?native? non-
contiguous bi-phrases, while A doesn?t.
Some libraries were built by simply extracting the
cepts from the alignments of the bi-phrase-building
corpus: these are the A1 and B1 libraries, and vari-
ants. Other libraries were obtained by combining
cepts that co-occur within the same pair of sen-
tences, to produce ?composite? bi-phrases. For in-
stance, the A2 libraries contain combinations of 1
or 2 cepts from alignment A; B3 contains combina-
tions of 1, 2 or 3 cepts, etc.
Some libraries were built using a ?gap-size? filter.
For instance library A2-g3 contains those bi-phrases
obtained by combining 1 or 2 cepts from alignment
A, and in which neither the source nor the target
phrase contains more than 3 gaps. In particular, li-
brary B1-g0 does not contain any non-contiguous
bi-phrases.
4Preliminary experiments on different data sets allowed us
to establish that 800 sentences constituted an acceptable size
for estimating model parameters. With such a corpus, the esti-
mation procedure converges after just 2 or 3 iterations.
Finally, all libraries were subjected to the same
two filtering procedures: the first excludes all bi-
phrases that occur only once in the training corpus;
the second, for any given source-language phrase,
retains only the 20 most frequent target-language
equivalents. While the first of these filters typically
eliminates a large number of entries, the second only
affects the most frequent source phrases, as most
phrases have less than 20 translations.
6.3 Experiments
The parameters of the model were optimized inde-
pendantly for each bi-phrase library. In all cases,
we performed only 2 iterations of the training proce-
dure, then measured the performance of the system
on the test set in terms of the NIST and BLEU scores
against one reference translation. As a point of com-
parison, we also trained an IBM-4 translation model
with the GIZA++ toolkit (Och and Ney, 2000), using
the combined bi-phrase building and training sets,
and translated the test set using the ReWrite decoder
(Germann et al, 2001)5.
Table 2 describes the various libraries that were
used for our experiments, and the results obtained
for each.
System/library bi-phrases NIST BLEU
ReWrite 6.6838 0.3324
A1 238 K 6.6695 0.3310
A2-g0 642 K 6.7675 0.3363
A2-g3 4.1 M 6.7068 0.3283
B1-g0 193 K 6.7898 0.3369
B1 267 K 6.9172 0.3407
B2-g0 499 K 6.7290 0.3391
B2-g3 3.3 M 6.9707 0.3552
B1-g1 206 K 6.8979 0.3441
B1-g2 213 K 6.9406 0.3454
B1-g3 218 K 6.9546 0.3518
B1-g4 222 K 6.9527 0.3423
Table 2: Bi-phrase libraries and results
The top part of the table presents the results for
the A libraries. As can be seen, library A1 achieves
approximately the same score as the baseline sys-
tem; this is expected, since this library is essentially
5Both the ReWrite and our own system relied on a trigram
language model trained on the English half of the bi-phrase
building set.
760
Subset sentences source words target words
bi-phrase-building set 931,000 17.2M 15.2M
training set 800 11,667 10,601
test set 500 6726 6041
Table 1: Data sets.
made up of one-to-one alignments computed using
IBM-4 translation models. Adding contiguous bi-
phrases obtained by combining pairs of alignments
does gain us some mileage (+0.1 NIST)6. Again, this
is consistent with results observed with other sys-
tems (Tillmann and Xia, 2003). However, the addi-
tion of non-contiguous bi-phrases (A2-g3) does not
seem to help.
The middle part of Table 2 presents analogous re-
sults for the corresponding B libraries, plus the B1-
g0 library, which contains only those cepts from the
B alignment that are contiguous. Interestingly, in
the experiments reported in (Goutte et al, 2004),
alignment method B did not compare favorably to A
under the widely used Alignment Error Rate (AER)
metric. Yet, the B1-g0 library performs better than
the analogous A1 library on the translation task.
This suggests that AER may not be an appropriate
metric to measure the potential of an alignment for
phrase-based translation.
Adding non-contiguous bi-phrases allows another
small gain. Again, this is interesting, as it sug-
gests that ?native? non-contiguous bi-phrases are in-
deed useful for the translation task, i.e. those non-
contiguous bi-phrases obtained directly as cepts in
the B alignment.
Surprisingly, however, combining cepts from the
B alignment to produce contiguous bi-phrases (B2-
G0) does not turn out to be fruitful. Why this
is so is not obvious and, certainly, more experi-
ments would be required to establish whether this
tendency continues with larger combinations (B3-
g0, B4-g0...). Composite non-contiguous bi-phrases
produced with the B alignments (B2-g3) seem
to bring improvements with regard to ?basic? bi-
phrases (B1), but it is not clear whether these are
significant.
6While the differences in scores in these and other experi-
ments are relatively small, we believe them to be significant, as
they have been confirmed systematically in other experiments
and, in our experience, by visual inspection of the translations.
Visual examination of the B1 library reveals
that many non-contiguous bi-phrases contain long-
spanning phrases (i.e. phrases containing long se-
quences of gaps). To verify whether or not these
were really useful, we tested a series of B1 libraries
with different gap-size filters. It must be noted that,
because of the final histogram filtering we apply on
libraries (retain only the 20 most frequent transla-
tions of any source phrase), library B1-g1 is not
a strict subset of B1-g2. Therefore, filtering on
gap-size usually represents a tradeoff between more
frequent long-spanning bi-phrases and less frequent
short-spanning ones.
The results of these experiments appear in the
lower part of Table 2. While the differences in score
are small, it seems that concentrating on bi-phrases
with 3 gaps or less affords the best compromise.
For small libraries such as those under consideration
here, this sort of filtering may not be very important.
However, for higher-order libraries (B2, B3, etc.) it
becomes crucial, because it allows to control the ex-
ponential growth of the libraries.
7 Conclusions
In this paper, we have proposed a phrase-based sta-
tistical machine translation method based on non-
contiguous phrases. We have also presented a esti-
mation procedure for the parameters of a log-linear
translation model, that maximizes a smooth version
of the NIST scoring function, and therefore lends
itself to standard gradient-based optimization tech-
niques.
From our experiments with these new methods,
we essentially draw two conclusions. The first and
most obvious is that non-contiguous bi-phrases can
indeed be fruitful in phrase-based statistical machine
translation. While we are not yet able to character-
ize which bi-phrases are most helpful, some of those
that we are currently capable of extracting are well
suited to cover some short-distance phenomena.
761
The second conclusion is that alignment quality is
crucial in producing good translations with phrase-
based methods. While this may sound obvious, our
experiments shed some light on two specific aspects
of this question. The first is that the alignment
method that produces the most useful bi-phrases
need not be the one with the best alignment error
rate (AER). The second is that, depending on the
alignments one starts with, constructing increasingly
large bi-phrases does not necessarily lead to better
translations. Some of our best results were obtained
with relatively small libraries (just over 200,000 en-
tries) of short bi-phrases. In other words, it?s not
how many bi-phrases you have, it?s how good they
are. This is the line of research that we intend to
pursue in the near future.
Acknowledgments
The authors are grateful to the anonymous reviewers
for their useful suggestions. 7
References
Peter F. Brown, Stephen A. Della Pietra, Vincent J.
Della Pietra, and Robert L. Mercer. 1993. The mathe-
matics of statistical machine translation: Parameter es-
timation. Computational Linguistics, 19(2):263?311.
David Chiang. 2005. A hierarchical phrase-based model
for statistical machine translation. In Proceedings of
the 43rd Annual Meeting of the ACL, pages 263?270,
Ann Arbor, Michigan.
George Doddington. 2002. Automatic evaluation of ma-
chine translation quality using n-gram co-occurrence
statistics. In Proc. ARPA Workshop on Human Lan-
guage Technology.
U. Germann, M. Jahr, K. Knight, D. Marcu, and K. Ya-
mada. 2001. Fast Decoding and Optimal Decoding
for Machine Translation. In Proceedings of ACL 2001,
Toulouse, France.
Cyril Goutte, Kenji Yamada, and Eric Gaussier. 2004.
Aligning words using matrix factorisation. In Proc.
ACL?04, pages 503?510.
Philipp Koehn. 2003. Noun Phrase Translation. Ph.D.
thesis, University of Southern California.
7This work was supported in part by the IST Programme
of the European Community, under the PASCAL Network of
Excellence, IST-2002-506778. This publication only reflects
the authors? views.
Daniel Marcu and William Wong. 2002. A phrase-based,
joint probability model for statistical machine transla-
tion. In Proc. of the Conf. on Empirical Methods in
Natural Language Processing (EMNLP 02), Philadel-
phia, PA.
J. C. Meza. 1994. OPT++: An Object-Oriented Class
Library for Nonlinear Optimization. Technical Report
SAND94-8225, Sandia National Laboratories, Albu-
querque, USA, March.
F. J. Och and H. Ney. 2000. Improved Statistical Align-
ment Models. In Proceedings of ACL 2000, pages
440?447, Hongkong, China, October.
Franz Josef Och and Hermann Ney. 2003. A Systematic
Comparison of Various Statistical Alignment Models.
Computational Linguistics, 29(1):19?51, March.
Franz Josef Och and Hermann Ney. 2004. The Align-
ment Template Approach to Statistical Machine Trans-
lation. Computational Linguistics, 30(4):417?449.
Franz Josef Och, Christoph Tillmann, and Hermann Ney.
1999. Improved alignment models for statistical ma-
chine translation. In Proc. of the Joint Conf. on Em-
pirical Methods in Natural Language Processing and
Very Large Corpora (EMNLP/VCL 99), College Park,
MD.
Franz Och. 2003. Minimum error rate training in statis-
tical machine translation. In ACL?03: 41st Ann. Meet.
of the Assoc. for Computational Linguistics.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: a method for automatic evalu-
tion of machine translation. In Proceedings of the 40th
Annual Meeting of the ACL, pages 311?318, Philadel-
phia, USA.
Harold Somers. 1999. Review Article: Example-based
Machine Translation. Machine Translation, 14:113?
157.
Christoph Tillmann and Fei Xia. 2003. A phrase-based
unigram model for statistical machine translation. In
Proc. of the HLT-NAACL 2003 Conference, Edmonton,
Canada.
Kenji Yamada and Kevin Knight. 2002. A decoder for
syntax-based statistical MT. In Proc. of the 40th An-
nual Conf. of the Association for Computational Lin-
guistics (ACL 02), Philadelphia, PA.
Richard Zens and Hermann Ney. 2003. Improvements
in Phrase-Based Statistical Machine Translation. In
Proc. of the HLT-NAACL 2003 Conference, Edmonton,
Canada.
762
Proceedings of the Second Workshop on Statistical Machine Translation, pages 203?206,
Prague, June 2007. c?2007 Association for Computational Linguistics
Rule-based Translation With Statistical Phrase-based Post-editing
Michel Simard, Nicola Ueffing, Pierre Isabelle and Roland Kuhn
Interactive Language Technologies Group
National Research Council of Canada
Gatineau, Canada, K1A 0R6
firstname.lastname@nrc-cnrc.gc.ca
Abstract
This article describes a machine translation
system based on an automatic post-editing
strategy: initially translate the input text into
the target-language using a rule-based MT
system, then automatically post-edit the out-
put using a statistical phrase-based system.
An implementation of this approach based
on the SYSTRAN and PORTAGE MT sys-
tems was used in the shared task of the Sec-
ond Workshop on Statistical Machine Trans-
lation. Experimental results on the test data
of the previous campaign are presented.
1 Introduction
Simard et al (2007) have recently shown how a sta-
tistical phrase-based machine translation system can
be used as an automatic post-editing (APE) layer,
on top of a rule-based machine translation system.
The motivation for their work is the repetitive nature
of the errors typically made by rule-based systems.
Given appropriate training material, a statistical MT
system can be trained to correct these systematic er-
rors, therefore reducing the post-editing effort. The
statistical system views the output of the rule-based
system as the source language, and reference hu-
man translations as the target language. Because the
training material for the APE layer will typically be
domain-specific, this process can be viewed as a way
of automatically adapting a rule-based system to a
specific application domain.
This approach has been shown experimentally
to produce large improvements in performance not
only over the baseline rule-based system that it cor-
rects, but also over a similar statistical phrase-based
MT system used in standalone mode, i.e. translating
the ?real? source text directly: Simard et al report a
reduction in post-editing effort of up to a third when
compared to the input rule-based translation, and as
much as 5 BLEU points improvement over the direct
SMT approach.
These impressive results, however, were obtained
in a very specific and somewhat unusual context:
the training and test corpora were extracted from
a collection of manually post-edited machine trans-
lations. The two corpora (one English-to-French,
one French-to-English) each contained three paral-
lel ?views? of the same data: 1) the source language
text, 2) a machine translation of that text into the
target language, as produced by a commercial rule-
based MT system, and 3) the final target-language
version of the text, produced by manually post-
editing the machine translation. Furthermore, the
corpus was very small, at least by SMT standards:
500K words of source-language data in the French-
to-English direction, 350K words in the English-to-
French. Because of this, the authors were left with
two important questions: 1) how would the results
scale up to much larger quantities of training data?
and 2) are the results related to the dependent nature
of the translations, i.e. is the automatic post-editing
approach still effective when the machine and hu-
man translations are produced independently of one
another?
With these two questions in mind, we partici-
pated in the shared task of the Second Workshop
on Statistical Machine Translation with an auto-
matic post-editing strategy: initially translate the in-
put text into the target-language using a rule-based
system, namely SYSTRAN, and automatically post-
edit the output using a statistical phrase-based sys-
tem, namely PORTAGE. We describe our system in
more detail in Section 2, and present some experi-
mental results in Section 3.
203
2 System description
Our system is composed of two main components:
a rule-based MT system, which handles the initial
translation into the target language, and a statistical
phrase-based post-editing system, which performs
domain-specific corrections and adaptations to the
output. We describe each component separately be-
low.
2.1 Rule-based Translation
The initial source-to-target language translation is
performed using the SYSTRAN machine translation
system, version 6. A detailed overview of SYS-
TRAN systems can be found in Dugast et al (2007).
For this shared task, we used the French-to-English
and English-to-French configurations of the system.
Although it is possible to provide the system with
specialized lexica, we did not rely on this feature,
and used the system in its basic ?out-of-the-box?
configuration.
2.2 Statistical Phrase-based Post-Editing
The output of the rule-based MT system described
above is fed into a post-editing layer that performs
domain-specific corrections and adaptation. This
operation is conceptually not very different from a
?target-to-target? translation; for this task, we used
the PORTAGE system, a state-of-the-art statistical
phrase-based machine translation system developed
at the National Research Council of Canada (NRC).1
A general description of PORTAGE can be found in
(Sadat et al, 2005).
For our participation in this shared task, we de-
cided to configure and train the PORTAGE system
for post-editing in a manner as much as possible
similar to the corresponding translation system, the
details of which can be found in (Ueffing et al,
2007). The main features of this configuration are:
? The use of two distinct phrase tables, contain-
ing phrase pairs extracted from the Europarl
and the News Commentary training corpora re-
spectively.
? Multiple phrase-probability feature functions
in the log-linear models, including a joint prob-
1A version of PORTAGE is made available by the NRC to
Canadian universities for research and education purposes.
ability estimate, a standard frequency-based
conditional probability estimate, and variants
thereof based on different smoothing methods
(Foster et al, 2006).
? A 4-gram language model trained on the com-
bined Europarl and News Commentary target-
language corpora.
? A 3-gram adapted language model: this is
trained on a mini-corpus of test-relevant target-
language sentences, extracted from the training
material using standard information retrieval
techniques.
? A 5-gram truecasing model, trained on the
combined Europarl and News Commentary
target-language corpora.
2.3 Training data
Ideally, the training material for the post-editing
layer of our system should consist in a corpus of
text in two parallel versions: on the one hand, raw
machine translation output, and on the other hand,
manually post-edited versions of these translations.
This is the type of data that was used in the initial
study of Simard et al (2007).
Unfortunately, this sort of training data is seldom
available. Instead, we propose using training ma-
terial derived directly from standard, source-target
parallel corpora. The idea is to translate the source
portion of the parallel corpus into the target lan-
guage, using the rule-based MT component. The
post-editing component can then be trained using
this translation as ?source? training material, and the
existing target portion of the parallel corpus as ?tar-
get? training material. Note how this sort of data
is subtly different from the data used by Simard et
al.: there, the ?target? text was dependent on the
?source?, in the sense that it was produced by manu-
ally post-editing the machine translation; here, the
two can be said to be independent, in the sense
that both ?source? and ?target? were produced inde-
pendently by man and machine (but from the same
?real? source, of course). It was one of the initial
motivations of the current work to verify to what ex-
tent the performance of the APE approach is affected
by using two different translations (human and ma-
204
en ? fr fr ? en
Europarl (>32M words/language)
SYSTRAN 23.06 20.11
PORTAGE 31.01 30.90
SYSTRAN+PORTAGE 31.11 30.61
News Commentary (1M words/language)
SYSTRAN 24.41 18.09
PORTAGE 25.98 25.17
SYSTRAN+PORTAGE 28.80 26.79
Table 1: System performances on WMT-06 test. All
figures are single-reference BLEU scores, computed
on truecased, detokenized translations.
chine) instead of two versions of the same transla-
tion (raw MT versus post-edited MT).
We concentrated our efforts on the English-
French language pair. For each translation direc-
tion, we prepared two systems: one for the Eu-
roparl domain, and one for the News Commentary
domain. The two systems have almost identical
configurations (phrase tables, log-linear model fea-
tures, etc.); the only differences between the two
are the adapted language model, which is computed
based on the specific text to be translated and the
parameters of the log-linear models, which are opti-
mized using domain-specific development sets. For
the Europarl domain system, we used the dev2006
and devtest2006 data sets, while for the News Com-
mentary, we used the nc-dev2007. Typically, the
optimization procedure will give higher weights to
Europarl-trained phrase tables for the Europarl do-
main systems, and inversely for the News Commen-
tary domain systems.
3 Experimental Results
We computed BLEU scores for all four systems on
the 2006 test data (test2006 for the Europarl do-
main and nc-devtest2007 for the News Commen-
tary). The results are presented in Table 1. As points
of comparison, we also give the scores obtained by
the SYSTRAN systems on their own (i.e. without a
post-editing layer), and by the PORTAGE MT sys-
tems on their own (i.e. translating directly source
into target).
The first observation is that, as was the case
in the Simard et al study, post-editing (SYS-
TRAN+PORTAGE lines) very significantly in-
creases the BLEU scores of the rule-based system
(SYSTRAN lines). This increase is more spectacu-
lar in the Europarl domain and when translating into
English, but it is visible for all four systems.
For the News Commentary domain, the APE
strategy (SYSTRAN+PORTAGE lines) clearly out-
performs the direct SMT strategy (PORTAGE lines):
translating into English, the gain exceeds 1.5 BLEU
points, while for French, it is close to 3 BLEU
points. In contrast, for the Europarl domain, both ap-
proaches display similar performances. Let us recall
that the News Commentary corpus contains less than
50K sentence pairs, totalling a little over one mil-
lion words in each language. With close to 1.3 mil-
lion sentence pairs, the Europarl corpus is almost 30
times larger. Our results therefore appear to confirm
one of the conjectures of the Simard et al study:
that APE is better suited for domains with limited
quantities of available training data. To better un-
derstand this behavior, we trained series of APE and
SMT systems on the Europarl data, using increas-
ing amounts of training data. The resulting learning
curves are presented in Figure 1.2
As observed in the Simard et al study, while both
the SMT and APE systems improve quite steadily
with more data (note the logarithmic scale), SMT
appears to improve more rapidly than APE. How-
ever, there doesn?t seem to be a clear ?crossover?
point, as initially conjectured by Simard et al In-
stead, SMT eventually catches up with APE (any-
where between 100K and 1M sentence pairs), be-
yond which point both approaches appear to be more
or less equivalent. Again, one impressive feature
of the APE strategy is how little data is actually re-
quired to improve upon the rule-based system upon
which it is built: around 5000 sentence pairs for
English-to-French, and 2000 for French-to-English.
4 Conclusions
We have presented a combination MT system based
on a post-editing strategy, in which a statistical
phrase-based system corrects the output of a rule-
based translation system. Experiments confirm the
2The systems used for this experiment are simplified ver-
sions of those described in Section 2, using only one phrase
table, a trigram language model and no rescoring; furthermore,
they were optimized and tested on short sentences only.
205
 0.1
 0.12
 0.14
 0.16
 0.18
 0.2
 0.22
 0.24
 0.26
 0.28
 0.3
 1  10  100  1000
BL
EU
 s
co
re
Training sentences (x 1000)
English to French
SYSTRAN
PORTAGE
SYSTRAN + PORTAGE
 0.1
 0.12
 0.14
 0.16
 0.18
 0.2
 0.22
 0.24
 0.26
 0.28
 0.3
 1  10  100  1000
BL
EU
 s
co
re
Training sentences (x 1000)
French to English
SYSTRAN
PORTAGE
SYSTRAN + PORTAGE
Figure 1: BLEU scores on Europarl data under increasing amounts of training data for PORTAGE SMT
alone and SYSTRAN MT with PORTAGE APE.
conclusions of earlier studies: not only can phrase-
based post-editing significantly improve the out-
put of a rule-based MT system (in terms of BLEU
score), but when training data is scarce, it also out-
performs a direct phrase-based MT strategy. Fur-
thermore, our results indicate that the training data
for the post-editing component does not need to be
manually post-edited translations, it can be gener-
ated from standard parallel corpora. Finally, our ex-
periments show that while post-editing is most effec-
tive when little training data is available, it remains
competitive with phrase-based translation even with
much larger amounts of data.
This work opens the door to a number of lines of
investigation. For example, it was mentioned earlier
that phrase-based APE could be seen as a form of au-
tomatic domain-adaptation for rule-based methods.
One thing we would like to verify is how this ap-
proach compares to the standard ?lexical customiza-
tion? method proposed by most rule-based MT ven-
dors. Also, in the experiments reported here, we
have used identical configurations for the APE and
direct SMT systems. However, it might be possible
to modify the phrase-based system so as to better
adapt it to the APE task. For example, it could be
useful for the APE layer to ?look? at the real source-
language text, in addition to the MT output it is post-
editing. Finally, we have so far considered the front-
end rule-based system as a ?black box?. But in the
end, the real question is: Which part of the rule-
based processing is really making things easier for
the phrase-based post-editing layer? Answering this
question will likely require diving into the internals
of the rule-based component. These are all direc-
tions that we are currently pursuing.
Acknowledgements
This work was done as part of a collaboration with
SYSTRAN S.A. Many thanks go to Jean Senellart,
Jens Stephan, Dimitris Sabatakakis and all those
people behind the scene at SYSTRAN.
References
L. Dugast, J. Senellart, and P. Koehn. 2007. StatisticalPost-Edition on SYSTRAN Rule-Based TranslationSystem. In Proceedings of the Second Workshop OnStatistical Machine Translation, Prague, Czech Re-
public.
G. Foster, R. Kuhn, and H. Johnson. 2006. PhrasetableSmoothing for Statistical Machine Translation. InProceedings of EMNLP 2006, pages 53?61, Sydney,
Australia.
F. Sadat, H. Johnson, A. Agbago, G. Foster, R. Kuhn,
J. Martin, and A. Tikuisis. 2005. PORTAGE: APhrase-Based Machine Translation System. In Pro-ceedings of the ACL Workshop on Building and UsingParallel Texts, pages 129?132, Ann Arbor, USA.
M. Simard, C. Goutte, and P. Isabelle. 2007. Sta-
tistical Phrase-Based Post-Editing. In Human Lan-guage Technologies 2007: The Conference of theNorth American Chapter of the Association for Com-putational Linguistics; Proceedings of the Main Con-ference, pages 508?515, Rochester, USA.
N. Ueffing, M. Simard, S. Larkin, and H. Johnson. 2007.NRC?s PORTAGE system for WMT 2007. In Pro-ceedings of the Second Workshop On Statistical Ma-chine Translation, Prague, Czech Republic.
206
c? 2003 Association for Computational Linguistics
Embedding Web-Based Statistical
Translation Models in Cross-Language
Information Retrieval
Wessel Kraaij? Jian-Yun Nie? Michel Simard?
TNO TPD Universite? de Montre?al Universite? de Montre?al
Although more and more language pairs are covered by machine translation (MT) services, there
are still many pairs that lack translation resources. Cross-language information retrieval (CLIR)
is an application that needs translation functionality of a relatively low level of sophistication,
since current models for information retrieval (IR) are still based on a bag of words. The Web
provides a vast resource for the automatic construction of parallel corpora that can be used to train
statistical translation models automatically. The resulting translation models can be embedded in
several ways in a retrieval model. In this article, we will investigate the problem of automatically
mining parallel texts from the Web and different ways of integrating the translation models
within the retrieval process. Our experiments on standard test collections for CLIR show that the
Web-based translation models can surpass commercial MT systems in CLIR tasks. These results
open the perspective of constructing a fully automatic query translation device for CLIR at a very
low cost.
1. Introduction
Finding relevant information in any language on the increasingly multilingual World
Wide Web poses a real challenge for current information retrieval (IR) systems. We
will argue that the Web itself can be used as a translation resource in order to build
effective cross-language IR systems.
1.1 Information Retrieval and Cross-Language Information Retrieval
The goal of IR is to find relevant documents from a large collection of documents or
from the World Wide Web. To do this, the user typically formulates a query, often in
free text, to describe the information need. The IR system then compares the query
with each document in order to evaluate its similarity (or probability of relevance)
to the query. The retrieval result is a list of documents presented in decreasing order
of similarity. The key problem in IR is that of effectiveness, that is, how good an IR
system is at retrieving relevant documents and discarding irrelevant ones.
Because of the information explosion that has occurred on the Web, people are
more in need of effective IR systems than ever before. The search engines currently
available on the Web are IR systems that have been created to answer this need. By
querying these search engines, users are able to identify quickly documents contain-
ing the same keywords as the query they enter. However, the existing search engines
provide only monolingual IR; that is, they retrieve documents only in the same lan-
? TNO TPD, PO BOX 155, 2600 AD Delft, The Netherlands. E-mail: kraaij@tpd.tno.nl
? DIRO, Universite? de Montre?al, CP. 6128, succ. Centre-ville, Montreal, Qc. H3C 3J7 Canada. E-mail:
{nie, simardm}@iro.umontreal.ca
382
Computational Linguistics Volume 29, Number 3
guage as the query. To be more precise: Search engines usually do not consider the
language of the keywords when the keywords of a query are matched against those
of the documents. Identical keywords are matched, whatever their languages are. For
example, the English word son can match the French word son (?his? or ?her?). Current
search engines do not provide the functionality for cross-language IR (CLIR), that is,
the ability to retrieve relevant documents written in languages different from that of
the query (without the query?s being translated manually into the other language(s)
of interest).
As the Web has grown, more and more documents on the Web have been written in
languages other than English, and many Internet users are non-native English speak-
ers. For many users, the barrier between tbe language of the searcher and the langage
in which documents are written represents a serious problem. Although many users
can read and understand rudimentary English, they feel uncomfortable formulating
queries in English, either because of their limited vocabulary in English, or because
of the possible misusage of English words. For example, a Chinese user may use eco-
nomic instead of cheap or economical or inexpensive in a query because these words have
a similar translation in Chinese. An automatic query translation tool would be very
helpful to such a user. On the other hand, even if a user masters several languages, it
is still a burden for him or her to formulate several queries in different languages. A
query translation tool would also allow such a user to retrieve relevant documents in
all the languages of interest with only one query. Even for users with no understand-
ing of a foreign language, a CLIR system might still be useful. For example, someone
monitoring a competitor?s developments with regard to products similar to those he
himself produces might be interested in retrieving documents describing the possible
products, even if he does not understand them. Such a user might use machine trans-
lation systems to get the gist of the contents of the documents he retrieves through
his query. For all these types of users, CLIR would represent a useful tool.
1.2 Possible Approaches to CLIR
From an implementation point of view, the only difference between CLIR and the
classical IR task is that the query language differs from the document language. It is
obvious that to perform in an effective way the task of retrieving documents that are
relevant to a query when the documents are written in a different language than the
query, some form of translation is required. One might conjecture that a combination
of two existing fields, IR and machine translation (MT), would be satisfactory for
accomplishing the combined translation and retrieval task. One could simply translate
the query by means of an MT system, then use existing IR tools, obviating the need
for a special CLIR system.
This approach, although feasible, is not the only possible approach, nor is it neces-
sarily the best one. MT systems try to translate text into a well-readable form governed
by morphological, syntactic, and semantic constraints. However, current IR models are
based on bag-of-words models. They are insensitive to word order and to the syntac-
tic structure of queries. For example, with current IR models, the query ?computer
science? will usually produce the same retrieval results as ?science computer.? The
complex process used in MT for producing a grammatical translation is not fully ex-
ploited by current IR models. This means that a simpler translation approach may
suffice to implement the translation step.
On the other hand, MT systems are far from perfect. They often produce incorrect
383
Kraaij, Nie, and Simard Embedding Web-Based Statistical Models in CLIR
translations. For example, Systran1 translates the word drug as drogue (illegal substance)
in French for both drug traffic and drug administration office. Such a translation error
will have a substantial negative impact on the effectiveness of any CLIR system that
incorporates it. So even if MT systems are used as translation devices, they may need to
be complemented by other, more robust translation tools to improve their effectiveness.
In the current study, we will use statistical translation models as such a complementary
tool.
Queries submitted to IR systems or search engines are often very short. In par-
ticular, the average length of queries submitted to the search engines on the Web is
about two words (Jansen et al 2001). Such short queries are generally insufficient to
describe the user?s information need in a precise and unambiguous way. Many im-
portant words are missing from them. For example, a user might formulate the query
?Internet connection? in order to retrieve documents about computer networks, Inter-
net service providers, or proxies. However, under the current bag-of-words approach,
the relevant documents containing these terms are unlikely to be retrieved. To solve
this problem, a common approach used in IR is query expansion, which tries to add
synonyms or related words to the original query, making the expanded query a more
exhaustive description of the information need. The words added to the query dur-
ing query expansion do not need to be strict synonyms to improve the query results.
However, they do have to be related, to some degree, to the user?s information need.
Ideally, the degree of the relatedness should be weighted, with a strongly related word
weighted more heavily in the expanded query than a less related one.
MT systems act in a way opposite to the query expansion process: Only one trans-
lation is generally selected to express a particular meaning. 2 In doing so, MT systems
employed in IR systems in fact restrict the possible query expansion effect during
the translation process. We believe that CLIR can benefit from query translation that
provides multiple translations for the same meaning. In this regard, the tests carried
out by Kwok (1999) with a commercial MT system for Chinese-English CLIR are quite
interesting. His experiments show that it is much better to use the intermediate transla-
tion data produced by the MT system than the final translation itself. The intermediate
data contain, among other things, all the possible translation words for query terms.
Kwok?s work clearly demonstrates that using an MT system as a black box is not the
most effective choice for query translation in CLIR. However, few MT systems allow
one to access the intermediate stages of the translations they produce.
Apart from the MT approach, queries can also be translated by using a machine-
readable bilingual dictionary or by exploiting a set of parallel texts (texts with their
translations). High-quality bilingual dictionaries are expensive, but there are many free
on-line translation dictionaries available on the Web that can be used for query trans-
lation. This approach has been applied in several studies (e.g., Hull and Grefenstette
1996; Hiemstra and Kraaij 1999). However, free bilingual dictionaries often suffer from
a poor coverage of the vocabulary in the two languages with which they deal, and from
the problem of translation ambiguity, because usually no information is provided to
allow for disambiguation. Several previous studies (e.g., Nie et al 1999), have shown
that using a translation dictionary alone would produce much lower effectiveness than
an MT system. However, a dictionary complemented by a statistical language model
(Gao et al 2001; Xu, Weischedel, and Nguyen 2001) has produced much better results
than when the dictionary is used alone.
1 We used the free translation service provided at ?http://babelfish.altavista.com/? in October 2002.
2 Although there is no inherent obstacle preventing MT systems from generating multiple translations, in
practice, only one translation is produced.
384
Computational Linguistics Volume 29, Number 3
In this article, the use of a bilingual dictionary is not our focus. We will concentrate
on a third alternative for query translation: an approach based on parallel texts. Paral-
lel texts are texts accompanied by their translation in one or several other languages (Ve?ronis
2000). They contain valuable translation examples for both human and machine trans-
lation. A number of studies in recent years (e.g., Nie et al 1999; Franz et al 2001;
Sheridan, Ballerini, and Scha?uble 1998; Yang et al 1998) have explored the possibil-
ity of using parallel texts for query translation in CLIR. One potential advantage of
such an approach is that it provides multiple translations for the same meaning. The
translation of a query would then contain not only words that are true translations
of the query, but also related words. This is the query expansion effect that we want
to produce in IR. Our experimental results have confirmed that this approach can be
very competitive with the MT approach and yield much better results than a simple
dictionary-based approach, while keeping the development cost low.
However, one major obstacle to the use of parallel texts for CLIR is the unavail-
ability of large parallel corpora for many language pairs. Hence, our first goal in the
research presented here was to develop an automatic mining system that collects par-
allel pages on the Web. The collected parallel Web pages are used to train statistical
translation models (TMs) that are then applied to query translation. Such an approach
offers the advantage of enabling us to build a CLIR system for a new language pair
without waiting for the release of an MT system for that language pair. The number
of potential language pairs supported by Web-based translation models is large if one
includes transitive translation using English as a pivot language. English is often one
of the languages of those Web pages for which parallel translations are available.
The main objectives of this article are twofold: (1) We will show that it is possible
to obtain large parallel corpora from the Web automatically that can form the basis for
an effective CLIR system, and (2) we will compare several ways to embed translation
models in an IR system to exploit these corpora for cross-language query expansion.
Our experiments will show that these translation tools can result in CLIR of com-
parable effectiveness to MT systems. This in turn will demonstrate the feasibility of
exploiting the Web as a large parallel corpus for the purpose of CLIR.
1.3 Problems in Query Translation
Now let us turn to query translation problems. Previous studies on CLIR have iden-
tified three problems for query translation (Grefenstette 1998): identifying possible
translations, pruning unlikely translations, and weighting the translation words.
Finding translations. First of all, whatever translation tool is employed in trans-
lating queries has to provide a good coverage of the source and target vocabularies.
In a dictionary-based approach to CLIR, we will encounter the same problems that
have been faced in MT research: phrases, collocations, idioms, and domain-specific
terminology are often translated incorrectly. These classes of expressions require a so-
phisticated morphological analysis, and furthermore, domain-specific terms challenge
the lexical coverage of the transfer dictionaries. A second important class of words
that can pose problems for CLIR, particularly that involving news article retrieval,
is proper names. The names of entities such as persons or locations are frequently
used in queries for news articles, and their translation is not always trivial. Often, the
more commonly used geographical names of countries or their capitals have a dif-
ferent spelling in different languages (e.g., Milan/Milano/Milaan) or translations that
are not related to the same morphological root (e.g., Germany/Allemagne/Duitsland).
The names of organizations and their abbreviations are also a notorious problem; for
example, the United Nations can be referred to as UN, ONU, VN, etc. (disregarding
the problem of morphological normalization of abbreviations). When proper names
385
Kraaij, Nie, and Simard Embedding Web-Based Statistical Models in CLIR
have to be translated from one language to another with a different script, like Cyril-
lic, Arabic, or Chinese, this problem is even more acute. The process of defining the
spelling of a named entity in a language with a different script from the originating
language is called transliteration and is based on a phonemic representation of the
named entity. Unfortunately different national ?standards? are used for transliteration
in different languages that use the same alphabet (e.g., the former Russian president?s
name in Latin script has been transliterated as Jeltsin, Eltsine, Yeltsin, and Jelzin.
Pruning translation alternatives. A word or a term often has multiple transla-
tions. Some of them are appropriate for a particular query and the others are not. An
important question is how to keep the appropriate translations while eliminating the
inappropriate ones. Because of the particularities of IR, it might improve the results
to retain multiple translations that display small differences in sense, as in query ex-
pansion. So it could be beneficial to keep all related senses for the matching process,
together with their probabilities.
Weighting translation alternatives. Closely related to the previous point is the
question of how to deal with translation alternatives. The weighting of words in doc-
uments and in the query is of crucial importance in IR. A word with a heavy weight
will influence the results of retrieval more than a low-weight word. In CLIR it is also
important to assign appropriate weights to translation words. Pruning translations can
be viewed as an extreme Boolean way of weighting translations. The intuition is that,
just as in query expansion, it may well be beneficial to assign a heavier weight to the
?main? translation and a lighter weight to related translations.
1.4 Integration of Query Translation with Retrieval
The problem of ?weighting of translation alternatives,? identified by Grefenstette,
refers to the more general problem of designing an architecture for a CLIR system
in which translation and document ranking are integrated in a way that maximizes
retrieval effectiveness.
The MT approach clearly separates translation from retrieval: A query is first
translated, and the result of the translation is subsequently submitted to an IR system
as a new query. At the retrieval phase, one no longer knows how certain a translated
word is with respect to the other translated words in the translated query. All the
translation words are treated as though they are totally certain. Indeed, an MT system
is used as a black box. In this article, we consider translation to be an integral part of
the IR process that has to be considered together with the retrieval step.
From a more theoretical point of view, CLIR is a process that, taken as a whole, is
composed of query translation, document indexing, and document matching. The two
first subprocesses try to transform the query and the documents into a comparable
internal representation form. The third subprocess tries to compare the representa-
tions to evaluate the similarity. In previous studies on CLIR, the first subprocess is
clearly separated from the latter two, which are integrated in classical IR systems. An
approach that considers all three subprocesses together will have the advantage of
accounting better for the uncertainty of translation during retrieval. More analysis on
this point is provided in Nie (2002). This article follows the same direction as Nie?s.
We will show in our experiments that an integrated approach can produce very high
CLIR effectiveness.
An attractive framework for integrating translation and retrieval is the probabilistic
framework, although estimating translation probabilities is not always straightforward
using this framework.
In summary, because CLIR does not necessarily require a unique translation of a
text (as MT does), approaches other than fully automatic MT might provide interesting
386
Computational Linguistics Volume 29, Number 3
characteristics for CLIR that are complementary to those of MT approaches. This could
result in greater precision,3 since an MT system might choose the wrong translation
for the query term(s), and/or a higher rate of recall,4 since multiple translations are
accommodated, which could retrieve documents via related terminology.
In this article we will investigate the effectiveness of CLIR systems based on
probabilistic translation models trained on parallel texts mined from the Web. Glob-
ally, our approach to the CLIR problem can be viewed informally as ?cross-lingual
sense matching.? Both query and documents are modeled as a distribution over se-
mantic concepts, which in reality is approximated by a distribution over words. The
challenge for CLIR is to measure to what extent these concepts (or word senses) are
related. From this point of view, our approach is similar in principle to that using
latent semantic analysis (LSI) (Dumais et al 1997), which also tries to create semantic
similarity between documents, queries, and terms by transposing them into a new
vector space. An alternative way of integrating translation and IR is to create ?struc-
tured queries,? in which translations are modeled as synonyms (Pirkola 1998). Since
this approach is simple and effective, we will use it as one of the reference systems in
our experiments.
The general approach of this article will be implemented in several different ways,
each fully embedded in the retrieval models tested. A series of experiments on CLIR
will be conducted in order to evaluate these models. The results will clearly show that
Web-based translation models are as effective as (and sometimes more effective than)
off-the-shelf commercial MT systems.
The remainder of the article is organized as follows: Section 2 discusses the pro-
cedure we used to construct parallel corpora from the Web, and Section 3 describes
the procedure we used to train the translation models. Section 4 describes the proba-
bilistic IR model that we employed and various ways of embedding translation into
a retrieval model. Section 5 presents our experimental results. The article ends with a
discussion and conclusion section.
2. PTMiner
It has been shown that by using a large parallel corpus, one can produce CLIR ef-
fectiveness close to that obtained with an MT system (Nie et al 1999). In previous
studies, parallel texts have been exploited in several ways: using a pseudofeedback ap-
proach, capturing global cross-language term associations, transposing to a language-
independent semantic space, and training a statistical translation model.
Using a pseudofeedback approach. In Yang et al (1998) parallel texts are used
as follows. A given query in the source language is first used to retrieve a subset
of texts from the parallel corpus. The corresponding subset in the target language is
considered to provide a description of the query in the target language. From this
subset of documents, a set of weighted words is extracted, and this set of words is
used as the query ?translation.?
Capturing global cross-language term associations. A more advanced and theo-
retically better-motivated approach is to index concatenated parallel documents in the
dual space of the generalized vector space model (GVSM), where terms are indexed
by documents (Yang et al 1998). An approach related to GVSM is to build a so-called
similarity thesaurus on the parallel or comparable corpus. A similarity thesaurus is an
3 Precision is defined as the proportion of relevant documents among all the retrieved documents.
4 Recall is the proportion of relevant documents retrieved among all the relevant documents in a
collection.
387
Kraaij, Nie, and Simard Embedding Web-Based Statistical Models in CLIR
information structure (also based on the dual space of indexing terms by documents)
in which associated terms are computed on the basis of global associations between
terms as measured by term co-occurrence on the document level (Sheridan, Ballerini,
and Scha?uble 1998). Recently, the idea of using the dual space of parallel documents
for cross-lingual query expansion has been recast in a language-modeling framework
(Lavrenko, Choquette, and Croft 2002).
Transposing to a language-independent semantic space. The concatenated doc-
uments can also be transposed in a language-independent space by applying latent
semantic indexing (Dumais et al 1997; Yang et al 1998). The disadvantage of this
approach is that the concepts in this space are hard to interpret and that LSI is com-
putationally demanding. It is currently not feasible to perform such a transposition on
a Web scale.
Training a statistical translation model. Approaches that involve training a statis-
tical translation model have been explored in, for example, Nie et al (1999) and Franz
et al (2001). In Nie et al?s approach, statistical translation models (usually IBM model
1) are trained on a parallel corpus. The models are used in a straightforward way: The
source query is submitted to the translation model, which proposes a set of translation
equivalents, together with their probability. The latter are then used as a query for the
retrieval process, which is based on a vector space model. Franz et al?s approach uses
a better founded theoretical framework: the OKAPI probabilistic IR model (Robert-
son and Walker 1994). The present study uses a different probabilistic IR model, one
based on statistical language models (Hiemstra 2001; Xu, Weischedel, and Nguyen
2001). This IR model facilitates a tighter integration of translation and retrieval. An
important difference between statistical translation approaches and approaches based
on document alignment discussed in the previous paragraph is that translation models
perform alignment at a much more refined level. Consequently, the alignments can
be used to estimate translation relations in a reliable way. On the other hand, the ad-
vantage of the CLIR approaches that rely simply on alignment at the document level
is that they can also handle comparable corpora, that is, documents that discuss the
same topic but are not necessarily translations of each other (Laffling 1992).
Most previous work on parallel texts has been conducted on a few manually
constructed parallel corpora, notably the Canadian Hansard corpus. This corpus5 con-
tains many years? debates in the Canadian parliament in both English and French,
amounting to several dozens of millions of words in each language. The European
parliament documents represent another large parallel corpus in several European
languages. However, the availability of this corpus is much more restricted than the
Canadian Hansard. The Hong Kong government publishes official documents in both
Chinese and English. They form a Chinese-English parallel corpus, but again, its size
is much smaller than that of the Canadian Hansard. For many other languages, no
large parallel corpora are available for the training of statistical models.
LDC has tried to collect additional parallel corpora, resorting at times to man-
ual collection (Ma 1999). Several other research groups (for example, the RALI lab
at Universite? de Montre?al) have also tried to acquire manually constructed parallel
corpora. However, manual collection of large corpora is a tedious task that is time-
and resource-consuming. On the other hand, we observe that the increasing usage of
different languages on the Web results in more and more bilingual and multilingual
sites. Many Web pages are now translated into different languages. The Web contains
5 LDC provides a version containing texts from the mid-1970s through 1988; see
?http://www.ldc.upenn.edu/?.
388
Computational Linguistics Volume 29, Number 3
a large number of parallel Web pages in many languages (usually with English). If
these can be extracted automatically, then this would help solve, to some extent, the
problem of parallel corpora. PTMiner (for Parallel Text Miner) was built precisely for
this purpose.
Of course, an automatic mining program is unable to understand the texts it ex-
tracts and hence to judge in a totally reliable way whether they are parallel. However,
CLIR is quite error-tolerant. As we will show, a noisy parallel corpus can still be very
useful for CLIR.
2.1 General Principles of Automatic Mining
Parallel Web pages usually are not published in isolation; they are often linked to
one another in some way. For example, Resnik (1998) observed that some parallel
Web pages are often referenced in the same parent index Web page. In addition,
the anchor text of such links usually identifies the language. For example, if a Web
page ?index.html? provides links to both English and French versions of a page it
references, and the anchor texts of the links are respectively ?English version? and
?French version,? then the referenced versions are probably parallel pages in English
and French. To locate such pages, Resnik first sends a query of the following form to
the Web search engine AltaVista, which returns the parent indexing pages:
anchor: English AND anchor: French
Then the referenced pages in both languages are retrieved and considered to be par-
allel. Applying this method, Resnik was able to mine 2,491 pairs of English-French
Web pages. Other researchers have adapted his system to mine 3,376 pairs of English-
Chinese pages and 59 pairs of English-Basque pages.
We observe, however, that only a small portion of parallel Web sites are organized
in this way. Many other parallel pages cannot be found with Resnik?s method. The
mining system we employ in the research presented here uses different criteria from
Resnik?s; and we also incorporate an exploration process (i.e., a host crawler) in order
to discover Web pages that have not been indexed by the existing search engines.
The mining process in PTMiner is divided into two main steps: identification of
candidate parallel pages, and verification of their parallelism. The overall process is
organized into the following steps:
1. Determining candidate sites. Identify Web sites that may contain
parallel pages. In our approach, we adopt a simple definition of Web
site: a host corresponding to a distinct DNS (domain name system)
address (e.g., ?www.altavista.com? and ?geocities.yahoo.com?).
2. File name fetching. Identify a set of Web pages on each Web site that are
indexed by search engines.
3. Host crawling. Use the URLs collected in the previous step as seeds to
further crawl each candidate site for more URLs.
4. Pair scanning by names. Construct pairs of Web pages on the basis of
pattern matching between URLs (e.g., ?index.html? vs. ?index f.html?).
5. Text filtering. Filter the candidate parallel pages further according to
several criteria that operate on their contents.
In the following subsections, we describe each of these steps in more detail.
389
Kraaij, Nie, and Simard Embedding Web-Based Statistical Models in CLIR
2.2 Identification of Candidate Web Sites
In addition to the organization of parallel Web pages exploited by Resnik?s method,
another common characteristic of parallel Web pages is that they cross-reference one
another. For example, an English Web page may contain a pointer to the French ver-
sion, and vice versa, and the anchor text of these pointers usually indicates the lan-
guage of the other page. This phenomenon is common because such an anchor text
shows the reader that a version in another language is available.
In considering both ways of organizing parallel Web pages, we see that a common
feature is the existence of a link with an anchor text identifying a language. This is
the criterion we use in PTMiner to detect candidate Web sites: the existence of at least
one Web page containing such a link. Candidate Web sites are identified via requests
sent to a search engine (e.g., AltaVista or Google). For example, the following request
asks for pages in English that contain a link with one of the required anchor texts.
anchor: French version, in French, en Franc?ais, . . .
language: English
The hosts extracted from the responses are considered to be candidate sites.
2.3 File Name Fetching
It is assumed that parallel pages are stored on the same Web site. This is not always
true, but this assumption allows us to minimize the exploration of the Web and to
avoid considering many unlikely candidates.
To search for parallel pairs of pages from each candidate site, PTMiner first asks
the search engine for all the Web pages from a particular site that it has indexed, via
a request of the following form:
host: <hostname>
However, the results of this step may not be exhaustive, because
? search engines typically do not index all the Web pages of a site.
? most search engines allow users to retrieve a limited number of
documents (e.g., 1,000 in AltaVista).
Therefore, we continue our search with a host crawler, which uses the Web pages
found by the search engines as seeds.
2.4 Host Crawling
A host crawler is slightly different from a Web crawler or a robot in that a host crawler
can only exploit one Web site at a time. A breadth-first crawling algorithm is used in
the host-crawling step of PTMiner?s mining process. The principle is that if a retrieved
Web page contains a link to an unexplored document on the same site, this document
is added to the list of pages to be explored later. This crawling step allows us to obtain
more Web pages from the candidate sites.
2.5 Pair Scanning by Names
Once a large set of URLs has been identified, the next task is to find parallel pairs
among them. In our experience, many parallel Web pages have very similar file names.
390
Computational Linguistics Volume 29, Number 3
For example, an English Web page with the file name ?index.html? often corresponds
to a French translation with a file name such as ?index f.html?. The only difference
between the two file names is a segment that identifies the language of the file. This
similarity in file names is by no means an accident. In fact, this is a common way for
Webmasters to keep track of a large number of documents in different versions.
This same observation also applies to URL paths. For example, the following two
URLs are also similar in name:
?http://www.asite.ca/en/afile.html? and ?http://www.asite.ca/fr/afile.html?.
To find similarly named URLs, we define lists of prefixes and suffixes for both the
source and the target languages. For example:
EnglishPrefix = {(emptychar), e, en, english, e , en , english , . . .}
Once a possible source language prefix is identified in an URL, it is replaced with a
prefix in the target language, and we then test if this URL is found on the Web site.
2.6 Filtering by Contents
The file pairs identified in previous steps are further verified in regard to their contents.
In PTMiner, the following criteria are used for verification: file length, HTML structure,
and language and character set.
2.6.1 File Length. The ratio of the lengths of a pair of parallel pages is usually com-
parable to the typical length ratio of the two languages (especially when the text is
long enough). Hence, a simple verification is to compare the lengths of the two files.
As many Web documents are quite short, we tolerate some difference (up to 40% from
the typical ratio).
2.6.2 HTML Structure. Parallel Web pages are usually designed to have similar lay-
outs. This often means that the two parallel pages have similar HTML structures.
However, the HTML structures of parallel pages may also be quite different from one
another. Pages may look similar and still have different HTML markups. Therefore, a
certain amount of flexibility is also employed in this step.
In our approach, we first determine a set of meaningful HTML tags that affect
the appearance of the page and extract them from both files (e.g., <p> and <H1>, but
not <meta> and <font>). A ?diff?-style comparison will reveal how different the two
extracted sequences of tags are. A threshold is set to filter out the pairs of pages that
are not similar enough in HTML structure.
At this stage, nontextual parts of the pages are also removed. If a page does not
contain enough text, it is also discarded.
2.6.3 Language and Character Set. When we query search engines for documents in
one specific language, the returned documents may actually be in a different language
from the one we specified. This problem is particularly serious for Asian languages.
When we ask for Chinese Web pages, we often obtain Korean Web pages, because the
language of the documents has not been identified accurately by the search engines.
Another, more important factor that makes it necessary to use a language detector is
that during host crawling and pair scanning, no verification is done with regard to
languages. All files with an en suffix in their names, for example, are assumed to be
English pages, which may be an erroneous assumption.
391
Kraaij, Nie, and Simard Embedding Web-Based Statistical Models in CLIR
To filter out the files not in the required languages, the SILC system (Isabelle,
Simard, and Plamondon 1998) is used. SILC employs n-gram statistical language mod-
els to determine the most probable language and encoding schema for a text. It has
been trained on a number of large corpora for several languages. The accuracy of the
system is very high. When a text contains at least 50 characters, its accuracy is almost
perfect. SILC can filter out a set of file pairs that are not in the required languages.
Our utilization of HTML structure to determine whether two pages are parallel
is similar to that of Resnik (1998), who also exploits an additional criterion similar to
length-based sentence alignment in order to determine whether the segments in corre-
sponding HTML structures have similar lengths. In the current PTMiner, this criterion
is not incorporated. However, we have included the sentence-alignment criterion as a
later filtering step in Nie and Cai (2001): If a pair of texts cannot be aligned reasonably
well, then that pair is removed. This technique is shown to bring a large improvement
for the English-Chinese corpus. A similar approach could also be envisioned for the
corpora of European languages, but in the present study, such an approach is not used.
2.7 Mining Results
PTMiner uses heuristics that are mostly language-independent. This allows us to adapt
it easily for different language pairs by changing a few parameters (e.g., prefix and
suffix lists of file name). It is surprising that so simple an approach is nevertheless very
effective. We have been able, using PTMiner, to construct large parallel corpora from
the Web for the following language pairs: English-French, English-Italian, English-
German, English-Dutch, and English-Chinese. The sizes of these corpora are shown in
Table 1.
One question that may be raised is how accurate the mining results are, or how
parallel the pages identified are. Actually, it is very difficult to answer this question. We
have not undertaken an extensive evaluation but have only performed a simple evalu-
ation with a set of samples. For English-French, from 60 randomly selected candidate
sites, AltaVista indexed about 8,000 pages in French. From these, the pair-scanning
step identified 4,000 pages with equivalents in English. This showed that the lower
bound of recall of pairscanning is 50%. The equivalence of the pair pages identified
was judged by an undergraduate student who participated in developing the prelim-
inary version of PTMiner. The criterion used to judge the equivalence of two pages
was subjective, with the general guideline being whether two pages describe the same
contents and whether they have similar structures. To evaluate precision, 164 pairs
of pages from the 4,000 identified were randomly selected and manually checked. It
Table 1
Automatically mined corpora. n.a. = not available.
English-French English-German English-Italian
Number of pairs 18,807 10,200 8,504
Size (MB) 174/198 77/100 50/68
Number of words (M) 6.7/7.1 1.8/1.8 1.2/1.3
English-Dutch English-Chinese
24,738 14,820
n.a. 74/51
n.a. 9.2/9.9
392
Computational Linguistics Volume 29, Number 3
turned out that 162 of them were truly parallel. This shows that the precision is close
to 99%.
For an English-Chinese corpus, a similar evaluation has been reported in Chen
and Nie (2000). This evaluation was done by a graduate student working on PTMiner.
Among 383 pairs randomly selected at the pair-scanning step, 302 pairs were found
to be really parallel. The precision ratio is 79%, which is not as good as that of the
English-French case. There are several reasons for this:
? Incorrect links. It may be that a page is outdated but still indexed by the
search engines. A pair including that page will be eliminated in the
content-filtering step.
? Pages that are designed to be parallel, although the contents are not all translated
yet. One version of a page may be a simplified version of the other. Some
cases of this type can also be filtered out in the content-filtering step, but
some will still remain.
? Pages that are valid parallel pairs yet consist mostly of graphics rather than text.
These pages cannot be used for the training of translation models.
? Pairs that are not parallel at all. Filenames of some nonparallel pages may
accidentally match the naming rules. For example, ? . . ./et.html? versus
? . . ./etc.html?.
Related to the last reason, we also observed that the names of parallel Chinese
and English pages may be very different from one another. For example, it is frequent
practice to use the Pinyin translation as the name of a Chinese page of the correspond-
ing English file name (e.g., ?fangwen.html? vs. ?visit.html?). Another convention is to
use numbers as the filenames. For example ?1.html? would correspond to ?2.html?.
In either of these cases, our pair-scanning approach based on name similarity will
fail to recognize the pair. Overall, the naming of Chinese files is much more variable
and flexible than the naming of files for European languages. Hence, there exist fewer
evident heuristics for Chinese than for the European languages that would allow us
to enlarge the coverage and improve the precision of pair scanning.
Given the potentially large number of erroneously identified parallel pairs, a ques-
tion naturally arises: Can such a noisy corpus actually help CLIR? We will examine
this question in Section 4. In the next section we will briefly describe how statistical
translation models are trained on parallel corpora. We will focus in our discussion on
the following languages: English, French, and Italian. The resulting translation models
will be evaluated in a CLIR task.
3. Building the Translation Models
Bilingual pairs of documents collected from the Web are used as training material
for the statistical translation models that we exploit for CLIR. In practice, this mate-
rial must be organized into a set of small pairs of corresponding segments (typically,
sentences), each consisting of a sequence of word tokens. We start by presenting the
details of this preparatory step and then discuss the actual construction of the trans-
lation models.
3.1 Preparing the Corpus
3.1.1 Format Conversion, Text Segmentation, and Sentence Alignment. The collec-
tion process described in the previous section provides us with a set of pairs of HTML
393
Kraaij, Nie, and Simard Embedding Web-Based Statistical Models in CLIR
files. The first step in preparing this material is to extract the textual data from the
files and organize them into small, manageable chunks (sentences).
In doing so, we try to take advantage of the HTML markup. For instance, we
know that <P> tags normally identify paragraphs, <LI> tags mark list items that can
also often be interpreted as paragraphs, <Hn> tags are normally used to mark section
headers and may therefore be taken as sentences, and so on.
Unfortunately, a surprisingly large number of HTML files on the Web are badly
formatted, which calls for much flexibility on the part of Web browsers. To help cope
with this situation, we employ a freely distributed tool called tidy (Ragget 1998), which
attempts to clean up HTML files, so as to make them XML-compliant. This cleanup
process mostly consists in normalizing tag names to the standard XHTML lower-
case convention, wrapping tag attributes within double quotes and, most importantly,
adding missing tags so as to end up with documents with balancing opening and
closing tags.
Once this cleanup is done, we can parse the files with a standard SGML parser
(we use nsgmls [Clark 2001]) and use the output to produce documents in the standard
cesAna format. This SGML format, proposed as part of the Corpus Encoding Standard
(CES) (Ide, Priest-Dorman, and Ve?ronis 1995) has provisions for annotating simple
textual structures such as sections, paragraphs, and sentences. In addition to the cues
provided by the HTML tags, we employ a number of heuristics, as well as language-
specific lists of common abbreviations and acronyms, to locate sentence boundaries
within paragraphs. When, as sometimes happens, the tidy program fails to make sense
of its input on a particular file, we simply remove all SGML markup from the file
and treat the document as plain text, which means that we must rely solely on our
heuristics to locate paragraph and sentence boundaries.
Once the textual data have been extracted from pairs of documents and are neatly
segmented into paragraphs and sentences, we can proceed with sentence alignment.
This operation produces what we call couples, that is, minimal-size pairs of corre-
sponding segments between two documents. In the vast majority of cases, couples
consist of a single pair of sentences that are translations of one another (what we
call 1-to-1 couples). However, there are sometimes ?larger? couples, as when a single
sentence in one language translates into two or more sentences in the other language
(1-to-N or N-to-1), or when sentences translate many to many (N-to-M). Conversely,
there are also ?smaller? couples, such as when a sentence from either one of the two
texts does not appear in the other (0-to-1 or 1-to-0).
Our sentence alignments are carried out by a program called sfial, an improved
implementation of the method described in Simard, Foster, and Isabelle (1992). For a
given pair of documents, this program uses dynamic programming to compute the
alignment that globally maximizes a statistical-based scoring function. This function
takes into account the statistical distribution of translation patterns (1-to-1, 1-to-N, etc.)
and the relative sizes of the aligned text segments, as well as the number of ?cognate?
words within couples, that is, pairs of words with similar orthographies in the two
languages (e.g. statistical in English vs. statistique in French).
The data produced up to this point in the preparation process constitutes what
we call a Web-aligned corpus (WAC).
3.1.2 Tokenization, Lemmatization, and Stopwords. Since our goal is to use trans-
lation models in an IR context, it seems natural to have both the translation models
and the IR system operate on the same type of data. The basic indexing units of our
IR systems are word stems. Stemming is an IR technique whereby morphologically
related word forms are reduced to a common form: a stem. Such a stem does not
394
Computational Linguistics Volume 29, Number 3
necessarily have to be a linguistic root form. The principal function of the stem is
to serve as an index term in the vocabulary of index terms. Stemming is a form of
conflation: Equivalence classes of tokens help to reduce the variance in index terms.
Most stemming algorithms fall into two categories: (1) suffix strippers, and (2) full
morphological normalization (sometimes referred to as ?linguistic stemming? in the
IR literature). Suffix strippers remove suffixes in an iterative fashion using rudimental
morphological knowledge encoded in context-sensitive patterns. The advantage of al-
gorithms of this type (e.g., Porter 1980) is their simplicity and efficiency, although this
advantage applies principally to languages with a relatively simple morphology, like
English. A different way of generating conflation classes is to employ full morpholog-
ical analysis. This process usually consists of two steps: First the texts are POS-tagged
in order to eliminate each token?s part-of-speech ambiguity, and then word forms are
reduced to their root form, a process that we refer to as lemmatization. More informa-
tion about the relative utility of morphological normalization techniques in IR systems
can be found in, for example, Hull (1996), Kraaij and Pohlmann (1996), and Braschler
and Ripplinger (2003).
Lemmatizing and removing stopwords from the training material is also beneficial
for statistical translation modeling, helping to reduce the problem of data sparseness
in the training set. Furthermore, function words and morpho-syntactic features typi-
cally arise from grammatical constraints intrinsic to a language, rather than as direct
realizations of translated concepts. Therefore, we expect that removing them helps
the translation model focus on meaning rather than form. In fact, it has been shown
in Chen and Nie (2000) that the removal of stopwords from English-Chinese train-
ing material improves both the translation accuracy of the translation models and the
effectiveness of CLIR. We expect a similar effect for European languages.
We also have to tokenize the texts, that is, to identify individual word forms.
Because we are dealing with Romance languages, this step is fairly straightforward:6
We essentially segment the text using blank spaces and punctuation. In addition, we
rely on a small number of language-specific rules to deal, for example, with elisions
in French (l?amour ? l? + amour) and Italian (dell?arte ? dell? + arte), contractions in
French (au ? a` + le), possessives in English (Bob?s ? Bob + ?s), etc.
Once we have identified word tokens, we can lemmatize or stem them. For Italian,
we relied on a simple, freely distributed stemmer from the Open Muscat project.7
For French and English, we have access to more sophisticated tools that compute
each token?s lemma based on its part of speech (we use the HMM-based POS tagger
proposed in Foster (1991) and extensive dictionaries with morphological information.
As a final step, we remove stopwords.
Usually, 1-1 alignments are more reliable than other types of alignment. It is a
common practice to use only these alignments for model training, and this is what we
do.
Table 2 provides some statistics on the processed corpora.
3.2 Translation Models
In statistical translation modeling, we take the view that each possible target language
text is a potential translation for any given source language text, but that some trans-
lations are more likely than others. In the terms of Brown et al (1990), a noisy-channel
translation model is one that captures this state of affairs in a statistical distribution
6 The processing on Chinese is described in Chen and Nie (2000).
7 Currently distributed by OMSEEK:
?http://cvs.sourceforge.net/cgi-bin/viewcvs.cgi/omseek/om/languages/?.
395
Kraaij, Nie, and Simard Embedding Web-Based Statistical Models in CLIR
Table 2
Sentence-aligned corpora.
English-French English-Italian
Number of 1-1 alignments 1018K 196K
Number of tokens 6.7M/7.1M 1.2M/1.3M
Number of unique stems 200K/173K 102K/87K
P(T | S), where S is a source language text and T is a target language text.8 With such
a model, translating S amounts to finding the target language text T? that maximizes
P(T | S).
Modeling P(T | S) is, of course, complicated by the fact that there is an infinite
number of possible source and target language texts, and so much of the work of the
last 15 years or so in statistical machine translation has been aimed at finding ways
to overcome this complexity by making various simplifying assumptions. Typically,
P(T | S) is rewritten as
P(T | S) = P(T)P(S | T)
P(S)
following Bayes? law. This decomposition of P(T | S) is useful in two ways: first,
it makes it possible to ignore P(S) when searching for T?; second, it allows us to
concentrate our efforts on the lexical aspects of P(S | T), leaving it to P(T) (the ?target
language model?) to take care of syntactic and other language-specific aspects.
In one of the simplest and earliest statistical translation models, IBM?s Model 1,
it is assumed that P(S | T) can be approximated by a computation that uses only
?lexical? probabilities P(s | t) over source and target language words s and t. In other
words, this model completely disregards the order in which the individual words of
S and T appear. Although this model is known to be too weak for general translation,
it appears that it can be quite useful for an application such as CLIR, because many
IR systems also disregard word order, viewing documents and queries as unordered
bags of words.
The P(s | t) distribution is estimated from a corpus of aligned sentences like the
one we have produced from our Web-mined collection of bilingual documents, using
the expectation maximization (EM) algorithm (Baum 1972) to find the parameters
that maximize the likelihood of the training set. As in all machine-learning problems,
especially those related to natural language, data sparseness is a critical issue in this
process. Even with a large training corpus, many pairs of words (s, t) occur at very
low frequencies, and most never occur at all, making it impossible to obtain reliable
estimates for the corresponding P(s | t). Without adequate smoothing techniques, low-
frequency events can have disastrous effects on the global behavior of the model, and
unfortunately, in natural languages, low-frequency events are the norm rather than
the exception.
The goal of translation in CLIR is different from that in general language process-
ing. In the latter case it is important to enable a model to handle low-frequency words
and unknown words. For CLIR the coverage of low-frequency words or unknown
words by the model is less problematic. Even if a low-frequency word is translated
8 The model is referred to as noisy-channel because it takes the view that S is the result of some input
signal T?s being corrupted while passing through a noisy channel. In this context, the goal is to recover
the initial input, given the corrupted output.
396
Computational Linguistics Volume 29, Number 3
incorrectly, the global IR effectiveness will often not be significantly affected, because
low-frequency words likely do not appear often in the document collection to be
searched or other terms in the query could compensate for this gap. Most IR algo-
rithms are based on a term-weighting function that favors terms that occur frequently
in a document but occur infrequently in the document collection. This means that the
best index terms have a medium frequency (Salton and McGill 1983). Stopwords and
(near) hapaxes are less important for IR; limited coverage of very infrequent words in
a translation model is therefore not critical for the performance of a CLIR system.
Proper nouns are special cases of unknown words. When they appear in a query,
they usually denote an important part of the user?s intention. However, we can adopt
a special approach to cope with these unknown words in CLIR without integrating
them as the generalized case in the model. For example, one can simply retain all the
unknown words in the query translation. This approach works well for most cases
in European languages. We have previously shown that a fuzzy-matching approach
based on n-grams offers an effective means of overcoming small spelling variations in
proper noun spelling (Kraaij, Pohlmann, and Hiemstra 2000).
The model pruning techniques developed in computational linguistics are also
useful for the models used in CLIR. The beneficial effect is that unreliable (or low-
probability) translations can be removed. In Section 4, model smoothing will be moti-
vated from a more theoretical point of view. Here, let us first outline the two variations
we used to prune the models.
The first one is simple, yet effective in our application: We consider unreliable all
parameters (translation probabilities) whose value falls below some preset threshold
(in practice, 0.1 works well). These parameters are simply discarded from the model.
The remaining parameters are then renormalized so that all marginal distributions
sum to one.
Another pruning technique is based on the relative contribution to the entropy
of the model. We retain the N most reliable parameters (in practice, N = 100K works
well). The reliability of a parameter is measured with regard to its contribution to the
model?s entropy (Foster 2000). In other words, we discard the parameters that least
affect the overall probability of the training set. The remaining parameters are then
renormalized so that all marginal distributions sum to one.
Of course, as a result of this, most pairs of words (s, t) are unknown to the trans-
lation model (translation probability equals zero). As previously discussed, however,
this will not have a disastrous effect on CLIR; on the contrary, some positive effect can
be expected as long as there is at least one translation for each source term.
One important characteristic of these noisy-channel models is that they are ?di-
rectional.? Depending on the intended use, it must be determined beforehand which
language is the source and which the target for each pair of languages. Although ?re-
verse? parameters can theoretically be obtained from the model through Bayes? rule,
it is often more practical to train two separate models if both directions are needed.
This topic is also discussed in the next section.
4. Embedding Translation into the IR Model
When CLIR is considered simply as a combination of separate MT and IR components,
the embedding of the two functions is not a problem. However, as we explained in
Section 1, there are theoretical motivations for embedding translation into the retrieval
model. Since translation models provide more than one translation, we will try to
exploit this extra information, in order to enhance retrieval effectiveness. In Section 4.1
we will first introduce a monolingual probabilistic IR model based on cross entropy
397
Kraaij, Nie, and Simard Embedding Web-Based Statistical Models in CLIR
between a unigram language model for the query and one for the document. We
discuss the relationship of this model to IR models based on generative language
models. Subsequently, we show several ways to add translation to the model: One
can either translate the query language model from the source language into the target
language (i.e., the document language) before measuring the cross entropy, or translate
the document model from the target language into the source language and then
measure the cross entropy.
4.1 Monolingual IR Based on Unigram Language Models
Recently, a new approach to IR based on statistical language models has gained wide
acceptance. The approach was developed independently by several groups (Ponte
and Croft 1998; Miller, Leek, and Schwartz 1999; Hiemstra 1998) and has yielded
results on several IR standardized evaluation tasks that are comparable to or better
than those obtained using the existing OKAPI probabilistic model. In comparison
with the OKAPI model, the IR model based on generative language models has some
important advantages: It contains fewer collection-dependent tuning parameters and
is easy to extend. For a more detailed discussion of the relationships between the
classical (discriminative) probabilistic IR models and recent generative probabilistic IR
models, we refer the reader to Kraaij and Spitters (2003). Probably the most important
idea in the language-modeling approach to IR is that documents are scored on the
probability that they generate the query; that is, the problem is reversed, an idea that
has successfully been applied in speech recognition. There are various reasons that
this approach has proven fruitful, probably the most important being that documents
contain much more data for estimating the parameters of a probabilistic model than do
ad hoc queries (Lafferty and Zhai 2001b). For ad hoc retrieval, one could describe the
query formulation process as follows: A user has an ideal relevant document in mind
and tries to describe it by mentioning some of the salient terms that he thinks occur in
the document, interspersed with some query stop phrasing like ?Relevant documents
mention. . . .? For each document in the collection, we can compute the probability
that the query is generated from a model representing that document. This generation
process can serve as a coarse way of modeling the user?s query formulation process.
The query likelihood given each document can directly be used as a document-ranking
function. Formula (1) shows the basic language model, in which a query Q consists of
a sequence of terms T1, T2, . . . , Tm that are sampled independently from a document
unigram model for document dk (Table 3 presents an explanation of the most important
symbols used in equations (1)?(12)):
P(Q | Dk) = P(T1, T2, . . . , Tm | Dk) ?
m
?
j=1
P(Tm | MDk) (1)
In this formula MDk denotes a language model of Dk. It is indeed an approximation of
Dk. Now, if a query is more probable given a language model based on document D1
than given a language model based on document D2, we can then hypothesize that
document D1 is more likely to be relevant to the query than document D2. Thus the
probability of generating a certain query given a document-based language model can
serve as a score for ranking documents with respect to topical relevance. It is common
practice to work with log probabilities, which has the advantage of reducing products
to summations. We will therefore rewrite (1) in logarithmic form. Since terms might
occur more than once in a query, we prefer to work with types ?i instead of tokens
398
Computational Linguistics Volume 29, Number 3
Ti. So c(Q, ?i) is the number of occurrences of ?i in Q (query term frequency); we will
also omit the document subscript k in the following presentation:
log P(Q | D) =
n
?
i=1
c(Q, ?i) log P(?i | MD) (2)
A second core technique from speech recognition that plays a vital role in language
models for IR is smoothing. One obvious reason for smoothing is to avoid assigning
zero probabilities for terms that do not occur in a document because the term prob-
abilities are estimated using maximum-likelihood estimation.9 If a single query term
does not occur in a document, this would result in a zero probability of generating
that query, which might not be desirable in many cases, since documents discuss a
certain topic using only a finite set of words. It is very well possible that a term that
is highly relevant for a particular topic may not appear in a given document, since
it is a synonym for other terms that are also highly relevant. Longer documents will
in most cases have a better coverage of relevant index terms (and consequently better
probability estimates) than short documents, so one could let the level of smoothing
depend on the length of the document (e.g., Dirichlet priors). A second reason for
smoothing probability estimates of a generative model for queries is that queries con-
sist of (1) terms that have a high probability of occurrence in relevant documents and
(2) terms that are merely used to formulate a proper query statement (e.g., ?Docu-
ments discussing only X are not relevant?). A mixture of a document language model
and a language model of typical query terminology (estimated on millions of queries)
would probably give good results (in terms of a low perplexity).
We have opted for a simple approach that addresses both issues, namely, applying
a smoothing step based on linear interpolation with a background model estimated on
a large document collection, since we do not have a collection of millions of queries:
log P(Q | D) =
n
?
i=1
c(Q, ?i) log((1 ? ?)P(?i | MD) + ?P(?i | MC)) (3)
Here, P(?i | MC) denotes the marginal probability of observing the term ?i, which can be
estimated on a large background corpus, and ? is the smoothing parameter. A common
range for ? is 0.5?0.7, which means that document models have to be smoothed quite
heavily for optimal performance. We hypothesize that this is mainly due to the query-
modeling role of smoothing. Linear interpolation with a background model has been
frequently used to smooth document models (e.g., Miller, Leek, and Schwartz 1999;
Hiemstra 1998). Recently other smoothing techniques (Dirichlet, absolute discounting)
have also been evaluated. An initial attempt to account for the two needs for smoothing
(sparse data problem, query modeling) with separate specialized smoothing functions
yielded positive results (Zhai and Lafferty 2002).
We have tested the model corresponding to formula (3) in several different IR
applications: monolingual information retrieval, filtering, topic detection, and topic
tracking (cf. Allen [2002] for a task description of the latter two tasks). For several
of these applications (topic tracking, topic detection, collection fusion), it is important
9 The fact that language models have to be smoothed seems to contradict the discussion in Section 3, in
which we stated that rare terms are not critical for IR effectiveness, but it actually does not. Smoothing
helps to make the distinction between absent important terms (middle-frequency terms) and absent
nonimportant terms (high-frequency terms). The score of a document that misses important terms
should be lowered more than that of a document that misses an unimportant term.
399
Kraaij, Nie, and Simard Embedding Web-Based Statistical Models in CLIR
Table 3
Common symbols used in equations (1)?(12) and their
explanations.
Symbol Explanation
Q Query has representation Q = {T1, T2, . . . , Tn}
D Query has representation D = {T1, T2, . . . , Tn}
MQ Query language model
MD Document language model
MC Background language model
?i index term
si term in the source language
ti term in the target language
? smoothing parameter
c(x) counts of x
that scores be comparable across different queries (Spitters and Kraaij 2001). The basic
model does not provide such comparability of scores, so it has to be extended with
score normalization. There are two important steps in doing this. First of all, we would
like to normalize across query specificity. The generative model will produce low scores
for specific queries (since the average probability of occurrence is low) and higher
scores for more general queries. Normalization can be accomplished by modeling the
IR task as a likelihood ratio (Ng 2000). For each term in the query, the log-likelihood
ratio (LLR) model judges how surprising it is to see the term, given the document
model in comparison with the background model:
LLR(Q | D) = log P(Q | MD)
P(Q | MC)
=
n
?
i=1
c(Q, ?i) log
((1 ? ?)P(?i | MD) + ?P(?i | MC))
P(?i | MC)
(4)
In (4), P(Q | MC) denotes the generative probability of the query given a language
model estimated on a large background corpus C. Note that P(Q | MC) is a query-
dependent constant and does not affect document ranking. Actually, model (4) has a
better justification than model (3), since it can be seen as a direct derivative of the
log-odds of relevance if we assume uniform priors for document relevance:
log
P(R | D, Q)
P(R? | D, Q)
= log
P(Q | R, D)
P(Q | R?, D)
+ log
P(R | D)
P(R? | D)
? log P(Q | MD)
P(Q | MC)
+ K (5)
In (5), R refers to the event that a user likes a particular document (i.e., the document
is relevant).
The scores of model (4) still depend on the query length, which can be easily
normalized by dividing the scores by the query length (
?
i c(Q, ?i)). This results in
formula (6) for the normalized log-likelihood ratio (NLLR) of the query:
NLLR(Q | D) =
n
?
i=1
c(Q, ?i)
?
i c(Q, ?i)
log
((1 ? ?)P(?i | MD) + ?P(?i | MC))
P(?i | MC)
(6)
A next step is to view the normalized query term counts c(Q, ?i)/
?
i c(Q, ?i) as
maximum-likelihood estimates of a probability distribution representing the query
P(?i | MQ). The NLLR formula can now be reinterpreted as a relationship between the
400
Computational Linguistics Volume 29, Number 3
two probability distributions P(? | MQ), P(? | MD) normalized by the the third distribu-
tion P(? | MC). The model measures how much better than the background model the
document model can encode events from the query model; or in information-theoretic
terms, it can be interpreted as the difference between two cross entropies:
NLLR(Q | D) =
n
?
i=1
P(?i | Q) log
P(?i | Dk)
P(?i | C)
= H(X | c)? H(X | d) (7)
In (7), X is a random variable with the probability distribution p(?i) = p(?i | MQ), and
c and d are probability mass functions representing the marginal distribution and the
document model. Cross entropy is a measure of our average surprise, so the better a
document model ?fits? a particular query distribution, the higher its score will be.10
The representation of both the query and a document as samples from a dis-
tribution representing, respectively, the user?s request and the document author?s
?mindset? has several advantages. Traditional IR techniques like query expansion
and relevance feedback can be reinterpreted in an intuitive framework of probabil-
ity distributions (Lafferty and Zhai 2001a; Lavrenko and Croft 2001). The framework
also seems suitable for cross-language retrieval. We need only to extend the model
with a translation function, which relates the probability distribution in one language
to the probability distribution function in another language. We will present several
solutions for this extension in the next section.
The NLLR also has a disadvantage: It is less easy in the NLLR to integrate prior
information about relevance into the model (Kraaij, Westerveld, and Hiemstra 2002),
which can be done in a straightforward way in formula (1), by simple multiplication.
CLIR is a special case of ad hoc retrieval, and usually a document length?based prior
can enhance results significantly. A remedy that has proven to be effective is linear
interpolation of the NLLR score with a prior log-odds ratio log (P(R | D)/P(?R | D)
(Kraaij 2002). For reasons of clarity, we have chosen not to include this technique in
the experiments presented here.
In the following sections, we will describe several ways to extend the monolingual
IR model with translation. The section headings include the run tags that will be used
in Section 5 to describe the experimental results.
4.2 Estimating the Query Model in the Target Language (QT)
In Section 4.1, we have seen that the basic retrieval model measures the cross entropy
between two language models: a language model of the query and a language model
of the document.11 Instead of translating a query before estimating a query model
(the external approach), we propose to estimate the query model directly in the target
language. We will do this by decomposing the problem into two components that are
easier to estimate:
P(ti | MQs) =
L
?
j
P(sj, ti | MQs) =
L
?
j
P(ti | sj, MQs)P(sj | MQs) ?
L
?
j
P(ti | sj)P(sj | MQs)
(8)
where L is the size of the source vocabulary. Thus, P(ti | MQs) can be approximated by
combining the translation model P(ti | sj), which we can estimate on the parallel Web
corpus, and the familiar P(sj | MQs), which can be estimated using relative frequencies.
10 The NLLR can also be reformulated as a difference of two Kullback-Leibler divergences (Ng 2000).
11 We omit the normalization with the background model in the formula for presentation reasons.
401
Kraaij, Nie, and Simard Embedding Web-Based Statistical Models in CLIR
This simplified model, from which we have dropped the dependency of P(ti | sj)
on Q, can be interpreted as a way of mapping the probability distribution function
in the source language event space P(sj | MQs) onto the event space of the target
language vocabulary. Since this probabilistic mapping function involves a summation
over all possible translations, mapping the query model from the source language can
be implemented as the matrix product of a vector representing the query probability
distribution over source language terms with the translation matrix P(ti | sj).12 The
result is a probability distribution function over the target language vocabulary.
Now we can substitute the query model P(?i | MQ) in formula (7) with the target
language query model in (8) and, after a similar substitution operation for P(?i | MC),
we arrive at CLIR model QT:
NLLR-QT(Qs | Dt) =
n
?
i=1
L
?
j=1
P(ti | sj)P(sj | MQs) log
(1 ? ?)P(ti | MDt) + ?P(ti | MCt)
P(ti | MCt)
(9)
4.3 Estimating the Document Model in the Source Language (DT)
Another way to embed translation into the IR model is to estimate the document
model in the source language:
P(si | MDt) =
N
?
j
P(si, tj | MDt) =
N
?
j
P(si | tj, MDt)P(tj | MDt) ?
N
?
j
P(si | tj)P(tj | MDt)
(10)
where N is the size of the target vocabulary. Obviously, we need a translation model
in the reverse direction for this approach. Now we can substitute (10) for P(?i | MD)
in formula (6), yielding CLIR model DT:
NLLR-DT(Qs | Dt) =
n
?
i=1
P(si | MQs) log
?N
j=1 P(si | tj)((1 ? ?)P(tj | MDt) + ?P(tj | MCt))
?N
j=1 P(si | tj)P(tj | MCt)
(11)
It is important to realize that both the QT and DT models are based on context-
insensitive translation, since translation is added to the IR model after the indepen-
dence assumption (1) has been made. Recently, a more complex CLIR model based on
relaxed assumptions?context-sensitive translation but term independence?based IR?
has been proposed in Federico and Bertoldi (2002). In experiments on the CLEF test
collections, the aforementioned model also proved to be more effective; however, it has
the disadvantage of reducing efficiency through its use of a Viterbi search procedure.
4.4 Variant Models and Baselines
In this section we will discuss several variant instantiations of the QT and DT models
that help us measure the importance of the number of translations (pruning) and the
weighting of translation alternatives. We also present several baseline CLIR algorithms
taken from the literature and discuss their relationship to the QT and DT models.
12 For presentation reasons, we have replaced the variable ? used in Section 4.1 with s and t for a term in
the source and target language, respectively.
402
Computational Linguistics Volume 29, Number 3
4.4.1 External Translation (MT, NAIVE). As we argued in Section 1, the most simple
solution to CLIR is to use an MT system to translate the query and use the translation
as the basis for a monolingual search operation in the target language. This solution
does not require any modification to the standard IR model as presented in Section 4.1.
We will refer to this model as the external-translation approach. The translated query
is used to estimate a probability distribution for the query in the target language. Thus,
the order of operations is: (1) translate the query using an external tool; (2) estimate
the parameters P(ti | MQt) of a language model based on this translated query.
In our experimental section below, we will list results with two different instantia-
tions of the external-translation approach: (1) MT: query translation by Systran, which
attempts to use high-level linguistic analysis, context-sensitive translation, extensive
dictionaries, etc., and (2) NAIVE: naive replacement of each query term with its trans-
lations (not weighted). The latter approach is often implemented using bilingual word
lists for CLIR. It is clear that this approach can be problematic for terms with many
translations, since they would then be assigned a higher relative importance. The
NAIVE method is included here only to study the effect of the number of translations
on the effectiveness of various models.
4.4.2 Best-Match Translation (QT-BM). In Section 3.2 we explained that there are
different possible strategies for pruning the translation model. An extreme pruning
method is best match, in which only the best translation is kept. A best-match transla-
tion model for query model translation (QT-BM) could also be viewed as an instance
of the external translation model, but one that uses a corpus-based disambiguation
method. Each query term is translated by the most frequent translation in the Web
corpus, disregarding the query context.
4.4.3 Equal Probabilities (QT-EQ). If we don?t know the precise probability of each
translation alternative for a given term, the best thing to do is to fall back on uniform
translation probabilities. This situation arises, for example, if we have only standard
bilingual dictionaries. We hypothesize that this approach will be more effective than
NAIVE but less effective than QT.
4.4.4 Synonym-Based Translation (SYN). An alternative way to embed translation
into the retrieval model is to view translation alternatives as synonyms. This is, of
course, something of an idealization, yet there is certainly some truth to the approach
when translations are looked up in a standard bilingual dictionary. Strictly speaking,
when terms are pure synonyms, they can be substituted for one another. Combining
translation alternatives with the synonym operator of the INQUERY IR system (Broglio
et al 1995), which conflates terms on the fly, has been shown to be an effective way
of improving the performance of dictionary-based CLIR systems (Pirkola 1998). In
our study of stemming algorithms (Kraaij and Pohlmann 1996), we independently
implemented the synonym operator in our system. This on-line conflation function
replaces the members of the equivalence class with a class ID, usually a morphological
root form. We have used this function to test the effectiveness of a synonymy-based
CLIR model in a language model IR setting.
The synonym operator for CLIR can be formalized as the following class equiva-
lence model (assuming n translations tj for term si and N unique terms in the target
language):
P(class(si) | MDt) =
?n
j c(tj, Dt)
?N
j c(tj, Dt)
=
N
?
j
?(si, tj)P(tj | MDt) (12)
403
Kraaij, Nie, and Simard Embedding Web-Based Statistical Models in CLIR
where P(class(si) | MDt) is the probability that a member of the equivalence class of si
is generated by the language model MDt and
?(si, tj) =
{
1 if tj ? class(si)
0 if tj /? class(si)
(13)
Here c(tj, Dt) is the term frequency (counts) of term tj in document Dt.
The synonym class function ?(si, tj) can be interpreted as a special instantiation
of the translation model P(si | tj) in (10), namely, P(si | tj) = 1 for all translations tj
of si. Of course, this does not yield a valid probability function, since the translation
probabilities for all translations si of a certain tj do not sum to one, because the pseudo?
synonym classes are not disjunct because of sense ambiguity. But the point is that the
structure of a probabilistic version of the SYN model is similar to that of the DT model,
namely, one in which all translations have a reverse translation probability P(si | tj)
equal to one. This is obviously just an approximation of reality. We therefore expect that
this model will be less effective than the QT and DT models. In our implementation
of the SYN model, we formed equivalence classes by looking up all translations of a
source term si in the translation model P(tj | si). The translations receive a weight of
one and are used as pseudo translation?probabilities13 in the model corresponding to
formula (11).
4.5 Related Work
In dictionary-based approaches, the number of translation alternatives is usually not
as high as in (unpruned) translation models, so these alternatives can be used in
some form of query expansion (Hull and Grefenstette 1996; Savoy 2002). However, it
is well known that most IR models break down when the number of translations is
high. To remedy this, researchers have tried to impose query structure, for example,
by collecting translation alternatives in an equivalence class (Pirkola 1998), or via a
quasi-Boolean structure (Hull 1997).
The idea of embedding a translation step into an IR model based on query like-
lihood was developed independently by several researchers (Hiemstra and de Jong
1999; Kraaij, Pohlmann, and Hiemstra 2000; Berger and Lafferty 2000). Initially trans-
lation probabilities were estimated from machine-readable dictionaries, using simple
heuristics (Hiemstra et al 2001). Other researchers have successfully used models sim-
ilar to DT, in combination with translation models trained on parallel corpora, though
not from the Web (McNamee and Mayfield 2001; Xu, Weischedel, and Nguyen 2001).
5. Experiments
We carried out several contrastive experiments to gain more insight into the relative
effectiveness of the various CLIR models presented in Sections 4.2?4.4. We will first
outline our research questions, before describing the experiments in more detail.
5.1 Research Questions
The research questions we are hoping to answer are the following:
1. How do CLIR systems based on translation models perform with respect
to reference systems (e.g., monolingual, MT )?
13 It may be better to view them as mixing weights in this case.
404
Computational Linguistics Volume 29, Number 3
2. Which manner of embedding a translation model is most effective for
CLIR? How does a probabilistically motivated embedding compare with
a synonym-based embedding?
3. Is there a query expansion effect, and if so, how can we exploit it?
4. What is the relative importance of pruning versus weighting?
5. Which models are robust against noisy translations?
The first two questions concern the main goal of our experiments: What is the effec-
tiveness of a probabilistic CLIR system in which translation models mined from the
Web are an integral part of the model, compared to that of CLIR models in which
translation is merely an external component? The remaining questions help us to un-
derstand the relative importance of various design choices in our approach, such as
pruning and translation model orientation.
5.2 Experimental Conditions
We have defined a set of contrastive experiments in order to help us answer the
research questions presented above. These experiments seek to compare:
1. The effectiveness of approaches incorporating a translation model
produced from the Web to that of a monolingual baseline and an
off-the-shelf external query translation approach based on Systran (MT).
2. The effectiveness of embedding query model translation (QT) and that of
document model translation (DT).
3. The effectiveness of using the entire set of translations, each of which is
weighted, (QT) to that of using just the most probable translation
(QT-BM).
4. The effectiveness of weighted query model translation (QT) to that of
equally weighted translations (QT-EQ) and nonweighted translations
(NAIVE).
5. The effectiveness of treating translations as synonyms (SYN) with that of
weighted translations (QT) and equally weighted translations (QT-EQ).
6. Different translation model pruning strategies: best N parameters or
thresholding probabilities.
Each strategy is represented by a run tag, as shown in Table 4.
Table 5 illustrates the differences among the different translation methods. It lists,
for several CLIR models, the French translations of the word drug taken from one of
the test queries that talks about drug policy.
The translations in Table 5 are provided by the translation models P(e | f ) and
P(f | e). The translation models have been pruned by discarding the translations with
P < 0.1 and renormalizing the model (except for SYN), or by retaining the 100K best
parameters of the translation model. The first pruning method (probability threshold)
has a very different effect on the DT method than on the QT method: The number of
terms that translate into drug, according to P(e | f ), is much larger than the number
of translations of drug found in P(f | e). There are several possible explanations for
this: Quite a few French terms, including the verb droguer and the compounds pharma-
core?sistance and pharmacothe?rapie, all translate into an English expression or compound
405
Kraaij, Nie, and Simard Embedding Web-Based Statistical Models in CLIR
Table 4
Explanation of run tags.
Matching
Run Tag Short Description Language Section
MONO monolingual run 4.1, 5.5
MT Systran external query translation target 4.4.1, 5.5
NAIVE equal probabilities target 4.4.1
QT translation of the query language model target 4.2
DT translation of the document language model source 4.3
QT-BM best match, one translation per word target 4.4.2
QT-EQ equal probabilities target 4.4.3
SYN synonym run based on forward equal probabilities source 4.4.4
Table 5
Example translations: Stems and probabilities with different CLIR methods.
Run ID Translation Translation
Model
MT drogues
QT <drogue, 0.55; medicament, 0.45> P(f | e) ? 0.1
QT-EQ <drogue, 0.5; medicament, 0.5>
QT-BM <drogue, 1.0>
SYN <drogue, 1.0; medicament, 1.0>
NAIVE <drogue, 1.0; medicament, 1.0>
DT <antidrogue, 1.0; drogue, 1.0; droguer, 1.0; drug,
1.0; me?dicament, 0.79; drugs, 0.70; drogue?, 0.61;
narcotrafiquants, 0.57; relargage, 0.53; pharmacovigi-
lance, 0.49; pharmacore?sistance, 0.47; me?dicamenteux,
0.36; ste?ro??diens, 0.35, stupe?fiant, 0.34; assurance-
me?dicaments, 0.33; surdose, 0.28; pharmacore?sistants,
0.28; pharmacode?pendance, 0.27; pharmacothe?rapie,
0.25; alcoolisme, 0.24; toxicomane, 0.23; bounce, 0.23; an-
ticance?reux, 0.22; anti-inflammatoire, 0.17; selby, 0.16; es-
cherichia, 0.14; homelessness, 0.14; anti-drogues, 0.14; an-
tidiarrhe?ique, 0.12; imodium, 0.12; surprescription, 0.10>
P(e | f ) ? 0.1
QT <drogue, 0.45; medicament, 0.35; consommation, 0.06; re-
lier, 0.03; consommer, 0.02; drug, 0.02; usage, 0.02; toxico-
manie, 0.01; substance, 0.01; antidrogue, 0.01; utilisation,
0.01; lier, 0.01; the?rapeutique, 0.01; actif, 0.01; pharmaceu-
tique, 0.01>
P(e | f ), 100K
DT <reflexions, 1; antidrogue, 1; narcotrafiquants, 1;
drug, 1; droguer, 0.87; drogue, 0.83; drugs, 0.81;
me?dicament, 0.67; pharmacore?sistance, 0.47; pharma-
core?sistants, 0.44; me?dicamenteux, 0.36; stupe?fiant, 0.34;
assurance-me?dicaments, 0.33; pharmacothe?rapie, 0.33;
amphe?tamine, 0.18; toxicomane, 0.17; me?morandum,
0.10; toxicomanie, 0.08; architectural, 0.08; pharmacie,
0.07; pharmaceutique, 0.06; the?rapeutique, 0.04; sub-
stance, 0.01>
P(f | e), 100K
406
Computational Linguistics Volume 29, Number 3
involving the word drug. Since our translation model is quite simple, these compound-
compound translations are not learned.14 A second factor that might play a role is the
greater verbosity of French texts compared to their English equivalents (cf. Table 2).
For the models that have been pruned using the 100K-best-parameters criterion, the
differences between QT and DT are smaller. Both methods yield multiple translations,
most of which seem related to drug, so there is a clear potential for improved recall as
a result of the query expansion effect. Notice, however, that the expansion concerns
both the medical and the narcotic senses of the word drug. We will see in the following
section that the CLIR model is able to take advantage of this query expansion effect,
even if the expansion set is noisy and not disambiguated.
5.3 The CLEF Test Collection
To answer the research questions stated in section 5.1, we carried out a series of ex-
periments on a combination of the CLEF-2000, -2001 and -2002 test collections.15 This
joint test collection consists of documents in several languages (articles from major
European newspapers from the year 1994), 140 topics describing different informa-
tion needs (also in several languages) and their corresponding relevance judgments.
(Relevance judgments are a human-produced resource that states, for a subset of a
document collection, whether a document is relevant for a particular query.) We used
only the English, Italian, and French data for the CLIR experiments reported here. The
main reason for this limitation was that the IR experiments and translation models
were developed at two different sites equipped with different proprietary tools. We
chose language pairs for which the lemmatization/stemming step for both the trans-
lation model training and indexing system were equivalent. A single test collection
was created by merging the three topic sets in order to increase the reliability of our
results and sensitivity of significance tests. Each CLEF topic consists of three parts:
title, description, and narrative. An example is given below:
<num> C001
<title> Architecture in Berlin
<description> Find documents on architecture in Berlin.
<narrative> Relevant documents report, in general, on the
architectural features of Berlin or, in particular, on the
reconstruction of some parts of the city after the fall of the
Wall.
We used only the title and description parts of the topics and concatenated
these simply to form the queries. Table 6 lists some statistics on the test collection.16
The documents were submitted to the preprocessing (stemming/lemmatization)
procedure we described in Section 3.1.2. However, for English and French lemmatiza-
tion, we used the Xelda tools from XRCE,17 which perform morphological normaliza-
tion slightly differently from the one described in Section 3.1.2. However, since the two
14 A more extreme case is query C044 about the ?tour de france.? According to the P(e | f ) > 0.1
translation model, there are 902 French words that translate into the ?English? word de. This is mostly
due to French proper names, which are left untranslated in the English parallel text.
15 CLEF = Cross Language Evaluation Forum, ?www.clef-campaign.org?.
16 Topics without relevant documents in a subcollection were discarded.
17 Available at ?http://www.xrce.xerox.com/competencies/ats/xelda/summary.html?.
407
Kraaij, Nie, and Simard Embedding Web-Based Statistical Models in CLIR
Table 6
Statistics on the test collection.
French English Italian
Document Source Le Monde Los Angeles Times La Stampa
Number of documents 44,013 110,250 58,051
Number of topics 124 122 125
Number of relevant documents 1,189 2,256 1,878
lemmatization strategies are based on the same principle (POS tagging plus inflection
removal), the small differences in morphological dictionaries and POS tagging had no
significant influence on retrieval effectiveness.18
All runs use a smoothing parameter ? = 0.3. This value had been shown to work
well for CLIR experiments with several other collections.
5.4 Measuring Retrieval Effectiveness
The effectiveness of retrieval systems can be evaluated using several measures. The
basic measures are precision and recall, which cannot be applied directly, since they
assume clearly separated classes of relevant and nonrelevant documents. The most
widely accepted measure for evaluating effectiveness of ranked retrieval systems is
the average uninterpolated precision, most often referred to as mean average precision
(MAP), since the measure is averaged first over relevant documents and then across
topics. Other measures, such as precision at a fixed rank, interpolated precision, or
R-precision, are strongly correlated to the mean average precision, so they do not really
provide additional information (Tague-Sutcliffe and Blustein 1995; Voorhees 1998).
The average uninterpolated precision for a given query and a given system version
can be computed as follows: First identify the rank number n of each relevant docu-
ment in a retrieval run. The corresponding precision at this rank number is defined
as the number of relevant documents found in the ranks equal to or higher than the
respective rank r divided by n. Relevant documents that are not retrieved are assigned
a precision of zero. The average precision for a given query is defined as the average
value of the precision pr over all known relevant documents dij for that query. Finally,
the mean average precision can be calculated by averaging the average precision over
all M queries:
MAP =
1
M
M
?
j=1
1
Nj
Nj
?
i=1
pr(dij), where pr(dij) =
{ rni
ni
, if dij retrieved and ni ? C
0, in other cases
(14)
Here, ni denotes the rank of the document dij, which has been retrieved and is relevant
for query j, rni is the number of relevant documents found up to and including rank
ni, Nj is the total number of relevant documents of query j, M is the total number of
queries, and C is the cutoff rank (C is 1,000 for TREC experiments).
18 We have not been able to substantiate this claim with quantitative figures but did analyze the lemmas
that were not found in the translation dictionaries during query translation. We did not find any
structural mismatches.
408
Computational Linguistics Volume 29, Number 3
Since we compared many different system versions, which do not always dis-
play a large difference in effectiveness, it is desirable to perform significance tests on
the results. However, it is well known that parametric tests for data resulting from
IR experiments are not very reliable, since the assumptions of these tests (normal or
symmetric distribution, homogeneity of variances) are usually not met. We checked
the assumptions for an analysis of variance (by fitting a linear model for a within-
subjects design) and found that indeed the distribution of the residual error was quite
skewed, even after transformation of the data. Therefore, we resorted to a nonpara-
metric alternative for the analysis of variance, the Friedman test (Conover 1980). This
test is preferable, for the analysis of groups of runs instead, to multiple sign-tests or
Wilcoxon signed-rank tests, since it provides overall alpha protection. This means that
we first test whether there is any significant difference at all between the runs, before
applying multiple-comparison tests. Applying just a large number of paired signifi-
cance tests at the 0.05 significance level without a global test leads very quickly to a
high overall alpha. After applying the Friedman test, we ran Fisher?s LSD multiple-
comparison tests (recommended by Hull) to identify equivalence classes of runs (Hull
1993; Hull, Kantor, and Ng 1999). An equivalence class is a group of runs that do
not differ significantly (e.g., in terms of mean average precision) from one another in
terms of performance.
5.5 Baseline Systems
We decided to have two types of baseline runs. It is standard practice to take a mono-
lingual run as a baseline. This run is based on an IR system using document ranking
formula (6). Contrary to runs described in Kraaij (2002), we did not use any additional
performance-enhancing devices, like document length?based priors or fuzzy match-
ing, in order to focus on the basic retrieval model extensions, avoiding interactions.
Systran was used as an additional cross-language baseline, to serve as a reference
point for cross-language runs. Notice that the lexical coverage of MT systems varies
considerably across language pairs; in particular, the French-English version of Systran
is quite good in comparison with those available for other language pairs. We accessed
the Web-based version of Systran (December 2002), marketed as Babelfish, using the
Perl utility babelfish.pm and converted the Unicode output to the ISO-Latin1 character
set to make it compatible with the Xelda-based morphology.
5.6 Results
Table 7 shows the results for the different experimental conditions in combination
with a translation model pruned with the probability threshold criterion P > 0.1 (cf.
Section 3.2). For each run, we computed the mean average precision using the standard
evaluation tool trec eval. We performed Friedman tests on all the runs based on the
Web translation models, because these are the runs in which we are most interested;
furthermore, one should avoid adding runs that are quite different to a group that is
relatively homogeneous, since this can easily lead to a false global-significance test.
The Friedman test (as measured on the F distribution) proved significant at the p <
0.05 level in all cases, so we created equivalence classes using Fisher?s LSD method,
which are denoted by letters. Letters are assigned to classes in decreasing order of
performance; so if a run is a member of equivalence class a, it is one of the best runs
for that particular experimental condition.
The last four rows of the table provide some additional statistics on the query
translation process. For both the forward (P(t | s),fw) and the reverse (P(s | t),rev)
409
Kraaij, Nie, and Simard Embedding Web-Based Statistical Models in CLIR
Table 7
Mean average precision and translation statistics (p > 0.1).
English- French- English- Italian-
Run ID French English Italian English
MONO 0.4233 0.4705 0.4542 0.4705
MT 0.3478 0.4043 0.3060 0.3249
QT a:0.3760 a:0.4126 a,b:0.3298 a:0.3526
DT a:0.3677 a,b:0.4090 a:0.3386 a,b:0.3328
SYN a:0.3730 b,c:0.3987 a,b:0.3114 b:0.3498
QT-EQ a:0.3554 a,b:0.3987 c,d:0.3035 b,c:0.3299
QT-BM a:0.3463 c,d:0.3769 b,c:0.3213 b:0.3221
NAIVE b:0.3303 d:0.3596 d:0.2881 c:0.3183
Percentage of missed forward 9.6 13.54 16.79 9.17
Percentage of missed reverse 9.08 14.04 15.48 11.31
Number of translations forward 1.65 1.66 1.86 2.13
Number of translations reverse 22.72 29.6 12.00 22.95
Table 8
Mean average precision and translation statistics (best 100K parameters).
English- French- English- Italian-
Run ID French English Italian English
MONO 0.4233 0.4705 0.4542 0.4705
MT 0.3478 0.4043 0.3060 0.3249
DT a:0.3909 a:0.4073 a:0.3728 a:0.3547
QT a,b:0.3878 a:0.4194 a:0.3519 a:0.3678
QT-BM b:0.3436 b:0.3702 b:0.3236 b:0.3124
SYN c:0.3270 b:0.3643 b:0.2958 c:0.2808
QT-EQ c:0.3102 b:0.3725 c:0.2602 c:0.2595
NAIVE d:0.2257 c:0.2329 d:0.2281 d:0.2021
Percentage of missed forward 11.04 14.65 16.06 9.36
Percentage of missed reverse 10.39 16.81 15.76 10.53
Number of translations forward 7.04 7.00 6.36 7.23
Number of translations reverse 10.51 12.34 13.32 17.20
translation model, we list the percentage of missed translations19 of unique query
terms and the average number of translations per unique query term. Table 8 shows
the results for the same experimental conditions, but this time the translation models
were pruned by taking the n best translation relations according to an entropy criterion,
where n = 100, 000.
Several other similar pruning methods have also been tested on the CLEF-2000
subset of the data (e.g. P > 0.01, P > 0.05, 1M parameters, 10K parameters). How-
ever, the two cases shown in Tables 7 and 8 represent the best of the two families
of pruning techniques. Our goal was not to do extensive parameter tuning in or-
der to find the best-performing combination of models, but rather to detect some
broad characteristics of the pruning methods and their interactions with the retrieval
model.
19 This figure includes proper nouns.
410
Computational Linguistics Volume 29, Number 3
Table 9
Mean average precision of combination run, compared to baselines.
Run ID English-French French-English English-Italian Italian-English
MONO 0.4233 0.4705 0.4542 0.4705
MT 0.3478 (82%) 0.4043 (86%) 0.3060 (67%) 0.3249 (69%)
DT+QT 0.4042 (96%) 0.4273 (87%) 0.3837 (84%) 0.3785 (80%)
Since the pruned forward and reverse translation models yield different translation
relations (cf. Table 5), we hypothesized that it might be effective to combine them.
Instead of combining the translation probabilities directly, we chose to combine the
results of the QT and DT models by interpolation of the document scores. Results
for combinations based on the 100K models are shown in Table 9. Indeed, for all
the language pairs, the combination run improves upon each of its component runs.
This means that each component run can compensate for missing translations in the
companion translation model.
5.7 Discussion
5.7.1 Web-Based CLIR versus MT-Based CLIR. Our first observation when examining
the data is that the runs based on translation models are comparable to or better than
the Systran run. Sign tests showed that there was no significant difference between the
MT and QT runs for English-French and French-English language pairs. The QT runs
were significantly better at the p = 0.01 level for the Italian-English and English-Italian
language pairs.
This is a very significant result, particularly since the performance of CLIR with
Systran has often been among the best in the previous CLIR experiments in TREC and
CLEF. These results show that the Web-based translation models are effective means for
CLIR tasks. The better results obtained with the Web-based translation models confirm
our intuition, stated in Section 1, that there are better tools for query translation in
CLIR than off-the-shelf commercial MT systems.
Compared to the monolingual runs, the best CLIR performance with Web-based
translation models varies from 74.1% to 93.7% (80% to 96% for the combined QT+DT
models) of the monolingual run. This is within the typical range of CLIR performance.
More generally, this research successfully demonstrates the enormous potential of par-
allel Web pages and Web-based MT.
We cannot really compare performance across target languages, since the relevant
documents are not distributed in a balanced way: Some queries do not yield any rele-
vant document in some languages. This partly explains why the retrieval effectiveness
of the monolingual Italian-Italian run is much higher than the monolingual French
and English runs. We can, however, compare methods within a given language pair.
5.7.2 Comparison of Query Model Translation (QT), Document Model Translation
(DT), and Translations Modeled as Synonyms (SYN). Our second question in Section
5.1 concerned the relative effectiveness of the QT and DT models. The experimental
results show that there is no clear winner; differences are small and not significant.
There seems to be some correlation with translation direction, however: The QT models
perform better than DT on the X-English pairs, and the DT models perform better
on the English-X pairs. This might indicate that the P(e | f ) and P(e | i) translation
models are more reliable than their reverse counterparts. A possible explanation for
411
Kraaij, Nie, and Simard Embedding Web-Based Statistical Models in CLIR
this could be that the average English sentence is shorter than the corresponding
French and Italian sentence. The average number of tokens per sentence is, respectively,
6.6/6.9 and 5.9/6.9 for English/French and English/Italian corpora. This may lead to
more reliable estimates for P(e | f ) and P(e | i) than the reverse. However, further
investigation is needed to confirm this, since differences in morphology could also
contribute to the observed effect. Still, the fact that QT models perform just as well as
DT models in combination with translation models is a new result.
We also compared our QT and DT models to the synonym-based approach (SYN)
(Pirkola 1998). Both the QT and DT models were significantly more effective than the
synonym-based model. The latter seems to work well when the number of translations
is relatively small but cannot effectively handle the large number of (pseudo)translations
produced by our 100K translation models. The synonym-based model usually per-
forms better than the models based on query translation with uniform probabilities,
but the differences are not significant in most cases.
5.7.3 Query Expansion Effect. In Section 1 we argued that using just one translation
(as MT does) is probably a suboptimal strategy for CLIR, since there is usually more
than one good translation for a particular term. Looking at probabilistic dictionaries,
we have also seen that the distinction between a translation and a closely related term
cannot really be made on the basis of some thresholding criterion. Since it is well
known in IR that adding closely related terms can improve retrieval effectiveness, we
hypothesized that adding more than one translation would also help. The experimen-
tal results confirm this effect. In all but one case (English-French, P > 0.1), using all
translations (QT) yielded significantly better performance than choosing just the most
probable translation (QT-BM). For the P > 0.1 models, the average number of transla-
tions in the forward direction is only 1.65, so the potential for a query expansion effect
is limited, which could explain the nonsignificant difference for the English-French
case.
Unfortunately, we cannot say whether the significant improvement in effectiveness
occurs mainly because the probability of giving at least one good translation (which is
probably the most important factor for retrieval effectiveness [Kraaij 2002; McNamee
and Mayfield 2002]) is higher for QT or indeed because of the query expansion effect.
A simulation experiment is needed to quantify the relative contributions. Still, it is
of great practical importance that more (weighted) translations can enhance retrieval
effectiveness significantly.
5.7.4 Pruning and Weighting. A related issue is the question of whether it is more
important to prune translations or to weight them. Grefenstette (cf. Section 1) originally
pointed out the importance of pruning and weighting translations for dictionary-based
CLIR. Pruning was seen as a means of removing unwanted senses in a dictionary-
based CLIR application. Our experiments confirm the importance of pruning and
weighting, but in a slightly different manner. In a CLIR approach based on a Web
translation model, the essential function of pruning is to remove spurious translations.
Polluted translation models will result in a very poor retrieval effectiveness. As far
as sense disambiguation is concerned, we believe that our CLIR models can handle
sense ambiguity quite well. Our best-performing runs, based on the 100K models,
have on average seven translations per term! Too much pruning (e.g., best match) is
suboptimal. However, the more translation alternatives we add, the more important
their relative weighting becomes.
We have compared weighted translations (QT) with uniform translation proba-
bilities (QT-EQ). In each of the eight comparisons (four language pairs, two pruning
412
Computational Linguistics Volume 29, Number 3
techniques), weighting results in an improved retrieval effectiveness. The difference
is significant in six of the eight cases. Differences are not significant for the P < 0.1
English-French and French-English translation models. We think this is due to the
small number of translations; a uniform translation probability will not differ radi-
cally from the estimated translation probabilities.
The importance of weighting is most evident when the 100K translation models
are used. These models yield seven translations on average for each term. The CLIR
models based on weighted translations are able to exploit the additional information
and show improved effectiveness with respect to the P < 0.1 models. The performance
of unweighted CLIR models (QT-EQ and SYN) is seriously impaired by the higher
number of translations.
The comparison of the naive dictionary-like replacement method, which does not
involve any normalization for the number of translations per term (NAIVE), with QT-
EQ shows that normalization (i.e. a minimal probabilistic embedding) is essential. The
NAIVE runs have the lowest effectiveness of all variant systems (with significant dif-
ferences). Interestingly, it seems better to select just the one most probable translation
than taking all translations unweighted.
5.7.5 Robustness. We pointed out in the previous section that the weighted models
are more robust, in the sense that they can handle a large number of translations.
We found, however, that the query model translation method (QT) and the docu-
ment model translation method (DT) display a considerable difference in robustness
to noisy translations. Initially we expected that the DT method (in which the match-
ing takes place in the source language) would yield the best results, since this model
has previously proven to be successful for several quite different language pairs (e.g.,
European languages, Chinese, and Arabic using parallel corpora or dictionaries as
translation devices [McNamee and Mayfield 2001; Xu, Weischedel, and Nguyen 2001;
Hiemstra et al 2001]).
However, our initial DT runs obtained extremely poor results. We discovered that
this was largely due to noisy translations from the translation model (pruned by the
P < 0.1 or 100K method), which is based on Web data. There are many terms in
the target language that occur very rarely in the parallel Web corpus. The translation
probabilities for these terms (based on the most probable alignments) are therefore
unreliable. Often these rare terms (and nonwords like xc64) are aligned with more
common terms in the other language and are not pruned by the default pruning criteria
(P > 0.1 or best 100K parameters), since they have high translation probabilities.
This especially poses a problem for the DT model, since it includes a summation
over all terms in the target language that occur in the document and have a nonzero
translation probability. We devised a supplementary pruning criterion to remove these
noisy translations, discarding all translations for which the source term has a marginal
probability in the translation model that is below a particular value (typically between
10?6 and 10?5). Later we discovered that a simple pruning method was even more
effective: discarding all translations for which either the source or target term contains
a digit. The results in Tables 7 and 8 are based on the latter additional pruning criterion.
The QT approach is less sensitive to noisy translations arising from rare terms in the
target language, because it is easy to remove these translations using a probability
threshold. We deduce that extra care therefore has to be taken to prune translation
models for the document model translation approach to CLIR.
413
Kraaij, Nie, and Simard Embedding Web-Based Statistical Models in CLIR
6. Conclusions
Statistical translation models require large parallel corpora, and unfortunately, only
a few manually constructed ones are available. In this article, we have explored the
possibility of automatically mining the Web for parallel texts in order to construct
such corpora. Translation models are then trained on these corpora. We subsequently
examined different ways to embed the resulting translation models in a cross-language
information retrieval system.
To mine parallel Web pages, we constructed a mining system called PTMiner. This
system employs a series of heuristics to locate candidate parallel pages and determine
whether they are indeed parallel. We have successfully used PTMiner to construct cor-
pora for a number of different language pairs: English-French, English-Italian, English-
German, English-Dutch, and English-Chinese. The language-independent characteris-
tics of PTMiner allowed us to adapt it quite easily to different language pairs.
The heuristics used in the mining process seem to be effective. Although the system
cannot collect all pairs of parallel pages, our preliminary evaluation shows that its
precision is quite high. (The recall ratio is less important in this context because of the
abundance of parallel pages on the Web.)
The mining results?parallel corpora?are subsequently used to train statistical
translation models, which are exploited in a CLIR system. The major advantage of
this approach is that it can be fully automated, avoiding the tedious work of man-
ual collection of parallel corpora. On the other hand, compared to manually prepared
parallel corpora, our mining results contain more noise (i.e., nonparallel pages). For
a general translation task this may be problematic; for CLIR, however, the noise con-
tained in the corpora is less dramatic. In fact, IR is strongly error tolerant. A small
proportion of incorrect translation words can be admitted without a major impact
on global effectiveness. Our experiments showed that a CLIR approach based on the
mined Web corpora can in fact outperform a good MT system (Systran). This confirms
our initial hypothesis that noisy parallel corpora can be very useful for applications
such as CLIR. Our demonstration that the Web can indeed be used as a large parallel
corpus for tasks such as CLIR is the main contribution of this article.
Most previous work on CLIR has separated the translation stage from the retrieval
stage (i.e., query translation is considered as a preprocessing step for monolingual IR).
In this article, we have integrated translation and retrieval within the same framework.
The advantage of this integration is that we do not need to obtain the optimal transla-
tion of a source query, and then an optimal retrieval result given a query translation,
but instead aim for the optimal global effect. The comparisons between our approach
and simulated external approaches clearly show that an integrated approach performs
better.
We also compared two ways of embedding translation models within a CLIR sys-
tem: (1) translating the source query model into the target (document) language, and
(2) translating the document model into the source language.20 Both embedding meth-
ods produced very good results compared to our reference run with Systran. However,
it is still too early to assert which embedding method is superior. We did observe a
significant difference in robustness between the two methods: The document model
translation method is much more sensitive to spurious translations, since the model
incorporates into a query term all source terms that have a nonzero translation proba-
bility. We devised two supplementary pruning techniques that effectively removed the
20 Another method that interprets multiple translations as synonyms is a special case of the latter.
414
Computational Linguistics Volume 29, Number 3
noisy terms: removing terms containing digits, and removing translations based on
source terms with a low marginal probability. (This latter approach is perhaps more
principled.)
On the use of statistical translation models for CLIR, we have demonstrated that
this naturally produces a desired query expansion effect, resulting in more related
documents being found. In our experimental evaluation, we saw that it is usually
better to include more than one translation, and to weigh these translations according
to the translation probabilities, rather than using the resulting translation model as
a bilingual lexicon for external translation. This effect partly accounts for the success
of our approach in comparison with an MT-based approach, which retains only one
translation per sense. However, this technique should not be exaggerated; otherwise,
too much noise will be introduced. To avoid this, it is important to incorporate pruning.
We investigated several ways to prune translation models. The best results were
obtained with a pruning method based on the top 100K parameters of the transla-
tion model. The translation models pruned with the best 100K parameters method
produced more than seven translations per word on average, demonstrating the ca-
pability of the CLIR model to handle translation ambiguity and exploit co-occurrence
information from the parallel corpus for query expansion.
There are several ways in which our approach can be improved. First, regarding
PTMiner, more or better heuristics could be integrated into the mining algorithm. As
we mentioned, parallel Web sites are not always organized in the ways we would
expect. This is particularly the case for those in non-European languages such as Chi-
nese and Japanese. Hence, one of the questions we wish to investigate is how to
extend the coverage of PTMiner to more parallel Web pages. One possible improve-
ment would be to integrate a component that ?learns? the organization patterns of
a particular Web site (assuming, of course, that the Web site is organized in a con-
sistent way). Preliminary tests have shown that this is possible to some extent: We
can recognize dynamically that the parallel pages on ?www.operationid.com? are at
?www.operationcarte.com? or that the file ?index1.html? corresponds to ?index2.html?.
Such criteria complement the ones currently employed in PTMiner.
In its current form, PTMiner scans candidates for parallel Web sites according to
similarities in their file names. This step does not exploit the hyperlinks between the
pages, whereas we know that two pages that are referenced at comparable structural
positions in two parallel pages have a very high chance of themselves being parallel.
Exploiting hyperlink structure to (help) find parallel Web pages could well improve
the quality of PTMiner.
When the mining results are not fully parallel, it would be interesting to attempt
to clean them in order to obtain a higher-quality training material. One possible ap-
proach for doing this would be to use sentence alignment as an additional filter, as
we mentioned earlier. This approach has been applied successfully to our English-
Chinese Web corpus. The cleaned corpus results in both higher translation accuracy
and higher CLIR effectiveness. However, this approach has still to be tested for the
European languages.
In this study, we hypothesized that IBM Model 1 is appropriate for CLIR, primarily
because word order is not important for IR. Although it is true that word order is not
important in current IR approaches, it is definitely important to consider context words
during the translation. For example, when deciding how to translate the French word
tableau (which may refer to a painting, a blackboard, a table [of data], etc.), if we
observe artistique (?artistic?) next to it, then it is pretty certain that tableau refers to a
painting. A more sophisticated translation model than IBM Model 1 could produce a
better selection of translation words.
415
Kraaij, Nie, and Simard Embedding Web-Based Statistical Models in CLIR
We also rely solely on word translation in our approach, although it is well known
that this simplistic approach cannot correctly translate compound terms such as pomme
de terre (?potato?) and cul de sac (?no exit?). Incorporating the translation of compound
terms in a translation model should result in additional improvements for CLIR. Our
preliminary experiments (Nie and Dufort 2002) on integrating the translation of com-
pounds certainly showed this, with improvement of up to 70% over a word-based
approach. This direction warrants further investigation.
Finally, all our efforts thus far to mine parallel Web pages have involved English.
How can we deal with CLIR between, say, Chinese and German, for which there are
few parallel Web sites? One possible solution would be to use English as a pivot
language, even though the two-step translation involved would certainly reduce ac-
curacy and introduce more noise. Nevertheless, several authors have shown that a
pivot approach can still produce effective retrieval and can at least complement a
dictionary-based approach (Franz, McCarley, and Ward 2000; Gollins and Sanderson
2001; Lehtokangas and Airio 2002).
Acknowledgments
This work was partly funded by a research
grant from the Dutch Telematics Institute:
DRUID project. We would like to thank
Xerox Research Center Europe (XRCE) for
making its Xelda toolkit available to us. We
would also like to thank George Foster for
making his statistical MT toolkit available
and for many interesting discussions.
Special thanks are due to Jiang Chen, who
contributed to the building of PTMiner.
Finally, we want to thank Elliott
Macklovitch and the two anonymous
reviewers for their constructive comments
and careful review. Part of this work was
carried out while the first author was
visiting the RALI laboratory at Universite?
de Montre?al.
References
Allen, James, editor. 2002. Event-Based
Information Organization. Kluwer
Academic, Boston.
Baum, L. E. 1972. An inequality and
associated maximization technique in
statistical estimations of probabilistic
functions of Markov processes.
Inequalities, 3:1?8.
Berger, Adam and John Lafferty. 2000. The
Weaver system for document retrieval. In
Ellen M. Voorhees and Donna K. Harman,
editors, The Eighth Text Retrieval Conference
(TREC-8), volume 8. National Institute of
Standards and Technology Special
Publication 500-246, Gaithersburg, MD.
Braschler, Martin and Ba?rbel Ripplinger.
2003. Stemming and decompounding for
German text retrieval. In Fabrizio
Sebastiani, editor, Advances in Information
Retrieval: 25th European Conference on IR
Research (ECIR 2003), Pisa, Italy, April 2003,
Proceedings. Lecture Notes in Computer
Science 2633. Springer, Berlin.
Broglio, John, James P. Callan, W. Bruce
Croft, and Daniel W. Nachbar. 1995.
Document retrieval and routing using the
INQUERY system. In Donna K. Harman,
editor, The Third Text Retrieval Conference,
volume 4. National Institute of Standards
and Technology Special Publication
500-236, Gaithersburg, MD, pages 29?38.
Brown, Peter F., John Cocke, Stephen
A. Della Pietra, Vincent J. Della Pietra,
Fredrick Jelinek, John D. Lafferty,
Robert L. Mercer, and Paul S. Roossin.
1990. A statistical approach to machine
translation. Computational Linguistics,
16(2):79?85.
Chen, Jiang and Jian-Yun Nie. 2000. Web
parallel text mining for Chinese-English
cross-language information retrieval. In
Proceedings of NAACL-ANLP, Seattle.
Clark, James. 2001. SP?An SGML System
Conforming to International Standard ISO
8879?Standard Generalized Markup
Language. Available at
?http://www.jclark.com/sp/?.
Conover, William Jay. 1980. Practical
Nonparametric Statistics. Wiley, London.
Croft, W. Bruce, Alistair Moffat,
C. J. ?Keith? van Rijsbergen, Ross
Wilkinson, and Justin Zobel, editors. 1998.
Proceedings of the 21st Annual International
ACM SIGIR Conference on Research and
Development in Information Retrieval (SIGIR
?98). ACM Press.
Dumais, Susan T., Todd A. Letsche,
Michael L. Littman, and Thomas K.
Landauer. 1997. Automatic cross-language
retrieval using latent semantic indexing.
In AAAI Spring Symposium on
416
Computational Linguistics Volume 29, Number 3
Cross-Language Text and Speech Retrieval,
Palo Alto, CA.
Federico, Marcello and Nicola Bertoldi.
2002. Statistical cross-language
information retrieval using N-best query
translations. In Micheline Beaulieu,
Ricardo Baeza-Yates, Sung Hyon Myaeng,
and Kalervo Ja?rvelin, editors, Proceedings
of the 25th Annual International ACM SIGIR
Conference on Research and Development in
Information Retrieval (SIGIR 2002). ACM
Press, New York.
Foster, George F. 1991. Statistical lexical
disambiguation. Master?s thesis, McGill
University, School of Computer Science.
Foster, George. 2000. A maximum
entropy/minimum divergence translation
model. In Proceedings of the 38th Annual
Meeting of the Association for Computational
Linguistics (ACL), Hong Kong.
Franz, Martin, J. Scott McCarley, and
R. Todd Ward. 2000. Ad hoc,
cross-language and spoken document
retrieval at IBM. In Ellen M. Voorhees and
Donna K. Harman, editors, The Eighth Text
Retrieval Conference (TREC-8), volume 8.
National Institute of Standards and
Technology Special Publication 500-246,
Gaithersburg, MD.
Franz, Martin, J. Scott McCarley, Todd Ward,
and Wei-Jing Zhu. 2001. Quantifying the
utility of parallel corpora. In W. Bruce
Croft, David J. Harper, Donald H. Kraft,
and Justin Zobel, editors, Proceedings of the
24th Annual International ACM SIGIR
Conference on Research and Development in
Information Retrieval (SIGIR 2001). ACM
Press, New York.
Gao, Jianfeng, Jian-Yun Nie, Endong Xun,
Jian Zhang, Ming Zhou, and Changning
Huang. 2001. Improving query translation
for cross-language information retrieval
using statistical models. In W. Bruce
Croft, David J. Harper, Donald H. Kraft,
and Justin Zobel, editors, Proceedings of the
24th Annual International ACM SIGIR
Conference on Research and Development in
Information Retrieval (SIGIR 2001). ACM
Press, New York.
Gollins, Tim and Mark Sanderson. 2001.
Improving cross language retrieval with
triangulated translation. In W. Bruce
Croft, David J. Harper, Donald H. Kraft,
and Justin Zobel, editors, Proceedings of the
24th Annual International ACM SIGIR
Conference on Research and Development in
Information Retrieval (SIGIR 2001). ACM
Press, New York.
Grefenstette, Gregory. 1998. The problem of
cross-language information retrieval. In
Gregory Grefenstette, editor,
Cross-Language Information Retrieval.
Kluwer Academic, Boston, pages 1?9.
Harman, Donna K., editor. 1995. The Third
Text Retrieval Conference (TREC-3),
volume 4. National Institute of Standards
and Technology Special Publication
500-236.
Hearst, Marti, Fred Gey, and Richard Tong,
editors. 1999. Proceedings of the 22nd
Annual International ACM SIGIR Conference
on Research and Development in Information
Retrieval (SIGIR ?99). ACM Press.
Hiemstra, Djoerd. 1998. A linguistically
motivated probabilistic model of
information retrieval. In Christos
Nicolaou and Constantine Stephanides,
editors, Research and Advanced Technology
for Digital Libraries?Second European
Conference (ECDL?98), Proceedings. Lecture
Notes in Computer Science 1513. Springer
Verlag, Berlin.
Hiemstra, Djoerd. 2001. Using Language
Models for Information Retrieval. Ph.D.
thesis, University of Twente, Enschede,
the Netherlands.
Hiemstra, Djoerd and Franciska de Jong.
1999. Disambiguation strategies for
cross-language information retrieval. In
European Conference on Digital Libraries,
pages 274?293.
Hiemstra, Djoerd and Wessel Kraaij. 1999.
Twenty-one at TREC-7: Ad hoc and cross
language track. In Ellen M. Voorhees and
Donna K. Harman, editors, The Seventh
Text Retrieval Conference (TREC-7),
volume 7. National Institute of Standards
and Technology Special Publication
500-242, Gaithersburg, MD.
Hiemstra, Djoerd, Wessel Kraaij, Rene?e
Pohlmann, and Thijs Westerveld. 2001.
Translation resources, merging strategies
and relevance feedback. In Carol Peters,
editor, Cross-Language Information Retrieval
and Evaluation. Lecture Notes in Computer
Science 2069. Springer Verlag, Berlin.
Hull, David. 1993. Using statistical testing
in the evaluation of retrieval experiments.
In Robert Korfhage, Edie Rasmussen, and
Peter Willett, editors, Proceedings of the
16th Annual International ACM SIGIR
Conference on Research and Development in
Information Retrieval (SIGIR ?93), pages
329?338. ACM Press, New York.
Hull, David. 1996. Stemming algorithms?a
case study for detailed evaluation. Journal
of the American Society for Information
Science, 47(1): 47?84.
Hull, David. 1997. Using structured queries
for disambiguation in cross-language
information retrieval. In David Hull and
Douglas Oard, editors, AAAI Symposium
417
Kraaij, Nie, and Simard Embedding Web-Based Statistical Models in CLIR
on Cross-Language Text and Speech Retrieval.
American Association for Artificial
Intelligence. Available at
?http://www.aaai.org/Press/Reports/
Symposia/Spring/ss-97-05.html?.
Hull, David and Gregory Grefenstette. 1996.
Querying across languages: A
dictionary-based approach to multilingual
information retrieval. In Hans-Peter Frei,
Donna Harman, Peter Scha?uble, and Ross
Wilkinson, editors, Proceedings of the
19th Annual International ACM SIGIR
Conference on Research and Development
in Information Retrieval (SIGIR ?96).
ACM Press, New York, pages 49?57.
Hull, David, Paul B. Kantor, and
Kwong Bor Ng. 1999. Advanced
approaches to the statistical analysis of
TREC information retrieval experiments.
Unpublished report, Rutgers University,
New Brunswick, NJ.
Ide, Nancy, G. Priest-Dorman and Jean
Ve?ronis. 1995. Corpus encoding standard.
Available at ?http://www.cs.vassar.edu/
CES/?.
Isabelle, Pierre, Michel Simard, and Pierre
Plamondon. 1998. Demo. Available at
?http://www.rali.iro.umontreal.ca/
SILC/SILC.en.cgi?.
Jansen, Bernard J., Amanda Spink, Deitmar
Wolfram, and Tefko Saracevic. 2001.
Searching the Web: The public and their
queries. Journal of the American Society for
Information Science and Technology,
53(3):226?234.
Kraaij, Wessel. 2002. TNO at CLEF-2001:
Comparing translation resources. In Carol
Peters, Martin Braschler, Julio Gonzalo,
and Michael Kluck, editors, Evaluation of
Cross-Language Information Retrieval
Systems: Second Workshop of the
Cross-Language Evaluation Forum (CLEF
2001). Springer Verlag, Berlin.
Kraaij, Wessel and Rene?e Pohlmann. 1996.
Viewing stemming as recall enhancement.
In Hans-Peter Frei, Donna Harman, Peter
Scha?uble, and Ross Wilkinson, editors,
Proceedings of the 19th Annual
International ACM SIGIR Conference on
Research and Development in
Information Retrieval (SIGIR ?96). ACM
Press, New York, pages 40?48.
Kraaij, Wessel, Rene?e Pohlmann, and Djoerd
Hiemstra. 2000. Twenty-one at TREC-8:
Using language technology for
information retrieval. In Ellen M.
Voorhees and Donna K. Harman, editors,
The Eighth Text Retrieval Conference
(TREC-8), volume 8. National Institute of
Standards and Technology Special
Publication 500-246, Gaithersburg, MD.
Kraaij, Wessel and Martijn Spitters. 2003.
Language models for topic tracking. In
Bruce Croft and John Lafferty, editors,
Language Models for Information Retrieval.
Kluwer Academic, Boston.
Kraaij, Wessel, Thijs Westerveld, and Djoerd
Hiemstra. 2002. The importance of prior
probabilities for entry page search. In
Micheline Beaulieu, Ricardo Baeza-Yates,
Sung Hyon Myaeng, and Kalervo
Ja?rvelin, editors, Proceedings of the 25th
Annual International ACM SIGIR Conference
on Research and Development in Information
Retrieval (SIGIR 2002). ACM Press, New
York.
Kwok, K. L. 1999. English-Chinese
cross-language retrieval based on a
translation package. In Workshop: Machine
Translation for Cross Language Information
Retrieval, Singapore, Machine Translation
Summit VII, pages 8?13.
Lafferty, John and Chengxiang Zhai. 2001a.
Document language models, query
models, and risk minimization for
information retrieval. In W. Bruce Croft,
David J. Harper, Donald H. Kraft, and
Justin Zobel, editors, Proceedings of the 24th
Annual International ACM SIGIR Conference
on Research and Development in Information
Retrieval (SIGIR 2001). ACM Press, New
York.
Lafferty, John and Chengxiang Zhai. 2001b.
Probabilistic IR models based on
document and query generation. In Jamie
Callan, Bruce Croft, and John Lafferty,
editors, Proceedings of the Workshop on
Language Modeling and Information
Retrieval, Pittsburgh.
Laffling, John. 1992. On constructing a
transfer dictionary for man and machine.
Target, 4(1):17?31.
Lavrenko, Victor, Martin Choquette, and W.
Bruce Croft. 2002. Cross-lingual relevance
models. In Micheline Beaulieu, Ricardo
Baeza-Yates, Sung Hyon Myaeng, and
Kalervo Ja?rvelin, editors, Proceedings of the
25th Annual International ACM SIGIR
Conference on Research and Development in
Information Retrieval (SIGIR 2002). ACM
Press, New York.
Lavrenko, Victor and W. Bruce Croft. 2001.
Relevance-based language models. In W.
Bruce Croft, David J. Harper, Donald H.
Kraft, and Justin Zobel, editors,
Proceedings of the 24th Annual International
ACM SIGIR Conference on Research and
Development in Information Retrieval (SIGIR
2001). ACM Press, New York.
Lehtokangas, Raija and Eija Airio. 2002.
Translation via a pivot language
challenges direct translation in CLIR. In
418
Computational Linguistics Volume 29, Number 3
Proceedings of the SIGIR 2002 Workshop:
Cross-Language Information Retrieval: A
Research Roadmap, Tampere, Finland.
Ma, Xiaoyi. 1999. Parallel text collections at
the Linguistic Data Consortium. In
Machine Translation Summit VII, Singapore.
McNamee, Paul and James Mayfield. 2001.
A language-independent approach to
European text retrieval. In Carol Peters,
editor, Cross-Language Information Retrieval
and Evaluation. Lecture Notes in Computer
Science 2069. Springer Verlag, Berlin.
McNamee, Paul and James Mayfield. 2002.
Comparing cross-language query
expansion techniques by degrading
translation reources. In Micheline
Beaulieu, Ricardo Baeza-Yates, Sung Hyon
Myaeng, and Kalervo Ja?rvelin, editors,
Proceedings of the 25th Annual International
ACM SIGIR Conference on Research and
Development in Information Retrieval (SIGIR
2002). ACM Press, New York.
Miller, David R. H., Tim Leek, and
Richard M. Schwartz. 1999. A hidden
Markov model information retrieval
system. In Marti Hearst, Fred Gey, and
Richard Tong, editors, Proceedings of the
22nd Annual International ACM SIGIR
Conference on Research and Development
in Information Retrieval (SIGIR ?99).
ACM Press, New York, pages 214?221.
Ng, Kenney. 2000. A maximum likelihood
ratio information retrieval model. In Ellen
M. Voorhees and Donna K. Harman,
editors, The Eighth Text Retrieval Conference
(TREC-8), volume 8. National Institute of
Standards and Technology Special
Publication 500-246, Gaithersburg, MD.
Nie, Jian-Yun. 2002. Query expansion and
query translation as logical inference.
Journal of the American Society for
Information Science and Technology, 54(4):
340?351.
Nie, Jian-Yun and Jian Cai. 2001. Filtering
noisy parallel corpora of Web pages. In
IEEE Symposium on NLP and Knowledge
Engineering, Tucson, AZ, pages 453?458.
Nie, Jian-Yun and Jean-Franc?ois Dufort.
2002. Combining words and compound
terms for monolingual and cross-language
information retrieval. In Proceedings of
Information 2002, Beijing, pages 453?458.
Nie, Jian-Yun, Michel Simard, Pierre
Isabelle, and Richard Durand. 1999.
Cross-language information retrieval
based on parallel texts and automatic
mining of parallel texts from the Web. In
Marti Hearst, Fred Gey, and Richard
Tong, editors, Proceedings of the 22nd
Annual International ACM SIGIR
Conference on Research and Development
in Information Retrieval (SIGIR ?99).
ACM Press, New York, pages 74?81.
Pirkola, Ari. 1998. The effects of query
structure and dictionary setups in
dictionary-based cross-language
information retrieval. In W. Bruce Croft,
Alistair Moffat, C. J. ?Keith? van
Rijsbergen, Ross Wilkinson, and Justin
Zobel, editors, Proceedings of the 21st
Annual International ACM SIGIR Conference
on Research and Development in Information
Retrieval (SIGIR ?98). ACM Press, New
York, pages 55?63.
Ponte, Jay M. and W. Bruce Croft. 1998. A
language modeling approach to
information retrieval. In W. Bruce Croft,
Alistair Moffat, C. J. ?Keith? van
Rijsbergen, Ross Wilkinson, and Justin
Zobel, editors, Proceedings of the 21st
Annual International ACM SIGIR Conference
on Research and Development in Information
Retrieval (SIGIR ?98). ACM Press, New
York, pages 275?281.
Porter, Martin F. 1980. An algorithm for
suffix stripping. Program, 14(3):130?137.
Ragget, Dave. 1998. Clean up your Web
pages with HTML TIDY. Available at
?http://www.w3.org/People/
Raggett/tidy/?.
Resnik, Philip. 1998. Parallel stands: A
preliminary investigation into mining the
Web for bilingual text. In Proceedings of
AMTA. Lecture Notes in Computer
Science 1529. Springer, Berlin.
Robertson, Stephen E. and Steve Walker.
1994. Some simple effective
approximations to the 2-Poisson model
for probabilistic weighted retrieval. In W.
Bruce Croft and C. J. ?Keith? van
Rijsbergen, editors, Proceedings of the 17th
Annual International ACM SIGIR Conference
on Research and Development in Information
Retrieval (SIGIR ?94), pages 232?241. ACM
Press, New York.
Salton, G. and M. J. McGill. 1983.
Introduction to Modern Information Retrieval.
McGraw-Hill, New York.
Savoy, Jacques. 2002. Report on CLEF-2001
experiments. In Carol Peters, Martin
Braschler, Julio Gonzalo, and Michael
Kluck, editors, Evaluation of Cross-Language
Information Retrieval Systems: Second
Workshop of the Cross-Language Evaluation
Forum (CLEF 2001). Springer Verlag,
Berlin.
Sheridan, Paraic, Jean Paul Ballerini, and
Peter Scha?uble. 1998. Building a large
multilingual text collection from
comparable news documents. In Gregory
Grefenstette, editor, Cross-Language
Information Retrieval. Kluwer Academic,
419
Kraaij, Nie, and Simard Embedding Web-Based Statistical Models in CLIR
pages 137?150.
Simard, Michel, George Foster, and Pierre
Isabelle. 1992. Using Cognates to Align
Sentences in Bilingual Corpora. In
Proceedings of the Fourth Conference on
Theoretical and Methodological Issues in
Machine Translation (TMI), pages 67?82,
Montre?al, Que?bec.
Spitters, Martijn and Wessel Kraaij. 2001.
Using language models for tracking
events of interest over time. In Proceedings
of the Workshop on Language Models for
Information Retrieval (LMIR2001),
Pittsburgh.
Tague-Sutcliffe, Jean and James Blustein.
1995. A statistical analysis of the TREC-3
data. In Donna K. Harman, editor, The
Third Text Retrieval Conference, volume 4.
National Institute of Standards and
Technology Special Publication 500-236,
Gaithersburg, MD, pages 385?398.
Ve?ronis, Jean, editor. 2000. Parallel Text
Processing. Kluwer Academic, Dordrecht,
the Netherlands.
Voorhees, Ellen M. 1998. Variations in
relevance judgements and the
measurement of retrieval effectiveness. In
W. Bruce Croft, Alistair Moffat, C. J.
?Keith? van Rijsbergen, Ross Wilkinson,
and Justin Zobel, editors, Proceedings of the
21st Annual International ACM SIGIR
Conference on Research and Development in
Information Retrieval (SIGIR ?98). ACM
Press, New York. pages 315?323.
Xu, Jinxi, Ralph Weischedel, and Chanh
Nguyen. 2001. Evaluating a probabilistic
model for cross-lingual information
retrieval. In W. Bruce Croft, David J.
Harper, Donald H. Kraft, and Justin
Zobel, editors, Proceedings of the 24th
Annual International ACM SIGIR Conference
on Research and Development in Information
Retrieval (SIGIR 2001). ACM Press, New
York.
Yang, Yiming, Jaime G. Carbonell, Ralph
Brown, and Robert E. Frederking. 1998.
Translingual information retrieval:
Learning from bilingual corpora. Artificial
Intelligence Journal, 103(1?2):323?345.
Zhai, ChengXiang and John Lafferty. 2002.
Two-stage language models for
information retrieval. In Micheline
Beaulieu, Ricardo Baeza-Yates, Sung Hyon
Myaeng, and Kalervo Ja?rvelin, editors,
Proceedings of the 25th Annual International
ACM SIGIR Conference on Research and
Development in Information Retrieval (SIGIR
2002). ACM Press, New York.
Statistical Translation Alignment
with Compositionality Constraints
Michel Simard and Philippe Langlais
Laboratoire de recherche applique?e en linguistique informatique (RALI)
De?partement d?informatique et de recherche ope?rationnelle
Universite? de Montre?al
C.P. 6128, succursale Centre-ville, Local 2241
Montre?al (Que?bec), Canada H3C 3J7
{simardm,felipe}@iro.umontreal.ca
Abstract
This article presents a method for aligning
words between translations, that imposes a
compositionality constraint on alignments pro-
duced with statistical translation models. Ex-
periments conducted within the WPT-03 shared
task on word alignment demonstrate the effec-
tiveness of the proposed approach.
1 Introduction
Since the pioneering work of the IBM machine trans-
lation team almost 15 years ago (Brown et al, 1990),
statistical methods have proven to be valuable tools in
approaching the automation of translation. Word align-
ments (WA) play a central role in the statistical modeling
process, and reliable WA techniques are crucial in acquir-
ing the parameters of the models (Och and Ney, 2000).
Yet, the very nature of these alignments, as defined in
the IBM modeling approach (Brown et al, 1993), lead
to descriptions of the correspondences between source-
language (SL) and target-language (TL) words of a trans-
lation that are often unsatisfactory, at least from a human
perspective.
One notion that is typically evacuated in the statisti-
cal modeling process is that of compositionality: a fun-
damental assumption in statistical machine translation is
that, ultimately, all the words of a SL segment S con-
tribute to produce all the words of its TL translation T , at
least to some degree. While this makes perfect sense from
a stochastic point of view, it contrasts with the hypothesis
at the basis of most (if not all) other MT approaches, as
well as with our natural intuitions about translation: that
individual portions of the SL text produce individual TL
portions autonomously, and that the final translation T is
obtained by somehow piecing together these TL portions.
In what follows, we show how re-integrating compo-
sitionality into the statistical translation word alignment
process leads to better alignments. We first take a closer
look at the ?standard? statistical WA techniques in section
2, and then propose a way of imposing a compositional-
ity constraint on these techniques in section 3. In section
4, we discuss various implementation issues, and finally
present the experimental results of this approach on the
WPT-03 shared task on WA in section 5.
2 Statistical Word Alignment
Brown et al (1993) define a word alignment as a vec-
tor a = a1...am that connects each word of a source-
language text S = s1...sm to a target-language word in
its translation T = t1...tn, with the interpretation that
word taj is the translation of word sj in S (aj = 0 is
used to denote words of s that do not produce anything in
T ).
The Viterbi alignment between source and target sen-
tences S and T is defined as the alignment a? whose prob-
ability is maximal under some translation model:
a? = argmaxa?APrM(a|S, T )
where A is the set of all possible alignments between S
and T , and PrM(a|S, T ) is the estimate of a?s probabil-
ity under model M, which we denote Pr(a|S, T ) from
hereon. In general, the size of A grows exponentially
with the sizes of S and T , and so there is no efficient way
of computing a? efficiently. However, under the indepen-
dence hypotheses of IBM Model 2, the Viterbi alignment
can be obtained by simply picking for each position i in
S, the alignment that maximizes t(si|tj)a(j, i,m, n), the
product of the model?s ?lexical? and ?alignment? proba-
bility estimates. This procedure can trivially be carried
out in O(mn) operations. Because of this convenient
property, we take the Viterbi-2 WA method (which we
later refer to as the V method) as the basis for the rest of
this work.
3 Compositionality
In IBM-style alignments, each SL token is connected to a
single (possibly null) TL token, typically the TL token
with which it has the most ?lexical affinities?, regard-
less of other existing connections in the alignment and,
more importantly, of the relationships it holds with other
SL tokens in its vicinity. In practice, this means that
some TL tokens can end up being connected to several
SL tokens, while other TL tokens are left unconnected.
This contrasts with alternative alignment models such as
those of Melamed (1998) and Wu (1997), which impose a
?one-to-one? constraint on alignments. Such a constraint
evokes the notion of compositionality in translation: it
suggests that each SL token operates independently in the
SL sentence to produce a single TL token in the TL sen-
tence, which then depends on no other SL token.
This view is, of course, extreme, and real-life transla-
tions are full of examples that show how this composi-
tionality principle breaks down as we approach the level
of word correspondences. Yet, if we can find a way of
imposing compositionality constraints on WA?s, at least
to the level where it applies, then we should obtain more
sensible results than with Viterbi alignments.
For instance, consider a procedure that splits both the
SL and TL sentences S and T into two independent parts,
in such a way as to maximise the probability of the two
resulting Viterbi alignments:
argmax?i,j,d?
?
???
???
d = 1 : Pr(a1|si1, t
j
1)
?Pr(a2|smi+1, t
n
j+1)
d = ?1 : Pr(a1|si1, t
n
j+1)
?Pr(a2|smi+1, t
j
1)
(1)
In the triple ?i, j, d? above, i represents a ?split point?
in the SL sentence S, j is the analog for TL sentence T ,
and d is the ?direction of correspondence?: d = 1 denotes
a ?parallel correspondence?, i.e. s1...si corresponds to
t1...tj and si+1...sm corresponds to tj+1...tn; d = ?1
denotes a ?crossing correspondence?, i.e. s1...si corre-
sponds to tj+1...tn and si+1...sm corresponds to t1...tj .
The triple ?I, J,D? produced by this procedure refers
to the most probable alignment between S and T , un-
der the hypothesis that both sentences are made up of
two independent parts (s1...sI and sI+1...sm on the one
hand, t1...tJ and tJ+1...tn on the other), that correspond
to each other two-by-two, following direction D. Such
an alignment suggests that translation T was obtained
by ?composing? the translation of s1...sI with that of
sI+1...sm.
In the above procedure, these ?composing parts? of
S and T are further assumed to be contiguous sub-
sequences of words. Once again, real-life translations are
full of examples that contradict this (negations in French
and particle verbs in German are two examples that im-
mediately spring to mind when aligning with English).
Yet, this contiguity assumption turns out to be very con-
venient, because examining pairings of non-contiguous
sequences would quickly become intractable. In con-
trast, the procedure above can find the optimal partition
in polynomial time.
The ?splitting? process described above can be re-
peated recursively on each pair of matching segments,
down to the point where the SL segment contains a sin-
gle token. (TL segments can always be split, even when
empty, because IBM-style alignments allow connecting
SL tokens to the ?null? TL token, which is always avail-
able.) This recursive procedure actually produces two
different outputs:
1. A parallel partition of S and T into m pairs of seg-
ments ?si, tkj ?, where each tkj is a (possibly null)
contiguous sub-sequence of T ; this partition can of
course be viewed as an alignment on the words of S
and T .
2. an IBM-style alignment, such that each SL and TL
token is linked to at most one token in the other lan-
guage: this alignment is actually the concatenation
of individual Viterbi alignments on the ?si, tkj ? pairs,
which connects each si to (at most) one of the tokens
in the corresponding tkj .
In this procedure, which we call Compositional WA (or
C for short), there are at least two problems. First, each
SL token finds itself ?isolated? in its own partition bin,
which makes it impossible to account for multiple SL to-
kens acting together to produce a TL sequence. Second,
the TL tokens that are not connected in the resulting IBM-
style alignment do not play any role in the computation
of the probability of the optimal alignment; therefore, the
pair ?si, tkj ? in which these ?superfluous? tokens end up
is more or less random.
To compensate in part for these, we propose using
two IBM-2 models to compute the optimal partition: the
?forward? (SL?TL) model, and the ?reverse? (TL?SL)
model. When examining a particular split ?i, j, d? for S
and T , we compute both Viterbi alignments, forward and
reverse, between all pairs of segments, and score each
pair with the product of the two alignments? probabili-
ties.
In this variant, which we call Combined Compositional
WA (CC), we can no longer allow ?empty? segments in
the TL, and so we stop the recursion as soon as either the
SL or TL segment contains a single token. The resulting
partition therefore consists in a series of 1-to-k or k-to-1
alignments, with k ? 1.
4 Implementation
The C and CC WA methods of section 3 were imple-
mented in a program called ralign (Recursive ? or RALI
? alignment, as you wish). As suggested above, this pro-
gram takes as input a pair of sentence-aligned texts, and
the parameters of two IBM-2 models (forward and re-
verse), and outputs WA?s for the given texts. This pro-
gram also implements plain Viterbi alignments, using the
forward (V) or reverse (RV) models, as well as what we
call the Reverse compositional WA (or RC), which is just
the C method using the reverse IBM-2 model.
The output format proposed for the WPT-03 shared
task on WA allowed participants to distinguish between
?sure? (S) and ?probable? (P) WA?s. We figured that our
alignment procedure implicitly incorporated a way of dis-
tinguishing between the two: within each produced pair
of segments, we marked as ?sure? all WA?s that were pre-
dicted by both (forward and reverse) Viterbi alignments,
and as ?probable? all the others.
The translation models for ralign were trained using
the programs of the EGYPT statistical translation toolkit
(Al-Onaizan et al, 1999). This training was done using
the data provided as part of the WPT-03 shared task on
WA (table 1). We thus produced two sets of models, one
for English and French (en-fr), and one for Romanian
and English (ro-en). All models were trained on both the
training and test datasets1. For en-fr, we considered all
words that appeared only once in the corpus to be ?un-
known words? (whittle option -f 2), so as to obtain de-
fault values of ?real? unknowns in the test corpus2. In the
case of ro-en, there was too little training data for this to
be beneficial, and so we chose to use all words.
English-French
corpus tokens (SL/TL) sentence pairs
training 20M/24M 1M
trial 772/832 37
test 8K/9K 447
Romanian-English
corpus tokens (SL/TL) sentence pairs
training 1M/1M 48K
trial 513/547 17
test 6K/6K 248
Table 1: WPT-03 shared task resources
We trained and tested a number of translation mod-
els before settling for this particular setup. All of these
1No cheating here: the test dataset did not contain reference
alignments
2This is necessary, even when training on the test corpus,
because the EGYPT toolkit?s training program (GIZA) ignores
excessively long sentences in the corpus.
tests were performed using the trial data provided for the
WPT-03 shared task.
5 Experimental Results
The different word-alignment methods described in sec-
tions 2 and 3 were run on the test corpora of the WPT-
03 shared task on alignment. Results were evaluated in
terms of alignment precision (P), recall (R), F-measure
and alignment error rate (AER) (Och and Ney, 2000). As
specified in the shared task description, all of these met-
rics were computed taking null-alignments into account
(i.e. tokens left unconnected in an alignment were actu-
ally counted as aligned to virtual word token ?0?). The
results of our experiments are reproduced in table 2.
We observe that imposing a ?contiguous composition-
ality? constraint (C and RC methods) allows for sub-
stantial gains with regard to plain Viterbi alignments (V
and RV respectively), especially in terms of precision
and AER (a slight decline in recall can be observed be-
tween the V and C methods on the ro-en corpus, but it
is not clear whether this is significant). These gains are
even more interesting when one considers that all pairs of
alignments (V and C, RV and RC) are obtained using ex-
actly the same data. This highlights both the deficiencies
of IBM Model-2 and the importance of compositionality.
Using both the forward and reverse models (CC) yields
yet more gains with regard to all metrics. This result is
interesting, because it shows the potential of the compo-
sitional alignment method for integrating various sources
of information.
With regard to language pairs, it is interesting to note
that all alignment methods produce figures that are sub-
stantially better in recall and worse in precision on the ro-
en data, compared to en-fr. Overall, ro-en alignments dis-
play significantly higher F-measures. This is surprising,
considering that the provided en-fr corpus contained 20
times more training material. This phenomenon is likely
due to the fact that the en-fr test reference contains much
more alignments per word (1.98 per target word) than the
ro-en (1.12). All alignment methods described here pro-
duce roughly between 1 and 1.25 alignments per target
words. This fact affects recall and F-measure figures pos-
itively on the ro-en test, while precision and AER (which
correlates strongly with precision in practice) are affected
inversely.
6 Conclusion
In this article, we showed how a compositionality con-
straint could be imposed when computing word align-
ments with IBM Models-2. Our experiments on the WPT-
03 shared task on WA demonstrated how this improves
the quality of resulting alignments, when compared to
standard Viterbi alignments. Our results also highlight
English-French Romanian-English
method P R F AER
V 0.6610 0.3387 0.4479 0.2700
RV 0.6260 0.3212 0.4245 0.2944
C 0.7248 0.3534 0.4751 0.2318
RC 0.7422 0.3586 0.4835 0.2152
CC 0.7756 0.3681 0.4992 0.1850
method P R F AER
V 0.5509 0.5442 0.5475 0.4524
RV 0.5409 0.5375 0.5391 0.4608
C 0.5818 0.5394 0.5597 0.4402
RC 0.5865 0.5415 0.5630 0.4369
CC 0.6361 0.5714 0.6020 0.3980
Table 2: Alignment results
the benefit of using both forward and reverse translation
models for this task.
One of the weaknesses of the proposed method is the
inability to produce many-to-many alignments. To allow
for such alignments, it would be necessary to establish a
?stopping condition? on the recursion process, so as to
prevent partitioning pairs of segments that display ?non-
compositional? phenomena in both SL and TL languages.
We have begun experimenting with various such mecha-
nisms. One of these is to stop the recursion as soon as the
pair of segments under consideration contains less than
two ?sure? alignments, i.e. connections predicted by both
the forward and reverse models. Another possibility is to
establish a threshold on the probability ?drop? incurred
by the optimal split on any given pair of segments. So
far, these experiments are inconclusive.
Another problem is with ?null? alignments, which the
program is also unable to account for. Currently, omis-
sions and insertions in translation find themselves incor-
porated into aligned segments. A simple way to deal with
this problem would be to exclude from the final alignment
links that are not predicted by either the forward or re-
verse Viterbi alignments. But early experiments with this
approach are unconvincing, and more elaborate filtering
mechanisms will probably be necessary.
Finally, IBM Model 2 is certainly not the state of the
art in statistical translation modeling. Thenagain, the
methods proposed here are not dependent on the underly-
ing translation model, and similar WA methods could be
based on more elaborate models, such as Models 3?5, or
the HMM-based models proposed by Och et al (1999)
for example. On the other hand, our compositional align-
ment method could be used during the training process
of higher-level models. Whether this would lead to better
estimates of the models? parameters remains to be seen,
but it is certainly a direction worth exploring.
References
[Al-Onaizan et al1999] Yaser Al-Onaizan, Jan Curin,
Michael Jahr, Kevin Knight, John Lafferty, Dan
Melamed, Franz-Josef Och, David Purdy, Noah H.
Smith, and David Yarowsky. 1999. Statistical Ma-
chine Translation - Final Report, JHU Workshop 1999.
Technical report, Johns Hopkins University.
[Brown et al1990] Peter F. Brown, John Cocke, Stephen
A. Della Pietra, Vincent J. Della Pietra, Fredrick Je-
linek, John D. Lafferty, Robert L. Mercer, and Paul S.
Roossin. 1990. A Statistical Approach to Machine
Translation. Computational Linguistics, 16(2):79?85,
June.
[Brown et al1993] Peter F. Brown, Stephen A. Della
Pietra, Vincent J. Della Pietra, and Robert L. Mer-
cer. 1993. The Mathematics of Machine Transla-
tion: Parameter Estimation. Computational Linguis-
tics, 19(2):263?311.
[Melamed1998] I. Dan Melamed. 1998. Word-to-Word
Models of Translational Equivalence. Technical Re-
port 98-08, Dept. of Computer and Information Sci-
ence, University of Pennsylvania, Philadelphia, USA.
[Och and Ney2000] Franz Josef Och and Hermann Ney.
2000. Improved statistical alignment models. In Pro-
ceedings of the 38th Annual Meeting of the Association
for Computational Linguistics (ACL), pages 440?447,
Hong-Kong, China, October.
[Och et al1999] Franz Josef Och, Christoph Tillmann,
and Hermann Ney. 1999. Improved Alignment Mod-
els for Statistical Machine Translation. In Proceedings
of the 4th Conference on Empirical Methods in Natu-
ral Language Processing (EMNLP)and 7th ACL Work-
shop on Very Large Corpora (WVLC), pages 20?28,
College Park, USA.
[Wu1997] Dekai Wu. 1997. Stochastic Inversion Trans-
duction Grammars and Bilingual Parsing of Parallel
Corpora. Computational Linguistics, 23(3):377?404,
September.
Translation Spotting for Translation Memories
Michel Simard
Laboratoire de recherche applique?e en linguistique informatique (RALI)
De?partement d?informatique et de recherche ope?rationnelle
Universite? de Montre?al
C.P. 6128, succursale Centre-ville, Local 2241
Montre?al (Que?bec), Canada H3C 3J7
simardm@iro.umontreal.ca
Abstract
The term translation spotting (TS) refers to
the task of identifying the target-language (TL)
words that correspond to a given set of source-
language (SL) words in a pair of text segments
known to be mutual translations. This arti-
cle examines this task within the context of a
sub-sentential translation-memory system, i.e.
a translation support tool capable of proposing
translations for portions of a SL sentence, ex-
tracted from an archive of existing translations.
Different methods are proposed, based on a sta-
tistical translation model. These methods take
advantage of certain characteristics of the ap-
plication, to produce TL segments submitted
to constraints of contiguity and composition-
ality. Experiments show that imposing these
constraints allows important gains in accuracy,
with regard to the most probable alignments
predicted by the model.
1 Introduction
Translation spotting is the term coined by Ve?ronis and
Langlais (2000) for the task of identifying the word-
tokens in a target-language (TL) translation that corre-
spond to some given word-tokens in a source-language
(SL) text. Translation spotting (TS) takes as input a cou-
ple, i.e. a pair of SL and TL text segments, which are
known to be translations of one another, and a SL query,
i.e. a subset of the tokens of the SL segment, on which the
TS will focus its attention. The result of the TS process
consists of two sets of tokens, i.e. one for each language.
We call these sets the SL and TL answers to the query.
In more formal terms:
? The input to the TS process is a pair of SL and TL
text segments ?S, T ?, and a contiguous, non-empty
sequence of word-tokens in S, q = si1 ...si2 (the
query).
? The output is a pair of sets of tokens ?rq(S), rq(T )?,
the SL answer and TL answer respectively.
Figure 1 shows some examples of TS, where the words
in italics represent the SL query, and the words in bold are
the SL and TL answers.
As can be seen in these examples, the tokens in the
query q and answers rq(S) and rq(T ) may or may not be
contiguous (examples 2 and 3), and the TL answer may
possibly be empty (example 4) when there is no satisfying
way of linking TL tokens to the query.
Translation spotting finds different applications, for
example in bilingual concordancers, such as the
TransSearch system (Macklovitch et al, 2000), and
example-based machine translation (Brown, 1996). In
this article, we focus on a different application: a sub-
sentential translation memory. We describe this applica-
tion context in section 2, and discuss how TS fits in to this
type of system. We then propose in section 3 a series of
TS methods, specifically adapted to this application con-
text. In section 4, we present an empirical evaluation of
the proposed methods.
2 Sub-sentential Translation Memory
Systems
A translation memory system is a type of translation sup-
port tool whose purpose is to avoid the re-translation of
segments of text for which a translation has previously
been produced. Typically, these systems are integrated
to a word-processing environment. Every sentence that
the user translates within this environment is stored in a
database (the translation memory ? or TM). Whenever
the system encounters some new text that matches a sen-
tence in the TM, its translation is retrieved and proposed
to the translator for reuse.
Sentence Pair
Query SL (English) TL (French)
1. and a growing gap Is this our model of the future, regional
disparity and a growing gap between
rich and poor?
Est ce la` le mode`le que nous visons,
soit la disparite? re?gionale et un fosse? de
plus en plus large entre les riches et les
pauvres?
2. the government?s com-
mitment
The government?s commitment was
laid out in the 1994 white paper.
Le gouvernement a expose? ses en-
gagements dans le livre blanc de 1994.
3. close to [...] years I have been fortunate to have been trav-
elling for close to 40 years.
J?ai eu la chance de voyager pendant
pre`s de 40 ans .
4. to the extent that To the extent that the Canadian govern-
ment could be open, it has been so.
Le gouvernement canadien a e?te? aussi
ouvert qu?il le pouvait.
Figure 1: Translation spotting examples
As suggested in the above paragraph, existing systems
essentially operate at the level of sentences: the TM is
typically made up of pairs of sentences, and the system?s
proposals consist in translations of complete sentences.
Because the repetition of complete sentences is an ex-
tremely rare phenomenon in general language, this level
of resolution limits the usability of TM?s to very spe-
cific application domains ? most notably the translation
of revised or intrinsically repetitive documents. In light
of these limitations, some proposals have recently been
made regarding the possibility of building TM systems
that operate ?below? the sentence level, or sub-sentential
translation memories (SSTM) ? see for example (Lange?
et al, 1997; McTait et al, 1999).
Putting together this type of system raises the prob-
lem of automatically establishing correspondences be-
tween arbitrary sequences of words in the TM, or, in other
words, of ?spotting translations?. This process (transla-
tion spotting) can be viewed as a by-product of word-
alignment, i.e. the problem of establishing correspon-
dences between the words of a text and those of its trans-
lation: obviously, given a complete alignment between
the words of the SL and TL texts, we can extract only
that part of the alignment that concerns the TS query;
conversely, TS may be seen as a sub-task of the word-
alignment problem: a complete word-alignment can be
obtained by combining the results of a series of TS oper-
ations, covering the entirety of the SL text.
From the point of view of an SSTM application, the
TS mechanism should find the TL segments that are the
most likely to be useful to the translator in producing the
translation of a given SL sentence. In the end, the final
criterion by which a SSTM will be judged is profitability:
to what extent do the system?s proposals enable the user
to save time and/or effort in producing a new translation.
From that perspective, the two most important charac-
teristics of the TL answers are relevance, i.e. whether
or not the system?s TL proposals constitute valid trans-
lations for some part of the source sentence; and co-
herence, i.e. whether the proposed segments are well-
formed, at least from a syntactic point of view. As sug-
gested by McTait et al (1999), ?linguistically motivated?
sub-sentential entities are more likely than arbitrary se-
quences of words to lead to useful proposals for the user.
Planas (2000) proposes a fairly simple approach for an
SSTM: his system would operate on sequences of syntac-
tic chunks, as defined by Abney (1991). Both the contents
of the TM and the new text under consideration would
be segmented into chunks; sequences of chunks from the
new text would then be looked up verbatim in the TM;
the translation of the matched sequences would be pro-
posed to the user as partial translations of the current in-
put. Planas?s case for using sequences of chunks as the
unit of translation for SSTM?s is supported by the coher-
ence criterion above: chunks constitute ?natural? textual
units, which users should find easier to grasp and reuse
than arbitrary sequences.
The coherence criterion also supports the case for con-
tiguous TL proposals, i.e. proposals that take the form
of contiguous sequences of tokens from the TM, as op-
posed to discontiguous sets such as those of examples 2
and 3, in figure 1. This also makes intuitive sense from
the more general point of view of profitability: manually
?filling holes? within a discontiguous proposal is likely to
be time-consuming and counter-productive. On the other
hand, filling those holes automatically, as proposed for
example by Lange? et al and McTait et al, raises numer-
ous problems with regard to syntactic and semantic well-
formedness of the TL proposals. In theory, contiguous
sequences of token from the TM should not suffer from
such ills.
Finally, and perhaps more importantly, in a SSTM ap-
plication such as that proposed by Planas, there appears
to be statistical argument in favor of contiguous TL pro-
posals: the more frequent a contiguous SL sequences, the
more likely it is that its TL equivalent is also contiguous.
In other words, there appears to be a natural tendency
for frequently-occurring phrases and formulations to cor-
respond to like-structured sequences in other languages.
This will be discussed further in section 4. But clearly,
a TS mechanism intended for such a SSTM should take
advantage of this tendency.
3 TS Methods
In this section, we propose various TS methods, specif-
ically adapted to a SSTM application such as that pro-
posed by Planas (2000), i.e. one which takes as transla-
tion unit contiguous sequences of syntactic chunks.
3.1 Viterbi TS
As mentioned earlier, TS can be seen as a bi-product of
word-level alignments. Such alignments have been the
focus of much attention in recent years, especially in the
field of statistical translation modeling, where they play
an important role in the learning process.
For the purpose of statistical translation modeling,
Brown et al (1993) define an alignment as a vector a =
a1...am that connects each word of a source-language
text S = s1...sm to a target-language word in its transla-
tion T = t1...tn, with the interpretation that word taj is
the translation of word sj in S (aj = 0 is used to denote
words of s that do not produce anything in T ).
Brown et al also define the Viterbi alignment between
source and target sentences S and T as the alignment
a? whose probability is maximal under some translation
model:
a? = argmaxa?APrM(a|S, T )
where A is the set of all possible alignments between S
and T , and PrM(a|S, T ) is the estimate of a?s probabil-
ity under model M, which we denote Pr(a|S, T ) from
hereon. In general, the size of A grows exponentially
with the sizes of S and T , and so there is no efficient way
of computing a? efficiently. However, under Model 2, the
probability of an alignment a is given by:
Pr(a|S, T ) =
m?
i=1
Pr(ai|i,m, n) (1)
where
Pr(j|i,m, n) =
?(j, i,m, n)
?n
J=0 ?(J, i,m, n)
, (2)
and
?(j, i,m, n) = t(si|tj)a(j, i,m, n)
In this last equation, t(si|tj) is the model?s estimate of
the ?lexical? distribution p(si|tj), while a(j, i,m, n) es-
timates the ?alignment? distribution p(j|i,m, n). There-
fore, with this model, the Viterbi alignment can be ob-
tained by simply picking for each position i in S, the
alignment that maximizes t(si|tj)a(j, i,m, n). This pro-
cedure can trivially be carried out in O(mn) operations.
Because of this convenient property, we base the rest of
this work on this model.
Adapting this procedure to the TS task is straightfor-
ward: given the TS query q, produce as TL answer the
corresponding set of TL tokens in the Viterbi alignment:
rq(T ) = {ta?i1 , ..., ta?i2} (the SL answer is simply q it-
self). We call this method Viterbi TS: it corresponds to
the most likely alignment between the query q and TL
text T , given the probability estimates of the translation
model. If q contains I tokens, the Model 2 Viterbi TS
can be computed in O(In) operations. Figure 2 shows
an example of the result of this process.
query : the government ?s commitment
couple:
S = Let us see where
the government?s commit-
ment is really at in terms of
the farm community.
T = Voyons quel est le
ve?ritable engagement du
gouvernement envers la
communaute? agricole.
Viterbi alignment on query tokens:
the ? le
government ? gouvernement
?s ? du
commitment ? engagement
TL answer:
T = Voyons quel est le ve?ritable engagement du gou-
vernement envers la communaute? agricole.
Figure 2: Viterbi TS example
3.2 Post-processings
The tokens of the TL answer produced by Viterbi TS are
not necessarily contiguous in T which, as remarked ear-
lier, is problematic in a TM application. Various a poste-
riori processings on rq(T ) are possible to fix this; we list
here only the most obvious:
expansion : Take the minimum and maximum val-
ues in {a?i1 , ..., a?i2}, and produce the sequence
tmin ai ...tmax ai ; in other words, produce as TL an-
swer the smallest contiguous sequence in T that con-
tains all the tokens of rq(T ).
longest-sequence : Produce the subset of rq(T ) that
constitutes the longest contiguous sequence in T .
zero-tolerance : If the tokens in rq(T ) cannot be ar-
ranged in a contiguous sequence of T , then simply
discard the whole TL answer.
Figure 3 illustrates how these three strategies affect the
Viterbi TS of figure 2.
3.3 Contiguous TS
The various independence assumptions underpinning
IBM Model 2 often have negative effects on the result-
ing Viterbi alignments. In particular, this model assumes
rq(T ) = {le, engagement, du, gouvernement}
post-processing:
expansion : X(rq(T )) = le ve?ritable engagement du gouvernement
longest-sequence : L(rq(T )) = engagement du gouvernement
zero-tolerance : Z(rq(T )) = ?
Figure 3: Post-processings on Viterbi TS
that all connections within an alignment are indepen-
dent of each other, which leads to numerous aberrations
in the alignments. Typically, each SL token gets con-
nected to the TL token with which it has the most ?lex-
ical affinities?, regardless of other existing connections
in the alignment and, more importantly, of the relation-
ships this token holds with other SL tokens in its vicinity.
Conversely, some TL tokens end up being connected to
several SL tokens, while other TL tokens are left uncon-
nected.
As mentioned in section 2, in a sub-sentential TM ap-
plication, contiguous sequences of tokens in the SL tend
to translate into contiguous sequences in the TL. This
suggests that it might be a good idea to integrate a ?con-
tiguity constraint? right into the alignment search proce-
dure.
For example, we can formulate a variant of the Viterbi
TS method above, which looks for the alignment that
maximizes Pr(a|S, T ), under the constraint that the TL
tokens aligned with the SL query must be contiguous.
Consider a procedure that seeks the (possibly null) se-
quence tj1 ...tj2 of T , that maximizes:
Pr(aq|s
i2
i1 , t
j2
j1)Pr(aq?|s
i1?1
1 s
m
i2+1, t
j1?1
1 t
n
j2+1)
Such a procedure actually produces two distinct align-
ments over S and T : an alignment aq, which connects the
query tokens (the sequence si2i1) with a sequence of con-
tiguous tokens in T (the sequence tj2j1), and an alignment
aq?, which connects the rest of sentence S (i.e. all the to-
kens outside the query) with the rest of T . Together, these
two alignments constitute the alignment a = aq ? aq?,
whose probability is maximal, under a double constraint:
1. the query tokens si2i1 can only be connected to tokens
within a contiguous region of T (the sequences tj2j1);
2. the tokens outside the query (in either one of the two
sequences si1?11 and smi2+1) can only get connected
to tokens outside tj2j1 .
With such an alignment procedure, we can trivially de-
vise a TS method, which will return the optimal tj2j1 as TL
answer. We call this method Contiguous TS. Alignments
satisfying the above constraints can be obtained directly,
by computing Viterbi alignments aq and aq? for each pair
of target positions ?j1, j2?. The TS procedure then re-
tains the pair of TL language positions that maximizes
the joint probability of alignments aq and aq?. This oper-
ation requires the computation of two Viterbi alignments
for each pair ?j1, j2?, i.e. n(n ? 1) Viterbi alignments,
plus a ?null? alignment, corresponding to the situation
where tj2j1 = ?. Overall, using IBM Model 2, the oper-
ation requires O(mn3) operations. Figure 4 illustrates a
contiguous TS obtained on the example of figure 2.
Alignment: Let us see ? Voyons
where ? quel
aq =
the ? engagement
government ? gouvernement
?s ? du
commitment ? engagement
is ? est
really ? ve?ritable
at ? la
in terms of ? envers
the ? la
farm ? agricole
community ? communaute?
. ? .
TL answer:
T = Voyons quel est le ve?ritable engagement du gou-
vernement envers la communaute? agricole.
Figure 4: Contiguous TS Example
3.4 Compositional TS
As pointed out in section 3.3, In IBM-style alignments,
a single TL token can be connected to several SL to-
kens, which sometimes leads to aberrations. This con-
trasts with alternative alignment models such as those
of Melamed (1998) and Wu (1997), which impose a
?one-to-one? constraint on alignments. Such a constraint
evokes the notion of compositionality in translation: it
suggests that each SL token operates independently in
the SL sentence to produce a single TL token in the
TL sentence, which then depends on no other SL token.
This view is, of course, extreme, and real-life translations
are full of examples (idiomatic expressions, terminology,
paraphrasing, etc.) that show how this compositionality
principle breaks down as we approach the level of word
correspondences.
However, in a TM application, TS usually needs not go
down to the level of individual words. Therefore, compo-
sitionality can often be assumed to apply, at least to the
level of the TS query. The contiguous TS method pro-
posed in the previous section implicitly made such an as-
sumption. Here, we push it a little further.
Consider a procedure that splits each the source and
target sentences S and T into two independent parts, in
such a way as to maximise the probability of the two re-
sulting Viterbi alignments:
argmax?i,j,d?
?
???
???
d = 1 : Pr(a1|si1, t
j
1)
?Pr(a2|smi+1, t
n
j+1)
d = ?1 : Pr(a1|si1, t
n
j+1)
?Pr(a2|smi+1, t
j
1)
In the triple ?i, j, d? above, i represents a ?split point?
in the SL sentence S, j is the analog for TL sentence T ,
and d is the ?direction of correspondence?: d = 1 denotes
a ?parallel correspondence?, i.e. s1...si corresponds to
t1...tj and si+1...sm corresponds to tj+1...tn; d = ?1
denotes a ?crossing correspondence?, i.e. s1...si corre-
sponds to tj+1...tn and si+1...sm corresponds to t1...tj .
The triple ?I, J,D? produced by this procedure refers
to the most probable alignment between S and T , un-
der the hypothesis that both sentences are made up of
two independent parts (s1...sI and sI+1...sm on the one
hand, t1...tJ and tJ+1...tn on the other), that correspond
to each other two-by-two, following direction D. Such
an alignment suggests that translation T was obtained
by ?composing? the translation of s1...sI with that of
sI+1...sm.
This ?splitting? process can be repeated recursively on
each pair of matching segments, down to the point where
each SL segment contains a single token. (TL segments
can always be split, even when empty, because IBM-style
alignments make it possible to connect SL tokens to the
?null? TL token, which is always available.) This gives
rise to a word-alignment procedure that we call Compo-
sitional word alignment.
This procedure actually produces two different out-
puts: first, a parallel partition of S and T into m pairs of
segments ?si, tkj ?, where each tkj is a (possibly null) con-
tiguous sub-sequence of T ; second, an IBM-style align-
ment, such that each SL and TL token is linked to at most
one token in the other language: this alignment is actually
the concatenation of individual Viterbi alignments on the
?si, tkj ? pairs, which connects each si to (at most) one of
the tokens in the corresponding tkj .
Of course, such alignments face even worst problems
than ordinary IBM-style alignments when confronted
with non-compositional translations. However, when
adapting this procedure to the TS task, we can hypoth-
esize that compositionality applies, at least to the level of
the SL query. This adaptation proceeds along the follow-
ing modifications to the alignment procedure described
above:
1. forbid splittings within the SL query: i1 ? i ? i2;
2. at each level of recursion, only consider that pair of
segments which contains the SL query;
3. stop the procedure as soon as it is no longer possible
to split the SL segment, i.e. it consists of si1 ...si2 .
The TL segment matched with si1 ...si2 when the proce-
dure terminates is the TL answer. We call this proce-
dure Compositional TS. It can be shown that it can be
carried out in O(m3n2) operations in the worst case, and
O(m2n2 logm) on average. Furthermore, by limiting the
search to split points yielding matching segments of com-
parable sizes, the number of required operations can be
cut by one order of magnitude (Simard, 2003).
Figure 5 shows how this procedure splits the example
pair of figure 2 (the query is shown in italics).
4 Evaluation
We describe here a series of experiments that were car-
ried out to evaluate the performance of the TS methods
described in section 3. We essentially identified a num-
ber of SL queries, looked up these segments in a TM to
extract matching pairs of SL-TL sentences, and manually
identified the TL tokens corresponding to the SL queries
in each of these pairs, hence producing manual TS?s. We
then submitted the same sentence-pairs and SL queries
to each of the proposed TS methods, and measured how
the TL answers produced automatically compared with
those produced manually. We describe this process and
the results we obtained in more details below.
4.1 Test Material
The test material for our experiments was gathered from a
translation memory, made up of approximately 14 years
of Hansard (English-French transcripts of the Canadian
parliamentary debates), i.e. all debates published be-
tween April 1986 and January 2002, totalling over 100
million words in each language. These documents were
mostly collected over the Internet, had the HTML markup
removed, were then segmented into paragraphs and sen-
tences, aligned at the sentence level using an implementa-
tion of the method described in (Simard et al, 1992), and
finally dumped into a document-retrieval system (MG
(Witten et al, 1999)). We call this the Hansard TM.
To identify SL queries, a distinct document from the
Hansard was used, the transcript from a session held
in March 2002. The English version of this document
was segmented into syntactic chunks, using an imple-
mentation of Osborne?s chunker (Osborne, 2000). All
sequences of chunks from this text that contained three
or more word tokens were then looked up in the Hansard
TM. Among the sequences that did match sentences in
the TM, 100 were selected at random. These made up the
test SL queries.
Recursion
Level SL segment TL segment direction (d)
1 [Let us see] [where the government ?s
commitment is really at in terms of the
farm community]
?? [Voyons] [quel est le ve?ritable engage-
ment du gouvernement envers la com-
munaute? agricole]
d = 1
2 [where the government ?s commitment
is really at] [in terms of the farm com-
munity]
?? [quel est le ve?ritable engagement du
gouvernement] [envers la communaute?
agricole]
d = 1
3 [where] [the government ?s commitment
is really at]
?? [quel] [est le ve?ritable engagement du
gouvernement]
d = 1
4 [the government ?s commitment] [is re-
ally at]
?? [est le ve?ritable] [engagement du gou-
vernement]
d = ?1
Answers: rq(S) =the government ?s commitment ?? rq(T ) =engagement du gouvernement
Figure 5: Compositional TS Example
While some SL queries yielded only a handful of
matches in the TM, others turned out to be very produc-
tive, producing hundreds (and sometimes thousands) of
couples. For each test segment, we retained only the 100
first matching pair of sentences from the TM. This pro-
cess yielded 4100 pairs of sentences from the TM, an av-
erage of 41 per SL query; we call this our test corpus.
Within each sentence pair, we spotted translations manu-
ally, i.e. we identified by hand the TL word-tokens cor-
responding to the SL query for which the pair had been
extracted. These annotations were done following the TS
guidelines proposed by Ve?ronis (1998); we call this the
reference TS.
4.2 Evaluation Metrics
The results of our TS methods on the test corpus were
compared to the reference TS, and performance was mea-
sured under different metrics. Given each pair ?S, T ?
from the test corpus, and the corresponding reference and
evaluated TL answers r? and r, represented as sets of to-
kens, we computed:
exactness : equal to 1 if r? = r, 0 otherwise;
recall : |r? ? r|/|r?|
precision : |r? ? r|/|r|
F-measure : 2 |r?r
?|
|r|+|r?|
In all the above computations, we considered that
?empty? TL answers (r = ?) actually contained a single
?null? word. These metrics were then averaged over all
pairs of the test corpus (and not over SL queries, which
means that more ?productive? queries weight more heav-
ily in the reported results).
4.3 Experiments
We tested all three methods presented in section 3, as
well as the three ?post-processings? on Viterbi TS pro-
posed in section 3.2. All of these methods are based on
IBM Model 2. The same model parameters were used for
all the experiments reported here, which were computed
with the GIZA program of the Egypt toolkit (Al-Onaizan
et al, 1999). Training was performed on a subset of about
20% of the Hansard TM. The results of our experiments
are presented in table 1.
Metric
method exact precision recall F
Viterbi 0.17 0.60 0.57 0.57
+ Expansion 0.26 0.51 0.71 0.55
+ Longest-sequence 0.03 0.63 0.20 0.29
+ Zero-tolerance 0.20 0.28 0.28 0.28
Contiguous 0.36 0.75 0.66 0.68
Compositional 0.40 0.72 0.70 0.69
Table 1: Results of experiments
The Zero-tolerance post-processing produces empty
TL answers whenever the TL tokens are not contigu-
ous. On our test corpus, over 70% of all Viterbi align-
ments turned out to be non-contiguous. These empty
TL answers were counted in the statistics above (Viterbi
+ Zero-tolerance row), which explains the low perfor-
mance obtained with this method. In practice, the in-
tention of Zero-tolerance post-processing is to filter out
non-contiguous answers, under the hypotheses that they
probably would not be usable in a TM application. Table
2 presents the performance of this method, taking into
account only non-empty answers.
Metric
method exact precision recall F
Viterbi
+ Zero-tolerance 0.56 0.83 0.82 0.81
Table 2: Performance of zero-tolerance filter on non-
empty TL answers
4.4 Discussion
Globally, in terms of exactness, compositional TS pro-
duces the best TL answers, with 40% correct answers, an
improvement of 135% over plain Viterbi TS. This gain
is impressive, particularily considering the fact that all
methods use exactly the same data. In more realistic
terms, the gain in F -measure is over 20%, which is still
considerable.
The best results in terms of precision are obtained with
contiguous TS, which in fact is not far behind composi-
tional TS in terms of recall either. This clearly demon-
strates the impact of a simple contiguity constraint in this
type of TS application. Overall, the best recall figures
are obtained with the simple Extension post-processing
on Viterbi TS, but at the cost of a sharp decrease in preci-
sion. Considering that precision is possibly more impor-
tant than recall in a TM application, the contiguous TS
would probably be a good choice.
The Zero-tolerance strategy, used as a filter on Viterbi
alignments, turns out to be particularily effective. It is in-
teresting to note that this method is equivalent to the one
proposed by Marcu (Marcu, 2001) to automatically con-
struct a sub-sentential translation memory. Taking only
non-null TS?s into consideration, it outclasses all other
methods, regardless of the metric. But this is at the cost
of eliminating numerous potentially useful TL answers
(more than 70%). This is particularily frustrating, con-
sidering that over 90% of all TL answers in the reference
are indeed contiguous.
To understand how this happens, one must go back to
the definition of IBM-style alignments, which specifies
that each SL token is linked to at most one TL token.
This has a direct consequence on Viterbi TS?s: if the SL
queries contains K word-tokens, then the TL answer will
itself contain at most that number of tokens. As a re-
sult, this method has systematic problems when the ac-
tual TL answer is longer than the SL query. It turns out
that this occurs very frequently, especially when aligning
from English to French, as is the case here. For exam-
ple, consider the English sequence airport security, most
often translated in French as se?curite? dans les ae?roports.
The Viterbi alignment normally produces links airport ?
ae?roport and security ? se?curite?, and the sequence dans
les is then left behind (or accidentally picked up by er-
roneous links from other parts of the SL sentence), thus
leaving a non-contiguous TL answer.
The Expansion post-processing, which finds the short-
est possible sequence that covers all the tokens of the
Viterbi TL answer, solves the problem in simple sit-
uations such as the one in the above example. But
in general, integrating contiguity constraints directly in
the search procedure (contiguous and compositional TS)
turns out to be much more effective, without solving the
problem entirely. This is explained in part by the fact that
these techniques are also based on IBM-style alignments.
When ?surplus? words appear at the boundaries of the
TL answer, these words are not counted in the alignment
probability, and so there is no particular reason to include
them in the TL answer. Consider the following example:
? These companies indicated their support for the
government ?s decision.
? Ces compagnies ont de?clare? qu? elles appuyaient la
de?cision du gouvernement .
When looking for the French equivalent to the English
indicated their support, we will probably end up with an
alignment that links indicated ? de?clare? and support ?
appuyaient. As a result of contiguity constraints, the TL
sequence qu? elle will naturally be included in the TL an-
swer, possibly forcing a link their ? elles in the process.
However, the only SL that could be linked to ont is the
verb indicated, which is already linked to de?clare?. As a
result, ont will likely be left behind in the final alignment,
and will not be counted when computing the alignment?s
probability.
5 Conclusion
We have presented different translation spottings meth-
ods, specifically adapted to a sub-sentential translation
memory system that proposes TL translations for SL
sequences of syntactic chunks, as proposed by Planas
(2000). These methods are based on IBM statistical trans-
lation Model 2 (Brown et al, 1993), but take advantage
of certain characteristics of the segments of text that can
typically be extracted from translation memories. By im-
posing contiguity and compositionality constraints on the
search procedure, we have shown that it is possible to per-
form translation spotting more accurately than by simply
relying on the most likely word alignment.
Yet, the accuracy of our methods still leave a lot to be
desired; on closer examination most of our problems can
be attributed to the underlying translation model. Com-
puting word alignments with IBM Model 2 is straightfor-
ward and efficient, which made it a good choice for ex-
perimenting; however, this model is certainly not the state
of the art in statistical translation modeling. Thenagain,
the methods proposed here were all based on the idea
of finding the most likely word-alignment under various
constraints. This approach is not dependent on the under-
lying translation model, and similar methods could cer-
tainly be devised based on more elaborate models, such
as IBM Models 3?5, or the HMM-based models proposed
by Och et al (1999) for example.
Alternatively, there are other ways to compensate for
Model 2?s weaknesses. Each IBM-style alignment be-
tween two segments of text denotes one particular expla-
nation of how the TL words emerged from the SL words,
but it doesn?t tell the whole story. Basing our TS meth-
ods on a set of likely alignments rather than on the single
most-likely alignment, as is normally done to estimate the
parameters of higher-level models, could possibly lead to
more accurate TS results. Similarly, TS applications are
not bound to translation directionality as statistical trans-
lation systems are; this means that we could also make
use of a ?reverse? model to obtain a better estimate of the
likelihood of two segments of text being mutual transla-
tion.
These are all research directions that we are currently
pursuing.
References
[Abney1991] Steven Abney. 1991. Parsing by Chunks.
In R.C. Berwick, editor, Principle-Based Parsing:
Computation and Psycholinguistics, pages 257?278.
Kluwer Academic Publishers, Dordrecht, The Nether-
lands.
[Al-Onaizan et al1999] Yaser Al-Onaizan, Jan Curin,
Michael Jahr, Kevin Knight, John Lafferty, Dan
Melamed, Franz-Josef Och, David Purdy, Noah H.
Smith, and David Yarowsky. 1999. Statistical Ma-
chine Translation - Final Report, JHU Workshop 1999.
Technical report, Johns Hopkins University.
[Brown et al1993] Peter F. Brown, Stephen A. Della
Pietra, Vincent J. Della Pietra, and Robert L. Mer-
cer. 1993. The Mathematics of Machine Transla-
tion: Parameter Estimation. Computational Linguis-
tics, 19(2):263?311.
[Brown1996] Ralf D. Brown. 1996. Example-Based Ma-
chine Translation in the Pangloss System. In Proceed-
ings of the International Conference on Computational
Linguistics (COLING) 1996, pages 169?174, Copen-
hagen, Denmark, August.
[Lange? et al1997] Jean-Marc Lange?, ?Eric Gaussier, and
Be?atrice Daille. 1997. Bricks and Skeletons: Some
Ideas for the Near Future of MAHT. Machine Trans-
lation, 12(1?2):39?51.
[Macklovitch et al2000] Elliott Macklovitch, Michel
Simard, and Philippe Langlais. 2000. TransSearch:
A Free Translation Memory on the World Wide
Web. In Proceedings of the Second International
Conference on Language Resources & Evaluation
(LREC), Athens, Greece.
[Marcu2001] Daniel Marcu. 2001. Towards a Unified
Approach to Memory- and Statistical-Based Machine
Translation. In Proceedings of the 39th Annual Meet-
ing of the Association for Computational Linguistics
(ACL), Toulouse, France, July.
[McTait et al1999] Kevin McTait, Maeve Olohan, and
Arturo Trujillo. 1999. A Building Blocks Approach to
Translation Memory. In Proceedings of the 21st ASLIB
International Conference on Translating and the Com-
puter, London, UK.
[Melamed1998] I. Dan Melamed. 1998. Word-to-Word
Models of Translational Equivalence. Technical Re-
port 98-08, Dept. of Computer and Information Sci-
ence, University of Pennsylvania, Philadelphia, USA.
[Och et al1999] Franz Josef Och, Christoph Tillmann,
and Hermann Ney. 1999. Improved Alignment Mod-
els for Statistical Machine Translation. In Proceedings
of the 4th Conference on Empirical Methods in Natu-
ral Language Processing (EMNLP)and 7th ACL Work-
shop on Very Large Corpora (WVLC), pages 20?28,
College Park, USA.
[Osborne2000] Miles Osborne. 2000. Shallow Parsing
as Part-of-Speech Tagging. In Claire Cardie, Wal-
ter Daelemans, Claire Ne?dellec, and Erik Tjong Kim
Sang, editors, Proceedings of the Fourth Conference
on Computational Natural Language Learning, Lis-
bon, Portugal, September.
[Planas2000] Emmanuel Planas. 2000. Extending Trans-
lation Memories. In EAMT Machine Translation
Workshop, Ljubljana, Slovenia, May.
[Simard et al1992] Michel Simard, George Foster, and
Pierre Isabelle. 1992. Using Cognates to Align Sen-
tences in Bilingual Corpora. In Proceedings of the
4th Conference on Theoretical and Methodological
Issues in Machine Translation (TMI), pages 67?82,
Montre?al, Canada.
[Simard2003] Michel Simard. 2003. Me?moires de tra-
duction sous-phrastiques. Ph.D. thesis, Universite? de
Montre?al. to appear.
[Ve?ronis and Langlais2000] Jean Ve?ronis and Philippe
Langlais. 2000. Evaluation of Parallel Text Alignment
Systems ? The ARCADE Project. In Jean Ve?ronis, ed-
itor, Parallel Text Processing, Text, Speech and Lan-
guage Technology. Kluwer Academic Publishers, Dor-
drecht, The Netherlands.
[Ve?ronis1998] Jean Ve?ronis. 1998. Tagging guidelines
for word alignment. http://www.up.univ-mrs.fr/ vero-
nis/arcade/2nd/word/guide/index.html, April.
[Witten et al1999] Ian H. Witten, Alistair Moffat, and
Timothy C. Bell. 1999. Managing Gigabytes: Com-
pressing and Indexing Documents and Images. Mor-
gan Kaufmann Publishing, San Francisco, USA, 2nd
edition edition.
[Wu1997] Dekai Wu. 1997. Stochastic Inversion Trans-
duction Grammars and Bilingual Parsing of Parallel
Corpora. Computational Linguistics, 23(3):377?404,
September.
Proceedings of the Human Language Technology Conference of the North American Chapter of the ACL, pages 25?32,
New York, June 2006. c?2006 Association for Computational Linguistics
Segment Choice Models: Feature-Rich Models for Global  
Distortion in Statistical Machine Translation 
 
 
Roland Kuhn, Denis Yuen, Michel Simard, Patrick Paul,  
George Foster, Eric Joanis, and Howard Johnson 
 
Institute for Information Technology, National Research Council of Canada 
Gatineau, Qu?bec, CANADA  
Email: {Roland.Kuhn, Michel.Simard, Patrick.Paul, George.Foster, Eric.Joanis, 
Howard.Johnson}@cnrc-nrc.gc.ca; Denis Yuen: mucous@gmail.com  
 
 
 
 
Abstract 
This paper presents a new approach to 
distortion (phrase reordering) in phrase-
based machine translation (MT). Distor-
tion is modeled as a sequence of choices 
during translation. The approach yields 
trainable, probabilistic distortion models 
that are global: they assign a probability 
to each possible phrase reordering. These 
?segment choice? models (SCMs) can be 
trained on ?segment-aligned? sentence 
pairs; they can be applied during decoding 
or rescoring. The approach yields a metric 
called ?distortion perplexity? (?disperp?) 
for comparing SCMs offline on test data, 
analogous to perplexity for language 
models. A decision-tree-based SCM is 
tested on Chinese-to-English translation, 
and outperforms a baseline distortion 
penalty approach at the 99% confidence 
level. 
1 Introduction: Defining SCMs  
The work presented here was done in the context 
of phrase-based MT (Koehn et al, 2003; Och and 
Ney, 2004). Distortion in phrase-based MT occurs 
when the order of phrases in the source-language 
sentence changes during translation, so the order of 
corresponding phrases in the target-language trans-
lation is different. Some MT systems allow arbi-
trary reordering of phrases, but impose a distortion 
penalty proportional to the difference between the 
new and the original phrase order (Koehn, 2004). 
Some interesting recent research focuses on reor-
dering within a narrow window of phrases (Kumar 
and Byrne, 2005; Tillmann and Zhang, 2005; Till-
mann, 2004). The (Tillmann, 2004) paper intro-
duced lexical features for distortion modeling. A 
recent paper (Collins et al, 2005) shows that major 
gains can be obtained by constructing a parse tree 
for the source sentence and then applying hand-
crafted reordering rules to rewrite the source in 
target-language-like word order prior to MT.  
 
Our model assumes that the source sentence is 
completely segmented prior to distortion. This 
simplifying assumption requires generation of hy-
potheses about the segmentation of the complete 
source sentence during decoding. The model also 
assumes that each translation hypothesis grows in a 
predetermined order. E.g., Koehn?s decoder 
(Koehn 2004) builds each new hypothesis by add-
ing phrases to it left-to-right (order is deterministic 
for the target hypothesis). Our model doesn?t re-
quire this order of operation ? it would support 
right-to-left or inwards-outwards hypothesis con-
struction ? but it does require a predictable order. 
 
One can keep track of how segments in the 
source sentence have been rearranged during de-
coding for a given hypothesis, using what we call a 
?distorted source-language hypothesis? (DSH). A 
similar concept appears in (Collins et al, 2005) 
(this paper?s preoccupations strongly resemble 
25
ours, though our method is completely different: 
we don?t parse the source, and use only automati-
cally generated rules). Figure 1 shows an example 
of a DSH for German-to-English translation (case 
information is removed). Here, German ?ich habe 
das buch gelesen .? is translated into English ?i 
have read the book .? The DSH shows the distor-
tion of the German segments into an English-like 
word order that occurred during translation (we 
tend to use the word ?segment? rather than the 
more linguistically-charged  ?phrase?). 
Figure 1. Example of German-to-English DSH 
From the DSH, one can reconstruct the series of 
segment choices. In Figure 1 - given a left-to-right 
decoder - ?[ich]? was chosen from five candidates 
to be the leftmost segment in the DSH. Next, 
?[habe]? was chosen from four remaining candi-
dates, ?[gelesen]? from three candidates, and ?[das 
buch]? from two candidates. Finally, the decoder 
was forced to choose ?[.]?.   
 
Segment Choice Models (SCMs) assign 
probabilities to segment choices made as the DSH 
is constructed. The available choices at a given 
time are called the ?Remaining Segments? (RS). 
Consider a valid (though stupid) SCM that assigns 
equal probabilities to all segments in the RS. This 
uniform SCM assigns a probability of 1/5! to the 
DSH in Figure 1: the probability of choosing 
?[ich]? from among 5 RS was 1/5, then the 
probability of ?[habe]? among 4 RS was  1/4 , etc. 
The uniform SCM would be of little use to an MT 
system. In the next two sections we describe some 
more informative SCMs, define the ?distortion 
perplexity? (?disperp?) metric for comparing 
SCMs offline on a test corpus, and show how to 
construct this corpus.  
2 Disperp and Distortion Corpora 
2.1 Defining Disperp 
The ultimate reason for choosing one SCM over 
another will be the performance of an MT system 
containing it, as measured by a metric like BLEU 
(Papineni et al, 2002). However, training and 
testing a large-scale MT system for each new SCM 
would be costly. Also, the distortion component?s 
effect on the total score is muffled by other 
components (e.g., the phrase translation and target 
language models). Can we devise a quick 
standalone metric for comparing SCMs? 
 
There is an offline metric for statistical language 
models: perplexity (Jelinek, 1990). By analogy, the 
higher the overall probability a given SCM assigns 
to a test corpus of representative distorted sentence 
hypotheses (DSHs), the better the quality of the 
SCM. To define distortion perplexity (?disperp?), 
let PrM(dk) = the probability an SCM M assigns to 
a DSH for sentence k, dk. If T is a test corpus 
comprising numerous DSHs, the probability of the 
corpus according to M is PrM(T) =   k PrM(dk).  
Let S(T) = total number of segments in T. Then 
disperp(M,T) = PrM(T)-1/S(T). This gives the mean 
number of choices model M allows; the lower the 
disperp for corpus T, the better M is as a model for 
T (a model X that predicts segment choice in T 
perfectly would have disperp(X,T) = 1.0).  
2.2 Some Simple A Priori SCMs 
The uniform SCM assigns to the DSH dk that has 
S(dk) segments the probability 1/[S(dk)!] . We call 
this Model A. Let?s define some other illustrative 
SCMs. Fig. 2 shows a sentence that has 7 segments 
with 10 words (numbered 0-9 by original order). 
Three segments in the source have been used; the 
decoder has a choice of four RS. Which of the RS 
has the highest probability of being chosen? Per-
haps [2 3], because it is the leftmost RS: the ?left-
most? predictor. Or, the last phrase in the DSH will 
be followed by the phrase that originally followed 
it, [8 9]: the ?following? predictor. Or, perhaps 
positions in the source and target should be close, 
so since the next DSH position to be filled is 4, 
phrase [4] should be favoured: the ?parallel? pre-
dictor. 
 
 
Figure 2. Segment choice prediction example 
Model B will be based on the ?leftmost? predic-
tor, giving the leftmost segment in the RS twice the 
probability of the other segments, and giving the 
Original German:   [ich] [habe] [das buch] [gelesen]    [.] 
DSH for German:  [ich] [habe]  [gelesen]    [das buch] [.] 
(English:                [i]     [have]   [read]        [the book] [.]) 
original:  [0 1] [2 3] [4] [5] [6] [7] [8 9] 
DSH:  [0 1] [5] [7],   RS:  [2 3], [4], [6], [8 9] 
26
others uniform probabilities. Model C will be 
based on the ?following? predictor, doubling the 
probability for the segment in the RS whose first 
word was the closest to the last word in the DSH, 
and otherwise assigning uniform probabilities. Fi-
nally, Model D combines ?leftmost? and ?follow-
ing?: where the leftmost and following segments 
are different, both are assigned double the uniform 
probability; if they are the same segment, that 
segment has four times the uniform probability. Of 
course, the factor of 2.0 in these models is arbi-
trary. For Figure 2, probabilities would be: 
? Model A: PrA([2 3])= PrA([4])= PrA([6])= 
PrA([8 9]) = 1/4; 
? Model B: PrB ([2 3])= 2/5, PrB([4])= 
PrB([6])= PrB([8 9]) = 1/5; 
? Model C: PrC ([2 3])= PrC ([4])= PrC([6]) 
= 1/5, PrC([8 9]) = 2/5; 
? Model D: PrD ([2 3]) = PrD([8 9]) = 1/3, 
PrD([4])= PrD([6]) = 1/6.  
 
Finally, let?s define an SCM derived from the 
distortion penalty used by systems based on the 
?following? predictor, as in (Koehn, 2004). Let ai = 
start position of source phrase translated into ith 
target phrase, bi -1= end position of source phrase 
that?s translated into (i-1)th target phrase. Then 
distortion penalty d(ai, bi-1) =   ?ai? bi-1 -1?; the total 
distortion is the product of the phrase distortion 
penalties. This penalty is applied as a kind of non-
normalized probability in the decoder. The value of 
   for given (source, target) languages is optimized 
on development data. 
To turn this penalty into an SCM, penalties are 
normalized into probabilities, at each decoding 
stage; we call the result Model P (for ?penalty?). 
Model P with    = 1.0 is the same as uniform 
Model A. In disperp experiments, Model P with    
optimized on held-out data performs better than 
Models A-D (see Figure 5), suggesting that dis-
perp is a realistic measure.  
Models A-D are models whose parameters were 
all defined a priori; Model P has one trainable pa-
rameter,  . Next, let?s explore distortion models 
with several trainable parameters.  
2.3 Constructing a Distortion Corpus 
To compare SCMs using disperp and to train 
complex SCMs, we need a corpus of representative 
examples of DSHs. There are several ways of ob-
taining such a corpus. For the experiments de-
scribed here, the MT system was first trained on a 
bilingual sentence-aligned corpus. Then, the sys-
tem was run in a second pass over its own training 
corpus, using its phrase table with the standard dis-
tortion penalty to obtain a best-fit phrase alignment 
between each (source, target) sentence pair. Each 
such alignment yields a DSH whose segments are 
aligned with their original positions in the source; 
we call such a source-DSH alignment a ?segment 
alignment?. We now use a leave-one-out procedure 
to ensure that information derived from a given 
sentence pair is not used to segment-align that sen-
tence pair. In our initial experiments we didn?t do 
this, with the result that the segment-aligned cor-
pus underrepresented the case where words or N-
grams not in the phrase table are seen in the source 
sentence during decoding.  
3 A Trainable Decision Tree SCM 
Almost any machine learning technique could be 
used to create a trainable SCM. We implemented 
one based on decision trees (DTs), not because 
DTs necessarily yield the best results but for soft-
ware engineering reasons: DTs are a quick way to 
explore a variety of features, and are easily inter-
preted when grown (so that examining them can 
suggest further features). We grew N DTs, each 
defined by the number of choices available at a 
given moment. The highest-numbered DT has a 
?+? to show it handles N+1 or more choices. E.g., 
if we set N=4, we grow a ?2-choice?, a ?3-choice?, 
a ?4-choice?, and a ?5+-choice tree?. The 2-choice 
tree handles cases where there are 2 segments in 
the RS, assigning a probability to each; the 3-
choice tree handles cases where there are 3 seg-
ments in the RS, etc. The 5+-choice tree is differ-
ent from the others: it handles cases where there 
are 5 segments in the RS to choose from, and 
cases where there are more than 5. The value of N 
is arbitrary; e.g., for N=8, the trees go from ?2-
choice? up to ?9+-choice?.  
Suppose a left-to-right decoder with an N=4 
SCM is translating a sentence with seven phrases. 
Initially, when the DSH is empty, the 5+-choice 
tree assigns probabilities to each of these seven. It 
27
will use the 5+-choice tree twice more, to assign 
probabilities to six RS, then to five. To extend the 
hypothesis, it will then use the 4-choice tree, the 3-
choice tree, and finally the 2-choice tree. Disperps 
for this SCM are calculated on test corpus DSHs in 
the same left-to-right way, using the tree for the 
number of choices in the RS to find the probability 
of each segment choice. 
Segments need labels, so the N-choice DT can 
assign probabilities to the N segments in the RS. 
We currently use a ?following? labeling scheme. 
Let X be the original source position of the last 
word put into the DSH, plus 1. In Figure 2, this 
was word 7, so X=8. In our scheme, the RS seg-
ment whose first word is closest to X is labeled 
?A?; the second-closest segment is labeled ?B?, 
etc. Thus, segments are labeled in order of the 
(Koehn, 2004) penalty; the ?A? segment gets the 
lowest penalty. Ties between segments on the right 
and the left of X are broken by first labeling the 
right segment. In Figure 2, the labels for the RS 
are ?A? = [8 9], ?B? = [6], ?C? = [4], ?D? = [2 3].  
 
 
 
 
 
 
 
 
Figure 3. Some question types for choice DTs 
Figure 3 shows the main types of questions used 
for tree-growing, comprising position questions 
and word-based questions. Position questions 
pertain to location, length, and ordering of seg-
ments. Some position questions ask about the dis-
tance between the first word of a segment and the 
?following? position X: e.g., if the answer to 
?pos(A)-pos(X)=0?? is yes, then segment A comes 
immediately after the last DSH segment in the 
source, and is thus highly likely to be chosen. 
There are also questions relating to the ?leftmost? 
and ?parallel? predictors (above, sec. 2.2). The 
fseg() and bseg() functions count segments in the 
RS from left to right and right to left respectively, 
allowing, e.g., the question whether a given seg-
ment is the second last segment in the RS. The 
only word-based questions currently implemented 
ask whether a given word is contained in a given 
segment (or anywhere in the DSH, or anywhere in 
the RS). This type could be made richer by allow-
ing questions about the position of a given word in 
a given segment, questions about syntax, etc.  
Figure 4 shows an example of a 5+-choice DT. 
The ?+? in its name indicates that it will handle 
cases where there are 5 or more segments in the 
RS. The counts stored in the leaves of this DT rep-
resent the number of training data items that ended 
up there; the counts are used to estimate probabili-
ties. Some smoothing will be done to avoid zero 
probabilities, e.g., for class C in node 3.  
 
Figure 4. Example of a 5+-choice tree 
For ?+? DTs, the label closest to the end of the 
alphabet (?E? in Figure 4) stands for a class that 
can include more than one segment. E.g., if this 
5+-choice DT is used to estimate probabilities for a 
7-segment RS, the segment closest to X is labeled 
?A?, the second closest ?B?, the third closest ?C?, 
and the fourth closest ?D?. That leaves 3 segments, 
all labeled ?E?. The DT shown yields probability 
Pr(E) that one of these three will be chosen. Cur-
rently, we apply a uniform distribution within this 
?furthest from X? class, so the probability of any 
one of the three ?E? segments is estimated as 
Pr(E)/3.  
To train the DTs, we generate data items from 
the second-pass DSH corpus. Each DSH generates 
several data items. E.g., moving across a seven-
segment DSH from left to right, there is an exam-
ple of the seven-choice case, then one of the six-
choice case, etc. Thus, this DSH provides three 
items for training the 5+-choice DT and one item 
     pos(A)-pos(X)<0? 
A:27 B:23 C:20 D:11 E:19  
        today    DSH? 
A:10 B:8 C:10 D:6 E:5 
A:8 B:6 C:0 D:2 E:4 A:2 B:2 C:10 D:4 E:1 
A:17 B:15 C:10 D:5 E:14 
yes no 
yes no 
1. 
3. 
2. 5. 
4. 
1. Position Questions 
Segment Length Questions 
E.g., ?lgth(DSH)<5??, ?lgth(B)=2??, ?lgth(RS)<6??, etc.  
Questions about Original Position 
Let pos(seg) = index of seg?s first word in source sentence 
E.g., ?pos(A)=9??, ?pos(C) <17??, etc.  
Questions With X (?following? word position)  
E.g., ?pos(X)=9??, ?pos(C) ? pos(X) <0??, etc.  
Segment Order Questions  
Let fseg = segment # (forward), bseg = segment # (back-
ward) 
E.g., ?fseg(D) = 1??, ?bseg(A) <5??, etc.  
2. Word-Based Questions  
E.g., ?and   DSH??, ?November   B??, etc.  
28
each for training the 4-choice, 3-choice, and 2-
choice DTs. The DT training method was based on 
Gelfand-Ravishankar-Delp expansion-pruning 
(Gelfand et al, 1991), for DTs whose nodes con-
tain probability distributions (Lazarid?s et al, 
1996).  
4 Disperp Experiments 
We carried out SCM disperp experiments for the 
English-Chinese task, in both directions. That is, 
we trained and tested models both for the distortion 
of English into Chinese-like phrase order, and the 
distortion of Chinese into English-like phrase or-
der. For reasons of space, details about the ?dis-
torted English? experiments won?t be given here. 
Training and development data for the distorted 
Chinese experiments were taken from the NIST 
2005 release of the FBIS corpus of Xinhua news 
stories. The training corpus comprised 62,000 
FBIS segment alignments, and the development 
?dev? corpus comprised a disjoint set of 2,306 
segment alignments from the same FBIS corpus. 
All disperp results are obtained by testing on ?dev? 
corpus. 
 
Distorted Chinese: Models A-D, P, & a four-DT 
Model
1
2
3
4
5
6
7
8
50
0
10
00
20
00
40
00
80
00
16
00
0
32
00
0
62
00
0
# training alignments (log scale)
Di
sp
er
p 
o
n
 
"
de
v
"
 
Model A
Model B
Model C
Model D
Model P (alpha =
0.77)
Four DTs: pos +
100-wd qns
 
Figure 5. Several SCMs for distorted Chinese 
Figure 5 shows disperp results for the models 
described earlier. The y axis begins at 1.0 (mini-
mum value of disperp). The x axis shows number 
of alignments (DSHs) used to train DTs, on a log 
scale. Models A-D are fixed in advance; Model P?s 
single parameter    was optimized once on the en-
tire training set of 62K FBIS alignments (to 0.77) 
rather than separately for each amount of training 
data. Model P, the normalized version of  Koehn?s 
distortion penalty, is superior to Models A-D, and 
the DT-based SCM is superior to Model P.  
The Figure 5 DT-based SCM had four trees (2-
choice, 3-choice, 4-choice, and 5+-choice) with 
position-based and word-based questions. The 
word-based questions involved only the 100 most 
frequent Chinese words in the training corpus. The 
system?s disperp drops from 3.1 to 2.8 as the num-
ber of alignments goes from 500 to 62K. 
Figure 6 examines the effect of allowing word-
based questions. These questions provide a signifi-
cant disperp improvement, which grows with the 
amount of training data. 
Distorted Chinese: effect of allowing word qns 
(four- DT models)
2.5
2.6
2.7
2.8
2.9
3
3.1
3.2
3.3
50
0
10
00
20
00
40
00
80
00
16
00
0
32
00
0
62
00
0
# training alignments (log scale) 
Di
sp
er
p 
o
n
 
"
de
v
"
 
Four DTs: pos qns
only
Four DTs: pos +
100-wd qns
 
Figure 6. Do word-based questions help? 
In the ?four-DT? results above, examples with 
five or more segments are handled by the same 
?5+-choice? tree. Increasing the number of trees 
allows finer modeling of multi-segment cases 
while spreading the training data more thinly. 
Thus, the optimal number of trees depends on the 
amount of training data. Fixing this amount to 32K 
alignments, we varied the number of trees. Figure 
7 shows that this parameter has a significant im-
pact on disperp, and that questions based on the 
most frequent 100 Chinese words help perform-
ance for any number of trees.  
29
Distorted Chinese: Disperp vs. # of trees (all 
trees grown on 32K alignments)
2.3
2.4
2.5
2.6
2.7
2.8
2.9
3
3.1
3.2
3 4 5 6 7 8 9 10 11 12 13 14
# of trees
Di
sp
er
p 
o
n
 
"
de
v"
 
pos qns only
pos + 100-wd qns
 
Figure 7. Varying the number of DTs  
In Figure 8 the number of the most frequent 
Chinese words for questions is varied (for a 13-DT 
system trained on 32K alignments). Most of the 
improvement came from the 8 most frequent 
words, especially from the most frequent, the 
comma ?,?. This behaviour seems to be specific to 
Chinese. In our ?distorted English? experiments, 
questions about the 8 most frequent words also 
gave a significant improvement, but each of the 8 
words had a fairly equal share in the improvement. 
Distorted Chinese: Disperp vs. #words (all trees 
grown on 32K alignments)
2.58
2.6
2.62
2.64
2.66
2.68
2.7
2.72
0 2 8 32 12
8
51
2
# words tried for qns (log scale)
Di
sp
er
p 
o
n
 
"
de
v
"
Performance of 13-
DT system
 
Figure 8. Varying #words (13-DT system) 
Finally, we grew the DT system used for the MT 
experiments: one with 13 trees and questions about 
the 25 most frequent Chinese words, grown on 
88K alignments. Its disperp on the ?dev? used for 
the MT experiments (a different ?dev? from the 
one above ? see Sec. 5.2) was 2.42 vs. 3.48 for the 
baseline Model P system: a 30% drop.  
5 Machine Translation Experiments 
5.1 SCMs for Decoding 
SCMs assume that the source sentence is fully 
segmented throughout decoding. Thus, the system 
must guess the segmentation for the unconsumed 
part of the source (?remaining source?: RS). For 
the results below, we used a simple heuristic: RS is 
broken into one-word segments. In future, we will 
apply a more realistic segmentation model to RS 
(or modify DT training to reflect accurately RS 
treatment during decoding).  
5.2 Chinese-to-English MT Experiments  
The training corpus for the MT system?s phrase 
tables consists of all parallel text available for the 
NIST MT05 Chinese-English evaluation, except 
the Xinhua corpora and part 3 of LDC's ?Multiple-
Translation Chinese Corpus? (MTCCp3). The Eng-
lish language model was trained on the same cor-
pora, plus 250M words from Gigaword. The DT-
based SCM was trained and tuned on a subset of 
this same training corpus (above). The dev corpus 
for optimizing component weights is MTCCp3. 
The experimental results below were obtained by 
testing on the evaluation set for MTeval NIST04.  
Phrase tables were learned from the training cor-
pus using the ?diag-and? method (Koehn et al, 
2003), and using IBM model 2 to produce initial 
word alignments (these authors found this worked 
as well as IBM4). Phrase probabilities were based 
on unsmoothed relative frequencies. The model 
used by the decoder was a log-linear combination 
of a phrase translation model (only in the 
P(source|target) direction), trigram language 
model, word penalty (lexical weighting), an op-
tional segmentation model (in the form of a phrase 
penalty) and distortion model. Weights on the 
components were assigned using the (Och, 2003) 
method for max-BLEU training on the develop-
ment set. The decoder uses a dynamic-
programming beam-search, like the one in (Koehn, 
2004). Future-cost estimates for all distortion mod-
els are assigned using the baseline penalty model. 
5.3 Decoding Results 
30
29,40
29,60
29,80
30,00
30,20
30,40
30,60
30,80
31,00
31,20
no PP PP no PP PP
DP DT
BL
EU
 
sc
o
re
1x beam
4x beam
 
Figure 9. BLEU on NIST04 (95% conf. = ?0.7) 
Figure 9 shows experimental results. The ?DP? 
systems use the distortion penalty in (Koehn, 2004) 
with    optimized on ?dev?, while ?DT? systems 
use the DT-based SCM. ?1x? is the default beam 
width, while ?4x? is a wider beam (our notation 
reflects decoding time, so ?4x? takes four times as 
long as ?1x?). ?PP? denotes presence of the phrase 
penalty component. The advantage of DTs as 
measured by difference between the score of the 
best DT system and the best DP system is 0.75 
BLEU at 1x and 0.5 BLEU at 4x. With a 95% 
bootstrap confidence interval of ?0.7 BLEU (based 
on 1000-fold resampling), the resolution of these 
results is too coarse to draw firm conclusions. 
Thus, we carried out another 1000-fold bootstrap 
resampling test on NIST04, this time for pairwise 
system comparison. Table 1 shows results for 
BLEU comparisons between the systems with the 
default (1x) beam. The entries show how often the 
A system (columns) had a better score than the B 
system (rows), in 1000 observations.  
   A    
vs. B   
DP,  
no PP 
DP, PP DT,  
no PP 
DT, PP 
DP,  
no PP 
x 2.95% 99.45% 99.55% 
DP, PP 97.05% x 99.95% 99.95% 
DT,  
no PP 
0.55% 0.05% x 65.68% 
DT, PP 0.45% 0.05% 34.32% x 
Table 1. Pairwise comparison for 1x systems 
The table shows that both DT-based 1x systems 
performed better than either of the DP systems 
more than 99% of the time (underlined results). 
Though not shown in the table, the same was true 
with 4x beam search. The DT 1x system with a 
phrase penalty had a higher score than the DT 1x 
system without one about 66% of the time. 
6 Summary and Discussion 
In this paper, we presented a new class of probabil-
istic model for distortion, based on the choices 
made during translation. Unlike some recent dis-
tortion models (Kumar and Byrne, 2005; Tillmann 
and Zhang, 2005; Tillmann, 2004) these Segment 
Choice Models (SCMs) allow phrases to be moved 
globally, between any positions in the sentence. 
They also lend themselves to quick offline com-
parison by means of a new metric called disperp. 
We developed a decision-tree (DT) based SCM 
whose parameters were optimized on a ?dev? cor-
pus via disperp. Two variants of the DT system 
were experimentally compared with two systems 
with a distortion penalty on a Chinese-to-English 
task. In pairwise bootstrap comparisons, the sys-
tems with DT-based distortion outperformed the 
penalty-based systems more than 99% of the time. 
The computational cost of training the DTs on 
large quantities of data is comparable to that of 
training phrase tables on the same data - large but 
manageable ? and increases linearly with the 
amount of training data. However, currently there 
is a major problem with DT training: the low pro-
portion of Chinese-English sentence pairs that can 
be fully segment-aligned and thus be used for DT 
training (about 27%). This may result in selection 
bias that impairs performance. We plan to imple-
ment an alignment algorithm with smoothed phrase 
tables (Johnson et al 2006) to achieve segment 
alignment on 100% of the training data. 
Decoding time with the DT-based distortion 
model is roughly proportional to the square of the 
number of tokens in the source sentence. Thus, 
long sentences pose a challenge, particularly dur-
ing the weight optimization step. In experiments on 
other language pairs reported elsewhere (Johnson 
et al 2006), we applied a heuristic: DT training 
and decoding involved source sentences with 60 or 
fewer tokens, while longer sentences were handled 
with the distortion penalty. A more principled ap-
31
proach would be to divide long source sentences 
into chunks not exceeding 60 or so tokens, within 
each of which reordering is allowed, but which 
cannot themselves be reordered.  
The experiments above used a segmentation 
model that was a count of the number of source 
segments (sometimes called ?phrase penalty?), but 
we are currently exploring more sophisticated 
models. Once we have found the best segmentation 
model, we will improve the system?s current na?ve 
single-word segmentation of the remaining source 
sentence during decoding, and construct a more 
accurate future cost function for beam search. An-
other obvious system improvement would be to 
incorporate more advanced word-based features in 
the DTs, such as questions about word classes 
(Tillmann and Zhang 2005, Tillmann 2004).  
We also plan to apply SCMs to rescoring N-best 
lists from the decoder. For rescoring, one could 
apply several SCMs, some with assumptions dif-
fering from those of the decoder. E.g., one could 
apply right-to-left SCMs, or ?distorted target? 
SCMs which assume a target hypothesis generated 
the source sentence, instead of vice versa.  
Finally, we are contemplating an entirely differ-
ent approach to DT-based SCMs for decoding. In 
this approach, only one DT would be used, with 
only two output classes that could be called ?C? 
and ?N?. The input to such a tree would be a par-
ticular segment in the remaining source sentence, 
with contextual information (e.g., the sequence of 
segments already chosen). The DT would estimate 
the probability Pr(C) that the specified segment is 
?chosen? and the probability Pr(N) that it is ?not 
chosen?. This would eliminate the need to guess 
the segmentation of the remaining source sentence.  
References  
P. Brown, S. Della Pietra, V. Della Pietra, and R. Mer-
cer. 1993. ?The Mathematics of Statistical Machine 
Translation: Parameter Estimation?. Computational 
Linguistics, 19(2), pp. 263-311.  
 
M. Collins, P. Koehn, and I. Ku   erov?. 2005. ?Clause 
Restructuring for Statistical Machine Translation?. 
Proc. ACL, Ann Arbor, USA, pp. 531-540. 
 
S. Gelfand, C. Ravishankar, and E. Delp. 1991. ?An 
Iterative Growing and Pruning Algorithm for Clas-
sification Tree Design?. IEEE Trans. Patt. Analy. 
Mach. Int. (IEEE PAMI), V. 13, no. 2, pp. 163-174.  
 
F. Jelinek. 1990. ?Self-Organized Language Modeling 
for Speech Recognition? in Readings in Speech 
Recognition (ed. A. Waibel and K. Lee, publ. Mor-
gan Kaufmann), pp. 450-506.  
 
H. Johnson, F. Sadat, G. Foster, R. Kuhn, M. Simard, E. 
Joanis, and S. Larkin. 2006. ?PORTAGE: with 
Smoothed Phrase Tables and Segment Choice Mod-
els?.  Submitted to NAACL 2006 Workshop on Statis-
tical Machine Translation, New York City. 
P. Koehn. 2004. ?Pharaoh: a Beam Search Decoder for 
Phrase-Based Statistical Machine Translation Mod-
els?. Assoc. Machine Trans. Americas (AMTA04). 
 
P. Koehn, F.-J. Och and D. Marcu. 2003. ?Statistical 
Phrase-Based Translation?. Proc. Human Lang. 
Tech. Conf. N. Am. Chapt. Assoc. Comp. Ling. 
(NAACL03), pp. 127-133.  
 
S. Kumar and W. Byrne. 2005. ?Local Phrase Reorder-
ing Models for Statistical Machine Translation?. 
HLT/EMNLP, pp. 161-168, Vancouver, Canada.  
 
A. Lazarid?s, Y. Normandin, and R. Kuhn. 1996. ?Im-
proving Decision Trees for Acoustic Modeling?. 
Int. Conf. Spoken Lang. Proc. (ICSLP96), V. 2, pp. 
1053-1056, Philadelphia, Pennsylvania, USA. 
 
F. Och and H. Ney. 2004. ?The Alignment Template 
Approach to Statistical Machine Translation?. 
Comp. Linguistics, V. 30, Issue 4, pp. 417-449.  
 
Franz Josef Och. 2003. ?Minimum Error Rate Training 
for Statistical Machine  Translation?. Proc. ACL, 
Sapporo, Japan. 
 
K. Papineni, S. Roukos, T. Ward, and W.-J. Zhu. 2002. 
?BLEU: A method for automatic evaluation of ma-
chine translation?. Proc. ACL, pp. 311-318. 
 
C. Tillmann and T. Zhang. 2005. ?A Localized Predic-
tion Model for Statistical Machine Translation?. 
Proc. ACL.  
 
C. Tillmann. 2004. ?A Block Orientation Model for 
Statistical Machine Translation?. HLT/NAACL. 
 
S. Vogel, H. Ney, and C. Tillmann. 1996. ?HMM-Based 
Word Alignment in Statistical Translation?. 
COLING, pp. 836-841. 
32
Proceedings of NAACL HLT 2007, pages 508?515,
Rochester, NY, April 2007. c?2007 Association for Computational Linguistics
Statistical Phrase-based Post-editing
Michel Simard Cyril Goutte
Interactive Language Technologies
National Research Council of Canada
Gatineau, Canada, K1A 0R6
FirstName.LastName@nrc.gc.ca
Pierre Isabelle
Abstract
We propose to use a statistical phrase-
based machine translation system in a
post-editing task: the system takes as in-
put raw machine translation output (from
a commercial rule-based MT system), and
produces post-edited target-language text.
We report on experiments that were per-
formed on data collected in precisely such
a setting: pairs of raw MT output and
their manually post-edited versions. In our
evaluation, the output of our automatic
post-editing (APE) system is not only bet-
ter quality than the rule-based MT (both
in terms of the BLEU and TER metrics),
it is also better than the output of a state-
of-the-art phrase-based MT system used
in standalone translation mode. These re-
sults indicate that automatic post-editing
constitutes a simple and efficient way of
combining rule-based and statistical MT
technologies.
1 Introduction
The quality of machine translation (MT) is gener-
ally considered insufficient for use in the field with-
out a significant amount of human correction. In the
translation world, the term post-editing is often used
to refer to the process of manually correcting MT
output. While the conventional wisdom is that post-
editing MT is usually not cost-efficient compared to
full human translation, there appear to be situations
where it is appropriate and even profitable. Unfortu-
nately, there are few reports in the literature about
such experiences (but see Allen (2004) for exam-
ples).
One of the characteristics of the post-editing task,
as opposed to the revision of human translation for
example, is its partly repetitive nature. Most MT
systems invariably produce the same output when
confronted with the same input; in particular, this
means that they tend to make the same mistakes over
and over again, which the post-editors must correct
repeatedly. Batch corrections are sometimes pos-
sible when multiple occurrences of the same mis-
take appear in the same document, but when it is
repeated over several documents, or equivalently,
when the output of the same machine translation
system is handled by multiple post-editors, then the
opportunities for factoring corrections become much
more complex. MT users typically try to reduce
the post-editing load by customizing their MT sys-
tems. However, in Rule-based Machine Translation
(RBMT), which still constitutes the bulk of the cur-
rent commercial offering, customization is usually
restricted to the development of ?user dictionaries?.
Not only is this time-consuming and expensive, it
can only fix a subset of the MT system?s problems.
The advent of Statistical Machine Translation,
and most recently phrase-based approaches (PBMT,
see Marcu and Wong (2002), Koehn et al (2003))
into the commercial arena seems to hold the promise
of a solution to this problem: because the MT sys-
tem learns directly from existing translations, it can
be automatically customized to new domains and
tasks. However, the success of this operation cru-
508
cially depends on the amount of training data avail-
able. Moreover, the current state of the technology
is still insufficient for consistently producing human
readable translations.
This state of affairs has prompted some to ex-
amine the possibility of automating the post-editing
process itself, at least as far as ?repetitive errors? are
concerned. Allen and Hogan (2000) sketch the out-
line of such an automated post-editing (APE) sys-
tem, which would automatically learn post-editing
rules from a tri-parallel corpus of source, raw MT
and post-edited text. Elming (2006) suggests using
tranformation-based learning to automatically ac-
quire error-correcting rules from such data; however,
the proposed method only applies to lexical choice
errors. Knight and Chander (1994) also argue in fa-
vor of using a separate APE module, which is then
portable across multiple MT systems and language
pairs, and suggest that the post-editing task could be
performed using statistical machine translation tech-
niques. To the best of our knowledge, however, this
idea has never been implemented.
In this paper, we explore the idea of using a
PBMT system as an automated post-editor. The un-
derlying intuition is simple: if we collect a paral-
lel corpus of raw machine-translation output, along
with its human-post-edited counterpart, we can train
the system to translate from the former into the lat-
ter. In section 2, we present the case study that mo-
tivates our work and the associated data. In section
3, we describe the phrase-based post-editing model
that we use for improving the output of the auto-
matic translation system. In section 4, we illus-
trate this on a dataset of moderate size containing
job ads and their translation. With less than 500k
words of training material, the phrase-based MT
system already outperforms the rule-based MT base-
line. However, a phrase-based post-editing model
trained on the output of that baseline outperforms
both by a fairly consistent margin. The resulting
BLEU score increases by up to 50% (relative) and
the TER is cut by one third.
2 Background
2.1 Context
The Canadian government?s department of Human
Resources and Social Development (HRSDC) main-
tains a web site called Job Bank,1 where poten-
tial employers can post ads for open positions in
Canada. Over one million ads are posted on Job
Bank every year, totalling more than 180 million
words. By virtue of Canada?s Official Language Act,
HRSDC is under legal obligation to post all ads in
both French and English. In practice, this means
that ads submitted in English must be translated into
French, and vice-versa.
To address this task, the department has put to-
gether a complex setup, involving text databases,
translation memories, machine translation and hu-
man post-editing. Employers submit ads to the Job
Bank website by means of HTML forms containing
?free text? data fields. Some employers do period-
ical postings of identical ads; the department there-
fore maintains a database of previously posted ads,
along with their translations, and new ads are sys-
tematically checked against this database. The trans-
lation of one third of all ads posted on the Job Bank
is actually recuperated this way. Also, employers
will often post ads which, while not entirely identi-
cal, still contain identical sentences. The department
therefore also maintains a translation memory of in-
dividual sentence pairs from previously posted ads;
another third of all text is typically found verbatim
in this way.
The remaining text is submitted to machine trans-
lation, and the output is post-edited by human ex-
perts. Overall, only a third of all submitted text re-
quires human intervention. This is nevertheless very
labour-intensive, as the department tries to ensure
that ads are posted at most 24 hours after submis-
sion. The Job Bank currently employs as many as
20 post-editors working full-time, most of whom are
junior translators.
2.2 The Data
HRSDC kindly provided us with a sample of data
from the Job Bank. This corpus consists in a collec-
tion of parallel ?blocks? of textual data. Each block
contains three parts: the source language text, as
submitted by the employer, its machine-translation,
produced by a commercial rule-based MT system,
and its final post-edited version, as posted on the
website.
1http://www.jobbank.gc.ca
509
The entire corpus contains less than one million
words in each language. This corresponds to the
data processed in less than a week by the Job Bank.
Basic statistics are given in Table 1 (see Section 4.1).
Most blocks contain only one sentence, but some
blocks may contain many sentences. The longest
block contains 401 tokens over several sentences.
Overall, blocks are quite short: the median number
of tokens per source block is only 9 for French-to-
English and 7 for English-to-French. As a conse-
quence, no effort was made to segment the blocks
further for processing.
We evaluated the quality of the Machine Transla-
tion contained in the corpus using the Translation
Edit Rate (TER, cf. Snover et al (2006)). The
TER counts the number of edit operations, including
phrasal shifts, needed to change a hypothesis trans-
lation into an adequate and fluent sentence, and nor-
malised by the length of the final sentence. Note
that this closely corresponds to the post-editing op-
eration performed on the Job Bank application. This
motivates the choice of TER as the main metric in
our case, although we also report BLEU scores in
our experiments. Note that the emphasis of our work
is on reducing the post-edition effort, which is well
estimated by TER. It is not directly on quality so the
question of which metric better estimates translation
quality is not so relevant here.
The global TER (over all blocks) are 58.77%
for French-to-English and 53.33% for English-to-
French. This means that more than half the words
have to be post-edited in some way (delete / substi-
tute / insert / shift). This apparently harsh result is
somewhat mitigated by two factors.
First, the distribution of the block-based TER2
shows a large disparity in performance, cf. Figure 1.
About 12% of blocks have a TER higher than 100%:
this is because the TER normalises on the length of
the references, and if the raw MT output is longer
than its post-edited counterpart, then the number of
edit operations may be larger than that length.3 At
the other end of the spectrum, it is also clear that
many blocks have low TER. In fact more than 10%
2Contrary to BLEU or NIST, the TER naturally decomposes
into block-based scores.
3A side effect of the normalisation is that larger TER are
measured on small sentences, e.g. 3 errors for 2 reference
words.
Histogram of TER for rule?based MT
TER for rule?based MT
Fr
eq
ue
nc
y
0 50 100 150
0
10
00
20
00
30
00
40
00
Figure 1: Distribution of TER on 39005 blocks from
the French-English corpus (thresholded at 150%).
have a TER of 0. The global score therefore hides a
large range of performance.
The second factor is that the TER measures the
distance to an adequate and uent result. A high
TER does not mean that the raw MT output is not
understandable. However, many edit operations may
be needed to make it fluent.
3 Phrase-based Post-editing
Translation post-editing can be viewed as a simple
transformation process, which takes as input raw
target-language text coming from a MT system, and
produces as output target-language text in which ?er-
rors? have been corrected. While the automation
of this process can be envisaged in many differ-
ent ways, the task is not conceptually very differ-
ent from the translation task itself. Therefore, there
doesn?t seem to be any good reason why a machine
translation system could not handle the post-editing
task. In particular, given such data as described in
Section 2.2, the idea of using a statistical MT system
for post-editing is appealing. Portage is precisely
such a system, which we describe here.
Portage is a phrase-based, statistical machine
translation system, developed at the National Re-
search Council of Canada (NRC) (Sadat et al,
510
2005). A version of the Portage system is made
available by the NRC to Canadian universities for
research and education purposes. Like other SMT
systems, it learns to translate from existing parallel
corpora.
The system translates text in three main phases:
preprocessing of raw data into tokens; decoding to
produce one or more translation hypotheses; and
error-driven rescoring to choose the best final hy-
pothesis. For languages such as French and English,
the first of these phases (tokenization) is mostly a
straightforward process; we do not describe it any
further here.
Decoding is the central phase in SMT, involv-
ing a search for the hypotheses t that have high-
est probabilities of being translations of the cur-
rent source sentence s according to a model for
P (t|s). Portage implements a dynamic program-
ming beam search decoding algorithm similar to that
of Koehn (2004), in which translation hypotheses
are constructed by combining in various ways the
target-language part of phrase pairs whose source-
language part matches the input. These phrase pairs
come from large phrase tables constructed by col-
lecting matching pairs of contiguous text segments
from word-aligned bilingual corpora.
Portage?s model for P (t|s) is a log-linear com-
bination of four main components: one or more n-
gram target-language models, one or more phrase
translation models, a distortion (word-reordering)
model, and a sentence-length feature. The phrase-
based translation model is similar to that of Koehn,
with the exception that phrase probability estimates
P (s?|t?) are smoothed using the Good-Turing tech-
nique (Foster et al, 2006). The distortion model is
also very similar to Koehn?s, with the exception of a
final cost to account for sentence endings.
Feature function weights in the loglinear model
are set using Och?s minium error rate algorithm
(Och, 2003). This is essentially an iterative two-step
process: for a given set of source sentences, generate
n-best translation hypotheses, that are representative
of the entire decoding search space; then, apply a
variant of Powell?s algorithm to find weights that op-
timize the BLEU score over these hypotheses, com-
pared to reference translations. This process is re-
peated until the set of translations stabilizes, i.e. no
new translations are produced at the decoding step.
To improve raw output from decoding, Portage re-
lies on a rescoring strategy: given a list of n-best
translations from the decoder, the system reorders
this list, this time using a more elaborate loglinear
model, incorporating more feature functions, in ad-
dition to those of the decoding model: these typ-
ically include IBM-1 and IBM-2 model probabili-
ties (Brown et al, 1993) and an IBM-1-based fea-
ture function designed to detect whether any word
in one language appears to have been left without
satisfactory translation in the other language; all of
these feature functions can be used in both language
directions, i.e. source-to-target and target-to-source.
In the experiments reported in the next section,
the Portage system is used both as a translation and
as an APE system. While we can think of a number
of modifications to such a system to better adapt it
to the post-editing task (some of which are discussed
later on), we have done no such modifications to the
system. In fact, whether the system is used for trans-
lation or post-editing, we have used exactly the same
translation model configuration and training proce-
dure.
4 Evaluation
4.1 Data and experimental setting
The corpus described in section 2.2 is available for
two language pairs: English-to-French and French-
to-English.4 In each direction, each block is avail-
able in three versions (or slices): the original text
(or source), the output of the commercial rule-based
MT system (or baseline) and the final, post-edited
version (or reference).
In each direction (French-to-English and English-
to-French), we held out two subsets of approxi-
mately 1000 randomly picked blocks. The valida-
tion set is used for testing the impact of various high-
level choices such as pre-processing, or for obtain-
ing preliminary results based on which we setup new
experiments. The test set is used only once, in order
to obtain the final experimental results reported here.
The rest of the data constitutes the training set,
which is split in two. We sampled a subset of
1000 blocks as train-2, which is used for optimiz-
4Note that, in a post-editing context, translation direction is
crucially important. It is not possible to use the same corpus in
both directions.
511
English-to-French French-to-English
Corpus words: words:
blocks source baseline reference blocks source baseline reference
train-1 28577 310k 382k 410k 36005 485k 501k 456k
train-2 1000 11k 14k 14k 1000 13k 14k 12k
validation 881 10k 13k 13k 966 13k 14k 12k
test 899 10k 12k 13k 953 13k 13k 12k
Table 1: Data and split used in our experiments, (in thousand words). ?baseline? is the output of the com-
mercial rule-based MT system and ?reference? is the final, post-edited text.
ing the log-linear model parameters used for decod-
ing and rescoring. The rest is the train-1 set, used
for estimating IBM translation models, constructing
phrasetables and estimating a target language model.
The composition of the various sets is detailed in
Table 1. All data was tokenized and lowercased;
all evaluations were performed independent of case.
Note that the validation and test sets were originally
made out of 1000 blocks sampled randomly from
the data. These sets turned out to contain blocks
identical to blocks from the training sets. Consider-
ing that these would normally have been handled by
the translation memory component (see the HRSDC
workflow description in Section 2.1), we removed
those blocks for which the source part was already
found in the training set (in either train-1 or train-2),
hence their smaller sizes.
In order to check the sensitivity of experimental
results to the choice of the train-2 set, we did a
run of preliminary experiments using different sub-
sets of 1000 blocks. The experimental results were
nearly identical and highly consistent, showing that
the choice of a particular train-2 subset has no in-
fluence on our conclusions. In the experiments re-
ported below, we therefore use a single identical
train-2 set.
We initially performed two sets of experiments
on this data. The first was intended to compare the
performance of the Portage PBMT system as an al-
ternative to the commercial rule-based MT system
on this type of data. In these experiments, English-
to-French and French-to-English translation systems
were trained on the source and reference (manually
post-edited target language) slices of the training set.
In addition to the target language model estimated
on the train-1 data, we used an external contribution,
Language TER BLEU
English-to-French
Baseline 53.5 32.9
Portage translation 53.7 36.0
Baseline + Portage APE 47.3 41.6
French-to-English
Baseline 59.3 31.2
Portage translation 43.9 41.0
Baseline + Portage APE 41.0 44.9
Table 2: Experimental Results: For TER, lower (er-
ror) is better, while for BLEU, higher (score) is bet-
ter. Best results are in bold.
a trigram target language model trained on a fairly
large quantity of data from the Canadian Hansard.
The goal of the second set of experiments was to
assess the potential of the Portage technology in au-
tomatic post-editing mode. Again, we built systems
for both language directions, but this time using the
existing rule-based MT output as source and the ref-
erence as target. Apart from the use of different
source data, the training procedure and system con-
figurations of the translation and post-editing sys-
tems were in all points identical.
4.2 Experimental results
The results of both experiments are presented in Ta-
ble 2. Results are reported both in terms of the TER
and BLEU metrics; Baseline refers to the commer-
cial rule-based MT output.
The first observation from these results is that,
while the performance of Portage in translation
mode is approximately equivalent to that of the base-
line system when translating into French, its perfor-
mance is much better than the baseline when trans-
lating into English. Two factors possibly contribute
512
to this result: first, the fact that the baseline system
itself performs better when translating into French;
second, and possibly more importantly, the fact that
we had access to less training data for English-to-
French translation.
The second observation is that when Portage is
used in automatic post-editing mode, on top of the
baseline MT system, it achieves better quality than
either of the two translation systems used on its own.
This appears to be true regardless of the translation
direction or metric. This is an extremely interesting
result, especially in light of how little data was actu-
ally available to train the post-editing system.
One aspect of statistical MT systems is that, con-
trary to rule-based systems, their performance (usu-
ally) increases as more training data is available. In
order to quantify this effect in our setting, we have
computed learning curves by training the Portage
translation and Portage APE systems on subsets of
the training data of increasing sizes. We start with
as little as 1000 blocks, which corresponds to around
10-15k words.
Figure 2 (next page) compares the learning rates
of the two competing approaches (Portage transla-
tion vs. Portage APE). Both approaches display very
steady learning rates (note the logarithmic scale for
training data size). These graphs strongly suggest
that both systems would continue to improve given
more training data. The most impressive aspect is
how little data is necessary to improve upon the
baseline, especially when translating into English:
as little as 8000 blocks (around 100k words) for di-
rect translation and 2000 blocks (around 25k words)
for automatic post-editing. This suggests that such
a post-editing setup might be worth implementing
even for specialized domains with very small vol-
umes of data.
4.3 Extensions
Given the encouraging results of the Portage APE
approach in the above experiments, we were curi-
ous to see whether a Portage+Portage combination
might be as successful: after all, if Portage was good
at correcting some other system?s output, could it
not manage to correct the output of another Portage
translator?
We tested this in two settings. First, we actu-
ally use the output of the Portage translation sys-
Language TER BLEU
English-to-French
Portage Job Bank 53.7 36.0
+ Portage APE 53.7 36.2
Portage Hansard 76.9 13.0
+ Portage APE 64.6 26.2
French-to-English
Portage Job Bank 43.9 41.0
+ Portage APE 43.9 41.4
Portage Hansard 80.1 14.0
+ Portage APE 57.7 28.6
Table 3: Portage translation - Portage APE system
combination experimental results.
tem obtained above, i.e. trained on the same data.
In our second experiment, we use the output of
a Portage translator trained on different domain
data (the Canadian Hansard), but with much larger
amounts of training material (over 85 million words
per language). In both sets of experiments, the
Portage APE system was trained as previously, but
using Portage translations of the Job Bank data as
input text.
The results of both experiments are presented in
Table 3. The first observation in these results is that
there is nothing to be gained from post-editing when
both the translation and APE systems are trained on
the same data sets (Portage Job Bank + Portage APE
experiments). In other words, the translation system
is apparently already making the best possible use of
the training data, and additional layers do not help
(but nor do they hurt, interestingly).
However, when the translation system has been
trained using distinct data (Portage Hansard +
Portage APE experiments), post-editing makes a
large difference, comparable to that observed with
the rule-based MT output provided with the Job
Bank data. In this case, however, the Portage trans-
lation system behaves very poorly in spite of the im-
portant size of the training set for this system, much
worse in fact than the ?baseline? system. This high-
lights the fact that both the Job Bank and Hansard
data are very much domain-specific, and that access
to appropriate training material is crucial for phrase-
based translation technology.
In this context, combining two phrase-based sys-
513
1000 2000 5000 10000 20000
40
45
50
55
60
TER learning curves
Training set size
TE
R
to English
to French
Post?edition
Translation
1000 2000 5000 10000 20000
0.
30
0.
35
0.
40
0.
45
BLEU learning curves
Training set size
BL
EU
to English
to French
Post?edition
Translation
Figure 2: TER and BLEU scores of the phrase-based post-editing models as the amount of training data
increases (log scale). The horizontal lines correspond to the performance of the baseline system (rule-based
translation).
tems as done here can be seen as a way of adapting
an existing MT system to a new text domain; the
APE system then acts as an ?adapter?, so to speak.
Note however that, in our experiments, this setup
doesn?t perform as well as a single Portage transla-
tion system, trained directly and exclusively on the
Job Bank data.
Such an adaptation strategy should be contrasted
with one in which the translation models of the
old and new domains are ?merged? to create a new
translation system. As mentioned earlier, Portage
allows using multiple phrase translation tables and
language models concurrently. For example, in the
current context, we can extract phrase tables and lan-
guage models from the Job Bank data, as when train-
ing the ?Portage Job Bank? translation system, and
then build a Portage translation model using both the
Hansard and Job Bank model components. Loglin-
ear model parameters are then optimized on the Job
Bank data, so as to find the model weights that best
fit the new domain.
In a straightforward implementation of this idea,
we obtained performances almost identical to those
of the Portage translation system trained solely on
Job Bank data. Upon closer examination of the
model parameters, we observed that Hansard model
components (language model, phrase tables, IBM
translation models) were systematically attributed
negligeable weights. Again, the amount of training
material for the new domain may be critical in chos-
ing between alternative adaptation mechanisms.
5 Conclusions and Future Work
We have proposed using a phrase-based MT sys-
tem to automatically post-edit the output of an-
other MT system, and have tested this idea with
the Portage MT system on the Job Bank data set, a
corpus of manually post-edited French-English ma-
chine translations. In our experiments, not only does
phrase-based APE significantly improve the quality
of the output translations, this approach outperforms
a standalone phrase-based translation system.
While these results are very encouraging, the
learning curves of Figure 2 suggest that the output
quality of the PBMT systems increases faster than
that of the APE systems as more data is used for
training. So while the combination strategy clearly
performs better with limited amounts of training
data, there is reason to believe that, given sufficient
training data, it would eventually be outperformed
514
by a direct phrase-based translation strategy. Of
course, this remains to be verified empirically, some-
thing which will obviously require more data than is
currently available to us. But this sort of behavior
is expectable: while both types of system improve
as more training data is used, inevitably some de-
tails of the source text will be lost by the front-end
MT system, which the APE system will never be
able to retrieve.5 Ultimately, the APE system will
be weighted down by the inherent limitations of the
front-end MT system.
One way around this problem would be to modify
the APE system so that it not only uses the base-
line MT output, but also the source-language input.
In the Portage system, this could be achieved, for
example, by introducing feature functions into the
log-linear model that relate target-language phrases
with the source-language text. This is one research
avenue that we are currently exploring.
Alternatively, we could combine these two in-
puts differently within Portage: for example, use
the source-language text as the primary input, and
use the raw MT output as a secondary source. In
this perspective, if we have multiple MT systems
available, nothing precludes using all of them as
providers of secondary inputs. In such a setting, the
phrase-based system becomes a sort of combination
MT system. We intend to explore such alternatives
in the near future as well.
Acknowledgements
The work reported here was part of a collaboration
between the National Research Council of Canada
and the department of Human Resources and Social
Development Canada. Special thanks go to Souad
Benayyoub, Jean-Fre?de?ric Hu?bsch and the rest of
the Job Bank team at HRSDC for preparing data that
was essential to this project.
References
Jeffrey Allen and Christofer Hogan. 2000. Towardthe development of a post-editing module for Ma-chine Translation raw output: a new productivity tool
for processing controlled language. In Third Inter-
5As a trivial example, imagine an MT system that ?deletes?
out-of-vocabulary words.
national Controlled Language Applications Workshop(CLAW2000), Washington, USA.
Jeffrey Allen. 2004. Case study: Implementing MT forthe translation of pre-sales marketing and post-salessoftware deployment documentation. In Proceedingsof AMTA-2004, pages 1?6, Washington, USA.
Peter F Brown, Stephen A Della Pietra, Vincent J DellaPietra, and Robert L Mercer. 1993. The Mathematicsof Statistical Machine Translation: Parameter Estima-
tion. Computational Linguistics, 19(2):263?311.
Jakob Elming. 2006. Transformation-based correctionsof rule-based MT. In Proceedings of the EAMT 11thAnnual Conference, Oslo, Norway.
George Foster, Roland Kuhn, and Howard Johnson.
2006. Phrasetable Smoothing for Statistical MachineTranslation. In Proceedings of EMNLP 2006, pages53?61, Sydney, Australia.
Kevin Knight and Ishwar Chander. 1994. Automated
Postediting of Documents. In Proceedings of NationalConference on Artificial Intelligence, pages 779?784,Seattle, USA.
Philipp Koehn, Franz J. Och, and Daniel Marcu. 2003.
Statistical Phrase-Based Translation. In Proceed-ings of HLT-NAACL 2003, pages 127?133, Edmonton,Canada.
Philipp Koehn. 2004. Pharaoh: a Beam Search De-
coder for Phrase-Based Statistical Machine Transla-tion Models. In Proceedings of AMTA 2004, pages115?124, Washington, USA.
Daniel Marcu and William Wong. 2002. A Phrase-
Based, Joint Probability Model for Statistical Ma-chine Translation. In Proceedings of EMNLP 2002,Philadelphia, USA.
Franz Josef Och. 2003. Minimum error rate trainingin Statistical Machine Translation. In Proceedings ofACL-2003, pages 160?167, Sapporo, Japan.
Fatiha Sadat, Howard Johnson, Akakpo Agbago, George
Foster, Roland Kuhn, Joel Martin, and Aaron Tikuisis.2005. PORTAGE: A Phrase-Based Machine Trans-lation System. In Proceedings of the ACL Workshopon Building and Using Parallel Texts, pages 129?132,
Ann Arbor, USA.
Matthew Snover, Bonnie Dorr, Richard Schwartz, Lin-nea Micciulla, and John Makhoul. 2006. A Study
of Translation Edit Rate with Targeted Human An-notation. In Proceedings of AMTA-2006, Cambridge,USA.
515
Proceedings of the Workshop on Statistical Machine Translation, pages 134?137,
New York City, June 2006. c?2006 Association for Computational Linguistics
PORTAGE: with Smoothed Phrase Tables
and Segment Choice Models
Howard Johnson
National Research Council
Institute for Information Technology
Interactive Information
1200 Montreal Road
Ottawa, ON, Canada K1A 0R6
Howard.Johnson@cnrc-nrc.gc.ca
Fatiha Sadat, George Foster, Roland Kuhn,
Michel Simard, Eric Joanis and Samuel Larkin
National Research Council
Institute for Information Technology
Interactive Language Technologies
101 St-Jean-Bosco Street
Gatineau, QC, Canada K1A 0R6
firstname.lastname@cnrc-nrc.gc.ca
Abstract
Improvements to Portage and its partici-
pation in the shared task of NAACL 2006
Workshop on Statistical Machine Trans-
lation are described. Promising ideas in
phrase table smoothing and global dis-
tortion using feature-rich models are dis-
cussed as well as numerous improvements
in the software base.
1 Introduction
The statistical machine translation system Portage is
participating in the NAACL 2006 Workshop on Sta-
tistical Machine Translation. This is a good opportu-
nity to do benchmarking against a publicly available
data set and explore the benefits of a number of re-
cently added features.
Section 2 describes the changes that have been
made to Portage in the past year that affect the par-
ticipation in the 2006 shared task. Section 3 outlines
the methods employed for this task and extensions
of it. In Section 4 the results are summarized in tab-
ular form. Following these, there is a conclusions
section that highlights what can be gleaned of value
from these results.
2 Portage
Because this is the second participation of Portage in
such a shared task, a description of the base system
can be found elsewhere (Sadat et al 2005). Briefly,
Portage is a research vehicle and development pro-
totype system exploiting the state-of-the-art in sta-
tistical machine translation (SMT). It uses a custom
built decoder followed by a rescoring module that
adjusts weights based on a number of features de-
fined on the source sentence. We will devote space
to discussing changes made since the 2005 shared
task.
2.1 Phrase-Table Smoothing
Phrase-based SMT relies on conditional distribu-
tions p(s|t) and p(t|s) that are derived from the joint
frequencies c(s, t) of source/target phrase pairs ob-
served in an aligned parallel corpus. Traditionally,
relative-frequency estimation is used to derive con-
ditional distributions, ie p(s|t) = c(s, t)/
?
s c(s, t).
However, relative-frequency estimation has the
well-known problem of favouring rare events. For
instance, any phrase pair whose constituents occur
only once in the corpus will be assigned a probabil-
ity of 1, almost certainly higher than the probabili-
ties of pairs for which much more evidence exists.
During translation, rare pairs can directly compete
with overlapping frequent pairs, so overestimating
their probabilities can significantly degrade perfor-
mance.
To address this problem, we implemented two
simple smoothing strategies. The first is based on
the Good-Turing technique as described in (Church
and Gale, 1991). This replaces each observed joint
frequency c with cg = (c + 1)nc+1/nc, where nc
is the number of distinct pairs with frequency c
(smoothed for large c). It also assigns a total count
mass of n1 to unseen pairs, which we distributed
in proportion to the frequency of each conditioning
134
phrase. The resulting estimates are:
pg(s|t) =
cg(s, t)
?
s cg(s, t) + p(t)n1
,
where p(t) = c(t)/
?
t c(t). The estimates for
pg(t|s) are analogous.
The second strategy is Kneser-Ney smoothing
(Kneser and Ney, 1995), using the interpolated vari-
ant described in (Chen and Goodman., 1998):1
pk(s|t) =
c(s, t) ? D + D n1+(?, t) pk(s)
?
s c(s, t)
where D = n1/(n1 + 2n2), n1+(?, t) is the num-
ber of distinct phrases s with which t co-occurs, and
pk(s) = n1+(s, ?)/
?
s n1+(s, ?), with n1+(s, ?)
analogous to n1+(?, t).
Our approach to phrase-table smoothing contrasts
to previous work (Zens and Ney, 2004) in which
smoothed phrase probabilities are constructed from
word-pair probabilities and combined in a log-linear
model with an unsmoothed phrase-table. We believe
the two approaches are complementary, so a combi-
nation of both would be worth exploring in future
work.
2.2 Feature-Rich DT-based distortion
In a recent paper (Kuhn et al 2006), we presented a
new class of probabilistic ?Segment ChoiceModels?
(SCMs) for distortion in phrase-based systems. In
some situations, SCMs will assign a better distortion
score to a drastic reordering of the source sentence
than to no reordering; in this, SCMs differ from the
conventional penalty-based distortion, which always
favours less rather than more distortion.
We developed a particular kind of SCM based on
decision trees (DTs) containing both questions of a
positional type (e.g., questions about the distance
of a given phrase from the beginning of the source
sentence or from the previously translated phrase)
and word-based questions (e.g., questions about the
presence or absence of given words in a specified
phrase).
The DTs are grown on a corpus consisting of
segment-aligned bilingual sentence pairs. This
1As for Good-Turing smoothing, this formula applies only
to pairs s, t for which c(s, t) > 0, since these are the only ones
considered by the decoder.
segment-aligned corpus is obtained by training a
phrase translation model on a large bilingual cor-
pus and then using it (in conjunction with a distor-
tion penalty) to carry out alignments between the
phrases in the source-language sentence and those
in the corresponding target-language sentence in a
second bilingual corpus. Typically, the first corpus
(on which the phrase translation model is trained) is
the same as the second corpus (on which alignment
is carried out). To avoid overfitting, the alignment
algorithm is leave-one-out: statistics derived from
a particular sentence pair are not used to align that
sentence pair.
Note that the experiments reported in (Kuhn et
al, 2006) focused on translation of Chinese into En-
glish. The interest of the experiments reported here
onWMT data was to see if the feature-rich DT-based
distortion model could be useful for MT between
other language pairs.
3 Application to the Shared Task: Methods
3.1 Restricted Resource Exercise
The first exercise that was done is to replicate the
conditions of 2005 as closely as possible to see the
effects of one year of research and development.
The second exercise was to replicate all three of
these translation exercises using the 2006 language
model, and to do the three exercises of translat-
ing out of English into French, Spanish, and Ger-
man. This was our baseline for other studies. A
third exercise involved modifying the generation
of the phrase-table to incorporate our Good-Turing
smoothing. All six language pairs were re-processed
with these phrase-tables. The improvement in the
results on the devtest set were compelling. This be-
came the baseline for further work. A fourth ex-
ercise involved replacing penalty-based distortion
modelling with the feature-rich decision-tree based
distortion modelling described above. A fifth ex-
ercise involved the use of a Kneser-Ney phrase-
table smoothing algorithm as an alternative to Good-
Turing.
For all of these exercises, 1-best results after de-
coding were calculated as well as rescoring on 1000-
best lists of results using 12 feature functions (13
in the case of decision-tree based distortion mod-
elling). The results submitted for the shared task
135
were the results of the third and fourth exercises
where rescoring had been applied.
3.2 Open Resource Exercise
Our goal in this exercise was to conduct a com-
parative study using additional training data for the
French-English shared task. Results of WPT 2005
showed an improvement of at least 0.3 BLEU point
when exploiting different resources for the French-
English pair of languages. In addition to the training
resources used in WPT 2005 for the French-English
task, i.e. Europarl and Hansard, we used a bilingual
dictionary, Le Grand Dictionnaire Terminologique
(GDT) 2 to train translation models and the English
side of the UN parallel corpus (LDC2004E13) to
train an English language model. Integrating termi-
nological lexicons into a statistical machine transla-
tion engine is not a straightforward operation, since
we cannot expect them to come with attached prob-
abilities. The approach we took consists on view-
ing all translation candidates of each source term or
phrase as equiprobable (Sadat et al 2006).
In total, the data used in this second part of our
contribution to WMT 2006 is described as follows:
(1) A set of 688,031 sentences in French and En-
glish extracted from the Europarl parallel corpus (2)
A set of 6,056,014 sentences in French and English
extracted from the Hansard parallel corpus, the offi-
cial record of Canada?s parliamentary debates. (3) A
set of 701,709 sentences in French and English ex-
tracted from the bilingual dictionary GDT. (4) Lan-
guage models were trained on the French and En-
glish parts of the Europarl and Hansard. We used
the provided Europarl corpus while omitting data
from Q4/2000 (October-December), since it is re-
served for development and test data. (5) An addi-
tional English language model was trained on 128
million words of the UN Parallel corpus.
For the supplied Europarl corpora, we relied on
the existing segmentation and tokenization, except
for French, which we manipulated slightly to bring
into line with our existing conventions (e.g., convert-
ing l ? an into l? an, aujourd ? hui into aujourd?hui).
For the Hansard corpus used to supplement our
French-English resources, we used our own align-
ment based on Moore?s algorithm, segmentation,
2http://www.granddictionnaire.com/
and tokenization procedures. English preprocessing
simply included lower-casing, separating punctua-
tion from words and splitting off ?s.
4 Results
The results are shown in Table 1. The numbers
shown are BLEU scores. The MC rows correspond
to the multi-corpora results described in the open re-
source exercise section above. All other rows are
from the restricted resource exercise.
The devtest results are the scores computed be-
fore the shared-task submission and were used to
drive the choice of direction of the research. The
test results were computed after the shared-task sub-
mission and serve for validation of the conclusions.
We believe that our use of multiple training cor-
pora as well as our re-tokenization for French and
an enhanced language model resulted in our overall
success in the English-French translation track. The
results for the in-domain test data puts our group at
the top of the ranking table drawn by the organizers
(first on Adequacy and fluency and third on BLEU
scores).
5 Conclusion
Benchmarking with same language model and pa-
rameters as WPT05 reproduces the results with a
tiny improvement. The larger language model used
in 2006 for English yields about half a BLEU. Good-
Turing phrase table smoothing yields roughly half
a BLEU point. Kneser-Ney phrase table smooth-
ing yields between a third and half a BLEU point
more than Good-Turing. Decision tree based distor-
tion yields a small improvement for the devtest set
when rescoring was not used but failed to show im-
provement on the test set.
In summary, the results from phrase-table
smoothing are extremely encouraging. On the other
hand, the feature-rich decision tree distortion mod-
elling requires additional work before it provides a
good pay-back. Fortunately we have some encour-
aging avenues under investigation. Clearly there is
more work needed for both of these areas.
Acknowledgements
We wish to thank Aaron Tikuisis and Denis Yuen
for important contributions to the Portage code base
136
Table 1: Restricted and open resource results
fr ?? en es ?? en de ?? en en ?? fr en ?? es en ?? de
devtest: with rescoring
WPT05 29.32 29.08 23.21
LM-2005 29.30 29.21 23.41
LM-2006 29.88 29.54 23.94 30.43 28.81 17.33
GT-PTS 30.35 29.84 24.60 30.89 29.54 17.62
GT-PTS+DT-dist 30.09 29.44 24.62 31.06 29.46 17.84
KN-PTS 30.55 30.12 24.66 31.28 29.90 17.78
MC WPT05 29.63
MC 30.09 31.30
MC+GT-PTS 30.75 31.37
devtest: 1-best after decoding
LM-2006 28.59 28.45 23.22 29.22 28.30 16.94
GT-PTS 29.23 28.91 23.67 30.07 28.86 17.32
GT-PTS+DT-dist 29.48 29.07 23.50 30.22 29.46 17.42
KN-PTS 29.77 29.76 23.27 30.73 29.62 17.78
MC WPT05 28.71
MC 29.63 31.01
MC+GT-PTS 29.90 31.22
test: with rescoring
LM-2006 26.64 28.43 21.33 28.06 28.01 15.19
GT-PTS 27.19 28.95 21.91 28.60 28.83 15.38
GT-PTS+DT-dist 26.84 28.56 21.84 28.56 28.59 15.45
KN-PTS 27.40 29.07 21.98 28.96 29.06 15.64
MC 26.95 29.12
MC+GT-PTS 27.10 29.46
test: 1-best after decoding
LM-2006 25.35 27.25 20.46 27.20 27.18 14.60
GT-PTS 25.95 28.07 21.06 27.85 27.96 15.05
GT-PTS+DT-dist 25.86 28.04 20.74 27.85 27.97 14.92
KN-PTS 26.83 28.66 21.36 28.62 28.71 15.42
MC 26.70 28.74
MC+GT-PTS 26.81 29.03
and the OQLF (Office Que?be?cois de la Langue
Franc?aise) for permission to use the GDT.
References
S. F. Chen and J. T. Goodman. 1998. An empirical
study of smoothing techniques for language modeling.
Technical Report TR-10-98, Computer Science Group,
Harvard University.
K. Church and W. Gale. 1991. A comparison of the en-
hanced Good-Turing and deleted estimation methods
for estimating probabilities of English bigrams. Com-
puter speech and language, 5(1):19?54.
R. Kneser and H. Ney. 1995. Improved backing-off for
m-gram language modeling. In Proc. International
Conference on Acoustics, Speech, and Signal Process-
ing (ICASSP) 1995, pages 181?184, Detroit, Michi-
gan. IEEE.
R. Kuhn, D. Yuen, M. Simard, G. Foster, P. Paul, E. Joa-
nis and J. H. Johnson. 2006. Segment Choice Models:
Feature-Rich Models for Global Distortion in Statisti-
cal Machine Translation (accepted for publication in
HLT-NAACL conference, to be held June 2006).
F. Sadat, J. H. Johnson, A. Agbago, G. Foster, R. Kuhn,
J. Martin and A. Tikuisis. 2005. PORTAGE: A
Phrase-based Machine Translation System In Proc.
ACL 2005 Workshop on building and using parallel
texts. Ann Arbor, Michigan.
F. Sadat, G. Foster and R. Kuhn. 2006. Syste`me de tra-
duction automatique statistique combinant diffe?rentes
ressources. In Proc. TALN 2006 (Traitement Automa-
tique des Langues Naturelles). Leuven, Belgium, April
10-13, 2006.
R. Zens and H. Ney. 2004. Improvements in phrase-
based statistical machine translation. In Proc. Human
Language Technology Conference / North American
Chapter of the ACL, Boston, May.
137
Proceedings of the Second Workshop on Statistical Machine Translation, pages 185?188,
Prague, June 2007. c?2007 Association for Computational Linguistics
NRC?s PORTAGE system for WMT 2007
Nicola Ueffing, Michel Simard, Samuel Larkin
Interactive Language Technologies Group
National Research Council Canada
Gatineau, Que?bec, Canada
firstname.lastname@nrc.gc.ca
Howard Johnson
Interactive Information Group
National Research Council Canada
Ottawa, Ontario, Canada
Howard.Johnson@nrc.gc.ca
Abstract
We present the PORTAGE statistical
machine translation system which par-
ticipated in the shared task of the ACL
2007 Second Workshop on Statistical
Machine Translation. The focus of this
description is on improvements which
were incorporated into the system over
the last year. These include adapted lan-
guage models, phrase table pruning, an
IBM1-based decoder feature, and rescor-
ing with posterior probabilities.
1 Introduction
The statistical machine translation (SMT) sys-
tem PORTAGE was developed at the National
Research Council Canada and has recently been
made available to Canadian universities and
research institutions. It is a state-of-the-art
phrase-based SMT system. We will shortly de-
scribe its basics in this paper and then high-
light the new methods which we incorporated
since our participation in the WMT 2006 shared
task. These include new scoring methods for
phrase pairs, pruning of phrase tables based
on significance, a higher-order language model,
adapted language models, and several new de-
coder and rescoring models. PORTAGE was
also used in a joint system developed in coop-
eration with Systran. The interested reader is
referred to (Simard et al, 2007).
Throughout this paper, let sJ1 := s1 . . . sJ de-
note a source sentence of length J , tI1 := t1 . . . tI
a target sentence of length I, and s? and t? phrases
in source and target language, respectively.
2 Baseline
As baseline for our experiments, we used a ver-
sion of PORTAGE corresponding to its state at
the time of the WMT 2006 shared task. We pro-
vide a basic description of this system here; for
more details see (Johnson et al, 2006).
PORTAGE implements a two-stage transla-
tion process: First, the decoder generates N -
best lists, using a basic set of models which are
then rescored with additional models in a sec-
ond step. In the baseline system, the decoder
uses the following models (or feature functions):
? one or several phrase table(s), which model
the translation direction p(s? | t?). They are
generated from the training corpus via the
?diag-and? method (Koehn et al, 2003)
and smoothed using Kneser-Ney smooth-
ing (Foster et al, 2006),
? one or several n-gram language model(s)
trained with the SRILM toolkit (Stolcke,
2002); in the baseline experiments reported
here, we used a trigram model,
? a distortion model which assigns a penalty
based on the number of source words which
are skipped when generating a new target
phrase,
? a word penalty.
These different models are combined log-
linearly. Their weights are optimized
w.r.t. BLEU score using the algorithm de-
scribed in (Och, 2003). This is done on the
provided development corpus. The search
algorithm implemented in the decoder is a
dynamic-programming beam-search algorithm.
185
After the decoding step, rescoring with addi-
tional models is performed. The baseline system
generates a 1,000-best list of alternative trans-
lations for each source sentence. These lists
are rescored with the different models described
above, a character penalty, and three different
features based on IBM Models 1 and 2 (Brown
et al, 1993) calculated in both translation di-
rections. The weights of these additional models
and of the decoder models are again optimized
to maximize BLEU score.
Note that we did not use the decision-tree-
based distortion models described in (Johnson
et al, 2006) here because they did not improve
translation quality.
In the following subsections, we will describe
the new models added to the system for our
WMT 2007 submissions.
3 Improvements in PORTAGE
3.1 Phrase translation models
Whereas the phrase tables used in the baseline
system contain only one score for each phrase
pair, namely conditional probabilities calculated
using Kneser-Ney smoothing, our current sys-
tem combines seven different phrase scores.
First, we used several types of phrase table
smoothing in the WMT 2007 system because
this proved helpful on other translation tasks:
relative frequency estimates, Kneser-Ney- and
Zens-Ney-smoothed probabilities (Foster et al,
2006). Furthermore, we added normalized joint
probability estimates to the phrase translation
model. The other three scores will be explained
at the end of this subsection.
We pruned the generated phrase tables fol-
lowing the method introduced in (Johnson et
al., 2007). This approach considers all phrase
pairs (s?, t?) in the phrase table. The count C(s?, t?)
of all sentence pairs containing (s?, t?) is deter-
mined, as well as the count of all source/target
sentences containing s?/t?. Using these counts,
Fisher?s exact test is carried out to calculate
the significance of the phrase pair. The phrase
tables are then pruned based on the p-value.
Phrase pairs with low significance, i.e. which are
only weakly supported by the training data, are
pruned. This reduces the size of the phrase ta-
bles to 8-16% on the different language pairs.
See (Johnson et al, 2007) for details.
Three additional phrase scores were derived
from information on which this pruning is based:
? the significance level (or p-value),
? the number C(s?, t?) of sentence pairs con-
taining the phrase pair, normalized by the
number of source sentences containing s?,
? C(s?, t?), normalized by the number of target
sentences containing t?.
For our submissions, we used the last three
phrase scores only when translating the Eu-
roParl data. Initial experiments showed that
they do not improve translation quality on the
News Commentary data. Apart from this, the
systems for both domains are identical.
3.2 Adapted language models
Concerning the language models, we made two
changes to our system since WMT 2006. First,
we replaced the trigram language model by a 4-
gram model trained on the WMT 2007 data. We
also investigated the use of a 5-gram, but that
did not improve translation quality. Second,
we included adapted language models which
are specific to the development and test cor-
pora. For each development or test corpus, we
built this language model using information re-
trieval1 to find relevant sentences in the train-
ing data. To this end, we merged the train-
ing corpora for EuroParl and News Commen-
tary. The source sentences from the develop-
ment or test corpus served as individual queries
to find relevant training sentence pairs. For
each source sentence, we retrieved 10 sentence
pairs from the training data and used their tar-
get sides as language model training data. On
this small corpus, we trained a trigram lan-
guage model, again using the SRILM toolkit.
The feature function weights in the decoder and
the rescoring model were optimized using the
adapted language model for the development
corpus. When translating the test corpus, we
kept these weights, but replaced the adapted
1We used the lemur toolkit for querying, see
http://www.lemurproject.org/
186
language model by that specific to the test cor-
pus.
3.3 New decoder and rescoring features
We integrated several new decoder and rescoring
features into PORTAGE. During decoding, the
system now makes use of a feature based on IBM
Model 1. This feature calculates the probability
of the (partial) translation over the source sen-
tence, using an IBM1 translation model in the
direction p(tI1 | sJ1 ).
In the rescoring process, we additionally in-
cluded several types of posterior probabilities.
One is the posterior probability of the sentence
length over the N -best list for this source sen-
tence. The others are determined on the level
of words, phrases, and n-grams, and then com-
bined into a value for the whole sentence. All
posterior probabilities are calculated over theN -
best list, using the sentence probabilities which
the baseline system assigns to the translation
hypotheses. For details on the posterior prob-
abilities, see (Ueffing and Ney, 2007; Zens and
Ney, 2006). This year, we increased the length
of the N -best lists from 1,000 to 5,000.
3.4 Post-processing
For truecasing the translation output, we used
the model described in (Agbago et al, 2005).
This model uses a combination of statisti-
cal components, including an n-gram language
model, a case mapping model, and a special-
ized language model for unknown words. The
language model is a 5-gram model trained on
the WMT 2007 data. The detokenizer which we
used is the one provided for WMT 2007.
4 Experimental results
We submitted results for six of the translation
directions of the shared task: French ? English,
German ? English, and Spanish ? English.
Table 1 shows the improvements result-
ing from incorporating new techniques into
PORTAGE on the Spanish ? English EuroParl
task. The baseline system is the one described
in section 2. Trained on the 2007 training cor-
pora, this yields a BLEU score of 30.48. Adding
the new phrase scores introduced in section 3.1
yields a slight improvement in translation qual-
ity. This improvement by itself is not signifi-
cant, but we observed it consistently across all
evaluation metrics and across the different devel-
opment and test corpora. Increasing the order
of the language model and adding an adapted
language model specific to the translation input
(see section 3.2) improves the BLEU score by
0.6 points. This is the biggest gain we observe
from introducing a new method. The incorpora-
tion of the IBM1-based decoder feature causes
a slight drop in translation quality. This sur-
prised us because we found this feature to be
very helpful on the NIST Chinese ? English
translation task. Adding the posterior proba-
bilities presented in section 3.3 in rescoring and
increasing the length of the N -best lists yielded
a small, but consistent gain in translation qual-
ity. The overall improvement compared to last
year?s system is around 1 BLEU point. The gain
achieved from introducing the new methods by
themselves are relatively small, but they add up.
Table 2 shows results on all six language pairs
we translated for the shared task. The trans-
lation quality achieved on the 2007 test set is
similar to that on the 2006 test set. The system
clearly performs better on the EuroParl domain
than on News Commentary.
Table 2: Translation quality in terms of
BLEU[%] and NIST score on all tasks. True-
cased and detokenized translation output.
test2006 test2007
task BLEU NIST BLEU NIST
Eu D?E 25.27 6.82 26.02 6.91
E?D 19.36 5.86 18.94 5.71
S?E 31.54 7.55 32.09 7.67
E?S 30.94 7.39 30.92 7.41
F?E 30.90 7.51 31.90 7.68
E?F 30.08 7.26 30.06 7.26
NC D?E 20.23 6.19 23.17 7.10
E?D 13.84 5.38 16.30 5.95
S?E 31.07 7.68 31.08 8.11
E?S 30.79 7.73 32.56 8.25
F?E 24.97 6.78 26.84 7.47
E?F 24.91 6.79 26.60 7.24
187
Table 1: Effect of integrating new models and methods into the PORTAGE system. Translation
quality in terms of BLEU and NIST score, WER and PER on the EuroParl Spanish?English 2006
test set. True-cased and detokenized translation output. Best results printed in boldface.
system BLEU[%] NIST WER[%] PER[%]
baseline 30.48 7.44 58.62 42.74
+ new phrase table features 30.66 7.48 58.25 42.46
+ 4-gram LM + adapted LM 31.26 7.53 57.93 42.26
+ IBM1-based decoder feature 31.18 7.51 58.13 42.53
+ refined rescoring 31.54 7.55 57.81 42.24
5 Conclusion
We presented the PORTAGE system with which
we translated six language pairs in the WMT
2007 shared task. Starting from the state of
the system during the WMT 2006 evaluation,
we analyzed the contribution of new methods
which were incorporated over the last year in
detail. Our experiments showed that most of
these changes result in (small) improvements in
translation quality. In total, we gain about 1
BLEU point compared to last year?s system.
6 Acknowledgments
Our thanks go to the PORTAGE team at NRC
for their contributions and valuable feedback.
References
A. Agbago, R. Kuhn, and G. Foster. 2005. True-
casing for the Portage system. In Recent Ad-
vances in Natural Language Processing, pages 21?
24, Borovets, Bulgaria, September.
P. F. Brown, S. A. Della Pietra, V. J. Della Pietra,
and R. L. Mercer. 1993. The mathematics of sta-
tistical machine translation: Parameter estima-
tion. Computational Linguistics, 19(2):263?311,
June.
G. Foster, R. Kuhn, and J. H. Johnson. 2006.
Phrasetable smoothing for statistical machine
translation. In Proc. of the Conf. on Empir-
ical Methods for Natural Language Processing
(EMNLP), pages 53?61, Sydney, Australia, July.
J. H. Johnson, F. Sadat, G. Foster, R. Kuhn,
M. Simard, E. Joanis, and S. Larkin. 2006.
Portage: with smoothed phrase tables and seg-
ment choice models. In Proc. HLT/NAACL
Workshop on Statistical Machine Translation
(WMT), pages 134?137, New York, NY, June.
H. Johnson, J. Martin, G. Foster, and R. Kuhn.
2007. Improving translation quality by discard-
ing most of the phrasetable. In Proc. of the
Conf. on Empirical Methods for Natural Language
Processing and Conf. on Computational Natural
Language Learning (EMNLP-CoNLL), to appear,
Prague, Czech Republic, June.
P. Koehn, F. J. Och, and D. Marcu. 2003. Statistical
phrase-based translation. In Proc. of the Human
Language Technology Conf. (HLT-NAACL), pages
127?133, Edmonton, Canada, May/June.
F. J. Och. 2003. Minimum error rate training in sta-
tistical machine translation. In Proc. of the 41st
Annual Meeting of the Association for Computa-
tional Linguistics (ACL), pages 160?167, Sapporo,
Japan, July.
M. Simard, J. Senellart, P. Isabelle, R. Kuhn,
J. Stephan, and N. Ueffing. 2007. Knowledge-
based translation with statistical phrase-based
post-editing. In Proc. ACL Second Workshop on
Statistical Machine Translation (WMT), to ap-
pear, Prague, Czech Republic, June.
A. Stolcke. 2002. SRILM ? an extensible language
modeling toolkit. In Proc. Int. Conf. on Spoken
Language Processing (ICSLP), volume 2, pages
901?904, Denver, CO.
N. Ueffing and H. Ney. 2007. Word-level confi-
dence estimation for machine translation. Com-
putational Linguistics, 33(1):9?40, March.
R. Zens and H. Ney. 2006. N-gram posterior
probabilities for statistical machine translation.
In Proc. HLT/NAACL Workshop on Statistical
Machine Translation (WMT), pages 72?77, New
York, NY, June.
188
Book Reviews
Bitext Alignment
? 2012 Association for Computational Linguistics
Jo?rg Tiedemann
(Uppsala University)
Morgan & Claypool (Synthesis Lectures on Human Language Technologies, edited by
Graeme Hirst, volume 14), 2011, 153 pp; paperbound, ISBN 978-1-60845-510-2, $45.00;
e-book, ISBN 978-1-60815-511-9, $30.00 or by subscription
Reviewed by
Michel Simard
National Research Council Canada
Bitext alignment techniques are at the heart of the revolution that swept through the
field of machine translation (MT) two decades ago. From the early attempts of pioneers
like Kay and Ro?scheisen (1993), to the recent success of [enter name of your current
favorite alignment method or researcher], the growth of translation alignment is so tightly
intertwined with that of statistical machine translation that it is actually difficult (and
most likely futile) to try to establish which was most instrumental to which. From the
beginning, researchers have been attracted by the alignment problem, not only because
of its essential link to MT and the numerous other applications that could be derived,
but also because bitext alignment seems like a ?neat? problem: one around which a
researcher can easily wrap his or her head until a clean solution emerges.
Of course, this neatness is mostly illusory, as many researchers eventually found
out, and all these ?possible? links in publicly available reference alignments are wit-
nesses to the fact that translation alignment is not a perfectly well defined problem,
and that explaining the subtle ways in which identical concepts are rendered across
languages by drawing lines between words is an oversimplification, to say the least.
In spite of this, Tiedemann?s book is a timely addition to the natural language
processing literature. After going through the motivations in Chapter 1, he introduces
the basic concepts and terminology in Chapter 2, discusses various alignment types,
models and search procedures, and presents the fundamentals of alignment evaluation.
Of particular interest is the discussion on the crucial role played by the segmentation
of the text into the units on which the alignment will operate. The process of collecting
and structuring parallel corpora is briefly outlined in Chapter 3.
In Chapter 4, Tiedemann gets to the heart of the matter with sentence-level align-
ment. Following a more-or-less historical line, he first covers approaches based on
surface features (sentence lengths, alignment type), a` la Gale and Church (1993), then
methods relying on lexical resources, before looking at combined and resource-specific
methods.
Chapter 5 covers word-alignment techniques. As one would expect, it makes up a
sizeable portion of the book (about a third). Half of this chapter is devoted to generative
translationmodels (essentially, the IBMmodels), the other half to various discriminative
models. Finally, Chapter 6 rapidly surveys phrase and tree alignment models.
Overall, Tiedemann?s book covers a lot of ground, possibly a bit too much. This is a
vast field, which has seen the publication of hundreds (thousands?) of publications over
the last 20 years or so. The author clearly knows his stuff, and manages to structure
its presentation in a logical and intuitive manner. The exposition?s clarity sometimes
Computational Linguistics Volume 38, Number 2
suffers, however, from the author?s obvious desire not to miss anything. Many research
avenues are just alluded to, and even fundamental notions are sometimes presented in a
sketchy manner. For instance, the topic that clearly gets the most elaborate presentation
is the IBM models, with 15 pages; yet it is unlikely that a reader new to this field will
manage to extract more than a general intuition about them.
It is also worth noting that the book probably requires more from the reader than
just ?familiarity with basic concepts? of machine learning. The means and methods of
machine learning have become so ubiquitous in computational linguistics that we tend
to forget how fundamental they are: the cycle of training and testing, feature engineer-
ing, the debates of generative versus discriminative modeling, supervised versus semi-
supervised versus unsupervised learning, and so forth. All of this is part of our daily
routine and has taken over our way of viewing the field.
All this is not to say that this is not a useful book. It is a well-structured, well-written
overview of the state-of-the-art in text-translation alignment. It reads like a roadmap of
the work accomplished more than a true travel guide. As such, it is rich with pointers to
the many variations on methods and approaches to the problem. It will most likely be
of interest to the bi-curious among us: graduate students and researchers who, although
already familiar with computational linguistics, may feel attracted to the other text.
References
Gale, W. A. and K. W. Church. 1993. A
program for aligning sentences in
bilingual corpora. Computational
Linguistics, 19(1):75?102.
Kay, M. and M. Ro?scheisen. 1993.
Text-translation alignment.
Computational Linguistics, 19(1):
121?142.
This book review was edited by Pierre Isabelle.
Michel Simard obtained a Ph.D. in computer science from the Universite? de Montre?al in 2003, but
has been actively involved in natural language processing research since 1986: He was part of
the machine-aided translation team of CITI, a research laboratory of the Canadian Department
of Industry; was a founding member of the RALI laboratory at the Universite? de Montre?al;
and was a postdoctoral researcher with the Machine Learning group at Xerox Research Cen-
tre Europe (Grenoble, France). Now a senior research officer at the National Research Coun-
cil Canada, his work focuses mainly on machine translation and machine-assisted translation.
Simard?s e-mail address is michel.simard@nrc-cnrc-gc.ca.
440
Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014), pages 192?197,
Dublin, Ireland, August 23-24, 2014.
CNRC-TMT:
Second Language Writing Assistant System Description
Cyril Goutte Michel Simard
National Research Council Canada
Multilingual Text Processing
1200 Montreal Road, Ottawa, Ontario K1A 0R6, Canada
FirstName.LastName@nrc.ca
Marine Carpuat
Abstract
We describe the system entered by the
National Research Council Canada in
the SemEval-2014 L2 writing assistant
task. Our system relies on a standard
Phrase-Based Statistical Machine Transla-
tion trained on generic, publicly available
data. Translations are produced by taking
the already translated part of the sentence
as fixed context. We show that translation
systems can address the L2 writing assis-
tant task, reaching out-of-five word-based
accuracy above 80 percent for 3 out of 4
language pairs. We also present a brief
analysis of remaining errors.
1 Introduction
The Semeval L2 writing assistant task simulates
the situation of an L2 language learner trying to
translate a L1 fragment in a L2 context. This is
clearly motivated by a L2 language learning sce-
nario.
However, a very similar scenario can be en-
countered in Computer-Aided Translation. Trans-
lation memories retrieve from a large corpus of al-
ready translated documents the source segments
that best match a new sentence to be translated.
If an exact source match is found, the correspond-
ing target translation can be expected to be suit-
able with little or no post-editing. However, when
only approximate matches are found, post-editing
will typically be required to adapt the target side
of the partially matching source segment to the
source sentence under consideration. It is possible
to automate this process: standard string matching
algorithms and word alignment techniques can be
used to locate the parts of the source segment that
do not match the sentence to translate, and from
c
?2014, The Crown in Right of Canada.
there the parts of the target segment that need to
be modified (Bic?ici and Dymetman, 2008; Simard
and Isabelle, 2009; Koehn and Senellart, 2010).
The task of translating a L1 fragment in L2 con-
text therefore has much broader application than
language learning. This motivation also provides
a clear link of this task to the Machine Translation
setting. There are also connections to the code-
switching and mixed language translation prob-
lems (Fung et al., 1999).
In our work, we therefore investigate the use
of a standard Phrase-Based Statistical Machine
Translation (SMT) system to translate L1 frag-
ments in L2 context. In the next section, we de-
scribe the SMT system that we used in our submis-
sion. We then describe the corpora used to train
the SMT engine (Section 3), and our results on the
trial and test data, as well as a short error analysis
(Section 4).
section
2 System Description
The core Machine Translation engine used for all
our submissions is Portage (Larkin et al., 2010),
the NRC?s phrase-based SMT system. Given a
suitably trained SMT system, the Task 5 input is
processed as follows. For each sentence with an
L1 fragment to translate, the already translated
parts are set as left and right context. The L1 frag-
ment in L2 context is sent to the decoder. The
output is a full sentence translation that ensures 1)
that the context is left untouched, and 2) that the
L1 fragment is translated in a way that fits with the
L2 context.
We now describe the key components of the MT
system (language, translation and reordering mod-
els), as well as the decoding and parameter tuning.
Translation Models We use a single static
phrase table including phrase pairs extracted from
the symmetrized HMM word-alignment learned
192
on the entire training data. The phrase table con-
tains four features per phrase pair: lexical esti-
mates of the forward and backward probabilities
obtained either by relative frequencies or using the
method of Zens and Ney (2004). These estimates
are derived by summing counts over all possible
alignments. This yields four corresponding pa-
rameters in the log-linear model.
Reordering Models We use standard reorder-
ing models: a distance-based distortion feature, as
well as a lexicalized distortion model (Tillmann,
2004; Koehn et al., 2005). For each phrase pair,
the orientation counts required for the lexicalized
distortion model are computed using HMM word-
alignment on the full training corpora. We esti-
mate lexicalized probabilities for monotone, swap,
and discontinuous ordering with respect to the pre-
vious and following target phrase. This results in
a total of 6 feature values per phrase pair, in addi-
tion to the distance-based distortion feature, hence
seven parameters to tune in the log-linear model.
Language Models When translating L1 frag-
ments in L2 context, the L2 language model (LM)
is particularly important as it is the only compo-
nent of the SMT system that scores how well the
translation of the L1 fragment fits in the existing
L2 context. We test two different LM configura-
tions. The first of these (run1) uses a single static
LM: a standard 4-gram, estimated using Kneser-
Ney smoothing (Kneser and Ney, 1995) on the tar-
get side of the bilingual corpora used for training
the translation models. In the second configuration
(run2), in order to further adapt the translations to
the test domain, a smaller LM trained on the L2
contexts of the test data is combined to the train-
ing corpus LM in a linear mixture model (Foster
and Kuhn, 2007). The linear mixture weights are
estimated on the L2 context of each test set in a
cross-validation fashion.
Decoding Algorithm and Parameter Tuning
Decoding uses the cube-pruning algorithm (Huang
and Chiang, 2007) with a 7-word distortion limit.
Log-linear parameter tuning is performed using a
lattice-based batch version of MIRA (Cherry and
Foster, 2012).
3 Data
SMT systems require large amounts of data to
estimate model parameters. In addition, transla-
tion performance largely depends on having in-
Europarl News Total
en-de train 1904k 177k 2081k
dev - 2000 2000
en-es train 1959k 174k 2133k
dev - 2000 2000
fr-en train 2002k 157k 2158k
dev - 2000 2000
nl-en train 1974k - 1974k
dev 1984 - 1984
Table 1: Number of training segments for each
language pair.
domain data to train on. As we had no informa-
tion on the domain of the test data for Task 5, we
chose to rely on general purpose publicly avail-
able data. Our main corpus is Europarl (Koehn,
2005), which is available for all 4 language pairs
of the evaluation. As Europarl covers parliamen-
tary proceedings, we added some news and com-
mentary (henceforth ?News?) data provided for
the 2013 workshop on Machine Translation shared
task (Bojar et al., 2013) for language pairs other
than nl-en. In all cases, we extracted from the cor-
pus a tuning (?dev?) set of around 2000 sentence
pairs. Statistics for the training data are given in
Table 1.
The trial and test data each consist of 500 sen-
tences with L1 fragments in L2 context provided
by the organizers. As the trial data came from Eu-
roparl, we filtered our training corpora in order to
remove close matches and avoid training on the
trial data (Table 1 takes this into account).
All translation systems were trained on lower-
cased data, and predictions were recased using a
standard (LM-based) truecasing approach.
4 Experimental Results
4.1 Results on Trial and Simulated Data
Our first evaluation was performed on the trial data
provided by the Task 5 organizers. Each example
was translated in context by two systems:
run1: Baseline, non-adapted system (marked 1
below);
run2: Linear LM mixture adaptation, using a
context LM (marked 2 below).
Table 2 shows that our run1 system already
yields high performance on the trial data, while
193
W@1 F@1 W@5 F@5 +BLEU
en-de1 78.1 77.0 95.6 94.8 12.4
en-de2 79.8 79.0 95.8 95.0 12.6
en-es1 81.8 80.2 97.7 97.2 12.1
en-es2 84.3 83.2 97.7 97.2 12.5
fr-en1 84.4 83.6 97.1 96.4 11.8
fr-en2 85.9 85.0 97.4 96.6 12.0
nl-en1 83.3 82.0 97.0 96.4 11.8
nl-en2 86.7 86.2 97.5 97.0 12.1
Table 2: Trial data performance, from official eval-
uation script: (W)ord and (F)ragment accuracy at
(1) and (5)-best and BLEU score gain.
adapting the language model on the L2 contexts
in run2 provides a clear gain in the top-1 results.
That improvement all but disappears when taking
into account the best out of five translations (ex-
cept maybe for nl-en). The BLEU scores
1
are
very high (97-98) and the word error rates (not
reported) are around 1%, suggesting that the sys-
tem output almost matches the references. This
is no doubt due to the proximity between the trial
data and the MT training corpus. Both are fully or
mainly drawn from Europarl material.
In order to get a less optimistic estimate of per-
formance, we automatically constructed a num-
ber of test examples from the WMT News Com-
mentary development test sets. The L1 source
segments and their L2 reference translations were
word aligned in both directions using the GIZA++
implementation of IBM4 (Och and Ney, 2003)
and the grow-diag-final-and combination heuris-
tic (Koehn et al., 2005). Test instances were cre-
ated by substituting some L2 fragments with their
word-aligned L1 source within L2 reference seg-
ments. Since the goal was to select examples that
were more ambiguous and harder to translate than
the trial data, a subset of interesting L1 phrases
was randomly selected among phrases that oc-
cured at least 4 times in the training corpus and
have a high entropy in the baseline phrase-table.
We selected roughly 1000 L1 phrases per language
pair. For each occurrence p
1
of these L1 phrases in
the news development sets, we identify the short-
est L2 phrase p
2
that is consistently aligned with
1
+BLEU in Tables 2-4 is the difference between our sys-
tem?s output and the sentence with untranslated L1 fragment.
W@1 F@1 W@5 F@5 +BLEU
en-de1 48.0 46.4 70.8 68.7 4.26
en-de2 52.3 50.6 71.0 68.9 4.63
en-es1 47.6 45.2 68.0 65.8 4.12
en-es2 50.0 47.9 67.8 65.5 4.34
fr-en1 50.1 49.2 73.6 71.8 5.18
fr-en2 51.1 49.5 73.1 71.2 5.19
Table 3: News data performance (cf Tab. 2).
p
1
.
2
A new mixed language test example is con-
structed by replacing p
2
with p
1
in L2 context.
Results on that simulated data are given in Ta-
ble 3. Performance is markedly lower than on the
trial data. This is due in part to the fact that the
News data is not as close to the training material
as the official trial set, and in part to the fact that
this automatically extracted data contains imper-
fect alignments with an unknown (but sizeable)
amount of ?noise?. However, it still appears run2
consistently provides several points of increase in
performance for the top-1 results, over the base-
line run1. Performance on the 5-best is either un-
affected or lower, and the gain in BLEU is much
lower than in Table 2 although the resulting BLEU
is around 96%.
4.2 Test Results
Official test results provided by the organizers
are presented in Table 4. While these results
are clearly above what we obtained on the syn-
thetic news data, they fall well below the perfor-
mance observed on the trial data. This is not un-
expected as the trial data is unrealistically close
to the training material, while the automatically
extracted news data is noisy. What we did not
expect, however, is the mediocre performance of
LM adaptation (run2): while consistently better
than run1 on both trial and news, it is consistently
worse on the official test data. This may be due to
the fact that test sentences were drawn from differ-
ent sources
3
such that it does not constitute a ho-
mogeneous domain on which we can easily adapt
a language model.
For German and Spanish, and to a lesser extent
2
As usual in phrase-based MT, two phrases are said to be
consistently aligned, if there is at least one link between their
words and no external links.
3
According to the task description, the test set is based
on ?language learning exercises with gaps and cloze-tests, as
well as learner corpora with annotated errors?.
194
W@1 F@1 W@5 F@5 +BLEU
en-de1 71.7 65.7 86.8 83.4 16.6
en-de2 70.2 64.5 86.5 82.8 16.4
en-es1 74.5 66.7 88.7 84.3 17.0
en-es2 73.5 65.1 88.4 83.7 17.5
fr-en1 69.4 55.6 83.9 73.9 10.2
fr-en2 68.6 53.3 83.4 73.1 9.9
nl-en1 61.0 45.0 72.3 60.6 5.03
nl-en2 60.9 44.4 72.1 60.2 5.02
Table 4: Test data performance, from official eval-
uation results (cf. Table 2).
OOV?s failed align
en-de 0.002 0.058
en-es 0.010 0.068
fr-en 0.026 0.139
nl-en 0.123 0.261
Table 5: Test data error analysis: OOV?s is the
proportion of all test fragments containing out-of-
vocabulary tokens; failed align is the proportion of
fragments which our system cannot align to any of
the reference translations by forced decoding.
for French and Dutch, the BLEU and Word Error
Rate (WER) gains are much higher on the test than
on the trial data, although the resulting BLEU are
around 86-92%. This results from the fact that the
amount of L1 material to translate relative to the
L2 context was significantly higher on the test data
than it was on the trial data (e.g. 17% of words on
en-es test versus 7% on trial).
4.3 Error Analysis
On the French and, especially, on the Dutch data,
our systems suffer from a high rate of out-of-
vocabulary (OOV) source words in the L1 frag-
ments, i.e. words that simply did not appear in our
training data (see Table 5). In the case of Dutch,
OOV?s impose a hard ceiling of 88% on fragment-
level accuracy. These problems could possibly be
alleviated by using more training data, and incor-
porating language-specific mechanisms to handle
morphology and compounding into the systems.
We also evaluate the proportion of reference tar-
get fragments that can not be reached by forced
decoding (Table 5). Note that to produce trial
and test translations, we use standard decoding to
Freq Type
77 Incorrect L2 sense chosen
75 Incorrect or mangled syntax
26 Incomplete reference
20 Non-idiomatic translation
13 Out-of-vocab. word in fragment
6 Problematic source fragment
3 Casing error
220 Total
Table 6: Analysis of the types of error on 220
French-English test sentences.
predict a translation that maximizes model score
given the input. Once we have the reference
translation, we use forced decoding to try to pro-
duce the exact reference given the source frag-
ment and our translation model. In some situa-
tions, the correct translations are simply not reach-
able by our systems, either because some target
word has not been observed in training, some part
of the correspondence between source and target
fragments has not been observed, or the system?s
word alignment mechanism is unable to account
for this correspondence, in whole or in part. Ta-
ble 5 shows that this happens between 6% ans
26% of cases, which gives a better upper bound on
the fragment-level accuracy that our system may
achieve. Again, many of these problems could be
solved by using more training data.
To better understand the behavior of our sys-
tems, we manually reviewed 220 sentences where
our baseline French-English system did not ex-
actly match any of the references. We annotated
several types of errors (Table 6). The most fre-
quent source of errors is incorrect sense (35%), i.e.
the system produced a translation of the fragment
that may be correct in some setting, but is not the
correct sense in that context. Those are presum-
ably the errors of interest in a sense disambigua-
tion setting. A close second (34%) were errors
involving incorrect syntax in the fragment transla-
tion, which points to limitations of the Statistical
MT approach, or to a limited language model.
The last third combines several sources of er-
rors. Most notable in this category are non-
idiomatic translations, where the system?s output
was both syntactically correct and understandable,
but clearly not fluent (e.g. ?take a siesta? for ?have
a nap?); We also identified a number of cases
195
where we felt that either the source segment was
incorrect (eg ?je vais ?evanouir? instead of ?je vais
m??evanouir?), or the references were incomplete.
Table 7 gives a few examples.
5 Conclusion
We described the systems used for the submissions
of the National Research Council Canada to the
L2 writing assistant task. We framed the problem
as a machine translation task, and used standard
statistical machine translation systems trained on
publicly available corpora for translating L1 frag-
ments in their L2 context. This approach lever-
ages the strengths of phrase-based statistical ma-
chine translation, and therefore performs particu-
larly well when the test examples are close to the
training domain. Conversely, it suffers from the
inherent weaknesses of phrase-based models, in-
cluding their inability to generalize beyond seen
vocabulary, as well as sense and syntax errors.
Overall, we showed that machine translation sys-
tems can be used to address the L2 writing assis-
tant task with a high level of accuracy, reaching
out-of-five word-based accuracy above 80 percent
for 3 out of 4 language pairs.
References
Ergun Bic?ici and Marc Dymetman. 2008. Dynamic
Translation Memory: Using Statistical Machine
Translation to Improve Translation Memory Fuzzy
Matches. In Computational Linguistics and Intelli-
gent Text Processing, pages 454?465. Springer.
Ond?rej Bojar, Christian Buck, Chris Callison-Burch,
Christian Federmann, Barry Haddow, Philipp
Koehn, Christof Monz, Matt Post, Radu Soricut, and
Lucia Specia. 2013. Findings of the 2013 Work-
shop on Statistical Machine Translation. In Pro-
ceedings of the Eighth Workshop on Statistical Ma-
chine Translation, pages 1?44, Sofia, Bulgaria, Au-
gust.
Colin Cherry and George Foster. 2012. Batch Tun-
ing Strategies for Statistical Machine Translation. In
Proceedings of the 2012 Conference of the North
American Chapter of the Association for Computa-
tional Linguistics: Human Language Technologies,
pages 427?436, Montr?eal, Canada, June.
George Foster and Roland Kuhn. 2007. Mixture-
model adaptation for SMT. In Proceedings of the
Second Workshop on Statistical Machine Transla-
tion, pages 128?135, Prague, Czech Republic, June.
Pascale Fung, Xiaohu Liu, and Chi Shun Cheung.
1999. Mixed Language Query Disambiguation. In
Proceedings of ACL?99, pages 333?340, Maryland,
June.
Liang Huang and David Chiang. 2007. Forest Rescor-
ing: Faster Decoding with Integrated Language
Models. In Proceedings of the 45th Annual Meet-
ing of the Association of Computational Linguistics,
pages 144?151, Prague, Czech Republic, June.
Reinhard Kneser and Hermann Ney. 1995. Improved
Backing-off for M-gram Language Modeling. In
Acoustics, Speech, and Signal Processing, 1995.
ICASSP-95., 1995 International Conference on, vol-
ume 1, pages 181?184. IEEE.
Philipp Koehn and Jean Senellart. 2010. Conver-
gence of Translation Memory and Statistical Ma-
chine Translation. In Proceedings of AMTA Work-
shop on MT Research and the Translation Industry,
pages 21?31.
Philipp Koehn, Amittai Axelrod, Alexandra Birch
Mayne, Chris Callison-Burch, Miles Osborne, and
David Talbot. 2005. Edinburgh System Description
for the 2005 IWSLT Speech Translation Evaluation.
In Proceedings of IWSLT-2005, pages 68?75, Pitts-
burgh, PA.
Philipp Koehn. 2005. Europarl: A Parallel Corpus for
Statistical Machine Translation. In Machine Trans-
lation Summit X, pages 79?86, Phuket, Thailand,
September.
Samuel Larkin, Boxing Chen, George Foster, Ull-
rich Germann,
?
Eric Joanis, J. Howard Johnson, and
Roland Kuhn. 2010. Lessons from NRC?s Portage
System at WMT 2010. In 5th Workshop on Statisti-
cal Machine Translation, pages 127?132.
Franz Josef Och and Hermann Ney. 2003. A System-
atic Comparison of Various Statistical Alignment
Models. Computational Linguistics, 29(1):19?52.
Michel Simard and Pierre Isabelle. 2009. Phrase-
based Machine Translation in a Computer-assisted
Translation Environment. Proceedings of the
Twelfth Machine Translation Summit (MT Summit
XII), pages 120?127.
Christoph Tillmann. 2004. A Unigram Orienta-
tion Model for Statistical Machine Translation. In
Daniel Marcu Susan Dumais and Salim Roukos, ed-
itors, HLT-NAACL 2004: Short Papers, pages 101?
104, Boston, Massachusetts, USA, May 2 - May 7.
Richard Zens and Hermann Ney. 2004. Improve-
ments in Phrase-Based Statistical Machine Trans-
lation. In Daniel Marcu Susan Dumais and Salim
Roukos, editors, HLT-NAACL 2004: Main Proceed-
ings, pages 257?264, Boston, Massachusetts, USA,
May 2 - May 7.
196
Incorrect L2 sense:
In: My dog usually barks au facteur - but look at that , for once , he is being friendly . . .
Out: My dog usually barks to the factor - but look at that , for once , he is being friendly . . .
Ref: My dog usually barks at the postman - but look at that , for once , he is being friendly . . .
In: Grapes ne poussent pas in northern climates , unless one keeps them in a hot-house .
Out: Grapes do not push in northern climates , unless one keeps them in a hot-house .
Ref: Grapes do not grow in northern climates , unless one keeps them in a hot-house .
Missing reference?
In: Twenty-two other people ont
?
et
?
e bless
?
ees in the explosion .
Out: Twenty-two other people were injured in the explosion .
Ref: Twenty-two other people have been wounded in the explosion .
Non-idiomatic translation:
In: After patiently stalking its prey , the lion makes a rapide comme l?
?
eclair charge for the kill .
Out: After patiently stalking its prey , the lion makes a rapid as flash charge for the kill .
Ref: After patiently stalking its prey , the lion makes a lightning-fast charge for the kill .
Problem with input:
In: every time I do n?t eat for a while and my blood sugar gets low I feel like je vais
?
evanouir .
Out: every time I do n?t eat for a while and my blood sugar gets low I feel like I will evaporate .
Ref: every time I do n?t eat for a while and my blood sugar gets low I feel like I ?m going to faint .
Table 7: Examples errors on French-English.
197
Proceedings of the 7th Workshop on Statistical Machine Translation, pages 442?449,
Montre?al, Canada, June 7-8, 2012. c?2012 Association for Computational Linguistics
The Trouble with SMT Consistency
Marine Carpuat and Michel Simard
National Research Council Canada
283 Alexandre-Tache? Boulevard
Building CRTL, Room F-2007
Gatineau (Que?bec) J8X 3X7
Firstname.Lastname@nrc.ca
Abstract
SMT typically models translation at the sen-
tence level, ignoring wider document context.
Does this hurt the consistency of translated
documents? Using a phrase-based SMT sys-
tem in various data conditions, we show that
SMT translates documents remarkably con-
sistently, even without document knowledge.
Nevertheless, translation inconsistencies often
indicate translation errors. However, unlike in
human translation, these errors are rarely due
to terminology inconsistency. They are more
often symptoms of deeper issues with SMT
models instead.
1 Introduction
While Statistical Machine Translation (SMT) mod-
els translation at the sentence level (Brown et al,
1993), human translators work on larger translation
units. This is partly motivated by the importance
of producing consistent translations at the document
level. Consistency checking is part of the quality as-
surance process, and complying with the terminol-
ogy requirements of each task or client is crucial.
In fact, many automatic tools have been proposed to
assist humans in this important task (Itagaki et al,
2007; Dagan and Church, 1994, among others).
This suggests that wider document-level context
information might benefit SMT models. However,
we do not have a clear picture of the impact of
sentence-based SMT on the translation of full doc-
uments. From a quality standpoint, it seems safe to
assume that translation consistency is as desirable
for SMT as for human translations. However, con-
sistency needs to be balanced with other quality re-
quirements. For instance, strict consistency might
result in awkward repetitions that make translations
less fluent. From a translation modeling standpoint,
while typical SMT systems do not explicitly enforce
translation consistency, they can learn lexical choice
preferences from training data in the right domain.
In this paper, we attempt to get a better under-
standing of SMT consistency. We conduct an em-
pirical analysis using a phrase-based SMT system in
a variety of experimental settings, focusing on two
simple, yet understudied, questions. Is SMT output
consistent at the document level? Do inconsistencies
indicate translation errors?
We will see that SMT consistency issues are quite
different from consistency issues in human transla-
tions. In fact, while inconsistency errors in SMT
output might be particularly obvious to the human
eye, SMT is globally about as consistent as human
translations. Furthermore, high translation consis-
tency does not guarantee quality: weaker SMT sys-
tems trained on less data translate more consistently
than stronger larger systems. Yet, inconsistent trans-
lations often indicate translation errors, possibly be-
cause words and phrases that translate inconsistently
are the hardest to translate.
After discussing related work on consistency and
document modeling for SMT (Section 2), we de-
scribe our corpora in Section 3 and our general
methodology in Section 4. In Section 5, we dis-
cuss the results of an automatic analysis of transla-
tion consistency, before turning to manual analysis
in Section 6.
442
2 Related work
While most SMT systems operate at the sentence
level, there is increased interest in modeling docu-
ment context and consistency in translation.
In earlier work (Carpuat, 2009), we investigate
whether the ?one sense per discourse? heuristic
commonly used in word sense disambiguation (Gale
et al, 1992) can be useful in translation. We show
that ?one translation per discourse? largely holds
in automatically word-aligned French-English news
stories, and that enforcing translation consistency as
a simple post-processing constraint can fix some of
the translation errors in a phrase-based SMT sys-
tem. Ture et al (2012) provide further empirical
support by studying the consistency of translation
rules used by a hierarchical phrase-based system to
force-decode Arabic-English news documents from
the NIST evaluation.
Several recent contributions integrate translation
consistency models in SMT using a two-pass de-
coding approach. In phrase-based SMT, Xiao et
al. (2011) show that enforcing translation consis-
tency using post-processing and redecoding tech-
niques similar to those introduced in Carpuat (2009)
can improve the BLEU score of a Chinese-English
system. Ture et al (2012) also show signifi-
cant BLEU improvements on Arabic-English and
Chinese-English hierarchical SMT systems. Dur-
ing the second decoding pass, Xiao et al (2011)
use only translation frequencies from the first pass
to encourage consistency, while Ture et al (2012)
also model word rareness by adapting term weight-
ing techniques from information retrieval.
Another line of work focuses on cache-based
adaptive models (Tiedemann, 2010a; Gong et al,
2011), which lets lexical choice in a sentence be in-
formed by translations of previous sentences. How-
ever, cache-based models are sensitive to error prop-
agation and can have a negative impact on some data
sets (Tiedemann, 2010b). Moreover, this approach
blurs the line between consistency and domain mod-
eling. In fact, Gong et al (2011) reports statistically
significant improvements in BLEU only when com-
bining pure consistency caches with topic and simi-
larity caches, which do not enforce consistency but
essentially perform domain or topic adaptation.
There is also work that indirectly addresses con-
sistency, by encouraging the re-use of translation
memory matches (Ma et al, 2011), or by using a
graph-based representation of the test set to promote
similar translations for similar sentences (Alexan-
drescu and Kirchhoff, 2009).
All these results suggest that consistency can be
a useful learning bias to improve overall translation
quality, as measured by BLEU score. However, they
do not yet give a clear picture of the translation con-
sistency issues faced by SMT systems. In this paper,
we directly check assumptions on SMT consistency
in a systematic analysis of a strong phrase-based
system in several large data conditions.
3 Translation Tasks
We use PORTAGE, the NRC?s state-of-the-art
phrase-based SMT system (Foster et al, 2009), in a
number of settings. We consider different language
pairs, translation directions, training sets of differ-
ent nature, domain and sizes. Dataset statistics are
summarized in Table 1, and a description follows.
Parliament condition These conditions are de-
signed to illustrate an ideal situation: a SMT system
trained on large high-quality in-domain data.
The training set consists of Canadian parliamen-
tary text, approximately 160 million words in each
language (Foster et al, 2010). The test set alo
consists of documents from the Canadian parlia-
ment: 807 English and 476 French documents. Each
document contains transcript of speech by a single
person, typically focusing on a single topic. The
source-language documents are relatively short: the
largest has 1079 words, the average being 116 words
for English documents, 124 for French. For each
document, we have two translations in the other lan-
guage: the first is our SMT output; the second is a
postedited version of that output, produced by trans-
lators of the Canadian Parliamentary Translation and
Interpretation services.
Web condition This condition illustrates a per-
haps more realistic situation: a ?generic? SMT sys-
tem, trained on large quantities of heterogeneous
data, used to translate slightly out-of-domain text.
The SMT system is trained on a massive corpus
of documents harvested from the Canadian federal
government?s Web domain ?gc.ca?: close to 40M
443
lang train data # tgt words test data #tgt words #docs BLEU WER
en-fr parl 167M parl 104k 807 45.2 47.1
fr-en parl 149M parl 51k 446 58.0 31.9
en-fr gov web 641M gov doc 336k 3419 29.4 60.4
zh-en small (fbis) 10.5M nist08 41k 109 23.6 68.9
zh-en large (nist09) 62.6M nist08 41k 109 27.2 66.1
Table 1: Experimental data
unique English-French sentence pairs. The test set
comes from a different source to guarantee that there
is no overlap with the training data. It consists of
more than 3000 English documents from a Canadian
provincial government organization, totalling 336k
words. Reference translations into French were pro-
duced by professional translators (not postedited).
Documents are quite small, each typically focus-
ing on a specific topic over a varied range of do-
mains: agriculture, environment, finance, human re-
sources, public services, education, social develop-
ment, health, tourism, etc.
NIST conditions These conditions illustrate the
situation with a very different language pair,
Chinese-to-English, under two different scenarios:
a system built using small in-domain data and one
using large more heterogeneous data.
Following Chen et al (2012), in the Small data
condition, the SMT system is trained using the FBIS
Chinese-English corpus (10.5M target words); the
Large data condition uses all the allowed bilingual
corpora from NIST Open Machine Translation Eval-
uation 2009 (MT09), except the UN, Hong Kong
Laws and Hong Kong Hansard datasets, for a total
of 62.6M target words. Each system is then used
to translate 109 Chinese documents from the 2008
NIST evaluations (MT08) test set. For this dataset,
we have access to four different reference transla-
tions. The documents are longer on average than
for the previous conditions, with approximately 470
words per document.
4 Consistency Analysis Method
We study repeated phrases, which we define as a
pair ?p, d? where d is a document and p a phrase
type that occurs more than once in d.
Since this study focuses on SMT lexical choice
consistency, we base our analysis on the actual trans-
lation lexicon used by our phrase-based translation
system (i.e., its phrase-table.) For each document
d in a given collection of documents, we identify
all source phrases p from the SMT phrase-table that
occur more than once. We only consider source
phrases that contain at least one content word.
We then collect the set of translations T for each
occurrence of the repeated phrase in d. Using the
word-alignment between source and translation, for
each occurrence of p in d, we check whether p is
aligned to one of its translation candidates in the
phrase-table. A repeated phrase is translated consis-
tently if all the strings in T are identical ? ignoring
differences due to punctuation and stopwords.
The word-alignment is given by the SMT decoder
in SMT output, and is automatically infered from
standard IBM models for the reference1.
Note that, by design, this approach introduces a
bias toward components of the SMT system. A hu-
man annotator asked to identify translation incon-
sistencies in the same data would not tag the exact
same set of instances. Our approach might detect
translation inconsistencies that a human would not
annotate, because of alignment noise or negligible
variation in translations for instance. We address
these limitations in Section 6. Conversely, a human
annotator would be able to identify inconsistencies
for phrases that are not in the phrase-table vocabu-
lary. Our approach is not designed to detect these in-
consistencies, since we focus on understanding lex-
ical choice inconsistencies based on the knowledge
available to our SMT system at translation time.
1We considered using forced decoding to align the reference
to the source, but lack of coverage led us to use IBM-style word
alignment instead.
444
lang train test translator #
re
pe
at
ed
ph
ra
se
s
co
ns
is
te
nt
(%
)
av
g
w
it
hi
n
do
c
fr
eq
(i
nc
on
si
st
en
t)
av
g
w
it
hi
n
do
c
fr
eq
(a
ll
)
#d
oc
s
w
it
h
re
-
pe
at
ed
ph
ra
se
s
%
co
ns
is
te
nt
th
at
m
at
ch
re
fe
re
nc
e
%
in
co
ns
is
te
nt
th
at
m
at
ch
re
fe
re
nc
e
%
ea
sy
fi
xe
s
en-fr parl parl SMT 4186 73.03 2.627 2.414 529 70.82 34.37 10.12
en-fr parl parl reference 3250 75.94 2.542 2.427 468
fr-en parl parl SMT 2048 85.35 2.453 2.351 303 82.72 52.67 3.52
fr-en parl parl reference 1373 82.08 2.455 2.315 283
en-fr gov web gov doc SMT 79248 88.92 6.262 3.226 2982 60.71 13.05 15.53
en-fr gov web gov doc reference 25300 82.73 4.071 2.889 2166
zh-en small nist08 SMT 2300 63.61 2.983 2.725 109 56.25 18.40 9.81
zh-en small nist08 reference 1431 71.49 2.904 2.695 109
zh-en large nist08 SMT 2417 60.20 3.055 2.717 109 60.00 17.88 10.89
zh-en large nist08 reference 1919 68.94 2.851 2.675 109
Table 2: Statistics on the translation consistency of repeated phrases for SMT and references in five translation tasks.
See Section 5 for details
5 Automatic Analysis
Table 2 reports various statistics for the translations
of repeated phrases in SMT and human references,
for all tasks described in Section 3.
5.1 Global SMT consistency
First, we observe that SMT is remarkably consis-
tent. This suggests that consistency in the source-
side local context is sufficient to constrain the SMT
phrase-table and language model to produce consis-
tent translations for most of the phrases considered
in our experiments.
The column ?consistent (%)? in Table 2 shows
that the majority of repeated phrases are translated
consistently for all translation tasks considered. For
French-English tasks, the percentage of repeated
phrases ranges from 73 to 89% . The consistency
percentages are lower for Chinese-English, a more
distant language pair. The Parliament task shows
that translating into the morphologically richer lan-
guage yields slightly lower consistency, all other di-
mensions being identical. However, morphological
variations only explain part of the difference: trans-
lating into French under the Web condition yields the
highest consistency percentage of all tasks, which
might be explained by the very short and repetitive
nature of the documents. As can be expected, incon-
sistently translated phrases are repeated in a docu-
ment more often than average for all tasks (columns
?avg within doc freq?).
Interestingly, the smaller and weaker Chinese-
English translation system (23.6 BLEU) is more
consistent than its stronger counterpart (27.2
BLEU) according to the consistency percent-
ages.The smaller training condition yields a smaller
phrase-table with a lower coverage of the nist08
source, fewer translation alternatives and there-
fore more consistent translations. Clearly consis-
tency does not correlate with translation quality, and
global consistency rates are not indicators of the
translation quality of particular system.
5.2 Consistency of reference translations
Surprisingly, the percentage of consistently trans-
lated phrases are very close in SMT output and hu-
man references, and even higher in SMT for 2 out of
5 tasks (Table 2).
Note that there are fewer instances of repeated
phrases for human references than for SMT, because
the phrase-table used as a translation lexicon natu-
rally covers SMT output better than independently
produced human translations. Word alignment is
also noisier between source and reference.
445
lang train test translator #
re
pe
at
ed
ph
ra
se
s
co
ns
is
te
nt
(%
)
av
g
w
it
hi
n
do
c
fr
eq
(i
nc
on
si
st
en
t)
av
g
w
it
hi
n
do
c
fr
eq
(a
ll
)
#d
oc
s
w
it
h
re
-
pe
at
ed
ph
ra
se
s
%
co
ns
is
te
nt
th
at
m
at
ch
re
fe
re
nc
e
%
in
co
ns
is
te
nt
th
at
m
at
ch
re
fe
re
nc
e
%
ea
sy
fi
xe
s
zh-en small nist08 human1 1496 71.59 2.974 2.725 109 68.91 34.59 9.71
human2 1356 69.40 2.913 2.687 109 73.22 36.63 7.60
human2 1296 71.60 2.870 2.671 109 71.88 36.68 8.15
zh-en large nist08 human1 2017 70.25 2.943 2.692 109 66.13 30.83 9.64
human2 1855 67.17 2.854 2.667 109 69.42 31.86 9.16
human3 1739 69.70 2.854 2.660 109 68.23 33.78 8.31
Table 3: Statistics on the translation consistency of repeated phrases in the multiple human references available on the
Chinese-English NIST08 test set. See Section 5 for details
There is a much wider gap in coherence per-
centages between references and SMT for Chinese-
English than French-English tasks, as can be ex-
pected for the harder language pair. In addition,
the same nist08 reference translations are more con-
sistent according to the phrase-table learned in the
small training condition than according to the larger
phrase-table. This confirms that consistency can sig-
nal a lack of coverage for new contexts.
5.3 Consistency and correctness
While translation consistency is generally assumed
to be desirable, it does not guarantee correctness:
SMT translations of repeated phrases can be consis-
tent and incorrect, or inconsistent and correct. In or-
der to evaluate correctness automatically, we check
whether translations of repeated phrases are found
in the corresponding reference sentences. This is
an approximation since the translation of a source
phrase can be correct even if it is not found in the
reference, and a target phrase found in the refer-
ence sentence is not necessarily a correct translation
of the source phrase considered. Post-edited refer-
ences alleviate some approximation errors for the
Parliament tasks: if the translated phrase matches
the references, it means that it was considered cor-
rect by the human post-editor who left it in. How-
ever, phrases modified during post-edition are not
necessarily incorrect. We will address this approxi-
mation in Section 6.
The columns ?% consistent that match reference?
and ?% inconsistent that match reference? in Ta-
ble 2 show that consistently translated phrases match
the references more often than the inconsistent ones.
With the post-edited references in the Parliament
condition, a non-negligible percentage of consis-
tently translated phrases are wrong: 17% when
translating into English, and 30% when translating
into French. In contrast, inconsistently translated
phrases are more likely to be incorrect: more than
65% into French and 47% into English. For all other
tasks, fewer translations match the references since
the references are not produced by post-edition, but
we still observe the same trend as in the Parliament
condition: inconsistent translations are more likely
to be incorrect than consistent translations overall.
Four reference translations are available for the
Chinese-English nist08 test set. We only use the first
one as a reference translation (in order to minimize
setting variations with French-English conditions.)
The three remaining human translations are used dif-
ferently. We compare them against the reference, ex-
actly as we do for SMT output. The resulting statis-
tics are given in Table 3. Since we know that the
human translations are correct, this shows that many
correct translations are not identified when using our
simple match technique to check correctness. How-
ever, it is interesting to note that (1) consistent hu-
man translations tend to match the human references
more often than the inconsistent ones, and (2) incon-
sistent MT translations match references much less
often than inconsistent human references.
446
Language Examples False Inconsistencies
?p, d? Same lemma Misaligned
en?fr 79 15 (19%) 8 (10%)
fr?en 92 12 (13%) 24 (26%)
Total 171 27 (16%) 32 (19%)
Table 4: False positives in the automatic identification of
translation inconsistencies.
What goes wrong when inconsistent translations
are incorrect? This question is hard to answer with
automatic analysis only. As a first approximation,
we check whether we could correct translations by
replacing them with machine translations produced
elsewhere in the document. In Table 2, we refer to
this as ?easy fixes? and show that only very few in-
consistency errors can be corrected this way. These
errors are therefore unlikely to be fixed by post-
processing approaches that enforce hard consistency
constraints (Carpuat, 2009).
6 Manual Analysis
In order to better understand what goes wrong with
inconsistent translations, we conduct a manual anal-
ysis of these errors in the Parliament test condition
(see Table 1). We randomly sample inconsistently
translated phrases, and examine a total of 174 re-
peated phrases (?p, d? pairs, as defined in Section 4.)
6.1 Methodological Issues
We first try to quantify the limitations of our ap-
proach, and verify whether the inconsistencies de-
tected automatically are indeed real inconsistencies.
The results of this analysis are presented in Table 4.
Given the set of translations for a repeated phrase,
we ask questions relating to morphology and auto-
matic word-level alignment:
Morphology Are some of the alternate transla-
tions for phrase p only different inflections of the
same lemma? Assuming that inflectional morphol-
ogy is governed by language-internal considerations
more often than translational constraints, it is prob-
ably inaccurate to label morphological variations of
the same word as inconsistencies. The annotations
reveal that this only happens for 16% of our sam-
ple (column ?Same lemma? in Table 4). Work is
under way to build an accurate French lemmatizer
to automatically abstract away from morphological
variations.
Alignment Are some of the alternate translations
only a by-product of word alignment errors? This
happens for instance when the French word partis
is identified as being translated in English some-
times as parties and sometimes as political in the
same document: the apparent inconsistency is ac-
tually due to an incorrect alignment within the fre-
quent phrase political parties. We identify 19% of
word alignment issues in our manually annotated
sample (column ?Misaligned? in Table 4). While
it is clear that alignment errors should be avoided,
it is worth noting that such errors are sometimes in-
dicative of translation problems: this happens, for
instance, when a key content word is left untrans-
lated by the SMT system.
Overall, this analysis confirms that, despite the
approximations used, a majority of the examples de-
tected by our method are real inconsistencies.
6.2 Analysis of Translation Errors
We then directly evaluate translation accuracy in our
sample by checking whether the system translation
match the post-edited references. Here we focus our
attention on those 112 examples from our sample of
inconsistently translated phrases that do not suffer
from lemmatization or misalignment problems. For
comparison, we also analyze 200 randomly sampled
examples of consistently translated phrases. Note
that the identification of consistent phrases is not
subject to alignment and lemmatization problems,
which we therefore ignore in this case. Details of
this analysis can be found in Table 5.
We first note that 40% of all inconsistently trans-
lated phrase types were not postedited at all: their
translation can therefore be considered correct. In
the case of consistently translated phrases, the rate
of unedited translations rises to 75%.
Focusing now on those phrases whose translation
was postedited, we classify each in one of three
broad categories of MT errors: meaning, terminol-
ogy, and style/syntax errors (columns labeled ?Type
of Correction? in Table 5).
Terminology Errors Surprisingly, among the in-
consistently translated phrases, we find only 13%
of true terminological consistency errors, where
447
Language Examples Unedited (%) Type of Correction (% of edited examples)
?p, d? Meaning Terminology Style/Syntax
Inconsistent en?fr 56 20 (36%) 8 (22%) 4 (11%) 27 (75%)
Translations fr?en 56 25 (45%) 10 (32%) 5 (16%) 20 (65%)
Total 112 45 (40%) 16 (24%) 9 (13%) 47 (70%)
Consistent en?fr 100 70 (70%) 3 (10%) 0 (0%) 27 (90%)
Translations fr?en 100 79 (79%) 5 (24%) 0 (0%) 16 (76%)
Total 200 149 (75%) 8 (16%) 0 (0%) 43 (84%)
Table 5: Manual Classification of Posteditor Corrections on the Parliament Task
the SMT output is acceptable but different from
standard terminology in the test domain. For in-
stance, the French term personnes handicape?es can
be translated as either persons with disabilities or
people with disabilities, but the former is prefered
in the Parliament domain. In the case of consis-
tently translated phrases, no such errors were de-
tected. This contrasts with human translation, where
enforcing term consistency is a major concern. In
the large-data in-domain condition considered here,
SMT mostly translates terminology consistently and
correctly. It remains to be seen whether this still
holds when translating out-of-domain, or for differ-
ent genres of documents.
Meaning Errors Meaning errors occur when the
SMT output fails to convey the meaning of the
source phrase. For example, in a medical con-
text, our MT system sometimes translates the French
word examen into English as review instead of the
correct test or investigation. Such errors make up
24% of all corrections on inconsistently translated
phrases, 16% in the case of consistent translations.
Style/Syntax Errors By far the most frequent cat-
egory turns out to be style/syntax errors (70% of cor-
rections on inconsistently translated phrases, 84%
on consistently translated phrases): these are situ-
ations where the SMT output preserves the mean-
ing of the source phrase, but is still post-edited for
syntactic or stylistic preference. This category actu-
ally covers a wide range of corrections. The more
benign cases are more cosmetic in nature, for ex-
ample when the posteditor changes the MT output
?In terms of the cost associated with...? into ?With
regard to spending related to...?. In the more se-
vere cases, the posteditor completely rewrites a seri-
ously disfluent machine translation. However, errors
to which we have assigned this label have a com-
mon denominator: the inconsistent phrase that is the
focus of our attention is not the source of the er-
ror, but rather ?collateral damage? in the war against
mediocre translations.
Taken together, these results show that transla-
tion inconsistencies in SMT tend to be symptoms of
generic SMT problems such as meaning and fluency
or syntax errors. Only a minority of observed in-
consistencies turn out to be the type of terminology
inconsistencies that are a concern in human transla-
tions.
7 Conclusion
We have presented an in-depth study of machine
translation consistency, using state-of-the-art SMT
systems trained and evaluated under various realis-
tic conditions. Our analysis highlights a number of
important, and perhaps overlooked, issues regarding
SMT consistency.
First, SMT systems translate documents remark-
ably consistently, even without explicit knowledge
of extra-sentential context. They even exhibit global
consistency levels comparable to that of professional
human translators.
Second, high translation consistency does not cor-
relate with better quality: as can be expected in
phrase-based SMT, weaker systems trained on less
data produce translations that are more consistent
than higher-quality systems trained on larger more
heterogeneous data sets.
However, this does not imply that inconsistencies
are good either: inconsistently translated phrases co-
incide with translation errors much more often than
consistent ones. In practice, translation inconsis-
tency could therefore be used to detect words and
phrases that are hard to translate for a given system.
Finally, manual inspection of inconsistent transla-
448
tions shows that only a small minority of errors are
the kind of terminology problems that are the main
concern in human translations. Instead, the major-
ity of errors highlighted by inconsistent translations
are symptoms of other problems, notably incorrect
meaning translation, and syntactic or stylistic issues.
These problems are just as prevalent with consistent
as with inconsistent translations.
While directly enforcing translation consistency
in MT may prove useful in some situations, our
analysis suggests that the phrase-based SMT sys-
tems considered here would benefit more from di-
rectly tackling the underlying ?- and admittedly
more complex ? problems of meaning and syntac-
tic errors.
In future work, we plan to improve our analysis by
extending our diagnosis methods, and consider ad-
ditional data conditions and genres. We also plan to
explore the potential of consistency for confidence
estimation and error detection.
Acknowledgments
We would like to thank the Canadian Translation
Bureau and the Portage team at the National Re-
search Council for providing the post-edited and ma-
chine translations used in this study.
References
Andrei Alexandrescu and Katrin Kirchhoff. 2009.
Graph-based learning for statistical machine transla-
tion. In Proceedings of Human Language Technolo-
gies: The 2009 Annual Conference of the North Ameri-
can Chapter of the Association for Computational Lin-
guistics, pages 119?127, Boulder, CO, June.
Peter E. Brown, Vincent J. Della Pietra, Stephen A. Della
Pietra, and Robert L. Mercer. 1993. The Mathematics
of Statistical Machine Translation: Parameter Estima-
tion. Computational Linguistics, 19(2):263?312.
Marine Carpuat. 2009. One translation per discourse.
In Proceedings of the Workshop on Semantic Evalu-
ations: Recent Achievements and Future Directions
(SEW-2009), pages 19?27, Boulder, CO, June.
Boxing Chen, Roland Kuhn, and Samuel Larkin. 2012.
PORT: a Precision-Order-Recall MT evaluation metric
for Tuning. In Proceedings of the 50th Annual Meet-
ing of the Association for Computational Linguistics
(ACL-2012).
Ido Dagan and Ken Church. 1994. Termight: Identify-
ing and translating technical terminology. In Proceed-
ings of the Fourth Conference on Applied Natural Lan-
guage Processing, pages 34?40, Stuttgart, Germany,
October.
George Foster, Boxing Chen, Eric Joanis, Howard John-
son, Roland Kuhn, and Samuel Larkin. 2009.
PORTAGE in the NIST 2009 MT Evaluation. Tech-
nical report, NRC-CNRC.
George Foster, Pierre Isabelle, and Roland Kuhn. 2010.
Translating structured documents. In Proceedings of
the Ninth Conference of the Association for Machine
Translation in the Americas (AMTA 2010), Denver,
Colorado, November.
William A. Gale, Kenneth W. Church, and David
Yarowsky. 1992. One Sense Per Discourse. In Pro-
ceedings of the workshop on Speech and Natural Lan-
guage, Harriman, NY, February.
Zhengxian Gong, Min Zhang, and Guodong Zhou. 2011.
Cache-based document-level statistical machine trans-
lation. In Proceedings of the 2011 Conference on
Empirical Methods in Natural Language Processing,
pages 909?919, July.
Masaki Itagaki, Takako Aikawa, and Xiaodong He.
2007. Automatic validation of terminology transla-
tion consistency with statistical method. In Proceed-
ings of Machine Translation Summit XI, pages 269?
274, September.
Yanjun Ma, Yifan He, Andy Way, and Josef van Gen-
abith. 2011. Consistent translation using discrim-
inative learning - a translation memory-inspired ap-
proach. In Proceedings of the 49th Annual Meeting
of the Association for Computational Linguistics: Hu-
man Language Technologies, pages 1239?1248, Port-
land, Oregon, USA, June.
Jo?rg Tiedemann. 2010a. Context adaptation in statistical
machine translation using models with exponentially
decaying cache. In Proceedings of the 2010 Workshop
on Domain Adaptation for Natural Language Process-
ing, pages 8?15, Uppsala, Sweden, July.
Jo?rg Tiedemann. 2010b. To Cache or Not To Cache?
Experiments with Adaptive Models in Statistical Ma-
chine Translation. In Joint Fifth Workshop on Sta-
tistical Machine Translation and MetricsMATR, pages
195?200, Uppsala, Sweden, July.
Ferhan Ture, Douglas W. Oard, and Philip Resnik. 2012.
Encouraging consistent translation choices. In Pro-
ceedings of the 2012 Conference of the North Amer-
ican Chapter of the Association for Computational
Linguistics: Human Language Technologies (NAACL
HLT 2012), Montreal, Canada, June.
Tong Xiao, Jingbo Zhu, Shujie Yao, and Hao Zhang.
2011. Document-level Consistency Verification in
Machine Translation. In Machine Translation Summit
XIII, pages 131?138, Xiamen, China, September.
449
