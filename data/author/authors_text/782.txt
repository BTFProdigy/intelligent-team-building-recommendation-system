Speaker Recognition with Mixtures of Gaussians with Sparse Regression
Matrices
Constantinos Boulis
University of Washington,
Electrical Engineering dept., SSLI Lab
boulis@ee.washington.edu
Abstract
When estimating a mixture of Gaussians there
are usually two choices for the covariance type
of each Gaussian component. Either diag-
onal or full covariance. Imposing a struc-
ture though may be restrictive and lead to de-
graded performance and/or increased compu-
tations. In this work, several criteria to esti-
mate the structure of regression matrices of a
mixture of Gaussians are introduced and eval-
uated. Most of the criteria attempt to estimate
a discriminative structure, which is suited for
classification tasks. Results are reported on
the 1996 NIST speaker recognition task and
performance is compared with structural EM,
a well-known, non-discriminative, structure-
finding algorithm.
1 Introduction
Most state-of-the-art systems in speech and speaker
recognition use mixtures of Gaussians when fitting a
probability distribution to data. Reasons for this choice
are the easily implementable estimation formulas and the
modeling power of mixtures of Gaussians. For example,
a mixture of diagonal Gaussians can still model depen-
dencies on the global level. An established practice when
applying mixtures of Gaussians is to use either full or di-
agonal covariances. However, imposing a structure may
not be optimum and a more general methodology should
allow for joint estimation of both the structure and pa-
rameter values.1.
The first question we have to answer is what type of
structure we want to estimate. For mixtures of Gaussians
there are three choices. Covariances, inverse covariances
or regression matrices. For all cases, we can see as se-
lecting a structure by introducing zeros in the respective
matrix. The three structures are distinctively different and
zeros in one matrix do not, in general, map to zeros in an-
other matrix. For example, we can have sparse covariance
1Here, we describe the Maximum Likelihood estimation
methodology for both structure and parameters. One alterna-
tive is Bayesian estimation.
but full inverse covariance or sparse inverse covariance
and full regression matrix.
There are no clear theoretical reasons why one choice
of structure is more suitable than others. However, in-
troducing zeros in the inverse covariance can be seen as
deleting arcs in an Undirected Graphical Model (UGM)
where each node represents each dimension of a single
Gaussian (Bilmes, 2000). Similarly, introducing zeros in
the regression matrix can be seen as deleting arcs in a Di-
rected Graphical Model (DGM). There is a rich body of
work on structure learning for UGM and DGM and there-
fore the view of a mixture of Gaussians as a mixture of
DGM or UGM may be advantageous. Under the DGM
framework, the problem of Gaussian parameter estima-
tion can be cast as a problem of estimating linear regres-
sion coefficients. Since the specific problem of selecting
features for linear regression has been encountered in dif-
ferent fields in the past, we adopt the view of a mixture
of Gaussians as a mixture of DGM.
In (Bilmes, 2000), the problem of introducing zeros in
regression matrices of a mixture of Gaussians was pre-
sented. The approach taken was to set to zero the pairs
with the lowest mutual information, i.e. bmi,j = 0 ??
I(Xi, Xj) ? 0, wherem is the Gaussian index and bi,j is
the (i, j) element of regression matrix B. The approach
was tested for the task of speech recognition in a lim-
ited vocabulary corpus and was shown to offer the same
performance with the mixture of full-covariance Gaus-
sians with 30% less parameters. full covariances. One
issue with the work in (Bilmes, 2000) is that the structure-
estimation criterion that was used was not discriminative.
For classification tasks, like speaker or speech recog-
nition, discriminative parameter estimation approaches
achieve better performance than generative ones, but are
in general hard to estimate especially for a high num-
ber of classes. In this work, a number of discrimina-
tive structure-estimation criteria tailored for the task of
speaker recognition are introduced. We avoid the com-
plexities of discriminative parameter estimation by esti-
mating a discriminative structure and then applying gen-
erative parameter estimation techniques. Thus, overall
the models attempt to model the discriminability between
classes without the numerical and implementation diffi-
X X X X1 2 3 4
Figure 1: A Directed Graphical Model
culties that such techniques have. A comparison of the
new discriminative structure-estimation criteria with the
structural EM algorithm is also presented.
This paper is structured as follows. In section 2, the
view of a Gaussian as a directed graphical model is
presented. In section 3, discriminative and generative
structure-estimation criteria for the task of speaker recog-
nition are detailed, along with a description of the struc-
tural EM algorithm. In section 4, the application task is
described and the experiments are presented. Finally, in
section 5, a summary and possible connections of this
work with the speaker adaptation problem are discussed.
2 Gaussians as Directed Graphical Models
Suppose that we have a mixture of M Gaussians:
p(x) =
M?
m
p(z = m)N(x;?m,?m) (1)
It is known from linear algebra that any square ma-
trix A can be decomposed as A = LDU , where L is a
lower triangular matrix, D is a diagonal matrix and U is
an upper triangular matrix. In the special case where A
is also symmetric and positive definite the decomposition
becomes A = UTDU where U is an upper triangular
matrix with ones in the main diagonal. Therefore we can
write U = I ?B with bij = 0 if i >= j.
The exponent of the Gaussian function can now be
written as (Bilmes, 2000):
(x??Bx?)TD(x??Bx?) (2)
where x? = x ? ?. The i-th element of (x? ? Bx?) can be
written as:
x?i ?
V?
k=i+1
bi,kx?k (3)
with V being the dimensionality of each vector. Equation
3 shows that the problem of Gaussian parameter estima-
tion can be casted as a linear regression problem. Regres-
sion schemes can be represented as Directed Graphical
Models. In fact, the multivariate Gaussian can be rep-
resented as a DGM as shown in Figure 1. Absent arcs
represent zeros in the regression matrix. For example the
B matrix in Figure 1 would have b1,4 = b2,3 = 0.
We can use the EM algorithm to estimate the param-
eters of a mixture of Gaussian ? = [?mBmDm]. This
formulation offers a number of advantages over the tra-
ditional formulation with means and covariances. First,
it avoids inversion of matrices and instead solves V + 1
linear systems of d equations each where d = 1 : V + 1.
If the number of components and dimensionality of input
vectors are high, as it is usually the case in speech recog-
nition applications, the amount of computations saved
can be important. Second, the number of computations
scale down with the number of regression coefficients
set to zero. This is not true in the traditional formu-
lation because introducing zeros in the covariance ma-
trix may result in a non-positive definite matrix and it-
erative techniques should be used to guarantee consis-
tency (Dempster, 1972). Third, for the traditional for-
mulation, adapting a mixture of non-diagonal Gaussians
with linear transformations leads to objective functions
that cannot be maximized analytically. Instead, iterative
maximization techniques, such as gradient descent, are
used. With the new formulation even with arbitrary Gaus-
sians, closed-form update equations are possible. Finally,
the new formulation offers flexibility in tying mecha-
nisms. Regression matrices Bm and variances Dm can
be tied in different ways, for example all the components
can share the same regression matrix but estimate a differ-
ent variance diagonal matrix for each component. Simi-
lar schemes were found to be succesful for speech recog-
nition (Gales, 1999) and this formulation can provide a
model that can extend such tying schemes. The advan-
tages of the new formulation are summarized in Table 1.
3 Structure Learning
In general, structure learning in DGM is an NP-hard
problem even when all the variables are observed (Chick-
ering et al, 1994). Our case is further complicated by
the fact that we have a hidden variable (the Gaussian in-
dex). Optimum structure-finding algorithms, like the one
in (Meila and Jordan, 2000) assume a mixture of trees
and therefore are making limiting assumptions about the
space of possible structures. In this paper, no prior as-
sumptions about the space of possible structures are made
but this leads to absence of guarantee for an optimum
structure. Two approaches for structure-learning are in-
troduced.
The first approach is to learn a discriminative struc-
ture, i.e. a structure that can discriminate between classes
even though the parameters are estimated in an ML fash-
ion. The algorithm starts from the fully connected model
and deletes arcs, i.e. sets bi,jm = 0 ?m = 1 : M (M is the
number of Gaussian components in a mixture). After set-
ting regression coefficients to zero, maximum likelihood
parameter estimation of the sparse mixture is employed.
A number of different structure-estimation criteria were
tested for the speaker recognition task (at the right of each
equation a shorthand for each criterion is defined):
? = [?m, Bm, Dm] ? = [?m,?m]
Computations
? Each component m=1:M re-
quires solution of V+1 linear
systems of d equations each,
d=1:V+1.
? Computations scale down with
the number of regression coef-
ficients set to zero.
? Each component m=1:M re-
quires an inversion of a rank V
matrix.
? Iterative techniques must be
employed for the sparse case.
Adaptation Easy EM equations for estimating a
linear transformation of a mixture of
arbitrary Gaussians.
Gradient descent techniques for the
M-step of EM algorithm for estimat-
ing a linear transformation of a mix-
ture of arbitrary Gaussians.
Tying More flexible tying mechanisms
across components, i.e. components
can share B but estimate Dm.
Components can share the entire ?.
Table 1: Comparison between viewing a mixture of Gaussians as a mixture of DGMs and the traditional representation
MI = I(Xi;Xj |target speaker s) (4)
DMIimp = I(Xi;Xj |target speaker s)?
(1/N)
?
n
I(Xi;Xj |impostor n) (5)
MIimp =
?
n
I(Xi;Xj |impostor n) (6)
DMIconf = I(Xi;Xj |target speaker s)?
I(Xi;Xj |target speaker k) (7)
where I(Xi;Xj) is the mutual information between el-
ements Xi and Xj of input vector X . The mutual in-
formations are estimated by first fitting a mixture of 30
diagonal Gaussians and then applying the methods de-
scribed in (Bilmes, 1999). All but MI and MIimp are
discriminative criteria and all are based on finding the
pairs (i, j) with the lowest values and zeroing the respec-
tive regression coefficients, for every component of the
mixture. MIimp assigns the same speaker-independent
structure for all speakers. For DMIconf target speaker
k is the most confusable for target speaker s in terms of
hits, i.e. when the truth is s, speaker k fires more than
any other target speaker. We can see that different criteria
aim at different goals. MI attempts to avoid the overfit-
ting problem by zeroing regression coefficients between
least marginally dependent feature elements. DMIimp
attempts to discriminate against impostors, MIimp at-
tempts to build a speaker-independent structure which
will be more robustly estimated since there are more data
to estimate the mutual informations and DMIconf at-
tempts to discriminate against the most confusable target
speaker. The most confusable target speaker k for a given
target speaker s should be determined from an indepen-
dent held-out set.
There are three main drawbacks that are shared by all
of the above criteria. First, they are limited by the fact
that all Gaussians will have the same structure. Second,
since we are estimating sparse regression matrices, it is
known that the absence of an arc is equivalent to con-
ditional independencies, yet the above criteria can only
test for marginal independencies. Third, we introduce
another free parameter (the number of regression coef-
ficients to be set to zero) which can be determined from
a held-out set but will require time consuming trial and
error techniques. Nevertheless, they may lead to better
discrimination between speakers.
The second approach we followed was one based on
an ML fashion which may not be optimum for classi-
fication tasks, but can assign a different structure for
each component. We used the structural EM (Friedman,
1997), (Thiesson et al, 1998) and adopt it for the case of
mixtures of Gaussians. Structural EM is an algorithm that
generalizes on the EM algorithm by searching in the com-
bined space of structure and parameters. One approach
to the problem of structure finding would be to start from
the full model, evaluate every possible combination of
arc removals in every Gaussian, and pick the ones with
the least decrease in likelihood. Unfortunately, this ap-
proach can be very expensive since every time we remove
an arc on one of the Gaussians we have to re-estimate all
the parameters, so the EM algorithm must be used for
each combination. Therefore, this approach alternates
parameter search with structure search and can be very
expensive even if we follow greedy approaches. On the
other hand, structural EM interleaves parameter search
with structure search. Instead of following the sequence
Estep ?Mstep ? structure search, structural EM fol-
lows Estep ? structure search ? Mstep. By treating
expected data as observed data, the scoring of likelihood
decomposes and therefore local changes do not influence
the likelihood on other parameters. In essence, structural
EM has the same core idea as standard EM. If M is the
structure, ? are the parameters and n is the iteration in-
dex, then the naive approach would be to do:
{Mn,?n} ? {Mn+1,?n+1} (8)
On the other hand, structural EM follows the sequence:
{Mn,?n} ? {Mn+1,?n} ? {Mn+1,?n+1} (9)
If we replace M with H , i.e. the hidden variables or
sufficient statistics, we will recognize the sequence of
steps as the standard EM algorithm. For a more thor-
ough discussion of structural EM, the reader is referred
to (Friedman, 1997). The paper in (Friedman, 1997) has
a general discussion on the structural EM algorithm for an
arbitrary graphical model. In this paper, we introduced a
greedy pruning algorithm with step size K for mixtures
of Gaussians. The algorithm is summarized in Table 2.
One thing to note about the scoring criterion is that it is
local, i.e. zeroing regression coefficient m, i, j will not
involve computations on other parameters.
4 Experiments
We evaluated our approach in the male subset of the 1996
NIST speaker recognition task (Przybocki and Martin,
1998). The problem can be described as following. Given
21 target speakers, perform 21 binary classifications (one
for each target speaker) for each one of the test sentences.
Each one of the binary classifications is a YES if the sen-
tence belongs to the target speaker and NO otherwise.
Under this setting, one sentence may be decided to have
been generated by more than one speaker, in which case
there will be at least one false alarm. Also, some of the
test sentences were spoken by non-target speakers (im-
postors) in which case the correct answer would be 21
NO. All speakers are male and the data are from the
Switchboard database (Godfrey et al, 1992). There are
approximately 2 minutes of training data for each target
speaker. All the training data for a speaker come from
the same session and the testing data come from different
sessions, but from the same handset type and phone num-
ber (matched conditions). The algorithms were evaluated
on sentence sizes of three and thirty seconds. The fea-
tures are 20-dimensional MFCC vectors, cepstrum mean
normalized and with all silences and pauses removed. In
the test data there are impostors who don?t appear in the
training data and may be of different gender than the tar-
get speakers.
A mixture of Gaussians is trained on each one of the
target speakers. For impostor modeling, a separate model
is estimated for each gender. There are 43 impostors for
each gender, each impostor with 2 minutes of speech.
Same-gender speakers are pooled together and a mixture
of 100 diagonal Gaussians is estimated on each pool. Im-
postor models remained fixed for all the experiments re-
ported in this work. During testing and because some
of the impostors are of different gender than the target
speakers, each test sentence is evaluated against both im-
postor models and the one with the highest log-likelihood
is chosen. For each test sentence the log-likelihood of
each target speaker?s model is subtracted from the log-
likelihood of the best impostor model. A decision for
YES is made if the difference of the log-likelihoods is
above a threshold. Although in real operation of the sys-
tem the thresholds are parameters that need to be esti-
mated from the training data, in this evaluation the thresh-
olds are optimized for the current test set. Therefore the
results reported should be viewed as a best case scenario,
but are nevertheless useful for comparing different ap-
proaches.
The metric used in all experiments was Equal Error
Rate (EER). EER is defined as the point where the proba-
bility of false alarms is equal to the probability of missed
detections. Standard NIST software tools were used for
the evalution of the algorithms (Martin et al, 1997).
It should be noted that the number of components per
Gaussian is kept the same for all speakers. A scheme that
allowed for different number of Gaussians per speaker
did not show any gains. Also, the number of components
is optimized on the test set which will not be the case in
the real operation of the system. However, since there are
only a few discrete values for the number of components
and EER was not particularly sensitive to that parameter,
we do not view this as a major problem.
Table 3 shows the EER obtained for different base-
line systems. Each cell contains two EER numbers, the
left is for 30-second test utterances and the right for 3-
second. For the Diagonal case 35 components were
used, while for the full case 12 components were used.
The Random case corresponds to randomly zeroing
10% of the regression coefficients of a mixture of 16 com-
ponents. This particular combination of number of pa-
rameters pruned and number of components was shown
to provide the best results for a subset of the test set.
All structure-finding experiments are with the same num-
ber of components and percent of regression coefficients
pruned.
Table 4 shows the EER obtained for different baseline
Algorithm: Finding both structure and parameter values using structural EM
Start with the full model for a given number of Gaussians
while (number of pruned regression coefficients < T )
E? step: Collect sufficient statistics for given structure, i.e, ?m(n) = p(zn = m|xn,Mold)
StructureSearch: Remove one arc from a Gaussian at a time, i.e. set bmi,j = 0.
The score associated with zeroing a single regression coefficient is.
Scorem,i,j = 2Dimb
m
i,j
?N
n ?m(n)x?
j
n,m(x?
i
n,m ?B
i
mx?n,m) +D
i
m(b
m
i,j)
2?N
n ?m(n)x?
j
n,m
Order coefficients in ascending order of score. P is the set of the first K coefficients.
Set the new structure Mnew as Mnew = Mold\{P}.
M? step: Calculate the new parameters given Mnew.
This step can be followed by a number of EM iterations to obtain better parameter values.
end
Table 2: The Structural EM algorithm for a mixture of Gaussians
Full Diagonal Random
6.3/10.3 5.6/9.0 6.3/10.3
Table 3: Baseline EER, left number is for 30-second test
utterances and right number for 3-second
sparse structures. SEM is structural EM. The first col-
umn is zeroing the pairs with the minimum values of the
corresponding criterion and the second column is zeroing
the pairs with the maximum values. The second column
is more of a consistency check. If the min entry of crite-
rion A is lower than the min entry of criterion B then the
max entry of criterion A should be higher than the max
entry of criterion B. For the structural EM, pruning step
sizes of 50 and 100 were tested and no difference was
observed.
min max
MI 6.3/10.0 6.6/10.6
DMIimp 5.9/9.0 6.6/10.3
MIimp 6.3/10.3 6.3/10.3
DMIconf 5.9/9.3 6.6/10.3
SEM 6.3/9.6 6.6/10.3
Table 4: EER for different sparse structures, left number
is for 30 second test utterances and right number for 3-
second.
From Table 4 we can see improved results from the
full-covariance case but results are not better than the
diagonal-covariance case. All criteria appear to perform
similarly. Table 4 also shows that zeroing the regression
coefficients with the maximum of each criterion func-
tion does not lead to systems with much different perfor-
mance. Also from Table 3 we can see that randomly ze-
roing regression coefficients performs approximately the
same as taking the minimum or maximum. These num-
bers, seem to suggest that the structure of a mixture of
Gaussians is not a critical issue for speaker recognition,
at least with the current structure-estimation criteria used.
5 Summary-Future work
In this work the problem of estimating sparse regression
matrices of mixtures of Gaussians was addressed. Dif-
ferent structure-estimation criteria were evaluated, both
discriminative and generative. The general problem of
finding the optimum structure of a mixture of Gaussians
has direct applications in speaker identification as well as
speech recognition.
Interesting connections can be drawn with Maximum
Likelihood Linear Regression (MLLR) speaker adapta-
tion (Leggetter and Woodland, 1995). Not surprisingly,
the estimation equations for the regression matrix bare
resemblance with the MLLR equations. However, re-
searchers have thus far barely looked into the problem of
structure-finding for speaker adaptation, focusing mostly
on parameter adaptation. An interesting new topic for
speaker adaptation could be joint structure and parameter
adaptation.
References
J. Bilmes. 1999. Natural Statistical Models for Auto-
matic Speech Recognition. Ph.D. thesis, U.C. Berke-
ley, Dept. of EECS.
J. Bilmes. 2000. Factored sparse inverse covariance ma-
trices. In Proceedings of International Conference on
Acoustics, Speech and Signal Processing (ICASSP).
D.M. Chickering, D. Geiger, and D.E. Heckerman. 1994.
Learning bayesian networks is NP-hard. Technical Re-
port MSR-TR-94-17, Microsoft Research.
A. P. Dempster. 1972. Covariance selection. Journal of
Biometrics, 28:157?75.
N. Friedman. 1997. Learning belief networks in the pres-
ence of missing values and hidden variables. In Proc.
14th International Conference on Machine Learning
(ICML), pages 125?133.
M. Gales. 1999. Semi-tied covariance matrices for hid-
den markov models. IEEE Transactions on Speech and
Audio Processing, 7:272?281.
J. Godfrey, E. Holliman, and J. McDaniel. 1992. Switch-
board: Telephone speech corpus for research develop-
ment. In Proceedings of ICASSP, pages 517?520.
C. J. Leggetter and P. C. Woodland. 1995. Maximum
likelihood linear regression for speaker adaptation of
continuous density hidden markov models. Computer
Speech and Language, 9:171?185.
A. Martin, G. Doddington, T. Kamm, M. Ordowski, and
M. Przybocki. 1997. The DET curve in assessment
of detection task performance. In Proceedings of Eu-
rospeech ?97, pages 1895?1898.
M. Meila and M. I. Jordan. 2000. Learning with mixtures
of trees. Journal of Machine Learning Research, 1:1?
48.
M. Przybocki and A. Martin. 1998. NIST speaker recog-
nition evaluations:1996-2001. In Proceedings of the
International Conference on Language Resources and
Evaluation (LREC), pages 331?335.
B. Thiesson, C. Meek, D. Chickering, and D. Heckerman.
1998. Learning mixtures of dag models. Technical
Report MSR-TR-97-30, Microsoft Research.
Proceedings of the 43rd Annual Meeting of the ACL, pages 435?442,
Ann Arbor, June 2005. c?2005 Association for Computational Linguistics
A Quantitative Analysis of Lexical Differences Between Genders in
Telephone Conversations
Constantinos Boulis
Department of Electrical Engineering
University of Washington
Seattle, 98195
boulis@ee.washington.edu
Mari Ostendorf
Department of Electrical Engineering
University of Washington
Seattle, 98195
mo@ee.washington.edu
Abstract
In this work, we provide an empiri-
cal analysis of differences in word use
between genders in telephone conversa-
tions, which complements the consid-
erable body of work in sociolinguistics
concerned with gender linguistic differ-
ences. Experiments are performed on a
large speech corpus of roughly 12000 con-
versations. We employ machine learn-
ing techniques to automatically catego-
rize the gender of each speaker given only
the transcript of his/her speech, achiev-
ing 92% accuracy. An analysis of the
most characteristic words for each gender
is also presented. Experiments reveal that
the gender of one conversation side influ-
ences lexical use of the other side. A sur-
prising result is that we were able to clas-
sify male-only vs. female-only conversa-
tions with almost perfect accuracy.
1 Introduction
Linguistic and prosodic differences between gen-
ders in American English have been studied for
decades. The interest in analyzing the gender lin-
guistic differences is two-fold. From the scientific
perspective, it will increase our understanding
of language production. From the engineering
perspective, it can help improve the performance
of a number of natural language processing tasks,
such as text classification, machine translation or
automatic speech recognition by training better lan-
guage models. Traditionally, these differences have
been investigated in the fields of sociolinguistics
and psycholinguistics, see for example (Coates,
1997), (Eckert and McConnell-Ginet, 2003) or
http://www.ling.lancs.ac.uk/groups/gal/genre.htm
for a comprehensive bibliography on language and
gender. Sociolinguists have approached the issue
from a mostly non-computational perspective using
relatively small and very focused data collections.
Recently, the work of (Koppel et al, 2002) has
used computational methods to characterize the
differences between genders in written text, such
as literary books. A number of monologues have
been analyzed in (Singh, 2001) in terms of lexical
richness using multivariate analysis techniques.
The question of gender linguistic differences
shares a number of issues with stylometry and
author/speaker attribution research (Stamatatos et
al., 2000), (Doddington, 2001), but novel issues
emerge with analysis of conversational speech, such
as studying the interaction of genders.
In this work, we focus on lexical differences be-
tween genders on telephone conversations and use
machine learning techniques applied on text catego-
rization and feature selection to characterize these
differences. Therefore our conclusions are entirely
data-driven. We use a very large corpus created for
automatic speech recognition - the Fisher corpus de-
scribed in (Cieri et al, 2004). The Fisher corpus is
annotated with the gender of each speaker making
it an ideal resource to study not only the character-
istics of individual genders but also of gender pairs
in spontaneous, conversational speech. The size and
435
scope of the Fisher corpus is such that robust results
can be derived for American English. The compu-
tational methods we apply can assist us in answer-
ing questions, such as ?To which degree are gender-
discriminative words content-bearing words?? or
?Which words are most characteristic for males in
general or males talking to females??.
In section 2, we describe the corpus we have
based our analysis on. In section 3, the machine
learning tools are explained, while the experimen-
tal results are described in section 4 with a specific
research question for each subsection. We conclude
in section 5 with a summary and future directions.
2 The Corpus and Data Preparation
The Fisher corpus (Cieri et al, 2004) was used in
all our experiments. It consists of telephone con-
versations between two people, randomly assigned
to speak to each other. At the beginning of each
conversation a topic is suggested at random from a
list of 40. The latest release of the Fisher collection
has more than 16 000 telephone conversations av-
eraging 10 minutes each. Each person participates
in 1-3 conversations, and each conversation is an-
notated with a topicality label. The topicality label
gives the degree to which the suggested topic was
followed and is an integer number from 0 to 4, 0
being the worse. In our site, we had an earlier ver-
sion of the Fisher corpus with around 12 000 con-
versations. After removing conversations where at
least one of the speakers was non-native1 and con-
versations with topicality 0 or 1 we were left with
10 127 conversations. The original transcripts were
minimally processed; acronyms were normalized to
a sequence of characters with no intervening spaces,
e.g. t. v. to tv; word fragments were converted to
the same token wordfragment; all words were lower-
cased; and punctuation marks and special characters
were removed. Some non-lexical tokens are main-
tained such as laughter and filled pauses such as uh,
um. Backchannels and acknowledgments such as
uh-huh, mm-hmm are also kept. The gender distri-
bution of the Fisher corpus is 53% female and 47%
male. Age distribution is 38% 16-29, 45% 30-49%
and 17% 50+. Speakers were connected at random
1About 10% of speakers are non-native making this corpus
suitable for investigating their lexical differences compared to
American English speakers.
from a pool recruited in a national ad campaign. It
is unlikely that the speakers knew their conversation
partner. All major American English dialects are
well represented, see (Cieri et al, 2004) for more de-
tails. The Fisher corpus was primarily created to fa-
cilitate automatic speech recognition research. The
subset we have used has about 17.8M words or about
1 600 hours of speech and it is the largest resource
ever used to analyze gender linguistic differences.
In comparison, (Singh, 2001) has used about 30 000
words for their analysis.
Before attempting to analyze the gender differ-
ences, there are two main biases that need to be re-
moved. The first bias, which we term the topic bias
is introduced by not accounting for the fact that the
distribution of topics in males and females is uneven,
despite the fact that the topic is pre-assigned ran-
domly. For example, if topic A happened to be more
common for males than females and we failed to ac-
count for that, then we would be implicitly building
a topic classifier rather than a gender classifier. Our
intention here is to analyze gender linguistic differ-
ences controlling for the topic effect as if both gen-
ders talk equally about the same topics. The sec-
ond bias, which we term speaker bias is introduced
by not accounting for the fact that specific speakers
have idiosyncratic expressions. If our training data
consisted of a small number of speakers appearing
in both training and testing data, then we will be
implicitly modeling speaker differences rather than
gender differences.
To normalize for these two important biases, we
made sure that both genders have the same percent
of conversation sides for each topic and there are
8899 speakers in training and 2000 in testing with no
overlap between the two sets. After these two steps,
there were 14969 conversation sides used for train-
ing and 3738 sides for testing. The median length of
a conversation side was 954.
3 Machine Learning Methods Used
The methods we have used for characterizing the
differences between genders and gender pairs are
similar to what has been used for the task of text
classification. In text classification, the objective is
to classify a document ~d to one (or more) of T pre-
defined topics y. A number of N tuples (~dn, yn)
436
are provided for training the classifier. A major
challenge of text classification is the very high di-
mensionality for representing each document which
brings forward the need for feature selection, i.e. se-
lecting the most discriminative words and discarding
all others.
In this study, we chose two ways for characteriz-
ing the differences between gender categories. The
first, is to classify the transcript of each speaker, i.e.
each conversation side, to the appropriate gender
category. This approach can show the cumulative
effect of all terms on the distinctiveness of gender
categories. The second approach is to apply feature
selection methods, similar to those used in text cate-
gorization, to reveal the most characteristic features
for each gender.
Classifying a transcript of speech according to
gender can be done with a number of different learn-
ing methods. We have compared Support Vector
Machines (SVMs), Naive Bayes, Maximum Entropy
and the tfidf/Rocchio classifier and found SVMs to
be the most successful. A possible difference be-
tween text classification and gender classification is
that different methods for feature weighting may be
appropriate. In text classification, inverse document
frequency is applied to the frequency of each term
resulting in the deweighting of common terms. This
weighting scheme is effective for text classification
because common terms do not contribute to the topic
of a document. However, the reverse may be true for
gender classification, where the common terms may
be the ones that mostly contribute to the gender cate-
gory. This is an issue that we will investigate in sec-
tion 4 and has implications for the feature weighting
scheme that needs to be applied to the vector repre-
sentation.
In addition to classification, we have applied fea-
ture selection techniques to assess the discrimina-
tive ability of each individual feature. Information
gain has been shown to be one of the most success-
ful feature selection methods for text classification
(Forman, 2003). It is given by:
IG(w) = H(C) ? p(w)H(C|w) ? p(w?)H(C|w?)
(1)
where H(C) = ??Cc=1 p(c) log p(c) denotes the
entropy of the discrete gender category random vari-
able C. Each document is represented with the
Bernoulli model, i.e. a vector of 1 or 0 depending
if the word appears or not in the document. We have
also implemented another feature selection mecha-
nism, the KL-divergence, which is given by:
KL(w) = D[p(c|w)||p(c)] =
C?
c=1
p(c|w) log
p(c|w)
p(c)
(2)
In the KL-divergence we have used the multinomial
model, i.e. each document is represented as a vector
of word counts. We smoothed the p(w|c) distribu-
tions by assuming that every word in the vocabulary
is observed at least 5 times for each class.
4 Experiments
Having explained the methods and data that we have
used, we set forward to investigate a number of
research questions concerning the nature of differ-
ences between genders. Each subsection is con-
cerned with a single question.
4.1 Given only the transcript of a conversation,
is it possible to classify conversation sides
according to the gender of the speaker?
The first hypothesis we investigate is whether sim-
ple features, such as counts of individual terms (un-
igrams) or pairs of terms (bigrams) have different
distributions between genders. The set of possible
terms consists of all words in the Fisher corpus plus
some non-lexical tokens such as laughter and filled
pauses. One way to assess the difference in their
distribution is by attempting to classify conversation
sides according to the gender of the speaker. The
results are shown in Table 1, where a number of
different text classification algorithms were applied
to classify conversation sides. 14969 conversation
sides are used for training and 3738 sides are used
for testing. No feature selection was performed; in
all classifiers a vocabulary of all unigrams or bi-
grams with 5 or more occurrences is used (20513 for
unigrams, 306779 for bigrams). For all algorithms,
except Naive Bayes, we have used the tf?idf repre-
sentation. The Rainbow toolkit (McCallum, 1996)
was used for training the classifiers. Results show
that differences between genders are clear and the
best results are obtained by using SVMs. The fact
that classification performance is significantly above
chance for a variety of learning methods shows that
437
lexical differences between genders are inherent in
the data and not in a specific choice of classifier.
From Table 1 we also observe that using bigrams
is consistently better than unigrams, despite the fact
that the number of unique terms rises from ?20K
to ?300K. This suggests that gender differences be-
come even more profound for phrases, a finding sim-
ilar to (Doddington, 2001) for speaker differences.
Table 1: Classification accuracy of different learn-
ing methods for the task of classifying the transcript
of a conversation side according to the gender -
male/female - of the speaker.
Unigrams Bigrams
Rocchio 76.3 86.5
Naive Bayes 83.0 89.2
MaxEnt 85.6 90.3
SVM 88.6 92.5
4.2 Does the gender of a conversation side
influence lexical usage of the other
conversation side?
Each conversation always consists of two people
talking to each other. Up to this point, we have only
attempted to analyze a conversation side in isola-
tion, i.e. without using transcriptions from the other
side. In this subsection, we attempt to assess the
degree to which, if any, the gender of one speaker
influences the language of the other speaker. In
the first experiment, instead of defining two cate-
gories we define four; the Cartesian product of the
gender of the current speaker and the gender of the
other speaker. These categories are symbolized with
two letters: the first characterizing the gender of the
current speaker and the second the gender of the
other speaker, i.e. FF, FM, MF, MM. The task re-
mains the same: given the transcript of a conver-
sation side, classify it according to the appropriate
category. This is a task much harder than the bi-
nary classification we had in subsection 4.1, because
given only the transcript of a conversation side we
must make inferences about the gender of the current
as well as the other conversation side. We have used
SVMs as the learning method. In their basic formu-
lation, SVMs are binary classifiers (although there
has been recent work on multi-class SVMs). We fol-
lowed the original binary formulation and converted
the 4-class problem to 6 2-class problems. The final
decision is taken by voting of the individual systems.
The confusion matrix of the 4-way classification is
shown in Table 2.
Table 2: Confusion matrix for 4-way classification
of gender of both sides using transcripts from one
side. Unigrams are used as features, SVMs as clas-
sification method. Each row represents the true cat-
egory and each column the hypothesized category.
FF FM MF MM F-measure
FF 1447 30 40 65 .778
FM 456 27 43 77 .074
MF 167 25 104 281 .214
MM 67 44 210 655 .638
The results show that although two of the four cat-
egories, FF and MM, are quite robustly detected the
other two, FM and MF, are mostly confused with FF
and MM respectively. These results can be mapped
to single gender detection, giving accuracy of 85.9%
for classifying the gender of the given transcript (as
in Table 1) and 68.5% for classifying the gender of
the conversational partner. The accuracy of 68.5% is
higher than chance (57.8%) showing that genders al-
ter their linguistic patterns depending on the gender
of their conversational partner.
In the next experiment we design two binary clas-
sifiers. In the first classifier, the task is to correctly
classify FF vs. MM transcripts, and in the second
classifier the task is to classify FM vs. MF tran-
scripts. Therefore, we attempt to classify the gender
of a speaker given knowledge of whether the con-
versation is same-gender or cross-gender. For both
classifiers 4526 sides were used for training equally
divided among each class. 2558 sides were used for
testing of the FF-MM classifier and 1180 sides for
the FM-MF classifier. The results are shown in Ta-
ble 3.
It is clear from Table 3 that there is a significant
difference in performance between the FF-MM and
FM-MF classifiers, suggesting that people alter their
linguistic patterns depending on the gender of the
person they are talking to. In same-gender conver-
sations, almost perfect accuracy is reached, indicat-
ing that the linguistic patterns of the two genders be-
438
Table 3: Classification accuracies in same-gender
and cross-gender conversations. SVMs are used as
the classification method; no feature selection is ap-
plied.
Unigrams Bigrams
FF-MM 98.91 99.49
FM-MF 69.15 78.90
come very distinct. In cross-gender conversations
the differences become less prominent since clas-
sification accuracy drops compared to same-gender
conversations. This result, however, does not re-
veal how this convergence of linguistic patterns is
achieved. Is it the case that the convergence is at-
tributed to one of the genders, for example males
attempting to match the patterns of females, or is it
collectively constructed? To answer this question,
we can examine the classification performance of
two other binary classifiers FF vs. FM and MM vs.
MF. The results are shown in Table 4. In both clas-
sifiers 4608 conversation sides are used for training,
equally divided in each class. The number of sides
used for testing is 989 and 689 for the FF-FM and
MM-MF classifier respectively.
Table 4: Classifying the gender of speaker B given
only the transcript of speaker A. SVMs are used as
the classification method; no feature selection is ap-
plied.
Unigrams Bigrams
FF-FM 57.94 59.66
MM-MF 60.38 59.80
The results in Table 4 suggest that both genders
equally alter their linguistic patterns to match the
opposite gender. It is interesting to see that the gen-
der of speaker B can be detected better than chance
given only the transcript and gender of speaker A.
The results are better than chance at the 0.0005 sig-
nificance level.
4.3 Are some features more indicative of
gender than other?
Having shown that gender lexical differences are
prominent enough to classify each speaker accord-
ing to gender quite robustly, another question is
whether the high classification accuracies can be at-
tributed to a small number of features or are rather
the cumulative effect of a high number of them. In
Table 5 we apply the two feature selection criteria
that were described in 3.
Table 5: Effect of feature selection criteria on gen-
der classification using SVM as the learning method.
Horizontal axis refers to the fraction of the original
vocabulary size (?20K for unigrams, ?300K for bi-
grams) that was used.
1.0 0.7 0.4 0.1 0.03
KL 1-gram 88.6 88.8 87.8 86.3 85.6
2-gram 92.5 92.6 92.2 91.9 90.3
IG 1-gram 88.6 88.5 88.9 87.6 87.0
2-gram 92.5 92.4 92.6 91.8 90.8
The results of Table 5 show that lexical differ-
ences between genders are not isolated in a small set
of words. The best results are achieved with 40%
(IG) and 70% (KL) of the features, using fewer fea-
tures steadily degrades the performance. Using the
5000 least discriminative unigrams and Naive Bayes
as the classification method resulted in 58.4% clas-
sification accuracy which is not statistically better
than chance (this is the test set of Tables 1 and 2 not
of Table 4) . Using the 15000 least useful unigrams
resulted in a classification accuracy of 66.4%, which
shows that the number of irrelevant features is rather
small, about 5K features.
It is also instructive to see which features are most
discriminative for each gender. The features that
when present are most indicative of each gender
(positive features) are shown in Table 6. They are
sorted using the KL distance and dropping the sum-
mation over both genders in equation (2). Looking
at the top 2000 features for each number we ob-
served that a number of swear words appear as
most discriminative for males and family-relation
terms are often associated with females. For ex-
ample the following words are in the top 2000 (out
of 20513) most useful features for males shit, bull-
shit, shitty, fuck, fucking, fucked, bitching, bastards,
ass, asshole, sucks, sucked, suck, sucker, damn, god-
damn, damned. The following words are in the
top 2000 features for females children, grandchild,
439
Table 6: The 10 most discriminative features for
each gender according to KL distance. Words higher
in the list are more discriminative.
Male Female
dude husband
shit husband?s
fucking refunding
wife goodness
wife?s boyfriend
matt coupons
steve crafts
bass linda
ben gosh
fuck cute
child, grandchildren, childhood, childbirth, kids,
grandkids, son, grandson, daughter, granddaugh-
ter, boyfriend, marriage, mother, grandmother. It
is also interesting to note that a number of non-
lexical tokens are strongly associated with a certain
gender. For example, [laughter] and acknowledg-
ments/backchannels such as uh-huh,uhuh were in
the top 2000 features for females. On the other hand,
filled pauses such as uh were strong male indicators.
Our analysis also reveals that a high number of use-
ful features are names. A possible explanation is
that people usually introduce themselves at the be-
ginning of the conversation. In the top 30 words per
gender, names represent over half of the words for
males and nearly a quarter for females. Nearly a
third were family-relations words for females, and
17
When examining cross-gender conversations, the
discriminative words were quite substantially differ-
ent. We can quantify the degree of change by mea-
suring KLSG(w) ?KLCG(w) where KLSG(w) is
the KL measure of word w for same-gender con-
versations. The analysis reveals that swear terms
are highly associated with male-only conversations,
while family-relation words are highly associated
with female-only conversations.
From the traditional sociolinguistic perspective,
these methods offer a way of discovering rather than
testing words or phrases that have distinct usage
between genders. For example, in a recent paper
(Kiesling, in press) the word dude is analyzed as
a male-to-male indicator. In our work, the word
dude emerged as a male feature. As another ex-
ample, our observation that some acknowledgments
and backchannels (uh-huh) are more common for fe-
males than males while the reverse is true for filled
pauses asserts a popular theory in sociolinguistics
that males assume a more dominant role than fe-
males in conversations (Coates, 1997). Males tend
to hold the floor more than women (more filled
pauses) and females tend to be more responsive
(more acknowledgments/backchannels).
4.4 Are gender-discriminative features
content-bearing words?
Do the most gender-discriminative words contribute
to the topic of the conversation, or are they simple
fill-in words with no content? Since each conversa-
tion is labeled with one of 40 possible topics, we can
rank features with IG or KL using topics instead of
genders as categories. In fact, this is the standard
way of performing feature selection for text classi-
fication. We can then compare the performance of
classifying conversations to topics using the top-N
features according to the gender or topic ranking.
The results are shown in Table 7.
Table 7: Classification accuracies using topic- and
gender-discriminative words, sorted using the infor-
mation gain criterion. When randomly selecting
5000 features, 10 independent runs were performed
and numbers reported are mean and standard devia-
tion. Using the bottom 5000 topic words resulted in
chance performance (?5.0)
Top 5K Bottom 5K Random 5K
Gender ranking 78.51 66.72 74.99?2.2
Topic ranking 87.72 - 74.99?2.2
From Table 7 we can observe that gender-
discriminative words are clearly not the most rele-
vant nor the most irrelevant features for topic clas-
sification. They are slightly more topic-relevant
features than topic-irrelevant but not by a signifi-
cant margin. The bottom 5000 features for gen-
der discrimination are more strongly topic-irrelevant
words.
These results show that gender linguistic differ-
ences are not merely isolated in a set of words that
440
would function as markers of gender identity but are
rather closely intertwined with semantics. We at-
tempted to improve topic classification by training
gender-dependent topic models but we did not ob-
serve any gains.
4.5 Can gender lexical differences be exploited
to improve automatic speech recognition?
Are the observed gender linguistic differences valu-
able from an engineering perspective as well? In
other words, can a natural language processing task
benefit from modeling these differences? In this sub-
section, we train gender-dependent language models
and compare their perplexities with standard base-
lines. An advantage of using gender information
for automatic speech recognition is that it can be
robustly detected using acoustic features. In Ta-
bles 8 and 9 the perplexities of different gender-
dependent language models are shown. The SRILM
toolkit (Stolcke, 2002) was used for training the lan-
guage models using Kneser-Ney smoothing (Kneser
and Ney, 1987). The perplexities reported include
the end-of-turn as a separate token. 2300 con-
versation sides are used for training each one of
{FF,FM,MF,MM} models of Table 8, while 7670
conversation sides are used for training each one of
{F,M} models of Table 9. In both tables, the same
1678 sides are used for testing.
Table 8: Perplexity of gender-dependent bigram lan-
guage models. Four gender categories are used.
Each column has the perplexities for a given test set,
each row for a train set.
FF FM MF MM
FF 85.3 91.1 96.5 99.9
FM 85.7 90.0 94.5 97.5
MF 87.8 91.4 93.3 95.4
MM 89.9 93.1 94.1 95.2
ALL 82.1 86.3 89.8 91.7
In Tables 8 and 9 we observe that we get lower
perplexities in matched than mismatched conditions
in training and testing. This is another way to show
that different data do exhibit different properties.
However, the best results are obtained by pooling
all the data and training a single language model.
Therefore, despite the fact there are different modes,
Table 9: Perplexity of gender-dependent bigram lan-
guage models. Two gender categories are used.
Each column has the perplexities for a given test set,
each row for a train set.
F M
F 82.8 94.2
M 86.0 90.6
ALL 81.8 89.5
the benefit of more training data outweighs the ben-
efit of gender-dependent models. Interpolating ALL
with F and ALL with M resulted in insignificant im-
provements (81.6 for F and 89.3 for M).
5 Conclusions
We have presented evidence of linguistic differences
between genders using a large corpus of telephone
conversations. We have approached the issue from
a purely computational perspective and have shown
that differences are profound enough that we can
classify the transcript of a conversation side ac-
cording to the gender of the speaker with accuracy
close to 93%. Our computational tools have al-
lowed us to quantitatively show that the gender of
one speaker influences the linguistic patterns of the
other speaker. Specifically, classifying same-gender
conversations can be done with almost perfect accu-
racy, while evidence of some convergence of male
and female linguistic patterns in cross-gender con-
versations was observed. An analysis of the fea-
tures revealed that the most characteristic features
for males are swear words while for females are
family-relation words. Leveraging these differences
in simple gender-dependent language models is not
a win, but this does not imply that more sophisti-
cated language model training methods cannot help.
For example, instead of conditioning every word in
the vocabulary on gender we can choose to do so
only for the top-N, determined by KL or IG. The
probability estimates for the rest of the words will
be tied for both genders. Future work will examine
empirical differences in other features such as dialog
acts or turntaking.
441
References
C. Cieri, D. Miller, and K. Walker. 2004. The Fisher
corpus: a resource for the next generations of speech-
to-text. In 4th International Conference on Language
Resources and Evaluation, LREC, pages 69?71.
J. Coates, editor. 1997. Language and Gender: A
Reader. Blackwell Publishers.
G. Doddington. 2001. Speaker recognition based on
idiolectal differences between speakers. In Proceed-
ings of the 7th European Conference on Speech Com-
munication and Technology (Eurospeech 2001), pages
2251?2254.
P. Eckert and S. McConnell-Ginet, editors. 2003. Lan-
guage and Gender. Cambridge University Press.
G. Forman. 2003. An extensive empirical study of fea-
ture selection metrics for text classification. Machine
Learning Research, 3:1289?1305.
S. Kiesling. in press. Dude. American Speech.
R. Kneser and H. Ney. 1987. Improved backing-off for
m-gram language modeling. In Proc. Intl. Conf. on
Acoustics, Speech and Signal Processing (ICASSP),
pages 181?184.
M. Koppel, S. Argamon, and A.R. Shimoni. 2002. Auto-
matically categorizing written texts by author gender.
Literary and Linguistic Computing, 17(4):401?412.
A. McCallum. 1996. Bow: A toolkit for statistical lan-
guage modeling, text retrieval, classification and clus-
tering. http://www.cs.cmu.edu/ mccallum/bow.
S. Singh. 2001. A pilot study on gender differences
in conversational speech on lexical richness measures.
Literary and Linguistic Computing, 16(3):251?264.
E. Stamatatos, N. Fakotakis, and G. Kokkinakis. 2000.
Automatic text categorization in terms of genre and
author. Computational Linguistics, 26:471?495.
A. Stolcke. 2002. An extensible language modeling
toolkit. In Proc. Intl. Conf. on Spoken Language Pro-
cessing (ICSLP), pages 901?904.
442
