Proceedings of the 22nd International Conference on Computational Linguistics (Coling 2008), pages 401?408
Manchester, August 2008
Using Hidden Markov Random Fields to Combine Distributional and
Pattern-based Word Clustering
Nobuhiro Kaji and Masaru Kitsuregawa
Institute of Industrial Science, University of Tokyo
4-6-1 Komaba, Meguro-ku, Tokyo 153-8505 Japan
{kaji,kitsure}@tkl.iis.u-tokyo.ac.jp
Abstract
Word clustering is a conventional and im-
portant NLP task, and the literature has
suggested two kinds of approaches to this
problem. One is based on the distribu-
tional similarity and the other relies on
the co-occurrence of two words in lexico-
syntactic patterns. Although the two meth-
ods have been discussed separately, it is
promising to combine them since they are
complementary with each other. This pa-
per proposes to integrate them using hid-
den Markov random fields and demon-
strates its effectiveness through experi-
ments.
1 Introduction
Word clustering is a technique of grouping similar
words together, and it is important for various NLP
systems. Applications of word clustering include
language modeling (Brown et al, 1992), text clas-
sification (Baker and McCallum, 1998), thesaurus
construction (Lin, 1998) and so on. Furthermore,
recent studies revealed that word clustering is use-
ful for semi-supervised learning in NLP (Miller et
al., 2004; Li and McCallum, 2005; Kazama and
Torisawa, 2008; Koo et al, 2008).
A well-known approach to grouping similar
words is to use distribution of contexts in which
target words appear. It is founded on the hypothe-
sis that similar words tend to appear in similar con-
texts (Harris, 1968). Based on this idea, some stud-
ies proposed probabilistic models for word cluster-
ing (Pereira et al, 1993; Li and Abe, 1998; Rooth
c
? 2008. Licensed under the Creative Commons
Attribution-Noncommercial-Share Alike 3.0 Unported li-
cense (http://creativecommons.org/licenses/by-nc-sa/3.0/).
Some rights reserved.
et al, 1999; Torisawa, 2002). Others proposed
distributional similarity measures between words
(Hindle, 1990; Lin, 1998; Lee, 1999; Weeds et al,
2004). Once such similarity is defined, it is trivial
to perform clustering.
On the other hand, some researchers utilized
co-occurrence for word clustering. The idea be-
hind it is that similar words tend to co-occur in
certain patterns. Considerable efforts have been
devoted to measure word similarity based on co-
occurrence frequency of two words in a window
(Church and Hanks, 1989; Turney, 2001; Terra and
Clarke, 2003; Matsuo et al, 2006). In addition to
the classical window-based technique, some stud-
ies investigated the use of lexico-syntactic patterns
(e.g., X or Y) to get more accurate co-occurrence
statistics (Chilovski and Pantel, 2004; Bollegala et
al., 2007).
These two approaches are complementary with
each other, because they are founded on different
hypotheses and utilize different corpus statistics.
Consider to cluster a set of words based on the dis-
tributional similarity. It is likely that some words
are difficult to cluster due to the data sparseness or
some other problems, while we can still expect that
those words are correctly classified using patterns.
This consideration leads us to combine distribu-
tional and pattern-based word clustering. In this
paper we propose to combine them using mixture
models based on hidden Markov random fields.
This model was originally proposed by (Basu et
al., 2004) for semi-supervised clustering. In semi-
supervised clustering, the system is provided with
supervision in the form of pair-wise constraints
specifying data points that are likely to belong to
the same cluster. These constraints are directly in-
corporated into the clustering process as a prior
knowledge. Our idea is to view the co-occurrence
401
of two words in lexico-syntactic patterns as con-
straints, and incorporate them into distributional
word clustering.
In summary, this paper discusses the problem
of integrating multiple approaches for word clus-
tering. We consider that the clustering results are
improved if multiple approaches are successfully
combined and if they are complementary with each
other. Our contribution is to provide a proba-
bilistic framework for this problem. Although our
proposal aims at combining the distributional and
pattern-based approaches, it is also applicable to
combine other approaches like (Lin et al, 2003),
as we will discuss in Section 5.4.
2 Distributional Clustering
This and next section describe distributional and
pattern-based word clustering respectively. Sec-
tion 4 will explain how to combine them.
2.1 Probabilistic model
In distributional word clustering, similarity be-
tween words (= nouns) is measured by the distribu-
tion of contexts in which they appear. As a context,
verbs that appear in certain grammatical relations
with the target nouns are typically used. Using the
distribution of such verbs, we can express a noun
n by a feature vector ?(n):
?(n) = (f
nv
1
, f
nv
2
, ...f
nv
V
)
where f
nv
i
denotes the frequency of noun-verb
pair (n, v
i
), and V denotes the number of distinct
verbs. The basic idea of using the distribution for
clustering is to group n and n? together if?(n) and
?(n
?
) are similar.
Let us consider a soft clustering model. We hy-
pothesize that ?(n) is a mixture of multinomial,
and the probability of n is defined by1
p(n) =
Z
?
z=1
p(z)p(?(n)|z)
=
Z
?
z=1
?
z
f
n
!
?
v
f
nv
!
?
v
?
f
nv
vz
where Z is the number of mixture components,
?
z
is the mixing coefficient (?
z
?
z
= 1), f
n
=
?
v
f
nv
is the total number of occurrence of n, and
1We ignored p(f
n
) by assuming that it is independent of
hidden variables. See (McCallum and Nigam, 1998) for detail
discussion.
?
vz
is the parameter of the multinomial distribu-
tion (?
v
?
vz
= 1). In this model the hidden vari-
ables can be interpreted as semantic class of nouns.
Now consider a set of nouns n = {n
i
}
N
i=1
. Let
z = {z
i
}
N
i=1
be a set of hidden variables corre-
sponding to n. Assuming that the hidden variables
are independent and n
i
is also independent of other
nouns given the hidden variables, the probability of
n is defined by
p(n) =
?
z
p(z)p(n|z)
where
p(z) =
N
?
i=1
p(z
i
)
p(n|z) =
N
?
i=1
p(n
i
|z
i
).
Hereafter, we use p(n|z) instead of p(?(n)|z) to
keep the notation simple. p(n|z) is the conditional
distribution on all nouns given all the hidden vari-
ables, and p(z) is the prior distribution on the hid-
den variables. Computing the log-likelihood of the
complete data (n, z), we found
log p(n, z) =
N
?
i=1
log p(z
i
)p(n
i
|z
i
). (1)
2.2 Parameter estimation
The parameters can be estimated by the EM algo-
rithm. In the E-step, p(z
i
|n
i
) is computed based
on current parameters. It is computed by
p(z
i
= k|n
i
) =
p(z
i
= k)p(n
i
|z
i
= k)
?
z
p(z)p(n
i
|z)
=
?
k
?
v
?
f
n
i
v
vk
?
z
?
z
?
v
?
f
n
i
v
vz
.
In the M-step, the parameters are re-estimated by
using the result of the E-step:
?
?k
=
? +
?
i
f
n
i
?
p(z
i
= k|n
i
)
?V +
?
v
?
i
f
n
i
v
p(z
i
= k|n
i
)
?
k
=
? +
?
i
p(z
i
= k|n
i
)
?Z +
?
z
?
i
p(z
i
= z|n
i
)
where ? is a smoothing factor.2 Both steps are
repeated until a convergence criteria is satisfied.
The important point to note is that the E-step can
be computed using the above equation because the
hidden variables are independent.
2
?=1.0 in our experiment.
402
X ya Y X mo Y mo X to Y to X, Y nado
(X or Y) (X and Y) (X and Y) (X, Y etc.)
Table 1: Four lexico-syntactic patterns, where X
and Y are extracted as co-occurring words. Note
that ya, mo, and to are Japanese postpositions, and
they correspond to or or and in English.
3 Pattern-based Clustering
A graph-based algorithm was employed in order to
cluster words using patterns.
3.1 Graph Construction
We first construct the graph in which vertices
and edges correspond to words and their co-
occurrences in patterns respectively (Figure 1). We
employed four lexico-syntactic patterns (Table 1)
to extract co-occurrence of two words from cor-
pus. Note that we target Japanese in this paper al-
though our proposal is independent of languages.
The edges are weighted by the strength of co-
occurrence that is computed by the Point-wise Mu-
tual Information (PMI):
PMI(n
i
, n
j
) = log
f(n
i
, n
j
)f(?, ?)
f(n
i
, ?)f(?, n
j
)
where f(n
i
, n
j
) is the co-occurrence frequency
of two nouns, and ??? means summation over all
nouns. If PMI is less than zero, the edge is re-
moved.
3.2 Graph Partitioning
Assuming that similar words tend to co-occur in
the lexico-syntactic patterns, it is reasonable to
consider that a dense subgraph is a good cluster
(Figure 1). Following (Matsuo et al, 2006), we
exploit the Newman clustering (Newman, 2004) to
partition the graph into such dense subgraphs.
We start by describing Newman?s algorithm for
unweighted graphs and we will generalize it to
weighted graphs later. The Newman clustering is
an algorithm that divides a graph into subgraphs
based on connectivity. Roughly speaking, it di-
vides a graph such that there are a lot of edges be-
tween vertices in the same cluster. In the algorithm
goodness of clustering is measured by score Q:
Q =
?
i
(
e
ii
? a
2
i
)
ramen
dumpling
pasta
steak
Japan U.S.A.
Germany
China
France
Figure 1: An example of the graph consisting of
two dense subgraphs.
where
e
ij
=
# of edges between two vertices in cluster i and j
# of all edges
a
i
=
?
k
e
ik
.
The term e
ij
is the fraction of edges between clus-
ter i and j. a
i
is the sum of e
ik
over all clusters,
and a2
i
represents the expected number of fraction
of edges within the cluster i when edges are given
at random. See (Newman, 2004) for the detail.
The Newman clustering optimizes Q in an ag-
glomerative fashion. At the beginning of the algo-
rithm every vertex forms a singleton cluster, and
we repeatedly merge two clusters so that the join
results in the largest increase in Q. The change in
Q when cluster i and j are merged is given by
?Q = e
ij
+ e
ji
? 2a
i
a
j
= 2(e
ij
? a
i
a
j
).
The above procedure is repeated until Q reaches
local maximum.
The algorithm can be easily generalized to
weighted graphs by substituting ?sum of weights
of edges? for ?# of edges? in the definition of e
ij
.
The other part of the algorithm remains the same.
4 Integration based on Hidden Markov
Random Fields
This section represents how to integrate the distri-
bution and pattern for word clustering.
4.1 Background and idea
Clustering has long been discussed as an unsu-
pervised learning problem. In some applications,
however, it is possible to provide some form of
supervision by hand in order to improve the clus-
tering result. This motivated researchers to inves-
tigate semi-supervised clustering, which uses not
only unlabeled data but supervision in the form of
pair-wise constraints (Basu et al, 2004). In this
403
framework, the clustering system is provided with
a set of pair-wise constraints specifying data points
that are likely to belong to the same cluster. These
constraints are directly incorporated into the clus-
tering process as a prior knowledge.
Our idea is to view the co-occurrence of two
words in lexico-syntactic patterns as constraints,
and incorporate them into the distributional clus-
tering. The rest of this section describes how to ex-
tend the distributional clustering so as to incorpo-
rate the constraints, and how to generate the con-
straints using the patterns.
4.2 Probabilistic model
Let C be a set of pair-wise constraints, and con-
sider to incorporate the constraints into the distri-
butional clustering (Section 2). In what follows we
assume each constraint ?i, j? ? C represents that
z
i
and z
j
are likely to have the same value, and it is
associated with a weight w
ij
(> 0) corresponding
to a penalty for constraint violation.
It is easy to extend the distributional cluster-
ing algorithm so as to incorporate the constraints.
This is done by just changing the prior distribution
on hidden variables p(z). Following (Basu et al,
2004), we construct the Markov random field on
the hidden variables so as to incorporate the con-
straints. The new prior distribution is defined as
p(z) =
N
?
i=1
p(z
i
) ?
1
G
exp{?
?
?i,j??C
?(z
i
6= z
j
)w
ij
}
where ?(?) is the delta function. ?(z
i
6= z
j
) takes
one if the constraint ?i, j? is violated and otherwise
zero. G is the normalization factor of the Markov
random field (the second term).
By examining the log-likelihood of the complete
data, we can see how violation of constraints is pe-
nalized. Using the new prior distribution, we get
log p(n, z) =
N
?
i=1
log p(z
i
)p(n
i
|z
i
)
?
?
?i,j??C
?(z
i
6= z
j
)w
ij
? log G.
The first term in the right-hand side is equal to the
log-likelihood of the multinomial mixture, namely
equation (1). The second term can be interpreted
as the penalty for constraint violation. The last
term is a constant.
It is worth pointing out that the resulting algo-
rithm makes a soft assignment and polysemous
words can belong to more than one clusters.
4.3 Parameter estimation
The parameters are estimated by the EM algo-
rithm. The M-step is exactly the same as discussed
in Section 2.2. The problem is that the hidden vari-
ables are no longer independent and the E-step re-
quires the calculation of
p(z
i
|n) =
?
z
?i
p(z
?i
, z
i
|n)
?
?
z
?i
p(z
?i
, z
i
)p(n|z
?i
, z
i
)
where z
?i
means all hidden variables but z
i
. The
computation of the above equation is intractable
because the summation in it requires O(ZN?1) op-
erations.
Instead of exactly computing p(z
i
|n), we ap-
proximate it by using the mean field approximation
(Lange et al, 2005). In the mean field approxima-
tion, p(z|n) is approximated by a factorized distri-
bution q(z), in which all hidden variables are inde-
pendent:
q(z) =
N
?
i=1
q
i
(z
i
). (2)
Using q(z) instead of p(z|n), computation of the
E-step can be written as follows:
p(z
i
|n) '
?
z
?i
q(z
?i
, z
i
) = q
i
(z
i
). (3)
The parameters of q(z) are determined such that
the KL divergence between q(z) and p(z|n) is
minimized. In other words, the approximate dis-
tribution q(z) is determined by minimizing
?
z
q(z) log
q(z)
p(z|n)
(4)
under the condition that
?
k
q
i
(z
i
= k) = 1
for all i. This optimization problem can be re-
solved by introducing Lagrange multipliers. Be-
cause we cannot get the solution in closed form, an
iterative method is employed. Taking the deriva-
tive of equation (4) with respect to a parameter
q
ik
= q
i
(z
i
= k) and setting it to zero, we get
the following updating formula:
q
(t+1)
ik
? p(n
i
, k) exp{?
?
j?N
i
(1 ? q
(t)
jk
)w
ij
} (5)
404
where N
i
= {j|?i, j? ? C} and q(t)
ik
is the value of
q
ik
at t-th iteration. The derivation of this formula
is found in Appendix.
4.4 Generation of constraints
It is often pointed out that even small amounts of
misspecified constraints significantly decrease the
performance of semi-supervised clustering. This
is because the error of misspecified constraints is
propagated to the entire transitive neighborhoods
of the constrained data (Nelson and Cohen, 2007).
As an example, consider that we have two con-
straints ?i, j? and ?j, k?. If the former is misspeci-
fied one, the error propagate to k through j.
To tackle this problem, we propose a technique
to put an upper bound ? on the size of the transitive
neighborhoods. Our constraint generation process
is as follows. To begin with, we modified the New-
man clustering so that the maximum cluster size
does not exceed ?. This can be done by prohibit-
ing such merge that results in larger cluster than
?. Given the result of the modified Newman clus-
tering, it is straightforward to generate constraints.
Constraints are generated between two nouns in
the same cluster if they co-occur in the lexico-
syntactic patterns at least one time. The penalty
for constraint violation w
ij
was set to PMI(n
i
, n
j
).
This procedure obviously ensures that the size of
the transitive neighborhoods is less than ?.
5 Experiments
5.1 Data sets
We parsed 15 years of news articles by KNP3 so
as to obtain data sets for the distributional and
pattern-based word clustering (Table 2). The num-
ber of distinct nouns in total was 297,719. Note
that, due to the computational efficiency, we re-
moved such nouns that appeared less than 10 times
with verbs and did not appear at all in the patterns.
A test set was created using manually tailored
Japanese thesaurus (Ikehara et al, 1997). We ran-
domly selected 500 unambiguous nouns from 25
categories (20 words for each category).
5.2 Baselines
For comparison we implemented the following
baseline systems.
? The multinomial mixture (Section 2).
? The Newman clustering (Newman, 2004).
3http://nlp.kuee.kyoto-u.ac.jp/nl-resource/
nouns 208,934
verbs 64,954
noun-verb pairs 4,804,715
nouns 245,465
noun-noun pairs 633,302
Table 2: Data sets statistics. The first and second
row shows the number of distinct words (and word
pairs) used for the distributional and pattern-based
word clustering respectively.
? Three K-means algorithms using different
distributional similarity or dissimilarity mea-
sures: cosine, ?-skew divergence (Lee,
1999)4, and Lin?s similarity (Lin, 1998).
? The CBC algorithm (Lin and Pantel, 2002;
Pantel and Lin, 2002).
5.3 Evaluation procedure
All the nouns in the data set were clustered by the
proposed and baseline systems.5 For the mixture
models and K-means, the number of clusters was
set to 1,000. The parameter ? was set to 100.
The result was assessed by precision and recall
using the test data. The precision and recall were
computed by the B-CUBED algorithm as follows
(Bagga and Baldwin, 1998). For each noun n
i
in
the test data, precision
i
and recall
i
are defined as
precision
i
=
|S
i
? T
i
|
|S
i
|
recall
i
=
|S
i
? T
i
|
| T
i
|
where S
i
is the system generated cluster contain-
ing n
i
and T
i
is the goldstandard cluster containing
n
i
. The precision and recall are defined as an av-
erage of precision
i
and recall
i
for all the nouns in
the test data respectively. The result of soft clus-
tering models cannot be directly evaluated by the
precision and recall. In such cases, each noun is
assigned to the cluster that maximizes p(z|n).
5.4 The result and discussion
Table 3 shows the experimental results. The best
results for each statistic are shown in bold. For the
mixture models and K-means, the precision and re-
call are an average of 10 trials.
Table 3 demonstrates the impact of combining
distribution and pattern. Our method outperformed
4
? = 0.99 in our experiment.
5Our implementation is available from
http://www.tkl.iis.u-tokyo.ac.jp/?kaji/clustering.
405
P R F
1
proposed .383 .437 .408
multinomial mixture .360 .374 .367
Newman (2004) .318 .353 .334
cosine .603 .114 .192
?-skew divergence (Lee, 1999) .730 .155 .255
Lin?s similarity (Lin, 1998) .691 .096 .169
CBC (Lin and Pantel, 2002) .981 .060 .114
Table 3: Precision, recall, and F-measure.
all the baseline systems. It was statistically signif-
icantly better than the multinomial mixture (P <
0.01, Mann-Whitney U-test). Note that it is possi-
ble to improve some baseline systems, especially
CBC, by tuning the parameters. For CBC we sim-
ply used the same parameter values as reported in
(Lin and Pantel, 2002).
Compared with the multinomial mixture, one
advantage of our method is that it has broad cov-
erage. Our method can successfully handle un-
known words, which do not appear with verbs at
all (i.e., f
n
= 0 and ?(n) is zero vector), if they
co-occur with other words in the lexico-syntactic
patterns. For unknown words, the hidden variables
are determined based only on p(z) because p(n|z)
takes the same value for all hidden variables. This
means that our method clusters unknown words
using pair-wise constraints. On the other hand,
the multinomial mixture assigns all the unknown
words to the cluster that maximizes p(z).
The test set included 51 unknown words.6 We
split the test set into two parts: f
n
= 0 and f
n
6= 0,
and calculated precision and recall for each subset
(Table 4). Although the improvement is especially
significant for the unknown words, we can clearly
confirm the improvement for both subsets. For the
Newman clustering we can discuss similar things
(Table 5). Different from the Newman clustering,
our method can handle nouns that do not co-occur
with other nouns if 0 < f
n
. In this case the test set
included 64 unknown words.
It is interesting to point out that our framework
can further incorporate lexico-syntactic patterns
for dissimilar words (Lin et al, 2003). Namely,
we can use patterns so as to prevent distribution-
ally similar but semantically different words (e.g.,
ally and supporter (Lin et al, 2003)) from being as-
signed to the same cluster. This can be achieved by
using cannot-link constraints, which specify data
points that are likely to belong to different clus-
6The baseline systems assigned the unknown words to a
default cluster as the multinomial mixture does.
f
n
= 0 f
n
6= 0
P R F
1
P R F
1
proposed .320 .632 .435 .412 .450 .430
multi. .099 1.000 .181 .402 .394 .398
Table 4: Detail comparison with the multinomial
mixture.
f(n
i
, ?) = 0 f(n
i
, ?) 6= 0
P R F
1
P R F
1
proposed .600 .456 .518 .380 .479 .424
Newman .071 1.000 .133 .354 .412 .381
Table 5: Detail comparison with the Newman clus-
tering.
ters (Basu et al, 2004). The remaining problem
is which patterns to use so as to extract dissimilar
words. Although this problem has already been
discussed by (Lin et al, 2003), they mainly ad-
dressed antonyms. We believe that a more exhaus-
tive investigation is required. In addition, it is still
unclear whether dissimilar words are really useful
to improve clustering results.
One problem that we did not examine is how to
determine optimal number of clusters. In the ex-
periment, the number was decided with trial-and-
error through our initial experiment. We leave it
as our future work to test methods of automat-
ically determining the cluster number (Pedersen
and Kulkarni, 2006; Blei and Jordan, 2006).
6 Related work
As far as we know, the distributional and pattern-
based word clustering have been discussed inde-
pendently (e.g., (Pazienza et al, 2006)). One of
the most relevant work is (Bollegala et al, 2007),
which proposed to integrate various patterns in or-
der to measure semantic similarity between words.
Although they extensively discussed the use of pat-
terns, they did not address the distributional ap-
proach.
Mirkin (2006) pointed out the importance of
integrating distributional similarity and lexico-
syntactic patterns, and showed how to combine the
two approaches for textual entailment acquisition.
Although their work inspired our research, we dis-
cussed word clustering, which is related to but dif-
ferent from entailment acquisition.
Lin (2003) also proposed to use both distribu-
tional similarity and lexico-syntactic patterns for
finding synonyms. However, they present an oppo-
site viewpoint from our research. Their proposal
is to exploit patterns in order to filter dissimilar
406
words. As we have already discussed, the integra-
tion of such patterns can also be formalized using
similar probabilistic model to ours.
A variety of studies discussed determining po-
larity of words. Because this problem is ternary
(positive, negative, and neutral) classification of
words, it can be seen as one kind of word clus-
tering. The literature suggested two methods of
determining polarity, and they are analogous to the
distributional and co-occurrence-based approaches
in word clustering (Takamura et al, 2005; Hi-
gashiyama et al, 2008). We consider it is also
promising to integrate them for polarity determi-
nation.
7 Conclusion
The distributional and pattern-based word cluster-
ing have long been discussed separately despite
the potentiality for their integration. In this paper,
we provided a probabilistic framework for com-
bining the two approaches, and demonstrated that
the clustering result is significantly improved.
Our important future work is to extend current
framework so as to incorporate patterns for dissim-
ilar words using cannot-link constraints. We con-
sider such patterns further improve the clustering
result.
Combining distribution and pattern is important
for other NLP problems as well (e.g., entailment
acquisition, polarity determination). Although this
paper examined word clustering, we consider a
part of our idea can be applied to other problems.
Acknowledgement
This work was supported by the Comprehensive
Development of e-Society Foundation Software
program of the Ministry of Education, Culture,
Sports, Science and Technology, Japan.
References
Bagga, Amit and Breck Baldwin. 1998. Entity-based
cross-document coreferencing using the vector space
model. In Proceedings of ACL, pages 79?85.
Baker, L. Douglas and Andrew Kachites McCallum.
1998. Distributional clustering of words for text
classification. In Proceedings of SIGIR, pages 96?
103.
Basu, Sugato, Mikhail Bilenko, and Raymond J.
Mooney. 2004. A probabilistic framework for semi-
supervised clustering. In Proceedings of SIGKDD,
pages 59?68.
Blei, David M. and Michael I. Jordan. 2006. Vari-
ational inference for Dirichlet process mixtures.
Bayesian Analysis, 1(1):121?144.
Bollegala, Danushka, Yutaka Matsuo, and Mitsuru
Ishizuka. 2007. An integrated approach to mea-
suring semantic similarity between words using in-
formation available on the web. In Proceedings of
NAACL, pages 340?347.
Brown, Peter F., Vincent J. Della Pietra, Peter V. deS-
ouza, Jenifer C. Lai, and Rober L. Mercer. 1992.
Class-based n-gram models of natural language.
Computational Linguistics, 18(4):467?479.
Chilovski, Timothy and Patrick Pantel. 2004. VER-
BOCEAN: Mining the web for fine-grained semantic
verb relations. In Proceedings of EMNLP, pages 33?
40.
Church, KennethWard and Patrick Hanks. 1989. Word
association norms, mutual information, and lexicog-
raphy. In Proceedings of ACL, pages 76?83.
Harris, Zellig. 1968. Mathematical Structure of Lan-
guage. New York: Wiley.
Higashiyama, Masahiko, Kentaro Inui, and Yuji Mat-
sumoto. 2008. Learning polarity of nouns by se-
lectional preferences of predicates (in Japanese). In
Proceedings of the Association for NLP, pages 584?
587.
Hindle, Donald. 1990. Noun classification from
predicate-argument structure. In Proceedings of
ACL, pages 268?275.
Ikehara, Satoru, Masahiro Miyazaki, Satoshi Shirai,
Akio Yokoo, Hiromi Nakaiwa, Kentarou Ogura,
and Yoshifumi Oyama Yoshihiko Hayashi, editors.
1997. Japanese Lexicon. Iwanami Publishing.
Kazama, Jun?ichi and Kentaro Torisawa. 2008. Induc-
ing gazetteers for named entity recognition by large-
scale clustering of dependency relations. In Pro-
ceedings of ACL, pages 407?415.
Koo, Terry, Xavier Carreras, and Michael Collins.
2008. Simple semi-supervised dependency parsing.
In Proceedings of ACL, pages 595?603.
Lange, Tilman, Martin H.C. Law, Anil K. Jain, and
Joachim M. Buhmann. 2005. Learning with con-
strained and unlabelled data. In Proceedings of
CVPR, pages 731?738.
Lee, Lillian. 1999. Measures of distributional similar-
ity. In Proceedings of ACL, pages 25?32.
Li, Hang and Naoki Abe. 1998. Word clustering and
disambiguation based on co-occurrence. InProceed-
ings of ACL-COLING, pages 749?755.
Li, Wei and Andrew McCallum. 2005. Semi-
supervised sequence modeling with syntactic topic
models. In Proceedings of AAAI, pages 813?818.
407
Lin, Dekang and Patrick Pantel. 2002. Concept discov-
ery from text. In Proceeodings of COLING, pages
577?583.
Lin, Dekang, Shaojun Zhao, Lijuan Qin, and Ming
Zhou. 2003. Identifying synonyms among distri-
butionally similar words. In Proceedings of IJCAI,
pages 1492?1493.
Lin, Dekang. 1998. Automatic retrieval and clustering
of similar words. In Proceedings of ACL-COLING,
pages 768?774.
Matsuo, Yutaka, Takeshi Sakaki, Koki Uchiyama, and
Mitsuru Ishizuka. 2006. Graph-based word cluster-
ing using a web search engine. In Proceedings of
EMNLP, pages 542?550.
McCallum, Andrew and Kamal Nigam. 1998. A com-
parison of event models for naive Bayes text classifi-
cation. In Proceedings of AAAI Workshop on Learn-
ing for Text Categorization, pages 41?48.
Miller, Scott, Jethran Guinness, and Alex Zamanian.
2004. Name tagging with word clusters and discrim-
inative training. In Proceedings of NAACL, pages
579?586.
Mirkin, Shachar, Ido Dagan, andMaayan Geffet. 2006.
Integrating pattern-based and distributional similar-
ity methods for lexical entailment acquisition. In
Proceedings of COLING-ACL Poster Sessions, pages
579?586.
Nelson, Blaine and Ira Cohen. 2007. Revisiting prob-
abilistic models for clustering with pair-wise con-
straints. In Proceedings of ICML, pages 673?680.
Newman, Mark. 2004. Fast algorithm for detecting
community structure in networks. In Phys. Rev. E
69.
Pantel, Patrick and Dekang Lin. 2002. Discovering
word senses from text. In Proceedings of SIGKDD,
pages 613?619.
Pazienza, Maria Teresa, Marco Pennacchiotti, and
Fabio Massimo Zanzotto. 2006. Discovering
verb relations in corpora: Distributional versus
non-distributional approaches. In Proceedings of
IEA/AIE, pages 1042?1052.
Pedersen, Ted and Anagha Kulkarni. 2006. Automatic
cluster stopping with criterion functions and the gap
statistic. In Proceedings of HLT/NAACL, Compan-
ion Volume, pages 276?279.
Pereira, Fernando, Naftali Tishby, and Lillian Lee.
1993. Distributional clustering of english words. In
Proceedings of ACL, pages 183?190.
Rooth, Mats, Stefan Riezler, Detlef Prescher, Glenn
Garrroll, and Franz Beil. 1999. Inducing a semanti-
cally annotated lexicon via EM-based clustering. In
Proceedings of ACL, pages 104?111.
Takamura, Hiroya, Takashi Inui, and Manabu Oku-
mura. 2005. Extracting semantic orientations of
words using spin model. In Proceedings of ACL,
pages 133?140.
Terra, Egidio and C.L.A. Clarke. 2003. Frequency es-
timates for statistical word similarity measures. In
Proceedings of NAACL, pages 165?172.
Torisawa, Kentaro. 2002. An unsupervised learning
method for associative relationships between verb
phrases. In Proceedings of COLING, pages 1009?
1015.
Turney, Peter. 2001. Mining the web for synonyms:
PMI-IR versus LSA on TOEFL. In Proceedings of
ECML, pages 491?502.
Weeds, Julie, David Weir, and Diana McCarthy. 2004.
Characterising measures of lexical distributional
similarity. In Proceedings of COLING, pages 1015?
1021.
Appendix. Derivation of the updating
formula
We can rewrite equation (4) as follows:
(4) =
?
z
q(z) log q(z) (6)
?
?
z
q(z)
N
?
i=1
log p(n
i
, z
i
) (7)
+
?
z
q(z)
?
?i,j??C
?(z
i
6= z
j
)w
ij
(8)
+ const (9)
where we made use of the fact that log p(z|n) =
log p(n|z)p(z) + const. Taking the derivative of
equation (6), (7), and (8) with respect to q
ik
, we
found
?(6)
?q
ik
= log q
ik
+ const
?(7)
?q
ik
= ? log p(n
i
, k) + const
?(8)
?q
ik
=
?
z
?i
q(z
?i
)
?
j?N
i
?(z
j
6= k)w
ij
+ const
=
?
j?N
i
?
z
?i
q(z
?i
)?(z
j
6= k)w
ij
+ const
=
?
j?N
i
(1 ? q
jk
)w
ij
+ const
where const denotes terms independent of k. Mak-
ing use of these results, the updating formula can
be derived by taking the derivative of equation (4)
with respect to q
ik
and setting it to zero.
408
Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational
Natural Language Learning, pp. 1075?1083, Prague, June 2007. c?2007 Association for Computational Linguistics
Building Lexicon for Sentiment Analysis from Massive Collection of HTML
Documents
Nobuhiro Kaji and Masaru Kitsuregawa
Institute of Industrial Science, University of Tokyo
4-6-1 Komaba, Meguro-ku, Tokyo 153-8505 Japan
 kaji,kitsure@tkl.iis.u-tokyo.ac.jp
Abstract
Recognizing polarity requires a list of po-
lar words and phrases. For the purpose of
building such lexicon automatically, a lot of
studies have investigated (semi-) unsuper-
vised method of learning polarity of words
and phrases. In this paper, we explore to
use structural clues that can extract polar
sentences from Japanese HTML documents,
and build lexicon from the extracted po-
lar sentences. The key idea is to develop
the structural clues so that it achieves ex-
tremely high precision at the cost of recall.
In order to compensate for the low recall,
we used massive collection of HTML docu-
ments. Thus, we could prepare enough polar
sentence corpus.
1 Introduction
Sentiment analysis is a recent attempt to deal with
evaluative aspects of text. In sentiment analysis, one
fundamental problem is to recognize whether given
text expresses positive or negative evaluation. Such
property of text is called polarity. Recognizing po-
larity requires a list of polar words and phrases such
as ?good?, ?bad? and ?high performance? etc. For
the purpose of building such lexicon automatically, a
lot of studies have investigated (semi-) unsupervised
approach.
So far, two kinds of approaches have been pro-
posed to this problem. One is based on a the-
saurus. This method utilizes synonyms or glosses of
a thesaurus in order to determine polarity of words
(Kamps et al, 2004; Hu and Liu, 2004; Kim and
Hovy, 2004; Esuli and Sebastiani, 2005). The sec-
ond approach exploits raw corpus. Polarity is de-
cided by using co-occurrence in a corpus. This is
based on a hypothesis that polar phrases conveying
the same polarity co-occur with each other. Typ-
ically, a small set of seed polar phrases are pre-
pared, and new polar phrases are detected based on
the strength of co-occurrence with the seeds (Hatzi-
vassiloglous and McKeown, 1997; Turney, 2002;
Kanayama and Nasukawa, 2006).
As for the second approach, it depends on the
definition of co-occurrence whether the hypothe-
sis is appropriate or not. In Turney?s work, the
co-occurrence is considered as the appearance in
the same window (Turney, 2002). Although this
idea is simple and feasible, there is a room for im-
provement. According to Kanayama?s investiga-
tion, the hypothesis is appropriate in only 60% of
cases if co-occurrence is defined as the appearance
in the same window1. In Kanayama?s method, the
co-occurrence is considered as the appearance in
intra- or inter-sentential context (Kanayama and Na-
sukawa, 2006). They reported that the precision was
boosted to 72.2%, but it is still not enough. There-
fore, we think that the above hypothesis is often in-
appropriate in practice, and this fact is the biggest
obstacle to learning lexicon from corpus.
In this paper, we explore to use structural clues
that can extract polar sentences from Japanese
HTML documents, and build lexicon from the ex-
1To be exact, the precision depends on window size and
ranges from 59.7 to 64.1%. See Table 4 in (Kanayama and Na-
sukawa, 2006) for the detail.
1075
Figure 1: Overview of the proposed method.
kono
this
software-no
software-POST
riten-ha
advantage-POST
hayaku
quickly
ugoku
run
koto-desu
to-POST
The advantage of this software is to run quickly.
Figure 2: Language structure.
tracted polar sentences. An overview of the pro-
posed method is represented in Figure 1. First, po-
lar sentences are extracted from HTML documents
by using structural clues (step 1). The set of po-
lar sentences is called polar sentence corpus. Next,
from the polar sentence corpus, candidates of po-
lar phrases are extracted together with their counts
in positive and negative sentences (step 2). Finally,
polar phrases are selected from the candidates and
added to our lexicon (step 3).
The key idea is to develop the structural clues so
that it achieves extremely high precision at the cost
of recall. As we will see in Section 2.3, the precision
was extremely high. It was around 92% even if am-
biguous cases were considered as incorrect. In order
to compensate for the low recall, we used massive
collection of HTML documents. Thus, we could
build enough polar sentence corpus. To be specific,
we extracted 500,000 polar sentences from one bil-
lion HTML documents.
The contribution of this paper is to empirically
show the effectiveness of an approach that makes
use of the strength of massive data. Nowadays, ter-
abyte is not surprisingly large, and larger corpus
would be obtained in the future. Therefore, we think
this kind of research direction is important.
2 Extracting Polar Sentences
Our method begins by automatically constructing
polar sentence corpus with structural clues (step 1).
The basic idea is exploiting certain language and
layout structures as clues to extract polar sentences.
The clues were carefully chosen so that it achieves
high precision. The original idea was represented in
our previous paper (Kaji and Kitsuregawa, 2006).
2.1 Language structure
Some polar sentences are described by using char-
acteristic language structures. Figure 2 illustrates
such Japanese polar sentence attached with English
translations. Japanese are written in italics and ?-
? denotes that the word is followed by postposi-
tional particles. For example, ?software-no? means
that ?software? is followed by postpositional particle
?no?. The arrow represents dependency relationship.
Translations are shown below the Japanese sentence.
?-POST? means postpositional particle.
What characterizes this sentence is the singly un-
derlined phrase. In this phrase, ?riten (advantage)?
is followed by postpositional particle ?-ha?, which is
Japanese topic marker. And hence, we can recognize
that something positive is the topic of the sentence.
This kind of linguistic structure can be recognized
by lexico-syntactic pattern. Hereafter, such words
like ?riten (advantage)? are called cue words.
1076
In order to handle the language structures, we uti-
lized lexico-syntactic patterns as illustrated below.
riten-ha
advantage-POST
(polar) koto-desu
to-POST
A sub-tree that matches (polar) is extracted as po-
lar sentence. It is obvious whether the polar sen-
tence is positive or negative one. In case of Figure
2, the doubly underlined part is extracted as polar
sentence2.
Besides ?riten (advantage)?, other cue words were
also used. A list of cue words (and phrases) were
manually created. For example, we used ?pros? or
?good point? for positive sentences, and ?cons?, ?bad
point? or ?disadvantage? for negative ones. This list
is also used when dealing with layout structures.
2.2 Layout structure
Two kinds of layout structures are utilized as clues.
The first clue is the itemization. In Figure 3, the
itemizations have headers and they are cue words
(?pros? and ?cons?). Note that we illustrated trans-
lations for the sake of readability. By using the cue
words, we can recognize that polar sentences are de-
scribed in these itemizations.
The other clue is table structure. In Figure 4, a
car review is summarized in the table format. The
left column acts as a header and there are cue words
(?plus? and ?minus?) in that column.
Pros:
  The sound is natural.
  Music is easy to find.
  Can enjoy creating my favorite play-lists.
Cons:
  The remote controller does not have an LCD dis-
play.
  The body gets scratched and fingerprinted easily.
  The battery drains quickly when using the back-
light.
Figure 3: Itemization structure.
2To be exact, the doubly underlined part is polar clause.
However, it is called polar sentence because of the consistency
with polar sentences extracted by using layout structures.
Mileage(urban) 7.0km/litter
Mileage(highway) 9.0km/litter
Plus This is a four door car, but it?s
so cool.
Minus The seat is ragged and the light
is dark.
Figure 4: Table structure.
It is easy to extract polar sentences from the item-
ization. Such itemizations as illustrated in Figure 3
can be detected by using the list of cue words and
HTML tags such as  h1 and  ul etc. Three
positive and negative sentences are extracted respec-
tively from Figure 3.
As for table structures, two kinds of tables are
considered (Figure 5). In the Figure,   and   rep-
resent positive and negative polar sentences, and 
 
and 
 
represent cue words. Type A is a table in
which the leftmost column acts as a header. Figure
4 is categorized into this type. Type B is a table in
which the first row acts as a header.
Type A
 
 
 
 
 

Type B
 
 
 
 
  
Figure 5: Two types of table structures.
In order to extract polar sentences, first of all, it
is necessary to determine the type of the table. The
table is categorized into type A if there are cue words
in the leftmost column. The table is categorized into
type B if it is not type A and there are cue words in
the first row. After the type of the table is decided,
we can extract polar sentences from the cells that
correspond to   and   in the Figure 5.
2.3 Result of corpus construction
The method was applied to one billion HTML doc-
uments. In order to get dependency tree, we used
KNP3. As the result, 509,471 unique polar sentences
were obtained. 220,716 are positive and the others
are negative4. Table 1 illustrates some translations
of the polar sentences.
3http://nlp.kuee.kyoto-u.ac.jp/nl-resource/knp.html
4The polar sentence corpus is available from
http://www.tkl.iis.u-tokyo.ac.jp/?kaji/acp/.
1077
Table 1: Examples of polar sentences.
Polarity Polar sentence
It becomes easy to compute cost.
positive It?s easy and can save time.
The soup is rich and flavorful.
Cannot use mails in HTML format.
negative The lecture is really boring.
There is no impressive music.
In order to investigate the quality of the corpus,
two human judges (judge A/B) assessed 500 polar
sentences in the corpus. According to the judge
A, the precision was 91.4%. 459 out of 500 polar
sentences were regarded as valid ones. According
to the judge B, the precision was 92.0% (460/500).
The agreement between the two judges was 93.5%
(Kappa value was 0.90), and thus we can conclude
that the polar sentence corpus has enough quality
(Kaji and Kitsuregawa, 2006).
After error analysis, we found that most of the er-
rors are caused by the lack of context. The following
is a typical example.
There is much information.
This sentence is categorized into positive one in the
corpus, and it was regarded as invalid by both judges
because the polarity of this sentence is ambiguous
without context.
As we described in Section 1, the hypothesis of
co-occurrence based method is often inappropriate.
(Kanayama and Nasukawa, 2006) reported that it
was appropriate in 72.2% of cases. On the other
hand, by using extremely precise clues, we could
build polar sentence corpus that have high preci-
sion (around 92%). Although the recall of structural
clues is low, we could build large corpus by using
massive collection of HTML documents. Of course,
we cannot directly compare these two percentages.
We think, however, the high precision of 92% im-
plies the strength of our approach.
3 Acquisition of Polar Phrases
The next step is to acquire polar phrases from the
polar sentence corpus (step 2 and 3 in Figure 1).
3.1 Counting candidates
From the corpus, candidates of polar phrases are ex-
tracted together with their counts (step 2).
As is often pointed out, adjectives are often used
to express evaluative content. Considering that po-
larity of isolate adjective is sometimes ambiguous
(e.g. high), not only adjectives but also adjective
phrases (noun + postpositional particle + adjective)
are treated as candidates. Adjective phrases are ex-
tracted by the dependency parser. To handle nega-
tion, an adjective with negation words such as ?not?
is annotated by  NEGATION tag. For the sake of
readability, we simply represent adjective phrases in
the form of ?noun-adjective? by omiting postposi-
tional particle, as in the Figure 1.
For each candidate, we count the frequency in
positive and negative sentences separately. Intu-
itively, we can expect that positive phrases often ap-
pear in positive sentences, and vice versa. However,
there are exceptional cases as follows.
Although the price is high, its shape is
beautiful.
Although this sentence as a whole expresses posi-
tive evaluation and it is positive sentence, negative
phrase ?price is high? appears in it. To handle this,
we hypothesized that positive/negative phrases tend
to appear in main clause of positive/negative sen-
tences, and we exploited only main clauses to count
the frequency.
3.2 Selecting polar phrases
For each candidate, we determine numerical value
indicating the strength of polarity, which is referred
as polarity value. On the basis of this value, we se-
lect polar phrases from the candidates and add them
to our lexicon (step 3).
For each candidate , we can create a contingency
table as follows.
Table 2: Contingency table
 
 	
  	
 
 	
  	
 
 	 is the frequency of  in positive sentences.
 	 is that of all candidates but .  

and  
 are similarly decided.
From this contingency table, ?s polarity value is
determined. Two ideas are examined for compari-
1078
son. One is based on chi-square value and the other
is based on Pointwise Mutual Information (PMI).
Chi-square based polarity value The chi-square
value is a statistical measure used to test the null hy-
pothesis that, in our case, the probability of a candi-
date in positive sentences is equal to the probability
in negative sentences. Given Table 2, the chi-square
value is calculated as follows.


 
 
 
 
	

   

 


 
Here,   is the expected value of   under
the null hypothesis.
Although   indicates the strength of
bias toward positive or negative sentences, its di-
rection is not clear. We determined polarity value
so that it is greater than zero if  appears in posi-
tive sentences more frequently than in negative sen-
tences and otherwise it is less than zero.


 



  
    	
 

 	

 	 is ?s probability in positive sentences, and
 
 is that in negative sentences. They are es-
timated by using Table 2.
 	 
 	
 	    	
 
 
 

 
    

PMI based polarity value Using PMI, the
strength of association between  and positive sen-
tences (and negative sentences) is defined as follows
(Church and Hanks, 1989).
 	  	

  	
  	
 
  	

  

  

PMI based polarity value is defined as their differ-
ence. This idea is the same as (Turney, 2002).


   	    

 	

  	 	
  
 

 	

 	
 

 	 and  
 are estimated in the same
way as shown above. 

 is (log of) the ra-
tio of ?s probability in positive sentences to that in
negative sentences. This formalization follows our
intuition. Similar to 


, 

 is greater
than zero if  
    	, otherwise it is
less than zero.
Selecting polar phrases By using polarity value
and threshold  , it is decided whether a can-
didate  is polar phrase or not. If     , the
candidate is regarded as positive phrase. Similarly, if
     , it is regarded as negative phrase. Oth-
erwise, it is regarded as neutral. Only positive and
negative phrases are added to our lexicon. By chang-
ing , the trade-off between precision and recall can
be adjusted. In order to avoid data sparseness prob-
lem, if both  	 and  
 are less than
three, such candidates were ignored.
4 Related Work
As described in Section 1, there have been two ap-
proaches to (semi-) unsupervised learning of polar-
ity. This Section introduces the two approaches and
other related work.
4.1 Thesaurus based approach
Kamps et al built lexical network by linking syn-
onyms provided by a thesaurus, and polarity was de-
fined by the distance from seed words (?good? and
?bad?) in the network (Kamps et al, 2004). This
method relies on a hypothesis that synonyms have
the same polarity. Hu and Liu used similar lexi-
cal network, but they considered not only synonyms
but antonyms (Hu and Liu, 2004). Kim and Hovy
proposed two probabilistic models to estimate the
strength of polarity (Kim and Hovy, 2004). In their
models, synonyms are used as features. Esuli et al
utilized glosses of words to determine polarity (Esuli
and Sebastiani, 2005; Esuli and Sebastiani, 2006).
Compared with our approach, the drawback of us-
ing thesaurus is the lack of scalability. It is diffi-
cult to handle such words that are not contained in
a thesaurus (e.g. newly-coined words or colloquial
words). In addition, phrases cannot be handled be-
cause the entry of usual thesaurus is not phrase but
word.
1079
4.2 Corpus based approach
Another approach is based on an idea that polar
phrases conveying the same polarity co-occur with
each other in corpus.
(Turney, 2002) is one of the most famous work
that discussed learning polarity from corpus. Turney
determined polarity value5 based on co-occurrence
with seed words (?excellent? and ?poor?). The co-
occurrence is measured by the number of hits re-
turned by a search engine. The polarity value pro-
posed by (Turney, 2002) is as follows.
	


  

		

  		



 means the number of hits returned by a
search engine when query  is issued.  
means NEAR operator, which enables to retrieve
only such documents that contain two queries within
ten words.
Hatzivassiloglou and McKeown constructed lex-
ical network and determine polarity of adjectives
(Hatzivassiloglous and McKeown, 1997). Although
this is similar to thesaurus based approach, they built
the network from intra-sentential co-occurrence.
Takamura et al built lexical network from not only
such co-occurrence but other resources including
thesaurus (Takamura et al, 2005). They used spin
model to predict polarity of words.
Popescu and Etzioni applied relaxation labeling to
polarity identification (Popescu and Etzioni, 2005).
This method iteratively assigns polarity to words by
using various features including intra-sentential co-
occurrence and synonyms of a thesaurus.
Kanayama and Nasukawa used both intra- and
inter-sentential co-occurrence to learn polarity of
words and phrases (Kanayama and Nasukawa,
2006). Their method covers wider range of co-
occurrence than other work such as (Hatzivas-
siloglous and McKeown, 1997). An interesting
point of this work is that they discussed building do-
main oriented lexicon. This is contrastive to other
work including ours that addresses to build domain
independent lexicon.
In summary, the strength of our approach is to ex-
ploit extremely precise structural clues, and to use
5Semantic Orientation in (Turney, 2002).
massive collection of HTML documents to compen-
sate for the low recall. Although Turney?s method
also uses massive collection of HTML documents,
his method does not make much of precision com-
pared with our method. As we will see in Section
5, our experimental result revealed that our method
overwhelms Turney?s method.
4.3 Other related work
In some review sites, pros and cons are stated using
such layout that we introduced in Section 2. Some
work examined the importance of such layout (Liu et
al., 2005; Kim and Hovy, 2006). However, they re-
garded layout structures as clues specific to a certain
review site. They did not propose to use layout struc-
ture to extract polar sentences from arbitrary HTML
documents.
Some studies addressed supervised approach to
learning polarity of phrases (Wilson et al, 2005;
Takamura et al, 2006). These are different from
ours in a sense that they require manually tagged
data.
Kobayashi et al proposed a framework to reduce
the cost of manually building lexicon (Kobayashi et
al., 2004). In the experiment, they compared the
framework with fully manual method and investi-
gated the effectiveness.
5 Experiment
A test set consisting of 405 adjective phrases were
created. From the test set, we extract polar phrases
by looking up our lexicon. The result was evaluated
through precision and recall6.
5.1 Setting
The test set was created in the following manner.
500 adjective phrases were randomly extracted from
the Web text. Note that there is no overlap between
our polar sentence corpus and this text. After remov-
ing parsing error and duplicates, 405 unique adjec-
tive phrases were obtained. Each phase was man-
ually annotated with polarity tag (positive, negative
and neutral), and we obtained 158 positive phrases,
150 negative phrases and 97 neutral phrases. In or-
der to check the reliability of annotation, another
6The lexicon is available from http://www.tkl.iis.u-
tokyo.ac.jp/?kaji/polardic/.
1080
Table 3: The experimental result (chi-square).
 0 10 20 30 40 50 60
Precision/Recall Positive 76.4/92.4 84.0/86.7 84.1/83.5 86.2/79.1 88.7/74.7 86.7/65.8 86.7/65.8
Negative 68.5/84.0 65.5/63.3 64.3/60.0 62.7/57.3 81.1/51.3 80.0/48.0 80.0/48.0
# of polar words and phrases 9,670 2,056 1,047 698 533 423 335
Table 4: The experimental result (PMI).
 0 0.5 1.0 1.5 2.0 2.5 3.0
Precision/Recall Positive 76.4/92.4 79.6/91.1 86.1/89.9 87.2/86.1 90.9/82.3 92.4/76.6 92.9/65.8
Negative 68.5/84.0 75.8/81.3 82.3/77.3 84.8/74.7 85.8/72.7 86.8/70.0 87.9/62.7
# of polar words and phrases 9,670 9,320 9,039 8,804 8,570 8,398 8,166
Table 5: The effect of data size (PMI, =1.0).
size 1/20 1/15 1/10 1/5 1
Precision/Recall Positive 87.0/63.9 84.6/65.8 85.1/75.9 85.4/84.8 86.1/89.9
Negative 76.9/55.8 86.2/50.0 82.1/58.0 80.3/62.7 82.3/77.3
human judge annotated the same data. The Kappa
value between the two judges was 0.73, and we think
the annotation is reliable.
From the test set, we extracted polar phrases by
looking up our lexicon. As for adjectives in the lex-
icon, partial match is allowed. For example, if the
lexicon contains an adjective ?excellent?, it matches
every adjective phrase that includes ?excellent? such
as ?view-excellent? etc.
As a baseline, we built lexicon similarly by using
polarity value of (Turney, 2002). As seed words, we
used ?saikou (best)? and ?saitei (worst)?. Some seeds
were tested and these words achieved the best result.
As a search engine, we tested Google and our local
engine, which indexes 150 millions Japanese docu-
ments. Its size is compatible to (Turney and Littman,
2002). Since Google does not support NEAR, we
used AND. Our local engine supports NEAR.
5.2 Results and discussion
We evaluated the result of polar phrase extraction.
By changing the threshold , we investigated recall-
precision curve (Figure 6 and 7). The detail is rep-
resented in Table 3 and 4. The second/third row
represents precision and recall of positive/negative
phrases. The fourth row is the size of the lexicon.
The Figures show that both of the proposed meth-
ods outperform the baselines. The best F-measure
was achieved by PMI (=1.0). Although Turney?s
method may be improved with minor configurations
(e.g. using other seeds etc.), we think this results
indicate the feasibility of the proposed method. Al-
Figure 6: Recall-precision curve (positive phrases)
though the size of lexicon is not surprisingly large, it
would be possible to make the lexicon larger by us-
ing more HTML documents. In addition, notice that
we focus on only adjectives and adjective phrases.
Comparing the two proposed methods, PMI is al-
ways better than chi-square. Especially, chi-square
suffers from low recall, because the size of lexicon
is extremely small. For example, when the thresh-
old is 60, the precision is 80% and the recall is 48%
for negative phrases. On the other hand, PMI would
achieve the same precision when recall is around
80% ( is between 0.5 and 1.0).
Turney?s method did not work well although they
reported 80% accuracy in (Turney and Littman,
2002). This is probably because our experimental
setting is different. Turney examined binary classi-
fication of positive and negative words, and we dis-
cussed extracting positive and negative phrases from
the set of positive, negative and neutral phrases.
1081
Figure 7: Recall-precision curve (negative phrases)
Error analysis revealed that most of the errors are
related to neutral phrases. For example, PMI (=1.0)
extracted 48 incorrect polar phrases, and 37 of them
were neutral phrases. We think one reason is that
we did not use neutral corpus. It is one future work
to exploit neutral corpus. The importance of neutral
category is also discussed in other literatures (Esuli
and Sebastiani, 2006).
To further assess our method, we did two addi-
tional experiments. In the first experiment, to inves-
tigate the effect of data size, the same experiment
was conducted using 1/n (n=1,5,10,15,20) of the en-
tire polar sentence corpus (Table 5). PMI (=1.0)
was also used. As the size of corpus increases, the
performance becomes higher. Especially, the re-
call is improved dramatically. Therefore, the recall
would be further improved using more corpus.
In the other experiment, the lexicon was evalu-
ated directly so that we can examine polar words and
phrases that are not in the test set. We think it is diffi-
cult to fully assess low frequency words in the previ-
ous setting. Two human judges assessed 200 unique
polar words and phrases in the lexicon (PMI, =1.0).
The average precision was 71.3% (Kappa value was
0.66). The precision is lower than the result in Table
4. This result indicates that it is difficult to handle
low frequency words.
The Table 6 illustrates examples of polar phrases
and their polarity values. We can see that both
phrases and colloquial words such as ?uncool? are
appropriately learned. They are difficult to handle
for thesaurus based approach, because such words
are not usually in thesaurus.
It is important to discuss how general our frame-
Table 6: Examples
polar phrase 
 
 
 


kenkyoda (modest) 38.3 12.1
exiting (exiting) 13.5 10.4
more-sukunai (leak-small) 9.2 9.8
dasai (uncool) -2.9 -3.3
yakkaida (annoying) -11.9 -3.9
shomo-hayai (consumption-quick) -17.7 -4.4
work is. Although the lexico-syntactic patterns
shown in Section 2 are specific to Japanese, we
think that the idea of exploiting language structure
is applicable to other languages including English.
Roughly speaking, the pattern we exploited can be
translated into ?the advantage/weakness of some-
thing is to ...? in English. It is worth pointing out
that lexico-syntactic patterns have been widely used
in English lexical acquisition (Hearst, 1992). Obvi-
ously, other parts of the proposed method does not
depend on Japanese.
6 Conclusion
In this paper, we explore to use structural clues that
can extract polar sentences from Japanese HTML
documents, and build lexicon from the extracted po-
lar sentences. The key idea is to develop the struc-
tural clues so that it achieves extremely high preci-
sion at the cost of recall. In order to compensate for
the low recall, we used massive collection of HTML
documents. Thus, we could prepare enough polar
sentence corpus. Experimental result demonstrated
the feasibility of our approach.
Acknowledgement This work was supported by
the Comprehensive Development of e-Society Foun-
dation Software program of the Ministry of Edu-
cation, Culture, Sports, Science and Technology,
Japan. We would like to thank Assistant Researcher
Takayuki Tamura for his development of the Web
crawler.
References
Kenneth Ward Church and Patric Hanks. 1989. Word
association norms, mutual information, and lexicogra-
phy. In Proceedings of ACL, pages 76?83.
Andrea Esuli and Fabrizio Sebastiani. 2005. Determin-
ing the semantic orientation of terms throush gloss
classification. In Proceedings of CIKM, pages 617?
624.
1082
Andrea Esuli and Fabrizio Sebastiani. 2006. Determin-
ing term subjectivity and term orientation for opinion
mining. In Proceedings of EACL, pages 193?200.
Vasileios Hatzivassiloglous and Kathleen R. McKeown.
1997. Predicting the semantic orientation of adjec-
tives. In Proceedings of ACL, pages 174?181.
Marti Hearst. 1992. Automatic acquisition of hyponyms
from large text corpora. In Proceedings of COLING,
pages 539?545.
Minqing Hu and Bing Liu. 2004. Mining and summariz-
ing customer reviews. In Proceedings of KDD, pages
168?177.
Nobuhiro Kaji and Masaru Kitsuregawa. 2006. Au-
tomatic construction of polarity-tagged corpus from
html documents. In Proceedings of COLING/ACL,
poster sessions, pages 452?459.
Jaap Kamps, Maarten Marx, Robert J. Mokken, and
Maarten de Rijke. 2004. Using wordnet to measure
semantic orientations of adjectives. In Proceedings of
LREC.
Hiroshi Kanayama and Tetsuya Nasukawa. 2006. Fully
automatic lexicon expansion for domain-oriented sen-
timent analysis. In Proceedings of ENMLP, pages
355?363.
Soo-Min Kim and Eduard Hovy. 2004. Determining the
sentiment of opinions. In Proceedings of COLING,
pages 1367?1373.
Soo-Min Kim and Eduard Hovy. 2006. Automatic iden-
tification of pro and con reasons in online reviews. In
Proceedings of COLING/ACL Poster Sessions, pages
483?490.
Nozomi Kobayashi, Kentaro Inui, Yuji Matsumoto, Kenji
Tateishi, and Toshikazu Fukushima. 2004. Collecting
evaluative expressions for opinion extraction. In Pro-
ceedings of IJCNLP, pages 584?589.
Bing Liu, Minqing Hu, and Junsheng Cheng. 2005.
Opinion observer: Analyzing and comparing opinions
on the web. In Proceedings of WWW.
Ana-Maria Popescu and Oren Etzioni. 2005. Extracting
product features and opinions from reviews. In Pro-
ceedings of HLT/EMNLP.
Hiroya Takamura, Takashi Inui, and Manabu Okumura.
2005. Extracting semantic orientation of words using
spin model. In Proceedings of ACL, pages 133?140.
Hiroya Takamura, Takashi Inui, and Manabu Okumura.
2006. Latent variable mdels for semantic orientations
of phrases. In Proceedings of EACL, pages 201?208.
Peter D. Turney and Michael L. Littman. 2002. Unsuper-
vised learning of semantic orientation from a hundred-
billion-word corpus. Technical report, National Re-
search Council Canada.
Peter D. Turney. 2002. Thumbs up or thumbs down ?
semantic orientation applied to unsupervised classifi-
cation of reviews. In Proceedings of ACL, pages 417?
424.
Theresa Wilson, Janyce Wiebe, and Paul Hoffmann.
2005. Recognizing contextual polarity in phrase-level
sentiment analysis. In Proceedings of HLT/EMNLP.
1083
Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 1542?1551,
Singapore, 6-7 August 2009.
c
?2009 ACL and AFNLP
Polynomial to Linear: Efficient Classification with Conjunctive Features
Naoki Yoshinaga
Institute of Industrial Science
University of Tokyo
4-6-1 Komaba, Meguro-ku, Tokyo
ynaga@tkl.iis.u-tokyo.ac.jp
Masaru Kitsuregawa
Institute of Industrial Science
University of Tokyo
4-6-1 Komaba, Meguro-ku, Tokyo
kitsure@tkl.iis.u-tokyo.ac.jp
Abstract
This paper proposes a method that speeds
up a classifier trained with many con-
junctive features: combinations of (prim-
itive) features. The key idea is to pre-
compute as partial results the weights of
primitive feature vectors that appear fre-
quently in the target NLP task. A trie
compactly stores the primitive feature vec-
tors with their weights, and it enables the
classifier to find for a given feature vec-
tor its longest prefix feature vector whose
weight has already been computed. Ex-
perimental results for a Japanese depen-
dency parsing task show that our method
speeded up the SVM and LLM classifiers
of the parsers, which achieved accuracy of
90.84/90.71%, by a factor of 10.7/11.6.
1 Introduction
Deep and accurate text analysis based on discrimi-
native models is not yet efficient enough as a com-
ponent of real-time applications, and it is inade-
quate to process Web-scale corpora for knowledge
acquisition (Pantel, 2007; Saeger et al, 2009) or
semi-supervised learning (McClosky et al, 2006;
Spoustov? et al, 2009). One of the main reasons
for this inefficiency is attributed to the inefficiency
of core classifiers trained with many feature com-
binations (e.g., word n-grams). Hereafter, we refer
to features that explicitly represent combinations
of features as conjunctive features and the other
atomic features as primitive features.
The feature combinations play an essential role
in obtaining a classifier with state-of-the-art ac-
curacy for several NLP tasks; recent examples in-
clude dependency parsing (Koo et al, 2008), parse
re-ranking (McClosky et al, 2006), pronoun reso-
lution (Nguyen and Kim, 2008), and semantic role
labeling (Liu and Sarkar, 2007). However, ?ex-
plicit? feature combinations significantly increase
the feature space, which slows down not only
training but also testing of the classifier.
Kernel-based methods such as support vector
machines (SVMs) consider feature combinations
space-efficiently by using a polynomial kernel
function (Cortes and Vapnik, 1995). The kernel-
based classification is, however, known to be very
slow in NLP tasks, so efficient classifiers should
sum up the weights of the explicit conjunctive fea-
tures (Isozaki and Kazawa, 2002; Kudo and Mat-
sumoto, 2003; Goldberg and Elhadad, 2008).
`
1
-regularized log-linear models (`
1
-LLMs), on
the other hand, provide sparse solutions, in which
weights of irrelevant features are exactly zero, by
assuming a Laplacian prior on the weights (Tibshi-
rani, 1996; Kazama and Tsujii, 2003; Goodman,
2004; Gao et al, 2007). However, as Kazama and
Tsujii (2005) have reported in a text categorization
task and we later confirm in a dependency pars-
ing task, when most features regarded as irrelevant
during training `
1
-LLMs appear rarely in the task,
we cannot greatly reduce the number of active fea-
tures in each classification. In the end, when effi-
ciency is a major concern, we must use exhaustive
feature selection (Wu et al, 2007; Okanohara and
Tsujii, 2009) or even restrict the order of conjunc-
tive features at the expense of accuracy.
In this study, we provide a simple, but effective
solution to the inefficiency of classifiers trained
with higher-order conjunctive features (or polyno-
mial kernel), by exploiting the Zipfian nature of
language data. The key idea is to precompute the
weights of primitive feature vectors and use them
as partial results to compute the weight of a given
feature vector. We use a trie called the feature
sequence trie to efficiently find for a given fea-
ture vector its longest prefix feature vector whose
weight has been computed. The trie is built from
feature vectors generated by applying the classifier
to actual data in the classification task. The time
complexity of the classifier approaches time that
1542
is linear with respect to the number of primitive
features when the retrieved feature vector covers
most of the features in the input feature vector.
We implemented our algorithm for SVM and
LLM classifiers and evaluated the performance of
the resulting classifiers in a Japanese dependency
parsing task. Experimental results show that it
successfully speeded up classifiers trained with
higher-order conjunctive features by a factor of 10.
The rest of this paper is organized as follows.
Section 2 introduces LLMs and SVMs. Section 3
proposes our classification algorithm. Section 4
presents experimental results. Section 5 concludes
with a summary and addresses future directions.
2 Preliminaries
In this paper, we focus on linear classifiers that cal-
culate the probability (or score) by summing up
weights of individual features. Examples include
not only log-linear models but also support vec-
tor machines with kernel expansion (Isozaki and
Kazawa, 2002; Kudo and Matsumoto, 2003). Be-
low, we introduce these two classifiers and their
ways to consider feature combinations.
In classification-based NLP, the target task is
modeled as one or more classification steps. For
example in part-of-speech (POS) tagging, each
classification decides whether to assign a partic-
ular label (POS tag) to a given sample (each word
in a given sentence). Each sample is then repre-
sented by a feature vector x, whose element x
i
is
a value of a feature function f
i
? F .
Here, we assume a binary feature function
f
i
(x) ? {0, 1}, in which a non-zero value means
that particular context data appears in the sample.
We say that a feature f
i
is active in sample xwhen
x
i
= f
i
(x) = 1 and |x| represents the number of
active features in x (|x| = |{f
i
|f
i
(x) = 1}|).
2.1 Log-Linear Models
The log-linear model (LLM), or also known as
maximum-entropy model (Berger et al, 1996), is
a linear classifier widely used in the NLP literature.
Let the training data of LLMs be {?x
i
, y
i
?}
L
i=1
,
where x
i
? {0, 1}
n
is a feature vector and y
i
is a
class label associated with x
i
. We assume a binary
label y
i
? {?1} here to simplify the argument.
The classifier provides conditional probability
p(y|x) for a given feature vector x and a label y:
p(y|x) =
1
Z(x)
exp
?
i
w
i,y
f
i,y
(x, y), (1)
where f
i,y
(x, y) is a feature function that returns
a non-zero value when f
i
(x) = 1 and the label is
y, w
i,y
? R is a weight associated with f
i,y
, and
Z(x) =
?
y
exp
?
i
w
i,y
f
i,y
(x, y) is the partition
function. We can consider feature combinations in
LLMs by explicitly introducing a new conjunctive
feature f
F
?
,y
(x, y) that is activated when a partic-
ular set of features F
?
? F to be combined is acti-
vated (namely, f
F
?
,y
(x, y) =
?
f
i,y
?F
?
f
i,y
(x, y)).
We then introduce an `
1
-regularized LLM (`
1
-
LLM), in which the weight vector w is tuned so
as to maximize the logarithm of the a posteriori
probability of the training data:
L(w) =
L
?
i=1
log p(y
i
|x
i
)? C?w?
1
. (2)
Hyper-parameter C thereby controls the degree of
over-fitting (solution sparseness). Interested read-
ers may refer to the cited literature (Andrew and
Gao, 2007) for the optimization procedures.
2.2 Support Vector Machines
A support vector machine (SVM) is a binary clas-
sifier (Cortes and Vapnik, 1995). Training with
samples {?x
i
, y
i
?}
L
i=1
where x
i
? {0, 1}
n
and
y
i
? {?1} yields the following decision function:
y(x) = sgn(g(x) + b)
g(x) =
?
x
j
?SV
y
j
?
j
?(x
j
)
T
?(x), (3)
where b ? R, ? : R
n
7? R
H
and support vec-
tors x
j
? SV (subset of training samples), each
of which is associated with weight ?
j
? R. We
hereafter call g(x) the weight function. Nonlinear
mapping function ? is chosen to make the train-
ing samples linearly separable in R
H
space. Ker-
nel function k(x
j
,x) = ?(x
j
)
T
?(x) is then in-
troduced to compute the dot product in R
H
space
without mapping x to ?(x).
To consider combinations of primitive features
f
j
? F , we use a polynomial kernel k
d
(x
j
,x) =
(x
T
j
x + 1)
d
. From Eq. 3, we obtain the weight
function for the polynomial kernel as:
g(x) =
?
x
j
?SV
y
j
?
j
(x
T
j
x+ 1)
d
. (4)
Since we assumed that x
i
is a binary value repre-
senting whether a (primitive) feature f
i
is active
in the sample, the polynomial kernel of degree d
implies a mapping ?
d
from x to ?
d
(x) that has
1543
H =
?
d
k=0
(
n
k
)
dimensions. Each dimension rep-
resents a (weighted) conjunction of d features in
the original sample x.
1
Kernel Expansion (SVM-KE) The time com-
plexity of Eq. 4 is O(|x| ? |SV|). This cost is usu-
ally high for classifiers used in NLP tasks because
they often have many support vectors (|SV| >
10, 000). Kernel expansion (KE) was proposed
by Isozaki and Kazawa (2002) to convert Eq. 4
into the linear sum of the weights in the mapped
feature space as in LLM (p(y|x) in Eq. 1):
g(x) = w
T
x
d
=
?
i
w
i
x
d
i
, (5)
where x
d
is a binary feature vector whose element
x
d
i
has a non-zero value when (?
d
(x))
i
> 0, w
is the weight vector for x
d
in the expanded fea-
ture space F
d
and is precalculated from the sup-
port vectors x
j
and their weights ?
j
. Interested
readers may refer to Kudo and Matsumoto (2003)
for the detailed computation for obtaining w.
The time complexity of Eq. 5 (and Eq. 1) is
O(|x
d
|), which is linear with respect to the num-
ber of active features in x
d
within the expanded
feature space F
d
.
Heuristic Kernel Expansion (SVM-HKE) To
make the weight vector sparse, Kudo and Mat-
sumoto (2003) proposed a heuristic method that
filters out less useful features whose absolute
weight values are less than a pre-defined threshold
?.
2
They reported that increased threshold value ?
resulted in a dramatically sparse feature space F
d
,
which had the side-effects of accuracy degradation
and classifier speed-up.
3 Proposed Method
In this section, we propose a method that speeds
up a classifier trained with many conjunctive fea-
tures. Below, we focus on a kernel-based classifier
trained with a polynomial kernel of degree d (here,
1
For example, given an input vector x = (x
1
, x
2
)
T
and a support vector x
?
= (x
?
1
, x
?
2
)
T
, the 2nd-order
polynomial kernel returns k
2
(x
?
,x) = (x
?
1
x
1
+ x
?
2
x
2
+
1)
2
= 3x
?
1
x
1
+ 3x
?
2
x
2
+ 2x
?
1
x
1
x
?
2
x
2
+ 1 (? x
?
i
, x
i
?
{0, 1}). This function thus implies a mapping ?
2
(x) =
(1,
?
3x
1
,
?
3x
2
,
?
2x
1
x
2
)
T
. In the following argument, we
ignore the dimension of the constant in the mapped space and
assume constant b is set to include it.
2
Precisely speaking, they set different thresholds to posi-
tive (?
j
> 0) and negative (?
j
< 0) support vectors, consid-
ering the proportion of positive and negative support vectors.
Figure 1: Efficient computation of g(x).
SVMs), but an analogous argument is possible for
linear classifiers (e.g., LLMs).
3
We hereafter represent a binary feature vector x
as a set of active features {f
i
|f
i
(x) = 1}. x can
thereby be represented as an element of the power
set 2
F
of the set of features F .
3.1 Idea
Let us remember that weight function g(x) in
Eq. 5 maps x ? 2
F
to W ? R. If we could cal-
culate W
x
= g(x) for all possible x in advance,
we could obtain g(x) by simply checking |x| ele-
ments, namely, in O(|x|) time. However, because
|{x|x ? 2
F
}| = 2
|F|
and |F| is likely to be very
large (often |F| > 10, 000 in NLP tasks), this cal-
culation is impractical.
We then compute and store weight W
x
?
=
g(x
?
) for x
?
? V
c
(? 2
F
), a certain subset of
the possible value space, and compute g(x) for
x /? V
c
by using precalculated weight W
x
c
for
x
c
?
4
x in the following way:
g(x) = W
x
c
+
?
f
i
?x
d
?x
d
c
w
i
. (6)
Intuitively speaking, starting from partial weight
W
x
c
, we add up remaining weights of primitive
features f ? F that are not active in x
c
but active
in x and conjunctive features that combine f and
the other active features in x.
An example of this computation (d = 2) is de-
picted in Figure 1. We can efficiently compute
g(x) for a vector x that has four active features
f
1
, f
2
, f
3
, and f
4
(and x
2
has their six conjunc-
tive features) using precalculated weight W
{1,2,3}
;
we should first check the three features f
1
, f
2
, and
f
3
to retrieve W
{1,2,3}
and next check the remain-
ing four features related to f
4
, namely f
4
, f
1,4
,
f
2,4
, and f
3,4
, in order to add up the remaining
3
When a feature vector x includes (explicit) conjunctive
features f ? F
d
, we assume weight function g
?
(y|x
?
) =
g(y|x), where x
?
is a projection of x (by ?
?1
d
: F
d
? F ).
4
This means that all active features in x
c
are active in x.
1544
weights, while the normal computation in Eq. 5
should check the four primitive and six conjunc-
tive features to get the individual weights.
Expected time complexity Counting the num-
ber of features to be checked in the computation,
we obtain the time complexity f(x, d) of Eq. 6 as:
f(x, d) = O(|x
c
|+ |x
d
| ? |x
d
c
|), (7)
where |x
d
| =
d
?
k=1
(
|x|
k
)
(8)
(e.g., |x
2
| =
|x|
2
+|x|
2
and |x
3
| =
|x|
3
+5|x|
6
).
5
Note
that when |x
c
| becomes close to |x|, this time
complexity actually approaches O(|x|).
Thus, to minimize this computational cost, x
c
is to be chosen from V
c
as follows:
x
c
= argmin
x
?
?V
c
,x
?
?x
(|x
?
|+ |x
d
| ? |x
?d
|). (9)
3.2 Construction of Feature Sequence Trie
There are two issues with speeding up the classi-
fier by the computation shown in Eq. 6. First, since
we can store weights for only a small fraction of
possible feature vectors (namely, |V
c
|  2
|F|
), we
should choose V
c
so as to maximize its impact on
the speed-up. Second, we should quickly find an
optimal x
c
from V
c
for a given feature vector x.
The solution to the first problem is to enumer-
ate partial feature vectors that frequently appear in
the target task. Note that typical linguistic features
used in NLP tasks usually consist of disjunctive
sets of features (e.g., word surface and POS), in
which each set is likely to follow Zipf?s law (Zipf,
1949) and correlate with each other. We can ex-
pect the distribution of feature vectors, the mixture
of Zipf distributions, to be Zipfian. This has been
confirmed for word n-grams (Egghe, 2000) and
itemset support distribution (Chuang et al, 2008).
We can thereby expect that a small set of partial
feature vectors commonly appear in the task.
To solve the second problem, we introduce a
feature sequence trie (fstrie), which represents a
hierarchy of feature vectors, to enable the clas-
sifier to efficiently retrieve (sub-)optimal x
c
(in
Eq. 9) for a given feature vector x. We build an
fstrie in the following steps:
Step 1: Apply the target classifier to actual (raw)
data in the task to enumerate possible feature
vectors (hereafter, source feature vectors).
5
This is the maximum number of conjunctive features.
Figure 2: Feature sequence trie and completion of
prefix feature vector weights.
Step 2: Sort the features in each source feature
vector according to their frequency in the
training data (in descending order).
Step 3: Build a trie from the source feature vec-
tors by regarding feature indices as characters
and store weights of all prefix feature vectors.
An fstrie built from six source feature vectors is
shown in Figure 2. In fstries, a path from the root
to another node represents a feature vector. An
important point here is that the fstrie stores the
weights of all prefix feature vectors of the source
feature vectors, and the trie structure enables us to
retrieve for a given feature vector x the weight of
its longest prefix vector x
c
? x in O(|x
c
|) time.
To handle feature functions in LLMs (Eq. 1), we
store partial weight W
x
c
,y
=
?
i
w
i,y
f
i,y
(x
c
, y)
for each label y on the node that expresses x
c
.
Since we sort the features in the source fea-
ture vectors according to their frequency, the pre-
fix feature vectors exclude less frequent features
in the source feature vectors. Lexical features or
finer-grained features (e.g., POS-subcategory) are
usually less frequent than coarse-grained features
(e.g., POS), so they lie in the latter part of the
feature vectors. This sorting helps us to retrieve
longer feature vector x
c
for input feature vector x
that will have diverse infrequent features. It also
minimizes the size of fstrie by sharing the com-
mon frequent prefix (e.g., {f
1
, f
2
} in Figure 2).
Pruning nodes from fstrie We have so far de-
scribed the way to construct an fstrie from the
source feature vectors. However, a naive enumer-
ation of source feature vectors will result in the
explosion of the fstrie size, and we want to have
a principled way to control the fstrie size rather
than reducing the processed data size. Below, we
present a method that prunes useless prefix feature
vectors (nodes) from the constructed fstrie to max-
imize its impact on the classifier efficiency.
1545
Algorithm 1 PRUNE NODES FROM FSTRIE
Input: fstrie T , node_limit N ? N
Output: fstrie T
1: while # of nodes in T > N do
2: x
c
? argmin
x
?
?leaf(T )
u(x
?
)
3: remove x
c
, T
4: end while
5: return T
We adopt a greedy strategy that iteratively
prunes a leaf node (one prefix feature vector and
its weight) from the fstrie built from all the source
feature vectors, according to a certain utility score
calculated for each node. In this study, we con-
sider two metrics for each prefix feature vector x
c
to calculate its utility score.
Probability p(x
c
), which denotes how often the
stored weight W
x
c
will be used in the tar-
get task. The maximum-likelihood estima-
tion provides probability:
p(x
c
) =
?
x
?
?x
c
n
x
?
?
x
n
x
, (10)
where n
x
? N is the frequency count of a
source feature vector x in the processed data.
Computation reduction ?
d
(x
c
), which denotes
how much computation is reduced by W
x
c
to
calculate a weight of x ? x
c
. This can be es-
timated by counting the number of conjunc-
tive features we additionally have to check
when we remove x
c
. Since the fstrie stores
the weight of a prefix feature vector x
c-
? x
c
such that |x
c-
| = |x
c
| ? 1 (e.g., in Figure 2,
x
c-
= {f
1
, f
2
} for x
c
= {f
1
, f
2
, f
4
}), we
can define the computation reduction as:
?
d
(x
c
) = (|x
d
c
| ? |x
d
c-
|)? (|x
c
| ? |x
c-
|)
=
d
?
k=2
(
|x
c
|
k
)
?
d
?
k=2
(
|x
c
| ? 1
k
)
(? Eq. 8).
?
2
(x
c
) = |x
c
| ? 1 and ?
3
(x
c
) =
|x
c
|
2
?|x
c
|
2
.
We calculate utility score of each node x
c
in the
fstrie as u(x
c
) = p(x
c
) ? ?
d
(x
c
), which means
the expected computation reduction by x
c
in the
target task, and prune the lowest-utility-score leaf
nodes from the fstrie one by one (Algorithm 1). If
several prefix vectors have the same utility score,
we eliminate them in numerical descending order.
Algorithm 2 COMPUTE WEIGHT WITH FSTRIE
Input: fstrie T , weight vector w ? R
|F
d
|
feature vector x ? 2
F
Output: weight W = g(x) ? R
1: x? sort(x)
2: ?x
c
,W
x
c
? ? prefix_search(T , x)
3: W ?W
x
c
4: for all feature f
j
? x
d
? x
d
c
do
5: W ?W + w
j
6: end for
7: return W
3.3 Classification Algorithm
Our classification algorithm is shown in detail in
Algorithm 2. The classifier first sorts the active
features in input feature vectorx according to their
frequency in the training data. Then, for x, it re-
trieves the longest common prefix vector x
c
from
the fstrie (line 2 in Algorithm 2). It then adds the
weights of the remaining features to partial weight
W
x
c
(line 5 in Algorithm 2).
Note that the remaining features whose weights
we sum up (line 4 in Algorithm 2) are primitive
and conjunctive features that relate to f ? x?x
c
,
which appear less frequently than f
?
? x
c
in the
training data. Thus, when we apply our algorithm
to classifiers with the sparse solution (e.g., SVM-
HKEs or `
1
-LLMs), |x
d
|?|x
d
c
| can be much smaller
than the theoretical expectation (Eq. 8). We con-
firmed this in the following experiments.
4 Evaluation
We applied our algorithm to SVM-KE, SVM-HKE,
and `
1
-LLM classifiers and evaluated the resulting
classifiers in a Japanese dependency parsing task.
To the best of our knowledge, there are no previous
reports of an exact weight calculation faster than
linear summation (Eqs. 1 and 5). We also com-
pared our SVM classifier with a classifier called
polynomial kernel inverted (PKI: Kudo and Mat-
sumoto (2003)), which uses the polynomial kernel
(Eq. 4) and inverted indexing to support vectors.
4.1 Experimental Settings
A Japanese dependency parser inputs bunsetsu-
segmented sentences and outputs the correct head
(bunsetsu) for each bunsetsu; here, a bunsetsu is
a grammatical unit in Japanese consisting of one
or more content words followed by zero or more
function words. A parser generates a feature vec-
1546
Modifier,
modifiee
bunsetsu
head word (surface-form, POS, POS-subcategory,
inflection form), functional word (surface-form,
POS, POS-subcategory, inflection form), brackets,
quotation marks, punctuation marks, position in
sentence (beginning, end)
Between
bunsetsus
distance (1, 2?5, 6?), case-particles, brackets,
quotation marks, punctuation marks
Table 1: Feature set used for experiments.
tor for a particular pair of bunsetsus (modifier and
modifiee candidates) by exploiting the head-final
and projective (Nivre, 2003) nature of dependency
relations in Japanese. The classifier then outputs
label y = ?+1? (dependent) or ??1? (independent).
Since our classifier is independent of individ-
ual parsing algorithms, we targeted speeding up
(a classifier in) the shift-reduce parser proposed
by Sassano (2004), which has been reported to be
the most efficient for this task, with almost state-
of-the-art accuracy (Iwatate et al, 2008). This
parser decreases the number of classification steps
by using the fact that a bunsetsu is likely to modify
a bunsetsu close to itself. Due to space limitations,
we omit the details of the parsing algorithm.
We used the standard feature set tailored for this
task (Kudo and Matsumoto, 2002; Sassano, 2004;
Iwatate et al, 2008) (Table 1). Note that features
listed in the ?Between bunsetsus? row represent
contexts between the target pair of bunsetsus and
appear independently from other features, which
will become an obstacle to finding the longest pre-
fix vector. This task is therefore a better measure
of our method than simple sequential labeling such
as POS tagging or named-entity recognition.
For evaluation, we used Kyoto Text Corpus Ver-
sion 4.0 (Kurohashi and Nagao, 2003), Mainichi
news articles in 1995 that have been manually an-
notated with dependency relations.
6
The train-
ing, development, and test sets included 24,283,
4833, and 9284 sentences, and 234,685, 47,571,
and 89,874 bunsetsus, respectively. The training
samples generated from the training set included
150,064 positive and 146,712 negative samples.
The following experiments were performed on
a server with an Intel
R
? Xeon
TM
3.20-GHz CPU.
We used TinySVM
7
and a simple C++ library for
maximum entropy classification
8
to train SVMs
and `
1
-LLMs, respectively. We used Darts-Clone,
9
6
http://nlp.kuee.kyoto-u.ac.jp/nl-resource/corpus-e.html
7
http://chasen.org/?taku/software/TinySVM/
8
http://www-tsujii.is.s.u-tokyo.ac.jp/?tsuruoka/maxent/
9
http://code.google.com/p/darts-clone/
Model type Model statistics Dep. Sent.
Model d ? / ? |F
d
| |x
d
| acc. acc.
SVM-KE 1 0 39712 27.3 88.29 46.49
SVM-KE 2 0 1478109 380.6 90.76 53.83
SVM-KE 3 0 26194354 3286.7 90.93

54.43

SVM-HKE 3 0.001 13247675 2725.9 90.92

54.39

SVM-HKE 3 0.002 2514385 2238.1 90.91

54.32
>
SVM-HKE 3 0.003 793195 1855.4 90.83 54.21
SVM-KE 4 0 293416102 20395.4 90.91

54.69

SVM-HKE 4 0.0002 96522236 15282.1 90.93

54.53
>
SVM-HKE 4 0.0004 19245076 11565.0 90.96

54.64

SVM-HKE 4 0.0006 7277592 8958.2 90.84 54.48
>
`
1
-LLM 1 1.0 9268 26.5 88.22 46.06
`
1
-LLM 2 2.0 32575 309.8 90.62 53.46
`
1
-LLM 3 3.0 129503 2088.3 90.71 54.09
>
`
1
-LLM 3 4.0 85419 1803.0 90.61 53.79
`
1
-LLM 3 5.0 63046 1699.5 90.59 53.55
Table 2: Specifications of LLMs and SVMs. The
accuracy marked with ?? or ?>? was signifi-
cantly better than the d = 2 counterpart (p < 0.01
or 0.01 ? p < 0.05 by McNemar?s test).
a double-array trie (Aoe, 1989; Yata et al, 2008),
as a compact trie implementation. All these li-
braries and algorithms are implemented in C++.
The code for building fstries occupies 100 lines,
while the code for the classifier occupies 20 lines
(except those for kernel expansion).
4.2 Results
Specifications of SVMs and LLMs used here are
shown in Table 2; |F
d
| is the number of active fea-
tures, while |x
d
| is the average number of active
features in each classification for the test corpus.
Dependency accuracy is the ratio of dependency
relations correctly identified by the parser, while
sentence accuracy is the exact match accuracy of
complete dependency relations in a sentence.
For LLM training, we designed explicit conjunc-
tive features for all the d or lower-order feature
combinations to make the results comparable to
those of SVMs. We could not train d = 4 LLMs
due to parameter explosion. We varied SVM soft
margin parameter c from 0.1 to 0.000001 and LLM
width factor parameter ?,
10
which controls the im-
pact of the prior, from 1.0 to 5.0, and adjusted
the values to maximize dependency accuracy for
the development set: (d, c) = (1, 0.1), (2, 0.005),
(3, 0.0001), (4, 0.000005) for SVMs and (d, ?) =
(1, 1.0), (2, 2.0), (3, 4.0) for `
1
-LLMs.
The accuracy of around 90.9% (SVM-KE, d =
3, 4) is close to the performance of state-of-the-
10
The parameter C of `
1
-LLM in Eq. 2 was set to ?/L
(referred to in Kazama and Tsujii (2003) as ?single width?).
1547
Model PKI Baseline Proposed w/ fstrie
S
Proposed w/ fstrie
M
Proposed w/ fstrie
L
Speed
type d classify Mem. Time [ms/sent.] Mem. Time [ms/sent.] Mem. Time [ms/sent.] Mem. Time [ms/sent.] up
[ms/sent.] (MB) classify (total) (MB) classify (total) (MB) classify (total) (MB) classify (total)
SVM-KE 1 13.480 0.2 0.003 (0.015) +0.6 0.006 (0.018) +20.2 0.007 (0.018) +662.9 0.016 (0.029) NA
SVM-KE 2 10.313 13.5 0.041 (0.054) +0.5 0.020 (0.032) +18.0 0.021 (0.034) +662.4 0.023 (0.036) 2.1
SVM-KE 3 10.945 142.2 0.345 (0.361) +0.5 0.163 (0.178) +18.2 0.108 (0.123) +667.0 0.079 (0.093) 4.4
SVM-KE 4 12.603 648.0 2.338 (2.363) +0.5 1.156 (1.178) +18.6 0.671 (0.690) +675.9 0.415 (0.432) 5.6
Table 3: Parsing results for test corpus: SVM-KE classifiers with dense feature space.
art parsers (Iwatate et al, 2008), and the model
statistics are considered to be complex (or re-
alistic) enough to evaluate our classifier?s util-
ity. The number of support vectors of SVMs was
71, 766 ? 9.2%, which is twice as many as those
used by Kudo and Matsumoto (2003) (34,996) in
their experiments on the same task.
We could clearly observe that the number of ac-
tive features |x
d
| increased dramatically according
to the order d of feature combinations. The den-
sity of |x
d
| for SVMs was very high (e.g., |x
3
| =
3286.7, close to the maximum shown in Eq. 8:
(27.3
3
+ 5? 27.3)/6 ' 3414.
For d ? 3 models, we attempted to control
the size of the feature space |F
d
| by changing
the model?s hyper-parameters: threshold ? for the
SVM-HKE and width factor ? for the `
1
-LLM. Al-
though we successfully reduced the size of the fea-
ture space |F
d
|, we could not dramatically reduce
the average number of active features |x
d
| in each
classification while keeping the accuracy advan-
tage. This confirms that the solution sparseness
does not suffice to obtain an efficient classifier.
We obtained source feature vectors to build
fstries by applying parsers with the target clas-
sifiers to a raw corpus in the target domain,
3,258,313 sentences of 1991?94 Mainichi news
articles that were morphologically analyzed by
JUMAN
6
and segmented into bunsetsus by KNP.
6
We first built fstrie
L
using all the source feature
vectors. We then attempted to reduce the number
of prefix feature vectors in fstrie
L
to 1/2
n
the size
by Algorithm 1. We refer to fstries built from 1/32
and 1/1024 of the prefix feature vectors in fstrie
L
as fstrie
M
and fstrie
S
in the following experiments.
Because we exploited Algorithm 2 to calcu-
late the weights of the prefix feature vectors, it
took less than one hour (59 min. 29 sec.) on the
3.20-GHz server to build fstrie
L
(and calculate the
utility score for all the nodes in it) for the slow-
est SVM-KE (d = 4) from the 40,409,190 source
feature vectors (62,654,549 prefix feature vectors)
generated by parsing the 3,258,313 sentences.
0
0.5
1
1.5
2
2.5
0 100 200 300 400 500 600 700A
ve
.
cla
ss
ific
atio
n
tim
e[m
s/s
en
t.]
Size of fstrie [MB]
SVM-KE (d = 1)
SVM-KE (d = 2)
SVM-KE (d = 3)
SVM-KE (d = 4)
Figure 3: Average classification time per sentence
plotted against size of fstrie: SVM-KE.
Results for SVM-KE with dense feature space
The performances of parsers having SVM-KE clas-
sifiers with and without the fstrie are given in Ta-
ble 3. The ?speed-up? column shows the speed-up
factor of the most efficient classifier (bold) ver-
sus the baseline classifier without fstries. Since
each classifier solved a slightly different num-
ber of classification steps (112, 853? 0.15%), we
show the (average) cumulative classification time
for a sentence. The Mem. columns show the size
of weight vectors for SVM-KE classifiers and the
size of fstries
S
, fstries
M
, and fstries
L
, respectively.
The fstries successfully speeded up SVM-KE
classifiers with the dense feature space.
11
The
SVM-KE classifiers without fstries were still faster
than PKI, but as expected from a large |x
d
| value,
the classifiers with higher conjunctive features
were much slower than the classifier with only
primitive features by factors of 13 (d = 2), 109
(d = 3) and 738 (d = 4) and the classification
time accounted for most of the parsing time.
The average classification time of our classifiers
plotted against fstrie size is shown in Figure 3.
Surprisingly, we obtained a significant speed-up
even with tiny fstrie sizes of < 1 MB. Further-
more, we naively controlled the fstrie size by sim-
11
The inefficiency of the classifier (d = 1) results from the
cost of the additional sort function (line 1 in Algorithm 2) and
CPU cache failure due to random accesses to the huge fstries.
1548
Model Baseline Proposed w/ fstrie
S
Proposed w/ fstrie
M
Proposed w/ fstrie
L
Speed
type d ? / ? Mem. Time [ms/sent.] Mem. Time [ms/sent.] Mem. Time [ms/sent.] Mem. Time [ms/sent.] up
(MB) classify (total) (MB) classify (total) (MB) classify (total) (MB) classify (total)
SVM-HKE 3 0.001 64.6 0.348 (0.363) +0.5 0.151 (0.166) +17.6 0.097 (0.111) +638.0 0.070 (0.084) 5.0
SVM-HKE 3 0.002 13.9 0.332 (0.346) +0.5 0.123 (0.137) +17.0 0.074 (0.088) +612.2 0.053 (0.067) 6.2
SVM-HKE 3 0.003 4.2 0.314 (0.328) +0.4 0.102 (0.115) +14.7 0.057 (0.070) +526.2 0.041 (0.054) 7.8
SVM-HKE 4 0.0002 235.0 2.258 (2.280) +0.5 1.022 (1.042) +17.7 0.558 (0.575) +637.1 0.330 (0.346) 6.8
SVM-HKE 4 0.0004 82.8 2.038 (2.058) +0.5 0.816 (0.835) +16.8 0.414 (0.430) +601.7 0.234 (0.249) 8.7
SVM-HKE 4 0.0006 32.2 1.802 (1.820) +0.4 0.646 (0.662) +15.7 0.311 (0.326) +558.9 0.168 (0.183) 10.7
`
1
-LLM 1 1.0 0.1 0.004 (0.016) +0.8 0.006 (0.018) +25.0 0.007 (0.019) +787.7 0.016 (0.029) NA
`
1
-LLM 2 2.0 0.4 0.043 (0.055) +0.6 0.016 (0.028) +20.5 0.015 (0.027) +698.0 0.018 (0.030) 2.9
`
1
-LLM 3 3.0 1.0 0.314 (0.326) +0.5 0.091 (0.103) +17.8 0.041 (0.054) +601.0 0.027 (0.040) 11.6
`
1
-LLM 3 4.0 0.7 0.300 (0.313) +0.5 0.082 (0.094) +16.3 0.036 (0.049) +550.1 0.024 (0.037) 12.4
`
1
-LLM 3 5.0 0.5 0.290 (0.302) +0.5 0.076 (0.088) +15.1 0.032 (0.045) +510.7 0.022 (0.035) 13.3
Table 4: Parsing results for test corpus: SVM-HKE and `
1
-LLM classifiers with sparse feature space.
0
0.5
1
1.5
2
2.5
0 10 20 30 40 50 60 70 80A
ve
.
cla
ss
ific
atio
n
tim
e[m
s/se
nt.
]
Size of fstrie [MB]
0.671 ms/sent.(18.6 MB)
0.680 ms/sent.(67.1 MB)
naive
utility score
Figure 4: Fstrie reduction: utility score vs. pro-
cessed sentence reduction for SVM-KE (d = 4).
ply reducing the number of sentences processed to
1/2
n
. The impact on the speed-up of the resulting
fstries (naive) and the fstries constructed by our
utility score (utility-score) on SVM-KE (d = 4)
is shown in Figure 4. The Zipfian nature of lan-
guage data let us obtain a substantial speed-up
even when we naively reduced the fstrie size, and
the utility score further decreased the fstrie size
required to obtain the same speed-up. We needed
less than 1/3 size fstries to achieve the same speed-
up: 0.671 ms./sent. (18.6 MB) (utility-score) vs.
0.680 ms./sent. (67.1 MB) (naive).
Results for SVM-HKE and `
1
-LLM classifiers
with sparse feature space The performances of
parsers having SVM-HKE and `
1
-LLM classifiers
with and without the fstrie are given in Table 4.
The fstries successfully speeded up the SVM-HKE
and `
1
-LLM classifiers by factors of 10.7 (SVM-
HKE, d = 4, ? = 0.0006) and 11.6 (`
1
-LLM,
d = 3, ? = 3.0). We obtained more speed-
up when we used fstries for classifiers with more
sparse feature space F
d
(Figures 5 and 6). The
parsing speed with d = 3 models are now compa-
rable to the parsing speed with d = 2 models.
0
0.05
0.1
0.15
0.2
0.25
0.3
0.35
0.4
0 100 200 300 400 500 600 700A
ve
.
cla
ss
ific
atio
n
tim
e[m
s/s
en
t.]
Size of fstrie [MB]
SVM-KE (d = 3)
SVM-HKE (d = 3, ? = 0.001)
SVM-HKE (d = 3, ? = 0.002)
SVM-HKE (d = 3, ? = 0.003)
Figure 5: Average classification time per sentence
plotted against size of fstrie: SVM-HKE (d = 3).
0
0.05
0.1
0.15
0.2
0.25
0.3
0.35
0.4
0 100 200 300 400 500 600 700A
ve
.
cla
ss
ific
atio
n
tim
e[m
s/s
en
t.]
Size of fstrie [MB]
?1-LLM (d = 3, ? = 3.0)?1-LLM (d = 3, ? = 4.0)?1-LLM (d = 3, ? = 5.0)
Figure 6: Average classification time per sentence
plotted against size of fstrie: `
1
-LLM (d = 3).
Without fstries, little speed-up of SVM-HKE
classifiers versus the SVM-KE classifiers (in Ta-
ble 3) was obtained due to the mild reduction in
the average number of active features |x
d
| in the
classification. This result conforms to the results
reported in (Kudo and Matsumoto, 2003).
The parsing speed reached 14,937 sentences
per second with accuracy of 90.91% (SVM-HKE,
d = 3, ? = 0.002). We used this parser to pro-
cess 1,005,918 sentences (5,934,184 bunsetsus)
randomly extracted from Japanese weblog feeds
1549
updated in November 2008, to see how much the
impact of fstries lessens when the test data and
the data processed to build fstries mismatch. The
parsing time was 156.4 sec. without fstrie
L
, while
it was just 35.9 sec. with fstrie
L
. The speed-up
factor of 4.4 on weblog feeds was slightly worse
than that on news articles (0.346/0.067 = 5.2)
but still evident. This implies that sorting features
in building fstries yielded prefix features vectors
that commonly appear in this task, by excluding
domain-specific features such as lexical features.
In summary, our algorithm successfully mini-
mized the efficiency gap among classifiers with
different degrees of feature combinations and
made accurate classifiers trained with higher-order
feature combinations practical.
5 Conclusion and Future Work
Our simple method speeds up a classifier trained
with many conjunctive features by using precal-
culated weights of (partial) feature vectors stored
in a feature sequence trie (fstrie). We experimen-
tally demonstrated that it speeded up SVM and
LLM classifiers for a Japanese dependency pars-
ing task by a factor of 10. We also confirmed that
the sparse feature space provided by `
1
-LLMs and
SVM-HKEs contributed much to size reduction of
the fstrie required to achieve the same speed-up.
The implementations of the proposed algorithm
for LLMs and SVMs (with a polynomial kernel) and
the Japanese dependency parser will be available
at http://www.tkl.iis.u-tokyo.ac.jp/?ynaga/.
We plan to apply our method to wider range of
classifiers used in various NLP tasks. To speed up
classifiers used in a real-time application, we can
build fstries incrementally by using feature vec-
tors generated from user inputs. When we run our
classifiers on resource-tight environments such as
cell-phones, we can use a random feature mix-
ing technique (Ganchev and Dredze, 2008) or a
memory-efficient trie implementation based on a
succinct data structure (Jacobson, 1989; Delpratt
et al, 2006) to reduce required memory usage.
We will combine our method with other tech-
niques that provide sparse solutions, for example,
kernel methods on a budget (Dekel and Singer,
2007; Dekel et al, 2008; Orabona et al, 2008) or
kernel approximation (surveyed in Kashima et al
(2009)). It is also easy to combine our method
with SVMs with partial kernel expansion (Gold-
berg and Elhadad, 2008), which will yield slower
but more space-efficient classifiers. We will in
the future consider an issue of speeding up decod-
ing with structured models (Lafferty et al, 2001;
Miyao and Tsujii, 2002; Sutton et al, 2004).
Acknowledgment The authors wish to thank
Susumu Yata and Yoshimasa Tsuruoka for letting
the authors to use their pre-release libraries. The
authors also thank Nobuhiro Kaji and the anony-
mous reviewers for their valuable comments.
References
Galen Andrew and Jianfeng Gao. 2007. Scalable train-
ing of L
1
-regularized log-linear models. In Proc.
ICML 2007, pages 33?40.
Jun?ichi Aoe. 1989. An efficient digital search al-
gorithm by using a double-array structure. IEEE
Transactions on Software Engineering, 15(9):1066?
1077, September.
Adam Berger, Stephen Della Pietra, and Vincent Della
Pietra. 1996. A maximum entropy approach to nat-
ural language processing. Computational Linguis-
tics, 22(1):39?71, March.
Kun-Ta Chuang, Jiun-Long Huang, and Ming-Syan
Chen. 2008. Power-law relationship and
self-similarity in the itemset support distribution:
analysis and applications. The VLDB Journal,
17(5):1121?1141, August.
Corinna Cortes and Vladimir Vapnik. 1995. Support-
vector networks. Machine Learning, 20(3):273?
297, September.
Ofer Dekel and Yoram Singer. 2007. Support vec-
tor machines on a budget. In Bernhard Sch?lkopf,
John Platt, and Thomas Hofmann, editors, Advances
in Neural Information Processing Systems 19, pages
345?352. The MIT Press.
Ofer Dekel, Shai Shalev-Shwartz, and Yoram Singer.
2008. The forgetron: A kernel-based perceptron on
a budget. SIAM Journal on Computing, 37(5):1342?
1372, January.
O?Neil Delpratt, Naila Rahman, and Rajeev Raman.
2006. Engineering the LOUDS succinct tree rep-
resentation. In Proc. WEA 2006, pages 134?145.
Leo Egghe. 2000. The distribution of n-grams. Scien-
tometrics, 47(2):237?252, February.
Kuzman Ganchev and Mark Dredze. 2008. Small sta-
tistical models by random feature mixing. In Proc.
ACL 2008 Workshop on Mobile Language Process-
ing, pages 19?20.
Jianfeng Gao, Galen Andrew, Mark Johnson, and
Kristina Toutanova. 2007. A comparative study
1550
of parameter estimation methods for statistical natu-
ral language processing. In Proc. ACL 2007, pages
824?831.
Yoav Goldberg and Michael Elhadad. 2008.
splitSVM: Fast, space-efficient, non-heuristic, poly-
nomial kernel computation for NLP applications. In
Proc. ACL 2008, Short Papers, pages 237?240.
Joshua Goodman. 2004. Exponential priors for max-
imum entropy models. In Proc. HLT-NAACL 2004,
pages 305?311.
Hideki Isozaki and Hideto Kazawa. 2002. Efficient
support vector classifiers for named entity recogni-
tion. In Proc. COLING 2002, pages 1?7.
Masakazu Iwatate, Masayuki Asahara, and Yuji Mat-
sumoto. 2008. Japanese dependency parsing using
a tournament model. In Proc. COLING 2008, pages
361?368.
Guy Jacobson. 1989. Space-efficient static trees and
graphs. In Proc. FOCS 1989, pages 549?554.
Hisashi Kashima, Tsuyoshi Id?, Tsuyoshi Kato, and
Masashi Sugiyama. 2009. Recent advances and
trends in large-scale kernel methods. IEICE Trans-
actions on on Information and Systems, E92-D. to
appear.
Jun?ichi Kazama and Jun?ichi Tsujii. 2003. Evaluation
and extension of maximum entropy models with in-
equality constraints. In Proc. EMNLP 2003, pages
137?144.
Jun?ichi Kazama and Jun?ichi Tsujii. 2005. Maxi-
mum entropy models with inequality constraints: A
case study on text categorization. Machine Learn-
ing, 60(1-3):159?194.
Terry Koo, Xavier Carreras, and Michael Collins.
2008. Simple semi-supervised dependency parsing.
In Proc. ACL 2008, pages 595?603.
Taku Kudo and Yuji Matsumoto. 2002. Japanese
dependency analysis using cascaded chunking. In
Proc. CoNLL 2002, pages 1?7.
Taku Kudo and Yuji Matsumoto. 2003. Fast methods
for kernel-based text analysis. In Proc. ACL 2003,
pages 24?31.
Sadao Kurohashi and Makoto Nagao. 2003. Build-
ing a Japanese parsed corpus. In Anne Abeill?, edi-
tor, Treebank: Building and Using Parsed Corpora,
pages 249?260. Kluwer Academic Publishers.
John D. Lafferty, Andrew McCallum, and Fernando
C. N. Pereira. 2001. Conditional random fields:
Probabilistic models for segmenting and labeling se-
quence data. In Proc. ICML 2001, pages 282?289.
Yudong Liu and Anoop Sarkar. 2007. Experimental
evaluation of LTAG-based features for semantic role
labeling. In Proc. EMNLP 2007, pages 590?599.
David McClosky, Eugene Charniak, and Mark John-
son. 2006. Effective self-training for parsing. In
Proc. HLT-NAACL 2006, pages 152?159.
Yusuke Miyao and Jun?ichi Tsujii. 2002. Maximum
entropy estimation for feature forests. In Proc. HLT
2002, pages 292?297.
Ngan L.T. Nguyen and Jin-Dong Kim. 2008. Explor-
ing domain differences for the design of a pronoun
resolution system for biomedical texts. In Proc.
COLING 2008, pages 625?632.
Joakim Nivre. 2003. An efficient algorithm for pro-
jective dependency parsing. In Proc. IWPT 2003,
pages 149?160.
Daisuke Okanohara and Jun?ichi Tsujii. 2009. Learn-
ing combination features with L
1
regularization. In
Proc. HLT-NAACL 2009, Short Papers, pages 97?
100.
Francesco Orabona, Joseph Keshet, and Barbara Ca-
puto. 2008. The projectron: a bounded kernel-based
perceptron. In Proc. ICML 2008, pages 720?727.
Patrick Pantel. 2007. Data catalysis: Facilitating large-
scale natural language data processing. In Proc.
ISUC, pages 201?204.
Stijn De Saeger, Kentaro Torisawa, and Jun?ichi
Kazama. 2009. Mining web-scale treebanks. In
Proc. NLP 2009, pages 837?840.
Manabu Sassano. 2004. Linear-time dependency anal-
ysis for Japanese. In Proc. COLING 2004, pages
8?14.
Drahom?ra ?Johanka? Spoustov?, Jan Haji?c, Jan Raab,
and Miroslav Spousta. 2009. Semi-supervised
training for the averaged perceptron POS tagger. In
Proc. EACL 2009, pages 763?771.
Charles Sutton, Khashayar Rohanimanesh, and An-
drew McCallum. 2004. Dynamic conditional ran-
dom fields: factorized probabilistic models for label-
ing and segmenting sequence data. In Proc. ICML
2004, pages 783?790.
Robert Tibshirani. 1996. Regression shrinkage and se-
lection via the lasso. Journal of the Royal Statistical
Society. Series B, 58(1):267?288, April.
Yu-Chieh Wu, Jie-Chi Yang, and Yue-Shi Lee. 2007.
An approximate approach for training polynomial
kernel SVMs in linear time. In Proc. ACL 2007
Poster and Demo Sessions, pages 65?68.
Susumu Yata, Kazuhiro Morita, Masao Fuketa, and
Jun?ichi Aoe. 2008. Fast string matching with
space-efficient word graphs. In Proc. Innovations
in Information Technology 2008, pages 79?83.
George K. Zipf. 1949. Human Behavior and the Prin-
ciple of Least-Effort. Addison-Wesley.
1551
Proceedings of the COLING/ACL 2006 Main Conference Poster Sessions, pages 452?459,
Sydney, July 2006. c?2006 Association for Computational Linguistics
Automatic Construction of Polarity-tagged Corpus from HTML
Documents
Nobuhiro Kaji and Masaru Kitsuregawa
Institute of Industrial Science
the University of Tokyo
4-6-1 Komaba, Meguro-ku, Tokyo 153-8505 Japan
 kaji,kitsure@tkl.iis.u-tokyo.ac.jp
Abstract
This paper proposes a novel method
of building polarity-tagged corpus from
HTML documents. The characteristics of
this method is that it is fully automatic and
can be applied to arbitrary HTML docu-
ments. The idea behind our method is
to utilize certain layout structures and lin-
guistic pattern. By using them, we can
automatically extract such sentences that
express opinion. In our experiment, the
method could construct a corpus consist-
ing of 126,610 sentences.
1 Introduction
Recently, there has been an increasing interest in
such applications that deal with opinions (a.k.a.
sentiment, reputation etc.). For instance, Mori-
naga et al developed a system that extracts and
analyzes reputations on the Internet (Morinaga et
al., 2002). Pang et al proposed a method of clas-
sifying movie reviews into positive and negative
ones (Pang et al, 2002).
In these applications, one of the most important
issue is how to determine the polarity (or semantic
orientation) of a given text. In other words, it is
necessary to decide whether a given text conveys
positive or negative content.
In order to solve this problem, we intend to
take statistical approach. More specifically, we
plan to learn the polarity of texts from a cor-
pus in which phrases, sentences or documents
are tagged with labels expressing the polarity
(polarity-tagged corpus).
So far, this approach has been taken by a lot of
researchers (Pang et al, 2002; Dave et al, 2003;
Wilson et al, 2005). In these previous works,
polarity-tagged corpus was built in either of the
following two ways. It is built manually, or created
from review sites such as AMAZON.COM. In some
review sites, the review is associated with meta-
data indicating its polarity. Those reviews can be
used as polarity-tagged corpus. In case of AMA-
ZON.COM, the review?s polarity is represented by
using 5-star scale.
However, both of the two approaches are not
appropriate for building large polarity-tagged cor-
pus. Since manual construction of tagged corpus
is time-consuming and expensive, it is difficult to
build large polarity-tagged corpus. The method
that relies on review sites can not be applied to
domains in which large amount of reviews are not
available. In addition, the corpus created from re-
views is often noisy as we discuss in Section 2.
This paper proposes a novel method of building
polarity-tagged corpus from HTML documents.
The idea behind our method is to utilize certain
layout structures and linguistic pattern. By using
them, we can automatically extract sentences that
express opinion (opinion sentences) from HTML
documents. Because this method is fully auto-
matic and can be applied to arbitrary HTML doc-
uments, it does not suffer from the same problems
as the previous methods.
In the experiment, we could construct a corpus
consisting of 126,610 sentences. To validate the
quality of the corpus, two human judges assessed
a part of the corpus and found that 92% opinion
sentences are appropriate ones. Furthermore, we
applied our corpus to opinion sentence classifica-
tion task. Naive Bayes classifier was trained on
our corpus and tested on three data sets. The re-
sult demonstrated that the classifier achieved more
than 80% accuracy in each data set.
The following of this paper is organized as fol-
452
lows. Section 2 shows the design of the corpus
constructed by our method. Section 3 gives an
overview of our method, and the detail follows in
Section 4. In Section 5, we discuss experimen-
tal results, and in Section 6 we examine related
works. Finally we conclude in Section 7.
2 Corpus Design
This Section explains the design of our corpus that
is built automatically. Table 1 represents a part
of our corpus that was actually constructed in the
experiment. Note that this paper treats Japanese.
The sentences in the Table are translations, and the
original sentences are in Japanese.
The followings are characteristics of our corpus:
  Our corpus uses two labels,   and . They
denote positive and negative sentences re-
spectively. Other labels such as ?neutral? are
not used.
  Since we do not use ?neutral? label, such sen-
tence that does not convey opinion is not
stored in our corpus.
  The label is assigned to not multiple sen-
tences (or document) but single sentence.
Namely, our corpus is tagged at sentence
level rather than document level.
It is important to discuss the reason that we in-
tend to build a corpus tagged at sentence level
rather than document level. The reason is that one
document often includes both positive and nega-
tive sentences, and hence it is difficult to learn
the polarity from the corpus tagged at document
level. Consider the following example (Pang et
al., 2002):
This film should be brilliant. It sounds
like a great plot, the actors are first
grade, and the supporting cast is good as
well, and Stallone is attempting to de-
liver a good performance. However, it
can?t hold up.
This document as a whole expresses negative
opinion, and should be labeled ?negative? if it is
tagged at document level. However, it includes
several sentences that represent positive attitude.
We would like to point out that polarity-tagged
corpus created from reviews prone to be tagged at
document-level. This is because meta-data (e.g.
stars in AMAZON.COM) is usually associated with
one review rather than individual sentences in a
review. This is one serious problem in previous
works.
Table 1: A part of automatically constructed
polarity-tagged corpus.
label opinion sentence
  It has high adaptability.
 The cost is expensive.
 The engine is powerless and noisy.
  The usage is easy to understand.
  Above all, the price is reasonable.
3 The Idea
This Section briefly explains our basic idea, and
the detail of our corpus construction method is
represented in the next Section.
Our idea is to use certain layout structures and
linguistic pattern in order to extract opinion sen-
tences from HTML documents. More specifically,
we used two kinds of layout structures: the item-
ization and the table. In what follows, we ex-
plain examples where opinion sentences can be
extracted by using the itemization, table and lin-
guistic pattern.
3.1 Itemization
The first idea is to extract opinion sentences from
the itemization (Figure 1). In this Figure, opinions
about a music player are itemized and these item-
izations have headers such as ?pros? and ?cons?.
By using the headers, we can recognize that opin-
ion sentences are described in these itemizations.
Pros:
  The sound is natural.
  Music is easy to find.
  Can enjoy creating my favorite play-lists.
Cons:
  The remote controller does not have an LCD dis-
play.
  The body gets scratched and fingerprinted easily.
  The battery drains quickly when using the back-
light.
Figure 1: Opinion sentences in itemization.
Hereafter, such phrases that indicate the pres-
453
ence of opinion sentences are called indicators.
Indicators for positive sentences are called positive
indicators. ?Pros? is an example of positive indi-
cator. Similarly, indicators for negative sentences
are called negative indicators.
3.2 Table
The second idea is to use the table structure (Fig-
ure 2). In this Figure, a car review is summarized
in the table.
Mileage(urban) 7.0km/litter
Mileage(highway) 9.0km/litter
Plus This is a four door car, but it?s
so cool.
Minus The seat is ragged and the light
is dark.
Figure 2: Opinion sentences in table.
We can predict that there are opinion sentences
in this table, because the left column acts as a
header and there are indicators (plus and minus)
in that column.
3.3 Linguistic pattern
The third idea is based on linguistic pattern. Be-
cause we treat Japanese, the pattern that is dis-
cussed in this paper depends on Japanese gram-
mar although we think there are similar patterns in
other languages including English.
Consider the Japanese sentences attached with
English translations (Figure 3). Japanese sen-
tences are written in italics and ?-? denotes that
the word is followed by postpositional particles.
For example, ?software-no? means that ?software?
is followed by postpositional particle ?no?. Trans-
lations of each word and the entire sentence are
represented below the original Japanese sentence.
?-POST? means postpositional particle.
In the examples, we focused on the singly un-
derlined phrases. Roughly speaking, they corre-
spond to ?the advantage/weakness is to? in En-
glish. In these phrases, indicators (?riten (ad-
vantage)? and ?ketten (weakness)?) are followed
by postpositional particle ?-ha?, which is topic
marker. And hence, we can recognize that some-
thing good (or bad) is the topic of the sentence.
Based on this observation, we crafted a linguis-
tic pattern that can detect the singly underlined
phrases. And then, we extracted doubly under-
lined phrases as opinions. They correspond to ?run
quickly? and ?take too much time?. The detail of
this process is discussed in the next Section.
4 Automatic Corpus Construction
This Section represents the detail of the corpus
construction procedure.
As shown in the previous Section, our idea uti-
lizes the indicator, and it is important to recognize
indicators in HTML documents. To do this, we
manually crafted lexicon, in which positive and
negative indicators are listed. This lexicon con-
sists of 303 positive and 433 negative indicators.
Using this lexicon, the polarity-tagged corpus is
constructed from HTML documents. The method
consists of the following three steps:
1. Preprocessing
Before extracting opinion sentences, HTML
documents are preprocessed. This process
involves separating texts form HTML tags,
recognizing sentence boundary, and comple-
menting omitted HTML tags etc.
2. Opinion sentence extraction
Opinion sentences are extracted from HTML
documents by using the itemization, table
and linguistic pattern.
3. Filtering
Since HTML documents are noisy, some of
the extracted opinion sentences are not ap-
propriate. They are removed in this step.
For the preprocessing, we implemented simple
rule-based system. We cannot explain its detail
for lack of space. In the remainder of this Section,
we describe three extraction methods respectively,
and then examine filtering technique.
4.1 Extraction based on itemization
The first method utilizes the itemization. In order
to extract opinion sentences, first of all, we have
to find such itemization as illustrated in Figure 1.
They are detected by using indicator lexicon and
HTML tags such as  h1 and  ul etc.
After finding the itemizations, the sentences in
the items are extracted as opinion sentences. Their
polarity labels are assigned according to whether
the header is positive or negative indicator. From
the itemization in Figure 1, three positive sen-
tences and three negative ones are extracted.
The problem here is how to treat such item that
has more than one sentences (Figure 4). In this
itemization, there are two sentences in each of the
454
(1) kono software-no riten-ha hayaku ugoku koto
this software-POST advantage-POST quickly run to
The advantage of this software is to run quickly.
(2) ketten-ha jikan-ga kakarisugiru koto-desu
weakness-POST time-POST take too much to-POST
The weakness is to take too much time.
Figure 3: Instances of the linguistic pattern.
third and fourth item. It is hard to precisely pre-
dict the polarity of each sentence in such items,
because such item sometimes includes both posi-
tive and negative sentences. For example, in the
third item of the Figure, there are two sentences.
One (?Has high pixel...?) is positive and the other
(?I was not satisfied...?) is negative.
To get around this problem, we did not use such
items. From the itemization in Figure 4, only two
positive sentences are extracted (?the color is re-
ally good? and ?this camera makes me happy while
taking pictures?).
Pros:
  The color is really good.
  This camera makes me happy while taking pic-
tures.
  Has high pixel resolution with 4 million pixels. I
was not satisfied with 2 million.
  EVF is easy to see. But, compared with SLR, it?s
hard to see.
Figure 4: Itemization where more than one sen-
tences are written in one item.
4.2 Extraction based on table
The second method extracts opinion sentences
from the table. Since the combination of  table
and other tags can represent various kinds of ta-
bles, it is difficult to craft precise rules that can
deal with any table.
Therefore, we consider only two types of tables
in which opinion sentences are described (Figure
5). Type A is a table in which the leftmost column
acts as a header, and there are indicators in that
column. Similarly, type B is a table in which the
first row acts as a header. The table illustrated in
Figure 2 is categorized into type A.
The type of the table is decided as follows. The
table is categorized into type A if there are both
type A
 
 
     
 
 
  
type B
?  
 
 
 
?
?    ?
?    ?
?    ?

 
:positive indicator  :positive sentence

 
:negative indicator :negative sentence
Figure 5: Two types of tables.
positive and negative indicators in the leftmost col-
umn. The table is categorized into type B if it is
not type A and there are both positive and negative
indicators in the first row.
After the type of the table is decided, we can
extract opinion sentences from the cells that cor-
respond to   and  in the Figure 5. It is obvi-
ous which label (positive or negative) should be
assigned to the extracted sentence.
We did not use such cell that contains more than
one sentences, because it is difficult to reliably
predict the polarity of each sentence. This is simi-
lar to the extraction from the itemization.
4.3 Extraction based on linguistic pattern
The third method uses linguistic pattern. The char-
acteristic of this pattern is that it takes dependency
structure into consideration.
First of all, we explain Japanese dependency
structure. Figure 6 depicts the dependency rep-
resentations of the sentences in the Figure 3.
Japanese sentence is represented by a set of de-
pendencies between phrasal units called bunsetsu-
phrases. Broadly speaking, bunsetsu-phrase is an
unit similar to baseNP in English. In the Fig-
ure, square brackets enclose bunsetsu-phrase and
arrows show modifier  head dependencies be-
tween bunsetsu-phrases.
In order to extract opinion sentences from these
dependency representations, we crafted the fol-
lowing dependency pattern.
455
[ kono
this
] [ software-no
software-POST
] [ riten-ha
advantage-POST
] [ hayaku
quickly
] [ ugoku
run
] [ koto
to
]
[ ketten-ha
weakness-POST
] [ jikan-ga
time-POST
] [ kakari sugiru
take too much
] [ koto-desu
to-POST
]
Figure 6: Dependency representations.
[ INDICATOR-ha ] [ koto-POST* ]
This pattern matches the singly underlined
bunsetsu-phrases in the Figure 6. In the modi-
fier part of this pattern, the indicator is followed
by postpositional particle ?ha?, which is topic
marker1. In the head part, ?koto (to)? is followed
by arbitrary numbers of postpositional particles.
If we find the dependency that matches this pat-
tern, a phrase between the two bunsetsu-phrases
is extracted as opinion sentence. In the Figure 6,
the doubly underlined phrases are extracted. This
heuristics is based on Japanese word order con-
straint.
4.4 Filtering
Sentences extracted by the above methods some-
times include noise text. Such texts have to be fil-
tered out. There are two cases that need filtering
process.
First, some of the extracted sentences do not ex-
press opinions. Instead, they represent objects to
which the writer?s opinion is directed (Table 7).
From this table, ?the overall shape? and ?the shape
of the taillight? are wrongly extracted as opinion
sentences. Since most of the objects are noun
phrases, we removed such sentences that have the
noun as the head.
Mileage(urban) 10.0km/litter
Mileage(highway) 12.0km/litter
Plus The overall shape.
Minus The shape of the taillight.
Figure 7: A table describing only objects to which
the opinion is directed.
Secondly, we have to treat duplicate opinion
sentences because there are mirror sites in the
1To be exact, some of the indicators such as ?strong point?
consists of more than one bunsetsu-phrase, and the modifier
part sometimes consists of more than one bunsetsu-phrase.
HTML documents. When there are more than one
sentences that are exactly the same, one of them is
held and the others are removed.
5 Experimental Results and Discussion
This Section examines the results of corpus con-
struction experiment. To analyze Japanese sen-
tence we used Juman and KNP2.
5.1 Corpus Construction
About 120 millions HTML documents were pro-
cessed, and 126,610 opinion sentences were ex-
tracted. Before the filtering, there were 224,002
sentences in our corpus. Table2 shows the statis-
tics of our corpus. The first column represents the
three extraction methods. The second and third
column shows the number of positive and nega-
tive sentences by extracted each method. Some
examples are illustrated in Table 3.
Table 2: # of sentences in the corpus.
Positive Negative Total
Itemization 18,575 15,327 33,902
Table 12,103 11,016 23,119
Linguistic Pattern 34,282 35,307 69,589
Total 64,960 61,650 126,610
The result revealed that more than half of the
sentences are extracted by linguistic pattern (see
the fourth row). Our method turned out to be ef-
fective even in the case where only plain texts are
available.
5.2 Quality assessment
In order to check the quality of our corpus,
500 sentences were randomly picked up and two
judges manually assessed whether appropriate la-
bels are assigned to the sentences.
The evaluation procedure is the followings.
2http://www.kc.t.u-tokyo.ac.jp/nl-resource/top.html
456
Table 3: Examples of opinion sentences.
label opinion sentence
 
cost keisan-ga yoininaru
cost computation-POST become easy
It becomes easy to compute cost.
 
kantan-de jikan-ga setsuyakudekiru
easy-POST time-POST can save
It?s easy and can save time.
 
soup-ha koku-ga ari oishii
soup-POST rich flavorful
The soup is rich and flavorful.

HTML keishiki-no mail-ni taioshitenai
HTML format-POST mail-POST cannot use
Cannot use mails in HTML format.

jugyo-ga hijoni tsumaranai
lecture-POST really boring
The lecture is really boring.

kokoro-ni nokoru ongaku-ga nai
impressive music-POST there is no
There is no impressive music.
  Each of the 500 sentences are shown to the
two judges. Throughout this evaluation, We
did not present the label automatically tagged
by our method. Similarly, we did not show
HTML documents from which the opinion
sentences are extracted.
  The two judges individually categorized each
sentence into three groups: positive, negative
and neutral/ambiguous. The sentence is clas-
sified into the third group, if it does not ex-
press opinion (neutral) or if its polarity de-
pends on the context (ambiguous). Thus, two
goldstandard sets were created.
  The precision is estimated using the goldstan-
dard. In this evaluation, the precision refers
to the ratio of sentences where correct la-
bels are assigned by our method. Since we
have two goldstandard sets, we can report
two different precision values. A sentence
that is categorized into neutral/ambiguous by
the judge is interpreted as being assigned in-
correct label by our method, since our corpus
does not have a label that corresponds to neu-
tral/ambiguous.
We investigated the two goldstandard sets, and
found that the judges agree with each other in 467
out of 500 sentences (93.4%). The Kappa value
was 0.901. From this result, we can say that the
goldstandard was reliably created by the judges.
Then, we estimated the precision. The precision
was 459/500 (91.5%) when one goldstandard was
used, and 460/500 (92%) when the other was used.
Since these values are nearly equal to the agree-
ment between humans (467/500), we can conclude
that our method successfully constructed polarity-
tagged corpus.
After the evaluation, we analyzed errors and
found that most of them were caused by the lack
of context. The following is a typical example.
You see, there is much information.
In our corpus this sentence is categorized into pos-
itive one. The below is a part of the original docu-
ment from which this sentence was extracted.
I recommend this guide book. The Pros.
of this book is that, you see, there is
much information.
On the other hand, both of the two judges catego-
rized the above sentence into neutral/ambiguous,
probably because they can easily assume context
where much information is not desirable.
You see, there is much information. But,
it is not at all arranged, and makes me
confused.
In order to precisely treat this kind of sentences,
we think discourse analysis is inevitable.
5.3 Application to opinion classification
Next, we applied our corpus to opinion sentence
classification. This is a task of classifying sen-
tences into positive and negative. We trained a
classifier on our corpus and investigated the result.
Classifier and data sets As a classifier, we
chose Naive Bayes with bag-of-words features,
because it is one of the most popular one in this
task. Negation was processed in a similar way as
previous works (Pang et al, 2002).
To validate the accuracy of the classifier, three
data sets were created from review pages in which
the review is associated with meta-data. To build
data sets tagged at sentence level, we used such re-
views that contain only one sentence. Table 4 rep-
resents the domains and the number of sentences
in each data set. Note that we confirmed there is
no duplicate between our corpus and the these data
sets.
The result and discussion Naive Bayes classi-
fier was trained on our corpus and tested on the
three data sets (Table 5). In the Table, the sec-
ond column represents the accuracy of the clas-
sification in each data set. The third and fourth
457
Table 5: Classification result.
Accuracy Positive Negative
Precision Recall Precision Recall
Computer 0.831 0.856 0.804 0.804 0.859
Restaurant 0.849 0.905 0.859 0.759 0.832
Car 0.833 0.860 0.844 0.799 0.819
Table 4: The data sets.
Domain # of sentences
Positive Negative
Computer 933 910
Restaurant 753 409
Car 1,056 800
columns represent precision and recall of positive
sentences. The remaining two columns show those
of negative sentences. Naive Bayes achieved over
80% accuracy in all the three domains.
In order to compare our corpus with a small
domain specific corpus, we estimated accuracy in
each data set using 10 fold crossvalidation (Ta-
ble 6). In two domains, the result of our corpus
outperformed that of the crossvalidation. In the
other domain, our corpus is slightly better than the
crossvalidation.
Table 6: Accuracy comparison.
Our corpus Crossvalidation
Computer 0.831 0.821
Restaurant 0.849 0.848
Car 0.833 0.808
One finding is that our corpus achieved good ac-
curacy, although it includes various domains and is
not accustomed to the target domain. Turney also
reported good result without domain customiza-
tion (Turney, 2002). We think these results can be
further improved by domain adaptation technique,
and it is one future work.
Furthermore, we examined the variance of the
accuracy between different domains. We trained
Naive Bayes on each data set and investigate the
accuracy in the other data sets (Table 7). For ex-
ample, when the classifier is trained on Computer
and tested on Restaurant, the accuracy was 0.757.
This result revealed that the accuracy is quite poor
when the training and test sets are in different do-
mains. On the other hand, when Naive Bayes is
trained on our corpus, there are little variance in
different domains (Table 5). This experiment in-
dicates that our corpus is relatively robust against
the change of the domain compared with small do-
main specific corpus. We think this is because our
corpus is large and balanced. Since we cannot al-
ways get domain specific corpus in real applica-
tion, this is the strength of our corpus.
Table 7: Cross domain evaluation.
Training
Computer Restaurant Car
Computer ? 0.701 0.773
Test Restaurant 0.757 ? 0.755
Car 0.751 0.711 ?
6 Related Works
6.1 Learning the polarity of words
There are some works that discuss learning the po-
larity of words instead of sentences.
Hatzivassiloglou and McKeown proposed a
method of learning the polarity of adjectives from
corpus (Hatzivassiloglou and McKeown, 1997).
They hypothesized that if two adjectives are con-
nected with conjunctions such as ?and/but?, they
have the same/opposite polarity. Based on this hy-
pothesis, their method predicts the polarity of ad-
jectives by using a small set of adjectives labeled
with the polarity.
Other works rely on linguistic resources such
as WordNet (Kamps et al, 2004; Hu and Liu,
2004; Esuli and Sebastiani, 2005; Takamura et al,
2005). For example, Kamps et al used a graph
where nodes correspond to words in the Word-
Net, and edges connect synonymous words in the
WordNet. The polarity of an adjective is defined
by its shortest paths from the node corresponding
to ?good? and ?bad?.
Although those researches are closely related to
our work, there is a striking difference. In those
researches, the target is limited to the polarity of
words and none of them discussed sentences. In
addition, most of the works rely on external re-
sources such as the WordNet, and cannot treat
words that are not in the resources.
458
6.2 Learning subjective phrases
Some researchers examined the acquisition of sub-
jective phrases. The subjective phrase is more gen-
eral concept than opinion and includes both posi-
tive and negative expressions.
Wiebe learned subjective adjectives from a set
of seed adjectives. The idea is to automatically
identify the synonyms of the seed and to add them
to the seed adjectives (Wiebe, 2000). Riloff et
al. proposed a bootstrapping approach for learn-
ing subjective nouns (Riloff et al, 2003). Their
method learns subjective nouns and extraction pat-
terns in turn. First, given seed subjective nouns,
the method learns patterns that can extract sub-
jective nouns from corpus. And then, the pat-
terns extract new subjective nouns from corpus,
and they are added to the seed nouns. Although
this work aims at learning only nouns, in the sub-
sequent work, they also proposed a bootstrapping
method that can deal with phrases (Riloff and
Wiebe, 2003). Similarly, Wiebe also proposes a
bootstrapping approach to create subjective and
objective classifier (Wiebe and Riloff, 2005).
These works are different from ours in a sense
that they did not discuss how to determine the po-
larity of subjective words or phrases.
6.3 Unsupervised sentiment classification
Turney proposed the unsupervised method for sen-
timent classification (Turney, 2002), and similar
method is utilized by many other researchers (Yu
and Hatzivassiloglou, 2003). The concept behind
Turney?s model is that positive/negative phrases
co-occur with words like ?excellent/poor?. The co-
occurrence statistic is measured by the result of
search engine. Since his method relies on search
engine, it is difficult to use rich linguistic informa-
tion such as dependencies.
7 Conclusion
This paper proposed a fully automatic method of
building polarity-tagged corpus from HTML doc-
uments. In the experiment, we could build a cor-
pus consisting of 126,610 sentences.
As a future work, we intend to extract more
opinion sentences by applying this method to
larger HTML document sets and enhancing ex-
traction rules. Another important direction is to
investigate more precise model that can classify or
extract opinions, and learn its parameters from our
corpus.
References
Kushal Dave, Steve Lawrence, and David M.Pennock.
2003. Mining the peanut gallery: Opinion extrac-
tion and semantic classification of product revews.
In Proceedings of the WWW, pages 519?528.
Andrea Esuli and Fabrizio Sebastiani. 2005. Deter-
mining the semantic orientation of terms throush
gloss classification. In Proceedings of the CIKM.
Vasileios Hatzivassiloglou and Katheleen R. McKe-
own. 1997. Predicting the semantic orientation of
adjectives. In Proceedings of the ACL, pages 174?
181.
Minqing Hu and Bing Liu. 2004. Mining and sum-
marizing customer reviews. In Proceedings of the
KDD, pages 168?177.
Jaap Kamps, Maarten Marx, Robert J. Mokken, and
Maarten de Rijke. 2004. Using wordnet to measure
semantic orientations of adjectives. In Proceedings
of the LREC.
Satoshi Morinaga, Kenji Yamanishi, Kenji Tateishi,
and Toshikazu Fukushima. 2002. Mining product
reputations on the web. In Proceedings of the KDD.
Bo Pang, Lillian Lee, and Shivakumar Vaihyanathan.
2002. Thumbs up? sentiment classification using
machine learning techniques. In Proceedings of the
EMNLP.
Ellen Riloff and Janyce Wiebe. 2003. Learning extrac-
tion patterns for subjective expressions. In Proceed-
ings of the EMNLP.
Ellen Riloff, JanyceWiebe, and TheresaWilson. 2003.
Learning subjective nouns using extraction pattern
bootstrapping. In Proceedings of the CoNLL.
Hiroya Takamura, Takashi Inui, andManabu Okumura.
2005. Extracting semantic orientation of words us-
ing spin model. In Proceedings of the ACL, pages
133?140.
Peter D. Turney. 2002. Thumbs up or thumbs down?
senmantic orientation applied to unsupervised clas-
sification of reviews. In Proceedings of the ACL,
pages 417?424.
Janyce Wiebe and Ellen Riloff. 2005. Creating subjec-
tive and objective sentence classifiers from unanno-
tated texts. In Proceedings of the CICLing.
Janyce M. Wiebe. 2000. Learning subjective adjec-
tives from corpora. In Proceedings of the AAAI.
Theresa Wilson, Janyce Wiebe, and Paul Hoffmann.
2005. Recognizing contextual polarity in phrase-
level sentiment analysis. In Proceedings of the
HLT/EMNLP.
Hong Yu and Yasileios Hatzivassiloglou. 2003. To-
wards answering opinion questions: Separating facts
from opinions and identifying the polarity of opinion
sentences. In Proceedings of the EMNLP.
459
Proceedings of the ACL-IJCNLP 2009 Conference Short Papers, pages 61?64,
Suntec, Singapore, 4 August 2009. c?2009 ACL and AFNLP
A Combination of Active Learning and Semi-supervised Learning 
Starting with Positive and Unlabeled Examples for Word Sense   
Disambiguation: An Empirical Study on Japanese Web Search Query 
Makoto Imamura 
and Yasuhiro Takayama 
Information Technology R&D Center,  
Mitsubishi Electric Corporation 
5-1-1 Ofuna, Kamakura, Kanagawa, Japan 
{Imamura.Makoto@bx,Takayama.Yasu 
hiro@ea}.MitsubishiElectric.co.jp 
Nobuhiro Kaji, Masashi Toyoda  
and Masaru Kitsuregawa 
Institute of Industrial Science, 
The University of Tokyo 
4-6-1 Komaba, Meguro-ku Tokyo, Japan 
{kaji,toyoda,kitsure} 
@tkl.iis.u-tokyo.ac.jp 
 
 
Abstract 
This paper proposes to solve the bottle-
neck of finding training data for word 
sense disambiguation (WSD) in the do-
main of web queries, where a complete set 
of ambiguous word senses are unknown. 
In this paper, we present a combination of 
active learning and semi-supervised learn-
ing method to treat the case when positive 
examples, which have an expected word 
sense in web search result, are only given. 
The novelty of our approach is to use 
?pseudo negative examples? with reliable 
confidence score estimated by a classifier 
trained with positive and unlabeled exam-
ples. We show experimentally that our 
proposed method achieves close enough 
WSD accuracy to the method with the 
manually prepared negative examples in 
several Japanese Web search data. 
1 Introduction 
In Web mining for sentiment or reputation 
analysis, it is important for reliable analysis to 
extract large amount of texts about certain prod-
ucts, shops, or persons with high accuracy. When 
retrieving texts from Web archive, we often suf-
fer from word sense ambiguity and WSD system 
is indispensable. For instance, when we try to 
analyze reputation of "Loft", a name of variety 
store chain in Japan, we found that simple text 
search retrieved many unrelated texts which con-
tain "Loft" with different senses such as an attic 
room, an angle of golf club face, a movie title, a 
name of a club with live music and so on. The 
words in Web search queries are often proper 
nouns. Then it is not trivial to discriminate these 
senses especially for the language like Japanese 
whose proper nouns are not capitalized. 
To train WSD systems we need a large 
amount of positive and negative examples. In the 
real Web mining application, how to acquire 
training data for a various target of analysis has 
become a major hurdle to use supervised WSD.  
Fortunately, it is not so difficult to create posi-
tive examples. We can retrieve positive examples 
from Web archive with high precision (but low 
recall) by manually augmenting queries with hy-
pernyms or semantically related words (e.g., 
"Loft AND shop" or "Loft AND stationary").  
On the other hand, it is often costly to create 
negative examples. In principle, we can create 
negative examples in the same way as we did to 
create positive ones. The problem is, however, 
that we are not sure of most of the senses of a 
target word. Because target words are often 
proper nouns, their word senses are rarely listed 
in hand-crafted lexicon. In addition, since the 
Web is huge and contains heterogeneous do-
mains, we often find a large number of unex-
pected senses. For example, all the authors did 
not know the music club meaning of Loft. As the 
result, we often had to spend much time to find 
such unexpected meaning of target words. 
This situation motivated us to study active 
learning for WSD starting with only positive ex-
amples. The previous techniques (Chan and Ng, 
2007; Chen et al 2006) require balanced positive 
and negative examples to estimate the score. In 
our problem setting, however, we have no nega-
tive examples at the initial stage. To tackle this 
problem, we propose a method of active learning 
for WSD with pseudo negative examples, which 
are selected from unlabeled data by a classifier 
trained with positive and unlabeled examples. 
McCallum and Nigam (1998) combined active 
learning and semi-supervised learning technique 
61
by using EM with unlabeled data integrated into 
active learning, but it did not treat our problem 
setting where only positive examples are given. 
The construction of this paper is as follows; 
Section 2 describes a proposed learning algo-
rithm. Section 3 shows the experimental results.  
2 Learning Starting with Positive and 
Unlabeled Examples for WSD 
We treat WSD problem as binary classification 
where desired texts are positive examples and 
other texts are negative examples. This setting is 
practical, because ambiguous senses other than 
the expected sense are difficult to know and are 
no concern in  most Web mining applications. 
2.1 Classifier 
For our experiment, we use naive Bayes classifi-
ers as learning algorithm. In performing WSD, 
the sense ?s? is assigned to an example charac-
terized with the probability of linguistic features 
f1,...,fn so as to maximize: 
?
=
n
j
pp
1
)|(f)( ss j               (1) 
The sense s is positive when it is the target 
meaning in Web mining application, otherwise s 
is negative. We use the following typical linguis-
tic features for Japanese sentence analysis, (a) 
Word feature within sentences, (b) Preceding 
word feature within bunsetsu (Japanese base 
phrase), (c) Backward word feature within bun-
setsu, (d) Modifier bunsetsu feature and (e) 
Modifiee bunsetsu feature. 
Using naive Bayes classifier, we can estimate 
the confidence score c(d, s) that the sense of a 
data instance ?d?, whose features are f1, f2, ..., fn, 
is predicted sense ?s?.  
?
=
+=
n
j
pp
1
)|(f log)( logs)c(d, ss j      (2) 
2.2 Proposed Algorithm 
At the beginning of our algorithm, the system is 
provided with positive examples and unlabeled 
examples. The positive examples are collected 
by full text queries with hypernyms or semanti-
cally related words. 
First we select positive dataset P from initial 
dataset by manually augmenting full text query.      
At each iteration of active learning, we select 
pseudo negative dataset Np (Figure 1 line 15). In 
selecting pseudo negative dataset, we predict 
word sense of each unlabeled example using the 
naive Bayes classifier with all the unlabeled ex-
amples as negative examples (Figure 2). In detail, 
if the prediction score (equation(3)) is more than 
?, which means the example is very likely to be 
negative, it is considered as the pseudo negative 
example (Figure 2 line 10-12). 
pos)c(d,neg)c(d,psdNeg)c(d, ?=          (3) 
 
01    # Definition 
02   ?(P, N): WSD system trained on P as Positive  
03                   examples, N as Negative examples.  
04   ?EM(P, N, U): WSD system trained on P as  
05   Positive examples, N as Negative examples, 
06   U as Unlabeled examples by using EM  
07   (Nigam et. all 2000) 
08    # Input 
09    T ? Initial unlabeled dataset which contain  
10            ambiguous words 
11    # Initialization 
12    P ?  positive training dataset by full text search on T 
13    N ? ? (initial negative training dataset) 
14    repeat 
15      # selecting pseudo negative examples Np  
16          by   the score of  ?(P, T-P)  (see figure 2) 
17      # building a classifier with  Np 
18      ?new ? ?EM (P,  N+Np, T-N-P)   
19      #  sampling data by using the score of ?new 
20      cmin   ? ? 
21      foreach d ? (T ? P ? N )  
22         classify d by WSD system?new 
23         s(d) ? word sense prediction for d using?new 
24         c(d, s(d)) ? the confidence of  prediction of d 
25         if c(d, s(d))  ? cmin   then  
26             cmin  ? c(d),   d min ? d 
27      end 
28    end 
29     provide correct sense s for d min  by human 
30     if s is positive then add d min   to P 
31                             else  add d min   to N 
32   until Training dataset reaches desirable size 
33   ?new  is the output classifier 
 Figure 1: A combination of active learning and 
semi-supervised learning starting with positive 
and unlabeled examples 
Next we use Nigam?s semi-supervised learning 
method using EM and a naive Bayes classifier 
(Nigam et. all, 2000) with pseudo negative data-
set Np  as negative training dataset to build the 
refined classifier ?EM (Figure 1 line 17).  
In building training dataset by active learning, 
we use uncertainty sampling like (Chan and Ng, 
2007) (Figure 1 line 30-31). This step selects the 
most uncertain example that is predicted with the 
lowest confidence in the refined classifier ?EM. 
Then, the correct sense for the most uncertain 
62
example is provided by human and added to the 
positive dataset P or the negative dataset N ac-
cording to the sense of d. 
The above steps are repeated until dataset 
reaches the predefined desirable size. 
 
01    foreach d ? ( T ? P ? N ) 
02       classify d by WSD system?(P, T-P) 
03       c(d, pos) ? the confidence score that d is  
04           predicted as positive defined in equation (2) 
05       c(d, neg) ? the confidence score that d is  
06           predicted as negative defined in equation (2) 
07       c(d, psdNeg) =  c(d, neg)  - c(d, pos)    
08                       (the confidence score that d is  
09                         predicted as pseudo negative)               
10        PN ? d ? ( T ? P ? N ) |  s(d) = neg ?  
11                                                  c(d, psdNeg)  ??} 
12                        (PN is pseudo negative dataset ) 
13     end 
Figure 2: Selection of pseudo negative examples 
3 Experimental Results 
3.1 Data and Condition of Experiments 
We select several example data sets from Japa-
nese blog data crawled from Web. Table 1 shows 
the ambiguous words and each ambiguous senses. 
Word Positive sense Other ambiguous senses 
Wega product name 
(TV) 
Las Vegas, football team 
name, nickname, star, horse 
race, Baccarat glass, atelier, 
wine, game, music 
Loft store name attic room, angle of golf 
club face, club with live 
music,  movie 
Honda personal name 
(football player) 
Personal names (actress, 
artists, other football play-
ers, etc.) hardware store, car 
company name 
Tsubaki product name 
(shampoo) 
flower name, kimono, horse 
race, camellia ingredient, 
shop name 
 Table 1: Selected examples for evaluation 
Table 2 shows the ambiguous words, the num-
ber of its senses, the number of its data instances, 
the number of feature, and the percentage of 
positive sense instances for each data set. 
Assigning the correct labels of data instances is 
done by one person and 48.5% of all the labels 
are checked by another person. The percentage 
of agreement between 2 persons for the assigned 
labels is 99.0%. The average time of assigning 
labels is 35 minutes per 100 instances. 
Selected instances for evaluation are randomly 
divided 10% test set and 90% training set. Table 
3 shows the each full text search query and the 
number of initial positive examples and the per-
centage of it in the training data set. 
word No. of 
senses
No. of  
instances
No. of  
features 
Percentage of  
positive sense
Wega 11 5,372 164,617 31.1%
Loft 5 1,582   38,491 39.4%
Honda 25 2,100   65,687 21.2%
Tsubaki 6 2,022   47,629 40.2%
Table 2: Selected examples for evaluation 
word Full text query for initial 
positive examples 
No. of positive 
examples (percent-
age in trainig set)  
Wega Wega  AND TV 316  (6.5%) 
Loft Loft AND (Grocery OR-
Stationery) 
64  (4.5%) 
Honda Honda AND Keisuke 86 (4.6%) 
Tsubaki Tsubaki AND Shiseido 380 (20.9%) 
Table 3: Initial positive examples 
The threshold value?in figure 2 is set to em-
pirically optimized value 50. Dependency on 
threshold value ? will be discussed in 3.3. 
3.2 Comparison Results 
Figure 3 shows the average WSD accuracy of 
the following 6 approaches. 
 
Figure 3: Average active learning process  
B-clustering is a standard unsupervised WSD, a 
clustering using naive Bayes classifier learned 
with two cluster numbers via EM algorithm. The 
given number of the clusters are two, negative 
and positive datasets.  
  M-clustering is a variant of b-clustering where 
the given number of clusters are each number of 
ambiguous word senses in table 2. 
Human labeling, abbreviated as human, is an 
active learning approach starting with human 
labeled negative examples. The number of hu-
56
58
60
62
64
66
68
70
72
0 10 20 30 40 50 60 70 80 90 100
75
77
79
81
83
85
87
89
91
human
with-EM
without-EM
random
m-clustering
b-clustering
63
man labeled negative examples in initial training 
data is the same as that of positive examples in 
figure 3. Human labeling is considered to be the 
upper accuracy in the variants of selecting 
pseudo negative examples.  
Random sampling with EM, abbreviated as 
with-EM, is the variant approach where dmin  in 
line 26 of figure 1 is randomly selected without 
using confidence score.  
Uncertainty sampling without EM (Takayama 
et al 2009), abbreviated as without-EM, is a vari-
ant approach where ?EM (P,  N+Np, T-N-P) in 
line 18 of figure 1 is replaced by ?(P, N+Np).  
Uncertainty Sampling with EM, abbreviated as un-
certain, is a proposed method described in figure 1. 
The accuracy of the proposed approach with-
EM is gradually increasing according to the per-
centage of added hand labeled examples. 
The initial accuracy of with-EM, which means 
the accuracy with no hand labeled negative ex-
amples, is the best score 81.4% except for that of 
human. The initial WSD accuracy of with-EM is 
23.4 and 4.2 percentage points higher than those 
of b-clustering (58.0%) and m-clustering 
(77.2%), respectively. This result shows that the 
proposed selecting method of pseudo negative 
examples is effective.  
The initial WSD accuracy of with-EM is 1.3 
percentage points higher than that of without-EM 
(80.1%). This result suggests semi-supervised 
learning using unlabeled examples is effective.  
The accuracies of with-EM, random and with-
out-EM are gradually increasing according to the 
percentage of added hand labeled examples and 
catch up that of human and converge at 30 per-
centage added points. This result suggests that 
our proposed approach can reduce the labor cost 
of assigning correct labels.  
The curve with-EM are slightly upper than the 
curve random at the initial stage of active learn-
ing. At 20 percentage added point, the accuracy 
with-EM is 87.0 %, 1.1 percentage points higher 
than that of random (85.9%). This result suggests 
that the effectiveness of proposed uncertainty 
sampling method is not remarkable depending on 
the word distribution of target data.  
There is really not much difference between the 
curve with-EM and without-EM. As a classifies 
to use the score for sampling examples in adapta-
tion iterations, it is indifferent whether with-EM 
or without-EM.  
Larger evaluation is the future issue to confirm 
if the above results could be generalized beyond 
the above four examples used as proper nouns. 
3.3 Dependency on Threshold Value ? 
Figure 4 shows the average WSD accuracies of 
with-EM at 0, 25, 50 and 75 as the values of ?.  
The each curve represents our proposed algorithm 
with threshold value ? in the parenthesis.  The 
accuracy in the case of ? = 75 is higher than that 
of? = 50 over 20 percentage data added point. 
This result suggests that as the number of hand 
labeled negative examples increasing, ? should 
be gradually decreasing, that is, the number of 
pseudo negative examples should be decreasing. 
Because, if sufficient number of hand labeled 
negative examples exist, a classifier does not need 
pseudo negative examples. The control of?
depending on the number of hand labeled examples 
during active learning iterations is a future issue. 
76
78
80
82
84
86
88
90
92
0 10 20 30 40 50 60 70 80 90 100
?=   0.0 
?= 25.0
?= 50.0 
?= 75.0 
 
Figure 4: Dependency of threshold value ? 
References  
Chan, Y. S. and Ng, H. T. 2007. Domain Adaptation 
with Active Learning for Word Sense Disambigua-
tion. Proc. of ACL 2007, 49-56. 
Chen, J., Schein, A., Ungar, L., and Palmer, M. 2006. 
An Empirical Study of the Behavior of Active 
Learning for Word Sense Disambiguation, Proc. of 
the main conference on Human Language Tech-
nology Conference of the North American Chapter 
of ACL, pp. 120-127. 
McCallum, A. and Nigam, K. 1998. Employing EM 
and Pool-Based Active Learning for Text Classifi-
cation. Proceedings of the Fifteenth international 
Conference on Machine Learning, 350-358. 
Nigam, K., McCallum, A., Thrun, S., and Mitchell, T. 
2000. Text Classification from Labeled and Unla-
beled Documents using EM, Machine Learning, 39, 
103-134.  
Takayama, Y., Imamura, M., Kaji N., Toyoda, M. and 
Kitsuregawa, M. 2009. Active Learning with 
Pseudo Negative Examples for Word Sense Dis-
ambiguation in Web Mining (in Japanese), Journal 
of IPSJ (in printing). 
64
Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010), pages 1245?1253,
Beijing, August 2010
Kernel Slicing: Scalable Online Training with Conjunctive Features
Naoki Yoshinaga
Institute of Industrial Science,
the University of Tokyo
ynaga@tkl.iis.u-tokyo.ac.jp
Masaru Kitsuregawa
Institute of Industrial Science,
the University of Tokyo
kitsure@tkl.iis.u-tokyo.ac.jp
Abstract
This paper proposes an efficient online
method that trains a classifier with many
conjunctive features. We employ kernel
computation called kernel slicing, which
explicitly considers conjunctions among
frequent features in computing the poly-
nomial kernel, to combine the merits of
linear and kernel-based training. To im-
prove the scalability of this training, we
reuse the temporal margins of partial fea-
ture vectors and terminate unnecessary
margin computations. Experiments on de-
pendency parsing and hyponymy-relation
extraction demonstrated that our method
could train a classifier orders of magni-
tude faster than kernel-based online learn-
ing, while retaining its space efficiency.
1 Introduction
The past twenty years have witnessed a growing
use of machine-learning classifiers in the field of
NLP. Since the classification target of complex
NLP tasks (e.g., dependency parsing and relation
extraction) consists of more than one constituent
(e.g., a head and a dependent in dependency pars-
ing), we need to consider conjunctive features,
i.e., conjunctions of primitive features that fo-
cus on the particular clues of each constituent, to
achieve a high degree of accuracy in those tasks.
Training with conjunctive features involves a
space-time trade-off in the way conjunctive fea-
tures are handled. Linear models, such as log-
linear models, explicitly estimate the weights of
conjunctive features, and training thus requires a
great deal of memory when we take higher-order
conjunctive features into consideration. Kernel-
based models such as support vector machines, on
the other hand, ensure space efficiency by using
the kernel trick to implicitly consider conjunctive
features. However, training takes quadratic time
in the number of examples, even with online algo-
rithms such as the (kernel) perceptron (Freund and
Schapire, 1999), and we cannot fully exploit am-
ple ?labeled? data obtained with semi-supervised
algorithms (Ando and Zhang, 2005; Bellare et al,
2007; Liang et al, 2008; Daume? III, 2008).
We aim at resolving this dilemma in train-
ing with conjunctive features, and propose online
learning that combines the time efficiency of lin-
ear training and the space efficiency of kernel-
based training. Following the work by Goldberg
and Elhadad (2008), we explicitly take conjunc-
tive features into account that frequently appear in
the training data, and implicitly consider the other
conjunctive features by using the polynomial ker-
nel. We then improve the scalability of this train-
ing by a method called kernel slicing, which al-
lows us to reuse the temporal margins of partial
feature vectors and to terminate computations that
do not contribute to parameter updates.
We evaluate our method in two NLP tasks: de-
pendency parsing and hyponymy-relation extrac-
tion. We demonstrate that our method is orders of
magnitude faster than kernel-based online learn-
ing while retaining its space efficiency.
The remainder of this paper is organized as fol-
lows. Section 2 introduces preliminaries and no-
tations. Section 3 proposes our training method.
Section 4 evaluates the proposed method. Sec-
tion 5 discusses related studies. Section 6 con-
cludes this paper and addresses future work.
1245
Algorithm 1 BASE LEARNER: KERNEL PA-I
INPUT: T = {(x, y)t}|T |t=1, k : Rn ? Rn 7? R, C ? R+OUTPUT: (S|T |,?|T |)
1: initialize: S0 ? ?, ?0 ? ?
2: for t = 1 to |T | do
3: receive example (x, y)t : x ? Rn, y ? {?1,+1}
4: compute margin: mt(x) =
?
si?St?1
?ik(si,x)
5: if `t = max {0, 1? ymt(x)} > 0 then
6: ?t ? min
?
C, `t?x?2
ff
7: ?t ? ?t?1 ? {?ty}, St ? St?1 ? {x}
8: else
9: ?t ? ?t?1, St ? St?1
10: end if
11: end for
12: return (S|T |,?|T |)
2 Preliminaries
This section first introduces a passive-aggressive
algorithm (Crammer et al, 2006), which we use
as a base learner. We then explain fast methods of
computing the polynomial kernel.
Each example x in a classification problem is
represented by a feature vector whose element xj
is a value of a feature function, fj ? F . Here, we
assume a binary feature function, fj(x) ? {0, 1},
which returns one if particular context data appear
in the example. We say that feature fj is active in
example x when xj = fj(x) = 1. We denote a
binary feature vector, x, as a set of active features
x = {fj | fj ? F , fj(x) = 1} for brevity; fj ? x
means that fj is active in x, and |x| represents the
number of active features in x.
2.1 Kernel Passive-Aggressive Algorithm
A passive-aggressive algorithm (PA) (Crammer et
al., 2006) represents online learning that updates
parameters for given labeled example (x, y)t ?
T in each round t. We assume a binary label,
y ? {?1,+1}, here for clarity. Algorithm 1
is a variant of PA (PA-I) that incorporates a ker-
nel function, k. In round t, PA-I first computes
a (signed) margin mt(x) of x by using the ker-
nel function with support set St?1 and coefficients
?t?1 (Line 4). PA-I then suffers a hinge-loss,
`t = max {0, 1? ymt(x)} (Line 5). If `t > 0,
PA-I adds x to St?1 (Line 7). Hyperparameter C
controls the aggressiveness of parameter updates.
The kernel function computes a dot product in
RH space without mapping x ? Rn to ?(x) ?
RH (k(x,x?) = ?(x)T?(x?)). We can implic-
itly consider (weighted) d or less order conjunc-
tions of primitive features by using polynomial
kernel function kd(s,x) = (sTx + 1)d. For ex-
ample, given support vector s = (s1, s2)T and
input example x = (x1, x2)T, the second-order
polynomial kernel returns k2(s,x) = (s1x1 +
s2x2 +1)2 = 1+3s1x1 +3s2x2 +2s1x1s2x2 (?
si, xi ? {0, 1}). This function thus implies map-
ping ?2(x) = (1,
?
3x1,
?
3x2,
?
2x1x2)T.
Although online learning is generally efficient,
the kernel spoils its efficiency (Dekel et al, 2008).
This is because the kernel evaluation (Line 4)
takes O(|St?1||x|) time and |St?1| increases as
training continues. The learner thus takes the most
amount of time in this margin computation.
2.2 Kernel Computation for Classification
This section explains fast, exact methods of com-
puting the polynomial kernel, which are meant to
test the trained model, (S,?), and involve sub-
stantial computational cost in preparation.
2.2.1 Kernel Inverted
Kudo and Matsumoto (2003) proposed polyno-
mial kernel inverted (PKI), which builds inverted
indices h(fj) ? {s | s ? S, fj ? s} from each
feature fj to support vector s ? S to only con-
sider support vector s relevant to given x such
that sTx 6= 0. The time complexity of PKI is
O(B ? |x| + |S|) where B ? 1|x|
?
fj?x |h(fj)|,
which is smaller than O(|S||x|) if x has many
rare features fj such that |h(fj)|  |S|.
To the best of our knowledge, this is the only
exact method that has been used to speed up mar-
gin computation in the context of kernel-based on-
line learning (Okanohara and Tsujii, 2007).
2.2.2 Kernel Expansion
Isozaki and Kazawa (2002) and Kudo and Mat-
sumoto (2003) proposed kernel expansion, which
explicitly maps both support set S and given ex-
ample x ? Rn into RH by mapping ?d imposed
by kd:
m(x) =
(
?
si?S
?i?d(si)
)T
?d(x) =
?
fi?xd
wi,
1246
where xd ? {0, 1}H is a binary feature vector
in which xdi = 1 for (?d(x))i 6= 0, and w is a
weight vector in the expanded feature space, Fd.
The weight vector w is computed from S and ?:
w =
?
si?S
?i
d?
k=0
ckdIk(sdi ), (1)
where ckd is a squared coefficient of k-th order con-
junctive features for d-th order polynomial kernel
(e.g., c02 = 1, c12 = 3, and c22 = 2)1 and Ik(sdi ) is
sdi ? {0, 1}H whose dimensions other than those
of k-th order conjunctive features are set to zero.
The time complexity of kernel expansion is
O(|xd|) where |xd| = ?dk=0
(|x|
k
)
? |x|d, which
can be smaller than O(|S||x|) in usual NLP tasks
(|x|  |S| and d ? 4).
2.2.3 Kernel Splitting
Since kernel expansion demands a huge mem-
ory volume to store the weight vector, w, in RH
(H =?dk=0
(|F|
k
)), Goldberg and Elhadad (2008)
only explicitly considered conjunctions among
features fC ? FC that commonly appear in sup-
port set S, and handled the other conjunctive fea-
tures relevant to rare features fR ? F \ FC by
using the polynomial kernel:
m(x) = m(x?) + m(x)?m(x?)
=
?
fi?x?d
w?i +
?
si?SR
?ik?d(si,x, x?), (2)
where x? is x whose dimensions of rare features
are set to zero, w? is a weight vector computed
with Eq. 1 for FdC , and k?d(s,x, x?) is defined as:
k?d(s,x, x?) ? kd(s,x)? kd(s, x?).
We can space-efficiently compute the first term
of Eq. 2 since |w?|  |w|, while we can
quickly compute the second term of Eq. 2 since
k?d(si,x, x?) = 0 when sTi x = sTi x?; we only
need to consider a small subset of the support set,
SR =
?
fR?x\x? h(fR), that has at least one of therare features, fR, appearing in x\x? (|SR|  |S|).
Counting the number of features examined, the
time complexity of Eq. 2 is O(|x?d|+ |SR||x?|).
1Following Lemma 1 in Kudo and Matsumoto (2003),
ckd =
?d
l=k
`d
l
? `?k
m=0(?1)k?m ?ml
` k
m
??.
3 Algorithm
This section first describes the way kernel splitting
is integrated into PA-I (Section 3.1). We then pro-
pose kernel slicing (Section 3.2), which enables
us to reuse the temporal margins computed in the
past rounds (Section 3.2.1) and to skip unneces-
sary margin computations (Section 3.2.2).
In what follows, we use PA-I as a base learner.
Note that an analogous argument can be applied
to other perceptron-like online learners with the
additive weight update (Line 7 in Algorithm 1).
3.1 Base Learner with Kernel Splitting
A problem in integrating kernel splitting into the
base learner presented in Algorithm 1 is how to
determine FC , features among which we explic-
itly consider conjunctions, without knowing the
final support set, S|T |. We heuristically solve
this by ranking feature f according to their fre-
quency in the training data and by using the top-
N frequent features in the training data as FC
(= {f | f ? F , RANK(f) ? N}).2 Since S|T |
is a subset of the examples, this approximates the
selection from S|T |. We empirically demonstrate
the validity of this approach in the experiments.
We then useFC to construct a base learner with
kernel splitting; we replace the kernel computa-
tion (Line 4 in Algorithm 1) with Eq. 2 where
(S,?) = (St?1,?t?1). To compute mt(x?) by
using kernel expansion, we need to additionally
maintain the weight vector w? for the conjunctions
of common features that appear in St?1.
The additive parameter update of PA-I enables
us to keep w? to correspond to (St?1,?t?1).
When we add x to support set St?1 (Line 7 in
Algorithm 1), we also update w? with Eq. 1:
w? ? w? + ?ty
d?
k=0
ckdIk(x?d).
Following (Kudo and Matsumoto, 2003), we
use a trie (hereafter, weight trie) to maintain con-
junctive features. Each edge in the weight trie is
labeled with a primitive feature, while each path
2The overhead of counting features is negligible com-
pared to the total training time. If we want to run the learner
in a purely online manner, we can alternatively choose first
N features that appear in the processed examples as FC .
1247
represents a conjunctive feature that combines all
the primitive features on the path. The weights
of conjunctive features are retrieved by travers-
ing nodes in the trie. We carry out an analogous
traversal in updating the parameters of conjunc-
tive features, while registering a new conjunctive
feature by adding an edge to the trie.
The base learner with kernel splitting combines
the virtues of linear training and kernel-based
training. It reduces to linear training when we in-
crease N to |F|, while it reduces to kernel-based
training when we decrease N to 0. The output
is support set S|T | and coefficients ?|T | (option-
ally, w?), to which the efficient classification tech-
niques discussed in Section 2.2 and the one pro-
posed by Yoshinaga and Kitsuregawa (2009) can
be applied.
Note on weight trie construction The time and
space efficiency of this learner strongly depends
on the way the weight trie is constructed. We
need to address two practical issues that greatly
affect efficiency. First, we traverse the trie from
the rarest feature that constitutes a conjunctive
feature. This rare-to-frequent mining helps us to
avoid enumerating higher-order conjunctive fea-
tures that have not been registered in the trie, when
computing margin. Second, we use RANK(f)
encoded into a dlog128 RANK(f)e-byte string by
using variable-byte coding (Williams and Zobel,
1999) as f ?s representation in the trie. This en-
coding reduces the trie size, since features with
small RANK(f) will appear frequently in the trie.
3.2 Base Learner with Kernel Slicing
Although a base learner with kernel splitting can
enjoy the merits of linear and kernel-based train-
ing, it can simultaneously suffer from their demer-
its. Because the training takes polynomial time
in the number of common features in x (|x?d| =?d
k=0
(|x?|
k
)
? |x?|d) at each round, we need to set
N to a smaller value when we take higher-order
conjunctive features into consideration. However,
since the margin computation takes linear time in
the number of support vectors |SR| relevant to rare
features fR ? F\FC , we need to setN to a larger
value when we handle a larger number of training
examples. The training thereby slows down when
we train a classifier with high-order conjunctive
features and a large number of training examples.
We then attempt to improve the scalability of
the training by exploiting a characteristic of la-
beled data in NLP. Because examples in NLP tasks
are likely to be redundant (Yoshinaga and Kitsure-
gawa, 2009), the learner computes margins of ex-
amples that have many features in common. If we
can reuse the ?temporal? margins of partial feature
vectors computed in past rounds, this will speed
up the computation of margins.
We propose kernel slicing, which generalizes
kernel splitting in a purely feature-wise manner
and enables us to reuse the temporal partial mar-
gins. Starting from the most frequent feature f1 in
x (f1 = argminf?x RANK(f)), we incrementally
compute mt(x) by accumulating a partial mar-
gin, mjt (x) ? mt(xj)?mt(xj?1), when we add
the j-th frequent feature fj in x:
mt(x) = m0t +
|x|?
j=1
mjt (x), (3)
where m0t =
?
si?St?1 ?ikd(si,?) =
?
i ?i, and
xj has the j most frequent features in x (x0 = ?,
xj =
?j?1
k=0{argminf?x\xk RANK(f)}).
Partial margin mjt (x) can be computed by us-
ing the polynomial kernel:
mjt (x) =
?
si?St?1
?ik?d(si,xj ,xj?1), (4)
or by using kernel expansion:
mjt (x) =
?
fi?xdj \xdj?1
w?i. (5)
Kernel splitting is a special case of kernel slicing,
which uses Eq. 5 for fj ? FC and Eq. 4 for fj ?
F \ FC .
3.2.1 Reuse of Temporal Partial Margins
We can speed up both Eqs. 4 and 5 by reusing
a temporal partial margin, ?jt? = mjt?(x) that hadbeen computed in past round t?(< t):
mjt (x) = ?
j
t? +
?
si?Sj
?ik?d(si,xj ,xj?1), (6)
where Sj = {s | s ? St?1 \ St??1, fj ? s}.
1248
Algorithm 2 KERNEL SLICING
INPUT: x ? 2F , St?1, ?t?1, FC ? F , ? : 2F 7? N? R
OUTPUT: mt(x)
1: initialize: x0 ? ?, j ? 1, mt(x)? m0t
2: repeat
3: xj ? xj?1 unionsq {argminf?x\xj?1 RANK(f)}
4: retrieve partial margin: (t?, ?jt?)? ?(xj)5: if fj ? F \ FC or Eq. 7 is true then
6: compute mjt(x) using Eq. 6 with ?jt?7: else
8: compute mjt(x) using Eq. 59: end if
10: update partial margin: ?(xj)? (t,mjt(x))
11: mt(x)? mt(x) + mjt(x)12: until xj 6= x
13: return mt(x)
Eq. 6 is faster than Eq. 4,3 and can even be
faster than Eq. 5.4 When RANK(fj) is high, xj ap-
pears frequently in the training examples and |Sj |
becomes small since t? will be close to t. When
RANK(fj) is low, xj rarely appears in the training
examples but we can still expect |Sj | to be small
since the number of support vectors in St?1\St??1
that have rare feature fj will be small.
To compute Eq. 3, we now have the choice to
choose Eq. 5 or 6 for fj ? FC . Counting the
number of features to be examined in computing
mjt (x), we have the following criteria to deter-
mine whether we can use Eq. 6 instead of Eq. 5:
1 + |Sj ||xj?1| ? |xdj \ xdj?1| =
d?
k=1
(j ? 1
k ? 1
)
,
where the left- and right-hand sides indicate the
number of features examined in Eq. 6 for the for-
mer and Eq. 5 for the latter. Expanding the right-
hand side for d = 2, 3 and dividing both sides with
|xj?1| = j ? 1, we have:
|Sj | ?
{
1 (d = 2)
j
2 (d = 3)
. (7)
If this condition is met after retrieving the tem-
poral partial margin, ?jt? , we can compute partial
margin mjt (x) with Eq. 6. This analysis reveals
3When a margin of xj has not been computed, we regard
t? = 0 and ?jt? = 0, which reduces Eq. 6 to Eq. 4.4We associate partial margins with partial feature se-
quences whose features are sorted by frequent-to-rare order,
and store them in a trie (partial margin trie). This enables us
to retrieve partial margin ?jt? for given xj in O(1) time.
that we can expect little speed-up for the second-
order polynomial kernel; we will only use Eq. 6
with third or higher-order polynomial kernel.
Algorithm 2 summarizes the margin computa-
tion with kernel slicing. It processes each feature
fj ? x in frequent-to-rare order, and accumulates
partial margin mjt (x) to have mt(x). Intuitively
speaking, when the algorithm uses the partial mar-
gin, it only considers support vectors on each fea-
ture that have been added since the last evaluation
of the partial feature vector, to avoid the repetition
in kernel evaluation as much as possible.
3.2.2 Termination of Margin Computation
Kernel slicing enables another optimization that
exploits a characteristic of online learning. Be-
cause we need an exact margin, mt(x), only when
hinge-loss `t = 1?ymt(x) is positive, we can fin-
ish margin computation as soon as we find that the
lower-bound of ymt(x) is larger than one.
When ymt(x) is larger than one after pro-
cessing feature fj in Eq. 3, we quickly examine
whether this will hold even after we process the
remaining features. We can compute a possible
range of partial margin mkt (x) with Eq. 4, hav-
ing the upper- and lower-bounds, k??d and k??d, of
k?d(si,xk,xk?1) (= kd(si,xk)? kd(si,xk?1)):
mkt (x) ? k??d
?
si?S+k
?i + k??d
?
si?S?k
?i (8)
mkt (x) ? k??d
?
si?S+k
?i + k??d
?
si?S?k
?i, (9)
where S+k = {si | si ? St?1, fk ? si, ?i > 0},
S?k = {si | si ? St?1, fk ? si, ?i < 0}, k??d =
(k+1)d? kd and k??d = 2d? 1 (? 0 ? sTi xk?1 ?
|xk?1| = k ? 1, sTi xk = sTi xk?1 + 1 for all
si ? S+k ? S?k ).We accumulate Eqs. 8 and 9 from rare to fre-
quent features, and use the intermediate results
to estimate the possible range of mt(x) before
Line 3 in Algorithm 2. If the lower bound of
ymt(x) turns out to be larger than one, we ter-
minate the computation of mt(x).
As training continues, the model becomes dis-
criminative and given x is likely to have a larger
margin. The impact of this termination will in-
crease as the amount of training data expands.
1249
4 Evaluation
We evaluated the proposed method in two NLP
tasks: dependency parsing (Sassano, 2004) and
hyponymy-relation extraction (Sumida et al,
2008). We used labeled data included in open-
source softwares to promote the reproducibility of
our results.5 All the experiments were conducted
on a server with an Intel R? XeonTM 3.2 GHz CPU.
We used a double-array trie (Aoe, 1989; Yata et
al., 2009) as an implementation of the weight trie
and the partial margin trie.
4.1 Task Descriptions
Japanese Dependency Parsing A parser inputs
a sentence segmented by a bunsetsu (base phrase
in Japanese), and selects a particular pair of bun-
setsus (dependent and head candidates); the clas-
sifier then outputs label y = +1 (dependent) or
?1 (independent) for the pair. The features con-
sist of the surface form, POS, POS-subcategory
and the inflection form of each bunsetsu, and sur-
rounding contexts such as the positional distance,
punctuations and brackets. See (Yoshinaga and
Kitsuregawa, 2009) for details on the features.
Hyponymy-Relation Extraction A hyponymy
relation extractor (Sumida et al, 2008) first ex-
tracts a pair of entities from hierarchical listing
structures in Wikipedia articles (hypernym and
hyponym candidates); a classifier then outputs la-
bel y = +1 (correct) or ?1 (incorrect) for the
pair. The features include a surface form, mor-
phemes, POS and the listing type for each entity,
and surrounding contexts such as the hierarchical
distance between the entities. See (Sumida et al,
2008) for details on the features.
4.2 Settings
Table 1 summarizes the training data for the two
tasks. The examples for the Japanese dependency
parsing task were generated for a transition-based
parser (Sassano, 2004) from a standard data set.6
We used the dependency accuracy of the parser
5The labeled data for dependency parsing is available
from: http://www.tkl.iis.u-tokyo.ac.jp/?ynaga/pecco/, and
the labeled data for hyponymy-relation extraction is avail-
able from: http://nlpwww.nict.go.jp/hyponymy/.
6Kyoto Text Corpus Version 4.0:
http://nlp.kuee.kyoto-u.ac.jp/nl-resource/corpus-e.html.
DATA SET DEP REL
|T | 296,776 201,664
(y = +1) 150,064 152,199
(y = ?1) 146,712 49,465
Ave. of |x| 27.6 15.4
Ave. of |x2| 396.1 136.9
Ave. of |x3| 3558.3 798.7
|F| 64,493 306,036
|F2| 3,093,768 6,688,886
|F3| 58,361,669 64,249,234
Table 1: Training data for dependency parsing
(DEP) and hyponymy-relation extraction (REL).
as model accuracy in this task. In the hyponymy-
relation extraction task, we randomly chosen two
sets of 10,000 examples from the labeled data for
development and testing, and used the remaining
examples for training. Note that the number of
active features, |Fd|, dramatically grows when we
consider higher-order conjunctive features.
We compared the proposed method, PA-I SL
(Algorithm 1 with Algorithm 2), to PA-I KER-
NEL (Algorithm 1 with PKI; Okanohara and Tsu-
jii (2007)), PA-I KE (Algorithm 1 with kernel ex-
pansion; viz., kernel splitting with N = |F|),
SVM (batch training of support vector machines),7
and `1-LLM (stochastic gradient descent training
of the `1-regularized log-linear model: Tsuruoka
et al (2009)). We refer to PA-I SL that does not
reuse temporal partial margins as PA-I SL?. To
demonstrate the impact of conjunctive features on
model accuracy, we also trained PA-I without con-
junctive features. The number of iterations in PA-I
was set to 20, and the parameters of PA-I were av-
eraged in an efficient manner (Daume? III, 2006).
We explicitly considered conjunctions among top-
N (N = 125 ? 2n;n ? 0) features in PA-I SL
and PA-I SL?. The hyperparameters were tuned to
maximize accuracy on the development set.
4.3 Results
Tables 2 and 3 list the experimental results for
the two tasks (due to space limitations, Tables 2
and 3 list PA-I SL with parameter N that achieved
the fastest speed). The accuracy of the models
trained with the proposed method was better than
`1-LLMs and was comparable to SVMs. The infe-
7http://chasen.org/?taku/software/TinySVM/
1250
METHOD d ACC. TIME MEMORY
PA-I 1 88.56% 3s 55MB
`1-LLM 2 90.55% 340s 1656MB
SVM 2 90.76% 29863s 245MB
PA-I KERNEL 2 90.68% 8361s 84MB
PA-I KE 2 90.67% 41s 155MB
PA-I SL?N=4000 2 90.71% 33s 95MB
`1-LLM 3 90.76% 4057s 21,499MB
SVM 3 90.93% 25912s 243MB
PA-I KERNEL 3 90.90% 8704s 83MB
PA-I KE 3 90.90% 465s 993MB
PA-I SLN=250 3 90.89% 262s 175MB
Table 2: Training time for classifiers used in de-
pendency parsing task.
0
300
600
900
1200
1500
102 103 104 105
Tra
inin
gti
m
e[s
]
N: # of expanded primitive features
PA-I SP
PA-I SL?
PA-I SL
Figure 1: Training time for PA-I variants as a func-
tion of the number of expanded primitive features
in dependency parsing task (d = 3).
rior accuracy of PA-I (d = 1) confirmed the ne-
cessity of conjunctive features in these tasks. The
minor difference among the model accuracy of the
three PA-I variants was due to rounding errors.
PA-I SL was the fastest of the training meth-
ods with the same feature set, and its space effi-
ciency was comparable to the kernel-based learn-
ers. PA-I SL could reduce the memory footprint
from 993MB8 to 175MB for d = 3 in the depen-
dency parsing task, while speeding up training.
Although linear training (`1-LLM and PA-I KE)
dramatically slowed down when we took higher-
order conjunctive features into account, kernel
slicing alleviated deterioration in speed. Espe-
cially in the hyponymy-relation extraction task,
PA-I SL took almost the same time regardless of
the order of conjunctive features.
8`1-LLM took much more memory than PA-I KE mainly
because `1-LLM expands conjunctive features in the exam-
ples prior to training, while PA-I KE expands conjunctive fea-
tures in each example on the fly during training. Interested
readers may refer to (Chang et al, 2010) for this issue.
METHOD d ACC. TIME MEMORY
PA-I 1 91.75% 2s 28MB
`1-LLM 2 92.67% 136s 1683MB
SVM 2 92.85% 12306s 139MB
PA-I KERNEL 2 92.91% 1251s 54MB
PA-I KE 2 92.96% 27s 143MB
PA-I SL?N=8000 2 92.88% 17s 77MB
`1-LLM 3 92.86% 779s 14,089MB
SVM 3 93.09% 17354s 140MB
PA-I KERNEL 3 93.14% 1074s 49MB
PA-I KE 3 93.11% 103s 751MB
PA-I SLN=125 3 93.05% 17s 131MB
Table 3: Training time for classifiers used in
hyponymy-relation extraction task.
0
30
60
90
120
150
102 103 104 105 106
Tra
inin
gti
m
e[s
]
N: # of expanded primitive features
PA-I SP
PA-I SL?
PA-I SL
Figure 2: Training time for PA-I variants as a func-
tion of the number of expanded primitive features
in hyponymy-relation extraction task (d = 3).
Figures 1 and 2 plot the trade-off between the
number of expanded primitive features and train-
ing time with PA-I variants (d = 3) in the two
tasks. Here, PA-I SP is PA-I with kernel slicing
without the techniques described in Sections 3.2.1
and 3.2.2, viz., kernel splitting. The early termi-
nation of margin computation reduces the train-
ing time when N is large. The reuse of temporal
margins makes the training time stable regardless
of parameter N . This suggests a simple, effec-
tive strategy for calibrating N ; we start the train-
ing with N = |F|, and when the learner reaches
the allowed memory size, we shrink N to N/2
by pruning sub-trees rooted by rarer features with
RANK(f) > N/2 in the weight trie.
Figures 3 and 4 plot training time with PA-I
variants (d = 3) for the two tasks as a function
of the training data size. PA-I SP inherited the de-
merit of PA-I KERNEL which takes quadratic time
in the number of examples, while PA-I SL took al-
most linear time in the number of examples.
1251
0
100
200
300
400
500
600
0 50000 100000 150000 200000 250000 300000
Tra
inin
gti
m
e[s
]
|T |: # of training examples
PA-I KERNEL
PA-I SPN=250
PA-I SLN=250
Figure 3: Training time for PA-I variants as a func-
tion of the number of training examples in depen-
dency parsing task (d = 3).
5 Related Work
There are several methods that learn ?simpler?
models with fewer variables (features or support
vectors), to ensure scalability in training.
Researchers have employed feature selection
to assure space-efficiency in linear training. Wu
et al (2007) used frequent-pattern mining to se-
lect effective conjunctive features prior to train-
ing. Okanohara and Tsujii (2009) revised graft-
ing for `1-LLM (Perkins et al, 2003) to prune use-
less conjunctive features during training. Iwakura
and Okamoto (2008) proposed a boosting-based
method that repeats the learning of rules repre-
sented by feature conjunctions. These methods,
however, require us to tune the hyperparameter to
trade model accuracy and the number of conjunc-
tive features (memory footprint and training time);
note that an accurate model may need many con-
junctive features (in the hyponymy-relation ex-
traction task, `1-LLM needed 15,828,122 features
to obtain the best accuracy, 92.86%). Our method,
on the other hand, takes all conjunctive features
into consideration regardless of parameter N .
Dekel et al (2008) and Cavallanti et al (2007)
improved the scalability of the (kernel) percep-
tron, by exploiting redundancy in the training data
to bound the size of the support set to given thresh-
old B (? |St|). However, Orabona et al (2009)
reported that the models trained with these meth-
ods were just as accurate as a naive method that
ceases training when |St| reaches the same thresh-
old, B. They then proposed budget online learn-
ing based on PA-I, and it reduced the size of the
support set to a tenth with a tolerable loss of accu-
0
50
100
150
200
0 50000 100000 150000 200000
Tra
inin
gti
m
e[s
]
|T |: # of training examples
PA-I KERNEL
PA-I SPN=125PA-I SLN=125
Figure 4: Training time for PA-I variants as a
function of the number of training examples in
hyponymy-relation extraction task (d = 3).
racy. Their method, however, requiresO(|St?1|2)
time in updating the parameters in round t, which
disables efficient training. We have proposed an
orthogonal approach that exploits the data redun-
dancy in evaluating the kernel to train the same
model as the base learner.
6 Conclusion
In this paper, we proposed online learning with
kernel slicing, aiming at resolving the space-time
trade-off in training a classifier with many con-
junctive features. The kernel slicing generalizes
kernel splitting (Goldberg and Elhadad, 2008) in
a purely feature-wise manner, to truly combine the
merits of linear and kernel-based training. To im-
prove the scalability of the training with redundant
data in NLP, we reuse the temporal partial margins
computed in past rounds and terminate unneces-
sary margin computations. Experiments on de-
pendency parsing and hyponymy-relation extrac-
tion demonstrated that our method could train a
classifier orders of magnitude faster than kernel-
based learners, while retaining its space efficiency.
We will evaluate our method with ample la-
beled data obtained by the semi-supervised meth-
ods. The implementation of the proposed algo-
rithm for kernel-based online learners is available
from http://www.tkl.iis.u-tokyo.ac.jp/?ynaga/.
Acknowledgment We thank Susumu Yata for
providing us practical lessons on the double-array
trie, and thank Yoshimasa Tsuruoka for making
his `1-LLM code available to us. We are also in-
debted to Nobuhiro Kaji and the anonymous re-
viewers for their valuable comments.
1252
References
Ando, Rie Kubota and Tong Zhang. 2005. A frame-
work for learning predictive structures from multi-
ple tasks and unlabeled data. Journal of Machine
Learning Research, 6:1817?1853.
Aoe, Jun?ichi. 1989. An efficient digital search al-
gorithm by using a double-array structure. IEEE
Transactions on Software Engineering, 15(9):1066?
1077.
Bellare, Kedar, Partha Pratim Talukdar, Giridhar Ku-
maran, Fernando Pereira, Mark Liberman, Andrew
McCallum, and Mark Dredze. 2007. Lightly-
supervised attribute extraction. In Proc. NIPS 2007
Workshop on Machine Learning for Web Search.
Cavallanti, Giovanni, Nicolo` Cesa-Bianchi, and Clau-
dio Gentile. 2007. Tracking the best hyperplane
with a simple budget perceptron. Machine Learn-
ing, 69(2-3):143?167.
Chang, Yin-Wen, Cho-Jui Hsieh, Kai-Wei Chang,
Michael Ringgaard, and Chih-Jen Lin. 2010. Train-
ing and testing low-degree polynomial data map-
pings via linear SVM. Journal of Machine Learning
Research, 11:1471?1490.
Crammer, Koby, Ofer Dekel, Joseph Keshet, Shai
Shalev-Shwartz, and Yoram Singer. 2006. Online
passive-aggressive algorithms. Journal of Machine
Learning Research, 7:551?585.
Daume? III, Hal. 2006. Practical Structured Learn-
ing Techniques for Natural Language Processing.
Ph.D. thesis, University of Southern California.
Daume? III, Hal. 2008. Cross-task knowledge-
constrained self training. In Proc. EMNLP 2008,
pages 680?688.
Dekel, Ofer, Shai Shalev-Shwartz, and Yoram Singer.
2008. The forgetron: A kernel-based percep-
tron on a budget. SIAM Journal on Computing,
37(5):1342?1372.
Freund, Yoav and Robert E. Schapire. 1999. Large
margin classification using the perceptron algo-
rithm. Machine Learning, 37(3):277?296.
Goldberg, Yoav and Michael Elhadad. 2008.
splitSVM: fast, space-efficient, non-heuristic, poly-
nomial kernel computation for NLP applications. In
Proc. ACL-08: HLT, Short Papers, pages 237?240.
Isozaki, Hideki and Hideto Kazawa. 2002. Efficient
support vector classifiers for named entity recogni-
tion. In Proc. COLING 2002, pages 1?7.
Iwakura, Tomoya and Seishi Okamoto. 2008. A fast
boosting-based learner for feature-rich tagging and
chunking. In Proc. CoNLL 2008, pages 17?24.
Kudo, Taku and Yuji Matsumoto. 2003. Fast methods
for kernel-based text analysis. In Proc. ACL 2003,
pages 24?31.
Liang, Percy, Hal Daume? III, and Dan Klein. 2008.
Structure compilation: trading structure for fea-
tures. In Proc. ICML 2008, pages 592?599.
Okanohara, Daisuke and Jun?ichi Tsujii. 2007. A dis-
criminative language model with pseudo-negative
samples. In Proc. ACL 2007, pages 73?80.
Okanohara, Daisuke and Jun?ichi Tsujii. 2009. Learn-
ing combination features with L1 regularization. In
Proc. NAACL HLT 2009, Short Papers, pages 97?
100.
Orabona, Francesco, Joseph Keshet, and Barbara Ca-
puto. 2009. Bounded kernel-based online learning.
Journal of Machine Learning Research, 10:2643?
2666.
Perkins, Simon, Kevin Lacker, and James Theiler.
2003. Grafting: fast, incremental feature selection
by gradient descent in function space. Journal of
Machine Learning Research, 3:1333?1356.
Sassano, Manabu. 2004. Linear-time dependency
analysis for Japanese. In Proc. COLING 2004,
pages 8?14.
Sumida, Asuka, Naoki Yoshinaga, and Kentaro Tori-
sawa. 2008. Boosting precision and recall of hy-
ponymy relation acquisition from hierarchical lay-
outs in Wikipedia. In Proc. LREC 2008, pages
2462?2469.
Tsuruoka, Yoshimasa, Jun?ichi Tsujii, and Sophia
Ananiadou. 2009. Stochastic gradient descent
training for L1-regularized log-linear models with
cumulative penalty. In Proc. ACL-IJCNLP 2009,
pages 477?485.
Williams, Hugh E. and Justin Zobel. 1999. Compress-
ing integers for fast file access. The Computer Jour-
nal, 42(3):193?201.
Wu, Yu-Chieh, Jie-Chi Yang, and Yue-Shi Lee. 2007.
An approximate approach for training polynomial
kernel SVMs in linear time. In Proc. ACL 2007, In-
teractive Poster and Demonstration Sessions, pages
65?68.
Yata, Susumu, Masahiro Tamura, Kazuhiro Morita,
Masao Fuketa, and Jun?ichi Aoe. 2009. Sequential
insertions and performance evaluations for double-
arrays. In Proc. the 71st National Convention of
IPSJ, pages 1263?1264. (In Japanese).
Yoshinaga, Naoki and Masaru Kitsuregawa. 2009.
Polynomial to linear: efficient classification with
conjunctive features. In Proc. EMNLP 2009, pages
1542?1551.
1253
Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers,
pages 1091?1102, Dublin, Ireland, August 23-29 2014.
A Self-adaptive Classifier for Efficient Text-stream Processing
Naoki Yoshinaga
Institute of Industrial Science,
the University of Tokyo,
Meguro-ku, Tokyo 153-8505, Japan
ynaga@tkl.iis.u-tokyo.ac.jp
Masaru Kitsuregawa
Institute of Industrial Science,
the University of Tokyo,
Meguro-ku, Tokyo 153-8505, Japan
and
National Institute of Informatics,
Chiyoda-ku, Tokyo 101-8430, Japan
kitsure@tkl.iis.u-tokyo.ac.jp
Abstract
A self-adaptive classifier for efficient text-stream processing is proposed. The proposed classifier
adaptively speeds up its classification while processing a given text stream for various NLP tasks.
The key idea behind the classifier is to reuse results for past classification problems to solve
forthcoming classification problems. A set of classification problems commonly seen in a text
stream is stored to reuse the classification results, while the set size is controlled by removing the
least-frequently-used or least-recently-used classification problems. Experimental results with
Twitter streams confirmed that the proposed classifier applied to a state-of-the-art base-phrase
chunker and dependency parser speeds up its classification by factors of 3.2 and 5.7, respectively.
1 Introduction
The rapid growth in popularity of microblogs (e.g., Twitter) is enabling more and more people to in-
stantly publish their experiences or thoughts any time they want from mobile devices. Since information
in text posted by hundreds of millions of those people covers every space and time in the real world,
analyzing such a text stream tells us what is going on in the real world and is therefore beneficial for re-
ducing damage caused by natural disasters (Sakaki et al., 2010; Neubig et al., 2011a; Varga et al., 2013),
monitoring political sentiment (Tumasjan et al., 2010) and disease epidemics (Aramaki et al., 2011), and
predicting stock market (Gilbert and Karahalios, 2010) and criminal incident (Wang et al., 2012).
Text-stream processing, however, faces a new challenge; namely, the quality (content) and quantity
(volume of flow) changes dramatically, reflecting a change in the real world. Current studies on pro-
cessing microblogs have focused mainly on the difference between the quality of microblogs (or spoken
languages) and news articles (or written languages) (Gimpel et al., 2011; Foster et al., 2011; Ritter et al.,
2011; Han and Baldwin, 2011), and they have not addressed the issue of so-called ?bursts? that increase
the volume of text. Although it is desirable to use NLP analyzers with the highest possible accuracy for
processing a text stream, high accuracy is generally attained by costly structured classification or classi-
fication with rich features, typically conjunctive features (Liang et al., 2008). It is therefore inevitable to
trade accuracy for speed by using only a small fraction of features to assure real-time processing.
In this study, the aforementioned text-quantity issue concerning processing a text stream is addressed,
and a self-adaptive algorithm that speeds up an NLP classifier trained with many conjunctive features (or
with a polynomial kernel) for a given text stream is proposed and validated. Since globally-observable
events such as natural disasters or sports events incur a rapid growth in the number of posts (Twitter, Inc.,
2011), a text stream is expected to contain similar contents concerning these events when the volume of
flow in a text stream increases. To adaptively speed up the NLP classifier, the proposed algorithm thus
enumerates common classification problems from seen classification problems and keeps their classifi-
cation results as partial results for use in solving forthcoming classification problems.
The proposed classifier was evaluated by applying it to streams of classification problems generated
during the processing of the Twitter streams on the day of the 2011 Great East Japan Earthquake and on
another day in March 2012 using a state-of-the-art base-phrase chunker (Sassano, 2008) and dependency
parser (Sassano, 2004), and the obtained results confirm the effectiveness of the proposed algorithm.
This work is licenced under a Creative Commons Attribution 4.0 International License. Page numbers and proceedings footer
are added by the organizers. License details: http://creativecommons.org/licenses/by/4.0/
1091
2 Related work
A sentence is the processing unit used for fundamental NLP tasks such as word segmentation, part-of-
speech tagging, phrase chunking, syntactic parsing, and semantic role labeling. Most efficient algorithms
solving these tasks thus aim at speeding up the processing based on this unit (Kaji et al., 2010; Koo et
al., 2010; Rush and Petrov, 2012), and few studies have attempted to speed up the processing of a given
text (a set of sentences) as a whole. In the following, reported algorithms that adaptively speed up NLP
analyzers for a given text are introduced.
A method of speeding up a classifier trained with many conjunctive features by using precomputed
results for common classification problems was proposed by Yoshinaga and Kitsuregawa (2009; 2012).
It solves classification problems that commonly appear in the processing of a large amount of text in
advance and stores the results in a trie, so that they can be reused as partial results for solving new classi-
fication problems. This method was reported to achieve speed-up factors of 3.3 and 10.6 for base-phrase
chunking and dependency parsing, respectively. An analogous algorithm for integer linear program (ILP)
used to solve structured classification was proposed by Srikumar, Kundu, and Roth (2012; 2013). The
algorithm was reported to achieve speed-up factors of 2.6 and 1.6 for semantic role labeling and entity-
relation extraction, respectively. Although these two algorithms can be applied to various NLP tasks that
can be solved by using a linear classifier or an ILP solver, how effective they are for processing a text
stream is not clear.
A method of feature sharing for beam-search incremental parsers was proposed by Goldberg et al.
(2013). Motivated by the observation that beam parsers solve similar classification problems in different
parts of the beam, this method reuses partial results computed in the previous beam items. It reportedly
achieved a speed-up factor of 1.2 for arc-standard and arc-eager dependency parsers. The key differences
between the method proposed in this study and their feature-sharing method are twofold. First, the feature
sharing in Goldberg et al. (2013) is performed in a token-wise manner in the sense that a key to retrieve a
cached result is represented by a bag of tokens that invoke features, which manner prevents fine-grained
caching. Second, the feature sharing is dynamically performed during parsing, but the cached results are
cleared after processing each sentence.
An adaptive pruning method for fast HPSG parsing was proposed by van Noord (2009). This method
preprocesses a large amount of text by using a target parser to collect derivation steps that are unlikely to
contribute to the best parse, and it speeds up the parser by filtering out those unpromising derivation steps.
Although this method was reported to attain a speed-up factor of four while keeping parsing accuracy, it
needs to be tuned to trade parsing accuracy and speed for each domain. It is difficult to derive the true
potential of their method in regard to processing a text stream whose domain shifts from time to time.
It has been demonstrated by Wachsmuth et al. (2011) that tuning a pipeline schedule of an information
extraction (IE) system improves the efficiency of the system. Furthermore, the self-supervised learning
algorithm devised by Wachsmuth et al. (2013) predicts the processing time for each possible pipeline
schedule of an IE system, and the prediction is used to adaptively change the pipeline schedule for a given
text stream. This method and the proposed method for speeding up an NLP classifier are complementary,
and a combination of both methods is expected to synergistically speed up various NLP-systems.
In this study, based on the classifier proposed by Yoshinaga and Kitsuregawa (2009), a self-adaptive
classifier that enumerates common classification problems from a given text stream and reuses their
results is proposed. As a result, the proposed classifier adaptively speeds up the classification of forth-
coming classification problems.
3 Preliminaries
As the basis of the proposed classifier, the previously-presented classifier that uses results of common
classification problems (Yoshinaga and Kitsuregawa, 2009) is described as follows. This base classifier
targets a linear classifier trained with many conjunctive features (including one converted from a classifier
trained with polynomial kernel (Isozaki and Kazawa, 2002)) that are widely used for many NLP tasks.
Although this classifier (and also the one proposed in this paper) can handle a multi-class classification
problem, a binary classification problem is assumed here for brevity.
1092
A binary classifier such as a perceptron and a support vector machine determines label y ? {+1,?1}
of input classification problem x by using the following equation (from which the bias term is omitted
for brevity):
m(x;?,w) =w
T
?(x) =
?
i
w
i
?
i
(x) (1)
y =
{
+1 (m(x;?,w) ? 0)
?1 (m(x;?,w) < 0).
(2)
Here, ?
i
is a feature function, w
i
is a weight for ?
i
obtained as a result of training, and m(x;?,w) is a
margin between x and the separating hyperplane.
In most NLP tasks, feature functions are mostly indicator (or binary) functions that typically represent
particular linguistic constraints. Here, feature functions are assumed to be indicator functions that return
{0, 1}, and margin m(x;?,w) is represented by the following equation:
m(x;?,w) =
?
i
w
i
?
i
(x) =
?
i,?
i
(x)=1
w
i
. (3)
Feature function ?
i
is hereafter referred to as feature ?
i
; when ?
i
(x) = 1 for given x, x is said to
?include? feature ?
i
or feature ?
i
is ?active? in x (denoted as ?
i
? x). Having the number of active
features, |?(x)| ? |{?
i
| ?
i
? x}|, Eq. 3 requires O(|?(x)|) when the weights for the active features
are summed up.
To speed up the summation in Eq. 3, classification results for some classification problems x
c
are
precomputed as M
x
c
? m(x
c
;?,w) in advance, and then these precomputed results are reused as
partial results for solving an input classification problem, x:
m(x;x
c
,?,w) =M
x
c
+
?
i,?
i
?x,?
i
/?x
c
w
i
(4)
where ??
j
? x
c
, ?
j
? x.
Note that for Eq. 4 to be computed faster than Eq. 3, M
x
c
must be retrieved in time less thanO(|?(x
c
)|).
It is actually possible when x
c
includes conjunctive feature ?
i,j
(x
c
) = ?
i
(x
c
)?
j
(x
c
). If it is necessary
to retrieve margin M
x
c
precomputed for x
c
including ?
i
, ?
j
, and ?
i,j
, it is necessary to check only
non-conjunctive (or primitive) features ?
i
and ?
j
, since ?
i,j
is active whenever ?
i
and ?
j
are active (so
checking ?
i,j
can be skipped). The second term of Eq. 4 sums up the weights of the remaining features
that are not included in x
c
but are included in x. For example, under the assumption that x includes
features ?
i
, ?
j
, ?
k
, ?
i,j
, ?
i,k
, and ?
j,k
and that margin M
x
c
has been obtained for x
c
(including ?
i
,
?
j
, and ?
i,j
), five features must be checked (two to retrieve M
x
c
and three to sum up the weights of
the remaining features ?
k
, ?
i,k
and ?
j,k
) by using Eq. 4. On the other hand, to compute m(x;?,w) by
Eq. 3, the weights for the six features must be checked.
To maximize the speed-up obtained by Eq. 4, reuse of margin M
x
c
of common classification problem
x
c
should minimize the number of remaining features included only in x. In other words, x
c
should
be as similar to x as possible (ideally, x
c
= x). It is not, however, realistic to precompute margin
M
x
? m(x;?,w) for every possible classification problem x since it requires O(2|??|) space where
|?
?
| is the number of primitive features (?? ? ?) and |??| is usually more than 10,000 in NLP tasks
due to lexical features. Yoshinaga and Kitsuregawa (2009) therefore preprocess a large amount of text
to enumerate possible classification problems, and select common classification problems, X
c
? 2
?
?
,
according to their probability and the reduction in the number of features to be checked by Eq 4.
Yoshinaga and Kitsuregawa (2009) then represent the common classification problems x
c
? X
c
by
sequences of active primitive feature indices, and store those feature (index) sequences as keys in a
prefix trie with precomputed margin M(x
c
) as their values. To reuse a margin of common classification
problem that is similar to input x in Eq. 4, features are ordered according to their frequency to form a
feature sequence of x
c
. A longest-prefix search for the trie thereby retrieves a common classification
problem similar to the input classification problem in linear time with respect to the number of primitive
features in x
c
, O(|?
?
(x
c
)|).
1093
Algorithm 1 A self-adaptive classifier for enumerating common classification problems
INPUT: x, ?, ?? ? ?,w ? R|?|, X
c
? 2
?
?
, k > 0
OUTPUT: m(x;?,w) ? R, X
c
1: INITIALIZE: x
c
s.t. ??(x
c
) = 0, M
x
c
? 0
2: repeat
3: xold
c
? x
c
4: ??
i
= argmax
?
?
i
?x,?
?
i
/?x
c
FREQ(??
i
) (extract a primitive feature according to its frequency)
5: ??
i
(x
c
)? 1 (construct a new common-classification problem)
6: if x
c
/? X
c
then
7: M
x
c
? m(x
c
;x
old
c
,?,w) (compute margin by using Eq. 4)
8: if |X
c
| = k then
9: X
c
? X
c
? {USELESS(X
c
)}
10: X
c
? X
c
? {x
c
}
11: until ??(x
c
) 6= ?
?
(x)
12: return m(x;?,w) = M
x
c
, X
c
4 Proposed method
The classifier described in Section 3 is extended so that it dynamically enumerates common classification
problems from a given text stream1 to adaptively speed up the classification. This classification ?speed
up? faces two challenges: which (partial) classification problems should be chosen to reuse their results
from a given stream of classification problems, and how to efficiently maintain the extracted common
classification problems. These two challenges are addressed in Sections 4.1 and 4.2, respectively.
4.1 Enumerating common classification problems dynamically from a text stream
Although Yoshinaga and Kitsuregawa (2009) select common classification problems according to their
probability, such statistics cannot be known before a target text stream is entirely seen. A set of common
classification problems was thus kept updated adaptively while processing a text stream; that is, classifi-
cation problems are added when they will be useful, while they are removed when they will be useless,
so that the number of common classification problems, |X
c
|, does not exceed a pre-defined threshold, k.
Algorithm 1 depicts the proposed self-adaptive classifier for enumerating common classification prob-
lems from an input classification problem, x. To incrementally construct common classification problem
x
c
(Line 4-5), the algorithm extracts the primitive features (??
i
) included in x one by one according to
their probability of appearing in the training data of the classifier. When the resulting x
c
is included in
the current set of common classification problems, X
c
, stored margin M
x
c
is reused. Otherwise, margin
M
x
c
= m(x
c
;x
old
c
,?,w) is computed by using Eq. 4 (Line 7), and x
c
is registered in X
c
as a new
common classification problem (Line 10).
An important issue is how to define function USELESS, which selects a common classification problem
that will not contribute to speeding up the forthcoming classification, when the number of common
classification problems, |X
c
|, reaches the pre-defined threshold k. To address this issue, the following
two policies (designed originally for CPU caching) are proposed and compared in terms of the efficiency
of the classifier in experiments:
Least Frequently Used (LFU) This policy counts frequency of common classification problems in a
seen text stream, and it maintains only the top-k common classification problems by removing the
least-common classification problem from X
c
:
USELESSLFU(Xc) = argmin
x
c
?X
c
FREQ(x
c
) (5)
1More precisely, a stream of classification problems generated during the analysis of a text stream.
1094
A space-saving algorithm, (Metwally et al., 2005), is used to efficiently count the approximated
frequency of k classification problems at most and to remove the common classification problem
rejected by the space-saving algorithm.
Least Recently Used (LRU) When the volume of flow in a text stream rapidly increases, it is likely to
relate to a burst of a certain topic. To exploit this characteristics, this policy preserves k common
classification problems whose results are most recently reused:
USELESSLRU(Xc) = argmin
x
c
?X
c
TIME(x
c
) (6)
Common classification problems are associated with the last timing when their results are reused,
and the least-recently-reused common classification problem is removed when |X
c
| = k. To realize
this policy, a circular linked-list of size k is used to maintain precomputed results, and the oldest
element is just overwritten while the corresponding classification problem is removed.
Fixed threshold k is used throughout the processing of a text stream, and its impact on classification
speed was evaluated by experiments.
Since Algorithm 1 naively constructs common classification problems using all the active primitive
features in input classification problem x, it might repeatedly add and remove classification problems
that include rare primitive features such as lexical features. This will incur serious overhead costs. To
avoid this situation, the margin computation is terminated as soon as it is determined that the remaining
computation does not change the sign of margin (namely, classification label y) of x.
When x and x
c
are given, lower- and upper-bounds of m(x;?,w) can be computed by accumulating
bounds of a partial margin computed by adding remaining active primitive features, {??
j
? x | ?
?
j
/? x
c
},
one by one to x
c
. It is assumed that primitive feature ??
i
is newly activated in x
c
and xold
c
refers to x
c
without ??
i
being activated. The partial margin, m(x
c
;?,w)?m(x
old
c
;?,w), is computed by summing
up the weights of primitive feature ??
i
and conjunctive features that are composed of ??
i
and one or more
primitive features ??
j
? x
old
c
. This partial margin is upper- and lower-bounded as follows:
m(x
c
;?,w)?m(x
old
c
;?,w)?max(w
min
i
|{?
j
? x
c
| ?
j
/? x
old
c
}|,W
?
i
) (7)
m(x
c
;?,w)?m(x
old
c
;?,w)?min(w
max
i
|{?
j
? x
c
| ?
j
/? x
old
c
}|,W
+
i
), (8)
where wmin
i
and wmax
i
refer to minimum and maximum weights among all the features regarding ??
i
,
while W+
i
and W?
i
refer to summations of all the features regarding ??
i
with positive and negative
weights, respectively; that is, this upper or lower-bound is computed by assuming all the features regard-
ing ??
i
to have a maximum or minimum weight (each bounded by W+
i
or W?
i
). Accumulating these
bounds for each remaining primitive feature makes it possible to obtain the bounds of m(x;?,w) and
thereby judge whether the sign of the margin can be changed by processing the remaining features.
4.2 Maintaining common classification problems with dynamic double-array trie
To maintain the enumerated common classification problems, a double-array trie (Aoe, 1989) is applied.
The trie associates common classification problem x
c
with unique index i(1 ? i ? k), which is fur-
ther associated with computed margin M
x
c
and frequency or access time as described in Section 4.1.
Although a double-array trie provides an extremely fast look-up, it had been considered that update
operation (adding a new key to a double-array trie) is slow. However, in a recent study (Yata et al.,
2009), the update speed of a double-array trie approaches that of a hash table. In the following section, a
double-array trie similar to that of Yata et al. (2009) is used to maintain common classification problems.
Efficient dynamic double-array trie with deletion
A double-array trie (Aoe, 1989) and an algorithm that allows a fast update (Yata et al., 2009) are briefly
introduced in the following. A double array is a data structure for a compact trie, which consists of two
one-dimensional arrays called BASE and CHECK. In a double-array trie, each trie node occupies one
element in BASE and CHECK, respectively.2 For each node, p, BASE stores the offset address of its child
2Although the original double-array (Aoe, 1989) realizes a minimal-prefix trie by using another array (called TAIL) to store
suffix nodes with only one child, TAIL is not adopted here since it is difficult to support space-efficient deletion with TAIL.
1095
nodes, so a child node takes the address c = BASE[p] XOR3 l when the node is traversed from p by label l.
For each node, c, CHECK stores the address of its parent node, p, and is used to confirm the validity of
the traversal by checking whether CHECK[c] = p is held after the node is reached by c = BASE[p] XOR l.
Adding a new node to a trie could cause a conflict, meaning that the newly added node could be
assigned to the address taken by an existing node in the trie. In such a case, it is necessary to collect all
the sibling nodes of either the newly added node or the existing node that took the conflicting address,
and then relocate either branching (with a lower number of child nodes) to empty addresses that are not
taken by other nodes in the trie. This relocation is time-consuming, and is the reason for the slow update.
To perform this relocation quickly, Yata et al. (2009) introduced two additional one-dimensional ar-
rays, called NLINK (node link) and BLOCK. For each node, NLINK stores the label needed to reach its
first child and the label needed to reach from its parent the sibling node next to the node. It thereby
makes it possible to quickly enumerate the sibling nodes for relocation. BLOCK stores information on
empty addresses within each 256 consecutive addresses called a block4 in BASE and CHECK. Each block
is classified into three types, called ?full,? ?closed? and ?open.? Full blocks have no empty addresses and
are excluded from the target of relocation. Closed blocks have only one empty address or have failed to
be relocated more times than a pre-specified threshold. Open blocks are other blocks, which have more
than one empty address. The efficient update of the trie is enabled by choosing appropriate blocks to
relocate a branching; a branching with one child node is relocated to a closed block, while a branching
with multiple child nodes is relocated to an open block.
The above-described double-array trie was modified to support a deletion operation, which simply
registers to each block empty addresses resulting from deletion. In consideration that a new key (common
classification problem) will be stored immediately after the deletion (Line 10 in Algorithm 1), the double-
array trie is not packed as in Yata et al. (2007) after a key is deleted.
Engineering a double-array trie to reduce trie size
To effectively maintain common classification problems in a trie, it is critical to reduce the number of
trie nodes accessed in look-up, update, and deletion operations. The number of trie nodes was therefore
reduced as much as possible by adopting a more compact representation of keys (common classification
problems) and by elaborating the way to store values for the keys in the double-array trie.
Gap-based key representation To compress representations of common classification problems (fea-
ture sequences) in the trie, frequency-based indices are allocated to primitive features (Yoshinaga and
Kitsuregawa, 2010). A gap representation (used to compress posting lists in information retrieval (Man-
ning et al., 2008, Chapter 5)) is used to encode feature sequences. Each feature index is replaced with a
gap from the preceding feature index (the first feature index is used as is). Each gap is then encoded by
variable-byte coding (Williams and Zobel, 1999) to obtain shorter representations of feature sequences.
A reduced double-array trie The standard implementation of a double-array trie stores an (integer)
index with a key at a child node (value node) traversed by a terminal symbol ?\0? (or an alphabet
not included in a key, e.g., ?#?) from the node reached after reading the entire key (Yoshinaga and
Kitsuregawa, 2009; Yasuhara et al., 2013). However, when a key is not a prefix to the other keys, the
value node has no sibling node, so a value can be directly embedded on the BASE of the node reached
after reading the entire key instead of the offset address of the child (value) node. All the value nodes for
the longest prefixes are thereby eliminated from the trie. The resulting double-array trie is referred to as
a reduced double-array trie.
These two tricks reduce the number of trie nodes (memory usage), and make the trie operations faster.
A reduced double-array trie is also used to compactly store the weights of conjunctive features, as de-
scribed in Yoshinaga and Kitsuregawa (2010). Interested readers may refer to cedar,5 open-source soft-
ware of a dynamic double-array trie, for further implementation details of the reduced double-array trie.
3The original double-array (Aoe, 1989) uses addition instead of XOR operation to obtain a child address.
4Note that the XOR operation guarantees that all the child nodes are located within a certain block i (assuming 1 byte (0-255)
for each label, l, child nodes of a node, p, are all located in addresses (256i ? c = BASE[p] XOR l < 256(i + 1)).
5http://www.tkl.iis.u-tokyo.ac.jp/
?
ynaga/cedar/
1096
base-phrase chunking dependency parsing
Number of features |?| 645,951 2,084,127
Number of primitive features |??| 11,509 27,063
Accuracy (partial) 99.01% 92.23%
Accuracy (complete) 94.16% 58.38%
Table 1: Model statistics for base-phrase chunking and dependency parsing.
5 Experiments
The proposed self-adaptive classifier was experimentally evaluated by applying it to streams of classifi-
cation problems. The streams of classification problems were generated by processing Twitter streams
using a state-of-the-art base-phrase chunker and dependency parser. All experiments were conducted
with an Intel R? CoreTM i7-3720QM 2.6-GHz CPU server with 16-GB main memory.
5.1 Setup
Since March 11, 2011 (the day of the Great East Japan Earthquake; ?3.11 earthquake? hereafter), Twitter
streams were crawled by using Twitter API.6 Tweets from famous Japanese users were crawled first.
Next, timelines of those users were obtained. Then, the set of users were repeatedly expanded by tracing
retweets and mentions in their timelines to collect as many tweets as possible. In the following experi-
ments, two sets of 24-hour Twitter streams from the crawled tweets were used. The first Twitter stream
was taken from the day of 3.11 earthquake (12:00 on Friday, March 11, 2011 to 12:00 on Saturday,
March 12, 2011), and the second one was taken from the second weekend in March, 2012 (12:00 on
Friday, March 9, 2012 to 12:00 on Saturday, March 10, 2012). The first Twitter stream is intended to
evaluate the classifier performance on days with a significant, continuous burst, while the second one is to
evaluate the performance on days without such a burst. No special events, other than a small earthquake
(02:25 on March 10), occurred from March 9 to 10, 2012. Because the input to base-phrase chunking
and dependency parsing is a sentence, each post was split by using punctuations as clues.
Although it might be better to evaluate the chunking and parsing speed with the proposed classifier
for a text stream, the classification speed was evaluated for streams of classification problems generated
in processing the Twitter streams by a deterministic base-phrase chunker (Sassano, 2008) and a shift-
reduce dependency parser (Sassano, 2004), which are implemented in J.DepP.12 Note that the chunker
and parser are known to spend most of the time for classification (Yoshinaga and Kitsuregawa, 2012),
and reducing the classification time leads to efficient processing of Twitter streams.
The base-phrase chunker processes each token in a sentence identified by a morphological analyzer,
MeCab,7 and judges whether the token is the beginning of a base-phrase chunk in Japanese (called a
bunsetsu8) or not. The shift-reduce dependency parser processes each chunk in the chunked sentences
and determines whether the head candidate chosen by the parser is correct head or not.
The classifiers for base-phrase chunking and dependency parsing were trained by using a variant of a
passive-aggressive algorithm (PA-I) (Crammer et al., 2006) with a standard split9 of the Kyoto-University
Text Corpus (Kawahara et al., 2002) Version 4.0.10 A third-order polynomial kernel was used to consider
combinations of up-to three primitive features. The features used for training the classifiers were identical
to those implemented in J.DepP. The polynomial kernel expanded (Kudo and Matsumoto, 2003) was
used to make the number of resulting conjunctive features tractable without harming the accuracy.
Table 1 lists the statistics of the models trained for chunking and parsing. In Table 1, ?accuracy
(partial)? is the ratio of chunks (or dependency arcs) correctly identified by the chunker (or the parser),
while ?accuracy (complete)? is the exact-match accuracy of complete chunks (or dependency arcs) in a
sentence. The accuracy of the resulting parser on the standard split was better than any published results
6https://dev.twitter.com/docs/api
7http:://mecab.sourceforge.net/
8A bunsetsu is a linguistic unit consisting of one or more content words followed by zero or more function words.
924,263, 4,833 and 9,284 sentences (234,685, 47,571 and 89,874 base phrases) for training, development, and testing.
10http://nlp.ist.i.kyoto-u.ac.jp/EN/index.php?Kyoto%20University%20Text%20Corpus
1097
March 11-12, 2011 March 9-10, 2012
Number of posts 9,291,767 6,360,392
Number of posts/s 108 74
Number of sentences 24,722,596 13,521,196
Number of classification problems (chunking) 220,490,401 109,452,133
Number of classification problems (parsing) 70,096,105 34,380,385
Table 2: Twitter stream used for evaluation.
0
1
2
3
4
5
6
1
2
:
0
0
1
4
:
0
0
1
6
:
0
0
1
8
:
0
0
2
0
:
0
0
2
2
:
0
0
0
0
:
0
0
0
2
:
0
0
0
4
:
0
0
0
6
:
0
0
0
8
:
0
0
1
0
:
0
0
1
2
:
0
0
R
e
l
a
t
i
v
e
#
p
o
s
t
s
/
m
i
n
.
time
March 11-12, 2011
March 9-10, 2012
(a) Change in number of posts per min.
0
1
2
3
4
5
6
1
2
:
0
0
1
4
:
0
0
1
6
:
0
0
1
8
:
0
0
2
0
:
0
0
2
2
:
0
0
0
0
:
0
0
0
2
:
0
0
0
4
:
0
0
0
6
:
0
0
0
8
:
0
0
1
0
:
0
0
1
2
:
0
0
R
e
l
a
t
i
v
e
#
p
o
s
t
s
/
m
i
n
.
time
March 11-12, 2011
March 9-10, 2012
(b) Change in number of classifications per min. (parsing)
Figure 1: Volume of flow of Twitter streams from March 11 to 12, 2011 and from March 9 to 10, 2012.
March 11-12, 2011 March 9-10, 2012
method space speed ratio space speed ratio
[MiB] [ms/sent.] [MiB] [ms/sent.]
baseline 12.01 0.0221 1.00 12.01 0.0188 1.00
Y&K ?09 30.46 0.0118 1.87 30.46 0.0112 1.69
proposed k = 216 18.05 0.0092 2.40 17.93 0.0098 1.93
(LFU) k = 220 90.70 0.0088 2.51 90.78 0.0089 2.12
k = 2
24 463.04 0.0081 2.73 473.60 0.0076 2.48
proposed k = 216 17.32 0.0086 2.57 17.32 0.0093 2.02
(LRU) k = 220 85.89 0.0077 2.88 86.09 0.0085 2.22
k = 2
24 399.17 0.0070 3.16 409.59 0.0068 2.76
Table 3: Experimental results obtained with the reduced double array trie: base phrase chunking.
for this dataset other than those reported for a parser based on ?stacking? (Iwatate, 2012).11
Table 2 lists the detail of the Twitter streams used for evaluating the proposed classifier. Figures 1(a)
and 1(b) show the change in the number of posts and classifications for parsing per minute, when the
average number of posts and classifications per minute before the 3.11 earthquake is counted as one,
respectively. The dataset shows a rapid growth in the number of posts after the 3.11 earthquake occurred
(14:46:18). This event also incurs a rapid growth in the number of classifications for parsing. Although
space limitations precluded the number of classifications for chunking, it had the same tendency as for
parsing. It should be noted that the official retweets (reposts) occupied 25.8% (2,394,025) and 8.5%
(542,726) of the entire posts from March 11 to 12, 2011 and from March 9 to 10, 2012, respectively.
5.2 Results
Tables 3 and 4 list the timings needed to solve the classification problems generated for each sentence by
processing the Twitter streams listed in Table 2 using the base-phrase chunker and the dependency parser,
respectively. In Table 3, ?baseline? refers to the classifier using Eq. 3, while ?Y&K ?09? refers to Yoshi-
naga and Kitsuregawa (2009) who used Eq. 4, and enumerates common classification problems from the
training corpus9 of the classifier in advance. To highlight the performance gap caused by the algorithmic
differences and make the memory consumptions comparable, the experiments were conducted using the
same implementation of the reduced double-array trie, described in Section 4.2, for all the methods. The
proposed classifier (with 65,536 (k = 216) common classification problems) achieved higher classifica-
11The best reported accuracy of a non-stacking parser is 91.96% (partial) and 57.44% (complete) for Kyoto-University Text
Corpus Version 3.0 (Iwatate et al., 2008), and is better than that achieved by the MST algorithm (McDonald et al., 2005).
1098
March 11-12, 2011 March 9-10, 2012
method space speed ratio space speed ratio
[MiB] [ms/sent.] [MiB] [ms/sent.]
baseline 31.50 0.1187 1.00 31.50 0.0979 1.00
Y&K ?09 99.91 0.0738 1.61 99.91 0.0651 1.51
proposed k = 216 43.21 0.0469 2.53 43.01 0.0542 1.81
(LFU) k = 220 113.40 0.0293 4.06 113.27 0.0399 2.45
k = 2
24 904.32 0.0222 5.35 905.62 0.0285 3.44
proposed k = 216 42.68 0.0497 2.39 42.66 0.0546 1.79
(LRU) k = 220 108.88 0.0283 4.20 108.94 0.0421 2.32
k = 2
24 840.85 0.0208 5.71 840.93 0.0280 3.50
Table 4: Experimental results obtained with the reduced double array trie: dependency parsing.
0
0.5
1.0
1.5
2.0
2
10
2
12
2
14
2
16
2
18
2
20
2
22
2
24
2
26
A
v
e
r
a
g
e
t
i
m
e
[
?
s
]
/
c
l
a
s
s
i
f
y
k
lru
lfu
Y&K ?09
(a) base-phrase chunking
0
5
10
15
20
25
30
2
10
2
12
2
14
2
16
2
18
2
20
2
22
2
24
2
26
A
v
e
r
a
g
e
t
i
m
e
[
?
s
]
/
c
l
a
s
s
i
f
y
k
lru
lfu
Y&K ?09
(b) dependency parsing
Figure 2: Average classification time per classification problem as a function of number of common
classification problems k (2011 tweet stream).
tion speed than that achieved by Y&K ?09 (with 943,864 (chunking) and 2,902,679 (parsing) common
classification problems). Although the speed up is evident for both tweet datasets, the speed-up is more
obvious in the case of the 2011 tweet stream. In the following experiments, in view of space limitations
and redundancy, the 2011 tweet stream was used; however, note that the same conclusions are drawn
from the results with the 2012 twitter stream.
Figure 2 shows the time needed for solving each classification problem for chunking and parsing of
the 2011 tweet stream when the threshold to the number of common classification problems k is varied
from 210 to 226, respectively. In both tasks, the proposed classifier with the LRU policy outperforms the
proposed classifier with the LFU policy when k was increased. This is not only because the LFU policy
has higher overheads than the LRU policy but also because the LFU policy selects useless classification
problems that include lexical features related to a burst in the past. The speed-up is saturated in the case
of base-phrase chunking at k = 222 (Figure 2(a)). This is because the proposed classifier often terminates
margin computation without seeing lexical features for base phrase chunking, so it rarely reuses results
of common classification problems including lexical features that are preserved when k is increased.
On the other hand in dependency parsing, the classifier relies on lexical features to resolve semantic
ambiguities, so it cannot terminate margin computation without seeing lexical features and thus exploits
common classification problems including lexical features.
Figure 3 shows the change in the time needed for solving classification problems generated from a
one-minute text stream for chunking and parsing of the 2011 tweet stream. The y-axis shows the relative
classification time, when the average classification time of the baseline method before the 3.11 earth-
quake is counted as one. The classification time of the baseline method and Yoshinaga and Kitsuregawa
(2009)?s method rapidly increased in response to the increase of the number of classification problems,
while the proposed classifier suppressed the increase in classification time. It is thus concluded that the
proposed classifier is more robust in terms of real-time processing for a text stream.
Finally, the contributions of the three tricks of the proposed classifier to the classification performance
for dependency parsing were evaluated. The three tricks are a gap-based key representation and a reduced
double-array trie (Section 4.1), as well as the early termination of margin computation (Section 4.1).
1099
01
2
3
4
5
6
1
2
:
0
0
1
4
:
0
0
1
6
:
0
0
1
8
:
0
0
2
0
:
0
0
2
2
:
0
0
0
0
:
0
0
0
2
:
0
0
0
4
:
0
0
0
6
:
0
0
0
8
:
0
0
1
0
:
0
0
1
2
:
0
0
R
e
l
a
t
i
v
e
t
i
m
e
/
m
i
n
.
p
o
s
t
s
time
baseline
Y&K ?09
lru (k = 2
16
)
lru (k = 2
20
)
lru (k = 2
24
)
(a) base-phrase chunking
0
1
2
3
4
5
6
1
2
:
0
0
1
4
:
0
0
1
6
:
0
0
1
8
:
0
0
2
0
:
0
0
2
2
:
0
0
0
0
:
0
0
0
2
:
0
0
0
4
:
0
0
0
6
:
0
0
0
8
:
0
0
1
0
:
0
0
1
2
:
0
0
R
e
l
a
t
i
v
e
t
i
m
e
/
m
i
n
.
p
o
s
t
s
time
baseline
Y&K ?09
lru (k = 2
16
)
lru (k = 2
20
)
lru (k = 2
24
)
(b) dependency parsing
Figure 3: Change in classification time of one-minute posts (2011 tweet stream).
plain (no tricks) + gap-based key + reduced double array + early termination
method space speed ratio space speed ratio space speed ratio space speed ratio
[MiB] [ms/sent.] [MiB] [ms/sent.] [MiB] [ms/sent.] [MiB] [ms/sent.]
baseline 39.88 0.1413 0.84 n/a n/a n/a 31.50 0.1187 1.00 38.21 0.0745 1.59
Y&K ?09 117.66 0.0922 1.29 110.52 0.0904 1.31 99.91 0.0738 1.61 106.61 0.0406 2.93
proposed k = 216 44.64 0.1037 1.14 44.50 0.1009 1.18 36.09 0.0845 1.41 42.68 0.0497 2.39
(LRU) k = 220 117.21 0.0590 2.01 114.57 0.0572 2.07 105.21 0.0492 2.41 108.88 0.0283 4.20
k = 2
24 969.48 0.0412 2.88 923.96 0.0398 2.98 897.70 0.0350 3.39 840.85 0.0208 5.71
Table 5: Contribution of each trick to classification performance; 2011 tweet dataset (underlined numbers
are quoted from Table 4).
Table 5 lists the classification times per sentence in the case of dependency parsing, when each trick
is cumulatively applied to plain classifiers without all the tricks. Classification is significantly speeded
up by early termination of the margin computation and the reduced double-array trie. These tricks also
contribute to speeding up the baseline method and Yoshinaga and Kitsuregawa (2009)?s method.
6 Conclusion
Aiming to efficiently process a real-world text stream (such as a Twitter stream) in real-time, a self-
adaptive classifier that becomes faster for a given text stream is proposed. It enumerates common clas-
sification problems that are generated during the processing of a text stream, and reuses the results of
those classification problems as partial results for solving forthcoming classification problems.
The proposed classifier was evaluated by applying it to the streams of classification problems generated
by processing two sets of Twitter streams on the day of the 2011 Great East Japan Earthquake and the
second weekend in March 2012 using a state-of-the-art base-phrase chunker and dependency parser. The
proposed classifier speeds up the classification by factors of 3.2 (chunking) and 5.7 (parsing), which are
significant factors in regard to processing a massive text stream.
It is planned to evaluate the classifier on other NLP tasks. A linear classifier with conjunctive fea-
tures is widely used for NLP tasks such as word segmentation, part-of-speech tagging (Neubig et al.,
2011b), and dependency parsing (Nivre and McDonald, 2008). Even for NLP tasks in which structured
classification is effective (e.g., named entity recognition), structure compilation (Liang et al., 2008) (or
?uptraining? (Petrov et al., 2010)) gives state-of-the-art accuracy when a linear classifier with many
conjunctive features is used. The proposed classifier is expected to be applied to a range of NLP tasks.
All the codes have been available for the research community as open-source software, including
pecco (a self-adaptive classifier)12 and J.DepP (a base-phrase chunker and dependency parser).13
Acknowledgments
This work was supported by the Research and Development on Real World Big Data Integration and
Analysis program of the Ministry of Education, Culture, Sports, Science, and Technology, JAPAN.
12http://www.tkl.iis.u-tokyo.ac.jp/
?
ynaga/pecco/
13http://www.tkl.iis.u-tokyo.ac.jp/
?
ynaga/jdepp/
1100
References
Jun?ichi Aoe. 1989. An efficient digital search algorithm by using a double-array structure. IEEE Transactions on
Software Engineering, 15(9):1066?1077.
Eiji Aramaki, Sachiko Maskawa, and Mizuki Morita. 2011. Twitter catches the flu: Detecting influenza epidemics
using Twitter. In Proceedings of EMNLP, pages 1568?1576.
Koby Crammer, Ofer Dekel, Joseph Keshet, Shai Shalev-Shwartz, and Yoram Singer. 2006. Online passive-
aggressive algorithms. JMLR, 7:551?585, March.
Jennifer Foster, ?Ozlem C?etinoglu, Joachim Wagner, Joseph Le Roux, Stephen Hogan, Joakim Nivre, Deirdre
Hogan, and Josef van Genabith. 2011. #hardtoparse: POS tagging and parsing the Twitterverse. In Proceedings
of the AAAI-11 Workshop on Analyzing Microtext.
Eric Gilbert and Karrie Karahalios. 2010. Widespread worry and the stock market. In Proceedings of ICWSM,
pages 58?65.
Kevin Gimpel, Nathan Schneider, Brendan O?Connor, Dipanjan Das, Daniel Mills, Jacob Eisenstein, Michael
Heilman, Dani Yogatama, Jeffrey Flanigan, and Noah A. Smith. 2011. Part-of-speech tagging for Twitter:
Annotation, features, and experiments. In Proceedings of ACL-HLT, pages 42?47.
Yoav Goldberg, Kai Zhao, and Liang Huang. 2013. Efficient implementation of beam-search incremental parsers.
In Proceedings of ACL, Short Papers, pages 628?633.
Bo Han and Timothy Baldwin. 2011. Lexical normalisation of short text messages: Makn sens a #twitter. In
Proceedings of ACL-HLT, pages 368?378.
Hideki Isozaki and Hideto Kazawa. 2002. Efficient support vector classifiers for named entity recognition. In
Proceedings of COLING, pages 1?7.
Masakazu Iwatate, Masayuki Asahara, and Yuji Matsumoto. 2008. Japanese dependency parsing using a tourna-
ment model. In Proceedings of COLING, pages 361?368.
Masakazu Iwatate. 2012. Development of Pairwise Comparison-based Japanese Dependency Parsers and Appli-
cation to Corpus Annotation. Ph.D. thesis, Graduate School of Information Science, Nara Institute of Science
and Technology.
Nobuhiro Kaji, Yasuhiro Fujiwara, Naoki Yoshinaga, and Masaru Kitsuregawa. 2010. Efficient staggered decod-
ing for sequence labeling. In Proceedings of ACL, pages 485?494.
Daisuke Kawahara, Sadao Kurohashi, and Ko?iti Hasida. 2002. Construction of a Japanese relevance-tagged
corpus. In Proceedings of LREC, pages 2008?2013.
Terry Koo, Alexander M. Rush, Michael Collins, Tommi Jaakkola, and David Sontag. 2010. Dual decomposition
for parsing with non-projective head automata. In Proceedings of EMNLP, pages 1288?1298.
Taku Kudo and Yuji Matsumoto. 2003. Fast methods for kernel-based text analysis. In Proceedings of ACL, pages
24?31.
Gourab Kundu, Vivek Srikumar, and Dan Roth. 2013. Margin-based decomposed amortized inference. In Pro-
ceedings of EMNLP, pages 905?913.
Percy Liang, Hal Daume? III, and Dan Klein. 2008. Structure compilation: trading structure for features. In
Proceedings of ICML, pages 592?599.
Christopher D. Manning, Prabhakar Raghavan, and Hinrich Schu?tze. 2008. Introduction to Information Retrieval.
Cambridge University Press.
Twitter, Inc. 2011. Twitter?s 2011 year in review. http://yearinreview.twitter.com/en/tps.
html.
Ryan McDonald, Koby Crammer, and Fernando Pereira. 2005. Online large-margin training of dependency
parsers. In Proceedings of ACL, pages 523?530.
Ahmed Metwally, Divyakant Agrawal, and Amr El Abbadi. 2005. Efficient computation of frequent and top-k
elements in data streams. In Proceedings of ICDT, pages 398?412.
1101
Graham Neubig, Yuichiroh Matsubayashi, Masato Hagiwara, and Koji Murakami. 2011a. Safety information
mining - what can NLP do in a disaster -. In Proceedings of IJCNLP, pages 965?973.
Graham Neubig, Yosuke Nakata, and Shinsuke Mori. 2011b. Pointwise prediction for robust, adaptable japanese
morphological analysis. In Proceedings of ACL, pages 529?533.
Joakim Nivre and Ryan McDonald. 2008. Integrating graph-based and transition-based dependency parsers. In
Proceedings of ACL-HLT, pages 950?958.
Slav Petrov, Pi-Chuan Chang,Michael Ringgaard, and Hiyan Alshawi. 2010. Uptraining for accurate deterministic
question parsing. In Proceedings of EMNLP, pages 705?713.
Alan Ritter, Sam Clark, Mausam, and Oren Etzioni. 2011. Named entity recognition in tweets: An experimental
study. In Proceedings of EMNLP, pages 1524?1534.
Alexander Rush and Slav Petrov. 2012. Vine pruning for efficient multi-pass dependency parsing. In Proceedings
of NAACL-HLT, pages 498?507.
Takeshi Sakaki, Makoto Okazaki, and Yutaka Matsuo. 2010. Earthquake shakes Twitter users: Real-time event
detection by social sensors. In Proceedings of WWW, pages 851?860.
Manabu Sassano. 2004. Linear-time dependency analysis for Japanese. In Proceedings of COLING, pages 8?14.
Manabu Sassano. 2008. An experimental comparison of the voted perceptron and support vector machines in
Japanese analysis tasks. In Proceedings of IJCNLP, pages 829?834.
Vivek Srikumar, Gourab Kundu, and Dan Roth. 2012. On amortizing inference cost for structured prediction. In
Proceedings of EMNLP-CoNLL, pages 1114?1124.
Andranik Tumasjan, Timm O. Sprenger, Philipp G. Sandner, and Isabell M. Welpe. 2010. Predicting elections
with Twitter: What 140 characters reveal about political sentiment. In Proceedings of ICWSM, pages 178?185.
Gertjan van Noord. 2009. Learning efficient parsing. In Proceeding of EACL, pages 817?825.
Istva?n Varga, Motoki Sano, Kentaro Torisawa, Chikara Hashimoto, Kiyonori Ohtake, Takao Kawai, Jong-Hoon
Oh, and Stijn De Saeger. 2013. Aid is out there: Looking for help from tweets during a large scale disaster. In
Proceedings of ACL, pages 1619?1629.
Henning Wachsmuth, Benno Stein, and Gregor Engels. 2011. Constructing efficient information extraction
pipelines. In Proceedings of CIKM, pages 2237?2240.
Henning Wachsmuth, Benno Stein, and Gregor Engels. 2013. Learning efficient information extraction on hetero-
geneous texts. In Proceedings of IJCNLP, pages 534?542.
Xiaofeng Wang, Matthew S. Gerber, and Donald E. Brown. 2012. Automatic crime prediction using events
extracted from Twitter posts. In Proceedings of SBP, pages 231?238.
Hugh E. Williams and Justin Zobel. 1999. Compressing integers for fast file access. The Computer Journal,
42(3):193?201.
Makoto Yasuhara, Toru Tanaka, Jun ya Norimatsu, and Mikio Yamamoto. 2013. An efficient language model
using double-array structures. In Proceedings of EMNLP, pages 222?232.
Susumu Yata, Masaki Oono, Kazuhiro Morita, Masao Fuketa, and Jun ichi Aoe. 2007. An efficient deletion
method for a minimal prefix double array. Journal of Software: Practice and Experience, 37(5):523?534.
Susumu Yata, Masahiro Tamura, Kazuhiro Morita, Masao Fuketa, and Jun?ichi Aoe. 2009. Sequential insertions
and performance evaluations for double-arrays. In Proceedings of the 71st National Convention of IPSJ, pages
1263?1264. (In Japanese).
Naoki Yoshinaga and Masaru Kitsuregawa. 2009. Polynomial to linear: Efficient classification with conjunctive
features. In Proceedings of EMNLP, pages 1542?1551.
Naoki Yoshinaga and Masaru Kitsuregawa. 2010. Kernel slicing: Scalable online training with conjunctive fea-
tures. In Proceedings of COLING, pages 1245?1253.
Naoki Yoshinaga and Masaru Kitsuregawa. 2012. Efficient classification with conjunctive features. Journal of
Information Processing, 20(1):228?227.
1102
Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 959?969,
Edinburgh, Scotland, UK, July 27?31, 2011. c?2011 Association for Computational Linguistics
Splitting Noun Compounds via Monolingual and Bilingual Paraphrasing:
A Study on Japanese Katakana Words
Nobuhiro Kaji
Institute of Industrial Science
University of Tokyo, Tokyo, Japan
kaji@tkl.iis.u-tokyo.ac.jp
Masaru Kitsuregawa
Institute of Industrial Science
University of Tokyo, Tokyo, Japan
kitsure@tkl.iis.u-tokyo.ac.jp
Abstract
Word boundaries within noun compounds are
not marked by white spaces in a number of
languages, unlike in English, and it is benefi-
cial for various NLP applications to split such
noun compounds. In the case of Japanese,
noun compounds made up of katakana words
(i.e., transliterated foreign words) are par-
ticularly difficult to split, because katakana
words are highly productive and are often out-
of-vocabulary. To overcome this difficulty,
we propose using monolingual and bilingual
paraphrases of katakana noun compounds for
identifying word boundaries. Experiments
demonstrated that splitting accuracy is sub-
stantially improved by extracting such para-
phrases from unlabeled textual data, the Web
in our case, and then using that information for
constructing splitting models.
1 Introduction
1.1 Japanese katakana words and noun
compound splitting
Borrowing is a major type of word formation
in Japanese, and numerous foreign words (proper
names or neologisms etc.) are continuously being
imported from other languages (Tsujimura, 2006).
Most borrowed words in modern Japanese are
transliterations1 from English and they are referred
to as katakana words because transliterated foreign
words are primarily spelled by using katakana char-
acters in the Japanese writing system.2 Compound-
1Some researchers use the term transcription rather than
transliteration (Breen, 2009). Our terminology is based on stud-
ies on machine transliteration (Knight and Graehl, 1998).
2The Japanese writing system has four character types: hi-
ragana, katakana, kanji, and Latin alphabet.
ing is another type of word formation that is com-
mon in Japanese (Tsujimura, 2006). In particu-
lar, noun compounds are frequently produced by
merging two or more nouns together. These two
types of word formation yield a significant amount
of katakana noun compounds, making Japanese a
highly productive language.
In Japanese as well as some European and Asian
languages (e.g., German, Dutch and Korean), con-
stituent words of compounds are not separated by
white spaces, unlike in English. In those languages,
it is beneficial for various NLP applications to split
such compounds. For example, compound splitting
enables SMT systems to translate a compound on a
word-by-word basis, even if the compound itself is
not found in the translation table (Koehn and Knight,
2003; Dyer, 2009). In the context of IR, decom-
pounding has an analogous effect to stemming, and
it significantly improves retrieval results (Braschler
and Ripplinger, 2004). In abbreviation recognition,
the definition of an abbreviation is often in the form
of a noun compound, and most abbreviation recogni-
tion algorithms assume that the definition is properly
segmented; see e.g., (Schwartz and Hearst, 2003;
Okazaki et al, 2008).
This has led NLP researchers to explore meth-
ods for splitting compounds, especially noun com-
pounds, in various languages (Koehn and Knight,
2003; Nakazawa et al, 2005; Alfonseca et al,
2008a). While many methods have been presented,
they basically require expensive linguistic resources
to achieve high enough accuracy. For example, Al-
fonseca et al (2008b) employed a word dictionary,
which is obviously useful for this task. Other stud-
ies have suggested using bilingual resources such as
parallel corpora (Brown, 2002; Koehn and Knight,
959
2003; Nakazawa et al, 2005). The idea behind those
methods is that compounds are basically split into
constituent words when they are translated into En-
glish, where the compounded words are separated
by white spaces, and hence splitting rules can be
learned by discovering word alignments in bilingual
resources.
The largest obstacle that makes compound split-
ting difficult is the existence of out-of-vocabulary
words, which are not found in the abovemen-
tioned linguistic resources. In the Japanese case,
it is known that katakana words constitute a large
source of out-of-vocabulary words (Brill et al, 2001;
Nakazawa et al, 2005; Breen, 2009). As we have
discussed, katakana words are very productive, and
thus we can no longer expect existent linguistic re-
sources to have sufficient coverage. According to
(Breen, 2009), as many as 20% of katakana words
in news articles, which we think include less out-of-
vocabulary words than Web and other noisy textual
data, are out-of-vocabulary. Those katakana words
often form noun compounds, and pose a substantial
difficulty for Japanese text processing (Nakazawa et
al., 2005).
1.2 Paraphrases as implicit word boundaries
To alleviate the errors caused by out-of-vocabulary
words, we explored the use of unlabeled textual
data for splitting katakana noun compounds. Since
the amount of unlabeled text available is generally
much larger than word dictionaries and other expen-
sive linguistic resources, it is crucial to establish a
methodology for taking full advantage of such eas-
ily available textual data. While several approaches
have already been proposed, their accuracies are still
unsatisfactory (section 2.1).
From a broad perspective, our approach can be
seen as using paraphrases of noun compounds. As
we will see in section 4 and 5, katakana noun com-
pounds can be paraphrased into various forms that
strongly indicate word boundaries within the origi-
nal noun compound. This paper empirically demon-
strates that splitting accuracy can be significantly
improved by extracting such paraphrases from un-
labeled text, the Web in our case, and then using that
information for constructing splitting models.
Specifically, two types of paraphrases are inves-
tigated in this paper. Section 4 explores monolin-
gual paraphrases that can be generated by inserting
certain linguistic markers between constituent words
of katakana noun compounds. Section 5, in turn,
explores bilingual paraphrases (specifically, back-
transliteration). Since katakana words are basically
transliterations from English, back-transliterating
katakana noun compounds is also useful for split-
ting. To avoid terminological confusion, mono-
lingual paraphrases are simply referred to as para-
phrases and bilingual paraphrases are referred to as
back-transliterations hereafter.
We did experiments to empirically evaluate our
method. The results demonstrated that both para-
phrase and back-transliteration substantially im-
proved the performance in terms of F1-score, and
the best performance was achieved when they
were combined. We also confirmed that our
method outperforms the previously proposed split-
ting methods by a wide margin. All these results
strongly suggest the effectiveness of paraphrasing
and back-transliteration for identifying word bound-
aries within katakana noun compounds.
2 Related Work
2.1 Compound splitting
A common approach to splitting compounds with-
out expensive linguistic resources is an unsuper-
vised method based on word or string frequen-
cies estimated from unlabeled text (Koehn and
Knight, 2003; Ando and Lee, 2003; Schiller, 2005;
Nakazawa et al, 2005; Holz and Biemann, 2008).
Amongst others, Nakazawa et al (2005) also in-
vestigated ways of splitting katakana noun com-
pounds. Although the frequency-based method gen-
erally achieves high recall, its precision is not satis-
factory (Koehn and Knight, 2003; Nakazawa et al,
2005). Our experiments empirically compared our
method with the frequency-based methods, and the
results demonstrate the advantage of our method.
Our approach can be seen as augmenting discrim-
inative models of compound splitting with large ex-
ternal linguistic resources, i.e., textual data on the
Web. In a similar spirit, Alfonseca et al (2008b) pro-
posed the use of query logs for compound splitting.3
Their experimental results, however, did not clearly
3Although they also proposed using anchor text, this slightly
degraded the performance.
960
demonstrate their method?s effectiveness. Without
the query logs, the accuracy is reported to drop
only slightly from 90.55% to 90.45%. In contrast,
our experimental results showed statistically signifi-
cant improvements as a result of using additional re-
sources. Moreover, we used only textual data, which
is easily available, unlike query logs.
Holz and Biemann (2008) proposed a method
for splitting and paraphrasing German compounds.
While their work is related to ours, their algorithm
is a pipeline model and paraphrasing result is not
employed during splitting.
2.2 Other research topics
Our study is closely related to word segmentation,
which is an important research topic in Asian lan-
guages including Japanese. Although we can use
existing word segmentation systems for splitting
katakana noun compounds, it is difficult to reach the
desired accuracy, as we will empirically demonstrate
in section 6. One reason for this is that katakana
noun compounds often include out-of-vocabulary
words, which are difficult for the existing segmen-
tation systems to deal with. See (Nakazawa et al,
2005) for a discussion of this point. From a word
segmentation perspective, our task can be seen as
a case study focusing on a certain linguistic phe-
nomenon of particular difficulty. More importantly,
we are unaware of any attempts to use paraphrases
or transliterations for word segmentation in the same
way as we do.
Recent studies have explored using paraphrase
statistics for parsing (Nakov and Hearst, 2005a;
Nakov and Hearst, 2005b; Bansal and Klein, 2011).
Although these studies successfully demonstrated
the usefulness of paraphrases for improving parsers,
the connection between paraphrases and word seg-
mentation (or noun compound splitting) was not at
all discussed.
Our method of using back-transliterations for
splitting katakana noun compounds (section 5) is
closely related to methods for mining transliteration
from the Web text (Brill et al, 2001; Cao et al,
2007; Oh and Isahara, 2008; Wu et al, 2009). What
most differentiates these studies from our work is
that their primary goal is to build a machine translit-
eration system or to build a bilingual dictionary it-
self; none of them explored splitting compounds.
Table 1: Basic features.
ID Feature Description
1 yi constituent word 1-gram
2 yi?1yi constituent word 2-gram
3 LEN(yi) #characters of yi (1, 2, 3, 4, or ?5)
4 DICT(yi) true if yi is in the dictionary
3 Supervised Approach
The task we examine in this paper is splitting
a katakana noun compound x into its constituent
words, y = (y1, y2 . . . y|y|). Note that the output
can be a single word, i.e., |y| = 1. Since it is pos-
sible that the input is an out-of-vocabulary word, it
is not at all trivial to identify a single word as such.
A naive method would erroneously split an out-of-
vocabulary word into multiple constituent words.
We formalize our task as a structure prediction
problem that, given a katakana noun compound x,
predicts the most probable splitting y?.
y? = argmax
y?Y(x)
w ? ?(y),
where Y(x) represents the set of all splitting options
of x, ?(y) is a feature vector representation of y,
and w is a weight vector to be estimated from la-
beled data.
Table 1 summarizes our basic feature set. Fea-
tures 1 and 2 are word 1-gram and 2-gram features,
respectively. Feature 3 represents the length of the
constituent word. LEN(y) returns the number of
characters of y (1, 2, 3, 4, or ?5). Feature 4 indi-
cates whether the constituent word is registered in
an external dictionary (see section 6.1). DICT(y) re-
turns true if the word y is in the dictionary.
In addition to those basic features, we also employ
paraphrases and back-transliterations of katakana
noun compounds as features. The features are de-
tailed in sections 4 and 5, respectively.
We can optimize the weight vector w using an ar-
bitrary training algorithm. Here we adopt the aver-
aged perceptron algorithm for the sake of time effi-
ciency (Freund and Schapire, 1999). The perceptron
offers efficient online training, and it performs com-
paratively well with batch algorithms such as SVMs.
Since we use only factored features (see table 1, sec-
tion 4 and section 5), dynamic programming can be
used to locate y?.
961
Table 2: Paraphrase rules and examples. The first column represents the type of linguistic marker to be inserted, the
second column shows the paraphrase rules, and the last column gives examples.
Type Rule Example
Centered dot X1X2 ? X1 ? X2 ????????
(anchovy pasta)
??????????
(anchovy ? pasta)
Possessive marker X1X2 ? X1 ? X2 ????????
(anchovy pasta)
? ??????
(with anchovy)
???
(pasta)
Verbal suffix X1X2 ? X1 ?? X2
X1X2 ? X1?? X2
??????????
(download file)
?????????
(downloaded)
????
(file)
Adjectival suffix X1X2 ? X1 ? X2
X1X2 ? X1 ? X2
X1X2 ? X1 ?? X2
????????
(surprise gift)
???????
(surprising)
???
(gift)
4 Paraphrasing
In this section, we argue that paraphrases of
katakana noun compounds provides useful informa-
tion on word boundaries. Consequently, we propose
using paraphrase frequencies as features for training
the discriminative model.
4.1 Paraphrasing noun compounds
A katakana noun compound can be paraphrased into
various forms, some of which provide information
on the word boundaries within the original com-
pound.
(1) a. ????????
(anchovy pasta)
b. ?????????
(anchovy ? pasta)
c. ??????
(with anchovy)
???
(pasta)
These examples are paraphrases of each other. (1a)
is in the form of a noun compound, within which
the word boundary is ambiguous. In (1b), on the
other hand, a centered dot ? is inserted between
the constituent words. In the Japanese writing sys-
tem, the centered dot is sometimes, but not always,
used to separate long katakana compounds for the
sake of readability. (1c) is the noun phrase gener-
ated from (1a) by inserting the possessive marker
???, which can be translated as with in this context,
between the constituent words. If we observe para-
phrases of (1a) such as (1b) and (1c), we can guess
that a word boundary exists between ??????
(anchovy)? and ???? (pasta)?.
4.2 Paraphrase rules
The above discussion led us to use paraphrase
frequencies estimated from Web text for splitting
katakana noun compounds. For this purpose, we
established the seven paraphrase rules illustrated in
Table 2. The rules are in the form of X1X2 ?
X1MX2, where X1 and X2 represent nouns, and
M is a certain linguistic marker (e.g., the posses-
sive marker ???). The left-hand term corresponds
to a compound to be paraphrased and the right-hand
term represents its paraphrase. For instance, X1 =
?????? (anchovy)?, X2 = ???? (pasta)?, and
M = ???. The paraphrase rules we use are based on
the rules proposed by Kageura et al (2004) for ex-
panding complex terms, primarily noun compounds,
into their variants.
4.3 Web-based frequency as features
We introduce a new feature using the paraphrase
rules and Web text. As preprocessing, we use reg-
ular expressions to count the frequencies of all po-
tential paraphrases of katakana noun compounds on
the Web in advance.
(katakana)+? (katakana)+
(katakana)+? (katakana)+
(katakana)+?? (katakana)+
. . .
where (katakana) corresponds to one katakana char-
acter. Given a candidate segmentation y at test time,
we generate paraphrases of the noun compound by
setting X1 = yi?1 and X2 = yi, and applying the
paraphrase rules. We then use log(F + 1), where F
is the sum of the Web-based frequencies of the gen-
962
erated paraphrases, as the feature of the boundary
between yi?1 and yi.
As the feature value, we use the logarithmic fre-
quency, rather than the raw frequency, for scaling.
Since the other features have binary value, we found,
in initial experiments, that the importance of this
feature is overemphasized if we use the raw fre-
quency. Note that we use log(F + 1) rather than
log F so as to avoid the feature value being zero
when F = 1.
5 Back-transliteration
Most katakana words are transliterations from En-
glish, where words are separated by white spaces.
It is, therefore, reasonable to think that back-
transliterating katakana noun compounds into En-
glish would provide information on word bound-
aries, in a similar way to paraphrasing.
This section presents a method for extracting
back-transliterations of katakana words from mono-
lingual Web text, and establishing word alignments
between those katakana and English words (Table
3). In what follows, the pair of katakana words
and its English back-transliteration is referred to as a
transliteration pair. If the transliteration pair is an-
notated with word alignment information as in Table
3, it is referred to as a word-aligned transliteration
pair.
Using word-aligned transliteration pairs extracted
from the Web text, we derive a binary feature in-
dicating whether katakana word yi corresponds to
a single English word. Additionally, we derive an-
other feature indicating whether a katakana word 2-
gram yi?1yi corresponds to an English word 2-gram.
5.1 Parenthetical expressions
In Japanese and other Asian languages, transliter-
ated words are sometimes followed by their English
back-transliterations inside parentheses:
(2) a. ?????? ?????
(junk)
???
(food)
(junk food)?...
b. ??????? ? ???
(spam)
(spam)?????...
where the underline indicates the Japanese text
that is followed by English back-transliteration.
We extract word-aligned transliteration pairs from
Table 3: Word-aligned transliteration pairs. The number
indicates the word alignment.
Japanese English
????1??? 2 junk1 food2
???3 spam3
such parenthetical expressions by establishing the
correspondences between pre-parenthesis and in-
parenthesis words.
To accomplish this, we have to resolve three prob-
lems: (a) English words inside parenthesis do not
always provide a back-transliteration of the pre-
parenthesis text, (b) the left boundary of the pre-
parenthesis text, denoted as ??? in the example, has
to be identified, and (c) pre-parenthesis text, which
is a katakana noun compound in our case, has to be
segmented into words.
Although several studies have explored mining
transliterations from such parenthetical expressions
(Cao et al, 2007; Wu et al, 2009), the last problem
has not been given much attention. In the past stud-
ies, the pre-parenthesis text is assumed to be cor-
rectly segmented by, typically, using existent word
segmentation systems. This is, however, not appro-
priate for our purpose, because pre-parenthesis text
is a katakana noun compound, which is hard for ex-
isting systems to handle, and hence the alignment
quality is inevitably affected by segmentation errors.
To handle these three problems, we use the pho-
netic properties of the transliterations. For the pur-
pose of explanation, we shall first focus on problem
(c). Since transliterated katakana words preserve the
pronunciation of the original English words to some
extent (Knight and Graehl, 1998), we can discover
the correspondences between substrings of the two
languages based on phonetic similarity:
(3) a. [???]1[?]2[??]3[? ]4
b. [jun]1[k]2 [foo]3[d]4
Note that these are the pre-parenthesis and in-
parenthesis text in (2a). The substrings surrounded
by square brackets with the same number corre-
spond to each other. Given such a correspondence,
we can segment the pre-parenthesis text (3a) accord-
ing to its English counterpart (3b), in which words
963
Table 4: Example of the substring alignment A between
f =???????? ? and e =?junkfood? (|A| = 4).
(fi, ei) log p(fi, ei)
(???, jun) ?10.767
(?, k) ?5.319
(??, foo) ?11.755
(? , d) ?5.178
are separated by white space. We can recognize that
the katakana string ??????, which is the con-
catenation of the first two substrings in (3a), forms
a single word because it corresponds to the English
word junk, and so on. Consequently, (3a) can be seg-
mented into two words, ????? (junk)? and ???
? (food)?. The word alignment is trivially estab-
lished.
For problems (a) and (b), we can also use the
phonetic similarity between pre-parenthesis and in-
parenthesis text. If the parenthetical expression does
not provide the transliteration, or if the left boundary
is erroneously identified, we can expect the phonetic
similarity to become small. Such situations thus can
be identified.
The remainder of this section details this ap-
proach. Section 5.2 presents a probabilistic model
for discovering substring alignment such as (3). Sec-
tion 5.3 shows how to extract word-aligned translit-
eration pairs by using the probabilistic model.
5.2 Phonetic similarity model
To establish the substring alignment between
katakana and Latin alphabet strings, we use the
probabilistic model proposed by (Jiampojamarn et
al., 2007). Let f and e be katakana and alphabet
strings, and A be the substring alignment between
them. More precisely, A is a set of corresponding
substring pairs (fi, ei) such that f = f1f2 . . . f|A|
and e = e1e2 . . . e|A|. The probability of such align-
ment is defined as
log p(f, e, A) =
?
(fi,ei)?A
log p(fi, ei).
Since A is usually unobservable, it is treated as a
hidden variable. Table 4 illustrates an example of
the substring alignment between f =???????
? ? and e =?junkfood?, and the likelihood of each
substring pair estimated in our experiment.
The model parameters are estimated from a set of
transliteration pairs (f, e) using the EM algorithm.
In the E-step, we estimate p(A|f, e) based on the
current parameters. In the parameter estimation, we
restrict both fi and ei to be at most three characters
long. Doing this not only makes the E-step compu-
tationally efficient but avoids over-fitting by forbid-
ding too-long substrings to be aligned. In the M-
step, the parameter is re-estimated using the result
of the E-step. We can accomplish this by using an
extension of the forward-backward algorithm. See
(Jiampojamarn et al, 2007) for details.
Given a new transliteration pair (f, e), we can de-
termine the substring alignment as
A? = argmax
A
log p(f, e, A).
In finding the substring alignment, a white space on
the English side is used as a constraint, so that the
English substring ei does not span a white space.
5.3 Extracting word-aligned transliteration
pairs
The word-aligned transliteration pairs are extracted
using the phonetic similarity model, as follows.
First, candidate transliteration pairs (f, e) are ex-
tracted from the parenthetical expressions. This is
done by extracting English words inside parenthe-
ses and pre-parenthesis text written in katakana. En-
glish words are normalized by lower-casing capital
letters.
Second, we determine the left boundary by using
the confidence score: 1N log p(f, e, A?), where N is
the number of English words. The term 1N prevents
the score from being unreasonably small when there
are many words. We truncate f by removing the
leftmost characters one by one, until the confidence
score exceeds a predefined threshold ?. If f becomes
empty, the pair is regarded as a non-transliteration
and discarded.
Finally, for the remaining pairs, the Japanese side
is segmented and the word alignment is established
according to A?. This results in a list of word-
aligned transliteration pairs (Table 3).
6 Experiments and Discussion
We conducted experiments to investigate how the
use of the paraphrasing and the back-transliteration
964
improves the performance of the discriminative
model.
6.1 Experimental setting
To train the phonetic similarity model, we used
a set of transliteration pairs extracted from the
Wikipedia.4 Since person names are almost always
transliterated when they are imported from English
into Japanese, we made use of the Wikipedia arti-
cles that belong to the Living people category. From
the titles of those articles, we automatically ex-
tracted person names written in katakana, together
with their English counterparts obtainable via the
multilingual links provided by the Wikipedia. This
yielded 17,509 transliteration pairs for training. In
performing the EM algorithm, we tried ten differ-
ent initial parameters and selected the model that
achieved the highest likelihood.
The data for training and testing the percep-
tron was built using a Japanese-English dictionary
EDICT.5 We randomly extracted 5286 entries writ-
ten in katakana from EDICT and manually anno-
tated word boundaries by establishing word corre-
spondences to their English transliterations. Since
English transliterations are already provided by
EDICT, the annotation can be trivially done by na-
tive speakers of Japanese. Using this data set, we
performed 2-fold cross-validation for testing the per-
ceptron. The number of iterations was set to 20 in all
the experiments.
To compute the dictionary-based feature DICT(y)
in our basic feature set, we used NAIST-jdic.6 It is
the largest dictionary used for Japanese word seg-
mentation, and it includes 19,885 katakana words.
As Web corpora, we used 1.7 G sentences of
blog articles. From the corpora, we extracted
14,966,205 (potential) paraphrases of katakana noun
compounds together with their frequencies. We
also extracted 151,195 word-aligned transliteration
pairs. In doing this, we ranged the threshold ? in
{?10,?20, ? ? ? ? 150} and chose the value that per-
formed the best (? = ?80).
The results were evaluated using precision, recall,
F1-score, and accuracy. Precision is the number of
correctly identified words divided by the number of
4http://ja.wikipedia.org/
5http://www.csse.monash.edu.au/?jwb/edict doc.html
6http://sourceforge.jp/projects/naist-jdic
all identified words, recall is the number of correctly
identified words divided by the number of all ora-
cle words, the F1-score is their harmonic mean, and
accuracy is the number of correctly split katakana
noun compounds divided by the number of all the
katakana noun compounds.
6.2 Baseline systems
We compared our system with three frequency-
based baseline system, two supervised baselines,
and two state-of-the-art word segmentation base-
lines. The first frequency-based baseline, UNI-
GRAM, performs compound splitting based on a
word 1-gram language model (Schiller, 2005; Al-
fonseca et al, 2008b):
y? = argmax
y?Y(x)
?
i
p(yi),
where p(yi) represents the probability of yi. The
second frequency-based baseline, GMF, outputs the
splitting option with the highest geometric mean fre-
quency of the constituent words (Koehn and Knight,
2003):
y? = argmax
y?Y(x)
GMF(y) = argmax
y?Y(x)
{?
i
f(yi)
}1/|y|
,
where f(yi) represents the frequency of yi. The
third frequency-based baseline, GMF2, is a mod-
ification of GMF proposed by Nakazawa et al
(2005). It is based on the following score instead
of GMF(y):
GMF2(y) =
?
??
??
GMF(y) (|y| = 1)
GMF(y)
C
Nl +?
(|y| ? 2),
where C , N , and ? are hyperparameters and l is the
average length of the constituent words. Following
(Nakazawa et al, 2005), the hyperparameters were
set as C = 2500, N = 4, and ? = 0.7. We estimated
p(y) and f(y) from the Web corpora.
The first supervised baseline, AP, is the aver-
aged perceptron model trained using only the ba-
sic feature set. The second supervised baseline,
AP+GMF2 is a combination of AP and GMF2,
which performed the best amongst the frequency-
based baselines. Following (Alfonseca et al,
965
Table 5: Comparison with baseline systems.
Type System P R F1 Acc
Frequency UNIGRAM 64.2 49.7 56.0 63.0
GMF 42.9 62.0 50.7 47.5
GMF2 67.4 76.0 71.5 72.5
Supervised AP 81.9 82.5 82.2 83.4
AP+GMF2 83.0 83.9 83.4 84.2
PROPOSED 86.4 87.4 87.1 87.6
Word seg. JUMAN 71.4 60.1 65.3 69.8
MECAB 72.4 73.7 67.8 71.6
2008b), GMF2 is integrated into AP as two bi-
nary features indicating whether GMF2(y) is larger
than any other candidates, and whether GMF2(y) is
larger than the non-split candidate. Although Alfon-
seca et al (2008b) also proposed using (the log of)
the geometric mean frequency as a feature, doing so
degraded performance in our experiment.
Regarding the two state-of-the-art word segmen-
tation systems, one is JUMAN,7 a rule-based word
segmentation system (Kurohashi and Nagao, 1994),
and the other is MECAB,8 a supervised word seg-
mentation system based on CRFs (Kudo et al,
2004). These two baselines were chosen in order to
show how well existing word segmentation systems
perform this task. Although the literature states that
it is hard for existing systems to deal with katakana
noun compounds (Nakazawa et al, 2005), no empir-
ical data on this issue has been presented until now.
6.3 Splitting result
Table 5 compares the performance of our system
(PROPOSED) with the baseline systems. First of all,
we can see that PROPOSED clearly improved the per-
formance of AP, demonstrating the effectiveness of
using paraphrases and back-transliterations.
Our system also outperformed all the frequency-
based baselines (UNIGRAM, GMF, and GMF2). This
is not surprising, since the simple supervised base-
line, AP, already outperformed the unsupervised
frequency-based ones. Indeed similar experimental
results were also reported by Alfonseca (2008a). An
interesting observation here is the comparison be-
tween PROPOSED and AP+GMF2. It reveals that
our approach improved the performance of AP more
than the frequency-based method did. These results
7http://nlp.kuee.kyoto-u.ac.jp/nl-resource/juman.html
8http://sourceforge.net/projects/mecab
indicate that paraphrasing and back-transliteration
are more informative clues than the simple fre-
quency of constituent words. We would like to
note that the higher accuracy of PROPOSED in com-
parison with the baselines is statistically significant
(p < 0.01, McNemar?s test).
The performance of the two word segmenta-
tion baselines (JUMAN and MECAB) is significantly
worse in our task than in the standard word segmen-
tation task, where nearly 99% precision and recall
are reported (Kudo et al, 2004). This demonstrates
that splitting a katakana noun compound is not at
all a trivial task to resolve, even for the state-of-the-
art word segmentation systems. On the other hand,
PROPOSED outperformed both JUMAN and MECAB
in this task, meaning that our technique can suc-
cessfully complement the weaknesses of the existing
word segmentation systems.
By analyzing the errors, we interestingly found
that some of the erroneous splitting results are still
acceptable to humans. For example, while ????
??? (upload)? was annotated as a single word in
the test data, our system split it into ???? (up)?
and ???? (load)?. Although the latter splitting
may be useful in some applications, it is judged as
wrong in our evaluation framework. This implies
the importance of evaluating the splitting results in
some extrinsic tasks. We leave it to a future work.
6.4 Investigation on out-of-vocabulary words
In our test data, 2681 out of the 5286 katakana noun
compounds contained at least one out-of-vocabulary
word that are not registered in NAIST-jdic. Table 6
illustrates the results of the supervised systems for
those 2681 and the remaining 2605 katakana noun
compounds (referred to as w/ OOV and w/o OOV
data, respectively). While the accuracy exceeds 90%
for w/o OOV data, it is substantially degraded for w/
OOV data. This is consistent with our claim that out-
of-vocabulary words are a major source of errors in
splitting noun compounds.
The three supervised systems performed almost
equally for w/o OOV data. This is because AP triv-
ially performs very well on this subset, and it is dif-
ficult to get any further improvement. On the other
hand, we can see that there are substantial perfor-
mance gaps between the systems for w/ OOV data.
This result reflects the effect of the additional fea-
966
Table 6: Splitting results of the supervised systems for w/ OOV and w/o OOV data.
w/ OOV data w/o OOV data
System P R F1 Acc P R F1 Acc
AP 66.9 69.9 68.3 72.8 95.4 93.2 94.3 94.2
AP+GMF2 69.7 73.7 71.6 75.2 95.2 92.4 93.7 93.6
PROPOSED 76.8 79.3 78.0 80.9 95.3 94.2 94.8 94.5
tures more directly than is shown in table 5.
6.5 Effect of the two new features
To see the effect of the new features in more detail,
we looked at the performances of our system using
different feature sets (Table 7). The first column
represents the feature set we used: BASIC, PARA,
TRANS, and ALL represent the basic features, the
paraphrase feature, the back-transliteration feature,
and all the features. The results demonstrate that
adding either of the new features improved the per-
formance, and the best result was when they were
used together. In all cases, the improvement over
BASIC was statistically significant (p < 0.01, Mc-
Nemar?s test).
Next, we investigated the coverage of the features.
Our test data comprised 7709 constituent words,
4937 (64.0%) of which were covered by NAIST-
jdic. The coverage was significantly improved when
using the back-transliteration feature. We observed
that 6216 words (80.6%) are in NAIST-jdic or word-
aligned transliteration pairs extracted from the Web
text. This shows that the back-transliteration fea-
ture successfully reduced the number of out-of-
vocabulary words. On the other hand, we observed
that the paraphrase and back-transliteration features
were activated for 79.5% (1926/2423) and 15.5%
(376/2423) of the word boundaries in our test data.
Overall, we see that the coverage of these fea-
tures is reasonably good, although there is still room
for further improvement. It would be beneficial to
use larger Web corpora or more paraphrase rules,
for example, by having a system that automatically
learns rules from the corpora (Barzilay and McKe-
own, 2001; Bannard and Callison-Burch, 2005).
6.6 Sensitivity on the threshold ?
Finally we investigated the influence of the thresh-
old ? (Figure 1 and 2). Figure 1 illustrates the system
performance in terms of F1-score for different values
Table 7: Effectiveness of paraphrase (PARA) and back-
transliteration feature (TRANS).
Feature set P R F1 Acc
BASIC 81.9 82.5 82.2 83.4
BASIC+PARA 85.1 85.3 85.2 85.9
BASIC+TRANS 85.1 86.3 85.7 86.5
ALL 86.4 87.4 87.1 87.6
of ?. While the F1-score drops when the value of ?
is too large (e.g., ?20), the F1-score is otherwise al-
most constant. This demonstrates it is generally easy
to set ? near the optimal value. More importantly,
the F1-score is consistently higher than BASIC irre-
spective of the value of ?. Figure 2 represents the
number of distinct word-aligned transliteration pairs
that were extracted from the Web corpora. We see
that most of the extracted transliteration pairs have
high confidence score.
7 Conclusion
In this paper, we explored the idea of using monolin-
gual and bilingual paraphrases for splitting katakana
noun compounds in Japanese. The experiments
demonstrated that our method significantly im-
proves the splitting accuracy by a large margin
in comparison with the previously proposed meth-
ods. This means that paraphrasing provides a sim-
ple and effective way of using unlabeled textual
data for identifying implicit word boundaries within
katakana noun compounds.
Although our investigation was restricted to
katakana noun compounds, one might expect that a
similar approach would be useful for splitting other
types of noun compounds (e.g., German noun com-
pounds), or for identifying general word boundaries,
not limited to those between nouns, in Asian lan-
guages. We think these are research directions worth
exploring in the future.
967
87
88
85
86
co
re
83
84F 1
-s
c
81
82
Threshold
Figure 1: Influence of the threshold ? (x-axis) on the F1-
score (y-axis). The triangles and squares represent sys-
tems using the ALL and BASIC feature sets, respectively.
200000
pa
irs
100000
150000
er
at
io
n 
p
50000f t
ra
ns
lit
e
0
# 
of
Threshold
Figure 2: The number of distinct word-aligned transliter-
ations pairs that were extracted from the Web corpora for
different values of ?.
Acknowledgement
This work was supported by the Multimedia Web
Analysis Framework towards Development of Social
Analysis Software program of the Ministry of Ed-
ucation, Culture, Sports, Science and Technology,
Japan.
References
Enrique Alfonseca, Slaven Bilac, and Stefan Pharies.
2008a. Decompoundig query keywords from com-
pounding languages. In Proceedings of ACL, Short
Papers, pages 253?256.
Enrique Alfonseca, Slaven Bilac, and Stefan Pharies.
2008b. German decompounding in a difficult corpus.
In Proceedings of CICLing, pages 128?139.
Rie Kubota Ando and Lillian Lee. 2003. Mostly-
unsupervised statistical segmentation of Japanese
Kanji sequences. Natural Language Engineering,
9(2):127?149.
Colin Bannard and Chris Callison-Burch. 2005. Para-
phrasing with bilingual parallel corpora. In Proceed-
ings of ACL, pages 597?604.
Mohit Bansal and Dan Klein. 2011. Web-scale features
for full-scale parsing. In Proceedings of ACL, pages
693?702.
Regina Barzilay and Kathleen McKeown. 2001. Extract-
ing paraphrases from a parallel corpus. In Proceedings
of ACL, pages 50?57.
Martin Braschler and Ba?rbel Ripplinger. 2004. How ef-
fective is stemming and decompounding for German
text retrieval? Information Retrieval, 7:291?316.
Jamese Breen. 2009. Identification of neologisms in
Japanese by corpus analysis. In Proceedings of eLexi-
cography in the 21st centry conference, pages 13?22.
Eric Brill, Gray Kacmarcik, and Chris Brockett. 2001.
Automatically harvesting katakana-English term pairs
from search engine query logs. In Proceedings of NL-
PRS, pages 393?399.
Ralf D. Brown. 2002. Corpus-driven splitting of com-
pound words. In Proceedings of TMI.
Guihong Cao, Jianfeng Gao, and Jian-Yun Nie. 2007. A
system to mine large-scale bilingual dictionaries from
monolingual Web pages. In Proceedings of MT Sum-
mit, pages 57?64.
Chris Dyer. 2009. Using a maximum entropy model to
build segmentation lattices for MT. In Proceedings of
NAACL, pages 406?414.
Yoav Freund and Robert E. Schapire. 1999. Large mar-
gin classification using the perceptron algorithm. Ma-
chine Learning, 37(3):277?296.
Florian Holz and Chris Biemann. 2008. Unsupervised
and knowledge-free learning of compound splits and
periphrases. In CICLing, pages 117?127.
Sittichai Jiampojamarn, Grzegorz Kondrak, and Tarek
Sherif. 2007. Applying many-to-many alignment and
hidden Markov models to letter-to-phoneme conver-
sion. In HLT-NAACL, pages 372?379.
Kyo Kageura, Fuyuki Yoshikane, and Takayuki Nozawa.
2004. Parallel bilingual paraphrase rules for noun
compounds: Concepts and rules for exploring Web
language resources. In Proceedings of Workshop on
Asian Language Resources, pages 54?61.
Kevin Knight and Jonathan Graehl. 1998. Machine
transliteration. Computational Linguistics, 24(4):599?
612.
Philip Koehn and Kevin Knight. 2003. Empirical meth-
ods for compound splitting. In Proceedings of EACL,
pages 187?193.
Taku Kudo, Kaoru Yamamoto, and Yuji Matsumoto.
2004. Applying conditional random fields to Japanese
morphological analysis. In Proceedings of EMNLP,
pages 230?237.
Sadao Kurohashi and Makoto Nagao. 1994. Im-
provements of Japanese morphological analyzer JU-
MAN. In Proceedings of the International Workshop
on Sharable Natural Language Resources, pages 22?
38.
968
Toshiaki Nakazawa, Daisuke Kawahara, and Sadao
Kurohashi. 2005. Automatic acquisition of basic
Katakana lexicon from a given corpus. In Proceedings
of IJCNLP, pages 682?693.
Preslav Nakov and Marti Hearst. 2005a. Search en-
gine statistics beyond the n-gram: Application to noun
compound bracketing. In Proceedings of CoNLL,
pages 17?24.
Preslav Nakov and Marti Hearst. 2005b. Using the Web
as an implicit training set: Application to structural
ambiguity resolution. In Proceedings of HLT/EMNLP,
pages 835?342.
Jong-Hoon Oh and Hitoshi Isahara. 2008. Hypothesis
selection in machine transliteration: A Web mining
approach. In Proceedings of IJCNLP, pages 233?240.
Naoaki Okazaki, Sophia Ananiadou, and Jin?ichi Tsujii.
2008. A discriminative alignment model for abbrevi-
ation recognition. In Proceedings of COLING, pages
657?664.
Anne Schiller. 2005. German compound analysis with
wfsc. In Proceedings of Finite State Methods and Nat-
ural Language Processing, pages 239?246.
Ariel S. Schwartz and Marti A. Hearst. 2003. A sim-
ple algorithm for identifying abbreviation definitions
in biomedical text. In Proceedings of PSB, pages 451?
462.
Natsuko Tsujimura. 2006. An Introduction to Japanese
Linguistics. Wiley-Blackwell.
Xianchao Wu, Naoaki Okazaki, and Jun?ichi Tsujii.
2009. Semi-supervised lexicon mining from paren-
thetical expressions in monolingual Web pages. In
Proceedings of NAACL, pages 424?432.
969
Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 99?109,
October 25-29, 2014, Doha, Qatar. c?2014 Association for Computational Linguistics
Accurate Word Segmentation and POS Tagging for Japanese Microblogs:
Corpus Annotation and Joint Modeling with Lexical Normalization
Nobuhiro Kaji?? and Masaru Kitsuregawa??
?National Institute of Information and Communications Technology
?Institute of Industrial Science, The University of Tokyo
?National Institute of Informatics
{kaji, kitsure}@tkl.iis.u-tokyo.ac.jp
Abstract
Microblogs have recently received
widespread interest from NLP re-
searchers. However, current tools for
Japanese word segmentation and POS
tagging still perform poorly on microblog
texts. We developed an annotated corpus
and proposed a joint model for over-
coming this situation. Our annotated
corpus of microblog texts enables not
only training of accurate statistical models
but also quantitative evaluation of their
performance. Our joint model with lexical
normalization handles the orthographic
diversity of microblog texts. We con-
ducted an experiment to demonstrate
that the corpus and model substantially
contribute to boosting accuracy.
1 Introduction
Microblogs, such as Twitter1 and Weibo2, have re-
cently become an important target of NLP tech-
nology. Since microblogs offer an instant way of
posting textual messages, they have been given
increasing attention as valuable sources for such
actions as mining opinions (Jiang et al., 2011)
and detecting sudden events such as earthquake
(Sakaki et al., 2010).
However, many studies have reported that cur-
rent NLP tools do not perform well on microblog
texts (Foster et al., 2011; Gimpel et al., 2011). In
the case of Japanese text processing, the most se-
rious problem is poor accuracy of word segmen-
tation and POS tagging. Since these two tasks
are positioned as the fundamental step in the text
processing pipeline, their accuracy is vital for all
downstream applications.
1https://twitter.com
2https://www.weibo.com
1.1 Development of annotated corpus
The main obstacle that makes word segmentation
and POS tagging in the microblog domain chal-
lenging is the lack of annotated corpora. Because
current annotated corpora are from other domains,
such as news articles, it is difficult to train models
that perform well on microblog texts. Moreover,
system performance cannot be evaluated quantita-
tively.
We remedied this situation by developing an an-
notated corpus of Japanese microblogs. We col-
lected 1831 sentences from Twitter and manually
annotated these sentences with word boundaries,
POS tags, and normalized forms of words (c.f.,
Section 1.2).
We, for the first time, present a comprehen-
sive empirical study of Japanese word segmenta-
tion and POS tagging on microblog texts by us-
ing this corpus. Specifically, we investigated how
well current models trained on existing corpora
perform in the microblog domain. We also ex-
plored performance gains achieved by using our
corpus for training, and by jointly performing lex-
ical normalization (c.f., Section 1.2).
1.2 Joint modeling with lexical normalization
Orthographic diversity in microblog texts causes a
problem when training a statistical model for word
segmentation and POS tagging. Microblog texts
frequently contain informal words that are spelled
in a non-standard manner, e.g., ?oredi (already)?,
?b4 (before)?, and ?talkin (talking)? (Han and
Baldwin, 2011). Such words, hereafter referred
to as ill-spelled words, are so productive that they
considerably increase the vocabulary size. This
makes training of statistical models difficult.
We address this problem by jointly conducting
lexical normalization. Although a wide variety
of ill-spelled words are used in microblog texts,
many can be normalized into well-spelled equiva-
lents, which conform to standard rules of spelling.
99
A joint model with lexical normalization is able
to handle orthographic diversity by exploiting in-
formation obtainable from the well-spelled equiv-
alents.
The proposed joint model was empirically eval-
uated on the microblog corpus we developed. Our
experiment demonstrated that the proposed model
can perform word segmentation and POS tag-
ging substantially better than current state-of-the-
art models.
1.3 Summary
Contributions of this paper are the following:
? We developed a microblog corpus that en-
ables not only training of accurate models but
also quantitative evaluation for word segmen-
tation and POS tagging in the microblog do-
main.3
? We propose a joint model with lexical nor-
malization for better handling of ortho-
graphic diversity in microblog texts. In par-
ticular, we present a new method of training
the joint model using a partially annotated
corpus (c.f., Section 7.4).
? We, for the first time, present a comprehen-
sive empirical study of word segmentation
and POS tagging for microblogs. The experi-
mental results demonstrated that both the mi-
croblog corpus and joint model greatly con-
tributes to training accurate models for word
segmentation and POS tagging.
The remainder of this paper is organized as fol-
lows. Section 2 reviews related work. Section 3
discusses the task of lexical normalization and in-
troduces terminology. Section 4 presents our mi-
croblog corpus and results of our corpus analysis.
Section 5 presents an overview of our joint model
with lexical normalization, and Sections 6 and 7
provide details of the model. Section 8 presents
experimental results and discussions, and Section
9 presents concluding remarks.
2 Related Work
Researchers have recently developed various mi-
croblog corpora annotated with rich linguistic in-
formation. Gimpel et al. (2011) and Foster et
al. (2011) annotated English microblog posts with
3Please contact the first author for this corpus.
POS tags. Han and Baldwin (2011) released a mi-
croblog corpus annotated with normalized forms
of words. A Chinese microblog corpus annotated
with word boundaries was developed for SIGHAN
bakeoff (Duan et al., 2012). However, there are
no microblog corpora annotated with word bound-
aries, POS tags, and normalized sentences.
There has been a surge of interest in lexical nor-
malization with the advent of microblogs (Han and
Baldwin, 2011; Liu et al., 2012; Han et al., 2012;
Wang and Ng, 2013; Zhang et al., 2013; Ling et
al., 2013; Yang and Eisenstein, 2013; Wang et al.,
2013). However, these studies did not address en-
hancing word segmentation.
Wang et al. (2013) proposed a method of joint
ill-spelled word recognition and word segmenta-
tion. With their method, informal spellings are
merely recognized and not normalized. Therefore,
they did not investigate how to exploit the infor-
mation obtainable from well-spelled equivalents
to increase word segmentation accuracy.
Some studies also explored integrating the lexi-
cal normalization process into word segmentation
and POS tagging (Ikeda et al., 2009; Sasano et al.,
2013). A strength of our joint model is that it uses
rich character-level and word-level features used
in state-of-the-art models of joint word segmenta-
tion and POS tagging (Kudo et al., 2004; Neubig
et al., 2011; Kaji and Kitsuregawa, 2013). Thanks
to these features, our model performed much bet-
ter than Sasano et al.?s system, which is the only
publicly available system that jointly conducts lex-
ical normalization, in the experiments (see Section
8). Another advantage is that our model can be
trained on a partially annotated corpus. Further-
more, we present a comprehensive evaluation in
terms of precision and recall on our microblog cor-
pus. Such an evaluation has not been conducted in
previous work due to the lack of annotated cor-
pora.4
3 Lexical Normalization Task
This section explains the task of lexical normal-
ization addressed in this paper. Since lexical nor-
malization is a relatively new research topic, there
are no precise definitions of a lexical normaliza-
tion task that are widely accepted by researchers.
4Very recently, Saito et al. (2014) conducted similar em-
pirical evaluation on microblog corpus. However, they used
biased dataset, in which every sentence includes at least one
ill-spelled words.
100
Table 1: Examples of our target ill-spelled words
and their well-spelled equivalents. Phonemes are
shown between slashes. English translations are
provided in parentheses.
Ill-spelled word Well-spelled equivalent
??? /sugee/ ??? /sugoi/ (great)
?? /modoro/ ??? /modorou/ (going to return)
?????? /umaiiii/ ??? /umai/ (yummy)
Therefore, it is important to clarify our task setting
before discussing our joint model.
3.1 Target ill-spelled words
Many studies on lexical normalization have
pointed out that phonological factors are deeply
involved in the process of deriving ill-spelled
words. Xia et al. (2006) investigated a Chi-
nese chat corpus and reported that 99.2% of the
ill-spelled words were derived by phonetic map-
ping from well-spelled equivalents. Wang and
Ng (2013) analyzed 200 Chinese messages from
Weibo and 200 English SMS messages from the
NUS SMS corpus (How and Kan, 2005). Their
analysis revealed that most ill-spelled words were
derived from well-spelled equivalents based on
pronunciation similarity.
On top of these investigations, we focused on
ill-spelled words that are derived by phonologi-
cal mapping from well-spelled words by assum-
ing that such ill-spelled words are dominant in
Japanese microblogs as well. We also assume
that these ill-spelled words can be normalized into
well-spelled equivalents on a word-to-word basis,
as assumed in a previous study (Han and Baldwin,
2011). The validity of these two assumptions is
empirically assessed in Section 4.
Table 1 lists examples of our target ill-spelled
words, their well-spelled equivalents, and their
phonemes. The ill-spelled word in the first row
is formed by changing the continuous two vowels
from /oi/ to /ee/. This type of change in pronun-
ciation is often observed in Japanese spoken lan-
guage. The second row presents contractions. The
last vowel character ??? /u/ of the well-spelled
word is dropped. The third row illustrates word
lengthening. The ill-spelled word is derived by re-
peating the vowel character ??? /i/.
3.2 Terminology
We now introduce the terminology that will be
used throughout the remainder of this paper. The
term word surface form (or surface form for short)
is used to refer to the word form observed in an
actual text, while word normal form (or normal
form) refers to the normalized word form. Note
that surface forms of well-spelled words are al-
ways identical to their normal forms.
It is possible that the word surface form and nor-
mal form have distinct POS tags, although they are
identical in most cases. Take the ill-spelled word ?
??? /modoro/ as an example (the second row of
Table 1). According to the JUMAN POS tag set,5
POS of its surface form is CONTRACTED VERB,
while that of its normal form is VERB.6 To handle
such a case, we strictly distinguish between these
two POS tags by referring to them as surface POS
tags and normal POS tags, respectively.
Given these terms, the tasks addressed in this
paper can be stated as follows. Word segmenta-
tion is a task of segmenting a sentence into a se-
quence of word surface forms, and POS tagging
is a task of providing surface POS tags. The task
of joint lexical normalization, word segmentation,
and POS tagging is to map a sentence into a se-
quence of quadruplets: word surface form, surface
POS tag, normal form, and normal POS tag.
4 Microblog Corpus
This section introduces our microblog corpus. We
first explain the process of developing the corpus
then present the results of our agreement study and
corpus analysis.
4.1 Data collection and annotation
The corpus was developed by manually annotating
text messages posted to Twitter.
The posts to be annotated were collected as fol-
lows. 171,386 Japanese posts were collected using
the Twitter API7 on December 6, 2013. Among
these, 1000 posts were randomly selected then
manually split into sentences. As a result, we ob-
tained 1831 sentences as a source of the corpus.
Two human participants annotated the 1831
sentences with surface forms and surface POS
tags. Since much effort has already been done to
annotate corpora with this information, the anno-
tation process here follows the guidelines used to
5http://nlp.ist.i.kyoto-u.ac.jp/index.php?JUMAN
6In this paper, we use simplified POS tags for explana-
tion purposes. Remind that these tags are different from the
original ones defined in JUMAN POS tag set.
7https://stream.twitter.com/1.1/statuses/sample.json
101
develop such corpora in previous studies (Kuro-
hashi and Nagao, 1998; Hashimoto et al., 2011).
The two participants also annotated ill-spelled
words with their normal forms and normal POS
tags. Although this paper targets only infor-
mal phonological variations (c.f., Section 3),
other types of ill-spelled words were also anno-
tated to investigate their frequency distribution
in microblog texts. Specifically, besides infor-
mal phonological variations, spelling errors and
Twitter-specific abbreviations were annotated. As
a result, 833 ill-spelled words were identified (Ta-
ble 2). They were all annotated with normal forms
and normal POS tags.
4.2 Agreement study
We investigated the inter-annotator agreement to
check the reliability of the annotation. During the
annotation process, the two participants collabo-
ratively annotated around 90% of the sentences
(specifically, 1647 sentences) with normal forms
and normal POS tags, and elaborated an annota-
tion guideline through discussion. They then inde-
pendently annotated the remaining 184 sentences
(1431 words), which were used for the agreement
study. Our annotation guideline is shown in the
supplementary material.
We first explored the extent to which the
two participants agreed in distinguishing between
well-spelled words and ill-spelled words. For this
task, we observed Cohen?s kappa of 0.96 (almost
perfect agreement). This results show that it is
easy for humans to distinguish between these two
types of words.
Next, we investigated whether the two partici-
pants could give ill-spelled words with the same
normal forms and normal POS tags. For this pur-
pose, we regarded the normal forms and normal
POS tags annotated by one participant as goldstan-
dards and calculated precision and recall achieved
by the other participant. We observed moder-
ate agreement between the two participants: 70%
(56/80) precision and 73% (56/76) recall. We
manually analyzed the conflicted examples and
found that there were more than one acceptable
normal form in many of these cases. Therefore,
we would like to note that the precision and recall
reported above are rather pessimistic estimations.
4.3 Analysis
We conducted corpus analysis to confirm the fea-
sibility of our approach.
Table 2: Frequency distribution over three types of
ill-spelled words in corpus.
Type Frequency
Informal phonological variation 804 (92.9%)
Spelling error 27 (3.1%)
Twitter-specific abbreviation 34 (3.9%)
Total 865 (100%)
Table 2 illustrates that phonological variations
constitute a vast majority of ill-spelled words in
Japanese microblog texts. In addition, analysis
of the 804 phonological variations showed that
793 of them can be normalized into single words.
These represent the validity of the two assump-
tions we made in Section 3.1.
We then investigated whether lexical normaliza-
tion can decrease the number of out-of-vocabulary
words. For the 793 ill-spelled words, we counted
how many of their surface forms and normal
forms were not registered in the JUMAN dictio-
nary.8 The result suggests that 411 (51.8%) and
74 (9.3%) are not registered in the dictionary. This
indicates the effectiveness of lexical normalization
for decreasing out-of-vocabulary words.
5 Overview of Joint Model
This section gives an overview of our joint model
with lexical normalization for accurate word seg-
mentation and POS tagging.
5.1 Lattice-based approach
A lattice-based approach has been commonly
adopted to perform joint word segmentation and
POS tagging (Jiang et al., 2008; Kudo et al., 2004;
Kaji and Kitsuregawa, 2013). In this approach, an
input sentence is transformed into a word lattice
in which the edges are labeled with surface POS
tags (Figure 1). Given such a lattice, word seg-
mentation and POS tagging can be performed at
the same time by traversing the lattice. A discrim-
inative model is typically used for the traversal.
An advantage of this approach is that, while the
lattice can represent an exponentially large num-
ber of candidate analyses, it can be quickly tra-
versed using dynamic programming (Kudo et al.,
2004; Kaji and Kitsuregawa, 2013) or beam search
(Jiang et al., 2008). In addition, a discriminative
model allows the use of rich word-level features
to find the correct analysis.
8http://nlp.ist.i.kyoto-u.ac.jp/index.php?JUMAN
102
? ? ? ? ??
1RXQ
9HUE
1RXQ
1RXQ 1RXQ 1RXQ
3DUWLFOH6XIIL[
,QSXWVHQWHQFH: ?????? 7ROLYHLQ7RN\RPHWURSROLV
:RUGODWWLFH:
Figure 1: Example lattice (Kudo et al., 2004; Kaji
and Kitsuregawa, 2013). Circle and arrow repre-
sent node and edge, respectively. Bold edges rep-
resent correct analysis.
,QSXWVHQWHQFH:??????? 1RWWRXQGHUVWDQG(QJOLVK
:RUGODWWLFH:
1RUPDOL]HGVHQWHQFH:?? ??? ??
1RXQ
??1RXQ
1RXQ
??1RXQ
6XIIL[
??6XIIL[
9HUE
??? 9HUE
?? ?? ? ??
3DUWLFOH
?3DUWLFOH
6XIIL[
??6XIIL[
Figure 2: Lattice used to perform joint task. Nor-
mal forms and normal POS tags are shown in
parentheses. As indicated by dotted arrows, nor-
malized sentence can be obtained by concatenat-
ing normal forms associated with edges in correct
analysis.
We propose extending the lattice-based ap-
proach to jointly perform lexical normalization,
word segmentation, and POS tagging. We trans-
form an input sentence into a word lattice in which
the edges are labeled with not only surface POS
tags but normal forms and normal POS tags (Fig-
ure 2). By traversing such a lattice, the three
tasks can be performed at the same time. This ap-
proach can not only exploit rich information ob-
tainable from word normal forms, but also achieve
efficiency similar to the original lattice-based ap-
proach.
5.2 Issues
Issues on how to develop this lattice-based ap-
proach is detailed in Sections 6 and 7.
Section 6 describes how to generate a word lat-
tice from an input sentence. This is done us-
ing a hybrid approach that combines a statistical
model and normalization dictionary. The normal-
ization dictionary is specifically a list of quadru-
Table 3: Normalization dictionary. Columns rep-
resent entry ID, surface form, surface POS, normal
form, and normal POS, respectively.
ID Surf. Surf. POS Norm. Norm. POS
A ??? ADJECTIVE ??? ADJECTIVE
B ??? ADJECTIVE ??? ADJECTIVE
C ??? VERB ??? VERB
D ?? CONTR. VERB ??? VERB
E ??? ADJECTIVE ??? ADJECTIVE
F ?????? ADJECTIVE ??? ADJECTIVE
Table 4: Tag dictionary.
ID Surf. form Surf. POS
a ??? (great) ADJECTIVE
b ??? (going to return) VERB
c ?? (gonna return) CONTR. VERB
d ??? (yummy) ADJECTIVE
plets: word surface form, surface POS tag, normal
form, and normal POS tag (Table 3).
Section 7 describes a discriminative model for
the lattice traversal. Our feature design as well as
two training methods are presented.
6 Word Lattice Generation
In this section, we first describe a method of con-
structing a normalization dictionary then present a
method of generating a word lattice from an input
sentence.
6.1 Construction of normalization dictionary
Although large-scale normalization dictionaries
are difficult to obtain, tag dictionaries, which list
pairs of word surface forms and their surface POS
tags (Table 4), are widely available in many lan-
guages including Japanese. Therefore, we use an
existing tag dictionary to construct the normaliza-
tion dictionary.
Due to space limitations, we give only a brief
overview of our construction method, omitting its
details. We note that our method uses hand-crafted
rules similar to those used in (Sasano et al., 2013);
hence, the proposal of this method is not an im-
portant contribution. To make our experimental
results reproducible, our normalization dictionary,
as well as a tool for constructing it, is released as
supplementary material.
Our method of constructing the normalization
dictionary takes three steps. The following ex-
plains each step using Tables 3 and 4 as running
examples.
103
Step 1 A tag dictionary generally contains a
small number of ill-spelled words, although well-
spelled words constitute a vast majority. We iden-
tify such ill-spelled words by using a manually-
tailored list of surface POS tags indicative of in-
formal spelling (e.g., CONTRACTED VERB). For
example, entry (c) in Table 4 is identified as an
ill-spelled word in this step.
Step 2 The tag dictionary is augmented with
normal forms and normal POS tags to construct
a small normalization dictionary. For ill-spelled
words identified in step 1, the normal forms and
normal POS tags are determined by hand-crafted
rules. For example, the normal form is derived by
appending the vowel character ??? /u/ to the sur-
face form, if the surface POS tag is CONTRACTED
VERB. This rule derives entry (D) in Table 3 from
entry (c) in Table 4. For well-spelled words, on
the other hand, the normal forms and normal POS
tags are simply set the same as the surface forms
and surface POS tags. For example, entries (A),
(C), and (E) in Table 3 are generated from entries
(a), (b), and (d) in Table 4, respectively.
Step 3 Because the normalization dictionary
constructed in step 2 contains only a few ill-
spelled words, it is expanded in this step. For this
purpose, we use hand-crafted rules to derive ill-
spelled words from the entries already registered
in the normalization dictionary. Some rules are
taken from (Sasano et al., 2013), while the others
are newly tailored. In Table 3, for example, entry
(B) is derived from entry (A) by applying the rule
that substitutes ???? /goi/ with ???? /gee/.
A small problem that arises in step 3 is how to
handle lengthened words, such as entry (F) in Ta-
ble 3. While lengthened words can be easily de-
rived using simple rules (Brody and Diakopoulos,
2011; Sasano et al., 2013), such rules infinitely
increase the number of entries because an unlim-
ited number of lengthened words can be derived
by repeating characters. To address this problem,
no lengthened words are added to the normaliza-
tion dictionary in step 3. We instead use rules
to skip repetitive characters in an input sentence
when performing dictionary match.
6.2 A hybrid approach
A word lattice is generated using both a statisti-
cal method (Kaji and Kitsuregawa, 2013) and the
normalization dictionary.
We begin by generating a word lattice which en-
codes only word surface forms and surface POS
tags (c.f., Figure 1) using the statistical method
proposed by Kaji and Kitsuregawa (2013). Inter-
ested readers may refer to their paper for details.
Each edge in the lattice is then labeled with nor-
mal forms and normal POS tags. Note that a sin-
gle edge can have more than one candidate normal
form and normal POS tag. In such a case, new
edges are accordingly added to the lattice.
The edges are labeled with normal forms and
normal POS tags in the following manner. First,
every edge is labeled with a normal form and
normal POS tag that are identical with the sur-
face form and surface POS tag. This is based on
our observation that most words are well-spelled
ones. The edge is not provided with further nor-
mal forms and normal POS tags, if the normaliza-
tion dictionary contains a well-spelled word that
has the same surface form as the edge. Otherwise,
we allow the edge to have all pairs of normal forms
and normal POS tags that are obtained by using the
normalization dictionary.
7 Discriminative Lattice Traversal
This section explains a discriminative model for
traversing the word lattice. The lattice traversal
with a discriminative model can formally be writ-
ten as
(w, t,v, s) = argmax
(w,t,v,s)?L(x)
f(x,w, t,v, s) ? ?.
Here, x denotes an input sentence, w, t, v, and s
denote a sequence of word surface forms, surface
POS tags, normal forms, and normal POS tags, re-
spectively, L(x) represents a set of candidate anal-
yses represented by the word lattice, and f(?) and
? are feature and weight vectors.
We now describe features, a decoding method,
and two training methods.
7.1 Features
We use character-level and word-level features
used for word segmentation and POS tagging in
(Kaji and Kitsuregawa, 2013). To take advan-
tage of joint model with lexical normalization, the
word-level features are extracted from not only
surface forms but also normal forms. See (Kaji
and Kitsuregawa, 2013) for the original features.
In addition, several new features are introduced
in this paper. We use the quadruplets (w
i
, t
i
, v
i
, s
i
)
104
and pairs of surface and normal POS tags (t
i
, s
i
)
as binary features to capture probable mappings
between ill-spelled words and their well-spelled
equivalents. We use another binary feature indi-
cating whether a quadruplet (w
i
, t
i
, v
i
, s
i
) is reg-
istered in the normalization dictionary. Also, we
use a bigram language model feature, which pre-
vents sentences from being normalized into un-
grammatical and/or incomprehensible ones. The
language model features are associated with nor-
malized bigrams, (v
i?1
, s
i?1
, v
i
, s
i
), and take as
the values the logarithmic frequency log
10
(f +1),
where f represents the bigram frequency (Kaji and
Kitsuregawa, 2011). Since it is difficult to obtain
a precise value of f , it is approximated by the fre-
quency of the surface bigram, (w
i?1
, t
i?1
, w
i
, t
i
),
calculated from a large raw corpus automatically
analyzed using a system of joint word segmenta-
tion and POS tagging. See Section 8.1 for the raw
corpus and system used in the experiments.
7.2 Decoding
It is easy to find the best analysis (w, t,v, s)
among the candidates represented by the word lat-
tice. Although we use several new features, we
can still locate the best analysis by using the same
dynamic programming algorithm as in previous
studies (Kudo et al., 2004; Kaji and Kitsuregawa,
2013).
7.3 Training on a fully annotated corpus
It is straightforward to train the joint model pro-
vided with a fully annotated corpus, which is la-
beled with word surface forms, surface POS tags,
normal forms, and normal POS tags.
We use structured perceptron (Collins, 2002)
for the training (Algorithm 1). The training be-
gins by initializing ? as a zero vector (line 1).
It then reads the annotated corpus C (line 2-9).
Given a training example, (x,w, t,v, s) ? C, the
algorithm locates the best analysis, ( ?w, ?t, ?v, ?s),
based on the current weight vector (line 4). If
the best analysis differs from the oracle analy-
sis, (w, t,v, s), the weight vector is updated (line
5-7). After going through the annotated corpus
m times (m=10 in our experiment), the averaged
weight vector is returned (line 10).
7.4 Training on a partially annotated corpus
Although the training with the perceptron algo-
rithm requires a fully annotated corpus, it is labor-
intensive to fully annotate sentences. This consid-
Algorithm 1 Perceptron training
1: ? ? 0
2: for i = 1 . . . m do
3: for (x,w, t,v, s) ? C do
4: ( ?w, ?t, ?v, ?s)? DECODING(x, ?)
5: if (w, t,v,s) 6= ( ?w, ?t, ?v, ?s) then
6: ? ? ? + f (x,w, t,v, s)? f (x, ?w, ?t, ?v, ?s)
7: end if
8: end for
9: end for
10: return AVERAGE(?)
Algorithm 2 Latent perceptron training
1: ? ? 0
2: for i = 1 . . . m do
3: for (x,w, t) ? C? do
4: ( ?w, ?t, ?v, ?s)? DECODING(x, ?)
5: (w, t, ?v, ?s)? CONSTRAINEDDECODING(x,?)
6: if w 6= ?w or t 6= ?t then
7: ? ? ? + f (x,w, t, ?v, ?s)? f (x, ?w, ?t, ?v, ?s)
8: end if
9: end for
10: end for
11: return AVERAGE(?)
eration motivates us to explore training our model
with less supervision. We specifically explore us-
ing a corpus annotated with only word boundaries
and POS tags.
We use the latent perceptron algorithm (Sun et
al., 2013) to train the joint model from such a par-
tially annotated corpus (Algorithm 2). In this sce-
nario, a training example is a sentence x paired
with a sequence of word surface forms w and sur-
face POS tags t (c.f., line 3). Similarly to the
perceptron algorithm, we locate the best analy-
sis ( ?w, ?t, ?v, ?s) for a given training example, (line
4). We also locate the best analysis, (w, t, ?v, ?s),
among those having the same surface forms w and
surface POS tags t as the training example (line
5). If the surface forms and surface POS tags of
the former analysis differ from the annotations of
the training example, parameter is updated by re-
garding the latter analysis as an oracle (line 6-8).
8 Experiments
We conducted experiments to investigate how the
microblog corpus and joint model contribute to
improving accuracy of word segmentation and
POS tagging in the microblog domain.
8.1 Setting
We constructed the normalization dictionary from
the JUMAN dictionary 7.0.9 While JUMAN dic-
9http://nlp.ist.i.kyoto-u.ac.jp/EN/index.php?JUMAN
105
tionary contains 750,156 entries, the normaliza-
tion dictionary contains 112,458,326 entries.
Some features taken from the previous study
(Kaji and Kitsuregawa, 2013) are induced using a
tag dictionary. For this we used two tag dictionar-
ies. One is JUMAN dictionary 7.0 and the other
is a tag dictionary constructed by listing surface
forms and surface POS tags in the normalization
dictionary.
To compute the language model features, one
billion sentences from Twitter posts were analyzed
using MeCab 0.996.10 We used all bigrams ap-
pearing at least 10 times in the auto-analyzed sen-
tences.
8.2 Results of word segmentation and POS
tagging
We first investigated the performance of models
trained on an existing annotated corpus form news
texts. For this experiment, our joint model as
well as three state-of-the-art models (Kudo et al.,
2004)11(Neubig et al., 2011)12(Kaji and Kitsure-
gawa, 2013) were trained on Kyoto University
Text corpus 4.0 (Kurohashi and Nagao, 1998).
Since this training corpus is not annotated with
normal forms and normal POS tags, our model
was trained using the latent perceptron. Table
5 summarizes the word-level F
1
-scores (Kudo et
al., 2004) on our microblog corpus. The two
columns represent the results for word segmenta-
tion (Seg) and joint word segmentation and POS
tagging (Seg+Tag), respectively.
We also conducted 5-fold crossvalidation on
our microblog corpus to evaluate performance im-
provement when these models are trained on mi-
croblog texts (Table 6). In addition to the models
in Table 5, results of a rule-based system (Sasano
et al., 2013)13 and our joint model trained using
the perceptron algorithm are also presented. No-
tice that Proposed and Proposed (latent) repre-
sent our model trained using perceptron and latent
perceptron, respectively.
From Tables 5 and 6, as expected, we see that
the models trained on news texts performed poorly
on microblog texts, while their performance sig-
nificantly boosted when trained on the microblog
texts. This demonstrates the importance of corpus
annotation. An exception was Kudo04. Its perfor-
10https://code.google.com/p/mecab
11https://code.google.com/p/mecab
12http://www.phontron.com/kytea/
13http://nlp.ist.i.kyoto-u.ac.jp/index.php?JUMAN
Table 5: Performance of models trained on the
news articles.
Seg Seg+Tag
Kudo04 81.8 71.0
Neubig11 80.5 69.1
Kaji13 83.2 73.1
Proposed (latent) 83.0 73.9
mance improved only slightly, even when it was
trained on the microblog texts. We believe this is
because their model uses dictionary-based rules to
prune candidate analyses; thus, it could not per-
form well in the microblog domain, where out-of-
vocabulary words are abundant.
Table 6 also illustrates that our joint models
achieved F
1
-score better than the state-of-the-art
models trained on the microblog texts. This
shows that modeling the derivation process of ill-
spelled words makes training easier. We con-
ducted bootstrap resampling (with 1000 samples)
to investigate the significance of the improvements
achieved with our joint model. The results showed
that all improvements over the baselines were sta-
tistically significant (p < 0.01). The difference
between Proposed and Proposed (latent) were
also statistically significant (p < 0.01).
The results of Proposed (latent) are interest-
ing. Table 5 illustrates that our joint model per-
forms well even when it is trained on a news cor-
pus that rarely contains ill-spelled words and is
not at all annotated with normal forms and nor-
mal POS tags. This indicates the robustness of our
training method and the importance of modeling
word derivation process in the microblog domain.
In Table 6, we observed that Proposed (latent),
which uses less supervision, performed better than
Proposed. The reason for this will be examined
later.
In summary, we can conclude that both the mi-
croblog corpus and joint model significantly con-
tribute to training accurate models for word seg-
mentation and POS tagging in the microblog do-
main.
8.3 Results of lexical normalization
While the main goal with this study was to en-
hance word segmentation and POS tagging in the
microblog domain, it is interesting to explore how
well our joint model can normalize ill-spelled
words.
Table 7 illustrates precision, recall, and F
1
-
score for the lexical normalization task. To put
106
Table 6: Results of 5-fold cross-validation on mi-
croblog corpus.
Seg Seg+Tag
Kudo04 82.7 71.7
Neubig11 88.6 75.9
Kaji13 90.9 82.1
Sasano13 82.7 73.3
Proposed 91.3 83.2
Proposed (latent) 91.4 83.7
Table 7: Results of lexical normalization task in
terms of precision, recall, and F
1
-score.
Precision Recall F
1
Neubig11 69.2 35.9 47.3
Proposed 77.1 44.6 56.6
Proposed (latent) 53.7 24.7 33.9
the results into context, we report on the baseline
results of a tagging model proposed by Neubig et
al. (2011). This baseline conducts lexical normal-
ization by regarding it as two independent tagging
tasks (i.e., tasks of tagging normal forms and nor-
mal POS tags). The result of the baseline model is
also obtained using 5-fold crossvalidation.
Table 7 illustrates that Proposed performed sig-
nificantly better than the simple tagging model,
Neubig11. This suggests the effectiveness of our
joint model. On the other hand, Proposed (latent)
performed poorly in this task. From this result, we
can argue that Proposed (latent) can achieve su-
perior performance in word segmentation and POS
tagging (Table 6) because it gave up correctly nor-
malizing ill-spelled words, focusing on word seg-
mentation and POS tagging.
The experimental results so far suggest the fol-
lowing strategy for training our joint model. If ac-
curacy of word segmentation and POS tagging is
the main concern, we can use the latent percep-
tron. This approach has the advantage of being
able to use a partially annotated corpus. On the
other hand, if performance of lexical normaliza-
tion is crucial, we have to use the standard percep-
tron algorithm.
8.4 Error analysis
We manually analyzed erroneous outputs and ob-
served several tendencies.
We found that a word lattice sometimes missed
the correct output. Such an error was, for example,
observed in a sentence including many ill-spelled
words, e.g., ?????????????? (be
nervous about what other people think!)?, where
the part ???????? is in ill-spelled words.
Improving the lattice generation algorithm is con-
sidered necessary to achieve further performance
gain.
Even if the correct analysis appears in the word
lattice, our model sometimes failed to handle
ill-spelled words, incorrectly analyzing them as
out-of-vocabulary words. For example, the pro-
posed method treated the phrase ????????
(snack time)? as a single out-of-vocabulary word,
even though the correct analysis was found in the
word lattice. More sophisticated features would
be required to accurately distinguish between ill-
spelled and out-of-vocabulary words.
9 Conclusion and Future Work
We presented our attempts towards developing an
accurate model for word segmentation and POS
tagging in the microblog domain. To this end, we,
for the first time, developed an annotated corpus
of microblogs. We also proposed a joint model
with lexical normalization to handle orthographic
diversity in the microblog text. Intensive exper-
iments demonstrated that we could successfully
improve the performance of word segmentation
and POS tagging on microblog texts. We believe
this study will have a large practical impact on a
various research areas that target microblogs.
One limitation of our approach is that it cannot
handle certain types of ill-spelled words. For ex-
ample, the current model cannot handle the cases
in which there are no one-to-one-mappings be-
tween well-spelled and ill-spelled words. Also,
our model cannot handle spelling errors, which
are considered relatively frequent in the microblog
than news domains. The treatment of these prob-
lems would require further research.
Another future research is to speed-up our
model. Since the joint model with lexical normal-
ization significantly increases the search space,
it is much slower than the original lattice-based
model for word segmentation and POS tagging.
Acknowledgments
The authors would like to thank Naoki Yoshinaga
for his help in developing the microblog corpus as
well as fruitful discussions.
107
References
Samuel Brody and Nicholas Diakopoulos. 2011.
Cooooooooooooooollllllllllllll!!!!!!!!!!!!!! using
word lengthening to detect sentiment in microblogs.
In Proceedings of EMNLP, pages 562?570.
Michael Collins. 2002. Discriminative training meth-
ods for hidden Markov models: Theory and exper-
iments with perceptron algorithms. In Proceedings
of EMNLP, pages 1?8.
Huiming Duan, Zhifang Sui, Ye Tian, and Wenjie Li.
2012. The CIPS-SIGHAN CLP 2012 Chinese word
segmentation on microblog corpora bakeoff. In Pro-
ceedings of the Second CIPS-SIGHAN Joint Conr-
erence on Chinese Language Processing, pages 35?
40.
Jennifer Foster, Ozlem Cetinoglu, Joachim Wagner,
Joseph Le Roux, Stephen Hogan, Joakim Nivre,
Deirdre Hogan, and Josef van Genabith. 2011.
#hardtoparse: POS tagging and parsing the twit-
terverse. In Proceedings of AAAI Workshop on
Analysing Microtext, pages 20?25.
Kevin Gimpel, Nathan Schneider, Brendan O?Connor,
Dipanjan Das, Daniel Mills, Jacob Eisenstein,
Michael Heilman, Dani Yogatama, Jeffrey Flanigan,
and Noah A. Smith. 2011. Part-of-speech tagging
for twitter: Annotation, features, and experiments.
In Proceedings of ACL, pages 42?47.
Bo Han and Timothy Baldwin. 2011. Lexical normal-
ization of short text messages: Makin sens a #twitter.
In Proceedings of ACL, pages 368?378.
Bo Han, Paul Cook, and Timothy Baldwin. 2012. Au-
tomatically constructing a normalisation dictionary
for microblogs. In Proceedings of EMNLP-CoNLL,
pages 421?432.
Chikara Hashimoto, Sadao Kurohashi, Daisuke Kawa-
hara, Keiji Shinzato, and Masaaki Nagata. 2011.
Construction of a blog corpus with syntac-
tic, anaphoric, and semantic annotations (in
Japanese). Journal of Natural Language Process-
ing, 18(2):175?201.
Yijiu How and Min-Yen Kan. 2005. Optimizing pre-
dictive text entry for short message service on mo-
bile phones. In Proceedings of Human Computer
Interfaces International.
Kazushi Ikeda, Tadashi Yanagihara, Kazunori Mat-
sumoto, and Yasuhiro Takishima. 2009. Unsuper-
vised text normalization approach for morphological
analysis of blog documents. In Proceedings of Aus-
tralasian Joint Conference on Advances in Artificial
Intelligence, pages 401?411.
Wenbin Jiang, Haitao Mi, and Qun Liu. 2008. Word
lattice reranking for Chinese word segmentation and
part-of-speech tagging. In Proceedings of Coling,
pages 385?392.
Long Jiang, Mo Yu, Ming Zhou, Xiaohua Liu, and
Tiejun Zhao. 2011. Target-dependent Twitter sen-
timent classification. In Proceedings of ACL, pages
151?160.
Nobuhiro Kaji and Masaru Kitsuregawa. 2011. Split-
ting noun compounds via monolingual and bilingual
paraphrasing: A study on Japanese Katakana words.
In Proceedings of EMNLP, pages 959?969.
Nobuhiro Kaji and Masaru Kitsuregawa. 2013. Effi-
cient word lattice generation for joint word segmen-
tation and POS tagging in Japanese. In Proceedings
of IJCNLP, pages 153?161.
Taku Kudo, Kaoru Yamamoto, and Yuji Matsumoto.
2004. Applying conditional random fields to
Japanese morphological analysis. In Proceedings of
EMNLP, pages 230?237.
Sadao Kurohashi and Makoto Nagao. 1998. Building a
Japanese parsed corpus while improving the parsing
system. In Proceedings of LREC, pages 719?724.
Wang Ling, Chris Dyer, Alan W Black, and Isabel
Trancoso. 2013. Paraphrasing 4 microblog normal-
ization. In Proceedings of EMNLP, pages 73?84.
Xiaohua Liu, Ming Zhou, Xiangyang Zhou,
Zhongyang Fu, and Furu Wei. 2012. Joint
inference of named entity recognition and normal-
ization for tweets. In Proceedings of ACL, pages
526?535.
Graham Neubig, Yousuke Nakata, and Shinsuke Mori.
2011. Pointwise prediction for robust adaptable
Japanese morphological analysis. In Proceedings of
ACL, pages 529?533.
Itsumi Saito, Kugatsu Sadamitsu, Hisako Asano, and
Yoshihiro Matsuo. 2014. Morphological analysis
for Japanese noisy text based on character-level and
word-level normalization. In Proceedings of COL-
ING, pages 1773?1782.
Takeshi Sakaki, Makoto Okazaki, and Yutaka Matsuo.
2010. Earthquak shakes Twitter users: real-time
event detection by social sensors. In Proceedings
of WWW, pages 851?860.
Ryohei Sasano, Sadao Kurohashi, and Manabu Oku-
mura. 2013. A simple approach to unknown word
processing in Japanese morphological analysis. In
Proceedings of IJCNLP, pages 162?170.
Xu Sun, Takuya Matsuzaki, and Wenjie Li. 2013.
Latent structured perceptrons for large-scale learn-
ing with hidden information. IEEE Transactions
on Knowledge and Data Engineering, 25(9):2063?
2075.
Aobo Wang and Min-Yen Kan. 2013. Mining informal
language from Chinese microtext: Joint word recog-
nition and segmentation. In Proceedings of ACL,
pages 731?741.
108
Pidong Wang and Hwee Tou Ng. 2013. A beam-search
decoder for normalization of social media text with
application to machine translation. In Proceedings
of NAACL, pages 471?481.
Aobo Wang, Min-Yen Kan, Daniel Andrade, Takashi
Onishi, and Kai Ishikawa. 2013. Chinese informal
word normalization: an experimental study. In Pro-
ceedings of IJCNLP, pages 127?135.
Yunqing Xia, Kam-Fai Wong, and Wenjie Li. 2006. A
phonetic-based approach to Chinese chat text nor-
malization. In Proceedings of ACL, pages 993?
1000.
Yi Yang and Jacob Eisenstein. 2013. A log-linear
model for unsupervised text normalization. In Pro-
ceedings of EMNLP, pages 61?72.
Congle Zhang, Tyler Baldwin, Howard Ho, Benny
Kimelfeld, and Yunyao Li. 2013. Adaptive parser-
centric text normalization. In Proceedings of ACL,
pages 1159?1168.
109
Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 485?494,
Uppsala, Sweden, 11-16 July 2010. c?2010 Association for Computational Linguistics
Efficient Staggered Decoding for Sequence Labeling
Nobuhiro Kaji Yasuhiro Fujiwara Naoki Yoshinaga Masaru Kitsuregawa
Institute of Industrial Science,
The University of Tokyo,
4-6-1, Komaba, Meguro-ku, Tokyo, 153-8505 Japan
{kaji,fujiwara,ynaga,kisture}@tkl.iis.u-tokyo.ac.jp
Abstract
The Viterbi algorithm is the conventional
decoding algorithm most widely adopted
for sequence labeling. Viterbi decoding
is, however, prohibitively slow when the
label set is large, because its time com-
plexity is quadratic in the number of la-
bels. This paper proposes an exact decod-
ing algorithm that overcomes this prob-
lem. A novel property of our algorithm is
that it efficiently reduces the labels to be
decoded, while still allowing us to check
the optimality of the solution. Experi-
ments on three tasks (POS tagging, joint
POS tagging and chunking, and supertag-
ging) show that the new algorithm is sev-
eral orders of magnitude faster than the
basic Viterbi and a state-of-the-art algo-
rithm, CARPEDIEM (Esposito and Radi-
cioni, 2009).
1 Introduction
In the past decade, sequence labeling algorithms
such as HMMs, CRFs, and Collins? perceptrons
have been extensively studied in the field of NLP
(Rabiner, 1989; Lafferty et al, 2001; Collins,
2002). Now they are indispensable in a wide range
of NLP tasks including chunking, POS tagging,
NER and so on (Sha and Pereira, 2003; Tsuruoka
and Tsujii, 2005; Lin and Wu, 2009).
One important task in sequence labeling is how
to find the most probable label sequence from
among all possible ones. This task, referred to as
decoding, is usually carried out using the Viterbi
algorithm (Viterbi, 1967). The Viterbi algorithm
has O(NL2) time complexity,1 where N is the
input size and L is the number of labels. Al-
though the Viterbi algorithm is generally efficient,
1The first-order Markov assumption is made throughout
this paper, although our algorithm is applicable to higher-
order Markov models as well.
it becomes prohibitively slow when dealing with
a large number of labels, since its computational
cost is quadratic in L (Dietterich et al, 2008).
Unfortunately, several sequence-labeling prob-
lems in NLP involve a large number of labels. For
example, there are more than 40 and 2000 labels
in POS tagging and supertagging, respectively
(Brants, 2000; Matsuzaki et al, 2007). These
tasks incur much higher computational costs than
simpler tasks like NP chunking. What is worse,
the number of labels grows drastically if we jointly
perform multiple tasks. As we shall see later,
we need over 300 labels to reduce joint POS tag-
ging and chunking into the single sequence label-
ing problem. Although joint learning has attracted
much attention in recent years, how to perform de-
coding efficiently still remains an open problem.
In this paper, we present a new decoding algo-
rithm that overcomes this problem. The proposed
algorithm has three distinguishing properties: (1)
It is much more efficient than the Viterbi algorithm
when dealing with a large number of labels. (2) It
is an exact algorithm, that is, the optimality of the
solution is always guaranteed unlike approximate
algorithms. (3) It is automatic, requiring no task-
dependent hyperparameters that have to be manu-
ally adjusted.
Experiments evaluate our algorithm on three
tasks: POS tagging, joint POS tagging and chunk-
ing, and supertagging2. The results demonstrate
that our algorithm is up to several orders of mag-
nitude faster than the basic Viterbi algorithm and a
state-of-the-art algorithm (Esposito and Radicioni,
2009); it makes exact decoding practical even in
labeling problems with a large label set.
2 Preliminaries
We first provide a brief overview of sequence la-
beling and introduce related work.
2Our implementation is available at http://www.tkl.iis.u-
tokyo.ac.jp/?kaji/staggered
485
2.1 Models
Sequence labeling is the problem of predicting la-
bel sequence y = {yn}Nn=1 for given token se-
quence x = {xn}Nn=1. This is typically done by
defining a score function f(x,y) and locating the
best label sequence: ymax = argmax
y
f(x,y).
The form of f(x,y) is dependent on the learn-
ing model used. Here, we introduce two models
widely used in the literature.
Generative models HMM is the most famous
generative model for labeling token sequences
(Rabiner, 1989). In HMMs, the score function
f(x,y) is the joint probability distribution over
(x,y). If we assume a one-to-one correspondence
between the hidden states and the labels, the score
function can be written as:
f(x,y) = log p(x,y)
= log p(x|y) + log p(y)
=
N
?
n=1
log p(xn|yn)+
N
?
n=1
log p(yn|yn?1).
The parameters log p(xn|yn) and log p(yn|yn?1)
are usually estimated using maximum likelihood
or the EM algorithm. Since parameter estimation
lies outside the scope of this paper, a detailed de-
scription is omitted.
Discriminative models Recent years have seen
the emergence of discriminative training methods
for sequence labeling (Lafferty et al, 2001; Tasker
et al, 2003; Collins, 2002; Tsochantaridis et al,
2005). Among them, we focus on the perceptron
algorithm (Collins, 2002). Although we do not
discuss the other discriminative models, our algo-
rithm is equivalently applicable to them. The ma-
jor difference between those models lies in param-
eter estimation; the decoding process is virtually
the same.
In the perceptron, the score function f(x,y) is
given as f(x,y) = w ? ?(x,y) where w is the
weight vector, and ?(x,y) is the feature vector
representation of the pair (x,y). By making the
first-order Markov assumption, we have
f(x,y) = w ? ?(x,y)
=
N
?
n=1
K
?
k=1
wk?k(x, yn?1, yn),
where K = |?(x,y)| is the number of features, ?k
is the k-th feature function, and wk is the weight
corresponding to it. Parameter w can be estimated
in the same way as in the conventional perceptron
algorithm. See (Collins, 2002) for details.
2.2 Viterbi decoding
Given the score function f(x,y), we have to lo-
cate the best label sequence. This is usually per-
formed by applying the Viterbi algorithm. Let
?(yn) be the best score of the partial label se-
quence ending with yn. The idea of the Viterbi
algorithm is to use dynamic programming to com-
pute ?(yn). In HMMs, ?(yn) can be can be de-
fined as
max
y
n?1
{?(yn?1) + log p(yn|yn?1)} + log p(xn|yn).
Using this recursive definition, we can evaluate
?(yn) for all yn. This results in the identification
of the best label sequence.
Although the Viterbi algorithm is commonly
adopted in past studies, it is not always efficient.
The computational cost of the Viterbi algorithm is
O(NL2), where N is the input length and L is
the number of labels; it is efficient enough if L
is small. However, if there are many labels, the
Viterbi algorithm becomes prohibitively slow be-
cause of its quadratic dependence on L.
2.3 Related work
To the best of our knowledge, the Viterbi algo-
rithm is the only algorithm widely adopted in the
NLP field that offers exact decoding. In other
communities, several exact algorithms have al-
ready been proposed for handling large label sets.
While they are successful to some extent, they de-
mand strong assumptions that are unusual in NLP.
Moreover, none were challenged with standard
NLP tasks.
Felzenszwalb et al (2003) presented a fast
inference algorithm for HMMs based on the as-
sumption that the hidden states can be embed-
ded in a grid space, and the transition probabil-
ity corresponds to the distance on that space. This
type of probability distribution is not common in
NLP tasks. Lifshits et al (2007) proposed a
compression-based approach to speed up HMM
decoding. It assumes that the input sequence is
highly repetitive. Amongst others, CARPEDIEM
(Esposito and Radicioni, 2009) is the algorithm
closest to our work. It accelerates decoding by
assuming that the adjacent labels are not strongly
correlated. This assumption is appropriate for
486
some NLP tasks. For example, as suggested in
(Liang et al, 2008), adjacent labels do not provide
strong information in POS tagging. However, the
applicability of this idea to other NLP tasks is still
unclear.
Approximate algorithms, such as beam search
or island-driven search, have been proposed for
speeding up decoding. Tsuruoka and Tsujii (2005)
proposed easiest-first deterministic decoding. Sid-
diqi and Moore (2005) presented the parameter ty-
ing approach for fast inference in HMMs. A simi-
lar idea was applied to CRFs as well (Cohn, 2006;
Jeong et al, 2009).
In general, approximate algorithms have the ad-
vantage of speed over exact algorithms. However,
both types of algorithms are still widely adopted
by practitioners, since exact algorithms have mer-
its other than speed. First, the optimality of the so-
lution is always guaranteed. It is hard for most of
the approximate algorithms to even bound the er-
ror rate. Second, approximate algorithms usually
require hyperparameters, which control the trade-
off between accuracy and efficiency (e.g., beam
width), and these have to be manually adjusted.
On the other hand, most of the exact algorithms,
including ours, do not require such a manual ef-
fort.
Despite these advantages, exact algorithms are
rarely used when dealing with a large number of
labels. This is because exact algorithms become
considerably slower than approximate algorithms
in such situations. The paper presents an exact al-
gorithm that avoids this problem; it provides the
research community with another option for han-
dling a lot of labels.
3 Algorithm
This section presents the new decoding algorithm.
The key is to reduce the number of labels ex-
amined. Our algorithm locates the best label se-
quence by iteratively solving labeling problems
with a reduced label set. This results in signifi-
cant time savings in practice, because each itera-
tion becomes much more efficient than solving the
original labeling problem. More importantly, our
algorithm always obtains the exact solution. This
is because the algorithm allows us to check the op-
timality of the solution achieved by using only the
reduced label set.
In the following discussions, we restrict our fo-
cus to HMMs for presentation clarity. Extension to
A A A A
B B B B
C
D
C
D
C
D
C
D
E E E E
F F F F
G G G G
H H H H
(a)
A
B
C
D
A A
B
A
B
C
D
(b)
Figure 1: (a) An example of a lattice, where the
letters {A, B, C, D, E, F, G, H} represent labels
associated with nodes. (b) The degenerate lattice.
the perceptron algorithm is presented in Section 4.
3.1 Degenerate lattice
We begin by introducing the degenerate lattice,
which plays a central role in our algorithm. Con-
sider the lattice in Figure 1(a). Following conven-
tion, we regard each path on the lattice as a label
sequence. Note that the label set is {A, B, C, D,
E, F, G, H}. By aggregating several nodes in the
same column of the lattice, we can transform the
original lattice into a simpler form, which we call
the degenerate lattice (Figure 1(b)).
Let us examine the intuition behind the degen-
erate lattice. Aggregating nodes can be viewed as
grouping several labels into a new one. Here, a
label is referred to as an active label if it is not ag-
gregated (e.g., A, B, C, and D in the first column
of Figure 1(b)), and otherwise as an inactive label
(i.e., dotted nodes). The new label, which is made
by grouping the inactive labels, is referred to as
a degenerate label (i.e., large nodes covering the
dotted ones). Two degenerate labels can be seen
as equivalent if their corresponding inactive label
sets are the same (e.g., degenerate labels in the first
and the last column). In this approach, each path
of the degenerate lattice can also be interpreted as
a label sequence. In this case, however, the label to
be assigned is either an active label or a degenerate
label.
We then define the parameters associated with
degenerate label z. For reasons that will become
clear later, they are set to the maxima among the
parameters of the inactive labels:
log p(x|z) = max
y??I(z)
log p(x|y?), (1)
log p(z|y) = max
y??I(z)
log p(y?|y), (2)
log p(y|z) = max
y??I(z)
log p(y|y?), (3)
log p(z|z?) = max
y??I(z),y???I(z?)
log p(y?|y??), (4)
487
A A A A
B B B B
C
D
C
D
C
D
C
D
E E E E
F F F F
G G G G
H H H H
(a)
A
B
C
D
A A
B
A
B
C
D
(b)
Figure 2: (a) The path y = {A, E, G, C} of the
original lattice. (b) The path z of the degenerate
lattice that corresponds to y.
where y is an active label, z and z? are degenerate
labels, and I(z) denotes one-to-one mapping from
z to its corresponding inactive label set.
The degenerate lattice has an important prop-
erty which is the key to our algorithm:
Lemma 1. If the best path of the degenerate lat-
tice does not include any degenerate label, it is
equivalent to the best path of the original lattice.
Proof. Let zmax be the best path of the degenerate
lattice. Our goal is to prove that if zmax does not
include any degenerate label, then
?y ? Y, log p(x,y) ? log p(x,zmax) (5)
where Y is the set of all paths on the original lat-
tice. We prove this by partitioning Y into two dis-
joint sets: Y
0
and Y
1
, where Y
0
is the subset of
Y appearing in the degenerate lattice. Notice that
zmax ? Y0. Since zmax is the best path of the
degenerate lattice, we have
?y ? Y
0
, log p(x,y) ? log p(x,zmax).
The equation holds when y = zmax. We next ex-
amine the label sequence y such that y ? Y
1
. For
each path y ? Y
1
, there exists a unique path z on
the degenerate lattice that corresponds to y (Fig-
ure 2). Therefore, we have
?y ? Y
1
, ?z ? Z, log p(x,y) ? log p(x,z)
< log p(x,zmax)
where Z is the set of all paths of the degenerate
lattice. The inequality log p(x,y) ? log p(x,z)
can be proved by using Equations (1)-(4). Using
these results, we can complete (5).
A A A A
(a)
A A
B
A
B
A
BB
(b)
A A
B
C
D
A
B
A
B
C
D
B
C
D
C
D
(c)
Figure 3: (a) The best path of the initial degenerate
lattice, which is denoted by the line, is located. (b)
The active labels are expanded and the best path is
searched again. (c) The best path without degen-
erate labels is obtained.
3.2 Staggered decoding
Nowwe can describe our algorithm, which we call
staggered decoding. The algorithm successively
constructs degenerate lattices and checks whether
the best path includes degenerate labels. In build-
ing each degenerate lattice, labels with high prob-
ability p(y), estimated from training data, are pref-
erentially selected as the active label; the expecta-
tion is that such labels are likely to belong to the
best path. The algorithm is detailed as follows:
Initialization step The algorithm starts by build-
ing a degenerate lattice in which there is only
one active label in each column. We select la-
bel y with the highest p(y) as the active label.
Search step The best path of the degenerate lat-
tice is located (Figure 3(a)). This is done
by using the Viterbi algorithm (and pruning
technique, as we describe in Section 3.3). If
the best path does not include any degenerate
label, we can terminate the algorithm since it
is identical with the best path of the original
lattice according to Lemma 1. Otherwise, we
proceed to the next step.
Expansion step We double the number of the ac-
tive labels in the degenerate lattice. The new
active labels are selected from the current in-
active label set in descending order of p(y).
If the inactive label set becomes empty, we
simply reconstructed the original lattice. Af-
ter expanding the active labels, we go back to
the previous step (Figure 3(b)). This proce-
dure is repeated until the termination condi-
tion in the search step is satisfied, i.e., the best
path has no degenerate label (Figure 3(c)).
Compared to the Viterbi algorithm, staggered
decoding requires two additional computations for
488
training. First, we have to estimate p(y) so as to
select active labels in the initialization and expan-
sion step. Second, we have to compute the pa-
rameters regarding degenerate labels according to
Equations (1)-(4). Both impose trivial computa-
tion costs.
3.3 Pruning
To achieve speed-up, it is crucial that staggered
decoding efficiently performs the search step. For
this purpose, we can basically use the Viterbi algo-
rithm. In earlier iterations, the Viterbi algorithm is
indeed efficient because the label set to be han-
dled is much smaller than the original one. In later
iterations, however, our algorithm drastically in-
creases the number of labels, making Viterbi de-
coding quite expensive.
To handle this problem, we propose a method of
pruning the lattice nodes. This technique is moti-
vated by the observation that the degenerate lattice
shares many active labels with the previous itera-
tion. In the remainder of Section3.3, we explain
the technique by taking the following steps:
? Section 3.3.1 examines a lower bound l such
that l ? max
y
log p(x,y).
? Section 3.3.2 examines the maximum score
MAX(yn) in case token xn takes label yn:
MAX(yn) = max
y?
n
=y
n
log p(x,y?).
? Section 3.3.3 presents our pruning procedure.
The idea is that if MAX(yn) < l, then the
node corresponding to yn can be removed
from consideration.
3.3.1 Lower bound
Lower bound l can be trivially calculated in the
search step. This can be done by retaining the
best path among those consisting of only active
labels. The score of that path is obviously the
lower bound. Since the search step is repeated un-
til the termination criteria is met, we can update
the lower bound at every search step. As the it-
eration proceeds, the degenerate lattice becomes
closer to the original one, so the lower bound be-
comes tighter.
3.3.2 Maximum score
The maximum score MAX(yn) can be computed
from the original lattice. Let ?(yn) be the best
score of the partial label sequence ending with yn.
Presuming that we traverse the lattice from left to
right, ?(yn) can be defined as
max
y
n?1
{?(yn?1) + log p(yn|yn?1)} + log p(xn|yn).
If we traverse the lattice from right to left, an anal-
ogous score ??(yn) can be defined as
log p(xn|yn) + max
y
n+1
{??(yn+1) + log p(yn|yn+1)}.
Using these two scores, we have
MAX(yn) = ?(yn) + ??(yn) ? log p(xn|yn).
Notice that updating ?(yn) or ??(yn) is equivalent
to the forward or backward Viterbi algorithm, re-
spectively.
Although it is expensive to compute ?(yn) and
??(yn), we can efficiently estimate their upper
bounds. Let ?(yn) and ??(yn) be scores analogous
to ?(yn) and ??(yn) that are computed using the
degenerate lattice. We have ?(yn) ? ?(yn) and
??(yn) ? ??(yn), by following similar discussions
as raised in the proof of Lemma 1. Therefore, we
can still check whether MAX(yn) is smaller than l
by using ?(yn) and ??(yn):
MAX(yn) = ?(yn) + ??(yn)? log p(xn|yn)
? ?(yn) + ??(yn) ? log p(xn|yn)
< l.
For the sake of simplicity, we assume that yn is an
active label. Although we do not discuss the other
cases, our pruning technique is also applicable to
them. We just point out that, if yn is an inactive
label, then there exists a degenerate label zn in the
n-th column such that yn ? I(zn), and we can use
?(zn) and ??(zn) instead of ?(yn) and ??(yn).
We compute ?(yn) and ??(yn) by using the
forward and backward Viterbi algorithm, respec-
tively. In the search step immediately following
initialization, we perform the forward Viterbi al-
gorithm to find the best path, that is, ?(yn) is
updated for all yn. In the next search step, the
backward Viterbi algorithm is carried out, and
??(yn) is updated. In the succeeding search steps,
these updates are alternated. As the algorithm pro-
gresses, ?(yn) and ??(yn) become closer to ?(yn)
and ??(yn).
3.3.3 Pruning procedure
We make use of the bounds in pruning the lattice
nodes. To do this, we keep the values of l, ?(yn)
489
and ??(yn). They are set as l = ?? and ?(yn) =
??(yn) = ? in the initialization step, and are up-
dated in the search step. The lower bound l is up-
dated at the end of the search step, while ?(yn)
and ??(yn) can be updated during the running of
the Viterbi algorithm. When ?(yn) or ??(yn) is
changed, we check whether MAX(yn) < l holds
and the node is pruned if the condition is met.
3.4 Analysis
We provide here a theoretical analysis of staggered
decoding. In the following proofs, L, V , and N
represent the number of original labels, the num-
ber of distinct tokens, and the length of input token
sequence, respectively. To simplify the discussion,
we assume that log
2
L is an integer (e.g., L = 64).
We first introduce three lemmas:
Lemma 2. Staggered decoding requires at most
(log
2
L + 1) iterations to terminate.
Proof. We have 2m?1 active labels in the m-th
search step (m = 1, 2 . . . ), which means we have
L active labels and no degenerate labels in the
(log
2
L + 1)-th search step. Therefore, the algo-
rithm always terminates within (log
2
L + 1) itera-
tions.
Lemma 3. The number of degenerate labels is
log
2
L.
Proof. Since we create one new degenerate label
in all but the last expansion step, we have log
2
L
degenerate labels.
Lemma 4. The Viterbi algorithm requires O(L2+
LV ) memory space and has O(NL2) time com-
plexity.
Proof. Since we need O(L2) and O(LV ) space to
keep the transition and emission probability ma-
trices, we need O(L2 + LV ) space to perform
the Viterbi algorithm. The time complexity of the
Viterbi algorithm is O(NL2) since there are NL
nodes in the lattice and it takes O(L) time to eval-
uate the score of each node.
The above statements allow us to establish our
main results:
Theorem 1. Staggered decoding requires O(L2+
LV ) memory space.
Proof. Since we have L original labels and log
2
L
degenerate labels, staggered decoding requires
O((L+log
2
L)2+(L+log
2
L)V ) = O(L2+LV )
A A A A
(a)
A A
B
A
B
A
B
(b)
A A
B
C
D
A
B
A
B
C
D
(c)
Figure 4: Staggered decoding with column-wise
expansion: (a) The best path of the initial degen-
erate lattice, which does not pass through the de-
generate label in the first column. (b) Column-
wise expansion is performed and the best path is
searched again. Notice that the active label in the
first column is not expanded. (c) The final result.
memory space to perform Viterbi decoding in the
search step.
Theorem 2. Staggered decoding has O(N) best
case time complexity and O(NL2)worst case time
complexity.
Proof. To perform the m-th search step, staggered
decoding requires the order of O(N4m?1) time
because we have 2m?1 active labels. Therefore, it
has O(
?M
m=1 N4
m?1) time complexity if it termi-
nates after the M -th search step. In the best case,
M = 1, the time complexity is O(N). In the worst
case, M = log
2
L + 1, the time complexity is the
order of O(NL2) because
?
log
2
L+1
m=1 N4
m?1 <
4
3
NL2.
Theorem 1 shows that staggered decoding
asymptotically requires the same order of mem-
ory space as the Viterbi algorithm. Theorem 2 re-
veals that staggered decoding has the same order
of time complexity as the Viterbi algorithm even
in the worst case.
3.5 Heuristic techniques
We present two heuristic techniques for further
speeding up our algorithm.
First, we can initialize the value of lower bound
l by selecting a path from the original lattice in
some way, and then computing the score of that
path. In our experiments, we use the path lo-
cated by the left-to-right deterministic decoding
(i.e., beam search with a beam width of 1). Al-
though this method requires an additional cost to
locate the path, it is very effective in practice. If
l is initialized in this manner, the best case time
complexity of our algorithm becomes O(NL).
490
The second technique is for the expansion step.
Instead of the expansion technique described in
Section 3.2, we can expand the active labels in a
heuristic manner to keep the number of active la-
bels small:
Column-wise expansion step We double the
number of the active labels in the column
only if the best path of the degenerate lattice
passes through the degenerate label of that
column (Figure 4).
A drawback of this strategy is that the algorithm
requires N(log
2
L+1) iterations in the worst case.
As the result, we can no longer derive a reasonable
upper bound for the time complexity. Neverthe-
less, column-wise expansion is highly effective in
practice as we will demonstrate in the experiment.
Note that Theorem 1 still holds true even if we use
column-wise expansion.
4 Extension to the Perceptron
The discussion we have made so far can be applied
to perceptrons. This can be clarified by comparing
the score functions f(x,y). In HMMs, the score
function can be written as
N
?
n=1
{
log(xn|yn) + log(yn|yn?1)
}
.
In perceptrons, on the other hand, it is given as
N
?
n=1
{
?
k
w1k?
1
k(x, yn) +
?
k
w2k?
2
k(x, yn?1, yn)
}
where we explicitly distinguish the unigram fea-
ture function ?1k and bigram feature function ?2k.
Comparing the form of the two functions, we can
see that our discussion on HMMs can be extended
to perceptrons by substituting
?
k w
1
k?
1
k(x, yn)
and
?
k w
2
k?
2
k(x, yn?1, yn) for log p(xn|yn) and
log p(yn|yn?1).
However, implementing the perceptron algo-
rithm is not straightforward. The problem is
that it is difficult, if not impossible, to compute
?
k w
1
k?
1
k(x, y) and
?
k w
2
k?
2
k(x, y, y
?) offline be-
cause they are dependent on the entire token se-
quence x, unlike log p(x|y) and log p(y|y?). Con-
sequently, we cannot evaluate the maxima analo-
gous to Equations (1)-(4) offline either.
For unigram features, we compute the maxi-
mum, maxy
?
k w
1
k?
1
k(x, y), as a preprocess in
the initialization step (cf. Equation (1)). This pre-
process requires O(NL) time, which is negligible
compared with the cost required by the Viterbi al-
gorithm.
Unfortunately, we cannot use the same tech-
nique for computing maxy,y?
?
k w
2
k?
2
k(x, y, y
?)
because a similar computation would take
O(NL2) time (cf. Equation (4)). For bigram fea-
tures, we compute its upper bound offline. For ex-
ample, the following bound was proposed by Es-
posito and Radicioni (2009):
max
y,y?
?
k
w2k?
2
k(x, y, y
?) ? max
y,y?
?
k
w2k?(0 < w
2
k)
where ?(?) is the delta function and the summa-
tions are taken over all feature functions associated
with both y and y?. Intuitively, the upper bound
corresponds to an ideal case in which all features
with positive weight are activated.3 It can be com-
puted without any task-specific knowledge.
In practice, however, we can compute better
bounds based on task-specific knowledge. The
simplest case is that the bigram features are inde-
pendent of the token sequence x. In such a situ-
ation, we can trivially compute the exact maxima
offline, as we did in the case of HMMs. Fortu-
nately, such a feature set is quite common in NLP
problems and we could use this technique in our
experiments. Even if bigram features are depen-
dent on x, it is still possible to compute better
bounds if several features are mutually exclusive,
as discussed in (Esposito and Radicioni, 2009).
Finally, it is worth noting that we can use stag-
gered decoding in training perceptrons as well, al-
though such application lies outside the scope of
this paper. The algorithm does not support train-
ing acceleration for other discriminative models.
5 Experiments and Discussion
5.1 Setting
The proposed algorithm was evaluated with three
tasks: POS tagging, joint POS tagging and chunk-
ing (called joint tagging for short), and supertag-
ging. To reduce joint tagging into a single se-
quence labeling problem, we produced the labels
by concatenating the POS tag and the chunk tag
(BIO format), e.g., NN/B-NP. In the two tasks
other than supertagging, the input token is the
word. In supertagging, the token is the pair of the
word and its oracle POS tag.
3We assume binary feature functions.
491
Table 1: Decoding speed (sent./sec).
POS tagging Joint tagging Supertagging
VITERBI 4000 77 1.1
CARPEDIEM 8600 51 0.26
SD 8800 850 121
SD+C-EXP. 14,000 1600 300
The data sets we used for the three experiments
are the Penn TreeBank (PTB) corpus, CoNLL
2000 corpus, and an HPSG treebank built from the
PTB corpus (Matsuzaki et al, 2007). We used sec-
tions 02-21 of PTB for training, and section 23 for
testing. The number of labels in the three tasks is
45, 319 and 2602, respectively.
We used the perceptron algorithm for train-
ing. The models were averaged over 10 itera-
tions (Collins, 2002). For features, we basically
followed previous studies (Tsuruoka and Tsujii,
2005; Sha and Pereira, 2003; Ninomiya et al,
2006). In POS tagging, we used unigrams of the
current and its neighboring words, word bigrams,
prefixes and suffixes of the current word, capital-
ization, and tag bigrams. In joint tagging, we also
used the same features. In supertagging, we used
POS unigrams and bigrams in addition to the same
features other than capitalization.
As the evaluation measure, we used the average
decoding speed (sentences/sec) to two significant
digits over five trials. To strictly measure the time
spent for decoding, we ignored the preprocessing
time, that is, the time for loading the model file
and converting the features (i.e., strings) into inte-
gers. We note that the accuracy was comparable to
the state-of-the-art in the three tasks: 97.08, 93.21,
and 91.20% respectively.
5.2 Results and discussions
Table 1 presents the performance of our algo-
rithm. SD represents the proposed algorithm with-
out column-wise expansion, while SD+C-EXP.
uses column-wise expansion. For comparison, we
present the results of two baseline algorithms as
well: VITERBI and CARPEDIEM (Esposito and
Radicioni, 2009). In almost all settings, we see
that both of our algorithms outperformed the other
two. We also find that SD+C-EXP. performed con-
sistently better than SD. This indicates the effec-
tiveness of column-wise expansion.
Following VITERBI, CARPEDIEM is the most
relevant algorithm, for sequence labeling in NLP,
as discussed in Section 2.3. However, our results
Table 2: The average number of iterations.
POS tagging Joint tagging Supertagging
SD 6.02 8.15 10.0
SD+C-EXP. 6.12 8.62 10.6
Table 3: Training time.
POS tagging Joint tagging Supertagging
VITERBI 100 sec. 20 min. 100 hour
SD+C-EXP. 37 sec. 1.5 min. 5.3 hour
demonstrated that CARPEDIEM worked poorly in
two of the three tasks. We consider this is because
the transition information is crucial for the two
tasks, and the assumption behind CARPEDIEM is
violated. In contrast, the proposed algorithms per-
formed reasonably well for all three tasks, demon-
strating the wide applicability of our algorithm.
Table 2 presents the average iteration num-
bers of SD and SD+C-EXP. We can observe
that the two algorithms required almost the same
number of iterations on average, although the
iteration number is not tightly bounded if we
use column-wise expansion. This indicates that
SD+C-EXP. virtually avoided performing extra it-
erations, while heuristically restricting active label
expansion.
Table 3 compares the training time spent by
VITERBI and SD+C-EXP. Although speeding up
perceptron training is a by-product, it is interest-
ing to see that our algorithm is in fact effective at
reducing the training time as well. The result also
indicates that the speed-up is more significant at
test time. This is probably because the model is
not predictive enough at the beginning of training,
and the pruning is not that effective.
5.3 Comparison with approximate algorithm
Table 4 compares two exact algorithms (VITERBI
and SD+E-XP.) with beam search, which is the ap-
proximate algorithm widely adopted for sequence
labeling in NLP. For this experiment, the beam
width, B, was exhaustively calibrated: we tried B
= {1, 2, 4, 8, ...} until the beam search achieved
comparable accuracy to the exact algorithms, i.e.,
the difference fell below 0.1 in our case.
We see that there is a substantial difference in
the performance between VITERBI and BEAM.
On the other hand, SD+C-EXP. reached speeds
very close to those of BEAM. In fact, they
achieved comparable performance in our exper-
iment. These results demonstrate that we could
successfully bridge the gap in the performance be-
492
Table 4: Comparison with beam search (sent./sec).
POS tagging Joint tagging Supertagging
VITERBI 4000 77 1.1
SD+C-EXP. 14,000 1600 300
BEAM 18,000 2400 180
tween exact and approximate algorithms, while re-
taining the advantages of exact algorithms.
6 Relation to coarse-to-fine approach
Before concluding remarks, we briefly examine
the relationship between staggered decoding and
coarse-to-fine PCFG parsing (2006). In coarse-to-
fine parsing, the candidate parse trees are pruned
by using the parse forest produced by a coarse-
grained PCFG. Since the degenerate label can be
interpreted as a coarse-level label, one may con-
sider that staggered decoding is an instance of
coarse-to-fine approach. While there is some re-
semblance, there are at least two essential differ-
ences. First, coarse-to-fine approach is a heuristic
pruning, that is, it is not an exact algorithm. Sec-
ond, our algorithm does not always perform de-
coding at the fine-grained level. It is designed to
be able to stop decoding at the coarse-level.
7 Conclusions
The sequence labeling algorithm is indispensable
to modern statistical NLP. However, the Viterbi
algorithm, which is the standard decoding algo-
rithm in NLP, is not efficient when we have to
deal with a large number of labels. In this paper
we presented staggered decoding, which provides
a principled way of resolving this problem. We
consider that it is a real alternative to the Viterbi
algorithm in various NLP tasks.
An interesting future direction is to extend the
proposed technique to handle more complex struc-
tures than the Markov chains, including semi-
Markov models and factorial HMMs (Sarawagi
and Cohen, 2004; Sutton et al, 2004). We hope
this work opens a new perspective on decoding al-
gorithms for a wide range of NLP problems, not
just sequence labeling.
Acknowledgement
We wish to thank the anonymous reviewers for
their helpful comments, especially on the com-
putational complexity of our algorithm. We also
thank Yusuke Miyao for providing us with the
HPSG Treebank data.
References
Thorsten Brants. 2000. TnT - a statistical part-of-
speech tagger. In Proceedings of ANLP, pages 224?
231.
Eugene Charniak, Mark Johnson, Micha Elsner, Joseph
Austerweil, David Ellis, Isaac Haxton, Catherine
Hill, R. Shrivaths, Jeremy Moore, Michael Pozar,
and Theresa Vu. 2006. Multi-level coarse-to-fine
PCFG parsing. In Proceedings of NAACL, pages
168?175.
Trevor Cohn. 2006. Efficient inference in large con-
ditional random fields. In Proceedings of ECML,
pages 606?613.
Michael Collins. 2002. Discriminative training meth-
ods for hidden Markov models: Theory and exper-
iments with perceptron algorithms. In Proceedings
of EMNLP, pages 1?8.
Thomas G. Dietterich, Pedro Domingos, Lise Getoor,
Stephen Muggleton, and Prasad Tadepalli. 2008.
Structured machine learning: the next ten years.
Machine Learning, 73(1):3?23.
Roberto Esposito and Daniele P. Radicioni. 2009.
CARPEDIEM: Optimizing the Viterbi algorithm
and applications to supervised sequential learning.
Jorunal of Machine Learning Research, 10:1851?
1880.
Pedro F. Felzenszwalb, Daniel P. Huttenlocher, and
Jon M. Kleinberg. 2003. Fast algorithms for large-
state-space HMMs with applications to Web usage
analysis. In Proceedings of NIPS, pages 409?416.
Minwoo Jeong, Chin-Yew Lin, and Gary Geunbae Lee.
2009. Efficient inference of CRFs for large-scale
natural language data. In Proceedings of ACL-
IJCNLP Short Papers, pages 281?284.
John Lafferty, Andrew McCallum, and Fernand
Pereira. 2001. Conditional random fields: Prob-
abilistic models for segmenting and labeling se-
quence data. In Proceedings of ICML, pages 282?
289.
Percy Liang, Hal Daume? III, and Dan Klein. 2008.
Structure compilation: Trading structure for fea-
tures. In Proceedings of ICML, pages 592?599.
Yury Lifshits, ShayMozes, OrenWeimann, andMichal
Ziv-Ukelson. 2007. Speeding up HMM decod-
ing and training by exploiting sequence repetitions.
Computational Pattern Matching, pages 4?15.
Dekang Lin and Xiaoyun Wu. 2009. Phrae clustering
for discriminative training. In Proceedings of ACL-
IJCNLP, pages 1030?1038.
493
Takuya Matsuzaki, Yusuke Miyao, and Jun?ichi Tsu-
jii. 2007. Efficient HPSG parsing with supertagging
and CFG-filtering. In Proceedings of IJCAI, pages
1671?1676.
Takashi Ninomiya, TakuyaMatsuzaki, Yoshimasa Tsu-
ruoka, Yusuke Miyao, and Jun?ichi Tsujii. 2006.
Extremely lexicalized models for accurate and fast
HPSG parsing. In Proceedings of EMNLP, pages
155?163.
Lawrence R. Rabiner. 1989. A tutorial on hidden
Markov models and selected applications in speech
recognition. In Proceedings of The IEEE, pages
257?286.
Sunita Sarawagi and Willian W. Cohen. 2004. Semi-
Markov conditional random fields for information
extraction. In Proceedings of NIPS, pages 1185?
1192.
Fei Sha and Fernando Pereira. 2003. Shallow parsing
with conditional random fields. In Proceedings of
HLT-NAACL, pages 134?141.
Sajid M. Siddiqi and Andrew W. Moore. 2005. Fast
inference and learning in large-state-space HMMs.
In Proceedings of ICML, pages 800?807.
Charles Sutton, Khashayar Rohanimanesh, and An-
drew McCallum. 2004. Dynamic conditional ran-
dom fields: Factorized probabilistic models for la-
beling and segmenting sequence data. In Proceed-
ings of ICML.
Ben Tasker, Carlos Guestrin, and Daphe Koller. 2003.
Max-margin Markov networks. In Proceedings of
NIPS, pages 25?32.
Ioannis Tsochantaridis, Thorsten Joachims, Thomas
Hofmann, and Yasemin Altun. 2005. Large margin
methods for structured and interdependent output
variables. Journal of Machine Learning Research,
6:1453?1484.
Yoshimasa Tsuruoka and Jun?ichi Tsujii. 2005. Bidi-
rectional inference with the easiest-first strategy
for tagging sequence data. In Proceedings of
HLT/EMNLP, pages 467?474.
Andrew J. Viterbi. 1967. Error bounds for convo-
lutional codes and an asymeptotically optimum de-
coding algorithm. IEEE Transactios on Information
Theory, 13(2):260?267.
494
