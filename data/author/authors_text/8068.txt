Proceedings of Human Language Technology Conference and Conference on Empirical Methods in Natural Language
Processing (HLT/EMNLP), pages 73?80, Vancouver, October 2005. c?2005 Association for Computational Linguistics
A Discriminative Matching Approach to Word Alignment
Ben Taskar Simon Lacoste-Julien Dan Klein
Computer Science Division, EECS Department
University of California, Berkeley
Berkeley, CA 94720
Abstract
We present a discriminative, large-
margin approach to feature-based
matching for word alignment. In this
framework, pairs of word tokens re-
ceive a matching score, which is based
on features of that pair, including mea-
sures of association between the words,
distortion between their positions, sim-
ilarity of the orthographic form, and so
on. Even with only 100 labeled train-
ing examples and simple features which
incorporate counts from a large unla-
beled corpus, we achieve AER perfor-
mance close to IBM Model 4, in much
less time. Including Model 4 predic-
tions as features, we achieve a relative
AER reduction of 22% in over inter-
sected Model 4 alignments.
1 Introduction
The standard approach to word alignment from
sentence-aligned bitexts has been to construct
models which generate sentences of one lan-
guage from the other, then fitting those genera-
tive models with EM (Brown et al, 1990; Och
and Ney, 2003). This approach has two primary
advantages and two primary drawbacks. In its
favor, generative models of alignment are well-
suited for use in a noisy-channel translation sys-
tem. In addition, they can be trained in an un-
supervised fashion, though in practice they do
require labeled validation alignments for tuning
model hyper-parameters, such as null counts or
smoothing amounts, which are crucial to pro-
ducing alignments of good quality. A primary
drawback of the generative approach to align-
ment is that, as in all generative models, explic-
itly incorporating arbitrary features of the in-
put is difficult. For example, when considering
whether to align two words in the IBM models
(Brown et al, 1990), one cannot easily include
information about such features as orthographic
similarity (for detecting cognates), presence of
the pair in various dictionaries, similarity of the
frequency of the two words, choices made by
other alignment systems on this sentence pair,
and so on. While clever models can implicitly
capture some of these information sources, it
takes considerable work, and can make the re-
sulting models quite complex. A second draw-
back of generative translation models is that,
since they are learned with EM, they require
extensive processing of large amounts of data
to achieve good performance. While tools like
GIZA++ (Och and Ney, 2003) do make it eas-
ier to build on the long history of the generative
IBM approach, they also underscore how com-
plex high-performance generative models can,
and have, become.
In this paper, we present a discriminative ap-
proach to word alignment. Word alignment is
cast as a maximum weighted matching problem
(Cormen et al, 1990) in which each pair of words
(e
j
, f
k
) in a sentence pair (e, f) is associated
with a score s
jk
(e, f) reflecting the desirability
of the alignment of that pair. The alignment
73
for the sentence pair is then the highest scoring
matching under some constraints, for example
the requirement that matchings be one-to-one.
This view of alignment as graph matching is
not, in itself, new: Melamed (2000) uses com-
petitive linking to greedily construct matchings
where the pair score is a measure of word-
to-word association, and Matusov et al (2004)
find exact maximum matchings where the pair
scores come from the alignment posteriors of
generative models. Tiedemann (2003) proposes
incorporating a variety of word association
?clues? into a greedy linking algorithm.
What we contribute here is a principled ap-
proach for tractable and efficient learning of the
alignment score s
jk
(e, f) as a function of ar-
bitrary features of that token pair. This con-
tribution opens up the possibility of doing the
kind of feature engineering for alignment that
has been so successful for other NLP tasks. We
first present the algorithm for large margin es-
timation of the scoring function. We then show
that our method can achieve AER rates com-
parable to unsymmetrized IBM Model 4, using
extremely little labeled data (as few as 100 sen-
tences) and a simple feature set. Remarkably,
by including bi-directional IBM Model 4 predic-
tions as features, we achieve an absolute AER
of 5.4 on the English-French Hansards alignment
task, a relative reduction of 22% in AER over in-
tersected Model 4 alignments and, to our knowl-
edge, the best AER result published on this task.
2 Algorithm
We model the alignment prediction task as a
maximum weight bipartite matching problem,
where nodes correspond to the words in the
two sentences. For simplicity, we assume here
that each word aligns to one or zero words in
the other sentence. The edge weight s
jk
repre-
sents the degree to which word j in one sentence
can translate into the word k in the other sen-
tence. Our goal is to find an alignment that
maximizes the sum of edge scores. We represent
a matching using a set of binary variables y
jk
that are set to 1 if word j is assigned to word
k in the other sentence, and 0 otherwise. The
score of an assignment is the sum of edge scores:
s(y) =
?
jk
s
jk
y
jk
. The maximum weight bi-
partite matching problem, arg maxy?Y s(y), can
be solved using well known combinatorial algo-
rithms or the following linear program:
max
z
?
jk
s
jk
z
jk
(1)
s.t.
?
j
z
jk
? 1,
?
k
z
jk
? 1, 0 ? z
jk
? 1,
where the continuous variables z
jk
correspond to
the binary variables y
jk
. This LP is guaranteed
to have integral (and hence optimal) solutions
for any scoring function s(y) (Schrijver, 2003).
Note that although the above LP can be used to
compute alignments, combinatorial algorithms
are generally more efficient. However, we use
the LP to develop the learning algorithm below.
For a sentence pair x, we denote position
pairs by x
jk
and their scores as s
jk
. We let
s
jk
= wf(x
jk
) for some user provided fea-
ture mapping f and abbreviate wf(x,y) =
?
jk
y
jk
wf(x
jk
). We can include in the fea-
ture vector the identity of the two words, their
relative positions in their respective sentences,
their part-of-speech tags, their string similarity
(for detecting cognates), and so on.
At this point, one can imagine estimating a
linear matching model in multiple ways, includ-
ing using conditional likelihood estimation, an
averaged perceptron update (see which match-
ings are proposed and adjust the weights ac-
cording to the difference between the guessed
and target structures (Collins, 2002)), or in
large-margin fashion. Conditional likelihood es-
timation using a log-linear model P (y | x) =
1
Z
w
(x)
exp{wf(x,y)} requires summing over all
matchings to compute the normalization Zw(x),
which is #P-complete (Valiant, 1979). In our
experiments, we therefore investigated the aver-
aged perceptron in addition to the large-margin
method outlined below.
2.1 Large-margin estimation
We follow the large-margin formulation of
Taskar et al (2005a). Our input is a set of
training instances {(x
i
,y
i
)}m
i=1
, where each in-
stance consists of a sentence pair x
i
and a target
74
alignment y
i
. We would like to find parameters
w that predict correct alignments on the train-
ing data:
y
i
= arg max
?y
i
?Y
i
wf(x
i
, y?
i
), ?i,
where Y
i
is the space of matchings appropriate
for the sentence pair i.
In standard classification problems, we typi-
cally measure the error of prediction, (y
i
, y?
i
),
using the simple 0-1 loss. In structured prob-
lems, where we are jointly predicting multiple
variables, the loss is often more complex. While
the F-measure is a natural loss function for this
task, we instead chose a sensible surrogate that
fits better in our framework: Hamming distance
between y
i
and y?
i
, which simply counts the
number of edges predicted incorrectly.
We use an SVM-like hinge upper bound on
the loss (y
i
, y?
i
), given by max
?y
i
?Y
i
[wf
i
(y?
i
) +

i
(y?
i
) ? wf
i
(y
i
)], where 
i
(y?
i
) = (y
i
, y?
i
), and
f
i
(y?
i
) = f(x
i
, y?
i
). Minimizing this upper bound
encourages the true alignment y
i
to be optimal
with respect to w for each instance i:
min
||w||??
?
i
max
?y
i
?Y
i
[wf
i
(y?
i
) + 
i
(y?
i
)] ? wf
i
(y
i
),
where ? is a regularization parameter.
In this form, the estimation problem is a mix-
ture of continuous optimization over w and com-
binatorial optimization over y
i
. In order to
transform it into a more standard optimization
problem, we need a way to efficiently handle the
loss-augmented inference, max
?y
i
?Y
i
[wf
i
(y?
i
) +

i
(y?
i
)]. This optimization problem has pre-
cisely the same form as the prediction prob-
lem whose parameters we are trying to learn
? max
?y
i
?Y
i
wf
i
(y?
i
) ? but with an additional
term corresponding to the loss function. Our as-
sumption that the loss function decomposes over
the edges is crucial to solving this problem. In
particular, we use weighted Hamming distance,
which counts the number of variables in which
a candidate solution y?
i
differs from the target
output y
i
, with different cost for false positives
(c+) and false negatives (c-):

i
(y?
i
) =
?
jk
[
c-y
i,jk
(1 ? y?
i,jk
) + c+y?
i,jk
(1 ? y
i,jk
)
]
=
?
jk
c-y
i,jk
+
?
jk
[c+ ? (c- + c+)y
i,jk
]y?
i,jk
.
The loss-augmented matching problem can then
be written as an LP similar to Equation 1 (with-
out the constant term
?
jk
c-y
i,jk
):
max
z
?
jk
z
i,jk
[wf(x
i,jk
) + c+ ? (c- + c+)y
i,jk
]
s.t.
?
j
z
i,jk
? 1,
?
k
z
i,jk
? 1, 0 ? z
i,jk
? 1.
Hence, without any approximations, we have a
continuous optimization problem instead of a
combinatorial one:
max
?y
i
?Y
i
wf
i
(y?
i
)+
i
(y?
i
) = d
i
+max
z
i
?Z
i
(wF
i
+c
i
)z
i
,
where d
i
=
?
jk
c-y
i,jk
is the constant term, F
i
is the appropriate matrix that has a column of
features f(x
i,jk
) for each edge jk, c
i
is the vector
of the loss terms c+ ? (c- + c+)y
i,jk
and finally
Z
i
= {z
i
:
?
j
z
i,jk
? 1,
?
k
z
i,jk
? 1, 0 ?
z
i,jk
? 1}.
Plugging this LP back into our estimation
problem, we have
min
||w||??
max
z?Z
?
i
wF
i
z
i
+ c
i
z
i
? wF
i
y
i
, (2)
where z = {z
1
, . . . , z
m
}, Z = Z
1
? . . .?Z
m
. In-
stead of the derivation in Taskar et al (2005a),
which produces a joint convex optimization
problem using Lagrangian duality, here we
tackle the problem in its natural saddle-point
form.
2.2 The extragradient method
For saddle-point problems, a well-known solu-
tion strategy is the extragradient method (Ko-
rpelevich, 1976), which is closely related to
projected-gradient methods.
The gradient of the objective in Equation 2
is given by:
?
i
F
i
(z
i
? y
i
) (with respect to w)
and F
i
w + c
i
(with respect to each z
i
). We de-
note the Euclidean projection of a vector onto
Z
i
as P
Z
i
(v) = arg minu?Z
i
||v ? u|| and pro-
jection onto the ball ||w|| ? ? as P
?
(w) =
?w/max(?, ||w||).
75
An iteration of the extragradient method con-
sists of two very simple steps, prediction:
w?t+1 = P
?
(wt + ?
k
?
i
F
i
(y
i
? zt
i
));
z?t+1
i
= P
Z
i
(zt
i
+ ?
k
(F
i
wt + c
i
));
and correction:
wt+1 = P
?
(wt + ?
k
?
i
F
i
(y
i
? z?t+1
i
));
zt+1
i
= P
Z
i
(zt
i
+ ?
k
(F
i
w?t+1 + c
i
)),
where ?
k
are appropriately chosen step sizes.
The method is guaranteed to converge linearly
to a solution w?, z? (Korpelevich, 1976; He and
Liao, 2002; Taskar et al, 2005b). Please see
www.cs.berkeley.edu/~taskar/extragradient.pdf
for more details.
The key subroutine of the algorithm is Eu-
clidean projection onto the feasible sets Z
i
. In
case of word alignment, Z
i
is the convex hull of
bipartite matchings and the problem reduces to
the much-studied minimum cost quadratic flow
problem (Bertsekas et al, 1997). The projection
problem P
Z
i
(z?
i
) is given by
min
z
?
jk
1
2
(z?
i,jk
? z
i,jk
)2
s.t.
?
j
z
i,jk
? 1,
?
k
z
i,jk
? 1, 0 ? z
i,jk
? 1.
We can now use a standard reduction of bipar-
tite matching to min cost flow by introducing a
source node connected to all the words in one
sentence and a sink node connected to all the
words in the other sentence, using edges of ca-
pacity 1 and cost 0. The original edges jk have
a quadratic cost 1
2
(z?
i,jk
? z
i,jk
)2 and capacity 1.
Now the minimum cost flow from the source to
the sink computes projection of z?
i
onto Z
i
We
use standard, publicly-available code for solving
this problem (Guerriero and Tseng, 2002).
3 Experiments
We applied this matching algorithm to word-
level alignment using the English-French
Hansards data from the 2003 NAACL shared
task (Mihalcea and Pedersen, 2003). This
corpus consists of 1.1M automatically aligned
sentences, and comes with a validation set of 39
sentence pairs and a test set of 447 sentences.
The validation and test sentences have been
hand-aligned (see Och and Ney (2003)) and are
marked with both sure and possible alignments.
Using these alignments, alignment error rate
(AER) is calculated as:
AER(A,S, P ) = 1 ? |A ? S| + |A ? P |
|A| + |S|
Here, A is a set of proposed index pairs, S is
the sure gold pairs, and P is the possible gold
pairs. For example, in Figure 1, proposed align-
ments are shown against gold alignments, with
open squares for sure alignments, rounded open
squares for possible alignments, and filled black
squares for proposed alignments.
Since our method is a supervised algorithm,
we need labeled examples. For the training data,
we split the original test set into 100 training
examples and 347 test examples. In all our ex-
periments, we used a structured loss function
(y
i
, y?
i
) that penalized false negatives 3 times
more than false positives, where 3 was picked by
testing several values on the validation set. In-
stead of selecting a regularization parameter ?
and running to convergence, we used early stop-
ping as a cheap regularization method, by set-
ting ? to a very large value (10000) and running
the algorithm for 500 iterations. We selected a
stopping point using the validation set by simply
picking the best iteration on the validation set in
terms of AER (ignoring the initial ten iterations,
which were very noisy in our experiments). All
selected iterations turned out to be in the first
50 iterations, as the algorithm converged fairly
rapidly.
3.1 Features and Results
Very broadly speaking, the classic IBM mod-
els of word-level translation exploit four primary
sources of knowledge and constraint: association
of words (all IBM models), competition between
alignments (all models), zero- or first-order pref-
erences of alignment positions (2,4+), and fer-
tility (3+). We model all of these in some way,
76
on
e of th
e
ma
jo
r
ob
je
ct
iv
es of
th
es
e
co
ns
ul
ta
ti
on
s is to
ma
ke
su
re
th
at th
e
re
co
ve
ry
be
ne
fi
ts al
l .
le
un
de
les
grands
objectifs
de
les
consultations
est
de
faire
en
sorte
que
la
relance
profite
e?galement
a`
tous
.
on
e of th
e
ma
jo
r
ob
je
ct
iv
es of
th
es
e
co
ns
ul
ta
ti
on
s is to
ma
ke
su
re
th
at th
e
re
co
ve
ry
be
ne
fi
ts al
l .
le
un
de
les
grands
objectifs
de
les
consultations
est
de
faire
en
sorte
que
la
relance
profite
e?galement
a`
tous
.
(a) Dice only (b) Dice and Distance
on
e of th
e
ma
jo
r
ob
je
ct
iv
es of
th
es
e
co
ns
ul
ta
ti
on
s is to
ma
ke
su
re
th
at th
e
re
co
ve
ry
be
ne
fi
ts al
l .
le
un
de
les
grands
objectifs
de
les
consultations
est
de
faire
en
sorte
que
la
relance
profite
e?galement
a`
tous
.
on
e of th
e
ma
jo
r
ob
je
ct
iv
es of
th
es
e
co
ns
ul
ta
ti
on
s is to
ma
ke
su
re
th
at th
e
re
co
ve
ry
be
ne
fi
ts al
l .
le
un
de
les
grands
objectifs
de
les
consultations
est
de
faire
en
sorte
que
la
relance
profite
e?galement
a`
tous
.
(c) Dice, Distance, Orthographic, and BothShort (d) All features
Figure 1: Example alignments for each successive feature set.
except fertility.1
First, and, most importantly, we want to in-
clude information about word association; trans-
lation pairs are likely to co-occur together in
a bitext. This information can be captured,
among many other ways, using a feature whose
1In principle, we can model also model fertility, by
allowing 0-k matches for each word rather than 0-1, and
having bias features on each word. However, we did not
explore this possibility.
value is the Dice coefficient (Dice, 1945):
Dice(e, f) = 2CEF (e, f)C
E
(e)C
F
(f)
Here, C
E
and C
F
are counts of word occurrences
in each language, while C
EF
is the number of
co-occurrences of the two words. With just this
feature on a pair of word tokens (which depends
only on their types), we can already make a stab
77
at word alignment, aligning, say, each English
word with the French word (or null) with the
highest Dice value (see (Melamed, 2000)), sim-
ply as a matching-free heuristic model. With
Dice counts taken from the 1.1M sentences, this
gives and AER of 38.7 with English as the tar-
get, and 36.0 with French as the target (in line
with the numbers from Och and Ney (2003)).
As observed in Melamed (2000), this use of
Dice misses the crucial constraint of competi-
tion: a candidate source word with high asso-
ciation to a target word may be unavailable for
alignment because some other target has an even
better affinity for that source word. Melamed
uses competitive linking to incorporate this con-
straint explicitly, while the IBM-style models
get this effect via explaining-away effects in EM
training. We can get something much like the
combination of Dice and competitive linking by
running with just one feature on each pair: the
Dice value of that pair?s words.2 With just a
Dice feature ? meaning no learning is needed
yet ? we achieve an AER of 29.8, between the
Dice with competitive linking result of 34.0 and
Model 1 of 25.9 given in Och and Ney (2003).
An example of the alignment at this stage is
shown in Figure 1(a). Note that most errors lie
off the diagonal, for example the often-correct
to-a` match.
IBM Model 2, as usually implemented, adds
the preference of alignments to lie near the di-
agonal. Model 2 is driven by the product of a
word-to-word measure and a (usually) Gaussian
distribution which penalizes distortion from the
diagonal. We can capture the same effect us-
ing features which reference the relative posi-
tions j and k of a pair (e
j
, f
k
). In addition to a
Model 2-style quadratic feature referencing rela-
tive position, we threw in the following proxim-
ity features: absolute difference in relative posi-
tion abs(j/|e|?k/|f |), and the square and square
root of this value. In addition, we used a con-
junction feature of the dice coefficient times the
proximity. Finally, we added a bias feature on
each edge, which acts as a threshold that allows
2This isn?t quite competitive linking, because we use
a non-greedy matching.
in
19
78
Am
er
ic
an
s
di
vo
rc
ed
1,
12
2,
00
0
ti
me
s .
en
1978
,
on
a
enregistre?
1,122,000
divorces
sur
le
continent
.
in
19
78
Am
er
ic
an
s
di
vo
rc
ed
1,
12
2,
00
0
ti
me
s .
en
1978
,
on
a
enregistre?
1,122,000
divorces
sur
le
continent
.
(a) (b)
Figure 2: Example alignments showing the ef-
fects of orthographic cognate features. (a) Dice
and Distance, (b) With Orthographic Features.
sparser, higher precision alignments. With these
features, we got an AER of 15.5 (compare to 19.5
for Model 2 in (Och and Ney, 2003)). Note that
we already have a capacity that Model 2 does
not: we can learn a non-quadratic penalty with
linear mixtures of our various components ? this
gives a similar effect to learning the variance of
the Gaussian for Model 2, but is, at least in
principle, more flexible.3 These features fix the
to-a` error in Figure 1(a), giving the alignment
in Figure 1(b).
On top of these features, we included other
kinds of information, such as word-similarity
features designed to capture cognate (and ex-
act match) information. We added a feature for
exact match of words, exact match ignoring ac-
cents, exact matching ignoring vowels, and frac-
tion overlap of the longest common subsequence.
Since these measures were only useful for long
words, we also added a feature which indicates
that both words in a pair are short. These or-
thographic and other features improved AER to
14.4. The running example now has the align-
ment in Figure 1(c), where one improvement
may be attributable to the short pair feature ? it
has stopped proposing the-de, partially because
the short pair feature downweights the score of
that pair. A clearer example of these features
making a difference is shown in Figure 2, where
both the exact-match and character overlap fea-
3The learned response was in fact close to a Gaussian,
but harsher near zero displacement.
78
tures are used.
One source of constraint which our model still
does not explicitly capture is the first-order de-
pendency between alignment positions, as in the
HMM model (Vogel et al, 1996) and IBM mod-
els 4+. The the-le error in Figure 1(c) is symp-
tomatic of this lack. In particular, it is a slightly
better pair according to the Dice value than the
correct the-les. However, the latter alignment
has the advantage that major-grands follows it.
To use this information source, we included a
feature which gives the Dice value of the words
following the pair.4 We also added a word-
frequency feature whose value is the absolute
difference in log rank of the words, discourag-
ing very common words from translating to very
rare ones. Finally, we threw in bilexical features
of the pairs of top 5 non-punctuation words in
each language.5 This helped by removing spe-
cific common errors like the residual tendency
for French de to mistakenly align to English the
(the two most common words). The resulting
model produces the alignment in Figure 1(d).
It has sorted out the the-le / the-les confusion,
and is also able to guess to-de, which is not the
most common translation for either word, but
which is supported by the good Dice value on
the following pair (make-faire).
With all these features, we got a final AER
of 10.7, broadly similar to the 8.9 or 9.7 AERs
of unsymmetrized IBM Model 4 trained on the
same data that the Dice counts were taken
from.6 Of course, symmetrizing Model 4 by in-
tersecting alignments from both directions does
yield an improved AER of 6.9, so, while our
model does do surprisingly well with cheaply ob-
tained count-based features, Model 4 does still
outperform it so far. However, our model can
4It is important to note that while our matching algo-
rithm has no first-order effects, the features can encode
such effects in this way, or in better ways ? e.g. using as
features posteriors from the HMM model in the style of
Matusov et al (2004).
5The number of such features which can be learned
depends on the number of training examples, and since
some of our experiments used only a few dozen training
examples we did not make heavy use of this feature.
6Note that the common word pair features affected
common errors and therefore had a particularly large im-
pact on AER.
Model AER
Dice (without matching) 38.7 / 36.0
Model 4 (E-F, F-E, intersected) 8.9 / 9.7/ 6.9
Discriminative Matching
Dice Feature Only 29.8
+ Distance Features 15.5
+ Word Shape and Frequency 14.4
+ Common Words and Next-Dice 10.7
+ Model 4 Predictions 5.4
Figure 3: AER on the Hansards task.
also easily incorporate the predictions of Model
4 as additional features. We therefore added
three new features for each edge: the prediction
of Model 4 in the English-French direction, the
prediction in the French-English direction, and
the intersection of the two predictions. With
these powerful new features, our AER dropped
dramatically to 5.4, a 22% improvement over the
intersected Model 4 performance.
Another way of doing the parameter estima-
tion for this matching task would have been
to use an averaged perceptron method, as in
Collins (2002). In this method, we merely run
our matching algorithm and update weights
based on the difference between the predicted
and target matchings. However, the perfor-
mance of the average perceptron learner on the
same feature set is much lower, only 8.1, not
even breaking the AER of its best single feature
(the intersected Model 4 predictions).
3.2 Scaling Experiments
We explored the scaling of our method by learn-
ing on a larger training set, which we created by
using GIZA++ intersected bi-directional Model
4 alignments for the unlabeled sentence pairs.
We then took the first 5K sentence pairs from
these 1.1M Model 4 alignments. This gave us
more training data, albeit with noisier labels.
On a 3.4GHz Intel Xeon CPU, GIZA++ took
18 hours to align the 1.1M words, while our
method learned its weights in between 6 min-
utes (100 training sentences) and three hours
(5K sentences).
79
4 Conclusions
We have presented a novel discriminative, large-
margin method for learning word-alignment
models on the basis of arbitrary features of word
pairs. We have shown that our method is suit-
able for the common situation where a moder-
ate number of good, fairly general features must
be balanced on the basis of a small amount of
labeled data. It is also likely that the method
will be useful in conjunction with a large labeled
alignment corpus (should such a set be created).
We presented features capturing a few separate
sources of information, producing alignments on
the order of those given by unsymmetrized IBM
Model 4 (using labeled training data of about
the size others have used to tune generative
models). In addition, when given bi-directional
Model 4 predictions as features, our method
provides a 22% AER reduction over intersected
Model 4 predictions alone. The resulting 5.4
AER on the English-French Hansarks task is,
to our knowledge, the best published AER fig-
ure for this training scenario (though since we
use a subset of the test set, evaluations are not
problem-free). Finally, our method scales to
large numbers of training sentences and trains
in minutes rather than hours or days for the
higher-numbered IBM models, a particular ad-
vantage when not using features derived from
those slower models.
References
D. P. Bertsekas, L. C. Polymenakos, and P. Tseng. 1997.
An e-relaxation method for separable convex cost net-
work flow problems. SIAM J. Optim., 7(3):853?870.
P. F. Brown, J. Cocke, S. A. Della Pietra, V. J. Della
Pietra, F. Jelinek, J. D. Lafferty, R. L. Mercer, and
P. S. Roossin. 1990. A statistical approach to machine
translation. Computational Linguistics, 16(2):79?85.
M. Collins. 2002. Discriminative training methods for
hidden markov models: Theory and experiments with
perceptron algorithms. In Proc. EMNLP.
T. H. Cormen, C. E. Leiserson, and R. L. Rivest. 1990.
Introduction to Algorithms. MIT Press, Cambridge,
MA.
L. R. Dice. 1945. Measures of the amount of ecologic as-
sociation between species. Journal of Ecology, 26:297?
302.
F. Guerriero and P. Tseng. 2002. Implementation
and test of auction methods for solving generalized
network flow problems with separable convex cost.
Journal of Optimization Theory and Applications,
115(1):113?144, October.
B.S. He and L. Z. Liao. 2002. Improvements of some
projection methods for monotone nonlinear variational
inequalities. JOTA, 112:111:128.
G. M. Korpelevich. 1976. The extragradient method for
finding saddle points and other problems. Ekonomika
i Matematicheskie Metody, 12:747:756.
E. Matusov, R. Zens, and H. Ney. 2004. Symmetric word
alignments for statistical machine translation. In Proc.
of COLING 2004.
I. D. Melamed. 2000. Models of translational equivalence
among words. Computational Linguistics, 26(2):221?
249.
R. Mihalcea and T. Pedersen. 2003. An evaluation ex-
ercise for word alignment. In Proceedings of the HLT-
NAACL 2003 Workshop, Building and Using parallel
Texts: Data Driven Machine Translation and Beyond,
pages 1?6, Edmonton, Alberta, Canada.
F. Och and H. Ney. 2003. A systematic comparison of
various statistical alignment models. Computational
Linguistics, 29(1):19?52.
A. Schrijver. 2003. Combinatorial Optimization: Poly-
hedra and Efficiency. Springer.
B. Taskar, V. Chatalbashev, D. Koller, and C. Guestrin.
2005a. Learning structured prediction models: a large
margin approach. In Proceedings of the International
Conference on Machine Learning.
B. Taskar, S. Lacoste-Julien, and M. Jordan. 2005b.
Structured prediction via the extragradient method.
In Proceedings of Neural Information Processing Sys-
tems.
J. Tiedemann. 2003. Combining clues for word align-
ment. In Proceedings of EACL.
L. G. Valiant. 1979. The complexity of computing the
permanent. Theoretical Computer Science, 8:189?201.
S. Vogel, H. Ney, and C. Tillmann. 1996. HMM-based
word alignment in statistical translation. In COLING
16, pages 836?841.
80
Proceedings of the Human Language Technology Conference of the North American Chapter of the ACL, pages 104?111,
New York, June 2006. c?2006 Association for Computational Linguistics
Alignment by Agreement
Percy Liang
UC Berkeley
Berkeley, CA 94720
pliang@cs.berkeley.edu
Ben Taskar
UC Berkeley
Berkeley, CA 94720
taskar@cs.berkeley.edu
Dan Klein
UC Berkeley
Berkeley, CA 94720
klein@cs.berkeley.edu
Abstract
We present an unsupervised approach to
symmetric word alignment in which two
simple asymmetric models are trained
jointly to maximize a combination of
data likelihood and agreement between
the models. Compared to the stan-
dard practice of intersecting predictions of
independently-trained models, joint train-
ing provides a 32% reduction in AER.
Moreover, a simple and efficient pair of
HMM aligners provides a 29% reduction
in AER over symmetrized IBM model 4
predictions.
1 Introduction
Word alignment is an important component of a
complete statistical machine translation pipeline
(Koehn et al, 2003). The classic approaches to un-
supervised word alignment are based on IBM mod-
els 1?5 (Brown et al, 1994) and the HMM model
(Ney and Vogel, 1996) (see Och and Ney (2003) for
a systematic comparison). One can classify these
six models into two groups: sequence-based models
(models 1, 2, and HMM) and fertility-based models
(models 3, 4, and 5).1 Whereas the sequence-based
models are tractable and easily implemented, the
more accurate fertility-based models are intractable
and thus require approximation methods which are
1IBM models 1 and 2 are considered sequence-based models
because they are special cases of HMMs with transitions that do
not depend on previous states.
difficult to implement. As a result, many practition-
ers use the complex GIZA++ software package (Och
and Ney, 2003) as a black box, selecting model 4 as
a good compromise between alignment quality and
efficiency.
Even though the fertility-based models are more
accurate, there are several reasons to consider av-
enues for improvement based on the simpler and
faster sequence-based models. First, even with
the highly optimized implementations in GIZA++,
models 3 and above are still very slow to train. Sec-
ond, we seem to have hit a point of diminishing re-
turns with extensions to the fertility-based models.
For example, gains from the new model 6 of Och
and Ney (2003) are modest. When models are too
complex to reimplement, the barrier to improvement
is raised even higher. Finally, the fertility-based
models are asymmetric, and symmetrization is com-
monly employed to improve alignment quality by
intersecting alignments induced in each translation
direction. It is therefore natural to explore models
which are designed from the start with symmetry in
mind.
In this paper, we introduce a new method for word
alignment that addresses the three issues above. Our
development is motivated by the observation that in-
tersecting the predictions of two directional models
outperforms each model alone. Viewing intersec-
tion as a way of finding predictions that both models
agree on, we take the agreement idea one step fur-
ther. The central idea of our approach is to not only
make the predictions of the models agree at test time,
but also encourage agreement during training. We
define an intuitive objective function which incor-
104
porates both data likelihood and a measure of agree-
ment between models. Then we derive an EM-like
algorithm to maximize this objective function. Be-
cause the E-step is intractable in our case, we use
a heuristic approximation which nonetheless works
well in practice.
By jointly training two simple HMM models, we
obtain 4.9% AER on the standard English-French
Hansards task. To our knowledge, this is the lowest
published unsupervised AER result, and it is com-
petitive with supervised approaches. Furthermore,
our approach is very practical: it is no harder to
implement than a standard HMM model, and joint
training is no slower than the standard training of
two HMM models. Finally, we show that word
alignments from our system can be used in a phrase-
based translation system to modestly improve BLEU
score.
2 Alignment models: IBM 1, 2 and HMM
We briefly review the sequence-based word align-
ment models (Brown et al, 1994; Och and Ney,
2003) and describe some of the choices in our
implementation. All three models are generative
models of the form p(f | e) = ?a p(a, f | e),
where e = (e1, . . . , eI) is the English sentence,
f = (f1, . . . , fJ) is the French sentence, and a =
(a1, . . . , aJ ) is the (asymmetric) alignment which
specifies the position of an English word aligned to
each French word. All three models factor in the
following way:
p(a, f | e) =
J
?
j=1
pd(aj | aj? , j)pt(fj | eaj ), (1)
where j? is the position of the last non-null-aligned
French word before position j.2
The translation parameters pt(fj | eaj ) are pa-
rameterized by an (unsmoothed) lookup table that
stores the appropriate local conditional probability
distributions. The distortion parameters pd(aj = i? |
aj? = i) depend on the particular model (we write
aj = 0 to denote the event that the j-th French word
2The dependence on aj? can in fact be implemented as a
first-order HMM (see Och and Ney (2003)).
is null-aligned):
pd(aj =0 | aj?= i) = p0
pd(aj = i? 6= 0 | aj?= i) ?
(1? p0) ?
?
?
?
?
?
1 (IBM 1)
c(i??b jIJ c) (IBM 2)
c(i??i) (HMM),
where p0 is the null-word probability and c(?) con-
tains the distortion parameters for each offset argu-
ment. We set the null-word probability p0 = 1I+1
depending on the length of the English sentence,
which we found to be more effective than using a
constant p0.
In model 1, the distortion pd(? | ?) specifies a uni-
form distribution over English positions. In model
2, pd(? | ?) is still independent of aj? , but it can now
depend on j and i? through c(?). In the HMM model,
there is a dependence on aj? = i, but only through
c(i? i?).
We parameterize the distortion c(?) using a multi-
nomial distribution over 11 offset buckets c(?
?5), c(?4), . . . , c(4), c(? 5).3 We use three sets of
distortion parameters, one for transitioning into the
first state, one for transitioning out of the last state,
and one for all other transitions. This works better
than using a single set of parameters or ignoring the
transitions at the two ends.
3 Training by agreement
To motivate our joint training approach, we first
consider the standard practice of intersecting align-
ments. While the English and French sentences
play a symmetric role in the word alignment task,
sequence-based models are asymmetric: they are
generative models of the form p(f | e) (E?F), or
p(e | f) (F?E) by reversing the roles of source and
target. In general, intersecting the alignment predic-
tions of two independently-trained directional mod-
els reduces AER, e.g., from 11% to 7% for HMM
models (Table 2). This suggests that two models
make different types of errors that can be eliminated
upon intersection. Figure 1 (top) shows a common
type of error that intersection can partly remedy. In
3For each sentence, the probability mass of each of the two
end buckets c(??5) or c(? 5) is uniformly divided among
those valid offsets.
105
In
de
pe
n
de
n
tt
ra
in
in
g
w
e
de
e
m
e
d it
in
a
dv
is
a
bl
e
t
o
a
t
t
e
n
d
t
he
m
e
e
t
in
g
a
n
d
s
o
in
fo
r
m
e
d
c
o
jo .
nous
ne
avons
pas
cru
bon
de
assister
a`
la
re?union
et
en
avons
informe?
le
cojo
en
conse?quence
.
w
e
de
e
m
e
d it
in
a
dv
is
a
bl
e
t
o
a
t
t
e
n
d
t
he
m
e
e
t
in
g
a
n
d
s
o
in
fo
r
m
e
d
c
o
jo .
nous
ne
avons
pas
cru
bon
de
assister
a`
la
re?union
et
en
avons
informe?
le
cojo
en
conse?quence
.
w
e
de
e
m
e
d it
in
a
dv
is
a
bl
e
t
o
a
t
t
e
n
d
t
he
m
e
e
t
in
g
a
n
d
s
o
in
fo
r
m
e
d
c
o
jo .
nous
ne
avons
pas
cru
bon
de
assister
a`
la
re?union
et
en
avons
informe?
le
cojo
en
conse?quence
.
E?F: 84.2/92.0/13.0 F?E: 86.9/91.1/11.5 Intersection: 97.0/86.9/7.6
Jo
in
tt
ra
in
in
g
w
e
de
e
m
e
d it
in
a
dv
is
a
bl
e
t
o
a
t
t
e
n
d
t
he
m
e
e
t
in
g
a
n
d
s
o
in
fo
r
m
e
d
c
o
jo .
nous
ne
avons
pas
cru
bon
de
assister
a`
la
re?union
et
en
avons
informe?
le
cojo
en
conse?quence
.
w
e
de
e
m
e
d it
in
a
dv
is
a
bl
e
t
o
a
t
t
e
n
d
t
he
m
e
e
t
in
g
a
n
d
s
o
in
fo
r
m
e
d
c
o
jo .
nous
ne
avons
pas
cru
bon
de
assister
a`
la
re?union
et
en
avons
informe?
le
cojo
en
conse?quence
.
w
e
de
e
m
e
d it
in
a
dv
is
a
bl
e
t
o
a
t
t
e
n
d
t
he
m
e
e
t
in
g
a
n
d
s
o
in
fo
r
m
e
d
c
o
jo .
nous
ne
avons
pas
cru
bon
de
assister
a`
la
re?union
et
en
avons
informe?
le
cojo
en
conse?quence
.
E?F: 89.9/93.6/8.7 F?E: 92.2/93.5/7.3 Intersection: 96.5/91.4/5.7
Figure 1: An example of the Viterbi output of a pair of independently trained HMMs (top) and a pair of
jointly trained HMMs (bottom), both trained on 1.1 million sentences. Rounded boxes denote possible
alignments, square boxes are sure alignments, and solid boxes are model predictions. For each model, the
overall Precision/Recall/AER on the development set is given. See Section 4 for details.
this example, COJO is a rare word that becomes a
garbage collector (Moore, 2004) for the models in
both directions. Intersection eliminates the spurious
alignments, but at the expense of recall.
Intersection after training produces alignments
that both models agree on. The joint training pro-
cedure we describe below builds on this idea by en-
couraging the models to agree during training. Con-
sider the output of the jointly trained HMMs in Fig-
ure 1 (bottom). The garbage-collecting rare word is
no longer a problem. Not only are the individual
E?F and F?E jointly-trained models better than
their independently-trained counterparts, the jointly-
trained intersected model also provides a signifi-
cant overall gain over the independently-trained in-
tersected model. We maintain both high precision
and recall.
Before we introduce the objective function for
joint training, we will write the two directional mod-
els in a symmetric way so that they share the same
106
alignment spaces. We first replace the asymmetric
alignments a with a set of indicator variables for
each potential alignment edge (i, j): z = {zij ?
{0, 1} : 1 ? i ? I, 1 ? j ? J}. Each z can be
thought of as an element in the set of generalized
alignments, where any subset of word pairs may be
aligned (Och and Ney, 2003). Sequence-based mod-
els p(a | e, f) induce a distribution over p(z | e, f)
by letting p(z | e, f) = 0 for any z that does not
correspond to any a (i.e., if z contains many-to-one
alignments).
We also introduce the more compact notation
x = (e, f) to denote an input sentence pair. We
put arbitrary distributions p(e) and p(f) to remove
the conditioning, noting that this has no effect on
the optimization problem in the next section. We
can now think of the two directional sequence-based
models as each inducing a distribution over the
same space of sentence pairs and alignments (x, z):
p1(x, z; ?1) = p(e)p(a, f | e; ?1)
p2(x, z; ?2) = p(f)p(a, e | f ; ?2).
3.1 A joint objective
In the next two sections, we describe how to jointly
train the two models using an EM-like algorithm.
We emphasize that this technique is quite general
and can be applied in many different situations
where we want to couple two tractable models over
input x and output z.
To train two models p1(x, z; ?1) and p2(x, z; ?2)
independently, we maximize the data likelihood
?
x pk(x; ?k) =
?
x
?
z pk(x, z; ?k) of each model
separately, k ? {1, 2}:
max
?1,?2
?
x
[log p1(x; ?1) + log p2(x; ?2)] . (2)
Above, the summation over x enumerates the sen-
tence pairs in the training data.
There are many possible ways to quantify agree-
ment between two models. We chose a particularly
simple and mathematically convenient measure ?
the probability that the alignments produced by the
two models agree on an example x:
?
z
p1(z | x; ?1)p2(z | x; ?2).
We add the (log) probability of agreement to the
standard log-likelihood objective to couple the two
models:
max
?1,?2
?
x
[log p1(x; ?1) + log p2(x; ?2) +
log
?
z
p1(z | x; ?1)p2(z | x; ?2)]. (3)
3.2 Optimization via EM
We first review the EM algorithm for optimizing a
single model, which consists of iterating the follow-
ing two steps:
E : q(z;x) := p(z | x; ?),
M : ?? := argmax
?
?
x,z
q(z;x) log p(x, z; ?).
In the E-step, we compute the posterior distribution
of the alignments q(z;x) given the sentence pair x
and current parameters ?. In the M-step, we use ex-
pected counts with respect to q(z;x) in the maxi-
mum likelihood update ? := ??.
To optimize the objective in Equation 3, we can
derive a similar and simple procedure. See the ap-
pendix for the derivation.
E: q(z;x) := 1Zxp1(z | x; ?1)p2(z | x; ?2),
M: ?? = argmax
?
?
x,z
q(z;x) log p1(x, z; ?1)
+
?
x,z
q(z;x) log p2(x, z; ?2),
where Zx is a normalization constant. The M-step
decouples neatly into two independent optimization
problems, which lead to single model updates using
the expected counts from q(z;x). To compute Zx in
the E-step, we must sum the product of two model
posteriors over the set of possible zs with nonzero
probability under both models. In general, if both
posterior distributions over the latent variables z
decompose in the same tractable manner, as in
the context-free grammar induction work of Klein
and Manning (2004), the summation could be
carried out efficiently, for example using dynamic
programming. In our case, we would have to sum
over the set of alignments where each word in
English is aligned to at most one word in French
and each word in French is aligned to at most one
107
word in English. Unfortunately, for even very
simple models such as IBM 1 or 2, computing the
normalization constant over this set of alignments
is a #P -complete problem, by a reduction from
counting matchings in a bipartite graph (Valiant,
1979). We could perhaps attempt to compute q us-
ing a variety of approximate probabilistic inference
techniques, for example, sampling or variational
methods. With efficiency as our main concern, we
opted instead for a simple heuristic procedure by
letting q be a product of marginals:
q(z;x) :=
?
i,j
p1(zij | x; ?1)p2(zij | x; ?2),
where each pk(zij | x; ?k) is the posterior marginal
probability of the (i, j) edge being present (or ab-
sent) in the alignment according to each model,
which can be computed separately and efficiently.
Now the new E-step only requires simple
marginal computations under each of the mod-
els. This procedure is very intuitive: edges on
which the models disagree are discounted in the E-
step because the product of the marginals p1(zij |
x; ?1)p2(zij | x; ?2) is small. Note that in general,
this new procedure is not guaranteed to increase our
joint objective. Nonetheless, our experimental re-
sults show that it provides an effective method of
achieving model agreement and leads to significant
accuracy gains over independent training.
3.3 Prediction
Once we have trained two models, either jointly
or independently, we must decide how to combine
those two models to predict alignments for new sen-
tences.
First, let us step back to the case of one model.
Typically, the Viterbi alignment argmaxz p(z | x)
is used. An alternative is to use posterior decoding,
where we keep an edge (i, j) if the marginal edge
posterior p(zij | x) exceeds some threshold 0 < ? <
1. In symbols, z = {zij = 1 : p(zij = 1 | x) ? ?}.4
Posterior decoding has several attractive advan-
tages over Viterbi decoding. Varying the threshold
? gives a natural way to tradeoff precision and re-
call. In fact, these posteriors could be used more di-
4See Matusov et al (2004) for an alternative use of these
marginals.
rectly in extracting phrases for phrase-based trans-
lation. Also, when we want to combine two mod-
els for prediction, finding the Viterbi alignment
argmaxz p1(z | x)p2(z | x) is intractable for
HMM models (by a reduction from quadratic as-
signment), and a hard intersection argmaxz1 p1(z1 |
x) ? argmaxz2 p2(z2 | x) might be too sparse.
On the other hand, we can threshold the product of
two edge posteriors quite easily: z = {zij = 1 :
p1(zij = 1 | x)p2(zij = 1 | x) ? ?}.
We noticed a 5.8% relative reduction in AER (for
our best model) by using posterior decoding with a
validation-set optimized threshold ? instead of using
hard intersection of Viterbi alignments.
4 Experiments
We tested our approach on the English-French
Hansards data from the NAACL 2003 Shared Task,
which includes a training set of 1.1 million sen-
tences, a validation set of 37 sentences, and a test set
of 447 sentences. The validation and test sentences
have been hand-aligned (see Och and Ney (2003))
and are marked with both sure and possible align-
ments. Using these alignments, alignment error rate
(AER) is calculated as:
(
1? |A ? S|+ |A ? P ||A|+ |S|
)
? 100%,
where A is a set of proposed edges, S is the sure
gold edges, and P is the possible gold edges.
As a preprocessing step, we lowercased all words.
Then we used the validation set and the first 100 sen-
tences of the test set as our development set to tune
our models. Lastly, we ran our models on the last
347 sentences of the test set to get final AER results.
4.1 Basic results
We trained models 1, 2, and HMM on the Hansards
data. Following past work, we initialized the trans-
lation probabilities of model 1 uniformly over word
pairs that occur together in some sentence pair.
Models 2 and HMM were initialized with uni-
form distortion probabilities and model 1 translation
probabilities. Each model was trained for 5 itera-
tions, using the same training regimen as in Och and
Ney (2003).
108
Model Indep. Joint Reduction
10K sentences
Model 1 27.4 23.6 13.8
Model 2 18.2 14.9 18.5
HMM 12.1 8.4 30.6
100K sentences
Model 1 21.5 19.2 10.9
Model 2 13.1 10.2 21.7
HMM 8.0 5.3 33.1
1.1M sentences
Model 1 20.0 16.5 17.5
Model 2 11.4 9.2 18.8
HMM 6.6 5.2 21.5
Table 1: Comparison of AER between independent
and joint training across different size training sets
and different models, evaluated on the development
set. The last column shows the relative reduction in
AER.
Table 1 shows a summary of the performance of
independently and jointly trained models under var-
ious training conditions. Quite remarkably, for all
training data sizes and all of the models, we see
an appreciable reduction in AER, especially on the
HMM models. We speculate that since the HMM
model provides a richer family of distributions over
alignments than either models 1 or 2, we can learn
to synchronize the predictions of the two models,
whereas models 1 and 2 have a much more limited
capacity to synchronize.
Table 2 shows the HMM models compared to
model 4 alignments produced by GIZA++ on the test
set. Our jointly trained model clearly outperforms
not only the standard HMM but also the more com-
plex IBM 4 model. For these results, the threshold
used for posterior decoding was tuned on the devel-
opment set. ?GIZA HMM? and ?HMM, indep? are
the same algorithm but differ in implementation de-
tails. The E?F and F?E models benefit a great
deal by moving from independent to joint training,
and the combined models show a smaller improve-
ment.
Our best performing model differs from standard
IBM word alignment models in two ways. First and
most importantly, we use joint training instead of
Model E?F F?E Combined
GIZA HMM 11.5 11.5 7.0
GIZA Model 4 8.9 9.7 6.9
HMM, indep 11.2 11.5 7.2
HMM, joint 6.1 6.6 4.9
Table 2: Comparison of test set AER between vari-
ous models trained on the full 1.1 million sentences.
Model I+V I+P J+V J+P
10K sentences
Model 1 29.4 27.4 22.7 23.6
Model 2 20.1 18.2 16.5 14.9
HMM 15.2 12.1 8.9 8.4
100K sentences
Model 1 22.9 21.5 18.6 19.2
Model 2 15.1 13.1 12.9 10.2
HMM 9.2 8.0 6.0 5.3
1.1M sentences
Model 1 20.0 19.4 16.5 17.3
Model 2 12.7 11.4 11.6 9.2
HMM 7.6 6.6 5.7 5.2
Table 3: Contributions of using joint training versus
independent training and posterior decoding (with
the optimal threshold) instead of Viterbi decoding,
evaluated on the development set.
independent training, which gives us a huge boost.
The second change, which is more minor and or-
thogonal, is using posterior decoding instead of
Viterbi decoding, which also helps performance for
model 2 and HMM, but not model 1. Table 3 quan-
tifies the contribution of each of these two dimen-
sions.
Posterior decoding In our results, we have tuned
our threshold to minimize AER. It turns out that
AER is relatively insensitive to the threshold as Fig-
ure 2 shows. There is a large range from 0.2 to 0.5
where posterior decoding outperforms Viterbi de-
coding.
Initialization and convergence In addition to im-
proving performance, joint training also enjoys cer-
tain robustness properties. Specialized initialization
is absolutely crucial for an independently-trained
109
 0
 2
 4
 6
 8
 10
 12
 14
 0  0.1  0.2  0.3  0.4  0.5  0.6  0.7  0.8  0.9
Pe
rfo
rm
an
ce
Posterior threshold
100-Precision
100-Recall
AER
Viterbi AER
Figure 2: The precision, recall, and AER as the
threshold is varied for posterior decoding in a jointly
trained pair of HMMs.
HMM model. If we initialize the HMM model with
uniform translation parameters, the HMM converges
to a completely senseless local optimum with AER
above 50%. Initializing the HMM with model 1 pa-
rameters alleviates this problem.
On the other hand, if we jointly train two HMMs
starting from a uniform initialization, the HMMs
converge to a surprisingly good solution. On the full
training set, training two HMMs jointly from uni-
form initialization yields 5.7% AER, only slightly
higher than 5.2% AER using model 1 initialization.
We suspect that the agreement term of the objective
forces the two HMMs to avoid many local optima
that each one would have on its own, since these lo-
cal optima correspond to posteriors over alignments
that would be very unlikely to agree. We also ob-
served that jointly trained HMMs converged very
quickly?in 5 iterations?and did not exhibit over-
fitting with increased iterations.
Common errors The major source of remaining
errors are recall errors that come from the shortcom-
ings of the HMM model. The E?F model gives 0
probability to any many-to-one alignments and the
F?E model gives 0 probability to any one-to-many
alignments. By enforcing agreement, the two mod-
els are effectively restricted to one-to-one (or zero)
alignments. Posterior decoding is in principle ca-
pable of proposing many-to-many alignments, but
these alignments occur infrequently since the poste-
riors are generally sharply peaked around the Viterbi
alignment. In some cases, however, we do get one-
to-many alignments in both directions.
Another common type of errors are precision er-
rors due to the models overly-aggressively prefer-
ring alignments that preserve monotonicity. Our
HMM model only uses 11 distortion parameters,
which means distortions are not sensitive to the lex-
ical context of the sentences. For example, in one
sentence, le is incorrectly aligned to the as a mono-
tonic alignment following another pair of correctly
aligned words, and then the monotonicity is broken
immediately following le?the. Here, the model is
insensitive to the fact that alignments following arti-
cles tend to be monotonic, but alignments preceding
articles are less so.
Another phenomenon is the insertion of ?stepping
stone? alignments. Suppose two edges (i, j) and
(i+4, j+4) have a very high probability of being in-
cluded in an alignment, but the words between them
are not good translations of each other. If the inter-
vening English words were null-aligned, we would
have to pay a big distortion penalty for jumping 4
positions. On the other hand, if the edge (i+2, j+2)
were included, that penalty would be mitigated. The
translation cost for forcing that edge is smaller than
the distortion cost.
4.2 BLEU evaluation
To see whether our improvement in AER also im-
proves BLEU score, we aligned 100K English-
French sentences from the Europarl corpus and
tested on 3000 sentences of length 5?15. Using
GIZA++ model 4 alignments and Pharaoh (Koehn
et al, 2003), we achieved a BLEU score of 0.3035.
By using alignments from our jointly trained HMMs
instead, we get a BLEU score of 0.3051. While this
improvement is very modest, we are currently inves-
tigating alternative ways of interfacing with phrase
table construction to make a larger impact on trans-
lation quality.
5 Related Work
Our approach is similar in spirit to co-training,
where two classifiers, complementary by the virtue
of having different views of the data, are trained
jointly to encourage agreement (Blum and Mitchell,
1998; Collins and Singer, 1999). One key difference
110
in our work is that we rely exclusively on data like-
lihood to guide the two models in an unsupervised
manner, rather than relying on an initial handful of
labeled examples.
The idea of exploiting agreement between two la-
tent variable models is not new; there has been sub-
stantial previous work on leveraging the strengths
of two complementary models. Klein and Man-
ning (2004) combine two complementary mod-
els for grammar induction, one that models con-
stituency and one that models dependency, in a man-
ner broadly similar to the current work. Aside from
investigating a different domain, one novel aspect of
this paper is that we present a formal objective and a
training algorithm for combining two generic mod-
els.
6 Conclusion
We have described an efficient and fully unsuper-
vised method of producing state-of-the-art word
alignments. By training two simple sequence-based
models to agree, we achieve substantial error re-
ductions over standard models. Our jointly trained
HMM models reduce AER by 29% over test-time
intersected GIZA++ model 4 alignments and also
increase our robustness to varying initialization reg-
imens. While AER is only a weak indicator of final
translation quality in many current translation sys-
tems, we hope that more accurate alignments can
eventually lead to improvements in the end-to-end
translation process.
Acknowledgments We thank the anonymous re-
viewers for their comments.
References
Avrim Blum and Tom Mitchell. 1998. Combining Labeled
and Unlabeled Data with Co-training. In Proceedings of the
COLT 1998.
Peter F. Brown, Stephen A. Della Pietra, Vincent J. Della Pietra,
and Robert L. Mercer. 1994. The Mathematics of Statistical
Machine Translation: Parameter Estimation. Computational
Linguistics, 19:263?311.
Michael Collins and Yoram Singer. 1999. Unsupervised Mod-
els for Named Entity Classification. In Proceedings of
EMNLP 1999.
Abraham Ittycheriah and Salim Roukos. 2005. A maximum
entropy word aligner for arabic-english machine translation.
In Proceedings of HLT-EMNLP.
Dan Klein and Christopher D. Manning. 2004. Corpus-Based
Induction of Syntactic Structure: Models of Dependency and
Constituency. In Proceedings of ACL 2004.
Philipp Koehn, Franz Josef Och, and Daniel Marcu. 2003. Sta-
tistical Phrase-Based Translation. In Proceedings of HLT-
NAACL 2003.
E. Matusov, Zens. R., and H. Ney. 2004. Symmetric word
alignments for statistical machine translation. In Proceed-
ings of the 20th International Conference on Computational
Linguistics, August.
Robert C. Moore. 2004. Improving IBM Word Alignment
Model 1. In Proceedings of ACL 2004.
Robert C. Moore. 2005. A discriminative framework for bilin-
gual word alignment. In Proceedings of EMNLP.
Hermann Ney and Stephan Vogel. 1996. HMM-Based Word
Alignment in Statistical Translation. In COLING.
Franz Josef Och and Hermann Ney. 2003. A Systematic Com-
parison of Various Statistical Alignment Models. Computa-
tional Linguistics, 29:19?51.
Ben Taskar, Simon Lacoste-Julien, and Dan Klein. 2005. A
Discriminative Matching Approach to Word Alignment. In
Proceedings of EMNLP 2005.
L. G. Valiant. 1979. The complexity of computing the perma-
nent. Theoretical Computer Science, 8:189?201.
Appendix: Derivation of agreement EM
To simplify notation, we drop the explicit reference
to the parameters ?. Lower bound the objective in
Equation 3 by introducing a distribution q(z;x) and
using the concavity of log:
X
x
log p1(x)p2(x)
X
z
p1(z | x)p2(z | x) (4)
?
X
x,z
q(z;x) log p1(x)p2(x)p1(z | x)p2(z | x)q(z;x) (5)
=
X
x,z
q(z;x) log p1(z | x)p2(z | x)q(z;x) + C (6)
=
X
x,z
q(z;x) log p1(x, z)p2(x, z) + D, (7)
where C depends only on ? but not q and D de-
pends only q but not ?. The E-step chooses q given
a fixed ? to maximize the lower bound. Equation 6
is exactly
?
x?KL(q||p1p2) + C , which is maxi-
mized by setting q proportional to p1p2. The M-step
chooses ? given a fixed q. Equation 7 decomposes
into two separate optimization problems.
111
Proceedings of the Human Language Technology Conference of the North American Chapter of the ACL, pages 112?119,
New York, June 2006. c?2006 Association for Computational Linguistics
Word Alignment via Quadratic Assignment
Simon Lacoste-Julien
UC Berkeley, Berkeley, CA 94720
slacoste@cs.berkeley.edu
Ben Taskar
UC Berkeley, Berkeley, CA 94720
taskar@cs.berkeley.edu
Dan Klein
UC Berkeley, Berkeley, CA 94720
klein@cs.berkeley.edu
Michael I. Jordan
UC Berkeley, Berkeley, CA 94720
jordan@cs.berkeley.edu
Abstract
Recently, discriminative word alignment methods
have achieved state-of-the-art accuracies by extend-
ing the range of information sources that can be
easily incorporated into aligners. The chief advan-
tage of a discriminative framework is the ability
to score alignments based on arbitrary features of
the matching word tokens, including orthographic
form, predictions of other models, lexical context
and so on. However, the proposed bipartite match-
ing model of Taskar et al (2005), despite being
tractable and effective, has two important limita-
tions. First, it is limited by the restriction that
words have fertility of at most one. More impor-
tantly, first order correlations between consecutive
words cannot be directly captured by the model. In
this work, we address these limitations by enrich-
ing the model form. We give estimation and infer-
ence algorithms for these enhancements. Our best
model achieves a relative AER reduction of 25%
over the basic matching formulation, outperform-
ing intersected IBM Model 4 without using any
overly compute-intensive features. By including
predictions of other models as features, we achieve
AER of 3.8 on the standard Hansards dataset.
1 Introduction
Word alignment is a key component of most end-
to-end statistical machine translation systems. The
standard approach to word alignment is to construct
directional generative models (Brown et al, 1990;
Och and Ney, 2003), which produce a sentence in
one language given the sentence in another lan-
guage. While these models require sentence-aligned
bitexts, they can be trained with no further super-
vision, using EM. Generative alignment models do,
however, have serious drawbacks. First, they require
extensive tuning and processing of large amounts
of data which, for the better-performing models, is
a non-trivial resource requirement. Second, condi-
tioning on arbitrary features of the input is difficult;
for example, we would like to condition on the or-
thographic similarity of a word pair (for detecting
cognates), the presence of that pair in various dic-
tionaries, the similarity of the frequency of its two
words, choices made by other alignment systems,
and so on.
Recently, Moore (2005) proposed a discrimina-
tive model in which pairs of sentences (e, f) and
proposed alignments a are scored using a linear
combination of arbitrary features computed from the
tuples (a, e, f). While there are no restrictions on
the form of the model features, the problem of find-
ing the highest scoring alignment is very difficult
and involves heuristic search. Moreover, the param-
eters of the model must be estimated using averaged
perceptron training (Collins, 2002), which can be
unstable. In contrast, Taskar et al (2005) cast word
alignment as a maximum weighted matching prob-
lem, in which each pair of words (ej , fk) in a sen-
tence pair (e, f) is associated with a score sjk(e, f)
reflecting the desirability of the alignment of that
pair. Importantly, this problem is computationally
tractable. The alignment for the sentence pair is the
highest scoring matching under constraints (such as
the constraint that matchings be one-to-one). The
scoring model sjk(e, f) can be based on a rich fea-
ture set defined on word pairs (ej , fk) and their con-
text, including measures of association, orthogra-
phy, relative position, predictions of generative mod-
els, etc. The parameters of the model are estimated
within the framework of large-margin estimation; in
particular, the problem turns out to reduce to the
112
solution of a (relatively) small quadratic program
(QP). The authors show that large-margin estimation
is both more stable and more accurate than percep-
tron training.
While the bipartite matching approach is a use-
ful first step in the direction of discriminative word
alignment, for discriminative approaches to com-
pete with and eventually surpass the most sophisti-
cated generative models, it is necessary to consider
more realistic underlying statistical models. Note in
particular two substantial limitations of the bipartite
matching model of Taskar et al (2005): words have
fertility of at most one, and there is no way to incor-
porate pairwise interactions among alignment deci-
sions. Moving beyond these limitations?while re-
taining computational tractability?is the next major
challenge for discriminative word alignment.
In this paper, we show how to overcome both lim-
itations. First, we introduce a parameterized model
that penalizes different levels of fertility. While this
extension adds very useful expressive power to the
model, it turns out not to increase the computa-
tional complexity of the aligner, for either the pre-
diction or the parameter estimation problem. Sec-
ond, we introduce a more thoroughgoing extension
which incorporates first-order interactions between
alignments of consecutive words into the model. We
do this by formulating the alignment problem as a
quadratic assignment problem (QAP), where in ad-
dition to scoring individual edges, we also define
scores of pairs of edges that connect consecutive
words in an alignment. The predicted alignment is
the highest scoring quadratic assignment.
QAP is an NP-hard problem, but in the range of
problem sizes that we need to tackle the problem can
be solved efficiently. In particular, using standard
off-the-shelf integer program solvers, we are able to
solve the QAP problems in our experiments in under
a second. Moreover, the parameter estimation prob-
lem can also be solved efficiently by making use of
a linear relaxation of QAP for the min-max formu-
lation of large-margin estimation (Taskar, 2004).
We show that these two extensions yield signif-
icant improvements in error rates when compared
to the bipartite matching model. The addition of a
fertility model improves the AER by 0.4. Model-
ing first-order interactions improves the AER by 1.8.
Combining the two extensions results in an improve-
ment in AER of 2.3, yielding alignments of better
quality than intersected IBM Model 4. Moreover,
including predictions of bi-directional IBM Model
4 and model of Liang et al (2006) as features, we
achieve an absolute AER of 3.8 on the English-
French Hansards alignment task?the best AER re-
sult published on this task to date.
2 Models
We begin with a quick summary of the maximum
weight bipartite matching model in (Taskar et al,
2005). More precisely, nodes V = Vs ? V t cor-
respond to words in the ?source? (Vs) and ?tar-
get? (V t) sentences, and edges E = {jk : j ?
Vs, k ? V t} correspond to alignments between word
pairs.1 The edge weights sjk represent the degree
to which word j in one sentence can be translated
using the word k in the other sentence. The pre-
dicted alignment is chosen by maximizing the sum
of edge scores. A matching is represented using a
set of binary variables yjk that are set to 1 if word
j is assigned to word k in the other sentence, and 0
otherwise. The score of an assignment is the sum of
edge scores: s(y) = ?jk sjkyjk. For simplicity, let
us begin by assuming that each word aligns to one or
zero words in the other sentence; we revisit the issue
of fertility in the next section. The maximum weight
bipartite matching problem, arg maxy?Y s(y), can
be solved using combinatorial algorithms for min-
cost max-flow, expressed in a linear programming
(LP) formulation as follows:
max
0?z?1
?
jk?E
sjkzjk (1)
s.t.
?
j?Vs
zjk ? 1, ?k ? V t;
?
k?Vt
zjk ? 1, ?j ? Vs,
where the continuous variables zjk are a relax-
ation of the corresponding binary-valued variables
yjk. This LP is guaranteed to have integral (and
hence optimal) solutions for any scoring function
s(y) (Schrijver, 2003). Note that although the above
LP can be used to compute alignments, combina-
torial algorithms are generally more efficient. For
1The source/target designation is arbitrary, as the models
considered below are all symmetric.
113
t
he
ba
ck
bo
ne of
o
u
r
e
c
o
n
o
m
y
de
e?pine
dorsale
a`
notre
e?conomie
t
he
ba
ck
bo
ne of
o
u
r
e
c
o
n
o
m
y
de
e?pine
dorsale
a`
notre
e?conomie
(a) (b)
Figure 2: An example fragment that requires fertility
greater than one to correctly label. (a) The guess of
the baseline M model. (b) The guess of the M+F
fertility-augmented model.
example, in Figure 1(a), we show a standard con-
struction for an equivalent min-cost flow problem.
However, we build on this LP to develop our exten-
sions to this model below. Representing the predic-
tion problem as an LP or an integer LP provides a
precise (and concise) way of specifying the model
and allows us to use the large-margin framework
of Taskar (2004) for parameter estimation described
in Section 3.
For a sentence pair x, we denote position pairs by
xjk and their scores as sjk. We let sjk = w>f(xjk)
for some user provided feature mapping f and ab-
breviate w>f(x,y) = ?jk yjkw>f(xjk). We can
include in the feature vector the identity of the two
words, their relative positions in their respective sen-
tences, their part-of-speech tags, their string similar-
ity (for detecting cognates), and so on.
2.1 Fertility
An important limitation of the model in Eq. (1) is
that in each sentence, a word can align to at most
one word in the translation. Although it is common
that words have gold fertility zero or one, it is cer-
tainly not always true. Consider, for example, the
bitext fragment shown in Figure 2(a), where back-
bone is aligned to the phrase e?pine dorsal. In this
figure, outlines are gold alignments, square for sure
alignments, round for possibles, and filled squares
are target algnments (for details on gold alignments,
see Section 4). When considering only the sure
alignments on the standard Hansards dataset, 7 per-
cent of the word occurrences have fertility 2, and 1
percent have fertility 3 and above; when considering
the possible alignments high fertility is much more
common?31 percent of the words have fertility 3
and above.
One simple fix to the original matching model is
to increase the right hand sides for the constraints
in Eq. (1) from 1 to D, where D is the maximum
allowed fertility. However, this change results in
an undesirable bimodal behavior, where maximum
weight solutions either have all words with fertil-
ity 0 or D, depending on whether most scores sjk
are positive or negative. For example, if scores tend
to be positive, most words will want to collect as
many alignments as they are permitted. What the
model is missing is a means for encouraging the
common case of low fertility (0 or 1), while allowing
higher fertility when it is licensed. This end can be
achieved by introducing a penalty for having higher
fertility, with the goal of allowing that penalty to
vary based on features of the word in question (such
as its frequency or identity).
In order to model such a penalty, we introduce
indicator variables zdj? (and zd?k) with the intended
meaning: node j has fertility of at least d (and node
k has fertility of at least d). In the following LP, we
introduce a penalty of
?
2?d?D sdj?zdj? for fertility
of node j, where each term sdj? ? 0 is the penalty
increment for increasing the fertility from d ? 1 to
d:
max
0?z?1
?
jk?E
sjkzjk (2)
?
?
j?Vs,2?d?D
sdj?zdj? ?
?
k?Vt,2?d?D
sd?kzd?k
s.t.
?
j?Vs
zjk ? 1 +
?
2?d?D
zd?k, ?k ? V t;
?
k?Vt
zjk ? 1 +
?
2?d?D
zdj?, ?j ? Vs.
We can show that this LP always has integral so-
lutions by a reduction to a min-cost flow problem.
The construction is shown in Figure 1(b). To ensure
that the new variables have the intended semantics,
we need to make sure that sdj? ? sd?j? if d ? d?,
so that the lower cost zdj? is used before the higher
cost zd?j? to increase fertility. This restriction im-
114
(a) (b) (c)
Figure 1: (a) Maximum weight bipartite matching as min-cost flow. Diamond-shaped nodes represent flow
source and sink. All edge capacities are 1, with edges between round nodes (j, k) have cost ?sjk, edges
from source and to sink have cost 0. (b) Expanded min-cost flow graph with new edges from source and to
sink that allow fertility of up to 3. The capacities of the new edges are 1 and the costs are 0 for solid edges
from source and to sink, s2j?, s2?k for dashed edges, and s3j?, s3?k for dotted edges. (c) Three types of pairs
of edges included in the QAP model, where the nodes on both sides correspond to consecutive words.
fo
r
m
o
r
e
t
ha
n a
y
e
a
r
depuis
plus
de
un
an
fo
r
m
o
r
e
t
ha
n a
y
e
a
r
depuis
plus
de
un
an
(a) (b)
Figure 3: An example fragment with a monotonic
gold alignment. (a) The guess of the baseline M
model. (b) The guess of the M+Q quadratic model.
plies that the penalty must be monotonic and convex
as a function of the fertility.
To anticipate the results that we report in Sec-
tion 4, adding fertility to the basic matching model
makes the target algnment of the backbone example
feasible and, in this case, the model correctly labels
this fragment as shown in Figure 2(b).
2.2 First-order interactions
An even more significant limitation of the model
in Eq. (1) is that the edges interact only indi-
rectly through the competition induced by the con-
straints. Generative alignment models like the
HMM model (Vogel et al, 1996) and IBM models 4
and above (Brown et al, 1990; Och and Ney, 2003)
directly model correlations between alignments of
consecutive words (at least on one side). For exam-
ple, Figure 3 shows a bitext fragment whose gold
alignment is strictly monotonic. This monotonicity
is quite common ? 46% of the words in the hand-
aligned data diagonally follow a previous alignment
in this way. We can model the common local align-
ment configurations by adding bonuses for pairs of
edges. For example, strictly monotonic alignments
can be encouraged by boosting the scores of edges
of the form ?(j, k), (j + 1, k + 1)?. Another trend,
common in English-French translation (7% on the
hand-aligned data), is the local inversion of nouns
and adjectives, which typically involves a pair of
edges ?(j, k + 1), (j + 1, k)?. Finally, a word in one
language is often translated as a phrase (consecutive
sequence of words) in the other language. This pat-
tern involves pairs of edges with the same origin on
one side: ?(j, k), (j, k+1)? or ?(j, k), (j+1, k)?. All
three of these edge pair patterns are shown in Fig-
ure 1(c). Note that the set of such edge pairs Q =
{jklm : |j ? l| ? 1, |k ? m| ? 1} is of linear size
in the number of edges.
Formally, we add to the model variables zjklm
which indicate whether both edge jk and lm are in
the alignment. We also add a corresponding score
sjklm, which we assume to be non-negative, since
the correlations we described are positive. (Nega-
tive scores can also be used, but the resulting for-
mulation we present below would be slightly differ-
ent.) To enforce the semantics zjklm = zjkzlm, we
use a pair of constraints zjklm ? zjk; zjklm ? zlm.
Since sjklm is positive, at the optimum, zjklm =
115
min(zjk, zlm). If in addition zjk, zlm are integral (0
or 1), then zjklm = zjkzlm. Hence, solving the fol-
lowing LP as an integer linear program will find the
optimal quadratic assignment for our model:
max
0?z?1
?
jk?E
sjkzjk +
?
jklm?Q
sjklmzjklm (3)
s.t.
?
j?Vs
zjk ? 1, ?k ? V t;
?
k?Vt
zjk ? 1, ?j ? Vs;
zjklm ? zjk, zjklm ? zlm, ?jklm ? Q.
Note that we can also combine this extension with
the fertility extension described above.
To once again anticipate the results presented in
Section 4, the baseline model of Taskar et al (2005)
makes the prediction given in Figure 3(a) because
the two missing alignments are atypical translations
of common words. With the addition of edge pair
features, the overall monotonicity pushes the align-
ment to that of Figure 3(b).
3 Parameter estimation
To estimate the parameters of our model, we fol-
low the large-margin formulation of Taskar (2004).
Our input is a set of training instances {(xi,yi)}mi=1,
where each instance consists of a sentence pair xi
and a target algnment yi. We would like to find
parameters w that predict correct alignments on the
training data: yi = arg max
y?i?Yi
w>f(xi, y?i) for each i,
where Yi is the space of matchings for the sentence
pair xi.
In standard classification problems, we typically
measure the error of prediction, `(yi, y?i), using the
simple 0-1 loss. In structured problems, where we
are jointly predicting multiple variables, the loss is
often more complex. While the F-measure is a nat-
ural loss function for this task, we instead chose a
sensible surrogate that fits better in our framework:
weighted Hamming distance, which counts the num-
ber of variables in which a candidate solution y? dif-
fers from the target output y, with different penalty
for false positives (c+) and false negatives (c?):
`(y, y?) =
?
jk
[
c+(1 ? yjk)y?jk + c?(1 ? y?jk)yjk
]
.
We use an SVM-like hinge upper bound on
the loss `(yi, y?i), given by maxy?i?Yi [w>fi(y?i) +
`i(y?i) ? w>fi(yi)], where `i(y?i) = `(yi, y?i), and
fi(y?i) = f(xi, y?i). Minimizing this upper bound
encourages the true alignment yi to be optimal with
respect to w for each instance i:
min
||w||??
?
i
max
y?i?Yi
[w>fi(y?i) + `i(y?i)] ? w>fi(yi),
where ? is a regularization parameter.
In this form, the estimation problem is a mixture
of continuous optimization over w and combinato-
rial optimization over yi. In order to transform it
into a more standard optimization problem, we need
a way to efficiently handle the loss-augmented in-
ference, maxy?i?Yi [w>fi(y?i) + `i(y?i)]. This opti-
mization problem has precisely the same form as the
prediction problem whose parameters we are trying
to learn ? maxy?i?Yi w>fi(y?i) ? but with an addi-
tional term corresponding to the loss function. Our
assumption that the loss function decomposes over
the edges is crucial to solving this problem. We omit
the details here, but note that we can incorporate the
loss function into the LPs for various models we de-
scribed above and ?plug? them into the large-margin
formulation by converting the estimation problem
into a quadratic problem (QP) (Taskar, 2004). This
QP can be solved using any off-the-shelf solvers,
such as MOSEK or CPLEX.2 An important differ-
ence that comes into play for the estimation of the
quadratic assignment models in Equation (3) is that
inference involves solving an integer linear program,
not just an LP. In fact the LP is a relaxation of the in-
teger LP and provides an upper bound on the value
of the highest scoring assignment. Using the LP re-
laxation for the large-margin QP formulation is an
approximation, but as our experiments indicate, this
approximation is very effective. At testing time, we
use the integer LP to predict alignments. We have
also experimented with using just the LP relaxation
at testing time and then independently rounding each
fractional edge value, which actually incurs no loss
in alignment accuracy, as we discuss below.
2When training on 200 sentences, the QP we obtain contains
roughly 700K variables and 300K constraints and is solved in
roughly 10 minutes on a 2.8 GHz Pentium 4 machine. Aligning
the whole training set with the flow formulation takes a few
seconds, whereas using the integer programming (for the QAP
formulation) takes 1-2 minutes.
116
t
he
ho
n.
m
e
m
be
r
fo
r
V
e
r
du
n
w
o
u
ld
n
o
t
ha
ve
de
ni
gr
at
ed my
p
o
s
it
io
n
le
de?pute?
de
Verdun
ne
aurait
pas
de?pre?cie?
ma
position
t
he
ho
n.
m
e
m
be
r
fo
r
V
e
r
du
n
w
o
u
ld
n
o
t
ha
ve
de
ni
gr
at
ed my
p
o
s
it
io
n
le
de?pute?
de
Verdun
ne
aurait
pas
de?pre?cie?
ma
position
t
he
ho
n.
m
e
m
be
r
fo
r
V
e
r
du
n
w
o
u
ld
n
o
t
ha
ve
de
ni
gr
at
ed my
p
o
s
it
io
n
le
de?pute?
de
Verdun
ne
aurait
pas
de?pre?cie?
ma
position
(a) (b) (c)
Figure 4: An example fragment with several multiple fertility sure alignments. (a) The guess of the M+Q
model with maximum fertility of one. (b) The guess of the M+Q+F quadratic model with fertility two
permitted. (c) The guess of the M+Q+F model with lexical fertility features.
4 Experiments
We applied our algorithms to word-level alignment
using the English-French Hansards data from the
2003 NAACL shared task (Mihalcea and Pedersen,
2003). This corpus consists of 1.1M automatically
aligned sentences, and comes with a validation set of
37 sentence pairs and a test set of 447 sentences. The
validation and test sentences have been hand-aligned
(see Och and Ney (2003)) and are marked with both
sure and possible alignments. Using these align-
ments, alignment error rate (AER) is calculated as:
(
1 ? |A ? S| + |A ? P ||A| + |S|
)
? 100%.
Here, A is a set of proposed index pairs, S is the
sure gold pairs, and P is the possible gold pairs.
For example, in Figure 4, proposed alignments are
shown against gold alignments, with open squares
for sure alignments, rounded open squares for possi-
ble alignments, and filled black squares for proposed
alignments.
The input to our algorithm is a small number of
labeled examples. In order to make our results more
comparable with Moore (2005), we split the origi-
nal set into 200 training examples and 247 test ex-
amples. We also trained on only the first 100 to
make our results more comparable with the exper-
iments of Och and Ney (2003), in which IBM model
4 was tuned using 100 sentences. In all our experi-
ments, we used a structured loss function that penal-
ized false negatives 10 times more than false posi-
tives, where the value of 10 was picked by using a
validation set. The regularization parameter ? was
also chosen using the validation set.
4.1 Features and results
We parameterized all scoring functions sjk, sdj?,
sd?k and sjklm as weighted linear combinations of
feature sets. The features were computed from
the large unlabeled corpus of 1.1M automatically
aligned sentences.
In the remainder of this section we describe the
improvements to the model performance as various
features are added. One of the most useful features
for the basic matching model is, of course, the set of
predictions of IBM model 4. However, computing
these features is very expensive and we would like to
build a competitive model that doesn?t require them.
Instead, we made significant use of IBM model 2 as
a source of features. This model, although not very
accurate as a predictive model, is simple and cheap
to construct and it is a useful source of features.
The Basic Matching Model: Edge Features In
the basic matching model of Taskar et al (2005),
called M here, one can only specify features on pairs
of word tokens, i.e. alignment edges. These features
117
include word association, orthography, proximity,
etc., and are documented in Taskar et al (2005). We
also augment those features with the predictions of
IBM Model 2 run on the training and test sentences.
We provided features for model 2 trained in each
direction, as well as the intersected predictions, on
each edge. By including the IBM Model 2 features,
the performance of the model described in Taskar et
al. (2005) on our test set (trained on 200 sentences)
improves from 10.0 AER to 8.2 AER, outperforming
unsymmetrized IBM Model 4 (but not intersected
model 4).
As an example of the kinds of errors the baseline
M system makes, see Figure 2 (where multiple fer-
tility cannot be predicted), Figure 3 (where a prefer-
ence for monotonicity cannot be modeled), and Fig-
ure 4 (which shows several multi-fertile cases).
The Fertility Model: Node Features To address
errors like those shown in Figure 2, we increased
the maximum fertility to two using the parameter-
ized fertility model of Section 2.1. The model learns
costs on the second flow arc for each word via fea-
tures not of edges but of single words. The score of
taking a second match for a word w was based on
the following features: a bias feature, the proportion
of times w?s type was aligned to two or more words
by IBM model 2, and the bucketed frequency of the
word type. This model was called M+F. We also in-
cluded a lexicalized feature for words which were
common in our training set: whether w was ever
seen in a multiple fertility alignment (more on this
feature later). This enabled the system to learn that
certain words, such as the English not and French
verbs like aurait commonly participate in multiple
fertility configurations.
Figure 5 show the results using the fertility exten-
sion. Adding fertility lowered AER from 8.5 to 8.1,
though fertility was even more effective in conjunc-
tion with the quadratic features below. The M+F set-
ting was even able to correctly learn some multiple
fertility instances which were not seen in the training
data, such as those shown in Figure 2.
The First-Order Model: Quadratic Features
With or without the fertility model, the model makes
mistakes such as those shown in Figure 3, where
atypical translations of common words are not cho-
sen despite their local support from adjacent edges.
In the quadratic model, we can associate features
with pairs of edges. We began with features which
identify each specific pattern, enabling trends of
monotonicity (or inversion) to be captured. We also
added to each edge pair the fraction of times that
pair?s pattern (monotonic, inverted, one to two) oc-
curred according each version of IBM model 2 (for-
ward, backward, intersected).
Figure 5 shows the results of adding the quadratic
model. M+Q reduces error over M from 8.5 to 6.7
(and fixes the errors shown in Figure 3). When both
the fertility and quadratic extensions were added,
AER dropped further, to 6.2. This final model is
even able to capture the diamond pattern in Figure 4;
the adjacent cycle of alignments is reinforced by the
quadratic features which boost adjacency. The ex-
ample in Figure 4 shows another interesting phe-
nomenon: the multi-fertile alignments for not and
de?pute? are learned even without lexical fertility fea-
tures (Figure 4b), because the Dice coefficients of
those words with their two alignees are both high.
However the surface association of aurait with have
is much higher than with would. If, however, lexi-
cal features are added, would is correctly aligned as
well (Figure 4c), since it is observed in similar pe-
riphrastic constructions in the training set.
We have avoided using expensive-to-compute fea-
tures like IBM model 4 predictions up to this point.
However, if these are available, our model can im-
prove further. By adding model 4 predictions to the
edge features, we get a relative AER reduction of
27%, from 6.5 to 4.5. By also including as features
the posteriors of the model of Liang et al (2006), we
achieve AER of 3.8, and 96.7/95.5 precision/recall.
It is comforting to note that in practice, the burden
of running an integer linear program at test time can
be avoided. We experimented with using just the LP
relaxation and found that on the test set, only about
20% of sentences have fractional solutions and only
0.2% of all edges are fractional. Simple rounding3
of each edge value in the LP solution achieves the
same AER as the integer LP solution, while using
about a third of the computation time on average.
3We slightly bias the system on the recall side by rounding
0.5 up, but this doesn?t yield a noticeable difference in the re-
sults.
118
Model Prec Rec AER
Generative
IBM 2 (E?F) 73.6 87.7 21.7
IBM 2 (F?E) 75.4 87.0 20.6
IBM 2 (intersected) 90.1 80.4 14.3
IBM 4 (E?F) 90.3 92.1 9.0
IBM 4 (F?E) 90.8 91.3 9.0
IBM 4 (intersected) 98.0 88.1 6.5
Discriminative (100 sentences)
Matching (M) 94.1 88.5 8.5
M + Fertility (F) 93.9 89.4 8.1
M + Quadratic (Q) 94.4 91.9 6.7
M + F + Q 94.8 92.5 6.2
M + F + Q + IBM4 96.4 94.4 4.5
Discriminative (200 sentences)
Matching (M) 93.4 89.7 8.2
M + Fertility (F) 93.6 90.1 8.0
M + Quadratic (Q) 95.0 91.1 6.8
M + F + Q 95.2 92.4 6.1
M + F + Q + IBM4 96.0 95.0 4.4
Figure 5: AER on the Hansards task.
5 Conclusion
We have shown that the discriminative approach to
word alignment can be extended to allow flexible
fertility modeling and to capture first-order inter-
actions between alignments of consecutive words.
These extensions significantly enhance the expres-
sive power of the discriminative approach; in partic-
ular, they make it possible to capture phenomena of
monotonicity, local inversion and contiguous fertil-
ity trends?phenomena that are highly informative
for alignment. They do so while remaining compu-
tationally efficient in practice both for prediction and
for parameter estimation.
Our best model achieves a relative AER reduc-
tion of 25% over the basic matching formulation,
beating intersected IBM Model 4 without the use
of any compute-intensive features. Including Model
4 predictions as features, we achieve a further rela-
tive AER reduction of 32% over intersected Model
4 alignments. By also including predictions of an-
other model, we drive AER down to 3.8. We are
currently investigating whether the improvement in
AER results in better translation BLEU score. Al-
lowing higher fertility and optimizing a recall bi-
ased cost function provide a significant increase in
recall relative to the intersected IBM model 4 (from
88.1% to 94.4%), with only a small degradation in
precision. We view this as a particularly promising
aspect of our work, given that phrase-based systems
such as Pharaoh (Koehn et al, 2003) perform better
with higher recall alignments.
References
P. F. Brown, J. Cocke, S. A. Della Pietra, V. J. Della
Pietra, F. Jelinek, J. D. Lafferty, R. L. Mercer, and
P. S. Roossin. 1990. A statistical approach to machine
translation. Computational Linguistics, 16(2):79?85.
M. Collins. 2002. Discriminative training methods for
hidden Markov models: Theory and experiments with
perceptron algorithms. In Proc. EMNLP.
Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical phrase-based translation. In Proceed-
ings of HLT-NAACL 2003.
P. Liang, B. Taskar, and D. Klein. 2006. Alignment by
agreement. In HLT-NAACL.
R. Mihalcea and T. Pedersen. 2003. An evaluation exer-
cise for word alignment. In Proceedings of the HLT-
NAACL 2003 Workshop, Building and Using parallel
Texts: Data Driven Machine Translation and Beyond,
pages 1?6, Edmonton, Alberta, Canada.
Robert C. Moore. 2005. A discriminative framework for
bilingual word alignment. In Proc. HLT/EMNLP.
F. Och and H. Ney. 2003. A systematic comparison of
various statistical alignment models. Computational
Linguistics, 29(1):19?52.
A. Schrijver. 2003. Combinatorial Optimization: Poly-
hedra and Efficiency. Springer.
B. Taskar, S. Lacoste-Julien, and D. Klein. 2005. A dis-
criminative matching approach to word alignment. In
EMNLP.
B. Taskar. 2004. Learning Structured Prediction Mod-
els: A Large Margin Approach. Ph.D. thesis, Stanford
University.
S. Vogel, H. Ney, and C. Tillmann. 1996. HMM-based
word alignment in statistical translation. In COLING
16, pages 836?841.
119
Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the ACL, pages 761?768,
Sydney, July 2006. c?2006 Association for Computational Linguistics
An End-to-End Discriminative Approach to Machine Translation
Percy Liang Alexandre Bouchard-Co?te? Dan Klein Ben Taskar
Computer Science Division, EECS Department
University of California at Berkeley
Berkeley, CA 94720
{pliang, bouchard, klein, taskar}@cs.berkeley.edu
Abstract
We present a perceptron-style discriminative ap-
proach to machine translation in which large feature
sets can be exploited. Unlike discriminative rerank-
ing approaches, our system can take advantage of
learned features in all stages of decoding. We first
discuss several challenges to error-driven discrim-
inative approaches. In particular, we explore dif-
ferent ways of updating parameters given a training
example. We find that making frequent but smaller
updates is preferable to making fewer but larger up-
dates. Then, we discuss an array of features and
show both how they quantitatively increase BLEU
score and how they qualitatively interact on spe-
cific examples. One particular feature we investi-
gate is a novel way to introduce learning into the
initial phrase extraction process, which has previ-
ously been entirely heuristic.
1 Introduction
The generative, noisy-channel paradigm has his-
torically served as the foundation for most of the
work in statistical machine translation (Brown et
al., 1994). At the same time, discriminative meth-
ods have provided substantial improvements over
generative models on a wide range of NLP tasks.
They allow one to easily encode domain knowl-
edge in the form of features. Moreover, param-
eters are tuned to directly minimize error rather
than to maximize joint likelihood, which may not
correspond well to the task objective.
In this paper, we present an end-to-end dis-
criminative approach to machine translation. The
proposed system is phrase-based, as in Koehn et
al. (2003), but uses an online perceptron training
scheme to learn model parameters. Unlike mini-
mum error rate training (Och, 2003), our system is
able to exploit large numbers of specific features
in the same manner as static reranking systems
(Shen et al, 2004; Och et al, 2004). However,
unlike static rerankers, our system does not rely
on a baseline translation system. Instead, it up-
dates based on its own n-best lists. As parameter
estimates improve, the system produces better n-
best lists, which can in turn enable better updates
in future training iterations. In this paper, we fo-
cus on two aspects of the problem of discrimina-
tive translation: the inherent difficulty of learning
from reference translations, and the challenge of
engineering effective features for this task.
Discriminative learning from reference transla-
tions is inherently problematic because standard
discriminative methods need to know which out-
puts are correct and which are not. However, a
proposed translation that differs from a reference
translation need not be incorrect. It may differ
in word choice, literalness, or style, yet be fully
acceptable. Pushing our system to avoid such al-
ternate translations is undesirable. On the other
hand, even if a system produces a reference trans-
lation, it may do so by abusing the hidden struc-
ture (sentence segmentation and alignment). We
can therefore never be entirely sure whether or not
a proposed output is safe to update towards. We
discuss this issue in detail in Section 5, where we
show that conservative updates (which push the
system towards a local variant of the current pre-
diction) are more effective than more aggressive
updates (which try to directly update towards the
reference).
The second major contribution of this work is
an investigation of an array of features for our
model. We show how our features quantitatively
increase BLEU score, as well as how they qual-
itatively interact on specific examples. We first
consider learning weights for individual phrases
and part-of-speech patterns, showing gains from
each. We then present a novel way to parameter-
ize and introduce learning into the initial phrase
extraction process. In particular, we introduce
alignment constellation features, which allow us
to weight phrases based on the word alignment
pattern that led to their extraction. This kind of
761
feature provides a potential way to initially extract
phrases more aggressively and then later down-
weight undesirable patterns, essentially learning a
weighted extraction heuristic. Finally, we use POS
features to parameterize a distortion model in a
limited distortion decoder (Zens and Ney, 2004;
Tillmann and Zhang, 2005). We show that over-
all, BLEU score increases from 28.4 to 29.6 on
French-English.
2 Approach
2.1 Translation as structured classification
Machine translation can be seen as a structured
classification task, in which the goal is to learn
a mapping from an input (French) sentence x to
an output (English) sentence y. Given this setup,
discriminative methods allow us to define a broad
class of features ? that operate on (x,y). For ex-
ample, some features would measure the fluency
of y and others would measure the faithfulness of
y as a translation of x.
However, the translation task in this framework
differs from traditional applications of discrimina-
tive structured classification such as POS tagging
and parsing in a fundamental way. Whereas in
POS tagging, there is a one-to-one correspondence
between the words x and the tags y, the correspon-
dence between x and y in machine translation is
not only much more complex, but is in fact un-
known. Therefore, we introduce a hidden corre-
spondence structure h and work with the feature
vector ?(x,y,h).
The phrase-based model of Koehn et al (2003)
is an instance of this framework. In their model,
the correspondence h consists of (1) the segmen-
tation of the input sentence into phrases, (2) the
segmentation of the output sentence into the same
number of phrases, and (3) a bijection between
the input and output phrases. The feature vec-
tor ?(x,y,h) contains four components: the log
probability of the output sentence y under a lan-
guage model, the score of translating x into y
based on a phrase table, a distortion score, and a
length penalty.1 In Section 6, we vastly increase
the number of features to take advantage of the full
power of discriminative training.
Another example of this framework is the hier-
archical model of Chiang (2005). In this model
the correspondence h is a synchronous parse tree
1More components can be added to the feature vector if
additional language models or phrase tables are available.
over input and output sentences, and features in-
clude the scores of various productions used in the
tree.
Given features ? and a corresponding set of pa-
rameters w, a standard classification rule f is to
return the highest scoring output sentence y, max-
imizing over correspondences h:
f(x;w) = argmax
y,h
w ? ?(x,y,h). (1)
In the phrase-based model, computing the
argmax exactly is intractable, so we approximate
f with beam decoding.
2.2 Perceptron-based training
To tune the parameters w of the model, we use the
averaged perceptron algorithm (Collins, 2002) be-
cause of its efficiency and past success on various
NLP tasks (Collins and Roark, 2004; Roark et al,
2004). In principle, w could have been tuned by
maximizing conditional probability or maximiz-
ing margin. However, these two options require
either marginalization or numerical optimization,
neither of which is tractable over the space of out-
put sentences y and correspondences h. In con-
trast, the perceptron algorithm requires only a de-
coder that computes f(x;w).
Recall the traditional perceptron update rule on
an example (xi,yi) is
w? w + ?(xi,yt)? ?(xi,yp), (2)
where yt = yi is the target output and yp =
f(xi;w) = argmaxyw ? ?(xi,y) is the predic-
tion using the current parameters w.
We adapt this update rule to work with hidden
variables as follows:
w? w + ?(xi,yt,ht)??(xi,yp,hp), (3)
where (yp,hp) is the argmax computation in
Equation 1, and (yt,ht) is the target that we up-
date towards. If (yt,ht) is the same argmax com-
putation with the additional constraint that yt =
yi, then Equation 3 can be interpreted as a Viterbi
approximation to the stochastic gradient
EP (h|xi,yi;w)?(xi,yi,h)?EP (y,h|xi;w)?(xi,y,h)
for the following conditional likelihood objective:
P (yi | xi) ?
?
h
exp(w ? ?(xi,yi,h)).
762
      
    	
 

   
 
 


 




      
	

  

 
  
  
	 
   
      
  
	 
   
	
     

  

 
 

 
	 
  
	
     

  

 
  
 
 	
       
	

	 
   
	
   
 
	


	 
   
Proceedings of ACL-08: HLT, pages 986?993,
Columbus, Ohio, USA, June 2008. c?2008 Association for Computational Linguistics
Better Alignments = Better Translations?
Kuzman Ganchev
Computer & Information Science
University of Pennsylvania
kuzman@cis.upenn.edu
Joa?o V. Grac?a
L2F INESC-ID
Lisboa, Portugal
javg@l2f.inesc-id.pt
Ben Taskar
Computer & Information Science
University of Pennsylvania
taskar@cis.upenn.edu
Abstract
Automatic word alignment is a key step in
training statistical machine translation sys-
tems. Despite much recent work on word
alignment methods, alignment accuracy in-
creases often produce little or no improve-
ments in machine translation quality. In
this work we analyze a recently proposed
agreement-constrained EM algorithm for un-
supervised alignment models. We attempt to
tease apart the effects that this simple but ef-
fective modification has on alignment preci-
sion and recall trade-offs, and how rare and
common words are affected across several lan-
guage pairs. We propose and extensively eval-
uate a simple method for using alignment
models to produce alignments better-suited
for phrase-based MT systems, and show sig-
nificant gains (as measured by BLEU score)
in end-to-end translation systems for six lan-
guages pairs used in recent MT competitions.
1 Introduction
The typical pipeline for a machine translation (MT)
system starts with a parallel sentence-aligned cor-
pus and proceeds to align the words in every sen-
tence pair. The word alignment problem has re-
ceived much recent attention, but improvements in
standard measures of word alignment performance
often do not result in better translations. Fraser and
Marcu (2007) note that none of the tens of papers
published over the last five years has shown that
significant decreases in alignment error rate (AER)
result in significant increases in translation perfor-
mance. In this work, we show that by changing
the way the word alignment models are trained and
used, we can get not only improvements in align-
ment performance, but also in the performance of
the MT system that uses those alignments.
We present extensive experimental results evalu-
ating a new training scheme for unsupervised word
alignment models: an extension of the Expecta-
tion Maximization algorithm that allows effective
injection of additional information about the desired
alignments into the unsupervised training process.
Examples of such information include ?one word
should not translate to many words? or that direc-
tional translation models should agree. The gen-
eral framework for the extended EM algorithm with
posterior constraints of this type was proposed by
(Grac?a et al, 2008). Our contribution is a large scale
evaluation of this methodology for word alignments,
an investigation of how the produced alignments dif-
fer and how they can be used to consistently improve
machine translation performance (as measured by
BLEU score) across many languages on training cor-
pora with up to hundred thousand sentences. In 10
out of 12 cases we improve BLEU score by at least 14
point and by more than 1 point in 4 out of 12 cases.
After presenting the models and the algorithm in
Sections 2 and 3, in Section 4 we examine how
the new alignments differ from standard models, and
find that the newmethod consistently improves word
alignment performance, measured either as align-
ment error rate or weighted F-score. Section 5 ex-
plores how the new alignments lead to consistent
and significant improvement in a state of the art
phrase base machine translation by using posterior
decoding rather than Viterbi decoding. We propose
a heuristic for tuning posterior decoding in the ab-
sence of annotated alignment data and show im-
provements over baseline systems for six different
986
language pairs used in recent MT competitions.
2 Statistical word alignment
Statistical word alignment (Brown et al, 1994) is
the task identifying which words are translations of
each other in a bilingual sentence corpus. Figure
2 shows two examples of word alignment of a sen-
tence pair. Due to the ambiguity of the word align-
ment task, it is common to distinguish two kinds of
alignments (Och and Ney, 2003). Sure alignments
(S), represented in the figure as squares with bor-
ders, for single-word translations and possible align-
ments (P), represented in the figure as alignments
without boxes, for translations that are either not ex-
act or where several words in one language are trans-
lated to several words in the other language. Possi-
ble alignments can can be used either to indicated
optional alignments, such as the translation of an
idiom, or disagreement between annotators. In the
figure red/black dots indicates correct/incorrect pre-
dicted alignment points.
2.1 Baseline word alignment models
We focus on the hidden Markov model (HMM) for
alignment proposed by (Vogel et al, 1996). This is
a generalization of IBM models 1 and 2 (Brown et
al., 1994), where the transition probabilities have a
first-order Markov dependence rather than a zeroth-
order dependence. The model is an HMM, where the
hidden states take values from the source language
words and generate target language words according
to a translation table. The state transitions depend on
the distance between the source language words. For
source sentence s the probability of an alignment a
and target sentence t can be expressed as:
p(t,a | s) =
?
j
pd(aj |aj ? aj?1)pt(tj |saj ), (1)
where aj is the index of the hidden state (source lan-
guage index) generating the target language word at
index j. As usual, a ?null? word is added to the
source sentence. Figure 1 illustrates the mapping be-
tween the usual HMM notation and the HMM align-
ment model.
2.2 Baseline training
All word alignment models we consider are nor-
mally trained using the Expectation Maximization
s1 s1
s2 s3
we know
the way
sabemos       el       camino      null
usual HMM word alignment meaning
Si (hidden) source language word i
Oj (observed) target language word j
aij (transition) distortion model
bij (emission) translation model
Figure 1: Illustration of an HMM for word alignment.
(EM) algorithm (Dempster et al, 1977). The EM
algorithm attempts to maximize the marginal likeli-
hood of the observed data (s, t pairs) by repeatedly
finding a maximal lower bound on the likelihood and
finding the maximal point of the lower bound. The
lower bound is constructed by using posterior proba-
bilities of the hidden alignments (a) and can be opti-
mized in closed form from expected sufficient statis-
tics computed from the posteriors. For the HMM
alignment model, these posteriors can be efficiently
calculated by the Forward-Backward algorithm.
3 Adding agreement constraints
Grac?a et al (2008) introduce an augmentation of the
EM algorithm that uses constraints on posteriors to
guide learning. Such constraints are useful for sev-
eral reasons. As with any unsupervised induction
method, there is no guarantee that the maximum
likelihood parameters correspond to the intended
meaning for the hidden variables, that is, more accu-
rate alignments using the resulting model. Introduc-
ing additional constraints into the model often re-
sults in intractable decoding and search errors (e.g.,
IBM models 4+). The advantage of only constrain-
ing the posteriors during training is that the model
remains simple while respecting more complex re-
quirements. For example, constraints might include
?one word should not translate to many words? or
that translation is approximately symmetric.
The modification is to add a KL-projection step
after the E-step of the EM algorithm. For each sen-
tence pair instance x = (s, t), we find the posterior
987
distribution p?(z|x) (where z are the alignments). In
regular EM, p?(z|x) is used to complete the data and
compute expected counts. Instead, we find the distri-
bution q that is as close as possible to p?(z|x) in KL
subject to constraints specified in terms of expected
values of features f(x, z)
argmin
q
KL(q(z) || p?(z|x)) s.t. Eq[f(x, z)] ? b.
(2)
The resulting distribution q is then used in place
of p?(z|x) to compute sufficient statistics for the
M-step. The algorithm converges to a local maxi-
mum of the log of the marginal likelihood, p?(x) =?
z p?(z,x), penalized by the KL distance of the
posteriors p?(z|x) from the feasible set defined by
the constraints (Grac?a et al, 2008):
Ex[log p?(x)? min
q:Eq [f(x,z)]?b
KL(q(z) || p?(z|x))],
whereEx is expectation over the training data. They
suggest how this framework can be used to encour-
age two word alignment models to agree during
training. We elaborate on their description and pro-
vide details of implementation of the projection in
Equation 2.
3.1 Agreement
Most MT systems train an alignment model in each
direction and then heuristically combine their pre-
dictions. In contrast, Grac?a et al encourage the
models to agree by training them concurrently. The
intuition is that the errors that the two models make
are different and forcing them to agree rules out
errors only made by one model. This is best ex-
hibited in the rare word alignments, where one-
sided ?garbage-collection? phenomenon often oc-
curs (Moore, 2004). This idea was previously pro-
posed by (Matusov et al, 2004; Liang et al, 2006)
although the the objectives differ.
In particular, consider a feature that takes on value
1 whenever source word i aligns to target word j in
the forward model and -1 in the backward model. If
this feature has expected value 0 under the mixture
of the two models, then the forward model and back-
ward model agree on how likely source word i is to
align to target word j. More formally denote the for-
ward model??p (z) and backward model??p (z) where
??p (z) = 0 for z /?
??
Z and ??p (z) = 0 for z /?
??
Z
(
??
Z and
??
Z are possible forward and backward align-
ments). Define a mixture p(z) = 12
??p (z) + 12
??p (z)
for z ?
??
Z ?
??
Z . Restating the constraints that en-
force agreement in this setup: Eq[f(x, z)] = 0 with
fij(x, z) =
8
><
>:
1 z ?
??
Z and zij = 1
?1 z ?
??
Z and zij = 1
0 otherwise
.
3.2 Implementation
EM training of hidden Markov models for word
alignment is described elsewhere (Vogel et al,
1996), so we focus on the projection step:
argmin
q
KL(q(z) || p?(z|x)) s.t. Eq[f(x, z)] = 0.
(3)
The optimization problem in Equation 3 can be effi-
ciently solved in its dual formulation:
argmin
?
log
?
z
p?(z | x) exp {?
>f(x, z)} (4)
where we have solved for the primal variables q as:
q?(z) = p?(z | x) exp{?
>f(x, z)}/Z, (5)
with Z a normalization constant that ensures q sums
to one. We have only one dual variable per con-
straint, and we optimize them by taking a few gra-
dient steps. The partial derivative of the objective
in Equation 4 with respect to feature i is simply
Eq? [fi(x, z)]. So we have reduced the problem to
computing expectations of our features under the
model q. It turns out that for the agreement fea-
tures, this reduces to computing expectations under
the normal HMM model. To see this, we have by the
definition of q? and p?,
q?(z) =
??p (z | x) +??p (z | x)
2
exp{?>f(x, z)}/Z
=
??q (z) +??q (z)
2
.
(To make the algorithm simpler, we have assumed
that the expectation of the feature f0(x, z) =
{1 if z ?
??
Z ; ?1 if z ?
??
Z} is set to zero to
ensure that the two models ??q ,??q are each properly
normalized.) For ??q , we have: (??q is analogous)
??p (z | x)e?
>f(x,z)
=
?
j
??p d(aj |aj ? aj?1)
??p t(tj |saj )
?
ij
e?ijfij(x,zij)
=
?
j,i=aj
??p d(i|i? aj?1)
??p t(tj |si)e?ijfij(x,zij)
=
?
j,i=aj
??p d(i|i? aj?1)
??p ?t(tj |si).
988
Where we have let ??p ?t(tj |si) =
??p t(tj |si)e?ij , and
retained the same form for the model. The final pro-
jection step is detailed in Algorithm1.
Algorithm 1 AgreementProjection(??p ,??p )
1: ?ij ? 0 ?i, j
2: for T iterations do
3: ??p ?t(j|i)?
??p t(tj |si)e?ij ?i, j
4: ??p ?t(i|j)?
??p t(si|tj)e??ij ?i, j
5: ??q ? forwardBackward(??p ?t,
??p d)
6: ??q ? forwardBackward(??p ?t,
??p d)
7: ?ij ? ?ij ?E??q [ai = j] + E??q [aj = i] ?i, j
8: end for
9: return (??q ,??q )
3.3 Decoding
After training, we want to extract a single alignment
from the distribution over alignments allowable for
the model. The standard way to do this is to find
the most probable alignment, using the Viterbi al-
gorithm. Another alternative is to use posterior de-
coding. In posterior decoding, we compute for each
source word i and target word j the posterior prob-
ability under our model that i aligns to j. If that
probability is greater than some threshold, then we
include the point i? j in our final alignment. There
are two main differences between posterior decod-
ing and Viterbi decoding. First, posterior decod-
ing can take better advantage of model uncertainty:
when several likely alignment have high probabil-
ity, posteriors accumulate confidence for the edges
common to many good alignments. Viterbi, by con-
trast, must commit to one high-scoring alignment.
Second, in posterior decoding, the probability that a
0 1 2 3 4 5 6 7 8 0 1 2 3 4 5 6 7 80 ? ? ? ? ? ? ? ? ? 0 ? ? ? ? ? ? ? ? ? it1 ? ? ? ? ? ? ? ? ? 1 ? ? ? ? ? ? ? ? ? was2 ? ? ? ? ? ? ? ? ? 2 ? ? ? ? ? ? ? ? ? an3 ? ? ? ? ? ? ? ? ? 3 ? ? ? ? ? ? ? ? ? animated4 ? ? ? ? ? ? ? ? ? 4 ? ? ? ? ? ? ? ? ? ,5 ? ? ? ? ? ? ? ? ? 5 ? ? ? ? ? ? ? ? ? very6 ? ? ? ? ? ? ? ? ? 6 ? ? ? ? ? ? ? ? ? convivial7 ? ? ? ? ? ? ? ? ? 7 ? ? ? ? ? ? ? ? ? game8 ? ? ? ? ? ? ? ? ? 8 ? ? ? ? ? ? ? ? ? .jugaban
de una manera
animada
y muycordial
. jugaban
de una manera
animada
y muycordial
.
Figure 2: An example of the output of HMM trained on
100k the EPPS data. Left: Baseline training. Right: Us-
ing agreement constraints.
target word aligns to none or more than one word is
much more flexible: it depends on the tuned thresh-
old.
4 Word alignment results
We evaluated the agreement HMM model on two
corpora for which hand-aligned data are widely
available: the Hansards corpus (Och and Ney, 2000)
of English/French parliamentary proceedings and
the Europarl corpus (Koehn, 2002) with EPPS an-
notation (Lambert et al, 2005) of English/Spanish.
Figure 2 shows two machine-generated alignments
of a sentence pair. The black dots represent the ma-
chine alignments and the shading represents the hu-
man annotation (as described in the previous sec-
tion), on the left using the regular HMM model and
on the right using our agreement constraints. The
figure illustrates a problem known as garbage collec-
tion (Brown et al, 1993), where rare source words
tend to align to many target words, since the prob-
ability mass of the rare word translations can be
hijacked to fit the sentence pair. Agreement con-
straints solve this problem, because forward and
backward models cannot agree on the garbage col-
lection solution.
Grac?a et al (2008) show that alignment error rate
(Och and Ney, 2003) can be improved with agree-
ment constraints. Since AER is the standard metric
for alignment quality, we reproduce their results us-
ing all the sentences of length at most 40. For the
Hansards corpus we improve from 15.35 to 7.01 for
the English ? French direction and from 14.45 to
6.80 for the reverse. For English? Spanish we im-
prove from 28.20 to 19.86 and from 27.54 to 19.18
for the reverse. These values are competitive with
other state of the art systems (Liang et al, 2006).
Unfortunately, as was shown by Fraser and Marcu
(2007) AER can have weak correlation with transla-
tion performance as measured by BLEU score (Pa-
pineni et al, 2002), when the alignments are used
to train a phrase-based translation system. Conse-
quently, in addition to AER, we focus on precision
and recall.
Figure 3 shows the change in precision and re-
call with the amount of provided training data for
the Hansards corpus. We see that agreement con-
straints improve both precision and recall when we
989
 65 70 75 80 85 90 95 100  1
 10
 100
 1000
Thousand
s of traini
ng senten
ces
Agreemen
t Baseline
 65 70 75 80 85 90 95 100  1
 10
 100
 1000
Thousand
s of traini
ng senten
ces
Agreemen
t Baseline
Figure 3: Effect of posterior constraints on precision
(left) and recall (right) learning curves for Hansards
En?Fr.
 10 20 30 40 50 60 70 80 90 100  1
 10
 100
 1000
Thousand
s of traini
ng senten
ces
Rare Common Agreemen
t Baseline
 10 20 30 40 50 60 70 80 90 100  1
 10
 100
 1000
Thousand
s of traini
ng senten
ces
Rare Common  Agreeme
nt Baseline
Figure 4: Left: Precision. Right: Recall. Learning curves
for Hansards En?Fr split by rare (at most 5 occurances)
and common words.
use Viterbi decoding, with larger improvements for
small amounts of training data. We see a similar im-
provement on the EPPS corpus.
Motivated by the garbage collection problem, we
also analyze common and rare words separately.
Figure 4 shows precision and recall learning curves
for rare and common words. We see that agreement
constraints improve precision but not recall of rare
words and improve recall but not precision of com-
mon words.
As described above an alternative to Viterbi de-
coding is to accept all alignments that have probabil-
ity above some threshold. By changing the thresh-
old, we can trade off precision and recall. Figure
5 compares this tradeoff for the baseline and agree-
ment model. We see that the precision/recall curve
for agreement is entirely above the baseline curve,
so for any recall value we can achieve higher preci-
sion than the baseline for either corpus. In Figure 6
we break down the same analysis into rare and non
rare words.
Figure 7 shows an example of the same sentence,
using the same model where in one case Viterbi de-
coding was used and in the other case Posterior de-
coding tuned to minimize AER on a development set
 0 0.2 0.4 0.6 0.8 1  0
 0.2 0.4
 0.6 0.8
 1
Recall
PrecisionBaseline Agreemen
t
 0 0.2 0.4 0.6 0.8 1  0
 0.2 0.4
 0.6 0.8
 1
Recall 
PrecisionBaseline Agreemen
t
Figure 5: Precision and recall trade-off for posterior de-
coding with varying threshold. Left: Hansards En?Fr.
Right: EPPS En?Es.
 0 0.2 0.4 0.6 0.8 1  0
 0.2 0.4
 0.6 0.8
 1
Recall
PrecisionBaseline Agreemen
t
 0 0.2 0.4 0.6 0.8 1  0
 0.2 0.4
 0.6 0.8
 1
Recall
PrecisionBaseline Agreemen
t
Figure 6: Precision and recall trade-off for posterior on
Hansards En?Fr. Left: rare words only. Right: common
words only.
was used. An interesting difference is that by using
posterior decoding one can have n-n alignments as
shown in the picture.
A natural question is how to tune the threshold in
order to improve machine translation quality. In the
next section we evaluate and compare the effects of
the different alignments in a phrase based machine
translation system.
5 Phrase-based machine translation
In this section we attempt to investigate whether our
improved alignments produce improved machine
0 1 2 3 4 5 6 7 8 0 1 2 3 4 5 6 7 80 ? ? ? ? ? ? ? ? ? 0 ? ? ? ? ? ? ? ? ? firstly1 ? ? ? ? ? ? ? ? ? 1 ? ? ? ? ? ? ? ? ? ,2 ? ? ? ? ? ? ? ? ? 2 ? ? ? ? ? ? ? ? ? we3 ? ? ? ? ? ? ? ? ? 3 ? ? ? ? ? ? ? ? ? have4 ? ? ? ? ? ? ? ? ? 4 ? ? ? ? ? ? ? ? ? a5 ? ? ? ? ? ? ? ? ? 5 ? ? ? ? ? ? ? ? ? legal6 ? ? ? ? ? ? ? ? ? 6 ? ? ? ? ? ? ? ? ? framework8 ? ? ? ? ? ? ? ? ? 8 ? ? ? ? ? ? ? ? ? .en primero
lugar
, tenemos
un marco
jur??dico
. en primero
lugar
, tenemos
un marco
jur??dico
.
Figure 7: An example of the output of HMM trained on
100k the EPPS data using agreement HMM. Left: Viterbi
decoding. Right: Posterior decoding tuned to minimize
AER. The addition is en-firstly and tenemos-have.
990
translation. In particular we fix a state of the art
machine translation system1 and measure its perfor-
mance when we vary the supplied word alignments.
The baseline system uses GIZA model 4 alignments
and the open source Moses phrase-based machine
translation toolkit2, and performed close to the best
at the competition last year.
For all experiments the experimental setup is as
follows: we lowercase the corpora, and train lan-
guage models from all available data. The reason-
ing behind this is that even if bilingual texts might
be scarce in some domain, monolingual text should
be relatively abundant. We then train the com-
peting alignment models and compute competing
alignments using different decoding schemes. For
each alignment model and decoding type we train
Moses and use MERT optimization to tune its pa-
rameters on a development set. Moses is trained us-
ing the grow-diag-final-and alignment symmetriza-
tion heuristic and using the default distance base
distortion model. We report BLEU scores using a
script available with the baseline system. The com-
peting alignment models are GIZA Model 4, our im-
plementation of the baseline HMM alignment and
our agreement HMM. We would like to stress that
the fair comparison is between the performance of
the baseline HMM and the agreement HMM, since
Model 4 is more complicated and can capture more
structure. However, we will see that for moderate
sized data the agreement HMM performs better than
both its baseline and GIZA Model 4.
5.1 Corpora
In addition to the Hansards corpus and the Europarl
English-Spanish corpus, we used four other corpora
for the machine translation experiments. Table 1
summarizes some statistics of all corpora. The Ger-
man and Finnish corpora are also from Europarl,
while the Czech corpus contains news commentary.
All three were used in recent ACL workshop shared
tasks and are available online3. The Italian corpus
consists of transcribed speech in the travel domain
and was used in the 2007 workshop on spoken lan-
guage translation4. We used the development and
1www.statmt.org/wmt07/baseline.html
2www.statmt.org/moses/
3http://www.statmt.org
4http://iwslt07.itc.it/
Corpus Train Len Test Rare (%) Unk (%)
En, Fr 1018 17.4 1000 0.3, 0.4 0.1, 0.2
En, Es 126 21.0 2000 0.3, 0.5 0.2, 0.3
En, Fi 717 21.7 2000 0.4, 2.5 0.2, 1.8
En, De 883 21.5 2000 0.3, 0.5 0.2, 0.3
En, Cz 57 23.0 2007 2.3, 6.6 1.3, 3.9
En, It 20 9.4 500 3.1, 6.2 1.4, 2.9
Table 1: Statistics of the corpora used in MT evaluation.
The training size is measured in thousands of sentences
and Len refers to average (English) sentence length. Test
is the number of sentences in the test set. Rare and Unk
are the percentage of tokens in the test set that are rare
and unknown in the training data, for each language.
 26 28 30 32 34 36  1000
0
 10000
0
 1e+06
Traini
ng dat
a size 
(sente
nces)
Agree
ment P
ost-pts Model
 4
Baseli
ne Vit
erbi
Figure 8: BLEU score as the amount of training data is
increased on the Hansards corpus for the best decoding
method for each alignment model.
tests sets from the workshops when available. For
Italian corpus we used dev-set 1 as development and
dev-set 2 as test. For Hansards we randomly chose
1000 and 500 sentences from test 1 and test 2 to be
testing and development sets respectively.
Table 1 summarizes the size of the training corpus
in thousands of sentences, the average length of the
English sentences as well as the size of the testing
corpus. We also report the percentage of tokens in
the test corpus that are rare or not encountered in the
training corpus.
5.2 Decoding
Our initial experiments with Viterbi decoding and
posterior decoding showed that for our agreement
model posterior decoding could provide better align-
ment quality. When labeled data is available, we can
tune the threshold to minimize AER. When labeled
data is not available we use a different heuristic to
991
tune the threshold: we choose a threshold that gives
the same number of aligned points as Viterbi decod-
ing produces. In principle, we would like to tune
the threshold by optimizing BLEU score on a devel-
opment set, but that is impractical for experiments
with many pairs of languages. We call this heuristic
posterior-points decoding. As we shall see, it per-
forms well in practice.
5.3 Training data size
The HMM alignment models have a smaller param-
eter space than GIZA Model 4, and consequently we
would expect that they would perform better when
the amount of training data is limited. We found that
this is generally the case, with the margin by which
we beat model 4 slowly decreasing until a crossing
point somewhere in the range of 105 - 106 sentences.
We will see in section 5.3.1 that the Viterbi decoding
performs best for the baseline HMM model, while
posterior decoding performs best for our agreement
HMM model. Figure 8 shows the BLEU score for
the baseline HMM, our agreement model and GIZA
Model 4 as we vary the amount of training data from
104 - 106 sentences. For all but the largest data sizes
we outperform Model 4, with a greater margin at
lower training data sizes. This trend continues as we
lower the amount of training data further. We see a
similar trend with other corpora.
5.3.1 Small to Medium Training Sets
Our next set of experiments look at our perfor-
mance in both directions across our 6 corpora, when
we have small to moderate amounts of training data:
for the language pairs with more than 100,000 sen-
tences, we use only the first 100,000 sentences. Ta-
ble 2 shows the performance of all systems on these
datasets. In the table, post-pts and post-aer stand
for posterior-points decoding and posterior decod-
ing tuned for AER. With the notable exception of
Czech and Italian, our system performs better than
or comparable to both baselines, even though it uses
a much more limited model than GIZA?s Model 4.
The small corpora for which our models do not per-
form as well as GIZA are the ones with a lot of rare
words. We suspect that the reason for this is that we
do not implement smoothing, which has been shown
to be important, especially in situations with a lot of
rare words.
X? En En? X
Base Agree Base Agree
GIZA M4 23.92 17.89
De Viterbi 24.08 23.59 18.15 18.13
post-pts 24.24 24.65(+) 18.18 18.45(+)
GIZA M4 18.29 11.05
Fi Viterbi 18.79 18.38 11.17 11.54
post-pts 18.88 19.45(++) 11.47 12.48(++)
GIZA M4 33.12 26.90
Fr Viterbi 32.42 32.15 25.85 25.48
post-pts 33.06 33.09(?) 25.94 26.54(+)
post-aer 31.81 33.53(+) 26.14 26.68(+)
GIZA M4 30.24 30.09
Es Viterbi 29.65 30.03 29.76 29.85
post-pts 29.91 30.22(++) 29.71 30.16(+)
post-aer 29.65 30.34(++) 29.78 30.20(+)
GIZA M4 51.66 41.99
It Viterbi 52.20 52.09 41.40 41.28
post-pts 51.06 51.14(??) 41.63 41.79(?)
GIZA M4 22.78 12.75
Cz Viterbi 21.25 21.89 12.23 12.33
post-pts 21.37 22.51(++) 12.16 12.47(+)
Table 2: BLEU scores for all language pairs using up to
100k sentences. Results are after MERT optimization.
The marks (++)and (+)denote that agreement with poste-
rior decoding is better by 1 BLEU point and 0.25 BLEU
points respectively than the best baseline HMM model;
analogously for (??), (?); while (?)denotes smaller dif-
ferences.
5.3.2 Larger Training Sets
For four of the corpora we have more than 100
thousand sentences. The performance of the sys-
tems on all the data is shown in Table 3. German
is not included because MERT optimization did not
complete in time. We see that even on over a million
instances, our model sometimes performs better than
GIZA model 4, and always performs better than the
baseline HMM.
6 Conclusions
In this work we have evaluated agreement-
constrained EM training for statistical word align-
ment models. We carefully studied its effects on
word alignment recall and precision. Agreement
training has a different effect on rare and com-
mon words, probably because it fixes different types
of errors. It corrects the garbage collection prob-
lem for rare words, resulting in a higher preci-
sion. The recall improvement in common words
992
X? En En? X
Base Agree Base Agree
GIZA M4 22.78 14.72
Fi Viterbi 22.92 22.89 14.21 14.09
post-pts 23.15 23.43 (+) 14.57 14.74 (?)
GIZA M4 35.65 31.15
Fr Viterbi 35.19 35.17 30.57 29.97
post-pts 35.49 35.95 (+) 29.78 30.02 (?)
post-aer 34.85 35.48 (+) 30.15 30.07 (?)
GIZA M4 31.62 32.40
Es Viterbi 31.75 31.84 31.17 31.09
post-pts 31.88 32.19 (+) 31.16 31.56 (+)
post-aer 31.93 32.29 (+) 31.23 31.36 (?)
Table 3: BLEU scores for all language pairs using all
available data. Markings as in Table 2.
can be explained by the idea that ambiguous com-
mon words are different in the two languages, so the
un-ambiguous choices in one direction can force the
choice for the ambiguous ones in the other through
agreement constraints.
To our knowledge this is the first extensive eval-
uation where improvements in alignment accuracy
lead to improvements in machine translation per-
formance. We tested this hypothesis on six differ-
ent language pairs from three different domains, and
found that the new alignment scheme not only per-
forms better than the baseline, but also improves
over a more complicated, intractable model. In or-
der to get the best results, it appears that posterior
decoding is required for the simplistic HMM align-
ment model. The success of posterior decoding us-
ing our simple threshold tuning heuristic is fortu-
nate since no labeled alignment data are needed:
Viterbi alignments provide a reasonable estimate of
aligned words needed for phrase extraction. The na-
ture of the complicated relationship between word
alignments, the corresponding extracted phrases and
the effects on the final MT system still begs for
better explanations and metrics. We have investi-
gated the distribution of phrase-sizes used in transla-
tion across systems and languages, following recent
investigations (Ayan and Dorr, 2006), but unfortu-
nately found no consistent correlation with BLEU
improvement. Since the alignments we extracted
were better according to all metrics we used, it
should not be too surprising that they yield better
translation performance, but perhaps a better trade-
off can be achieved with a deeper understanding of
the link between alignments and translations.
Acknowledgments
J. V. Grac?a was supported by a fellowship from
Fundac?a?o para a Cie?ncia e Tecnologia (SFRH/ BD/
27528/ 2006). K. Ganchev was partially supported
by NSF ITR EIA 0205448.
References
N. F. Ayan and B. J. Dorr. 2006. Going beyond AER: An
extensive analysis of word alignments and their impact
on MT. In Proc. ACL.
P. F. Brown, S. A. Della Pietra, V. J. Della Pietra, M. J.
Goldsmith, J. Hajic, R. L. Mercer, and S. Mohanty.
1993. But dictionaries are data too. In Proc. HLT.
P. F. Brown, S. Della Pietra, V. J. Della Pietra, and R. L.
Mercer. 1994. The mathematics of statistical machine
translation: Parameter estimation. Computational Lin-
guistics, 19(2):263?311.
A. P. Dempster, N. M. Laird, and D. B. Rubin. 1977.
Maximum likelihood from incomplete data via the EM
algorithm. Royal Statistical Society, Ser. B, 39(1):1?
38.
A. Fraser and D. Marcu. 2007. Measuring word align-
ment quality for statistical machine translation. Com-
put. Linguist., 33(3):293?303.
J. Grac?a, K. Ganchev, and B. Taskar. 2008. Expecta-
tion maximization and posterior constraints. In Proc.
NIPS.
P. Koehn. 2002. Europarl: A multilingual corpus for
evaluation of machine translation.
P. Lambert, A.De Gispert, R. Banchs, and J. B. Marin?o.
2005. Guidelines for word alignment evaluation and
manual alignment. In Language Resources and Eval-
uation, Volume 39, Number 4.
P. Liang, B. Taskar, and D. Klein. 2006. Alignment by
agreement. In Proc. HLT-NAACL.
E. Matusov, Zens. R., and H. Ney. 2004. Symmetric
word alignments for statistical machine translation. In
Proc. COLING.
R. C. Moore. 2004. Improving IBM word-alignment
model 1. In Proc. ACL.
F. J. Och and H. Ney. 2000. Improved statistical align-
ment models. In ACL.
F. J. Och and H. Ney. 2003. A systematic comparison
of various statistical alignment models. Comput. Lin-
guist., 29(1):19?51.
K. Papineni, S. Roukos, T. Ward, and W.-J. Zhu. 2002.
BLEU: A Method for Automatic Evaluation of Ma-
chine Translation. In Proc. ACL.
S. Vogel, H. Ney, and C. Tillmann. 1996. Hmm-based
word alignment in statistical translation. In Proc.
COLING.
993
Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP, pages 369?377,
Suntec, Singapore, 2-7 August 2009. c?2009 ACL and AFNLP
Dependency Grammar Induction via Bitext Projection Constraints
Kuzman Ganchev and Jennifer Gillenwater and Ben Taskar
Department of Computer and Information Science
University of Pennsylvania, Philadelphia PA, USA
{kuzman,jengi,taskar}@seas.upenn.edu
Abstract
Broad-coverage annotated treebanks nec-
essary to train parsers do not exist for
many resource-poor languages. The wide
availability of parallel text and accurate
parsers in English has opened up the pos-
sibility of grammar induction through par-
tial transfer across bitext. We consider
generative and discriminative models for
dependency grammar induction that use
word-level alignments and a source lan-
guage parser (English) to constrain the
space of possible target trees. Unlike
previous approaches, our framework does
not require full projected parses, allowing
partial, approximate transfer through lin-
ear expectation constraints on the space
of distributions over trees. We consider
several types of constraints that range
from generic dependency conservation to
language-specific annotation rules for aux-
iliary verb analysis. We evaluate our ap-
proach on Bulgarian and Spanish CoNLL
shared task data and show that we con-
sistently outperform unsupervised meth-
ods and can outperform supervised learn-
ing for limited training data.
1 Introduction
For English and a handful of other languages,
there are large, well-annotated corpora with a vari-
ety of linguistic information ranging from named
entity to discourse structure. Unfortunately, for
the vast majority of languages very few linguis-
tic resources are available. This situation is
likely to persist because of the expense of creat-
ing annotated corpora that require linguistic exper-
tise (Abeill?, 2003). On the other hand, parallel
corpora between many resource-poor languages
and resource-rich languages are ample, motivat-
ing recent interest in transferring linguistic re-
sources from one language to another via parallel
text. For example, several early works (Yarowsky
and Ngai, 2001; Yarowsky et al, 2001; Merlo
et al, 2002) demonstrate transfer of shallow pro-
cessing tools such as part-of-speech taggers and
noun-phrase chunkers by using word-level align-
ment models (Brown et al, 1994; Och and Ney,
2000).
Alshawi et al (2000) and Hwa et al (2005)
explore transfer of deeper syntactic structure:
dependency grammars. Dependency and con-
stituency grammar formalisms have long coex-
isted and competed in linguistics, especially be-
yond English (Mel?c?uk, 1988). Recently, depen-
dency parsing has gained popularity as a simpler,
computationally more efficient alternative to con-
stituency parsing and has spurred several super-
vised learning approaches (Eisner, 1996; Yamada
and Matsumoto, 2003a; Nivre and Nilsson, 2005;
McDonald et al, 2005) as well as unsupervised in-
duction (Klein and Manning, 2004; Smith and Eis-
ner, 2006). Dependency representation has been
used for language modeling, textual entailment
and machine translation (Haghighi et al, 2005;
Chelba et al, 1997; Quirk et al, 2005; Shen et al,
2008), to name a few tasks.
Dependency grammars are arguably more ro-
bust to transfer since syntactic relations between
aligned words of parallel sentences are better con-
served in translation than phrase structure (Fox,
2002; Hwa et al, 2005). Nevertheless, sev-
eral challenges to accurate training and evalua-
tion from aligned bitext remain: (1) partial word
alignment due to non-literal or distant transla-
tion; (2) errors in word alignments and source lan-
guage parses, (3) grammatical annotation choices
that differ across languages and linguistic theo-
ries (e.g., how to analyze auxiliary verbs, conjunc-
tions).
In this paper, we present a flexible learning
369
framework for transferring dependency grammars
via bitext using the posterior regularization frame-
work (Gra?a et al, 2008). In particular, we ad-
dress challenges (1) and (2) by avoiding com-
mitment to an entire projected parse tree in the
target language during training. Instead, we ex-
plore formulations of both generative and discrim-
inative probabilistic models where projected syn-
tactic relations are constrained to hold approxi-
mately and only in expectation. Finally, we ad-
dress challenge (3) by introducing a very small
number of language-specific constraints that dis-
ambiguate arbitrary annotation choices.
We evaluate our approach by transferring from
an English parser trained on the Penn treebank to
Bulgarian and Spanish. We evaluate our results
on the Bulgarian and Spanish corpora from the
CoNLL X shared task. We see that our transfer
approach consistently outperforms unsupervised
methods and, given just a few (2 to 7) language-
specific constraints, performs comparably to a su-
pervised parser trained on a very limited corpus
(30 - 140 training sentences).
2 Approach
At a high level our approach is illustrated in Fig-
ure 1(a). A parallel corpus is word-level aligned
using an alignment toolkit (Gra?a et al, 2009) and
the source (English) is parsed using a dependency
parser (McDonald et al, 2005). Figure 1(b) shows
an aligned sentence pair example where depen-
dencies are perfectly conserved across the align-
ment. An edge from English parent p to child c is
called conserved if word p aligns to word p? in the
second language, c aligns to c? in the second lan-
guage, and p? is the parent of c?. Note that we are
not restricting ourselves to one-to-one alignments
here; p, c, p?, and c? can all also align to other
words. After filtering to identify well-behaved
sentences and high confidence projected depen-
dencies, we learn a probabilistic parsing model us-
ing the posterior regularization framework (Gra?a
et al, 2008). We estimate both generative and dis-
criminative models by constraining the posterior
distribution over possible target parses to approxi-
mately respect projected dependencies and other
rules which we describe below. In our experi-
ments we evaluate the learned models on depen-
dency treebanks (Nivre et al, 2007).
Unfortunately the sentence in Figure 1(b) is
highly unusual in its amount of dependency con-
servation. To get a feel for the typical case, we
used off-the-shelf parsers (McDonald et al, 2005)
for English, Spanish and Bulgarian on two bi-
texts (Koehn, 2005; Tiedemann, 2007) and com-
pared several measures of dependency conserva-
tion. For the English-Bulgarian corpus, we ob-
served that 71.9% of the edges we projected were
edges in the corpus, and we projected on average
2.7 edges per sentence (out of 5.3 tokens on aver-
age). For Spanish, we saw conservation of 64.4%
and an average of 5.9 projected edges per sentence
(out of 11.5 tokens on average).
As these numbers illustrate, directly transfer-
ring information one dependency edge at a time
is unfortunately error prone for two reasons. First,
parser and word alignment errors cause much of
the transferred information to be wrong. We deal
with this problem by constraining groups of edges
rather than a single edge. For example, in some
sentence pair we might find 10 edges that have
both end points aligned and can be transferred.
Rather than requiring our target language parse to
contain each of the 10 edges, we require that the
expected number of edges from this set is at least
10?, where ? is a strength parameter. This gives
the parser freedom to have some uncertainty about
which edges to include, or alternatively to choose
to exclude some of the transferred edges.
A more serious problem for transferring parse
information across languages are structural differ-
ences and grammar annotation choices between
the two languages. For example dealing with aux-
iliary verbs and reflexive constructions. Hwa et al
(2005) also note these problems and solve them by
introducing dozens of rules to transform the trans-
ferred parse trees. We discuss these differences
in detail in the experimental section and use our
framework introduce a very small number of rules
to cover the most common structural differences.
3 Parsing Models
We explored two parsing models: a generative
model used by several authors for unsupervised in-
duction and a discriminative model used for fully
supervised training.
The discriminative parser is based on the
edge-factored model and features of the MST-
Parser (McDonald et al, 2005). The parsing
model defines a conditional distribution p?(z | x)
over each projective parse tree z for a particular
sentence x, parameterized by a vector ?. The prob-
370
(a)
(b)
Figure 1: (a) Overview of our grammar induction approach via bitext: the source (English) is parsed and word-aligned with
target; after filtering, projected dependencies define constraints over target parse tree space, providing weak supervision for
learning a target grammar. (b) An example word-aligned sentence pair with perfectly projected dependencies.
ability of any particular parse is
p?(z | x) ?
?
z?z
e???(z,x), (1)
where z is a directed edge contained in the parse
tree z and ? is a feature function. In the fully su-
pervised experiments we run for comparison, pa-
rameter estimation is performed by stochastic gra-
dient ascent on the conditional likelihood func-
tion, similar to maximum entropy models or con-
ditional random fields. One needs to be able to
compute expectations of the features ?(z,x) under
the distribution p?(z | x). A version of the inside-
outside algorithm (Lee and Choi, 1997) performs
this computation. Viterbi decoding is done using
Eisner?s algorithm (Eisner, 1996).
We also used a generative model based on de-
pendency model with valence (Klein and Man-
ning, 2004). Under this model, the probability of
a particular parse z and a sentence with part of
speech tags x is given by
p?(z,x) = proot(r(x)) ? (2)
(?
z?z
p?stop(zp, zd, vz) pchild(zp, zd, zc)
)
?
(?
x?x
pstop(x, left, vl) pstop(x, right, vr)
)
where r(x) is the part of speech tag of the root
of the parse tree z, z is an edge from parent zp
to child zc in direction zd, either left or right, and
vz indicates valency?false if zp has no other chil-
dren further from it in direction zd than zc, true
otherwise. The valencies vr/vl are marked as true
if x has any children on the left/right in z, false
otherwise.
4 Posterior Regularization
Gra?a et al (2008) introduce an estimation frame-
work that incorporates side-information into un-
supervised problems in the form of linear con-
straints on posterior expectations. In grammar
transfer, our basic constraint is of the form: the
expected proportion of conserved edges in a sen-
tence pair is at least ? (the exact proportion we
used was 0.9, which was determined using un-
labeled data as described in Section 5). Specifi-
cally, let Cx be the set of directed edges projected
from English for a given sentence x, then given
a parse z, the proportion of conserved edges is
f(x, z) = 1|Cx|
?
z?z 1(z ? Cx) and the expected
proportion of conserved edges under distribution
p(z | x) is
Ep[f(x, z)] =
1
|Cx|
?
z?Cx
p(z | x).
The posterior regularization framework (Gra?a
et al, 2008) was originally defined for gener-
ative unsupervised learning. The standard ob-
jective is to minimize the negative marginal
log-likelihood of the data : E?[? log p?(x)] =
E?[? log
?
z p?(z,x)] over the parameters ? (we
use E? to denote expectation over the sample sen-
tences x). We typically also add standard regular-
ization term on ?, resulting from a parameter prior
? log p(?) = R(?), where p(?) is Gaussian for the
MST-Parser models and Dirichlet for the valence
model.
To introduce supervision into the model, we de-
fine a set Qx of distributions over the hidden vari-
ables z satisfying the desired posterior constraints
in terms of linear equalities or inequalities on fea-
ture expectations (we use inequalities in this pa-
per):
Qx = {q(z) : E[f(x, z)] ? b}.
371
Basic Uni-gram Features
xi-word, xi-pos
xi-word
xi-pos
xj-word, xj-pos
xj-word
xj-pos
Basic Bi-gram Features
xi-word, xi-pos, xj-word, xj-pos
xi-pos, xj-word, xj-pos
xi-word, xj-word, xj-pos
xi-word, xi-pos, xj-pos
xi-word, xi-pos, xj-word
xi-word, xj-word
xi-pos, xj-pos
In Between POS Features
xi-pos, b-pos, xj-pos
Surrounding Word POS Features
xi-pos, xi-pos+1, xj-pos-1, xj-pos
xi-pos-1, xi-pos, xj-pos-1, xj-pos
xi-pos, xi-pos+1, xj-pos, xj-pos+1
xi-pos-1, xi-pos, xj-pos, xj-pos+1
Table 1: Features used by the MSTParser. For each edge (i, j), xi-word is the parent word and xj-word is the child word,
analogously for POS tags. The +1 and -1 denote preceeding and following tokens in the sentence, while b denotes tokens
between xi and xj .
In this paper, for example, we use the conserved-
edge-proportion constraint as defined above. The
marginal log-likelihood objective is then modi-
fied with a penalty for deviation from the de-
sired set of distributions, measured by KL-
divergence from the set Qx, KL(Qx||p?(z|x)) =
minq?Qx KL(q(z)||p?(z|x)). The generative
learning objective is to minimize:
E?[? log p?(x)] +R(?) + E?[KL(Qx||p?(z | x))].
For discriminative estimation (Ganchev et al,
2008), we do not attempt to model the marginal
distribution of x, so we simply have the two regu-
larization terms:
R(?) + E?[KL(Qx||p?(z | x))].
Note that the idea of regularizing moments is re-
lated to generalized expectation criteria algorithm
of Mann and McCallum (2007), as we discuss in
the related work section below. In general, the
objectives above are not convex in ?. To opti-
mize these objectives, we follow an Expectation
Maximization-like scheme. Recall that standard
EM iterates two steps. An E-step computes a prob-
ability distribution over the model?s hidden vari-
ables (posterior probabilities) and an M-step that
updates the model?s parameters based on that dis-
tribution. The posterior-regularized EM algorithm
leaves the M-step unchanged, but involves project-
ing the posteriors onto a constraint set after they
are computed for each sentence x:
argmin
q
KL(q(z) ? p?(z|x))
s.t. Eq[f(x, z)] ? b,
(3)
where p?(z|x) are the posteriors. The new poste-
riors q(z) are used to compute sufficient statistics
for this instance and hence to update the model?s
parameters in the M-step for either the generative
or discriminative setting.
The optimization problem in Equation 3 can be
efficiently solved in its dual formulation:
argmin
??0
b>?+log
?
z
p?(z | x) exp {??
>f(x, z)}.
(4)
Given ?, the primal solution is given by: q(z) =
p?(z | x) exp{??>f(x, z)}/Z, where Z is a nor-
malization constant. There is one dual variable per
expectation constraint, and we can optimize them
by projected gradient descent, similar to log-linear
model estimation. The gradient with respect to ?
is given by: b ? Eq[f(x, z)], so it involves com-
puting expectations under the distribution q(z).
This remains tractable as long as features factor by
edge, f(x, z) =
?
z?z f(x, z), because that en-
sures that q(z) will have the same form as p?(z |
x). Furthermore, since the constraints are per in-
stance, we can use incremental or online version
of EM (Neal and Hinton, 1998), where we update
parameters ? after posterior-constrained E-step on
each instance x.
5 Experiments
We conducted experiments on two languages:
Bulgarian and Spanish, using each of the pars-
ing models. The Bulgarian experiments transfer a
parser from English to Bulgarian, using the Open-
Subtitles corpus (Tiedemann, 2007). The Span-
ish experiments transfer from English to Spanish
using the Spanish portion of the Europarl corpus
(Koehn, 2005). For both corpora, we performed
word alignments with the open source PostCAT
(Gra?a et al, 2009) toolkit. We used the Tokyo
tagger (Tsuruoka and Tsujii, 2005) to POS tag
the English tokens, and generated parses using
the first-order model of McDonald et al (2005)
with projective decoding, trained on sections 2-21
of the Penn treebank with dependencies extracted
using the head rules of Yamada and Matsumoto
(2003b). For Bulgarian we trained the Stanford
POS tagger (Toutanova et al, 2003) on the Bul-
372
Discriminative model Generative model
Bulgarian Spanish Bulgarian Spanish
no rules 2 rules 7 rules no rules 3 rules no rules 2 rules 7 rules no rules 3 rules
Baseline 63.8 72.1 72.6 67.6 69.0 66.5 69.1 71.0 68.2 71.3
Post.Reg. 66.9 77.5 78.3 70.6 72.3 67.8 70.7 70.8 69.5 72.8
Table 2: Comparison between transferring a single tree of edges and transferring all possible projected edges. The transfer
models were trained on 10k sentences of length up to 20, all models tested on CoNLL train sentences of up to 10 words.
Punctuation was stripped at train time.
gtreebank corpus from CoNLL X. The Spanish
Europarl data was POS tagged with the FreeLing
language analyzer (Atserias et al, 2006). The dis-
criminative model used the same features as MST-
Parser, summarized in Table 1.
In order to evaluate our method, we a baseline
inspired by Hwa et al (2005). The baseline con-
structs a full parse tree from the incomplete and
possibly conflicting transferred edges using a sim-
ple random process. We start with no edges and
try to add edges one at a time verifying at each
step that it is possible to complete the tree. We
first try to add the transferred edges in random or-
der, then for each orphan node we try all possible
parents (both in random order). We then use this
full labeling as supervision for a parser. Note that
this baseline is very similar to the first iteration of
our model, since for a large corpus the different
random choices made in different sentences tend
to smooth each other out. We also tried to cre-
ate rules for the adoption of orphans, but the sim-
ple rules we tried added bias and performed worse
than the baseline we report. Table 2 shows at-
tachment accuracy of our method and the baseline
for both language pairs under several conditions.
By attachment accuracy we mean the fraction of
words assigned the correct parent. The experimen-
tal details are described in this section. Link-left
baselines for these corpora are much lower: 33.8%
and 27.9% for Bulgarian and Spanish respectively.
5.1 Preprocessing
Preliminary experiments showed that our word
alignments were not always appropriate for syn-
tactic transfer, even when they were correct for
translation. For example, the English ?bike/V?
could be translated in French as ?aller/V en
v?lo/N?, where the word ?bike? would be aligned
with ?v?lo?. While this captures some of the se-
mantic shared information in the two languages,
we have no expectation that the noun ?v?lo?
will have a similar syntactic behavior to the verb
?bike?. To prevent such false transfer, we filter
out alignments between incompatible POS tags. In
both language pairs, filtering out noun-verb align-
ments gave the biggest improvement.
Both corpora also contain sentence fragments,
either because of question responses or frag-
mented speech in movie subtitles or because of
voting announcements and similar formulaic sen-
tences in the parliamentary proceedings. We over-
come this problem by filtering out sentences that
do not have a verb as the English root or for which
the English root is not aligned to a verb in the
target language. For the subtitles corpus we also
remove sentences that end in an ellipsis or con-
tain more than one comma. Finally, following
(Klein and Manning, 2004) we strip out punctu-
ation from the sentences. For the discriminative
model this did not affect results significantly but
improved them slightly in most cases. We found
that the generative model gets confused by punctu-
ation and tends to predict that periods at the end of
sentences are the parents of words in the sentence.
Our basic model uses constraints of the form:
the expected proportion of conserved edges in a
sentence pair is at least ? = 90%.1
5.2 No Language-Specific Rules
We call the generic model described above ?no-
rules? to distinguish it from the language-specific
constraints we introduce in the sequel. The no
rules columns of Table 2 summarize the perfor-
mance in this basic setting. Discriminative models
outperform the generative models in the majority
of cases. The left panel of Table 3 shows the most
common errors by child POS tag, as well as by
true parent and guessed parent POS tag.
Figure 2 shows that the discriminative model
continues to improve with more transfer-type data
1We chose ? in the following way: we split the unlabeled
parallel text into two portions. We trained a models with dif-
ferent ? on one portion and ran it on the other portion. We
chose the model with the highest fraction of conserved con-
straints on the second portion.
373
 0.52 0.54 0.56 0.58 0.6 0.62 0.64 0.66 0.68
 0.1
 1
 10
accuracy (%)
trainin
g data 
size (th
ousand
s of se
ntence
s)
our me
thod baselin
e
Figure 2: Learning curve of the discriminative no-rules
transfer model on Bulgarian bitext, testing on CoNLL train
sentences of up to 10 words.
Figure 3: A Spanish example where an auxiliary verb dom-
inates the main verb.
up to at least 40 thousand sentences.
5.3 Annotation guidelines and constraints
Using the straightforward approach outlined
above is a dramatic improvement over the standard
link-left baseline (and the unsupervised generative
model as we discuss below), however it doesn?t
have any information about the annotation guide-
lines used for the testing corpus. For example, the
Bulgarian corpus has an unusual treatment of non-
finite clauses. Figure 4 shows an example. We see
that the ?da? is the parent of both the verb and its
object, which is different than the treatment in the
English corpus.
We propose to deal with these annotation dis-
similarities by creating very simple rules. For
Spanish, we have three rules. The first rule sets
main verbs to dominate auxiliary verbs. Specifi-
cally, whenever an auxiliary precedes a main verb
the main verb becomes its parent and adopts its
children; if there is only one main verb it becomes
the root of the sentence; main verbs also become
Figure 4: An example where transfer fails because of
different handling of reflexives and nonfinite clauses. The
alignment links provide correct glosses for Bulgarian words.
?Bh? is a past tense marker while ?se? is a reflexive marker.
parents of pronouns, adverbs, and common nouns
that directly preceed auxiliary verbs. By adopt-
ing children we mean that we change the parent
of transferred edges to be the adopting node. The
second Spanish rule states that the first element
of an adjective-noun or noun-adjective pair domi-
nates the second; the first element also adopts the
children of the second element. The third and fi-
nal Spanish rule sets all prepositions to be chil-
dren of the first main verb in the sentence, unless
the preposition is a ?de? located between two noun
phrases. In this later case, we set the closest noun
in the first of the two noun phrases as the preposi-
tion?s parent.
For Bulgarian the first rule is that ?da? should
dominate all words until the next verb and adopt
their noun, preposition, particle and adverb chil-
dren. The second rule is that auxiliary verbs
should dominate main verbs and adopt their chil-
dren. We have a list of 12 Bulgarian auxiliary
verbs. The ?seven rules? experiments add rules for
5 more words similar to the rule for ?da?, specif-
ically ?qe?, ?li?, ?kakvo?, ?ne?, ?za?. Table 3
compares the errors for different linguistic rules.
When we train using the ?da? rule and the rules for
auxiliary verbs, the model learns that main verbs
attach to auxiliary verbs and that ?da? dominates
its nonfinite clause. This causes an improvement
in the attachment of verbs, and also drastically re-
duces words being attached to verbs instead of par-
ticles. The latter is expected because ?da? is an-
alyzed as a particle in the Bulgarian POS tagset.
We see an improvement in root/verb confusions
since ?da? is sometimes errenously attached to a
the following verb rather than being the root of the
sentence.
The rightmost panel of Table 3 shows similar
analysis when we also use the rules for the five
other closed-class words. We see an improvement
in attachments in all categories, but no qualitative
change is visible. The reason for this is probably
that these words are relatively rare, but by encour-
aging the model to add an edge, it also rules out in-
correct edges that would cross it. Consequently we
are seeing improvements not only directly from
the constraints we enforce but also indirectly as
types of edges that tend to get ruled out.
5.4 Generative parser
The generative model we use is a state of the art
model for unsupervised parsing and is our only
374
No Rules Two Rules Seven Rules
child POS parent POS
acc(%) errors errors
V 65.2 2237 T/V 2175
N 73.8 1938 V/V 1305
P 58.5 1705 N/V 1112
R 70.3 961 root/V 555
child POS parent POS
acc(%) errors errors
N 78.7 1572 N/V 938
P 70.2 1224 V/V 734
V 84.4 1002 V/N 529
R 79.3 670 N/N 376
child POS parent POS
acc(%) errors errors
N 79.3 1532 N/V 1116
P 75.7 998 V/V 560
R 69.3 993 V/N 507
V 86.2 889 N/N 450
Table 3: Top 4 discriminative parser errors by child POS tag and true/guess parent POS tag in the Bulgarian CoNLL train data
of length up to 10. Training with no language-specific rules (left); two rules (center); and seven rules (right). POS meanings:
V verb, N noun, P pronoun, R preposition, T particle. Accuracies are by child or parent truth/guess POS tag.
 0.6 0.65 0.7 0.75
 20 4
0 60
 80 1
00 12
0 140
accuracy (%)
supervis
ed traini
ng data s
ize
supervis
ed no rules two rule
s
seven ru
les
 0.65 0.7 0.75 0.8
 20 4
0 60
 80 1
00 12
0 140
accuracy (%)
supervis
ed traini
ng data s
ize
supervis
ed no rules three rul
es
 0.65 0.7 0.75 0.8
 20 4
0 60
 80 1
00 12
0 140
accuracy (%)
supervis
ed train
ing data
 size
supervis
ed no rules two rule
s
seven ru
les
 0.65 0.7 0.75 0.8
 20 4
0 60
 80 1
00 12
0 140
accuracy (%)
supervis
ed train
ing data
 sizesupervis
ed no rules three ru
les
Figure 5: Comparison to parsers with supervised estimation and transfer. Top: Generative. Bottom: Discriminative. Left:
Bulgarian. Right: Spanish. The transfer models were trained on 10k sentences all of length at most 20, all models tested
on CoNLL train sentences of up to 10 words. The x-axis shows the number of examples used to train the supervised model.
Boxes show first and third quartile, whiskers extend to max and min, with the line passing through the median. Supervised
experiments used 30 random samples from CoNLL train.
fully unsupervised baseline. As smoothing we add
a very small backoff probability of 4.5 ? 10?5 to
each learned paramter. Unfortunately, we found
generative model performance was disappointing
overall. The maximum unsupervised accuracy it
achieved on the Bulgarian data is 47.6% with ini-
tialization from Klein and Manning (2004) and
this result is not stable. Changing the initialization
parameters, training sample, or maximum sen-
tence length used for training drastically affected
the results, even for samples with several thousand
sentences. When we use the transferred informa-
tion to constrain the learning, EM stabilizes and
achieves much better performance. Even setting
all parameters equal at the outset does not prevent
the model from learning the dependency structure
of the aligned language. The top panels in Figure 5
show the results in this setting. We see that perfor-
mance is still always below the accuracy achieved
by supervised training on 20 annotated sentences.
However, the improvement in stability makes the
algorithm much more usable. As we shall see be-
low, the discriminative parser performs even better
than the generative model.
5.5 Discriminative parser
We trained our discriminative parser for 100 iter-
ations of online EM with a Gaussian prior vari-
ance of 100. Results for the discriminative parser
are shown in the bottom panels of Figure 5. The
supervised experiments are given to provide con-
text for the accuracies. For Bulgarian, we see that
without any hints about the annotation guidelines,
the transfer system performs better than an unsu-
375
pervised parser, comparable to a supervised parser
trained on 10 sentences. However, if we spec-
ify just the two rules for ?da? and verb conjuga-
tions performance jumps to that of training on 60-
70 fully labeled sentences. If we have just a lit-
tle more prior knowledge about how closed-class
words are handled, performance jumps above 140
fully labeled sentence equivalent.
We observed another desirable property of the
discriminative model. While the generative model
can get confused and perform poorly when the
training data contains very long sentences, the dis-
criminative parser does not appear to have this
drawback. In fact we observed that as the maxi-
mum training sentence length increased, the pars-
ing performance also improved.
6 Related Work
Our work most closely relates to Hwa et al (2005),
who proposed to learn generative dependency
grammars using Collins? parser (Collins, 1999) by
constructing full target parses via projected de-
pendencies and completion/transformation rules.
Hwa et al (2005) found that transferring depen-
dencies directly was not sufficient to get a parser
with reasonable performance, even when both
the source language parses and the word align-
ments are performed by hand. They adjusted for
this by introducing on the order of one or two
dozen language-specific transformation rules to
complete target parses for unaligned words and
to account for diverging annotation rules. Trans-
ferring from English to Spanish in this way, they
achieve 72.1% and transferring to Chinese they
achieve 53.9%.
Our learning method is very closely related to
the work of (Mann and McCallum, 2007; Mann
and McCallum, 2008) who concurrently devel-
oped the idea of using penalties based on pos-
terior expectations of features not necessarily in
the model in order to guide learning. They call
their method generalized expectation constraints
or alternatively expectation regularization. In this
volume (Druck et al, 2009) use this framework
to train a dependency parser based on constraints
stated as corpus-wide expected values of linguis-
tic rules. The rules select a class of edges (e.g.
auxiliary verb to main verb) and require that the
expectation of these be close to some value. The
main difference between this work and theirs is
the source of the information (a linguistic infor-
mant vs. cross-lingual projection). Also, we de-
fine our regularization with respect to inequality
constraints (the model is not penalized for exceed-
ing the required model expectations), while they
require moments to be close to an estimated value.
We suspect that the two learning methods could
perform comparably when they exploit similar in-
formation.
7 Conclusion
In this paper, we proposed a novel and effec-
tive learning scheme for transferring dependency
parses across bitext. By enforcing projected de-
pendency constraints approximately and in expec-
tation, our framework allows robust learning from
noisy partially supervised target sentences, instead
of committing to entire parses. We show that dis-
criminative training generally outperforms gener-
ative approaches even in this very weakly super-
vised setting. By adding easily specified language-
specific constraints, our models begin to rival
strong supervised baselines for small amounts of
data. Our framework can handle a wide range of
constraints and we are currently exploring richer
syntactic constraints that involve conservation of
multiple edge constructions as well as constraints
on conservation of surface length of dependen-
cies.
Acknowledgments
This work was partially supported by an Integra-
tive Graduate Education and Research Trainee-
ship grant from National Science Foundation
(NSFIGERT 0504487), by ARO MURI SUB-
TLE W911NF-07-1-0216 and by the European
Projects AsIsKnown (FP6-028044) and LTfLL
(FP7-212578).
References
A. Abeille?. 2003. Treebanks: Building and Using
Parsed Corpora. Springer.
H. Alshawi, S. Bangalore, and S. Douglas. 2000.
Learning dependency translation models as collec-
tions of finite state head transducers. Computational
Linguistics, 26(1).
J. Atserias, B. Casas, E. Comelles, M. Gonza?lez,
L. Padro?, and M. Padro?. 2006. Freeling 1.3: Syn-
tactic and semantic services in an open-source nlp
library. In Proc. LREC, Genoa, Italy.
376
P. F. Brown, S. Della Pietra, V. J. Della Pietra, and R. L.
Mercer. 1994. The mathematics of statistical ma-
chine translation: Parameter estimation. Computa-
tional Linguistics, 19(2):263?311.
C. Chelba, D. Engle, F. Jelinek, V. Jimenez, S. Khudan-
pur, L. Mangu, H. Printz, E. Ristad, R. Rosenfeld,
A. Stolcke, and D. Wu. 1997. Structure and perfor-
mance of a dependency language model. In Proc.
Eurospeech.
M. Collins. 1999. Head-Driven Statistical Models for
Natural Language Parsing. Ph.D. thesis, University
of Pennsylvania.
G. Druck, G. Mann, and A. McCallum. 2009. Semi-
supervised learning of dependency parsers using
generalized expectation criteria. In Proc. ACL.
J. Eisner. 1996. Three new probabilistic models for de-
pendency parsing: an exploration. In Proc. CoLing.
H. Fox. 2002. Phrasal cohesion and statistical machine
translation. In Proc. EMNLP, pages 304?311.
K. Ganchev, J. Graca, J. Blitzer, and B. Taskar.
2008. Multi-view learning over structured and non-
identical outputs. In Proc. UAI.
J. Grac?a, K. Ganchev, and B. Taskar. 2008. Expec-
tation maximization and posterior constraints. In
Proc. NIPS.
J. Grac?a, K. Ganchev, and B. Taskar. 2009. Post-
cat - posterior constrained alignment toolkit. In The
Third Machine Translation Marathon.
A. Haghighi, A. Ng, and C. Manning. 2005. Ro-
bust textual inference via graph matching. In Proc.
EMNLP.
R. Hwa, P. Resnik, A. Weinberg, C. Cabezas, and
O. Kolak. 2005. Bootstrapping parsers via syntactic
projection across parallel texts. Natural Language
Engineering, 11:11?311.
D. Klein and C. Manning. 2004. Corpus-based induc-
tion of syntactic structure: Models of dependency
and constituency. In Proc. of ACL.
P. Koehn. 2005. Europarl: A parallel corpus for statis-
tical machine translation. In MT Summit.
S. Lee and K. Choi. 1997. Reestimation and best-
first parsing algorithm for probabilistic dependency
grammar. In In WVLC-5, pages 41?55.
G. Mann and A. McCallum. 2007. Simple, robust,
scalable semi-supervised learning via expectation
regularization. In Proc. ICML.
G. Mann and A. McCallum. 2008. Generalized expec-
tation criteria for semi-supervised learning of con-
ditional random fields. In Proc. ACL, pages 870 ?
878.
R. McDonald, K. Crammer, and F. Pereira. 2005. On-
line large-margin training of dependency parsers. In
Proc. ACL, pages 91?98.
I. Mel?c?uk. 1988. Dependency syntax: theory and
practice. SUNY. inci.
P. Merlo, S. Stevenson, V. Tsang, and G. Allaria. 2002.
A multilingual paradigm for automatic verb classifi-
cation. In Proc. ACL.
R. M. Neal and G. E. Hinton. 1998. A new view of the
EM algorithm that justifies incremental, sparse and
other variants. In M. I. Jordan, editor, Learning in
Graphical Models, pages 355?368. Kluwer.
J. Nivre and J. Nilsson. 2005. Pseudo-projective de-
pendency parsing. In Proc. ACL.
J. Nivre, J. Hall, S. Ku?bler, R. McDonald, J. Nils-
son, S. Riedel, and D. Yuret. 2007. The CoNLL
2007 shared task on dependency parsing. In Proc.
EMNLP-CoNLL.
F. J. Och and H. Ney. 2000. Improved statistical align-
ment models. In Proc. ACL.
C. Quirk, A. Menezes, and C. Cherry. 2005. De-
pendency treelet translation: syntactically informed
phrasal smt. In Proc. ACL.
L. Shen, J. Xu, and R. Weischedel. 2008. A new
string-to-dependency machine translation algorithm
with a target dependency language model. In Proc.
of ACL.
N. Smith and J. Eisner. 2006. Annealing structural
bias in multilingual weighted grammar induction. In
Proc. ACL.
J. Tiedemann. 2007. Building a multilingual parallel
subtitle corpus. In Proc. CLIN.
K. Toutanova, D. Klein, C. Manning, and Y. Singer.
2003. Feature-rich part-of-speech tagging with a
cyclic dependency network. In Proc. HLT-NAACL.
Y. Tsuruoka and J. Tsujii. 2005. Bidirectional infer-
ence with the easiest-first strategy for tagging se-
quence data. In Proc. HLT/EMNLP.
H. Yamada and Y. Matsumoto. 2003a. Statistical de-
pendency analysis with support vector machines. In
Proc. IWPT, pages 195?206.
H. Yamada and Y. Matsumoto. 2003b. Statistical de-
pendency analysis with support vector machines. In
Proc. IWPT.
D. Yarowsky and G. Ngai. 2001. Inducing multilin-
gual pos taggers and np bracketers via robust pro-
jection across aligned corpora. In Proc. NAACL.
D. Yarowsky, G. Ngai, and R. Wicentowski. 2001.
Inducing multilingual text analysis tools via robust
projection across aligned corpora. In Proc. HLT.
377
Max-Margin Parsing
Ben Taskar
Computer Science Dept.
Stanford University
btaskar@cs.stanford.edu
Dan Klein
Computer Science Dept.
Stanford University
klein@cs.stanford.edu
Michael Collins
CS and AI Lab
MIT
mcollins@csail.mit.edu
Daphne Koller
Computer Science Dept.
Stanford University
koller@cs.stanford.edu
Christopher Manning
Computer Science Dept.
Stanford University
manning@cs.stanford.edu
Abstract
We present a novel discriminative approach to parsing
inspired by the large-margin criterion underlying sup-
port vector machines. Our formulation uses a factor-
ization analogous to the standard dynamic programs for
parsing. In particular, it allows one to efficiently learn
a model which discriminates among the entire space of
parse trees, as opposed to reranking the top few candi-
dates. Our models can condition on arbitrary features of
input sentences, thus incorporating an important kind of
lexical information without the added algorithmic com-
plexity of modeling headedness. We provide an efficient
algorithm for learning such models and show experimen-
tal evidence of the model?s improved performance over
a natural baseline model and a lexicalized probabilistic
context-free grammar.
1 Introduction
Recent work has shown that discriminative
techniques frequently achieve classification ac-
curacy that is superior to generative techniques,
over a wide range of tasks. The empirical utility
of models such as logistic regression and sup-
port vector machines (SVMs) in flat classifica-
tion tasks like text categorization, word-sense
disambiguation, and relevance routing has been
repeatedly demonstrated. For sequence tasks
like part-of-speech tagging or named-entity ex-
traction, recent top-performing systems have
also generally been based on discriminative se-
quence models, like conditional Markov mod-
els (Toutanova et al, 2003) or conditional ran-
dom fields (Lafferty et al, 2001).
A number of recent papers have consid-
ered discriminative approaches for natural lan-
guage parsing (Johnson et al, 1999; Collins,
2000; Johnson, 2001; Geman and Johnson,
2002; Miyao and Tsujii, 2002; Clark and Cur-
ran, 2004; Kaplan et al, 2004; Collins, 2004).
Broadly speaking, these approaches fall into two
categories, reranking and dynamic programming
approaches. In reranking methods (Johnson
et al, 1999; Collins, 2000; Shen et al, 2003),
an initial parser is used to generate a number
of candidate parses. A discriminative model
is then used to choose between these candi-
dates. In dynamic programming methods, a
large number of candidate parse trees are repre-
sented compactly in a parse tree forest or chart.
Given sufficiently ?local? features, the decod-
ing and parameter estimation problems can be
solved using dynamic programming algorithms.
For example, (Johnson, 2001; Geman and John-
son, 2002; Miyao and Tsujii, 2002; Clark and
Curran, 2004; Kaplan et al, 2004) describe ap-
proaches based on conditional log-linear (max-
imum entropy) models, where variants of the
inside-outside algorithm can be used to effi-
ciently calculate gradients of the log-likelihood
function, despite the exponential number of
trees represented by the parse forest.
In this paper, we describe a dynamic pro-
gramming approach to discriminative parsing
that is an alternative to maximum entropy
estimation. Our method extends the max-
margin approach of Taskar et al (2003) to
the case of context-free grammars. The present
method has several compelling advantages. Un-
like reranking methods, which consider only
a pre-pruned selection of ?good? parses, our
method is an end-to-end discriminative model
over the full space of parses. This distinction
can be very significant, as the set of n-best
parses often does not contain the true parse. For
example, in the work of Collins (2000), 41% of
the correct parses were not in the candidate pool
of ?30-best parses. Unlike previous dynamic
programming approaches, which were based on
maximum entropy estimation, our method in-
corporates an articulated loss function which
penalizes larger tree discrepancies more severely
than smaller ones.1
Moreover, like perceptron-based learning, it
requires only the calculation of Viterbi trees,
rather than expectations over all trees (for ex-
ample using the inside-outside algorithm). In
practice, it converges in many fewer iterations
than CRF-like approaches. For example, while
our approach generally converged in 20-30 iter-
ations, Clark and Curran (2004) report exper-
iments involving 479 iterations of training for
one model, and 1550 iterations for another.
The primary contribution of this paper is the
extension of the max-margin approach of Taskar
et al (2003) to context free grammars. We
show that this framework allows high-accuracy
parsing in cubic time by exploiting novel kinds
of lexical information.
2 Discriminative Parsing
In the discriminative parsing task, we want to
learn a function f : X ? Y, where X is a set
of sentences, and Y is a set of valid parse trees
according to a fixed grammar G. G maps an
input x ? X to a set of candidate parses G(x) ?
Y.2
We assume a loss function L : X ? Y ?
Y ? R+. The function L(x, y, y?) measures the
penalty for proposing the parse y? for x when y
is the true parse. This penalty may be defined,
for example, as the number of labeled spans on
which the two trees do not agree. In general we
assume that L(x, y, y?) = 0 for y = y?. Given
labeled training examples (xi, yi) for i = 1 . . . n,
we seek a function f with small expected loss
on unseen sentences.
The functions we consider take the following
linear discriminant form:
fw(x) = arg max
y?G(x)
?w,?(x, y)?,
1This articulated loss is supported by empirical suc-
cess and theoretical generalization bound in Taskar et al
(2003).
2For all x, we assume here that G(x) is finite. The
space of parse trees over many grammars is naturally in-
finite, but can be made finite if we disallow unary chains
and empty productions.
where ??, ?? denotes the vector inner product,
w ? Rd and ? is a feature-vector representation
of a parse tree ? : X ? Y ? Rd (see examples
below).3
Note that this class of functions includes
Viterbi PCFG parsers, where the feature-vector
consists of the counts of the productions used
in the parse, and the parameters w are the log-
probabilities of those productions.
2.1 Probabilistic Estimation
The traditional method of estimating the pa-
rameters of PCFGs assumes a generative gram-
mar that defines P (x, y) and maximizes the
joint log-likelihood ?i log P (xi, yi) (with some
regularization). A alternative probabilistic
approach is to estimate the parameters dis-
criminatively by maximizing conditional log-
likelihood. For example, the maximum entropy
approach (Johnson, 2001) defines a conditional
log-linear model:
Pw(y | x) =
1
Zw(x)
exp{?w,?(x, y)?},
where Zw(x) =
?
y?G(x) exp{?w,?(x, y)?}, and
maximizes the conditional log-likelihood of the
sample, ?i log P (yi | xi), (with some regular-
ization).
2.2 Max-Margin Estimation
In this paper, we advocate a different estima-
tion criterion, inspired by the max-margin prin-
ciple of SVMs. Max-margin estimation has been
used for parse reranking (Collins, 2000). Re-
cently, it has also been extended to graphical
models (Taskar et al, 2003; Altun et al, 2003)
and shown to outperform the standard max-
likelihood methods. The main idea is to forego
the probabilistic interpretation, and directly en-
sure that
yi = arg max
y?G(xi)
?w,?(xi, y)?,
for all i in the training data. We define the
margin of the parameters w on the example i
and parse y as the difference in value between
the true parse yi and y:
?w,?(xi, yi)? ? ?w,?(xi, y)? = ?w,?i,yi ??i,y?,
3Note that in the case that two members y1 and y2
have the same tied value for ?w,?(x, y)?, we assume that
there is some fixed, deterministic way for breaking ties.
For example, one approach would be to assume some
default ordering on the members of Y.
where ?i,y = ?(xi, y), and ?i,yi = ?(xi, yi). In-
tuitively, the size of the margin quantifies the
confidence in rejecting the mistaken parse y us-
ing the function fw(x), modulo the scale of the
parameters ||w||. We would like this rejection
confidence to be larger when the mistake y is
more severe, i.e. L(xi, yi, y) is large. We can ex-
press this desideratum as an optimization prob-
lem:
max ? (1)
s.t. ?w,?i,yi ? ?i,y? ? ?Li,y ?y ? G(xi);
||w||2 ? 1,
where Li,y = L(xi, yi, y). This quadratic pro-
gram aims to separate each y ? G(xi) from
the target parse yi by a margin that is propor-
tional to the loss L(xi, yi, y). After a standard
transformation, in which maximizing the mar-
gin is reformulated as minimizing the scale of
the weights (for a fixed margin of 1), we get the
following program:
min 12?w?
2 + C
?
i
?i (2)
s.t. ?w,?i,yi ? ?i,y? ? Li,y ? ?i ?y ? G(xi).
The addition of non-negative slack variables ?i
allows one to increase the global margin by pay-
ing a local penalty on some outlying examples.
The constant C dictates the desired trade-off
between margin size and outliers. Note that this
formulation has an exponential number of con-
straints, one for each possible parse y for each
sentence i. We address this issue in section 4.
2.3 The Max-Margin Dual
In SVMs, the optimization problem is solved by
working with the dual of a quadratic program
analogous to Eq. 2. For our problem, just as for
SVMs, the dual has important computational
advantages, including the ?kernel trick,? which
allows the efficient use of high-dimensional fea-
tures spaces endowed with efficient dot products
(Cristianini and Shawe-Taylor, 2000). More-
over, the dual view plays a crucial role in cir-
cumventing the exponential size of the primal
problem.
In Eq. 2, there is a constraint for each mistake
y one might make on each example i, which rules
out that mistake. For each mistake-exclusion
constraint, the dual contains a variable ?i,y. In-
tuitively, the magnitude of ?i,y is proportional
to the attention we must pay to that mistake in
order not to make it.
The dual of Eq. 2 (after adding additional
variables ?i,yi and renormalizing by C) is given
by:
max C
?
i,y
?i,yLi,y ?
1
2
?
?
?
?
?
?
?
?
?
?
?
?
C
?
i,y
(Ii,y ? ?i,y)?i,y
?
?
?
?
?
?
?
?
?
?
?
?
2
s.t.
?
y
?i,y = 1, ?i; ?i,y ? 0, ?i, y, (3)
where Ii,y = I(xi, yi, y) indicates whether y is
the true parse yi. Given the dual solution ??,
the solution to the primal problem w? is sim-
ply a weighted linear combination of the feature
vectors of the correct parse and mistaken parses:
w? = C
?
i,y
(Ii,y ? ??i,y)?i,y.
This is the precise sense in which mistakes with
large ? contribute more strongly to the model.
3 Factored Models
There is a major problem with both the pri-
mal and the dual formulations above: since each
potential mistake must be ruled out, the num-
ber of variables or constraints is proportional to
|G(x)|, the number of possible parse trees. Even
in grammars without unary chains or empty el-
ements, the number of parses is generally ex-
ponential in the length of the sentence, so we
cannot expect to solve the above problem with-
out any assumptions about the feature-vector
representation ? and loss function L.
For that matter, for arbitrary representa-
tions, to find the best parse given a weight vec-
tor, we would have no choice but to enumerate
all trees and score them. However, our gram-
mars and representations are generally struc-
tured to enable efficient inference. For exam-
ple, we usually assign scores to local parts of
the parse such as PCFG productions. Such
factored models have shared substructure prop-
erties which permit dynamic programming de-
compositions. In this section, we describe how
this kind of decomposition can be done over the
dual ? distributions. The idea of this decom-
position has previously been used for sequences
and other Markov random fields in Taskar et
al. (2003), but the present extension to CFGs
is novel.
For clarity of presentation, we restrict the
grammar to be in Chomsky normal form (CNF),
where all rules in the grammar are of the form
?A ? B C? or ?A ? a?, where A,B and C are
SNP
DT
The
NN
screen
VP
VBD
was
NP
NP
DT
a
NN
sea
PP
IN
of
NP
NN
red
0
1
2
3
4
5
6
0 1 2 3 4 5 6 7
DT
NN
VBD
DT
NN
IN
NN
NP
NP
PP
VP
S
NP
r = ?NP, 3, 5?
q = ?S ? NP VP, 0, 2, 7?
(a) (b)
Figure 1: Two representations of a binary parse tree: (a) nested tree structure, and (b) grid of labeled spans.
non-terminal symbols, and a is some terminal
symbol. For example figure 1(a) shows a tree
in this form.
We will represent each parse as a set of two
types of parts. Parts of the first type are sin-
gle constituent tuples ?A, s, e, i?, consisting of
a non-terminal A, start-point s and end-point
e, and sentence i, such as r in figure 1(b). In
this representation, indices s and e refer to po-
sitions between words, rather than to words
themselves. These parts correspond to the tra-
ditional notion of an edge in a tabular parser.
Parts of the second type consist of CF-rule-
tuples ?A ? B C, s,m, e, i?. The tuple specifies
a particular rule A ? B C, and its position,
including split point m, within the sentence i,
such as q in figure 1(b), and corresponds to the
traditional notion of a traversal in a tabular
parser. Note that parts for a basic PCFG model
are not just rewrites (which can occur multiple
times), but rather anchored items.
Formally, we assume some countable set of
parts, R. We also assume a function R which
maps each object (x, y) ? X ? Y to a finite
subset of R. Thus R(x, y) is the set of parts be-
longing to a particular parse. Equivalently, the
function R(x, y) maps a derivation y to the set
of parts which it includes. Because all rules are
in binary-branching form, |R(x, y)| is constant
across different derivations y for the same input
sentence x. We assume that the feature vector
for a sentence and parse tree (x, y) decomposes
into a sum of the feature vectors for its parts:
?(x, y) =
?
r?R(x,y)
?(x, r).
In CFGs, the function ?(x, r) can be any func-
tion mapping a rule production and its posi-
tion in the sentence x, to some feature vector
representation. For example, ? could include
features which identify the rule used in the pro-
duction, or features which track the rule iden-
tity together with features of the words at po-
sitions s,m, e, and neighboring positions in the
sentence x.
In addition, we assume that the loss function
L(x, y, y?) also decomposes into a sum of local
loss functions l(x, y, r) over parts, as follows:
L(x, y, y?) =
?
r?R(x,y?)
l(x, y, r).
One approach would be to define l(x, y, r) to
be 0 only if the non-terminal A spans words
s . . . e in the derivation y and 1 otherwise. This
would lead to L(x, y, y?) tracking the number of
?constituent errors? in y?, where a constituent is
a tuple such as ?A, s, e, i?. Another, more strict
definition would be to define l(x, y, r) to be 0
if r of the type ?A ? B C, s,m, e, i? is in the
derivation y and 1 otherwise. This definition
would lead to L(x, y, y?) being the number of CF-
rule-tuples in y? which are not seen in y.4
Finally, we define indicator variables I(x, y, r)
which are 1 if r ? R(x, y), 0 otherwise. We
also define sets R(xi) = ?y?G(xi)R(xi, y) for the
training examples i = 1 . . . n. Thus, R(xi) is
the set of parts that is seen in at least one of
the objects {(xi, y) : y ? G(xi)}.
4 Factored Dual
The dual in Eq. 3 involves variables ?i,y for
all i = 1 . . . n, y ? G(xi), and the objec-
tive is quadratic in these ? variables. In addi-
tion, it turns out that the set of dual variables
?i = {?i,y : y ? G(xi)} for each example i is
constrained to be non-negative and sum to 1.
It is interesting that, while the parameters w
lose their probabilistic interpretation, the dual
variables ?i for each sentence actually form a
kind of probability distribution. Furthermore,
the objective can be expressed in terms of ex-
pectations with respect to these distributions:
C
?
i
E?i [Li,y]?
1
2
?
?
?
?
?
?
?
?
?
?
C
?
i
?i,yi ?E?i [?i,y]
?
?
?
?
?
?
?
?
?
?
2
.
We now consider how to efficiently solve
the max-margin optimization problem for a
factored model. As shown in Taskar et al
(2003), the dual in Eq. 3 can be reframed using
?marginal? terms. We will also find it useful to
consider this alternative formulation of the dual.
Given dual variables ?, we define the marginals
?i,r(?) for all i, r, as follows:
?i,r(?i) =
?
y
?i,yI(xi, y, r) = E?i [I(xi, y, r)] .
Since the dual variables ?i form probability dis-
tributions over parse trees for each sentence i,
the marginals ?i,r(?i) represent the proportion
of parses that would contain part r if they were
drawn from a distribution ?i. Note that the
number of such marginal terms is the number
of parts, which is polynomial in the length of
the sentence.
Now consider the dual objective Q(?) in
Eq. 3. It can be shown that the original ob-
jective Q(?) can be expressed in terms of these
4The constituent loss function does not exactly cor-
respond to the standard scoring metrics, such as F1 or
crossing brackets, but shares the sensitivity to the num-
ber of differences between trees. We have not thoroughly
investigated the exact interplay between the various loss
choices and the various parsing metrics. We used the
constituent loss in our experiments.
marginals as Qm(?(?)), where ?(?) is the vector
with components ?i,r(?i), and Qm(?) is defined
as:
C
?
i,r?R(xi)
?i,rli,r ?
1
2
?
?
?
?
?
?
?
?
?
?
?
?
C
?
i,r?R(xi)
(Ii,r ? ?i,r)?i,r
?
?
?
?
?
?
?
?
?
?
?
?
2
where li,r = l(xi, yi, r), ?i,r = ?(xi, r) and Ii,r =
I(xi, yi, r).
This follows from substituting the factored
definitions of the feature representation ? and
loss function L together with definition of
marginals.
Having expressed the objective in terms of a
polynomial number of variables, we now turn to
the constraints on these variables. The feasible
set for ? is
? = {? : ?i,y ? 0, ?i, y
?
y
?i,y = 1, ?i}.
Now let ?m be the space of marginal vectors
which are feasible:
?m = {? : ?? ? ? s.t. ? = ?(?)}.
Then our original optimization problem can be
reframed as max???m Qm(?).
Fortunately, in case of PCFGs, the domain
?m can be described compactly with a polyno-
mial number of linear constraints. Essentially,
we need to enforce the condition that the ex-
pected proportions of parses having particular
parts should be consistent with each other. Our
marginals track constituent parts ?A, s, e, i? and
CF-rule-tuple parts ?A ? B C, s,m, e, i? The
consistency constraints are precisely the inside-
outside probability relations:
?i,A,s,e =
?
B,C
s<m<e
?i,A?B C,s,m,e
and
?i,A,s,e =
?
B,C
e<m?ni
?i,B?AC +
?
B,C
0?m<s
?i,B?CA
where ni is the length of the sentence. In ad-
dition, we must ensure non-negativity and nor-
malization to 1:
?i,r ? 0;
?
A
?i,A,0,ni = 1.
The number of variables in our factored dual
for CFGs is cubic in the length of the sentence,
Model P R F1
GENERATIVE 87.70 88.06 87.88
BASIC 87.51 88.44 87.98
LEXICAL 88.15 88.62 88.39
LEXICAL+AUX 89.74 90.22 89.98
Figure 2: Development set results of the various
models when trained and tested on Penn treebank
sentences of length ? 15.
Model P R F1
GENERATIVE 88.25 87.73 87.99
BASIC 88.08 88.31 88.20
LEXICAL 88.55 88.34 88.44
LEXICAL+AUX 89.14 89.10 89.12
COLLINS 99 89.18 88.20 88.69
Figure 3: Test set results of the various models when
trained and tested on Penn treebank sentences of
length ? 15.
while the number of constraints is quadratic.
This polynomial size formulation should be con-
trasted with the earlier formulation in Collins
(2004), which has an exponential number of
constraints.
5 Factored SMO
We have reduced the problem to a polynomial
size QP, which, in principle, can be solved us-
ing standard QP toolkits. However, although
the number of variables and constraints in the
factored dual is polynomial in the size of the
data, the number of coefficients in the quadratic
term in the objective is very large: quadratic in
the number of sentences and dependent on the
sixth power of sentence length. Hence, in our
experiments we use an online coordinate descent
method analogous to the sequential minimal op-
timization (SMO) used for SVMs (Platt, 1999)
and adapted to structured max-margin estima-
tion in Taskar et al (2003).
We omit the details of the structured SMO
procedure, but the important fact about this
kind of training is that, similar to the basic per-
ceptron approach, it only requires picking up
sentences one at a time, checking what the best
parse is according to the current primal and
dual weights, and adjusting the weights.
6 Results
We used the Penn English Treebank for all of
our experiments. We report results here for
each model and setting trained and tested on
only the sentences of length ? 15 words. Aside
from the length restriction, we used the stan-
dard splits: sections 2-21 for training (9753 sen-
tences), 22 for development (603 sentences), and
23 for final testing (421 sentences).
As a baseline, we trained a CNF transforma-
tion of the unlexicalized model of Klein and
Manning (2003) on this data. The resulting
grammar had 3975 non-terminal symbols and
contained two kinds of productions: binary non-
terminal rewrites and tag-word rewrites.5 The
scores for the binary rewrites were estimated us-
ing unsmoothed relative frequency estimators.
The tagging rewrites were estimated with a
smoothed model of P (w|t), also using the model
from Klein and Manning (2003). Figure 3 shows
the performance of this model (generative):
87.99 F1 on the test set.
For the basic max-margin model, we used
exactly the same set of allowed rewrites (and
therefore the same set of candidate parses) as in
the generative case, but estimated their weights
according to the discriminative method of sec-
tion 4. Tag-word production weights were fixed
to be the log of the generative P (w|t) model.
That is, the only change between genera-
tive and basic is the use of the discriminative
maximum-margin criterion in place of the gen-
erative maximum likelihood one. This change
alone results in a small improvement (88.20 vs.
87.99 F1).
On top of the basic model, we first added lex-
ical features of each span; this gave a lexical
model. For a span ?s, e? of a sentence x, the
base lexical features were:
? xs, the first word in the span
? xs?1, the preceding adjacent word
? xe?1, the last word in the span
? xe, the following adjacent word
? ?xs?1, xs?
? ?xe?1, xe?
? xs+1 for spans of length 3
These base features were conjoined with the
span length for spans of length 3 and below,
since short spans have highly distinct behaviors
(see the examples below). The features are lex-
ical in the sense than they allow specific words
5Unary rewrites were compiled into a single com-
pound symbol, so for example a subject-gapped sentence
would have label like s+vp. These symbols were ex-
panded back into their source unary chain before parses
were evaluated.
and word pairs to influence the parse scores, but
are distinct from traditional lexical features in
several ways. First, there is no notion of head-
word here, nor is there any modeling of word-to-
word attachment. Rather, these features pick
up on lexical trends in constituent boundaries,
for example the trend that in the sentence The
screen was a sea of red., the (length 2) span
between the word was and the word of is un-
likely to be a constituent. These non-head lex-
ical features capture a potentially very differ-
ent source of constraint on tree structures than
head-argument pairs, one having to do more
with linear syntactic preferences than lexical
selection. Regardless of the relative merit of
the two kinds of information, one clear advan-
tage of the present approach is that inference in
the resulting model remains cubic, since the dy-
namic program need not track items with distin-
guished headwords. With the addition of these
features, the accuracy jumped past the genera-
tive baseline, to 88.44.
As a concrete (and particularly clean) exam-
ple of how these features can sway a decision,
consider the sentence The Egyptian president
said he would visit Libya today to resume the
talks. The generative model incorrectly consid-
ers Libya today to be a base np. However, this
analysis is counter to the trend of today being a
one-word constituent. Two features relevant to
this trend are: (constituent ? first-word =
today ? length = 1) and (constituent ? last-
word = today ? length = 1). These features rep-
resent the preference of the word today for being
the first and and last word in constituent spans
of length 1.6 In the lexical model, however,
these features have quite large positive weights:
0.62 each. As a result, this model makes this
parse decision correctly.
Another kind of feature that can usefully be
incorporated into the classification process is
the output of other, auxiliary classifiers. For
this kind of feature, one must take care that its
reliability on the training not be vastly greater
than its reliability on the test set. Otherwise,
its weight will be artificially (and detrimentally)
high. To ensure that such features are as noisy
on the training data as the test data, we split
the training into two folds. We then trained the
auxiliary classifiers in jacknife fashion on each
6In this length 1 case, these are the same feature.
Note also that the features are conjoined with only one
generic label class ?constituent? rather than specific con-
stituent types.
fold, and using their predictions as features on
the other fold. The auxiliary classifiers were
then retrained on the entire training set, and
their predictions used as features on the devel-
opment and test sets.
We used two such auxiliary classifiers, giving
a prediction feature for each span (these classi-
fiers predicted only the presence or absence of a
bracket over that span, not bracket labels). The
first feature was the prediction of the genera-
tive baseline; this feature added little informa-
tion, but made the learning phase faster. The
second feature was the output of a flat classi-
fier which was trained to predict whether sin-
gle spans, in isolation, were constituents or not,
based on a bundle of features including the list
above, but also the following: the preceding,
first, last, and following tag in the span, pairs
of tags such as preceding-first, last-following,
preceding-following, first-last, and the entire tag
sequence.
Tag features on the test sets were taken from
a pretagging of the sentence by the tagger de-
scribed in Toutanova et al (2003). While the
flat classifier alone was quite poor (P 78.77 /
R 63.94 / F1 70.58), the resulting max-margin
model (lexical+aux) scored 89.12 F1. To sit-
uate these numbers with respect to other mod-
els, the parser in Collins (1999), which is genera-
tive, lexicalized, and intricately smoothed scores
88.69 over the same train/test configuration.
It is worth considering the cost of this kind of
method. At training time, discriminative meth-
ods are inherently expensive, since they all in-
volve iteratively checking current model perfor-
mance on the training set, which means parsing
the training set (usually many times). In our
experiments, 10-20 iterations were generally re-
quired for convergence (except the basic model,
which took about 100 iterations.) There are
several nice aspects of the approach described
here. First, it is driven by the repeated extrac-
tion, over the training examples, of incorrect
parses which the model currently prefers over
the true parses. The procedure that provides
these parses need not sum over all parses, nor
even necessarily find the Viterbi parses, to func-
tion. This allows a range of optimizations not
possible for CRF-like approaches which must
extract feature expectations from the entire set
of parses.7 Nonetheless, generative approaches
7One tradeoff is that this approach is more inherently
sequential and harder to parallelize.
are vastly cheaper to train, since they must only
collect counts from the training set.
On the other hand, the max-margin approach
does have the potential to incorporate many
new kinds of features over the input, and the
current feature set alows limited lexicalization
in cubic time, unlike other lexicalized models
(including the Collins model which it outper-
forms in the present limited experiments).
7 Conclusion
We have presented a maximum-margin ap-
proach to parsing, which allows a discriminative
SVM-like objective to be applied to the parsing
problem. Our framework permits the use of a
rich variety of input features, while still decom-
posing in a way that exploits the shared sub-
structure of parse trees in the standard way. On
a test set of ? 15 word sentences, the feature-
rich model outperforms both its own natural
generative baseline and the Collins parser on
F1. While like most discriminative models it is
compute-intensive to train, it allows fast pars-
ing, remaining cubic despite the incorporation
of lexical features. This trade-off between the
complexity, accuracy and efficiency of a parsing
model is an important area of future research.
Acknowledgements
This work was supported in part by the Depart-
ment of the Interior/DARPA under contract
number NBCHD030010, a Microsoft Graduate
Fellowship to the second author, and National
Science Foundation grant 0347631 to the third
author.
References
Y. Altun, I. Tsochantaridis, and T. Hofmann.
2003. Hidden markov support vector ma-
chines. In Proc. ICML.
S. Clark and J. R. Curran. 2004. Parsing
the wsj using ccg and log-linear models. In
Proceedings of the 42nd Annual Meeting of
the Association for Computational Linguis-
tics (ACL ?04).
M. Collins. 1999. Head-Driven Statistical Mod-
els for Natural Language Parsing. Ph.D. the-
sis, University of Pennsylvania.
M. Collins. 2000. Discriminative reranking for
natural language parsing. In ICML 17, pages
175?182.
M. Collins. 2004. Parameter estimation for sta-
tistical parsing models: Theory and practice
of distribution-free methods. In Harry Bunt,
John Carroll, and Giorgio Satta, editors, New
Developments in Parsing Technology. Kluwer.
N. Cristianini and J. Shawe-Taylor. 2000. An
Introduction to Support Vector Machines and
Other Kernel-Based Learning Methods. Cam-
bridge University Press.
S. Geman and M. Johnson. 2002. Dynamic
programming for parsing and estimation of
stochastic unification-based grammars. In
Proceedings of the 40th Annual Meeting of the
Association for Computational Linguistics.
M. Johnson, S. Geman, S. Canon, Z. Chi, and
S. Riezler. 1999. Estimators for stochastic
?unification-based? grammars. In Proceed-
ings of ACL 1999.
M. Johnson. 2001. Joint and conditional es-
timation of tagging and parsing models. In
ACL 39.
R. Kaplan, S. Riezler, T. King, J. Maxwell,
A. Vasserman, and R. Crouch. 2004. Speed
and accuracy in shallow and deep stochastic
parsing. In Proceedings of HLT-NAACL?04).
D. Klein and C. D. Manning. 2003. Accurate
unlexicalized parsing. In ACL 41, pages 423?
430.
J. Lafferty, A. McCallum, and F. Pereira.
2001. Conditional random fields: Probabi-
listic models for segmenting and labeling se-
quence data. In ICML.
Y. Miyao and J. Tsujii. 2002. Maximum
entropy estimation for feature forests. In
Proceedings of Human Language Technology
Conference (HLT 2002).
J. Platt. 1999. Using sparseness and analytic
QP to speed training of support vector ma-
chines. In NIPS.
L. Shen, A. Sarkar, and A. K. Joshi. 2003. Us-
ing ltag based features in parse reranking. In
Proc. EMNLP.
B. Taskar, C. Guestrin, and D. Koller. 2003.
Max margin Markov networks. In NIPS.
K. Toutanova, D. Klein, C. D. Manning, and
Y. Singer. 2003. Feature-rich part-of-speech
tagging with a cyclic dependency network. In
NAACL 3, pages 252?259.
Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural
Language Learning, pages 710?720, Jeju Island, Korea, 12?14 July 2012. c?2012 Association for Computational Linguistics
Discovering Diverse and Salient Threads in Document Collections
Jennifer Gillenwater Alex Kulesza
Department of Computer and Information Science
University of Pennsylvania
Philadelphia, PA 19104
{jengi,kulesza,taskar}@cis.upenn.edu
Ben Taskar
Abstract
We propose a novel probabilistic technique
for modeling and extracting salient struc-
ture from large document collections. As
in clustering and topic modeling, our goal
is to provide an organizing perspective into
otherwise overwhelming amounts of infor-
mation. We are particularly interested in
revealing and exploiting relationships be-
tween documents. To this end, we focus on
extracting diverse sets of threads?singly-
linked, coherent chains of important doc-
uments. To illustrate, we extract research
threads from citation graphs and construct
timelines from news articles. Our method
is highly scalable, running on a corpus of
over 30 million words in about four minutes,
more than 75 times faster than a dynamic
topic model. Finally, the results from our
model more closely resemble human news
summaries according to several metrics and
are also preferred by human judges.
1 Introduction
The increasing availability of large document
collections has the potential to revolutionize
our ability to understand the world. However,
the scale and complexity of such collections fre-
quently make it difficult to quickly grasp the
important details and the relationships between
them. As a result, automatic interfaces for data
navigation, exploration, aggregation, and analy-
sis are becoming increasingly valuable.
In this work we propose a novel approach:
threading structured document collections. Con-
sider a large graph, with documents as nodes
and edges indicating relationships, as in Figure 1.
Our goal is to find a diverse set of paths (or
threads) through the collection that are indi-
vidually coherent and together cover the most
salient parts. For example, given a collection
of academic papers, we might want to identify
the most significant lines of research, threading
the citation graph to produce chains of impor-
tant papers. Or, given news articles connected
chronologically, we might want to extract threads
of articles to form timelines describing the ma-
jor events from the most significant news stories.
Top-tier news organizations like The New York
Times and The Guardian regularly publish such
timelines, but have so far been limited to creat-
ing them by hand. Other possibile applications
might include discovering trends on social media
sites, or perhaps mining blog entries for impor-
tant conversations through trackback links. We
show how these kinds of threading tasks can be
done efficiently, providing a simple, practical tool
for representing graph-based data that offers new
possibilities compared with existing models.
The Topic Detection and Tracking (TDT) pro-
gram (Wayne, 2000) has recently led to some
research in this direction. Several of TDT?s core
tasks, like link detection, topic detection, and
topic tracking, can be seen as subroutines for
the threading problem. Our work, however, ad-
dresses these tasks jointly, using a global prob-
abilistic model with a tractable inference algo-
rithm. To achieve this, we employ structured
determinantal point processes (SDPPs) (Kulesza
710
Figure 1: An illustration of document collection threading. We first build a graph from the collection, using
measures of importance and relatedness to weight nodes (documents) and build edges (relationships). Then,
from this graph, we extract a diverse, salient set of threads to represent the collection. The supplement
contains a version of this figure for our real-world news dataset.
and Taskar, 2010), which offer a natural prob-
abilistic model over sets of structures (such as
threads) where diversity is desired, and we incor-
porate k-DPP extensions to control the number
of threads (Kulesza and Taskar, 2011).
We apply our model to two real-world datasets,
extracting threads of research papers and time-
lines of news articles. An example of news
threads extracted using our model is shown in
Figure 2. Quantitative evaluation shows that our
model significantly outperforms multiple base-
lines, including dynamic topic models, in com-
parisons with human-produced news summaries.
It also outperforms baseline methods in a user
evaluation of thread coherence, and runs 75 times
faster than a dynamic topic model.
The primary contributions of this paper
are: (1) proposing a novel framework for finding
diverse and salient sets of document threads; (2)
combining SDPPs and k-DPPs to implement the
proposed model; (3) introducing random projec-
tions to improve efficiency with only bounded
deviation; and (4) demonstrating the model on
large-scale, real-world datasets.
2 Related Work
A variety of papers from the topic tracking liter-
ature are broadly related to our work (Mei and
Zhai, 2005; Blei and Lafferty, 2006; Leskovec et
al., 2009; Ahmed and Xing, 2010). Blei and Laf-
ferty (2006) recently introduced dynamic topic
models (DTMs). Assuming a division of doc-
uments into time slices, a DTM draws in each
slice a set of topics from a Gaussian distribution
whose mean is determined by the topics from
the previous slice. In this way, a DTM generates
topic threads. In this work we are interested in
the related but not identical task of generating
document threads. We engineer a baseline for
constructing document threads from DTM topic
threads (see Section 6.2.2), but the topic-centric
nature of DTMs means they are not ideal for
this task. Figure 2 illustrates some of the issues.
The work of Ahmed and Xing (2010) general-
izes DTMs to iDTMs (infinite DTMs) by allowing
topics to span only a subset of time slices, and
allowing an arbitrary number of topics. However,
iDTMs still require placing documents into dis-
crete epochs, and the issue of generating topic
rather than document threads remains. In Sec-
tion 6 we compare to DTMs but not iDTMs
because an implementation of iDTMs was not
readily available.
In the information retrieval community there
has also been work on extracting temporal in-
formation from document collections. Swan and
Jensen (2000) proposed a system for finding tem-
porally clustered named entities in news text and
presenting them on a timeline. Allan, Gupta,
and Khandelwal (2001) introduced the task of
temporal summarization, which takes a stream
of news articles on a particular topic and tries to
extract sentences describing important events as
they occur. Yan et al011) evaluated methods
for choosing sentences from temporally clustered
documents that are relevant to a query. Here, we
are interested not in extracting topically grouped
entities or sentences, but instead in organizing a
subset of the articles themselves into timelines,
with topic identification as a side effect.
There has also been some prior work focus-
ing more directly on threading. Shahaf and
711
Jan 08 Jan 28 Feb 17 Mar 09 Mar 29 Apr 18 May 08 May 28 Jun 17
pope vatican church parkinson 
israel palestinian iraqi israeli gaza abbas baghdad 
owen nominees senate democrats judicial filibusters 
social tax security democrats rove accounts 
iraq iraqi killed baghdad arab marines deaths forces 
Jan 08 Jan 28 Feb 17 Mar 09 Mar 29 Apr 18 May 08 May 28 Jun 17
cancer heart breast women disease aspirin risk study 
palestinian israel baghdad palestinians sunni korea gaza israeli 
social security accounts retirement benefits tax workers 401 payroll 
mets rangers dodgers delgado martinez astacio angels mientkiewicz 
hotel kitchen casa inches post shade monica closet 
Feb 24: Parkinson?s Disease Increases Risks to Pope
Feb 26: Pope?s Health Raises Questions About His Ability to Lead
Mar 13: Pope Returns Home After 18 Days at Hospital
Apr 01: Pope?s Condition Worsens as World Prepares for End of Pa-
pacy
Apr 02: Pope, Though Gravely Ill, Utters Thanks for Prayers
Apr 18: Europeans Fast Falling Away from Church
Apr 20: In Developing World, Choice [of Pope] Met with Skepticism
May 18: Pope Sends Message with Choice of Name
Jan 11: Study Backs Meat, Colon Tumor Link
Feb 07: Patients?and Many Doctors?Still Don?t Know How Often
Women Get Heart Disease
Mar 07: Aspirin Therapy Benefits Women, but Not in the Way It
Aids Men
Mar 16: Study Shows Radiation Therapy Doesn?t Increase Heart Dis-
ease Risk for Breast Cancer Patients
Apr 11: Personal Health: Women Struggle for Parity of the Heart
May 16: Black Women More Likely to Die from Breast Cancer
May 24: Studies Bolster Diet, Exercise for Breast Cancer Patients
Jun 21: Another Reason Fish is Good for You
Figure 2: A set of five news threads generated by our method (left) and a dynamic topic model (right) for
the first half of 2005. Above, the threads are shown on a timeline with the most salient words superimposed;
below, the dates and headlines from the threads appearing at the bottom are listed. Topic models are not
designed for threading and often link together topically similar documents that do not constitute a coherent
news story, as on the right.
Guestrin (2010) and Chieu and Lee (2004) pro-
posed selecting a single thread, whereas we seek
a set of threads, which is a more general task.
Shahaf, Guestrin, and Horvitz (2012) recently
proposed metro maps as alternative structured
representations of related news stories. Metro
maps are effectively sets of non-chronological
threads that are encouraged to intersect and thus
create a ?map? of events and topics. However,
these approaches assume some prior knowledge
about content. Shahaf and Guestrin (2010), for
example, assume the thread endpoints are spec-
ified, and Chieu and Lee (2004) require a set
of query words. These inputs make it possible
to quickly pare down the document graph. In
constrast, we work with very large graphs and
consider all possible threads. Furthermore, while
some prior work has relied on heuristics and ap-
proximate optimization, we can efficiently sample
a joint probabilistic model with approximation
guarantees.
In previous work on SDPPs (structured DPPs),
which we use here to model threads, Kulesza and
Taskar (2010) derived exact polynomial-time al-
gorithms for sampling and other inference. How-
ever, their experiments involved feature vectors
of only 32 dimensions. For text, natural features
like word occurrences typically yield dimension-
ality in the tens of thousands, making SDPP
inference prohibitively expensive. We solve this
problem by reducing the feature space using ran-
dom projections (see Section 5). We prove that
even a logarithmic number of projections is suffi-
cient to yield a close approximation to the origi-
nal SDPP distribution.
3 Framework
Before presenting our probabilistic model, we
describe a natural framework for representing
document collections. We assume that the collec-
tion has been transformed into a directed graph
G = (V,E) on n vertices, where each node cor-
responds to a document and each edge repre-
sents a relationship between documents whose
semantics depend on the task. We also as-
sume the existence of a weight function w on
nodes and edges, which measures the impor-
tance or salience of documents and the relative
strength of the relationships between them. For-
mally, we define the weight of a path (or thread)
y = (y(1), y(2), . . . , y(T )), (y(t), y(t+1)) ? E by:
w(y) =
T?
t=1
w
(
y(t)
)
+
T?1?
t=1
w
(
y(t), y(t+1)
)
. (1)
712
Lastly, we also assume the existence of node
features. Specifically, let ? represent a feature
mapping from nodes to RD (for example, tf-idf
word vectors). The feature map on a thread is
then just a sum over the nodes in the thread:
?(y) =
T?
t=1
?
(
y(t)
)
. (2)
(If it is convenient to have features on edges as
well as on nodes, it is possible to accommodate
them without affecting asymptotic performance.)
Given this framework, our goal is to develop
a probabilistic model over sets of k threads of
length T , favoring sets whose threads have large
weight but are also distinct from one another with
respect to ?. In other words, a high-probability
set under the model should include threads that
are both salient and diverse.
This is a daunting problem, given that the
number of possible sets of threads is O(nkT ).
For the datasets we use later, the actual number
is around 21000. However, we will show how to
construct the desired model in a way that allows
efficient inference, even for large datasets, using
determinantal point processes (DPPs). We begin
with some background.
4 Determinantal point processes
A DPP is a type of distribution over subsets.
Formally, a DPP P on a set of items Y =
{y1, . . . , yN} is a probability measure on 2Y , the
set of all subsets of Y . (In our setting, Y will be
the set of all possible threads.) For every Y ? Y
we have:
P(Y ) =
det(LY )
?
Y?Y
det(LY )
=
det(LY )
det(L+ I)
, (3)
where L is a positive semidefinite matrix and I
is the N ?N identity matrix. LY ? [Lij ]yiyj?Y
denotes the restriction of L to the entries indexed
by elements of Y , and det(L?) = 1. We can
define the entries of L as follows:
Lij = q(yi)?(yi)
>?(yj)q(yj) , (4)
where we can think of q(yi) ? R+ as the ?qual-
ity? of an item yi, and ?(yi) ? RD, ??(yi)?2 = 1
Figure 3: (a) The DPP probability of a set Y depends
on the volume spanned by vectors q(yi)?(yi) for i ? Y .
(b) As quality (length) increases, so does volume. (c)
As similarity increases, volume decreases.
as a normalized D-dimensional feature vector
such that ?(yi)>?(yj) ? [?1, 1] is a measure of
similarity between items yi and yj . This simple
definition gives rise to a distribution that places
most of its weight on sets that are both high qual-
ity and diverse. To understand why this is the
case, note that determinants are closely related
to volumes; in particular, det(LY ) is proportional
to the volume spanned by the vectors q(yi)?(yi)
for yi ? Y . Thus, sets with high-quality, diverse
items have the highest probability; see Figure 3
for an illustration.
4.1 Structured DPPs
Kulesza and Taskar (2010) introduced structured
DPPs (SDPPs) to efficiently handle Y containing
exponentially many structures. In our setting, Y
contains all threads of length T , so each yi ? Y is
a sequence (y(1)i , . . . , y
(T )
i ), where y
(t)
i is the docu-
ment included in the thread at position t. When
G is a complete graph, there are nT possible
sequences, so |Y| = N = nT .
In order to allow for efficient normalization
and sampling, SDPPs assume a factorization
of the quality score q(yi) and similarity score
?(yi)>?(yj) into parts, decomposing quality mul-
tiplicatively and similarity additively:
q(yi) =
T?
t=1
q
(
y(t)i
)
?(yi) =
T?
t=1
?
(
y(t)i
)
(5)
For threading, the definition of ? is just as given
in Equation (2). However, in order to convert the
weight function defined in Equation (1) to the
713
appropriate multiplicative form, we use a sim-
ple log-linear model, setting q(yi) = exp(?w(yi)),
where ? is a hyperparameter that effectively gov-
erns the balance between quality and diversity
by adjusting the dynamic range of the quality
function.
An efficient algorithm for sampling structures
(in this case, sets of threads) from an SDPP is
derived in Kulesza and Taskar (2010). While
the details are beyond the scope of this paper,
we note that the sampling algorithm requires
O(Tn2D2) time. If the node degrees are bounded
by r then the time is reduced to O(TrnD2). This
is not quite efficient enough when the number
of features, D, is large, as it often is for textual
tasks, but we will show in Section 5 how to
overcome this last hurdle.
Note that, in our later experiments, we fix T
to moderate values (T = 5, 8) for ease of analysis
and display. However, it is possible (and effi-
cient, due to the linear scaling) to allow longer
threads, as well as threads of variable length.
The latter effect can be achieved by adding a sin-
gle ?dummy? node to the document graph, with
incoming edges from all other documents and a
single outgoing self-loop edge. Shorter threads
will simply transition to this dummy node when
they are complete.
4.2 k-DPPs
SDPPs allow us to efficiently model all sets of
threads; however, for practical reasons we would
prefer to focus only on sets of exactly k threads.
To do so we exploit recently developed methods
for working with DPPs of fixed size (Kulesza
and Taskar, 2011). A k-DPP Pk is a DPP con-
ditioned on the event that the subset Y ? Y has
cardinality k; formally, whenever |Y | = k:
Pk(Y ) =
det(LY )
?
|Y ?|=k det(LY ?)
. (6)
In this work we combine k-DPPs with SDPPs,
referring to the result as a k-SDPP. We note that
using k-SDPPs instead of SDPPs does not affect
efficiency of sampling; it merely affords a mecha-
nism for controlling the number of threads.
5 Random projections
As described above, the time complexity for sam-
pling sets from SDPPs is O(TrnD2). Although
this is polynomial, for practical problems nD2
is prohibitively large. While previous work has
dealt only with small datasets, in our experi-
ments we typically have n,D > 30,000; storing
a single message for the message-passing routine
involved in SDPP sampling would require over
200 terabytes of memory. To make the model
practical, therefore, we turn to techniques for
dimensionality reduction.
Standard PCA requires O(D3) time and would
be much too slow. But a classic result of John-
son and Lindenstrauss (1984) shows that high-
dimensional points can be randomly projected
onto a logarithmic number of dimensions while
approximately preserving the distances between
them. More recently, Magen and Zouzias (2008)
extended this idea to the preservation of volumes
spanned by sets of points. Here, we use a rela-
tionship between determinants and volumes to
adapt the latter result. We will prove the follow-
ing bound on the variational distance between
the original k-SDPP and a randomly projected
version.
Theorem 1. Fix , ? < 1/2, and set d =
max
{
2k

,
24
2
(
log(3/?)
logN
+ 1
)
log 2N + k ? 1
}
.
(7)
Let Pk be the k-SDPP distribution in Equa-
tion (6), let G be a d ? D random matrix
whose entries are independently sampled from
N (0, 1/d), and let P?k(Y ) be the k-SDPP distri-
bution after projecting ? by G?that is, replacing
? with G?. Then with probability at least 1? ?,
?Pk?P?k?1 =
?
|Y |=k
|Pk(Y )?P?k(Y )| ? e6k?1 .
(8)
Note that e6k ? 1 ? 6k when k is small, and
d = O(max{k/, (log(1/?) + T log n)/2}).
Practically, Theorem 1 says that if we project
? down to dimension d logarithmic in the number
of documents and linear in thread length, the L1
variational distance between the true model and
the projected model is bounded.
714
To prove Theorem 1, we will first state a vari-
ant of Magen and Zouzias? result, which bounds
the ratio of volumes before and after projection
from D down to d dimensions.
Lemma 1. Let X be a D?N matrix. Fix k < N
and , ? < 1/2, and set d and G as in Theorem 1.
Then with probability at least 1? ? we have, for
all D ? k matrices Y formed by a subset of k
columns from X:
(1? )k ?
Vol(GY )
Vol(Y )
? (1 + )k ,
where Vol(Y ) is the k-dimensional volume
spanned by the columns of Y and the origin.
We can make use of the following fact to con-
vert this bound on volumes to a bound on deter-
minants:
Vol(Y ) =
1
k!
?
det(Y >Y ) . (9)
In order to handle the k-SDPP normalization
constant
?
|Y |=k
?
?
?
yi?Y
q2(yi)
?
?det(?(Y )>?(Y )) , (10)
we also must adapt Lemma 1 to sums of deter-
minants. The following lemma gives the details.
Lemma 2. Under the same conditions as
Lemma 1, with probability at least 1? ?,
(1+2)?2k ?
?
|Y |=k det((GY )
>(GY ))
?
|Y |=k det(Y
>Y )
? (1+)2k .
Proof.
?
|Y |=k
det((GY )>(GY ))
=
?
|Y |=k
(k!Vol(GY ))2
?
?
|Y |=k
(
k!Vol(Y )(1? )k
)2
? (1 + 2)?2k
?
|Y |=k
det(Y >Y ) ,
0 50 100 1500
0.2
0.4
0.6
0.8
1
1.2
L1 
var
iati
ona
l di
sta
nce
Projection dimension
0
1
2
3
4x 10
8
Me
mo
ry u
se 
(byt
es)
Figure 4: The effect of random projections. In black,
on the left, we estimate the L1 variational distance
between the true and projected models. In blue, on
the right, we plot the memory required for sampling.
Running time is proportional to memory use.
where the first inequality holds with probability
at least 1?? by Lemma 1, and the second follows
from the fact that (1? )(1 + 2) ? 1 (since  <
1/2), thus (1? )2k ? (1 + 2)?2k. A symmetric
argument gives the upper bound.
Proof (of Theorem 1). Let B be the matrix
whose columns are given by Bi = q(yi)?(yi).
We have
?Pk ? P?k?1 =
?
|Y |=k
|Pk(Y )? P?k(Y )|
=
?
|Y |=k
Pk(Y )
?
?
?
?
?
1?
P?k(Y )
Pk(Y )
?
?
?
?
?
=
?
|Y |=k
Pk(Y )
?
?
?
?1?
det([GB>Y ][GBY ])
det(B>Y BY )
?
?
|Y ?|=k det(B
>
Y ?BY ?)
?
|Y ?|=k det([GB
>
Y ? ][GBY ? ])
?
?
?
?
?
?
?
?
?1? (1 + )2k(1 + 2)2k
?
?
?
?
|Y |=k
Pk(Y )
? e6k ? 1 ,
where the first inequality follows from Lemma 1
and Lemma 2, which hold simultaneously with
probability at least 1? ?, and the second follows
from (1 + a)b ? eab for a, b ? 0.
715
delay interconnectwiresizing elmore-basedrouting tree
mobile clients hoard server client database
policy decisionmarkov pomdpspartially uncertainty
learning lifelong training tasks invariances control learning lifelong training tasks invariances control
? Locally Weighted Learning for Control
? Discovering Structure in Multiple Learning Tasks: The TC Al-
gorithm
? Learning One More Thing
? Explanation Based Learning for Mobile Robot Perception
? Learning Analytically and Inductively
mobile clients hoard server client database
? A Database Architecture for Handling Mobile Clients
? An Architecture for Mobile Databases
? Database Server Organization for Handling Mobile Clients
? Mobile Wireless Computing: Solutions and Challenges in Data
Management
? Energy Efficient Query Optimization
Figure 5: Example threads sampled from a 4-SDPP with thread length T = 5 on the Cora dataset. We
project from word-space to two dimensions by running PCA on the centroids of the threads. The nodes not
on the thread paths form a representative subset of the other documents from Cora. Displayed beside each
thread are a few of its maximum-tfidf words. Paper titles from two of the threads are shown to the right.
6 Experiments
We begin by showing the performance of random
projections on a small, synthetic threading task
where the exact model is tractable, with n = 600
and D = 150. Figure 4 shows the L1 variational
distance (estimated by sampling) as well as the
actual memory required for a variety of projec-
tion dimensions d. Note that, as predicted by
Theorem 1, fidelity to the true model increases
rapidly with d.
6.1 Cora citation graph
To qualitatively illustrate our model, we apply
it to Cora (McCallum et al2000). Cora is a
large collection of academic papers on computer
science topics, plus citations between them. We
construct a directed graph with papers as nodes
and citations as edges; after removing papers
with missing metadata or zero outgoing citations,
our graph contains n = 28,155 papers.
To obtain useful threads, we set edge weights
to reflect the degree of textual similarity between
the citing and cited papers, and set node weights
to reflect paper ?importance?. Edge weights
are given by normalized cosine similarity (NCS),
which for two documents i and j is the dot prod-
uct of their normalized tfidf vectors:
?
w?W tfidfi(w)tfidfj(w)??
w?W tfidfi(w)
2
??
w?W tfidfj(w)
2
,
where W is a subset of the words found in the
documents. We select W by filtering according
to document frequency; that is, we remove words
that are too common or too rare. After filtering,
there are 50,912 unique words. The node weights
are given by LexRank scores (Erkan and Radev,
2004), which are similar to node degrees.
Finally, we build a similarity feature map ? to
encourage diversity. We represent each document
by the 1000 documents to which it is most similar
according to NCS; this results in binary ? of
dimension m = n with exactly 1000 non-zeros.
The dot product between the similarity features
of two documents is thus proportional to the
fraction of top-1000 similar documents they have
in common. As described in Section 5, we then
randomly project this large feature set from D ?
28,000 to d = 50 dimensions.
We illustrate the behavior of the resulting
model in Figure 5. The discovered threads oc-
cupy distinct regions of word-space, standing
apart visually, and contain diverse salient terms.
6.2 News articles
For quantitative evaluation, we use newswire
data. Our dataset comprises over 200,000 arti-
cles from the New York Times, collected from
2005-2007 as part of the English Gigaword cor-
pus (Graff and Cieri, 2009). We split the articles
into six-month time periods, with an average of
716
n = 34,504 articles per period. After filtering,
there are a total of 36,356 unique words.
For each time period, we generate a graph with
articles as nodes. We use NCS for edge weights,
and throw away edges with weight < 0.1. We
also require that edges go forward in time; this
enforces the chronological ordering of our threads.
The supplement contains illustrations of one of
the resulting graphs. We use LexRank for node
weights and the top-1000 similar documents as
similarity features ?, projecting to d = 50, as
before (Section 6.1). We also add a constant fea-
ture ? to ?, which controls the overall degree of
repulsion; large values of ? make all documents
more similar. This makes the k-SDPP distri-
bution more peaked around diverse sets. For
all of the following results, we use T = 8 and
k = 10 so that the resulting timelines are of a
manageable size for analysis. However, we tried
several values of k and T in our experiments, and
did not see significant differences in relative per-
formance. We report all metrics averaged over
100 random samples from the model for each
six-month period.
6.2.1 Graph visualizations
The (very large) news graph for the first
half of 2005 can be viewed interactively at
http://zoom.it/jOKV. In this graph each node
(dark circle) represents a news article, and is an-
notated with its headline. Node size corresponds
to weight (LexRank score). Nodes are laid out
chronologically, left-to-right, from January to
June of 2005. The five colored paths indicate a
set of threads sampled from the k-SDPP. Head-
lines of the articles in each thread are colored
to match the thread. Edges are included as de-
scribed in the paper, but due to the scale of this
dataset, only 1% of the edges are shown. Edge
thickness corresponds to weight (NCS).
We provide a view of a small subgraph for
illustration purposes in Figure 6, which shows
the incoming and outgoing edges for a single
node. A zoomable version of this subgraph is
available at http://zoom.it/GUCR.
STUDY ANALYZES DATA ON ILLEGAL IMMIGRANTS
WELCOME TO 'TEH-JAS,' LAND OF THE ALAMO AND COWBOYFOR IMMIGRANTS, SUCCESS OFTEN STARTS WITH BIZARRE STREET SIGNSDOMINICANS TAKE THEIR PLACE AS AN AMERICAN SUCCESS STORY
MEXICAN MANUAL ON ILLEGAL IMMIGRATION DRAWS FIRECALIFORNIA REPUBLICAN COUNTERS BUSH IMMIGRATION PLANHOUSE BILL TARGETS FAKE SOCIAL SECURITY CARDS; PROPOSAL CALLS FOR DIGITAL PHOTO, ELECTRONIC STRIPGARCIA SEEKS MAJOR FOR HIS RESUMEVIDEOCONFERENCES LINK IMMIGRANTS AND LOVED ONESBORDER CROSSING: A GUIDE FOR THE ILLEGAL MIGRANTPOLITICIANS' DREAM WOULD BECOME A NIGHTMARE FOR COPSNEW SOCIAL SECURITY CARD COULD THWART ILLEGAL IMMIGRANTSAS HISPANICS EMBRACE LIFE IN U.S., SOME PREFER TO LIMIT FAMILY SIZETEXAS CONGRESSMAN CORNYN TO LEAD IMMIGRATION PANEL; SENATOR'S BILL, REFLECTIVE OF WHITE HOUSE REFORM GOALS, MEETS OPPOSITIONFIGHTING FOR U.S., AND FOR GREEN CARDSBUSH VOWS TO PUSH IMMIGRATION PLAN THROUGH THIS TIMEON SCREEN, TACKLING EUROPE'S NEW REALITYBUSH AGENDA FACES SOME GOP RESISTANCE
EDITORIAL: THE PRESIDENT'S SHOELACESPROBLEMS WITH SPEAKING ENGLISH MULTIPLY IN A DECADEGUEST WORKER PLAN DIVIDES LAWMAKERS PRESIDENT SUPPORTS IDEA TO HELP ILLEGAL ALIENS TOILING IN U.S.ALLEGED LEADER IN HUMAN-SMUGGLING DEATHS WANTS TO WITHDRAW GUILTY PLEAJURY SELECTION TO BEGIN TUESDAY IN HUMAN-SMUGGLING TRIAL OF ACCUSED TRUCKER IN DEATHS OF 19 ILLEGAL IMMIGRANTSNEW MIGRANT LAW IRKS MEXICOHELPING EDUCATORS THROW OUT OLD RULES AND TAKE A FEW RISKSRECORD IMMIGRATION CHANGING NEW YORK'S NEIGHBORHOODSATTORNEYS SAY TESTIMONY WILL SHOW OFFICIALS LET TRUCK PASS WITH ILLEGAL IMMIGRANTSFEINSTEIN BILL WOULD PROTECT FOREIGN KIDS IN U.S. CUSTODY SENATE BILL WOULD PROTECT FOREIGN CHILDREN IN U.S. CUSTODYIMMIGRATION BOOM COOLINGREPUBLICANS SQUARING OFF OVER BUSH PLAN ON IMMIGRATIONSMUGGLING-DEFENDANT-HNSBUSH VOWS COOPERATION ON IMMIGRATION REFORM; DIFFERENCES OVER SCOPE, AGENDA MAY STALL PLAN
JUDGE SAYS GUILTY PLEA IN DEADLY TEXAS SMUGGLING MUST STANDHOUSING, IMMIGRATION CALLED KEYS TO THE FUTUREPRESIDENT REIGNITES EMOTIONAL DEBATE OVER IMMIGRATION POLICYGUEST WORKER PLAN WILL BE TOUGH SELL FOR BUSHMEXICAN POLITICIANS FIND BENEFITS IN U.S. CAMPAIGNSSEN. CORNYN FOCUESES IN IMMIGRATIONTANCREDO WEIGHS PRESIDENTIAL RUN WITH PILGRIMAGE TO N.H.BRITAIN, SPAIN, BOTH IN EU, ANNOUNCE DIVERGENT IMMIGRATION POLICIESSPAIN LETS ILLEGAL IMMIGRANTS SEEK RESIDENCYIMMIGRANT-LICENSES-HNSBUSH BACKS DRIVER'S LICENSE BANDEPORTED FROM MEXICO, 3 MORE IN ALLEGED SMUGGLING RING MAY FACE CAPITAL PUNISHMENT IN DEATHS OF 19 IMMIGRANTSHOUSE APPROVES TOUGHER IMMIGRATION BILLTAKING HARD LINE ON ILLEGAL IMMIGRANTS: HOUSE PASSES BILL TO MAKE IT TOUGHER TO GET ASYLUM OR DRIVER'S LICENSES
HOUSE PASSES TIGHTENING OF LAWS ON IMMIGRATIONHOUSE OKS BAN ON LICENSES FOR ILLEGAL IMMIGRANTSMEXICANS HELP TRANSFORM HOMES THEY LEFTANDY GARCIA NEARS END OF HIS QUEST: A FILM ON CUBAMEXICO HAS JOBS PLAN FOR CITIZENS DEPORTED FROM U.S.REPORT LINKS SOCIAL SECURITY FINANCES IN PART TO LEVELS OF IMMIGRATIONIN DEPORTATIONS OF PARENTS, FATE OF CHILDREN IS OFTEN AN AFTERTHOUGHTJUDGE BLOCKS NEW YORK DENIAL OF IMMIGRANT DRIVER LICENSESIMMIGRANT VOTERS DEFY POLITICAL PATTERNSKC-AFGHAN-NEWSPOLICY SHIFT IN GERMANY TRIMS JEWISH MIGRATIONIN JOB MARKET, SOME WIN, SOME LOSEHUNDREDS GET AID AT VALLEY SHELTERSPOLITICAL PRESSURE MOUNTING TO BOOST BORDER PATROL AGENTS ALONG BORDER
P1 A23 MEXICO-HNSMEXICO-VOTE-HNSBILL TO LET MEXICAN MIGRANTS VOTE HITS ROADBLOCKSMORE DUTCH PLAN TO EMIGRATE AS MUSLIM INFLUX TIPS SCALESGONZALES LAYS OUT HIS PRIORITIES AT JUSTICE DEPT.MOST UNDOCUMENTED IMMIGRANTS RECEPTIVE TO GUEST WORKER PROGRAMSURVEY: MOST MEXICAN IMMIGRANTS WOULD USE GUEST WORKER PROGRAMSURVEY: MOST UNDOCUMENTED ALIENS SUPPORT GUEST WORKER PLANNEW STUDY PAINTS CLEARER PICTURE OF MEXICANS IN NEW YORK CITYIMMIGRATION CHANGES COULD CUT BACK ASYLUM SEEKERSTEXAS TO HOST U.S.-MEXICO-CANDADA SUMMITYOUNG BULLDOGS LEARN HARD WAYTRIAL STARTS IN NATION'S DEADLIEST HUMAN SMUGGLING CASERICE SAYS AL-QAIDA FOCUSED ON BREACHING U.S. BORDERS, ANNOUNCES WATER AGREEMENT WITH MEXICOBORDER-PATROL-HNS
RICE SEEKS THAW IN MEXICO-U.S. RELATIONSCASE FOCUSES ON DEFINITION OF TORTURE FOR DEPORTEESFIRST THEY WERE SOLDIERS -- NOW THEY'RE CITIZENS; IMMIGRANTS WHO FOUGHT FOR U.S. ARE NATURALIZED, GREETED BY BUSH SR.ADVANCE FOR SUNDAY, MARCH 13 IMMIGRANTS MAY GET TO VOTE HERE IN MEXICO'S 2006 ELECTIONDESPITE NEW EFFORTS ALONG ARIZONA BORDER, 'SERIOUS PROBLEMS' REMAINMYTH CITED REPEATEDLY IN IMMIGRATION DEBATESWEEP NETS 103 SUSPECTS OF MS-13 GANG IN SEVEN CITIESFEDS SAY SWEEP NETS 100 MEMBERS OF IMMIGRANT GANGU.S.-MEXICO BORDER STILL TOO POROUS, OFFICIALS SAYALLEGED OBSCENE GESTURE DELAYS IMMIGRANT SMUGGLING TRIAL; DEATH PENALTY PROTESTERS CLAIM JUROR HAS ALREADY MADE UP HIS MINDFOX REFUTES U.S. CLAIMS ON AL-QAIDA, VOWS LEGAL ACTION TO HALT VIGILANTESFOX TO PUSH IMMIGRATION, SECURITY, TRADE ISSUES DURING MEETING WITH BUSH, CANADA'S PRIME MINISTERTESTIMONY IN TRUCK DRIVER'S IMMIGRANT SMUGGLING CASE HALTED AFTER PROSECUTION RESTS; JUDGE QUESTIONS HARBORING CHARGES57 BRAZILIANS HELD AFTER BRIBE IS ALLEGED
WAL-MART TO PAY $11 MILLION IN ILLEGAL IMMIGRANT CASEWAL-MART SETTLES ILLEGAL IMMIGRANT CASE FOR $11 MILLIONGARCIA WANTS TO BE PART OF THE CONVERSATIONEDITORIAL OBSERVER: ENLIGHTENED IMMIGRATIONEDITORIAL: OUR TERRORIST-FRIENDLY BORDERS
10.3 MILLION FROM MEXICO IN U.S. ILLEGALLY, RESEARCHER ON LATINOS SAYSBUSH FOCUSES ON BORDER ISSUES WITH MEXICO, CANADALANGUAGE PLAYS A LOUD VOICE IN DEBATE ABOUT IMMIGRATION REFORMU.S. BEGINS TO SEE NATIONAL SECURITY GAP IN MEXICAN SMUGGLINGMEXICANS VOTING IN U.S. COULD ALTER POLITICSTEXAS ADVOCATES FOR IMMIGRATION REFORMS JOIN OTHERS AROUND NATION IN RALLIES URGING BUSH, FOX TO ACT QUICKLYSECURITY, TRADE TO BE PRIMARY FOCUS OF BUSH-FOX-MARTIN SUMMITNORTH AMERICAN LEADERS MAKE BORDERS AND TRADE A PRIORITYBUSH TELLS MEXICAN LEADER HE'LL CONTINUE TO SEEK IMMIGRATION LAW CHANGESBUSH TELLS MEXICAN LEADER HE'LL SEEK IMMIGRATION LAW CHANGESLEAVING A YEAR EARLY, BUT A YEAR TOO LATEBUSH SUMMIT VOWS CLOSER TIES, BETTER TIMES AHEADU.S. SIGNS TRADE, SECURITY DEAL WITH MEXICO, CANADA; BUSH PUSHES FOR IMPROVED TIES WITH SOUTH AMERICAKEEPING IMMIGRATION LEGAL
DUKE BLOCKS GEORGIA'S ROAD TO INDYTHAT BURGER-FLIPPER IS NO KID ANYMOREMOTHERS IMMIGRATING TO BECOME BREADWINNERSLAST OF THREE PARTS; WITH PHOTOS, GRAPHIC CANADA'S OPEN BORDER BOON TO HUMAN TRAFFICKERSBEST, BRIGHTEST MUST CHOOSE BETWEEN MENIAL JOBS IN U.S., ROCKY FUTURE AT HOMEU.S. TO REINFORCE POROUS ARIZONA BORDERREPORT URGES CUTS IN CARE FOR ILLEGAL IMMIGRANTSDNA HELPS IDENTIFIES MEXICAN MIGRANTS IN PAUPERS' GRAVESCIVILIAN PATROL TO RAISE BORDER CONCERNSFALLEN BROTHER INSPIRATION FOR GARCIAARMED VOLUNTEERS WAIT ALONG ARIZONA BORDER TO STOP ILLEGAL IMMIGRANTSKC-5LOUVILLE1,WANTED: BORDER HOPPERS. AND SOME EXCITEMENT, TOO.VOLUNTEERS SET TO PATROL ARIZ. BORDER
IMMIGRATION FOES BEGIN ARIZONA BORDER WATCHHOW SOCIAL SECURITY BALANCES BOOKS ON BACKS OF IMMIGRANTSCITIZEN PATROL SPREADS FEAR, RESOLVE AT US-MEXICO BORDERCITIZEN PATROL SPREADS FEAR, RESOLVE AT BORDERFEW VOLUNTEERS FOR BORDER PROJECTWHITE POWER GROUPS TRY NEW TACTICS AND TOOLSPOLICE SAY IMMIGRANT POLICY IS A HINDRANCEBATTLE OVER LICENSES FOR IMMIGRANTS BACK IN COURTTHE INVISIBLE DELIVERYMANGIRL CALLED WOULD-BE BOMBER WAS DRAWN TO ISLAMRAID NETS 53 ILLEGAL IMMIGRANTS IN SOUTHWEST HOUSTON HOMEBUSINESSES MAKING A PUSH FOR GUEST WORKER PLAN MOVING IN WASHINGTON AND FINANCIAL CATEGORIES FOR RELEASE SUNDAY, APRIL 10.OUTRAGE AT ARREST OF GIRL, 16, AS TERRORIST THREATADVANCE FOR USE SUNDAY, APRIL 10, AND THEREAFTER. "MINUTEMEN" SEE LITTLE ACTION ALONG BORDER
COMMENTARY: AILING HEALTH CARELOCAL BRAZILIANS SAY THEY'RE TARGETED UNFAIRLYEDITORIAL: A WEST TOO WILDSIERRA CLUB ASKS MEMBER VOTE ON IMMIGRATION LIMITSSIERRA CLUB SPLIT AGAIN ON IMMIGRATION STANCEFRIST OPPOSES AMENDMENTS ON IMMIGRANTSBORDER RESIDENTS SAY 'MINUTEMAN' PATROLS HIGHLIGHT A CRISISIMMIGRATION MEASURE HITS SENATE ROADBLOCKHOTEL FIRE SHEDS LIGHT ON FRANCE'S ILLEGAL IMMIGRANTSDEEPLY SPLIT SENATE REJECTS GUEST FARMWORKER BILLSENATE CLEARS WAY FOR VOTE ON SPENDING FOR MILITARYSENATE APPROVES $81.26 BILLION IN A MILITARY EMERGENCY BILLIMMIGRATION CONTROL ADVOCATES DESCEND ON CAPITOL HILLPOLICE REPORT NONCITIZENS TO U.S., OFFICIAL SAYSBRITISH ELECTION DEBATE SPOTLIGHTS CONCERN ABOUT IMMIGRATIONTOP DOGS! GYM DOGS TAKE TITLE
ILLEGAL IMMIGRATION FOES DEMANDING ACTIONSIERRA CLUB STANDS PAT ON IMMIGRATION POLICYKOSOVAR FEARS ID PROPOSAL WILL JEOPARDIZE SAFE LIFE IN U.S.A MISTAKEN ID LAW (FOR USETRAFFICKING LEADS LATINO SUMMIT AGENDAIMMIGRATION-SCAM-HNSLATINO KIDS LAG IN HEALTH COVERAGELAWMAKERS TO DECIDE FATE OF DRIVER'S LICENSE IMMIGRATION BILLWHITE HOUSE BACKS LEGISLATION THAT WOULD TOUGHEN IMMIGRATION RULESIN RARE ACCORD, SPURNED ASYLUM SEEKER TO GET $87,500COMMENTARY: A PRIVATE OBSESSIONEX-VALLEY MAN IN VANGUARD OF MINUTEMAN PROJECTSCHWARZENEGGER ENDORSES ARMED VOLUNTEERS ON BORDERGOVERNOR SIGNALS HE'D WELCOME MINUTEMEN ON CALIFORNIA BORDER
VALLEY HOSPITAL BOOM UNDER WAYACTIVISTS, OPPONENTS CLASH AT IMMIGRATION RALLYMEXICAN SENATOR WANTS TO BLOCK WOULD-BE ILLEGAL IMMIGRANTS FROM ENTERING U.S.MAYANS HERE TRY TO SAVE OLD WAYSSTATE OFFICIALS WARY OF NEW DRIVER'S LICENSE REQUIREMENTSEDITORIAL: AN UNREALISTIC 'REAL ID'ROUTINE LICENSE CHECK CAN MEAN JAIL AND DEPORTATIONHOUSE PASSES EMERGENCY SPENDING BILLBILL WOULD PROTECT ILLEGAL IMMIGRANT DRIVERS' CARS FROM IMPOUNDHOUSE OKS $82 BILLION MORE FOR WARS
IMMIGRANTS IN TENNESSEE ISSUED CERTIFICATES TO DRIVE ARIEL HART CONTRIBUTED REPORTING FOR THIS ARTICLE FROM ATLANTA.
PAYMENTS TO HELP HOSPITALS CARE FOR ILLEGAL IMMIGRANTSIMMIGRANTS' PLIGHT BECOMES A RALLYING CRY AMONG LATINO, U.S. MUSICIANSCATHOLIC GROUPS LAUNCH IMMIGRATION REFORM CAMPAIGNBORDER STATES COMPLAIN THAT U.S. ISN'T FOOTING THE BILL FOR JAILING ILLEGAL IMMIGRANTSNATIONAL CHILDREN'S STUDY STARVING FOR FUNDS, BACKERS SAYSENATE APPROVES MONEY FOR IRAQ WAR; RESTRICTS DRIVER'S LICENSES FOR ILLEGAL IMMIGRANTSIMMIGRANTS ENCOURAGED TO RIDE BUSIMMIGRATION-CRACKDOWN-HNSSENATE UNANIMOUSLY OKS WAR FUNDING AND DRIVERS LICENSE RESTRICTIONS FOR IMMIGRANTSDENIAL OF DRIVER'S LICENSES TO MANY IMMIGRANTS VOIDED IN NEW YORKMINUTEMEN-IMMIGRANTS-HNSMAJOR IMMIGRATION REFORM MEASURE TO BE INTRODUCEDGARCIA MAY HAVE CRASHED, BUT HE'S NOT BURNED UPBILL WOULD ALLOW ILLEGAL IMMIGRANTS TO BECOME LEGAL TEMPORARY WORKERS
MCCAIN, KENNEDY BILL WOULD PUT MILLIONS OF ILLEGALS ON PATH TO GREEN CARDKENNEDY, MCCAIN BILL ADDRESSES IMMIGRANTSIMMIGRATION-REFORM-HNSIMMIGRANT LABOR BILL CREATES 3-YEAR VISAS FOR GUEST WORKERSU.S. OFFICIALS, AFRICAN AMERICAN LEADERS SEEK APOLOGY OVER MEXICAN PRESIDENT'S REMARKSSMUGGLING OF IMMIGRANTS IS DETAILED AS TRIAL STARTSFOX MEETS JACKSON SEEKING TO EASE UPROAR OVER REMARKSEDITORIAL: MAJOR IMMIGRATION SURGERYN.H. POLICE CHIEF'S TACTICS STIR A STORM ON IMMIGRATIONNH-IMMIGRATION-ART-BOSPOST-9/11 PROGRAM MAY END FAMILY'S AMERICAN DREAMSTRESSFUL LIVES BURDEN REFUGEESECUADORANS LEAD DANBURY IMMIGRATION PROTEST RALLYEARLY HEAT WAVE KILLS 12 ILLEGAL IMMIGRANTS IN THE ARIZONA DESERT
FEDERAL RESERVE PROGRAM GIVES BANKS A SHOT AT TRANSFERS TO MEXICOBILL WOULD FORCE SAVINGS ON MEDICAID SPENDINGBILL BY GOP SENATORS INCREASES BORDER GUARDS; NEW SECURITY IS PART OF AN OVERALL IMMIGRATION PLANA BATTLE AGAINST ILLEGAL WORKERS, WITH AN UNLIKELY DRIVING FORCEPOLICE ACROSS U.S. DON'T CHECK IMMIGRANT STATUS DURING STOPSBOOK REVIEW: EXPLORING IMMIGRANT SMUGGLING TRAGEDYIMMIGRATION MAY BE MAJOR ISSUE IN 2008 ELECTION EUNICE MOSCOSOBULLDOGS SET PACE IN NCAASTEXAN PLANS TO BRING MINUTEMEN PATROLS TO MEXICAN BORDERGEORGIA TO BATTLE JACKETS FOR TITLESOME SKILLED FOREIGNERS FIND JOBS SCARCE IN CANADAAT VATICAN'S DOORSTEP, A CONTEST FOR IMMIGRANT SOULSBABY SURVIVES AGAINST ALL ODDSIDENTITY CRISIS: SOCIAL SECURITY NUMBERS FOR RENT
NATION PONDERS IMMIGRANT WORKER PARADOXWEB CLASSES FROM MEXICO HELP MIGRANTSNUMBER OF NON-MEXICAN ALIENS CROSSING SOUTHERN BORDER SKYROCKETINGIMMIGRATION OFFICIALS SEEK EXPANSION OF PROGRAM THAT ALLOWS BORDER AGENTS TO QUICKLY DEPORT ILLEGAL IMMIGRANTSLAZARUS AT LARGE COLUMN HEALTH CARE A DRAG ON U.S. BUSINESSMOST ILLEGAL ALIENS FREED ON BAIL, OWN RECOGNIZANCEDELAY SAYS BUSH PROMISES BETTER EFFORT ON IMMIGRATION LAWBUSH-IMMIGRATION-HNSGROWTH RATE OF HISPANIC POPULATION IS RISING, CENSUS BUREAU SAYSREPORT DESCRIBES IMMIGRANTS AS YOUNGER, MORE DIVERSESHARED LANGUAGE (FOR USEDIPLOMAT: MIGRANT BILL NEEDEDIMMIGRATION REFORM AT TOP OF MANY AGENDAS; SIMILAR PROPOSALS BY BUSH, SEN. CORNYN TO TACKLE GUEST WORKERS, BORDER SECURITYSOUTH TEXAS COUNTY OVERWHELMED BY ILLEGAL IMMIGRANTS
STUDY TRACKS SURGE IN ILLEGAL IMMIGRATION FROM MEXICONO WORRIES AT PINEHURST FOR 'EL NINO'ONE IN 11 MEXICAN NATIVES IN U.S., HALF ILLEGALLOW-PROFILE KENTUCKY TOBACCO MAN BUYS UP TEXAS RANCH LANDBOOK REVIEW: CREATING A NEW AMERICANISMOCORNYN-IMMIGRATION-HNSLAWMAKER SAYS ILLEGAL IMMIGRANTS SHOULDN'T COUNT IN THE CENSUSGEORGIA STATE LOOKS AT FOOTBALLGARCIA HAS ALL THE SHOTS BUT NOT A MAJOR TITLEGUARDSMAN KILLED IN AFGHANISTAN BURIEDTWO IMMIGRATION PLANS TAKE SHAPE IN SENATEUP TO 64 LABORERS LIVED IN A SMALL HOUSE, AUTHORITIES SAY
THE VALUE OF IMMIGRANTSFEDS FAIL TO GO AFTER COMPANIES HIRING ILLEGAL IMMIGRANTSMINUTEMAN GROUP MAKES PLANS FOR TEXAS PATROL GEORGIA LAGS BEHIND IN LOCAL EMERGENCY PLANNING GROUPSEDITORIAL: SHAM SANCTIONSON LONG ISLAND, A RAID STIRS DISPUTE OVER INFLUX OF IMMIGRANTSHISPANIC POLITICAL POWER LAGS BEHIND RECORD GROWTH , STUDY SAYSLEGISLATION TO LICENSE UNDOCUMENTED IMMIGRANTS MOVES FORWARDBUSH ADMINISTRATION BORDER SURVEY NOT RELEASEDMEXICO TO LET MIGRANTS VOTE BY MAILLAWMAKERS IN MEXICO APPROVE ABSENTEE VOTING FOR MIGRANTSGARCIA: TOO GOOD TO BE TRUE?BUSH'S STAND ON IMMIGRATION RILES SOME OF THE PARTY'S BASE
BRAZILIANS STREAMING INTO U.S. THROUGH MEXICAN BORDERBUSH ADMINISTRATION SAYS MEXICAN STAMPS ARE INAPPROPRIATETECH ASSISTANT TAPPED FOR GEORGIA STATE ADLONG ISLAND OFFICIALS TRY A DIFFERENT APPROACH TO IMMIGRANT CRACKDOWN
Figure 6: Snapshot of a single article node
and all of its neghboring article nodes. See
http://zoom.it/GUCR for the zoomable image.
6.2.2 Baselines
k-means baseline: A simple baseline is to
split each six-month period of articles into T
equal time slices, then apply k-means clustering
to each slice, using NCS to measure distance.
We then select the most central article from each
cluster, and finally match the k articles from
time slice i one-to-one with those from slice i+ 1
by computing the pairing that maximizes the
average NCS of the pairs, i.e., the coherence of
the threads. The result is a set of k threads
of length T , where no two threads contain the
same article. In its use of clustering, this base-
line is somewhat similar to the ?event threading?
baseline of Shahaf and Guestrin (2010).
DTM baseline: A more sophisticated base-
line is the dynamic topic model (Blei and Lafferty,
2006), which explicitly attempts to find topics
that are smooth through time. We use code
provided by the authors to fit DTMs with the
number of topics set to k and with the data split
into T equal slices, as before. We then choose,
for each topic at each time step, the document
with the highest per-word probability of being
generated by that topic. Documents from the
same topic form a single thread.
717
CosSim
ROUGE-1 ROUGE-2 ROUGE-SU4
F Prec/ Rec F Prec / Rec F Prec/ Rec
k-means 29.9 16.5 17.3/15.8 0.695 0.725 / 0.669 3.76 3.94/3.60
DTM 27.0 14.7 15.5/14.0 0.750 0.813 / 0.698 3.44 3.63/3.28
k-SDPP 33.2 17.2 17.7/16.7 0.892 0.917/0.870 3.98 4.11/3.87
Table 1: Similarity of automatically generated timelines to human summaries. Bold entries are significantly
higher than others in the column at 99% confidence, computed using bootstrapping (Hesterberg et al2003).
6.2.3 Comparison to human summaries
We compare the threads generated by our
baselines and sampled from the k-SDPP to a
set of human-generated news summaries. The
human summaries are not threaded; they are
flat, roughly daily news summaries published by
Agence France-Presse and found in the Gigaword
corpus, distinguished by their ?multi? type tag.
A sample summary is included in the supplement.
These summaries tend to focus on world news,
which is only a subset of the contents of our
dataset. However, they allow us to provide an
extrinsic evaluation of our method without gold
standard timelines. We compute four statistics:
? Cosine similarity: NCS (in percent) be-
tween the concatenated threads and con-
catenated human summaries. The hyper-
parameters for all methods?such as the
constant feature magnitude ? for k-SDPPs
and the parameter governing topic propor-
tions for DTMs?were tuned to optimize
cosine similarity on a development set from
January-June 2005.
? ROUGE-1, 2, and SU4: Standard
ROUGE scores for summarization evalua-
tion (Lin, 2004).
Table 1 shows the results of these comparisons,
averaged across all six half-year intervals. Under
each measure, the k-SDPP threads more closely
resemble human summaries.
6.2.4 Mechanical Turk evaluation
An important distinction between the base-
lines and the k-SDPP is that the former are
topic-oriented, choosing articles that relate to
broad subject areas, while our approach is story-
oriented, chaining together articles with direct
Rating Interlopers
k-means 2.73 0.71
DTM 3.19 1.10
k-SDPP 3.31 1.15
Table 2: Rating: average coherence score from 1
(worst) to 5 (best). Interlopers: average number of
interloper articles identified (out of 2). Bold entries
are significantly higher with 95% confidence.
individual relationships. An example of this dis-
tinction can be seen in Figure 2.
To obtain a large-scale evaluation of thread co-
herence, we turn to Mechanical Turk. We asked
Turkers to read the headlines and first few sen-
tences of each article in a timeline and then rate
the overall narrative coherence of the timeline on
a scale of 1 (?the articles are totally unrelated?)
to 5 (?the articles tell a single clear story?). Five
separate Turkers rated each timeline; the average
ratings are shown in Table 2. Note that k-means
does particularly poorly in terms of coherence
since it has no way to ensure that clusters are
similar between time slices.
We also had Turkers evaluate threads implic-
itly by performing a simple task. We showed
them timelines into which two additional ?in-
terloper? articles selected at random had been
inserted, and asked them to remove the two ar-
ticles that they thought should be removed to
?improve the flow of the timeline?. A screenshot
of the task is provided in the supplement. Intu-
itively, the interlopers should be selected more
often when the original timeline is coherent. The
average number of interloper articles correctly
identified is shown in Table 2.
718
Runtime
k-means 625.63
DTM 19,433.80
k-SDPP 252.38
Table 3: Time (in seconds) required to produce a
complete set of threads. The test machine has eight
Intel Xeon E5450 cores and 32GB of memory.
6.2.5 Runtimes
Finally, we report in Table 3 the time required
to produce a complete set of threads for each
method. This time includes clustering for k-
means, model fitting for DTM and random pro-
jections, computation of the covariance matrix,
and sampling for k-SDPP. We view the graph
as an input (much like tfidf vectors for the base-
lines), and so do not include its computation in
the runtime for the k-SDPP. Constructing the
graph only requires an additional 160 seconds
though.
6.3 Analysis
Below we briefly summarize the main differences
between the k-SDPP and the baselines, and dis-
cuss their significance.
? Neither baseline directly models the docu-
ment threads themselves. In contrast, the
k-SDPP defines a probability distribution
over all possible sets of document threads.
This makes the k-SDPP a better choice for
applications where, for instance, the coher-
ence of individual threads is important.
? While the baselines seek threads that cover
or explain as much of the dataset as possible,
k-SDPPs are better suited for tasks where
a balance between quality and diversity is
key, since its hyperparameters correspond
to weights on these quantities. With news
timelines, for example, we want not just
topical diversity but also a focus on the
most important stories.
? Both baselines require input to be split into
time slices, whereas the k-SDPP does not;
this flexibility allows the k-SDPP to put
multiple articles from a single time slice in
a thread, or to build threads that span only
part of the input period.
? While clustering and topic models rely on
EM to approximately optimize their objec-
tives, the k-SDPP comes with an exact,
polynomial-time sampling algorithm.
Revisiting Figure 2, we can see all of these
advantages in action. The k-SDPP produces
more consistent threads due to its use of graph
information, while the DTM threads, though
topic-focused, are less coherent as a story. Fur-
thermore, DTM threads span the entire time
period, while our method selects threads cover-
ing only relevant spans. The quantitative results
in this section underscore the empirical value of
these characteristics.
7 Conclusion
We introduced the novel problem of finding di-
verse and salient threads in graphs of large doc-
ument collections. We developed a probabilistic
approach, combining SDPPs and k-SDPPs, and
showed how random projections make inference
efficient and yield an approximate model with
bounded variational distance to the original. We
then demonstrated that the method produces
qualitatively reasonable results, and, relative to
several baslines, reproduces human news sum-
maries more faithfully, builds more coherent story
threads, and is significantly faster. It would be
interesting to extend our model to structures be-
yond linear chains to trees and other structures.
8 Acknowledgements
This material is based upon work supported un-
der a National Science Foundation Graduate Re-
search Fellowship and NSF award 0803256.
References
[Ahmed and Xing2010] A. Ahmed and E. Xing. 2010.
Timeline: A Dynamic Hierarchical Dirichlet Pro-
cess Model for Recovering Birth/Death and Evolu-
tion of Topics in Text Stream. In Proc. UAI.
[Allan et al01] J. Allan, R. Gupta, and V. Khan-
delwal. 2001. Temporal Summaries of New Topics.
In Proc. SIGIR.
719
[Blei and Lafferty2006] D. Blei and J. Lafferty. 2006.
Dynamic Topic Models. In Proc. ICML.
[Chieu and Lee2004] H. Chieu and Y. Lee. 2004.
Query Based Event Extraction along a Timeline.
In Proc. SIGIR.
[Erkan and Radev2004] G. Erkan and D.R. Radev.
2004. LexRank: Graph-Based Lexical Central-
ity as Salience in Text Summarization. Journal of
Artificial Intelligence Research, 22(1):457?479.
[Graff and Cieri2009] D. Graff and C. Cieri. 2009. En-
glish Gigaword.
[Hesterberg et al03] T. Hesterberg, S. Monaghan,
D. Moore, A. Clipson, and R. Epstein. 2003. Boot-
strap Methods and Permutation Tests.
[Johnson and Lindenstrauss1984] W. B. Johnson and
J. Lindenstrauss. 1984. Extensions of Lipschitz
Mappings into a Hilbert Space. Contemporary
Mathematics, 26:189?206.
[Kulesza and Taskar2010] A. Kulesza and B. Taskar.
2010. Structured Determinantal Point Processes.
In Proc. NIPS.
[Kulesza and Taskar2011] A. Kulesza and B. Taskar.
2011. k-DPPs: Fixed-Size Determinantal Point
Processes. In Proc. ICML.
[Leskovec et al09] J. Leskovec, L. Backstrom, and
J. Kleinberg. 2009. Meme-tracking and the Dy-
namics of the News Cycle. In Proc. KDD.
[Lin2004] C.Y. Lin. 2004. Rouge: A package for
automatic evaluation of summaries. In Proc. WAS.
[Magen and Zouzias2008] A. Magen and A. Zouzias.
2008. Near Optimal Dimensionality Reductions
that Preserve Volumes. Approximation, Random-
ization and Combinatorial Optimization. Algo-
rithms and Techniques, pages 523?534.
[McCallum et al00] A. McCallum, K. Nigam,
J. Rennie, and K. Seymore. 2000. Automating
the Construction of Internet Portals with Machine
Learning. Information Retrieval Journal, 3:127?
163.
[Mei and Zhai2005] W. Mei and C. Zhai. 2005. Dis-
covering Evolutionary Theme Patterns From Text:
An Exploration of Temporal Text Mining. In Proc.
KDD.
[Shahaf and Guestrin2010] D. Shahaf and
C. Guestrin. 2010. Connecting the Dots
Between News Articles. In Proc. KDD.
[Shahaf et al12] D. Shahaf, C. Guestrin, and
E. Horvitz. 2012. Trains of Thought: Generating
Information Maps. In Proc. WWW.
[Swan and Jensen2000] R. Swan and D. Jensen. 2000.
TimeMines: Constructing Timelines with Statisti-
cal Models of Word Usage. In Proc. KDD.
[Wayne2000] C. Wayne. 2000. Multilingual Topic De-
tection and Tracking: Successful Research Enabled
by Corpora and Evaluation. In Proc. LREC.
[Yan et al11] R. Yan, X. Wan, J. Otterbacher,
L. Kong, X. Li, and Y. Zhang. 2011. Evolutionary
Timeline Summarization: A Balanced Optimiza-
tion Framework via Iterative Substitution. In Proc.
SIGIR.
720
Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural
Language Learning, pages 1389?1398, Jeju Island, Korea, 12?14 July 2012. c?2012 Association for Computational Linguistics
Wiki-ly Supervised Part-of-Speech Tagging
Shen Li
Computer & Information Science
University of Pennsylvania
shenli@seas.upenn.edu
Joa?o V. Grac?a
L2F INESC-ID
Lisboa, Portugal
javg@l2f.inesc-id.pt
Ben Taskar
Computer & Information Science
University of Pennsylvania
taskar@cis.upenn.edu
Abstract
Despite significant recent work, purely unsu-
pervised techniques for part-of-speech (POS)
tagging have not achieved useful accuracies
required by many language processing tasks.
Use of parallel text between resource-rich and
resource-poor languages is one source of weak
supervision that significantly improves accu-
racy. However, parallel text is not always
available and techniques for using it require
multiple complex algorithmic steps. In this
paper we show that we can build POS-taggers
exceeding state-of-the-art bilingual methods
by using simple hidden Markov models and
a freely available and naturally growing re-
source, the Wiktionary. Across eight lan-
guages for which we have labeled data to eval-
uate results, we achieve accuracy that signifi-
cantly exceeds best unsupervised and parallel
text methods. We achieve highest accuracy re-
ported for several languages and show that our
approach yields better out-of-domain taggers
than those trained using fully supervised Penn
Treebank.
1 Introduction
Part-of-speech categories are elementary building
blocks that play an important role in many natu-
ral language processing tasks, from machine trans-
lation to information extraction. Supervised learn-
ing of taggers from POS-annotated training text is
a well-studied task, with several methods achieving
near-human tagging accuracy (Ratnaparkhi, 1996;
Toutanova et al 2003; Shen et al 2007). How-
ever, while English and a handful of other languages
are fortunate enough to have comprehensive POS-
annotated corpora such as the Penn Treebank (Mar-
cus et al 1993), most of the world?s languages have
no labeled corpora. The annotated corpora that do
exist were costly to build (Abeille?, 2003), and are
often not freely available or restricted to research-
only use. Furthermore, much of the annotated text is
of limited genre, normally focusing on newswire or
literary text. Performance of treebank-trained sys-
tems degrades significantly when applied to new do-
mains (Blitzer et al 2006).
Unsupervised induction of POS taggers offers the
possibility of avoiding costly annotation, but de-
spite recent progress, the accuracy of unsupervised
POS taggers still falls far behind supervised sys-
tems, and is not suitable for most applications (Berg-
Kirkpatrick et al 2010; Grac?a et al 2011; Lee et
al., 2010). Using additional information, in the form
of tag dictionaries or parallel text, seems unavoid-
able at present. Early work on using tag dictionaries
used a labeled corpus to extract all allowed word-tag
pairs (Merialdo, 1994), which is quite an unrealis-
tic scenario. More recent work has used a subset of
the observed word-tag pairs and focused on gener-
alizing dictionary entries (Smith and Eisner, 2005;
Haghighi and Klein, 2006; Toutanova and Johnson,
2007; Goldwater and Griffiths, 2007). Using corpus-
based dictionaries greatly biases the test results, and
gives little information about the capacity to gener-
alize to different domains.
Recent work by Das and Petrov (2011) builds
a dictionary for a particular language by transfer-
ring annotated data from a resource-rich language
through the use of word alignments in parallel text.
1389
The main idea is to rely on existing dictionaries for
some languages (e.g. English) and use parallel data
to build a dictionary in the desired language and ex-
tend the dictionary coverage using label propaga-
tion. However, parallel text does not exist for many
pairs of languages and the proposed bilingual pro-
jection algorithms are fairly complex.
In this work we use the Wiktionary, a freely avail-
able, high coverage and constantly growing dic-
tionary for a large number of languages. We ex-
periment with a very simple second-order Hidden
Markov Model with feature-based emissions (Berg-
Kirkpatrick et al 2010; Grac?a et al 2011). We out-
perform best current results using parallel text su-
pervision across 8 different languages, even when
the word type coverage is as low as 20%. Further-
more, using the Brown corpus as out-of-domain data
we show that using the Wiktionary produces bet-
ter taggers than using the Penn Treebank dictionary
(88.5% vs 85.9%). Our empirical analysis and the
natural growth rate of the Wiktionary suggest that
free, high-quality and multi-domain POS-taggers for
a large number of languages can be obtained by stan-
dard and efficient models.
The source code, the dictionary mappings and
the trained models described in this work are
available at http://code.google.com/p/
wikily-supervised-pos-tagger/.
2 Related Work
The scarcity of labeled corpora for resource poor
languages and the challenges of domain adaptation
have led to several efforts to build systems for unsu-
pervised POStagging.
Several lines of research have addressed the fully
unsupervised POS-tagging task: mutual information
clustering (Brown et al 1992; Clark, 2003) has been
used to group words according to their distributional
context. Using dimensionality reduction on word
contexts followed by clustering has led to accuracy
gains (Schu?tze, 1995; Lamar et al 2010). Sequence
models, HMMs in particular, have been used to rep-
resent the probabilistic dependencies between con-
secutive tags. In these approaches, each observa-
tion corresponds to a particular word and each hid-
den state corresponds to a cluster. However, us-
ing maximum likelihood training for such models
does not achieve good results (Clark, 2003): max-
imum likelihood training tends to result in very am-
biguous distributions for common words, in contra-
diction with the rather sparse word-tag distribution.
Several approaches have been proposed to mitigate
this problem, including Bayesian approaches using
an improper Dirichlet prior to favor sparse model
parameters (Johnson, 2007; Gao and Johnson, 2008;
Goldwater and Griffiths, 2007), or using the Poste-
rior Regularization to penalize ambiguous posteri-
ors distributions of tags given tokens (Grac?a et al
2009). Berg-Kirkpatrick et al(2010) and Grac?a et
al. (2011) proposed replacing the multinomial emis-
sion distributions of standard HMMs by maximum
entropy (ME) feature-based distributions. This al-
lows the use of features to capture morphological in-
formation, and achieves very promising results. De-
spite these improvements, fully unsupervised sys-
tems require an oracle to map clusters to true tags
and the performance still fails to be of practical use.
In this paper we follow a different line of work
where we rely on a prior tag dictionary indicating for
each word type what POS tags it can take on (Meri-
aldo, 1994). The task is then, for each word token
in the corpus, to disambiguate between the possible
POS tags. Even when using a tag dictionary, disam-
biguating from all possible tags is still a hard prob-
lem and the accuracy of these methods is still fall far
behind their supervised counterparts. The scarcity
of large, manually-constructed tag dictionaries led
to the development of methods that try to generalize
from a small dictionary with only a handful of en-
tries (Smith and Eisner, 2005; Haghighi and Klein,
2006; Toutanova and Johnson, 2007; Goldwater and
Griffiths, 2007), however most previous works build
the dictionary from the labeled corpus they learn on,
which does not represent a realistic dictionary. In
this paper, we argue that the Wiktionary can serve as
an effective and much less biased tag dictionary.
We note that most of the previous dictionary
based approaches can be applied using the Wik-
tionary and would likely lead to similar accuracy in-
creases that we show in this paper. For example, the
work if Ravi and Knight (2009) minimizes the num-
ber of possible tag-tag transitions in the HMM via
a integer program, hence discarding unlikely tran-
sitions that would confuse the model. Models can
also be trained jointly using parallel corpora in sev-
1390
eral languages, exploiting the fact that different lan-
guages present different ambiguities (Snyder et al
2008).
The Wiktionary has been used extensively for
other tasks such as domain specific information
retrieval (Mu?ller and Gurevych, 2009), ontology
matching (Krizhanovsky and Lin, 2009), synonymy
detection (Navarro et al 2009), sentiment classifi-
cation (Chesley et al 2006). Recently, Ding (2011)
used the Wiktionary to initialize an HMM for Chi-
nese POS tagging combined with label propagation.
3 The Wiktionary and tagged corpora
The Wiktionary1 is a collaborative project that aims
to produce a free, large-scale multilingual dictio-
nary. Its goal is to describe all words from all lan-
guages (currently more than 400) using definitions
and descriptions in English. The coverage of the
Wiktionary varies greatly between languages: cur-
rently there are around 75 languages for which there
exists more than 1000 word types, and 27 for which
there exists more than 10,000 word types. Neverthe-
less, the Wiktionary has been growing at a consid-
erable rate (see Figure 1), and the number of avail-
able words has almost doubled in the last three years.
As more people use the Wiktionary, it is likely to
grow. Unlike tagged corpora, the Wiktionary pro-
vides natural incentives for users to contribute miss-
ing entries and expand this communal resource akin
to Wikipedia. As with Wikipedia, the questions of
accuracy, bias, consistency across languages, and se-
lective coverage are paramount. In this section, we
explore these concerns by comparing Wiktionary to
dictionaries derived from tagged corpora.
3.1 Labeled corpora and Universal tags
We collected part-of-speech tagged corpora for
9 languages, from CoNLL-X and CoNLL-2007
shared tasks on dependency parsing (Buchholz and
Marsi, 2006; Nivre et al 2007). In this work we
use the Universal POS tag set (Petrov et al 2011)
that defines 12 universal categories with a relatively
stable functional definition across languages. These
categories include NOUN, VERB, ADJ = adjective,
ADV = adverb, NUM = number, ADP = adposition,
CONJ = conjunction, DET = determiner, PRON =
1http://www.wiktionary.org/
!"#""$%
!&#""$%
!'#""$%
!(#""$%
!)#""$%
!*#""$%
!+#""$%
!,#""$%
!!#""$%
!-#""$%
-"#""$%
"%
*"""""%
&""""""%
&*"""""%
'""""""%
'*"""""%
(""""""%
(*"""""%
)""""""%
)*"""""%
*""""""%
./01&"%
2301&"%
./41&"%
5671&"%
5681&"%
2691&"%
:;31&"%
<=>1&"%
?@A1&"%
B;=1&"%
5/71&&%
C;D1&&%
./01&&%
2301&&%
./41&&%
5671&&%
5681&&%
2691&&%
:;31&&%
<=>1&&%
?@A1&&%
B;=1&&%
5/71&'%
C;D1&'%
288% -E/79F% 2==60/=4%
Figure 1: Growth of the Wiktionary over the last three
years, showing total number of entries for all languages
and for the 9 languages we consider (left axis). We
also show the corresponding increase in average accuracy
(right axis) achieved by our model across the 9 languages
(see details below).
pronoun, PUNC = punctuation, PRT = particle, and
X = residual (a category for language-specific cat-
egories which defy cross-linguistic classification).
We found several small problems with the mapping2
which we corrected as follows. In Spanish, the fine-
level tag for date (?w?) is mapped to universal tag
NUM, while it should be mapped to NOUN. In Dan-
ish there were no PRT, NUM, PUNC, or DET tags in
the mapping. After examining the corpus guidelines
and the mapping more closely, we found that the tag
AC (Cardinal numeral) and AO (Ordinal numeral)
are mapped to ADJ. Although the corpus guidelines
indicate the category SsCatgram ?adjective? that en-
compasses both ?normal? adjectives (AN) as well as
cardinal numeral (AC) and ordinal numerals (AO),
we decided to tag AC and AO as NUM, since this
assignment better fits the existing mapping. We also
reassigned all punctuation marks, which were erro-
neously mapped to X, to PUNC and the tag U which
is used for words at, de and som, to PRT.
3.2 Wiktionary to Universal tags
There are a total of 330 distinct POS-type tags
in Wiktionary across all languages which we have
mapped to the Universal tagset. Most of the map-
ping was straightforward since the tags used in the
Wiktionary are in fact close to the Universal tag
set. Some exceptions like ?Initialism?, ?Suffix?
2http://code.google.com/p/
universal-pos-tags/
1391
were discarded. We also mapped relatively rare tags
such as ?Interjection?, ?Symbol? to the ?X? tag.
A example of POS tags for several words in the
Wiktionary is shown in Table 1. All the mappings
are available at http://code.google.com/
p/wikily-supervised-pos-tagger/.
3.3 Wiktionary coverage
There are two kinds of coverage of interest: type
coverage and token coverage. We define type cov-
erage as the proportion of word types in the corpus
that simply appear in the Wiktionary (accuracy of
the tag sets are considered in the next subsection).
Token coverage is defined similarly as the portion
of all word tokens in the corpus that appear in the
Wiktionary. These statistics reflect two aspects of
the usefulness of a dictionary that affect learning in
different ways: token coverage increases the density
of supervised signal while type coverage increases
the diversity of word shape supervision. At one ex-
treme, with 100% word and token coverage, we re-
cover the POS tag disambiguation scenario and, on
the other extreme of 0% coverage, we recover the
unsupervised POS induction scenario.
The type and token coverage of Wiktionary for
each of the languages we are using for evaluation
is shown in Figure 2. We plot the coverage bar for
three different versions of Wiktionary (v20100326,
v20110321, v20120320), arranged chronologically.
We chose these three versions of the Wiktionary
simply by date, not any other factors like coverage,
quality or tagging accuracy.
As expected, the newer versions of the Wiktionary
generally have larger coverage both on type level
and token level. Nevertheless, even for languages
whose type coverage is relatively low, such as Greek
(el), the token level coverage is still quite good
(more than half of the tokens are covered). The rea-
son for this is likely the bias of the contributors to-
wards more frequent words. This trend is even more
evident when we break up the coverage by frequency
of the words. Since the number of words varies from
corpus to corpus, we normalize the word counts by
the count of the most frequent word(s) in its corpus
and group the normalized frequency into three cat-
egories labeled as ?low?, ?medium? and ?high? and
for each category, we calculate the word type cover-
age, shown in Figure 3.
Figure 2: Type-level (top) and token-level (bottom) cov-
erage for the nine languages in three versions of the Wik-
tionary.
We also compared the coverage provided by the
Wiktionary versus the Penn Treebank (PTB) ex-
tracted dictionary on the Brown corpus. Figure 4
shows that the Wiktionary provides a greater cover-
age for all sections of the Brown corpus, hence being
a better dictionary for tagging English text in gen-
eral. This is also reflected in the gain in accuracy on
Brown over the taggers learned from the PTB dic-
tionary in our experiments.
3.4 Wiktionary accuracy
A more refined notion of quality is the accuracy of
the tag sets for covered words, as measured against
dictionaries extracted from labeled tree bank cor-
pora. We consider word types that are in both the
Wiktionary (W) and the tree bank dictionaries (T).
For each word type, we compare the two tag sets
and distinguish five different possibilities:
1. Identical: W = T
2. Superset: W ? T
3. Subset: W ? T
4. Overlap: W ? T 6= ?
1392
Wiktionary Entries
Universal POS Set
Language Word POS Definition
English today Adverb # In the current [[era]]; nowadays.
{ADV, NOUN}English today Adverb # On the current [[day]] or [[date]].
English today Noun # A current day or date.
German achtzig Numeral # [[eighty]] {NUM}
Swedish SCB Acronym # [[statistiska]] ... {NOUN}
Portuguese nessa Contraction # {{contraction ... discard entry
Table 1: Examples of constructing Universal POS tag sets from the Wiktionary.
!"!!#$
%!"!!#$
&!"!!#$
'!"!!#$
(!"!!#$
)!!"!!#$
*+$ *,$ ,-$ ,.$ ,/$ 01$ .-$ 21$ /3$
-45$ 6,*076$ 8098$
Figure 3: Word type coverage by normalized frequency:
words are grouped by word count / highest word count
ratio: low [0, 0.01), medium [0.01, 0.1), high [0.1, 1].
5. Disjoint: W ? T = ?.
In Figure 5, the word types are grouped into the
categories described above. Most of the tag sets
(around 90%) in the Wiktionary are identical to or
supersets of the tree bank tag sets for our nine lan-
guages, which is surprisingly accurate. About 10%
of the Wiktionary tag sets are subsets of, partially
overlapping with, or disjoint from the tree bank tag
sets. Our learning methods, which assume the given
tag sets are correct, may be somewhat hurt by these
word types, as we discuss in Section 5.6.
4 Models
Our basic models are first and second order Hidden
Markov Models (HMM and SHMM). We also used
feature-based max-ent emission models with both
(HMM-ME and SHMM-ME). Below, we denote the
sequence of words in a sentence as boldface x and
the sequence of hidden states which correspond to
part-of-speech tags as boldface y. To simplify nota-
tion, we assume that every tag sequence is prefixed
!"!!#$
%!"!!#$
&!"!!#$
'!"!!#$
(!"!!#$
)!!"!!#$
*$ +$ ,$ -$ .$ /$ 0$ 1$ 2$ 3$ 4$ 5$ 6$ 7$ 8$9+-$ :;<=>?@AB$
Figure 4: PTB vs. Wiktionary type coverage across sec-
tions of the Brown corpus.
with two conventional start tags y0 = start, y?1 =
start, allowing us to write as p(y1|y0, y?1) the ini-
tial state probability of the SHMM.
The probability of a sentence x along with a par-
ticular hidden state sequence y in the SHMM is
given by:
p(x,y) =
length(x)?
i=1
pt(yi | yi?1, yi?2)po(xi | yi),
(1)
where po(xi | yi) is the probability of observ-
ing word xi in state yi (emission probability), and
pt(yi | yi?1, yi?2) is the probability of being in state
yi, given two previous states yi?1, yi?2 (transition
probability).
In this work, we compare multinomial and maxi-
mum entropy (log-linear) emission models. Specifi-
cally, the max-ent emission model is:
po(x|y) =
exp(? ? f(x, y))
?
x? exp(? ? f(x
?, y))
(2)
where f(x, y) is a feature function, x ranges over all
1393
!"#$!"#
%!"#&!"#
'!"#(!"#
)!"#*!"#
+!"#,!"#
$!!"#
-.# -/# /0# /1# /2# 34# 10# 54# 26#
3-/178.0# 295/:2/4# 29;2/4# <6/:0.5# -32=<314#
Figure 5: The Wiktionary vs. tree bank tag sets. Around
90% of the Wiktionary tag sets are identical or subsume
tree bank tag sets. See text for details.
word types, and ? are the model parameters. We use
the following feature templates:
? Word identity - lowercased word form if the
word appears more than 10 times in the corpus.
? Hyphen - word contains a hyphen
? Capital - word is uppercased
? Suffix - last 2 and 3 letters of a word if they
appear in more than 20 different word types.
? Number - word contains a digit
The idea of replacing the multinomial models of an
HMM by maximum entropy models has been ap-
plied before in different domains (Chen, 2003), as
well as in POS induction (Berg-Kirkpatrick et al
2010; Grac?a et al 2011).
We use the EM algorithm to learn the models,
restricting the tags of each word to those specified
by the dictionary. For each tag y, the observa-
tions probabilities po(x | y) were initialized ran-
domly for every word type that allows tag y accord-
ing to the Wiktionary and zero otherwise. For the
M-step in max-ent models, there is no closed form
solution so we need to solve an unconstrained op-
timization problem. We use L-BFGS with Wolfe?s
rule line search (Nocedal and Wright, 1999). We
found that EM achieved higher accuracy across lan-
guages compared to direct gradient approach (Berg-
Kirkpatrick et al 2010).
5 Results
We evaluate the accuracy of taggers trained using
the Wiktionary using the 4 different models: A
first order Hidden Markov Model (HMM), a sec-
ond order Hidden Markov Model (SHMM), a first
order Hidden Markov Model with Maximum En-
tropy emission models (HMM-ME) and a second or-
der Hidden Markov Model with Maximum Entropy
emission models (SHMM-ME). For each model we
ran EM for 50 iterations, which was sufficient for
convergence of the likelihood. Following previous
work (Grac?a et al 2011), we used a Gaussian prior
with variance of 10 for the max-ent model param-
eters. We obtain hard assignments using posterior
decoding, where for each position we pick the la-
bel with highest posterior probability: this produces
small but consistent improvements over Viterbi de-
coding.
5.1 Upper and lower bounds
We situate our results against several upper bounds
that use more supervision. We trained the SHMM-
ME model with a dictionary built from the train-
ing and test tree bank (ALL TBD) and also with
tree bank dictionary intersected with the Wiktionary
(Covered TBD). The Covered TBD dictionary is
more supervised than the Wiktionary in the sense
that some of the tag set mismatches of the Wik-
tionary are cleaned using the true corpus tags. We
also report results from training the SHMM-ME in
the standard supervised fashion, using 50 (50 Sent.),
100 (100 Sent.) and all sentences (All Sent.).
As a lower bound we include the results for un-
supervised systems: a regular HMM model trained
with EM (Johnson, 2007) and an HMM model using
a ME emission model trained using direct gradient
(Berg-Kirkpatrick et al 2010)3.
5.2 Bilingual baselines
Finally, we also compare our system against a strong
set of baselines that use bilingual data. These ap-
proaches build a dictionary by transferring labeled
data from a resource rich language (English) to a re-
source poor language (Das and Petrov, 2011). We
compare against two such methods. The first, pro-
jection, builds a dictionary by transferring the pos
3Values for these systems where taken from the D&P paper.
1394
tags from English to the new language using word
alignments. The second method, D&P, is the cur-
rent state-of-the-art system, and runs label propaga-
tion on the dictionary resulting from the projected
method. We note that both of these approaches are
orthogonal to ours and could be used simultaneously
with the Wiktionary.
5.3 Analysis
Table 2 shows results for the different models across
languages. We note that the results are not di-
rectly comparable since both the Unsupervised and
the Bilingual results use a different setup, using the
number of fine grained tags for each language as hid-
den states instead of 12 (as we do). This greatly in-
creases the degrees of freedom of the model allow-
ing it to capture more fine grained distinctions.
The first two observations are that using the ME
entropy emission model always improves over the
standard multinomial model, and using a second or-
der model always performs better. Comparing with
the work of D&P, we see that our model achieves
better accuracy on average and on 5 out of 8 lan-
guages.
The most common errors are due to tag set id-
iosyncrasies. For instance, for English the symbol %
is tagged as NUM by our system while in the Penn
treebank it is tagged as Noun. Other common mis-
takes for English include tagging to as an adposition
(preposition) instead of particle and tagging which
as a pronoun instead of determiner. In the next sub-
sections we analyze the errors in more detail.
Finally, for English we also trained the SHMM-
ME model using the Celex2 dictionary available
from LDC4. Celex2 coverage for the PTB cor-
pus is much smaller than the coverage provided
by the Wiktionary (43.8% type coverage versus
80.0%). Correspondingly, the accuracy of the model
trained using Celex2 is 75.5% compared 87.1%
when trained using the Wiktionary.
5.4 Performance vs. Wiktionary ambiguity
While many words overwhelmingly appear with one
tag in a given genre, in the Wiktionary a large pro-
portion of words are annotated with several tags,
even when those are extremely rare events. Around
4http://www.ldc.upenn.edu/Catalog/
catalogEntry.jsp?catalogId=LDC96L14
35% of word types in English have more than one
tag according to the Wiktionary. This increases the
difficulty of predicting the correct tag as compared
to having a corpus-based dictionary, where words
have a smaller level of ambiguity. For example, in
English, for words with one tag, the accuracy is 95%
(the reason it is not 100% is due to a discrepancy be-
tween the Wiktionary and the tree bank.) For words
with two possible tags, accuracy is 81% and for three
tags, it drops to 63%.
5.5 Generalization to unknown words
Comparing the performance of the proposed model
for words in the Wiktionary against words not in
the Wiktionary, we see an average drop from 89%
to 63% for out-of-vocabulary words across nine lan-
guages. Table 2 shows that the average loss of accu-
racy between All TBD and Covered TBD of 4.5%
(which is due purely to decrease in coverage) is
larger than the loss between Covered TBD and the
best Wiktionary model, of 3.2% (which is due to tag
set inconsistency).
One advantage of the Wiktionary is that it is a gen-
eral purpose dictionary and not tailored for a partic-
ular domain. To illustrate this we compared several
models on the Brown corpus: the SHMM-ME model
using the Wiktionary (Wik), against using a model
trained using a dictionary extracted from the PTB
corpus (PTBD), or trained fully supervised using the
PTB corpus (PTB). We tested all these models on the
15 different sections of the Brown corpus. We also
compare against a state-of-the-art POS-tagger tagger
(ST)5.
Figure 6 shows the accuracy results for each
model on the different sections. The fully super-
vised SHMM-ME model did not perform as well as
the the Stanford tagger (about 3% behind on aver-
age), most likely because of generative vs. discrim-
inate training of the two models and feature differ-
ences. However, quite surprisingly, the Wiktionary-
tag-set-trained model performs much better not only
than the PTB-tag-set-trained model but also the su-
pervised model on the Brown corpus.
5Available at http://nlp.stanford.edu/
software/tagger.shtml
1395
Danish Dutch German Greek English Italian Portuguese Spanish Swedish avg.
Unsupervised
HMM 68.7 57.0 75.9 65.8 63.7 62.9 71.5 68.4 66.7
HMM-ME 69.1 65.1 81.3 71.8 68.1 78.4 80.2 70.1 73.0
Bilingual
Projection 73.6 77.0 83.2 79.3 79.7 82.6 80.1 74.7 78.8
D&P 83.2 79.5 82.8 82.5 86.8 87.9 84.2 80.5 83.4
Wiktionary
HMM 71.8 80.8 77.1 73.1 85.4 84.6 79.1 83.9 76.7 78.4
HMM-ME 82.8 86.1 81.2 80.1 86.1 85.4 83.7 84.6 85.9 83.7
SHMM 74.5 81.6 81.2 73.1 85.0 85.2 79.9 84.5 78.7 79.8
SHMM-ME 83.3 86.3 85.8 79.2 87.1 86.5 84.5 86.4 86.1 84.8
Supervised
Covered TBD 90.1 91.4 89.4 79.7 92.7 86.3 91.5 85.1 91.0 88.6
All TBD 93.6 91.2 95.6 87.9 90.6 92.9 91.2 92.1 83.8 91.0
50 Sent. 65.3 48.5 74.5 74.2 70.2 76.2 79.2 76.2 54.7 68.6
100 Sent. 73.9 52.3 80.9 81.6 77.3 75.3 82.0 80.1 64.8 73.9
All Sent. 93.9 90.9 97.4 95.1 95.8 93.8 95.5 93.8 95.5 94.5
Table 2: Accuracy for Unsupervised, Bilingual, Wiktionary and Supervised models. Avg. is the average of all lan-
guages except English. Unsupervised models are trained without dictionary and use an oracle to map tags to clusters.
Bilingual systems are trained using a dictionary transferred from English into the target language using word align-
ments. The Projection model uses a dictionary build directly from the part-of-speech projection. The D&P model
extends the Projection model dictionary by using Label Propagation. Supervised models are trained using tree bank
information with SHMM-ME: Covered TBD used tree bank tag set for the words only if they are also in the Wiktionary
and All TBD uses tree bank tag sets for all words. 50, 100 and All Sent. models are trained in a supervised manner
using increasing numbers of training sentences.
Figure 6: Model accuracy across the Brown cor-
pus sections. ST: Stanford tagger, Wik: Wiktionary-
tag-set-trained SHMM-ME, PTBD: PTB-tag-set-trained
SHMM-ME, PTB: Supervised SHMM-ME. Wik outper-
forms PTB and PTBD overall.
5.6 Error breakdown
In Section 3.4 we discussed the accuracy of the
Wiktionary tag sets and as Table 2 shows, a dictio-
nary with better tag set quality generally (except for
Greek) improves the POS tagging accuracy. In Fig-
ure 7, we group actual errors by the word type clas-
sified into the five cases discussed above: identical,
superset, subset, overlap, disjoint. We also add oov ?
out-of-vocabulary word types. The largest source of
error across languages are out-of-vocabulary (oov)
word types at around 45% of the errors, followed
by tag set mismatch types: subset, overlap, dis-
joint, which together comprise another 50% of the
errors. As Wiktionary grows, these types of errors
will likely diminish.
Figure 7: Tag errors broken down by the word type clas-
sified into the six classes: oov, identical, superset, subset,
overlap, disjoint (see text for detail). The largest source of
error across languages are out-of-vocabulary (oov) word
types, followed by tag set mismatch types: subset, over-
lap, disjoint.
6 Conclusion
We have shown that the Wiktionary can be used
to train a very simple model to achieve state-of-
art weakly-supervised and out-of-domain POS tag-
gers. The methods outlined in the paper are stan-
dard and easy to replicate, yet highly accurate and
should serve as baselines for more complex propos-
1396
als. These encouraging results show that using free,
collaborative NLP resources can in fact produce re-
sults of the same level or better than using expensive
annotations for many languages. Furthermore, the
Wiktionary contains other possibly useful informa-
tion, such as glosses and translations. It would be
very interesting and perhaps necessary to incorpo-
rate this additional data in order to tackle challenges
that arise across a larger number of language types,
specifically non-European languages.
Acknowledgements
We would like to thank Slav Petrov, Kuzman
Ganchev and Andre? Martins for their helpful feed-
back in early versions of the manuscript. We would
also like to thank to our anonymous reviewers for
their comments and suggestions. Ben Taskar was
partially supported by a Sloan Fellowship, ONR
2010 Young Investigator Award and NSF Grant
1116676.
References
A. Abeille?. 2003. Treebanks: Building and Using Parsed
Corpora. Springer.
Taylor Berg-Kirkpatrick, Alexandre Bouchard-Co?te?,
John DeNero, and Dan Klein. 2010. Painless unsuper-
vised learning with features. In Proc. NAACL, June.
John Blitzer, Ryan McDonald, and Fernando Pereira.
2006. Domain adaptation with structural correspon-
dence learning. In Conference on Empirical Methods
in Natural Language Processing, Sydney, Australia.
Peter F. Brown, Peter V. deSouza, Robert L. Mercer, Vin-
cent J. Della Pietra, and Jenifer C. Lai. 1992. Class-
based n-gram models of natural language. Computa-
tional Linguistics, 18:467?479.
S. Buchholz and E. Marsi. 2006. Conll-x shared task
on multilingual dependency parsing. In Proceedings
of the Tenth Conference on Computational Natural
Language Learning, pages 149?164. Association for
Computational Linguistics.
S.F. Chen. 2003. Conditional and joint models for
grapheme-to-phoneme conversion. In Proc. ECSCT.
P. Chesley, B. Vincent, L. Xu, and R.K. Srihari. 2006.
Using verbs and adjectives to automatically classify
blog sentiment. Training, 580(263):233.
Alexander Clark. 2003. Combining distributional and
morphological information for part of speech induc-
tion. In Proc. EACL.
Dipanjan Das and Slav Petrov. 2011. Unsupervised
part-of-speech tagging with bilingual graph-based pro-
jections. In Proceedings of the 49th Annual Meet-
ing of the Association for Computational Linguistics:
Human Language Technologies, pages 600?609, Port-
land, Oregon, USA, June. Association for Computa-
tional Linguistics.
Weiwei Ding. 2011. Weakly supervised part-of-speech
tagging for chinese using label propagation. Master?s
thesis, University of Texas at Austin.
Jianfeng Gao and Mark Johnson. 2008. A comparison of
Bayesian estimators for unsupervised hidden Markov
model POS taggers. In In Proc. EMNLP, pages 344?
352, Honolulu, Hawaii, October. ACL.
S. Goldwater and T. Griffiths. 2007. A fully Bayesian
approach to unsupervised part-of-speech tagging. In
In Proc. ACL, volume 45, page 744.
J.V. Grac?a, K. Ganchev, L. Coheur, F. Pereira, and
B. Taskar. 2011. Controlling complexity in part-of-
speech induction. Journal of Artificial Intelligence Re-
search, 41(2):527?551.
J. Grac?a, K. Ganchev, F. Pereira, and B. Taskar. 2009.
Parameter vs. posterior sparisty in latent variable mod-
els. In Proc. NIPS.
A. Haghighi and D. Klein. 2006. Prototype-driven learn-
ing for sequence models. In Proc. HTL-NAACL. ACL.
M Johnson. 2007. Why doesn?t EM find good HMM
POS-taggers. In In Proc. EMNLP-CoNLL.
AA Krizhanovsky and F. Lin. 2009. Related
terms search based on wordnet/wiktionary and its
application in ontology matching. Arxiv preprint
arXiv:0907.2209.
Michael Lamar, Yariv Maron, Mark Johnson, and Elie
Bienenstock. 2010. SVD and clustering for unsuper-
vised POS tagging. In Proceedings of the ACL 2010
Conference: Short Papers, pages 215?219, Uppsala,
Sweden, July. Association for Computational Linguis-
tics.
Yoong Keok Lee, Aria Haghighi, and Regina Barzilay.
2010. Simple type-level unsupervised POS tagging.
In Proceedings of the 2010 Conference on Empirical
Methods in Natural Language Processing, pages 853?
861, Cambridge, MA, October. Association for Com-
putational Linguistics.
M.P. Marcus, M.A. Marcinkiewicz, and B. Santorini.
1993. Building a large annotated corpus of En-
glish: The Penn Treebank. Computational linguistics,
19(2):313?330.
B. Merialdo. 1994. Tagging English text with a proba-
bilistic model. Computational linguistics, 20(2):155?
171.
C. Mu?ller and I. Gurevych. 2009. Using wikipedia and
wiktionary in domain-specific information retrieval.
1397
Evaluating Systems for Multilingual and Multimodal
Information Access, pages 219?226.
E. Navarro, F. Sajous, B. Gaume, L. Pre?vot, H. ShuKai,
K. Tzu-Yi, P. Magistry, and H. Chu-Ren. 2009. Wik-
tionary and nlp: Improving synonymy networks. In
Proceedings of the 2009 Workshop on The People?s
Web Meets NLP: Collaboratively Constructed Seman-
tic Resources, pages 19?27. Association for Computa-
tional Linguistics.
J. Nivre, J. Hall, S. Ku?bler, R. McDonald, J. Nilsson,
S. Riedel, and D. Yuret. 2007. The conll 2007 shared
task on dependency parsing. In Proceedings of the
CoNLL Shared Task Session of EMNLP-CoNLL 2007.
Association for Computational Linguistics.
J. Nocedal and Stephen J. Wright. 1999. Numerical op-
timization. Springer.
S. Petrov, D. Das, and R. McDonald. 2011. A
universal part-of-speech tagset. Arxiv preprint
ArXiv:1104.2086.
A. Ratnaparkhi. 1996. A maximum entropy model for
part-of-speech tagging. In Proc. EMNLP. ACL.
Sujith Ravi and Kevin Knight. 2009. Minimized models
for unsupervised part-of-speech tagging. In In Proc.
ACL.
H. Schu?tze. 1995. Distributional part-of-speech tagging.
In Proc. EACL, pages 141?148.
Libin Shen, Giorgio Satta, and Aravind Joshi. 2007.
Guided learning for bidirectional sequence classifica-
tion. In Proc. ACL, Prague, Czech Republic, June.
N. Smith and J. Eisner. 2005. Contrastive estimation:
Training log-linear models on unlabeled data. In Proc.
ACL. ACL.
B. Snyder, T. Naseem, J. Eisenstein, and R. Barzilay.
2008. Unsupervised multilingual learning for POS
tagging. In Proceedings of the Conference on Empiri-
cal Methods in Natural Language Processing, pages
1041?1050. Association for Computational Linguis-
tics.
K. Toutanova and M. Johnson. 2007. A Bayesian LDA-
based model for semi-supervised part-of-speech tag-
ging. In Proc. NIPS, 20.
K. Toutanova, D. Klein, C. Manning, and Y. Singer.
2003. Feature-rich part-of-speech tagging with a
cyclic dependency network. In In Proc. HLT-NAACL.
1398
Learning Tractable Word Alignment Models
with Complex Constraints
Joa?o V. Grac?a?
L2F INESC-ID
Kuzman Ganchev??
University of Pennsylvania
Ben Taskar?
University of Pennsylvania
Word-level alignment of bilingual text is a critical resource for a growing variety of tasks. Proba-
bilistic models for word alignment present a fundamental trade-off between richness of captured
constraints and correlations versus efficiency and tractability of inference. In this article, we
use the Posterior Regularization framework (Grac?a, Ganchev, and Taskar 2007) to incorporate
complex constraints into probabilistic models during learning without changing the efficiency
of the underlying model. We focus on the simple and tractable hidden Markov model, and
present an efficient learning algorithm for incorporating approximate bijectivity and symmetry
constraints. Models estimated with these constraints produce a significant boost in performance
as measured by both precision and recall of manually annotated alignments for six language
pairs. We also report experiments on two different tasks where word alignments are required:
phrase-based machine translation and syntax transfer, and show promising improvements over
standard methods.
1. Introduction
The seminal work of Brown et al (1993b) introduced a series of probabilistic models
(IBM Models 1?5) for statistical machine translation and the concept of ?word-by-
word? alignment, the correspondence between words in source and target languages.
Although no longer competitive as end-to-end translation models, the IBM Models,
as well as the hidden Markov model (HMM) of Vogel, Ney, and Tillmann (1996),
are still widely used for word alignment. Word alignments are used primarily for
extracting minimal translation units for machine translation (MT) (e.g., phrases [Koehn,
Och, and Marcu 2003] and rules [Galley et al 2004; Chiang et al 2005]) as well as for
? INESC-ID Lisboa, Spoken Language Systems Lab, R. Alves Redol 9, 1000-029 LISBOA, Portugal.
E-mail: joao.graca@l2f.inesc-id.pt.
?? University of Pennsylvania, Department of Computer and Information Science, Levine Hall, 3330 Walnut
Street, Philadelphia, PA 19104-6309. E-mail: kuzman@cis.upenn.edu.
? University of Pennsylvania, Department of Computer and Information Science, 3330 Walnut Street,
Philadelphia, PA 19104-6389. E-mail: taskar@cis.upenn.edu.
Submission received: 1 August 2009; revised submission received: 24 December 2009; accepted for
publication: 10 March 2010.
? 2010 Association for Computational Linguistics
Computational Linguistics Volume 36, Number 3
MT system combination (Matusov, Ueffing, and Ney 2006). But their importance has
grown far beyond machine translation: for instance, transferring annotations between
languages (Yarowsky and Ngai 2001; Hwa et al 2005; Ganchev, Gillenwater, and
Taskar 2009); discovery of paraphrases (Bannard and Callison-Burch 2005); and joint
unsupervised POS and parser induction across languages (Snyder and Barzilay 2008).
IBM Models 1 and 2 and the HMM are simple and tractable probabilistic models,
which produce the target sentence one target word at a time by choosing a source word
and generating its translation. IBM Models 3, 4, and 5 attempt to capture fertility (the
tendency of each source word to generate several target words), resulting in probabilis-
tically deficient, intractable models that require local heuristic search and are difficult to
implement and extend. Many researchers use the GIZA++ software package (Och and
Ney 2003) as a black box, selecting IBM Model 4 as a compromise between alignment
quality and efficiency. All of the models are asymmetric (switching target and source
languages produces drastically different results) and the simpler models (IBM Models 1,
2, and HMM) do not enforce bijectivity (the majority of words translating as a single
word). Although there are systematic translation phenomena where one cannot hope to
obtain 1-to-1 alignments, we observe that in over 6 different European language pairs
the majority of alignments are in fact 1-to-1 (86?98%). This leads to the common practice
of post-processing heuristics for intersecting directional alignments to produce nearly
bijective and symmetric results (Koehn, Och, and Marcu 2003).
In this article we focus on the HMM word alignment model (Vogel, Ney, and
Tillmann 1996), using a novel unsupervised learning framework that significantly
boosts its performance. The new training framework, called Posterior Regulariza-
tion (Grac?a, Ganchev, and Taskar 2007), incorporates prior knowledge in the form of
constraints on the model?s posteriors. The constraints are expressed as inequalities on
the expected value under the posterior distribution of user-defined features. Although
the base model remains unchanged, learning guides the model to satisfy these con-
straints. We propose two such constraints: (i) bijectivity: one word should not translate
to many words; and (ii) symmetry: directional alignments should agree. Both of these
constraints significantly improve the performance of the model both in precision and
recall, with the symmetry constraint generally producing more accurate alignments.
Section 3 presents the Posterior Regularization (PR) framework and describes how to
encode such constraints in an efficient manner, requiring only repeated inference in the
original model to enforce the constraints. Section 4 presents a detailed evaluation of
the alignments produced. The constraints over posteriors consistently and significantly
outperform the unconstrained HMM model, evaluated against manual annotations.
Moreover, this training procedure outperforms the more complex IBM Model 4 nine
times out of 12. We examine the influence of constraints on the resulting posterior dis-
tributions and find that they are especially effective for increasing alignment accuracy
for rare words. We also demonstrate a new methodology to avoid overfitting using a
small development corpus. Section 5 evaluates the new framework on two different
tasks that depend on word alignments. Section 5.1 focuses on MT and shows that the
better alignments also lead to better translation systems, adding to similar evidence
presented in Ganchev, Grac?a, and Taskar (2008). Section 5.2 shows that the alignments
we produce are better suited for transfer of syntactic dependency parse annotations.
An implementation of this work (Grac?a, Ganchev, and Taskar 2009) is available under a
GPL license.1
1 www.seas.upenn.edu/?strctlrn/CAT/.
482
Grac?a, Ganchev, and Taskar Learning Alignment Models with Complex Constraints
2. Background
A word alignment for a parallel sentence pair represents the correspondence between
words in a source language and their translations in a target language (Brown et al
1993b). There are many reasons why a simple word-to-word (1-to-1) correspondence
is not possible for every sentence pair: for instance, auxiliary verbs used in one lan-
guage but not the other (e.g., English He walked and French Il est alle?), articles required
in one language but optional in the other (e.g., English Cars use gas and Portuguese
Os carros usam gasolina), cases where the content is expressed using multiple words
in one language and a single word in the other language (e.g., agglutination such as
English weapons of mass destruction and German Massenvernichtungswaffen), and expres-
sions translated indirectly. Due to this inherent ambiguity, manual annotations usually
distinguish between sure correspondences for unambiguous translations, and possible,
for ambiguous translations (Och and Ney 2003). The top row of Figure 1 shows two
word alignments between an English?French sentence pair. We use the following nota-
tion: the alignment on the left (right) will be referenced as source?target (target?source)
and contains source (target) words as rows and target (source) words as columns. Each
entry in the matrix corresponds to a source?target word pair, and is the candidate for an
alignment link. Sure links are represented as squares with borders, and possible links
Figure 1
Posterior marginal distributions for different models for an English to French sentence
translation. Left: EN?FR model. Right: FR? EN model. Top: Regular HMM posteriors.
Middle: After applying bijective constraint. Bottom: After applying symmetric constraint. Sure
alignments are squares with borders; possible alignments are squares without borders. Circle
size indicates probability value. Circle color in the middle and bottom rows indicates differences
in posterior from the top row: green = higher probability; red = lower probability.
483
Computational Linguistics Volume 36, Number 3
Table 1
Test corpora statistics: English?French, English?Spanish, English?Portuguese,
Portuguese?Spanish, Portuguese?French, and Spanish?French.
Corpus Sentence Pairs Ave Length Max Length % Sure % 1-1
En/Fr 447 16/17 30/30 21 98
En/Es 400 29/31 90/99 67 86
En/Pt 60 11/11 20/20 54 91
Pt/Es 60 11/11 20/20 69 92
Pt/Fr 60 11/12 20/20 77 88
Es/Fr 60 11/12 20/20 79 87
are represented as squares without borders. Circles indicate the posterior probability
associated with a given link and will be explained latter.
We use six manually annotated corpora whose characteristics are summarized in
Table 1. The corpora are: the Hansard corpus (Och and Ney 2000) of English/French
Canadian Parliamentary proceedings (En-Fr), and the English/Spanish portion of the
Europarl corpus (Koehn 2005) where the annotation is from EPPS (Lambert et al 2005)
(En-Es) using standard test and development set split. We also used the English/
Portuguese (En-Pt), Portuguese/Spanish (Pt-Es), Portuguese/French (Pt-Fr), and
Spanish/French (Es-Fr) portions of the Europarl corpus using annotations described
by Grac?a et al (2008), where we split the gold alignments into a dev/test set in a ratio of
40%/60%. Table 1 shows some of the variety of challenges presented by each corpus.
For example, En-Es has longer sentences and hence more ambiguity for alignment.
Furthermore, it has a smaller percentage of bijective (1-to-1) alignments, which makes
word fertility more important. Overall, the great majority of links are bijective across
the corpora (86?98%). This characteristic will be explored by the constraints described
in this article. For the evaluations in Section 4, the percentage of sure links (out of all
links) will correlate with difficulty because only sure links are considered for recall.
2.1 HMM Word Alignment Model
In this article we focus on the HMM for word alignment proposed by Vogel, Ney, and
Tillmann (1996). This model generalizes IBM Models 1 and 2 (Brown et al 1993b),
by introducing a first-order Markov dependence between consecutive alignment link
decisions. The model is an (input?output) HMM with I positions whose hidden state
sequence z = (z1, . . . , zI ) with zi ? {null, 1, . . . , J} corresponds to a sequence of source
word positions, where J is the source sentence length, and with null representing un-
aligned target words. Each observation corresponds to a word in the target language xi.
The probability of an alignment z and target sentence x given a source sentence y can
be expressed as:
p?(x, z | y) =
I
?
i=1
pd(zi | zi?1)pt(xi | yzi ) (1)
where pt(xi | yzi ) is the probability of a target word at position i being a translation of the
source word at position zi (translation probability), and pd(zi | zi?1) is the probability
484
Grac?a, Ganchev, and Taskar Learning Alignment Models with Complex Constraints
of translating a word at position zi, given that the previous translated word was at
position zi?1 (distortion probability). Note that this model is directional: Each target
word (observation) can be aligned to at most one source word (hidden state), whereas a
source word could be used multiple times.
We refer to translation parameters pt and distortions parameters pd jointly as ?.
There are several important standard details of the parametrization: The distortion
probability pd(zi | zi?1) depends only on the distance (zi ? zi?1) between the source po-
sitions the states represent. Only distances in the range ?5 are modeled explicitly, with
larger distances assigned equal probabilities. The probability of the initial hidden state,
pd(z1 | z0) is modeled separately from the other distortion probabilities. To incorporate
null links, we add a translation probability given null: pt(xi | ynull). Following standard
practice, null links also maintain position information and do not allow distortion. To
implement this, we create position-specific null hidden states for each source position,
and set pd(nulli|yi? ) = 0 and pd(nulli|nulli? ) = 0 for all i = i?. The model is simple, with
complexity of inference O(I ? J2). There are several problems with the model that arise
from its directionality, however.
 Non-bijective: Multiple target words can be linked to a single source
word. This is rarely desirable. For instance, the model produces
non-bijective links 22% of the time for En-Fr instead of 2%.
 Asymmetric: By switching the (arbitrary) choice of which language is
source and which is target, the HMM produces very different results. For
example, intersecting the sets of alignments produced by the two possible
choices for source preserves less than half of their union for both En-Fr
and En-Pt.2
2.2 Training
Standard HMM training seeks model parameters ? that maximize the log-likelihood of
the parallel corpus:
Log-Likelihood : L(?) = ?E[log p?(x | y)] = ?E[log
?
z
p?(x, z | y)] (2)
where ?E[ f (x, y)] = 1N
?N
n=1 f (x
n, yn) denotes the empirical average of a function f (xn, yn)
over the N pairs of sentences {(x1, y1) . . . , (xN, yN )} in the training corpus. Because
of the latent alignment variables z, the log-likelihood function for the HMM model
is not concave, and the model is fit using the Expectation Maximization (EM) algo-
rithm (Dempster, Laird, and Rubin 1977). EM maximizes L(?) via block-coordinate
ascent on a lower bound F(q, ?) using an auxiliary distribution over the latent variables
q(z | x, y) (Neal and Hinton 1998):
EM Lower Bound : L(?) ? F(q, ?) = ?E
[
?
z
q(z | x, y) log p?(x, z | y)
q(z | x, y)
]
(3)
2 For both of these points, see the experimental setup in Section 4.1.
485
Computational Linguistics Volume 36, Number 3
To simplify notation, we will drop the dependence on y and will write p?(x, z | y) as
p?(x, z), p?(z | x, y) as p?(z | x) and q(z | x, y) as q(z | x). The alternating E and M steps
at iteration t + 1 are given by:
E : qt+1(z | x) = arg max
q(z|x)
F(q, ?t) = arg min
q(z|x)
KL(q(z | x) || p?t (z | x)) = p?t (z | x) (4)
M : ?t+1 = arg max
?
F(qt+1, ?) = arg max
?
?E
[
?
z
qt+1(z | x) log p?(x, z)
]
(5)
where KL(q||p) = Eq[log q(?)p(?) ] is the Kullback-Leibler divergence. The EM algorithm is
guaranteed to converge to a local maximum of L(?) under mild conditions (Neal and
Hinton 1998). The E step computes the posteriors qt+1(z | x) = p?t (z | x) over the latent
variables (alignments) given the observed variables (sentence pair) and current param-
eters ?t, which is accomplished by the forward-backward algorithm for HMMs. The M
step uses qt+1 to ?softly fill in? the values of alignments z and estimate parameters ?t+1.
This step is particularly easy for HMMs, where ?t+1 simply involves normalizing (ex-
pected) counts. This modular split into two intuitive and straightforward steps accounts
for the vast popularity of EM.
In Figure 1, each entry in the alignment matrix contains a circle indicating the align-
ment link posterior for that particular word pair after training an HMM model with the
EM algorithm (see the experimental set up in Section 4.1). Note that the link posteriors
are concentrated around particular source words (rare words occurring less than five
times in the corpus) in both directions, instead of being spread across different words.
This is a well-known problem when training using EM called the ?garbage collector ef-
fect? (Brown et al 1993a). A rare word in the source language links to many words in the
target language that we would ideally like to see unaligned, or aligned to other words
in the sentence. The reason this happens is that the generative model has to distribute
translation probability for each source word among different candidate target words.
If one translation is much more common than another, but the rare translation is used
in the sentence, the model might have a very low translation probability for the correct
alignment. On the other hand, because the rare source word occurs only in a few sen-
tences it needs to spread its probability mass over fewer competing translations. In this
case, choosing to align the rare word to all of these words leads to a higher likelihood
than correctly linking them or linking them to the special null word, because it increases
the likelihood of this sentence without lowering the likelihood of many other sentences.
2.3 Decoding
Alignments are normally predicted using the Viterbi algorithm (which selects the single
most probable path through the HMM?s lattice).
Another possibility that often works better is to use Minimum Bayes-Risk (MBR)
decoding (Kumar and Byrne 2002; Liang, Taskar, and Klein 2006; Grac?a, Ganchev, and
Taskar 2007). Using this decoding we include an alignment link i ? j if the posterior
probability that word i aligns to word j is above some threshold. This allows the
accumulation of probability from several low-scoring alignments that agree on one
alignment link. The threshold is tuned on some small amount of labeled data?in
our case the development set?to minimize some loss. Kumar and Byrne (2002) study
different loss functions that incorporate linguistic knowledge, and show significant
486
Grac?a, Ganchev, and Taskar Learning Alignment Models with Complex Constraints
improvement over likelihood decoding. Note that this could potentially result in an
alignment having zero probability under the model, as many-to-many alignments can
be produced in this way. MBR decoding has several advantages over Viterbi decoding.
First, independently of the particular choice of the loss function, by picking a specific
threshold we can trade off precision and recall of the predicted word alignments. In
fact, in this work when comparing different alignment sets we do not commit to any
loss function but instead compare precision vs recall curves, by generating alignments
for different thresholds (0..1). Second, with this method we can ignore the null word
probabilities, which tend to be poorly estimated.
3. Posterior Regularization
Word alignment models in general and the HMM in particular are very gross over-
simplifications of the translation process and the optimal likelihood parameters learned
often do not correspond to sensible alignments. One solution to this problem is to
add more complexity to the model to better reflect the translation process. This is the
approach taken by IBM Models 4+ (Brown et al 1993b; Och and Ney 2003), and more
recently by the LEAF model (Fraser and Marcu 2007). Unfortunately, these changes
make the models probabilistically deficient and intractable, requiring approximations
and heuristic learning and inference prone to search errors. Instead, we propose to
use a learning framework called Posterior Regularization (Grac?a, Ganchev, and Taskar
2007) that incorporates side information into unsupervised estimation in the form of
constraints on the model?s posteriors. The constraints are expressed as inequalities on
the expected values under the posterior distribution of user-defined constraint features
(not necessarily the same features used by the model). Because in most applications
what we are interested in are the latent variables (in this case the alignments), con-
straining the posteriors allows a more direct way to achieve the desired behavior.
On the other hand, constraining the expected value of the features instead of adding
them to the model allows us to express features that would otherwise make the model
intractable. For example, enforcing that each hidden state of an HMM model should be
used at most once per sentence would break the Markov property and make the model
intractable. In contrast, we will show how to enforce the constraint that each hidden
state is used at most once in expectation. The underlying model remains unchanged,
but the learning method changes. During learning, our method is similar to the EM
algorithm with the addition of solving an optimization problem similar to a maximum
entropy problem inside the E Step. The following subsections present the Posterior
Regularization framework, followed by a description of how to encode two pieces of
prior information aimed at solving the problems described at the end of Section 2.
3.1 Posterior Regularization Framework
The goal of the Posterior Regularization (PR) framework is to guide a model during
learning towards satisfying some prior knowledge about the desired latent variables
(in this case word alignments), encoded as constraints over their expectations. The
key advantage of using regularization on posterior expectations is that the base model
remains unchanged, but during learning, it is driven to obey the constraints by setting
appropriate parameters ?. Moreover, experiments show that enforcing constraints in ex-
pectation results in predicted alignments that also satisfy the constraints. More formally,
posterior information in PR is specified with sets Qx of allowed distributions over the
487
Computational Linguistics Volume 36, Number 3
hidden variables z which satisfy inequality constraints on some user-defined feature
expectations, with violations bounded by  ? 0:
Constrained Posterior Set : Qx = {q(z | x) : ??, Eq[f(x, z)] ? bx ? ?; ||?||22 ? 2} (6)
Qx denotes the set of valid distributions where some feature expectations are bounded
by bx and  ? 0 is an allowable violation slack. Setting  = 0 enforces inequality
constraints strictly. In order to introduce equality constraints, we use two inequality
constraints with opposite signs. We assume that Qx is non-empty for each example x.
Furthermore, the set Qx needs to be convex. In this work we restrict ourselves to
linear inequalities because, as will be shown, subsequently this simplifies the learning
algorithm. Note that Qx, f(x, z), and bx also depend on y, the corresponding source
sentence, but we suppress the dependence for brevity. In PR, the log-likelihood of a
model is penalized with the KL-divergence between the desired distribution space Qx
and the model posteriors, KL(Qx ? p?(z|x)) = min
q(z|x)?Qx
KL(q(z | x) ? p?(z|x)). The regu-
larized objective is:
Posterior Regularized Likelihood : L(?) ? ?E[KL(Qx ? p?(z|x))]. (7)
The objective trades off likelihood and distance to the desired posterior subspace (mod-
ulo getting stuck in local maxima) and provides an effective method of controlling the
posteriors.
Another way of interpreting the objective is to express the marginal log-likelihood
L(?) as a KL distance: KL(?(xn) ? p?(x)) where ?(xn) is a delta function at xn. Hence the
objective is a sum of two average KL terms, one in the space of distributions over x and
one in the space of distributions over z:
?L(?) + ?E[KL(Qx ? p?(z|x))] = 1N
N
?
n=1
KL(?(xn) ? p?(x)) + KL(Qxn ? p?(z|xn)) (8)
This view of the PR objective is illustrated in Figure 2.
Figure 2
Maximizing the PR objective is equivalent to minimizing the empirical average of two
KL divergences: The negative log-likelihood ?L(?) = 1N
?N
n=1 KL(?(x
n) ? p?(x)) plus posterior
regularization 1N
?N
n=1 KL(Qxn ? p?(z|xn)), where ?(xn) is a delta function at xn. The diagram
illustrates the effect of the likelihood term and the regularization term operating over the two
spaces of distributions: the observed variables x and the latent variables z. (The effect of the prior
on ? is not shown.)
488
Grac?a, Ganchev, and Taskar Learning Alignment Models with Complex Constraints
Computing the PR objective involves solving the optimization problem for each x:
Primal Projection : KL(Qx ? p?(z|x)) = min
q(z|x)?Qx
KL(q(z | x) ? p?(z|x)) (9)
Directly minimizing this objective is hard because there is an exponential number of
alignments z; however, the problem becomes easy to solve in its dual formulation (see
Appendix A for derivation):
Dual Projection : arg min
??0
bx ? + log Z(?) +  ||?||2 (10)
where Z(?) =
?
z p?(z|x) exp(?? ? f(x, z)) is the normalization constant and the primal
solution is q(z|x) = p?(z|x) exp{??f(x, z)}/Z(?). There is one dual variable per ex-
pectation constraint, and the dual gradient at ? = 0 is ?(?) = bx ? Eq[f(x, z)] +  ?i||?||2 .
Note that this primal?dual relationship is very similar to the one between maximum
likelihood and maximum entropy. If bx corresponds to empirical expectations and
p?(z|x) is uniform, then Equation (10) would be a log-likelihood and Equation (14) (fol-
lowing) would be a maximum entropy problem. As with maximum entropy, gradient
computation involves computing an expectation under q(z | x), which can be performed
efficiently if the features f(x, z) factor in the same way as the model p?(x, z), and the
constraints are linear. The conditional distribution over z represented by a graphical
model such as HMM can be written as a product of factors over cliques C:
Factored Posterior : p(z | x) = 1Z
?
c?C
?(x, zc) (11)
In an HMM, the cliques C are simply the nodes zi and the edges (zi, zi+1) and the factors
correspond to the distortion and translation probabilities. We will assume f is factorized
as a sum over the same cliques (we will show below how symmetry and bijectivity
constraints can be expressed in this way):
Factored Features : f(x, z) =
?
c?C
f(x, zc) (12)
Then q(z | x) has the same form as p?(z | x):
q(z | x) = 1Zp(z | x) exp(??
f(x, z)) = 1Z
?
c?C
?(x, zc) exp
??f(x,zc ) (13)
Hence the projection step uses the same inference algorithm (forward?backward for
HMMs) to compute the gradient, only modifying the local factors using the current
setting of ?.
?i ? 0;1
while ||?(?)||2 > ? do2
??(x, zc) ? ?(x, zc) exp??
f(x,zc );3
q(z | x) ? forwardBackward(??(x, zc));4
? ? ? + ???(?);5
end6
Algorithm 1: Computing KL(Qx ? p?(z|x)) = min
q?Qx
KL(q(z|x) ? p?(z|x))
489
Computational Linguistics Volume 36, Number 3
We optimize the dual objective using the gradient based methods shown in
Algorithm 1. Here ? is an optimization precision, ? is a step size chosen with the strong
Wolfe?s rule (Nocedal and Wright 1999). Here, ??(?) represents an ascent direction
chosen as follows: For inequality constraints, it is the projected gradient (Bertsekas
1999); for equality constraints with slack, we use conjugate gradient (Nocedal and
Wright 1999), noting that when ? = 0, the objective is not differentiable. In practice
this only happens at the start of optimization and we use a sub-gradient for the first
direction.
Computing the projection requires an algorithm for inference in the original model,
and uses that inference as a subroutine. For HMM word alignments, we need to make
several calls to forward?backward in order to choose ?. Setting the optimization pre-
cision ? more loosely allows the optimization to terminate more quickly but at a less
accurate value. We found that aggressive optimization significantly improves alignment
quality for both constraints we used and consequently choose ? so that tighter values
do not significantly improve performance. This explains why we report better results
here in this paper than in Ganchev, Grac?a, and Taskar (2008), which uses a more naive
optimization (see Section 4.1).
3.2 Posterior Regularization via Expectation Maximization
We can optimize the PR objective using a procedure very similar to the expectation
maximization (EM) algorithm. Recall from Equation (4) that in the E step, q(z | x) is
set to the posterior over hidden variables given the current ?. To converge to the PR
objective, we must modify the E step so that q(z | x) is a projection of the posteriors onto
the constraint set Qx for each example x (Grac?a, Ganchev, and Taskar 2007).
E? : arg min
q,?
KL(q(z|x) ? p?t (z|x)) s.t. Eq[f(x, z)] ? bx ? ?; ||?||22 ? 2 (14)
The new posteriors q(z|x) are used to compute sufficient statistics for this instance and
hence to update the model?s parameters in the M step (Equation (5)), which remains
unchanged. This scheme is illustrated in Figure 3 and in Algorithm 2. The only imple-
mentation difference is that we must now perform the KL projection before collecting
sufficient statistics. We found it can help to also perform this projection at test time,
using q(z | x) = arg min
q(z|x)?Qx
KL(q(z | x)|p?(z | x)) instead of p?(z | x) to decode.
for t = 1..T do1
for each training sentence x do2
E?-Step: qt+1(z | x) = arg min
q(z|x)?Qx
KL(q(z | x)||p?t (z | x))
3
end4
M-Step: ?t+1 = arg max? ?E
[
?
z q
t+1(z | x) log p?(z, x)
]
5
end6
Algorithm 2: PR optimization via modified EM. E?-Step is computed using
Algorithm 1.
490
Grac?a, Ganchev, and Taskar Learning Alignment Models with Complex Constraints
Figure 3
Modified EM for optimizing PR objective L(?) ? ?E[KL(Qx ? p?(z|x))].
3.3 Bijectivity Constraints
We observed in Table 1 that most alignments are 1-to-1 and we would like to introduce
this prior information into the model. Unfortunately including such a constraint in the
model directly breaks the Markov property in a fairly fundamental way. In particular
computing the normalization would require the summation of 1-to-1 or near 1-to-1
weighted matchings, which is a classic #P-complete problem. Introducing alignment
degree constraints in expectation using the PR framework is easy and tractable. We
encode them as the constraint E[f(x, z)] ? 1 where we have one feature f for each source
word j that counts how many times it is aligned to a target word in the alignment z:
Bijective Features : fj(x, z) =
?
i
1(zi = j).
The second row of Figure 1 shows an example of the posteriors after applying bijectivity
constraints; the first row is before the projection. Green (respectively, red) circles indicate
that the probability mass for that particular link increased (respectively, decreased)
when compared with the EM-trained HMM. For example, in the top left panel, the
word schism is used more than once, causing erroneous alignments. Projecting to the
bijectivity constraint set prevents this and most of the mass is (for this example) moved
to the correct word pairs. Enforcing the constraint at training and decoding increases
the fraction of 1-to-1 alignment links from 78% to 97.3% for En-Fr (manual annotations
have 98.1%); for En-Pt the increase is from 84.7% to 95.8% (manual annotations have
90.8%) (see Section 4.1).
3.4 Symmetry Constraints
The directional nature of the generative models used to recover word alignments con-
flicts with their interpretation as translations. In practice, we see that the choice of which
language is source versus target matters and changes the mistakes made by the model
(the first row of panels in Figure 1). The standard approach is to train two models
independently and then intersect their predictions (Och and Ney 2003). However, we
show that it is much better to train two directional models concurrently, coupling
their posterior distributions over alignments to approximately agree. Let the directional
models be defined as: ??p (??z ) (source?target) and ??p (??z ) (target?source). We suppress
dependence on x and y for brevity. Define z to range over the union of all possible
491
Computational Linguistics Volume 36, Number 3
directional alignments
??
Z ???Z . We define a mixture model p(z) = 12
??p (z) + 12
??p (z)
where ??p (??z ) = 0 and vice versa (i.e., the alignment of one directional model has prob-
ability zero according to the other model). We then define the following feature for each
target?source position pair i, j:
Symmetric Features : fij(x, z) =
?
?
?
?
?
+1 z ? ??Z and ??z i = j
?1 z ? ??Z and ??z j = i
0 otherwise
.
If the feature fij has an expected value of zero, then both models predict the i, j link
with equal probability. We therefore impose the constraint Eq[ fij(x, z)] = 0 (possibly with
some small slack). Note that satisfying this implies satisfying the bijectivity constraint
presented earlier. To compute expectations of these features under the model q we only
need to be able to compute them under each directional HMM. To see this, we have by
the definition of q? and p?,
q?(z|x) =
??p (z | x) + ??p (z | x)
2
exp{??f(x, z)}
Z?
=
??q (z|x) Z??q??p (x) +
??q (z|x) Z??q??p (x)
2Z?
(15)
where we have defined:
??q (z|x) = 1Z??q
??p (z, x) exp{??f(x, z)} with Z??q =
?
z
??p (z, x) exp{??f(x, z)}
??q (z|x) = 1Z??q
??p (z, x) exp{??f(x, z)} with Z??q =
?
z
??p (z, x) exp{??f(x, z)}
All these quantities can be computed separately in each model using forward?backward
and, furthermore, Z? = 12 (
Z??q
??p (x) +
Z??q
??p (x) ). The effect of this constraint is illustrated in
the bottom panels of Figure 1. The projected link posteriors are equal for the two
models, and in most cases the probability mass was moved to the correct alignment
links. The exception is the word pair internal/le. In this case, the model chose to incor-
rectly have a high posterior for the alignment link rather than generating internal from
null in one direction and le from null in the other.
We can measure symmetry of predicted alignments as the ratio of the size of the
intersection to the size of the union. Symmetry constraints increase symmetry from 48%
to 89.9% for En-Fr and from 48% to 94.2% for En-Pt (see Section 4.1).
4. Alignment Quality Evaluation
We begin with a comparison of word alignment quality evaluated against manually
annotated alignments as measured by precision and recall. We use the six parallel
corpora with gold annotations described in the beginning of Section 2.
4.1 Experimental Setup
We discarded all training data sentence pairs where one of the sentences contained
more than 40 words. Following common practice, we added the unlabeled development
and test data sets to the pool of unlabeled sentences. We initialized the IBM Model 1
translation table with uniform probabilities over word pairs that occur together in the
same sentence and trained the IBM Model 1 for 5 iterations. All HMM alignment models
492
Grac?a, Ganchev, and Taskar Learning Alignment Models with Complex Constraints
were initialized with the translation table from IBM Model 1 and uniform distortion
probabilities. We run each training procedure until the area under the precision/recall
curve measured on a development corpus stops increasing (see Figure 4 for an example
of such a curve). Using the precision/recall curve gives a broader sense of the model?s
performance than using a single point (by tuning a threshold for a particular metric). In
most cases this meant four iterations for normal EM training and two iterations using
posterior regularization. We suspect that the constraints make the space easier to search.
The convergence criterion for the projection algorithm was the normalized l2 norm
of the gradient (gradient norm divided by number of constraints) being smaller than
? (see Algorithm 1). For bijective constraints, we set ? to 0.005 and used zero slack.
For symmetric constraints, ? and slack were set to 0.001. We chose ? aggressively
and lower values did not significantly increase performance. Less aggressive settings
cause degradation of performance: For example, for En-Fr using 10k sentences, and
running four iterations of constrained EM, the area under the precision/recall curve for
the symmetric model changed from 70% with ? = 0.1 to 85% using ? = 0.001. On the
other hand, the number of iterations required to project the constraints increases for
smaller values of ?. The number of forward?backward calls for normal HMM is 40k
(one for each sentence and EM iteration), for the symmetric model using ? = 0.1 was
around 41k and using ? = 0.001 was around 26M (14 minutes to 4 hours 14 minutes
of training time, 17 times slower, for the different settings of ?). We note that better
optimization methods, such as L-BFGS, or using a warm start for the parameters at each
EM iteration (parameters from the previous iteration), or training the models online,
would potentially decrease the running time of our method.
The intent of this experimental section is to evaluate the gains from using con-
straints during learning, hence the main comparison is between HMM trained with
normal EM vs. trained with PR plus constraints. We also report results for IBM Model 4,
because it is often used as the default word alignment model, and can be used as a
reference. However, we would like to note that IBM Model 4 is a more complex model,
able to capture more structure, albeit at the cost of intractable inference. Because our
approach is orthogonal to the base model used, the constraints described here could
be applied in principle to IBM Model 4 if exact inference was efficient, hopefully
yielding similar improvements. We used a standard implementation of IBM Model
4 (Och and Ney 2003) and because changing the existing code is not trivial, we could
not use the same stopping criterion to avoid overfitting and we are not able to produce
precision/recall curves. We trained IBM Model 4 using the default configuration of the
Figure 4
Precision/Recall curves for different models using 1,000k sentences. Precision on the horizontal
axis. Left: Hansard EN-FR direction. Right: EN-PT Portuguese-English direction.
493
Computational Linguistics Volume 36, Number 3
Figure 5
Word alignment precision when the threshold is chosen to achieve IBM Model 4 recall with a
difference of ? 0.005. The average relative increase in precision (against the HMM model) is
10% for IBM Model 4, 11% for B-HMM, and 14% for S-HMM.
MOSES training script.3 This performs five iterations of IBM Model 1, five iterations of
HMM, and five iterations of IBM Model 4.
4.2 Alignment Results
In this section we present results on alignment quality. All comparisons are made using
MBR decoding because this decoding method always outperforms Viterbi decoding.4
For the models with constraints we project the posteriors at decode time (i.e., we use
q(z | x) to decode). This gives a small but consistent improvement. Figure 4 shows
precision/recall curves for the different models on the En-Fr corpus using English as
the source language (left), and on the En-Pt corpus using Portuguese as the source.
Precision/recall curves are obtained by varying the posterior threshold from 0 to 1 and
then plotting the different precision and recall values obtained.
We observe several trends from Figure 4. First, both types of constraints improve
over the HMM in terms of both precision and recall (their precision/recall curve is
always above). Second, S-HMM performs slightly better than B-HMM. IBM Model 4
is comparable with both constraints (after symmetrization). The results for all language
pairs are in Figure 5. For ease of comparison, we choose a decoding threshold for HMM
models to achieve the recall of the corresponding IBM Model 4 and report precision.
Our methods always improve over the HMM by 10% to 15%, and improve over IBM
Model 4 nine times out of 12. Comparing the constraints with each other we see that
3 www.statmt.org/moses/?n=FactoredTraining.HomePage.
4 IBM Model 4 uses Viterbi decoding as Giza++ does not support MBR decoding.
494
Grac?a, Ganchev, and Taskar Learning Alignment Models with Complex Constraints
Figure 6
Word alignment precision as a function of training data size (number of sentence pairs).
Posterior decoding threshold chosen to achieve IBM Model 4 recall in the Hansard corpus. Right:
English as source. Left: French as source.
S-HMM performs better than B-HMM in 10 out of 12 cases. Because S-HMM indirectly
enforces bijectivity and models sequential correlations on both sides, this is perhaps not
surprising.
Figure 6 shows performance as a function of training data size. As before, we decode
to achieve the recall of IBM Model 4. For small training corpora adding the constraints
provides larger improvements (20?30%) but we still achieve significant gains even with
a million parallel sentences (15%). Greater improvements for small data sizes indicate
that our approach can be especially effective for resource-poor language pairs.
4.3 Rare vs. Common Words
One of the main benefits of using the posterior regularization constraints described is
an alleviation of the garbage collector effect (Brown et al 1993a). Figure 7 breaks down
performance improvements by common versus rare words. As before, we use posterior
decoding, tuning the threshold to match IBM Model 4 recall. For common words, this
tuning maintains recall very close for all models so we do not show this in the figure. In
the top left panel of Figure 7, we see that precision of common words follows the pattern
we saw for the corpus overall: Symmetric and bijective outperform both IBM Model 4
and the baseline HMM, with symmetric slightly better than bijective. The results for
common words vary more slowly as we increase the quantity of training data than they
did for the full corpus. In the top right panel of Figure 7 we show the precision for rare
words. For the baseline HMM as well as for IBM Model 4, this is very low precisely
because of the garbage collector problem: Rare words become erroneously aligned to
untranslated words, leading to low precision. In fact the constrained models achieve
absolute precision improvements of up to 50% over the baseline. By removing these
erroneous alignments the translation table becomes more accurate, allowing higher re-
call on the full corpus. In the bottom panel of Figure 7, we observe a slightly diminished
recall for rare words. This slight drop in recall is due to moving the mass corresponding
to rare words to null.
4.4 Symmetrization
As discussed earlier, the word alignment models are asymmetric, whereas most appli-
cations require a single alignment for each sentence pair. Typically this is achieved by
a symmetrization heuristic that takes two directional alignments and produces a single
495
Computational Linguistics Volume 36, Number 3
Figure 7
Precision and Recall as a function of training data size for En-Fr by common and rare words.
Top Left: Common Precision, Top Right: Rare Precision, Bottom: Rare Recall.
alignment. For MT the most commonly used heuristic is called grow diagonal final
(Och and Ney 2003). This starts with the intersection of the sets of aligned points and
adds points around the diagonal that are in the union of the two sets of aligned points.
The alignment produced has high recall relative to the intersection and only slightly
lower recall than the union. In syntax transfer the intersection heuristic is normally
used, because one wants to have high precision links to transfer knowledge between
languages. One pitfall of these symmetrization heuristics is that they can obfuscate the
link between the original alignment and the ones used for a specific task, making errors
more difficult to analyze. Because they are heuristics tuned for a particular phrase-
based translation system, it is not clear when they will help and when they will hinder
system performance. In this work we followed a more principled approach that uses
Figure 8
Precision/recall curves for the different models after soft union symmetrization. Precision is on
the horizontal axis. Left EN-FR, Right PT-ES.
496
Grac?a, Ganchev, and Taskar Learning Alignment Models with Complex Constraints
the knowledge about the posterior distributions of each directional model. We include a
point in the final alignment if the average of the posteriors under the two models for that
point is above a threshold. This heuristic is called soft union (DeNero and Klein 2007).
Figure 8 shows the Precision/Recall curves after symmetrization for the En-Fr corpus.
The posterior regularization?trained models still performed better, but the differences
get smaller after doing the symmetrization. This should not be very surprising, because
the soft union symmetrization can be viewed as an approximation of our symmetry
constraint applied only at decode time. Applying the symmetrization to the model with
symmetry constraints does not affect performance.
4.5 Analysis
In this section we discuss some scenarios in which the constraints make the alignments
better, and some scenarios where they fail. We have already discussed the garbage
collector effect and how both models address it. Both of the constraints also bias the
model to have at most probability one in any row or column of the posterior matrix,
encouraging 1-to-1 alignments. Obviously whenever alignments are systematically not
1-to-1 , this can lead to errors (for instance the examples described in Section 2).
An example presented in Figure 9 shows the posterior marginal distributions for an
English/French sentence pair using the same notation as in Figure 1. In the top panel of
Figure 9
Posterior distributions for different models for an English to French sentence translation. Left:
EN?FR model. Right: FR? EN model. Top: Regular HMM posteriors. Middle: After applying
the bijective constraint. Bottom: After applying the symmetric constraint. Sure alignments are
squares with borders; possible alignments are squares without borders. Circle size indicates
probability value. Circle color in the middle and bottom rows indicates differences in posterior
from the top row; green = higher probability; red = lower probability.
497
Computational Linguistics Volume 36, Number 3
Figure 9, we see the baseline models, where the English word met is incorrectly being
aligned to se?ance est ouverte. This makes it impossible to recover the correct alignment
house/se?ance. Either constraint corrects this problem. On the other hand, by enforcing
a 1-to-1 mapping the correct alignment met / est ouverte is lost. Going back to the first
row (regular HMM) this alignment is correct in one direction and absent in the other
(due to the many-to-1 model restriction) but we can recover that information using the
symmetrization heuristics, since the point is present at least in one direction with high
probability mass. This is not the case for the constraint-based models that reduce the
mass of that alignment in both directions. Going back to the right panel of Figure 8, we
can see that for low values of precision the HMM model actually achieves better recall
than the constraint-based methods. There are two possible solutions to alleviate this
type of problem, both with their caveats. One solution is to model the fertility of each
word in a way similar to IBM Model 4, or more generally to model alignments of multi-
ple words. This can lead to significant computational burden, and is not guaranteed to
improve results. A more complicated model may require approximations that destroy
its performance gain, or require larger corpora to estimate its parameters. Another
option is to perform some linguistically motivated pre-processing of the language pair
to conjoin words. This of course has the disadvantage that it needs to be specific to a
language pair in order to include information such as ?English simple past is written
using a single word, so join together French passe? compose?.? An additional problem
with joining words to alleviate inter-language divergences is that it can increase data
sparsity.
5. Task-Specific Alignment Evaluation
In this section we evaluate the alignments resulting from using the proposed constraints
in two different tasks: Statistical machine translation where alignments are used to
restrict the number of possible minimal translation units; and syntax transfer, where
alignments are used to decide how to transfer dependency links.
5.1 Phrase-Based Machine Translation
We now investigate whether our alignments produce improvements in an end-to-end
phrase-based machine translation system. We use a state-of-the-art machine translation
system,5 and follow the experimental setup used for the 2008 shared task on machine
translation (ACL 2008 Third Workshop on Statistical Machine Translation). The full
pipeline consists of the following steps: (1) prepare the data (lowercase, tokenize, and
filter long sentences); (2) build language models; (3) create word alignments in each
direction; (4) symmetrize directional word alignments; (5) build phrase table; (6) tune
weights for the phrase table. For more details consult the shared task description.6 To
evaluate the quality of the produced alignments, we keep the pipeline unchanged, and
use the models described earlier to generate the word alignments in Step 3. For Step 4,
we use the soft union symmetrization heuristic. Symmetrization has almost no effect on
alignments produced by S-HMM, but we use it for uniformity in the experiments. We
tested three values of the threshold (0.2, 0.4, 0.6) which try to capture different tradeoffs
5 The open source Moses (Hoang et al 2007) toolkit from www.statmt.org/moses/.
6 www.statmt.org/wmt08/baseline.html.
498
Grac?a, Ganchev, and Taskar Learning Alignment Models with Complex Constraints
Table 2
BLEU scores for all language pairs. The best threshold was selected according to the
development set after the last MERT iteration. Bold denotes the best score.
Fr ? En En ? Fr Es ? En En ? Es Pt ? En En ? Pt
IBM M4 GDF 35.7 31.2 32.4 31.6 31.4 28.9
HMM SU 35.9 28.9 32.3 31.6 30.9 31.6
B-HMM SU 36.0 31.5 32.6 31.7 31.0 32.2
S-HMM SU 35.5 31.2 31.9 32.5 31.4 32.3
of precision vs. recall, and pick the best according to the translation performance on
development data. Table 2 summarizes the results for the different corpora. For refer-
ence we include IBM Model 4 as suggested in the task description. PR training always
outperforms EM training and outperforms IBM Model 4 in all but one experiment.
Differences in BLEU range from 0.2 to 0.9. The two constraints help to a different extent
for different corpora and translation directions, in a somewhat unpredictable manner.
In general our impression is that the connection between alignment quality and BLEU
scores is complicated, and changes are difficult to explain and justify. The number of
iterations for MERT optimization to converge varied from 2 to 28; and the best choice of
threshold on the development set did not always correspond to the best on the test set.
Contrary to conventional wisdom in the MT community, bigger phrase tables did not
always perform better. In 14 out of 18 cases, the threshold picked was 0.4 (medium size
phrase tables) and the other four times 0.2 was picked (smaller phrase tables). When
we include only high confidence alignments, more phrases are extracted but many of
these are erroneous. Potentially this leads to a poor estimate of the phrase probabilities.
See Lopez and Resnik (2006) for further discussion.
5.2 Syntax Transfer
In this section, we compare the different alignments produced with and without PR
based on how well they can be used for transfer of linguistic resources across languages.
We used the system proposed by Ganchev, Gillenwater, and Taskar (2009). This system
uses a word-aligned corpus and a parser for a resource-rich language (source language)
in order to create a parser for a resource-poor language (target language). We consider
a parse tree on the source language as a set of dependency edges to be transferred. For
each such edge, if both end points are aligned to words in the target language, then
the edge is transferred. These edges are then used as weak supervision when training
a generative or discriminative dependency parser. In order to evaluate the alignments
we computed the fraction of correctly transferred edges as a function of the average
number of edges transferred by using supervised parse trees on the target side. By
changing the threshold in MBR decoding of alignments, we can trade off accuracy of the
transferred edges vs. transferring more edges. We generated supervised parses using
the first-order model from the MST parser (McDonald, Crammer, and Pereira 2005)
trained on the Penn Treebank for English and the CoNLL X parses for Bulgarian and
Spanish. Following Ganchev, Gillenwater, and Taskar (2009), we filter alignment links
between words with incompatible POS tags. Figure 10 shows our results for transferring
from English to Bulgarian (En?Bg) and from English to Spanish (En?Es). The En?Bg
499
Computational Linguistics Volume 36, Number 3
Figure 10
Edge conservation for cross-lingual grammar induction. Left: En?Bg subtitle corpus; Right:
En?Es parliamentary proceedings. Vertical axis: percentage of transferred edges that are correct.
Horizontal axis: average number of transferred edges per sentence.
results are based on a corpus of movie subtitles (Tiedemann 2007), and are consequently
shorter sentences, whereas the En?Es results are based on a corpus of parliamentary
proceedings (Koehn 2005). We see in Figure 10 that for both domains, the models trained
using posterior regularization perform better than the baseline model trained using EM.
6. Related Work
The idea of introducing constraints over a model to better guide the learning process
has appeared before. In the context of word alignment, Deng and Byrne (2005) use a
state-duration HMM in order to model word-to-phrase translations. The fertility of each
source word is implicitly encoded in the durations of the HMM states. Without any
restrictions, likelihood prefers to always use longer phrases and the authors try to con-
trol this behavior by multiplying every transition probability by a constant ? > 1. This
encourages more transitions and hence shorter phrases. For the task of unsupervised
dependency parsing, Smith and Eisner (2006) add a constraint of the form ?the average
length of dependencies should be X? to capture the locality of syntax (at least half
of the dependencies are between adjacent words), using a scheme they call structural
annealing. They modify the model?s distribution over trees p?(y) by a penalty term
as: p
?
?(y) ? p?(y)e
(?
?
e?y length(e)), where length(e) is the surface length of edge e. The
factor ? changes from a high value to a lower one so that the preference for short edges
(hence a smaller sum) is stronger at the start of training.
These two approaches also have the goal of controlling unsupervised learning, and
the form of the modified distributions is reminiscent of the form that the projected
posteriors take. However, the approaches differ substantially from PR. Smith and Eisner
(2006) make a statement of the form ?scale the total length of edges?, which depending
on the value of ? will prefer to have more shorter/longer edges. Such statements are
not data dependent. Depending on the value of ?, for instance if ? ? 0, even if the data
is such that the model already uses too many short edges on average, this value of
? will push for more short edges. By contrast the statements we can make in PR are
of the form ?there should be more short edges than long edges?. Such a statement is
data-dependent in the sense that if the model satisfies the constraints then we do not
need to change it; if it is far from satisfying it we might need to make very dramatic
changes.
500
Grac?a, Ganchev, and Taskar Learning Alignment Models with Complex Constraints
PR is closely related to the work of Mann and McCallum (2007, 2008), who concur-
rently developed the idea of using penalties based on posterior expectations of features
to guide semi-supervised learning. They call their method generalized expectation (GE)
constraints or alternatively expectation regularization. In the original GE framework,
the posteriors of the model on unlabeled data are regularized directly. They train a
discriminative model, using conditional likelihood on labeled data and an ?expectation
regularization? penalty term on the unlabeled data:
arg max
?
Llabeled(?) ? ??E[||Ep? [f(x, z) ? b||
2
2]. (16)
Notice that there is no intermediate distribution q. For some kinds of constraints this
objective is difficult to optimize in ? and in order to improve efficiency, Bellare, Druck,
and McCallum (2009) propose interpreting the PR framework as an approximation
to the GE objective in Equation (16). They compare the two frameworks on several
data sets and find that performance is similar. Liang, Jordan, and Klein (2009) cast
the problem of incorporating partial information about latent variables into a Bayesian
framework using ?measurements,? and after several approximation steps, they arrive
at the objective we optimize.
The idea of jointly training two directional models has been explored by Liang,
Taskar, and Klein (2006), although under a very different formalization. They de-
fine a joint objective max
?1,?2
?E
[
log??p ?1 (x) + log
??p ?2 (x) + log
?
z
??p ?1 (z | x)
??p ?2 (z | x)
]
. However, the
product distribution ??p ?1 (z | x)
??p ?2 (z | x) ranges over all one-to-one alignments and
computing it is #P-complete (Liang, Taskar, and Klein 2006). They approximate this
distribution as a product of marginals: q(z) =
?
i,j
??p ?1 (zi,j | x)
??p ?2 (zi,j | x), but it is not
clear what objective the approximate procedure actually optimizes.
7. Conclusion
In this article we explored a novel learning framework, Posterior Regularization, for
incorporating rich constraints over the posterior distributions of word alignments. We
focused on the HMM word alignment model, and showed how we could incorpo-
rate complex constraints like bijectivity and symmetry while keeping the inference
in the model tractable. Using these constraints we showed consistent and significant
improvements in six different language pairs even when compared to a more complex
model such as IBM Model 4. In addition to alleviating the ?garbage collector? effect, we
show that the obtained posterior distributions better reflect the desired alignments. Both
constraints are biasing the models towards 1-to-1 alignments, which may be inappro-
priate in some situations, and we show some systematic mistakes that the constraints
introduce and suggest possible fixes.
We experimented with two different tasks that rely on word alignments, phrase-
based MT and syntax transfer. For phrase-based MT, the improved alignments lead
to a modest increase in BLEU performance. For syntax transfer, we have shown that
the number of edges of a dependency tree that can be accurately transferred from one
language to another increases as a result of improved alignments.
Our framework opens up the possibility of efficiently adding many other con-
straints that are directly applicable to word alignments, such as preferring alignments
that respect dependency tree structure, part of speech tags, or syntactic boundaries.
501
Computational Linguistics Volume 36, Number 3
Appendix A: Modified E-Step Dual Derivation
The modified E step involves a projection step that minimizes the Kullback-Leibler
divergence:
E? : arg min
q(z|x),?
KL( q(z|x) ? p?(z|x)) s.t. Eq[f(x, z)] ? bx ? ?; ||?||22 ? 2.
Assuming the set Qx = { q(z|x) : ??, Eq[f(x, z)] ? bx ? ?; ||?||22 ? 2} is non-empty, the
corresponding Lagrangian is max
?,?,?
min
q(z|x),?
L( q(z|x), ?, ?, ?, ?) with ? ? 0 and ? ? 0,
where
L( q(z|x), ?, ?, ?, ?) = KL( q(z|x) ? p?(z|x)) + ?(Eq[f(x, z)] ? bx ? ?)
+ ?(||?||22 ? 2) + ?(
?
z
q(z|x) ? 1)
?L( q(z|x), ?, ?, ?, ?)
? q(z|x) = log( q(z|x)) + 1 ? log( p?(z|x)) + ?
f(x, z) + ? = 0
=? q(z|x) = p?(z|x) exp(??
f(x, z))
e exp(?)
?L( q(z|x), ?, ?, ?, ?)
??i
= 2??i ? ?i = 0 =? ?i =
?i
2?
Plugging q(z|x) and ? in L( q(z|x), ?, ?, ?, ?) and taking the derivative with respect to ?:
?L(?, ?, ?)
?? =
?
z
p?(z|x) exp(??f(x, z))
e exp(?)
? 1 = 0 =? ? = log(
?
z p?(z|x) exp(??f(x, z))
e )
Simplifying q(z|x) = p?(z|x) exp(??
f(x,z))
Z?
where Z? =
?
z p?(z|x) exp(??f(x, z)) en-
sures that q(z|x) is properly normalized. Plugging ? into L(?, ?, ?) and taking the
derivative with respect to ?, we get:
L(?, ?) = ? log(Z?) ? bx ??
||?||22
2? +
||?||22
4? ? ?
2 (A.1)
?L(?, ?)
?? =
||?||22
2?2
? ||?||
2
2
4?2
? 2 = 0 =? ? = ||?||22 (A.2)
Replacing back into L(?, ?) we get the dual objective:
Dual E? : arg max
??0
?bx ?? log(Z?) ? ||?||2  (A.3)
Acknowledgments
J. V. Grac?a was supported by a fellowship
from Fundac?a?o para a Cie?ncia e Tecnologia
(SFRH/ BD/ 27528/ 2006) and by FCT
project CMU-PT/HuMach/0039/2008.
K. Ganchev was partially supported by
NSF ITR EIA 0205448. Ben Taskar was
partially supported by DARPA CSSG
2009 grant.
References
Bannard, Colin and Chris Callison-Burch.
2005. Paraphrasing with bilingual parallel
corpora. In ACL ?05: Proceedings of the
43rd Annual Meeting of the Association for
Computational Linguistics, pages 597?604,
Morristown, NJ.
Bellare, Kedar, Gregory Druck, and Andrew
McCallum. 2009. Alternating projections
502
Grac?a, Ganchev, and Taskar Learning Alignment Models with Complex Constraints
for learning with expectation constraints.
In Proceedings of the Twenty-Fifth Conference
Annual Conference on Uncertainty in
Artificial Intelligence, pages 43?50,
Corvallis, OR.
Bertsekas, Dimitri P. 1999. Nonlinear
Programming: 2nd Edition. Athena
Scientific, Nashua, NH.
Brown, Peter F., Stephen A. Della Pietra,
Vincent J. Della Pietra, Meredith J.
Goldsmith, Jan Hajic, Robert L. Mercer,
and Surya Mohanty. 1993a. But
dictionaries are data too. In HLT ?93:
Proceedings of the Workshop on Human
Language Technology, pages 202?205,
Morristown, NJ.
Brown, Peter F., Stephen A. Della Pietra,
Vincent J. Della Pietra, and Robert L.
Mercer. 1993b. The mathematics of
statistical machine translation: Parameter
estimation. Computational Linguistics,
19(2):263?311.
Chiang, David, Adam Lopez, Nitin
Madnani, Christof Monz, Philip Resnik,
and Michael Subotin. 2005. The Hiero
machine translation system: Extensions,
evaluation, and analysis. In Proceedings
of the Human Language Technology
Conference and Conference on Empirical
Methods in Natural Language Processing,
pages 779?786, Vancouver.
Dempster, Arthur P., Nan M. Laird, and
Donald B. Rubin. 1977. Maximum
likelihood from incomplete data via the
em algorithm. Royal Statistical Society,
Series B, 39(1):1?38.
DeNero, John and Dan Klein. 2007. Tailoring
word alignments to syntactic machine
translation. In Proceedings of the 45th
Annual Meeting of the Association of
Computational Linguistics, pages 17?24,
Prague.
Deng, Yonggang and William Byrne. 2005.
HMM word and phrase alignment for
statistical machine translation. In HLT ?05:
Proceedings of the Conference on Human
Language Technology and Empirical
Methods in Natural Language Processing,
pages 169?176, Morristown, NJ.
Association for Computational Linguistics.
Fraser, Alexander and Daniel Marcu. 2007.
Getting the structure right for word
alignment: Leaf. In Proceedings of the Joint
Conference on Empirical Methods in Natural
Language Processing and Computational
Natural Language Learning
(EMNLP-CoNLL), pages 51?60, Prague.
Galley, Michel, Mark Hopkins, Kevin Knight,
and Daniel Marcu. 2004. What?s in a
translation rule? In HLT-NAACL 2004:
Main Proceedings, pages 273?280,
Boston, MA.
Ganchev, Kuzman, Jennifer Gillenwater, and
Ben Taskar. 2009. Dependency grammar
induction via bitext projection constraints.
In ACL-IJCNLP ?09: Proceedings of the Joint
Conference of the 47th Annual Meeting
of the ACL and the 4th International Joint
Conference on Natural Language Processing
of the AFNLP: Volume 1, pages 369?377,
Morristown, NJ.
Ganchev, Kuzman, Joa?o V. Grac?a, and Ben
Taskar. 2008. Better alignments = better
translations? In Proceedings of ACL-08: HLT,
pages 986?993, Columbus, OH.
Grac?a, Joa?o V., Kuzman Ganchev, and Ben
Taskar. 2007. Expectation maximization
and posterior constraints. In J. C. Platt,
D. Koller, Y. Singer, and S. Roweis, editors,
Advances in Neural Information Processing
Systems 20. MIT Press, Cambridge, MA,
pages 569?576.
Grac?a, Joa?o V., Kuzman Ganchev, and
Ben Taskar. 2009. Postcat - posterior
constrained alignment toolkit. The Prague
Bulletin Of Mathematical Linguistics - Special
Issue: Open Source Tools for Machine
Translation, 91:27?37.
Grac?a, Joa?o V., Joana P. Pardal, Lu??sa Coheur,
and Diamantino Caseiro. 2008. Building
a golden collection of parallel
multi-language word alignment. In
Proceedings of the Sixth International
Language Resources and Evaluation
(LREC?08), Marrakech.
Hoang, Hieu, Alexandra Birch, Chris
Callison-Burch, Richard Zens, Rwth
Aachen, Alexandra Constantin, Marcello
Federico, Nicola Bertoldi, Chris Dyer,
Brooke Cowan, Wade Shen, Christine
Moran, and Ondrej Bojar. 2007. Moses:
Open source toolkit for statistical machine
translation. In Proceedings of the 45th
Annual Meeting of the Association for
Computational Linguistics Companion
Volume Proceedings of the Demo and Poster
Sessions, pages 177?180, Prague.
Hwa, Rebecca, Philip Resnik, Amy
Weinberg, Clara Cabezas, and Okan Kolak.
2005. Bootstrapping parsers via syntactic
projection across parallel texts. Natural
Language Engineering, 11:11?311.
Koehn, Philipp. 2005. Europarl: A parallel
corpus for statistical machine translation.
In Machine Translation Summit,
12?15 September, Phuket.
Koehn, Philipp, Franz Josef Och, and Daniel
Marcu. 2003. Statistical phrase-based
503
Computational Linguistics Volume 36, Number 3
translation. In Proceedings of the 2003
Conference of the North American Chapter of
the Association for Computational Linguistics
on Human Language Technology (NAACL),
pages 48?54, Morristown, NJ.
Kumar, Shankar and William Byrne. 2002.
Minimum Bayes-Risk word alignments of
bilingual texts. In Proceedings of the ACL-02
Conference on Empirical Methods in Natural
Language Processing, pages 140?147,
Philadelphia, PA.
Lambert, Patrik, Adria` De Gispert, Rafael
Banchs, and Jose? B. Marino. 2005.
Guidelines for word alignment evaluation
and manual alignment. Language Resources
and Evaluation, 39(4):267?285.
Liang, Percy, Michael I. Jordan, and Dan
Klein. 2009. Learning from measurements
in exponential families. In ICML ?09:
Proceedings of the 26th Annual International
Conference on Machine Learning,
pages 641?648, New York, NY.
Liang, Percy, Ben Taskar, and Dan Klein.
2006. Alignment by agreement. In
Proceedings of the Human Language
Technology Conference of the NAACL, Main
Conference, pages 104?111, New York, NY.
Lopez, Adam and Philip Resnik. 2006.
Word-based alignment, phrase-based
translation: Whats the link? In Proceedings
of the 7th Conference of the Association for
Machine Translation in the Americas
(AMTA): Visions for the Future of Machine
Translation, pages 90?99, Boston, MA.
Mann, G. and A. McCallum. 2007. Simple,
robust, scalable semi-supervised learning
via expectation regularization. In
Proceedings of the 24th International
Conference on Machine Learning, page 600,
Corvallis, OR.
Mann, Gideon S. and Andrew McCallum.
2008. Generalized expectation criteria
for semi-supervised learning of
conditional random fields. In Proceedings
of ACL-08: HLT, pages 870?878,
Columbus, OH.
Matusov, Evgeny, Nicola Ueffing, and
Hermann Ney. 2006. Computing
consensus translation from multiple
machine translation systems using
enhanced hypotheses alignment. In
Proceedings of the EACL, pages 33?40,
Cambridge.
McDonald, Ryan, Koby Crammer, and
Fernando Pereira. 2005. Online
large-margin training of dependency
parsers. In ACL ?05: Proceedings of the
43rd Annual Meeting of the Association for
Computational Linguistics, pages 91?98,
Morristown, NJ.
Neal, Radford M. and Geoffrey E. Hinton.
1998. A new view of the EM algorithm that
justifies incremental, sparse and other
variants. In M. I. Jordan, editor, Learning in
Graphical Models. Kluwer, Amsterdam,
pages 355?368.
Nocedal, Jorge and Stephen J. Wright.
1999. Numerical Optimization. Springer,
Berlin.
Och, Franz Josef and Hermann Ney. 2000.
Improved statistical alignment models.
In ACL ?00: Proceedings of the 38th
Annual Meeting on Association for
Computational Linguistics, pages 440?447,
Morristown, NJ.
Och, Franz Josef and Hermann Ney. 2003.
A systematic comparison of various
statistical alignment models. Computational
Linguistics, 29(1):19?51.
Smith, Noah A. and Jason Eisner. 2006.
Annealing structural bias in multilingual
weighted grammar induction. In ACL-44:
Proceedings of the 21st International
Conference on Computational Linguistics and
the 44th Annual Meeting of the Association for
Computational Linguistics, pages 569?576,
Morristown, NJ.
Snyder, Benjamin and Regina Barzilay.
2008. Unsupervised multilingual learning
for morphological segmentation. In
Proceedings of ACL-08: HLT, pages 737?745,
Columbus, OH.
Tiedemann, Jo?rg. 2007. Building a
multilingual parallel subtitle corpus. In
Proceedings of the 17th Conference on
Computational Linguistics in the Netherlands
(CLIN 17), Leuven.
Vogel, Stephan, Hermann Ney, and
Christoph Tillmann. 1996. Hmm-based
word alignment in statistical translation.
In Proceedings of the 16th Conference on
Computational Linguistics, pages 836?841,
Morristown, NJ.
Yarowsky, David and Grace Ngai. 2001.
Inducing multilingual POS taggers and NP
bracketers via robust projection across
aligned corpora. In Proceedings of the North
American Chapter Of The Association For
Computational Linguistics, pages 1?8,
Morristown, NJ.
504
Proceedings of the ACL 2010 Conference Short Papers, pages 194?199,
Uppsala, Sweden, 11-16 July 2010. c?2010 Association for Computational Linguistics
Sparsity in Dependency Grammar Induction
Jennifer Gillenwater and Kuzman Ganchev
University of Pennsylvania
Philadelphia, PA, USA
{jengi,kuzman}@cis.upenn.edu
Jo?o Gra?a
L2F INESC-ID
Lisboa, Portugal
joao.graca@l2f.inesc-id.pt
Fernando Pereira
Google Inc.
Mountain View, CA, USA
pereira@google.com
Ben Taskar
University of Pennsylvania
Philadelphia, PA, USA
taskar@cis.upenn.edu
Abstract
A strong inductive bias is essential in un-
supervised grammar induction. We ex-
plore a particular sparsity bias in de-
pendency grammars that encourages a
small number of unique dependency
types. Specifically, we investigate
sparsity-inducing penalties on the poste-
rior distributions of parent-child POS tag
pairs in the posterior regularization (PR)
framework of Gra?a et al (2007). In ex-
periments with 12 languages, we achieve
substantial gains over the standard expec-
tation maximization (EM) baseline, with
average improvement in attachment ac-
curacy of 6.3%. Further, our method
outperforms models based on a standard
Bayesian sparsity-inducing prior by an av-
erage of 4.9%. On English in particular,
we show that our approach improves on
several other state-of-the-art techniques.
1 Introduction
We investigate an unsupervised learning method
for dependency parsing models that imposes spar-
sity biases on the dependency types. We assume
a corpus annotated with POS tags, where the task
is to induce a dependency model from the tags for
corpus sentences. In this setting, the type of a de-
pendency is defined as a pair: tag of the dependent
(also known as the child), and tag of the head (also
known as the parent). Given that POS tags are de-
signed to convey information about grammatical
relations, it is reasonable to assume that only some
of the possible dependency types will be realized
for a given language. For instance, in English it
is ungrammatical for nouns to dominate verbs, ad-
jectives to dominate adverbs, and determiners to
dominate almost any part of speech. Thus, the re-
alized dependency types should be a sparse subset
of all possible types.
Previous work in unsupervised grammar induc-
tion has tried to achieve sparsity through priors.
Liang et al (2007), Finkel et al (2007) and John-
son et al (2007) proposed hierarchical Dirichlet
process priors. Cohen et al (2008) experimented
with a discounting Dirichlet prior, which encour-
ages a standard dependency parsing model (see
Section 2) to limit the number of dependent types
for each head type.
Our experiments show a more effective sparsity
pattern is one that limits the total number of unique
head-dependent tag pairs. This kind of sparsity
bias avoids inducing competition between depen-
dent types for each head type. We can achieve the
desired bias with a constraint on model posteri-
ors during learning, using the posterior regulariza-
tion (PR) framework (Gra?a et al, 2007). Specifi-
cally, to implement PR we augment the maximum
marginal likelihood objective of the dependency
model with a term that penalizes head-dependent
tag distributions that are too permissive.
Although not focused on sparsity, several other
studies use soft parameter sharing to couple dif-
ferent types of dependencies. To this end, Cohen
et al (2008) and Cohen and Smith (2009) inves-
tigated logistic normal priors, and Headden III et
al. (2009) used a backoff scheme. We compare to
their results in Section 5.
The remainder of this paper is organized as fol-
194
lows. Section 2 and 3 review the models and sev-
eral previous approaches for learning them. Sec-
tion 4 describes learning with PR. Section 5 de-
scribes experiments across 12 languages and Sec-
tion 6 analyzes the results. For additional details
on this work see Gillenwater et al (2010).
2 Parsing Model
The models we use are based on the generative de-
pendency model with valence (DMV) (Klein and
Manning, 2004). For a sentence with tags x, the
root POS r(x) is generated first. Then the model
decides whether to generate a right dependent con-
ditioned on the POS of the root and whether other
right dependents have already been generated for
this head. Upon deciding to generate a right de-
pendent, the POS of the dependent is selected by
conditioning on the head POS and the direction-
ality. After stopping on the right, the root gener-
ates left dependents using the mirror reversal of
this process. Once the root has generated all its
dependents, the dependents generate their own de-
pendents in the same manner.
2.1 Model Extensions
For better comparison with previous work we
implemented three model extensions, borrowed
from Headden III et al (2009). The first exten-
sion alters the stopping probability by condition-
ing it not only on whether there are any depen-
dents in a particular direction already, but also on
how many such dependents there are. When we
talk about models with maximum stop valency Vs
= S, this means it distinguishes S different cases:
0, 1, . . . , S?2, and? S?1 dependents in a given
direction. The basic DMV has Vs = 2.
The second model extension we implement is
analogous to the first, but applies to dependent tag
probabilities instead of stop probabilities. Again,
we expand the conditioning such that the model
considers how many other dependents were al-
ready generated in the same direction. When we
talk about a model with maximum child valency
Vc = C, this means we distinguish C different
cases. The basic DMV has Vc = 1. Since this
extension to the dependent probabilities dramati-
cally increases model complexity, the third model
extension we implement is to add a backoff for the
dependent probabilities that does not condition on
the identity of the parent POS (see Equation 2).
More formally, under the extended DMV the
probability of a sentence with POS tags x and de-
pendency tree y is given by:
p?(x,y) = proot(r(x))?
Y
y?y
pstop(false | yp, yd, yvs)pchild(yc | yp, yd, yvc)?
Y
x?x
pstop(true | x, left, xvl) pstop(true | x, right, xvr )
(1)
where y is the dependency of yc on head yp in di-
rection yd, and yvc , yvs , xvr , and xvl indicate va-
lence. For the third model extension, the backoff
to a probability not dependent on parent POS can
be formally expressed as:
?pchild(yc | yp, yd, yvc) + (1? ?)pchild(yc | yd, yvc) (2)
for ? ? [0, 1]. We fix ? = 1/3, which is a crude
approximation to the value learned by Headden III
et al (2009).
3 Previous Learning Approaches
In our experiments, we compare PR learning
to standard expectation maximization (EM) and
to Bayesian learning with a sparsity-inducing
prior. The EM algorithm optimizes marginal like-
lihood L(?) = log
?
Y p?(X,Y), where X =
{x1, . . . ,xn} denotes the entire unlabeled corpus
and Y = {y1, . . . ,yn} denotes a set of corre-
sponding parses for each sentence. Neal and Hin-
ton (1998) view EM as block coordinate ascent on
a function that lower-bounds L(?). Starting from
an initial parameter estimate ?0, the algorithm it-
erates two steps:
E : qt+1 = argmin
q
KL(q(Y) ? p?t(Y | X)) (3)
M : ?t+1 = argmax
?
Eqt+1 [log p?(X,Y)] (4)
Note that the E-step just sets qt+1(Y) =
p?t(Y|X), since it is an unconstrained minimiza-
tion of a KL-divergence. The PR method we
present modifies the E-step by adding constraints.
Besides EM, we also compare to learning with
several Bayesian priors that have been applied to
the DMV. One such prior is the Dirichlet, whose
hyperparameter we will denote by ?. For ? < 0.5,
this prior encourages parameter sparsity. Cohen
et al (2008) use this method with ? = 0.25 for
training the DMV and achieve improvements over
basic EM. In this paper we will refer to our own
implementation of the Dirichlet prior as the ?dis-
counting Dirichlet? (DD) method. In addition to
195
the Dirichlet, other types of priors have been ap-
plied, in particular logistic normal priors (LN) and
shared logistic normal priors (SLN) (Cohen et al,
2008; Cohen and Smith, 2009). LN and SLN aim
to tie parameters together. Essentially, this has a
similar goal to sparsity-inducing methods in that it
posits a more concise explanation for the grammar
of a language. Headden III et al (2009) also im-
plement a sort of parameter tying for the E-DMV
through a learning a backoff distribution on child
probabilities. We compare against results from all
these methods.
4 Learning with Sparse Posteriors
We would like to penalize models that predict a
large number of distinct dependency types. To en-
force this penalty, we use the posterior regular-
ization (PR) framework (Gra?a et al, 2007). PR
is closely related to generalized expectation con-
straints (Mann and McCallum, 2007; Mann and
McCallum, 2008; Bellare et al, 2009), and is also
indirectly related to a Bayesian view of learning
with constraints on posteriors (Liang et al, 2009).
The PR framework uses constraints on posterior
expectations to guide parameter estimation. Here,
PR allows a natural and tractable representation of
sparsity constraints based on edge type counts that
cannot easily be encoded in model parameters. We
use a version of PR where the desired bias is a
penalty on the log likelihood (see Ganchev et al
(2010) for more details). For a distribution p?, we
define a penalty as the (generic) ?-norm of expec-
tations of some features ?:
||Ep? [?(X,Y)]||? (5)
For computational tractability, rather than penaliz-
ing the model?s posteriors directly, we use an aux-
iliary distribution q, and penalize the marginal log-
likelihood of a model by the KL-divergence of p?
from q, plus the penalty term with respect to q.
For a fixed set of model parameters ? the full PR
penalty term is:
min
q
KL(q(Y) ? p?(Y|X)) + ? ||Eq[?(X,Y)]||? (6)
where ? is the strength of the regularization. PR
seeks to maximize L(?) minus this penalty term.
The resulting objective can be optimized by a vari-
ant of the EM (Dempster et al, 1977) algorithm
used to optimize L(?).
4.1 `1/`? Regularization
We now define precisely how to count dependency
types. For each child tag c, let i range over an enu-
meration of all occurrences of c in the corpus, and
let p be another tag. Let the indicator ?cpi(X,Y)
have value 1 if p is the parent tag of the ith occur-
rence of c, and value 0 otherwise. The number of
unique dependency types is then:
X
cp
max
i
?cpi(X,Y) (7)
Note there is an asymmetry in this count: occur-
rences of child type c are enumerated with i, but
all occurrences of parent type p are or-ed in ?cpi.
That is, ?cpi = 1 if any occurrence of p is the par-
ent of the ith occurrence of c. We will refer to PR
training with this constraint as PR-AS. Instead of
counting pairs of a child token and a parent type,
we can alternatively count pairs of a child token
and a parent token by letting p range over all to-
kens rather than types. Then each potential depen-
dency corresponds to a different indicator ?cpij ,
and the penalty is symmetric with respect to par-
ents and children. We will refer to PR training
with this constraint as PR-S. Both approaches per-
form very well, so we report results for both.
Equation 7 can be viewed as a mixed-norm
penalty on the features ?cpi or ?cpij : the sum cor-
responds to an `1 norm and the max to an `?
norm. Thus, the quantity we want to minimize
fits precisely into the PR penalty framework. For-
mally, to optimize the PR objective, we complete
the following E-step:
argmin
q
KL(q(Y)||p?(Y|X)) + ?
X
cp
max
i
Eq[?(X,Y)],
(8)
which can equivalently be written as:
min
q(Y),?cp
KL(q(Y) ? p?(Y|X)) + ?
X
cp
?cp
s. t. ?cp ? Eq[?(X,Y)]
(9)
where ?cp corresponds to the maximum expecta-
tion of ? over all instances of c and p. Note that
the projection problem can be solved efficiently in
the dual (Ganchev et al, 2010).
5 Experiments
We evaluate on 12 languages. Following the ex-
ample of Smith and Eisner (2006), we strip punc-
tuation from the sentences and keep only sen-
tences of length ? 10. For simplicity, for all mod-
els we use the ?harmonic? initializer from Klein
196
Model EM PR Type ?
DMV 45.8 62.1 PR-S 140
2-1 45.1 62.7 PR-S 100
2-2 54.4 62.9 PR-S 80
3-3 55.3 64.3 PR-S 140
4-4 55.1 64.4 PR-AS 140
Table 1: Attachment accuracy results. Column 1: Vc-
Vs used for the E-DMV models. Column 3: Best PR re-
sult for each model, which is chosen by applying each of
the two types of constraints (PR-S and PR-AS) and trying
? ? {80, 100, 120, 140, 160, 180}. Columns 4 & 5: Con-
straint type and ? that produced the values in column 3.
and Manning (2004), which we refer to as K&M.
We always train for 100 iterations and evaluate
on the test set using Viterbi parses. Before eval-
uating, we smooth the resulting models by adding
e?10 to each learned parameter, merely to remove
the chance of zero probabilities for unseen events.
(We did not tune this as it should make very little
difference for final parses.) We score models by
their attachment accuracy ? the fraction of words
assigned the correct parent.
5.1 Results on English
We start by comparing English performance for
EM, PR, and DD. To find ? for DD we searched
over five values: {0.01, 0.1, 0.25, 1}. We found
0.25 to be the best setting for the DMV, the same
as found by Cohen et al (2008). DD achieves ac-
curacy 46.4% with this ?. For the E-DMV we
tested four model complexities with valencies Vc-
Vs of 2-1, 2-2, 3-3, and 4-4. DD?s best accuracy
was 53.6% with the 4-4 model at ? = 0.1. A
comparison between EM and PR is shown in Ta-
ble 1. PR-S generally performs better than the PR-
AS for English. Comparing PR-S to EM, we also
found PR-S is always better, independent of the
particular ?, with improvements ranging from 2%
to 17%. Note that in this work we do not perform
the PR projection at test time; we found it detri-
mental, probably due to a need to set the (corpus-
size-dependent) ? differently for the test set. We
also note that development likelihood and the best
setting for ? are not well-correlated, which un-
fortunately makes it hard to pick these parameters
without some supervision.
5.2 Comparison with Previous Work
In this section we compare to previously published
unsupervised dependency parsing results for En-
glish. It might be argued that the comparison is
unfair since we do supervised selection of model
Learning Method Accuracy
? 10 ? 20 all
PR-S (? = 140) 62.1 53.8 49.1
LN families 59.3 45.1 39.0
SLN TieV & N 61.3 47.4 41.4
PR-AS (? = 140) 64.4 55.2 50.5
DD (? = 1, ? learned) 65.0 (?5.7)
Table 2: Comparison with previous published results. Rows
2 and 3 are taken from Cohen et al (2008) and Cohen and
Smith (2009), and row 5 from Headden III et al (2009).
complexity and regularization strength. However,
we feel the comparison is not so unfair as we per-
form only a very limited search of the model-?
space. Specifically, the only values of ? we search
over are {80, 100, 120, 140, 160, 180}.
First, we consider the top three entries in Ta-
ble 2, which are for the basic DMV. The first en-
try was generated using our implementation of
PR-S. The second two entries are logistic nor-
mal and shared logistic normal parameter tying re-
sults (Cohen et al, 2008; Cohen and Smith, 2009).
The PR-S result is the clear winner, especially as
length of test sentences increases. For the bot-
tom two entries in the table, which are for the E-
DMV, the last entry is best, corresponding to us-
ing a DD prior with ? = 1 (non-sparsifying), but
with a special ?random pools? initialization and a
learned weight ? for the child backoff probabil-
ity. The result for PR-AS is well within the vari-
ance range of this last entry, and thus we conjec-
ture that combining PR-AS with random pools ini-
tialization and learned ? would likely produce the
best-performing model of all.
5.3 Results on Other Languages
Here we describe experiments on 11 additional
languages. For each we set ? and model complex-
ity (DMV versus one of the four E-DMV exper-
imented with previously) based on the best con-
figuration found for English. This likely will not
result in the ideal parameters for all languages, but
provides a realistic test setting: a user has avail-
able a labeled corpus in one language, and would
like to induce grammars for many other languages.
Table 3 shows the performance for all models and
training procedures. We see that the sparsifying
methods tend to improve over EM most of the
time. For the basic DMV, average improvements
are 1.6% for DD, 6.0% for PR-S, and 7.5% for
PR-AS. PR-AS beats PR-S in 8 out of 12 cases,
197
Bg Cz De Dk En Es Jp Nl Pt Se Si Tr
DMV Model
EM 37.8 29.6 35.7 47.2 45.8 40.3 52.8 37.1 35.7 39.4 42.3 46.8
DD 0.25 39.3 30.0 38.6 43.1 46.4 47.5 57.8 35.1 38.7 40.2 48.8 43.8
PR-S 140 53.7 31.5 39.6 44.0 62.1 61.1 58.8 31.0 47.0 42.2 39.9 51.4
PR-AS 140 54.0 32.0 39.6 42.4 61.9 62.4 60.2 37.9 47.8 38.7 50.3 53.4
Extended Model
EM (3,3) 41.7 48.9 40.1 46.4 55.3 44.3 48.5 47.5 35.9 48.6 47.5 46.2
DD 0.1 (4,4) 47.6 48.5 42.0 44.4 53.6 48.9 57.6 45.2 48.3 47.6 35.6 48.9
PR-S 140 (3,3) 59.0 54.7 47.4 45.8 64.3 57.9 60.8 33.9 54.3 45.6 49.1 56.3
PR-AS 140 (4,4) 59.8 54.6 45.7 46.6 64.4 57.9 59.4 38.8 49.5 41.4 51.2 56.9
Table 3: Attachment accuracy results. The parameters used are the best settings found for English. Values for hyperparameters
(? or ?) are given after the method name. For the extended model (Vc, Vs) are indicated in parentheses. En is the English Penn
Treebank (Marcus et al, 1993) and the other 11 languages are from the CoNLL X shared task: Bulgarian [Bg] (Simov et al,
2002), Czech [Cz] (Bohomov? et al, 2001), German [De] (Brants et al, 2002), Danish [Dk] (Kromann et al, 2003), Spanish
[Es] (Civit and Mart?, 2004), Japanese [Jp] (Kawata and Bartels, 2000), Dutch [Nl] (Van der Beek et al, 2002), Portuguese
[Pt] (Afonso et al, 2002), Swedish [Se] (Nilsson et al, 2005), Slovene [Sl] (D?eroski et al, 2006), and Turkish [Tr] (Oflazer et
al., 2003).
Unad
papeleranc esvs und
objetonc civilizadoaq
Unad
papeleranc esvs und
objetonc civilizadoaq
1.00
1.00 1.000.49
0.51
1.00
0.57
0.43
Unad
papeleranc esvs und
objetonc civilizadoaq
1.00 0.83 0.75 0.990.92
0.35
0.48
Figure 1: Posterior edge probabilities for an example sen-
tence from the Spanish test corpus. At the top are the gold
dependencies, the middle are EM posteriors, and bottom are
PR posteriors. Green indicates correct dependencies and red
indicates incorrect dependencies. The numbers on the edges
are the values of the posterior probabilities.
though the average increase is only 1.5%. PR-S
is also better than DD for 10 out of 12 languages.
If we instead consider these methods for the E-
DMV, DD performs worse, just 1.4% better than
the E-DMV EM, while both PR-S and PR-AS con-
tinue to show substantial average improvements
over EM, 6.5% and 6.3%, respectively.
6 Analysis
One common EM error that PR fixes in many lan-
guages is the directionality of the noun-determiner
relation. Figure 1 shows an example of a Span-
ish sentence where PR significantly outperforms
EM because of this. Sentences such as ?Lleva
tiempo entenderlos? which has tags ?main-verb
common-noun main-verb? (no determiner tag)
provide an explanation for PR?s improvement?
when PR sees that sometimes nouns can appear
without determiners but that the opposite situation
does not occur, it shifts the model parameters to
make nouns the parent of determiners instead of
the reverse. Then it does not have to pay the cost
of assigning a parent with a new tag to cover each
noun that doesn?t come with a determiner.
7 Conclusion
In this paper we presented a new method for unsu-
pervised learning of dependency parsers. In con-
trast to previous approaches that constrain model
parameters, we constrain model posteriors. Our
approach consistently outperforms the standard
EM algorithm and a discounting Dirichlet prior.
We have several ideas for further improving our
constraints, such as: taking into account the direc-
tionality of the edges, using different regulariza-
tion strengths for the root probabilities than for the
child probabilities, and working directly on word
types rather than on POS tags. In the future, we
would also like to try applying similar constraints
to the more complex task of joint induction of POS
tags and dependency parses.
Acknowledgments
J. Gillenwater was supported by NSF-IGERT
0504487. K. Ganchev was supported by
ARO MURI SUBTLE W911NF-07-1-0216.
J. Gra?a was supported by FCT fellowship
SFRH/BD/27528/2006 and by FCT project CMU-
PT/HuMach/0039/2008. B. Taskar was partly
supported by DARPA CSSG and ONR Young
Investigator Award N000141010746.
198
References
S. Afonso, E. Bick, R. Haber, and D. Santos. 2002.
Floresta Sinta(c)tica: a treebank for Portuguese. In
Proc. LREC.
K. Bellare, G. Druck, and A. McCallum. 2009. Al-
ternating projections for learning with expectation
constraints. In Proc. UAI.
A. Bohomov?, J. Hajic, E. Hajicova, and B. Hladka.
2001. The prague dependency treebank: Three-level
annotation scenario. In Anne Abeill?, editor, Tree-
banks: Building and Using Syntactically Annotated
Corpora.
S. Brants, S. Dipper, S. Hansen, W. Lezius, and
G. Smith. 2002. The TIGER treebank. In Proc.
Workshop on Treebanks and Linguistic Theories.
M. Civit and M.A. Mart?. 2004. Building cast3lb: A
Spanish Treebank. Research on Language & Com-
putation.
S.B. Cohen and N.A. Smith. 2009. The shared logistic
normal distribution for grammar induction. In Proc.
NAACL.
S.B. Cohen, K. Gimpel, and N.A. Smith. 2008. Lo-
gistic normal priors for unsupervised probabilistic
grammar induction. In Proc. NIPS.
A.P. Dempster, N.M. Laird, and D.B. Rubin. 1977.
Maximum likelihood from incomplete data via the
EM algorithm. Journal of the Royal Statistical So-
ciety, 39(1):1?38.
S. D?eroski, T. Erjavec, N. Ledinek, P. Pajas,
Z. ?abokrtsky, and A. ?ele. 2006. Towards a
Slovene dependency treebank. In Proc. LREC.
J. Finkel, T. Grenager, and C. Manning. 2007. The
infinite tree. In Proc. ACL.
K. Ganchev, J. Gra?a, J. Gillenwater, and B. Taskar.
2010. Posterior regularization for structured latent
variable models. Journal of Machine Learning Re-
search.
J. Gillenwater, K. Ganchev, J. Gra?a, F. Pereira, and
B. Taskar. 2010. Posterior sparsity in unsupervised
dependency parsing. Technical report, MS-CIS-10-
19, University of Pennsylvania.
J. Gra?a, K. Ganchev, and B. Taskar. 2007. Expec-
tation maximization and posterior constraints. In
Proc. NIPS.
W.P. Headden III, M. Johnson, and D. McClosky.
2009. Improving unsupervised dependency pars-
ing with richer contexts and smoothing. In Proc.
NAACL.
M. Johnson, T.L. Griffiths, and S. Goldwater. 2007.
Adaptor grammars: A framework for specifying
compositional nonparametric Bayesian models. In
Proc. NIPS.
Y. Kawata and J. Bartels. 2000. Stylebook for the
Japanese Treebank in VERBMOBIL. Technical re-
port, Eberhard-Karls-Universitat Tubingen.
D. Klein and C. Manning. 2004. Corpus-based induc-
tion of syntactic structure: Models of dependency
and constituency. In Proc. ACL.
M.T. Kromann, L. Mikkelsen, and S.K. Lynge. 2003.
Danish Dependency Treebank. In Proc. TLT.
P. Liang, S. Petrov, M.I. Jordan, and D. Klein. 2007.
The infinite PCFG using hierarchical Dirichlet pro-
cesses. In Proc. EMNLP.
P. Liang, M.I. Jordan, and D. Klein. 2009. Learn-
ing from measurements in exponential families. In
Proc. ICML.
G. Mann and A. McCallum. 2007. Simple, robust,
scalable semi-supervised learning via expectation
regularization. In Proc. ICML.
G. Mann and A. McCallum. 2008. Generalized expec-
tation criteria for semi-supervised learning of condi-
tional random fields. In Proc. ACL.
M. Marcus, M. Marcinkiewicz, and B. Santorini.
1993. Building a large annotated corpus of En-
glish: The Penn Treebank. Computational Linguis-
tics, 19(2):313?330.
R. Neal and G. Hinton. 1998. A new view of the EM
algorithm that justifies incremental, sparse and other
variants. In M. I. Jordan, editor, Learning in Graph-
ical Models, pages 355?368. MIT Press.
J. Nilsson, J. Hall, and J. Nivre. 2005. MAMBA meets
TIGER: Reconstructing a Swedish treebank from
antiquity. NODALIDA Special Session on Tree-
banks.
K. Oflazer, B. Say, D.Z. Hakkani-T?r, and G. T?r.
2003. Building a Turkish treebank. Treebanks:
Building and Using Parsed Corpora.
K. Simov, P. Osenova, M. Slavcheva, S. Kolkovska,
E. Balabanova, D. Doikoff, K. Ivanova, A. Simov,
E. Simov, and M. Kouylekov. 2002. Building a lin-
guistically interpreted corpus of bulgarian: the bul-
treebank. In Proc. LREC.
N. Smith and J. Eisner. 2006. Annealing structural
bias in multilingual weighted grammar induction. In
Proc. ACL.
L. Van der Beek, G. Bouma, R. Malouf, and G. Van No-
ord. 2002. The Alpino dependency treebank. Lan-
guage and Computers.
199
Proceedings of the Seventeenth Conference on Computational Natural Language Learning, pages 38?46,
Sofia, Bulgaria, August 8-9 2013. c?2013 Association for Computational Linguistics
Graph-Based Posterior Regularization
for Semi-Supervised Structured Prediction
Luheng He Jennifer Gillenwater
Computer and Information Science
University of Pennsylvania
{luhe,jengi}@cis.upenn.edu
Ben Taskar
Computer Science and Engineering
University of Washington
taskar@cs.washington.edu
Abstract
We present a flexible formulation of semi-
supervised learning for structured mod-
els, which seamlessly incorporates graph-
based and more general supervision by ex-
tending the posterior regularization (PR)
framework. Our extension allows for any
regularizer that is a convex, differentiable
function of the appropriate marginals. We
show that surprisingly, non-linearity of
such regularization does not increase the
complexity of learning, provided we use
multiplicative updates of the structured ex-
ponentiated gradient algorithm. We il-
lustrate the extended framework by learn-
ing conditional random fields (CRFs) with
quadratic penalties arising from a graph
Laplacian. On sequential prediction tasks
of handwriting recognition and part-of-
speech (POS) tagging, our method makes
significant gains over strong baselines.
1 Introduction
Recent success of graph-based semi-supervised
learning builds on access to plentiful unsupervised
data and accurate similarity measures between
data examples (Zhu et al, 2003; Joachims, 2003;
Belkin et al, 2005; Zhu and Lafferty, 2005; Al-
tun et al, 2005; Zhu, 2005; Chapelle et al, 2006;
Subramanya and Bilmes, 2009; Subramanya et
al., 2010; Das and Petrov, 2011). Many ap-
proaches, such as Joachims (2003) and Subra-
manya and Bilmes (2009) use graph-based learn-
ing in the transductive setting, where unlabeled ex-
amples are classified without learning a parametric
predictive model. While predicted labels can then
be leveraged to learn such a model (e.g. a CRF),
this pipelined approach misses out on the benefits
of modeling sequential correlations during graph
propagation. In this work we seek to better inte-
grate graph propagation with estimation of a struc-
tured, parametric predictive model.
To do so, we build on the posterior regulariza-
tion (PR) framework of Ganchev et al (2010). PR
is a principled means of providing weak super-
vision during structured model estimation. More
concretely, PR introduces a penalty whenever the
model?s posteriors over latent variables contra-
dict the specified weak supervision. Ganchev
et al (2010) show how to efficiently optimize a
likelihood-plus-posterior-penalty type objective in
the case where the penalty is linear in the model?s
marginals. Yet, there are many forms of supervi-
sion that cannot be expressed as a linear function
of marginals. For example, graph Laplacian regu-
larization. In this work, we extend PR to allow for
penalties expressed as any convex, differentiable
function of the marginals and derive an efficient
optimization method for such penalties.
In our experiments, we explore graph Lapla-
cian posterior regularizers for two applications:
handwriting recognition and POS tagging. The
methods of Altun et al (2005), Subramanya et al
(2010), and Das and Petrov (2011) are the most
closely related to this work. Altun et al (2005)
describes coupling a graph regularizer with a max-
margin objective for pitch accent prediction and
handwriting recognition tasks. Their method suf-
fers from scalability issues though; it relies on op-
timization in the dual, which requires inversion of
a matrix whose dimension grows with graph size.
The more recent work of Subramanya et al
(2010) tackles the POS tagging task and pro-
vides a more scalable method. Their method
is a multi-step procedure that iterates two main
steps, graph propagation and likelihood optimiza-
tion, until convergence. Actually computing the
optimum for the graph propagation step would re-
quire a matrix inversion similar to that used by Al-
tun et al (2005), but they skirt this issue by using
an heuristic update rule. Unfortunately though, no
38
guarantees for the quality of this update are es-
tablished. Das and Petrov (2011) proceed very
similarly, adapting the iterative procedure to in-
clude supervision from bi-text data, but applying
the same heuristic update rule.
The work we present here similarly avoids the
complexity of a large matrix inversion and iter-
ates steps related to graph propagation and likeli-
hood optimization. But in contrast to Subramanya
et al (2010) and Das and Petrov (2011) it comes
with guarantees for the optimality of each step and
convergence of the overall procedure. Further, our
approach is based on optimizing a joint objective,
which affords easier analysis and extensions us-
ing other constraints or optimization methods. The
key enabling insight is a surprising factorization of
the non-linear regularizer, which can be exploited
using multiplicative updates.
2 Posterior regularization
We focus on the semi-supervised setting, showing
how to extend the discriminative, penalty-based
version of PR for a linear chain CRF. Our results
apply more generally though to the unsupervised
setting, the constraint-based versions of PR, and
other graphical models.
In the standard semi-supervised setting we are
given n data instances, {x1, . . . ,xn}, and labels
{y1, . . . ,yl} for the first l  n instances. For
simplicity of notation, we?ll assume each xi has
T components. Modeling this data with a linear
chain CRF, the standard conditional log-likelihood
objective with a Gaussian prior (variance? ?2) is:
L(?) =
l?
i=1
log p?(yi | xi)?
||?||22
2?2 . (1)
Note that this discriminative objective does not at-
tempt to leverage the unlabeled data. Since p? de-
composes according to the independence assump-
tions of a linear chain CRF, it can be expressed as:
p?(y | x) =
exp
[?T
t=1 ?>f(yt, yt?1,x)
]
Zp(x)
(2)
where the Zp(x) is a normalizer:
Zp(x) =
?
y?
exp
[ T?
t=1
?>f(y?t, y?t?1,x)
]
(3)
and the f are arbitrary feature functions. We as-
sume f(y1, y0,x) receives a special ?start? marker
for y0. In what follows, we refer to functions
over the (yt, yt?1,x) as local factors, or p-factors;
p?(y | x) decomposes as a product of p-factors.
Given this decomposition, L and its gradient
with respect to ? can be efficiently computed using
the forward-backward algorithm for linear chains.
This amounts to computing posterior marginals
for each p-factor (yt, yt?1,x). Following the gra-
dient suffices to find the global optimum of L,
since likelihood is concave, and the Gaussian prior
makes it strictly concave.
Penalty-based posterior regularization (PR)
modifies the likelihood objective by adding a
?penalty? term expressing prior knowledge about
the posteriors (Ganchev et al, 2010). To allow for
more efficient optimization, penalty terms are im-
posed on an auxiliary joint distribution q over the
labels instead of directly on p?. Agreement be-
tween q and p? is encouraged by a KL term:
KL(q ? p?) =
n?
i=1
KL(q(Y | xi) ? p?(Y | xi))
where Y is a random variable that can take on any
possible labeling y, and q(Y |xi) is a an arbitrary
distribution over Y for each i1. The penalty term
itself is restricted to be an essentially linear func-
tion of the p-factor marginals of q(Y | xi). To
compactly express this, we first define some no-
tation. Let mi denote the p-factor marginals of
q(Y | xi). For first-order linear chain models,
if K is the total number of labels a y variable
can take on, then mi contains the marginals for
t ? {1, . . . , T} and all K2 possible (yt, yt?1) la-
bel pairs. That is, mi is a length O(TK2) vector
with entries:
mit,k,j =
?
y
1(yt = k, yt?1 = j)q(y | xi) .
(4)
Stacking all these mi, we let m represent the
O(nTK2) vector [m1, . . . ,mn]. We further de-
fine a matrix A of constraint features. The product
Am is then the expectation of these features under
q. Finally we have, with a vector b of limits, the
following expression for the penalty term:
hlin(m) = ||max (Am? b,0)||? (5)
where ||?||? denotes an arbitrary norm. This ex-
pression will be non-zero if the expected value of
1We use a notation that is slightly different than, but
equivalent to, that of prior work, in order to facilitate our ex-
tensions later.
39
Am is larger than the limit b. The full posterior
regularizer is then:
R(?, q) = KL(q ? p?) + ?hlin(m) , (6)
where ? is a hyperparameter that controls the
strength of the second term.
Running example: Consider the task of part-
of-speech (POS) tagging, where the y are tags
and the x are words. To encourage every sen-
tence to contain at least one verb, we can pe-
nalize if the expected number of verbs under
the q distribution is less than 1. Specifically,
if ?verb? is represented by tag number v, for
sentence i we penalize unless:
1 ?
T?
t=1
K?
yt?1=1
mit,v,yt?1 . (7)
In the notation of Equation (5), these penal-
ties correspond to: an n-row A matrix, where
row i has?1?s to select exactly the portion of
m from Equation (7), and a limit b = ?1.
We briefly note here that generalized expec-
tation (Mann and McCallum, 2007; Mann and
McCallum, 2008) can be used to impose similar
penalties, but without the auxiliary q distribution.
Unfortunately though, this means the expectation
of the A features is with respect to p?, so comput-
ing the gradient requires the covariance between
the constraint features in A and the model features
f , under ?. For a linear chain CRF, this means the
run time of forward-backward is squared, although
some optimizations are possible. PR?s use of the
auxiliary q allows us to optimize more efficiently
by splitting the problem into easier blocks.
The new objective that combines likelihood
with the PR penalty is: J (?, q) = L(?) ?
R(?, q). While optimizing L(?) is easy, finding
max?,q J (?, q) is NP-hard even for the simplest
models. To optimize J , Ganchev et al (2010)
employ an expectation maximization (EM) based
method. At iteration t + 1, the algorithm updates
q and ? as follows:
E : qt+1 = argmin
q
R(?t, q) (8)
M : ?t+1 = argmax
?
L(?) + (9)
?
n?
i=l+1
?
y
qt+1(y | xi) log p?(y | xi)
where ? here is a hyperparameter that trades off
between the labeled and unlabeled data. Though
not stated above, note that in the E-step minimiza-
tion over q(Y | xi) is constrained to the probabil-
ity simplex. Ganchev et al (2010) show that this
E-step can be efficiently implemented, via pro-
jected gradient descent on the dual. The M-step
is similar to optimizing the original L, but with a
contribution from the unlabeled data that further
encourages q and p? to agree. Thus, the M-step
can be implemented via the same gradient ascent
methods as used for L. As with standard EM,
this method monotonically increases J and thus
is guaranteed to converge to a local optimum.
In this work, we contemplate what other types
of posterior penalty terms besides hlin(m) are
possible. In the subsequent section, we show that
it is possible to extend the class of efficiently-
optimizable PR penalties to encompass all convex,
differentiable functions of the marginals.
3 Non-linear PR
Let h(m) denote an arbitrary convex, differen-
tiable function of the marginals of q. Replacing
R?s penalty term with h, we have:
R?(?, q) = KL(q ? p?) + ?h(m) (10)
Let J? represent the full objective with R?. We
show that J? can be efficiently optimized.
Running example: Returning to our POS
tagging example, let?s consider one type of
non-linear convex penalty that might be use-
ful. Suppose our corpus has N unique
trigrams, and we construct a graph G =
(V,E,W ) where each vertex in V is a trigram
and each edge (a, b) ? E has a weight wab
that indicates the similarity of trigrams a and
b. To use the information from this graph to
inform our CRF, we can use the graph Lapla-
cian: L = D?W , where D is a diagonal de-
gree matrix with daa =?Nj=1waj . The form
of L is such that for every vector v ? RN :
v>Lv = 12
N?
a=1
N?
b=1
wab(va ? vb)2 . (11)
The larger the disparity in v values of similar
vertices, the larger the value of v>Lv. The
matrix L is positive semi-definite, so v>Lv is
40
convex in v. If each entry va is a linear func-
tion of the vector of marginals m described
above, then v(m)>Lv(m) is convex in m.
Thus, for any linear v(m), we can use this
Laplacian expression as a PR penalty.
For example, we can define v(m) such that
h(m) applies a penalty if trigrams that are
similar according to the graph have different
expected taggings under the CRF model. To
state this more formally, let?s define a map-
pingB : ({1, . . . , n}, {1, . . . , T}) 7? V from
words in the corpus to vertices in the graph:
B(i, t) = a implies word xit maps to vertex a.
Then, for a given tag k, we have the following
formula for the value of vertex a:
va,k = m?a,k =
n?
i=1
T?
t=1
B(i,t)=a
K?
yt?1=1
mit,k,yt?1
?n
i=1
?T
t=1 1(B(i, t) = a)
There are several issues to overcome in showing
that EM with these more general h(m) can still
be run efficiently and will still reach a local opti-
mum. First, we have to show that the optimal q
for the E-step minimization can still be compactly
representable as a product of p-factors.
3.1 Decomposition
Theorem 1. If h(m) is a convex, differen-
tiable function of q?s p-factor marginals, q? =
argminq R?(?, q) decomposes as a product of p-
factors.
Proof. Consider the E-step gradient of R?(?, q)
with respect to q. Using the shorthand qiy for
q(y | xi), the gradient is:
?R?
?qiy
= log qiy + 1? log p?(y | xi) + (12)
??h(m)?m
>?m
?qiy
.
Here, ?m?qiy is just a 0-1 vector indicating which of
the marginals from m apply to qiy. For example,
for yt = k and yt?1 = j, the marginal mit,k,j is
relevant. We can more simply write:
?h(m)
?m
>?m
?qiy
=
T?
t=1
?h(m)
?mit,yt,yt?1
. (13)
Setting the gradient equal to zero and solving for
qiy, we see that it must take the following form:
qiy =
p?(y | xi) exp
[
??
T?
t=1
?h(m)
?mit,yt,yt?1
]
Zq(xi)
.
(14)
From this expression, it is clear that qiy is propor-
tional to a product of p-factors.
Running example: Recall the graph Lapla-
cian penalty, discussed above for a particular
tag k. Summing over all tags, the penalty is:
h(m) = 12
K?
k=1
N?
a=1
N?
b=1
wab(m?a,k ? m?b,k)2 .
The derivative ?h(m)?mit,yt,yt?1 is then:
2
K?
k=1
N?
a=1
wa,B(i,t)(m?B(i,t),k ? m?a,k) . (15)
In words: for a given k, this gradient is pos-
itive if node B(i, t) has larger probability of
taking tag k than its close neighbors. Moving
in the direction opposite the gradient encour-
ages similar taggings for similar trigrams.
Theorem 1 confirms that the optimal q will de-
compose as desired, but does not address whether
we can efficiently find this q. Previous PR work
optimized the E-step in the dual. But while the
dual is easy to compute in closed form for norms
or linear functions, for arbitrary convex functions
the dual is often non-trivial.
Running example: For the case of a
graph Laplacian regularizer, in the primal the
penalty takes the form of a quadratic pro-
gram: v>Lv. Unfortunately, the dual of a
quadratic program contains a matrix inverse,
L?1 (van de Panne and Whinston, 1964).
Taking a matrix inverse is expensive, which
makes optimization in the dual unattractive.
Since moving to the dual would be inefficient,
optimizing R? will require some form of gradient
descent on the qiy. However, the standard gradient
descent update:
qiy ? qiy ? ?
?R?
?qiy
(16)
41
where ? is the step size, does not result in a fea-
sible optimization scheme, for several reasons.
First, it is possible for the updated q to be outside
the probability simplex. To be sure it remains in
the simplex would require a projection step on the
full, exponential-size set of all qiy, for each exam-
ple xi. Second, the updated q may not be propor-
tional to a product of p-factors. To be concrete,
suppose the starting point is qiy = p?(y | xi),
which does decompose as a product of p-factors.
Then after the first gradient update, we have:
qiy = p?(y | xi)? ?
(
1 + ?
T?
t=1
?h(m)
?mit,yt,yt?1
)
.
Unfortunately, while p?(y | xi) decomposes as a
product of p-factors, the other term decomposes
as a sum. Naturally, as we discuss in the following
section, multiplicative updates are more suitable.
3.2 Exponentiated Gradient
The exponentiated gradient descent (EGD) algo-
rithm was proposed by Kivinen and Warmuth
(1995), who illustrate its application to linear pre-
diction. More recently, Collins et al (2005) and
Collins et al (2008) extended EGD to exploit fac-
torization in structured models. The most impor-
tant aspect of EGD for us is that a variable?s up-
date formula takes a multiplicative rather than an
additive form. Specifically, the update for qiy is:
qiy ? qiy exp
[
?? ?R??qiy
]
. (17)
Lemma 2. EGD update Equation (17) preserves
decomposition of q into p-factors.
Proof. Applying the multiplicative EGD update
formula to qiy, we see that its new value equals the
following product:
(qiy)1??p?(y | xi)? exp
[
???
T?
t=1
?h(m)
?mit,yt,yt?1
]
,
up to a normalization constant. Since qiy and
p?(y | xi) both decompose as a product of p-
factors and since the update term is another prod-
uct of p-factors, the updated expression is itself a
product of p-factors (up to normalization).
Note that normalization is not an issue with
the EGD updates. Since q retains its decompo-
sition, the normalization can be efficiently com-
puted using forward-backward. Thus, Lemma 2
moves us much closer to the goal of running EM
efficiently, though there remain several stumbling
blocks. First and foremost, we cannot afford to ac-
tually apply EGD to each qiy, as there are an expo-
nential number of them. Thankfully, we can show
these EGD updates are equivalent to following the
gradient on a much smaller set of values. In par-
ticular, letting F represent the dimension of m,
which for example is O(nTK2) for linear chains,
we have the following result.
Lemma 3. Given the gradient vector ?h(m)?m , one
step of EGD on R?(?, q) can be completed in time
O(F ), where F is the dimension ofm.
Proof. First, we re-express qiy in log-linear form.
Applying Lemma 2, we know that qiy is propor-
tional to a product of p-factors This means that
there must exist some factors r such that qiy can
be written:
qiy =
1
Zq(xi)
exp
[ T?
t=1
ri,t(yt, yt?1)
]
. (18)
Re-expressing ?R??qiy given these r, we have:
?R?
?qiy
= C +
T?
t=1
[
ri,t(yt, yt?1)? (19)
?>f(yt, yt?1,xi) + ?
?h(m)
?mit,yt,yt?1
]
,
whereC = 1?logZq(xi)+logZp(xi) is constant
with respect to y. This means that we can just
update the individual r factors as follows:
ri,t(yt, yt?1)? (1? ?)ri,t(yt, yt?1) +
??>f(yt, yt?1,xi)? ??
?h(m)
?mit,yt,yt?1
. (20)
Note that if we start from qiy = p?(y | xi), then
the initial ri,t(yt, yt?1) are just ?>f(yt, yt?1,xi).
To conclude, since the number of r functions is
equal to the dimension ofm, the overall update is
linear in the number of marginals.
At this point, just one small issue remains: how
expensive is computing ?h(m)?m ? Work analyzingthe reverse mode of automatic differentiation in-
dicates that if computing a function h requires c
operations, then computing its gradient vector re-
quires no more than O(c) operations (Griewank,
1988). Thus, as long as our penalty function is
42
itself efficiently computable, the gradient vector
will be too. We conclude by observing that our
efficient algorithm converges to a local optimum.
Theorem 4. The above EGD-based EM algorithm
for optimizing J? (?, q) converges to a local opti-
mum of this objective.
Proof. The M-step remains unchanged from stan-
dard PR EM, and as such is strictly convex in
?. The E-step is strictly convex in q, since KL-
divergence is strictly convex and h(m) is convex.
Applying EGD, we know that we can efficiently
find the E-step optimum. Therefore, the EGD-
based EM algorithm efficiently implements coor-
dinate ascent on J? (?, q), with each step monoton-
ically increasing J? :
J? (?t, qt) ? J? (?t, qt+1) ? J? (?t+1, qt+1) .
Hence, we have shown that it is possible to
efficiently use an arbitrary convex, differentiable
function of the marginals, h(m), as a PR penalty
function. In the following section, we apply one
such function ? the graph Laplacian quadratic
from the running example ? to several tasks.
4 Experiments
We evaluate the effect of a graph Laplacian PR
penalty on two different sequence prediction tasks:
part-of-speech (POS) tagging and handwriting
recognition. Our experiments are conducted in a
semi-supervised setting, where only a small num-
ber, l, of labeled sequences are available during
training. Both the l labeled sequences and the re-
mainder of the dataset (instances l + 1 through n)
are used to construct a graph Laplacian2. We train
a second-order CRF using the methods described
in Section 3 and report results for a test set con-
sisting of instances l + 1 through n.
4.1 Graph construction
For each task we define a symmetric similarity
function on the task?s vertices V , sim : V ? V 7?
R, and build the graph based on its values. Specif-
ically, denoting the k nearest neighbors (NN) of
node u by Nk(u), we use the following mutual k-
NN criterion to decide which edges to include:
(u, v) ? E ?? u ? Nk(v) ? v ? Nk(u) .
2While these particular experiments are transductive, our
method can easily be applied inductively as well.
Entries in the final edge weight matrix are: wuv =
1[(u, v) ? E]sim(u, v).
4.2 Part-of-speech tagging
We experiment on ten languages. Our English
(EN) data is from the Penn Treebank (Marcus et
al., 1993), Italian (IT) and Greek (EL) are from
CoNLL-2007 (Nivre et al, 2007), and the remain-
ing languages in Figure 1 (a): German (DE), Span-
ish (ES), Portuguese (PT), Danish (DA), Slovene
(SL), Swedish (SV), and Dutch (NL) are from
CoNLL-X (Buchholz and Marsi, 2006). We use
a universal tag set (Das et al, 2012) throughout.
For each language, we first construct a mu-
tual 60-NN graph3 on trigram types, excluding
trigrams whose center word is punctuation. Our
smallest graph (Slovene) contains 25,198 nodes
while the largest (English) has 611,730.
For the similarity function sim(u, v), we follow
the method used in (Subramanya et al, 2010) and
(Das and Petrov, 2011), but with a somewhat mod-
ified feature set. For instance, while (Subramanya
et al, 2010) uses suffixes of the trigram?s center
word, we find this type of feature is too easy for
unrelated trigrams to match, leading to a noisy
graph. Let a trigram and its left/right context be
denoted by the 5-gram (w0, w1, w2, w3, w4). Then
the features we use to build the graph are:
? Trigram features: w12, w13, w23, w2,
suffix(w3)w2, suffix(w1)w2
? Context features: w0134, w012, w023, w024,
w124, w234, w01, w02, w24, w34
where suffix indicates common suffixes collected
from Wiktionary data. For a given feature f and
trigram type t, the value of the feature is deter-
mined by pointwise mutual information (PMI):
log #(f?t)#(f)#(t) . Then, for each pair of trigram types,
sim(u, v) is given by the cosine similarity of the
trigrams? feature vectors.
For the second-order CRF, we use a fairly stan-
dard set of features:
? Emission features: 1(yt = k ? f(xt?)),
where k can be any POS tag and t? ? {t, t ?
1, t + 1}. The f(xt?) takes the form of a
function from the following set: one indica-
tor for each word, lowercased word, and suf-
3In preliminary experiments we tested graphs with 20, 40,
60, 80, and 100 NNs and found that beyond 60 NNs addi-
tional performance gains are small.
43
fix, and also is-capitalized, is-punctuation, is-
digit, contains-hyphen, and contains-period.
? Transition features: For any POS tags
k1, k2, k3, we have a feature 1(yt =
k1, yt?1 = k2, yt+1 = k3) and its backoffs
(indicators for one or two matching tags).
4.3 Handwriting recognition
The handwriting dataset we use was collected by
Kassel (1995) and filtered to 6,877 words (Taskar
et al, 2003). For each word, the first letter is re-
moved so that every remaining letter is one of the
English language?s 26 lowercase letters.
Again, we first build a mutual NN graph. In this
case, we use 20-NN, since our graph has fewer
nodes and a larger set of possible node identi-
ties (26 letters instead of 12 tags). Each node
in this graph is one letter from the dataset, for a
total of 52,152 nodes. As a first step, we com-
pute cosine similarity on the pixels of each pair of
nodes, and then consider only pairs with a similar-
ity greater than 0.3. Next, we apply the Fast Earth
Mover?s distance E?MD(u, v) (Pele and Werman,
2009) with default parameters to compute the dis-
similarity of each pair of images. We convert these
into similarities via:
s(u, v) = exp
{
?E?MD(u, v)
?2EMD
}
(21)
where we set the variance ?EMD = 10. The fi-
nal similarity function sim(u, v) is the weighted
combination of the similarity of the nodes (u, v)
and their left neighbors (ul, vl) and right neigh-
bors (ur, vr) from their respective words:
sim(u, v) = ?s(u, v)+(1??)(s(ul, vl)+s(ur, vr))
where we fix ? = 0.8.
For the second-order CRF, the transition fea-
tures are same as for POS tagging, but with tags re-
placed by the English alphabet. The emission fea-
tures take a similar form, but with different mean-
ings for the f(xt?) indicator functions. Specifi-
cally, there is one indicator for each pixel loca-
tion, with value 1 if the pixel is turned on. As
there are many more emission than transition fea-
tures, we count the number of fired emission and
transition features, say fe and ft, then discount all
emission features, multiplying them by ftfe to bal-ance the amount of supervision.
4.4 Baselines
We compare our posterior regularization (PR) re-
sults with three baselines. We also include results
for the first EM iteration of our PR method (PR1),
to show there is still significant optimization oc-
curring after the first iteration.
The first baseline is graph propagation (GP).
Specifically, we start from uniform posteriors for
all the unlabeled nodes in the graph, then for each
tag/letter k and each node v we apply the gradient
update:
qk,v ? qk,v ? ?
?
u?Nk(v)
wkuv(qk,v ? qk,u) (22)
until convergence. We then select the tag/letter
with the largest probability as the prediction for
a node. If multiple tokens are mapped to a node,
then all receive the same prediction.
The second baseline incorporates both graph
propagation and sequence information. As a first
step, we run the GP baseline, then use the decod-
ing as additional labeled data to train a second-
order CRF (see GP?CRF results). The third base-
line is simply a second-order CRF, trained on the l
labeled examples.
4.5 Training details
For optimizing the CRF, we use L-BFGS (Bert-
sekas, 2004) and a Gaussian prior with ? = 100
(chosen by cross-validation on the labeled train-
ing examples). The final predictions are obtained
via posterior decoding. For PR, we run EM for
at most 20 iterations, which is enough for con-
vergence of the combined objective J? (?, q). We
cross-validate the constraint strength parameter ?
over the following values: {0.1, 0.5, 1.0, 2.0}, ul-
timately selecting ? = 1 for the POS tagging task
and ? = 0.1 for the handwriting recognition task.
4.6 Results and analysis
POS tagging. For each language, we randomly
sample 1000 labeled examples and split them into
10 non-overlapping training sets of size l = 100.
Figure 1 (a) shows the average error and its stan-
dard deviation for these training sets. If for each
language we take the difference between the aver-
age error of PR and that of the best of the three
baselines, the min, average, and max improve-
ments are: 2.69%, 4.06%, and 5.35%. When
analyzing the results, we observed that one re-
gion where PR makes substantial gains over the
44
EN DE ES PT DA SL SV EL IT NL Avg0
5
10
15
20
25
Language
POS T
agging
 Error
 
 GP GP? CRF CRF PR1 PR
(a)
0 100 200 300 400 5000
5
10
15
20
# of Labeled Examples
Portugue
se Taggi
ng Error
 
 
GP GP? CRF CRF PR1 PR
(b)
Figure 1: (a): POS results for 10 languages. Each bar in each group corresponds to the average POS
tagging error of one method; the left-to-right order of the methods is the same as in the legend. Whiskers
indicate standard deviations. The final set of bars is an average across all languages. See supplement for
a table with the exact numbers. (b): POS results on one language for a range of l.
CRF baseline is on unseen words (words that do
not occur in the set of l labeled examples). If
we measure performance only on such words, the
gain of PR over CRF is 6.7%. We also test with
l = {50, 100, 150, 200, 300, 400, 500} on one lan-
guage to illustrate how PR performs with different
amounts of supervision. Figure 1 (b) shows that
even when l = 500 our PR method is still able to
provide improvement over the best baseline.
Handwriting recognition. For this task, the
overall dataset contains 55 distinct word types.
Thus, we set l = 110 and sample 10 training
sets such that each contains 2 examples of each
of word. Note that due to the well-balanced train-
ing sets, baselines are fairly high here compared
to other similar work with this dataset. Table 1
shows there is an average improvement of 4.93%
over the best of the three baselines.
GP GP?CRF CRF PR1 PR
Mean 17.57 15.07 9.82 6.03 4.89
StdDev 0.30 0.35 0.48 0.20 0.42
Table 1: Handwriting recognition errors.
Even in a simpler setting closer to that of POS
tagging, where we just draw l = 100 samples ran-
domly, there are many cases where PR beats the
baselines. Figure 2 shows predictions from such
a setting and provides general intuition as to why
PR does well on handwriting recognition. For the
word ?Wobble? (with the first letter removed), the
CRF predicts ?obble? as ?ovely?, because of it re-
lies heavily on sequential information; in our small
training set, bigrams ?ov? (2 times) and ?ly? (12
times) are more frequent than ?ob? (1 time) and
?le? (7 times). GP correctly predicts these letters
because the graph connects them to good neigh-
bors. However, GP mislabels ?l? as ?i?, since most
of this letter?s neighbors are i?s. The coupling of
GP and CRF via PR links the neighbor informa-
tion with bigram information ? ?bl? (5 times) is
more frequent than ?bi? in the training set ? to
yield the correct labeling.
      l    t    l    l    l
      i    i    i    l    i
      l    l    l    l    l 
  CRF   b    b    b    b    m
  GP    b    b    b    b    b
  PR    b    b    b    b    b
CRF  o  v  e  l  y
GP   o  b  b  i  e
PR   o  b  b  l  e  
Figure 2: Predictions on the word ?Wobble? and
the 5-NNs of its first ?b? and ?l?.
5 Conclusion
We have presented an efficient extension of the
posterior regularization (PR) framework to a more
general class of penalty functions. Encouraging
results using a graph Laplacian penalty suggest
potential applications to a much larger class of
weakly supervised problems.
Acknowledgements
J. Gillenwater was supported by a National Sci-
ence Foundation Graduate Research Fellowship.
L. He and B. Taskar were partially supported by
ONR Young Investigator Award N000141010746.
45
References
[Altun et al2005] Y. Altun, D. McAllester, and
M. Belkin. 2005. Maximum Margin Semi-
Supervised Learning for Structured Variables. In
Proc. NIPS.
[Belkin et al2005] M. Belkin, P. Niyogi, and V. Sind-
hwani. 2005. On Manifold Regularization. In Proc.
AISTATS.
[Bertsekas2004] D. Bertsekas. 2004. Nonlinear Pro-
gramming.
[Buchholz and Marsi2006] S. Buchholz and E. Marsi.
2006. CoNLL-X Shared Task on Multilingual De-
pendency Parsing. In Proc. CoNLL.
[Chapelle et al2006] O. Chapelle, B. Scho?lkopf, and
A. Zien, editors. 2006. Semi-Supervised Learning.
[Collins et al2005] M. Collins, P. Bartlett,
D. McAllester, and B. Taskar. 2005. Expo-
nentiated Gradient Algorithms for Large-Margin
Structured Classification. In Proc. NIPS.
[Collins et al2008] M. Collins, A. Globerson, T. Koo,
and X. Carreras. 2008. Exponentiated Gradient Al-
gorithms for Conditional Random Fields and Max-
Margin Markov Networks. JMLR.
[Das and Petrov2011] D. Das and S. Petrov. 2011. Un-
supervised Part-of-Speech Tagging with Bilingual
Graph-Based Projections. In Proc. ACL.
[Das et al2012] D. Das, S. Petrov, and R. McDonald.
2012. A Universal Part-of-Speech Tagset. In Proc.
LREC.
[Ganchev et al2010] K. Ganchev, J. Grac?a, J. Gillen-
water, and B. Taskar. 2010. Posterior Regulariza-
tion for Structured Latent Variable Models. JMLR.
[Griewank1988] A. Griewank. 1988. On Automatic
Differentiation. Technical report, Argonne National
Laboratory.
[Joachims2003] T. Joachims. 2003. Transductive
Learning via Spectral Graph Partitioning. In Proc.
ICML.
[Kassel1995] R. Kassel. 1995. A Comparison of Ap-
proaches to On-line Handwritten Character Recog-
nition. Ph.D. thesis, Massachusetts Institute of
Technology.
[Kivinen and Warmuth1995] J. Kivinen and M. War-
muth. 1995. Additive Versus Exponentiated Gra-
dient Updates for Linear Prediction. In Proc. STOC.
[Mann and McCallum2007] G. Mann and A. McCal-
lum. 2007. Simple, Robust, Scalable Semi-
Supervised Learning via Expectation Regulariza-
tion. In Proc. ICML.
[Mann and McCallum2008] G. Mann and A. McCal-
lum. 2008. Generalized Expectation Criteria for
Semi-Supervised Learning of Conditional Random
Fields. In Proc. ACL.
[Marcus et al1993] M. Marcus, M. Marcinkiewicz, and
B. Santorini. 1993. Building a Large Annotated
Corpus of English: Then Penn Treebank. Compu-
tational Linguistics.
[Nivre et al2007] J. Nivre, J. Hall, S. Ku?bler, R. Mc-
Donald, J. Nilsson, S. Riedel, and D. Yuret. 2007.
The CoNLL 2007 Shared Task on Dependency Pars-
ing. In Proc. CoNLL.
[Pele and Werman2009] O. Pele and M. Werman.
2009. Fast and Robust Earth Mover?s Distances. In
Proc. ICCV.
[Subramanya and Bilmes2009] A. Subramanya and
J. Bilmes. 2009. Entropic Graph Regularization in
Non-Parametric Semi-Supervised Classification. In
Proc. NIPS.
[Subramanya et al2010] A. Subramanya, S. Petrov, and
F. Pereira. 2010. Efficient Graph-Based Semi-
Supervised Learning of Structured Tagging Models.
In Proc. EMNLP.
[Taskar et al2003] B. Taskar, C. Guestrin, and
D. Koller. 2003. Max Margin Markov Networks.
In Proc. NIPS.
[van de Panne and Whinston1964] C. van de Panne and
A. Whinston. 1964. The Simplex and the Dual
Method for Quadratic Programming. Operational
Research Quarterly.
[Zhu and Lafferty2005] X. Zhu and J. Lafferty. 2005.
Harmonic Mixtures: Combining Mixture Models
and Graph-Based Methods for Inductive and Scal-
able Semi-Supervised Learning. In Proc. ICML.
[Zhu et al2003] X. Zhu, Z. Ghahramani, and J. Laf-
ferty. 2003. Semi-Supervised Learning Using
Gaussian Fields and Harmonic Functions. In Proc.
ICML.
[Zhu2005] X. Zhu. 2005. Semi-Supervised Learning
Literature Survey. Technical report, University of
Wisconsin-Madison.
46
