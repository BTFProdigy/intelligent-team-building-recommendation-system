First Joint Conference on Lexical and Computational Semantics (*SEM), pages 643?647,
Montre?al, Canada, June 7-8, 2012. c?2012 Association for Computational Linguistics
DERI&UPM: Pushing Corpus Based Relatedness to Similarity: Shared
Task System Description
Nitish Aggarwal? Kartik Asooja? Paul Buitelaar?
?Unit for Natural Language Processing
Digital Enterprise Research Institute
National University of Ireland, Galway, Ireland
firstname.lastname@deri.org
?Ontology Engineering Group
Universidad Politecnica de Madrid
Madrid, Spain
asooja@gmail.com
Abstract
In this paper, we describe our system submit-
ted for the semantic textual similarity (STS)
task at SemEval 2012. We implemented two
approaches to calculate the degree of simi-
larity between two sentences. First approach
combines corpus-based semantic relatedness
measure over the whole sentence with the
knowledge-based semantic similarity scores
obtained for the words falling under the same
syntactic roles in both the sentences. We fed
all these scores as features to machine learn-
ing models to obtain a single score giving the
degree of similarity of the sentences. Lin-
ear Regression and Bagging models were used
for this purpose. We used Explicit Semantic
Analysis (ESA) as the corpus-based seman-
tic relatedness measure. For the knowledge-
based semantic similarity between words, a
modified WordNet based Lin measure was
used. Second approach uses a bipartite based
method over the WordNet based Lin measure,
without any modification. This paper shows
a significant improvement in calculating the
semantic similarity between sentences by the
fusion of the knowledge-based similarity mea-
sure and the corpus-based relatedness measure
against corpus based measure taken alone.
1 Introduction
Similarity between sentences is a central concept
of text analysis, however previous studies about
semantic similarities have mainly focused either
on single word similarity or complete document
similarity. Sentence similarity can be defined by the
degree of semantic equivalence of two given sen-
tences, where sentences are typically 10-20 words
long. The role of sentence semantic similarity mea-
sures in text-related research is increasing due to
potential number of applications such as document
summarization, question answering, information
extraction & retrieval and machine translation.
One plausible limitation of existing methods for
sentence similarity is their adaptation from long text
(e.g. documents) similarity methods, where word
co-occurrence plays a significant role. However,
sentences are too short, thats why taking syntac-
tic role of each word with its narrow semantic
meaning into account, can be highly relevant to
reflect the semantic equivalence of two sentences.
These narrow semantics can be reflected from any
existing large lexicons [(Wu and Palmer, 1994)
and (Lin, 1998)]; nevertheless, these lexicons can
not provide the semantics of words which are out
of lexicon (e.g. guy) or multiword expressions.
These semantics can be represented by a large
distributed semantic space such as Wikipedia and
similarity can be reflected by relatedness of these
extracted semantics. However, relatedness covers
broader space than similarity, which forced us to
tune the Wikipedia based relatedness with lexical
structure (e.g. WordNet) based similarities driven
by linguistic syntactic structure, in reflecting more
sophisticated similarity of two given sentences.
In this work, we present a sentence similarity using
ESA and syntactic similarities. The rest of this
paper is organized as follows. Section 2 explores the
related work. Section 3 describes our approaches
643
in detail. Section 4 explains our three different
submitted runs for STS task. Section 5 shows the
results and finally we conclude in section 6.
2 Related Work
In recent years, there have been a variety of efforts
in improving semantic similarity measures, however
most of these approaches address this problem from
the viewpoint of large document similarity based on
word co-occurrence using string pattern or corpus
statistics. Corpus based approaches such as Latent
Semantic Analysis (LSA) [(Landauer et. al, 1998)
and (Foltz et. al, 1998)] and ESA (Gabrilovich and
Markovitch, 2007) use corpus statistics information
about all words and reflect their semantics in
distributional high semantic space. However, these
approaches perform quite well for long texts as they
use word co-occurrence and relying on the principle
that words which are used in the same contexts
tend to have related meanings. In case of short text
similarities, syntactic role of each word with its
meaning plays an important role.
There are several linguistic measures [( Achananu-
parp et. al, 2008) and (Islam and Inkpen, 2008)],
which can account for pseudo-syntactic information
by analyzing their word order using n-gram. To do
this, Islam and Inkpen defined a syntactic measure,
which considers the word order between two
strings by computing the maximal ordered word
overlapping. (Oliva et. al, 2011) present a similarity
measure for sentences and short text that takes
syntactic information, such as morphology and
parsing tree, into account and calculate similarities
between words with same syntactic role, by using
WordNet.
Our work takes inspiration from existing approaches
that exploit a combination of Wikipedia based re-
latedness with lexical structure based similarities
driven by linguistic syntactic structure.
3 Methodology
We implemented two approaches for the STS
task [(Agirre et. al, 2012)]. First approach is a
fusion of corpus-based semantic relatedness and
knowledge-based semantic similarity measures.
The core of this combination is the corpus-based
measure because the combination includes the
corpus-based semantic relatedness score over the
whole sentences and the knowledge-based semantic
similarity scores for the words falling under the
same syntactic roles in both the sentences. Machine
learning models are trained by taking all these
scores as different features. For the submission,
we used Linear regression and Bagging models.
Also, the equation obtained after training the linear
regression model shows more weightage to the score
obtained by the corpus-based relatedness measure
as this is the only score (feature), which reflects the
semantic relatedness/similarity score over the full
sentences, out of all the considered features for the
model. We used ESA as the corpus based semantic
relatedness measure and modified WordNet-based
Lin measure as the knowledge-based similarity.
The WordNet-based Lin relatedness measure was
modified to reflect better the similarity between
the words. For the knowledge-based similarity,
currently we considered only the words lying in the
three major syntactic role categories i.e. subjects,
actions and the objects. We see the first approach
as the corpus-based measure ESA tuned with the
knowledge-based measure. Thus, it is referred as
TunedESA later in the paper.
Our second approach is based on the bipartite
method over the WordNet based semantic relat-
edness measures. WordNet-based Lin measure
(without any modification) was used for calcu-
lating the relatedness scores for all the possible
corresponding pair of words appearing in both the
sentences. Then, the similarity/relatedness score
for the sentences is calculated by perceiving the
problem as the computation of a maximum total
matching weight of a bipartite graph having the
words as nodes and the relatedness scores as the
weight of the edges between the nodes. To solve
this, we used Hungarian method. Later, we refer
this method as WordNet-Bipartite.
3.1 TunedESA
In this approach, the ESA based relatedness score
for the full sentences is combined with the modified
WordNet-based Lin similarity scores calculated for
the words falling under the corresponding syntactic
role category in both the sentences.
644
ALL Rank-ALL ALLnrm RankNrm Mean RankMean
Baseline 0.3110 87 0.6732 85 0.4356 70
Run1 0.5777 52 0.8158 20 0.5466 52
Run2 0.5833 51 0.8183 17 0.5683 42
Run3 0.4911 67 0.7696 57 0.5377 53
Table 1: Overall Rank and Pearson Correlation of all runs
MSRpar MSRvid SMTeuro OnWN SMTnews
Baseline 0.4334 0.2996 0.4542 0.5864 0.3908
ESA? 0.2778 0.8178 0.3914 0.6541 0.4366
Run1 0.3675 0.8427 0.3534 0.6030 0.4430
Run2 0.3720 0.8330 0.4238 0.6513 0.4489
Run3 0.5320 0.6874 0.4514 0.5827 0.2818
Table 2: Pearson Correlation of all runs with all five STS test datasets
TunedESA could be summarized as these four
basic steps:
? Calculate the ESA relatedness score between
the sentences.
? Find the words corresponding to the linguistic
syntactical categories like subject, action and
object of both the sentences.
? Calculate the semantic similarity between the
words falling in the corresponding subjects, ac-
tions and objects in both the sentences using
modified WordNet-based measure Lin.
? Combine these four scores for ESA, Subject,
Action and Object to get the final similarity
score on the basis of an already learned ma-
chine learning model with the training data.
ESA is a promising technique to find the relatedness
between documents. The texts which need to be
compared are represented as high dimensional vec-
tors containing the TF-IDF weight between the term
and the Wikipedia article. The semantic relatedness
measure is calculated by taking the cosine measure
between these vectors. In this implementation of
ESA 1, the score was calculated by considering the
1ESA? considering full sentence at a time to make the vector
i.e. different from standard ESA
full sentence at a time for making the Wikipedia
article vector while in the standard ESA, vectors
are made for each word of the text followed by the
addition of all these vectors to represent the final
vector for the text/sentence. It was done just to
reduce the time complexity.
To calculate the lexical similarity between the
words, we implemented WordNet-based semantic
relatedness measure Lin. This score was modified to
reflect a better similarity between the words. In the
current system, basic linguistic syntactic categories
i.e. subjects, actions and objects were used. For
instance, below is a sentences pair from the training
MSRvid dataset with the gold standard score and
the syntactic roles.
Sentence 1: A man is playing a guitar.
Subject: Man, Action: play, Object: guitar
Sentence 2: A man is playing a flute.
Subject: Man, Action: play, Object: flute
Gold Standard Score (0-5): 2.2
As the modification, the scores given by Lin
measure were used only for the cases where sub-
sumption relation or hypernymy/hyponymy exists
645
between the words. This modification was done
only for the words falling under the category of
subjects and objects.
3.2 WordNet Bipartite
WordNet-based semantic relatedness measure was
used for the second approach.
Following steps are performed :
? Each sentence is tokenized to obtain the words.
? Semantic relatedness between every possible
pair of words in both the sentences is calculated
using WordNet-based measure e.g. Lin.
? Using the scores obtained in the second step,
the semantic similarity/relatedness between the
sentences is calculated by transforming the
problem as that of computing the maximum to-
tal matching weight of a bipartite graph, which
can be done by using Hungarian method.
4 System Description
We submitted three runs in the semantic textual
similarity task. The first two runs are based on the
first approach i.e. TunedESA and they differ only in
the machine learning algorithm used for obtaining
the final similarity score based on all the considered
scores/features.
ESA was implemented on the current Wikipedia
dump. WordNet based relatedness measure Lin
was modified to give a better semantic similarity
degree. Stanford Core-NLP library was used for
obtaining the words with their syntactic roles.
All the required scores/feature i.e. ESA based
relatedness for the complete sentences and mod-
ified WordNet-based Lin similarity scores were
calculated for the corresponding words lying in
the same syntactic categories. Bagging and Linear
Regression models were built using the training data
for the first and second runs respectively. Based on
the category of the test dataset, model was trained
on the corresponding training dataset.
For the surprise test datasets, we trained our
model with the training dataset of the MSRvid data
based on the fact that we obtained good results with
this category. Then the built models were used for
calculating the similarity scores for the test data.
For the third run, WordNet Bipartite method
was used to calculate the similarity scores. It didn?t
require any training.
5 Results and Discussion
All above described runs are evaluated on STS
test dataset. Table 1 shows the overall results2 of
our three runs against the baseline system which
follows the bag of words approach. Table 2 shows
the Pearson correlation on different test datasets for
all the three runs. It provides a comparison between
corpus based relatedness measure ESA and our
system TunedESA (Run 1 & Run 2).
The results show significant improvement against
ESA. Although, it can be seen that the baseline
results are even better than of the ESA in the cases
of MSRpar and SMTeuro. It may be because this
implementation of ESA is not the standard one.
6 Conclusion
We presented a method to calculate the degree of
sentence similarity based on tuning the corpus based
relatedness measure with the knowledge-based sim-
ilarity measure over the syntactic roles. The results
show a definite improvement by the fusion. As
future work, we plan to improve the syntactic role
handling and considering more syntactical cate-
gories. Also, experimentation3 with standard ESA
and other semantic similarity/relatedness measures
needs to be performed.
Acknowledgments
This work is supported in part by the European
Union under Grant No. 248458 for the Monnet
project as well as by the Science Foundation Ireland
under Grant No. SFI/08/CE/I1380 (Lion-2).
2results can also be found at http://www.cs.york.
ac.uk/semeval-2012/task6/index.php?id=
results-update with the name nitish aggarwal
3We plan to provide the further results and information at
http://www.itssimilar.com/
646
References
Achananuparp Palakorn and Xiaohua Hu and Xiajiong
Shen 2008 The Evaluation of Sentence Similarity
Measures, In: DaWaK. pp. 305-316
Agirre Eneko , Cer Daniel, Diab Mona and Gonzalez-
Agirre Aitor 2012 SemEval-2012 Task 6: A Pilot on
Semantic Textual Similarity. In: Proceedings of the
6th International Workshop on Semantic Evaluation
(SemEval 2012), in conjunction with the First Joint
Conference on Lexical and Computational Semantics
(*SEM 2012).
Foltz P. W., Kintsch W. and Landauer T. K. 1998. In:
journal of the Discourse Processes. pp. 285-307, The
measurement of textual Coherence with Latent Se-
mantic Analysis,
Gabrilovich Evgeniy and Markovitch Shaul 2007 Com-
puting Semantic Relatedness using Wikipedia-based
Explicit Semantic Analysis, In: Proceedings of The
Twentieth International Joint Conference for Artificial
Intelligence. pp. 1606?1611,
Islam, Aminul and Inkpen, Diana 2008 Semantic
text similarity using corpus-based word similarity and
string similarity, In: journal of ACM Trans. Knowl.
Discov. Data. pp. 10:1?10:25
Landauer Thomas K. ,Foltz Peter W. and Laham Darrell
1998. An Introduction to Latent Semantic Analysis,
In: Journal of the Discourse Processes. pp. 259-284,
Lin Dekang 1998 Proceeding of the 15th International
Conference on Machine Learning. pp. 296?304 An
information-theoretic definition of similarity
Oliva, Jesu?s and Serrano, Jose? Ignacio and del Castillo,
Mar??a Dolores and Iglesias, A?ngel April, 2011
SyMSS: A syntax-based measure for short-text seman-
tic similarity In: journal of Data Knowledge Engineer-
ing. pp. 390?405
Wu, Zhibiao and Palmer, Martha 1994 Verbs seman-
tics and lexical selection, In: Proceedings of the 32nd
annual meeting on Association for Computational Lin-
guistics,
647
Proceedings of the Third Joint Conference on Lexical and Computational Semantics (*SEM 2014), pages 51?56,
Dublin, Ireland, August 23-24 2014.
Exploring ESA to Improve Word Relatedness
Nitish Aggarwal Kartik Asooja Paul Buitelaar
Insight Centre for Data Analytics
National University of Ireland
Galway, Ireland
firstname.lastname@deri.org
Abstract
Explicit Semantic Analysis (ESA) is an ap-
proach to calculate the semantic relatedness
between two words or natural language texts
with the help of concepts grounded in human
cognition. ESA usage has received much at-
tention in the field of natural language pro-
cessing, information retrieval and text analy-
sis, however, performance of the approach de-
pends on several parameters that are included
in the model, and also on the text data type
used for evaluation. In this paper, we investi-
gate the behavior of using different number of
Wikipedia articles in building ESA model, for
calculating the semantic relatedness for differ-
ent types of text pairs: word-word, phrase-
phrase and document-document. With our
findings, we further propose an approach to
improve the ESA semantic relatedness scores
for words by enriching the words with their
explicit context such as synonyms, glosses and
Wikipedia definitions.
1 Introduction
Explicit Semantic Analysis (ESA) is a distributional
semantic model (Harris, 1954) that computes the
relatedness scores between natural language texts
by using high dimensional vectors. ESA builds
the high dimensional vectors by using the explicit
concepts defined in human cognition. Gabrilovich
and Markovitch (2007) introduced the ESA model
in which Wikipedia and Open Directory Project
1
was used to obtain the explicit concepts. ESA con-
siders every Wikipedia article as a unique explicit
1
http://www.dmoz.org
topic. It also assumes that the articles are topically
orthogonal. However, recent work (Gottron et
al., 2011) has shown that by using the documents
from Reuters corpus instead of Wikipedia articles
can also achieve comparable results. ESA model
includes various parameters (Sorg and Cimiano,
2010) that play important roles on its performance.
Therefore, the model requires further investigation
in order to better tune the parameters.
ESA model has been adapted very quickly in
different fields related to text analysis, due to the
simplicity of its implementation and the availability
of Wikipedia corpus. Gabrilovich and Markovitch
(2007) evaluated the ESA against word relatedness
dataset WN353 (Finkelstein et al., 2001) and doc-
ument relatedness dataset Lee50 (Lee et al., 2005)
by using all the articles from Wikipedia snapshot of
11 Nov, 2005. However, the results reported using
different implementations (Polajnar et al., 2013)
(Hassan and Mihalcea, 2011) of ESA on same
datasets (WN353 and Lee50) vary a lot, due the
specificity of ESA implementation. For instance,
Hassan and Mihalcea (2011) found a significant
difference between the scores obtained from their
own implementation and the scores reported in the
original article (Gabrilovich and Markovitch, 2007).
In this paper, first, we investigate the behavior
of ESA model in calculating the semantic related-
ness for different types of text pairs: word-word,
phrase-phrase and document-document by using
different number of Wikipedia articles for building
the model. Second, we propose an approach
51
for context enrichment of words to improve the
performance of ESA on word relatedness task.
2 Background
The ESA model can be described as a method of
obtaining the relatedness score between two texts by
quantifying the distance between two high dimen-
sional vectors. Every explicit concept represents a
dimension of the ESA vector, and the associativity
weight of a given word with the explicit concept
can be taken as magnitude of the corresponding
dimension. For instance, there is a word t, ESA
builds a vector v, where v =
?
N
i=0
a
i
? c
i
and c
i
is
i
th
concept from the explicit concept space, and a
i
is the associativity weight of word t with the concept
c
i
. Here, N represents the total number of concepts.
In our implementation, we build ESA model by
using Wikipedia articles as explicit concepts, and
take the TFIDF weights as associativity strength.
Similarly, ESA builds the vector for natural lan-
guage text by considering it as a bag of words. Let
T = {t
1
, t
2
, t
3
...t
n
}, where T is a natural language
text that has n words. ESA generates the vector
V, where V =
?
t
k
T
v
k
and v =
?
N
i=0
a
i
? c
i
. v
k
represents the ESA vector of a individual words as
explained above. The relatedness score between two
natural language texts is calculated by computing
cosine product of their corresponding ESA vectors.
In recent years, some extensions (Polajnar et
al., 2013) (Hassan and Mihalcea, 2011) (Scholl et
al., 2010) have been proposed to improve the ESA
performance, however, they have not discussed the
consistency in the performance of ESA. Polajnar
et al. (2013) used only 10,000 Wikipedia articles
as the concept space, and got significantly different
results on the previously evaluated datasets. Hassan
and Mihalcea (2011) have not discussed the ESA
implementation in detail but obtained significantly
different scores. Although, these proposed exten-
sions got different baseline ESA scores but they
improve the relatedness scores with their additions.
Polajnar et al. (2013) used the concept-concept
correlation to improve the ESA model. Hassan and
Mihalcea (2011) proposed a model similar to ESA,
which builds the high dimensional vector of salient
concepts rather than explicit concepts. Gortton et
al. (2011) investigated the ESA performance for
document relatedness and showed that ESA scores
are not tightly dependent on the explicit concept
spaces.
Minimum unique Total number of
words (K) articles (N)
100 438379
300 110900
500 46035
700 23608
900 13718
1100 8322
1300 5241
1500 3329
1700 2126
1900 1368
Table 1: The total number of retrieved articles for differ-
ent values of K
3 Investigation of ESA model
Although Gortton et al. (2011) has shown that ESA
performance on document pairs does not get af-
fected by using different number of Wikipedia ar-
ticles, we further examine it for word-word and
phrase-phrase pairs. We use three different datasets
WN353, SemEvalOnWN (Agirre et al., 2012) and
Lee50. WN353 contains 353 word pairs, SemEval-
OnWN consists of 750 short phrase/sentence pairs,
and Lee50 is a collection of 50 document pairs.
All these datasets contain relatedness scores given
by human annotators. We evaluate ESA model
on these three datasets against different number of
Wikipedia articles. In order to select different num-
ber of Wikipedia articles, we sort them according to
the total number of unique words appearing in each
article. We select N articles, where N is total num-
ber of articles which have at least K unique words.
Table 1 shows the total number of retrieved articles
for different values of K. We build 20 different ESA
models with the different values of N retrieved by
varying K from 100 to 2000 with an interval of 100.
Figure 1 illustrates Spearman?s rank correlation of
all the three types of text pairs on Y-axis while X-
axis shows the different values of N which are taken
to build the model. It shows that ESA model gener-
ates very consistent results for phrase pairs similar
to the one reported in (Aggarwal et al., 2012), how-
52
Figure 1: ESA performance on different types of text
pairs by varying the total number of articles
ever, the correlation scores decreases monotonously
in the case of word pairs as the number of articles
goes down. In the case of document pairs, ESA pro-
duces similar results until the value of N is chosen
according to K = 1000, but after that, it decreases
quickly because the number of articles becomes too
low for making a good enough ESA model. All this
indicates that word-word relatedness scores have a
strong impact of changing the N in comparison of
document-document or phrase-phrase text pairs. An
explanation to this is that the size of the ESA vec-
tor for a word solely depends upon the popularity
of the given word, however, in the case of text, the
vector size depends on the popularity summation of
all the words appearing in the given text. It suggests
that the word relatedness problem can be reduced to
short text relatedness by adding some related con-
text with the given word. Therefore, to improve
the ESA performance for word relatedness, we pro-
pose an approach for context enrichment of words.
We perform context enrichment by concatenating re-
lated context with the given word and use this con-
text to build the ESA vector, which transforms the
word relatedness problem to phrase relatedness.
4 Context Enrichment
Context enrichment is performed by concatenating
the context defining text to the given word before
building the ESA vector. Therefore, instead of build-
ing the ESA vector of a word, the vector is built for
the short text that is obtained after concatenating the
related context. This is similar to classical query ex-
pansion task (Aggarwal and Buitelaar, 2012; Pan-
tel and Fuxman, 2011), where related concepts are
concatenated with a query to improve the informa-
tion retrieval performance. We propose three differ-
ent methods to obtain related context: 1) WordNet-
based Context Enrichment 2) Wikipedia-based Con-
text Enrichment, and 3) WikiDefinition-based Con-
text Enrichment.
4.1 WordNet-based Context Enrichment
WordNet-based context enrichment uses the Word-
Net synonyms to obtain the context, and concate-
nates them into the given word to build the ESA vec-
tor. However, WordNet may contain more than one
synset for a word, where each synset represents a
different semantic sense. Therefore, we obtain more
than one contexts for a given word, by concatenat-
ing the different synsets. Further, we calculate ESA
score of every context of a given word against all the
contexts of the other word which is being compared,
and consider the highest score as the final related-
ness score. For instance, there is a given word pair
?train and car?, car has 8 different synsets that build
8 different contexts, and train has 6 different synsets
that build 6 different contexts. We calculate the ESA
score of these 8 contexts of car to the 6 contexts of
train, and finally select the highest obtained score
from all of the 24 calculated scores.
4.2 Wikipedia-based Context Enrichment
In this method, the context is defined by the word
usage in Wikipedia articles. We retrieve top 5
Wikipedia articles by querying the articles? content,
and concatenate the short abstracts of the retrieved
articles to the given word to build the ESA vector.
Short abstract is the first two sentences of Wikipedia
article and has a maximum limit of 500 characters.
In order to retrieve the top 5 articles from Wikipedia
for a given word, we build an index of all Wikipedia
articles and use TF-IDF scores. We further explain
53
our implementation in Section 5.1.
4.3 WikiDefinition-based Context Enrichment
This method uses the definition of a given word from
Wikipedia. To obtain a definition from Wikipedia,
we first try to find a Wikipedia article on the given
word by matching the Wikipedia title. As definition,
we take the short abstract of the Wikipedia article.
For instance, for a given word ?train?, we take the
Wikipedia article with the title ?Train?
2
. If there is
no such Wikipedia article, then we use the previous
method ?Wikipedia-based Context Enrichment? to
get the context defining text for the given word. In
contrary to the previous method for defining context,
here we first try to get a more precise context as it
comes from the Wikipedia article on that word only.
After obtaining the definition, we concatenate it to
the given word to build the ESA vector. At the time
of experimentation, we were able to find 339 words
appearing as Wikipedia articles out of 437 unique
words in the WN353 dataset.
Figure 2: Effect of different types of context enrichments
on WN353 gold standard
2
http://en.wikipedia.org/wiki/Train
5 Experiment
5.1 ESA implementation
In this section, we describe the implementation of
ESA and the parameters used to build the model.
We build an index over all Wikipedia articles from
the pre-processed Wikipedia dump from November
11, 2005 (Gabrilovich, 2006). We use Lucene
3
to
build the index and retrieve the articles using TF-
IDF scores. As described in section 3, we build 20
different indices with different values of total num-
ber of articles (N).
5.2 Results and Discussion
To evaluate the effect of the aforementioned
approaches for context enrichment, we compare
the results obtained by them against the results
generated by ESA model as a baseline. We cal-
culated the scores on WN353 word pairs dataset
by using ESA, WordNet-based Context Enrich-
ment (ESA CEWN), Wikipedia-based Context
Enrichment (ESA CEWiki) and WikiDefition-based
Context Enrichment (ESA CEWikiDef). Further,
we examine the performance of context enrichment
approaches by reducing the total number of articles
taken to build the model. Figure 2 shows that the
proposed methods of context enrichment signifi-
cantly improve over the ESA scores for different
values of N.
Table 2 reports the results obtained by using
different context enrichments and ESA model.
It shows Spearman?s rank correlation on four
different values of N. All the proposed con-
text enrichment methods improve over the ESA
baseline scores. Context enrichments based on
Wikipedia outperforms the other methods, and
ESA CEWikiDef significantly improves over the
ESA baseline. Moreover, given a very less number
of Wikipedia articles used for building the model,
ESA CEWikiDef obtains a correlation score which
is considerably higher than the one obtained by
ESA baseline. ESA CEWN and ESA CEWiki can
include some unrelated context as they do not care
about the semantic sense of the given word, for
instance, for a given word ?car?, ESA CEWiki
3
https://lucene.apache.org/
54
K Total articles (N) ESA ESA CEWN ESA CEWiki ESA CEWikiDef
100 438,379 0.711 0.692 0.724 0.741
200 221,572 0.721 0.707 0.726 0.743
500 46,035 0.673 0.670 0.679 0.698
1000 10,647 0.563 0.593 0.598 0.614
Table 2: Spearman rank correlation scores on WN353 gold standard
includes the context about the word ?car? at surface
level rather than at the semantic level. However,
ESA CEWikiDef only includes the definition if it
does not refer to more than one semantic sense,
therefore, ESA CEWikiDef outperforms all other
types of context enrichment.
We achieved best results in all the cases by tak-
ing all the articles which has a minimum of 200
unique words (K=200). This indicates that further
increasing the value of K considerably decreases
the value of N, consequently, it harms the overall
distributional knowledge of the language, which is
the core of ESA model. However, decreasing the
value of K introduces very small Wikipedia articles
or stubs, which do not provide enough content on a
subject.
6 Conclusion
In this paper, we investigated the ESA performance
for three different types of text pairs: word-word,
phrase-phrase and document-document. We showed
that ESA scores varies significantly for word re-
latedness measure with the change in the number
(N) and length (?K which is the number of unique
words) of the Wikipedia articles used for building
the model. Further, we proposed context enrichment
approaches for improving word relatedness compu-
tation by ESA. To this end, we presented three dif-
ferent approaches: 1) WordNet-based, 2) Wikipedia-
based, and 3) WikiDefinition-based, and we real-
ized that concatenating the context defining text im-
proves the ESA performance for word relatedness
task.
Acknowledgments
This work has been funded in part by a research
grant from Science Foundation Ireland (SFI) under
Grant Number SFI/12/RC/2289 (INSIGHT) and by
the EU FP7 program in the context of the project
LIDER (610782).
References
Nitish Aggarwal and Paul Buitelaar. 2012. Query expan-
sion using wikipedia and dbpedia. In CLEF.
Nitish Aggarwal, Kartik Asooja, and Paul Buitelaar.
2012. DERI&UPM: Pushing corpus based relatedness
to similarity: Shared task system description. In Pro-
ceedings of the First Joint Conference on Lexical and
Computational Semantics - Volume 1: Proceedings of
the Main Conference and the Shared Task, and Volume
2: Proceedings of the Sixth International Workshop on
Semantic Evaluation, SemEval ?12, pages 643?647,
Stroudsburg, PA, USA. Association for Computational
Linguistics.
Eneko Agirre, Mona Diab, Daniel Cer, and Aitor
Gonzalez-Agirre. 2012. Semeval-2012 task 6: A pilot
on semantic textual similarity. In Proceedings of the
First Joint Conference on Lexical and Computational
Semantics-Volume 1: Proceedings of the main confer-
ence and the shared task, and Volume 2: Proceedings
of the Sixth International Workshop on Semantic Eval-
uation, pages 385?393. Association for Computational
Linguistics.
Lev Finkelstein, Evgeniy Gabrilovich, Yossi Matias,
Ehud Rivlin, Zach Solan, Gadi Wolfman, and Eytan
Ruppin. 2001. Placing search in context: The con-
cept revisited. In Proceedings of the 10th international
conference on World Wide Web, pages 406?414. ACM.
Evgeniy Gabrilovich and Shaul Markovitch. 2007. Com-
puting semantic relatedness using wikipedia-based
explicit semantic analysis. In Proceedings of the
20th international joint conference on Artifical intel-
ligence, IJCAI?07, pages 1606?1611, San Francisco,
CA, USA. Morgan Kaufmann Publishers Inc.
Evgeniy Gabrilovich. 2006. Feature generation for
textual information retrieval using world knowledge.
Ph.D. thesis, Technion - Israel Institute of Technology,
Haifa, Israel, December.
Thomas Gottron, Maik Anderka, and Benno Stein. 2011.
Insights into explicit semantic analysis. In Proceed-
ings of the 20th ACM international conference on In-
formation and knowledge management, pages 1961?
1964. ACM.
Zellig Harris. 1954. Distributional structure. In Word 10
(23), pages 146?162.
55
Samer Hassan and Rada Mihalcea. 2011. Semantic re-
latedness using salient semantic analysis. In AAAI.
Michael David Lee, BM Pincombe, and Matthew Brian
Welsh. 2005. An empirical evaluation of models of
text document similarity. Cognitive Science.
Patrick Pantel and Ariel Fuxman. 2011. Jigs and lures:
Associating web queries with structured entities. In
ACL, pages 83?92.
Tamara Polajnar, Nitish Aggarwal, Kartik Asooja, and
Paul Buitelaar. 2013. Improving esa with docu-
ment similarity. In Advances in Information Retrieval,
pages 582?593. Springer.
Philipp Scholl, Doreen B?ohnstedt, Renato Dom??nguez
Garc??a, Christoph Rensing, and Ralf Steinmetz. 2010.
Extended explicit semantic analysis for calculating
semantic relatedness of web resources. In Sustain-
ing TEL: From Innovation to Learning and Practice,
pages 324?339. Springer.
Philipp Sorg and Philipp Cimiano. 2010. An experi-
mental comparison of explicit semantic analysis im-
plementations for cross-language retrieval. In Natural
Language Processing and Information Systems, pages
36?48. Springer.
56
