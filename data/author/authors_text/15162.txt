Proceedings of the CoNLL Shared Task Session of EMNLP-CoNLL 2007, pp. 1051?1055,
Prague, June 2007. c?2007 Association for Computational Linguistics
Frustratingly Hard Domain Adaptation for Dependency Parsing
Mark Dredze1 and John Blitzer1 and Partha Pratim Talukdar1 and
Kuzman Ganchev1 and Joa?o V. Grac?a2 and Fernando Pereira1
1CIS Dept., University of Pennsylvania, Philadelphia, PA 19104
{mdredze|blitzer|partha|kuzman|pereira}@seas.upenn.edu
2L2F ? INESC-ID Lisboa/IST, Rua Alves Redol 9, 1000-029, Lisboa, Portugal
javg@l2f.inesc-id.pt
Abstract
We describe some challenges of adaptation
in the 2007 CoNLL Shared Task on Domain
Adaptation. Our error analysis for this task
suggests that a primary source of error is
differences in annotation guidelines between
treebanks. Our suspicions are supported by
the observation that no team was able to im-
prove target domain performance substan-
tially over a state of the art baseline.
1 Introduction
Dependency parsing, an important NLP task, can be
done with high levels of accuracy. However, adapt-
ing parsers to new domains without target domain
labeled training data remains an open problem. This
paper outlines our participation in the 2007 CoNLL
Shared Task on Domain Adaptation (Nivre et al,
2007). The goal was to adapt a parser trained on
a single source domain to a new target domain us-
ing only unlabeled data. We were given around
15K sentences of labeled text from the Wall Street
Journal (WSJ) (Marcus et al, 1993; Johansson and
Nugues, 2007) as well as 200K unlabeled sentences.
The development data was 200 sentences of labeled
biomedical oncology text (BIO, the ONCO portion
of the Penn Biomedical Treebank), as well as 200K
unlabeled sentences (Kulick et al, 2004). The two
test domains were a collection of medline chem-
istry abstracts (pchem, the CYP portion of the Penn
Biomedical Treebank) and the Child Language Data
Exchange System corpus (CHILDES) (MacWhin-
ney, 2000; Brown, 1973). We used the second or-
der two stage parser and edge labeler of McDonald
et al (2006), which achieved top results in the 2006
CoNLL-X shared task. Preliminary experiments in-
dicated that the edge labeler was fairly robust to do-
main adaptation, lowering accuracy by 3% in the de-
velopment domain as opposed to 2% in the source,
so we focused on unlabeled dependency parsing.
Our system did well, officially coming in 3rd
place out of 12 teams and within 1% of the top sys-
tem (Table 1). 1 In unlabeled parsing, we scored
1st and 2nd on CHILDES and pchem respectively.
However, our results were obtained without adap-
tation. Given our position in the ranking, this sug-
gests that no team was able to significantly improve
performance on either test domain beyond that of a
state-of-the-art parser.
After much effort in developing adaptation meth-
ods, it is critical to understand the causes of these
negative results. In what follows, we provide an er-
ror analysis that attributes domain loss for this task
to a difference in annotation guidelines between do-
mains. We then overview our attempts to improve
adaptation. While we were able to show limited
adaptation on reduced training data or with first-
order features, no modifications improved parsing
with all the training data and second-order features.
2 Parsing Challenges
We begin with an error analysis for adaptation be-
tween WSJ and BIO. We divided the available WSJ
data into a train and test set, trained a parser on
the train set and compared errors on the test set
and BIO. Accuracy dropped from 90% on WSJ to
84% on BIO. We then computed the fraction of er-
rors involving each POS tag. For the most common
1While only 8 teams participated in the closed track with us,
our score beat all of the teams in the open track.
1051
pchem l pchem ul childes ul bio ul
Ours 80.22 83.38 61.37 83.93
Best 81.06 83.42 61.37 -
Mean 73.03 76.42 57.89 -
Rank 3rd 2nd 1st -
Table 1: Official labeled (l) and other unlabeled (ul)
submitted results for the two test domains (pchem
and childes) and development data accuracy (bio).
The parser was trained on the provided WSJ data.
POS types, the loss (difference in source and tar-
get error) was: verbs (2%), conjunctions (5%), dig-
its (23%), prepositions (4%), adjectives (3%), de-
terminers (4%) and nouns (9%). 2 Two POS types
stand out: digits and nouns. Digits are less than
4% of the tokens in BIO. Errors result from the BIO
annotations for long sequences of digits which do
not appear in WSJ. Since these annotations are new
with respect to the WSJ guidelines, it is impossi-
ble to parse these without injecting knowledge of
the annotation guidelines. 3 Nouns are far more
common, comprising 33% of BIO and 30% of WSJ
tokens, the most popular POS tag by far. Addi-
tionally, other POS types listed above (adjectives,
prepositions, determiners, conjunctions) often attach
to nouns. To confirm that nouns were problem-
atic, we modified a first-order parser (no second or-
der features) by adding a feature indicating correct
noun-noun edges, forcing the parser to predict these
edges correctly. Adaptation performance rose on
BIO from 78% without the feature to 87% with the
feature. This indicates that most of the loss comes
from missing these edges.
The primary problem for nouns is the difference
between structures in each domain. The annota-
tion guidelines for the Penn Treebank flattened noun
phrases to simplify annotation (Marcus et al, 1993),
so there is no complex structure to NPs. Ku?bler
(2006) showed that it is difficult to compare the
Penn Treebank to other treebanks with more com-
plex noun structures, such as BIO. Consider theWSJ
phrase ?the New York State Insurance Department?.
The annotation indicates a flat structure, where ev-
2We measured these drops on several other dependency
parsers and found similar results.
3For example, the phrase ?(R = 28% (10/26); K=10% (3/29);
chi2 test: p=0.014).?
ery token is headed by ?Department?. In contrast,
a similar BIO phrase has a very different structure,
pursuant to the BIO guidelines. For ?the detoxi-
cation enzyme glutathione transferase P1-1?, ?en-
zyme? is the head of the NP, ?P1-1? is the head of
?transferase?, and ?transferase? is the head of ?glu-
tathione?. Since the guidelines differ, we observe no
corresponding structure in the WSJ. It is telling that
the parser labels this BIO example by attaching ev-
ery token to the final proper noun ?P1-1?, exactly as
the WSJ guidelines indicate. Unlabeled data cannot
indicate that BIO uses a different standard.
Another problem concerns appositives. For ex-
ample, the phrase ?Howard Mosher, president and
chief executive officer,? has ?Mosher? as the head
of ?Howard? and of the appositive NP delimited by
commas. While similar constructions occur in BIO,
there are no commas to indicate this. An example is
the above BIO NP, in which the phrase ?glutathione
transferase P1-1? is an appositive indicating which
?enzyme? is meant. However, since there are no
commas, the parser thinks ?P1-1? is the head. How-
ever, there are not many right to left attaching nouns.
In addition to a change in the annotation guide-
lines for NPs, we observed an important difference
in the distribution of POS tags. NN tags were almost
twice as likely in the BIO domain (14% in WSJ and
25% in BIO). NNP tags, which are close to 10% of
the tags in WSJ, are nonexistent in BIO (.24%). The
cause for this is clear when the annotation guide-
lines are considered. The proper nouns in WSJ are
names of companies, people and places, while in
BIO they are names of genes, proteins and chemi-
cals. However, for BIO these nouns are labeled NN
instead of NNP. This decision effectively removes
NNP from the BIO domain and renders all features
that depend on the NNP tag ineffective. In our above
BIO NP example, all nouns are labeled NN, whereas
the WSJ example contains NNP tags. The largest
tri-gram differences involve nouns, such as NN-NN-
NN, NNP-NNP-NNP, NN-IN-NN, and IN-NN-NN.
However, when we examine the coarse POS tags,
which do not distinguish between nouns, these dif-
ferences disappear. This indicates that while the
overall distribution of POS tags is similar between
the domains, the fine grained tags differ. These fine
grained tags provide more information than coarse
tags; experiments that removed fine grained tags
1052
hurt WSJ performance but did not affect BIO.
Finally, we examined the effect of unknown
words. Not surprisingly, the most significant dif-
ferences in error rates concerned dependencies be-
tween words of which one or both were unknown
to the parser. For two words that were seen in the
training data loss was 4%, for a single unknown
word loss was 15%, and 26% when both words were
unknown. Both words were unknown only 5% of
the time in BIO, while one of the words being un-
known was more common, reflecting 27% of deci-
sions. Upon further investigation, the majority of
unknown words were nouns, which indicates that
unknown word errors were caused by the problems
discussed above.
Recent theoretical work on domain adapta-
tion (Ben-David et al, 2006) attributes adaptation
loss to two sources: the difference in the distribu-
tion between domains and the difference in label-
ing functions. Adaptation techniques focus on the
former since it is impossible to determine the lat-
ter without knowledge of the labeling function. In
parsing adaptation, the former corresponds to a dif-
ference between the features seen in each domain,
such as new words in the target domain. The de-
cision function corresponds to differences between
annotation guidelines between two domains. Our er-
ror analysis suggests that the primary cause of loss
from adaptation is from differences in the annotation
guidelines themselves. Therefore, significant im-
provements cannot be made without specific knowl-
edge of the target domain?s annotation standards. No
amount of source training data can help if no rele-
vant structure exists in the data. Given the results
for the domain adaptation track, it appears no team
successfully adapted a state-of-the-art parser.
3 Adaptation Approaches
We survey the main approaches we explored for this
task. While some of these approaches provided a
modest performance boost to a simple parser (lim-
ited data and first-order features), no method added
any performance to our best parser (all data and
second-order features).
3.1 Features
A natural approach to improving parsing is to mod-
ify the feature set, both by removing features less
likely to transfer and by adding features that are
more likely to transfer. We began with the first ap-
proach and removed a large number of features that
we believed transfered poorly, such as most features
for noun-noun edges. We obtained a small improve-
ment in BIO performance on limited data only. We
then added several different types of features, specif-
ically designed to improve noun phrase construc-
tions, such as features based on the lexical position
of nouns (common position in NPs), frequency of
occurrence, and NP chunking information. For ex-
ample, trained on in-domain data, nouns that occur
more often tend to be heads. However, none of these
features transfered between domains.
A final type of feature we added was based on
the behavior of nouns, adjectives and verbs in each
domain. We constructed a feature representation
of words based on adjacent POS and words and
clustered words using an algorithm similar to that
of Saul and Pereira (1997). For example, our clus-
tering algorithm grouped first names in one group
and measurements in another. We then added the
cluster membership as a lexical feature to the parser.
None of the resulting features helped adaptation.
3.2 Diversity
Training diversity may be an effective source for
adaptation. We began by adding information from
multiple different parsers, which has been shown
to improve in-domain parsing. We added features
indicating when an edge was predicted by another
parser and if an edge crossed a predicted edge, as
well as conjunctions with edge types. This failed
to improve BIO accuracy since these features were
less reliable at test time. Next, we tried instance
bagging (Breiman, 1996) to generate some diversity
among parsers. We selected with replacement 2000
training examples from the training data and trained
three parsers. Each parser then tagged the remain-
ing 13K sentences, yielding 39K parsed sentences.
We then shuffled these sentences and trained a final
parser. This failed to improve performance, possibly
because of conflicting annotations or because of lack
of sufficient diversity. To address conflicting annota-
1053
tions, we added slack variables to the MIRA learn-
ing algorithm (Crammer et al, 2006) used to train
the parsers, without success. We measured diversity
by comparing the parses of each model. The dif-
ference in annotation agreement between the three
instance bagging parsers was about half the differ-
ence between these parsers and the gold annotations.
While we believe this is not enough diversity, it was
not feasible to repeat our experiment with a large
number of parsers.
3.3 Target Focused Learning
Another approach to adaptation is to favor training
examples that are similar to the target. We first mod-
ified the weight given by the parser to each training
sentence based on the similarity of the sentence to
target domain sentences. This can be done by mod-
ifying the loss to limit updates in cases where the
sentence does not reflect the target domain. We tried
a number of criteria to weigh sentences without suc-
cess, including sentence length and number of verbs.
Next, we trained a discriminative model on the pro-
vided unlabeled data to predict the domain of each
sentence based on POS n-grams in the sentence.
Training sentences with a higher probability of be-
ing in the target domain received higher weights,
also without success. Further experiments showed
that any decrease in training data hurt parser perfor-
mance. It would seem that the parser has no dif-
ficulty learning important training sentences in the
presence of unimportant training examples.
A related idea focused on words, weighing highly
tokens that appeared frequently in the target domain.
We scaled the loss associated with a token by a fac-
tor proportional to its frequency in the target do-
main. We found certain scaling techniques obtained
tiny improvements on the target domain that, while
significant compared to competition results, are not
statistically significant. We also attempted a sim-
ilar approach on the feature level. A very predic-
tive source domain feature is not useful if it does
not appear in the target domain. However, limiting
the feature space to target domain features had no
effect. Instead, we scaled each feature?s value by a
factor proportional to its frequency in the target do-
main and trained the parser on these scaled feature
values. We obtained small improvements on small
amounts of training data.
4 Future Directions
Given our pessimistic analysis and the long list of
failed methods, one may wonder if parser adapta-
tion is possible at all. We believe that it is. First,
there may be room for adaptation with our domains
if a common annotation scheme is used. Second,
we have stressed that typical adaptation, modifying
a model trained on the source domain, will fail but
there may be unsupervised parsing techniques that
improve performance after adaptation, such as a rule
based NP parser for BIO based on knowledge of the
annotations. However, this approach is unsatisfying
as it does not allow general purpose adaptation.
5 Acknowledgments
We thank Joel Wallenberg and Nikhil Dinesh for
their informative and helpful linguistic expertise,
Kevin Lerman for his edge labeler code, and Koby
Crammer for helpful conversations. Dredze is sup-
ported by a NDSEG fellowship; Ganchev and Taluk-
dar by NSF ITR EIA-0205448; and Blitzer by
DARPA under Contract No. NBCHD03001. Any
opinions, findings, and conclusions or recommen-
dations expressed in this material are those of the
author(s) and do not necessarily reflect the views of
the DARPA or the Department of Interior-National
Business Center (DOI-NBC).
References
Shai Ben-David, John Blitzer, Koby Crammer, and Fer-
nando Pereira. 2006. Analysis of representations for
domain adaptation. In NIPS.
Leo Breiman. 1996. Bagging predictors. Machine
Learning, 24(2):123?140.
R. Brown. 1973. A First Language: The Early Stages.
Harvard University Press.
Koby Crammer, Ofer Dekel, Joseph Keshet, Shai Shalev-
Shwartz, and Yoram Singer. 2006. Online passive-
aggressive algorithms. Journal of Machine Learning
Research, 7:551?585, Mar.
R. Johansson and P. Nugues. 2007. Extended
constituent-to-dependency conversion for English. In
Proc. of the 16th Nordic Conference on Computational
Linguistics (NODALIDA).
Sandra Ku?bler. 2006. How do treebank annotation
schemes influence parsing results? or how not to com-
pare apples and oranges. In RANLP.
1054
S. Kulick, A. Bies, M. Liberman, M. Mandel, R. Mc-
Donald, M. Palmer, A. Schein, and L. Ungar. 2004.
Integrated annotation for biomedical information ex-
traction. In Proc. of the Human Language Technol-
ogy Conference and the Annual Meeting of the North
American Chapter of the Association for Computa-
tional Linguistics (HLT/NAACL).
B. MacWhinney. 2000. The CHILDES Project: Tools
for Analyzing Talk. Lawrence Erlbaum.
M. Marcus, B. Santorini, and M. Marcinkiewicz. 1993.
Building a large annotated corpus of English: the Penn
Treebank. Computational Linguistics, 19(2):313?330.
Ryan McDonald, Kevin Lerman, and Fernando Pereira.
2006. Multilingual dependency parsing with a two-
stage discriminative parser. In Conference on Natural
Language Learning (CoNLL).
J. Nivre, J. Hall, S. Ku?bler, R. McDonald, J. Nils-
son, S. Riedel, and D. Yuret. 2007. The CoNLL
2007 shared task on dependency parsing. In Proc.
of the CoNLL 2007 Shared Task. Joint Conf. on Em-
pirical Methods in Natural Language Processing and
Computational Natural Language Learning (EMNLP-
CoNLL).
Lawrence Saul and Fernando Pereira. 1997. Aggre-
gate and mixed-order markov models for statistical
language modeling. In EMNLP.
1055
Online Learning of Approximate Dependency Parsing Algorithms
Ryan McDonald Fernando Pereira
Department of Computer and Information Science
University of Pennsylvania
Philadelphia, PA 19104
{ryantm,pereira}@cis.upenn.edu
Abstract
In this paper we extend the maximum
spanning tree (MST) dependency parsing
framework of McDonald et al (2005c)
to incorporate higher-order feature rep-
resentations and allow dependency struc-
tures with multiple parents per word.
We show that those extensions can make
the MST framework computationally in-
tractable, but that the intractability can be
circumvented with new approximate pars-
ing algorithms. We conclude with ex-
periments showing that discriminative on-
line learning using those approximate al-
gorithms achieves the best reported pars-
ing accuracy for Czech and Danish.
1 Introduction
Dependency representations of sentences (Hud-
son, 1984; Me?lc?uk, 1988) model head-dependent
syntactic relations as edges in a directed graph.
Figure 1 displays a dependency representation for
the sentence John hit the ball with the bat. This
sentence is an example of a projective (or nested)
tree representation, in which all edges can be
drawn in the plane with none crossing. Sometimes
a non-projective representations are preferred, as
in the sentence in Figure 2.1 In particular, for
freer-word order languages, non-projectivity is a
common phenomenon since the relative positional
constraints on dependents is much less rigid. The
dependency structures in Figures 1 and 2 satisfy
the tree constraint: they are weakly connected
graphs with a unique root node, and each non-root
node has a exactly one parent. Though trees are
1Examples are drawn from McDonald et al (2005c).
more common, some formalisms allow for words
to modify multiple parents (Hudson, 1984).
Recently, McDonald et al (2005c) have shown
that treating dependency parsing as the search
for the highest scoring maximum spanning tree
(MST) in a graph yields efficient algorithms for
both projective and non-projective trees. When
combined with a discriminative online learning al-
gorithm and a rich feature set, these models pro-
vide state-of-the-art performance across multiple
languages. However, the parsing algorithms re-
quire that the score of a dependency tree factors
as a sum of the scores of its edges. This first-order
factorization is very restrictive since it only allows
for features to be defined over single attachment
decisions. Previous work has shown that condi-
tioning on neighboring decisions can lead to sig-
nificant improvements in accuracy (Yamada and
Matsumoto, 2003; Charniak, 2000).
In this paper we extend the MST parsing frame-
work to incorporate higher-order feature represen-
tations of bounded-size connected subgraphs. We
also present an algorithm for acyclic dependency
graphs, that is, dependency graphs in which a
word may depend on multiple heads. In both cases
parsing is in general intractable and we provide
novel approximate algorithms to make these cases
tractable. We evaluate these algorithms within
an online learning framework, which has been
shown to be robust with respect approximate in-
ference, and describe experiments displaying that
these new models lead to state-of-the-art accuracy
for English and the best accuracy we know of for
Czech and Danish.
2 Maximum Spanning Tree Parsing
Dependency-tree parsing as the search for the
maximum spanning tree (MST) in a graph was
81
root John saw a dog yesterday which was a Yorkshire Terrier
Figure 2: An example non-projective dependency structure.
root
hit
John ball with
the bat
the
root0 John1 hit2 the3 ball4 with5 the6 bat7
Figure 1: An example dependency structure.
proposed byMcDonald et al (2005c). This formu-
lation leads to efficient parsing algorithms for both
projective and non-projective dependency trees
with the Eisner algorithm (Eisner, 1996) and the
Chu-Liu-Edmonds algorithm (Chu and Liu, 1965;
Edmonds, 1967) respectively. The formulation
works by defining the score of a dependency tree
to be the sum of edge scores,
s(x,y) =
?
(i,j)?y
s(i, j)
where x = x1 ? ? ? xn is an input sentence and y
a dependency tree for x. We can view y as a set
of tree edges and write (i, j) ? y to indicate an
edge in y from word xi to word xj . Consider the
example from Figure 1, where the subscripts index
the nodes of the tree. The score of this tree would
then be,
s(0, 2) + s(2, 1) + s(2, 4) + s(2, 5)
+ s(4, 3) + s(5, 7) + s(7, 6)
We call this first-order dependency parsing since
scores are restricted to a single edge in the depen-
dency tree. The score of an edge is in turn com-
puted as the inner product of a high-dimensional
feature representation of the edge with a corre-
sponding weight vector,
s(i, j) = w ? f(i, j)
This is a standard linear classifier in which the
weight vector w are the parameters to be learned
during training. We should note that f(i, j) can be
based on arbitrary features of the edge and the in-
put sequence x.
Given a directed graph G = (V,E), the maxi-
mum spanning tree (MST) problem is to find the
highest scoring subgraph of G that satisfies the
tree constraint over the vertices V . By defining
a graph in which the words in a sentence are the
vertices and there is a directed edge between all
words with a score as calculated above, McDon-
ald et al (2005c) showed that dependency pars-
ing is equivalent to finding the MST in this graph.
Furthermore, it was shown that this formulation
can lead to state-of-the-art results when combined
with discriminative learning algorithms.
Although the MST formulation applies to any
directed graph, our feature representations and one
of the parsing algorithms (Eisner?s) rely on a linear
ordering of the vertices, namely the order of the
words in the sentence.
2.1 Second-Order MST Parsing
Restricting scores to a single edge in a depen-
dency tree gives a very impoverished view of de-
pendency parsing. Yamada and Matsumoto (2003)
showed that keeping a small amount of parsing
history was crucial to improving parsing perfor-
mance for their locally-trained shift-reduce SVM
parser. It is reasonable to assume that other pars-
ing models might benefit from features over previ-
ous decisions.
Here we will focus on methods for parsing
second-order spanning trees. These models fac-
tor the score of the tree into the sum of adjacent
edge pair scores. To quantify this, consider again
the example from Figure 1. In the second-order
spanning tree model, the score would be,
s(0,?, 2) + s(2,?, 1) + s(2,?, 4) + s(2, 4, 5)
+ s(4,?, 3) + s(5,?, 7) + s(7,?, 6)
Here we use the second-order score function
s(i, k, j), which is the score of creating a pair of
adjacent edges, from word xi to words xk and xj .
For instance, s(2, 4, 5) is the score of creating the
edges from hit to with and from hit to ball. The
score functions are relative to the left or right of
the parent and we never score adjacent edges that
are on different sides of the parent (for instance,
82
there is no s(2, 1, 4) for the adjacent edges from
hit to John and ball). This independence between
left and right descendants allow us to use a O(n3)
second-order projective parsing algorithm, as we
will see later. We write s(xi,?, xj) when xj is
the first left or first right dependent of word xi.
For example, s(2,?, 4) is the score of creating a
dependency from hit to ball, since ball is the first
child to the right of hit. More formally, if the word
xi0 has the children shown in this picture,
xi0
xi1 . . . xij xij+1 . . . xim
the score factors as follows:
?j?1
k=1 s(i0, ik+1, ik) + s(i0,?, ij)
+ s(i0,?, ij+1) +
?m?1
k=j+1 s(i0, ik, ik+1)
This second-order factorization subsumes the
first-order factorization, since the score function
could just ignore the middle argument to simulate
first-order scoring. The score of a tree for second-
order parsing is now
s(x,y) =
?
(i,k,j)?y
s(i, k, j)
where k and j are adjacent, same-side children of
i in the tree y.
The second-order model allows us to condition
on the most recent parsing decision, that is, the last
dependent picked up by a particular word, which
is analogous to the the Markov conditioning of in
the Charniak parser (Charniak, 2000).
2.2 Exact Projective Parsing
For projective MST parsing, the first-order algo-
rithm can be extended to the second-order case, as
was noted by Eisner (1996). The intuition behind
the algorithm is shown graphically in Figure 3,
which displays both the first-order and second-
order algorithms. In the first-order algorithm, a
word will gather its left and right dependents in-
dependently by gathering each half of the subtree
rooted by its dependent in separate stages. By
splitting up chart items into left and right com-
ponents, the Eisner algorithm only requires 3 in-
dices to be maintained at each step, as discussed in
detail elsewhere (Eisner, 1996; McDonald et al,
2005b). For the second-order algorithm, the key
insight is to delay the scoring of edges until pairs
2-order-non-proj-approx(x, s)
Sentence x = x0 . . . xn, x0 = root
Weight function s : (i, k, j) ? R
1. Let y = 2-order-proj(x, s)
2. while true
3. m = ??, c = ?1, p = ?1
4. for j : 1 ? ? ?n
5. for i : 0 ? ? ?n
6. y? = y[i ? j]
7. if ?tree(y?) or ?k : (i, k, j) ? y continue
8. ? = s(x,y?) ? s(x,y)
9. if ? > m
10. m = ?, c = j, p = i
11. end for
12. end for
13. if m > 0
14. y = y[p ? c]
15. else return y
16. end while
Figure 4: Approximate second-order non-
projective parsing algorithm.
of dependents have been gathered. This allows for
the collection of pairs of adjacent dependents in
a single stage, which allows for the incorporation
of second-order scores, while maintaining cubic-
time parsing.
The Eisner algorithm can be extended to an
arbitrary mth-order model with a complexity of
O(nm+1), for m > 1. An mth-order parsing al-
gorithm will work similarly to the second-order al-
gorithm, except that we collect m pairs of adjacent
dependents in succession before attaching them to
their parent.
2.3 Approximate Non-projective Parsing
Unfortunately, second-order non-projective MST
parsing is NP-hard, as shown in appendix A. To
circumvent this, we designed an approximate al-
gorithm based on the exact O(n3) second-order
projective Eisner algorithm. The approximation
works by first finding the highest scoring projec-
tive parse. It then rearranges edges in the tree,
one at a time, as long as such rearrangements in-
crease the overall score and do not violate the tree
constraint. We can easily motivate this approxi-
mation by observing that even in non-projective
languages like Czech and Danish, most trees are
primarily projective with just a few non-projective
edges (Nivre and Nilsson, 2005). Thus, by start-
ing with the highest scoring projective tree, we are
typically only a small number of transformations
away from the highest scoring non-projective tree.
The algorithm is shown in Figure 4. The ex-
pression y[i ? j] denotes the dependency graph
identical to y except that xi?s parent is xi instead
83
FIRST-ORDER
h1
h3
?
h1 r r+1 h3
(A)
h1
h3
h1 h3
(B)
SECOND-ORDER
h1
h2 h2 h3
?
h1 h2 h2 r r+1 h3
(A)
h1
h2 h2 h3
?
h1 h2 h2 h3
(B)
h1
h3
h1 h3
(C)
Figure 3: A O(n3) extension of the Eisner algorithm to second-order dependency parsing. This figure
shows how h1 creates a dependency to h3 with the second-order knowledge that the last dependent of
h1 was h2. This is done through the creation of a sibling item in part (B). In the first-order model, the
dependency to h3 is created after the algorithm has forgotten that h2 was the last dependent.
of what it was in y. The test tree(y) is true iff the
dependency graph y satisfies the tree constraint.
In more detail, line 1 of the algorithm sets y to
the highest scoring second-order projective tree.
The loop of lines 2?16 exits only when no fur-
ther score improvement is possible. Each iteration
seeks the single highest-scoring parent change to
y that does not break the tree constraint. To that
effect, the nested loops starting in lines 4 and 5
enumerate all (i, j) pairs. Line 6 sets y ? to the de-
pendency graph obtained from y by changing xj?s
parent to xi. Line 7 checks that the move from y
to y? is valid by testing that xj?s parent was not al-
ready xi and that y? is a tree. Line 8 computes the
score change from y to y?. If this change is larger
than the previous best change, we record how this
new tree was created (lines 9-10). After consid-
ering all possible valid edge changes to the tree,
the algorithm checks to see that the best new tree
does have a higher score. If that is the case, we
change the tree permanently and re-enter the loop.
Otherwise we exit since there are no single edge
switches that can improve the score.
This algorithm allows for the introduction of
non-projective edges because we do not restrict
any of the edge changes except to maintain the
tree property. In fact, if any edge change is ever
made, the resulting tree is guaranteed to be non-
projective, otherwise there would have been a
higher scoring projective tree that would have al-
ready been found by the exact projective parsing
algorithm. It is not difficult to find examples for
which this approximation will terminate without
returning the highest-scoring non-projective parse.
It is clear that this approximation will always
terminate ? there are only a finite number of de-
pendency trees for any given sentence and each it-
eration of the loop requires an increase in score
to continue. However, the loop could potentially
take exponential time, so we will bound the num-
ber of edge transformations to a fixed value M .
It is easy to argue that this will not hurt perfor-
mance. Even in freer-word order languages such
as Czech, almost all non-projective dependency
trees are primarily projective, modulo a few non-
projective edges. Thus, if our inference algorithm
starts with the highest scoring projective parse, the
best non-projective parse only differs by a small
number of edge transformations. Furthermore, it
is easy to show that each iteration of the loop takes
O(n2) time, resulting in a O(n3 + Mn2) runtime
algorithm. In practice, the approximation termi-
nates after a small number of transformations and
we do not need to bound the number of iterations
in our experiments.
We should note that this is one of many possible
approximations we could have made. Another rea-
sonable approach would be to first find the highest
scoring first-order non-projective parse, and then
re-arrange edges based on second order scores in
a similar manner to the algorithm we described.
We implemented this method and found that the
results were slightly worse.
3 Danish: Parsing Secondary Parents
Kromann (2001) argued for a dependency formal-
ism called Discontinuous Grammar and annotated
a large set of Danish sentences using this formal-
ism to create the Danish Dependency Treebank
(Kromann, 2003). The formalism allows for a
84
root Han spejder efter og ser elefanterne
He looks for and sees elephants
Figure 5: An example dependency tree from
the Danish Dependency Treebank (from Kromann
(2003)).
word to have multiple parents. Examples include
verb coordination in which the subject or object is
an argument of several verbs, and relative clauses
in which words must satisfy dependencies both in-
side and outside the clause. An example is shown
in Figure 5 for the sentence He looks for and sees
elephants. Here, the pronoun He is the subject for
both verbs in the sentence, and the noun elephants
the corresponding object. In the Danish Depen-
dency Treebank, roughly 5% of words have more
than one parent, which breaks the single parent
(or tree) constraint we have previously required
on dependency structures. Kromann also allows
for cyclic dependencies, though we deal only with
acyclic dependency graphs here. Though less
common than trees, dependency graphs involving
multiple parents are well established in the litera-
ture (Hudson, 1984). Unfortunately, the problem
of finding the dependency structure with highest
score in this setting is intractable (Chickering et
al., 1994).
To create an approximate parsing algorithm
for dependency structures with multiple parents,
we start with our approximate second-order non-
projective algorithm outlined in Figure 4. We use
the non-projective algorithm since the Danish De-
pendency Treebank contains a small number of
non-projective arcs. We then modify lines 7-10
of this algorithm so that it looks for the change in
parent or the addition of a new parent that causes
the highest change in overall score and does not
create a cycle2. Like before, we make one change
per iteration and that change will depend on the
resulting score of the new tree. Using this sim-
ple new approximate parsing algorithm, we train a
new parser that can produce multiple parents.
4 Online Learning and Approximate
Inference
In this section, we review the work of McDonald
et al (2005b) for online large-margin dependency
2We are not concerned with violating the tree constraint.
parsing. As usual for supervised learning, we as-
sume a training set T = {(xt,yt)}Tt=1, consist-
ing of pairs of a sentence xt and its correct depen-
dency representation yt.
The algorithm is an extension of the Margin In-
fused Relaxed Algorithm (MIRA) (Crammer and
Singer, 2003) to learning with structured outputs,
in the present case dependency structures. Fig-
ure 6 gives pseudo-code for the algorithm. An on-
line learning algorithm considers a single training
instance for each update to the weight vector w.
We use the common method of setting the final
weight vector as the average of the weight vec-
tors after each iteration (Collins, 2002), which has
been shown to alleviate overfitting.
On each iteration, the algorithm considers a
single training instance. We parse this instance
to obtain a predicted dependency graph, and find
the smallest-norm update to the weight vector w
that ensures that the training graph outscores the
predicted graph by a margin proportional to the
loss of the predicted graph relative to the training
graph, which is the number of words with incor-
rect parents in the predicted tree (McDonald et al,
2005b). Note that we only impose margin con-
straints between the single highest-scoring graph
and the correct graph relative to the current weight
setting. Past work on tree-structured outputs has
used constraints for the k-best scoring tree (Mc-
Donald et al, 2005b) or even all possible trees by
using factored representations (Taskar et al, 2004;
McDonald et al, 2005c). However, we have found
that a single margin constraint per example leads
to much faster training with a negligible degrada-
tion in performance. Furthermore, this formula-
tion relates learning directly to inference, which is
important, since we want the model to set weights
relative to the errors made by an approximate in-
ference algorithm. This algorithm can thus be
viewed as a large-margin version of the perceptron
algorithm for structured outputs Collins (2002).
Online learning algorithms have been shown
to be robust even with approximate rather than
exact inference in problems such as word align-
ment (Moore, 2005), sequence analysis (Daume?
and Marcu, 2005; McDonald et al, 2005a)
and phrase-structure parsing (Collins and Roark,
2004). This robustness to approximations comes
from the fact that the online framework sets
weights with respect to inference. In other words,
the learning method sees common errors due to
85
Training data: T = {(xt,yt)}Tt=1
1. w(0) = 0; v = 0; i = 0
2. for n : 1..N
3. for t : 1..T
4. min
?
?
?
w(i+1) ? w(i)
?
?
?
s.t. s(xt,yt; w(i+1))
?s(xt,y?; w(i+1)) ? L(yt,y?)
where y? = arg maxy? s(xt,y?; w(i))
5. v = v + w(i+1)
6. i = i + 1
7. w = v/(N ? T )
Figure 6: MIRA learning algorithm. We write
s(x,y; w(i)) to mean the score of tree y using
weight vector w(i).
approximate inference and adjusts weights to cor-
rect for them. The work of Daume? and Marcu
(2005) formalizes this intuition by presenting an
online learning framework in which parameter up-
dates are made directly with respect to errors in the
inference algorithm. We show in the next section
that this robustness extends to approximate depen-
dency parsing.
5 Experiments
The score of adjacent edges relies on the defini-
tion of a feature representation f(i, k, j). As noted
earlier, this representation subsumes the first-order
representation of McDonald et al (2005b), so we
can incorporate all of their features as well as the
new second-order features we now describe. The
old first-order features are built from the parent
and child words, their POS tags, and the POS tags
of surrounding words and those of words between
the child and the parent, as well as the direction
and distance from the parent to the child. The
second-order features are built from the following
conjunctions of word and POS identity predicates
xi-pos, xk-pos, xj-pos
xk-pos, xj-pos
xk-word, xj-word
xk-word, xj-pos
xk-pos, xj-word
where xi-pos is the part-of-speech of the ith word
in the sentence. We also include conjunctions be-
tween these features and the direction and distance
from sibling j to sibling k. We determined the use-
fulness of these features on the development set,
which also helped us find out that features such as
the POS tags of words between the two siblings
would not improve accuracy. We also ignored fea-
English
Accuracy Complete
1st-order-projective 90.7 36.7
2nd-order-projective 91.5 42.1
Table 1: Dependency parsing results for English.
Czech
Accuracy Complete
1st-order-projective 83.0 30.6
2nd-order-projective 84.2 33.1
1st-order-non-projective 84.1 32.2
2nd-order-non-projective 85.2 35.9
Table 2: Dependency parsing results for Czech.
tures over triples of words since this would ex-
plode the size of the feature space.
We evaluate dependencies on per word accu-
racy, which is the percentage of words in the sen-
tence with the correct parent in the tree, and on
complete dependency analysis. In our evaluation
we exclude punctuation for English and include it
for Czech and Danish, which is the standard.
5.1 English Results
To create data sets for English, we used the Ya-
mada and Matsumoto (2003) head rules to ex-
tract dependency trees from the WSJ, setting sec-
tions 2-21 as training, section 22 for development
and section 23 for evaluation. The models rely
on part-of-speech tags as input and we used the
Ratnaparkhi (1996) tagger to provide these for
the development and evaluation set. These data
sets are exclusively projective so we only com-
pare the projective parsers using the exact projec-
tive parsing algorithms. The purpose of these ex-
periments is to gauge the overall benefit from in-
cluding second-order features with exact parsing
algorithms, which can be attained in the projective
setting. Results are shown in Table 1. We can see
that there is clearly an advantage in introducing
second-order features. In particular, the complete
tree metric is improved considerably.
5.2 Czech Results
For the Czech data, we used the predefined train-
ing, development and testing split of the Prague
Dependency Treebank (Hajic? et al, 2001), and the
automatically generated POS tags supplied with
the data, which we reduce to the POS tag set
from Collins et al (1999). On average, 23% of
the sentences in the training, development and
test sets have at least one non-projective depen-
dency, though, less than 2% of total edges are ac-
86
Danish
Precision Recall F-measure
2nd-order-projective 86.4 81.7 83.9
2nd-order-non-projective 86.9 82.2 84.4
2nd-order-non-projective w/ multiple parents 86.2 84.9 85.6
Table 3: Dependency parsing results for Danish.
tually non-projective. Results are shown in Ta-
ble 2. McDonald et al (2005c) showed a substan-
tial improvement in accuracy by modeling non-
projective edges in Czech, shown by the difference
between two first-order models. Table 2 shows
that a second-order model provides a compara-
ble accuracy boost, even using an approximate
non-projective algorithm. The second-order non-
projective model accuracy of 85.2% is the highest
reported accuracy for a single parser for these data.
Similar results were obtained by Hall and No?va?k
(2005) (85.1% accuracy) who take the best out-
put of the Charniak parser extended to Czech and
rerank slight variations on this output that intro-
duce non-projective edges. However, this system
relies on a much slower phrase-structure parser
as its base model as well as an auxiliary rerank-
ing module. Indeed, our second-order projective
parser analyzes the test set in 16m32s, and the
non-projective approximate parser needs 17m03s
to parse the entire evaluation set, showing that run-
time for the approximation is completely domi-
nated by the initial call to the second-order pro-
jective algorithm and that the post-process edge
transformation loop typically only iterates a few
times per sentence.
5.3 Danish Results
For our experiments we used the Danish Depen-
dency Treebank v1.0. The treebank contains a
small number of inter-sentence and cyclic depen-
dencies and we removed all sentences that con-
tained such structures. The resulting data set con-
tained 5384 sentences. We partitioned the data
into contiguous 80/20 training/testing splits. We
held out a subset of the training data for develop-
ment purposes.
We compared three systems, the standard
second-order projective and non-projective pars-
ing models, as well as our modified second-order
non-projective model that allows for the introduc-
tion of multiple parents (Section 3). All systems
use gold-standard part-of-speech since no trained
tagger is readily available for Danish. Results are
shown in Figure 3. As might be expected, the non-
projective parser does slightly better than the pro-
jective parser because around 1% of the edges are
non-projective. Since each word may have an ar-
bitrary number of parents, we must use precision
and recall rather than accuracy to measure perfor-
mance. This also means that the correct training
loss is no longer the Hamming loss. Instead, we
use false positives plus false negatives over edge
decisions, which balances precision and recall as
our ultimate performance metric.
As expected, for the basic projective and non-
projective parsers, recall is roughly 5% lower than
precision since these models can only pick up at
most one parent per word. For the parser that can
introduce multiple parents, we see an increase in
recall of nearly 3% absolute with a slight drop in
precision. These results are very promising and
further show the robustness of discriminative on-
line learning with approximate parsing algorithms.
6 Discussion
We described approximate dependency parsing al-
gorithms that support higher-order features and
multiple parents. We showed that these approxi-
mations can be combined with online learning to
achieve fast parsing with competitive parsing ac-
curacy. These results show that the gain from al-
lowing richer representations outweighs the loss
from approximate parsing and further shows the
robustness of online learning algorithms with ap-
proximate inference.
The approximations we have presented are very
simple. They start with a reasonably good baseline
and make small transformations until the score
of the structure converges. These approximations
work because freer-word order languages we stud-
ied are still primarily projective, making the ap-
proximate starting point close to the goal parse.
However, we would like to investigate the benefits
for parsing of more principled approaches to ap-
proximate learning and inference techniques such
as the learning as search optimization framework
of (Daume? and Marcu, 2005). This framework
will possibly allow us to include effectively more
global features over the dependency structure than
87
those in our current second-order model.
Acknowledgments
This work was supported by NSF ITR grants
0205448.
References
E. Charniak. 2000. A maximum-entropy-inspired
parser. In Proc. NAACL.
D.M. Chickering, D. Geiger, and D. Heckerman. 1994.
Learning bayesian networks: The combination of
knowledge and statistical data. Technical Report
MSR-TR-94-09, Microsoft Research.
Y.J. Chu and T.H. Liu. 1965. On the shortest arbores-
cence of a directed graph. Science Sinica, 14:1396?
1400.
M. Collins and B. Roark. 2004. Incremental parsing
with the perceptron algorithm. In Proc. ACL.
M. Collins, J. Hajic?, L. Ramshaw, and C. Tillmann.
1999. A statistical parser for Czech. In Proc. ACL.
M. Collins. 2002. Discriminative training methods
for hidden Markov models: Theory and experiments
with perceptron algorithms. In Proc. EMNLP.
K. Crammer and Y. Singer. 2003. Ultraconservative
online algorithms for multiclass problems. JMLR.
H. Daum?e and D. Marcu. 2005. Learning as search op-
timization: Approximate large margin methods for
structured prediction. In Proc. ICML.
J. Edmonds. 1967. Optimum branchings. Journal
of Research of the National Bureau of Standards,
71B:233?240.
J. Eisner. 1996. Three new probabilistic models for
dependency parsing: An exploration. In Proc. COL-
ING.
J. Hajic?, E. Hajicova, P. Pajas, J. Panevova, P. Sgall, and
B. Vidova Hladka. 2001. The Prague Dependency
Treebank 1.0 CDROM. Linguistics Data Consor-
tium Cat. No. LDC2001T10.
K. Hall and V. N?ov?ak. 2005. Corrective modeling for
non-projective dependency parsing. In Proc. IWPT.
R. Hudson. 1984. Word Grammar. Blackwell.
M. T. Kromann. 2001. Optimaility parsing and local
cost functions in discontinuous grammars. In Proc.
FG-MOL.
M. T. Kromann. 2003. The danish dependency tree-
bank and the dtag treebank tool. In Proc. TLT.
R. McDonald, K. Crammer, and F. Pereira. 2005a.
Flexible text segmentation with structured multil-
abel classifi cation. In Proc. HLT-EMNLP.
R. McDonald, K. Crammer, and F. Pereira. 2005b. On-
line large-margin training of dependency parsers. In
Proc. ACL.
R. McDonald, F. Pereira, K. Ribarov, and J. Hajic?.
2005c. Non-projective dependency parsing using
spanning tree algorithms. In Proc. HLT-EMNLP.
I.A. Me?lc?uk. 1988. Dependency Syntax: Theory and
Practice. State University of New York Press.
R. Moore. 2005. A discriminative framework for bilin-
gual word alignment. In Proc. HLT-EMNLP.
J. Nivre and J. Nilsson. 2005. Pseudo-projective de-
pendency parsing. In Proc. ACL.
A. Ratnaparkhi. 1996. A maximum entropy model for
part-of-speech tagging. In Proc. EMNLP.
B. Taskar, D. Klein, M. Collins, D. Koller, and C. Man-
ning. 2004. Max-margin parsing. In Proc. EMNLP.
H. Yamada and Y. Matsumoto. 2003. Statistical de-
pendency analysis with support vector machines. In
Proc. IWPT.
A 2nd-Order Non-projective MST
Parsing is NP-hard
Proof by a reduction from 3-D matching (3DM).
3DM: Disjoint sets X,Y,Z each withm distinct elements
and a set T ? X?Y ?Z. Question: is there a subset S ? T
such that |S| = m and each v ? X?Y ?Z occurs in exactly
one element of S.
Reduction: Given an instance of 3DM we defi ne a graph
in which the vertices are the elements from X ? Y ? Z as
well as an artifi cial root node. We insert edges from root to
all xi ? X as well as edges from all xi ? X to all yi ? Y
and zi ? Z. We order the words s.t. the root is on the left
followed by all elements of X, then Y , and fi nally Z. We
then defi ne the second-order score function as follows,
s(root, xi, xj) = 0, ?xi, xj ? X
s(xi,?, yj) = 0, ?xi ? X, yj ? Y
s(xi, yj , zk) = 1, ?(xi, yj , zk) ? T
All other scores are defi ned to be ??, including for edges
pairs that were not defi ned in the original graph.
Theorem: There is a 3D matching iff the second-order
MST has a score of m. Proof: First we observe that no tree
can have a score greater thanm since that would require more
than m pairs of edges of the form (xi, yj , zk). This can only
happen when some xi has multiple yj ? Y children or mul-
tiple zk ? Z children. But if this were true then we would
introduce a?? scored edge pair (e.g. s(xi, yj , y?j)). Now, if
the highest scoring second-order MST has a score of m, that
means that every xi must have found a unique pair of chil-
dren yj and zk which represents the 3D matching, since there
would be m such triples. Furthermore, yj and zk could not
match with any other x?i since they can only have one incom-
ing edge in the tree. On the other hand, if there is a 3DM, then
there must be a tree of weight m consisting of second-order
edges (xi, yj , zk) for each element of the matching S. Since
no tree can have a weight greater than m, this must be the
highest scoring second-order MST. Thus if we can fi nd the
highest scoring second-order MST in polynomial time, then
3DM would also be solvable. 
88
Proceedings of Human Language Technology Conference and Conference on Empirical Methods in Natural Language
Processing (HLT/EMNLP), pages 523?530, Vancouver, October 2005. c?2005 Association for Computational Linguistics
Non-projective Dependency Parsing using Spanning Tree Algorithms
Ryan McDonald Fernando Pereira
Department of Computer and Information Science
University of Pennsylvania
{ryantm,pereira}@cis.upenn.edu
Kiril Ribarov Jan Hajic?
Institute of Formal and Applied Linguistics
Charles University
{ribarov,hajic}@ufal.ms.mff.cuni.cz
Abstract
We formalize weighted dependency pars-
ing as searching for maximum spanning
trees (MSTs) in directed graphs. Using
this representation, the parsing algorithm
of Eisner (1996) is sufficient for search-
ing over all projective trees in O(n3) time.
More surprisingly, the representation is
extended naturally to non-projective pars-
ing using Chu-Liu-Edmonds (Chu and
Liu, 1965; Edmonds, 1967) MST al-
gorithm, yielding an O(n2) parsing al-
gorithm. We evaluate these methods
on the Prague Dependency Treebank us-
ing online large-margin learning tech-
niques (Crammer et al, 2003; McDonald
et al, 2005) and show that MST parsing
increases efficiency and accuracy for lan-
guages with non-projective dependencies.
1 Introduction
Dependency parsing has seen a surge of inter-
est lately for applications such as relation extrac-
tion (Culotta and Sorensen, 2004), machine trans-
lation (Ding and Palmer, 2005), synonym genera-
tion (Shinyama et al, 2002), and lexical resource
augmentation (Snow et al, 2004). The primary
reasons for using dependency structures instead of
more informative lexicalized phrase structures is
that they are more efficient to learn and parse while
still encoding much of the predicate-argument infor-
mation needed in applications.
root John hit the ball with the bat
Figure 1: An example dependency tree.
Dependency representations, which link words to
their arguments, have a long history (Hudson, 1984).
Figure 1 shows a dependency tree for the sentence
John hit the ball with the bat. We restrict ourselves
to dependency tree analyses, in which each word de-
pends on exactly one parent, either another word or a
dummy root symbol as shown in the figure. The tree
in Figure 1 is projective, meaning that if we put the
words in their linear order, preceded by the root, the
edges can be drawn above the words without cross-
ings, or, equivalently, a word and its descendants
form a contiguous substring of the sentence.
In English, projective trees are sufficient to ana-
lyze most sentence types. In fact, the largest source
of English dependency trees is automatically gener-
ated from the Penn Treebank (Marcus et al, 1993)
and is by convention exclusively projective. How-
ever, there are certain examples in which a non-
projective tree is preferable. Consider the sentence
John saw a dog yesterday which was a Yorkshire Ter-
rier. Here the relative clause which was a Yorkshire
Terrier and the object it modifies (the dog) are sep-
arated by an adverb. There is no way to draw the
dependency tree for this sentence in the plane with
no crossing edges, as illustrated in Figure 2. In lan-
guages with more flexible word order than English,
such as German, Dutch and Czech, non-projective
dependencies are more frequent. Rich inflection
systems reduce reliance on word order to express
523
root John saw a dog yesterday which was a Yorkshire Terrier
root O to nove? ve?ts?inou nema? ani za?jem a taky na to ve?ts?inou nema? pen??ze
He is mostly not even interested in the new things and in most cases, he has no money for it either.
Figure 2: Non-projective dependency trees in English and Czech.
grammatical relations, allowing non-projective de-
pendencies that we need to represent and parse ef-
ficiently. A non-projective example from the Czech
Prague Dependency Treebank (Hajic? et al, 2001) is
also shown in Figure 2.
Most previous dependency parsing models have
focused on projective trees, including the work
of Eisner (1996), Collins et al (1999), Yamada and
Matsumoto (2003), Nivre and Scholz (2004), and
McDonald et al (2005). These systems have shown
that accurate projective dependency parsers can be
automatically learned from parsed data. However,
non-projective analyses have recently attracted some
interest, not only for languages with freer word order
but also for English. In particular, Wang and Harper
(2004) describe a broad coverage non-projective
parser for English based on a hand-constructed con-
straint dependency grammar rich in lexical and syn-
tactic information. Nivre and Nilsson (2005) pre-
sented a parsing model that allows for the introduc-
tion of non-projective edges into dependency trees
through learned edge transformations within their
memory-based parser. They test this system on
Czech and show improved accuracy relative to a pro-
jective parser. Our approach differs from those ear-
lier efforts in searching optimally and efficiently the
full space of non-projective trees.
The main idea of our method is that dependency
parsing can be formalized as the search for a maxi-
mum spanning tree in a directed graph. This formal-
ization generalizes standard projective parsing mod-
els based on the Eisner algorithm (Eisner, 1996) to
yield efficient O(n2) exact parsing methods for non-
projective languages like Czech. Using this span-
ning tree representation, we extend the work of Mc-
Donald et al (2005) on online large-margin discrim-
inative training methods to non-projective depen-
dencies.
The present work is related to that of Hirakawa
(2001) who, like us, reduces the problem of depen-
dency parsing to spanning tree search. However, his
parsing method uses a branch and bound algorithm
that is exponential in the worst case, even though
it appears to perform reasonably in limited experi-
ments. Furthermore, his work does not adequately
address learning or measure parsing accuracy on
held-out data.
Section 2 describes an edge-based factorization
of dependency trees and uses it to equate depen-
dency parsing to the problem of finding maximum
spanning trees in directed graphs. Section 3 out-
lines the online large-margin learning framework
used to train our dependency parsers. Finally, in
Section 4 we present parsing results for Czech. The
trees in Figure 1 and Figure 2 are untyped, that
is, edges are not partitioned into types representing
additional syntactic information such as grammati-
cal function. We study untyped dependency trees
mainly, but edge types can be added with simple ex-
tensions to the methods discussed here.
2 Dependency Parsing and Spanning Trees
2.1 Edge Based Factorization
In what follows, x = x1 ? ? ? xn represents a generic
input sentence, and y represents a generic depen-
dency tree for sentence x. Seeing y as the set of tree
edges, we write (i, j) ? y if there is a dependency
in y from word xi to word xj .
In this paper we follow a common method of fac-
toring the score of a dependency tree as the sum of
the scores of all edges in the tree. In particular, we
define the score of an edge to be the dot product be-
524
tween a high dimensional feature representation of
the edge and a weight vector,
s(i, j) = w ? f(i, j)
Thus the score of a dependency tree y for sentence
x is,
s(x,y) =
?
(i,j)?y
s(i, j) =
?
(i,j)?y
w ? f(i, j)
Assuming an appropriate feature representation as
well as a weight vector w, dependency parsing is the
task of finding the dependency tree y with highest
score for a given sentence x.
For the rest of this section we assume that the
weight vector w is known and thus we know the
score s(i, j) of each possible edge. In Section 3 we
present a method for learning the weight vector.
2.2 Maximum Spanning Trees
We represent the generic directed graph G = (V,E)
by its vertex set V = {v1, . . . , vn} and set E ? [1 :
n]? [1 : n] of pairs (i, j) of directed edges vi ? vj .
Each such edge has a score s(i, j). Since G is di-
rected, s(i, j) does not necessarily equal s(j, i). A
maximum spanning tree (MST) of G is a tree y ? E
that maximizes the value
?
(i,j)?y s(i, j) such that
every vertex in V appears in y. The maximum pro-
jective spanning tree of G is constructed similarly
except that it can only contain projective edges rel-
ative to some total order on the vertices of G. The
MST problem for directed graphs is also known as
the maximum arborescence problem.
For each sentence x we define the directed graph
Gx = (Vx, Ex) given by
Vx = {x0 = root, x1, . . . , xn}
Ex = {(i, j) : i 6= j, (i, j) ? [0 : n] ? [1 : n]}
That is, Gx is a graph with the sentence words and
the dummy root symbol as vertices and a directed
edge between every pair of distinct words and from
the root symbol to every word. It is clear that de-
pendency trees for x and spanning trees for Gx co-
incide, since both kinds of trees are required to be
rooted at the dummy root and reach all the words
in the sentence. Hence, finding a (projective) depen-
dency tree with highest score is equivalent to finding
a maximum (projective) spanning tree in Gx.
Chu-Liu-Edmonds(G, s)
Graph G = (V, E)
Edge weight function s : E ? R
1. Let M = {(x?, x) : x ? V, x? = arg maxx? s(x?, x)}
2. Let GM = (V, M)
3. If GM has no cycles, then it is an MST: return GM
4. Otherwise, find a cycle C in GM
5. Let GC = contract(G, C, s)
6. Let y = Chu-Liu-Edmonds(GC , s)
7. Find a vertex x ? C s. t. (x?, x) ? y, (x??, x) ? C
8. return y ? C ? {(x??, x)}
contract(G = (V, E), C, s)
1. Let GC be the subgraph of G excluding nodes in C
2. Add a node c to GC representing cycle C
3. For x ? V ? C : ?x??C(x?, x) ? E
Add edge (c, x) to GC with
s(c, x) = maxx??C s(x?, x)
4. For x ? V ? C : ?x??C(x, x?) ? E
Add edge (x, c) to GC with
s(x, c) = maxx??C [s(x, x?) ? s(a(x?), x?) + s(C)]
where a(v) is the predecessor of v in C
and s(C) = Pv?C s(a(v), v)
5. return GC
Figure 3: Chu-Liu-Edmonds algorithm for finding
maximum spanning trees in directed graphs.
2.2.1 Non-projective Trees
To find the highest scoring non-projective tree we
simply search the entire space of spanning trees with
no restrictions. Well-known algorithms exist for the
less general case of finding spanning trees in undi-
rected graphs (Cormen et al, 1990).
Efficient algorithms for the directed case are less
well known, but they exist. We will use here the
Chu-Liu-Edmonds algorithm (Chu and Liu, 1965;
Edmonds, 1967), sketched in Figure 3 follow-
ing Leonidas (2003). Informally, the algorithm has
each vertex in the graph greedily select the incoming
edge with highest weight. If a tree results, it must be
the maximum spanning tree. If not, there must be a
cycle. The procedure identifies a cycle and contracts
it into a single vertex and recalculates edge weights
going into and out of the cycle. It can be shown that
a maximum spanning tree on the contracted graph is
equivalent to a maximum spanning tree in the orig-
inal graph (Leonidas, 2003). Hence the algorithm
can recursively call itself on the new graph. Naively,
this algorithm runs in O(n3) time since each recur-
sive call takes O(n2) to find the highest incoming
edge for each word and to contract the graph. There
are at most O(n) recursive calls since we cannot
contract the graph more then n times. However,
525
Tarjan (1977) gives an efficient implementation of
the algorithm with O(n2) time complexity for dense
graphs, which is what we need here.
To find the highest scoring non-projective tree for
a sentence, x, we simply construct the graph Gx
and run it through the Chu-Liu-Edmonds algorithm.
The resulting spanning tree is the best non-projective
dependency tree. We illustrate here the application
of the Chu-Liu-Edmonds algorithm to dependency
parsing on the simple example x = John saw Mary,
with directed graph representation Gx,
root
saw
John Mary
10
9
9
30
3020
3
0
11
The first step of the algorithm is to find, for each
word, the highest scoring incoming edge
root
saw
John Mary30
3020
If the result were a tree, it would have to be the
maximum spanning tree. However, in this case we
have a cycle, so we will contract it into a single node
and recalculate edge weights according to Figure 3.
root
saw
John Mary
40
9
30
31
wjs
The new vertex wjs represents the contraction of
vertices John and saw. The edge from wjs to Mary
is 30 since that is the highest scoring edge from any
vertex in wjs. The edge from root into wjs is set to
40 since this represents the score of the best span-
ning tree originating from root and including only
the vertices in wjs. The same leads to the edge
from Mary to wjs. The fundamental property of the
Chu-Liu-Edmonds algorithm is that an MST in this
graph can be transformed into an MST in the orig-
inal graph (Leonidas, 2003). Thus, we recursively
call the algorithm on this graph. Note that we need
to keep track of the real endpoints of the edges into
and out of wjs for reconstruction later. Running the
algorithm, we must find the best incoming edge to
all words
root
saw
John Mary
40
30
wjs
This is a tree and thus the MST of this graph. We
now need to go up a level and reconstruct the graph.
The edge from wjs to Mary originally was from the
word saw, so we include that edge. Furthermore, the
edge from root to wjs represented a tree from root to
saw to John, so we include all those edges to get the
final (and correct) MST,
root
saw
John Mary
10
3030
A possible concern with searching the entire space
of spanning trees is that we have not used any syn-
tactic constraints to guide the search. Many lan-
guages that allow non-projectivity are still primarily
projective. By searching all possible non-projective
trees, we run the risk of finding extremely bad trees.
We address this concern in Section 4.
2.2.2 Projective Trees
It is well known that projective dependency pars-
ing using edge based factorization can be handled
with the Eisner algorithm (Eisner, 1996). This al-
gorithm has a runtime of O(n3) and has been em-
ployed successfully in both generative and discrimi-
native parsing models (Eisner, 1996; McDonald et
al., 2005). Furthermore, it is trivial to show that
the Eisner algorithm solves the maximum projective
spanning tree problem.
The Eisner algorithm differs significantly from
the Chu-Liu-Edmonds algorithm. First of all, it is a
bottom-up dynamic programming algorithm as op-
posed to a greedy recursive one. A bottom-up al-
gorithm is necessary for the projective case since it
must maintain the nested structural constraint, which
is unnecessary for the non-projective case.
2.3 Dependency Trees as MSTs: Summary
In the preceding discussion, we have shown that nat-
ural language dependency parsing can be reduced to
finding maximum spanning trees in directed graphs.
This reduction results from edge-based factoriza-
tion and can be applied to projective languages with
526
the Eisner parsing algorithm and non-projective lan-
guages with the Chu-Liu-Edmonds maximum span-
ning tree algorithm. The only remaining problem is
how to learn the weight vector w.
A major advantage of our approach over other
dependency parsing models is its uniformity and
simplicity. By viewing dependency structures as
spanning trees, we have provided a general frame-
work for parsing trees for both projective and non-
projective languages. Furthermore, the resulting
parsing algorithms are more efficient than lexi-
calized phrase structure approaches to dependency
parsing, allowing us to search the entire space with-
out any pruning. In particular the non-projective
parsing algorithm based on the Chu-Liu-Edmonds
MST algorithm provides true non-projective pars-
ing. This is in contrast to other non-projective meth-
ods, such as that of Nivre and Nilsson (2005), who
implement non-projectivity in a pseudo-projective
parser with edge transformations. This formulation
also dispels the notion that non-projective parsing is
?harder? than projective parsing. In fact, it is eas-
ier since non-projective parsing does not need to en-
force the non-crossing constraint of projective trees.
As a result, non-projective parsing complexity is just
O(n2), against the O(n3) complexity of the Eis-
ner dynamic programming algorithm, which by con-
struction enforces the non-crossing constraint.
3 Online Large Margin Learning
In this section, we review the work of McDonald et
al. (2005) for online large-margin dependency pars-
ing. As usual for supervised learning, we assume a
training set T = {(xt,yt)}Tt=1, consisting of pairs
of a sentence xt and its correct dependency tree yt.
In what follows, dt(x) denotes the set of possible
dependency trees for sentence x.
The basic idea is to extend the Margin Infused
Relaxed Algorithm (MIRA) (Crammer and Singer,
2003; Crammer et al, 2003) to learning with struc-
tured outputs, in the present case dependency trees.
Figure 4 gives pseudo-code for the MIRA algorithm
as presented by McDonald et al (2005). An on-
line learning algorithm considers a single training
instance at each update to w. The auxiliary vector
v accumulates the successive values of w, so that the
final weight vector is the average of the weight vec-
Training data: T = {(xt, yt)}Tt=1
1. w0 = 0; v = 0; i = 0
2. for n : 1..N
3. for t : 1..T
4. min
?
?
?
w(i+1) ? w(i)
?
?
?
s.t. s(xt, yt) ? s(xt, y?) ? L(yt, y?), ?y? ? dt(xt)
5. v = v + w(i+1)
6. i = i + 1
7. w = v/(N ? T )
Figure 4: MIRA learning algorithm.
tors after each iteration. This averaging effect has
been shown to help overfitting (Collins, 2002).
On each update, MIRA attempts to keep the new
weight vector as close as possible to the old weight
vector, subject to correctly classifying the instance
under consideration with a margin given by the loss
of the incorrect classifications. For dependency
trees, the loss of a tree is defined to be the number
of words with incorrect parents relative to the correct
tree. This is closely related to the Hamming loss that
is often used for sequences (Taskar et al, 2003).
For arbitrary inputs, there are typically exponen-
tially many possible parses and thus exponentially
many margin constraints in line 4 of Figure 4.
3.1 Single-best MIRA
One solution for the exponential blow-up in number
of trees is to relax the optimization by using only the
single margin constraint for the tree with the highest
score, s(x,y). The resulting online update (to be
inserted in Figure 4, line 4) would then be:
min
?
?w(i+1) ? w(i)
?
?
s.t. s(xt,yt) ? s(xt,y?) ? L(yt,y?)
where y? = arg maxy? s(xt,y?)
McDonald et al (2005) used a similar update with
k constraints for the k highest-scoring trees, and
showed that small values of k are sufficient to
achieve the best accuracy for these methods. How-
ever, here we stay with a single best tree because k-
best extensions to the Chu-Liu-Edmonds algorithm
are too inefficient (Hou, 1996).
This model is related to the averaged perceptron
algorithm of Collins (2002). In that algorithm, the
single highest scoring tree (or structure) is used to
update the weight vector. However, MIRA aggres-
sively updates w to maximize the margin between
527
the correct tree and the highest scoring tree, which
has been shown to lead to increased accuracy.
3.2 Factored MIRA
It is also possible to exploit the structure of the out-
put space and factor the exponential number of mar-
gin constraints into a polynomial number of local
constraints (Taskar et al, 2003; Taskar et al, 2004).
For the directed maximum spanning tree problem,
we can factor the output by edges to obtain the fol-
lowing constraints:
min
?
?w(i+1) ? w(i)
?
?
s.t. s(l, j) ? s(k, j) ? 1
?(l, j) ? yt, (k, j) /? yt
This states that the weight of the correct incoming
edge to the word xj and the weight of all other in-
coming edges must be separated by a margin of 1.
It is easy to show that when all these constraints
are satisfied, the correct spanning tree and all incor-
rect spanning trees are separated by a score at least
as large as the number of incorrect incoming edges.
This is because the scores for all the correct arcs can-
cel out, leaving only the scores for the errors causing
the difference in overall score. Since each single er-
ror results in a score increase of at least 1, the entire
score difference must be at least the number of er-
rors. For sequences, this form of factorization has
been called local lattice preference (Crammer et al,
2004). Let n be the number of nodes in graph Gx.
Then the number of constraints is O(n2), since for
each node we must maintain n ? 1 constraints.
The factored constraints are in general more re-
strictive than the original constraints, so they may
rule out the optimal solution to the original prob-
lem. McDonald et al (2005) examines briefly fac-
tored MIRA for projective English dependency pars-
ing, but for that application, k-best MIRA performs
as well or better, and is much faster to train.
4 Experiments
We performed experiments on the Czech Prague De-
pendency Treebank (PDT) (Hajic?, 1998; Hajic? et al,
2001). We used the predefined training, develop-
ment and testing split of this data set. Furthermore,
we used the automatically generated POS tags that
are provided with the data. Czech POS tags are very
complex, consisting of a series of slots that may or
may not be filled with some value. These slots rep-
resent lexical and grammatical properties such as
standard POS, case, gender, and tense. The result
is that Czech POS tags are rich in information, but
quite sparse when viewed as a whole. To reduce
sparseness, our features rely only on the reduced
POS tag set from Collins et al (1999). The num-
ber of features extracted from the PDT training set
was 13, 450, 672, using the feature set outlined by
McDonald et al (2005).
Czech has more flexible word order than English
and as a result the PDT contains non-projective de-
pendencies. On average, 23% of the sentences in
the training, development and test sets have at least
one non-projective dependency. However, less than
2% of total edges are actually non-projective. There-
fore, handling non-projective edges correctly has a
relatively small effect on overall accuracy. To show
the effect more clearly, we created two Czech data
sets. The first, Czech-A, consists of the entire PDT.
The second, Czech-B, includes only the 23% of sen-
tences with at least one non-projective dependency.
This second set will allow us to analyze the effec-
tiveness of the algorithms on non-projective mate-
rial. We compared the following systems:
1. COLL1999: The projective lexicalized phrase-structure
parser of Collins et al (1999).
2. N&N2005: The pseudo-projective parser of Nivre and
Nilsson (2005).
3. McD2005: The projective parser of McDonald et al
(2005) that uses the Eisner algorithm for both training and
testing. This system uses k-best MIRA with k=5.
4. Single-best MIRA: In this system we use the Chu-Liu-
Edmonds algorithm to find the best dependency tree for
Single-best MIRA training and testing.
5. Factored MIRA: Uses the quadratic set of constraints
based on edge factorization as described in Section 3.2.
We use the Chu-Liu-Edmonds algorithm to find the best
tree for the test data.
4.1 Results
Results are shown in Table 1. There are two main
metrics. The first and most widely recognized is Ac-
curacy, which measures the number of words that
correctly identified their parent in the tree. Complete
measures the number of sentences in which the re-
sulting tree was completely correct.
Clearly, there is an advantage in using the Chu-
Liu-Edmonds algorithm for Czech dependency pars-
528
Czech-A Czech-B
Accuracy Complete Accuracy Complete
COLL1999 82.8 - - -
N&N2005 80.0 31.8 - -
McD2005 83.3 31.3 74.8 0.0
Single-best MIRA 84.1 32.2 81.0 14.9
Factored MIRA 84.4 32.3 81.5 14.3
Table 1: Dependency parsing results for Czech. Czech-B is the subset of Czech-A containing only sentences
with at least one non-projective dependency.
ing. Even though less than 2% of all dependencies
are non-projective, we still see an absolute improve-
ment of up to 1.1% in overall accuracy over the
projective model. Furthermore, when we focus on
the subset of data that only contains sentences with
at least one non-projective dependency, the effect
is amplified. Another major improvement here is
that the Chu-Liu-Edmonds non-projective MST al-
gorithm has a parsing complexity of O(n2), versus
the O(n3) complexity of the projective Eisner algo-
rithm, which in practice leads to improvements in
parsing time. The results also show that in terms
of Accuracy, factored MIRA performs better than
single-best MIRA. However, for the factored model,
we do have O(n2) margin constraints, which re-
sults in a significant increase in training time over
single-best MIRA. Furthermore, we can also see that
the MST parsers perform favorably compared to the
more powerful lexicalized phrase-structure parsers,
such as those presented by Collins et al (1999) and
Zeman (2004) that use expensive O(n5) parsing al-
gorithms. We should note that the results in Collins
et al (1999) are different then reported here due to
different training and testing data sets.
One concern raised in Section 2.2.1 is that search-
ing the entire space of non-projective trees could
cause problems for languages that are primarily pro-
jective. However, as we can see, this is not a prob-
lem. This is because the model sets its weights with
respect to the parsing algorithm and will disfavor
features over unlikely non-projective edges.
Since the space of projective trees is a subset of
the space of non-projective trees, it is natural to won-
der how the Chu-Liu-Edmonds parsing algorithm
performs on projective data since it is asymptotically
better than the Eisner algorithm. Table 2 shows the
results for English projective dependency trees ex-
tracted from the Penn Treebank (Marcus et al, 1993)
using the rules of Yamada and Matsumoto (2003).
English
Accuracy Complete
McD2005 90.9 37.5
Single-best MIRA 90.2 33.2
Factored MIRA 90.2 32.3
Table 2: Dependency parsing results for English us-
ing spanning tree algorithms.
This shows that for projective data sets, training
and testing with the Chu-Liu-Edmonds algorithm is
worse than using the Eisner algorithm. This is not
surprising since the Eisner algorithm uses the a pri-
ori knowledge that all trees are projective.
5 Discussion
We presented a general framework for parsing de-
pendency trees based on an equivalence to maxi-
mum spanning trees in directed graphs. This frame-
work provides natural and efficient mechanisms
for parsing both projective and non-projective lan-
guages through the use of the Eisner and Chu-Liu-
Edmonds algorithms. To learn these structures we
used online large-margin learning (McDonald et al,
2005) that empirically provides state-of-the-art per-
formance for Czech.
A major advantage of our models is the abil-
ity to naturally model non-projective parses. Non-
projective parsing is commonly considered more
difficult than projective parsing. However, under
our framework, we show that the opposite is actually
true that non-projective parsing has a lower asymp-
totic complexity. Using this framework, we pre-
sented results showing that the non-projective model
outperforms the projective model on the Prague De-
pendency Treebank, which contains a small number
of non-projective edges.
Our method requires a tree score that decomposes
according to the edges of the dependency tree. One
might hope that the method would generalize to
529
include features of larger substructures. Unfortu-
nately, that would make the search for the best tree
intractable (Ho?ffgen, 1993).
Acknowledgments
We thank Lillian Lee for bringing an important
missed connection to our attention, and Koby Cram-
mer for his help with learning algorithms. This work
has been supported by NSF ITR grants 0205448 and
0428193.
References
Y.J. Chu and T.H. Liu. 1965. On the shortest arbores-
cence of a directed graph. Science Sinica, 14:1396?
1400.
M. Collins, J. Hajic?, L. Ramshaw, and C. Tillmann. 1999.
A statistical parser for Czech. In Proc. ACL.
M. Collins. 2002. Discriminative training methods for
hidden Markov models: Theory and experiments with
perceptron algorithms. In Proc. EMNLP.
T.H. Cormen, C.E. Leiserson, and R.L. Rivest. 1990. In-
troduction to Algorithms. MIT Press/McGraw-Hill.
K. Crammer and Y. Singer. 2003. Ultraconservative on-
line algorithms for multiclass problems. JMLR.
K. Crammer, O. Dekel, S. Shalev-Shwartz, and Y. Singer.
2003. Online passive aggressive algorithms. In Proc.
NIPS.
K. Crammer, R. McDonald, and F. Pereira. 2004. New
large margin algorithms for structured prediction. In
Learning with Structured Outputs Workshop (NIPS).
A. Culotta and J. Sorensen. 2004. Dependency tree ker-
nels for relation extraction. In Proc. ACL.
Y. Ding and M. Palmer. 2005. Machine translation using
probabilistic synchronous dependency insertion gram-
mars. In Proc. ACL.
J. Edmonds. 1967. Optimum branchings. Journal of Re-
search of the National Bureau of Standards, 71B:233?
240.
J. Eisner. 1996. Three new probabilistic models for de-
pendency parsing: An exploration. In Proc. COLING.
J. Hajic?, E. Hajicova, P. Pajas, J. Panevova, P. Sgall, and
B. Vidova Hladka. 2001. The Prague Dependency
Treebank 1.0 CDROM. Linguistics Data Consortium
Cat. No. LDC2001T10.
J. Hajic?. 1998. Building a syntactically annotated cor-
pus: The Prague dependency treebank. Issues of Va-
lency and Meaning, pages 106?132.
H. Hirakawa. 2001. Semantic dependency analysis
method for Japanese based on optimum tree search al-
gorithm. In Proc. of PACLING.
Klaus-U. Ho?ffgen. 1993. Learning and robust learning
of product distributions. In Proceedings of COLT?93,
pages 77?83.
W. Hou. 1996. Algorithm for finding the first k shortest
arborescences of a digraph. Mathematica Applicata,
9(1):1?4.
R. Hudson. 1984. Word Grammar. Blackwell.
G. Leonidas. 2003. Arborescence optimization problems
solvable by Edmonds? algorithm. Theoretical Com-
puter Science, 301:427 ? 437.
M. Marcus, B. Santorini, and M. Marcinkiewicz. 1993.
Building a large annotated corpus of English: the Penn
Treebank. Computational Linguistics, 19(2):313?330.
R. McDonald, K. Crammer, and F. Pereira. 2005. Online
large-margin training of dependency parsers. In Proc.
ACL.
J. Nivre and J. Nilsson. 2005. Pseudo-projective depen-
dency parsing. In Proc. ACL.
J. Nivre and M. Scholz. 2004. Deterministic dependency
parsing of english text. In Proc. COLING.
Y. Shinyama, S. Sekine, K. Sudo, and R. Grishman.
2002. Automatic paraphrase acquisition from news ar-
ticles. In Proc. HLT.
R. Snow, D. Jurafsky, and A. Y. Ng. 2004. Learning
syntactic patterns for automatic hypernym discovery.
In NIPS 2004.
R.E. Tarjan. 1977. Finding optimum branchings. Net-
works, 7:25?35.
B. Taskar, C. Guestrin, and D. Koller. 2003. Max-margin
Markov networks. In Proc. NIPS.
B. Taskar, D. Klein, M. Collins, D. Koller, and C. Man-
ning. 2004. Max-margin parsing. In Proc. EMNLP.
W. Wang and M. P. Harper. 2004. A statistical constraint
dependency grammar (CDG) parser. In Workshop on
Incremental Parsing: Bringing Engineering and Cog-
nition Together (ACL).
H. Yamada and Y. Matsumoto. 2003. Statistical depen-
dency analysis with support vector machines. In Proc.
IWPT.
D. Zeman. 2004. Parsing with a Statistical Dependency
Model. Ph.D. thesis, Univerzita Karlova, Praha.
530
Proceedings of Human Language Technology Conference and Conference on Empirical Methods in Natural Language
Processing (HLT/EMNLP), pages 987?994, Vancouver, October 2005. c?2005 Association for Computational Linguistics
Flexible Text Segmentation with Structured Multilabel Classification
Ryan McDonald Koby Crammer Fernando Pereira
Department of Computer and Information Science
University of Pennsylvania
Philadelphia, PA 19104
{ryantm,crammer,pereira}@cis.upenn.edu
Abstract
Many language processing tasks can be re-
duced to breaking the text into segments
with prescribed properties. Such tasks
include sentence splitting, tokenization,
named-entity extraction, and chunking.
We present a new model of text segmenta-
tion based on ideas from multilabel clas-
sification. Using this model, we can natu-
rally represent segmentation problems in-
volving overlapping and non-contiguous
segments. We evaluate the model on en-
tity extraction and noun-phrase chunking
and show that it is more accurate for over-
lapping and non-contiguous segments, but
it still performs well on simpler data sets
for which sequential tagging has been the
best method.
1 Introduction
Text segmentation is a basic task in language pro-
cessing, with applications such as tokenization, sen-
tence splitting, named-entity extraction, and chunk-
ing. Many parsers, translation systems, and extrac-
tion systems rely on such segmentations to accu-
rately process the data. Depending on the applica-
tion, segments may be tokens, phrases, or sentences.
However, in this paper we primarily focus on seg-
menting sentences into tokens.
The most common approach to text segmenta-
tion is to use finite-state sequence tagging mod-
els, in which each atomic text element (character
or token) is labeled with a tag representing its role
in a segmentation. Models of that form include
hidden Markov models (Rabiner, 1989; Bikel et
al., 1999) as well as discriminative tagging mod-
els based on maximum entropy classification (Rat-
naparkhi, 1996; McCallum et al, 2000), conditional
random fields (Lafferty et al, 2001; Sha and Pereira,
2003), and large-margin techniques (Kudo and Mat-
sumoto, 2001; Taskar et al, 2003). Tagging mod-
els are the best previous methods for text segmen-
tation. However, their purely sequential form limits
their ability to naturally handle overlapping or non-
contiguous segments.
We present here an alternative view of segmenta-
tion as structured multilabel classification. In this
view, a segmentation of a text is a set of segments,
each of which is defined by the set of text positions
that belong to the segment. Thus, a particular seg-
ment may not be a set of consecutive positions in
the text, and segments may overlap. Given a text
x = x1 ? ? ? xn, the set of possible segments, which
corresponds to the set of possible classification la-
bels, is seg(x) = {O,I}n; for y ? seg(x), yi = I
iff xi belongs to the segment. Then, our segmen-
tation task is to determine which labels are correct
segments in a given text. We have thus a structured
multilabel classification problem: each instance, a
text, may have multiple structured labels, represent-
ing each of its segments. These labels are structured
in that they do not come from a predefined set, but
instead are built from sets of choices associated to
the elements of arbitrarily long instances.
More generally, we may be interested in typed
segments, e.g. segments naming different types of
987
entities. In that case, the set of segment labels is
seg(x) = T ? {O,I}n, where T is the set of seg-
ment types. Since the extension is straightforward,
we frame the discussion in terms of untyped seg-
ments, and only discuss segment types as needed.
At first sight, it might appear that we have made
the segmentation problem intractably harder by turn-
ing it into a classification problem with a number
of labels exponential on the length of the instance.
However, we can bound the number of labels under
consideration and take advantage of the structure of
labels to find the k most likely labels efficiently. This
will allow us to exploit recent advances in online dis-
criminative methods for multilabel classification and
ranking (Crammer and Singer, 2002).
Though multilabel classification has been well
studied (Schapire and Singer, 1999; Elisseeff and
Weston, 2001), as far as we are aware, this is the
first study involving structured labels.
2 Segmentation as Tagging
The standard approach to text segmentation is to use
tagging techniques with a BIO tag set. Elements in
the input text are tagged with one of B for the be-
ginning of a contiguous segment, I for the inside
of a contiguous segment, or O for outside a seg-
ment. Thus, segments must be contiguous and non-
overlapping. For instance, consider the sentence Es-
timated volume was a light 2.4 million ounces. Fig-
ure 1a shows how this sentence would be labeled
using the BIO tag set for the problem of identifying
base NPs in text. Given a particular tagging for a
sentence, it is trivial to find all the segments, those
whose tag sequences are longest matches for the reg-
ular expression BI?. For typed segments, the BIO
tag set is easily augmented to indicate not only seg-
ment boundaries, but also the type of each segment.
Figure 1b exemplifies the tags for the task of finding
people and organizations in text.
Sequential tagging with the BIO tag set has
proven quite accurate for shallow parsing and named
entity extraction tasks (Kudo and Matsumoto, 2001;
Sha and Pereira, 2003; Tjong Kim Sang and
De Meulder, 2003). However, this approach
can only identify non-overlapping, contiguous seg-
ments. This is sufficient for some applications, and
in any case, most training data sets are annotated
without concern for overlapping or non-contiguous
segments. However, there are instances in which se-
quential labeling techniques using the BIO label set
will encounter problems.
Figure 2 shows two simple examples of segmen-
tations involving overlapping, non-contiguous seg-
ments. In both cases, it is difficult to see how a
sequential tagger could extract the segments cor-
rectly. It would be possible to grow the tag set to
represent a bounded number of overlapping, non-
contiguous segments by representing all possible
combinations of segment membership over k over-
lapping segments, but this would require an arbitrary
upper bound on k and would lead to models that gen-
eralize poorly and are expensive to train.
Dickinson and Meurers (2005) point out that, as
language processing begins to tackle problems in
free-word order languages and discourse analysis,
annotating and extracting non-contiguous segmen-
tations of text will become increasingly important.
Though we focus primarily on entity extraction and
NP chunking in this paper, there is no reason why
ideas presented here could not be extended to man-
aging other non-contiguous phenomena.
3 Structured Multilabel Classification
As outlined in Section 1, we represent segmentation
as multilabel classification, assigning to each text
the set of segments it contains. Figure 3 shows the
segments for the examples of Figure 2. Each seg-
ment is given by a O/I assignment to its words, in-
dicating which words belong to the segment.
By representing the segmentation problems as
multilabel classification, we have fundamentally
changed the objective of our learning and inference
algorithms. The sequential tagging formulation is
aimed to learn and find the best possible tagging of
a text. In multilabel classification, we train model
parameters so that correct labels ? that is, correct
segments ? receive higher score than all incorrect
ones. Likewise, inference becomes the problem of
finding the set of correct labels for a text, that is, the
set of correct segments.
We now describe the learning problem using the
decision-theoretic multilabel classification and rank-
ing framework of Crammer and Singer (2002) and
Crammer (2005) as our starting point. In Sec-
988
a. Estimated volume was a light 2.4 million ounces .
B I O B I I I I O
b. Bill Clinton and Microsoft founder Bill Gates met today for 20 minutes .
B-PER I-PER O B-ORG O B-PER I-PER O O O O O O
Figure 1: Sequential labeling formulation of text segmentation using the BIO label set. a) NP-chunking
tasks. b) Named-entity extraction task.
a) Today, Bill and Hilary Clinton traveled to Canada.
- Person: Bill Clinton
- Person: Hilary Clinton
b) ... purified bovine P450 11 beta / 18 / 19 - hydroxylase was ...
- Enzyme: P450 11 beta-hydroxylase
- Enzyme: P450 18-hydroxylase
- Enzyme: P450 19-hydroxilase
Figure 2: Examples of overlapping and non-contiguous text segmentations.
tion 3.2, we describe a polynomial-time inference
algorithm for finding up to k correct segments.
3.1 Training Multilabel Classifiers
Our model is based on a linear score s(x,y; w) for
each segment y of text x, defined as
s(x,y; w) = w ? f(x,y)
where f(x,y) is a feature vector representation of
the sentence-segment pair, and w is a vector of
feature weights. For a given text x, act(x) ?
seg(x) denotes the set of correct segments for x, and
bestk(x; w) denotes the set of k segments with high-
est score relative to the weight vector w. For learn-
ing, we use a training set T = {(xt, act(xt))}|T |t=1 of
texts labeled with the correct segmentation.
We will discuss later the design of f(x,y) and an
efficient algorithm for finding the k highest scoring
segments (where k is sufficiently large to include
all correct segments). In this section, we present a
method for learning a weight vector w that seeks to
score correct segments above all incorrect segments.
Crammer and Singer (2002), extended by Cram-
mer (2005), provide online learning algorithms for
multilabel classification and ranking that take one
instance at a time, construct a set of scoring con-
straints for the instance, and adjust the weight vec-
tor to satisfy the constraints. The constraints en-
force a margin between the scores of correct labels
and those of incorrect labels. The benefits of large-
margin learning are best known from SVMs (Cris-
tianini and Shawe-Taylor, 2000; Scho?lkopf and
Training data: T = {(xt, act(xt))}|T |t=1
1. w(0) = 0; i = 0
2. for n : 1..N
3. for t : 1..|T |
4. w(i+1) = arg minw
?
?
?
w ? w(i)
?
?
?
2
s.t. s(xt, y; w) ? s(xt, y?; w) + 1
?y ? act(xt), ?y? ? bestk(xt; w(i)) ? act(xt)
6. i = i + 1
7. w = w(N?|T |)
Figure 4: A simplified version of the multilabel
learning algorithm of Crammer and Singer (2002).
Smola, 2002), and are analyzed in detail by Cram-
mer (2005) for online multilabel classification.
For segmentation, the number of possible labels
(segments) is exponential on the length of the text.
We make the problem tractable by including only the
margin constraints between correct segments and at
most k highest scoring incorrect segments. Figure 4
sketches an online learning algorithm for multilabel
classification based on the work of Crammer (2005).
In the algorithm, w(i+1) is the projection of w(i) onto
the set of weight vectors such that the scores of cor-
rect segments are separated by a margin of at least
1 from the scores of incorrect segments among the
k top-scoring segments. This update is conservative
in that there is no weight change if the constraint set
is already satisfied or empty; if some constraints are
not satisfied, we make the smallest weight change
that satisfies the constraints. Since, the objective is
quadratic in w and the constraints are linear, the op-
timization problem can be solved by Hildreth?s al-
989
a) Today , Bill and Hilary Clinton traveled to Canada .
O O I O O I O O O O
O O O O I I O O O O
b) ... purified bovine P450 11 beta / 18 / 19 - hydroxylase was ...
O O I I I O O O O I I O
O O I O O O I O O I I O
O O I O O O O O I I I O
Figure 3: Correct segments for two examples.
gorithm (Censor and Zenios, 1997).
Using standard arguments for linear classifiers
(add constant feature, rescale weights) and the fact
that all the correct scores in line 4 of Figure 4 are re-
quired to be above all the incorrect scores in the top
k, that line can be replaced by
w(i+1) = arg minw
?
?w ? w(i)
?
?
2
s.t. s(xt,y; w) ? 1 and s(xt,y?; w) ? ?1
?y ? act(xt),?y? ? bestk(xt; w(i)) ? act(xt)
If v is the number of correct segments for x,
this transformation replaces O(kv) constraints with
O(k + v) constraints: segment scores are compared
to a single positive or negative threshold rather then
to each other. At test time, we find the segments
with positive score by finding the k highest scoring
segments and discarding those with a negative score.
3.2 Inference
During learning and at test time we require a method
for finding the k highest scoring segments. At test
time, we predict as correct all the segments with pos-
itive score in the top k. In this section we give an
algorithm that calculates this precisely.
For inference, tagging models typically use the
Viterbi algorithm (Rabiner, 1989). The algorithm is
given by the following standard recurrences:
S[i, t] = maxt? s(t?, t, i) + S[i ? 1, t?]
B[i, t] = arg maxt? s(t?, t, i) + S[i ? 1, t?]
with appropriate initial conditions, where s(t?, t, i)
is the score for going from tag t? at i ? 1 to tag t
at i. The dynamic programming table S[i, t] stores
the score of the best tag sequence ending at posi-
tion i with tag t, and B[i, t] is a back-pointer to the
previous tag in the best sequence ending at i with
t, which allows us to reconstruct the best sequence.
The Viterbi algorithm has easy k-best extensions.
We could find the k highest scoring segments us-
ing Viterbi. However, for the case of non-contiguous
segments, we would like to represent higher-order
dependencies that are difficult to model in Viterbi. In
particular, in Figure 3b we definitely want a feature
bridging the gap between Bill and Clinton, which
could not be captured with a standard first-order
model. But moving to higher-order models would
require adding dimensions to the dynamic program-
ming tables S and B, with corresponding multipliers
to the complexity of inference.
To represent dependencies between non-
contiguous text positions, for any given segment
y = y1 ? ? ? yn, let i(y) = 0i1 ? ? ? im(n + 1) be the
increasing sequence of indices ij such that yij = I,
padded for convenience with the dummy first index
0 and last index n + 1. Also for convenience, set
x0 = -s- and xn+1 = -e- for fixed start and
end markers. Then, we restrict ourselves to feature
functions f(x,y) that factor relative to the input as
f(x,y) =
|i(y)|
?
j=1
g(i(y)j?1, i(y)j) (1)
where i(y)j is the jth integer in i(y) and g is a fea-
ture function depending on arbitrary properties of
the input relative to the indices i(y)j?1 and i(y)j .
Applying (1) to the segment Bill Clinton in Fig-
ure 3, its score would be
w ? [g(0, 3) + g(3, 6) + g(6, 11)]
This feature representation allows us to include de-
pendencies between non-contiguous segment posi-
tions, as well as dependencies on any properties of
the input, including properties of skipped positions.
We now define the following dynamic program
S[i] = maxj<i S[j] + w ? g(j, i)
B[i] = arg maxj<i S[j] + w ? g(j, i)
990
These recurrences compute the score S[i] of the best
partial segment ending at i as the sum of the max-
imum score of a partial segment ending at position
j < i, and the score of skipping from j to i. The
back-pointer table B allows us to reconstruct the se-
quence of positions included in the segment.
Clearly, this program requires O(n2) time for a
text of length n. Furthermore we can easily augment
this algorithm in the standard fashion to find the k
best segments, and multiple segment types, result-
ing in a runtime of O(n2kT ), where T is the number
of types. O(n2kT ) is not ideal, but is still practical
since in this work we are segmenting sentences. If
we can bound the largest gap in any non-contiguous
segment by a constant g  n, then the runtime can
be improved to O(ngkT ). This runtime does not
compare favorably to the standard Viterbi algorithm
that runs in O(nT 2), especially for large k. How-
ever, we found that for even large k we could still
train large models in a matter of hours and test on
unseen data in a few minutes.
3.2.1 Restrictions
Often a segmentation task or data set will restrict
particular kinds of segments. For instance, it may be
the case that a data set does not have any overlap-
ping or non-contiguous segments. Embedded seg-
mentations ? those in which one segment?s tokens
are a subset of another?s ? is also a phenomenon that
sometimes does not occur.
It is easy to restrict the inference algorithm to dis-
allow such segments if they are unnecessary. For ex-
ample, if two segments overlap or are embedded, the
inference algorithm can just return the highest scor-
ing one. Or it can simply ignore all non-contiguous
segments if it is known that they do not occur in the
data. In Section 4 we will augment the inference
algorithm accordingly for each data set.
3.3 Feature Representation
We now discuss the design of the feature function
for two consecutive segment positions g(j, i), where
j < i. We build individual binary-valued features
from predicates over the input, for instance, the iden-
tities of words in the sentence at particular posi-
tions relative to i and j. The selection of predicates
varies by task, and we provide specific predicate sets
in Section 4 for various data sets. In this section,
we use for illustration word-pair identity predicates
such as xj = Bill & xi = Clinton.
For sequential tagging models, predicates are
combined with the set of states (or tags) to create
a feature representation. For our model, we define
the following possible states:
start ? j = 0
end ? i = n + 1
next ? j = i ? 1
skip ? j < i ? 1
For example, the following features would be on for
g(0, 3)1 and g(3, 6), respectively, in Figure 3a:
xj = -s- & xi = Bill & start
xj = Bill & xi = Clinton & skip
These features indicate a predicate?s role in the seg-
ment: at the beginning, at the end, over contiguous
segment words or skipping over some words. All
features can be augmented to indicate specific seg-
ment types for multi-type segmentation tasks. No
matter what the task, we always add predicates that
represent ranges of the distance i?j, as well as what
words or part-of-speech tags occur between the two
words. For instance, g(3, 6) might contain
word-in-between= and & skip
These features are designed to identify common
characteristics of non-contiguous segments such
as the presence of conjunctions or punctuation in
skipped portions. Although we have considered only
binary features here, the model in principle allows
arbitrary real-valued feature.
3.4 Summary
We presented a method for text segmentation that
equates the problem to structured multilabel classi-
fication where each label corresponds to a segment.
We showed that learning and inference can be man-
aged tractably in the formulation by efficiently find-
ing the k highest scoring segments through a dy-
namic programming algorithm that factors the struc-
ture of each segment. The only concern is that k
must be large enough to include all correct segments,
1Note that ?skip? is not on for g(0, 3) even though j < i?1.
Start and end states override other states.
991
which we will discuss further in Section 4. This
method naturally models all possible segmentations
including those with overlapping or non-contiguous
segments. Out approach can be seen as multilabel
variant of the work of McDonald et al (2004), which
creates a set of constraints to separate the score of
the single correct output from the k highest scoring
outputs with an appropriate large margin.
4 Experiments
We now describe a set of experiments on named en-
tity and base NP segmentation. For these experi-
ments, we set k = n, where n is the length of the
sentence. This represents a reasonable upper bound
on the number of entities or chunks in a sentence and
results in a time complexity of O(n3T ).
We compare our methods with both the averaged
perceptron (Collins, 2002) and conditional random
fields (Lafferty et al, 2001) using identical predicate
sets. Though all systems use identical predicates, the
actual features of the systems are different due to
the fundamental differences between the multilabel
classification and sequential tagging models.
4.1 Standard data sets
Our first experiments are standard named entity and
base NP data sets with no overlapping, embedded or
non-contiguous segments. These experiments will
show that, for simple segmentations, our model is
competitive with sequential tagging models.
For the named entity experiments we used the
CoNLL 2003 (Tjong Kim Sang and De Meulder,
2003) data with people, organizations, locations and
miscellaneous entities. We used standard predicates
based on word, POS and orthographic information
over a previous to next word window. For the NP
chunking experiments we used the standard CoNLL
2000 data set (Kudo and Matsumoto, 2001; Sha and
Pereira, 2003) using the predicate set defined by Sha
and Pereira (2003).
The first three rows of Table 1 compare the mul-
tilabel classification approach to standard sequen-
tial classifiers. As one might expect, the perfor-
mance of the multilabel classification method is be-
low that of the sequential tagging methods. This is
because those methods model contiguous segments
well without the need for thresholds or k-best infer-
ence. In addition, the multilabel method shows sig-
nificantly higher precision then recall. One possible
reason for this is that during the course of learning,
the model will see many segments that are nearly
correct, e.g., segments that overlap correct segments
and differ by a single token. As a result, the model
learns to score all segments containing even a small
amount of negative evidence as invalid in order to
ensure that these nearly correct segments have a suf-
ficiently low score.
One way to alleviate this problem is to restrict the
inference algorithm to not return any overlapping,
non-contiguous or embedded segmentations as dis-
cussed in Section 3.2.1, since this data set does not
contain segments of this kind. This way, the learning
stage only updates the parameters when a nearly cor-
rect segment actually out scores the correct one. The
results of this system are shown in row 4 of Table 1.
We can see that this change did lead to a more bal-
anced precision/recall, however it is clear that more
investigation is required.
4.2 Chemical substance extraction
The second set of experiments involves extract-
ing chemical substance names from MEDLINE ab-
stracts that relevant to the inhibition of the enzyme
CYP450 (PennBioIE, 2005). We focus on abstracts
that have at least one overlapping or non-contiguous
annotation. This data set contains 6164 annotated
chemical substances, including 6% that are both
overlapping and non-contiguous. Figure 3b is an
example from the corpus. We use identical predi-
cates to the named entity experiments in Section 4.1.
Though the data does contain overlapping and non-
contiguous segments, it does not contain embedded
segments. Results are shown in Table 2 using 10-
fold cross validation. The sequential tagging models
were trained using only sentences with no overlap-
ping or non-contiguous entities. We found this pro-
vided the best performance. Row 4 of Table 2 shows
the multilabel approach with the inference algorithm
restricted to not allow embedded segments.
We can see that our method does significantly bet-
ter on this data set (up to a 26% reduction in er-
ror). It is also apparent that the model is picking up
some overlapping and non-contiguous entities (see
Table 2). However, the models performance on these
kinds of entities is lower than overall performance.
992
a. Named-Entity Extraction b. NP-chunking
Precision Recall F-measure Precision Recall F-measure
Avg. Perceptron 82.46 83.14 82.80 94.22 93.88 94.05
CRFs 83.36 83.57 83.47 94.57 94.00 94.29
Multilabel 92.47 74.19 82.33 94.65 92.28 93.45
Multilabel with Restrictions 91.08 76.68 83.26 94.10 93.70 93.90
Table 1: Results for named-entity extraction and NP-chunking on data sets with only non-overlapping and
contiguous segments annotated.
Chem Substance Extraction - A Chem Substance Extraction - B
Precision Recall F-measure Precision Recall F-measure
Avg. Perceptron 82.98 79.40 81.15 1.0 0.0 0.0
CRFs 85.85 79.06 82.31 1.0 0.0 0.0
Multilabel 88.24 80.84 84.38 62.56 33.67 43.78
Multilabel with Restrictions 88.55 84.59 86.53 72.58 45.92 56.25
Table 2: Results for chemical substance extraction. Table A is for all entities in the data set and Table B is
only for those entities that are overlapping and non-contiguous.
4.3 Tuning Precision and Recall
The learning algorithm in Section 3.1 seeks a sep-
arator through the origin, though, our experimental
results suggest that this tends to favor precision at
the expense of recall. However, at test time we can
use a separation threshold different from zero. This
parameter allows us to trade off precision against re-
call, and could be tuned on held-out data.
Figure 5 plots precision, recall and f-measure
against the threshold for the basic multilabel model
on the chemical substance, NP chunking and person
entity extraction data sets. These plots clearly show
what is expected: higher thresholds give higher pre-
cision, and lower thresholds give higher recall. In
these data sets at least, a zero threshold is almost
always near optimal, though sometimes we would
benefit from a slightly lower threshold.
5 Discussion
We have presented a method for text segmentation
that is base on discriminatively learning structured
multilabel classifications. The benefits include
? Competitive performance with sequential tag-
ging models.
? Flexible modeling of complex segmentations,
including overlapping, embedded and non-
contiguous segments.
? Adjustable precision-recall trade off.
However, there is a computation cost for our models.
For a text of length n, training and testing require
O(n3T ) time, where T is the number of segment
types. Fortunately, this still results in training times
on the order of hours.
Our approach is related to the work of Bockhorst
and Craven (2004). In this work, a conditional ran-
dom field model is trained to allow for overlapping
segments with an O(n2) inference algorithm. The
model is applied to biological sequence modeling
with promising results. However, our approaches
differ in two major respects. First, their model is
probabilistic, and trained to maximize segmenta-
tion likelihood, while our model is trained to max-
imize margin. Second, our method allows for non-
contiguous segments, at the cost of a slower O(n3)
inference algorithm.
In further work, the classification threshold
should also be learned to achieve the desired balance
between precision and recall. It would also be useful
to investigate methods for combining these models
with standard sequential tagging models to get top
performance on simple segmentations as well as on
overlapping or non-contiguous ones.
A broader area of investigation are other problems
in language processing that can benefit from struc-
tured multilabel classification, e.g., ambiguities in
language often result in multiple acceptable parses
for sentences. It may be possible to extend the al-
gorithms presented here to learn to distinguish all
acceptable parses from unacceptable ones instead of
just finding a single parse when many are valid.
993
?1 ?0.8 ?0.6 ?0.4 ?0.2 0 0.2 0.4 0.6 0.8 1
0.4
0.5
0.6
0.7
0.8
0.9
1
CHEM
?1 ?0.8 ?0.6 ?0.4 ?0.2 0 0.2 0.4 0.6 0.8 1
0.7
0.75
0.8
0.85
0.9
0.95
1
NP
?1 ?0.8 ?0.6 ?0.4 ?0.2 0 0.2 0.4 0.6 0.8 1
0.4
0.5
0.6
0.7
0.8
0.9
1
PER
Figure 5: Precision (squares), Recall (circles) and F-measure (line) plotted against threshold values. CHEM:
chemical substance extraction, NP: noun-phrase chunking, and PER: person name extraction.
Acknowledgments
We thank the members of the Penn BioIE project
for the development of the CYP450 corpus that we
used for our experiments. In particular, Seth Kulick
answered many questions about the data. This work
has been supported by the NSF ITR grant 0205448.
References
D.M. Bikel, R. Schwartz, and R.M. Weischedel. 1999.
An algorithm that learns what?s in a name. Machine
Learning Journal Special Issue on Natural Language
Learning, 34(1/3):221?231.
J. Bockhorst and M. Craven. 2004. Markov networks for
detecting overlapping elements in sequence data. In
Proc. NIPS.
Y. Censor and S.A. Zenios. 1997. Parallel optimization :
theory, algorithms, and applications. Oxford Univer-
sity Press.
M. Collins. 2002. Discriminative training methods for
hidden Markov models: Theory and experiments with
perceptron algorithms. In Proc. EMNLP.
K. Crammer and Y. Singer. 2002. A new family of online
algorithms for category ranking. In Proc SIGIR.
K. Crammer. 2005. Online Learning for Complex Cat-
egorial Problems. Ph.D. thesis, Hebrew University of
Jerusalem. to appear.
N. Cristianini and J. Shawe-Taylor. 2000. An Introduc-
tion to Support Vector Machines. Cambridge Univer-
sity Press.
M. Dickinson and W.D. Meurers. 2005. Detecting errors
in discontinuous structural annotation. In Proc. ACL.
A. Elisseeff and J. Weston. 2001. A kernel method for
multi-labeled classification. In Proc. NIPS.
T. Kudo and Y. Matsumoto. 2001. Chunking with sup-
port vector machines. In Proc. NAACL.
J. Lafferty, A. McCallum, and F. Pereira. 2001. Con-
ditional random fields: Probabilistic models for seg-
menting and labeling sequence data. In Proc. ICML.
A. McCallum, D. Freitag, and F. Pereira. 2000. Maxi-
mum entropy Markov models for information extrac-
tion and segmentation. In Proceedings of ICML.
R. McDonald, K. Crammer, and F. Pereira. 2004. Large
margin online learning algorithms for scalable struc-
tured classication. In NIPS Workshop on Structured
Outputs.
PennBioIE. 2005. Mining The Bibliome Project.
http://bioie.ldc.upenn.edu/.
L. R. Rabiner. 1989. A tutorial on hidden Markov mod-
els and selected applications in speech recognition.
Proceedings of the IEEE, 77(2):257?285, February.
A. Ratnaparkhi. 1996. A maximum entropy model for
part-of-speech tagging. In Proc. EMNLP.
R. E. Schapire and Y. Singer. 1999. Improved boosting
algorithms using confidence-rated predictions. Ma-
chine Learning, 37(3):1?40.
B. Scho?lkopf and A. J. Smola. 2002. Learning with Ker-
nels: Support Vector Machines, Regularization, Opti-
mization and Beyond. MIT Press.
F. Sha and F. Pereira. 2003. Shallow parsing with condi-
tional random fields. In Proc. HLT-NAACL.
B. Taskar, C. Guestrin, and D. Koller. 2003. Max-margin
Markov networks. In Proc. NIPS.
E. F. Tjong Kim Sang and F. De Meulder. 2003. Intro-
duction to the CoNLL-2003 shared task: Language-
independent named entity recognition. In Proceedings
of CoNLL-2003.
http://www.cnts.ua.ac.be/conll2003/ner.
994
Shallow Parsing with Conditional Random Fields
Fei Sha and Fernando Pereira
Department of Computer and Information Science
University of Pennsylvania
200 South 33rd Street, Philadelphia, PA 19104
(feisha|pereira)@cis.upenn.edu
Abstract
Conditional random fields for sequence label-
ing offer advantages over both generative mod-
els like HMMs and classifiers applied at each
sequence position. Among sequence labeling
tasks in language processing, shallow parsing
has received much attention, with the devel-
opment of standard evaluation datasets and ex-
tensive comparison among methods. We show
here how to train a conditional random field to
achieve performance as good as any reported
base noun-phrase chunking method on the
CoNLL task, and better than any reported sin-
gle model. Improved training methods based
on modern optimization algorithms were crit-
ical in achieving these results. We present ex-
tensive comparisons between models and train-
ing methods that confirm and strengthen pre-
vious results on shallow parsing and training
methods for maximum-entropy models.
1 Introduction
Sequence analysis tasks in language and biology are of-
ten described as mappings from input sequences to se-
quences of labels encoding the analysis. In language pro-
cessing, examples of such tasks include part-of-speech
tagging, named-entity recognition, and the task we shall
focus on here, shallow parsing. Shallow parsing iden-
tifies the non-recursive cores of various phrase types in
text, possibly as a precursor to full parsing or informa-
tion extraction (Abney, 1991). The paradigmatic shallow-
parsing problem is NP chunking, which finds the non-
recursive cores of noun phrases called base NPs. The
pioneering work of Ramshaw and Marcus (1995) in-
troduced NP chunking as a machine-learning problem,
with standard datasets and evaluation metrics. The task
was extended to additional phrase types for the CoNLL-
2000 shared task (Tjong Kim Sang and Buchholz, 2000),
which is now the standard evaluation task for shallow
parsing.
Most previous work used two main machine-learning
approaches to sequence labeling. The first approach re-
lies on k-order generative probabilistic models of paired
input sequences and label sequences, for instance hidden
Markov models (HMMs) (Freitag and McCallum, 2000;
Kupiec, 1992) or multilevel Markov models (Bikel et al,
1999). The second approach views the sequence labeling
problem as a sequence of classification problems, one for
each of the labels in the sequence. The classification re-
sult at each position may depend on the whole input and
on the previous k classifications. 1
The generative approach provides well-understood
training and decoding algorithms for HMMs and more
general graphical models. However, effective genera-
tive models require stringent conditional independence
assumptions. For instance, it is not practical to make the
label at a given position depend on a window on the in-
put sequence as well as the surrounding labels, since the
inference problem for the corresponding graphical model
would be intractable. Non-independent features of the
inputs, such as capitalization, suffixes, and surrounding
words, are important in dealing with words unseen in
training, but they are difficult to represent in generative
models.
The sequential classification approach can handle
many correlated features, as demonstrated in work on
maximum-entropy (McCallum et al, 2000; Ratnaparkhi,
1996) and a variety of other linear classifiers, including
winnow (Punyakanok and Roth, 2001), AdaBoost (Ab-
ney et al, 1999), and support-vector machines (Kudo and
Matsumoto, 2001). Furthermore, they are trained to min-
imize some function related to labeling error, leading to
smaller error in practice if enough training data are avail-
able. In contrast, generative models are trained to max-
imize the joint probability of the training data, which is
1Ramshaw and Marcus (1995) used transformation-based
learning (Brill, 1995), which for the present purposes can be
tought of as a classification-based method.
                                                               Edmonton, May-June 2003
                                                             Main Papers , pp. 134-141
                                                         Proceedings of HLT-NAACL 2003
not as closely tied to the accuracy metrics of interest if the
actual data was not generated by the model, as is always
the case in practice.
However, since sequential classifiers are trained to
make the best local decision, unlike generative mod-
els they cannot trade off decisions at different positions
against each other. In other words, sequential classifiers
are myopic about the impact of their current decision
on later decisions (Bottou, 1991; Lafferty et al, 2001).
This forced the best sequential classifier systems to re-
sort to heuristic combinations of forward-moving and
backward-moving sequential classifiers (Kudo and Mat-
sumoto, 2001).
Conditional random fields (CRFs) bring together the
best of generative and classification models. Like classi-
fication models, they can accommodate many statistically
correlated features of the inputs, and they are trained dis-
criminatively. But like generative models, they can trade
off decisions at different sequence positions to obtain a
globally optimal labeling. Lafferty et al (2001) showed
that CRFs beat related classification models as well as
HMMs on synthetic data and on a part-of-speech tagging
task.
In the present work, we show that CRFs beat all re-
ported single-model NP chunking results on the standard
evaluation dataset, and are statistically indistinguishable
from the previous best performer, a voting arrangement of
24 forward- and backward-looking support-vector clas-
sifiers (Kudo and Matsumoto, 2001). To obtain these
results, we had to abandon the original iterative scal-
ing CRF training algorithm for convex optimization al-
gorithms with better convergence properties. We provide
detailed comparisons between training methods.
The generalized perceptron proposed by Collins
(2002) is closely related to CRFs, but the best CRF train-
ing methods seem to have a slight edge over the general-
ized perceptron.
2 Conditional Random Fields
We focus here on conditional random fields on sequences,
although the notion can be used more generally (Laf-
ferty et al, 2001; Taskar et al, 2002). Such CRFs define
conditional probability distributions p(Y |X) of label se-
quences given input sequences. We assume that the ran-
dom variable sequences X and Y have the same length,
and use x = x1 ? ? ?xn and y = y1 ? ? ? yn for the generic
input sequence and label sequence, respectively.
A CRF on (X, Y ) is specified by a vector f of local
features and a corresponding weight vector ?. Each local
feature is either a state feature s(y, x, i) or a transition
feature t(y, y?, x, i), where y, y? are labels, x an input
sequence, and i an input position. To make the notation
more uniform, we also write
s(y, y?, x, i) = s(y?, x, i)
s(y, x, i) = s(yi, x, i)
t(y, x, i) =
{
t(yi?1, yi, x, i) i > 1
0 i = 1
for any state feature s and transition feature t. Typically,
features depend on the inputs around the given position,
although they may also depend on global properties of the
input, or be non-zero only at some positions, for instance
features that pick out the first or last labels.
The CRF?s global feature vector for input sequence x
and label sequence y is given by
F (y, x) =
?
i
f(y, x, i)
where i ranges over input positions. The conditional
probability distribution defined by the CRF is then
p?(Y |X) =
exp ? ? F (Y , X)
Z?(X)
(1)
where
Z?(x) =
?
y
exp ? ? F (y, x)
Any positive conditional distribution p(Y |X) that obeys
the Markov property
p(Yi|{Yj}j 6=i, X) = p(Yi|Yi?1, Yi+1, X)
can be written in the form (1) for appropriate choice of
feature functions and weight vector (Hammersley and
Clifford, 1971).
The most probable label sequence for input sequence
x is
y? = argmax
y
p?(y|x) = argmax
y
? ? F (y, x)
because Z?(x) does not depend on y. F (y, x) decom-
poses into a sum of terms for consecutive pairs of labels,
so the most likely y can be found with the Viterbi algo-
rithm.
We train a CRF by maximizing the log-likelihood of a
given training set T = {(xk, yk)}Nk=1, which we assume
fixed for the rest of this section:
L? =
?
k log p?(yk|xk)
=
?
k [? ? F (yk, xk) ? log Z?(xk)]
To perform this optimization, we seek the zero of the gra-
dient
?L? =
?
k
[
F (yk, xk) ? Ep?(Y |xk)F (Y , xk)
] (2)
In words, the maximum of the training data likelihood
is reached when the empirical average of the global fea-
ture vector equals its model expectation. The expectation
Ep?(Y |x)F (Y , x) can be computed efficiently using avariant of the forward-backward algorithm. For a given
x, define the transition matrix for position i as
Mi[y, y?] = exp ? ? f(y, y?, x, i)
Let f be any local feature, fi[y, y?] = f(y, y?, x, i),
F (y, x) =
?
i f(yi?1, yi, x, i), and let ? denotecomponent-wise matrix product. Then
Ep?(Y |x)F (Y , x) =
?
y
p?(y|x)F (y, x)
=
?
i
?i?1(fi ? Mi)?>i
Z?(x)
Z?(x) = ?n ? 1>
where ?i and ?i the forward and backward state-cost
vectors defined by
?i =
{
?i?1Mi 0 < i ? n
1 i = 0
?>i =
{
Mi+1?>i+1 1 ? i < n
1 i = n
Therefore, we can use a forward pass to compute the ?i
and a backward bass to compute the ?i and accumulate
the feature expectations.
To avoid overfitting, we penalize the likelihood with
a spherical Gaussian weight prior (Chen and Rosenfeld,
1999):
L?? =
?
k
[? ? F (yk, xk) ? log Z?(xk)]
? ???
2
2?2 + const
with gradient
?L?? =
?
k
[
F (yk, xk) ? Ep?(Y |xk)F (Y , xk)
]
? ??2
3 Training Methods
Lafferty et al (2001) used iterative scaling algorithms
for CRF training, following earlier work on maximum-
entropy models for natural language (Berger et al, 1996;
Della Pietra et al, 1997). Those methods are very sim-
ple and guaranteed to converge, but as Minka (2001) and
Malouf (2002) showed for classification, their conver-
gence is much slower than that of general-purpose convex
optimization algorithms when many correlated features
are involved. Concurrently with the present work, Wal-
lach (2002) tested conjugate gradient and second-order
methods for CRF training, showing significant training
speed advantages over iterative scaling on a small shal-
low parsing problem. Our work shows that precon-
ditioned conjugate-gradient (CG) (Shewchuk, 1994) or
limited-memory quasi-Newton (L-BFGS) (Nocedal and
Wright, 1999) perform comparably on very large prob-
lems (around 3.8 million features). We compare those
algorithms to generalized iterative scaling (GIS) (Dar-
roch and Ratcliff, 1972), non-preconditioned CG, and
voted perceptron training (Collins, 2002). All algorithms
except voted perceptron maximize the penalized log-
likelihood: ?? = argmax? L??. However, for ease ofexposition, this discussion of training methods uses the
unpenalized log-likelihood L?.
3.1 Preconditioned Conjugate Gradient
Conjugate-gradient (CG) methods have been shown to
be very effective in linear and non-linear optimization
(Shewchuk, 1994). Instead of searching along the gra-
dient, conjugate gradient searches along a carefully cho-
sen linear combination of the gradient and the previous
search direction.
CG methods can be accelerated by linearly trans-
forming the variables with preconditioner (Nocedal and
Wright, 1999; Shewchuk, 1994). The purpose of the pre-
conditioner is to improve the condition number of the
quadratic form that locally approximates the objective
function, so the inverse of Hessian is reasonable precon-
ditioner. However, this is not applicable to CRFs for two
reasons. First, the size of the Hessian is dim(?)2, lead-
ing to unacceptable space and time requirements for the
inversion. In such situations, it is common to use instead
the (inverse of) the diagonal of the Hessian. However in
our case the Hessian has the form
H? def= ?2L?
= ?
?
k
{E [F (Y , xk) ? F (Y , xk)]
?EF (Y , xk) ? EF (Y , xk)}
where the expectations are taken with respect to
p?(Y |xk). Therefore, every Hessian element, includ-
ing the diagonal ones, involve the expectation of a prod-
uct of global feature values. Unfortunately, computing
those expectations is quadratic on sequence length, as the
forward-backward algorithm can only compute expecta-
tions of quantities that are additive along label sequences.
We solve both problems by discarding the off-diagonal
terms and approximating expectation of the square of a
global feature by the expectation of the sum of squares of
the corresponding local features at each position. The ap-
proximated diagonal term Hf for feature f has the form
Hf = Ef(Y , xk)2
?
?
i
?
?
?
y,y?
Mi[y, y?]
Z?(x)
f(Y , xk)
?
?
2
If this approximation is semidefinite, which is trivial to
check, its inverse is an excellent preconditioner for early
iterations of CG training. However, when the model is
close to the maximum, the approximation becomes un-
stable, which is not surprising since it is based on fea-
ture independence assumptions that become invalid as
the weights of interaction features move away from zero.
Therefore, we disable the preconditioner after a certain
number of iterations, determined from held-out data. We
call this strategy mixed CG training.
3.2 Limited-Memory Quasi-Newton
Newton methods for nonlinear optimization use second-
order (curvature) information to find search directions.
As discussed in the previous section, it is not practi-
cal to obtain exact curvature information for CRF train-
ing. Limited-memory BFGS (L-BFGS) is a second-order
method that estimates the curvature numerically from
previous gradients and updates, avoiding the need for
an exact Hessian inverse computation. Compared with
preconditioned CG, L-BFGS can also handle large-scale
problems but does not require a specialized Hessian ap-
proximations. An earlier study indicates that L-BFGS
performs well in maximum-entropy classifier training
(Malouf, 2002).
There is no theoretical guidance on how much infor-
mation from previous steps we should keep to obtain
sufficiently accurate curvature estimates. In our exper-
iments, storing 3 to 10 pairs of previous gradients and
updates worked well, so the extra memory required over
preconditioned CG was modest. A more detailed descrip-
tion of this method can be found elsewhere (Nocedal and
Wright, 1999).
3.3 Voted Perceptron
Unlike other methods discussed so far, voted perceptron
training (Collins, 2002) attempts to minimize the differ-
ence between the global feature vector for a training in-
stance and the same feature vector for the best-scoring
labeling of that instance according to the current model.
More precisely, for each training instance the method
computes a weight update
?t+1 = ?t + F (yk, xk) ? F (y?k, xk) (3)
in which y?k is the Viterbi path
y?k = argmax
y
?t ? F (y,xk)
Like the familiar perceptron algorithm, this algorithm re-
peatedly sweeps over the training instances, updating the
weight vector as it considers each instance. Instead of
taking just the final weight vector, the voted perceptron
algorithm takes the average of the ?t. Collins (2002) re-
ported and we confirmed that this averaging reduces over-
fitting considerably.
4 Shallow Parsing
Figure 1 shows the base NPs in an example sentence. Fol-
lowing Ramshaw and Marcus (1995), the input to the
NP chunker consists of the words in a sentence anno-
tated automatically with part-of-speech (POS) tags. The
chunker?s task is to label each word with a label indi-
cating whether the word is outside a chunk (O), starts
a chunk (B), or continues a chunk (I). For example,
the tokens in first line of Figure 1 would be labeled
BIIBIIOBOBIIO.
4.1 Data Preparation
NP chunking results have been reported on two slightly
different data sets: the original RM data set of Ramshaw
and Marcus (1995), and the modified CoNLL-2000 ver-
sion of Tjong Kim Sang and Buchholz (2000). Although
the chunk tags in the RM and CoNLL-2000 are somewhat
different, we found no significant accuracy differences
between models trained on these two data sets. There-
fore, all our results are reported on the CoNLL-2000 data
set. We also used a development test set, provided by
Michael Collins, derived from WSJ section 21 tagged
with the Brill (1995) POS tagger.
4.2 CRFs for Shallow Parsing
Our chunking CRFs have a second-order Markov depen-
dency between chunk tags. This is easily encoded by
making the CRF labels pairs of consecutive chunk tags.
That is, the label at position i is yi = ci?1ci, where ci is
the chunk tag of word i, one of O, B, or I. Since Bmust be
used to start a chunk, the label OI is impossible. In addi-
tion, successive labels are constrained: yi?1 = ci?2ci?1,
yi = ci?1ci, and c0 = O. These contraints on the model
topology are enforced by giving appropriate features a
weight of ??, forcing all the forbidden labelings to have
zero probability.
Our choice of features was mainly governed by com-
puting power, since we do not use feature selection and
all features are used in training and testing. We use the
following factored representation for features
f(yi?1, yi, x, i) = p(x, i)q(yi?1, yi) (4)
where p(x, i) is a predicate on the input sequence x and
current position i and q(yi?1, yi) is a predicate on pairs
of labels. For instance, p(x, i) might be ?word at posi-
tion i is the? or ?the POS tags at positions i ? 1, i are
Rockwell International Corp. ?s Tulsa unit said it signed a tentative agreement extending
its contract with Boeing Co. to provide structural parts for Boeing ?s 747 jetliners .
Figure 1: NP chunks
q(yi?1, yi) p(x, i)
yi = y true
yi = y, yi?1 = y?
c(yi) = c
yi = y wi = w
or wi?1 = w
c(yi) = c wi+1 = w
wi?2 = w
wi+2 = w
wi?1 = w?, wi = w
wi+1 = w?, wi = w
ti = t
ti?1 = t
ti+1 = t
ti?2 = t
ti+2 = t
ti?1 = t?, ti = t
ti?2 = t?, ti?1 = t
ti = t?, ti+1 = t
ti+1 = t?, ti+2 = t
ti?2 = t??, ti?1 = t?, ti = t
ti?1 = t??, ti = t?, ti+1 = t
ti = t??, ti+1 = t?, ti+2 = t
Table 1: Shallow parsing features
DT, NN.? Because the label set is finite, such a factoring
of f(yi?1, yi, x, i) is always possible, and it allows each
input predicate to be evaluated just once for many fea-
tures that use it, making it possible to work with millions
of features on large training sets.
Table 1 summarizes the feature set. For a given po-
sition i, wi is the word, ti its POS tag, and yi its label.
For any label y = c?c, c(y) = c is the corresponding
chunk tag. For example, c(OB) = B. The use of chunk
tags as well as labels provides a form of backoff from
the very small feature counts that may arise in a second-
order model, while allowing significant associations be-
tween tag pairs and input predicates to be modeled. To
save time in some of our experiments, we used only the
820,000 features that are supported in the CoNLL train-
ing set, that is, the features that are on at least once. For
our highest F score, we used the complete feature set,
around 3.8 million in the CoNLL training set, which con-
tains all the features whose predicate is on at least once in
the training set. The complete feature set may in princi-
ple perform better because it can place negative weights
on transitions that should be discouraged if a given pred-
icate is on.
4.3 Parameter Tuning
As discussed previously, we need a Gaussian weight prior
to reduce overfitting. We also need to choose the num-
ber of training iterations since we found that the best F
score is attained while the log-likelihood is still improv-
ing. The reasons for this are not clear, but the Gaussian
prior may not be enough to keep the optimization from
making weight adjustments that slighly improve training
log-likelihood but cause large F score fluctuations. We
used the development test set mentioned in Section 4.1 to
set the prior and the number of iterations.
4.4 Evaluation Metric
The standard evaluation metrics for a chunker are preci-
sion P (fraction of output chunks that exactly match the
reference chunks), recall R (fraction of reference chunks
returned by the chunker), and their harmonic mean, the
F1 score F1 = 2 ? P ? R/(P + R) (which we call just
F score in what follows). The relationships between F
score and labeling error or log-likelihood are not direct,
so we report both F score and the other metrics for the
models we tested. For comparisons with other reported
results we use F score.
4.5 Significance Tests
Ideally, comparisons among chunkers would control for
feature sets, data preparation, training and test proce-
dures, and parameter tuning, and estimate the statistical
significance of performance differences. Unfortunately,
reported results sometimes leave out details needed for
accurate comparisons. We report F scores for comparison
with previous work, but we also give statistical signifi-
cance estimates using McNemar?s test for those methods
that we evaluated directly.
Testing the significance of F scores is tricky because
the wrong chunks generated by two chunkers are not
directly comparable. Yeh (2000) examined randomized
tests for estimating the significance of F scores, and in
particular the bootstrap over the test set (Efron and Tib-
shirani, 1993; Sang, 2002). However, bootstrap variances
in preliminary experiments were too high to allow any
conclusions, so we used instead a McNemar paired test
on labeling disagreements (Gillick and Cox, 1989).
Model F score
SVM combination 94.39%
(Kudo and Matsumoto, 2001)
CRF 94.38%
Generalized winnow 93.89%
(Zhang et al, 2002)
Voted perceptron 94.09%
MEMM 93.70%
Table 2: NP chunking F scores
5 Results
All the experiments were performed with our Java imple-
mentation of CRFs,designed to handle millions of fea-
tures, on 1.7 GHz Pentium IV processors with Linux and
IBM Java 1.3.0. Minor variants support voted perceptron
(Collins, 2002) and MEMMs (McCallum et al, 2000)
with the same efficient feature encoding. GIS, CG, and
L-BFGS were used to train CRFs and MEMMs.
5.1 F Scores
Table 2 gives representative NP chunking F scores for
previous work and for our best model, with the com-
plete set of 3.8 million features. The last row of the table
gives the score for an MEMM trained with the mixed CG
method using an approximate preconditioner. The pub-
lished F score for voted perceptron is 93.53% with a dif-
ferent feature set (Collins, 2002). The improved result
given here is for the supported feature set; the complete
feature set gives a slightly lower score of 94.07%. Zhang
et al (2002) reported a higher F score (94.38%) with gen-
eralized winnow using additional linguistic features that
were not available to us.
5.2 Convergence Speed
All the results in the rest of this section are for the smaller
supported set of 820,000 features. Figures 2a and 2b
show how preconditioning helps training convergence.
Since each CG iteration involves a line search that may
require several forward-backward procedures (typically
between 4 and 5 in our experiments), we plot the progress
of penalized log-likelihood L?? with respect to the num-ber of forward-backward evaluations. The objective func-
tion increases rapidly, achieving close proximity to the
maximum in a few iterations (typically 10). In contrast,
GIS training increases L?? rather slowly, never reachingthe value achieved by CG. The relative slowness of it-
erative scaling is also documented in a recent evaluation
of training methods for maximum-entropy classification
(Malouf, 2002). In theory, GIS would eventually con-
verge to the L?? optimum, but in practice convergencemay be so slow that L?? improvements may fall belownumerical accuracy, falsely indicating convergence.
training method time F score L??
Precond. CG 130 94.19% -2968
Mixed CG 540 94.20% -2990
Plain CG 648 94.04% -2967
L-BFGS 84 94.19% -2948
GIS 3700 93.55% -5668
Table 3: Runtime for various training methods
null hypothesis p-value
CRF vs. SVM 0.469
CRF vs. MEMM 0.00109
CRF vs. voted perceptron 0.116
MEMM vs. voted perceptron 0.0734
Table 4: McNemar?s tests on labeling disagreements
Mixed CG training converges slightly more slowly
than preconditioned CG. On the other hand, CG without
preconditioner converges much more slowly than both
preconditioned CG and mixed CG training. However, it
is still much faster than GIS. We believe that the superior
convergence rate of preconditioned CG is due to the use
of approximate second-order information. This is con-
firmed by the performance of L-BFGS, which also uses
approximate second-order information.2
Although there is no direct relationship between F
scores and log-likelihood, in these experiments F score
tends to follow log-likelihood. Indeed, Figure 3 shows
that preconditioned CG training improves test F scores
much more rapidly than GIS training.
Table 3 compares run times (in minutes) for reaching a
target penalized log-likelihood for various training meth-
ods with prior ? = 1.0. GIS is the only method that failed
to reach the target, after 3,700 iterations. We cannot place
the voted perceptron in this table, as it does not opti-
mize log-likelihood and does not use a prior. However,
it reaches a fairly good F-score above 93% in just two
training sweeps, but after that it improves more slowly, to
a somewhat lower score, than preconditioned CG train-
ing.
5.3 Labeling Accuracy
The accuracy rate for individual labeling decisions is
over-optimistic as an accuracy measure for shallow pars-
ing. For instance, if the chunk BIIIIIII is labled as
OIIIIIII, the labeling accuracy is 87.5%, but recall is
0. However, individual labeling errors provide a more
convenient basis for statistical significance tests. One
2Although L-BFGS has a slightly higher penalized log-
likelihood, its log-likelihood on the data is actually lower than
that of preconditioned CG and mixed CG training.
6 56 106 156 206 256
?35000
?30000
?25000
?20000
?15000
?10000
?5000
0
# of Forward?backward evaluations
Pe
na
liz
ed
 L
og
?l
ike
lih
oo
d
Comparison of Fast Training Algorithms for CRF
Preconditioned CG
Mixed CG Training
L?BFGS
(a) L??: CG (precond., mixed), L-BFGS
0 50 100 150 200 250 300 350 400 450 500
?200000
?180000
?160000
?140000
?120000
?100000
?80000
?60000
?40000
?20000
0
# of Forward?backward evaluations
Pe
na
liz
ed
 L
og
?l
ike
lih
oo
d
Comparison of CG Methods to GIS
Preconditioned CG
CG w/o Preconditioner
GIS
(b) L??: CG (precond., plain), GIS
Figure 2: Training convergence for various methods
0 50 100 150 200 250 300 350 400 450 500
0.45
0.5
0.55
0.6
0.65
0.7
0.75
0.8
0.85
0.9
0.95
# of Forward?backward evaluations
F 
sc
or
e
Comparison of CG Methods to GIS
Preconditioned CG
CG w/o Preconditioner
GIS
Figure 3: Test F scores vs. training time
such test is McNemar test on paired observations (Gillick
and Cox, 1989).
With McNemar?s test, we compare the correctness of
the labeling decisions of two models. The null hypothesis
is that the disagreements (correct vs. incorrect) are due to
chance. Table 4 summarizes the results of tests between
the models for which we had labeling decisions. These
tests suggest that MEMMs are significantly less accurate,
but that there are no significant differences in accuracy
among the other models.
6 Conclusions
We have shown that (log-)linear sequence labeling mod-
els trained discriminatively with general-purpose opti-
mization methods are a simple, competitive solution to
learning shallow parsers. These models combine the best
features of generative finite-state models and discrimina-
tive (log-)linear classifiers, and do NP chunking as well
as or better than ?ad hoc? classifier combinations, which
were the most accurate approach until now. In a longer
version of this work we will also describe shallow pars-
ing results for other phrase types. There is no reason why
the same techniques cannot be used equally successfully
for the other types or for other related tasks, such as POS
tagging or named-entity recognition.
On the machine-learning side, it would be interest-
ing to generalize the ideas of large-margin classification
to sequence models, strengthening the results of Collins
(2002) and leading to new optimal training algorithms
with stronger guarantees against overfitting.
On the application side, (log-)linear parsing models
have the potential to supplant the currently dominant
lexicalized PCFG models for parsing by allowing much
richer feature sets and simpler smoothing, while avoid-
ing the label bias problem that may have hindered earlier
classifier-based parsers (Ratnaparkhi, 1997). However,
work in that direction has so far addressed only parse
reranking (Collins and Duffy, 2002; Riezler et al, 2002).
Full discriminative parser training faces significant algo-
rithmic challenges in the relationship between parsing al-
ternatives and feature values (Geman and Johnson, 2002)
and in computing feature expectations.
Acknowledgments
John Lafferty and Andrew McCallum worked with the
second author on developing CRFs. McCallum helped
by the second author implemented the first conjugate-
gradient trainer for CRFs, which convinced us that train-
ing of large CRFs on large datasets would be practical.
Michael Collins helped us reproduce his generalized per-
cepton results and compare his method with ours. Erik
Tjong Kim Sang, who has created the best online re-
sources on shallow parsing, helped us with details of the
CoNLL-2000 shared task. Taku Kudo provided the out-
put of his SVM chunker for the significance test.
References
S. Abney. Parsing by chunks. In R. Berwick, S. Abney, and
C. Tenny, editors, Principle-based Parsing. Kluwer Aca-
demic Publishers, 1991.
S. Abney, R. E. Schapire, and Y. Singer. Boosting applied to
tagging and PP attachment. In Proc. EMNLP-VLC, New
Brunswick, New Jersey, 1999. ACL.
A. L. Berger, S. A. Della Pietra, and V. J. Della Pietra. A maxi-
mum entropy approach to natural language processing. Com-
putational Linguistics, 22(1), 1996.
D. M. Bikel, R. L. Schwartz, and R. M. Weischedel. An algo-
rithm that learns what?s in a name. Machine Learning, 34:
211?231, 1999.
L. Bottou. Une Approche the?orique de l?Apprentissage Con-
nexionniste: Applications a` la Reconnaissance de la Parole.
PhD thesis, Universite? de Paris XI, 1991.
E. Brill. Transformation-based error-driven learning and natural
language processing: a case study in part of speech tagging.
Computational Linguistics, 21:543?565, 1995.
S. F. Chen and R. Rosenfeld. A Gaussian prior for smoothing
maximum entropy models. Technical Report CMU-CS-99-
108, Carnegie Mellon University, 1999.
M. Collins. Discriminative training methods for hidden Markov
models: Theory and experiments with perceptron algo-
rithms. In Proc. EMNLP 2002. ACL, 2002.
M. Collins and N. Duffy. New ranking algorithms for parsing
and tagging: Kernels over discrete structures, and the voted
perceptron. In Proc. 40th ACL, 2002.
J. N. Darroch and D. Ratcliff. Generalized iterative scaling for
log-linear models. The Annals of Mathematical Statistics, 43
(5):1470?1480, 1972.
S. Della Pietra, V. Della Pietra, and J. Lafferty. Inducing fea-
tures of random fields. IEEE PAMI, 19(4):380?393, 1997.
B. Efron and R. J. Tibshirani. An Introduction to the Bootstrap.
Chapman & Hall/CRC, 1993.
D. Freitag and A. McCallum. Information extraction with
HMM structures learned by stochastic optimization. In
Proc. AAAI 2000, 2000.
S. Geman and M. Johnson. Dynamic programming for parsing
and estimation of stochastic unification-based grammars. In
Proc. 40th ACL, 2002.
L. Gillick and S. Cox. Some statistical issues in the compairson
of speech recognition algorithms. In International Confer-
ence on Acoustics Speech and Signal Processing, volume 1,
pages 532?535, 1989.
J. Hammersley and P. Clifford. Markov fields on finite graphs
and lattices. Unpublished manuscript, 1971.
T. Kudo and Y. Matsumoto. Chunking with support vector ma-
chines. In Proc. NAACL 2001. ACL, 2001.
J. Kupiec. Robust part-of-speech tagging using a hidden
Markov model. Computer Speech and Language, 6:225?242,
1992.
J. Lafferty, A. McCallum, and F. Pereira. Conditional random
fields: Probabilistic models for segmenting and labeling se-
quence data. In Proc. ICML-01, pages 282?289, 2001.
R. Malouf. A comparison of algorithms for maximum entropy
parameter estimation. In Proc. CoNLL-2002, 2002.
A. McCallum, D. Freitag, and F. Pereira. Maximum entropy
Markov models for information extraction and segmentation.
In Proc. ICML 2000, pages 591?598, Stanford, California,
2000.
T. P. Minka. Algorithms for maximum-likelihood logistic re-
gression. Technical Report 758, CMU Statistics Department,
2001.
J. Nocedal and S. J. Wright. Numerical Optimization. Springer,
1999.
V. Punyakanok and D. Roth. The use of classifiers in sequential
inference. In NIPS 13, pages 995?1001. MIT Press, 2001.
L. A. Ramshaw and M. P. Marcus. Text chunking using
transformation-based learning. In Proc. Third Workshop on
Very Large Corpora. ACL, 1995.
A. Ratnaparkhi. A maximum entropy model for part-of-speech
tagging. In Proc. EMNLP, New Brunswick, New Jersey,
1996. ACL.
A. Ratnaparkhi. A linear observed time statistical parser
based on maximum entropy models. In C. Cardie and
R. Weischedel, editors, EMNLP-2. ACL, 1997.
S. Riezler, T. H. King, R. M. Kaplan, R. Crouch, J. T.
Maxwell III, and M. Johnson. Parsing the Wall Street Journal
using a lexical-functional grammar and discriminative esti-
mation techniques. In Proc. 40th ACL, 2002.
E. F. T. K. Sang. Memory-based shallow parsing. Journal of
Machine Learning Research, 2:559?594, 2002.
J. R. Shewchuk. An introduction to the conjugate gradient
method without the agonizing pain, 1994. URL http://
www-2.cs.cmu.edu/?jrs/jrspapers.html#cg.
B. Taskar, P. Abbeel, and D. Koller. Discriminative probabilis-
tic models for relational data. In Eighteenth Conference on
Uncertainty in Artificial Intelligence, 2002.
E. F. Tjong Kim Sang and S. Buchholz. Introduction to the
CoNLL-2000 shared task: Chunking. In Proc. CoNLL-2000,
pages 127?132, 2000.
H. Wallach. Efficient training of conditional random fields. In
Proc. 6th Annual CLUK Research Colloquium, 2002.
A. Yeh. More accurate tests for the statistical significance of
result differences. In COLING-2000, pages 947?953, Saar-
bruecken, Germany, 2000.
T. Zhang, F. Damerau, and D. Johnson. Text chunking based
on a generalization of winnow. Journal of Machine Learning
Research, 2:615?637, 2002.
Proceedings of the 43rd Annual Meeting of the ACL, pages 91?98,
Ann Arbor, June 2005. c?2005 Association for Computational Linguistics
Online Large-Margin Training of Dependency Parsers
Ryan McDonald Koby Crammer Fernando Pereira
Department of Computer and Information Science
University of Pennsylvania
Philadelphia, PA
{ryantm,crammer,pereira}@cis.upenn.edu
Abstract
We present an effective training al-
gorithm for linearly-scored dependency
parsers that implements online large-
margin multi-class training (Crammer and
Singer, 2003; Crammer et al, 2003) on
top of efficient parsing techniques for de-
pendency trees (Eisner, 1996). The trained
parsers achieve a competitive dependency
accuracy for both English and Czech with
no language specific enhancements.
1 Introduction
Research on training parsers from annotated data
has for the most part focused on models and train-
ing algorithms for phrase structure parsing. The
best phrase-structure parsing models represent gen-
eratively the joint probability P (x,y) of sentence
x having the structure y (Collins, 1999; Charniak,
2000). Generative parsing models are very conve-
nient because training consists of computing proba-
bility estimates from counts of parsing events in the
training set. However, generative models make com-
plicated and poorly justified independence assump-
tions and estimations, so we might expect better per-
formance from discriminatively trained models, as
has been shown for other tasks like document classi-
fication (Joachims, 2002) and shallow parsing (Sha
and Pereira, 2003). Ratnaparkhi?s conditional max-
imum entropy model (Ratnaparkhi, 1999), trained
to maximize conditional likelihood P (y|x) of the
training data, performed nearly as well as generative
models of the same vintage even though it scores
parsing decisions in isolation and thus may suffer
from the label bias problem (Lafferty et al, 2001).
Discriminatively trained parsers that score entire
trees for a given sentence have only recently been
investigated (Riezler et al, 2002; Clark and Curran,
2004; Collins and Roark, 2004; Taskar et al, 2004).
The most likely reason for this is that discrimina-
tive training requires repeatedly reparsing the train-
ing corpus with the current model to determine the
parameter updates that will improve the training cri-
terion. The reparsing cost is already quite high
for simple context-free models with O(n3) parsing
complexity, but it becomes prohibitive for lexical-
ized grammars with O(n5) parsing complexity.
Dependency trees are an alternative syntactic rep-
resentation with a long history (Hudson, 1984). De-
pendency trees capture important aspects of func-
tional relationships between words and have been
shown to be useful in many applications includ-
ing relation extraction (Culotta and Sorensen, 2004),
paraphrase acquisition (Shinyama et al, 2002) and
machine translation (Ding and Palmer, 2005). Yet,
they can be parsed in O(n3) time (Eisner, 1996).
Therefore, dependency parsing is a potential ?sweet
spot? that deserves investigation. We focus here on
projective dependency trees in which a word is the
parent of all of its arguments, and dependencies are
non-crossing with respect to word order (see Fig-
ure 1). However, there are cases where crossing
dependencies may occur, as is the case for Czech
(Hajic?, 1998). Edges in a dependency tree may be
typed (for instance to indicate grammatical func-
tion). Though we focus on the simpler non-typed
91
root John hit the ball with the bat
Figure 1: An example dependency tree.
case, all algorithms are easily extendible to typed
structures.
The following work on dependency parsing is
most relevant to our research. Eisner (1996) gave
a generative model with a cubic parsing algorithm
based on an edge factorization of trees. Yamada and
Matsumoto (2003) trained support vector machines
(SVM) to make parsing decisions in a shift-reduce
dependency parser. As in Ratnaparkhi?s parser, the
classifiers are trained on individual decisions rather
than on the overall quality of the parse. Nivre and
Scholz (2004) developed a history-based learning
model. Their parser uses a hybrid bottom-up/top-
down linear-time heuristic parser and the ability to
label edges with semantic types. The accuracy of
their parser is lower than that of Yamada and Mat-
sumoto (2003).
We present a new approach to training depen-
dency parsers, based on the online large-margin
learning algorithms of Crammer and Singer (2003)
and Crammer et al (2003). Unlike the SVM
parser of Yamada and Matsumoto (2003) and Ratna-
parkhi?s parser, our parsers are trained to maximize
the accuracy of the overall tree.
Our approach is related to those of Collins and
Roark (2004) and Taskar et al (2004) for phrase
structure parsing. Collins and Roark (2004) pre-
sented a linear parsing model trained with an aver-
aged perceptron algorithm. However, to use parse
features with sufficient history, their parsing algo-
rithm must prune heuristically most of the possible
parses. Taskar et al (2004) formulate the parsing
problem in the large-margin structured classification
setting (Taskar et al, 2003), but are limited to pars-
ing sentences of 15 words or less due to computation
time. Though these approaches represent good first
steps towards discriminatively-trained parsers, they
have not yet been able to display the benefits of dis-
criminative training that have been seen in named-
entity extraction and shallow parsing.
Besides simplicity, our method is efficient and ac-
curate, as we demonstrate experimentally on English
and Czech treebank data.
2 System Description
2.1 Definitions and Background
In what follows, the generic sentence is denoted by
x (possibly subscripted); the ith word of x is de-
noted by xi. The generic dependency tree is denoted
by y. If y is a dependency tree for sentence x, we
write (i, j) ? y to indicate that there is a directed
edge from word xi to word xj in the tree, that is, xi
is the parent of xj . T = {(xt,yt)}Tt=1 denotes the
training data.
We follow the edge based factorization method of
Eisner (1996) and define the score of a dependency
tree as the sum of the score of all edges in the tree,
s(x,y) =
?
(i,j)?y
s(i, j) =
?
(i,j)?y
w ? f(i, j)
where f(i, j) is a high-dimensional binary feature
representation of the edge from xi to xj . For exam-
ple, in the dependency tree of Figure 1, the following
feature would have a value of 1:
f(i, j) =
{
1 if xi=?hit? and xj=?ball?
0 otherwise.
In general, any real-valued feature may be used, but
we use binary features for simplicity. The feature
weights in the weight vector w are the parameters
that will be learned during training. Our training al-
gorithms are iterative. We denote by w(i) the weight
vector after the ith training iteration.
Finally we define dt(x) as the set of possi-
ble dependency trees for the input sentence x and
bestk(x; w) as the set of k dependency trees in dt(x)
that are given the highest scores by weight vector w,
with ties resolved by an arbitrary but fixed rule.
Three basic questions must be answered for mod-
els of this form: how to find the dependency tree y
with highest score for sentence x; how to learn an
appropriate weight vector w from the training data;
and finally, what feature representation f(i, j) should
be used. The following sections address each of
these questions.
2.2 Parsing Algorithm
Given a feature representation for edges and a
weight vector w, we seek the dependency tree or
92
h1 h1 h2 h2
?
s h1 h1 r r+1 h2 h2 t
h1
h1 h2 h2
?
s h1 h1 h2 h2 t
h1
h1
s h1 h1 t
Figure 2: O(n3) algorithm of Eisner (1996), needs to keep 3 indices at any given stage.
trees that maximize the score function, s(x,y). The
primary difficulty is that for a given sentence of
length n there are exponentially many possible de-
pendency trees. Using a slightly modified version of
a lexicalized CKY chart parsing algorithm, it is pos-
sible to generate and represent these sentences in a
forest that is O(n5) in size and takes O(n5) time to
create.
Eisner (1996) made the observation that if the
head of each chart item is on the left or right periph-
ery, then it is possible to parse in O(n3). The idea is
to parse the left and right dependents of a word inde-
pendently and combine them at a later stage. This re-
moves the need for the additional head indices of the
O(n5) algorithm and requires only two additional
binary variables that specify the direction of the item
(either gathering left dependents or gathering right
dependents) and whether an item is complete (avail-
able to gather more dependents). Figure 2 shows
the algorithm schematically. As with normal CKY
parsing, larger elements are created bottom-up from
pairs of smaller elements.
Eisner showed that his algorithm is sufficient for
both searching the space of dependency parses and,
with slight modification, finding the highest scoring
tree y for a given sentence x under the edge fac-
torization assumption. Eisner and Satta (1999) give
a cubic algorithm for lexicalized phrase structures.
However, it only works for a limited class of lan-
guages in which tree spines are regular. Further-
more, there is a large grammar constant, which is
typically in the thousands for treebank parsers.
2.3 Online Learning
Figure 3 gives pseudo-code for the generic online
learning setting. A single training instance is con-
sidered on each iteration, and parameters updated
by applying an algorithm-specific update rule to the
instance under consideration. The algorithm in Fig-
ure 3 returns an averaged weight vector: an auxil-
iary weight vector v is maintained that accumulates
Training data: T = {(xt, yt)}Tt=1
1. w0 = 0; v = 0; i = 0
2. for n : 1..N
3. for t : 1..T
4. w(i+1) = update w(i) according to instance (xt, yt)
5. v = v + w(i+1)
6. i = i + 1
7. w = v/(N ? T )
Figure 3: Generic online learning algorithm.
the values of w after each iteration, and the returned
weight vector is the average of all the weight vec-
tors throughout training. Averaging has been shown
to help reduce overfitting (Collins, 2002).
2.3.1 MIRA
Crammer and Singer (2001) developed a natural
method for large-margin multi-class classification,
which was later extended by Taskar et al (2003) to
structured classification:
min ?w?
s.t. s(x,y) ? s(x,y?) ? L(y,y?)
?(x,y) ? T , y? ? dt(x)
where L(y,y?) is a real-valued loss for the tree y?
relative to the correct tree y. We define the loss of
a dependency tree as the number of words that have
the incorrect parent. Thus, the largest loss a depen-
dency tree can have is the length of the sentence.
Informally, this update looks to create a margin
between the correct dependency tree and each incor-
rect dependency tree at least as large as the loss of
the incorrect tree. The more errors a tree has, the
farther away its score will be from the score of the
correct tree. In order to avoid a blow-up in the norm
of the weight vector we minimize it subject to con-
straints that enforce the desired margin between the
correct and incorrect trees1.
1The constraints may be unsatisfiable, in which case we can
relax them with slack variables as in SVM training.
93
The Margin Infused Relaxed Algorithm
(MIRA) (Crammer and Singer, 2003; Cram-
mer et al, 2003) employs this optimization directly
within the online framework. On each update,
MIRA attempts to keep the norm of the change to
the parameter vector as small as possible, subject to
correctly classifying the instance under considera-
tion with a margin at least as large as the loss of the
incorrect classifications. This can be formalized by
substituting the following update into line 4 of the
generic online algorithm,
min
?
?w(i+1) ? w(i)
?
?
s.t. s(xt,yt) ? s(xt,y?) ? L(yt,y?)
?y? ? dt(xt)
(1)
This is a standard quadratic programming prob-
lem that can be easily solved using Hildreth?s al-
gorithm (Censor and Zenios, 1997). Crammer and
Singer (2003) and Crammer et al (2003) provide
an analysis of both the online generalization error
and convergence properties of MIRA. In equation
(1), s(x,y) is calculated with respect to the weight
vector after optimization, w(i+1).
To apply MIRA to dependency parsing, we can
simply see parsing as a multi-class classification
problem in which each dependency tree is one of
many possible classes for a sentence. However, that
interpretation fails computationally because a gen-
eral sentence has exponentially many possible de-
pendency trees and thus exponentially many margin
constraints.
To circumvent this problem we make the assump-
tion that the constraints that matter for large margin
optimization are those involving the incorrect trees
y? with the highest scores s(x,y?). The resulting
optimization made by MIRA (see Figure 3, line 4)
would then be:
min
?
?w(i+1) ? w(i)
?
?
s.t. s(xt,yt) ? s(xt,y?) ? L(yt,y?)
?y? ? bestk(xt; w(i))
reducing the number of constraints to the constant k.
We tested various values of k on a development data
set and found that small values of k are sufficient to
achieve close to best performance, justifying our as-
sumption. In fact, as k grew we began to observe a
slight degradation of performance, indicating some
overfitting to the training data. All the experiments
presented here use k = 5. The Eisner (1996) algo-
rithm can be modified to find the k-best trees while
only adding an additional O(k log k) factor to the
runtime (Huang and Chiang, 2005).
A more common approach is to factor the struc-
ture of the output space to yield a polynomial set of
local constraints (Taskar et al, 2003; Taskar et al,
2004). One such factorization for dependency trees
is
min
?
?w(i+1) ? w(i)
?
?
s.t. s(l, j) ? s(k, j) ? 1
?(l, j) ? yt, (k, j) /? yt
It is trivial to show that if these O(n2) constraints
are satisfied, then so are those in (1). We imple-
mented this model, but found that the required train-
ing time was much larger than the k-best formu-
lation and typically did not improve performance.
Furthermore, the k-best formulation is more flexi-
ble with respect to the loss function since it does not
assume the loss function can be factored into a sum
of terms for each dependency.
2.4 Feature Set
Finally, we need a suitable feature representation
f(i, j) for each dependency. The basic features in
our model are outlined in Table 1a and b. All fea-
tures are conjoined with the direction of attachment
as well as the distance between the two words being
attached. These features represent a system of back-
off from very specific features over words and part-
of-speech tags to less sparse features over just part-
of-speech tags. These features are added for both the
entire words as well as the 5-gram prefix if the word
is longer than 5 characters.
Using just features over the parent-child node
pairs in the tree was not enough for high accuracy,
because all attachment decisions were made outside
of the context in which the words occurred. To solve
this problem, we added two other types of features,
which can be seen in Table 1c. Features of the first
type look at words that occur between a child and
its parent. These features take the form of a POS
trigram: the POS of the parent, of the child, and of
a word in between, for all words linearly between
the parent and the child. This feature was particu-
larly helpful for nouns identifying their parent, since
94
a)
Basic Uni-gram Features
p-word, p-pos
p-word
p-pos
c-word, c-pos
c-word
c-pos
b)
Basic Big-ram Features
p-word, p-pos, c-word, c-pos
p-pos, c-word, c-pos
p-word, c-word, c-pos
p-word, p-pos, c-pos
p-word, p-pos, c-word
p-word, c-word
p-pos, c-pos
c)
In Between POS Features
p-pos, b-pos, c-pos
Surrounding Word POS Features
p-pos, p-pos+1, c-pos-1, c-pos
p-pos-1, p-pos, c-pos-1, c-pos
p-pos, p-pos+1, c-pos, c-pos+1
p-pos-1, p-pos, c-pos, c-pos+1
Table 1: Features used by system. p-word: word of parent node in dependency tree. c-word: word of child
node. p-pos: POS of parent node. c-pos: POS of child node. p-pos+1: POS to the right of parent in sentence.
p-pos-1: POS to the left of parent. c-pos+1: POS to the right of child. c-pos-1: POS to the left of child.
b-pos: POS of a word in between parent and child nodes.
it would typically rule out situations when a noun
attached to another noun with a verb in between,
which is a very uncommon phenomenon.
The second type of feature provides the local con-
text of the attachment, that is, the words before and
after the parent-child pair. This feature took the form
of a POS 4-gram: The POS of the parent, child,
word before/after parent and word before/after child.
The system also used back-off features to various tri-
grams where one of the local context POS tags was
removed. Adding these two features resulted in a
large improvement in performance and brought the
system to state-of-the-art accuracy.
2.5 System Summary
Besides performance (see Section 3), the approach
to dependency parsing we described has several
other advantages. The system is very general and
contains no language specific enhancements. In fact,
the results we report for English and Czech use iden-
tical features, though are obviously trained on differ-
ent data. The online learning algorithms themselves
are intuitive and easy to implement.
The efficient O(n3) parsing algorithm of Eisner
allows the system to search the entire space of de-
pendency trees while parsing thousands of sentences
in a few minutes, which is crucial for discriminative
training. We compare the speed of our model to a
standard lexicalized phrase structure parser in Sec-
tion 3.1 and show a significant improvement in pars-
ing times on the testing data.
The major limiting factor of the system is its re-
striction to features over single dependency attach-
ments. Often, when determining the next depen-
dent for a word, it would be useful to know previ-
ous attachment decisions and incorporate these into
the features. It is fairly straightforward to modify
the parsing algorithm to store previous attachments.
However, any modification would result in an as-
ymptotic increase in parsing complexity.
3 Experiments
We tested our methods experimentally on the Eng-
lish Penn Treebank (Marcus et al, 1993) and on the
Czech Prague Dependency Treebank (Hajic?, 1998).
All experiments were run on a dual 64-bit AMD
Opteron 2.4GHz processor.
To create dependency structures from the Penn
Treebank, we used the extraction rules of Yamada
and Matsumoto (2003), which are an approximation
to the lexicalization rules of Collins (1999). We split
the data into three parts: sections 02-21 for train-
ing, section 22 for development and section 23 for
evaluation. Currently the system has 6, 998, 447 fea-
tures. Each instance only uses a tiny fraction of these
features making sparse vector calculations possible.
Our system assumes POS tags as input and uses the
tagger of Ratnaparkhi (1996) to provide tags for the
development and evaluation sets.
Table 2 shows the performance of the systems
that were compared. Y&M2003 is the SVM-shift-
reduce parsing model of Yamada and Matsumoto
(2003), N&S2004 is the memory-based learner of
Nivre and Scholz (2004) and MIRA is the the sys-
tem we have described. We also implemented an av-
eraged perceptron system (Collins, 2002) (another
online learning algorithm) for comparison. This ta-
ble compares only pure dependency parsers that do
95
English Czech
Accuracy Root Complete Accuracy Root Complete
Y&M2003 90.3 91.6 38.4 - - -
N&S2004 87.3 84.3 30.4 - - -
Avg. Perceptron 90.6 94.0 36.5 82.9 88.0 30.3
MIRA 90.9 94.2 37.5 83.3 88.6 31.3
Table 2: Dependency parsing results for English and Czech. Accuracy is the number of words that correctly
identified their parent in the tree. Root is the number of trees in which the root word was correctly identified.
For Czech this is f-measure since a sentence may have multiple roots. Complete is the number of sentences
for which the entire dependency tree was correct.
not exploit phrase structure. We ensured that the
gold standard dependencies of all systems compared
were identical.
Table 2 shows that the model described here per-
forms as well or better than previous comparable
systems, including that of Yamada and Matsumoto
(2003). Their method has the potential advantage
that SVM batch training takes into account all of
the constraints from all training instances in the op-
timization, whereas online training only considers
constraints from one instance at a time. However,
they are fundamentally limited by their approximate
search algorithm. In contrast, our system searches
the entire space of dependency trees and most likely
benefits greatly from this. This difference is am-
plified when looking at the percentage of trees that
correctly identify the root word. The models that
search the entire space will not suffer from bad ap-
proximations made early in the search and thus are
more likely to identify the correct root, whereas the
approximate algorithms are prone to error propaga-
tion, which culminates with attachment decisions at
the top of the tree. When comparing the two online
learning models, it can be seen that MIRA outper-
forms the averaged perceptron method. This differ-
ence is statistically significant, p < 0.005 (McNe-
mar test on head selection accuracy).
In our Czech experiments, we used the depen-
dency trees annotated in the Prague Treebank, and
the predefined training, development and evaluation
sections of this data. The number of sentences in
this data set is nearly twice that of the English tree-
bank, leading to a very large number of features ?
13, 450, 672. But again, each instance uses just a
handful of these features. For POS tags we used the
automatically generated tags in the data set. Though
we made no language specific model changes, we
did need to make some data specific changes. In par-
ticular, we used the method of Collins et al (1999) to
simplify part-of-speech tags since the rich tags used
by Czech would have led to a large but rarely seen
set of POS features.
The model based on MIRA also performs well on
Czech, again slightly outperforming averaged per-
ceptron. Unfortunately, we do not know of any other
parsing systems tested on the same data set. The
Czech parser of Collins et al (1999) was run on a
different data set and most other dependency parsers
are evaluated using English. Learning a model from
the Czech training data is somewhat problematic
since it contains some crossing dependencies which
cannot be parsed by the Eisner algorithm. One trick
is to rearrange the words in the training set so that
all trees are nested. This at least allows the train-
ing algorithm to obtain reasonably low error on the
training set. We found that this did improve perfor-
mance slightly to 83.6% accuracy.
3.1 Lexicalized Phrase Structure Parsers
It is well known that dependency trees extracted
from lexicalized phrase structure parsers (Collins,
1999; Charniak, 2000) typically are more accurate
than those produced by pure dependency parsers
(Yamada and Matsumoto, 2003). We compared
our system to the Bikel re-implementation of the
Collins parser (Bikel, 2004; Collins, 1999) trained
with the same head rules of our system. There are
two ways to extract dependencies from lexicalized
phrase structure. The first is to use the automatically
generated dependencies that are explicit in the lex-
icalization of the trees, we call this system Collins-
auto. The second is to take just the phrase structure
output of the parser and run the automatic head rules
over it to extract the dependencies, we call this sys-
96
English
Accuracy Root Complete Complexity Time
Collins-auto 88.2 92.3 36.1 O(n5) 98m 21s
Collins-rules 91.4 95.1 42.6 O(n5) 98m 21s
MIRA-Normal 90.9 94.2 37.5 O(n3) 5m 52s
MIRA-Collins 92.2 95.8 42.9 O(n5) 105m 08s
Table 3: Results comparing our system to those based on the Collins parser. Complexity represents the
computational complexity of each parser and Time the CPU time to parse sec. 23 of the Penn Treebank.
tem Collins-rules. Table 3 shows the results compar-
ing our system, MIRA-Normal, to the Collins parser
for English. All systems are implemented in Java
and run on the same machine.
Interestingly, the dependencies that are automati-
cally produced by the Collins parser are worse than
those extracted statically using the head rules. Ar-
guably, this displays the artificialness of English de-
pendency parsing using dependencies automatically
extracted from treebank phrase-structure trees. Our
system falls in-between, better than the automati-
cally generated dependency trees and worse than the
head-rule extracted trees.
Since the dependencies returned from our system
are better than those actually learnt by the Collins
parser, one could argue that our model is actu-
ally learning to parse dependencies more accurately.
However, phrase structure parsers are built to max-
imize the accuracy of the phrase structure and use
lexicalization as just an additional source of infor-
mation. Thus it is not too surprising that the de-
pendencies output by the Collins parser are not as
accurate as our system, which is trained and built to
maximize accuracy on dependency trees. In com-
plexity and run-time, our system is a huge improve-
ment over the Collins parser.
The final system in Table 3 takes the output of
Collins-rules and adds a feature to MIRA-Normal
that indicates for given edge, whether the Collins
parser believed this dependency actually exists, we
call this system MIRA-Collins. This is a well known
discriminative training trick ? using the sugges-
tions of a generative system to influence decisions.
This system can essentially be considered a correc-
tor of the Collins parser and represents a significant
improvement over it. However, there is an added
complexity with such a model as it requires the out-
put of the O(n5) Collins parser.
k=1 k=2 k=5 k=10 k=20
Accuracy 90.73 90.82 90.88 90.92 90.91
Train Time 183m 235m 627m 1372m 2491m
Table 4: Evaluation of k-best MIRA approximation.
3.2 k-best MIRA Approximation
One question that can be asked is how justifiable is
the k-best MIRA approximation. Table 4 indicates
the accuracy on testing and the time it took to train
models with k = 1, 2, 5, 10, 20 for the English data
set. Even though the parsing algorithm is propor-
tional to O(k log k), empirically, the training times
scale linearly with k. Peak performance is achieved
very early with a slight degradation around k=20.
The most likely reason for this phenomenon is that
the model is overfitting by ensuring that even un-
likely trees are separated from the correct tree pro-
portional to their loss.
4 Summary
We described a successful new method for training
dependency parsers. We use simple linear parsing
models trained with margin-sensitive online training
algorithms, achieving state-of-the-art performance
with relatively modest training times and no need
for pruning heuristics. We evaluated the system on
both English and Czech data to display state-of-the-
art performance without any language specific en-
hancements. Furthermore, the model can be aug-
mented to include features over lexicalized phrase
structure parsing decisions to increase dependency
accuracy over those parsers.
We plan on extending our parser in two ways.
First, we would add labels to dependencies to rep-
resent grammatical roles. Those labels are very im-
portant for using parser output in tasks like infor-
mation extraction or machine translation. Second,
97
we are looking at model extensions to allow non-
projective dependencies, which occur in languages
such as Czech, German and Dutch.
Acknowledgments: We thank Jan Hajic? for an-
swering queries on the Prague treebank, and Joakim
Nivre for providing the Yamada and Matsumoto
(2003) head rules for English that allowed for a di-
rect comparison with our systems. This work was
supported by NSF ITR grants 0205456, 0205448,
and 0428193.
References
D.M. Bikel. 2004. Intricacies of Collins parsing model.
Computational Linguistics.
Y. Censor and S.A. Zenios. 1997. Parallel optimization :
theory, algorithms, and applications. Oxford Univer-
sity Press.
E. Charniak. 2000. A maximum-entropy-inspired parser.
In Proc. NAACL.
S. Clark and J.R. Curran. 2004. Parsing the WSJ using
CCG and log-linear models. In Proc. ACL.
M. Collins and B. Roark. 2004. Incremental parsing with
the perceptron algorithm. In Proc. ACL.
M. Collins, J. Hajic?, L. Ramshaw, and C. Tillmann. 1999.
A statistical parser for Czech. In Proc. ACL.
M. Collins. 1999. Head-Driven Statistical Models for
Natural Language Parsing. Ph.D. thesis, University
of Pennsylvania.
M. Collins. 2002. Discriminative training methods for
hidden Markov models: Theory and experiments with
perceptron algorithms. In Proc. EMNLP.
K. Crammer and Y. Singer. 2001. On the algorithmic
implementation of multiclass kernel based vector ma-
chines. JMLR.
K. Crammer and Y. Singer. 2003. Ultraconservative on-
line algorithms for multiclass problems. JMLR.
K. Crammer, O. Dekel, S. Shalev-Shwartz, and Y. Singer.
2003. Online passive aggressive algorithms. In Proc.
NIPS.
A. Culotta and J. Sorensen. 2004. Dependency tree ker-
nels for relation extraction. In Proc. ACL.
Y. Ding and M. Palmer. 2005. Machine translation using
probabilistic synchronous dependency insertion gram-
mars. In Proc. ACL.
J. Eisner and G. Satta. 1999. Efficient parsing for bilexi-
cal context-free grammars and head-automaton gram-
mars. In Proc. ACL.
J. Eisner. 1996. Three new probabilistic models for de-
pendency parsing: An exploration. In Proc. COLING.
J. Hajic?. 1998. Building a syntactically annotated cor-
pus: The Prague dependency treebank. Issues of Va-
lency and Meaning.
L. Huang and D. Chiang. 2005. Better k-best parsing.
Technical Report MS-CIS-05-08, University of Penn-
sylvania.
Richard Hudson. 1984. Word Grammar. Blackwell.
T. Joachims. 2002. Learning to Classify Text using Sup-
port Vector Machines. Kluwer.
J. Lafferty, A. McCallum, and F. Pereira. 2001. Con-
ditional random fields: Probabilistic models for seg-
menting and labeling sequence data. In Proc. ICML.
M. Marcus, B. Santorini, and M. Marcinkiewicz. 1993.
Building a large annotated corpus of english: the penn
treebank. Computational Linguistics.
J. Nivre and M. Scholz. 2004. Deterministic dependency
parsing of english text. In Proc. COLING.
A. Ratnaparkhi. 1996. A maximum entropy model for
part-of-speech tagging. In Proc. EMNLP.
A. Ratnaparkhi. 1999. Learning to parse natural
language with maximum entropy models. Machine
Learning.
S. Riezler, T. King, R. Kaplan, R. Crouch, J. Maxwell,
and M. Johnson. 2002. Parsing the Wall Street Journal
using a lexical-functional grammar and discriminative
estimation techniques. In Proc. ACL.
F. Sha and F. Pereira. 2003. Shallow parsing with condi-
tional random fields. In Proc. HLT-NAACL.
Y. Shinyama, S. Sekine, K. Sudo, and R. Grishman.
2002. Automatic paraphrase acquisition from news ar-
ticles. In Proc. HLT.
B. Taskar, C. Guestrin, and D. Koller. 2003. Max-margin
Markov networks. In Proc. NIPS.
B. Taskar, D. Klein, M. Collins, D. Koller, and C. Man-
ning. 2004. Max-margin parsing. In Proc. EMNLP.
H. Yamada and Y. Matsumoto. 2003. Statistical depen-
dency analysis with support vector machines. In Proc.
IWPT.
98
Proceedings of the 43rd Annual Meeting of the ACL, pages 491?498,
Ann Arbor, June 2005. c?2005 Association for Computational Linguistics
Simple Algorithms for Complex Relation Extraction
with Applications to Biomedical IE
Ryan McDonald1 Fernando Pereira1 Seth Kulick2
1CIS and 2IRCS, University of Pennsylvania, Philadelphia, PA
{ryantm,pereira}@cis.upenn.edu, skulick@linc.cis.upenn.edu
Scott Winters Yang Jin Pete White
Division of Oncology, Children?s Hospital of Pennsylvania, Philadelphia, PA
{winters,jin,white}@genome.chop.edu
Abstract
A complex relation is any n-ary relation
in which some of the arguments may be
be unspecified. We present here a simple
two-stage method for extracting complex
relations between named entities in text.
The first stage creates a graph from pairs
of entities that are likely to be related, and
the second stage scores maximal cliques
in that graph as potential complex relation
instances. We evaluate the new method
against a standard baseline for extracting
genomic variation relations from biomed-
ical text.
1 Introduction
Most research on text information extraction (IE)
has focused on accurate tagging of named entities.
Successful early named-entity taggers were based
on finite-state generative models (Bikel et al, 1999).
More recently, discriminatively-trained models have
been shown to be more accurate than generative
models (McCallum et al, 2000; Lafferty et al, 2001;
Kudo and Matsumoto, 2001). Both kinds of mod-
els have been developed for tagging entities such
as people, places and organizations in news mate-
rial. However, the rapid development of bioinfor-
matics has recently generated interest on the extrac-
tion of biological entities such as genes (Collier et
al., 2000) and genomic variations (McDonald et al,
2004b) from biomedical literature.
The next logical step for IE is to begin to develop
methods for extracting meaningful relations involv-
ing named entities. Such relations would be ex-
tremely useful in applications like question answer-
ing, automatic database generation, and intelligent
document searching and indexing. Though not as
well studied as entity extraction, relation extraction
has still seen a significant amount of work. We dis-
cuss some previous approaches at greater length in
Section 2.
Most relation extraction systems focus on the spe-
cific problem of extracting binary relations, such
as the employee of relation or protein-protein in-
teraction relation. Very little work has been done
in recognizing and extracting more complex rela-
tions. We define a complex relation as any n-ary
relation among n typed entities. The relation is
defined by the schema (t1, . . . , tn) where ti ? T
are entity types. An instance (or tuple) in the rela-
tion is a list of entities (e1, . . . , en) such that either
type(ei) = ti, or ei =? indicating that the ith ele-
ment of the tuple is missing.
For example, assume that the entity types
are T = {person, job, company} and we are
interested in the ternary relation with schema
(person, job, company) that relates a person
to their job at a particular company. For
the sentence ?John Smith is the CEO at Inc.
Corp.?, the system would ideally extract the tu-
ple (John Smith, CEO, Inc. Corp.). However, for
the sentence ?Everyday John Smith goes to his
office at Inc. Corp.?, the system would extract
(John Smith,?, Inc. Corp.), since there is no men-
tion of a job title. Hence, the goal of complex re-
lation extraction is to identify all instances of the
relation of interest in some piece of text, including
491
incomplete instances.
We present here several simple methods for ex-
tracting complex relations. All the methods start by
recognized pairs of entity mentions, that is, binary
relation instances, that appear to be arguments of the
relation of interest. Those pairs can be seen as the
edges of a graph with entity mentions as nodes. The
algorithms then try to reconstruct complex relations
by making tuples from selected maximal cliques in
the graph. The methods are general and can be ap-
plied to any complex relation fitting the above def-
inition. We also assume throughout the paper that
the entities and their type are known a priori in the
text. This is a fair assumption given the current high
standard of state-of-the-art named-entity extractors.
A primary advantage of factoring complex rela-
tions into binary relations is that it allows the use of
standard classification algorithms to decide whether
particular pairs of entity mentions are related. In ad-
dition, the factoring makes training data less sparse
and reduces the computational cost of extraction.
We will discuss these benefits further in Section 4.
We evaluated the methods on a large set of anno-
tated biomedical documents to extract relations re-
lated to genomic variations, demonstrating a consid-
erable improvement over a reasonable baseline.
2 Previous work
A representative approach to relation extraction is
the system of Zelenko et al (2003), which attempts
to identify binary relations in news text. In that
system, each pair of entity mentions of the correct
types in a sentence is classified as to whether it is
a positive instance of the relation. Consider the bi-
nary relation employee of and the sentence ?John
Smith, not Jane Smith, works at IBM?. The pair
(John Smith, IBM) is a positive instance, while the
pair (Jane Smith, IBM) is a negative instance. In-
stances are represented by a pair of entities and their
position in a shallow parse tree for the containing
sentence. Classification is done by a support-vector
classifier with a specialized kernel for that shallow
parse representation.
This approach ? enumerating all possible en-
tity pairs and classifying each as positive or nega-
tive ? is the standard method in relation extraction.
The main differences among systems are the choice
of trainable classifier and the representation for in-
stances.
For binary relations, this approach is quite
tractable: if the relation schema is (t1, t2), the num-
ber of potential instances is O(|t1| |t2|), where |t| is
the number of entity mentions of type t in the text
under consideration.
One interesting system that does not belong to
the above class is that of Miller et al (2000), who
take the view that relation extraction is just a form
of probabilistic parsing where parse trees are aug-
mented to identify all relations. Once this augmen-
tation is made, any standard parser can be trained
and then run on new sentences to extract new re-
lations. Miller et al show such an approach can
yield good results. However, it can be argued that
this method will encounter problems when consid-
ering anything but binary relations. Complex re-
lations would require a large amount of tree aug-
mentation and most likely result in extremely sparse
probability estimates. Furthermore, by integrating
relation extraction with parsing, the system cannot
consider long-range dependencies due to the local
parsing constraints of current probabilistic parsers.
The higher the arity of a relation, the more likely
it is that entities will be spread out within a piece
of text, making long range dependencies especially
important.
Roth and Yih (2004) present a model in which en-
tity types and relations are classified jointly using a
set of global constraints over locally trained classi-
fiers. This joint classification is shown to improve
accuracy of both the entities and relations returned
by the system. However, the system is based on con-
straints for binary relations only.
Recently, there has also been many results from
the biomedical IE community. Rosario and Hearst
(2004) compare both generative and discriminative
models for extracting seven relationships between
treatments and diseases. Though their models are
very flexible, they assume at most one relation per
sentence, ruling out cases where entities participate
in multiple relations, which is a common occurrence
in our data. McDonald et al (2004a) use a rule-
based parser combined with a rule-based relation
identifier to extract generic binary relations between
biological entities. As in predicate-argument extrac-
tion (Gildea and Jurafsky, 2002), each relation is
492
always associated with a verb in the sentence that
specifies the relation type. Though this system is
very general, it is limited by the fact that the design
ignores relations not expressed by a verb, as the em-
ployee of relation in?John Smith, CEO of Inc. Corp.,
announced he will resign?.
Most relation extraction systems work primarily
on a sentential level and never consider relations that
cross sentences or paragraphs. Since current data
sets typically only annotate intra-sentence relations,
this has not yet proven to be a problem.
3 Definitions
3.1 Complex Relations
Recall that a complex n-ary relation is specified by
a schema (t1, . . . , tn) where ti ? T are entity types.
Instances of the relation are tuples (e1, . . . , en)
where either type(ei) = ti, or ei =? (missing ar-
gument). The only restriction this definition places
on a relation is that the arity must be known. As we
discuss it further in Section 6, this is not required by
our methods but is assumed here for simplicity. We
also assume that the system works on a single rela-
tion type at a time, although the methods described
here are easily generalizable to systems that can ex-
tract many relations at once.
3.2 Graphs and Cliques
An undirected graph G = (V,E) is specified by a
set of vertices V and a set of edges E, with each
edge an unordered pair (u, v) of vertices. G? =
(V ?, E?) is a subgraph of G if V ? ? V and E? =
{(u, v) : u, v ? V ?, (u, v) ? E}. A clique C of G is
a subgraph of G in which there is an edge between
every pair of vertices. A maximal clique of G is a
clique C = (VC , EC) such that there is no other
clique C ? = (VC? , EC?) such that VC ? VC? .
4 Methods
We describe now a simple method for extracting
complex relations. This method works by first fac-
toring all complex relations into a set of binary re-
lations. A classifier is then trained in the standard
manner to recognize all pairs of related entities. Fi-
nally a graph is constructed from the output of this
classifier and the complex relations are determined
from the cliques of this graph.
a. All possible
relation instances
(John, CEO, Inc. Corp.)
(John,?, Inc. Corp.)
(John, CEO, Biz. Corp.)
(John,?, Biz. Corp.)
(John, CEO,?)
(Jane, CEO, Inc. Corp.)
(Jane,?, Inc. Corp.)
(Jane, CEO, Biz. Corp.)
(Jane,?, Biz. Corp.)
(Jane, CEO,?)
(?, CEO, Inc. Corp.)
(?, CEO, Biz. Corp.)
b. All possible
binary relations
(John, CEO)
(John, Inc. Corp.)
(John, Biz. Corp.)
(CEO, Inc. Corp.)
(CEO, Biz. Corp.)
(Jane, CEO)
(Jane, Inc. Corp.)
(Jane, Biz. Corp.)
Figure 1: Relation factorization of the sentence:
John and Jane are CEOs at Inc. Corp. and Biz.
Corp. respectively.
4.1 Classifying Binary Relations
Consider again the motivating example of the
(person, job, company) relation and the sentence
?John and Jane are CEOs at Inc. Corp. and Biz.
Corp. respectively?. This sentence contains two
people, one job title and two companies.
One possible method for extracting the rela-
tion of interest would be to first consider all 12
possible tuples shown in Figure 1a. Using all
these tuples, it should then be possible to train
a classifier to distinguish valid instances such as
(John, CEO, Inc. Corp.) from invalid ones such as
(Jane, CEO, Inc. Corp.). This is analogous to the
approach taken by Zelenko et al (2003) for binary
relations.
There are problems with this approach. Computa-
tionally, for an n-ary relation, the number of possi-
ble instances is O(|t1| |t2| ? ? ? |tn|). Conservatively,
letting m be the smallest |ti|, the run time is O(mn),
exponential in the arity of the relation. The second
problem is how to manage incomplete but correct
instances such as (John,?, Inc. Corp.) when train-
ing the classifier. If this instance is marked as neg-
ative, then the model might incorrectly disfavor fea-
tures that correlate John to Inc. Corp.. However,
if this instance is labeled positive, then the model
may tend to prefer the shorter and more compact in-
complete relations since they will be abundant in the
positive training examples. We could always ignore
instances of this form, but then the data would be
heavily skewed towards negative instances.
493
Instead of trying to classify all possible relation
instances, in this work we first classify pairs of en-
tities as being related or not. Then, as discussed in
Section 4.2, we reconstruct the larger complex rela-
tions from a set of binary relation instances.
Factoring relations into a set of binary decisions
has several advantages. The set of possible pairs is
much smaller then the set of all possible complex
relation instances. This can be seen in Figure 1b,
which only considers pairs that are consistent with
the relation definition. More generally, the num-
ber of pairs to classify is O((?i |ti|)2) , which is
far better than the exponentially many full relation
instances. There is also no ambiguity when label-
ing pairs as positive or negative when constructing
the training data. Finally, we can rely on previous
work on classification for binary relation extraction
to identify pairs of related entities.
To train a classifier to identify pairs of related
entities, we must first create the set of all positive
and negative pairs in the data. The positive in-
stances are all pairs that occur together in a valid
tuple. For the example sentence in Figure 1, these
include the pairs (John, CEO), (John, Inc. Corp.),
(CEO, Inc. Corp.), (CEO, Biz. Corp.), (Jane, CEO)
and (Jane, Biz. Corp.). To gather negative in-
stances, we extract all pairs that never occur to-
gether in a valid relation. From the same exam-
ple these would be the pairs (John, Biz. Corp.) and
(Jane, Inc. Corp.).
This leads to a large set of positive and negative
binary relation instances. At this point we could em-
ploy any binary relation classifier and learn to iden-
tify new instances of related pairs of entities. We
use a standard maximum entropy classifier (Berger
et al, 1996) implemented as part of MALLET (Mc-
Callum, 2002). The model is trained using the fea-
tures listed in Table 1.
This is a very simple binary classification model.
No deep syntactic structure such as parse trees is
used. All features are basically over the words sepa-
rating two entities and their part-of-speech tags. Of
course, it would be possible to use more syntactic
information if available in a manner similar to that
of Zelenko et al (2003). However, the primary pur-
pose of our experiments was not to create a better
binary relation extractor, but to see if complex re-
lations could be extracted through binary factoriza-
Feature Set
entity type of e1 and e2
words in e1 and e2
word bigrams in e1 and e2
POS of e1 and e2
words between e1 and e2
word bigrams between e1 and e2
POS between e1 and e2
distance between e1 and e2
concatenations of above features
Table 1: Feature set for maximum entropy binary
relation classifier. e1 and e2 are entities.
a. Relation graph G
John Jane
CEO
Inc. Corp. Biz. Corp.
b. Tuples from G
(John, CEO,?)
(John,?, Inc. Corp.)
(John,?, Biz. Corp.)
(Jane, CEO,?)
(?, CEO, Inc. Corp.)
(?, CEO, Biz. Corp.)
(John, CEO, Inc. Corp.)
(John, CEO, Biz. Corp.)
Figure 2: Example of a relation graph and tuples
from all the cliques in the graph.
tion followed by reconstruction. In Section 5.2 we
present an empirical evaluation of the binary relation
classifier.
4.2 Reconstructing Complex Relations
4.2.1 Maximal Cliques
Having identified all pairs of related entities in the
text, the next stage is to reconstruct the complex re-
lations from these pairs. Let G = (V,E) be an undi-
rected graph where the vertices V are entity men-
tions in the text and the edges E represent binary
relations between entities. We reconstruct the com-
plex relation instances by finding maximal cliques
in the graphs.
The simplest approach is to create the graph
so that two entities in the graph have an edge
if the binary classifier believes they are related.
For example, consider the binary factoriza-
tion in Figure 1 and imagine the classifier
identified the following pairs as being related:
(John, CEO), (John, Inc. Corp.), (John, Biz. Corp.),
(CEO, Inc. Corp.), (CEO, Biz. Corp.) and
(Jane, CEO). The resulting graph can be seen
in Figure 2a.
Looking at this graph, one solution to construct-
494
ing complex relations would be to consider all the
cliques in the graph that are consistent with the def-
inition of the relation. This is equivalent to having
the system return only relations in which the binary
classifier believes that all of the entities involved are
pairwise related. All the cliques in the example are
shown in Figure 2b. We add ? fields to the tuples to
be consistent with the relation definition.
This could lead to a set of overlapping
cliques, for instance (John, CEO, Inc. Corp.) and
(John, CEO,?). Instead of having the system re-
turn all cliques, our system just returns the maximal
cliques, that is, those cliques that are not subsets of
other cliques. Hence, for the example under con-
sideration in Figure 2, the system would return the
one correct relation, (John, CEO, Inc. Corp.), and
two incorrect relations, (John, CEO, Biz. Corp.) and
(Jane, CEO,?). The second is incorrect since it
does not specify the company slot of the relation
even though that information is present in the text.
It is possible to find degenerate sentences in which
perfect binary classification followed by maximal
clique reconstruction will lead to errors. One such
sentence is, ?John is C.E.O. and C.F.O. of Inc. Corp.
and Biz. Corp. respectively and Jane vice-versa?.
However, we expect such sentences to be rare; in
fact, they never occur in our data.
The real problem with this approach is that an ar-
bitrary graph can have exponentially many cliques,
negating any efficiency advantage over enumerating
all n-tuples of entities. Fortunately, there are algo-
rithms for finding all maximal cliques that are effi-
cient in practice. We use the algorithm of Bron and
Kerbosch (1973). This is a well known branch and
bound algorithm that has been shown to empirically
run linearly in the number of maximal cliques in the
graph. In our experiments, this algorithm found all
maximal cliques in a matter of seconds.
4.2.2 Probabilistic Cliques
The above approach has a major shortcom-
ing in that it assumes the output of the bi-
nary classifier to be absolutely correct. For
instance, the classifier may have thought with
probability 0.49, 0.99 and 0.99 that the fol-
lowing pairs were related: (Jane, Biz. Corp.),
(CEO, Biz. Corp.) and (Jane, CEO) respectively.
The maximal clique method would not produce the
tuple (Jane, CEO, Biz. Corp.) since it never consid-
ers the edge between Jane and Biz. Corp. However,
given the probability of the edges, we would almost
certainly want this tuple returned.
What we would really like to model is a belief
that on average a clique represents a valid relation
instance. To do this we use the complete graph
G = (V,E) with edges between all pairs of entity
mentions. We then assign weight w(e) to edge e
equal to the probability that the two entities in e are
related, according to the classifier. We define the
weight of a clique w(C) as the mean weight of the
edges in the clique. Since edge weights represent
probabilities (or ratios), we use the geometric mean
w(C) =
?
?
?
e?EC
w(e)
?
?
1/|EC |
We decide that a clique C represents a valid tuple if
w(C) ? 0.5. Hence, the system finds all maximal
cliques as before, but considers only those where
w(C) ? 0.5, and it may select a non-maximal clique
if the weight of all larger cliques falls below the
threshold. The cutoff of 0.5 is not arbitrary, since it
ensures that the average probability of a clique rep-
resenting a relation instance is at least as large as
the average probability of it not representing a rela-
tion instance. We ran experiments with varying lev-
els of this threshold and found that, roughly, lower
thresholds result in higher precision at the expense
of recall since the system returns fewer but larger
tuples. Optimum results were obtained for a cutoff
of approximately 0.4, but we report results only for
w(C) ? 0.5.
The major problem with this approach is that
there will always be exponentially many cliques
since the graph is fully connected. However, in our
experiments we pruned all edges that would force
any containing clique C to have w(C) < 0.5. This
typically made the graphs very sparse.
Another problem with this approach is the as-
sumption that the binary relation classifier outputs
probabilities. For maximum entropy and other prob-
abilistic frameworks this is not an issue. However,
many classifiers, such as SVMs, output scores or
distances. It is possible to transform the scores from
those models through a sigmoid to yield probabili-
495
ties, but there is no guarantee that those probability
values will be well calibrated.
5 Experiments
5.1 Problem Description and Data
We test these methods on the task of extracting ge-
nomic variation events from biomedical text (Mc-
Donald et al, 2004b). Briefly, we define a varia-
tion event as an acquired genomic aberration: a spe-
cific, one-time alteration at the genomic level and
described at the nucleic acid level, amino acid level
or both. Each variation event is identified by the re-
lationship between a type of variation, its location,
and the corresponding state change from an initial-
state to an altered-state. This can be formalized as
the following complex schema
(var-type, location, initial-state, altered-state)
A simple example is the sentence
?At codons 12 and 61, the occurrence of
point mutations from G/A to T/G were observed?
which gives rise to the tuples
(point mutation, codon 12, G, T)
(point mutation, codon 61, A, G)
Our data set consists of 447 abstracts selected
from MEDLINE as being relevant to populating a
database with facts of the form: gene X with vari-
ation event Y is associated with malignancy Z. Ab-
stracts were randomly chosen from a larger corpus
identified as containing variation mentions pertain-
ing to cancer.
The current data consists of 4691 sentences that
have been annotated with 4773 entities and 1218 re-
lations. Of the 1218 relations, 760 have two ? ar-
guments, 283 have one ? argument, and 175 have
no ? arguments. Thus, 38% of the relations tagged
in this data cannot be handled using binary relation
classification alone. In addition, 4% of the relations
annotated in this data are non-sentential. Our sys-
tem currently only produces sentential relations and
is therefore bounded by a maximum recall of 96%.
Finally, we use gold standard entities in our exper-
iments. This way we can evaluate the performance
of the relation extraction system isolated from any
kind of pipelined entity extraction errors. Entities in
this domain can be found with fairly high accuracy
(McDonald et al, 2004b).
It is important to note that just the presence of two
entity types does not entail a relation between them.
In fact, 56% of entity pairs are not related, due either
to explicit disqualification in the text (e.g. ?... the
lack of G to T transversion ...?) or ambiguities that
arise from multiple entities of the same type.
5.2 Results
Because the data contains only 1218 examples of re-
lations we performed 10-fold cross-validation tests
for all results. We compared three systems:
? MC: Uses the maximum entropy binary classi-
fier coupled with the maximal clique complex
relation reconstructor.
? PC: Same as above, except it uses the proba-
bilistic clique complex relation reconstructor.
? NE: A maximum entropy classifier that naively
enumerates all possible relation instances as
described in Section 4.1.
In training system NE, all incomplete but correct
instances were marked as positive since we found
this had the best performance. We used the same
pairwise entity features in the binary classifier of
the above two systems. However, we also added
higher order versions of the pairwise features. For
this system we only take maximal relations,that is,
if (John, CEO, Inc. Corp.) and (John,?, Inc. Corp.)
are both labeled positive, the system would only re-
turn the former.
Table 2 contains the results of the maximum en-
tropy binary relation classifier (used in systems MC
and PC). The 1218 annotated complex relations pro-
duced 2577 unique binary pairs of related entities.
We can see that the maximum entropy classifier per-
forms reasonably well, although performance may
be affected by the lack of rich syntactic features,
which have been shown to help performance (Miller
et al, 2000; Zelenko et al, 2003).
Table 3 compares the three systems on the real
problem of extracting complex relations. An ex-
tracted complex relation is considered correct if and
only if all the entities in the relation are correct.
There is no partial credit. All training and clique
finding algorithms took under 5 minutes for the en-
tire data set. Naive enumeration took approximately
26 minutes to train.
496
ACT PRD COR
2577 2722 2101
Prec Rec F-Meas
0.7719 0.8153 0.7930
Table 2: Binary relation classification results for the
maximum entropy classifier. ACT: actual number of
related pairs, PRD: predicted number of related pairs
and COR: correctly identified related pairs.
System Prec Rec F-Meas
NE 0.4588 0.6995 0.5541
MC 0.5812 0.7315 0.6480
PC 0.6303 0.7726 0.6942
Table 3: Full relation classification results. For a
relation to be classified correctly, all the entities in
the relation must be correctly identified.
First we observe that the maximal clique method
combined with maximum entropy (system MC) re-
duces the relative error rate over naively enumer-
ating and classifying all instances (system NE) by
21%. This result is very positive. The system based
on binary factorization not only is more efficient
then naively enumerating all instances, but signifi-
cantly outperforms it as well. The main reason naive
enumeration does so poorly is that all correct but
incomplete instances are marked as positive. Thus,
even slight correlations between partially correct en-
tities would be enough to classify an instance as cor-
rect, which results in relatively good recall but poor
precision. We tried training only with correct and
complete positive instances, but the result was a sys-
tem that only returned few relations since negative
instances overwhelmed the training set. With fur-
ther tuning, it may be possible to improve the per-
formance of this system. However, we use it only as
a baseline and to demonstrate that binary factoriza-
tion is a feasible and accurate method for extracting
complex relations.
Furthermore, we see that using probabilistic
cliques (system PC) provides another large im-
provement, a relative error reduction of 13%
over using maximal cliques and 31% reduction
over enumeration. Table 4 shows the breakdown
of relations returned by type. There are three
types of relations, 2-ary, 3-ary and 4-ary, each
with 2, 1 and 0 ? arguments respectively, e.g.
System 2-ary 3-ary 4-ary
NE 760:1097:600 283:619:192 175:141:60
MC 760:1025:601 283:412:206 175:95:84
PC 760:870:590 283:429:223 175:194:128
Table 4: Breakdown of true positive relations by
type that were returned by each system. Each cell
contains three numbers, Actual:Predicted:Correct,
which represents for each arity the actual, predicted
and correct number of relations for each system.
(point mutation, codon 12,?,?) is a 2-ary relation.
Clearly the probabilistic clique method is much
more likely to find larger non-binary relations, veri-
fying the motivation that there are some low proba-
bility edges that can still contribute to larger cliques.
6 Conclusions and Future Work
We presented a method for complex relation extrac-
tion, the core of which was to factorize complex re-
lations into sets of binary relations, learn to identify
binary relations and then reconstruct the complex re-
lations by finding maximal cliques in graphs that
represent relations between pairs of entities. The
primary advantage of this method is that it allows
for the use of almost any binary relation classifier,
which have been well studied and are often accu-
rate. We showed that such a method can be suc-
cessful with an empirical evaluation on a large set
of biomedical data annotated with genomic varia-
tion relations. In fact, this approach is both signifi-
cantly quicker and more accurate then enumerating
and classifying all possible instances. We believe
this work provides a good starting point for contin-
ued research in this area.
A distinction may be made between the factored
system presented here and one that attempts to clas-
sify complex relations without factorization. This
is related to the distinction between methods that
learn local classifiers that are combined with global
constraints after training and methods that incorpo-
rate the global constraints into the learning process.
McCallum and Wellner (2003) showed that learning
binary co-reference relations globally improves per-
formance over learning relations in isolation. How-
ever, their model relied on the transitive property in-
herent in the co-reference relation. Our system can
be seen as an instance of a local learner. Punyakanok
497
et al (2004) argued that local learning actually out-
performs global learning in cases when local deci-
sions can easily be learnt by the classifier. Hence, it
is reasonable to assume that our binary factorization
method will perform well when binary relations can
be learnt with high accuracy.
As for future work, there are many things that we
plan to look at. The binary relation classifier we em-
ploy is quite simplistic and most likely can be im-
proved by using features over a deeper representa-
tion of the data such as parse trees. Other more pow-
erful binary classifiers should be tried such as those
based on tree kernels (Zelenko et al, 2003). We also
plan on running these algorithms on more data sets
to test if the algorithms empirically generalize to dif-
ferent domains.
Perhaps the most interesting open problem is how
to learn the complex reconstruction phase. One pos-
sibility is recent work on supervised clustering. Let-
ting the edge probabilities in the graphs represent a
distance in some space, it may be possible to learn
how to cluster vertices into relational groups. How-
ever, since a vertex/entity can participate in one or
more relation, any clustering algorithm would be re-
quired to produce non-disjoint clusters.
We mentioned earlier that the only restriction of
our complex relation definition is that the arity of
the relation must be known in advance. It turns out
that the algorithms we described can actually handle
dynamic arity relations. All that is required is to
remove the constraint that maximal cliques must be
consistent with the structure of the relation. This
represents another advantage of binary factorization
over enumeration, since it would be infeasible to
enumerate all possible instances for dynamic arity
relations.
Acknowledgments
The authors would like to thank Mark Liberman,
Mark Mandel and Eric Pancoast for useful discus-
sion, suggestions and technical support. This work
was supported in part by NSF grant ITR 0205448.
References
A. L. Berger, S. A. Della Pietra, and V. J. Della Pietra.
1996. A maximum entropy approach to natural lan-
guage processing. Computational Linguistics, 22(1).
D.M. Bikel, R. Schwartz, and R.M. Weischedel. 1999.
An algorithm that learns what?s in a name. Machine
Learning Journal Special Issue on Natural Language
Learning, 34(1/3):221?231.
C. Bron and J. Kerbosch. 1973. Algorithm 457: finding
all cliques of an undirected graph. Communications of
the ACM, 16(9):575?577.
N. Collier, C. Nobata, and J. Tsujii. 2000. Extracting
the names of genes and gene products with a hidden
Markov model. In Proc. COLING.
D. Gildea and D. Jurafsky. 2002. Automatic labeling of
semantic roles. Computational Linguistics.
Taku Kudo and Yuji Matsumoto. 2001. Chunking with
support vector machines. In Proc. NAACL.
J. Lafferty, A. McCallum, and F. Pereira. 2001. Con-
ditional random fields: Probabilistic models for seg-
menting and labeling sequence data. In Proc. ICML.
A. McCallum and B. Wellner. 2003. Toward condi-
tional models of identity uncertainty with application
to proper noun coreference. In IJCAI Workshop on In-
formation Integration on the Web.
A. McCallum, D. Freitag, and F. Pereira. 2000. Maxi-
mum entropy Markov models for information extrac-
tion and segmentation. In Proc. ICML.
A. K. McCallum. 2002. MALLET: A machine learning
for language toolkit.
D.M. McDonald, H. Chen, H. Su, and B.B. Marshall.
2004a. Extracting gene pathway relations using a hy-
brid grammar: the Arizona Relation Parser. Bioinfor-
matics, 20(18):3370?78.
R.T. McDonald, R.S. Winters, M. Mandel, Y. Jin, P.S.
White, and F. Pereira. 2004b. An entity tagger for
recognizing acquired genomic variations in cancer lit-
erature. Bioinformatics, 20(17):3249?3251.
S. Miller, H. Fox, L.A. Ramshaw, and R.M. Weischedel.
2000. A novel use of statistical parsing to extract in-
formation from text. In Proc. NAACL.
V. Punyakanok, D. Roth, W. Yih, and D. Zimak. 2004.
Learning via inference over structurally constrained
output. In Workshop on Learning Structured with Out-
put, NIPS.
Barbara Rosario and Marti A. Hearst. 2004. Classifying
semantic relations in bioscience texts. In ACL.
D. Roth and W. Yih. 2004. A linear programming formu-
lation for global inference in natural language tasks. In
Proc. CoNLL.
D. Zelenko, C. Aone, and A. Richardella. 2003. Kernel
methods for relation extraction. JMLR.
498
Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 440?447,
Prague, Czech Republic, June 2007. c?2007 Association for Computational Linguistics
Biographies, Bollywood, Boom-boxes and Blenders:
Domain Adaptation for Sentiment Classification
John Blitzer Mark Dredze
Department of Computer and Information Science
University of Pennsylvania
{blitzer|mdredze|pereria@cis.upenn.edu}
Fernando Pereira
Abstract
Automatic sentiment classification has been
extensively studied and applied in recent
years. However, sentiment is expressed dif-
ferently in different domains, and annotating
corpora for every possible domain of interest
is impractical. We investigate domain adap-
tation for sentiment classifiers, focusing on
online reviews for different types of prod-
ucts. First, we extend to sentiment classifi-
cation the recently-proposed structural cor-
respondence learning (SCL) algorithm, re-
ducing the relative error due to adaptation
between domains by an average of 30% over
the original SCL algorithm and 46% over
a supervised baseline. Second, we identify
a measure of domain similarity that corre-
lates well with the potential for adaptation
of a classifier from one domain to another.
This measure could for instance be used to
select a small set of domains to annotate
whose trained classifiers would transfer well
to many other domains.
1 Introduction
Sentiment detection and classification has received
considerable attention recently (Pang et al, 2002;
Turney, 2002; Goldberg and Zhu, 2004). While
movie reviews have been the most studied domain,
sentiment analysis has extended to a number of
new domains, ranging from stock message boards
to congressional floor debates (Das and Chen, 2001;
Thomas et al, 2006). Research results have been
deployed industrially in systems that gauge market
reaction and summarize opinion from Web pages,
discussion boards, and blogs.
With such widely-varying domains, researchers
and engineers who build sentiment classification
systems need to collect and curate data for each new
domain they encounter. Even in the case of market
analysis, if automatic sentiment classification were
to be used across a wide range of domains, the ef-
fort to annotate corpora for each domain may be-
come prohibitive, especially since product features
change over time. We envision a scenario in which
developers annotate corpora for a small number of
domains, train classifiers on those corpora, and then
apply them to other similar corpora. However, this
approach raises two important questions. First, it
is well known that trained classifiers lose accuracy
when the test data distribution is significantly differ-
ent from the training data distribution 1. Second, it is
not clear which notion of domain similarity should
be used to select domains to annotate that would be
good proxies for many other domains.
We propose solutions to these two questions and
evaluate them on a corpus of reviews for four differ-
ent types of products from Amazon: books, DVDs,
electronics, and kitchen appliances2. First, we show
how to extend the recently proposed structural cor-
1For surveys of recent research on domain adaptation, see
the ICML 2006 Workshop on Structural Knowledge Transfer
for Machine Learning (http://gameairesearch.uta.
edu/) and the NIPS 2006 Workshop on Learning when test
and training inputs have different distribution (http://ida.
first.fraunhofer.de/projects/different06/)
2The dataset will be made available by the authors at publi-
cation time.
440
respondence learning (SCL) domain adaptation al-
gorithm (Blitzer et al, 2006) for use in sentiment
classification. A key step in SCL is the selection of
pivot features that are used to link the source and tar-
get domains. We suggest selecting pivots based not
only on their common frequency but also according
to their mutual information with the source labels.
For data as diverse as product reviews, SCL can
sometimes misalign features, resulting in degrada-
tion when we adapt between domains. In our second
extension we show how to correct misalignments us-
ing a very small number of labeled instances.
Second, we evaluate the A-distance (Ben-David
et al, 2006) between domains as measure of the loss
due to adaptation from one to the other. The A-
distance can be measured from unlabeled data, and it
was designed to take into account only divergences
which affect classification accuracy. We show that it
correlates well with adaptation loss, indicating that
we can use the A-distance to select a subset of do-
mains to label as sources.
In the next section we briefly review SCL and in-
troduce our new pivot selection method. Section 3
describes datasets and experimental method. Sec-
tion 4 gives results for SCL and the mutual informa-
tion method for selecting pivot features. Section 5
shows how to correct feature misalignments using a
small amount of labeled target domain data. Sec-
tion 6 motivates the A-distance and shows that it
correlates well with adaptability. We discuss related
work in Section 7 and conclude in Section 8.
2 Structural Correspondence Learning
Before reviewing SCL, we give a brief illustrative
example. Suppose that we are adapting from re-
views of computers to reviews of cell phones. While
many of the features of a good cell phone review are
the same as a computer review ? the words ?excel-
lent? and ?awful? for example ? many words are to-
tally new, like ?reception?. At the same time, many
features which were useful for computers, such as
?dual-core? are no longer useful for cell phones.
Our key intuition is that even when ?good-quality
reception? and ?fast dual-core? are completely dis-
tinct for each domain, if they both have high correla-
tion with ?excellent? and low correlation with ?aw-
ful? on unlabeled data, then we can tentatively align
them. After learning a classifier for computer re-
views, when we see a cell-phone feature like ?good-
quality reception?, we know it should behave in a
roughly similar manner to ?fast dual-core?.
2.1 Algorithm Overview
Given labeled data from a source domain and un-
labeled data from both source and target domains,
SCL first chooses a set ofm pivot features which oc-
cur frequently in both domains. Then, it models the
correlations between the pivot features and all other
features by training linear pivot predictors to predict
occurrences of each pivot in the unlabeled data from
both domains (Ando and Zhang, 2005; Blitzer et al,
2006). The `th pivot predictor is characterized by
its weight vector w`; positive entries in that weight
vector mean that a non-pivot feature (like ?fast dual-
core?) is highly correlated with the corresponding
pivot (like ?excellent?).
The pivot predictor column weight vectors can be
arranged into a matrix W = [w`]n`=1. Let ? ? R
k?d
be the top k left singular vectors of W (here d indi-
cates the total number of features). These vectors are
the principal predictors for our weight space. If we
chose our pivot features well, then we expect these
principal predictors to discriminate among positive
and negative words in both domains.
At training and test time, suppose we observe a
feature vector x. We apply the projection ?x to ob-
tain k new real-valued features. Now we learn a
predictor for the augmented instance ?x, ?x?. If ?
contains meaningful correspondences, then the pre-
dictor which uses ? will perform well in both source
and target domains.
2.2 Selecting Pivots with Mutual Information
The efficacy of SCL depends on the choice of pivot
features. For the part of speech tagging problem
studied by Blitzer et al (2006), frequently-occurring
words in both domains were good choices, since
they often correspond to function words such as
prepositions and determiners, which are good indi-
cators of parts of speech. This is not the case for
sentiment classification, however. Therefore, we re-
quire that pivot features also be good predictors of
the source label. Among those features, we then
choose the ones with highest mutual information to
the source label. Table 1 shows the set-symmetric
441
SCL, not SCL-MI SCL-MI, not SCL
book one <num> so all a must a wonderful loved it
very about they like weak don?t waste awful
good when highly recommended and easy
Table 1: Top pivots selected by SCL, but not SCL-
MI (left) and vice-versa (right)
differences between the two methods for pivot selec-
tion when adapting a classifier from books to kitchen
appliances. We refer throughout the rest of this work
to our method for selecting pivots as SCL-MI.
3 Dataset and Baseline
We constructed a new dataset for sentiment domain
adaptation by selecting Amazon product reviews for
four different product types: books, DVDs, electron-
ics and kitchen appliances. Each review consists of
a rating (0-5 stars), a reviewer name and location,
a product name, a review title and date, and the re-
view text. Reviews with rating > 3 were labeled
positive, those with rating < 3 were labeled neg-
ative, and the rest discarded because their polarity
was ambiguous. After this conversion, we had 1000
positive and 1000 negative examples for each do-
main, the same balanced composition as the polarity
dataset (Pang et al, 2002). In addition to the labeled
data, we included between 3685 (DVDs) and 5945
(kitchen) instances of unlabeled data. The size of the
unlabeled data was limited primarily by the number
of reviews we could crawl and download from the
Amazon website. Since we were able to obtain la-
bels for all of the reviews, we also ensured that they
were balanced between positive and negative exam-
ples, as well.
While the polarity dataset is a popular choice in
the literature, we were unable to use it for our task.
Our method requires many unlabeled reviews and
despite a large number of IMDB reviews available
online, the extensive curation requirements made
preparing a large amount of data difficult 3.
For classification, we use linear predictors on un-
igram and bigram features, trained to minimize the
Huber loss with stochastic gradient descent (Zhang,
3For a description of the construction of the polarity
dataset, see http://www.cs.cornell.edu/people/
pabo/movie-review-data/.
2004). On the polarity dataset, this model matches
the results reported by Pang et al (2002). When we
report results with SCL and SCL-MI, we require that
pivots occur in more than five documents in each do-
main. We set k, the number of singular vectors of the
weight matrix, to 50.
4 Experiments with SCL and SCL-MI
Each labeled dataset was split into a training set of
1600 instances and a test set of 400 instances. All
the experiments use a classifier trained on the train-
ing set of one domain and tested on the test set of
a possibly different domain. The baseline is a lin-
ear classifier trained without adaptation, while the
gold standard is an in-domain classifier trained on
the same domain as it is tested.
Figure 1 gives accuracies for all pairs of domain
adaptation. The domains are ordered clockwise
from the top left: books, DVDs, electronics, and
kitchen. For each set of bars, the first letter is the
source domain and the second letter is the target
domain. The thick horizontal bars are the accura-
cies of the in-domain classifiers for these domains.
Thus the first set of bars shows that the baseline
achieves 72.8% accuracy adapting from DVDs to
books. SCL-MI achieves 79.7% and the in-domain
gold standard is 80.4%. We say that the adaptation
loss for the baseline model is 7.6% and the adapta-
tion loss for the SCL-MImodel is 0.7%. The relative
reduction in error due to adaptation of SCL-MI for
this test is 90.8%.
We can observe from these results that there is a
rough grouping of our domains. Books and DVDs
are similar, as are kitchen appliances and electron-
ics, but the two groups are different from one an-
other. Adapting classifiers from books to DVDs, for
instance, is easier than adapting them from books
to kitchen appliances. We note that when transfer-
ring from kitchen to electronics, SCL-MI actually
outperforms the in-domain classifier. This is possi-
ble since the unlabeled data may contain information
that the in-domain classifier does not have access to.
At the beginning of Section 2 we gave exam-
ples of how features can change behavior across do-
mains. The first type of behavior is when predictive
features from the source domain are not predictive
or do not appear in the target domain. The second is
442
657075
808590
D->B E->B K->B B->D E->D K->D
baseline SCL SCL-MIbooks
72.8 76.8
79.7
70.7 75.4 75.4 70.9 66.1 68.6
80.4 82.477.2 74.0 75.8 70.6 74.3 76.2 72.7 75.4 76.9
dvd
6570
7580
8590
B->E D->E K->E B->K D->K E->K
electronics kitchen
70.8 77.5 75.9 73.0 74.1 74.1
82.7 83.7 86.884.4
87.7
74.5 78.7 78.9 74.079.4
81.4 84.0 84.4 85.9
Figure 1: Accuracy results for domain adaptation between all pairs using SCL and SCL-MI. Thick black
lines are the accuracies of in-domain classifiers.
domain\polarity negative positive
books plot <num> pages predictable reader grisham engaging
reading this page <num> must read fascinating
kitchen the plastic poorly designed excellent product espresso
leaking awkward to defective are perfect years now a breeze
Table 2: Correspondences discovered by SCL for books and kitchen appliances. The top row shows features
that only appear in books and the bottom features that only appear in kitchen appliances. The left and right
columns show negative and positive features in correspondence, respectively.
when predictive features from the target domain do
not appear in the source domain. To show how SCL
deals with those domain mismatches, we look at the
adaptation from book reviews to reviews of kitchen
appliances. We selected the top 1000 most infor-
mative features in both domains. In both cases, be-
tween 85 and 90% of the informative features from
one domain were not among the most informative
of the other domain4. SCL addresses both of these
issues simultaneously by aligning features from the
two domains.
4There is a third type, features which are positive in one do-
main but negative in another, but they appear very infrequently
in our datasets.
Table 2 illustrates one row of the projection ma-
trix ? for adapting from books to kitchen appliances;
the features on each row appear only in the corre-
sponding domain. A supervised classifier trained on
book reviews cannot assign weight to the kitchen
features in the second row of table 2. In con-
trast, SCL assigns weight to these features indirectly
through the projection matrix. When we observe
the feature ?predictable? with a negative book re-
view, we update parameters corresponding to the
entire projection, including the kitchen-specific fea-
tures ?poorly designed? and ?awkward to?.
While some rows of the projection matrix ? are
443
useful for classification, SCL can also misalign fea-
tures. This causes problems when a projection is
discriminative in the source domain but not in the
target. This is the case for adapting from kitchen
appliances to books. Since the book domain is
quite broad, many projections in books model topic
distinctions such as between religious and political
books. These projections, which are uninforma-
tive as to the target label, are put into correspon-
dence with the fewer discriminating projections in
the much narrower kitchen domain. When we adapt
from kitchen to books, we assign weight to these un-
informative projections, degrading target classifica-
tion accuracy.
5 Correcting Misalignments
We now show how to use a small amount of target
domain labeled data to learn to ignore misaligned
projections from SCL-MI. Using the notation of
Ando and Zhang (2005), we can write the supervised
training objective of SCL on the source domain as
min
w,v
?
i
L
(
w?xi + v??xi, yi
)
+ ?||w||2 + ?||v||2 ,
where y is the label. The weight vector w ? Rd
weighs the original features, while v ? Rk weighs
the projected features. Ando and Zhang (2005) and
Blitzer et al (2006) suggest ? = 10?4, ? = 0, which
we have used in our results so far.
Suppose now that we have trained source model
weight vectors ws and vs. A small amount of tar-
get domain data is probably insufficient to signif-
icantly change w, but we can correct v, which is
much smaller. We augment each labeled target in-
stance xj with the label assigned by the source do-
main classifier (Florian et al, 2004; Blitzer et al,
2006). Then we solve
minw,v
?
j L (w
?xj + v??xj , yj) + ?||w||2
+?||v ? vs||2 .
Since we don?t want to deviate significantly from the
source parameters, we set ? = ? = 10?1.
Figure 2 shows the corrected SCL-MI model us-
ing 50 target domain labeled instances. We chose
this number since we believe it to be a reasonable
amount for a single engineer to label with minimal
effort. For reasons of space, for each target domain
dom \ model base base scl scl-mi scl-mi
+targ +targ
books 8.9 9.0 7.4 5.8 4.4
dvd 8.9 8.9 7.8 6.1 5.3
electron 8.3 8.5 6.0 5.5 4.8
kitchen 10.2 9.9 7.0 5.6 5.1
average 9.1 9.1 7.1 5.8 4.9
Table 3: For each domain, we show the loss due to transfer
for each method, averaged over all domains. The bottom row
shows the average loss over all runs.
we show adaptation from only the two domains on
which SCL-MI performed the worst relative to the
supervised baseline. For example, the book domain
shows only results from electronics and kitchen, but
not DVDs. As a baseline, we used the label of the
source domain classifier as a feature in the target, but
did not use any SCL features. We note that the base-
line is very close to just using the source domain
classifier, because with only 50 target domain in-
stances we do not have enough data to relearn all of
the parameters inw. As we can see, though, relearn-
ing the 50 parameters in v is quite helpful. The cor-
rected model always improves over the baseline for
every possible transfer, including those not shown in
the figure.
The idea of using the regularizer of a linear model
to encourage the target parameters to be close to the
source parameters has been used previously in do-
main adaptation. In particular, Chelba and Acero
(2004) showed how this technique can be effective
for capitalization adaptation. The major difference
between our approach and theirs is that we only pe-
nalize deviation from the source parameters for the
weights v of projected features, while they work
with the weights of the original features only. For
our small amount of labeled target data, attempting
to penalize w using ws performed no better than
our baseline. Because we only need to learn to ig-
nore projections that misalign features, we can make
much better use of our labeled data by adapting only
50 parameters, rather than 200,000.
Table 3 summarizes the results of sections 4 and
5. Structural correspondence learning reduces the
error due to transfer by 21%. Choosing pivots by
mutual information allows us to further reduce the
error to 36%. Finally, by adding 50 instances of tar-
get domain data and using this to correct the mis-
aligned projections, we achieve an average relative
444
657075
808590
E->B K->B B->D K->D B->E D->E B->K E->K
base+50-targ SCL-MI+50-targbooks kitchen
70.9 76.0 70.7 76.8
78.5 72.7
80.4 87.776.6 70.8 76.6 73.0 77.9 74.3
80.7 84.3
dvd electronics82.4 84.4
73.2
85.9
Figure 2: Accuracy results for domain adaptation with 50 labeled target domain instances.
reduction in error of 46%.
6 Measuring Adaptability
Sections 2-5 focused on how to adapt to a target do-
main when you had a labeled source dataset. We
now take a step back to look at the problem of se-
lecting source domain data to label. We study a set-
ting where an engineer knows roughly her domains
of interest but does not have any labeled data yet. In
that case, she can ask the question ?Which sources
should I label to obtain the best performance over
all my domains?? On our product domains, for ex-
ample, if we are interested in classifying reviews
of kitchen appliances, we know from sections 4-5
that it would be foolish to label reviews of books or
DVDs rather than electronics. Here we show how to
select source domains using only unlabeled data and
the SCL representation.
6.1 The A-distance
We propose to measure domain adaptability by us-
ing the divergence of two domains after the SCL
projection. We can characterize domains by their
induced distributions on instance space: the more
different the domains, the more divergent the distri-
butions. Here we make use of the A-distance (Ben-
David et al, 2006). The key intuition behind the
A-distance is that while two domains can differ in
arbitrary ways, we are only interested in the differ-
ences that affect classification accuracy.
Let A be the family of subsets of Rk correspond-
ing to characteristic functions of linear classifiers
(sets on which a linear classifier returns positive
value). Then theA distance between two probability
distributions is
dA(D,D
?) = 2 sup
A?A
|PrD [A] ? PrD? [A]| .
That is, we find the subset in A on which the distri-
butions differ the most in the L1 sense. Ben-David
et al (2006) show that computing the A-distance for
a finite sample is exactly the problem of minimiz-
ing the empirical risk of a classifier that discrimi-
nates between instances drawn fromD and instances
drawn from D?. This is convenient for us, since it al-
lows us to use classification machinery to compute
the A-distance.
6.2 Unlabeled Adaptability Measurements
We follow Ben-David et al (2006) and use the Hu-
ber loss as a proxy for the A-distance. Our proce-
dure is as follows: Given two domains, we compute
the SCL representation. Then we create a data set
where each instance ?x is labeled with the identity
of the domain from which it came and train a linear
classifier. For each pair of domains we compute the
empirical average per-instance Huber loss, subtract
it from 1, and multiply the result by 100. We refer
to this quantity as the proxy A-distance. When it is
100, the two domains are completely distinct. When
it is 0, the two domains are indistinguishable using a
linear classifier.
Figure 3 is a correlation plot between the proxy
A-distance and the adaptation error. Suppose we
wanted to label two domains out of the four in such a
445
024
6810
1214
60 65 70 75 80 85 90 95 100Proxy A-distanceAd
aptation Loss EK BD DE
DK BE, BK
Figure 3: The proxy A-distance between each do-
main pair plotted against the average adaptation loss
of as measured by our baseline system. Each pair of
domains is labeled by their first letters: EK indicates
the pair electronics and kitchen.
way as to minimize our error on all the domains. Us-
ing the proxy A-distance as a criterion, we observe
that we would choose one domain from either books
or DVDs, but not both, since then we would not be
able to adequately cover electronics or kitchen appli-
ances. Similarly we would also choose one domain
from either electronics or kitchen appliances, but not
both.
7 Related Work
Sentiment classification has advanced considerably
since the work of Pang et al (2002), which we use
as our baseline. Thomas et al (2006) use discourse
structure present in congressional records to perform
more accurate sentiment classification. Pang and
Lee (2005) treat sentiment analysis as an ordinal
ranking problem. In our work we only show im-
provement for the basic model, but all of these new
techniques also make use of lexical features. Thus
we believe that our adaptation methods could be also
applied to those more refined models.
While work on domain adaptation for senti-
ment classifiers is sparse, it is worth noting that
other researchers have investigated unsupervised
and semisupervised methods for domain adaptation.
The work most similar in spirit to ours that of Tur-
ney (2002). He used the difference in mutual in-
formation with two human-selected features (the
words ?excellent? and ?poor?) to score features in
a completely unsupervised manner. Then he clas-
sified documents according to various functions of
these mutual information scores. We stress that our
method improves a supervised baseline. While we
do not have a direct comparison, we note that Tur-
ney (2002) performs worse on movie reviews than
on his other datasets, the same type of data as the
polarity dataset.
We also note the work of Aue and Gamon (2005),
who performed a number of empirical tests on do-
main adaptation of sentiment classifiers. Most of
these tests were unsuccessful. We briefly note their
results on combining a number of source domains.
They observed that source domains closer to the tar-
get helped more. In preliminary experiments we
confirmed these results. Adding more labeled data
always helps, but diversifying training data does not.
When classifying kitchen appliances, for any fixed
amount of labeled data, it is always better to draw
from electronics as a source than use some combi-
nation of all three other domains.
Domain adaptation alone is a generally well-
studied area, and we cannot possibly hope to cover
all of it here. As we noted in Section 5, we are
able to significantly outperform basic structural cor-
respondence learning (Blitzer et al, 2006). We also
note that while Florian et al (2004) and Blitzer et al
(2006) observe that including the label of a source
classifier as a feature on small amounts of target data
tends to improve over using either the source alone
or the target alne, we did not observe that for our
data. We believe the most important reason for this
is that they explore structured prediction problems,
where labels of surrounding words from the source
classifier may be very informative, even if the cur-
rent label is not. In contrast our simple binary pre-
diction problem does not exhibit such behavior. This
may also be the reason that the model of Chelba and
Acero (2004) did not aid in adaptation.
Finally we note that while Blitzer et al (2006) did
combine SCL with labeled target domain data, they
only compared using the label of SCL or non-SCL
source classifiers as features, following the work of
Florian et al (2004). By only adapting the SCL-
related part of the weight vector v, we are able to
make better use of our small amount of unlabeled
data than these previous techniques.
446
8 Conclusion
Sentiment classification has seen a great deal of at-
tention. Its application to many different domains
of discourse makes it an ideal candidate for domain
adaptation. This work addressed two important
questions of domain adaptation. First, we showed
that for a given source and target domain, we can
significantly improve for sentiment classification the
structural correspondence learning model of Blitzer
et al (2006). We chose pivot features using not only
common frequency among domains but also mutual
information with the source labels. We also showed
how to correct structural correspondence misalign-
ments by using a small amount of labeled target do-
main data.
Second, we provided a method for selecting those
source domains most likely to adapt well to given
target domains. The unsupervised A-distance mea-
sure of divergence between domains correlates well
with loss due to adaptation. Thus we can use the A-
distance to select source domains to label which will
give low target domain error.
In the future, we wish to include some of the more
recent advances in sentiment classification, as well
as addressing the more realistic problem of rank-
ing. We are also actively searching for a larger and
more varied set of domains on which to test our tech-
niques.
Acknowledgements
We thank Nikhil Dinesh for helpful advice through-
out the course of this work. This material is based
upon work partially supported by the Defense Ad-
vanced Research Projects Agency (DARPA) un-
der Contract No. NBCHD03001. Any opinions,
findings, and conclusions or recommendations ex-
pressed in this material are those of the authors and
do not necessarily reflect the views of DARPA or
the Department of Interior-National BusinessCenter
(DOI-NBC).
References
Rie Ando and Tong Zhang. 2005. A framework for
learning predictive structures from multiple tasks and
unlabeled data. JMLR, 6:1817?1853.
Anthony Aue and Michael Gamon. 2005. Customiz-
ing sentiment classifiers to new domains: a case study.
http://research.microsoft.com/ anthaue/.
Shai Ben-David, John Blitzer, Koby Crammer, and Fer-
nando Pereira. 2006. Analysis of representations for
domain adaptation. In Neural Information Processing
Systems (NIPS).
John Blitzer, Ryan McDonald, and Fernando Pereira.
2006. Domain adaptation with structural correspon-
dence learning. In Empirical Methods in Natural Lan-
guage Processing (EMNLP).
Ciprian Chelba and Alex Acero. 2004. Adaptation of
maximum entropy capitalizer: Little data can help a
lot. In EMNLP.
Sanjiv Das and Mike Chen. 2001. Yahoo! for ama-
zon: Extracting market sentiment from stock message
boards. In Proceedings of Athe Asia Pacific Finance
Association Annual Conference.
R. Florian, H. Hassan, A.Ittycheriah, H. Jing, N. Kamb-
hatla, X. Luo, N. Nicolov, and S. Roukos. 2004. A
statistical model for multilingual entity detection and
tracking. In of HLT-NAACL.
Andrew Goldberg and Xiaojin Zhu. 2004. Seeing
stars when there aren?t many stars: Graph-based semi-
supervised learning for sentiment categorization. In
HLT-NAACL 2006 Workshop on Textgraphs: Graph-
based Algorithms for Natural Language Processing.
Bo Pang and Lillian Lee. 2005. Seeing stars: Exploiting
class relationships for sentiment categorization with
respect to rating scales. In Proceedings of Association
for Computational Linguistics.
Bo Pang, Lillian Lee, and Shivakumar Vaithyanathan.
2002. Thumbs up? sentiment classification using ma-
chine learning techniques. In Proceedings of Empiri-
cal Methods in Natural Language Processing.
Matt Thomas, Bo Pang, and Lillian Lee. 2006. Get out
the vote: Determining support or opposition from con-
gressional floor-debate transcripts. In Empirical Meth-
ods in Natural Language Processing (EMNLP).
Peter Turney. 2002. Thumbs up or thumbs down? se-
mantic orientation applied to unsupervised classifica-
tion of reviews. In Proceedings of Association for
Computational Linguistics.
Tong Zhang. 2004. Solving large scale linear predic-
tion problems using stochastic gradient descent al-
gorithms. In International Conference on Machine
Learning (ICML).
447
Proceedings of the 2006 Conference on Empirical Methods in Natural Language Processing (EMNLP 2006), pages 120?128,
Sydney, July 2006. c?2006 Association for Computational Linguistics
Domain Adaptation with Structural Correspondence Learning
John Blitzer Ryan McDonald Fernando Pereira
{blitzer|ryantm|pereira}@cis.upenn.edu
Department of Computer and Information Science, University of Pennsylvania
3330 Walnut Street, Philadelphia, PA 19104, USA
Abstract
Discriminative learning methods are
widely used in natural language process-
ing. These methods work best when their
training and test data are drawn from the
same distribution. For many NLP tasks,
however, we are confronted with new
domains in which labeled data is scarce
or non-existent. In such cases, we seek
to adapt existing models from a resource-
rich source domain to a resource-poor
target domain. We introduce structural
correspondence learning to automatically
induce correspondences among features
from different domains. We test our tech-
nique on part of speech tagging and show
performance gains for varying amounts
of source and target training data, as well
as improvements in target domain parsing
accuracy using our improved tagger.
1 Introduction
Discriminative learning methods are ubiquitous in
natural language processing. Discriminative tag-
gers and chunkers have been the state-of-the-art
for more than a decade (Ratnaparkhi, 1996; Sha
and Pereira, 2003). Furthermore, end-to-end sys-
tems like speech recognizers (Roark et al, 2004)
and automatic translators (Och, 2003) use increas-
ingly sophisticated discriminative models, which
generalize well to new data that is drawn from the
same distribution as the training data.
However, in many situations we may have a
source domain with plentiful labeled training data,
but we need to process material from a target do-
main with a different distribution from the source
domain and no labeled data. In such cases, we
must take steps to adapt a model trained on the
source domain for use in the target domain (Roark
and Bacchiani, 2003; Florian et al, 2004; Chelba
and Acero, 2004; Ando, 2004; Lease and Char-
niak, 2005; Daume? III and Marcu, 2006). This
work focuses on using unlabeled data from both
the source and target domains to learn a common
feature representation that is meaningful across
both domains. We hypothesize that a discrimi-
native model trained in the source domain using
this common feature representation will general-
ize better to the target domain.
This representation is learned using a method
we call structural correspondence learning (SCL).
The key idea of SCL is to identify correspon-
dences among features from different domains by
modeling their correlations with pivot features.
Pivot features are features which behave in the
same way for discriminative learning in both do-
mains. Non-pivot features from different domains
which are correlated with many of the same pivot
features are assumed to correspond, and we treat
them similarly in a discriminative learner.
Even on the unlabeled data, the co-occurrence
statistics of pivot and non-pivot features are likely
to be sparse, and we must model them in a com-
pact way. There are many choices for modeling
co-occurrence data (Brown et al, 1992; Pereira
et al, 1993; Blei et al, 2003). In this work we
choose to use the technique of structural learn-
ing (Ando and Zhang, 2005a; Ando and Zhang,
2005b). Structural learning models the correla-
tions which are most useful for semi-supervised
learning. We demonstrate how to adapt it for trans-
fer learning, and consequently the structural part
of structural correspondence learning is borrowed
from it.1
SCL is a general technique, which one can ap-
ply to feature based classifiers for any task. Here,
1Structural learning is different from learning with struc-
tured outputs, a common paradigm for discriminative nat-
ural language processing models. To avoid terminologi-
cal confusion, we refer throughout the paper to a specific
structural learning method, alternating structural optimiza-
tion (ASO) (Ando and Zhang, 2005a).
120
(a) Wall Street Journal
DT JJ VBZ DT NN IN DT JJ NN
The clash is a sign of a new toughness
CC NN IN NNP POS JJ JJ NN .
and divisiveness in Japan ?s once-cozy financial circles .
(b) MEDLINE
DT JJ VBN NNS IN DT NN NNS VBP
The oncogenic mutated forms of the ras proteins are
RB JJ CC VBP IN JJ NN NN .
constitutively active and interfere with normal signal transduction .
Figure 1: Part of speech-tagged sentences from both corpora
we investigate its use in part of speech (PoS) tag-
ging (Ratnaparkhi, 1996; Toutanova et al, 2003).
While PoS tagging has been heavily studied, many
domains lack appropriate training corpora for PoS
tagging. Nevertheless, PoS tagging is an impor-
tant stage in pipelined language processing sys-
tems, from information extractors to speech syn-
thesizers. We show how to use SCL to transfer a
PoS tagger from the Wall Street Journal (financial
news) to MEDLINE (biomedical abstracts), which
use very different vocabularies, and we demon-
strate not only improved PoS accuracy but also
improved end-to-end parsing accuracy while using
the improved tagger.
An important but rarely-explored setting in do-
main adaptation is when we have no labeled
training data for the target domain. We first
demonstrate that in this situation SCL significantly
improves performance over both supervised and
semi-supervised taggers. In the case when some
in-domain labeled training data is available, we
show how to use SCL together with the classifier
combination techniques of Florian et al (2004) to
achieve even greater performance.
In the next section, we describe a motivating
example involving financial news and biomedical
data. Section 3 describes the structural correspon-
dence learning algorithm. Sections 6 and 7 report
results on adapting from the Wall Street Journal to
MEDLINE. We discuss related work on domain
adaptation in section 8 and conclude in section 9.
2 A Motivating Example
Figure 1 shows two PoS-tagged sentences, one
each from the Wall Street Journal (hereafter WSJ)
and MEDLINE. We chose these sentences for two
reasons. First, we wish to visually emphasize the
difference between the two domains. The vocab-
ularies differ significantly, and PoS taggers suf-
fer accordingly. Second, we want to focus on the
(a) An ambiguous instance
JJ vs. NN
with normal signal transduction
(b) MEDLINE occurrences of
signal, together with pivot
features
the signal required to
stimulatory signal from
essential signal for
(c) Corresponding WSJ
words, together with pivot
features
of investment required
of buyouts from buyers
to jail for violating
Figure 2: Correcting an incorrect biomedical tag.
Corresponding words are in bold, and pivot fea-
tures are italicized
phrase ?with normal signal transduction? from the
MEDLINE sentence, depicted in Figure 2(a). The
word ?signal? in this sentence is a noun, but a tag-
ger trained on the WSJ incorrectly classifies it as
an adjective. We introduce the notion of pivot fea-
tures. Pivot features are features which occur fre-
quently in the two domains and behave similarly
in both. Figure 2(b) shows some pivot features
that occur together with the word ?signal? in our
biomedical unlabeled data. In this case our pivot
features are all of type <the token on the
right>. Note that ?signal? is unambiguously a
noun in these contexts. Adjectives rarely precede
past tense verbs such as ?required? or prepositions
such as ?from? and ?for?.
We now search for occurrences of the pivot fea-
tures in the WSJ. Figure 2(c) shows some words
that occur together with the pivot features in the
WSJ unlabeled data. Note that ?investment?,
?buy-outs?, and ?jail? are all common nouns in the
financial domain. Furthermore, since we have la-
beled WSJ data, we expect to be able to label at
least some of these nouns correctly.
This example captures the intuition behind
structural correspondence learning. We want to
use pivot features from our unlabeled data to put
domain-specific words in correspondence. That is,
121
Input: labeled source data {(xt, yt)Tt=1},
unlabeled data from both domains {xj}
Output: predictor f : X ? Y
1. Choose m pivot features. Create m binary
prediction problems, p`(x), ` = 1 . . . m
2. For ` = 1 to m
w?` = argmin
w
?
P
j L(w ? xj , p`(xj))+
?||w||2
?
end
3. W = [w?1| . . . |w?m], [U D V T ] = SVD(W ),
? = UT[1:h,:]
4. Return f , a predictor trained
on
(
??
xt
?xi
?
, yt
?T
t=1
)
Figure 3: SCL Algorithm
we want the pivot features to model the fact that in
the biomedical domain, the word signal behaves
similarly to the words investments, buyouts and
jail in the financial news domain. In practice, we
use this technique to find correspondences among
all features, not just word features.
3 Structural Correspondence Learning
Structural correspondence learning involves a
source domain and a target domain. Both domains
have ample unlabeled data, but only the source do-
main has labeled training data. We refer to the task
for which we have labeled training data as the su-
pervised task. In our experiments, the supervised
task is part of speech tagging. We require that the
input x in both domains be a vector of binary fea-
tures from a finite feature space. The first step of
SCL is to define a set of pivot features on the unla-
beled data from both domains. We then use these
pivot features to learn a mapping ? from the orig-
inal feature spaces of both domains to a shared,
low-dimensional real-valued feature space. A high
inner product in this new space indicates a high de-
gree of correspondence.
During supervised task training, we use both
the transformed and original features from the
source domain. During supervised task testing, we
use the both the transformed and original features
from the target domain. If we learned a good map-
ping ?, then the classifier we learn on the source
domain will also be effective on the target domain.
The SCL algorithm is given in Figure 3, and the
remainder of this section describes it in detail.
3.1 Pivot Features
Pivot features should occur frequently in the un-
labeled data of both domains, since we must esti-
mate their covariance with non-pivot features ac-
curately, but they must also be diverse enough
to adequately characterize the nuances of the su-
pervised task. A good example of this tradeoff
are determiners in PoS tagging. Determiners are
good pivot features, since they occur frequently
in any domain of written English, but choosing
only determiners will not help us to discriminate
between nouns and adjectives. Pivot features cor-
respond to the auxiliary problems of Ando and
Zhang (2005a).
In section 2, we showed example pivot fea-
tures of type <the token on the right>.
We also use pivot features of type <the token
on the left> and <the token in the
middle>. In practice there are many thousands
of pivot features, corresponding to instantiations
of these three types for frequent words in both do-
mains. We choose m pivot features, which we in-
dex with `.
3.2 Pivot Predictors
From each pivot feature we create a binary clas-
sification problem of the form ?Does pivot fea-
ture ` occur in this instance??. One such ex-
ample is ?Is <the token on the right>
required?? These binary classification problems
can be trained from the unlabeled data, since they
merely represent properties of the input. If we rep-
resent our features as a binary vector x, we can
solve these problems using m linear predictors.
f`(x) = sgn(w?` ? x), ` = 1 . . . m
Note that these predictors operate on the original
feature space. This step is shown in line 2 of Fig-
ure 3. Here L(p, y) is a real-valued loss func-
tion for binary classification. We follow Ando and
Zhang (2005a) and use the modified Huber loss.
Since each instance contains features which are
totally predictive of the pivot feature (the feature
itself), we never use these features when making
the binary prediction. That is, we do not use any
feature derived from the right word when solving
a right token pivot predictor.
The pivot predictors are the key element in SCL.
The weight vectors w?` encode the covariance of
the non-pivot features with the pivot features. If
the weight given to the z?th feature by the `?th
122
pivot predictor is positive, then feature z is posi-
tively correlated with pivot feature `. Since pivot
features occur frequently in both domains, we ex-
pect non-pivot features from both domains to be
correlated with them. If two non-pivot features are
correlated in the same way with many of the same
pivot features, then they have a high degree of cor-
respondence. Finally, observe that w?` is a linear
projection of the original feature space onto R.
3.3 Singular Value Decomposition
Since each pivot predictor is a projection onto R,
we could create m new real-valued features, one
for each pivot. For both computational and statis-
tical reasons, though, we follow Ando and Zhang
(2005a) and compute a low-dimensional linear ap-
proximation to the pivot predictor space. Let W
be the matrix whose columns are the pivot pre-
dictor weight vectors. Now let W = UDV T be
the singular value decomposition of W , so that
? = UT[1:h,:] is the matrix whose rows are the top
left singular vectors of W .
The rows of ? are the principal pivot predictors,
which capture the variance of the pivot predictor
space as best as possible in h dimensions. Further-
more, ? is a projection from the original feature
space onto Rh. That is, ?x is the desired mapping
to the (low dimensional) shared feature represen-
tation. This is step 3 of Figure 3.
3.4 Supervised Training and Inference
To perform inference and learning for the super-
vised task, we simply augment the original fea-
ture vector with features obtained by applying the
mapping ?. We then use a standard discrimina-
tive learner on the augmented feature vector. For
training instance t, the augmented feature vector
will contain all the original features xt plus the
new shared features ?xt. If we have designed the
pivots well, then ? should encode correspondences
among features from different domains which are
important for the supervised task, and the classi-
fier we train using these new features on the source
domain will perform well on the target domain.
4 Model Choices
Structural correspondence learning uses the tech-
niques of alternating structural optimization
(ASO) to learn the correlations among pivot and
non-pivot features. Ando and Zhang (2005a) de-
scribe several free paramters and extensions to
ASO, and we briefly address our choices for these
here. We set h, the dimensionality of our low-rank
representation to be 25. As in Ando and Zhang
(2005a), we observed that setting h between 20
and 100 did not change results significantly, and a
lower dimensionality translated to faster run-time.
We also implemented both of the extensions de-
scribed in Ando and Zhang (2005a). The first is
to only use positive entries in the pivot predictor
weight vectors to compute the SVD. This yields
a sparse representation which saves both time and
space, and it also performs better. The second is to
compute block SVDs of the matrix W , where one
block corresponds to one feature type. We used
the same 58 feature types as Ratnaparkhi (1996).
This gave us a total of 1450 projection features for
both semisupervised ASO and SCL.
We found it necessary to make a change to the
ASO algorithm as described in Ando and Zhang
(2005a). We rescale the projection features to al-
low them to receive more weight from a regular-
ized discriminative learner. Without any rescaling,
we were not able to reproduce the original ASO
results. The rescaling parameter is a single num-
ber, and we choose it using heldout data from our
source domain. In all our experiments, we rescale
our projection features to have average L1 norm on
the training set five times that of the binary-valued
features.
Finally, we also make one more change to make
optimization faster. We select only half of the
ASO features for use in the final model. This
is done by running a few iterations of stochas-
tic gradient descent on the PoS tagging problem,
then choosing the features with the largest weight-
variance across the different labels. This cut in
half training time and marginally improved perfor-
mance in all our experiments.
5 Data Sets and Supervised Tagger
5.1 Source Domain: WSJ
We used sections 02-21 of the Penn Treebank
(Marcus et al, 1993) for training. This resulted in
39,832 training sentences. For the unlabeled data,
we used 100,000 sentences from a 1988 subset of
the WSJ.
5.2 Target Domain: Biomedical Text
For unlabeled data we used 200,000 sentences that
were chosen by searching MEDLINE for abstracts
pertaining to cancer, in particular genomic varia-
123
company
transaction
investors
officials yourpretty
short-term
political
receptors mutation
assays
lesions functional
transientneuronal
metastatic
WSJ Only
MEDLINE Only
Figure 4: An example projection of word features onto R. Words on the left (negative valued) behave
similarly to each other for classification, but differently from words on the right (positive valued). The
projection distinguishes nouns from adjectives and determiners in both domains.
tions and mutations. For labeled training and test-
ing purposes we use 1061 sentences that have been
annotated by humans as part of the Penn BioIE
project (PennBioIE, 2005). We use the same 561-
sentence test set in all our experiments. The part-
of-speech tag set for this data is a superset of
the Penn Treebank?s including the two new tags
HYPH (for hyphens) and AFX (for common post-
modifiers of biomedical entities such as genes).
These tags were introduced due to the importance
of hyphenated entities in biomedical text, and are
used for 1.8% of the words in the test set. Any
tagger trained only on WSJ text will automatically
predict wrong tags for those words.
5.3 Supervised Tagger
Since SCL is really a method for inducing a set
of cross-domain features, we are free to choose
any feature-based classifier to use them. For
our experiments we use a version of the discrim-
inative online large-margin learning algorithm
MIRA (Crammer et al, 2006). MIRA learns and
outputs a linear classification score, s(x,y;w) =
w ? f(x,y), where the feature representation f can
contain arbitrary features of the input, including
the correspondence features described earlier. In
particular, MIRA aims to learn weights so that
the score of correct output, yt, for input xt is
separated from the highest scoring incorrect out-
puts2, with a margin proportional to their Ham-
ming losses. MIRA has been used successfully for
both sequence analysis (McDonald et al, 2005a)
and dependency parsing (McDonald et al, 2005b).
As with any structured predictor, we need to
factor the output space to make inference tractable.
We use a first-order Markov factorization, allow-
ing for an efficient Viterbi inference procedure.
2We fix the number of high scoring incorrect outputs to 5.
6 Visualizing ?
In section 2 we claimed that good representations
should encode correspondences between words
like ?signal? from MEDLINE and ?investment?
from the WSJ. Recall that the rows of ? are pro-
jections from the original feature space onto the
real line. Here we examine word features under
these projections. Figure 4 shows a row from
the matrix ?. Applying this projection to a word
gives a real value on the horizontal dashed line
axis. The words below the horizontal axis occur
only in the WSJ. The words above the axis occur
only in MEDLINE. The verticle line in the mid-
dle represents the value zero. Ticks to the left or
right indicate relative positive or negative values
for a word under this projection. This projection
discriminates between nouns (negative) and adjec-
tives (positive). A tagger which gives high pos-
itive weight to the features induced by applying
this projection will be able to discriminate among
the associated classes of biomedical words, even
when it has never observed the words explicitly in
the WSJ source training set.
7 Empirical Results
All the results we present in this section use the
MIRA tagger from Section 5.3. The ASO and
structural correspondence results also use projec-
tion features learned using ASO and SCL. Sec-
tion 7.1 presents results comparing structural cor-
respondence learning with the supervised baseline
and ASO in the case where we have no labeled
data in the target domain. Section 7.2 gives results
for the case where we have some limited data in
the target domain. In this case, we use classifiers
as features as described in Florian et al (2004).
Finally, we show in Section 7.3 that our SCL PoS
124
(a)
100  500  1k 5k 40k75
80
85
90
Results for 561 MEDLINE Test Sentences
Number of WSJ Training Sentences
Ac
cu
ra
cy
supervised
semi?ASO
SCL
(b) Accuracy on 561-sentence test set
Words
Model All Unknown
Ratnaparkhi (1996) 87.2 65.2
supervised 87.9 68.4
semi-ASO 88.4 70.9
SCL 88.9 72.0
(c) Statistical Significance (McNemar?s)
for all words
Null Hypothesis p-value
semi-ASO vs. super 0.0015
SCL vs. super 2.1 ? 10?12
SCL vs. semi-ASO 0.0003
Figure 5: PoS tagging results with no target labeled training data
(a)
50 100 200 500
86
88
90
92
94
96
Number of MEDLINE Training Sentences
Ac
cu
ra
cy
Results for 561 MEDLINE Test Sentences
40k?SCL
40k?super
1k?SCL
1k?super
nosource
(b) 500 target domain training sentences
Model Testing Accuracy
nosource 94.5
1k-super 94.5
1k-SCL 95.0
40k-super 95.6
40k-SCL 96.1
(c) McNemar?s Test (500 training sentences)
Null Hypothesis p-value
1k-super vs. nosource 0.732
1k-SCL vs. 1k-super 0.0003
40k-super vs. nosource 1.9 ? 10?12
40k-SCL vs. 40k-super 6.5 ? 10?7
Figure 6: PoS tagging results with no target labeled training data
tagger improves the performance of a dependency
parser on the target domain.
7.1 No Target Labeled Training Data
For the results in this section, we trained a
structural correspondence learner with 100,000
sentences of unlabeled data from the WSJ and
100,000 sentences of unlabeled biomedical data.
We use as pivot features words that occur more
than 50 times in both domains. The supervised
baseline does not use unlabeled data. The ASO
baseline is an implementation of Ando and Zhang
(2005b). It uses 200,000 sentences of unlabeled
MEDLINE data but no unlabeled WSJ data. For
ASO we used as auxiliary problems words that oc-
cur more than 500 times in the MEDLINE unla-
beled data.
Figure 5(a) plots the accuracies of the three
models with varying amounts of WSJ training
data. With one hundred sentences of training
data, structural correspondence learning gives a
19.1% relative reduction in error over the super-
vised baseline, and it consistently outperforms
both baseline models. Figure 5(b) gives results
for 40,000 sentences, and Figure 5(c) shows cor-
responding significance tests, with p < 0.05 be-
ing significant. We use a McNemar paired test for
labeling disagreements (Gillick and Cox, 1989).
Even when we use all the WSJ training data avail-
able, the SCL model significantly improves accu-
racy over both the supervised and ASO baselines.
The second column of Figure 5(b) gives un-
known word accuracies on the biomedical data.
125
Of thirteen thousand test instances, approximately
three thousand were unknown. For unknown
words, SCL gives a relative reduction in error of
19.5% over Ratnaparkhi (1996), even with 40,000
sentences of source domain training data.
7.2 Some Target Labeled Training Data
In this section we give results for small amounts of
target domain training data. In this case, we make
use of the out-of-domain data by using features of
the source domain tagger?s predictions in training
and testing the target domain tagger (Florian et al,
2004). Though other methods for incorporating
small amounts of training data in the target domain
were available, such as those proposed by Chelba
and Acero (2004) and by Daume? III and Marcu
(2006), we chose this method for its simplicity and
consistently good performance. We use as features
the current predicted tag and all tag bigrams in a
5-token window around the current token.
Figure 6(a) plots tagging accuracy for varying
amounts of MEDLINE training data. The two
horizontal lines are the fixed accuracies of the
SCL WSJ-trained taggers using one thousand and
forty thousand sentences of training data. The five
learning curves are for taggers trained with vary-
ing amounts of target domain training data. They
use features on the outputs of taggers from sec-
tion 7.1. The legend indicates the kinds of features
used in the target domain (in addition to the stan-
dard features). For example, ?40k-SCL? means
that the tagger uses features on the outputs of an
SCL source tagger trained on forty thousand sen-
tences of WSJ data. ?nosource? indicates a tar-
get tagger that did not use any tagger trained on
the source domain. With 1000 source domain sen-
tences and 50 target domain sentences, using SCL
tagger features gives a 20.4% relative reduction
in error over using supervised tagger features and
a 39.9% relative reduction in error over using no
source features.
Figure 6(b) is a table of accuracies for 500 tar-
get domain training sentences, and Figure 6(c)
gives corresponding significance scores. With
1000 source domain sentences and 500 target do-
main sentences, using supervised tagger features
gives no improvement over using no source fea-
tures. Using SCL features still does, however.
7.3 Improving Parser Performance
We emphasize the importance of PoS tagging in a
pipelined NLP system by incorporating our SCL
100  500  1k 5k 40k
58
62
66
70
74
78
82
Dependency Parsing for 561 Test Sentences
Number of WSJ Training Sentences
Ac
cu
ra
cy
supervised
SCL
gold
Figure 7: Dependency parsing results using differ-
ent part of speech taggers
tagger into a WSJ-trained dependency parser and
and evaluate it on MEDLINE data. We use the
parser described by McDonald et al (2005b). That
parser assumes that a sentence has been PoS-
tagged before parsing. We train the parser and PoS
tagger on the same size of WSJ data.
Figure 7 shows dependency parsing accuracy on
our 561-sentence MEDLINE test set. We parsed
the sentences using the PoS tags output by our
source domain supervised tagger, the SCL tagger
from subsection 7.1, and the gold PoS tags. All
of the differences in this figure are significant ac-
cording to McNemar?s test. The SCL tags consis-
tently improve parsing performance over the tags
output by the supervised tagger. This is a rather in-
direct method of improving parsing performance
with SCL. In the future, we plan on directly incor-
porating SCL features into a discriminative parser
to improve its adaptation properties.
8 Related Work
Domain adaptation is an important and well-
studied area in natural language processing. Here
we outline a few recent advances. Roark and Bac-
chiani (2003) use a Dirichlet prior on the multi-
nomial parameters of a generative parsing model
to combine a large amount of training data from a
source corpus (WSJ), and small amount of train-
ing data from a target corpus (Brown). Aside
from Florian et al (2004), several authors have
also given techniques for adapting classification to
new domains. Chelba and Acero (2004) first train
a classifier on the source data. Then they use max-
imum a posteriori estimation of the weights of a
126
maximum entropy target domain classifier. The
prior is Gaussian with mean equal to the weights
of the source domain classifier. Daume? III and
Marcu (2006) use an empirical Bayes model to es-
timate a latent variable model grouping instances
into domain-specific or common across both do-
mains. They also jointly estimate the parameters
of the common classification model and the do-
main specific classification models. Our work fo-
cuses on finding a common representation for fea-
tures from different domains, not instances. We
believe this is an important distinction, since the
same instance can contain some features which are
common across domains and some which are do-
main specific.
The key difference between the previous four
pieces of work and our own is the use of unlabeled
data. We do not require labeled training data in
the new domain to demonstrate an improvement
over our baseline models. We believe this is essen-
tial, since many domains of application in natural
language processing have no labeled training data.
Lease and Charniak (2005) adapt a WSJ parser
to biomedical text without any biomedical tree-
banked data. However, they assume other labeled
resources in the target domain. In Section 7.3 we
give similar parsing results, but we adapt a source
domain tagger to obtain the PoS resources.
To the best of our knowledge, SCL is the first
method to use unlabeled data from both domains
for domain adaptation. By using just the unlabeled
data from the target domain, however, we can view
domain adaptation as a standard semisupervised
learning problem. There are many possible ap-
proaches for semisupservised learning in natural
language processing, and it is beyond the scope
of this paper to address them all. We chose to
compare with ASO because it consistently outper-
forms cotraining (Blum and Mitchell, 1998) and
clustering methods (Miller et al, 2004). We did
run experiments with the top-k version of ASO
(Ando and Zhang, 2005a), which is inspired by
cotraining but consistently outperforms it. This
did not outperform the supervised method for do-
main adaptation. We speculate that this is because
biomedical and financial data are quite different.
In such a situation, bootstrapping techniques are
likely to introduce too much noise from the source
domain to be useful.
Structural correspondence learning is most sim-
ilar to that of Ando (2004), who analyzed a
situation with no target domain labeled data.
Her model estimated co-occurrence counts from
source unlabeled data and then used the SVD of
this matrix to generate features for a named en-
tity recognizer. Our ASO baseline uses unlabeled
data from the target domain. Since this consis-
tently outperforms unlabeled data from only the
source domain, we report only these baseline re-
sults. To the best of our knowledge, this is the first
work to use unlabeled data from both domains to
find feature correspondences.
One important advantage that this work shares
with Ando (2004) is that an SCL model can be
easily combined with all other domain adaptation
techniques (Section 7.2). We are simply induc-
ing a feature representation that generalizes well
across domains. This feature representation can
then be used in all the techniques described above.
9 Conclusion
Structural correspondence learning is a marriage
of ideas from single domain semi-supervised
learning and domain adaptation. It uses unla-
beled data and frequently-occurring pivot features
from both source and target domains to find corre-
spondences among features from these domains.
Finding correspondences involves estimating the
correlations between pivot and non-pivot feautres,
and we adapt structural learning (ASO) (Ando and
Zhang, 2005a; Ando and Zhang, 2005b) for this
task. SCL is a general technique that can be ap-
plied to any feature-based discriminative learner.
We showed results using SCL to transfer a PoS
tagger from the Wall Street Journal to a corpus
of MEDLINE abstracts. SCL consistently out-
performed both supervised and semi-supervised
learning with no labeled target domain training
data. We also showed how to combine an SCL
tagger with target domain labeled data using the
classifier combination techniques from Florian et
al. (2004). Finally, we improved parsing perfor-
mance in the target domain when using the SCL
PoS tagger.
One of our next goals is to apply SCL directly
to parsing. We are also focusing on other po-
tential applications, including chunking (Sha and
Pereira, 2003), named entity recognition (Florian
et al, 2004; Ando and Zhang, 2005b; Daume? III
and Marcu, 2006), and speaker adaptation (Kuhn
et al, 1998). Finally, we are investigating more
direct ways of applying structural correspondence
127
learning when we have labeled data from both
source and target domains. In particular, the la-
beled data of both domains, not just the unlabeled
data, should influence the learned representations.
Acknowledgments
We thank Rie Kubota Ando and Tong Zhang
for their helpful advice on ASO, Steve Carroll
and Pete White of The Children?s Hospital of
Philadelphia for providing the MEDLINE data,
and the PennBioIE annotation team for the anno-
tated MEDLINE data used in our test sets. This
material is based upon work partially supported by
the Defense Advanced Research Projects Agency
(DARPA) under Contract No. NBCHD030010.
Any opinions, findings, and conclusions or rec-
ommendations expressed in this material are those
of the author(s) and do not necessarily reflect
the views of the DARPA or the Department
of Interior-National Business Center (DOI-NBC).
Additional support was provided by NSF under
ITR grant EIA-0205448.
References
R. Ando and T. Zhang. 2005a. A framework for learn-
ing predictive structures from multiple tasks and un-
labeled data. JMLR, 6:1817?1853.
R. Ando and T. Zhang. 2005b. A high-performance
semi-supervised learning method for text chunking.
In ACL.
R. Ando. 2004. Exploiting unannotated corpora for
tagging and chunking. In ACL. Short paper.
D. Blei, A. Ng, and M. Jordan. 2003. Latent dirichlet
allocation. JMLR, 3:993?1022.
A. Blum and T. Mitchell. 1998. Combining labeled
and unlabeled data with co-training. In Workshop
on Computational Learning Theory.
P. Brown, V. Della Pietra, P. deSouza, J. Lai, and
R. Mercer. 1992. Class-based n-gram models
of natural language. Computational Linguistics,
18(4):467?479.
C. Chelba and A. Acero. 2004. Adaptation of maxi-
mum entropy capitalizer: Little data can help a lot.
In EMNLP.
K. Crammer, Dekel O, J. Keshet, S. Shalev-Shwartz,
and Y. Singer. 2006. Online passive-aggressive al-
gorithms. JMLR, 7:551?585.
H. Daum e? III and D. Marcu. 2006. Domain adaptation
for statistical classifiers. JAIR.
R. Florian, H. Hassan, A.Ittycheriah, H. Jing,
N. Kambhatla, X. Luo, N. Nicolov, and S. Roukos.
2004. A statistical model for multilingual entity de-
tection and tracking. In of HLT-NAACL.
L. Gillick and S. Cox. 1989. Some statistical issues in
the comparison of speech recognition algorithms. In
ICASSP.
R. Kuhn, P. Nguyen, J.C. Junqua, L. Goldwasser,
N. Niedzielski, S. Fincke, K. Field, and M. Con-
tolini. 1998. Eigenvoices for speaker adaptation.
In ICSLP.
M. Lease and E. Charniak. 2005. Parsing biomedical
literature. In IJCNLP.
M. Marcus, B. Santorini, and M. Marcinkiewicz.
1993. Building a large annotated corpus of En-
glish: The Penn Treebank. Computational Linguis-
tics, 19(2):313?330.
R. McDonald, K. Crammer, and F. Pereira. 2005a.
Flexible text segmentation with structured multil-
abel classification. In HLT-EMNLP.
R. McDonald, K. Crammer, and F. Pereira. 2005b. On-
line large-margin training of dependency parsers. In
ACL.
S. Miller, J. Guinness, and A. Zamanian. 2004. Name
tagging with word clusters and discriminative train-
ing. In HLT-NAACL.
F. Och. 2003. Minimum error rate training in statistical
machine translation. In Proc. of ACL.
PennBioIE. 2005. Mining The Bibliome Project.
http://bioie.ldc.upenn.edu/.
F. Pereira, N. Tishby, and L. Lee. 1993. Distributional
clustering of english words. In ACL.
A. Ratnaparkhi. 1996. A maximum entropy model for
part-of-speech tagging. In EMNLP.
B. Roark and M. Bacchiani. 2003. Supervised and
unsupervised PCFG adaptation to novel domains. In
HLT-NAACL.
B. Roark, M. Saraclar, M. Collins, and M. Johnson.
2004. Discriminative language modeling with con-
ditional random fields and the perceptron algorithm.
In ACL.
F. Sha and F. Pereira. 2003. Shallow parsing with con-
ditional random fields. In HLT-NAACL.
K. Toutanova, D. Klein, C. D. Manning, and Y. Singer.
2003. Feature-rich part-of-speech tagging with a
cyclic dependency network. In NAACL.
128
Proceedings of the 10th Conference on Computational Natural Language Learning (CoNLL-X),
pages 141?148, New York City, June 2006. c?2006 Association for Computational Linguistics
A Context Pattern Induction Method for Named Entity Extraction
Partha Pratim Talukdar
CIS Department
University of Pennsylvania
Philadelphia, PA 19104
partha@cis.upenn.edu
Thorsten Brants
Google, Inc.
1600 Amphitheatre Pkwy.
Mountain View, CA 94043
brants@google.com
Mark Liberman Fernando Pereira
CIS Department
University of Pennsylvania
Philadelphia, PA 19104
{myl,pereira}@cis.upenn.edu
Abstract
We present a novel context pattern in-
duction method for information extrac-
tion, specifically named entity extraction.
Using this method, we extended several
classes of seed entity lists into much larger
high-precision lists. Using token member-
ship in these extended lists as additional
features, we improved the accuracy of a
conditional random field-based named en-
tity tagger. In contrast, features derived
from the seed lists decreased extractor ac-
curacy.
1 Introduction
Partial entity lists and massive amounts of unla-
beled data are becoming available with the growth
of the Web as well as the increased availability of
specialized corpora and entity lists. For example,
the primary public resource for biomedical research,
MEDLINE, contains over 13 million entries and is
growing at an accelerating rate. Combined with
these large corpora, the recent availability of entity
lists in those domains has opened up interesting op-
portunities and challenges. Such lists are never com-
plete and suffer from sampling biases, but we would
like to exploit them, in combination with large un-
labeled corpora, to speed up the creation of infor-
mation extraction systems for different domains and
languages. In this paper, we concentrate on explor-
ing utility of such resources for named entity extrac-
tion.
Currently available entity lists contain a small
fraction of named entities, but there are orders of
magnitude more present in the unlabeled data1. In
this paper, we test the following hypotheses:
i. Starting with a few seed entities, it is possible
to induce high-precision context patterns by ex-
ploiting entity context redundancy.
ii. New entity instances of the same category can
be extracted from unlabeled data with the in-
duced patterns to create high-precision exten-
sions of the seed lists.
iii. Features derived from token membership in the
extended lists improve the accuracy of learned
named-entity taggers.
Previous approaches to context pattern induc-
tion were described by Riloff and Jones (1999),
Agichtein and Gravano (2000), Thelen and Riloff
(2002), Lin et al (2003), and Etzioni et al (2005),
among others. The main advance in the present
method is the combination of grammatical induction
and statistical techniques to create high-precision
patterns.
The paper is organized as follows. Section 2 de-
scribes our pattern induction algorithm. Section 3
shows how to extend seed sets with entities extracted
by the patterns from unlabeled data. Section 4 gives
experimental results, and Section 5 compares our
method with previous work.
1For example, based on approximate matching, there is an
overlap of only 22 organizations between the 2403 organiza-
tions present in CoNLL-2003 shared task training data and the
Fortune-500 list.
141
2 Context Pattern Induction
The overall method for inducing entity context pat-
terns and extending entity lists is as follows:
1. Let E = seed set, T = text corpus.
2. Find the contexts C of entities in E in the cor-
pus T (Section 2.1).
3. Select trigger words from C (Section 2.2).
4. For each trigger word, induce a pattern automa-
ton (Section 2.3).
5. Use induced patterns P to extract more entities
E? (Section 3).
6. Rank P and E? (Section 3.1).
7. If needed, add high scoring entities in E? to E
and return to step 2. Otherwise, terminate with
patterns P and extended entity list E ? E? as
results.
2.1 Extracting Context
Starting with the seed list, we first find occurrences
of seed entities in the unlabeled data. For each such
occurrence, we extract a fixed number W (context
window size) of tokens immediately preceding and
immediately following the matched entity. As we
are only interested in modeling the context here, we
replace all entity tokens by the single token -ENT-.
This token now represents a slot in which an entity
can occur. Examples of extracted entity contexts are
shown in Table 1. In the work presented in this pa-
pers, seeds are entity instances (e.g. Google is a seed
for organization category).
increased expression of -ENT- in vad mice
the expression of -ENT- mrna was greater
expression of the -ENT- gene in mouse
Table 1: Extracted contexts of known genes with
W = 3.
The set of extracted contexts is denoted by C . The
next step is to automatically induce high-precision
patterns containing the token -ENT- from such ex-
tracted contexts.
2.2 Trigger Word Selection
To induce patterns, we need to determine their starts.
It is reasonable to assume that some tokens are more
specific to particular entity classes than others. For
example, in the examples shown above, expression
can be one such word for gene names. Whenever
one comes across such a token in text, the proba-
bility of finding an entity (of the corresponding en-
tity class) in its vicinity is high. We call such start-
ing tokens trigger words. Trigger words mark the
beginning of a pattern. It is important to note that
simply selecting the first token of extracted contexts
may not be a good way to select trigger words. In
such a scheme, we would have to vary W to search
for useful pattern starts. Instead of that brute-force
technique, we propose an automatic way of select-
ing trigger words. A good set of trigger words is
very important for the quality of induced patterns.
Ideally, we want a trigger word to satisfy the follow-
ing:
? It is frequent in the set C of extracted contexts.
? It is specific to entities of interest and thereby
to extracted contexts.
We use a term-weighting method to rank candi-
date trigger words from entity contexts. IDF (In-
verse Document Frequency) was used in our experi-
ments but any other suitable term-weighting scheme
may work comparably. The IDF weight fw for a
word w occurring in a corpus is given by:
fw = log
( N
nw
)
where N is the total number of documents in the
corpus and nw is the total number of documents con-
taining w. Now, for each context segment c ? C , we
select a dominating word dc given by
dc = argmaxw?c fw
There is exactly one dominating word for each
c ? C . All dominating words for contexts in C form
multiset M . Let mw be the multiplicity of the dom-
inating word w in M . We sort M by decreasing mw
and select the top n tokens from this list as potential
trigger words.
142
Selection criteria based on dominating word fre-
quency work better than criteria based on simple
term weight because high term weight words may
be rare in the extracted contexts, but would still be
misleadingly selected for pattern induction. This can
be avoided by using instead the frequency of domi-
nating words within contexts, as we did here.
2.3 Automata Induction
Rather than using individual contexts directly, we
summarize them into automata that contain the most
significant regularities of the contexts sharing a
given trigger word. This construction allows us to
determine the relative importance of different con-
text features using a variant of the forward-backward
algorithm from HMMs.
2.3.1 Initial Induction
For each trigger word, we list the contexts start-
ing with the word. For example, with ?expression?
as the trigger word, the contexts in Table 1 are re-
duced to those in Table 2. Since ?expression? is a
left-context trigger word, only one token to the right
of -ENT- is retained. Here, the predictive context
lies to the left of the slot -ENT- and a single to-
ken is retained on the right to mark the slot?s right
boundary. To model predictive right contexts, the to-
ken string can be reversed and the same techniques
as here applied on the reversed string.2
expression of -ENT- in
expression of -ENT- mrna
expression of the -ENT- gene
Table 2: Context segments corresponding to trigger
word ?expression?.
Similar contexts are prepared for each trigger
word. The context set for each trigger word is then
summarized by a pattern automaton with transitions
that match the trigger word and also the wildcard
-ENT- . We expect such automata to model the po-
sition in context of the entity slot and help us extract
more entities of the same class with high precision.
2Experiments reported in this paper use predictive left con-
text only.
10
11
12
of
of
of
the
the
a
...
...
a
Figure 1: Fragment of a 1-reversible automaton
We use a simple form of grammar induction to
learn the pattern automata. Grammar induction tech-
niques have been previously explored for informa-
tion extraction (IE) and related tasks. For instance,
Freitag (1997) used grammatical inference to im-
prove precision in IE tasks.
Context segments are short and typically do not
involve recursive structures. Therefore, we chose to
use 1-reversible automata to represent sets of con-
texts. An automaton A is k-reversible iff (1) A is
deterministic and (2) Ar is deterministic with k to-
kens of lookahead, where Ar is the automaton ob-
tained by reversing the transitions of A. Wrapper in-
duction using k-reversible grammar is discussed by
Chidlovskii (2000).
In the 1-reversible automaton induced for each
trigger word, all transitions labeled by a given token
go to the same state, which is identified with that
token. Figure 1 shows a fragment of a 1-reversible
automaton. Solan et al (2005) describe a similar au-
tomaton construction, but they allow multiple transi-
tions between states to distinguish among sentences.
Each transition e = (v,w) in a 1-reversible au-
tomaton A corresponds to a bigram vw in the con-
texts used to create A. We thus assign each transition
the probability
P (w|v) = C(v,w)?w?C(v,w?)
where C(v,w) is the number of occurrences of the
bigram vw in contexts for W . With this construc-
tion, we ensure words will be credited in proportion
to their frequency in contexts. The automaton may
overgenerate, but that potentially helps generaliza-
tion.
143
2.3.2 Pruning
The initially induced automata need to be pruned
to remove transitions with weak evidence so as to
increase match precision.
The simplest pruning method is to set a count
threshold c below which transitions are removed.
However, this is a poor method. Consider state 10 in
the automaton of Figure 2, with c = 20. Transitions
(10, 11) and (10, 12) will be pruned. C(10, 12)  c
but C(10, 11) just falls short of c. However, from
the transition counts, it looks like the sequence ?the
-ENT-? is very common. In such a case, it is not
desirable to prune (10, 11). Using a local threshold
may lead to overpruning.
We would like instead to keep transitions that are
used in relatively many probable paths through the
automaton. The probability of path p is P (p) =
?
(v,w)?p P (w|v). Then the posterior probability of
edge (v,w) is
P (v,w) =
?
(v,w)?p P (p)
?
p P (p)
,
which can be efficiently computed by the forward-
backward algorithm (Rabiner, 1989). We can now
remove transitions leaving state v whose posterior
probability is lower than pv = k(maxw P (v,w)),
where 0 < k ? 1 controls the degree of pruning,
with higher k forcing more pruning. All induced and
pruned automata are trimmed to remove unreachable
states.
10
11
12
of
of
of
the
the
an
 (98)
13a
an
... (40)
... (7)
(5)
(80)
(18)
(40)(20)
(20)
(20)
(2)
-ENT-
Figure 2: Automaton to be pruned at state 10. Tran-
sition counts are shown in parenthesis.
3 Automata as Extractor
Each automaton induced using the method described
in Sections 2.3-2.3.2 represents high-precision pat-
terns that start with a given trigger word. By scan-
ning unlabeled data using these patterns, we can ex-
tract text segments which can be substituted for the
slot token -ENT-. For example, assume that the in-
duced pattern is ?analyst at -ENT- and? and that
the scanned text is ?He is an analyst at the Univer-
sity of California and ...?. By scanning this text us-
ing the pattern mentioned above, we can figure out
that the text ?the University of California? can sub-
stitute for ?-ENT-?. This extracted segment is a
candidate extracted entity. We now need to decide
whether we should retain all tokens inside a candi-
date extraction or purge some tokens, such as ?the?
in the example.
One way to handle this problem is to build a
language model of content tokens and retain only
the maximum likelihood token sequence. However,
in the current work, the following heuristic which
worked well in practice is used. Each token in the
extracted text segment is labeled either keep (K) or
droppable (D). By default, a token is labeled K. A
token is labeled D if it satisfies one of the droppable
criteria. In the experiments reported in this paper,
droppable criteria were whether the token is present
in a stopword list, whether it is non-capitalized, or
whether it is a number.
Once tokens in a candidate extraction are labeled
using the above heuristic, the longest token sequence
corresponding to the regular expression K[D K]?K is
retained and is considered a final extraction. If there
is only one K token, that token is retained as the fi-
nal extraction. In the example above, the tokens are
labeled ?the/D University/K of/D California/K?, and
the extracted entity will be ?University of Califor-
nia?.
To handle run-away extractions, we can set a
domain-dependent hard limit on the number of to-
kens which can be matched with ?-ENT-?. This
stems from the intuition that useful extractions are
not very long. For example, it is rare that a person
name longer than five tokens.
3.1 Ranking Patterns and Entities
Using the method described above, patterns and
the entities extracted by them from unlabeled data
are paired. But both patterns and extractions vary
in quality, so we need a method for ranking both.
Hence, we need to rank both patterns and entities.
This is difficult given that there we have no nega-
144
tive labeled data. Seed entities are the only positive
instances that are available.
Related previous work tried to address this prob-
lem. Agichtein and Gravano (2000) seek to extract
relations, so their pattern evaluation strategy consid-
ers one of the attributes of an extracted tuple as a
key. They judge the tuple as a positive or a negative
match for the pattern depending on whether there are
other extracted values associated with the same key.
Unfortunately, this method is not applicable to entity
extraction.
The pattern evaluation mechanism used here is
similar in spirit to those of Etzioni et al (2005) and
Lin et al (2003). With seeds for multiple classes
available, we consider seed instances of one class
as negative instances for the other classes. A pat-
tern is penalized if it extracts entities which belong
to the seed lists of the other classes. Let pos(p) and
neg(p) be respectively the number of distinct pos-
itive and negative seeds extracted by pattern p. In
contrast to previous work mentioned above, we do
not combine pos(p) and neg(p) to calculate a single
accuracy value. Instead, we discard all patterns p
with positive neg(p) value, as well as patterns whose
total positive seed (distinct) extraction count is less
than certain threshold ?pattern. This scoring is very
conservative. There are several motivations for such
a conservative scoring. First, we are more interested
in precision than recall. We believe that with mas-
sive corpora, large number of entity instances can
be extracted anyway. High accuracy extractions al-
low us to reliably (without any human evaluation)
use extracted entities in subsequent tasks success-
fully (see Section 4.3). Second, in the absence of
sophisticated pattern evaluation schemes (which we
are investigating ? Section 6), we feel it is best to
heavily penalize any pattern that extracts even a sin-
gle negative instance.
Let G be the set of patterns which are retained
by the filtering scheme described above. Also, let
I(e, p) be an indicator function which takes value 1
when entity e is extracted by pattern p and 0 other-
wise. The score of e, S(e), is given by
S(e) = ?p?GI(e, p)
This whole process can be iterated by includ-
ing extracted entities whose score is greater than or
equal to a certain threshold ?entity to the seed list.
4 Experimental Results
For the experiments described below, we used 18
billion tokens (31 million documents) of news data
as the source of unlabeled data. We experimented
with 500 and 1000 trigger words. The results pre-
sented were obtained after a single iteration of the
Context Pattern Induction algorithm (Section 2).
4.1 English LOC, ORG and PER
For this experiment, we used as seed sets subsets of
the entity lists provided with CoNLL-2003 shared
task data.3 Only multi-token entries were included
in the seed lists of respective categories (location
(LOC), person (PER) & organization (ORG) in this
case). This was done to partially avoid incorrect
context extraction. For example, if the seed entity is
?California?, then the same string present in ?Uni-
versity of California? can be incorrectly considered
as an instance of LOC. A stoplist was used for drop-
ping tokens from candidate extractions, as described
in Section 3. Examples of top ranking induced pat-
terns and extracted entities are shown in Table 9.
Seed list sizes and experimental results are shown
in Table 3. The precision numbers shown in Table 3
were obtained by manually evaluating 100 randomly
selected instances from each of the extended lists.
Category Seed
Size
Patterns
Used
Extended
Size
Precision
LOC 379 29 3001 70%
ORG 1597 276 33369 85%
PER 3616 265 86265 88%
Table 3: Results of LOC, ORG & PER entity list ex-
tension experiment with ?pattern = 10 set manually.
The overlap4 between the induced ORG list and
the Fortune-500 list has 357 organization names,
which is significantly higher than the seed list over-
lap of 22 (see Section 1). This shows that we have
been able to improve coverage considerably.
4.2 Watch Brand Name
A total of 17 watch brand names were used as
seeds. In addition to the pattern scoring scheme
3A few locally available entities in each category were also
added. These seeds are available upon request from the authors.
4Using same matching criteria as in Section 1.
145
of Section 3.1, only patterns containing sequence
?watch? were finally retained. Entities extracted
with ?entity = 2 are shown in Table 5. Extraction
precision is 85.7%.
Corum, Longines, Lorus, Movado, Accutron, Au-
demars Piguet, Cartier, Chopard, Franck Muller,
IWC, Jaeger-LeCoultre, A. Lange & Sohne, Patek
Philippe, Rolex, Ulysse, Nardin, Vacheron Con-
stantin
Table 4: Watch brand name seeds.
Rolex Fossil Swatch
Cartier Tag Heuer Super Bowl
Swiss Chanel SPOT
Movado Tiffany Sekonda
Seiko TechnoMarine Rolexes
Gucci Franck Muller Harry Winston
Patek Philippe Versace Hampton Spirit
Piaget Raymond Weil Girard Perregaux
Omega Guess Frank Mueller
Citizen Croton David Yurman
Armani Audemars Piguet Chopard
DVD DVDs Chinese
Breitling Montres Rolex Armitron
Tourneau CD NFL
Table 5: Extended list of watch brand names after
single iteration of pattern induction algorithm.
This experiment is interesting for several reasons.
First, it shows that the method presented in this pa-
per is effective even with small number of seed in-
stances. From this we conclude that the unambigu-
ous nature of seed instances is much more important
than the size of the seed list. Second, no negative
information was used during pattern ranking in this
experiment. This suggests that for relatively unam-
biguous categories, it is possible to successfully rank
patterns using positive instances only.
4.3 Extended Lists as Features in a Tagger
Supervised models normally outperform unsuper-
vised models in extraction tasks. The downside of
supervised learning is expensive training data. On
the other hand, massive amounts of unlabeled data
are readily available. The goal of semi-supervised
learning to combine the best of both worlds. Recent
research have shown that improvements in super-
vised taggers are possible by including features de-
rived from unlabeled data (Miller et al, 2004; Liang,
2005; Ando and Zhang, 2005). Similarly, automati-
cally generated entity lists can be used as additional
features in a supervised tagger.
System F1 (Precision, Recall)
Florian et al (2003),
best single, no list
89.94 (91.37, 88.56)
Zhang and Johnson
(2003), no list
90.26 (91.00, 89.53)
CRF baseline, no list 89.52 (90.39, 88.66)
Table 6: Baseline comparison on 4 categories (LOC,
ORG, PER, MISC) on Test-a dataset.
For this experiment, we started with a conditional
random field (CRF) (Lafferty et al, 2001) tagger
with a competitive baseline (Table 6). The base-
line tagger was trained5 on the full CoNLL-2003
shared task data. We experimented with the LOC,
ORG and PER lists that were automatically gener-
ated in Section 4.1. In Table 7, we show the accuracy
of the tagger for the entity types for which we had
induced lists. The test conditions are just baseline
features with no list membership, baseline plus seed
list membership features, and baseline plus induced
list membership features. For completeness, we also
show in Table 8 accuracy on the full CoNLL task
(four entity types) without lists, with seed list only,
and with the three induced lists. The seed lists (Sec-
tion 4.1) were prepared from training data itself and
hence with increasing training data size, the model
overfitted as it became completely reliant on these
seed lists. From Tables 7 & 8 we see that incor-
poration of token membership in the extended lists
as additional membership features led to improve-
ments across categories and at all sizes of training
data. This also shows that the extended lists are of
good quality, since the tagger is able to extract useful
evidence from them.
Relatively small sizes of training data pose inter-
esting learning situation and is the case with practi-
cal applications. It is encouraging to observe that the
list features lead to significant improvements in such
cases. Also, as can be seen from Table 7 & 8, these
lists are effective even with mature taggers trained
on large amounts of labeled data.
5Standard orthographic information, such as character n-
grams, capitalization, tokens in immediate context, chunk tags,
and POS were used as features.
146
Training Data Test-a Test-b
(Tokens) No List Seed List Unsup. List No List Seed List Unsup. List
9268 68.16 70.91 72.82 60.30 63.83 65.56
23385 78.36 79.21 81.36 71.44 72.16 75.32
46816 82.08 80.79 83.84 76.44 75.36 79.64
92921 85.34 83.03 87.18 81.32 78.56 83.05
203621 89.71 84.50 91.01 84.03 78.07 85.70
Table 7: CRF tagger F-measure on LOC, ORG, PER extraction.
Training Data Test-a Test-b
(Tokens) No List Seed List Unsup. List No List Seed List Unsup. List
9229 68.27 70.93 72.26 61.03 64.52 65.60
204657 89.52 84.30 90.48 83.17 77.20 84.52
Table 8: CRF tagger F-measure on LOC, ORG, PER and MISC extraction.
5 Related Work
The method presented in this paper is similar in
many respects to some of the previous work on
context pattern induction (Riloff and Jones, 1999;
Agichtein and Gravano, 2000; Lin et al, 2003; Et-
zioni et al, 2005), but there are important differ-
ences. Agichtein and Gravano (2000) focus on rela-
tion extraction while we are interested in entity ex-
traction. Moreover, Agichtein and Gravano (2000)
depend on an entity tagger to initially tag unlabeled
data whereas we do not have such requirement. The
pattern learning methods of Riloff and Jones (1999)
and the generic extraction patterns of Etzioni et al
(2005) use language-specific information (for exam-
ple, chunks). In contrast, the method presented here
is language independent. For instance, the English
pattern induction system presented here was applied
on German data without any change. Also, in the
current method, induced automata compactly repre-
sent all induced patterns. The patterns induced by
Riloff and Jones (1999) extract NPs and that deter-
mines the number of tokens to include in a single
extraction. We avoid using such language dependent
chunk information as the patterns in our case include
right6 boundary tokens thus explicitly specifying the
slot in which an entity can occur. Another interest-
ing deviation here from previous work on context
pattern induction is the fact that on top of extending
6In case of predictive left context.
seed lists at high precision, we have successfully in-
cluded membership in these automatically generated
lexicons as features in a high quality named entity
tagger improving its performance.
6 Conclusion
We have presented a novel language-independent
context pattern induction method. Starting with a
few seed examples, the method induces in an unsu-
pervised way context patterns and extends the seed
list by extracting more instances of the same cat-
egory at fairly high precision from unlabeled data.
We were able to improve a CRF-based high quality
named entity tagger by using membership in these
automatically generated lists as additional features.
Pattern and entity ranking methods need further
investigation. Thorough comparison with previ-
ously proposed methods also needs to be carried out.
Also, it will be interesting to see whether the fea-
tures generated in this paper complement some of
the other methods (Miller et al, 2004; Liang, 2005;
Ando and Zhang, 2005) that also generate features
from unlabeled data.
7 Acknowledgements
We thank the three anonymous reviewers as well as
Wojciech Skut, Vrishali Wagle, Louis Monier, and
Peter Norvig for valuable suggestions. This work is
supported in part by NSF grant EIA-0205448.
147
Induced LOC Patterns
troops in -ENT-to
Cup qualifier against -ENT-in
southern -ENT-town
war - torn -ENT-.
countries including -ENT-.
Bangladesh and -ENT-,
England in -ENT-in
west of -ENT-and
plane crashed in -ENT-.
Cup qualifier against -ENT-,
Extracted LOC Entities
US
United States
Japan
South Africa
China
Pakistan
France
Mexico
Israel
Pacific
Induced PER Patterns
compatriot -ENT-.
compatriot -ENT-in
Rep. -ENT-,
Actor -ENT-is
Sir -ENT-,
Actor -ENT-,
Tiger Woods , -ENT-and
movie starring -ENT-.
compatriot -ENT-and
movie starring -ENT-and
Extracted PER Entities
Tiger Woods
Andre Agassi
Lleyton Hewitt
Ernie Els
Serena Williams
Andy Roddick
Retief Goosen
Vijay Singh
Jennifer Capriati
Roger Federer
Induced ORG Patterns
analyst at -ENT-.
companies such as -ENT-.
analyst with -ENT-in
series against the -ENT-tonight
Today ?s Schaeffer ?s Option Activity Watch features -ENT-(
Cardinals and -ENT-,
sweep of the -ENT-with
joint venture with -ENT-(
rivals -ENT-Inc.
Friday night ?s game against -ENT-.
Extracted ORG Entities
Boston Red Sox
St. Louis Cardinals
Chicago Cubs
Florida Marlins
Montreal Expos
San Francisco Giants
Red Sox
Cleveland Indians
Chicago White Sox
Atlanta Braves
Table 9: Top ranking LOC, PER, ORG induced pattern and extracted entity examples.
References
Eugene Agichtein and Luis Gravano. 2000. Snowball:
Extracting relations from large plain-text collections.
In Proceedings of the Fifth ACM International Con-
ference on Digital Libraries.
Rie Ando and Tong Zhang. 2005. A high-performance
semi-supervised learning method for text chunking. In
Proceedings of ACL-2005. Ann Arbor, USA.
Boris Chidlovskii. 2000. Wrapper generation by k-
reversible grammar induction. ECAI Workshop on
Machine Learning for Information Extraction.
Oren Etzioni, Michael Cafarella, Doug Downey, Ana-
Maria Popescu, Tal Shaked, Stephen Soderland,
Daniel S. Weld, and Alexander Yates. 2005. Unsuper-
vised named-entity extraction from the web - an exper-
imental study. Artificial Intelligence Journal.
Radu Florian, Abe Ittycheriah, Hongyan Jing, and Tong
Zhang. 2003. Named entity recognition through clas-
sifier combination. In Proceedings of CoNLL-2003.
Dayne Freitag. 1997. Using grammatical inference to
improve precision in information extraction. In ICML-
97 Workshop on Automata Induction, Grammatical In-
ference, and Language Acquisition, Nashville.
John Lafferty, Andrew McCallum, and Fernando Pereira.
2001. Conditional random fields: Probabilistic models
for segmenting and labeling sequence data. In Proc.
ICML 2001.
Percy Liang. 2005. Semi-supervised learning for natural
language. MEng. Thesis, MIT.
Winston Lin, Roman Yangarber, and Ralph Grishman.
2003. Bootstrapped learning of semantic classes from
positive and negative examples. In Proceedings of
ICML-2003 Workshop on The Continuum from La-
beled to Unlabeled Data.
Scott Miller, Jethran Guinness, and Alex Zamanian.
2004. Name tagging with word clusters and discrimi-
native training. In Proceedings of HLT-NAACL 2004.
L. R. Rabiner. 1989. A tutorial on hidden markov mod-
els and selected applications in speech recognition. In
Proc. of IEEE, 77, 257?286.
Ellen Riloff and Rosie Jones. 1999. Learning Dictio-
naries for Information Extraction by Multi-level Boot-
strapping. In Proceedings of the Sixteenth National
Conference on Artificial Intelligence.
Zach Solan, David Horn, Eytan Ruppin, and Shimon
Edelman. 2005. Unsupervised learning of natural lan-
guages. In Proceedings of National Academy of Sci-
iences. 102:11629-11634.
Michael Thelen and Ellen Riloff. 2002. A bootstrapping
method for learning semantic lexicons using extraction
pattern contexts. In Proceedings of EMNLP 2002.
Tong Zhang and David Johnson. 2003. A robust risk
minimization based named entity recognition system.
In Proceedings of CoNLL-2003.
148
Proceedings of the 10th Conference on Computational Natural Language Learning (CoNLL-X),
pages 216?220, New York City, June 2006. c?2006 Association for Computational Linguistics
Multilingual Dependency Analysis with a Two-Stage Discriminative Parser
Ryan McDonald Kevin Lerman Fernando Pereira
Department of Computer and Information Science
University of Pennsylvania
Philadelphia, PA
{ryantm,klerman,pereira}@cis.upenn.edu
Abstract
We present a two-stage multilingual de-
pendency parser and evaluate it on 13
diverse languages. The first stage is
based on the unlabeled dependency pars-
ing models described by McDonald and
Pereira (2006) augmented with morpho-
logical features for a subset of the lan-
guages. The second stage takes the out-
put from the first and labels all the edges
in the dependency graph with appropri-
ate syntactic categories using a globally
trained sequence classifier over compo-
nents of the graph. We report results on
the CoNLL-X shared task (Buchholz et
al., 2006) data sets and present an error
analysis.
1 Introduction
Often in language processing we require a deep syn-
tactic representation of a sentence in order to assist
further processing. With the availability of resources
such as the Penn WSJ Treebank, much of the fo-
cus in the parsing community had been on producing
syntactic representations based on phrase-structure.
However, recently their has been a revived interest
in parsing models that produce dependency graph
representations of sentences, which model words
and their arguments through directed edges (Hud-
son, 1984; Mel?c?uk, 1988). This interest has gener-
ally come about due to the computationally efficient
and flexible nature of dependency graphs and their
ability to easily model non-projectivity in freer-word
order languages. Nivre (2005) gives an introduction
to dependency representations of sentences and re-
cent developments in dependency parsing strategies.
Dependency graphs also encode much of the deep
syntactic information needed for further process-
ing. This has been shown through their success-
ful use in many standard natural language process-
ing tasks, including machine translation (Ding and
Palmer, 2005), sentence compression (McDonald,
2006), and textual inference (Haghighi et al, 2005).
In this paper we describe a two-stage discrimi-
native parsing approach consisting of an unlabeled
parser and a subsequent edge labeler. We evaluate
this parser on a diverse set of 13 languages using
data provided by the CoNLL-X shared-task organiz-
ers (Buchholz et al, 2006; Hajic? et al, 2004; Simov
et al, 2005; Simov and Osenova, 2003; Chen et al,
2003; Bo?hmova? et al, 2003; Kromann, 2003; van
der Beek et al, 2002; Brants et al, 2002; Kawata
and Bartels, 2000; Afonso et al, 2002; Dz?eroski et
al., 2006; Civit Torruella and Mart?? Anton??n, 2002;
Nilsson et al, 2005; Oflazer et al, 2003; Atalay et
al., 2003). The results are promising and show the
language independence of our system under the as-
sumption of a labeled dependency corpus in the tar-
get language.
For the remainder of this paper, we denote by
x = x1, . . . xn a sentence with n words and by
y a corresponding dependency graph. A depen-
dency graph is represented by a set of ordered pairs
(i, j) ? y in which xj is a dependent and xi is the
corresponding head. Each edge can be assigned a la-
bel l(i,j) from a finite set L of predefined labels. We
216
assume that all dependency graphs are trees but may
be non-projective, both of which are true in the data
sets we use.
2 Stage 1: Unlabeled Parsing
The first stage of our system creates an unlabeled
parse y for an input sentence x. This system is
primarily based on the parsing models described
by McDonald and Pereira (2006). That work ex-
tends the maximum spanning tree dependency pars-
ing framework (McDonald et al, 2005a; McDonald
et al, 2005b) to incorporate features over multiple
edges in the dependency graph. An exact projec-
tive and an approximate non-projective parsing al-
gorithm are presented, since it is shown that non-
projective dependency parsing becomes NP-hard
when features are extended beyond a single edge.
That system uses MIRA, an online large-margin
learning algorithm, to compute model parameters.
Its power lies in the ability to define a rich set of fea-
tures over parsing decisions, as well as surface level
features relative to these decisions. For instance, the
system of McDonald et al (2005a) incorporates fea-
tures over the part of speech of words occurring be-
tween and around a possible head-dependent rela-
tion. These features are highly important to over-
all accuracy since they eliminate unlikely scenarios
such as a preposition modifying a noun not directly
to its left, or a noun modifying a verb with another
verb occurring between them.
We augmented this model to incorporate morpho-
logical features derived from each token. Consider a
proposed dependency of a dependent xj on the head
xi, each with morphological features Mj and Mi re-
spectively. We then add to the representation of the
edge: Mi as head features, Mj as dependent fea-
tures, and also each conjunction of a feature from
both sets. These features play the obvious role of
explicitly modeling consistencies and commonali-
ties between a head and its dependents in terms of
attributes like gender, case, or number. Not all data
sets in our experiments include morphological fea-
tures, so we use them only when available.
3 Stage 2: Label Classification
The second stage takes the output parse y for sen-
tence x and classifies each edge (i, j) ? y with a
particular label l(i,j). Ideally one would like to make
all parsing and labeling decisions jointly so that the
shared knowledge of both decisions will help resolve
any ambiguities. However, the parser is fundamen-
tally limited by the scope of local factorizations that
make inference tractable. In our case this means
we are forced only to consider features over single
edges or pairs of edges. However, in a two stage
system we can incorporate features over the entire
output of the unlabeled parser since that structure is
fixed as input. The simplest labeler would be to take
as input an edge (i, j) ? y for sentence x and find
the label with highest score,
l(i,j) = argmax
l
s(l, (i, j),y,x)
Doing this for each edge in the tree would pro-
duce the final output. Such a model could easily be
trained using the provided training data for each lan-
guage. However, it might be advantageous to know
the labels of other nearby edges. For instance, if we
consider a head xi with dependents xj1 , . . . , xjM , it
is often the case that many of these dependencies
will have correlated labels. To model this we treat
the labeling of the edges (i, j1), . . . , (i, jM ) as a se-
quence labeling problem,
(l(i,j1), . . . , l(i,jM )) = l? = argmax
l?
s(l?, i,y,x)
We use a first-order Markov factorization of the
score
l? = argmax
l?
M
?
m=2
s(l(i,jm), l(i,jm?1), i,y,x)
in which each factor is the score of labeling the adja-
cent edges (i, jm) and (i, jm?1) in the tree y. We at-
tempted higher-order Markov factorizations but they
did not improve performance uniformly across lan-
guages and training became significantly slower.
For score functions, we use simple dot products
between high dimensional feature representations
and a weight vector
s(l(i,jm), l(i,jm?1), i,y,x) =
w ? f(l(i,jm), l(i,jm?1), i,y,x)
Assuming we have an appropriate feature repre-
sentation, we can find the highest scoring label se-
quence with Viterbi?s algorithm. We use the MIRA
217
online learner to set the weights (Crammer and
Singer, 2003; McDonald et al, 2005a) since we
found it trained quickly and provide good perfor-
mance. Furthermore, it made the system homoge-
neous in terms of learning algorithms since that is
what is used to train our unlabeled parser (McDon-
ald and Pereira, 2006). Of course, we have to define
a set of suitable features. We used the following:
? Edge Features: Word/pre-suffix/part-of-speech
(POS)/morphological feature identity of the head and the
dependent (affix lengths 2 and 3). Does the head and its
dependent share a prefix/suffix? Attachment direction.
What morphological features do head and dependent
have the same value for? Is the dependent the first/last
word in the sentence?
? Sibling Features: Word/POS/pre-suffix/morphological
feature identity of the dependent?s nearest left/right sib-
lings in the tree (siblings are words with same parent in
the tree). Do any of the dependent?s siblings share its
POS?
? Context Features: POS tag of each intervening word be-
tween head and dependent. Do any of the words between
the head and the dependent have a parent other than the
head? Are any of the words between the head and the de-
pendent not a descendant of the head (i.e. non-projective
edge)?
? Non-local: How many children does the dependent have?
What morphological features do the grandparent and the
dependent have identical values? Is this the left/right-
most dependent for the head? Is this the first dependent
to the left/right of the head?
Various conjunctions of these were included
based on performance on held-out data. Note that
many of these features are beyond the scope of the
edge based factorizations of the unlabeled parser.
Thus a joint model of parsing and labeling could not
easily include them without some form of re-ranking
or approximate parameter estimation.
4 Results
We trained models for all 13 languages provided
by the CoNLL organizers (Buchholz et al, 2006).
Based on performance from a held-out section of the
training data, we used non-projective parsing algo-
rithms for Czech, Danish, Dutch, German, Japanese,
Portuguese and Slovene, and projective parsing al-
gorithms for Arabic, Bulgarian, Chinese, Spanish,
Swedish and Turkish. Furthermore, for Arabic and
Spanish, we used lemmas instead of inflected word
DATA SET UA LA
ARABIC 79.3 66.9
BULGARIAN 92.0 87.6
CHINESE 91.1 85.9
CZECH 87.3 80.2
DANISH 90.6 84.8
DUTCH 83.6 79.2
GERMAN 90.4 87.3
JAPANESE 92.8 90.7
PORTUGUESE 91.4 86.8
SLOVENE 83.2 73.4
SPANISH 86.1 82.3
SWEDISH 88.9 82.5
TURKISH 74.7 63.2
AVERAGE 87.0 80.8
Table 1: Dependency accuracy on 13 languages.
Unlabeled (UA) and Labeled Accuracy (LA).
forms, again based on performance on held-out
data1.
Results on the test set are given in Table 1. Per-
formance is measured through unlabeled accuracy,
which is the percentage of words that modify the
correct head in the dependency graph, and labeled
accuracy, which is the percentage of words that
modify the correct head and label the dependency
edge correctly in the graph. These results show that
the discriminative spanning tree parsing framework
(McDonald et al, 2005b; McDonald and Pereira,
2006) is easily adapted across all these languages.
Only Arabic, Turkish and Slovene have parsing ac-
curacies significantly below 80%, and these lan-
guages have relatively small training sets and/or are
highly inflected with little to no word order con-
straints. Furthermore, these results show that a two-
stage system can achieve a relatively high perfor-
mance. In fact, for every language our models per-
form significantly higher than the average perfor-
mance for all the systems reported in Buchholz et
al. (2006).
For the remainder of the paper we provide a gen-
eral error analysis across a wide set of languages
plus a detailed error analysis of Spanish and Arabic.
5 General Error Analysis
Our system has several components, including the
ability to produce non-projective edges, sequential
1Using the non-projective parser for all languages does not
effect performance significantly. Similarly, using the inflected
word form instead of the lemma for all languages does not
change performance significantly.
218
SYSTEM UA LA
N+S+M 86.3 79.7
P+S+M 85.6 79.2
N+S+B 85.5 78.6
N+A+M 86.3 79.4
P+A+B 84.8 77.7
Table 2: Error analysis of parser components av-
eraged over Arabic, Bulgarian, Danish, Dutch,
Japanese, Portuguese, Slovene, Spanish, Swedish
and Turkish. N/P: Allow non-projective/Force pro-
jective, S/A: Sequential labeling/Atomic labeling,
M/B: Include morphology features/No morphology
features.
assignment of edge labels instead of individual as-
signment, and a rich feature set that incorporates
morphological properties when available. The bene-
fit of each of these is shown in Table 2. These results
report the average labeled and unlabeled precision
for the 10 languages with the smallest training sets.
This allowed us to train new models quickly.
Table 2 shows that each component of our system
does not change performance significantly (rows 2-
4 versus row 1). However, if we only allow projec-
tive parses, do not use morphological features and
label edges with a simple atomic classifier, the over-
all drop in performance becomes significant (row
5 versus row 1). Allowing non-projective parses
helped with freer word order languages like Dutch
(78.8%/74.7% to 83.6%/79.2%, unlabeled/labeled
accuracy). Including rich morphology features natu-
rally helped with highly inflected languages, in par-
ticular Spanish, Arabic, Turkish, Slovene and to a
lesser extent Dutch and Portuguese. Derived mor-
phological features improved accuracy in all these
languages by 1-3% absolute.
Sequential classification of labels had very lit-
tle effect on overall labeled accuracy (79.4% to
79.7%)2. The major contribution was in helping to
distinguish subjects, objects and other dependents
of main verbs, which is the most common label-
ing error. This is not surprising since these edge
labels typically are the most correlated (i.e., if you
already know which noun dependent is the subject,
then it should be easy to find the object). For in-
stance, sequential labeling improves the labeling of
2This difference was much larger for experiments in which
gold standard unlabeled dependencies are used.
objects from 81.7%/75.6% to 84.2%/81.3% (la-
beled precision/recall) and the labeling of subjects
from 86.8%/88.2% to 90.5%/90.4% for Swedish.
Similar improvements are common across all lan-
guages, though not as dramatic. Even with this im-
provement, the labeling of verb dependents remains
the highest source of error.
6 Detailed Analysis
6.1 Spanish
Although overall unlabeled accuracy is 86%, most
verbs and some conjunctions attach to their head
words with much lower accuracy: 69% for main
verbs, 75% for the verb ser, and 65% for coor-
dinating conjunctions. These words form 17% of
the test corpus. Other high-frequency word classes
with relatively low attachment accuracy are preposi-
tions (80%), adverbs (82%) and subordinating con-
junctions (80%), for a total of another 23% of the
test corpus. These weaknesses are not surprising,
since these decisions encode the more global as-
pects of sentence structure: arrangement of clauses
and adverbial dependents in multi-clause sentences,
and prepositional phrase attachment. In a prelimi-
nary test of this hypothesis, we looked at all of the
sentences from a development set in which a main
verb is incorrectly attached. We confirmed that the
main clause is often misidentified in multi-clause
sentences, or that one of several conjoined clauses
is incorrectly taken as the main clause. To test this
further, we added features to count the number of
commas and conjunctions between a dependent verb
and its candidate head. Unlabeled accuracy for all
verbs increases from 71% to 73% and for all con-
junctions from 71% to 74%. Unfortunately, accu-
racy for other word types decreases somewhat, re-
sulting in no significant net accuracy change. Nev-
ertheless, this very preliminary experiment suggests
that wider-range features may be useful in improv-
ing the recognition of overall sentence structure.
Another common verb attachment error is a
switch between head and dependent verb in phrasal
verb forms like dejan intrigar or qiero decir, possi-
bly because the non-finite verb in these cases is often
a main verb in training sentences. We need to look
more carefully at verb features that may be useful
here, in particular features that distinguish finite and
219
non-finite forms.
In doing this preliminary analysis, we noticed
some inconsistencies in the reference dependency
structures. For example, in the test sentence Lo
que decia Mae West de si misma podr??amos decirlo
tambie?n los hombres:..., decia?s head is given as de-
cirlo, although the main verbs of relative clauses are
normally dependent on what the relative modifies, in
this case the article Lo.
6.2 Arabic
A quick look at unlabeled attachment accuracies in-
dicate that errors in Arabic parsing are the most
common across all languages: prepositions (62%),
conjunctions (69%) and to a lesser extent verbs
(73%). Similarly, for labeled accuracy, the hard-
est edges to label are for dependents of verbs, i.e.,
subjects, objects and adverbials. Note the differ-
ence in error between the unlabeled parser and the
edge labeler: the former makes mistakes on edges
into prepositions, conjunctions and verbs, and the
latter makes mistakes on edges into nouns (sub-
ject/objects). Each stage by itself is relatively ac-
curate (unlabeled accuracy is 79% and labeling ac-
curacy3 is also 79%), but since there is very little
overlap in the kinds of errors each makes, overall la-
beled accuracy drops to 67%. This drop is not nearly
as significant for other languages.
Another source of potential error is that the aver-
age sentence length of Arabic is much higher than
other languages (around 37 words/sentence). How-
ever, if we only look at performance for sentences
of length less than 30, the labeled accuracy is still
only 71%. The fact that Arabic has only 1500 train-
ing instances might also be problematic. For exam-
ple if we train on 200, 400, 800 and the full training
set, labeled accuracies are 54%, 60%, 62% and 67%.
Clearly adding more data is improving performance.
However, when compared to the performance of
Slovene (1500 training instances) and Spanish (3300
instances), it appears that Arabic parsing is lagging.
7 Conclusions
We have presented results showing that the spanning
tree dependency parsing framework of McDonald et
3Labeling accuracy is the percentage of words that correctly
label the dependency between the head that they modify, even
if the right head was not identified.
al. (McDonald et al, 2005b; McDonald and Pereira,
2006) generalizes well to languages other than En-
glish. In the future we plan to extend these mod-
els in two ways. First, we plan on examining the
performance difference between two-staged depen-
dency parsing (as presented here) and joint parsing
plus labeling. It is our hypothesis that for languages
with fine-grained label sets, joint parsing and label-
ing will improve performance. Second, we plan on
integrating any available morphological features in
a more principled manner. The current system sim-
ply includes all morphological bi-gram features. It
is our hope that a better morphological feature set
will help with both unlabeled parsing and labeling
for highly inflected languages.
References
S. Buchholz, E. Marsi, A. Dubey, and Y. Krymolowski.
2006. CoNLL-X shared task on multilingual depen-
dency parsing. SIGNLL.
K. Crammer and Y. Singer. 2003. Ultraconservative on-
line algorithms for multiclass problems. JMLR.
Y. Ding and M. Palmer. 2005. Machine translation using
probabilistic synchronous dependency insertion gram-
mars. In Proc. ACL.
A. Haghighi, A. Ng, and C. Manning. 2005. Robust
textual inference via graph matching. In Proc. HTL-
EMNLP.
R. Hudson. 1984. Word Grammar. Blackwell.
R. McDonald and F. Pereira. 2006. Online learning of
approximate dependency parsing algorithms. In Proc.
EACL.
R. McDonald, K. Crammer, and F. Pereira. 2005a. On-
line large-margin training of dependency parsers. In
Proc. ACL.
R. McDonald, F. Pereira, K. Ribarov, and J. Hajic?. 2005b.
Non-projective dependency parsing using spanning
tree algorithms. In Proc. HLT-EMNLP.
R. McDonald. 2006. Discriminative sentence compres-
sion with soft syntactic constraints. In Proc. EACL.
I.A. Mel?c?uk. 1988. Dependency Syntax: Theory and
Practice. State University of New York Press.
J. Nivre. 2005. Dependency grammar and dependency
parsing. Technical Report MSI report 05133, Va?xjo?
University: School of Mathematics and Systems Engi-
neering.
220
TextGraphs-2: Graph-Based Algorithms for Natural Language Processing, pages 37?44,
Rochester, April 2007 c?2007 Association for Computational Linguistics
Transductive Structured Classification through Constrained
Min-Cuts
Kuzman Ganchev Fernando Pereira
Computer and Information Science
University of Pennsylvania
Philadelphia PA
{kuzman,pereira}@cis.upenn.edu
Abstract
We extend the Blum and Chawla
(2001) graph min-cut algorithm to
structured problems. This extension
can alternatively be viewed as a joint
inference method over a set of train-
ing and test instances where parts of
the instances interact through a pre-
specified associative network. The
method has has an efficient approxima-
tion through a linear-programming re-
laxation. On small training data sets,
the method achieves up to 34.8% rela-
tive error reduction.
1 Introduction
We describe a method for transductive classifi-
cation in structured problems. Our method ex-
tends the Blum and Chawla (2001) algorithm for
transductive classification. In that algorithm,
each training and test instance is represented
by a vertex in a graph. The algorithm finds the
min-cut that separates the positively and nega-
tively labeled instances. We give a linear pro-
gram that implements an approximation of this
algorithm and extend it in several ways. First,
our formulation can be used in cases where there
are more than two labels. Second, we can use
the output of a classifier to provide a prior pref-
erence of each instance for a particular label.
This lets us trade off the strengths of the min-
cut algorithm against those of a standard classi-
fier. Finally, we extend the algorithm further to
deal with structured output spaces, by encoding
parts of instances as well as constraints that en-
sure a consistent labeling of an entire instance.
The rest of this paper is organized as follows.
Section 2 explains what we mean by transduc-
tive classification and by structured problems.
Section 3 reviews the Blum and Chawla (2001)
algorithm, how we formulate it as a linear pro-
gram and our proposed extensions. Section 4
relates our proposal to previous work. Section 5
describes our experimental results on real and
synthetic data and Section 6 concludes the pa-
per.
2 Concepts and Notation
In this work we combine two separate ap-
proaches to learning: transductive methods, in
which classification of test instances arises from
optimizing a single objective involving both
training and test instances; and structured clas-
sification, in which instances involve several in-
terdependent classification problems. The de-
scription of structured problems also introduces
useful terminology for the rest of the paper.
2.1 Transductive Classification
In supervised classification, training instances
are used to induce a classifier that is then ap-
plied to individual test instances that need to
be classified. In transductive classification, a
single optimization problem is set up involving
all training and test instances; the solution of
the optimization problem yields labels for the
test instances. In this way, the test instances
provide evidence about the distribution of the
data, which may be useful when the labeled data
is limited and the distribution of unlabeled data
37
Figure 1: An example where unlabeled data
helps to reveal the underlying distribution of
the data points, borrowed from Sindhwani et al
(2005). The circles represent data points (unla-
beled are empty, positive have a ?+? and neg-
ative have a ?-?). The dashed lines represent
decision boundaries for a classifier. The first fig-
ure shows the labeled data and the max-margin
decision boundary (we use a linear boundary to
conform with Occam?s razor principle). The sec-
ond figure shows the unlabeled data points re-
vealing the distribution from which the training
examples were selected. This distribution sug-
gests that a linear boundary might not be ap-
propriate for this data. The final figure shows
a more appropriate decision boundary given the
distribution of the unlabeled data.
is informative about the location of the decision
boundary. Figure 1 illustrates this.
2.2 Structured Classification
The usual view of structured classification is as
follows. An instance consists of a set of classifi-
cation problems in which the labels of the differ-
ent problems are correlated according to a cer-
tain graphical structure. The collection of clas-
sification labels in the instance forms a single
structured label. A typical structured problem
is part of speech (POS) tagging. The parts of
speech of consecutive words are strongly corre-
lated, while the POS of words that are far away
do not influence each other much. In the natu-
ral language processing tasks that motivate this
work, we usually formalize this observation with
a Markov assumption, implemented by breaking
up the instance into parts consisting of pairs of
consecutive words. We assign a score for each
possible label of each part and then use a dy-
namic programming algorithm to find the high-
est scoring label of the entire instance.
In the rest of this paper, it will be sometimes
more convenient to think of all the (labeled and
unlabeled) instances of interest as forming a sin-
gle joint classification problem on a large graph.
In this joint problem, the atomic classification
problems are linked according to the graphical
structure imposed by their partition into struc-
tured classification instances. As we will see,
other links between atomic problems arise in our
setting that may cross between different struc-
tured instances.
2.3 Terminology
For structured problems, instance refers to an
entire problem (for example, an entire sentence
for POS tagging). A token refers to the smallest
unit that receives a label. In POS tagging, a to-
ken is a word. A part is one or more tokens and
is a division used by a learning algorithm. For
all our experiments, a part is a pair of consecu-
tive tokens, but extension to other types of parts
is trivial. If two parts share a token then a con-
sistent label for those parts has to have the same
label on the shared token. For example in the
sentence ?I love learning .? we have parts for
?I love? and ?love learning?. These share the
token ?love? and two labels for the two parts
has to agree on the label for the token in order
to be consistent. In all our experiments, a part
is a pair of consecutive tokens so two parts are
independent unless one immediately follows the
other.
3 Approach
We extend the min-cut formulation of Blum and
Chawla (2001) to multiple labels and structured
variables by adapting a linear-programming en-
coding of metric labeling problems. By relaxing
the linear program, we obtain an efficient ap-
proximate inference algorithm. To understand
our method, it is useful to review the min-
cut transductive classification algorithm (Sec-
tion 3.1) as well as the metric labeling prob-
lem and its linear programming relaxation (Sec-
tion 3.2). Section 3.3 describes how to encode
a multi-way min-cut problem as an instance of
metric labeling as well as a trivial extension that
lets us introduce a bias when computing the cut.
38
Section 3.4 extends this formalism to structured
classification.
3.1 Min-Cuts for Transductive
Classification
Blum and Chawla (2001) present an efficient
algorithm for semi-supervised machine learning
in the unstructured binary classification setting.
At a high level, the algorithm is as follows:
? Construct a graph where each instance cor-
responds to a vertex;
? Add weighted edges between similar ver-
tices with weight proportional to a measure
of similarity;
? Find the min-cut that separates positively
and negatively labeled training instances;
? Label all instances on the positive side of
the cut as positive and all others as nega-
tive.
For our purposes we need to consider two exten-
sions to this problem: multi-way classification
and constrained min-cut.
For multi-way classification, instead of com-
puting the binary min-cut as above, we need
to find the multi-way min-cut. Unfortunately,
doing this in general is NP-hard, but a poly-
nomial time approximation exists (Dahlhaus et
al., 1992). In Section 3.3 we describe how we
approximate this problem.
We extend this approach to structured data
by constructing a graph whose vertices corre-
spond to different parts of the instance, and add
weighted edges between similar parts. We then
find the multi-way min-cut that separates ver-
tices with different labels subject to some con-
straints: if two parts overlap then the labels have
to be consistent. Our main contribution is an al-
gorithm that approximately computes this con-
strained multi-way min-cut with a linear pro-
gramming relaxation.
3.2 Metric Labeling
Kleinberg and Tardos (1999) introduce the met-
ric labeling problem as a common inference
problem in a variety of fields. The inputs to
the problem are a weighted graph G = (V,E), a
set of labels L = {i|i ? 1 . . . k}, a cost function
c(v, i) which represents the preference of each
vertex for each possible label and a metric d(i, j)
between labels i and j. The goal is to assign a
label to each vertex l : V ? L so as to minimize
the cost given by:
c(l) =
?
v?V c(v, l(v))
+
?
(u,v)?E d(l(u), l(v)) ? w(u, v) .
(1)
Kleinberg and Tardos (1999) give a linear pro-
gramming approximation for this problem with
an approximation factor of two and explain how
this can be extended to an O(log k) approxima-
tion for arbitrary metrics by creating a hierar-
chy of labels. Chekuri et al (2001) present an
improved linear program that incorporates arbi-
trary metrics directly and provides an approxi-
mation at least as good as that of Kleinberg and
Tardos (1999). The idea in the new linear pro-
gram is to have a variable for each edge labeling
as well as one for each vertex labeling.
Following Chekuri et al (2001), we represent
the event that vertex u has label i by the vari-
able x(u, i) having the value 1; if x(u, i) = 0 then
vertex v must have some other label. Similarly,
we use the variable and value x(u, i, v, j) = 1 to
mean that the vertices u and v (which are con-
nected by an edge) have label i and j respec-
tively. The edge variables allow us to encode
the costs associated with violated edges in the
metric labeling problem. Edge variables should
agree with vertex labels, and by symmetry we
should have x(u, i, v, j) = x(v, j, u, i). If the
linear program gives an integer solution, this is
clearly the optimal solution to the original met-
ric labeling instance. Chekuri et al (2001) de-
scribe a rounding procedure to compute an in-
teger solution to the LP that is guaranteed to
be an approximation of the optimal integer so-
lution. For the problems we considered, this was
very rarely necessary. Their linear program re-
laxation is shown in Figure 2. The cost function
is the sum of the vertex costs and edge costs.
The first constraint requires that each vertex
have a total of one labeling unit distributed over
its labels, that is, we cannot assign more or less
than one label per vertex. The second constraint
39
min
X
u?V
X
i?L
c(u, i)x(u, i)
+
X
(u,v)?E
X
k,j?L
w(u, v)d(i, j)x(u, i, v, j)
subject to
X
i?L
x(u, i) = 1 ?u ? V
x(u, i)?
X
j?L
x(u, i, v, j) = 0 ?u ? V, v ? N(u), i ? L
x(u, i, v, j)? x(v, j, u, i) = 0 ?u, v ? V, i, j ? L
x(u, i, v, j), x(u, i) ? [0, 1] ?u, v ? V, i, j ? L
Figure 2: The Chekuri et al (2001) linear pro-
gram used to approximate metric labeling. See
text for discussion.
requires that vertex- and edge-label variables are
consistent: the label that vertex variables give
a vertex should agree with the labels that edge
variables give that vertex. The third constraint
imposes the edge-variable symmetry condition,
and the final constraint requires that all the vari-
ables be in the range [0, 1].
3.3 Min Cut as an Instance of Metric
Labeling
Given an instance of the (multi-way) min-cut
problem, we can translate it to an instance of
metric labeling as follows. The underlying graph
and edge weights will be the same as min-cut
problem. We add vertex costs (c(u, i) ?u ?
V, i ? L) and a label metric (d(i, j) ?i, j ? L).
For all unlabeled vertices set the vertex cost to
zero for all labels. For labeled vertices set the
cost of the correct label to zero and all other la-
bels to infinity. Finally let d(i, j) be one if i 6= j
and zero otherwise.
The optimal solution to this instance of metric
labeling will be the same as the optimal solution
of the initial min cut instance: the cost of any
labeling is the number of edges that link vertices
with different labels, which is exactly the num-
ber of cut edges. Also by the same argument,
every possible labeling will correspond to some
cut and approximations of the metric labeling
formulation will be approximations of the origi-
nal min-cut problem.
Since the metric labeling problem allows ar-
bitrary affinities between a vertex in the graph
and possible labels for that vertex, we can triv-
ially extend the algorithm by introducing a bias
at each vertex for labels more compatible with
that vertex. We use the output of a classifier to
bias the cost towards agreement with the clas-
sifier. Depending on the strength of the bias,
we can trade off our confidence in the perfor-
mance of the min-cut algorithm against the our
confidence in a fully-supervised classifier.
3.4 Extension to Structured
Classification
To extend this further to structured classifica-
tion we modify the Chekuri et al (2001) linear
program (Figure 2). In the structured case, we
construct a vertex for every part of an instance.
Since we want to find a consistent labeling for an
entire instance composed of overlapping parts,
we need to add some more constraints to the lin-
ear program. We want to ensure that if two ver-
tices correspond to two overlapping parts, then
they are assigned consistent labels, that is, the
token shared by two parts is given the same label
by both. First we add a new zero-weight edge
between every pair of vertices corresponding to
overlapping parts. Since its weight is zero, this
edge will not affect the cost. We then add a
constraint to the linear-program that the edge
variables for inconsistent labelings of the new
edges have a value of zero.
More formally, let (u, i, v, j) ? ? denote that
the part u having label i is consistent with the
part v having label j; if u and v do not share any
tokens, then any pair of labels for those parts are
consistent. Now add zero-weight edges between
overlapping parts. Then the only modification
to the linear program is that
x(u, i)?
?
j?L x(u, i, v, j) = 0
?u ? V, v ? N(u), i ? L
will become
x(u, i)?
?
j:(u,i,v,j)?? x(u, i, v, j) = 0
?u ? V, v ? N(u), i ? L .
40
min
X
u?V
X
i?L
c(u, i)x(u, i)
+
X
(u,v)?E
X
k,j?L
w(u, v)d(i, j)x(u, i, v, j)
subject to
X
i?L
x(u, i) = 1 ?u ? V
x(u, i)?
X
j:(u,i,v,j)??
x(u, i, v, j) = 0 ?u ? V, v ? N(u), i ? L
x(u, i, v, j)? x(v, j, u, i) = 0 ? (u, i, v, j) ? ?
x(u, i, v, j), x(u, i) ? [0, 1] ?u, v ? V, i, j ? L
Figure 3: The modified linear program used to
approximate metric labeling. See text for dis-
cussion.
What this modification does is to ensure that all
the mass of the edge variables between vertices
u and v lies in consistent labelings for their edge.
The modified linear program is shown in Figure
3. We can show that this can be encoded as
a larger instance of the metric labeling problem
(with roughly |V |+|E| more vertices and a label
set that is four times as large), but modifying the
linear program directly results in a more efficient
implementation. The final LP has one variable
for each labeling of each edge in the graph, so
we have O(|E||L|2) variables. Note that |L| is
the number of labelings of a pair of tokens for
us ? even so, computation of a single dataset
took on the order of minutes using the Xpress
MP package.
4 Relation to Previous work
Our work is set of extensions to the work of-
Blum and Chawla (2001), which we have already
described. Our extensions allow us to handle
multi-class and structured data, as well as to
take hints from a classifier. We can also spec-
ify a similarity metric between labels so that a
cut-edge can cost different amounts depending
on what partitions it spans.
Taskar et al (2004a) describe a class of
Markov networks with associative clique poten-
tials. That is, the clique potentials always prefer
that all the nodes in the clique have the same
label. The inference problem in these networks
is to find the assignment of labels to all nodes in
the graph that maximizes the sum of the clique
potentials. Their paper describes a linear pro-
gramming relaxation to find (or approximate)
this inference problem which is very similar to
the LP formulation of Chekuri et al (2001) when
all cliques are of size 2. They generalize this
to larger cliques and prove that their LP gives
an integral solution when the label alphabet has
size 2 (even for large cliques). For the learn-
ing problem they exploit the dual of the LP for-
mulation and use a maximum margin objective
similar to the one used by Taskar et al (2004b).
If we ignore the learning problem and focus on
inference, one could view our work as inference
over a Markov network created by combining a
set of linear chain conditional random fields with
an associative Markov network (with arbitrary
structure). A direction for future work would be
to train the associative Markov network either
independently from the chain-structured model
or jointly with it. This would be very similar to
the joint inference work described in the next
paragraph, and could be seen as a particular
instantiation of either a non-linear conditional
random field (Lafferty et al, 2001) or relational
Markov network (Taskar et al, 2002).
Sutton and McCallum (2004) consider the use
of linear chain CRFs augmented with extra skip
edges which encode a probabilistic belief that
the labels of two entities might be correlated.
They provide experimental results on named en-
tity recognition for e-mail messages announcing
seminars, and their system achieves a 13.7% rel-
ative reduction in error on the ?Speaker? field.
Their work differs from ours in that they add
skip edges only between identical capitalized
words and only within an instance, which for
them is an e-mail message. In particular, they
can never have an edge between labeled and un-
labeled parts. Their approach is useful for iden-
tification of personal names but less helpful for
other named entity tasks where the names may
not be capitalized.
Lafferty et al (2004) show a representer the-
orem allowing the use of Mercer kernels with
41
CRFs. They use a kernel CRF with a graph
kernel (Smola and Kondor, 2003) to do semi-
supervised learning. For them, the graph de-
fines an implicit representation of the data, but
inference is still performed only on the (chain)
structure of the CRF. By contrast, we perform
inference over the whole set of examples at the
same time.
Altun et al (2006) extend the use of graph-
based regularization to structured variables.
Their work is in the framework of maximum
margin learning for structured variables where
learning is framed as an optimization problem.
They modify the objective function by adding
a penalty whenever two parts that are expected
to have a similar label assign a different score to
the same label. They show improvements of up
to 5.3% on two real tasks: pitch accent predic-
tion and optical character recognition (OCR).
Unfortunately, to solve their optimization prob-
lem they have to invert an n?n matrix, where n
is the number of parts in the training and test-
ing data times the number of possible labels for
each part. Because of this they are forced to
train on an unrealistically small amount of data
(4-40 utterances for pitch accent prediction and
10 words for OCR).
5 Experiments
We performed experiments using our approach
on three different datasets using a conditional
random field as the base classifier. Unless oth-
erwise noted this was regularized using a zero-
mean Gaussian prior with a variance of 1.
The first dataset is the pitch-accent prediction
dataset used in semi-supervised learning by Al-
tun et al (2006). There are 31 real and binary
features (all are encoded as real values) and only
two labels. Instances correspond to an utterance
and each token corresponds to a word. Altun
et al (2006) perform experiments on 4 and 40
training instances using at most 200 unlabeled
instances.
The second dataset is the reference part of
the Cora information extraction dataset.1 This
1The Cora IE dataset has been used in Seymore et
al. (1999), Peng and McCallum (2004), McCallum et
al. (2000) and Han et al (2003), among others. We
consists of 500 computer science research paper
citations. Each token in a citation is labeled as
being part of the name of an author, part of the
title, part of the date or one of several other
labels that we combined into a single category
(?other?).
The third dataset is the chunking dataset
from the CoNLL 2000 (Sang and Buchholz,
2000) shared task restricted to noun phrases.
The task for this dataset is, given the words in a
sentence as well as automatically assigned parts
of speech for these words, label each word with
B-NP if it is the first word in a base noun phrase,
I-NP if it is part of a base noun phrase but not
the first word and O if it is not part of a noun
phrase.
For all experiments, we let each word be a
token and consider parts consisting of two con-
secutive tokens.
5.1 Pitch Accent Prediction
For the pitch accent prediction dataset, we used
the 5-nearest neighbors of each instance accord-
ing to the Euclidean distance in the original fea-
ture space to construct the graph for min-cut.
Table 1 shows the results of our experiments on
this data, as well as the results reported by Al-
tun et al (2006). The numbers in the table are
per-token accuracy and each entry is the mean
of 10 random train-test data selections.
For this problem, our method improves per-
formance over the base CRF classifier (except
when the training data consists of only 4 utter-
ances), but we do not see improvements as dra-
matic as those observed by Altun et al (2006).
Note that even the larger dataset here is quite
small ? 40 utterances where each token has been
annotated with a binary value.
5.2 Cora-IE
For the Cora information extraction dataset, we
used the first 100 principal components of the
feature space to find 5 nearest neighbors of each
part. This approximation is due to the cost of
comuting nearest neighbors in high dimensions.
In these experiments we trained on 40 instances
obtained the dataset from http://www.cs.umass.edu/
~mccallum/data/cora-ie.tar.gz.
42
Method 4:80 40:80 40:200
CRF 71.2 72.5 73.1
MinCut 69.4 74.4 74.3
STR 70.7 75.7 77.5
SVM 69.9 72.0 73.1
Table 1: Results on the pitch accent prediction
task. The methods we compare are as follows.
CRF is supervised CRF training. MinCut is our
method with a CRF as base classifier. STR and
SVM are the semi-supervised results reported in
Altun et al (2006). The experiments are 4 la-
beled and 80 unlabeled, 40 labeled and 80 unla-
beled and 40 labeled and 200 unlabeled respec-
tively.
Variance 10 100 1000
CRF 84.5% 84.3% 83.9%
MinCut 88.8% 89.6% 89.9%
Table 2: Accuracy on the Cora-IE dataset as
a percentage of tokens correctly classified at dif-
ferent settings for the CRF variance. Results for
training on 40 instances and testing on 80. In
all cases the scores are the mean of 10 random
selections of 120 instances from the set of 500
available.
and used 80 as testing data. In all cases we
randomly selected training and testing instances
10 times from the total set of 500. Table 2
shows the average accuracies for the 10 repe-
titions, with different values for the variance of
the Gaussian prior used to regularize the CRF.
If we choose the optimal value for each method,
our approach gives a 34.8% relative reduction
in error over the CRF, and improves over it in
each of the 10 random data selections, and all
settings of the Guassian prior variance.
5.3 CoNLL NP-Chunking
Our results are worst for the CoNLL NP-
Chunking dataset. As above, we used 10 ran-
dom selections of training and test sets, and
used the 100 principal components of the fea-
ture space to find 5 nearest neighbors of each
part. Table 3 shows the results of our experi-
ments. The numbers in the table are per-token
Method 20:40 40:80
CRF 87.6 90.6
MinCut(CRF) 88.2 89.6
Table 3: Results on the NP-chunking task. The
table compares a CRF with our method using a
CRF as a base classifier. The experiments use
20 labeled and 40 unlabeled and 40 labeled and
80 unlabeled instances.
accuracy as before. When the amount of train-
ing data is very small (20 instances) we improve
slightly over the base CRF classifier, but with
an increased amount of training data, the small
improvement is replaced with a small loss.
6 Discussion
We have presented a new transductive algorithm
for structured classification, which achieves er-
ror reductions on some real-world problems. Un-
fortunately, those gains are not always realized,
and sometimes our approach leads to an increase
in error. The main reason that our approach
does not always work seems to be that our mea-
sure of similarity between different parts is very
coarse. In general, finding all the pairs of parts
have the same label is as difficult as finding the
correct labeling of all instances, but it might be
possible to use unlabeled data to learn the sim-
ilarity measure.
References
Yasemin Altun, David McAllester, and Mikhail
Belkin. 2006. Maximum margin semi-supervised
learning for structured variables. In Y. Weiss,
B. Scho?lkopf, and J. Platt, editors, Advances in
Neural Information Processing Systems 18, pages
33?40. MIT Press, Cambridge, MA.
Avrim Blum and Shuchi Chawla. 2001. Learn-
ing from labeled and unlabeled data using graph
mincuts. In Proceedings of the 18th International
Conf. on Machine Learning, pages 19?26. Morgan
Kaufmann, San Francisco, CA.
Chandra Chekuri, Sanjeev Khanna, Joseph Naor,
and Leonid Zosin. 2001. Approximation algo-
rithms for the metric labeling problem via a new
linear programming formulation. In Symposium
on Discrete Algorithms, pages 109?118.
43
E. Dahlhaus, D. S. Johnson, C. H. Papadimitriou,
P. D. Seymour, and M. Yannakakis. 1992. The
complexity of multiway cuts (extended abstract).
In Proceedings of the twenty-fourth annual ACM
symposium on Theory of computing, pages 241?
251, New York, NY, USA. ACM Press.
H. Han, C. Giles, E. Manavoglu, H. Zha, Z. Zhang,
and E. Fox. 2003. Automatic document meta-
data extraction using support vector machines. In
Joint Conference on Digital Libraries.
Jon Kleinberg and Eva Tardos. 1999. Approx-
imation algorithms for classification problems
with pairwise relationships: Metric labeling and
markov random fields. In Proceedings of the 40th
Annual Symposium on Foundations of Computer
Science, page 14, Washington, DC, USA. IEEE
Computer Society.
John D. Lafferty, Andrew McCallum, and Fernando
C. N. Pereira. 2001. Conditional random fields:
Probabilistic models for segmenting and labeling
sequence data. In Proceedings of the 10th Inter-
national Conference on Machine Learning, pages
282?289, San Francisco, CA, USA. Morgan Kauf-
mann Publishers Inc.
John Lafferty, Xiaojin Zhu, and Yan Liu. 2004.
Kernel conditional random fields: representation
and clique selection. In Proceedings of the twenty-
first international conference on Machine learn-
ing, page 64, New York, NY, USA. ACM Press.
A. McCallum, K. Nigam, J. Rennie, and K. Sey-
more. 2000. Automating the construction of in-
ternet portals with machine learning. Information
Retrieval, 3:127?163.
Fuchun Peng and Andrew McCallum. 2004.
Accurate information extraction from research
papers using conditional random fields. In
Daniel Marcu Susan Dumais and Salim Roukos,
editors, Main Proceedings of HLT-NAACL, pages
329?336, Boston, Massachusetts, USA, May 2 -
May 7. Association for Computational Linguistics.
Erik Tjong Kim Sang and Sabine Buchholz. 2000.
Introduction to the CoNLL-2000 shared task:
Chunking. In Proceedings of the Fourth Confer-
ence on Computational Natural Language Learn-
ing and of the Second Learning Language in Logic
Workshop. Association for Computational Lin-
guistics.
K. Seymore, A. McCallum, and R. Rosenfeld. 1999.
Learning hidden markov model structure for in-
formation extraction. In AAAI?99 Workshop on
Machine Learning for Information Extraction.
Vikas Sindhwani, Partha Niyogi, and Mikhail Belkin.
2005. Beyond the point cloud: from transductive
to semi-supervised learning. In Proceedings of the
22nd International Conference on Machine Learn-
ing, pages 824?831.
Alexander Smola and Risi Kondor. 2003. Kernels
and regularization on graphs. In M. Warmuth and
B. Scholkopf, editors, Proceedings of the Sixteenth
Annual Conference on Learning Theory and Ker-
nels Workshop.
Charles Sutton and Andrew McCallum. 2004. Col-
lective segmentation and labeling of distant enti-
ties in information extraction. Technical Report
TR # 04-49, University of Massachusetts, July.
Presented at ICML Workshop on Statistical Re-
lational Learning and Its Connections to Other
Fields.
Ben Taskar, Abbeel Pieter, and Daphne Koller.
2002. Discriminative probabilistic models for re-
lational data. In Proceedings of the 18th An-
nual Conference on Uncertainty in Artificial Intel-
ligence (UAI-02), pages 485?492, San Francisco,
CA. Morgan Kaufmann Publishers.
B. Taskar, V. Chatalbashev, and D. Koller. 2004a.
Learning associative markov networks. In Pro-
ceedings of the Twenty-First International Con-
ference on Machine Learning (ICML).
Ben Taskar, Carlos Guestrin, and Daphne Koller.
2004b. Max-margin markov networks. In Se-
bastian Thrun, Lawrence Saul, and Bernhard
Scho?lkopf, editors, Advances in Neural Informa-
tion Processing Systems 16. MIT Press, Cam-
bridge, MA.
Ioannis Tsochantaridis, Thorsten Joachims, Thomas
Hofmann, and Yasemin Altun. 2005. Large mar-
gin methods for structured and interdependent
output variables. JMLR, 6:1453?1484.
44
Proceedings of the Linguistic Annotation Workshop, pages 53?56,
Prague, June 2007. c?2007 Association for Computational Linguistics
Semi-Automated Named Entity Annotation
Kuzman Ganchev and Fernando Pereira
Computer and Information Science,
University of Pennsylvania,
Philadelphia PA
{ kuzman and pereira } @cis.upenn.edu
Mark Mandel
Linguistic Data Consortium,
University of Pennsylvania, Philadelphia PA
mamandel@ldc.upenn.edu
Steven Carroll and Peter White
Division of Oncology, Children?s Hospital of Philadelphia Philadelphia PA
{ carroll and white }@genome.chop.edu
Abstract
We investigate a way to partially automate
corpus annotation for named entity recogni-
tion, by requiring only binary decisions from
an annotator. Our approach is based on a lin-
ear sequence model trained using a k-best
MIRA learning algorithm. We ask an an-
notator to decide whether each mention pro-
duced by a high recall tagger is a true men-
tion or a false positive. We conclude that our
approach can reduce the effort of extending
a seed training corpus by up to 58%.
1 Introduction
Semi-automated text annotation has been the subject
of several previous studies. Typically, a human an-
notator corrects the output of an automatic system.
The idea behind our approach is to start annota-
tion manually and to partially automate the process
in the later stages. We assume that some data has
already been manually tagged and use it to train a
tagger specifically for high recall. We then run this
tagger on the rest of our corpus and ask an annotator
to filter the list of suggested gene names.
The rest of this paper is organized as follows. Sec-
tion 2 describes the model and learning algorithm.
Section 3 relates our approach to previous work.
Section 4 describes our experiments and Section 5
concludes the paper.
2 Methods
Throughout this work, we use a linear sequence
model. This class of models includes popular tag-
ging models for named entities such as conditional
random fields, maximum entropy Markov models
and max-margin Markov networks. Linear sequence
models score possible tag sequences for a given in-
put as the dot product between a learned weight vec-
tor and a feature vector derived from the input and
proposed tas sequence. Linear sequence models dif-
fer principally on how the weight vector is learned.
Our experiments use the MIRA algorithm (Cram-
mer et al, 2006; McDonald et al, 2005) to learn
the weight vector.
2.1 Notation
In what follows, x denotes the generic input sen-
tence, Y (x) the set of possible labelings of x, and
Y +(x) the set of correct labelings of x. There is
also a distinguished ?gold? labeling y(x) ? Y +(x).
For each pair of a sentence x and labeling y ?
Y (x), we compute a vector-valued feature represen-
tation f(x, y). Given a weight vector w, the score
w ? f(x, y) ranks possible labelings of x, and we de-
note by Yk,w(x) the set of k top scoring labelings for
x.
We use the standard B,I,O encoding for named
entities (Ramshaw and Marcus, 1995). Thus Y (x)
for x of length n is the set of all sequences of length
n matching the regular expression (O|(BI?))?. In a
linear sequence model, for suitable feature functions
f , Yk,w(x) can be computed efficiently with Viterbi
decoding.
2.2 k-best MIRA and Loss Functions
The learning portion of our method finds a weight
vector w that scores the correct labelings of the test
data higher than incorrect labelings. We used a k-
53
best version of the MIRA algorithm (Crammer et
al., 2006; McDonald et al, 2005). This is an online
learning algorithm that starts with a zero weight vec-
tor and for each training sentence makes the small-
est possible update that would score the correct la-
bel higher than the old top k labels. That is, for each
training sentence x we update the weight vector w
according to the rule:
wnew = argminw ?w ? wold?
s. t. w ? f(x, y(x)) ? w ? f(x, y) ? L(Y +(x), y)
?y ? Yk,wold(x)
where L(Y +(x), y) is the loss, which measures the
errors in labeling y relative to the set of correct la-
belings Y +(x).
An advantage of the MIRA algorithm (over many
other learning algorithms such as conditional ran-
dom fields) is that it allows the use of arbitrary loss
functions. For our experiments, the loss of a label-
ing is a weighted combination of the number of false
positive mentions and the number of false negative
mentions in that labeling.
2.3 Semi-Automated Tagging
For our semi-automated annotation experiments, we
imagine the following scenario: We have already an-
notated half of our training corpus and want to anno-
tate the remaining half. The goal is to save annotator
effort by using a semi-automated approach instead
of annotating the rest entirely manually.
In particular we investigate the following method:
train a high-recall named entity tagger on the anno-
tated data and use that to tag the remaining corpus.
Now ask a human annotator to filter the resulting
mentions. The mentions rejected by the annotator
are simply dropped from the annotation, leaving the
remaining mentions.
3 Relation to Previous Work
This section relates our approach to previous work
on semi-automated approaches. First we discuss
how semi-automated annotation is different from ac-
tive learning and then discuss some previous semi-
automated annotation work.
3.1 Semi-Automated versus Active Learning
It is important not to confuse semi-automated anno-
tation with active learning. While they both attempt
to alleviate the burden of creating an annotated cor-
pus, they do so in a completely orthogonal manner.
Active learning tries to select which instances should
be labeled in order to make the most impact on learn-
ing. Semi-automated annotation tries to make the
annotation of each instance faster or easier. In par-
ticular, it is possible to combine active learning and
semi-automated annotation by using an active learn-
ing method to select which sentences to label and
then using a semi-automated labeling method.
3.2 Previous work on semi-automated
annotation
The most common approach to semi-automatic an-
notation is to automatically tag an instance and then
ask an annotator to correct the results. We restrict
our discussion to this paradigm due to space con-
straints. Marcus et al (1994), Chiou et al (2001)
and Xue et al (2002) apply this approach with some
minor modifications to part of speech tagging and
phrase structure parsing. The automatic system of
Marcus et al only produces partial parses that are
then assembled by the annotators, while Chiou et al
modified their automatic parser specifically for use
in annotation. Chou et al (2006) use this tag and
correct approach to create a corpus of predicate ar-
gument structures in the biomedical domain. Culota
et al (2006) use a refinement of the tag and correct
approach to extract addressbook information from e-
mail messages. They modify the system?s best guess
as the user makes corrections, resulting in less anno-
tation actions.
4 Experiments
We now evaluate to what extent our semi-automated
annotation framework can be useful, and how much
effort it requires. For both questions we compare
semi-automatic to fully manual annotation. In our
first set of experiments, we measured the usefulness
of semi-automatically annotated corpora for training
a gene mention tagger. In the second set of exper-
iments, we measured the annotation effort for gene
mentions with the standard fully manual method and
with the semi-automated methods.
4.1 Measuring Effectiveness
The experiments in this section use the training data
from the the Biocreative II competition (Tanabe et
54
Sentence Expression of SREBP-1a stimulated StAR promoter activity in the context of COS-1 cells
gold label Expression of SREBP-1a stimulated StAR promoter activity in . . .
alternative Expression of SREBP-1a stimulated StAR promoter activity in . . .
alternative Expression of SREBP-1a stimulated StAR promoter activity in . . .
Figure 1: An example sentence and its annotation in Biocreative II. The evaluation metric would give full
credit for guessing one of the alternative labels rather than the ?gold? label.
al., 2005). The data is supplied as a set of sentences
chosen randomly fromMEDLINE and annotated for
gene mentions.
Each sentence in the corpus is provided as a list of
?gold? gene mentions as well as a set of alternatives
for each mention. The alternatives are generated by
the annotators and count as true positives. Figure 1
shows an example sentence with its gold and alter-
native mentions. The evaluation metric for these ex-
periments is F-score augmented with the possibility
of alternatives (Yeh et al, 2005).
We used 5992 sentences as the data that has al-
ready been annotated manually (set Data-1), and
simulated different ways of annotating the remain-
ing 5982 sentences (set Data-2). We compare the
quality of annotation by testing taggers trained us-
ing these corpora on a 1493 sentence test set.
We trained a high-recall tagger (recall of 89.6%)
on Data-1, and ran it on Data-2. Since we have
labels available for Data-2, we simulated an anno-
tator filtering these proposed mentions by accepting
them only if they exactly match a ?gold? or alterna-
tive mention. This gave us an F-score of 94.7% on
Data-2 and required 9981 binary decisions.
Figure 2 shows F1 score as a function of the num-
ber of extra sentences annotated. Without any ad-
ditional data, the F-measure of the tagger is 81.0%.
The two curves correspond to annotation with and
without alternatives. The horizontal line at 82.8%
shows the level achieved by the semi-automatic
method (when using all of Data-2).
From the figure, we can see that to get compa-
rable performance to the semi-automatic approach,
we need to fully manually annotate roughly a third
as much data with alternatives, or about two thirds as
much data without alternatives. The following sec-
tion examines what this means in terms of annotator
time by providing timing results for semi-automatic
and fully-manual annotation without alternatives.
 81 81.5 82 82.5 83 83.5 84 84.5 85
 0
 1000
 2000
 3000
 4000
 5000
 6000
Extra
 Anno
tated 
Sente
nces (
from 
Data-
2)
Manu
al Wi
th Alt
ernati
ves
Manu
al w/o
 Alter
native
s
Semi-
Autom
atic (o
n all o
f Data
-2)
Figure 2: Effect of the number of annotated in-
stances on F1 score. In all cases the original 5992
instances were used; the curves show manual an-
notation while the level line is the semi-automatic
method. The curves are averages over 3 trials.
4.2 Measuring Effort
The second set of experiments compares annotator
effort between fully manual and semi-automatic an-
notation. Because we did not have access to an expe-
rienced annotator from the Biocreative project, and
gene mention annotations vary subtly among anno-
tation efforts, we evaluated annotator effort on on the
PennBioIE named entity corpus.1 Furthermore, we
have not yet annotated enough data locally to per-
form both effectiveness and effort experiments on
the local corpus alone. However, both corpora an-
notate gene mentions in MEDLINE abstracts, so we
expect that the timing results will not be significantly
different.
We asked an experienced annotator to tag 194
MEDLINE abstracts: 96 manually and 98 using the
semi-automated method. Manual annotation was
done using annotation software familiar to the an-
notator. Semi-automatic annotation was done with a
1Available from http://bioie.ldc.upenn.edu/
55
Web-based tool developed for the task. The new tool
highlights potential gene mentions in the text and al-
lows the annotator to filter them with a mouse click.
The annotator had been involved in the creation of
the local manually annotated corpus, and had a lot of
experience annotating named entities. The abstracts
for annotation were selected randomly so that they
did not contain any abstracts tagged earlier. There-
fore, we did not expect the annotator to have seen
any of them before the experiment.
To generate potential gene mentions for the semi-
automated annotation, we ran two taggers on the
data: a high recall tagger trained on the local corpus
and a high recall tagger trained on the Biocreative
corpus. At decode time, we took the gene mentions
from the top two predictions of each of these taggers
whenever there were any gene mentions predicted.
As a result, the annotator had to make more binary
decisions per sentence than they would have for ei-
ther training corpus alone. For the semi-automated
annotation, the annotator had to examine 682 sen-
tences and took on average 10 seconds per sentence.
For the fully-manual annotation, they examined 667
sentences and took 40 seconds per sentence on av-
erage. We did not ask the annotator to tag alterna-
tives because they did not have any experience with
tagging alternatives and we do not have a tool that
makes the annotation of alternatives easy. Conse-
quently, effort totals for annotation with alternatives
would have been skewed in our favor. The four-fold
speedup should be compared to the lower curve in
Figure 2.
5 Discussion and Further Work
We can use the effort results to estimate the relative
effort of annotating without alternatives and of semi-
automated annotation. To obtain the same improve-
ment in F-score, we need to semi-automatically an-
notate roughly a factor of 1.67 more data than using
the fully manual approach. Multiplying that by the
0.25 factor reduction in annotation time, we get that
the time required for a comparable improvement in
F-score is 0.42 times as long ? a 58% reduction in
annotator time.
We do not have any experiments on annotating
alternatives, but the main difference between semi-
automated and fully-manual annotation is that the
former does not require the annotator to decide on
boundaries. Consequently, we expect that annota-
tion with alternatives will be considerably more ex-
pensive than without alternatives, since more bound-
aries have to be outlined.
In future work, it would be interesting to compare
this approach to the traditional approach of manually
correcting output of a system. Due to constraints
on annotator time, it was not possible to do these
experiments as part of the current work.
References
Fu-Dong Chiou, David Chiang, and Martha Palmer.
2001. Facilitating treebank annotation using a statisti-
cal parser. In HLT ?01. ACL.
Wen-Chi Chou, Richard Tzong-Han Tsai, Ying-Shan Su,
Wei Ku, Ting-Yi Sung, and Wen-Lian Hsu. 2006.
A semi-automatic method for annotating a biomedical
proposition bank. In FLAC?06. ACL.
Koby Crammer, Ofer Dekel, Joseph Keshet, Shai Shalev-
Shwartz, and Yoram Singer. 2006. Online passive-
aggressive algorithms. JMLR, 7.
Aron Culota, Trausti Kristjansson, Andrew McCallum,
and Paul Viola. 2006. Corrective feedback and per-
sistent learning for information extraction. Artificial
Intelligence, 170:1101?1122.
Mitchell P. Marcus, Beatrice Santorini, and Mary Ann
Marcinkiewicz. 1994. Building a large annotated cor-
pus of english: The penn treebank. Computational
Linguistics, 19(2):313?330.
Ryan McDonald, Koby Crammer, and Fernando Pereira.
2005. Online large-margin training of dependency
parsers. In ACL?05. ACL.
Lance Ramshaw and Mitch Marcus. 1995. Text chunk-
ing using transformation-based learning. In David
Yarovsky and Kenneth Church, editors, Proceedings
of the Third Workshop on Very Large Corpora. ACL.
Lorraine Tanabe, Natalie Xie, Lynne H. Thom, Wayne
Matten, and W. John Wilbur. 2005. GENETAG: a
tagged corpus for gene/protein named entity recogni-
tion. BMC Bioinformatics, 6(Suppl. 1).
Nianwen Xue, Fu-Dong Chiou, and Martha Palmer.
2002. Building a large-scale annotated chinese corpus.
In Proceedings of the 19th international conference on
Computational linguistics. ACL.
Alexander Yeh, Alexander Morgan, Marc Colosimo, and
Lynette Hirschman. 2005. BioCreAtIvE Task 1A:
gene mention finding evaluation . BMC Bioinformat-
ics, 6(Suppl. 1).
56
Proceedings of the 22nd International Conference on Computational Linguistics (Coling 2008), pages 473?480
Manchester, August 2008
Reading the Markets:
Forecasting Public Opinion of Political Candidates by News Analysis
Kevin Lerman
Dept. of Computer Science
Columbia University
New York, NY USA
klerman@cs.columbia.edu
Ari Gilder and Mark Dredze
Dept. of CIS
University of Pennsylvania
Philadelphia, PA USA
agilder@alumni.upenn.edu
mdredze@cis.upenn.edu
Fernando Pereira
Google, Inc.
1600 Amphitheatre Parkway
Mountain View, CA USA
pereira@google.com
Abstract
Media reporting shapes public opinion
which can in turn influence events, partic-
ularly in political elections, in which can-
didates both respond to and shape public
perception of their campaigns. We use
computational linguistics to automatically
predict the impact of news on public per-
ception of political candidates. Our sys-
tem uses daily newspaper articles to pre-
dict shifts in public opinion as reflected
in prediction markets. We discuss various
types of features designed for this problem.
The news system improves market predic-
tion over baseline market systems.
1 Introduction
The mass media can affect world events by sway-
ing public opinion, officials and decision makers.
Financial investors who evaluate the economic per-
formance of a company can be swayed by positive
and negative perceptions about the company in the
media, directly impacting its economic position.
The same is true of politics, where a candidate?s
performance is impacted by media influenced pub-
lic perception. Computational linguistics can dis-
cover such signals in the news. For example, De-
vitt and Ahmad (2007) gave a computable metric
of polarity in financial news text consistent with
human judgments. Koppel and Shtrimberg (2004)
used a daily news analysis to predict financial mar-
ket performance, though predictions could not be
used for future investment decisions. Recently,
c
? 2008. Licensed under the Creative Commons
Attribution-Noncommercial-Share Alike 3.0 Unported li-
cense (http://creativecommons.org/licenses/by-nc-sa/3.0/).
Some rights reserved.
a study conducted of the 2007 French presiden-
tial election showed a correlation between the fre-
quency of a candidate?s name in the news and elec-
toral success (V?eronis, 2007).
This work forecasts day-to-day changes in pub-
lic perception of political candidates from daily
news. Measuring daily public perception with
polls is problematic since they are conducted by a
variety of organizations at different intervals and
are not easily comparable. Instead, we rely on
daily measurements from prediction markets.
We present a computational system that uses
both external linguistic information and internal
market indicators to forecast public opinion mea-
sured by prediction markets. We use features from
syntactic dependency parses of the news and a
user-defined set of market entities. Successive
news days are compared to determine the novel
component of each day?s news resulting in features
for a machine learning system. A combination sys-
tem uses this information as well as predictions
from internal market forces to model prediction
markets better than several baselines. Results show
that news articles can be mined to predict changes
in public opinion.
Opinion forecasting differs from that of opin-
ion analysis, such as extracting opinions, evaluat-
ing sentiment, and extracting predictions (Kim and
Hovy, 2007). Contrary to these tasks, our system
receives objective news, not subjective opinions,
and learns what events will impact public opinion.
For example, ?oil prices rose? is a fact but will
likely shape opinions. This work analyzes news
(cause) to predict future opinions (effect). This af-
fects the structure of our task: we consider a time-
series setting since we must use past data to predict
future opinions, rather than analyzing opinions in
batch across the whole dataset.
473
We begin with an introduction to prediction
markets. Several methods for new feature extrac-
tion are explored as well as market history base-
lines. Systems are evaluated on prediction markets
from the 2004 US Presidential election. We close
with a discussion of related and future work.
2 Prediction Markets
Prediction markets, such as TradeSports and the
Iowa Electronic Markets
1
, provide a setting sim-
ilar to financial markets wherein shares represent
not companies or commodities, but an outcome
of a sporting, financial or political event. For ex-
ample, during the 2004 US Presidential election,
one could purchase a share of ?George W. Bush
to win the 2004 US Presidential election? or ?John
Kerry to win the 2004 US Presidential election.?
A pay-out of $1 is awarded to winning sharehold-
ers at market?s end, e.g. Bush wins the election.
In the interim, price fluctuations driven by supply
and demand indicate the perception of the event?s
likelihood, which indicates public opinion of an
event. Several studies show the accuracy of predic-
tion markets in predicting future events (Wolfers
and Zitzewitz, 2004; Servan-Schreiber et al, 2004;
Pennock et al, 2000), such as the success of up-
coming movies (Jank and Foutz, 2007), political
stock markets (Forsythe et al, 1999) and sports
betting markets (Williams, 1999).
Market investors rely on daily news reports to
dictate investment actions. If something positive
happens for Bush (e.g. Saddam Hussein is cap-
tured), Bush will appear more likely to win, so
demand increases for ?Bush to win? shares, and
the price rises. Likewise, if something negative for
Bush occurs (e.g. casualties in Iraq increase), peo-
ple will think he is less likely to win, sell their
shares, and the price drops. Therefore, predic-
tion markets can be seen as rapid response indi-
cators of public mood concerning political candi-
dates. Market-internal factors, such as general in-
vestor mood and market history, also affect price.
For instance, a positive news story for a candidate
may have less impact if investors dislike the can-
didate. Explaining market behavior requires mod-
eling news information external to the market and
internal trends to the market.
This work uses the 2004 US Presidential elec-
tion markets from Iowa Electronic Markets. Each
market provides a daily average price, which indi-
1
www.tradesports.com, www.biz.uiowa.edu/iem/
cates the overall market sentiment for a candidate
on a given day. The goal of the prediction system
is to predict the price direction for the next day (up
or down) given all available information up to the
current day: previous days? market pricing/volume
information and the morning news. Market his-
tory represents information internal to the market:
if an investor has no knowledge of external events,
what is the most likely direction for the market?
This information can capture general trends and
volatility of the market. The daily news is the ex-
ternal information that influences the market. This
provides information, independent of any internal
market effects to which investors will respond. A
learning system for each information source is de-
veloped and combined to explain market behavior.
The following sections describe these systems.
3 External Information: News
Changes in market price are likely responses to
current events reported in the news. Investors read
the morning paper and act based on perceptions of
events. Can a system with access to this same in-
formation make good investment decisions?
Our system operates in an iterative (online) fash-
ion. On each day (round) the news for that day is
used to construct a new instance. A logistic re-
gression classifier is trained on all previous days
and the resulting classifier predicts the price move-
ment of the new instance. The system either prof-
its or loses money according to this prediction. It
then receives the actual price movement and labels
the instance accordingly (up or down). This set-
ting is straightforward; the difficulty is in choosing
a good feature representation for the classifier. We
now explore several representation techniques.
3.1 Bag-of-Words Features
The prediction task can be treated as a document
classification problem, where the document is the
day?s news and the label is the direction of the mar-
ket. Document classification systems typically rely
on bag-of-words features, where each feature indi-
cates the number of occurrences of a word in the
document. The news for a given day is represented
by a normalized unit length vector of counts, ex-
cluding common stop words and features that oc-
cur fewer than 20 times in our corpus.
474
3.2 News Focus Features
Simple bag-of-words features may not capture rel-
evant news information. Public opinion is influ-
enced by new events ? a change in focus. The day
after a debate, most papers may declare Bush the
winner, yielding a rise in the price of a ?Bush to
win? share. However, while the debate may be
discussed for several days after the event, public
opinion of Bush will probably not continue to rise
on old news. Changes in public opinion should
reflect changes in daily news coverage. Instead of
constructing features for a single day, they can rep-
resent differences between two days of news cov-
erage, i.e. the novelty of the coverage. Given the
counts of feature i on day t as c
t
i
, where feature i
may be the unigram ?scandal,? and the set of fea-
tures on day t as C
t
, the fraction of news focus for
each feature is f
t
i
=
c
t
i
|C
t
|
. The news focus change
(?) for feature i on day t is defined as,
?f
t
i
= log
(
f
t
i
1
3
(f
t?1
i
+ f
t?2
i
+ f
t?3
i
)
)
, (1)
where the numerator is the focus of news on fea-
ture i today and the denominator is the average
focus over the previous three days. The resulting
value captures the change in focus on day t, where
a value greater than 0 means increased focus and a
value less than 0 decreased focus. Feature counts
were smoothed by adding a constant (10).
3.3 Entity Features
As shown by Wiebe et al (2005), it is important to
know not only what is being said but about whom it
is said. The term ?victorious? by itself is meaning-
less when discussing an election ? meaning comes
from the subject. Similarly, the word ?scandal?
is bad for a candidate but good for the opponent.
Subjects can often be determined by proximity. If
the word ?scandal? and Bush are mentioned in the
same sentence, this is likely to be bad for Bush. A
small set of entities relevant to a market can be de-
fined a priori to give context to features. For exam-
ple, the entities ?Bush,? ?Kerry? and ?Iraq? were
known to be relevant before the general election.
Kim and Hovy (2007) make a similar assumption.
News is filtered for sentences that mention ex-
actly one of these entities. Such sentences are
likely about that entity, and the extracted features
are conjunctions of the word and the entity. For ex-
ample, the sentence ?Bush is facing another scan-
dal? produces the feature ?bush-scandal? instead
of just ?scandal.?
2
Context disambiguation comes
at a high cost: about 70% of all sentences do not
contain any predefined entities and about 7% con-
tain more than one entity. These likely relevant
sentences are unfortunately discarded, although
future work could reduce the number of discarded
sentences using coreference resolution.
3.4 Dependency Features
While entity features are helpful they cannot pro-
cess multiple entity sentences, nearly a quarter of
the entity sentences. These sentences may be the
most helpful since they indicate entity interactions.
Consider the following three example sentences:
? Bush defeated Kerry in the debate.
? Kerry defeated Bush in the debate.
? Kerry, a senator from Massachusetts, de-
feated President Bush in last night?s debate.
Obviously, the first two sentences have very dif-
ferent meanings for each candidate?s campaign.
However, representations considered so far do not
differentiate between these sentences, nor would
any heuristic using proximity to an entity.
3
Effec-
tive features rely on the proper identification of the
subject and object of ?defeated.? Longer n-grams,
which would be very sparse, would succeed for the
first two sentences but not the third.
To capture these interactions, features were ex-
tracted from dependency parses of the news ar-
ticles. Sentences were part of speech tagged
(Toutanova et al, 2003), parsed with a depen-
dency parser and labeled with grammatical func-
tion labels (McDonald et al, 2006). The result-
ing parses encode dependencies for each sentence,
where word relationships are expressed as parent-
child links. The parse for the third sentence above
indicates that ?Kerry? is the subject of ?defeated,?
and ?Bush? is the object. Features are extracted
from parse trees containing the pre-defined enti-
ties (section 3.3), using the parent, grandparent,
aunts, nieces, children, and siblings of any in-
stances of the pre-defined entities we observe. Fea-
tures are conjoined indicators of the node?s lexical
entry, part of speech tag and dependency relation
2
Other methods can identify the subject of sentiment ex-
pressions, but our text is objective news. Therefore, we em-
ploy this approximate method.
3
Several failed heuristics were tried, such as associating
each word to an entity within a fixed window in the sentence
or the closer entity if two were in the window.
475
Feature Good For
Kerry? plan? the Kerry
poll? showed? Bush Bush
won? Kerry
4
Kerry
agenda? ?s? Bush Kerry
Kerry? spokesperson? campaign Bush
Table 1: Simplified examples of features from the
general election market. Arrows point from parent
to child. Features also include the word?s depen-
dency relation labels and parts of speech.
label. For aunts, nieces, and children, the com-
mon ancestor is used, and in the case of grand-
parent, the intervening parent is included. Each
of these conjunctions includes the discovered en-
tity and back-off features are included by remov-
ing some of the other information. Note that be-
sides extracting more precise information from the
news text, this handles sentences with multiple en-
tities, since it associates parts of a sentence with
different entities. In practice, we use this in con-
junction with News Focus. Useful features from
the general election market are in table 1. Note
that they capture events and not opinions. For ex-
ample, the last feature indicates that a statement by
the Kerry campaign was good for Bush, possibly
because Kerry was reacting to criticism.
4 Internal Information: Market History
News cannot explain all market trends. Momen-
tum in the market, market inefficiencies, and slow
news days can affect share price. A candidate who
does well will likely continue to do well unless
new events occur. Learning general market behav-
ior can help explain these price movements.
For each day t, we create an instance using fea-
tures for the price and volume at day t ? 1 and
the price and volume change between days t ? 1
and t ? 2. We train using a ridge regression
5
on
all previous days (labeled with their actual price
movements) to forecast the movement for day t,
which we convert into a binary value: up or down.
4
This feature matches phrases like ?Kerry won [the de-
bate]? and ?[something] won Kerry [support]?
5
This outperformed more sophisticated algorithms, in-
cluding the logistic regression used earlier. This may be due
to the fact that many market history features (e.g. previous
price movements) are very similar in nature to the future price
movements being predicted.
5 Combined System
Since both news and internal market information
are important for modeling market behavior, each
one cannot be evaluated in isolation. For example,
a successful news system may learn to spot impor-
tant events for a candidate, but cannot explain the
price movements of a slow news day. A combina-
tion of the market history system and news features
is needed to model the markets.
Expert algorithms for combining prediction sys-
tems have been well studied. However, experi-
ments with the popular weighted majority algo-
rithm (Littlestone and Warmuth, 1989) yielded
poor performance since it attempts to learn the
optimal balance between systems while our set-
ting has rapidly shifting quality between few ex-
perts with little data for learning. Instead, a sim-
ple heuristic was used to select the best perform-
ing predictor on each day. We compare the 3-
day prediction accuracy (measured in total earn-
ings) for each system (news and market history)
to determine the current best system. The use of
a small window allows rapid change in systems.
When neither system has a better 3-day accuracy
the combined system will only predict if the two
systems agree and abstain otherwise. This strategy
measures how accurately a news system can ac-
count for price movements when non-news move-
ments are accounted for by market history. The
combined system improved over individual evalu-
ations of each system on every market
6
.
6 Evaluation
Daily pricing information was obtained from the
Iowa Electronic Markets for the 2004 US Presi-
dential election for six Democratic primary con-
tenders (Clark, Clinton, Dean, Gephardt, Kerry
and Lieberman) and two general election candi-
dates (Bush and Kerry). Market length varied as
some candidates entered the race later than others:
the DNC markets for Clinton, Gephardt, Kerry,
and Lieberman were each 332 days long, while
Dean?s was 130 days and Clark?s 106. The general
election market for Bush was 153 days long, while
Kerry?s was 142.
7
The price delta for each day
was taken as the difference between the average
6
This outperformed a single model built over all features,
perhaps due to the differing natures of the feature types we
used.
7
The first 11 days of the Kerry general election market
were removed due to strange price fluctuations in the data.
476
price between the previous and current day. Mar-
ket data also included the daily volume that was
used as a market history feature. Entities selected
for each market were the names of all candidates
involved in the election and ?Iraq.?
News articles covering the election were ob-
tained from Factiva
8
, an online news archive run
by Dow Jones. Since the system must make a pre-
diction at the beginning of each day, only articles
from daily newspapers released early in the morn-
ing were included. The corpus contained approxi-
mately 50 articles per day over a span of 3 months
to almost a year, depending on the market.
9
While most classification systems are evaluated
by measuring their accuracy on cross-validation
experiments, both the method and the metric are
unsuitable to our task. A decision for a given day
must be made with knowledge of only the previ-
ous days, ruling out cross validation. In fact, we
observed improved results when the system was
allowed access to future articles through cross-
validation. Further, raw prediction accuracy is not
a suitable metric for evaluation because it ignores
the magnitude in price shifts each day. A sys-
tem should be rewarded proportional to the signif-
icance of the day?s market change.
To address these issues we used a chronological
evaluation where systems were rewarded for cor-
rect predictions in proportion to the magnitude of
that day?s shift, i.e. the ability to profit from the
market. This metric is analogous to weighted accu-
racy. On each day, the system is provided with all
available morning news and market history from
which an instance is created using one of the fea-
ture schemes described above. We then predict
whether the market price will rise or fall and the
system either earns or loses the price change for
that day if it was right or wrong respectively. The
system then learns the correct price movement and
the process is repeated for the next day.
10
Sys-
tems that correctly forecast public opinions from
the news will make more money. In economic
terms, this is equivalent to buying or short-selling a
single share of the market and then selling or cov-
ering the short at the end of the day.
11
Scores were
8
http://www.factiva.com/
9
While 50 articles may not seem like much, humans read
far less text before making investment decisions.
10
This scheme is called ?online learning? for which a
whole class of algorithms apply. We used batch algorithms
since training happens only once per day.
11
More complex investment schemes are possible than
what has been described here. We choose a simple scheme
Market History Baseline
DNC Clark 20 13
Clinton 38 -8
Dean 23 24
Gephardt 8 1
Kerry -6 6
Lieberman 3 2
General Kerry 2 15
Bush 21 20
Average (% omniscience) 13.6 9.1
Table 2: Results using history features for predic-
tion compared with a baseline system that invests
according to the previous day?s result.
normalized for comparison across markets using
the maximum profit obtainable by an omniscient
system that always predicts correctly.
Baseline systems for both news and market his-
tory are included. The news baseline follows the
spirit of a study of the French presidential elec-
tion (V?eronis, 2007), which showed that candidate
mentions correlate to electoral success. Attempts
to follow this method directly ? predicting mar-
ket movement based on raw candidate mentions ?
did very poorly. Instead, we trained our learning
system with features representing daily mention
counts of each entity. For a market history base-
line, we make a simple assumption about market
behavior: the current market trend will continue,
predict today?s behavior for tomorrow.
There were too many features to learn in the
short duration of the markets so only features that
appeared at least 20 times were included, reduc-
ing bag-of-words features from 88.8k to 28.3k and
parsing features from 1150k to 15.9k. A real world
system could use online feature selection.
6.1 Results
First, we establish performance without news in-
formation by testing the market history system
alone. Table 2 shows the profit of the history pre-
diction and baseline systems. While learning beats
the rule based system on average, both earn im-
pressive profits considering that random trading
would break even. These results corroborate the
inefficient market observation of Pennock et al
(2000). Additionally, the general election markets
sometimes both increased or decreased, an impos-
sible result in an efficient zero-sum market.
to make the evaluation more transparent.
477
Figure 1: Results for the different news features and combined system across five markets. Bottom
bars can be compared to evaluate news components and combined with the stacked black bars (history
system) give combined performance. The average performance (far right) shows improved performance
from each news system over the market history system.
During initial news evaluations with the com-
bined system, the primary election markets did ei-
ther very poorly or quite well. The news predic-
tion component lost money for Clinton, Gephardt,
and Lieberman while Clark, Dean and Kerry all
made money. Readers familiar with the 2004 elec-
tion will immediately see the difference between
the groups. The first three candidates were minor
contenders for the nomination and were not news-
makers. Hillary Clinton never even declared her
candidacy. The average number of mentions per
day for these candidates in our data was 20. In con-
trast, the second group were all major contenders
for the nomination and an average mention of 94 in
our data. Clearly, the news system can only do well
when it observes news that effects the market. The
system does well on both general election markets
where the average candidate mention per day was
503. Since the Clinton, Gephardt and Lieberman
campaigns were not newsworthy, they are omitted
from the results.
Results for news based prediction systems are
shown in figure 1. The figure shows the profit
made from both news features (bottom bars) and
market history (top black bars) when evaluated as
a combined system. Bottom bars can be compared
to evaluate news systems and each is combined
with its top bar to indicate total performance. Neg-
ative bars indicate negative earnings (i.e. weighted
accuracy below 50%). Averages across all mar-
kets for the news systems and the market history
system are shown on the right. In each market,
the baseline news system makes a small profit, but
the overall performance of the combined system is
worse than the market history system alone, show-
ing that the news baseline is ineffective. However,
all news features improve over the market history
system; news information helps to explain market
behaviors. Additionally, each more advanced set
of news features improves, with dependency fea-
tures yielding the best system in a majority of mar-
kets. The dependency system was able to learn
more complex interactions between words in news
articles. As an example, the system learns that
when Kerry is the subject of ?accused? his price in-
creases but decreased when he is the object. Sim-
ilarly, when ?Bush? is the subject of ?plans? (i.e.
Bush is making plans), his price increased. But
when he appears as a modifier of the plural noun
?plans? (comments about Bush policies), his price
falls. Earning profit indicates that our systems
were able to correctly forecast changes in public
opinion from objective news text.
The combined system proved an effective way
of modeling the market with both information
sources. Figure 2 shows the profits of the depen-
dency news system, the market history system, and
the combined system?s profits and decision on two
segments from the Kerry DNC market. In the first
segment, the history system predicts a downward
trend in the market (increasing profit) and the sec-
ond segment shows the final days of the market,
where Kerry was winning primaries and the news
system correctly predicted a market increase.
V?eronis (2007) observed a connection between
electoral success and candidate mentions in news
media. The average daily mentions in the general
election was 520 for Bush (election winner) and
478
485 for Kerry. However, for the three major DNC
candidates, Dean had 183, Clark 56 and Kerry
(election winner) had the least at 43. Most Kerry
articles occurred towards the end of the race when
it was clear he would win, while early articles fo-
cused on the early leader Dean. Also, news activity
did not indicate market movement direction; me-
dian candidate mentions for a positive market day
was 210 and 192 for a negative day.
Dependency news system accuracy was corre-
lated with news activity. On days when the news
component was correct ? although not always cho-
sen ? there were 226 median candidate mentions
compared to 156 for incorrect days. Additionally,
the system was more successful at predicting neg-
ative days. While days for which it was incorrect
the market moved up or down equally, when it was
correct and selected it predicted buy 42% of the
time and sell 58%, indicating that the system bet-
ter tracked negative news impacts.
7 Related Work
Many studies have examined the effects of news on
financial markets. Koppel and Shtrimberg (2004)
found a low correlation between news and the
stock market, likely because of the extreme effi-
ciency of the stock market (Gid?ofalvi, 2001). Two
studies reported success but worked with a very
small time granularity (10 minutes) (Lavrenko et
al., 2000; Mittermayer and Knolmayer, 2006). It
appears that neither system accounts for the time-
series nature of news during learning, instead us-
ing cross-validation experiments which is unsuit-
able for evaluation of time-series data. Our own
preliminary cross-validation experiments yielded
much better results than chronological evaluation
since the system trains using future information,
and with much more training data than is actu-
ally available for most days. Recent work has ex-
amined prediction market behavior and underlying
principles (Serrano-Padial, 2007).
12
Pennock et
al. (2000) found that prediction markets are some-
what efficient and some have theorized that news
could predict these markets, which we have con-
firmed (Debnath et al, 2003; Pennock et al, 2001;
Servan-Schreiber et al, 2004).
Others have explored the concurrent modeling
of text corpora and time series, such as using stock
market data and language modeling to identify
12
For a sample of the literature on prediction markets, see
the proceedings of the recent Prediction Market workshops
(http://betforgood.com/events/pm2007/index.html).
Figure 2: Two selections from the Kerry DNC mar-
ket showing profits over time (days) for depen-
dency news, history and combined systems. Each
day?s chosen system is indicated by the bottom
stripe as red (upper) for news, blue (lower) for his-
tory, and black for ties.
influential news stories (Lavrenko et al, 2000).
Hurst and Nigam (2004) combined syntactic and
semantic information for text polarity extraction.
Our task is related to but distinct from sentiment
analysis, which focuses on judgments in opin-
ions and, recently, predictions given by opinions.
Specifically, Kim and Hovy (2007) identify which
political candidate is predicted to win by an opin-
ion posted on a message board and aggregate opin-
ions to correctly predict an election result. While
the domain and some techniques are similar to our
own, we deal with fundamentally different prob-
lems. We do not consider opinions but instead ana-
lyze objective news to learn events that will impact
opinions. Opinions express subjective statements
about elections whereas news reports events. We
use public opinion as a measure of an events im-
pact. Additionally, they use generalized features
similar to our own identification of entities by re-
placing (a larger set of) known entities with gen-
eralized terms. In contrast, we use syntactic struc-
tures to create generalized ngram features. Note
that our features (table 1) do not indicate opinions
in contrast to the Kim and Hovy features. Finally,
Kim and Hovy had a batch setting to predict elec-
tion winners while we have a time-series setting
that tracked daily public opinion of candidates.
8 Conclusion and Future Work
We have presented a system for forecasting public
opinion about political candidates using news me-
479
dia. Our results indicate that computational sys-
tems can process media reports and learn which
events impact political candidates. Additionally,
the system does better when the candidate appears
more frequently and for negative events. A news
source analysis could reveal which outlets most in-
fluence public opinion. A feature analysis could
reveal which events trigger public reactions. While
these results and analyses have significance for po-
litical analysis they could extend to other genres,
such as financial markets. We have shown that fea-
ture extraction using syntactic parses can general-
ize typical bag-of-word features and improve per-
formance, a non-trivial result as dependency parses
contain significant errors and can limit the selec-
tion of words. Also, combining the internal mar-
ket baseline with a news system improved perfor-
mance, suggesting that forecasting future public
opinions requires a combination of new informa-
tion and continuing trends, neither of which can be
captured by the other.
References
Debnath, S., D. M. Pennock, C. L. Giles, and
S. Lawrence. 2003. Information incorporation in
online in-game sports betting markets. In Electronic
Commerce.
Devitt, Ann and Khurshid Ahmad. 2007. Sentiment
polarity identification in financial news: A cohesion-
based approach. In Association for Computational
Linguistics (ACL).
Forsythe, R., T.A. Rietz, , and T.W. Ross. 1999.
Wishes, expectations, and actions: A survey on price
formation in election stock markets. Journal of Eco-
nomic Behavior and Organization, 39:83?110.
Gid?ofalvi, G. 2001. Using news articles to predict
stock price movements. Technical report, Univ. of
California San Diego, San Diego.
Hurst, Matthew and Kamal Nigam. 2004. Retrieving
topical sentiments from online document collections.
In Document Recognition and Retrieval XI.
Jank, Wolfgang and Natasha Foutz. 2007. Using vir-
tual stock exchanges to forecast box-office revenue
via functional shape analysis. In The Prediction
Markets Workshop at Electronic Commerce.
Kim, Soo-Min and Eduard Hovy. 2007. Crystal: Ana-
lyzing predictive opinions on the web. In Empirical
Methods in Natural Language Processing (EMNLP).
Koppel, M. and I. Shtrimberg. 2004. Good news or
bad news? let the market decide. In AAAI Spring
Symposium on Exploring Attitude and Affect in Text:
Theories and Applications.
Lavrenko, V., M. Schmill, D. Lawrie, P. Ogilvie,
D. Jensen, and J. Allan. 2000. Mining of concur-
rent text and time series. In KDD.
Littlestone, Nick and Manfred K. Warmuth. 1989. The
weighted majority algorithm. In IEEE Symposium
on Foundations of Computer Science.
McDonald, R., K. Lerman, and F. Pereira. 2006. Mul-
tilingual dependency parsing with a two-stage dis-
criminative parser. In Conference on Natural Lan-
guage Learning (CoNLL).
Mittermayer, M. and G. Knolmayer. 2006. News-
CATS: A news categorization and trading system. In
International Conference in Data Mining.
Pennock, D. M., S. Lawrence, C. L. Giles, and F. A.
Nielsen. 2000. The power of play: Efficiency and
forecast accuracy in web market games. Technical
Report 2000-168, NEC Research Institute.
Pennock, D. M., S. Lawrence, F. A. Nielsen, and C. L.
Giles. 2001. Extracting collective probabilistic fore-
casts from web games. In KDD.
Serrano-Padial, Ricardo. 2007. Strategic foundations
of prediction markets and the efficient markets hy-
pothesis. In The Prediction Markets Workshop at
Electronic Commerce.
Servan-Schreiber, E., J. Wolfers, D. M. Pennock, and
B. Galebach. 2004. Prediction markets: Does
money matter? Electronic Markets, 14.
Toutanova, K., D. Klein, C. Manning, and Y. Singer.
2003. Feature-rich part-of-speech tagging with a
cyclic dependency network. In HLT-NAACL.
V?eronis, Jean. 2007. La presse a fait mieux que les
sondeurs. http://aixtal.blogspot.com/2007/04/2007-
la-presse-fait-mieux-que-les.html.
Wiebe, Janyce, Theresa Wilson, and Claire Cardie.
2005. Annotating expressions of opinions and emo-
tions in language. LREC, 39:165?210.
Williams, L.V. 1999. Information efficiency in betting
markets: A survey. Bulletin of Economic Research,
51:1?30.
Wolfers, J. and E. Zitzewitz. 2004. Prediction markets.
Journal of Economic Perspectives, 18(2):107?126.
480
Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, pages 582?590,
Honolulu, October 2008. c?2008 Association for Computational Linguistics
Weakly-Supervised Acquisition of Labeled Class Instances using Graph
Random Walks
Partha Pratim Talukdar?
University of Pennsylvania
Philadelphia, PA 19104
partha@cis.upenn.edu
Joseph Reisinger?
University of Texas at Austin
Austin, TX 78712
joeraii@cs.utexas.edu
Marius Pas?ca
Google Inc.
Mountain View, CA 94043
mars@google.com
Deepak Ravichandran
Google Inc.
Mountain View, CA 94043
deepakr@google.com
Rahul Bhagat?
USC Information Sciences Institute
Marina Del Rey, CA 90292
rahul@isi.edu
Fernando Pereira
Google Inc.
Mountain View, CA 94043
pereira@google.com
Abstract
We present a graph-based semi-supervised la-
bel propagation algorithm for acquiring open-
domain labeled classes and their instances
from a combination of unstructured and struc-
tured text sources. This acquisition method
significantly improves coverage compared to
a previous set of labeled classes and instances
derived from free text, while achieving com-
parable precision.
1 Introduction
1.1 Motivation
Users of large document collections can readily ac-
quire information about the instances, classes, and
relationships described in the documents. Such rela-
tions play an important role in both natural language
understanding andWeb search, as illustrated by their
prominence in both Web documents and among the
search queries submitted most frequently by Web
users (Jansen et al, 2000). These observations moti-
vate our work on algorithms to extract instance-class
information from Web documents.
While work on named-entity recognition tradi-
tionally focuses on the acquisition and identifica-
tion of instances within a small set of coarse-grained
classes, the distribution of instances within query
logs indicates that Web search users are interested
in a wider range of more fine-grained classes. De-
pending on prior knowledge, personal interests and
immediate needs, users submit for example medi-
cal queries about the symptoms of leptospirosis or
?Contributions made during internships at Google.
the treatment of monkeypox, both of which are in-
stances of zoonotic diseases, or the risks and benefits
of surgical procedures such as PRK and angioplasty.
Other users may be more interested in African coun-
tries such as Uganda and Angola, or active volca-
noes like Etna and Kilauea. Note that zoonotic dis-
eases, surgical procedures, African countries and
active volcanoes serve as useful class labels that cap-
ture the semantics of the associated sets of class in-
stances. Such interest in a wide variety of specific
domains highlights the utility of constructing large
collections of fine-grained classes.
Comprehensive and accurate class-instance in-
formation is useful not only in search but also
in a variety of other text processing tasks includ-
ing co-reference resolution (McCarthy and Lehn-
ert, 1995), named entity recognition (Stevenson and
Gaizauskas, 2000) and seed-based information ex-
traction (Riloff and Jones, 1999).
1.2 Contributions
We study the acquisition of open-domain, labeled
classes and their instances from both structured
and unstructured textual data sources by combin-
ing and ranking individual extractions in a princi-
pled way with the Adsorption label-propagation al-
gorithm (Baluja et al, 2008), reviewed in Section 3
below.
A collection of labeled classes acquired from
text (Van Durme and Pas?ca, 2008) is extended in two
ways:
1. Class label coverage is increased by identify-
ing additional class labels (such as public agen-
cies and governmental agencies) for existing
582
instances such as Office of War Information),
2. The overall instance coverage is increased by
extracting additional instances (such as Addi-
son Wesley and Zebra Books) for existing class
labels (book publishers).
The WebTables database constructed by Cafarella
et al (2008) is used as the source of additional
instances. Evaluations on gold-standard labeled
classes and instances from existing linguistic re-
sources (Fellbaum, 1998) indicate coverage im-
provements relative to that of Van Durme and Pas?ca
(2008), while retaining similar precision levels.
2 First Phase Extractors
To show Adsorption?s ability to uniformly combine
extractions from multiple sources and methods, we
apply it to: 1) high-precision open-domain extrac-
tions from free Web text (Van Durme and Pas?ca,
2008), and 2) high-recall extractions from WebTa-
bles, a large database of HTML tables mined from
the Web (Cafarella et al, 2008). These two meth-
ods were chosen to be representative of two broad
classes of extraction sources: free text and structured
Web documents.
2.1 Extraction from Free Text
Van Durme and Pas?ca (2008) produce an open-
domain set of instance clusters C ? C that parti-
tions a given set of instances I using distributional
similarity (Lin and Pantel, 2002), and labels using
is-a patterns (Hearst, 1992). By filtering the class
labels using distributional similarity, a large number
of high-precision labeled clusters are extracted. The
algorithm proceeds iteratively: at each step, all clus-
ters are tested for label coherence and all coherent
labels are tested for high cluster specificity. Label
L is coherent if it is shared by at least J% of the
instances in cluster C, and it is specific if the total
number of other clusters C ? ? C, C ? 6= C containing
instances with label L is less thanK. When a cluster
is found to match these criteria, it is removed from
C and added to an output set. The procedure termi-
nates when no new clusters can be removed from C.
Table 1 shows a few randomly chosen classes and
representative instances obtained by this procedure.
2.2 Extraction from Structured Text
To expand the instance sets extracted from free
text, we use a table-based extraction method that
mines structured Web data in the form of HTML
tables. A significant fraction of the HTML ta-
bles in Web pages is assumed to contain coherent
lists of instances suitable for extraction. Identifying
such tables from scratch is hard, but seed instance
lists can be used to identify potentially coherent ta-
ble columns. In this paper we use the WebTables
database of around 154 million tables as our struc-
tured data source (Cafarella et al, 2008).
We employ a simple ranking scheme for candi-
date instances in the WebTables corpus T . Each ta-
ble T ? T consists of one or more columns. Each
column g ? T consists of a set of candidate in-
stances i ? g corresponding to row elements. We
define the set of unique seed matches in g relative to
semantic class C ? C as
MC(g)
def
= {i ? I(C) : i ? g}
where I(C) denotes the set of instances in seed class
C. For each column g, we define its ?-unique class
coverage, that is, the set of classes that have at least
? unique seeds in g,
Q(g;?)
def
= {C ? C : |MC(g)| ? ?}.
Using M and Q we define a method for scoring
columns relative to each class. Intuitively, such a
score should take into account not only the number
of matches from class C, but also the total num-
ber of classes that contribute to Q and their relative
overlap. Towards this end, we introduce the scoring
function
score(C, g;?)
def
= |MC(g)|
? ?? ?
seed matches
?
class coherence
? ?? ?
|MC(g)|
|
?
C??Q(g;?) I(C
?)|
which is the simplest scoring function combining
the number of seed matches with the coherence of
the table column. Coherence is a critical notion
in WebTables extraction, as some tables contain in-
stances across many diverse seed classes, contribut-
ing to extraction noise. The class coherence intro-
duced here also takes into account class overlap; that
583
Class Size Examples of Instances
Book Publishers 70 crown publishing, kluwer academic, prentice hall, puffin
Federal Agencies 161 catsa, dhs, dod, ex-im bank, fsis, iema, mema, nipc, nmfs, tdh, usdot
Mammals 956 armadillo, elephant shrews, long-tailed weasel, river otter, weddell seals, wild goat
NFL Players 180 aikman, deion sanders, fred taylor, jamal lewis, raghib ismail, troy vincent
Scientific Journals 265 biometrika, european economic review, nature genetics, neuroscience
Social Issues 210 gender inequality, lack of education, substandard housing, welfare dependency
Writers 5089 bronte sisters, hemingway, kipling, proust, torquato tasso, ungaretti, yeats
Table 1: A sample of the open-domain classes and associated instances from (Van Durme and Pas?ca, 2008).
is, a column containing many semantically similar
classes is penalized less than one containing diverse
classes.1 Finally, an extracted instance i is assigned
a score relative to class C equal to the sum of all its
column scores,
score(i, C;?)
def
=
1
ZC
?
g?T,T?T
score(C, g;?)
where ZC is a normalizing constant set to the max-
imum score of any instance in class C. This scor-
ing function assigns high rank to instances that oc-
cur frequently in columns with many seed matches
and high class specificity.
The ranked list of extracted instances is post-
filtered by removing all instances that occur in less
than d unique Internet domains.
3 Graph-Based Extraction
To combine the extractions from both free and struc-
tured text, we need a representation capable of en-
coding efficiently all the available information. We
chose a graph representation for the following rea-
sons:
? Graphs can represent complicated relationships
between classes and instances. For example,
an ambiguous instance such as Michael Jor-
dan could belong to the class of both Profes-
sors and NBA players. Similarly, an instance
may belong to multiple nodes in the hierarchy
of classes. For example, Blue Whales could be-
long to both classes Vertebrates and Mammals,
because Mammals are a subset of Vertebrates.
1Note that this scoring function does not take into account
class containment: if all seeds are both wind Instruments and
instruments, then the column should assign higher score to the
more specific class.
? Extractions frommultiple sources, such asWeb
queries, Web tables, and text patterns can be
represented in a single graph.
? Graphs make explicit the potential paths of in-
formation propagation that are implicit in the
more common local heuristics used for weakly-
supervised information extraction. For exam-
ple, if we know that the instance Bill Clinton
belongs to both classes President and Politician
then this should be treated as evidence that the
class of President and Politician are related.
Each instance-class pair (i, C) extracted in the
first phase (Section 2) is represented as a weighted
edge in a graph G = (V,E,W ), where V is the set
of nodes, E is the set of edges and W : E ? R+
is the weight function which assigns positive weight
to each edge. In particular, for each (i, C,w) triple
from the set of base extractions, i and C are added
to V and (i, C) is added to E, 2 with W (i, C) = w.
The weight w represents the total score of all extrac-
tions with that instance and class. Figure 1 illustrates
a portion of a sample graph. This simple graph rep-
resentation could be refined with additional types of
nodes and edges, as we discuss in Section 7.
In what follows, all nodes are treated in the same
way, regardless of whether they represent instances
or classes. In particular, all nodes can be assigned
class labels. For an instance node, that means that
the instance is hypothesized to belong to the class;
for a class node, that means that the node?s class is
hypothesized to be semantically similar to the label?s
class (Section 5).
We now formulate the task of assigning labels to
nodes as graph label propagation. We are given a
2In practice, we use two directed edges, from i to C and
from C to i, both with weight w.
584
bob dylan
musician
0.95
johnny cash
0.87
singer
0.73
billy joel
0.82
0.75
Figure 1: Section of a graph used as input into Adsorp-
tion. Though the nodes do not have any type associated
with them, for readability, instance nodes are marked in
pink while class nodes are shown in green.
set of instances I and a set of classes C represented
as nodes in the graph, with connecting edges as de-
scribed above. We annotate a few instance nodes
with labels drawn from C. That is, classes are used
both as nodes in the graph and as labels for nodes.
There is no necessary alignment between a class
node and any of the (class) labels, as the final labels
will be assigned by the Adsorption algorithm.
The Adsorption label propagation algo-
rithm (Baluja et al, 2008) is now applied to
the given graph. Adsorption is a general framework
for label propagation, consisting of a few nodes
annotated with labels and a rich graph structure
containing the universe of all labeled and unlabeled
nodes. Adsorption proceeds to label all nodes
based on the graph structure, ultimately producing a
probability distribution over labels for each node.
More specifically, Adsorption works on a graph
G = (V,E,W ) and computes for each node v a la-
bel distribution Lv that represents which labels are
more or less appropriate for that node. Several in-
terpretations of Adsorption-type algorithms have ap-
peared in various fields (Azran, 2007; Zhu et al,
2003; Szummer and Jaakkola, 2002; Indyk and Ma-
tousek, 2004). For details, the reader is referred to
(Baluja et al, 2008). We use two interpretations
here:
Adsorption through Random Walks: Let Gr =
(V,Er,Wr) be the edge-reversed version of the
original graph G = (V,E,W ) where (a, b) ?
Er iff (b, a) ? E; and Wr(a, b) = W (b, a).
Now, choose a node of interest q ? V . To es-
timate Lq for q, we perform a random walk on
Gr starting from q to generate values for a ran-
dom label variable L. After reaching a node v
during the walk, we have three choices:
1. With probability pcontv , continue the ran-
dom walk to a neighbor of v.
2. With probability pabndv , abandon the ran-
dom walk. This abandonment proba-
bility makes the random walk stay rela-
tively close to its source when the graph
has high-degree nodes. When the ran-
dom walk passes through such a node,
it is likely that further transitions will be
into regions of the graph unrelated to the
source. The abandonment probability mit-
igates that effect.
3. With probability pinjv , stop the random
walk and emit a label L from Iv.
Lq is set to the expectation of all labels L emit-
ted from random walks initiated from node q.
Adsorption through Averaging: For this interpre-
tation we make some changes to the original
graph structure and label set. We extend the la-
bel distributions Lv to assign a probability not
only to each label in C but also to the dummy
label ?, which represents lack of information
about the actual label(s). We represent the ini-
tial knowledge we have about some node labels
in an augmented graph G? = (V ?, E?,W ?) as
follows. For each v ? V , we define an ini-
tial distribution Iv = L?, where L? is the
dummy distribution with L?(?) = 1, repre-
senting lack of label information for v. In addi-
tion, let Vs ? V be the set of nodes for which
we have some actual label knowledge, and let
V ? = V ? {v? : v ? Vs}, E? = E ? {(v?, v) :
v ? Vs}, and W ?(v?, v) = 1 for v ? Vs,
W ?(u, v) = W (u, v) for u, v ? V . Finally,
let Iv? (seed labels) specify the knowledge about
possible labels for v ? Vs. Less formally, the
v? nodes in G? serve to inject into the graph the
prior label distributions for each v ? Vs.
The algorithm proceeds as follows: For each
node use a fixed-point computation to find label
585
distributions that are weighted averages of the
label distributions for all their neighbors. This
causes the non-dummy initial distribution of Vs
nodes to be propagated across the graph.
Baluja et al (2008) show that those two views are
equivalent. Algorithm 1 combines the two views:
instead of a random walk, for each node v, it itera-
tively computes the weighted average of label distri-
butions from neighboring nodes, and then uses the
random walk probabilities to estimate a new label
distribution for v.
For the experiments reported in Section 4, we
used the following heuristics from Baluja et al
(2008) to set the random walk probabilities:
? Let cv =
log ?
log(? + expH(v)) where H(v) =
?
?
u puv ? log(puv) with puv =
W (u,v)
P
u
? W (u
? ,v)
.
H(v) can be interpreted as the entropy of v?s
neighborhood. Thus, cv is lower if v has many
neighbors. We set ? = 2.
? jv = (1 ? cv) ?
?
H(v) if Iv 6= L> and 0
otherwise.
? Then let
zv = max(cv + jv, 1)
pcontv = cv/zv
pinjv = jv/zv
pabndv = 1? p
cont
v ? p
abnd
v
Thus, abandonment occurs only when the con-
tinuation and injection probabilities are low
enough.
The algorithm is run until convergence which is
achieved when the label distribution on each node
ceases to change within some tolerance value. Alter-
natively, the algorithm can be run for a fixed number
of iterations which is what we used in practice3.
Finally, since Adsorption is memoryless, it eas-
ily scales to tens of millions of nodes with dense
edges and can be easily parallelized, as described
by Baluja et al (2008).
3The number of iterations was set to 10 in the experiments
reported in this paper.
Algorithm 1 Adsorption Algorithm.
Input: G? = (V
?
, E
?
,W ?), Iv (?v ? V ?).
Output: Distributions {Lv : v ? V }.
1: Lv = Iv ?v ? V
?
2:
3: repeat
4: Nv =
?
u W (u, v)
5: Dv = 1Nv
?
u W (u, v)Lu ?v ? V
?
6: for all v ? V
?
do
7: Lv = pcontv ?Dv +p
inj
v ? Iv +pabndv ?L
>
8: end for
9: until convergence
4 Experiments
4.1 Data
As mentioned in Section 3, one of the benefits of
using Adsorption is that we can combine extrac-
tions by different methods from diverse sources into
a single framework. To demonstrate this capabil-
ity, we combine extractions from free-text patterns
and from Web tables. To the best of our knowl-
edge, this is one of the first attempts in the area of
minimally-supervised extraction algorithms where
unstructured and structured text are used in a prin-
cipled way within a single system.
Open-domain (instance, class) pairs were ex-
tracted by applying the method described by Van
Durme and Pas?ca (2008) on a corpus of over 100M
English web documents. A total of 924K (instance,
class) pairs were extracted, containing 263K unique
instances in 9081 classes. We refer to this dataset as
A8.
Using A8, an additional 74M unique (in-
stance,class) pairs are extracted from a random 10%
of the WebTables data, using the method outlined in
Section 2.2. For maximum coverage we set ? = 2
and d = 2, resulting in a large, but somewhat noisy
collection. We refer to this data set as WT.
4.2 Graph Creation
We applied the graph construction scheme described
in Section 3 on the A8 and WT data combined, re-
sulting in a graph with 1.4M nodes and 75M edges.
Since extractions in A8 are not scored, weight of all
586
Seed Class Seed Instances
Book Publishers millbrook press, academic press, springer verlag, chronicle books, shambhala publications
Federal Agencies dod, nsf, office of war information, tsa, fema
Mammals african wild dog, hyaena, hippopotamus, sperm whale, tiger
NFL Players ike hilliard, isaac bruce, torry holt, jon kitna, jamal lewis
Scientific Journals american journal of roentgenology, pnas, journal of bacteriology, american economic review,
ibm systems journal
Table 2: Classes and seeds used to initialize Adsorption.
edges originating from A8 were set at 14. This graph
is used in all subsequent experiments.
5 Evaluation
We evaluated the Adsorption algorithm under two
experimental settings. First, we evaluate Adsorp-
tion?s extraction precision on (instance, class) pairs
obtained by Adsorption but not present in A8 (Sec-
tion 5.1). This measures whether Adsorption can
add to the A8 extractions at fairly high precision.
Second, we measured Adsorption?s ability to assign
labels to a fixed set of gold instances drawn from
various classes (Section 5.2).
Book Publishers Federal Agencies NFL Players Scientific Journals Mammals20
40
60
80
100
 
 Adsorption A8
Book
Publishers
Federal
Agencies
NFL
Players
Scientific
Journals
Mammals
A8 Adsorption
Figure 2: Precision at 100 comparisons for A8 and Ad-
sorption.
5.1 Instance Precision
First we manually evaluated precision across five
randomly selected classes from A8: Book Publish-
ers, Federal Agencies, NFL Players, Scientific Jour-
nals and Mammals. For each class, 5 seed in-
stances were chosen manually to initialize Adsorp-
tion. These classes and seeds are shown in Table 2.
Adsorption was run for each class separately and the
4A8 extractions are assumed to be high-precision and hence
we assign them the highest possible weight.
resulting ranked extractions were manually evalu-
ated.
Since the A8 system does not produce ranked lists
of instances, we chose 100 random instances from
the A8 results to compare to the top 100 instances
produced by Adsorption. Each of the resulting 500
instance-class pairs (i, C) was presented to two hu-
man evaluators, who were asked to evaluate whether
the relation ?i is a C? was correct or incorrect. The
user was also presented with Web search link to ver-
ify the results against actual documents. Results
from these experiments are presented in Figure 2
and Table 4. The results in Figure 2 show that the
A8 system has higher precision than the Adsorption
system. This is not surprising since the A8 system is
tuned for high precision. When considering individ-
ual evaluation classes, changes in precision scores
between the A8 system and the Adsorption system
vary from a small increase from 87% to 89% for the
class Book Publishers, to a significant decrease from
52% to 34% for the class Federal Agencies, with a
decrease of 10% as an average over the 5 evaluation
classes.
Class Precision at 100
(non-A8 extractions)
Book Publishers 87.36
Federal Agencies 29.89
NFL Players 94.95
Scientific Journals 90.82
Mammal Species 84.27
Table 4: Precision of top 100 Adsorption extractions (for
five classes) which were not present in A8.
Table 4 shows the precision of the Adsorption sys-
tem for instances not extracted by the A8 system.
587
Seed Class Non-Seed Class Labels Discovered by Adsorption
Book Publishers small presses, journal publishers, educational publishers, academic publishers,
commercial publishers
Federal Agencies public agencies, governmental agencies, modulation schemes, private sources,
technical societies
NFL Players sports figures, football greats, football players, backs, quarterbacks
Scientific Journals prestigious journals, peer-reviewed journals, refereed journals, scholarly journals,
academic journals
Mammal Species marine mammal species, whale species, larger mammals, common animals, sea mammals
Table 3: Top class labels ranked by their similarity to a given seed class in Adsorption.
Seed Class Sample of Top Ranked Instances Discovered by Adsorption
Book Publishers small night shade books, house of anansi press, highwater books,
distributed art publishers, copper canyon press
NFL Players tony gonzales, thabiti davis, taylor stubblefield, ron dixon, rodney hannah
Scientific Journals journal of physics, nature structural and molecular biology,
sciences sociales et sante?, kidney and blood pressure research,
american journal of physiology?cell physiology
Table 5: Random examples of top ranked extractions (for three classes) found by Adsorption which were not present
in A8.
Such an evaluation is important as one of the main
motivations of the current work is to increase cov-
erage (recall) of existing high-precision extractors
without significantly affecting precision. Results in
Table 4 show that Adsorption is indeed able to ex-
traction with high precision (in 4 out of 5 cases)
new instance-class pairs which were not extracted
by the original high-precision extraction set (in this
case A8). Examples of a few such pairs are shown
in Table 5. This is promising as almost all state-
of-the-art extraction methods are high-precision and
low-recall. The proposed method shows a way to
overcome that limitation.
As noted in Section 3, Adsorption ignores node
type and hence the final ranked extraction may also
contain classes along with instances. Thus, in ad-
dition to finding new instances for classes, it also
finds additional class labels similar to the seed class
labels with which Adsorption was run, at no extra
cost. Some of the top ranked class labels extracted
by Adsorption for the corresponding seed class la-
bels are shown in Table 3. To the best of our knowl-
edge, there are no other systems which perform both
tasks simultaneously.
5.2 Class Label Recall
Next we evaluated each extraction method on its rel-
ative ability to assign labels to class instances. For
each test instance, the five most probably class la-
bels are collected using each method and the Mean
Reciprocal Rank (MRR) is computed relative to a
gold standard target set. This target set, WN-gold,
consists of the 38 classes in Wordnet containing 100
or more instances.
In order to extract meaningful output from Ad-
sorption, it is provided with a number of labeled seed
instances (1, 5, 10 or 25) from each of the 38 test
classes. Regardless of the actual number of seeds
used as input, all 25 seed instances from each class
are removed from the output set from all methods,
in order to ensure fair comparison.
The results from this evaluation are summarized
in Table 6; AD x refers to the adsorption run with x
seed instances. Overall, Adsorption exhibits higher
MRR than either of the baseline methods, with MRR
increasing as the amount of supervision is increased.
Due to its high coverage, WT assigns labels to
a larger number of the instance in WN-gold than
any other method. However, the average rank of
the correct class assignment is lower, resulting is
588
MRR MRR # found
Method (full) (found only)
A8 0.16 0.47 2718
WT 0.15 0.21 5747
AD 1 0.26 0.45 4687
AD 5 0.29 0.48 4687
AD 10 0.30 0.51 4687
AD 25 0.32 0.55 4687
Table 6: Mean-Reciprocal Rank scores of instance class
labels over 38 Wordnet classes (WN-gold). MRR (full)
refers to evaluation across the entire gold instance set.
MRR (found only) computes MRR only on recalled in-
stances.
lower MRR scores compared to Adsorption. This
result highlights Adsorption?s ability to effectively
combine high-precision, low-recall (A8) extractions
with low-precision, high-recall extractions (WT) in
a manner that improves both precision and coverage.
6 Related Work
Graph based algorithms for minimally supervised
information extraction methods have recently been
proposed. For example, Wang and Cohen (2007)
use a random walk on a graph built from entities and
relations extracted from semi-structured text. Our
work differs both conceptually, in terms of its focus
on open-domain extraction, as well as methodologi-
cally, as we incorporate both unstructured and struc-
tured text. The re-ranking algorithm of Bellare et al
(2007) also constructs a graph whose nodes are in-
stances and attributes, as opposed to instances and
classes here. Adsorption can be seen as a general-
ization of the method proposed in that paper.
7 Conclusion
The field of open-domain information extraction has
been driven by the growth of Web-accessible data.
We have staggering amounts of data from various
structured and unstructured sources such as general
Web text, online encyclopedias, query logs, web ta-
bles, or link anchor texts. Any proposed algorithm
to extract information needs to harness several data
sources and do it in a robust and scalable manner.
Our work in this paper represents a first step towards
that goal. In doing so, we achieved the following:
1. Improved coverage relative to a high accuracy
instance-class extraction system while main-
taining adequate precision.
2. Combined information from two different
sources: free text and web tables.
3. Demonstrated a graph-based label propagation
algorithm that given as little as five seeds per
class achieved good results on a graph with
more than a million nodes and 70 million
edges.
In this paper, we started off with a simple graph.
For future work, we plan to proceed along the fol-
lowing lines:
1. Encode richer relationships between nodes,
for example instance-instance associations and
other types of nodes.
2. Combine information from more data sources
to answer the question of whether more data or
diverse sources are more effective in increasing
precision and coverage.
3. Apply similar ideas to other information extrac-
tion tasks such as relation extraction.
Acknowledgments
We would like to thank D. Sivakumar for useful dis-
cussions and the anonymous reviewers for helpful
comments.
References
A. Azran. 2007. The rendezvous algorithm: multiclass
semi-supervised learning with markov random walks.
Proceedings of the 24th international conference on
Machine learning, pages 49?56.
S. Baluja, R. Seth, D. Sivakumar, Y. Jing, J. Yagnik,
S. Kumar, D. Ravichandran, and M. Aly. 2008. Video
suggestion and discovery for youtube: taking random
walks through the view graph.
K. Bellare, P. Talukdar, G. Kumaran, F. Pereira, M. Liber-
man, A. McCallum, and M. Dredze. 2007. Lightly-
Supervised Attribute Extraction. NIPS 2007Workshop
on Machine Learning for Web Search.
M. Cafarella, A. Halevy, D. Wang, E. Wu, and Y. Zhang.
2008. Webtables: Exploring the power of tables on the
web. VLDB.
589
C. Fellbaum, editor. 1998. WordNet: An Electronic Lexi-
cal Database and Some of its Applications. MIT Press.
M. Hearst. 1992. Automatic acquisition of hyponyms
from large text corpora. In Proceedings of the 14th In-
ternational Conference on Computational Linguistics
(COLING-92), pages 539?545, Nantes, France.
P. Indyk and J. Matousek. 2004. Low-distortion embed-
dings of finite metric spaces. Handbook of Discrete
and Computational Geometry.
B. Jansen, A. Spink, and T. Saracevic. 2000. Real life,
real users, and real needs: a study and analysis of user
queries on the Web. Information Processing and Man-
agement, 36(2):207?227.
D. Lin and P. Pantel. 2002. Concept discovery from text.
In Proceedings of the 19th International Conference
on Computational linguistics (COLING-02), pages 1?
7.
K. McCarthy and W. Lehnert. 1995. Using decision
trees for coreference resolution. In Proceedings of the
14th International Joint Conference on Artificial Intel-
ligence (IJCAI-95), pages 1050?1055, Montreal, Que-
bec.
E. Riloff and R. Jones. 1999. Learning dictionaries for
information extraction by multi-level bootstrapping.
In Proceedings of the 16th National Conference on
Artificial Intelligence (AAAI-99), pages 474?479, Or-
lando, Florida.
M. Stevenson and R. Gaizauskas. 2000. Using corpus-
derived name lists for named entity recognition. In
Proceedings of the 6th Conference on Applied Natu-
ral Language Processing (ANLP-00), Seattle, Wash-
ington.
M. Szummer and T. Jaakkola. 2002. Partially labeled
classification with markov random walks. Advances in
Neural Information Processing Systems 14: Proceed-
ings of the 2002 NIPS Conference.
B. Van Durme and M. Pas?ca. 2008. Finding cars, god-
desses and enzymes: Parametrizable acquisition of la-
beled instances for open-domain information extrac-
tion. Twenty-Third AAAI Conference on Artificial In-
telligence.
R. Wang and W. Cohen. 2007. Language-Independent
Set Expansion of Named Entities Using theWeb. Data
Mining, 2007. ICDM 2007. Seventh IEEE Interna-
tional Conference on, pages 342?350.
X. Zhu, Z. Ghahramani, and J. Lafferty. 2003. Semi-
supervised learning using gaussian fields and har-
monic functions. ICML-03, 20th International Con-
ference on Machine Learning.
590
Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 167?176,
MIT, Massachusetts, USA, 9-11 October 2010. c?2010 Association for Computational Linguistics
Efficient Graph-Based Semi-Supervised Learning
of Structured Tagging Models
Amarnag Subramanya
Google Research
Mountain View, CA 94043
asubram@google.com
Slav Petrov
Google Research
New York, NY 10011
slav@google.com
Fernando Pereira
Google Research
Mountain View, CA 94043
pereira@google.com
Abstract
We describe a new scalable algorithm for
semi-supervised training of conditional ran-
dom fields (CRF) and its application to part-
of-speech (POS) tagging. The algorithm uses
a similarity graph to encourage similar n-
grams to have similar POS tags. We demon-
strate the efficacy of our approach on a do-
main adaptation task, where we assume that
we have access to large amounts of unlabeled
data from the target domain, but no additional
labeled data. The similarity graph is used dur-
ing training to smooth the state posteriors on
the target domain. Standard inference can be
used at test time. Our approach is able to scale
to very large problems and yields significantly
improved target domain accuracy.
1 Introduction
Semi-supervised learning (SSL) is the use of
small amounts of labeled data with relatively large
amounts of unlabeled data to train predictors. In
some cases, the labeled data can be sufficient to pro-
vide reasonable accuracy on in-domain data, but per-
formance on even closely related out-of-domain data
may lag far behind. Annotating training data for all
sub-domains of a varied domain such as all of Web
text is impractical, giving impetus to the develop-
ment of SSL techniques that can learn from unla-
beled data to perform well across domains. The ear-
liest SSL algorithm is self-training (Scudder, 1965),
where one makes use of a previously trained model
to annotate unlabeled data which is then used to
re-train the model. While self-training is widely
used and can yield good results in some applica-
tions (Yarowsky, 1995), it has no theoretical guaran-
tees except under certain stringent conditions, which
rarely hold in practice(Haffari and Sarkar, 2007).
Other SSL methods include co-training (Blum
and Mitchell, 1998), transductive support vector ma-
chines (SVMs) (Joachims, 1999), and graph-based
SSL (Zhu et al, 2003). Several surveys cover a
broad range of methods (Seeger, 2000; Zhu, 2005;
Chapelle et al, 2007; Blitzer and Zhu, 2008). A ma-
jority of SSL algorithms are computationally expen-
sive; for example, solving a transductive SVM ex-
actly is intractable. Thus we have a conflict between
wanting to use SSL with large unlabeled data sets
for best accuracy, but being unable to do so because
of computational complexity. Some researchers at-
tempted to resolve this conflict by resorting to ap-
proximations (Collobert et al, 2006), but those lead
to suboptimal results (Chapelle et al, 2007).
Graph-based SSL algorithms (Zhu et al, 2003;
Joachims, 2003; Corduneanu and Jaakkola, 2003;
Belkin et al, 2005; Subramanya and Bilmes, 2009)
are an important subclass of SSL techniques that
have received much attention in the recent past, as
they outperform other approaches and also scale eas-
ily to large problems. Here one assumes that the data
(both labeled and unlabeled) is represented by ver-
tices in a graph. Graph edges link vertices that are
likely to have the same label. Edge weights govern
how strongly the labels of the nodes linked by the
edge should agree.
Most previous work in SSL has focused on un-
structured classification problems, that is, problems
with a relatively small set of atomic labels. There
167
has been much less work on SSL for structured pre-
diction where labels are composites of many atomic
labels with constraints between them. While the
number of atomic labels might be small, there will
generally be exponentially many ways to combine
them into the final structured label. Structured pre-
diction problems over sequences appear for exam-
ple in speech recognition, named-entity recogni-
tion, and part-of-speech tagging; in machine trans-
lation and syntactic parsing, the output may be tree-
structured.
Altun et al (2005) proposed a max-margin ob-
jective for semi-supervised learning over structured
spaces. Their objective is similar to that of manifold
regularization (Belkin et al, 2005) and they make
use of a graph as a smoothness regularizer. However
their solution involves inverting a matrix whose size
depends on problem size, making it impractical for
very large problems. Brefeld and Scheffer (2006)
present a modified version of the co-training algo-
rithm for structured output spaces. In both of the
above cases, the underlying model is based on struc-
tured SVM, which does not scale well to very large
datasets. More recently Wang et al (2009) proposed
to train a conditional random field (CRF) (Lafferty et
al., 2001) using an entropy-based regularizer. Their
approach is similar to the entropy minimization al-
gorithm (Grandvalet and Bengio, 2005). The prob-
lem here is that their objective is not convex and thus
can pose issues for large problems. Further, graph-
based SSL algorithms outperform algorithms based
on entropy minimization (Chapelle et al, 2007).
In this work, we propose a graph-based SSL
method for CRFs that is computationally practical
for very large problems, unlike the methods in the
studies cited above. Our method is scalable be-
cause it trains with efficient standard building blocks
for CRF inference and learning and also standard
graph label propagation machinery. Graph regular-
izer computations are only used for training, so at
test time, standard CRF inference can be used, un-
like in graph-based transductive methods. Briefly,
our approach starts by training a CRF on the source
domain labeled data, and then uses it to decode unla-
beled data from the target domain. The state posteri-
ors on the target domain are then smoothed using the
graph regularizer. Best state sequences for the unla-
beled target data are then created by Viterbi decod-
ing with the smoothed state posteriors, and this au-
tomatic target domain annotation is combined with
the labeled source domain data to retrain the CRF.
We demonstrate our new method in domain adap-
tation for a CRF part-of-speech (POS) tagger. While
POS tagging accuracies have reached the level of
inter-annotator agreement (>97%) on the standard
PennTreebank test set (Toutanova et al, 2003; Shen
et al, 2007), performance on out-of-domain data is
often well below 90%, impairing language process-
ing tasks that need syntactic information. For exam-
ple, on the question domain used in this paper, the
tagging accuracy of a supervised CRF is only 84%.
Our domain adaptation algorithm improves perfor-
mance to 87%, which is still far below in-domain
performance, but a significant reduction in error.
2 Supervised CRF
We assume that we have a set of labeled source do-
main examples Dl = {(xi,yi)}li=1, but only un-
labeled target domain examples Du = {xi}
l+u
i=l+1.
Here xi = x
(1)
i x
(2)
i ? ? ?x
(|xi|)
i is the sequence of
words in sentence i and yi = y
(1)
i y
(2)
i ? ? ? y
(|xi|)
i is
the corresponding POS tag sequence, with y(j)i ? Y
where Y is the set of POS tags. Our goal is to learn
a CRF of the form:
p(yi|xi; ?)?exp
(Ni?
j=1
K?
k=1
?kfk(y
(j?1)
i ,y
(j)
i ,xi, j)
)
for the target domain. In the above equation, ? =
{?1, . . . , ?K} ? RK , fk(y
(j?1)
i , y
(j)
i ,xi, j) is the k-
th feature function applied to two consecutive CRF
states and some window of the input sequence, and
?k is the weight of that feature. We discuss our fea-
tures in detail in Section 6. Given only labeled data
Dl, the optimal feature weights are given by:
??=argmin
??RK
[
?
l?
i=1
log p(yi|xi; ?)+????2
]
(1)
Here ???2 is the squared `2-norm and acts as the
regularizer, and ? is a trade-off parameter whose set-
ting we discuss in Section 6. In our case, we also
have access to the unlabeled data Du from the target
domain which we would like to use for training the
CRF. We first describe how we construct a similarity
168
graph over the unlabeled which will be used in our
algorithm as a graph regularizer.
3 Graph Construction
Graph construction is the most important step in
graph-based SSL. The standard approach for un-
structured problems is to construct a graph whose
vertices are labeled and unlabeled examples, and
whose weighted edges encode the degree to which
the examples they link should have the same la-
bel (Zhu et al, 2003). Then the main graph con-
struction choice is what similarity function to use
for the weighted edges between examples. How-
ever, in structured problems the situation is more
complicated. Consider the case of sequence tag-
ging we are studying. While we might be able to
choose some appropriate sequence similarity to con-
struct the graph, such as edit distance or a string
kernel, it is not clear how to use whole sequence
similarity to constrain whole tag sequences assigned
to linked examples in the learning algorithm. Al-
tun et al (2005) had the nice insight of doing the
graph construction not for complete structured ex-
amples but instead for the parts of structured exam-
ples (also known as factors in graphical model ter-
minology), which encode the local dependencies be-
tween input data and output labels in the structured
problem. However, their approach is too demanding
computationally (see Section 5), so instead we use
local sequence contexts as graph vertices, exploting
the empirical observation that the part of speech of
a word occurrence is mostly determined by its local
context.
Specifically, the set V of graph vertices consists
of all the word n-grams1 (types) that have occur-
rences (tokens) in training sentences (labeled and
unlabeled). We partition V = Vl ? Vu where Vl cor-
responds to n-grams that occur at least once in the
labeled data, and Vu corresponds to n-grams that oc-
cur only in the unlabeled data.
Given a symmetric similarity function between
types to be defined below, we link types u and v with
1We pad the n-grams at the beginning and end of sentences
with appropriate dummy symbols.
Description Feature
Trigram + Context x1 x2 x3 x4 x5
Trigram x2 x3 x4
Left Context x1 x2
Right Context x4 x5
Center Word x2
Trigram ? Center Word x2 x4
Left Word + Right Context x2 x4 x5
Left Context + Right Word x1 x2 x4
Suffix HasSuffix(x3)
Table 1: Features we extract given a sequence of words
?x1 x2 x3 x4 x5? where the trigram is ?x2 x3 x4?.
an edge of weight wuv, defined as:
wuv =
{
sim(u, v) if v ? K(u) or u ? K(v)
0 otherwise
whereK(u) is the set of k-nearest neighbors of u ac-
cording to the given similarity. For all experiments
in this paper, n = 3 and k = 5.
To define the similarity function, for each token
of a given type in the labeled and unlabeled data,
we extract a set of context features. For example,
for the token x2 x3 x4 occurring in the sequence
x1 x2 x3 x4 x5, we use feature templates that cap-
ture the left (x1 x2) and right contexts (x4 x5). Addi-
tionally, we extract suffix features from the word in
the middle. Table 1 gives an overview of the features
that we used. For each n-gram type, we compute the
vector of pointwise mutual information (PMI) val-
ues between the type and each of the features that
occur with tokens of that type. Finally, we use the
cosine distance between those PMI vectors as our
similarity function.
We have thus circumvented the problem of defin-
ing similarities over sequences by defining the graph
over types that represent local sequence contexts.
Since our CRF tagger only uses local features of the
input to score tag pairs, we believe that the graph
we construct captures all significant context infor-
mation. Figure 1 shows an excerpt from our graph.
The figure shows the neighborhoods of a subset of
the vertices with the center word ?book.? To reduce
clutter, we included only closest neighbors and the
edges that involve the nodes of interest.
169
[the conference on]
[whose book on]
[the auction on]
[U.N.-backed conference on]
[the conference speakers]
[to schedule a]
[to postpone a]
VB
[to ace a]
[to book a]
[to run a]
[to start a]
NN
NN
NN
VB
VB
[you book a]
[you rent a]
[you log a]
[you unrar a]
[to book some]
[to approve some]
VB
[to fly some]
[to approve parental-consent]
6
4
3
[the book that]
[the job that]
[the constituition that]
[the movie that]
[the city that]
NN
NN
[a movie agent]
[a clearing agent]
[a book agent]
7
4
6
Figure 1: Vertices with center word ?book? and their local neighborhoods, as well as the shortest-path distance between
them. Note that the noun (NN) and verb (VB) interpretations form two disjoint connected components.
It is remarkable that the neighborhoods are co-
herent, showing very similar syntactic configura-
tions. Furthermore, different vertices that (should)
have the same label are close to each other, form-
ing connected components for each part-of-speech
category (for nouns and verbs in the figure). We ex-
pect the similarity graph to provide information that
cannot be expressed directly in a sequence model.
In particular, it is not possible in a CRF to directly
enforce the constraint that similar trigrams appear-
ing in different sentences should have similar POS
tags. This constraint however is important dur-
ing (semi-supervised) learning, and is what makes
our approach different and more effective than self-
training.
In practice, we expect two main benefits from
our graph-based approach. First, the graph allows
new features to be discovered. Many words occur
only in the unlabeled data and a purely supervised
CRF would not be able to learn feature weights for
those observations. We could use self-training to
learn weights for those features, but self-training just
tends to reinforce the knowledge that the supervised
model already has. The similarity graph on the other
hand can link events that occur only in the unlabeled
data to similar events in the labeled data. Further-
more, because the graph is built over types rather
than tokens, it will encourage the same interpreta-
tion to be chosen for similar trigrams occurring in
different sentences. For example, the word ?unrar?
will most likely not occur in the labeled training
data. Seeing it in the neighborhood of words for
which we know the POS tag will help us learn the
correct POS tag for this otherwise unknown word
(see Figure 1).
Second, the graph propagates adjustments to the
weights of known features. Many words occur only
a handful of times in our labeled data, resulting in
poor estimates of their contributions. Even for fre-
quently occurring events, their distribution in the tar-
get domain might be different from their distribution
in the source domain. While self-training might be
able to help adapt to such domain changes, its ef-
fectiveness will be limited because the model will
always be inherently biased towards the source do-
main. In contrast, labeled vertices in the similar-
ity graph can help disambiguate ambiguous contexts
and correct (some of) the errors of the supervised
model.
4 Semi-Supervised CRF
Given unlabeled data Du, we only have access to
the prior p(x). As the CRF is a discriminative
model, the lack of label information renders the
CRF weights independent of p(x) and thus we can-
not directly utilize the unlabeled data when train-
ing the CRF. Therefore, semi-supervised approaches
to training discriminative models typically use the
unlabeled data to construct a regularizer that is
used to guide the learning process (Joachims, 1999;
Lawrence and Jordan, 2005). Here we use the graph
as a smoothness regularizer to train CRFs in a semi-
supervised manner.
Our algorithm iterates between the following five
170
Algorithm 1 Semi-Supervised CRF Training
?s = crf-train(Dl, ?0)
Set ?(t)0 = ?
(s)
while not converged do
{p} = posterior decode(Du, ?old)
{q} = token to type({p})
{q?} = graph propagate({q})
D(1)u = viterbi decode({q?}, ?old)
?(t)n+1 = crf-train(Dl ? D
(1)
u , ?
(t)
n )
end while
Return last ?(t)
simple (and convex) steps: Given a set of CRF pa-
rameters, we first compute marginals over the un-
labeled data (posterior decode). The marginals
over tokens are then aggregated to marginals over
types (token to type), which are used to initial-
ize the graph label distributions. After running la-
bel propagation (graph propagate), the posteriors
from the graph are used to smooth the state posteri-
ors. Decoding the unlabeled data (viterbi decode)
produces a new set of automatic annotations that can
be combined with the labeled data to retrain the CRF
using the supervised CRF training objective (crf-
train). These steps, summarized in Algorithm 1, are
iterated until convergence.
4.1 Posterior Decoding
Let ?(t)n (t refers to target domain) represent the esti-
mate of the CRF parameters for the target domain af-
ter the n-th iteration.2 In this step, we use the current
parameter estimates to compute the marginal proba-
bilities
p(y(j)i |xi; ?
(t)
n ) 1 ? j ? |xi|, i ? Dl
over POS tags for every word position j for i index-
ing over sentences in Dl ? Du.
4.2 Token-to-Type Mapping
Recall that our graph is defined over types while
the posteriors computed above involve particular to-
kens. We accumulate token-based marginals to cre-
ate type marginals as follows. For a sentence i and
word position j in that sentence, let T (i, j) be the
2In the first iteration, we initialize the target domain param-
eters to the source domain parameters: ?(t)0 = ?
(s).
trigram (graph node) centered at position j. Con-
versely, for a trigram type u, let T?1(u) be the set
of actual occurrences (tokens) of that trigram u; that
is, all pairs (i, j) where i is the index of a sentence
where u occurs and j is the position of the center
word of an occurrence of u in that sentence. We cal-
culate type-level posteriors as follows:
qu(y) ,
1
|T?1(u)|
?
(i,j)?T?1(u)
p(y(j)i |xi; ?
(t)
n ) .
This combination rule connects the token-centered
CRF with the type-centered graph. Other ways
of combining the token marginals, such as using
weights derived from the entropies of marginals,
might be worth investigating.
4.3 Graph Propagation
We now use our similarity graph (Section 3) to
smooth the type-level marginals by minimizing the
following convex objective:
C(q) =
?
u?Vl
?ru ? qu?
2
+ ?
?
u?V,v?N (i)
wuv?qu ? qv?
2 + ?
?
u?V
?qu ? U?
2
s.t.
?
y
qu(y) = 1 ?u & qu(y) ? 0 ?u, y (2)
where q = {q1, q2, . . . q|V |}. The setting of the
hyperparameters ? and ? will be discussed in Sec-
tion 6, N (u) is the set of neighbors of node u, and
ru is the empirical marginal label distribution for tri-
gram u in the labeled data. We use a squared loss to
penalize neighboring nodes that have different label
distributions: ?qu ? qv?2 =
?
y(qu(y) ? qv(y))
2,
additionally regularizing the label distributions to-
wards the uniform distribution U over all possible
labels Y . It can be shown that the above objective is
convex in q.
Our graph propagation objective can be seen as a
multi-class generalization of the quadratic cost crite-
rion (Bengio et al, 2007). The first term in the above
objective requires that we respect the information
in our labeled data. The second term is the graph
smoothness regularizer which requires that the qi?s
be smooth with respect to the graph. In other words,
if wuv is large, then qu and qv should be close in the
171
squared-error sense. This implies that vertices u and
v are likely to have similar marginals over POS tags.
The last term is a regularizer and encourages all type
marginals to be uniform to the extent that is allowed
by the first two terms. If a unlabeled vertex does
not have a path to any labeled vertex, this term en-
sures that the converged marginal for this vertex will
be uniform over all tags, ensuring that our algorithm
performs at least as well as a standard self-training
based algorithm, as we will see later.
While the objective in Equation 2 admits a closed
form solution, it involves inverting a matrix of or-
der |V | and thus we use instead the simple iterative
update given by
q(m)u (y) =
?u(y)
?u
where
?u(y) = ru(y)?(u ? Vl)
+
?
v?N (u)
wuvq
(m?1)
v (y) + ?U(y),
?u = ?(u ? Vl) + ? + ?
?
v?N (u)
wuv (3)
where m is the iteration index and ? is the indica-
tor function that returns 1 if and only if the con-
dition is true. The iterative procedure starts with
q(0)u (y) = qu(y) as given in the previous section.
In all our experiments we run 10 iterations of the
above algorithm, and we denote the type marginals
at completion by q?u(y).
4.4 Viterbi Decoding
Given the type marginals computed in the previous
step, we interpolate them with the original CRF to-
ken marginals. This interpolation between type and
token marginals encourages similar n-grams to have
similar posteriors, while still allowing n-grams in
different sentences to differ in their posteriors. For
each unlabeled sentence i and word position j in it,
we calculate the following interpolated tag marginal:
p?(y(j)i = y|xi) = ?p(y
(j)
i = y|xi; ?
(t)
n )
+ (1? ?)q?T (m,n)(y) (4)
where ? is a mixing coefficient which reflects the
relative confidence between the original posteriors
from the CRF and the smoothed posteriors from the
graph. We discuss how we set ? in Section 6.
The interpolated marginals summarize all the in-
formation obtained so far about the tag distribution
at each position. However, if we were to use them on
their own to select the most likely POS tag sequence,
the first-order tag dependencies modeled by the CRF
would be mostly ignored. This happens because the
type marginals obtained from the graph after label
propagation will have lost most of the sequence in-
formation. To enforce the first-order tag dependen-
cies we therefore use Viterbi decoding over the com-
bined interpolated marginals and the CRF transition
potentials to compute the best POS tag sequence for
each unlabeled sentence. We refer to these 1-best
transcripts as y?i , i ? Du.
4.5 Re-training the CRF
Now that we have successfully labeled the unlabeled
target domain data, we can use it in conjunction with
the source domain labeled data to re-train the CRF:
?(t)n+1 =argmin
??RK
[
?
l?
i=1
log p(yi|xi; ?(t)n )
? ?
l+u?
i=l+1
log p(y?i |xi; ?
(t)
n )+????
2
]
(5)
where ? and ? are hyper-parameters whose setting
we discuss in Section 6. Given the new CRF pa-
rameters ? we loop back to step 1 (Section 4.1) and
iterate until convergence. It is important to note that
every step of our algorithm is convex, although their
combination clearly is not.
5 Related Work
Our work differs from previous studies of
SSL (Blitzer et al, 2006; III, 2007; Huang
and Yates, 2009) for improving POS tagging in
several ways. First, our algorithm can be general-
ized to other structured semi-supervised learning
problems, although POS tagging is our motivating
task and test application. Unlike III (2007), we
do not require target domain labeled data. While
the SCL algorithm (Blitzer et al, 2006) has been
evaluated without target domain labeled data, that
evaluation was to some extent transductive in that
the target test data (unlabeled) was included in the
unsupervised stage of SCL training that creates the
structural correspondence between the two domains.
172
We mentioned already the algorithm of Altun et
al. (2005), which is unlikely to scale up because
its dual formulation requires the inversion of a ma-
trix whose size depends on the graph size. Gupta
et al (2009) also constrain similar trigrams to have
similar POS tags by forming cliques of similar tri-
grams and maximizing the agreement score over
these cliques. Computing clique agreement poten-
tials however is NP-hard and so they propose ap-
proximation algorithms that are still quite complex
computationally. We achieve similar effects by us-
ing our simple, scalable convex graph regularization
framework. Further, unlike other graph-propagation
algorithms (Alexandrescu and Kirchhoff, 2009), our
approach is inductive. While one might be able
to make inductive extensions of transductive ap-
proaches (Sindhwani et al, 2005), these usually re-
quire extensive computational resources at test time.
6 Experiments and Results
We use the Wall Street Journal (WSJ) section of
the Penn Treebank as our labeled source domain
training set. We follow standard setup procedures
for this task and train on sections 00-18, compris-
ing of 38,219 POS-tagged sentences with a total of
912,344 words. To evaluate our domain-adaptation
approach, we consider two different target domains:
questions and biomedical data. Both target do-
mains are relatively far from the source domain
(newswire), making this a very challenging task.
The QuestionBank (Judge et al, 2006), provides
an excellent corpus consisting of 4,000 questions
that were manually annotated with POS tags and
parse trees. We used the first half as our develop-
ment set and the second half as our test set. Ques-
tions are difficult to tag with WSJ-trained taggers
primarily because the word order is very different
than that of the mostly declarative sentences in the
training data. Additionally, the unknown word rate
is more than twice as high as on the in-domain de-
velopment set (7.29% vs. 3.39%). As our unla-
beled data, we use a set of 10 million questions
collected from anonymized Internet search queries.
These queries were selected to be similar in style
and length to the questions in the QuestionBank.3
3In particular, we selected queries that start with an English
function word that can be used to start a question (what, who,
As running the CRF over 10 million sentences can
be rather cumbersome and probably unnecessary, we
randomly select 100,000 of these queries and treat
this asDu. Because the graph nodes and the features
used in the similarity function are based on n-grams,
data sparsity can be a serious problem, and we there-
fore use the entire unlabeled data set for graph con-
struction. We estimate the mutual information-based
features for each trigram type over all the 10 million
questions, and then construct the graph over only
the set of trigram types that actually occurs in the
100,000 random subset and the WSJ training set.
For our second target domain, we use the Penn
BioTreebank (PennBioIE, 2005). This corpus con-
sists of 1,061 sentences that have been manually an-
notated with POS tags. We used the first 500 sen-
tences as a development set and the remaining 561
sentences as our final test set. The high unknown
word rate (23.27%) makes this corpus very difficult
to tag. Furthermore, the POS tag set for this data is a
super-set of the Penn Treebank?s, including the two
new tags HYPH (for hyphens) and AFX (for com-
mon post-modifiers of biomedical entities such as
genes). These tags were introduced due to the im-
portance of hyphenated entities in biomedical text,
and are used for 1.8% of the words in the test set.
Any tagger trained only on WSJ text will automati-
cally predict wrong tags for those words. For unla-
beled data we used 100,000 sentences that were cho-
sen by searching MEDLINE for abstracts pertaining
to cancer, in particular genomic variations and muta-
tions (Blitzer et al, 2006). Since we did not have ac-
cess to additional unlabeled data, we used the same
set of sentences as target domain unlabeled data,Du.
The graph here was constructed over the 100,000 un-
labeled sentences and the WSJ training set. Finally,
we remind the reader that we did not use label infor-
mation for graph construction in either corpus.
6.1 Baselines
Our baseline supervised CRF is competitive
with state-of-the-art discriminative POS taggers
(Toutanova et al, 2003; Shen et al, 2007), achieving
97.17% on the WSJ development set (sections 19-
21). We use a fairly standard set of features, includ-
ing word identity, suffixes and prefixes and detectors
when, etc.), and have between 30 and 160 characters.
173
Questions Bio
Dev Eval Dev Eval
Supervised CRF 84.8 83.8 86.5 86.2
Self-trained CRF 85.4 84.0 87.5 87.1
Semi-supervised CRF 87.6 86.8 87.5 87.6
Table 2: Domain adaptation experiments. POS tagging accuracies in %.
for special characters such as dashes and digits. We
do not use of observation-dependent transition fea-
tures. Both supervised and semi-supervised models
are regularized with a squared `2-norm regularizer
with weight set to 0.01.
In addition to the supervised baseline trained ex-
clusively on the WSJ, we also consider a semi-
supervised self-trained baseline (?Self-trained CRF?
in Table 2). In this approach, we first train a su-
pervised CRF on the labeled data and then do semi-
supervised training without label propagation. This
is different from plain self-training because it aggre-
gates the posteriors over tokens into posteriors over
types. This aggregation step allows instances of the
same trigram in different sentences to share infor-
mation and works better in practice than direct self-
training on the output of the supervised CRF.
6.2 Domain Adaptation Results
The data set obtained concatenating the WSJ train-
ing set with the 10 million questions had about 20
million trigram types. Of those, only about 1.1 mil-
lion trigram types occurred in the WSJ training set
or in the 100,000 sentence sub-sample. For the
biomedical domain, the graph had about 2.2 mil-
lion trigrams. For all our experiments we set hy-
perparameters as follows: for graph propagation,
? = 0.5, ? = 0.01, for Viterbi decoding mixing,
? = 0.6, for CRF re-training, ? = 0.001, ? = 0.01.
These parameters were chosen based on develop-
ment set performance. All CRF objectives were op-
timized using L-BFGS (Bertsekas, 2004).
Table 2 shows the results for both domains. For
the question corpus, the supervised CRF performs
at only 85% on the development set. While it is al-
most impossible to improve in-domain tagging ac-
curacy and tagging is therefore considered a solved
problem by many, these results clearly show that
the problem is far from solved. Self-training im-
proves over the baseline by about 0.6% on the de-
velopment set. However the gains from self-training
are more modest (0.2%) on the evaluation (test) set.
Our approach is able to provide a more solid im-
provement of about 3% absolute over the super-
vised baseline and about 2% absolute over the self-
trained system on the question development set. Un-
like self-training, on the question evaluation set, our
approach provides about 3% absolute improvement
over the supervised baseline. For the biomedical
data, while the performances of our approach and
self-training are statistically indistinguishable on the
development set, we see modest gains of about 0.5%
absolute on the evaluation set. On the same data, we
see that our approach provides about 1.4% absolute
improvement over the supervised baseline.
7 Analysis & Conclusion
The results suggest that our proposed approach pro-
vides higher gains relative to self-training on the
question data than on the biomedical corpus. We
hypothesize that this caused by sparsity in the graph
generated from the biomedical dataset. For the ques-
tions graph, the PMI statistics were estimated over
10 million sentences while in the case of the biomed-
ical dataset, the same statistics were computed over
just 100,000 sentences. We hypothesize that the lack
of well-estimated features in the case of the biomed-
ical dataset leads to a sparse graph.
To verify the above hypothesis, we measured the
percentage of trigrams that occur in the target do-
main (unlabeled) data that do not have any path to
a trigram in the source domain data, and the aver-
age minimum path length between a trigram in the
target data and a trigram in the source data (when
such a path exists). The results are shown in Ta-
ble 3. For the biomedical data, close to 50% of the
trigrams from the target data do not have a path to
a trigram from the source data. Even when such a
path exists, the average path length is about 22. On
174
Questions Bio
% of unlabeled trigrams
12.4 46.8not connected to
any labeled trigrams
average path length
9.4 22.4
between an unlabeled
trigram and its nearest
labeled trigram
Table 3: Analysis of the graphs constructed for the two
datasets discussed in Section 6. Unlabeled trigrams occur
in the target domain only. Labeled trigrams occur at least
once in the WSJ training data.
the other hand, for the question corpus, only about
12% of the target domain trigrams are disconnected,
and the average path length is about 9. These re-
sults clearly show the sparse nature of the biomed-
ical graph. We believe that it is this sparsity that
causes the graph propagation to not have a more no-
ticeable effect on the final performance. It is note-
worthy that making use of even such a sparse graph
does not lead to any degradation in results, which we
attribute to the choice of graph-propagation regular-
izer (Section 4.3).
We presented a simple, scalable algorithm for
training structured prediction models in a semi-
supervised manner. The approach is based on using
as a regularizer a nearest-neighbor graph constructed
over trigram types. Our results show that the ap-
proach not only scales to large datasets but also pro-
duces significantly improved tagging accuracies.
References
A. Alexandrescu and K. Kirchhoff. 2009. Graph-based
learning for statistical machine translation. In NAACL.
Y. Altun, D. McAllester, and M. Belkin. 2005. Max-
imum margin semi-supervised learning for structured
variables. In Advances in Neural Information Process-
ing Systems 18, page 18.
M. Belkin, P. Niyogi, and V. Sindhwani. 2005. On man-
ifold regularization. In Proc. of the Conference on Ar-
tificial Intelligence and Statistics (AISTATS).
Y. Bengio, O. Delalleau, and N. L. Roux, 2007. Semi-
Supervised Learning, chapter Label Propogation and
Quadratic Criterion. MIT Press.
D Bertsekas. 2004. Nonlinear Programming. Athena
Scientific Publishing.
J. Blitzer and J. Zhu. 2008. ACL 2008 tutorial on Semi-
Supervised learning.
J. Blitzer, R. McDonald, and F. Pereira. 2006. Domain
adaptation with structural correspondence learning. In
EMNLP ?06.
A. Blum and T. Mitchell. 1998. Combining labeled and
unlabeled data with co-training. In COLT: Proceed-
ings of the Workshop on Computational Learning The-
ory.
U. Brefeld and T. Scheffer. 2006. Semi-supervised learn-
ing for structured output variables. In ICML06, 23rd
International Conference on Machine Learning.
O. Chapelle, B. Scholkopf, and A. Zien. 2007. Semi-
Supervised Learning. MIT Press.
R. Collobert, F. Sinz, J. Weston, L. Bottou, and
T. Joachims. 2006. Large scale transductive svms.
Journal of Machine Learning Research.
A. Corduneanu and T. Jaakkola. 2003. On informa-
tion regularization. In Uncertainty in Artificial Intelli-
gence.
Y. Grandvalet and Y. Bengio. 2005. Semi-supervised
learning by entropy minimization. In CAP.
R. Gupta, S. Sarawagi, and A. A. Diwan. 2009. General-
ized collective inference with symmetric clique poten-
tials. CoRR, abs/0907.0589.
G. R. Haffari and A. Sarkar. 2007. Analysis of semi-
supervised learning with the Yarowsky algorithm. In
UAI.
F. Huang and A. Yates. 2009. Distributional represen-
tations for handling sparsity in supervised sequence-
labeling. In ACL-IJCNLP ?09: Proceedings of the
Joint Conference of the 47th Annual Meeting of the
ACL and the 4th International Joint Conference on
Natural Language Processing of the AFNLP: Volume
1. Association for Computational Linguistics.
H. Daume III. 2007. Frustratingly easy domain adapta-
tion. In Proceedings of the 45th Annual Meeting of
the Association of Computational Linguistics, pages
256?263, Prague, Czech Republic, June. Association
for Computational Linguistics.
T. Joachims. 1999. Transductive inference for text clas-
sification using support vector machines. In Proc. of
the International Conference on Machine Learning
(ICML).
Thorsten Joachims. 2003. Transductive learning via
spectral graph partitioning. In Proc. of the Interna-
tional Conference on Machine Learning (ICML).
J. Judge, A. Cahill, and J. van Genabith. 2006. Question-
bank: Creating a corpus of parse-annotated questions.
In Proceedings of the 21st International Conference
on Computational Linguist ics and 44th Annual Meet-
ing of the Association for Computational Linguistics,
pages 497?504.
175
J. Lafferty, A. McCallum, and F. Pereira. 2001. Con-
ditional random fields: Probabilistic models for seg-
menting and labeling sequence data. In Proc. of the In-
ternational Conference on Machine Learning (ICML).
N. D. Lawrence and M. I. Jordan. 2005. Semi-supervised
learning via gaussian processes. In NIPS.
PennBioIE. 2005. Mining the bibliome project. In
http://bioie.ldc.upenn.edu/.
H. J. Scudder. 1965. Probability of Error of some Adap-
tive Pattern-Recognition Machines. IEEE Transac-
tions on Information Theory, 11.
M. Seeger. 2000. Learning with labeled and unlabeled
data. Technical report, University of Edinburgh, U.K.
L. Shen, G. Satta, and A. Joshi. 2007. Guided learning
for bidirectional sequence classification. In ACL ?07.
V. Sindhwani, P. Niyogi, and M. Belkin. 2005. Beyond
the point cloud: from transductive to semi-supervised
learning. In Proc. of the International Conference on
Machine Learning (ICML).
A. Subramanya and J. A. Bilmes. 2009. Entropic graph
regularization in non-parametric semi-supervised clas-
sification. In Neural Information Processing Society
(NIPS), Vancouver, Canada, December.
K. Toutanova, D. Klein, C. D. Manning, and Y. Singer.
2003. Feature-rich part-of-speech tagging with a
cyclic dependency network. In HLT-NAACL ?03.
Y. Wang, G. Haffari, S. Wang, and G. Mori. 2009.
A rate distortion approach for semi-supervised condi-
tional random fields.
D. Yarowsky. 1995. Unsupervised word sense disam-
biguation rivaling supervised methods. In Proceed-
ings of the 33rd Annual Meeting of the Association for
Computational Linguistics.
X. Zhu, Z. Ghahramani, and J. Lafferty. 2003. Semi-
supervised learning using gaussian fields and har-
monic functions. In Proc. of the International Con-
ference on Machine Learning (ICML).
X. Zhu. 2005. Semi-supervised learning literature sur-
vey. Technical Report 1530, Computer Sciences, Uni-
versity of Wisconsin-Madison.
176
Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural
Language Learning, pages 1017?1026, Jeju Island, Korea, 12?14 July 2012. c?2012 Association for Computational Linguistics
Reading The Web with Learned Syntactic-Semantic Inference Rules
Ni Lao1?, Amarnag Subramanya2, Fernando Pereira2, William W. Cohen1
1Carnegie Mellon University, 5000 Forbes Avenue, Pittsburgh, PA 15213, USA
2Google Research, 1600 Amphitheatre Parkway, Mountain View, CA 94043, USA
nlao@cs.cmu.edu, {asubram, pereira}@google.com, wcohen@cs.cmu.edu
Abstract
We study how to extend a large knowledge
base (Freebase) by reading relational informa-
tion from a large Web text corpus. Previous
studies on extracting relational knowledge
from text show the potential of syntactic
patterns for extraction, but they do not exploit
background knowledge of other relations
in the knowledge base. We describe a
distributed, Web-scale implementation of a
path-constrained random walk model that
learns syntactic-semantic inference rules for
binary relations from a graph representation
of the parsed text and the knowledge base.
Experiments show significant accuracy im-
provements in binary relation prediction over
methods that consider only text, or only the
existing knowledge base.
1 Introduction
Manually-created knowledge bases (KBs) often lack
basic information about some entities and their
relationships, either because the information was
missing in the initial sources used to create the
KB, or because human curators were not confident
about the status of some putative fact, and so they
excluded it from the KB. For instance, as we will
see in more detail later, many person entries in
Freebase (Bollacker et al 2008) lack nationality
information. To fill those KB gaps, we might use
general rules, ideally automatically learned, such as
?if person was born in town and town is in country
?This research was carried out during an internship at
Google Research
then the person is a national of the country.? Of
course, rules like this may be defeasible, in this case
for example because of naturalization or political
changes. Nevertheless, many such imperfect rules
can be learned and combined to yield useful KB
completions, as demonstrated in particular with the
Path-Ranking Algorithm (PRA) (Lao and Cohen,
2010; Lao et al 2011), which learns such rules on
heterogenous graphs for link prediction tasks.
Alternatively, we may attempt to fill KB gaps by
applying relation extraction rules to free text. For
instance, Snow et al(2005) and Suchanek et al
(2006) showed the value of syntactic patterns in
extracting specific relations. In those approaches,
KB tuples of the relation to be extracted serve as
positive training examples to the extraction rule
induction algorithm. However, the KB contains
much more knowledge about other relations that
could potentially be helpful in improving relation
extraction accuracy and coverage, but that is not
used in such purely text-based approaches.
In this work, we use PRA to learn weighted
rules (represented as graph path patterns) that
combine both semantic (KB) and syntactic infor-
mation encoded respectively as edges in a graph-
structured KB, and as syntactic dependency edges
in dependency-parsed Web text. Our approach can
easily incorporate existing knowledge in extraction
tasks, and its distributed implementation scales to
the whole of the Freebase KB and 60 million parsed
documents. To the best of our knowledge, this is the
first successful attempt to apply relational learning
methods to heterogeneous data with this scale.
1017
1.1 Terminology and Notation
In this study, we use a simplified KB consisting of a
set C of concepts and a set R of labels. Each label r
denotes some binary relation partially represented in
the KB. The concrete KB is a directed, edge-labeled
graph G = (C, T ) where T ? C ? R ? C is the
set of labeled edges (also known as triples) (c, r, c?).
Each triple represents an instance r(c, c?) of the
relation r ? R. The KB may be incomplete, that
is, r(c, c?) holds in the real world but (c, r, c?) 6? T .
Our method will attempt to learn rules to infer such
missing relation instances by combining the KB
with parsed text.
We denote by r?1 the inverse relation of r:
r(c, c?) ? r?1(c?, c). For instance Parent?1 is
equivalent to Children. It is convenient to take G
as containing triple (c?, r?1, c) whenever it contains
triple (c, r, c?).
A path type in G is a sequence pi = ?r1, . . . , rm?.
An instance of the path type is a sequence of nodes
c0, . . . , cm such that ri(ci?1, ci). For instance, ?the
persons who were born in the same town as the
query person?, and ?the nationalities of persons who
were born in the same town as the query person? can
be reached respectively through paths matching the
following types
pi1 :
?
BornIn,BornIn?1
?
pi2 :
?
BornIn,BornIn?1,Nationality
?
1.2 Learning Syntactic-Semantic Rules with
Path-Constrained Random Walks
Given a query concept s ? C and a relation
r ? R, PRA begins by enumerating a large set of
bounded-length path types. These path types are
treated as ranking ?experts,? each generating some
random instance of the path type starting from s, and
ranking end nodes t by their weights in the resulting
distribution. Finally, PRA combines the weights
contributed by different ?experts? by using logistic
regression to predict the probability that the relation
r(s, t) holds.
In this study, we test the hypothesis that PRA can
be used to find useful ?syntactic-semantic patterns?
? that is, patterns that exploit both semantic
and syntactic relationships, thereby using semantic
knowledge as background in interpreting syntactic
 
wrote
She
Mention
dobj
Charlotte
was
nsubj
nsubj
Jane Eyre
Charlotte
Bronte
Mention
Jane Eyre
Mention
Coreference Resolution
Entity 
Resolution
Freebase
News Corpus
Dependency Trees
Write
Patrick Bront?HasFather
?
Profession
Writer
Figure 1: Knowledge base and parsed text as a labeled
graph. For clarity, some word nodes are omitted.
relationships. As shown in Figure 1, we extend the
KB graph G with nodes and edges from text that
has been syntactically analyzed with a dependency
parser1 and where pronouns and other anaphoric
referring expressions have been clustered with their
antecedents. The text nodes are word/phrase
instances, and the edges are syntactic dependencies
labeled by the corresponding dependency type.
Mentions of entities in the text are linked to KB
concepts by mention edges created by an entity
resolution process.
Given for instance the query
Profession(CharlotteBronte, ?), PRA produces
a ranked list of answers that may have the relation
Profession with the query node CharlotteBronte.
The features used to score answers are the
random walk probabilities of reaching a certain
profession node from the query node by paths
with particular path types. PRA can learn path
types that combine background knowledge in
the database with syntactic patterns in the text
corpus. We now exemplify some path types
involving relations described in Table 3. Type
?
M, conj,M?1,Profession
?
is active (matches
paths) for professions of persons who are mentioned
in conjunction with the query person as in
?collaboration between McDougall and Simon
1Stanford dependencies (de Marneffe and Manning, 2008).
1018
Philips?. For a somewhat subtler example, type
?
M,TW,CW?1,Profession?1,Profession
?
is active
for persons who are mentioned by their titles as in
?President Barack Obama?. The type subsequence
?
Profession?1,Profession
?
ensures that only
profession concepts are activated. The features
generated from these path types combine syntactic
dependency relations (conj) and textual information
relations (TW and CW) with semantic relations in
the KB (Profession).
Experiments on three Freebase relations (profes-
sion, nationality and parents) show that exploiting
existing background knowledge as path features
can significantly improve the quality of extraction
compared with using either Freebase or the text
corpus alone.
1.3 Related Work
Information extraction from varied unstructured and
structured sources involves both complex relational
structure and uncertainty at all levels of the extrac-
tion process. Statistical Relational Learning (SRL)
seeks to combine statistical and relational learning
methods to address such tasks. However, most SRL
approaches (Friedman et al 1999; Richardson and
Domingos, 2006) suffer the complexity of inference
and learning when applied to large scale problems.
Recently, Lao and Cohen (2010) introduced Path
Ranking algorithm, which is applicable to larger
scale problems such as literature recommendation
(Lao and Cohen, 2010) and inference on a large
knowledge base (Lao et al 2011).
Much of the previous work on automatic relation
extraction was based on certain lexico-syntactic
patterns. Hearst (1992) first noticed that patterns
such as ?NP and other NP? and ?NP such as NP?
often imply hyponym relations (NP here refers to
a noun phrase). However, such approaches to
relation extraction are limited by the availability of
domain knowledge. Later systems for extracting
arbitrary relations from text mostly use shallow
surface text patterns (Etzioni et al 2004; Agichtein
and Gravano, 2000; Ravichandran and Hovy, 2002).
The idea of using sequences of dependency edges
as features for relation extraction was explored by
Snow et al(2005) and Suchanek et al(2006). They
define features to be shortest paths on dependency
trees which connect pairs of NP candidates.
This study is most closely related to work of
Mintz et al(2009), who also study the problem of
extending Freebase with extraction from parsed text.
As in our work, they use a logistic regression model
with path features. However, their approach does not
exploit existing knowledge in the KB. Furthermore,
their path patterns are used as binary-values features.
We show experimentally that fractional-valued
features generated by random walks provide much
higher accuracy than binary-valued ones.
Culotta et al(2006)?s work is similar to our
approach in the sense of relation extraction by
discovering relational patterns. However while
they focus on identifying relation mentions in text
(microreading),this work attempts to infer new
tuples by gathering path evidence over the whole
corpus (macroreading). In addition, their work
involves a few thousand examples, while we aim for
Web-scale extraction.
Do and Roth (2010) use a KB (YAGO) to
aid the generation of features from free text.
However their method is designed specifically for
extracting hierarchical taxonomic structures, while
our algorithm can be used to discover relations for
general general graph-based KBs.
In this paper we extend the PRA algorithm along
two dimensions: combining syntactic and semantic
cues in text with existing knowledge in the KB;
and a distributed implementation of the learning and
inference algorithms that works at Web scale.
2 Path Ranking Algorithm
We briefly review the Path Ranking algorithm
(PRA), described in more detail by Lao and Cohen
(2010). Each path type pi = ?r1, r2, ..., r`? specifies
a real-valued feature. For a given query-answer node
pair (s, t), the value of the feature pi is P (s? t;pi),
the probability of reaching t from s by a random
walk that instantiates the type. More specifically,
suppose that the random walk has just reached vi by
traversing edges labeled r1, . . . , ri with s=v0. Then
vi+1 is drawn at random from all nodes reachable
from vi by edges labeled ri+1. A path type pi is
active for pair (s, t) if P (s? t;pi) > 0.
Let B = {?, pi1, ..., pin} be the set of all path
types of length no greater than ` that occur in
the graph together with the dummy type ?, which
1019
represents the bias feature. For convenience, we set
P (s ? t;?) = 1 for any nodes s, t. The score for
whether query node s is related to another node t by
relation r is given by
score(s, t) =
?
pi?B
P (s? t;pi)?pi ,
where ?pi is the weight of feature pi. The model
parameters to be learned are the vector ? =
??pi?pi?B . The procedures used to discover B and
estimate ? are described in the following. Finally,
note that we train a separate PRA model for each
relation r.
Path Discovery: Given a graph and a target
relation r, the total number of path types is an
exponential function of the maximum path length
` and considering all possible paths would be
computationally very expensive. As a result, B is
constructed using only path types that satisfy the
following two constraints:
1. the path type is active for more than K training
query nodes, and
2. the probability of reaching any correct target
node t is larger than a threshold ? on average
for the training query nodes s.
We will discuss how K, ? and the training queries
are chosen in Section 5. In addition to making the
training more efficient, these constraints are also
helpful in removing low quality path types.
Training Examples: For each relation r of inter-
est, we start with a set of node pairs Sr = {(si, ti)}.
From Sr, we create the training setDr = {(xi, yi)},
where xi = ?P (si ? ti;pi)?pi?B is the vector
of path feature values for the pair (si, ti), and yi
indicates whether r(si, ti) holds.
Following previous work (Lao and Cohen, 2010;
Mintz et al 2009), node pairs that are in r in
the KB are legitimate positive training examples2.
One can generate negative training examples by
considering all possible pairs of concepts whose
type is compatible with r (as given by the schema)
and are not present in the KB. However this
2In our experiments we subsample the positive examples.
See section 3.2 for more details.
procedure leads to a very large number of negative
examples (e.g., for the parents relation, any pair of
person concepts which are related by this relation
would be valid negative examples) which not only
makes training very expensive but also introduces
an incorrect bias in the training set. Following
Lao and Cohen (2010) we use a simple biased
sampling procedure to generate negative examples:
first, the path types discovered in the previous (path
discovery) step are used to construct an initial PRA
model (all feature weights are set to 1.0); then, for
each query node si, this model is used to retrieve
candidate answer nodes, which are then sorted in
descending order by their scores; finally, nodes at
the k(k + 1)/2-th positions are selected as negative
samples, where k = 0, 1, 2, ....
Logistic Regression Training: Given a training
set D, we estimate parameters ? by maximizing the
following objective
F(?) =
1
|D|
?
(x,y)?D
f(x, y;?)? ?1???1 ? ?2???22
where ?1 and ?2 control the strength of the L1-
regularization which helps with structure selection
and L22-regularization which prevents overfitting.
The log-likelihood f(x, y;?) of example (x, y) is
given by
f(x, y,?) = y ln p(x,?) + (1? y) ln(1? p(x,?))
p(x,?) =
exp(?Tx)
1 + exp(?Tx)
.
Inference: After a model is trained for a relation
r in the knowledge base, it can be used to produce
new instances of r. We first generate unlabeled
queries s which belong to the domain of r. Queries
which appear in the training set are excluded. For
each unlabeled query node s, we apply the trained
PRA model to generate a list of candidate t nodes
together with their scores. We then sort all the
predictions (s, t) by their scores in descending order,
and evaluate the top ones.
3 Extending PRA
As described in the previous section, the PRA model
is trained on positive and negative queries generated
from the KB. As Freebase contains millions of
1020
concepts and edges, training on all the generated
queries is computationally challenging. Further,
we extend the Freebase graph with parse paths of
mentions of concepts in Freebase in millions of Web
pages. Yet another issue is that the training queries
generated using Freebase are inherently biased
towards the distribution of concepts in Freebase
and may not reflect the distribution of mentions of
these concepts in text data. As one of the goals of
our approach is to learn relation instances that are
missing in Freebase, training on such a set biased
towards the distribution of concepts in Freebase may
not lead to good performance. In this section we
explain how we modified the PRA algorithm to
address those issues.
3.1 Scaling Up
Most relations in Freebase have a large set of
existing triples. For example, for the profession
relation, there are around 2 million persons in
Freebase, and about 0.3 million of them have known
professions. This results in more than 0.3 million
training queries (persons), each with one or more
positive answers (professions), and many negative
answers, which make training computationally
challenging. Generating all the paths for millions
of queries over a graph with millions of concepts
and edges further complicates the computational
issues. Incorporating the parse path features from
the text only exacerbates the matter. Finally once we
have trained a PRA model for a given relation, say
profession, we would like to infer the professions for
all the 1.7 million persons whose professions are not
known to Freebase (and possibly predict changes to
the profession information of the 0.3 million people
whose professions were given).
We use distributed computing to deal with the
large number of training and prediction queries
over a large graph. A key observation is that the
different stages of the PRA algorithm are based
on independent computations involving individual
queries. Therefore, we can use the MapReduce
framework to distribute the computation (Dean and
Ghemawat, 2008). For path discovery, we modify
Lao et als path finding (2011) approach to decouple
the queries: instead of using one depth-first search
that involves all the queries, we first find all paths
up to certain length for each query node in the
map stage, and then collect statistics for each path
from all the query nodes in the reduce stage. We
used a 500-machine, 8GB/machine cluster for these
computations.
Another challenge associated with applying PRA
to a graph constructed using a large amounts of
text is that we cannot load the entire graph on a
single machine. To circumvent this problem, we first
index all parsed sentences by the concepts that they
mention. Therefore, to perform a random walk for a
query concept s, we only load the sentences which
mention s.
3.2 Sampling Training Data
Using the r-edges in the KB as positive examples
distorts the training set. For example, for the
profession relation, there are 0.3 million persons
for whom Freebase has profession information, and
amongst these 0.24 million are either politicians
or actors. This may not reflect the distribution
of professions of persons mentioned in Web data.
Using all of these as training queries will most
certainly bias the trained model towards these
professions as PRA is trained discriminatively. In
other words, training directly with this data would
lead to a model that is more likely to predict
professions that are popular in Freebase. To avoid
this distortion, we use stratified sampling. For each
relation r and concept t ? C, we count the number
of r edges pointing to t
Nr,t = |{(s, r, t) ? T}| .
Given a training query (s, r, t) we sample it
according to
Pr,t = min
(
1,
?
m+Nr,t
Nr,t
)
We fix m = 100 in our experiments. If we take the
profession relation as an example, the above implies
that for popular professions, we only sample about
?
Nr,t out of the Nr,t possible queries that end in t,
whereas for the less popular professions we would
accept all the training queries.
3.3 Text Graph Construction
As we are processing Web text data (see following
section for more detail), the number of mentions
1021
of a concept follows a somewhat heavy-tailed
distribution: there are a small number of very
popular concepts (head) and a large number of not
so popular concepts (tail). For instance the concept
BarackObama is mentioned about 8.9 million times
in our text corpus. To prevent the text graph from
being dominated by the head concepts, for each
sentence that mentions concept c ? C, we accept
it as part of the text graph with probability:
Pc = min
(
1,
?
k + Sc
Sc
)
where Sc is the number of sentences in which c is
mentioned in the whole corpus. In our experiments
we use k = 105. This means that if Sc  k, then we
only sample about
?
Sc of the sentences that contain
a mention of the concept, while if Sc  k, then all
mentions of that concept will likely be included.
4 Datasets
We use Freebase as our knowledge base. Freebase
data is harvested from many sources, including
Wikipedia, AMG, and IMDB.3 As of this writing,
it contains more than 21 million concepts and 70
million labeled edges. For a large majority of con-
cepts that appear both in Freebase and Wikipedia,
Freebase maintains a link to the Wikipedia page of
that concept.
We also collect a large Web corpus and identify
60 million pages that mention concepts relevant
to this study. The free text on those pages
are POS-tagged and dependency parsed with an
accuracy comparable to that of the current Stanford
dependency parser (Klein and Manning, 2003). The
parser produces a dependency tree for each sentence
with each edge labeled with a standard dependency
tag (see Figure 1).
In each of the parsed documents, we use POS tags
and dependency edges to identify potential referring
noun phrases (NPs). We then use a within-document
coreference resolver comparable to that of Haghighi
and Klein (2009) to group referring NPs into
co-referring clusters. For each cluster that contains a
proper-name mention, we find the Freebase concept
or concepts, if any, with a name or alias that matches
3www.wikipedia.org, www.allmusic.com, www.
imdb.com.
Table 1: Size of training and test sets for each relation.
Task Training Set Test Set
Profession 22,829 15,219
Nationality 14,431 9,620
Parents 21,232 14,155
the mention. If a cluster has multiple possible
matching Freebase concepts, we choose a single
sense based on the following simple model. For
each Freebase concept c ? C, we computeN(c,m),
the number of times the concept c is referred by
mention m by using both the alias information
in Freebase and the anchors of the corresponding
Wikipedia page for that concept. Based on N(c,m)
we can calculate the empirical probability p(c|m) =
N(c,m)/
?
c? N(c
?,m). If u is a cluster with
mention set M(u) in the document, and C(m) the
set of concepts in KB with name or alias m, we
assign u to concept c? = argmax
c?C(m),m?M(u)
p(c|m),
provided that there exists at least one c ? C(m) and
m ? M(u) such that p(c|m) > 0. Note that M(c)
only contains the proper-name mentions in cluster c.
5 Results
We use three relations profession, nationality and
parents for our experiments. For each relation, we
select its current set of triples in Freebase, and apply
the stratified sampling (Section 3.2) to each of the
three triple sets. The resulting triple sets are then
randomly split into training (60% of the triples) and
test (the remaining triples). However, the parents
relation yields 350k triples after stratified sampling,
so to reduce experimental effort we further randomly
sub-sample 10% of that as input to the train-test
split. Table 1 shows the sizes of the training and
test sets for each relation.
To encourage PRA to find paths involving the
text corpus, we do not count relation M (which
connects concepts to their mentions) or M?1 when
calculating path lengths. We use L1/L22-regularized
logistic regression to learn feature weights. The
PRA hyperparameters (? and K as defined in
Section 2) and regularizer hyperparameters are
tuned by threefold cross validation (CV) on the
training set. We average the models across all
the folds and choose the model that gives the best
1022
Table 2: Mean Reciprocal Rank (MRR) for different approaches under closed-world assumption. Here KB, Text and
KB+Text columns represent results obtained by training a PRA model with only the KB, only text, and both KB and
text. KB+Text[b] is the binarized PRA approach trained on both KB and text. The best performing system (results
shown in bold font) is significant at 0.0001 level over its nearest competitor according to a difference of proportions
significance test.
Task KB Text KB+Text KB+Text[b]
Profession 0.532 0.516 0.583 0.453
Nationality 0.734 0.729 0.812 0.693
Parents 0.329 0.332 0.392 0.319
performance on the training set for each relation.
We report results of two evaluations. First, we
evaluate the performance of the PRA algorithm
when trained on a subset of existing Freebase facts
and tested on the rest. Second, we had human
annotators verify facts proposed by PRA that are not
in Freebase.
5.1 Evaluation with Existing Knowledge
Previous work in relation extraction from parsed
text (Mintz et al 2009) has mostly used binary
features to indicate whether a pattern is present in
the sentences where two concepts are mentioned.
To investigate the benefit of having fractional valued
features generated by random walks (as in PRA), we
also evaluate a binarized PRA approach, for which
we use the same syntactic-semantic pattern features
as PRA does, but binarize the feature values from
PRA: if the original fractional feature value was
zero, the feature value is set to zero (equivalent to
not having the feature in that example), otherwise it
is set to 1.
Table 2 shows a comparison of the results
obtained using the PRA algorithm trained using
only Freebase (KB), using only the text corpus
graph (Text), trained with both Freebase and the
text corpus (KB+Text) and the binarized PRA
algorithm using both Freebase and the text corpus
(KB+Text[b]). We report Mean Reciprocal Rank
(MRR) where, given a set of queries Q,
MRR =
1
|Q|
?
q?Q
1
rank of q?s first correct answer
.
Comparing the results of first three columns we
see that combining Freebase and text achieves
significantly better results than using either Freebase
or text alone. Further comparing the results of last
two columns we also observe a significant drop in
MRR for the binarized version of PRA. This clearly
shows the importance of using the random walk
probabilities. It can also be seen that the MRR for
the parents relation is lower than those for other
relations. This is mainly because there are larger
number of potential answers for each query node of
Parent relation than for each query node of the other
two relations ? all persons in Freebase versus all
professions or nationalities. Finally, it is important
to point out that our evaluations are actually lower
bounds of actual performance, because, for instance,
a person might have a profession besides the ones in
Freebase and in such cases, this evaluation does not
give any credit for predicting those professions ?
they are treated as errors. We try to address this issue
with the manual evaluations in the next section.
Table 2 only reports results for the maximum path
length ` = 4 case. We found that shorter maximum
path lengths give worse results: for instance, with
` = 3 for the profession relation, MRR drops to
0.542, from 0.583 for ` = 4 when using both
Freebase and text. This difference is significant
at the 0.0001 level according to a difference of
proportions test. Further we find that using longer
path length takes much longer time to train and test,
but does not lead to significant improvements over
the ` = 4 case. For example, for profession, ` = 5
gives a MRR of 0.589.
Table 3 shows the top weighted features that
involve text edges for PRA models trained on both
Freebase and the text corpus. To make them
easier to understand, we group them based on their
functionality. For the profession and nationality
tasks, the conjunction dependency relation (in group
1,4) plays an important role: these features first find
persons mentioned in conjunction with the query
1023
Table 3: Top weighted path types involving text edges for each task grouped according to functionality. M relations
connect each concept in knowledge base to its mentions in the corpus. TW relations connect each token in a sentence to
the words in the text representation of this token. CW relations connect each concept in knowledge base to the words
in the text representation of this concept. We use lower case names to denote dependency edges, word capitalized
names to denote KB edges, and ??1 ? to denote the inverse of a relation.
Profession Top Weighted Features Comments
1
?
M, conj,M?1,Profession
?
Professions of persons mentioned in conjunction
with the query person: ?McDougall and Simon
Phillips collaborated ...?
?
M, conj?1,M?1,Profession
?
2
?
M,TW,CW?1,Profession?1,Profession
?
Active if a person is mentioned by his profession:
?The president said ...?
3
?
M,TW,TW?1,M?1,Children,Profession
?
First find persons with similar names or
mentioned in similar ways, then aggregate the
professions of their children/parents/advisors:
starting from the concept BarackObama, words
such as ?Obama?, ?leader?, ?president?, and
?he? are reachable through path ?M,TW?
?
M,TW,TW?1,M?1,Parents,Profession
?
?
M,TW,TW?1,M?1,Advisors,Profession
?
Nationality Top Weighted Features Comments
4
?
M, conj,TW,CW?1,Nationality
?
The nationalities of persons mentioned in
conjunction with the query person: ?McDougall
and Simon Phillips collaborated ...?
?
M, conj?1,TW,CW?1,Nationality
?
5
?
M, nc?1,TW,CW?1,Nationality
?
The nationalities of persons mentioned close to
the query person through other dependency
relations.
?
M, tmod?1,TW,CW?1,Nationality
?
?
M, nn,TW,CW?1,Nationality
?
6
?
M, poss, poss?1,M?1,PlaceOfBirth,ContainedBy
?
The birth/death places of the query person with
restrictions to different syntactic constructions.
?
M, title, title?1,M?1,PlaceOfDeath,ContainedBy
?
Parents Top Weighted Features Comments
7
?
M,TW,CW?1,Parents
?
The parents of persons with similar names or
mentioned in similar ways: starting from the
concept CharlotteBronte words such as
?Bronte?, ?Charlotte?, ?Patrick??, and ?she? are
reachable through path ?M,TW?.
8
?
M, nsubj, nsubj?1,TW,CW?1
?
Persons with similar names or mentioned in
similar ways to the query person with various
restrictions or expansions.
?
nsubj, nsubj?1
?
and
?
nc?1, nc
?
require the query to be subject and
noun compound respectively.
?
TW?1,TW
?
expands further by word similarities.
?
M, nsubj, nsubj?1,M?1,CW,CW?1
?
?
M, nc?1, nc,TW,CW?1
?
?
M,TW,CW?1
?
?
M,TW,TW?1,TW,CW?1
?
1024
person, and then find their professions or nation-
alities. The features in group 2 capture the fact
that sometimes people are mentioned by their pro-
fessions. The subpath
?
Profession?1,Profession
?
ensures that only profession related concepts are
activated. Features in group 3 first find persons
with similar names or mentioned in similar ways
to the query person, and then aggregate the
professions of their children, parents, or advisors.
Features in group 6 can be seen as special
versions of feature ?PlaceOfBirth,ContainedBy?
and ?PlaceOfDeath,ContainedBy?. The subpaths
?
M, poss, poss?1,M?1
?
and
?
M, title, title?1,M?1
?
return the random walks back to the query node only
if the mentions of the query node have poss (stands
for possessive modifier, e.g. ?Bill?s clothes?) or title
(stands for person?s title, e.g. ?President Obama?)
edges in text; otherwise these features are inactive.
Therefore, these features are active only for specific
subsets of queries. Features in group 8 generally find
persons with similar names or mentioned in similar
ways to the query person. However, they further
expand or restrict this person set in various ways.
Typically, each trained model includes hundreds
of paths with non-zero weights, so the bulk of
classifications are not based on a few high-precision-
recall patterns, but rather on the combination of
a large number of lower-precision high-recall or
high-precision lower-recall rules.
5.2 Manual Evaluation
We performed two sets of manual evaluations. In
each case, an annotator is presented with the triples
predicted by PRA, and asked if they are correct. The
annotator has access to the Freebase and Wikipedia
pages for the concepts (and is able to issue search
queries about the concepts).
In the first evaluation, we compared the perfor-
mance of two PRA models, one trained using the
stratified sampled queries and another trained using
a randomly sampled set of queries for the profession
relation. For each model, we randomly sample 100
predictions from the top 1000 predictions (sorted by
the scores returned by the model). We found that the
PRA model trained with stratified sampled queries
has 0.92 precision, while the other model has only
0.84 precision (significant at the 0.02 level). This
shows that stratified sampling leads to improved
Table 4: Human judgement for predicted new beliefs.
Task p@100 p@1k p@10k
Profession 0.97 0.92 0.84
Nationality 0.98 0.97 0.90
Parents 0.86 0.81 0.79
performance.
We also evaluated the new beliefs proposed by
the models trained for all the three relations using
stratified sampled queries. We estimated precision
for the top 100 predictions and randomly sampled
100 predictions each from the top 1,000 and 10,000
predictions. Here we use the PRA model trained
using both KB and text. The results of this
evaluation are shown in Table 4. It can be seen
that the PRA model is able to produce very high
precision predications even when one considers the
top 10,000 predictions.
Finally, note that our model is inductive. For
instance, for the profession relation, we are able to
predict professions for the around 2 million persons
in Freebase. The top 1000 profession facts extracted
by our system involve 970 distinct people, the top
10,000 facts involve 8,726 distinct people, and the
top 100,000 facts involve 79,885 people.
6 Conclusion
We have shown that path constrained random walk
models can effectively infer new beliefs from a
large scale parsed text corpus with background
knowledge. Evaluation by human annotators shows
that by combining syntactic patterns in parsed
text with semantic patterns in the background
knowledge, our model can propose new beliefs
with high accuracy. Thus, the proposed random
walk model can be an effective way to automate
knowledge acquisition from the web.
There are several interesting directions to con-
tinue this line of work. First, bidirectional search
from both query and target nodes can be an efficient
way to discover long paths. This would especially
useful for parsed text. Second, relation paths that
contain constant nodes (lexicalized features) and
conjunction of random walk features are potentially
very useful for extraction tasks.
1025
Acknowledgments
We thank Rahul Gupta, Michael Ringgaard, John
Blitzer and the anonymous reviewers for helpful
comments. The first author was supported by a
Google Research grant.
References
Eugene Agichtein and Luis Gravano. 2000. Snowball:
extracting relations from large plain-text collections.
In Proceedings of the fifth ACM conference on Digital
libraries, DL ?00, pages 85?94, New York, NY, USA.
ACM.
Kurt Bollacker, Colin Evans, Praveen Paritosh, Tim
Sturge, and Jamie Taylor. 2008. Freebase: a
collaboratively created graph database for structuring
human knowledge. In Proceedings of the 2008 ACM
SIGMOD international conference on Management of
data, SIGMOD ?08, pages 1247?1250, New York, NY,
USA. ACM.
Aron Culotta, Andrew McCallum, and Jonathan Betz.
2006. Integrating probabilistic extraction models and
data mining to discover relations and patterns in text.
In Proceedings of the Human Language Technology
Conference of the NAACL, Main Conference, pages
296?303, New York City, USA, June. Association for
Computational Linguistics.
Marie-Catherine de Marneffe and Chris Manning.
2008. Stanford dependencies. http:
//www.tex.ac.uk/cgi-bin/texfaq2html?
label=citeURL.
Jeffrey Dean and Sanjay Ghemawat. 2008. Mapreduce:
simplified data processing on large clusters. Commun.
ACM, 51(1):107?113, January.
Quang Do and Dan Roth. 2010. Constraints based
taxonomic relation classification. In Proceedings of
the 2010 Conference on Empirical Methods in Natural
Language Processing, pages 1099?1109, Cambridge,
MA, October. Association for Computational Linguis-
tics.
Oren Etzioni, Michael Cafarella, Doug Downey, Stanley
Kok, Ana-Maria Popescu, Tal Shaked, Stephen
Soderland, Daniel S. Weld, and Alexander Yates.
2004. Web-scale information extraction in knowitall:
(preliminary results). In Proceedings of the 13th
international conference on World Wide Web, WWW
?04, pages 100?110, New York, NY, USA. ACM.
Nir Friedman, Lise Getoor, Daphne Koller, and Avi
Pfeffer. 1999. Learning Probabilistic Relational
Models. In IJCAI, volume 16, pages 1300?1309.
Aria Haghighi and Dan Klein. 2009. Simple coref-
erence resolution with rich syntactic and semantic
features. In Proceedings of the 2009 Conference on
Empirical Methods in Natural Language Processing,
pages 1152?1161, Singapore, August. Association for
Computational Linguistics.
Marti A. Hearst. 1992. Automatic acquisition of
hyponyms from large text corpora. In Proceedings
of COLING-92, pages 539?545. Association for
Computational Linguistics, August.
Dan Klein and Christopher D. Manning. 2003. Accurate
unlexicalized parsing. In Erhard Hinrichs and Dan
Roth, editors, Proceedings of the 41st Annual Meeting
on Association for Computational Linguistics, pages
423?430. Association for Computational Linguistics,
July.
Ni Lao and William Cohen. 2010. Relational retrieval
using a combination of path-constrained random
walks. Machine Learning, 81:53?67.
Ni Lao, Tom Mitchell, and William W. Cohen. 2011.
Random walk inference and learning in a large
scale knowledge base. In Proceedings of the
2011 Conference on Empirical Methods in Natural
Language Processing, pages 529?539, Edinburgh,
Scotland, UK., July. Association for Computational
Linguistics.
Mike Mintz, Steven Bills, Rion Snow, and Daniel
Jurafsky. 2009. Distant supervision for relation
extraction without labeled data. In Proceedings of the
Joint Conference of the 47th Annual Meeting of the
ACL and the 4th International Joint Conference on
Natural Language Processing of the AFNLP, pages
1003?1011, Suntec, Singapore, August. Association
for Computational Linguistics.
Deepak Ravichandran and Eduard Hovy. 2002.
Learning surface text patterns for a question answering
system. In Proceedings of 40th Annual Meeting
of the Association for Computational Linguistics,
pages 41?47, Philadelphia, Pennsylvania, USA, July.
Association for Computational Linguistics.
Matthew Richardson and Pedro Domingos. 2006.
Markov logic networks. Machine Learning, 62:107?
136.
Rion Snow, Daniel Jurafsky, and Andrew Y. Ng. 2005.
Learning syntactic patterns for automatic hypernym
discovery. In Lawrence K. Saul, Yair Weiss, and
Le?on Bottou, editors, Advances in Neural Information
Processing Systems 17, pages 1297?1304, Cambridge,
MA. NIPS Foundation, MIT Press.
Fabian M. Suchanek, Georgiana Ifrim, and Gerhard
Weikum. 2006. Combining linguistic and statistical
analysis to extract relations from web documents. In
Proceedings of the 12th ACM SIGKDD international
conference on Knowledge discovery and data mining,
KDD ?06, pages 712?717, New York, NY, USA.
ACM.
1026
Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 1473?1481,
Uppsala, Sweden, 11-16 July 2010. c?2010 Association for Computational Linguistics
Experiments in Graph-based Semi-Supervised Learning Methods for
Class-Instance Acquisition
Partha Pratim Talukdar?
Search Labs, Microsoft Research
Mountain View, CA 94043
partha@talukdar.net
Fernando Pereira
Google, Inc.
Mountain View, CA 94043
pereira@google.com
Abstract
Graph-based semi-supervised learning
(SSL) algorithms have been successfully
used to extract class-instance pairs from
large unstructured and structured text col-
lections. However, a careful comparison
of different graph-based SSL algorithms
on that task has been lacking. We com-
pare three graph-based SSL algorithms
for class-instance acquisition on a variety
of graphs constructed from different do-
mains. We find that the recently proposed
MAD algorithm is the most effective. We
also show that class-instance extraction
can be significantly improved by adding
semantic information in the form of
instance-attribute edges derived from
an independently developed knowledge
base. All of our code and data will be
made publicly available to encourage
reproducible research in this area.
1 Introduction
Traditionally, named-entity recognition (NER) has
focused on a small number of broad classes such
as person, location, organization. However, those
classes are too coarse to support important ap-
plications such as sense disambiguation, seman-
tic matching, and textual inference in Web search.
For those tasks, we need a much larger inventory
of specific classes and accurate classification of
terms into those classes. While supervised learn-
ing methods perform well for traditional NER,
they are impractical for fine-grained classification
because sufficient labeled data to train classifiers
for all the classes is unavailable and would be very
expensive to obtain.
? Research carried out while at the University of Penn-
sylvania, Philadelphia, PA, USA.
To overcome these difficulties, seed-based in-
formation extraction methods have been devel-
oped over the years (Hearst, 1992; Riloff and
Jones, 1999; Etzioni et al, 2005; Talukdar et
al., 2006; Van Durme and Pas?ca, 2008). Start-
ing with a few seed instances for some classes,
these methods, through analysis of unstructured
text, extract new instances of the same class. This
line of work has evolved to incorporate ideas from
graph-based semi-supervised learning in extrac-
tion from semi-structured text (Wang and Cohen,
2007), and in combining extractions from free
text and from structured sources (Talukdar et al,
2008). The benefits of combining multiple sources
have also been demonstrated recently (Pennac-
chiotti and Pantel, 2009).
We make the following contributions:
? Even though graph-based SSL algorithms
have achieved early success in class-instance
acquisition, there is no study comparing dif-
ferent graph-based SSL methods on this task.
We address this gap with a series of experi-
ments comparing three graph-based SSL al-
gorithms (Section 2) on graphs constructed
from several sources (Metaweb Technolo-
gies, 2009; Banko et al, 2007).
? We investigate whether semantic informa-
tion in the form of instance-attribute edges
derived from an independent knowledge
base (Suchanek et al, 2007) can improve
class-instance acquisition. The intuition be-
hind this is that instances that share attributes
are more likely to belong to the same class.
We demonstrate that instance-attribute edges
significantly improve the accuracy of class-
instance extraction. In addition, useful class-
attribute relationships are learned as a by-
product of this process.
? In contrast to previous studies involving pro-
1473
prietary datasets (Van Durme and Pas?ca,
2008; Talukdar et al, 2008; Pennacchiotti
and Pantel, 2009), all of our experiments use
publicly available datasets and we plan to re-
lease our code1.
In Section 2, we review three graph-based
SSL algorithms that are compared for the class-
instance acquisition task in Section 3. In Section
3.6, we show how additional instance-attribute
based semantic constraints can be used to improve
class-instance acquisition performance. We sum-
marize the results and outline future work in Sec-
tion 4.
2 Graph-based SSL
We now review the three graph-based SSL algo-
rithms for class inference over graphs that we have
evaluated.
2.1 Notation
All the algorithms compute a soft assignment of
labels to the nodes of a graph G = (V,E,W ),
where V is the set of nodes with |V | = n, E is
the set of edges, and W is an edge weight ma-
trix. Out of the n = nl + nu nodes in G, nl
nodes are labeled, while the remaining nu nodes
are unlabeled. If edge (u, v) 6? E, Wuv = 0.
The (unnormalized) Laplacian, L, ofG is given by
L = D?W , whereD is an n?n diagonal degree
matrix with Duu =
?
vWuv. Let S be an n ? n
diagonal matrix with Suu = 1 iff node u ? V is
labeled. That is, S identifies the labeled nodes in
the graph. C is the set of labels, with |C| = m
representing the total number of labels. Y is the
n ? m matrix storing training label information,
if any. Y? is an n ?m matrix of soft label assign-
ments, with Y?vl representing the score of label l
on node v. A graph-based SSL computes Y? from
{G,SY }.
2.2 Label Propagation (LP-ZGL)
The label propagation method presented by Zhu
et al (2003), which we shall refer to as LP-ZGL
in this paper, is one of the first graph-based SSL
methods. The objective minimized by LP-ZGL is:
min
Y?
?
l?C
Y? >l LY?l, s.t. SYl = SY?l (1)
1
http://www.talukdar.net/datasets/class inst/
where Y?l of size n ? 1 is the lth column of Y? .
The constraint SY = SY? makes sure that the su-
pervised labels are not changed during inference.
The above objective can be rewritten as:
?
l?C
Y? >l LY?l =
?
u,v?V,l?C
Wuv(Y?ul ? Y?vl)
2
From this, we observe that LP-ZGL penalizes any
label assignment where two nodes connected by a
highly weighted edge are assigned different labels.
In other words, LP-ZGL prefers smooth labelings
over the graph. This property is also shared by the
two algorithms we shall review next. LP-ZGL has
been the basis for much subsequent work in the
graph-based SSL area, and is still one of the most
effective graph-based SSL algorithms.
2.3 Adsorption
Adsorption (Baluja et al, 2008) is a graph-based
SSL algorithm which has been used for open-
domain class-instance acquisition (Talukdar et al,
2008). Adsorption is an iterative algorithm, where
label estimates on node v in the (t+ 1)th iteration
are updated using estimates from the tth iteration:
Y? (t+1)v ? p
inj
v ?Yv+p
cont
v ?B
(t)
v +p
abnd
v ?r (2)
where,
B(t)v =
?
u
Wuv
?
u? Wu?v
Y? (t)u
In (2), pinjv , p
cont
v , and p
abnd
v are three proba-
bilities defined on each node v ? V by Ad-
sorption; and r is a vector used by Adsorption
to express label uncertainty at a node. On each
node v, the three probabilities sum to one, i.e.,
pinjv + p
cont
v + p
abnd
v = 1, and they are based on
the random-walk interpretation of the Adsorption
algorithm (Talukdar et al, 2008). The main idea
of Adsorption is to control label propagation more
tightly by limiting the amount of information that
passes through a node. For instance, Adsorption
can reduce the importance of a high-degree node
v during the label inference process by increas-
ing pabndv on that node. For more details on these,
please refer to Section 2 of (Talukdar and Cram-
mer, 2009). In contrast to LP-ZGL, Adsorption
allows labels on labeled (seed) nodes to change,
which is desirable in case of noisy input labels.
1474
2.4 Modified Adsorption (MAD)
Talukdar and Crammer (2009) introduced a modi-
fication of Adsorption called MAD, which shares
Adsorption?s desirable properties but can be ex-
pressed as an unconstrained optimization problem:
min
Y?
?
l?C
[
?1
(
Yl ? Y?l
)>
S
(
Yl ? Y?l
)
+
?2Y?
>
l L
?
Y?l + ?3
?
?
?
?
?
?Y?l ?Rl
?
?
?
?
?
?
2
]
(3)
where ?1, ?2, and ?3 are hyperparameters; L
?
is the Laplacian of an undirected graph derived
from G, but with revised edge weights; and R is
an n ? m matrix of per-node label prior, if any,
with Rl representing the lth column of R. As in
Adsorption, MAD allows labels on seed nodes to
change. In case of MAD, the three random-walk
probabilities, pinjv , p
cont
v , and p
abnd
v , defined by
Adsorption on each node are folded inside the ma-
trices S,L
?
, andR, respectively. The optimization
problem in (3) can be solved with an efficient iter-
ative algorithm described in detail by Talukdar and
Crammer (2009).
These three algorithms are all easily paralleliz-
able in a MapReduce framework (Talukdar et al,
2008; Rao and Yarowsky, 2009), which makes
them suitable for SSL on large datasets. Addition-
ally, all three algorithms have similar space and
time complexity.
3 Experiments
We now compare the experimental performance
of the three graph-based SSL algorithms reviewed
in the previous section, using graphs constructed
from a variety of sources described below. Fol-
lowing previous work (Talukdar et al, 2008), we
use Mean Reciprocal Rank (MRR) as the evalua-
tion metric in all experiments:
MRR =
1
|Q|
?
v?Q
1
rv
(4)
where Q ? V is the set of test nodes, and rv is the
rank of the gold label among the labels assigned to
node v. Higher MRR reflects better performance.
We used iterative implementations of the graph-
based SSL algorithms, and the number of itera-
tions was treated as a hyperparameter which was
tuned, along with other hyperparameters, on sep-
arate held-out sets, as detailed in a longer version
of this paper. Statistics of the graphs used during
experiments in this section are presented in Table
1.
3.1 Freebase-1 Graph with Pantel Classes
Table ID: people-person
Name Place of Birth Gender
? ? ? ? ? ? ? ? ?
Isaac Newton Lincolnshire Male
Bob Dylan Duluth Male
Johnny Cash Kingsland Male
? ? ? ? ? ? ? ? ?
Table ID: film-music contributor
Name Film Music Credits
? ? ? ? ? ?
Bob Dylan No Direction Home
? ? ? ? ? ?
Figure 1: Examples of two tables from Freebase,
one table is from the people domain while the
other is from the film domain.
0.5
0.575
0.65
0.725
0.8
23 x 2 23 x 10
Freebase-1 Graph, 23 Pantel Classes
M
e
a
n
 
R
e
c
i
p
r
o
c
a
l
 
R
a
n
k
 
(
M
R
R
)
Amount of Supervision (# classes x seeds per class)
LP-ZGL Adsorption MAD
Figure 3: Comparison of three graph transduction
methods on a graph constructed from the Freebase
dataset (see Section 3.1), with 23 classes. All re-
sults are averaged over 4 random trials. In each
group, MAD is the rightmost bar.
Freebase (Metaweb Technologies, 2009)2 is
a large collaborative knowledge base. The
knowledge base harvests information from many
open data sets (for instance Wikipedia and Mu-
sicBrainz), as well as from user contributions. For
our current purposes, we can think of the Freebase
2http://www.freebase.com/
1475
Graph Vertices Edges Avg. Min. Max.
Deg. Deg. Deg.
Freebase-1 (Section 3.1) 32970 957076 29.03 1 13222
Freebase-2 (Section 3.2) 301638 2310002 7.66 1 137553
TextRunner (Section 3.3) 175818 529557 3.01 1 2738
YAGO (Section 3.6) 142704 777906 5.45 0 74389
TextRunner + YAGO (Section 3.6) 237967 1307463 5.49 1 74389
Table 1: Statistics of various graphs used in experiments in Section 3. Some of the test instances in the
YAGO graph, added for fair comparison with the TextRunner graph in Section 3.6, had no attributes in
YAGO KB, and hence these instance nodes had degree 0 in the YAGO graph.
Bob Dylan
film-music_contributor-name
Johnny 
Cash
people-person-name
Isaac Newton
Bob Dylan
film-music_contributor-name
Johnny 
Cash
people-person-name
Isaac Newton
has_attribute:albums
(a) (b)
Figure 2: (a) Example of a section of the graph constructed from the two tables in Figure 1. Rectangular
nodes are properties, oval nodes are entities or cell values. (b) The graph in part (a) augmented with
an attribute node, has attribue:albums, along with the edges incident on it. This results is additional
constraints for the nodes Johnny Cash and Bob Dylan to have similar labels (see Section 3.6).
dataset as a collection of relational tables, where
each table is assigned a unique ID. A table con-
sists of one or more properties (column names)
and their corresponding cell values (column en-
tries). Examples of two Freebase tables are shown
in Figure 1. In this figure, Gender is a property
in the table people-person, and Male is a corre-
sponding cell value. We use the following process
to convert the Freebase data tables into a single
graph:
? Create a node for each unique cell value
? Create a node for each unique property name,
where unique property name is obtained by
prefixing the unique table ID to the prop-
erty name. For example, in Figure 1, people-
person-gender is a unique property name.
? Add an edge of weight 1.0 from cell-value
node v to unique property node p, iff value
v is present in the column corresponding to
property p. Similarly, add an edge in the re-
verse direction.
By applying this graph construction process on
the first column of the two tables in Figure 1, we
end up with the graph shown in Figure 2 (a). We
note that even though the resulting graph consists
of edges connecting nodes of different types: cell
value nodes to property nodes; the graph-based
SSL methods (Section 2) can still be applied on
such graphs as a cell value node and a property
node connected by an edge should be assigned
same or similar class labels. In other words, the la-
bel smoothness assumption (see Section 2.2) holds
on such graphs.
We applied the same graph construction pro-
cess on a subset of the Freebase dataset consist-
ing of topics from 18 randomly selected domains:
astronomy, automotive, biology, book, business,
1476
chemistry, comic books, computer, film, food, ge-
ography, location, people, religion, spaceflight,
tennis, travel, and wine. The topics in this subset
were further filtered so that only cell-value nodes
with frequency 10 or more were retained. We call
the resulting graph Freebase-1 (see Table 1).
Pantel et al (2009) have made available
a set of gold class-instance pairs derived
from Wikipedia, which is downloadable from
http://ow.ly/13B57. From this set, we selected
all classes which had more than 10 instances
overlapping with the Freebase graph constructed
above. This resulted in 23 classes, which along
with their overlapping instances were used as the
gold standard set for the experiments in this sec-
tion.
Experimental results with 2 and 10 seeds (la-
beled nodes) per class are shown in Figure 3. From
the figure, we see that that LP-ZGL and Adsorp-
tion performed comparably on this dataset, with
MAD significantly outperforming both methods.
3.2 Freebase-2 Graph with WordNet Classes
0.25
0.285
0.32
0.355
0.39
192 x 2 192 x 10
Freebase-2 Graph, 192 WordNet Classes
M
e
a
n
 
R
e
c
i
p
r
o
c
a
l
 
R
a
n
k
 
(
M
R
R
)
Amount of Supervision (# classes x seeds per class)
LP-ZGL Adsorption MAD
Figure 4: Comparison of graph transduction meth-
ods on a graph constructed from the Freebase
dataset (see Section 3.2). All results are averaged
over 10 random trials. In each group, MAD is the
rightmost bar.
To evaluate how the algorithms scale up, we
construct a larger graph from the same 18 domains
as in Section 3.1, and using the same graph con-
struction process. We shall call the resulting graph
Freebase-2 (see Table 1). In order to scale up the
number of classes, we selected all Wordnet (WN)
classes, available in the YAGO KB (Suchanek et
al., 2007), that had more than 100 instances over-
lapping with the larger Freebase graph constructed
above. This resulted in 192 WN classes which we
use for the experiments in this section. The reason
behind imposing such frequency constraints dur-
ing class selection is to make sure that each class
is left with a sufficient number of instances during
testing.
Experimental results comparing LP-ZGL, Ad-
sorption, and MAD with 2 and 10 seeds per class
are shown in Figure 4. A total of 292k test nodes
were used for testing in the 10 seeds per class con-
dition, showing that these methods can be applied
to large datasets. Once again, we observe MAD
outperforming both LP-ZGL and Adsorption. It is
interesting to note that MAD with 2 seeds per class
outperforms LP-ZGL and adsorption even with 10
seeds per class.
3.3 TextRunner Graph with WordNet
Classes
0.15
0.2
0.25
0.3
0.35
170 x 2 170 x 10
TextRunner Graph, 170 WordNet Classes
M
e
a
n
 
R
e
c
i
p
r
o
c
a
l
 
R
a
n
k
 
(
M
R
R
)
Amount of Supervision (# classes x seeds per class)
LP-ZGL Adsorption MAD
Figure 5: Comparison of graph transduction meth-
ods on a graph constructed from the hypernym tu-
ples extracted by the TextRunner system (Banko
et al, 2007) (see Section 3.3). All results are aver-
aged over 10 random trials. In each group, MAD
is the rightmost bar.
In contrast to graph construction from struc-
tured tables as in Sections 3.1, 3.2, in this section
we use hypernym tuples extracted by TextRun-
ner (Banko et al, 2007), an open domain IE sys-
tem, to construct the graph. Example of a hyper-
nym tuple extracted by TextRunner is (http, proto-
col, 0.92), where 0.92 is the extraction confidence.
To convert such a tuple into a graph, we create a
node for the instance (http) and a node for the class
(protocol), and then connect the nodes with two
1477
directed edges in both directions, with the extrac-
tion confidence (0.92) as edge weights. The graph
created with this process from TextRunner out-
put is called the TextRunner Graph (see Table 1).
As in Section 3.2, we use WordNet class-instance
pairs as the gold set. In this case, we considered
all WordNet classes, once again from YAGO KB
(Suchanek et al, 2007), which had more than 50
instances overlapping with the constructed graph.
This resulted in 170 WordNet classes being used
for the experiments in this section.
Experimental results with 2 and 10 seeds per
class are shown in Figure 5. The three methods
are comparable in this setting, with MAD achiev-
ing the highest overall MRR.
3.4 Discussion
If we correlate the graph statistics in Table 1 with
the results of sections 3.1, 3.2, and 3.3, we see
that MAD is most effective for graphs with high
average degree, that is, graphs where nodes tend
to connect to many other nodes. For instance,
the Freebase-1 graph has a high average degree
of 29.03, with a corresponding large advantage
for MAD over the other methods. Even though
this might seem mysterious at first, it becomes
clearer if we look at the objectives minimized
by different algorithms. We find that the objec-
tive minimized by LP-ZGL (Equation 1) is under-
regularized, i.e., its model parameters (Y? ) are not
constrained enough, compared to MAD (Equation
3, specifically the third term), resulting in overfit-
ting in case of highly connected graphs. In con-
trast, MAD is able to avoid such overfitting be-
cause of its minimization of a well regularized ob-
jective (Equation 3). Based on this, we suggest
that average degree, an easily computable struc-
tural property of the graph, may be a useful indica-
tor in choosing which graph-based SSL algorithm
should be applied on a given graph.
Unlike MAD, Adsorption does not optimize
any well defined objective (Talukdar and Cram-
mer, 2009), and hence any analysis along the lines
described above is not possible. The heuristic
choices made in Adsorption may have lead to its
sub-optimal performance compared to MAD; we
leave it as a topic for future investigation.
3.5 Effect of Per-Node Class Sparsity
For all the experiments in Sections 3.1, 3.2, and
3.6, each node was allowed to have a maximum
of 15 classes during inference. After each update
0.3
0.33
0.36
0.39
0.42
5 15 25 35 45
Effect of Per-node Sparsity Constraint
M
e
a
n
 
R
e
c
i
p
r
o
c
a
l
 
R
a
n
k
 
(
M
R
R
)
Maximum Allowed Classes per Node
Figure 6: Effect of per node class sparsity (maxi-
mum number of classes allowed per node) during
MAD inference in the experimental setting of Fig-
ure 4 (one random split).
on a node, all classes except for the top scoring
15 classes were discarded. Without such sparsity
constraints, a node in a connected graph will end
up acquiring all the labels injected into the graph.
This is undesirable for two reasons: (1) for ex-
periments involving a large numbers of classes (as
in the previous section and in the general case of
open domain IE), this increases the space require-
ment and also slows down inference; (2) a partic-
ular node is unlikely to belong to a large num-
ber of classes. In order to estimate the effect of
such sparsity constraints, we varied the number
of classes allowed per node from 5 to 45 on the
graph and experimental setup of Figure 4, with 10
seeds per class. The results for MAD inference
over the development split are shown in Figure
6. We observe that performance can vary signifi-
cantly as the maximum number of classes allowed
per node is changed, with the performance peak-
ing at 25. This suggests that sparsity constraints
during graph based SSL may have a crucial role to
play, a question that needs further investigation.
3.6 TextRunner Graph with additional
Semantic Constraints from YAGO
Recently, the problem of instance-attribute extrac-
tion has started to receive attention (Probst et al,
2007; Bellare et al, 2007; Pasca and Durme,
2007). An example of an instance-attribute pair
is (Bob Dylan, albums). Given a set of seed
instance-attribute pairs, these methods attempt to
extract more instance-attribute pairs automatically
1478
0.18
0.23
0.28
0.33
0.38
LP-ZGL Adsorption MAD
170 WordNet Classes, 2 Seeds per Class
M
e
a
n
 
R
e
c
i
p
r
o
c
a
l
 
R
a
n
k
 
(
M
R
R
)
Algorithms
TextRunner Graph
YAGO Graph
TextRunner + YAGO Graph
0.3
0.338
0.375
0.413
0.45
LP-ZGL Adsorption MAD
170 WordNet Classes, 10 Seeds per Class
M
e
a
n
 
R
e
c
i
p
r
o
c
a
l
 
R
a
n
k
 
(
M
R
R
)
Algorithms
TextRunner Graph
YAGO Graph
TextRunner + YAGO Graph
Figure 7: Comparison of class-instance acquisition performance on the three different graphs described
in Section 3.6. All results are averaged over 10 random trials. Addition of YAGO attributes to the
TextRunner graph significantly improves performance.
YAGO Top-2 WordNet Classes Assigned by MAD
Attribute (example instances for each class are shown in brackets)
has currency wordnet country 108544813 (Burma, Afghanistan)
wordnet region 108630039 (Aosta Valley, Southern Flinders Ranges)
works at wordnet scientist 110560637 (Aage Niels Bohr, Adi Shamir)
wordnet person 100007846 (Catherine Cornelius, Jamie White)
has capital wordnet state 108654360 (Agusan del Norte, Bali)
wordnet region 108630039 (Aosta Valley, Southern Flinders Ranges)
born in wordnet boxer 109870208 (George Chuvalo, Fernando Montiel)
wordnet chancellor 109906986 (Godon Brown, Bill Bryson)
has isbn wordnet book 106410904 (Past Imperfect, Berlin Diary)
wordnet magazine 106595351 (Railway Age, Investors Chronicle)
Table 2: Top 2 (out of 170) WordNet classes assigned by MAD on 5 randomly chosen YAGO attribute
nodes (out of 80) in the TextRunner + YAGO graph used in Figure 7 (see Section 3.6), with 10 seeds per
class used. A few example instances of each WordNet class is shown within brackets. Top ranked class
for each attribute is shown in bold.
from various sources. In this section, we ex-
plore whether class-instance assignment can be
improved by incorporating new semantic con-
straints derived from (instance, attribute) pairs. In
particular, we experiment with the following type
of constraint: two instances with a common at-
tribute are likely to belong to the same class. For
example, in Figure 2 (b), instances Johnny Cash
and Bob Dylan are more likely to belong to the
same class as they have a common attribute, al-
bums. Because of the smooth labeling bias of
graph-based SSL methods (see Section 2.2), such
constraints are naturally captured by the methods
reviewed in Section 2. All that is necessary is the
introduction of bidirectional (instance, attribute)
edges to the graph, as shown in Figure 2 (b).
In Figure 7, we compare class-instance acqui-
sition performance of the three graph-based SSL
methods (Section 2) on the following three graphs
(also see Table 1):
TextRunner Graph: Graph constructed
from the hypernym tuples extracted by Tex-
tRunner, as in Figure 5 (Section 3.3), with
175k vertices and 529k edges.
YAGO Graph: Graph constructed from the
(instance, attribute) pairs obtained from the
YAGO KB (Suchanek et al, 2007), with 142k
nodes and 777k edges.
TextRunner + YAGO Graph: Union of the
1479
two graphs above, with 237k nodes and 1.3m
edges.
In all experimental conditions with 2 and 10
seeds per class in Figure 7, we observe that the
three methods consistently achieved the best per-
formance on the TextRunner + YAGO graph. This
suggests that addition of attribute based seman-
tic constraints from YAGO to the TextRunner
graph results in a better connected graph which
in turn results in better inference by the graph-
based SSL algorithms, compared to using either
of the sources, i.e., TextRunner output or YAGO
attributes, in isolation. This further illustrates
the advantage of aggregating information across
sources (Talukdar et al, 2008; Pennacchiotti and
Pantel, 2009). However, we are the first, to the
best of our knowledge, to demonstrate the effec-
tiveness of attributes in class-instance acquisition.
We note that this work is similar in spirit to the
recent work by Carlson et al (2010) which also
demonstrates the benefits of additional constraints
in SSL.
Because of the label propagation behavior,
graph-based SSL algorithms assign classes to all
nodes reachable in the graph from at least one
of the labeled instance nodes. This allows us
to check the classes assigned to nodes corre-
sponding to YAGO attributes in the TextRunner
+ YAGO graph, as shown in Table 2. Even
though the experiments were designed for class-
instance acquisition, it is encouraging to see that
the graph-based SSL algorithm (MAD in Table
2) is able to learn class-attribute relationships,
an important by-product that has been the fo-
cus of recent studies (Reisinger and Pasca, 2009).
For example, the algorithm is able to learn that
works at is an attribute of the WordNet class word-
net scientist 110560637, and thereby its instances
(e.g. Aage Niels Bohr, Adi Shamir).
4 Conclusion
We have started a systematic experimental com-
parison of graph-based SSL algorithms for class-
instance acquisition on a variety of graphs con-
structed from different domains. We found that
MAD, a recently proposed graph-based SSL algo-
rithm, is consistently the most effective across the
various experimental conditions. We also showed
that class-instance acquisition performance can be
significantly improved by incorporating additional
semantic constraints in the class-instance acqui-
sition process, which for the experiments in this
paper were derived from instance-attribute pairs
available in an independently developed knowl-
edge base. All the data used in these experiments
was drawn from publicly available datasets and we
plan to release our code3 to foster reproducible
research in this area. Topics for future work in-
clude the incorporation of other kinds of semantic
constraint for improved class-instance acquisition,
further investigation into per-node sparsity con-
straints in graph-based SSL, and moving beyond
bipartite graph constructions.
Acknowledgments
We thank William Cohen for valuable discussions,
and Jennifer Gillenwater, Alex Kulesza, and Gre-
gory Malecha for detailed comments on a draft of
this paper. We are also very grateful to the authors
of (Banko et al, 2007), Oren Etzioni and Stephen
Soderland in particular, for providing TextRunner
output. This work was supported in part by NSF
IIS-0447972 and DARPA HRO1107-1-0029.
References
S. Baluja, R. Seth, D. Sivakumar, Y. Jing, J. Yagnik,
S. Kumar, D. Ravichandran, and M. Aly. 2008.
Video suggestion and discovery for youtube: taking
random walks through the view graph. Proceedings
of WWW-2008.
M. Banko, M.J. Cafarella, S. Soderland, M. Broadhead,
and O. Etzioni. 2007. Open information extraction
from the web. Procs. of IJCAI.
K. Bellare, P. Talukdar, G. Kumaran, F. Pereira,
M. Liberman, A. McCallum, and M. Dredze. 2007.
Lightly-Supervised Attribute Extraction. NIPS 2007
Workshop on Machine Learning for Web Search.
A. Carlson, J. Betteridge, R.C. Wang, E.R. Hruschka Jr,
and T.M. Mitchell. 2010. Coupled Semi-Supervised
Learning for Information Extraction. In Proceed-
ings of the Third ACM International Conference on
Web Search and Data Mining (WSDM), volume 2,
page 110.
O. Etzioni, Michael Cafarella, Doug Downey, Ana-
Maria Popescu, Tal Shaked, Stephen Soderland,
Daniel S. Weld, and Alexander Yates. 2005. Unsu-
pervised named-entity extraction from the web - an
experimental study. Artificial Intelligence Journal.
M. Hearst. 1992. Automatic acquisition of hyponyms
from large text corpora. In Fourteenth International
3
http://www.talukdar.net/datasets/class inst/
1480
Conference on Computational Linguistics, Nantes,
France.
Metaweb Technologies. 2009. Freebase data dumps.
http://download.freebase.com/datadumps/.
P. Pantel, E. Crestan, A. Borkovsky, A.M. Popescu, and
V. Vyas. 2009. Web-scale distributional similarity
and entity set expansion. Proceedings of EMNLP-
09, Singapore.
M. Pasca and Benjamin Van Durme. 2007. What you
seek is what you get: Extraction of class attributes
from query logs. In IJCAI-07. Ferbruary, 2007.
M. Pennacchiotti and P. Pantel. 2009. Entity Ex-
traction via Ensemble Semantics. Proceedings of
EMNLP-09, Singapore.
K. Probst, R. Ghani, M. Krema, A. Fano, and Y. Liu.
2007. Semi-supervised learning of attribute-value
pairs from product descriptions. In IJCAI-07, Fer-
bruary, 2007.
D. Rao and D. Yarowsky. 2009. Ranking and Semi-
supervised Classification on Large Scale Graphs Us-
ing Map-Reduce. TextGraphs.
J. Reisinger and M. Pasca. 2009. Bootstrapped extrac-
tion of class attributes. In Proceedings of the 18th
international conference on World wide web, pages
1235?1236. ACM.
E. Riloff and R. Jones. 1999. Learning dictionar-
ies for information extraction by multi-level boot-
strapping. In Proceedings of the 16th National Con-
ference on Artificial Intelligence (AAAI-99), pages
474?479, Orlando, Florida.
F.M. Suchanek, G. Kasneci, and G. Weikum. 2007.
Yago: a core of semantic knowledge. In Proceed-
ings of the 16th international conference on World
Wide Web, page 706. ACM.
P. P. Talukdar and Koby Crammer. 2009. New regular-
ized algorithms for transductive learning. In ECML-
PKDD.
P. P. Talukdar, T. Brants, F. Pereira, and M. Liberman.
2006. A context pattern induction method for named
entity extraction. In Tenth Conference on Computa-
tional Natural Language Learning, page 141.
P. P. Talukdar, J. Reisinger, M. Pasca, D. Ravichan-
dran, R. Bhagat, and F. Pereira. 2008. Weakly-
Supervised Acquisition of Labeled Class Instances
using Graph Random Walks. In Proceedings of the
2008 Conference on Empirical Methods in Natural
Language Processing, pages 581?589.
B. Van Durme and M. Pas?ca. 2008. Finding cars, god-
desses and enzymes: Parametrizable acquisition of
labeled instances for open-domain information ex-
traction. Twenty-Third AAAI Conference on Artifi-
cial Intelligence.
R. Wang and W. Cohen. 2007. Language-Independent
Set Expansion of Named Entities Using the Web.
Data Mining, 2007. ICDM 2007. Seventh IEEE In-
ternational Conference on, pages 342?350.
X. Zhu, Z. Ghahramani, and J. Lafferty. 2003. Semi-
supervised learning using gaussian fields and har-
monic functions. ICML-03, 20th International Con-
ference on Machine Learning.
1481
Proceedings of the ACL 2010 Conference Short Papers, pages 194?199,
Uppsala, Sweden, 11-16 July 2010. c?2010 Association for Computational Linguistics
Sparsity in Dependency Grammar Induction
Jennifer Gillenwater and Kuzman Ganchev
University of Pennsylvania
Philadelphia, PA, USA
{jengi,kuzman}@cis.upenn.edu
Jo?o Gra?a
L2F INESC-ID
Lisboa, Portugal
joao.graca@l2f.inesc-id.pt
Fernando Pereira
Google Inc.
Mountain View, CA, USA
pereira@google.com
Ben Taskar
University of Pennsylvania
Philadelphia, PA, USA
taskar@cis.upenn.edu
Abstract
A strong inductive bias is essential in un-
supervised grammar induction. We ex-
plore a particular sparsity bias in de-
pendency grammars that encourages a
small number of unique dependency
types. Specifically, we investigate
sparsity-inducing penalties on the poste-
rior distributions of parent-child POS tag
pairs in the posterior regularization (PR)
framework of Gra?a et al (2007). In ex-
periments with 12 languages, we achieve
substantial gains over the standard expec-
tation maximization (EM) baseline, with
average improvement in attachment ac-
curacy of 6.3%. Further, our method
outperforms models based on a standard
Bayesian sparsity-inducing prior by an av-
erage of 4.9%. On English in particular,
we show that our approach improves on
several other state-of-the-art techniques.
1 Introduction
We investigate an unsupervised learning method
for dependency parsing models that imposes spar-
sity biases on the dependency types. We assume
a corpus annotated with POS tags, where the task
is to induce a dependency model from the tags for
corpus sentences. In this setting, the type of a de-
pendency is defined as a pair: tag of the dependent
(also known as the child), and tag of the head (also
known as the parent). Given that POS tags are de-
signed to convey information about grammatical
relations, it is reasonable to assume that only some
of the possible dependency types will be realized
for a given language. For instance, in English it
is ungrammatical for nouns to dominate verbs, ad-
jectives to dominate adverbs, and determiners to
dominate almost any part of speech. Thus, the re-
alized dependency types should be a sparse subset
of all possible types.
Previous work in unsupervised grammar induc-
tion has tried to achieve sparsity through priors.
Liang et al (2007), Finkel et al (2007) and John-
son et al (2007) proposed hierarchical Dirichlet
process priors. Cohen et al (2008) experimented
with a discounting Dirichlet prior, which encour-
ages a standard dependency parsing model (see
Section 2) to limit the number of dependent types
for each head type.
Our experiments show a more effective sparsity
pattern is one that limits the total number of unique
head-dependent tag pairs. This kind of sparsity
bias avoids inducing competition between depen-
dent types for each head type. We can achieve the
desired bias with a constraint on model posteri-
ors during learning, using the posterior regulariza-
tion (PR) framework (Gra?a et al, 2007). Specifi-
cally, to implement PR we augment the maximum
marginal likelihood objective of the dependency
model with a term that penalizes head-dependent
tag distributions that are too permissive.
Although not focused on sparsity, several other
studies use soft parameter sharing to couple dif-
ferent types of dependencies. To this end, Cohen
et al (2008) and Cohen and Smith (2009) inves-
tigated logistic normal priors, and Headden III et
al. (2009) used a backoff scheme. We compare to
their results in Section 5.
The remainder of this paper is organized as fol-
194
lows. Section 2 and 3 review the models and sev-
eral previous approaches for learning them. Sec-
tion 4 describes learning with PR. Section 5 de-
scribes experiments across 12 languages and Sec-
tion 6 analyzes the results. For additional details
on this work see Gillenwater et al (2010).
2 Parsing Model
The models we use are based on the generative de-
pendency model with valence (DMV) (Klein and
Manning, 2004). For a sentence with tags x, the
root POS r(x) is generated first. Then the model
decides whether to generate a right dependent con-
ditioned on the POS of the root and whether other
right dependents have already been generated for
this head. Upon deciding to generate a right de-
pendent, the POS of the dependent is selected by
conditioning on the head POS and the direction-
ality. After stopping on the right, the root gener-
ates left dependents using the mirror reversal of
this process. Once the root has generated all its
dependents, the dependents generate their own de-
pendents in the same manner.
2.1 Model Extensions
For better comparison with previous work we
implemented three model extensions, borrowed
from Headden III et al (2009). The first exten-
sion alters the stopping probability by condition-
ing it not only on whether there are any depen-
dents in a particular direction already, but also on
how many such dependents there are. When we
talk about models with maximum stop valency Vs
= S, this means it distinguishes S different cases:
0, 1, . . . , S?2, and? S?1 dependents in a given
direction. The basic DMV has Vs = 2.
The second model extension we implement is
analogous to the first, but applies to dependent tag
probabilities instead of stop probabilities. Again,
we expand the conditioning such that the model
considers how many other dependents were al-
ready generated in the same direction. When we
talk about a model with maximum child valency
Vc = C, this means we distinguish C different
cases. The basic DMV has Vc = 1. Since this
extension to the dependent probabilities dramati-
cally increases model complexity, the third model
extension we implement is to add a backoff for the
dependent probabilities that does not condition on
the identity of the parent POS (see Equation 2).
More formally, under the extended DMV the
probability of a sentence with POS tags x and de-
pendency tree y is given by:
p?(x,y) = proot(r(x))?
Y
y?y
pstop(false | yp, yd, yvs)pchild(yc | yp, yd, yvc)?
Y
x?x
pstop(true | x, left, xvl) pstop(true | x, right, xvr )
(1)
where y is the dependency of yc on head yp in di-
rection yd, and yvc , yvs , xvr , and xvl indicate va-
lence. For the third model extension, the backoff
to a probability not dependent on parent POS can
be formally expressed as:
?pchild(yc | yp, yd, yvc) + (1? ?)pchild(yc | yd, yvc) (2)
for ? ? [0, 1]. We fix ? = 1/3, which is a crude
approximation to the value learned by Headden III
et al (2009).
3 Previous Learning Approaches
In our experiments, we compare PR learning
to standard expectation maximization (EM) and
to Bayesian learning with a sparsity-inducing
prior. The EM algorithm optimizes marginal like-
lihood L(?) = log
?
Y p?(X,Y), where X =
{x1, . . . ,xn} denotes the entire unlabeled corpus
and Y = {y1, . . . ,yn} denotes a set of corre-
sponding parses for each sentence. Neal and Hin-
ton (1998) view EM as block coordinate ascent on
a function that lower-bounds L(?). Starting from
an initial parameter estimate ?0, the algorithm it-
erates two steps:
E : qt+1 = argmin
q
KL(q(Y) ? p?t(Y | X)) (3)
M : ?t+1 = argmax
?
Eqt+1 [log p?(X,Y)] (4)
Note that the E-step just sets qt+1(Y) =
p?t(Y|X), since it is an unconstrained minimiza-
tion of a KL-divergence. The PR method we
present modifies the E-step by adding constraints.
Besides EM, we also compare to learning with
several Bayesian priors that have been applied to
the DMV. One such prior is the Dirichlet, whose
hyperparameter we will denote by ?. For ? < 0.5,
this prior encourages parameter sparsity. Cohen
et al (2008) use this method with ? = 0.25 for
training the DMV and achieve improvements over
basic EM. In this paper we will refer to our own
implementation of the Dirichlet prior as the ?dis-
counting Dirichlet? (DD) method. In addition to
195
the Dirichlet, other types of priors have been ap-
plied, in particular logistic normal priors (LN) and
shared logistic normal priors (SLN) (Cohen et al,
2008; Cohen and Smith, 2009). LN and SLN aim
to tie parameters together. Essentially, this has a
similar goal to sparsity-inducing methods in that it
posits a more concise explanation for the grammar
of a language. Headden III et al (2009) also im-
plement a sort of parameter tying for the E-DMV
through a learning a backoff distribution on child
probabilities. We compare against results from all
these methods.
4 Learning with Sparse Posteriors
We would like to penalize models that predict a
large number of distinct dependency types. To en-
force this penalty, we use the posterior regular-
ization (PR) framework (Gra?a et al, 2007). PR
is closely related to generalized expectation con-
straints (Mann and McCallum, 2007; Mann and
McCallum, 2008; Bellare et al, 2009), and is also
indirectly related to a Bayesian view of learning
with constraints on posteriors (Liang et al, 2009).
The PR framework uses constraints on posterior
expectations to guide parameter estimation. Here,
PR allows a natural and tractable representation of
sparsity constraints based on edge type counts that
cannot easily be encoded in model parameters. We
use a version of PR where the desired bias is a
penalty on the log likelihood (see Ganchev et al
(2010) for more details). For a distribution p?, we
define a penalty as the (generic) ?-norm of expec-
tations of some features ?:
||Ep? [?(X,Y)]||? (5)
For computational tractability, rather than penaliz-
ing the model?s posteriors directly, we use an aux-
iliary distribution q, and penalize the marginal log-
likelihood of a model by the KL-divergence of p?
from q, plus the penalty term with respect to q.
For a fixed set of model parameters ? the full PR
penalty term is:
min
q
KL(q(Y) ? p?(Y|X)) + ? ||Eq[?(X,Y)]||? (6)
where ? is the strength of the regularization. PR
seeks to maximize L(?) minus this penalty term.
The resulting objective can be optimized by a vari-
ant of the EM (Dempster et al, 1977) algorithm
used to optimize L(?).
4.1 `1/`? Regularization
We now define precisely how to count dependency
types. For each child tag c, let i range over an enu-
meration of all occurrences of c in the corpus, and
let p be another tag. Let the indicator ?cpi(X,Y)
have value 1 if p is the parent tag of the ith occur-
rence of c, and value 0 otherwise. The number of
unique dependency types is then:
X
cp
max
i
?cpi(X,Y) (7)
Note there is an asymmetry in this count: occur-
rences of child type c are enumerated with i, but
all occurrences of parent type p are or-ed in ?cpi.
That is, ?cpi = 1 if any occurrence of p is the par-
ent of the ith occurrence of c. We will refer to PR
training with this constraint as PR-AS. Instead of
counting pairs of a child token and a parent type,
we can alternatively count pairs of a child token
and a parent token by letting p range over all to-
kens rather than types. Then each potential depen-
dency corresponds to a different indicator ?cpij ,
and the penalty is symmetric with respect to par-
ents and children. We will refer to PR training
with this constraint as PR-S. Both approaches per-
form very well, so we report results for both.
Equation 7 can be viewed as a mixed-norm
penalty on the features ?cpi or ?cpij : the sum cor-
responds to an `1 norm and the max to an `?
norm. Thus, the quantity we want to minimize
fits precisely into the PR penalty framework. For-
mally, to optimize the PR objective, we complete
the following E-step:
argmin
q
KL(q(Y)||p?(Y|X)) + ?
X
cp
max
i
Eq[?(X,Y)],
(8)
which can equivalently be written as:
min
q(Y),?cp
KL(q(Y) ? p?(Y|X)) + ?
X
cp
?cp
s. t. ?cp ? Eq[?(X,Y)]
(9)
where ?cp corresponds to the maximum expecta-
tion of ? over all instances of c and p. Note that
the projection problem can be solved efficiently in
the dual (Ganchev et al, 2010).
5 Experiments
We evaluate on 12 languages. Following the ex-
ample of Smith and Eisner (2006), we strip punc-
tuation from the sentences and keep only sen-
tences of length ? 10. For simplicity, for all mod-
els we use the ?harmonic? initializer from Klein
196
Model EM PR Type ?
DMV 45.8 62.1 PR-S 140
2-1 45.1 62.7 PR-S 100
2-2 54.4 62.9 PR-S 80
3-3 55.3 64.3 PR-S 140
4-4 55.1 64.4 PR-AS 140
Table 1: Attachment accuracy results. Column 1: Vc-
Vs used for the E-DMV models. Column 3: Best PR re-
sult for each model, which is chosen by applying each of
the two types of constraints (PR-S and PR-AS) and trying
? ? {80, 100, 120, 140, 160, 180}. Columns 4 & 5: Con-
straint type and ? that produced the values in column 3.
and Manning (2004), which we refer to as K&M.
We always train for 100 iterations and evaluate
on the test set using Viterbi parses. Before eval-
uating, we smooth the resulting models by adding
e?10 to each learned parameter, merely to remove
the chance of zero probabilities for unseen events.
(We did not tune this as it should make very little
difference for final parses.) We score models by
their attachment accuracy ? the fraction of words
assigned the correct parent.
5.1 Results on English
We start by comparing English performance for
EM, PR, and DD. To find ? for DD we searched
over five values: {0.01, 0.1, 0.25, 1}. We found
0.25 to be the best setting for the DMV, the same
as found by Cohen et al (2008). DD achieves ac-
curacy 46.4% with this ?. For the E-DMV we
tested four model complexities with valencies Vc-
Vs of 2-1, 2-2, 3-3, and 4-4. DD?s best accuracy
was 53.6% with the 4-4 model at ? = 0.1. A
comparison between EM and PR is shown in Ta-
ble 1. PR-S generally performs better than the PR-
AS for English. Comparing PR-S to EM, we also
found PR-S is always better, independent of the
particular ?, with improvements ranging from 2%
to 17%. Note that in this work we do not perform
the PR projection at test time; we found it detri-
mental, probably due to a need to set the (corpus-
size-dependent) ? differently for the test set. We
also note that development likelihood and the best
setting for ? are not well-correlated, which un-
fortunately makes it hard to pick these parameters
without some supervision.
5.2 Comparison with Previous Work
In this section we compare to previously published
unsupervised dependency parsing results for En-
glish. It might be argued that the comparison is
unfair since we do supervised selection of model
Learning Method Accuracy
? 10 ? 20 all
PR-S (? = 140) 62.1 53.8 49.1
LN families 59.3 45.1 39.0
SLN TieV & N 61.3 47.4 41.4
PR-AS (? = 140) 64.4 55.2 50.5
DD (? = 1, ? learned) 65.0 (?5.7)
Table 2: Comparison with previous published results. Rows
2 and 3 are taken from Cohen et al (2008) and Cohen and
Smith (2009), and row 5 from Headden III et al (2009).
complexity and regularization strength. However,
we feel the comparison is not so unfair as we per-
form only a very limited search of the model-?
space. Specifically, the only values of ? we search
over are {80, 100, 120, 140, 160, 180}.
First, we consider the top three entries in Ta-
ble 2, which are for the basic DMV. The first en-
try was generated using our implementation of
PR-S. The second two entries are logistic nor-
mal and shared logistic normal parameter tying re-
sults (Cohen et al, 2008; Cohen and Smith, 2009).
The PR-S result is the clear winner, especially as
length of test sentences increases. For the bot-
tom two entries in the table, which are for the E-
DMV, the last entry is best, corresponding to us-
ing a DD prior with ? = 1 (non-sparsifying), but
with a special ?random pools? initialization and a
learned weight ? for the child backoff probabil-
ity. The result for PR-AS is well within the vari-
ance range of this last entry, and thus we conjec-
ture that combining PR-AS with random pools ini-
tialization and learned ? would likely produce the
best-performing model of all.
5.3 Results on Other Languages
Here we describe experiments on 11 additional
languages. For each we set ? and model complex-
ity (DMV versus one of the four E-DMV exper-
imented with previously) based on the best con-
figuration found for English. This likely will not
result in the ideal parameters for all languages, but
provides a realistic test setting: a user has avail-
able a labeled corpus in one language, and would
like to induce grammars for many other languages.
Table 3 shows the performance for all models and
training procedures. We see that the sparsifying
methods tend to improve over EM most of the
time. For the basic DMV, average improvements
are 1.6% for DD, 6.0% for PR-S, and 7.5% for
PR-AS. PR-AS beats PR-S in 8 out of 12 cases,
197
Bg Cz De Dk En Es Jp Nl Pt Se Si Tr
DMV Model
EM 37.8 29.6 35.7 47.2 45.8 40.3 52.8 37.1 35.7 39.4 42.3 46.8
DD 0.25 39.3 30.0 38.6 43.1 46.4 47.5 57.8 35.1 38.7 40.2 48.8 43.8
PR-S 140 53.7 31.5 39.6 44.0 62.1 61.1 58.8 31.0 47.0 42.2 39.9 51.4
PR-AS 140 54.0 32.0 39.6 42.4 61.9 62.4 60.2 37.9 47.8 38.7 50.3 53.4
Extended Model
EM (3,3) 41.7 48.9 40.1 46.4 55.3 44.3 48.5 47.5 35.9 48.6 47.5 46.2
DD 0.1 (4,4) 47.6 48.5 42.0 44.4 53.6 48.9 57.6 45.2 48.3 47.6 35.6 48.9
PR-S 140 (3,3) 59.0 54.7 47.4 45.8 64.3 57.9 60.8 33.9 54.3 45.6 49.1 56.3
PR-AS 140 (4,4) 59.8 54.6 45.7 46.6 64.4 57.9 59.4 38.8 49.5 41.4 51.2 56.9
Table 3: Attachment accuracy results. The parameters used are the best settings found for English. Values for hyperparameters
(? or ?) are given after the method name. For the extended model (Vc, Vs) are indicated in parentheses. En is the English Penn
Treebank (Marcus et al, 1993) and the other 11 languages are from the CoNLL X shared task: Bulgarian [Bg] (Simov et al,
2002), Czech [Cz] (Bohomov? et al, 2001), German [De] (Brants et al, 2002), Danish [Dk] (Kromann et al, 2003), Spanish
[Es] (Civit and Mart?, 2004), Japanese [Jp] (Kawata and Bartels, 2000), Dutch [Nl] (Van der Beek et al, 2002), Portuguese
[Pt] (Afonso et al, 2002), Swedish [Se] (Nilsson et al, 2005), Slovene [Sl] (D?eroski et al, 2006), and Turkish [Tr] (Oflazer et
al., 2003).
Unad
papeleranc esvs und
objetonc civilizadoaq
Unad
papeleranc esvs und
objetonc civilizadoaq
1.00
1.00 1.000.49
0.51
1.00
0.57
0.43
Unad
papeleranc esvs und
objetonc civilizadoaq
1.00 0.83 0.75 0.990.92
0.35
0.48
Figure 1: Posterior edge probabilities for an example sen-
tence from the Spanish test corpus. At the top are the gold
dependencies, the middle are EM posteriors, and bottom are
PR posteriors. Green indicates correct dependencies and red
indicates incorrect dependencies. The numbers on the edges
are the values of the posterior probabilities.
though the average increase is only 1.5%. PR-S
is also better than DD for 10 out of 12 languages.
If we instead consider these methods for the E-
DMV, DD performs worse, just 1.4% better than
the E-DMV EM, while both PR-S and PR-AS con-
tinue to show substantial average improvements
over EM, 6.5% and 6.3%, respectively.
6 Analysis
One common EM error that PR fixes in many lan-
guages is the directionality of the noun-determiner
relation. Figure 1 shows an example of a Span-
ish sentence where PR significantly outperforms
EM because of this. Sentences such as ?Lleva
tiempo entenderlos? which has tags ?main-verb
common-noun main-verb? (no determiner tag)
provide an explanation for PR?s improvement?
when PR sees that sometimes nouns can appear
without determiners but that the opposite situation
does not occur, it shifts the model parameters to
make nouns the parent of determiners instead of
the reverse. Then it does not have to pay the cost
of assigning a parent with a new tag to cover each
noun that doesn?t come with a determiner.
7 Conclusion
In this paper we presented a new method for unsu-
pervised learning of dependency parsers. In con-
trast to previous approaches that constrain model
parameters, we constrain model posteriors. Our
approach consistently outperforms the standard
EM algorithm and a discounting Dirichlet prior.
We have several ideas for further improving our
constraints, such as: taking into account the direc-
tionality of the edges, using different regulariza-
tion strengths for the root probabilities than for the
child probabilities, and working directly on word
types rather than on POS tags. In the future, we
would also like to try applying similar constraints
to the more complex task of joint induction of POS
tags and dependency parses.
Acknowledgments
J. Gillenwater was supported by NSF-IGERT
0504487. K. Ganchev was supported by
ARO MURI SUBTLE W911NF-07-1-0216.
J. Gra?a was supported by FCT fellowship
SFRH/BD/27528/2006 and by FCT project CMU-
PT/HuMach/0039/2008. B. Taskar was partly
supported by DARPA CSSG and ONR Young
Investigator Award N000141010746.
198
References
S. Afonso, E. Bick, R. Haber, and D. Santos. 2002.
Floresta Sinta(c)tica: a treebank for Portuguese. In
Proc. LREC.
K. Bellare, G. Druck, and A. McCallum. 2009. Al-
ternating projections for learning with expectation
constraints. In Proc. UAI.
A. Bohomov?, J. Hajic, E. Hajicova, and B. Hladka.
2001. The prague dependency treebank: Three-level
annotation scenario. In Anne Abeill?, editor, Tree-
banks: Building and Using Syntactically Annotated
Corpora.
S. Brants, S. Dipper, S. Hansen, W. Lezius, and
G. Smith. 2002. The TIGER treebank. In Proc.
Workshop on Treebanks and Linguistic Theories.
M. Civit and M.A. Mart?. 2004. Building cast3lb: A
Spanish Treebank. Research on Language & Com-
putation.
S.B. Cohen and N.A. Smith. 2009. The shared logistic
normal distribution for grammar induction. In Proc.
NAACL.
S.B. Cohen, K. Gimpel, and N.A. Smith. 2008. Lo-
gistic normal priors for unsupervised probabilistic
grammar induction. In Proc. NIPS.
A.P. Dempster, N.M. Laird, and D.B. Rubin. 1977.
Maximum likelihood from incomplete data via the
EM algorithm. Journal of the Royal Statistical So-
ciety, 39(1):1?38.
S. D?eroski, T. Erjavec, N. Ledinek, P. Pajas,
Z. ?abokrtsky, and A. ?ele. 2006. Towards a
Slovene dependency treebank. In Proc. LREC.
J. Finkel, T. Grenager, and C. Manning. 2007. The
infinite tree. In Proc. ACL.
K. Ganchev, J. Gra?a, J. Gillenwater, and B. Taskar.
2010. Posterior regularization for structured latent
variable models. Journal of Machine Learning Re-
search.
J. Gillenwater, K. Ganchev, J. Gra?a, F. Pereira, and
B. Taskar. 2010. Posterior sparsity in unsupervised
dependency parsing. Technical report, MS-CIS-10-
19, University of Pennsylvania.
J. Gra?a, K. Ganchev, and B. Taskar. 2007. Expec-
tation maximization and posterior constraints. In
Proc. NIPS.
W.P. Headden III, M. Johnson, and D. McClosky.
2009. Improving unsupervised dependency pars-
ing with richer contexts and smoothing. In Proc.
NAACL.
M. Johnson, T.L. Griffiths, and S. Goldwater. 2007.
Adaptor grammars: A framework for specifying
compositional nonparametric Bayesian models. In
Proc. NIPS.
Y. Kawata and J. Bartels. 2000. Stylebook for the
Japanese Treebank in VERBMOBIL. Technical re-
port, Eberhard-Karls-Universitat Tubingen.
D. Klein and C. Manning. 2004. Corpus-based induc-
tion of syntactic structure: Models of dependency
and constituency. In Proc. ACL.
M.T. Kromann, L. Mikkelsen, and S.K. Lynge. 2003.
Danish Dependency Treebank. In Proc. TLT.
P. Liang, S. Petrov, M.I. Jordan, and D. Klein. 2007.
The infinite PCFG using hierarchical Dirichlet pro-
cesses. In Proc. EMNLP.
P. Liang, M.I. Jordan, and D. Klein. 2009. Learn-
ing from measurements in exponential families. In
Proc. ICML.
G. Mann and A. McCallum. 2007. Simple, robust,
scalable semi-supervised learning via expectation
regularization. In Proc. ICML.
G. Mann and A. McCallum. 2008. Generalized expec-
tation criteria for semi-supervised learning of condi-
tional random fields. In Proc. ACL.
M. Marcus, M. Marcinkiewicz, and B. Santorini.
1993. Building a large annotated corpus of En-
glish: The Penn Treebank. Computational Linguis-
tics, 19(2):313?330.
R. Neal and G. Hinton. 1998. A new view of the EM
algorithm that justifies incremental, sparse and other
variants. In M. I. Jordan, editor, Learning in Graph-
ical Models, pages 355?368. MIT Press.
J. Nilsson, J. Hall, and J. Nivre. 2005. MAMBA meets
TIGER: Reconstructing a Swedish treebank from
antiquity. NODALIDA Special Session on Tree-
banks.
K. Oflazer, B. Say, D.Z. Hakkani-T?r, and G. T?r.
2003. Building a Turkish treebank. Treebanks:
Building and Using Parsed Corpora.
K. Simov, P. Osenova, M. Slavcheva, S. Kolkovska,
E. Balabanova, D. Doikoff, K. Ivanova, A. Simov,
E. Simov, and M. Kouylekov. 2002. Building a lin-
guistically interpreted corpus of bulgarian: the bul-
treebank. In Proc. LREC.
N. Smith and J. Eisner. 2006. Annealing structural
bias in multilingual weighted grammar induction. In
Proc. ACL.
L. Van der Beek, G. Bouma, R. Malouf, and G. Van No-
ord. 2002. The Alpino dependency treebank. Lan-
guage and Computers.
199
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 793?803,
Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational Linguistics
Large-Scale Cross-Document Coreference Using
Distributed Inference and Hierarchical Models
Sameer Singh? Amarnag Subramanya? Fernando Pereira? Andrew McCallum?
? Department of Computer Science, University of Massachusetts, Amherst MA 01002
? Google Research, Mountain View CA 94043
sameer@cs.umass.edu, asubram@google.com, pereira@google.com, mccallum@cs.umass.edu
Abstract
Cross-document coreference, the task of
grouping all the mentions of each entity in a
document collection, arises in information ex-
traction and automated knowledge base con-
struction. For large collections, it is clearly
impractical to consider all possible groupings
of mentions into distinct entities. To solve
the problem we propose two ideas: (a) a dis-
tributed inference technique that uses paral-
lelism to enable large scale processing, and
(b) a hierarchical model of coreference that
represents uncertainty over multiple granular-
ities of entities to facilitate more effective ap-
proximate inference. To evaluate these ideas,
we constructed a labeled corpus of 1.5 million
disambiguated mentions in Web pages by se-
lecting link anchors referring to Wikipedia en-
tities. We show that the combination of the
hierarchical model with distributed inference
quickly obtains high accuracy (with error re-
duction of 38%) on this large dataset, demon-
strating the scalability of our approach.
1 Introduction
Given a collection of mentions of entities extracted
from a body of text, coreference or entity resolu-
tion consists of clustering the mentions such that
two mentions belong to the same cluster if and
only if they refer to the same entity. Solutions to
this problem are important in semantic analysis and
knowledge discovery tasks (Blume, 2005; Mayfield
et al, 2009). While significant progress has been
made in within-document coreference (Ng, 2005;
Culotta et al, 2007; Haghighi and Klein, 2007;
Bengston and Roth, 2008; Haghighi and Klein,
2009; Haghighi and Klein, 2010), the larger prob-
lem of cross-document coreference has not received
as much attention.
Unlike inference in other language processing
tasks that scales linearly in the size of the corpus,
the hypothesis space for coreference grows super-
exponentially with the number of mentions. Conse-
quently, most of the current approaches are devel-
oped on small datasets containing a few thousand
mentions. We believe that cross-document coref-
erence resolution is most useful when applied to a
very large set of documents, such as all the news ar-
ticles published during the last 20 years. Such a cor-
pus would have billions of mentions. In this paper
we propose a model and inference algorithms that
can scale the cross-document coreference problem
to corpora of that size.
Much of the previous work in cross-document
coreference (Bagga and Baldwin, 1998; Ravin and
Kazi, 1999; Gooi and Allan, 2004; Pedersen et al,
2006; Rao et al, 2010) groups mentions into entities
with some form of greedy clustering using a pair-
wise mention similarity or distance function based
on mention text, context, and document-level statis-
tics. Such methods have not been shown to scale up,
and they cannot exploit cluster features that cannot
be expressed in terms of mention pairs. We provide
a detailed survey of related work in Section 6.
Other previous work attempts to address some of
the above concerns by mapping coreference to in-
ference on an undirected graphical model (Culotta
et al, 2007; Poon et al, 2008; Wellner et al, 2004;
Wick et al, 2009a). These models contain pair-
wise factors between all pairs of mentions captur-
ing similarity between them. Many of these mod-
els also enforce transitivity and enable features over
793
Filmmaker
Rapper
BEIJING, Feb. 21? Kevin Smith, who played the god of war in the "Xena"...
... The Physiological Basis of Politics,? by Kevin B. Smith, Douglas Oxley, Matthew Hibbing...
The filmmaker Kevin Smith returns to the role of Silent Bob...
Like Back in 2008, the Lions drafted Kevin Smith, even though Smith was badly...
Firefighter Kevin Smith spent almost 20 years preparing for Sept. 11. When he...
...shorthanded backfield in the wake of Kevin Smith's knee injury, and the addition of Haynesworth...
...were coming,'' said Dallas cornerback Kevin Smith. ''We just didn't know when...
...during the late 60's and early 70's, Kevin Smith worked with several local...
...the term hip-hop is attributed to Lovebug Starski. What does it actually mean...
Nothing could be more irrelevant to Kevin Smith's audacious ''Dogma'' than ticking off...
Cornerback
Firefighter
Actor
Running back
Author
Figure 1: Cross-Document Coreference Problem: Example mentions of ?Kevin Smith? from New York
Times articles, with the true entities shown on the right.
entities by including set-valued variables. Exact in-
ference in these models is intractable and a number
of approximate inference schemes (McCallum et al,
2009; Rush et al, 2010; Martins et al, 2010) may
be used. In particular, Markov chain Monte Carlo
(MCMC) based inference has been found to work
well in practice. However as the number of men-
tions grows to Web scale, as in our problem of cross-
document coreference, even these inference tech-
niques become infeasible, motivating the need for
a scalable, parallelizable solution.
In this work we first distribute MCMC-based in-
ference for the graphical model representation of
coreference. Entities are distributed across the ma-
chines such that the parallel MCMC chains on the
different machines use only local proposal distribu-
tions. After a fixed number of samples on each ma-
chine, we redistribute the entities among machines
to enable proposals across entities that were pre-
viously on different machines. In comparison to
the greedy approaches used in related work, our
MCMC-based inference provides better robustness
properties.
As the number of mentions becomes large, high-
quality samples for MCMC become scarce. To
facilitate better proposals, we present a hierarchi-
cal model. We add sub-entity variables that repre-
sent clusters of similar mentions that are likely to
be coreferent; these are used to propose composite
jumps that move multiple mentions together. We
also introduce super-entity variables that represent
clusters of similar entities; these are used to dis-
tribute entities among the machines such that similar
entities are assigned to the same machine. These ad-
ditional levels of hierarchy dramatically increase the
probability of beneficial proposals even with a large
number of entities and mentions.
To create a large corpus for evaluation, we iden-
tify pages that have hyperlinks to Wikipedia, and ex-
tract the anchor text and the context around the link.
We treat the anchor text as the mention, the con-
text as the document, and the title of the Wikipedia
page as the entity label. Using this approach, 1.5
million mentions were annotated with 43k entity la-
bels. On this dataset, our proposed model yields a
B3 (Bagga and Baldwin, 1998) F1 score of 73.7%,
improving over the baseline by 16% absolute (corre-
sponding to 38% error reduction). Our experimen-
tal results also show that our proposed hierarchical
model converges much faster even though it contains
many more variables.
2 Cross-document Coreference
The problem of coreference is to identify the sets of
mention strings that refer to the same underlying en-
tity. The identities and the number of the underlying
entities is not known. In within-document corefer-
ence, the mentions occur in a single document. The
number of mentions (and entities) in each document
is usually in the hundreds. The difficulty of the task
arises from a large hypothesis space (exponential in
the number of mentions) and challenge in resolv-
ing nominal and pronominal mentions to the correct
named mentions. In most cases, named mentions
794
are not ambiguous within a document. In cross-
document coreference, the number of mentions and
entities is in the millions, making the combinatorics
even more daunting. Furthermore, naming ambigu-
ity is much more common as the same string can
refer to multiple entities in different documents, and
distinct strings may refer to the same entity in differ-
ent documents.
We show examples of ambiguities in Figure 1.
Resolving the identity of individuals with the same
name is a common problem in cross-document
coreference. This problem is further complicated
by the fact that in some situations, these individ-
uals may belong to the same field. Another com-
mon ambiguity is that of alternate names, in which
the same entity is referred to by different names or
aliases (e.g. ?Bill? is often used as a substitute for
?William?). The figure also shows an example of
the renaming ambiguity ? ?Lovebug Starski? refers
to ?Kevin Smith?, and this is an extreme form of al-
ternate names. Rare singleton entities (like the fire-
fighter) that may appear only once in the whole cor-
pus are also often difficult to isolate.
2.1 Pairwise Factor Model
Factor graphs are a convenient representation for a
probability distribution over a vector of output vari-
ables given observed variables. The model that we
use for coreference represents mentions (M) and en-
tities (E) as random variables. Each mention can
take an entity as its value, and each entity takes a set
of mentions as its value. Each mention also has a
feature vector extracted from the observed text men-
tion and its context. More precisely, the probability
of a configuration E = e is defined by
p(e) ? exp
?
e?e
{?
m,n?e,n 6=m ?a(m,n)
+
?
m?e,n/?e ?r(m,n)
}
where factor ?a represents affinity between men-
tions that are coreferent according to e, and factor
?r represents repulsion between mentions that are
not coreferent. Different factors are instantiated for
different predicted configurations. Figure 2 shows
the model instantiated with five mentions over a two-
entity hypothesis.
For the factor potentials, we use cosine sim-
ilarity of mention context pairs (?mn) such that
m1
m2
m3
m4
m5
e1
e2
Figure 2: Pairwise Coreference Model: Factor
graph for a 2-entity configuration of 5 mentions.
Affinity factors are shown with solid lines, and re-
pulsion factors with dashed lines.
?a(m,n) = ?mn ? b and ?r(m,n) = ?(?mn ? b),
where b is the bias. While one can certainly make
use of a more sophisticated feature set, we leave this
for future work as our focus is to scale up inference.
However, it should be noted that this approach is
agnostic to the particular set of features used. As
we will note in the next section, we do not need to
calculate features between all pairs of mentions (as
would be prohibitively expensive for large datasets);
instead we only compute the features as and when
required.
2.2 MCMC-based Inference
Given the above model of coreference, we seek the
maximum a posteriori (MAP) configuration:
e? = argmaxe p(e)
= argmaxe
?
e?e
{?
m,n?e,n 6=m ?a(m,n)
+
?
m?e,n/?e ?r(m,n)
}
Computing e? exactly is intractable due to the
large space of possible configurations.1 Instead,
we employ MCMC-based optimization to discover
the MAP configuration. A proposal function q is
used to propose a change e? to the current config-
uration e. This jump is accepted with the following
Metropolis-Hastings acceptance probability:
?(e, e?) = min
(
1,
(
p(e?)
p(e)
)1/t q(e)
q(e?)
)
(1)
1Number of possible entities is Bell(n) in the number of
mentions, i.e. number of partitions of n items
795
where t is the annealing temperature parameter.
MCMC chains efficiently explore the high-
density regions of the probability distribution. By
slowly reducing the temperature, we can decrease
the entropy of the distribution to encourage con-
vergence to the MAP configuration. MCMC has
been used for optimization in a number of related
work (McCallum et al, 2009; Goldwater and Grif-
fiths, 2007; Changhe et al, 2004).
The proposal function moves a randomly chosen
mention l from its current entity es to a randomly
chosen entity et. For such a proposal, the log-model
ratio is:
log
p(e?)
p(e)
=
?
m?et
?a(l,m) +
?
n?es
?r(l, n)
?
?
n?es
?a(l, n)?
?
m?et
?r(l,m) (2)
Note that since only the factors between mention l
and mentions in es and et are involved in this com-
putation, the acceptance probability of each proposal
is calculated efficiently.
In general, the model may contain arbitrarily
complex set of features over pairs of mentions, with
parameters associated with them. Given labeled
data, these parameters can be learned by Percep-
tron (Collins, 2002), which uses the MAP config-
uration according to the model (e?). There also exist
more efficient training algorithms such as SampleR-
ank (McCallum et al, 2009; Wick et al, 2009b) that
update parameters during inference. However, we
only focus on inference in this work, and the only
parameter that we set manually is the bias b, which
indirectly influences the number of entities in e?. Un-
less specified otherwise, in this work the initial con-
figuration for MCMC is the singleton configuration,
i.e. all entities have a size of 1.
This MCMC inference technique, which has been
used in McCallum and Wellner (2004), offers sev-
eral advantages over other inference techniques: (a)
unlike message-passing-methods, it does not require
the full ground graph, (b) we only have to exam-
ine the factors that lie within the changed entities
to evaluate a proposal, and (c) inference may be
stopped at any point to obtain the current best con-
figuration. However, the super exponential nature of
the hypothesis space in cross-doc coreference ren-
ders this algorithm computationally unsuitable for
large scale coreference tasks. In particular, fruit-
ful proposals (that increase the model score) are ex-
tremely rare, resulting in a large number of propos-
als that are not accepted. We describe methods to
speed up inference by 1) evaluating multiple pro-
posal simultaneously (Section 3), and 2) by aug-
menting our model with hierarchical variables that
enable better proposal distributions (Section 4).
3 Distributed MAP Inference
The key observation that enables distribution is that
the acceptance probability computation of a pro-
posal only examines a few factors that are not com-
mon to the previous and next configurations (Eq. 2).
Consider a pair of proposals, one that moves men-
tion l from entity es to entity et, and the other that
moves mention l? from entity e?s to entity e
?
t. The
set of factors to compute acceptance of the first pro-
posal are factors between l and mentions in es and
et, while the set of factors required to compute ac-
ceptance of the second proposal lie between l? and
mentions in e?s and e
?
t. Since these set of factors
are completely disjoint from each other, and the re-
sulting configurations do not depend on each other,
these two proposals are mutually-exclusive. Differ-
ent orders of evaluating such proposals are equiv-
alent, and in fact, these proposals can be proposed
and evaluated concurrently. This mutual-exclusivity
is not restricted only to pairs of proposals; a set of
proposals are mutually-exclusive if no two propos-
als require the same factor for evaluation.
Using this insight, we introduce the following ap-
proach to distributed cross-document coreference.
We divide the mentions and entities among multiple
machines, and propose moves of mentions between
entities assigned to the same machine. These jumps
are evaluated exactly and accepted without commu-
nication between machines. Since acceptance of a
mention?s move requires examining factors that lie
between other mentions in its entity, we ensure that
all mentions of an entity are assigned the same ma-
chine. Unless specified otherwise, the distribution is
performed randomly. To enable exploration of the
complete configuration space, rounds of sampling
are interleaved by redistribution stages, in which the
entities are redistributed among the machines (see
Figure 3). We use MapReduce (Dean and Ghe-
796
Distributor
Inference
Inference
Figure 3: Distributed MCMC-based Inference:
Distributor divides the entities among the machines,
and the machines run inference. The process is re-
peated by the redistributing the entities.
mawat, 2004) to manage the distributed computa-
tion.
This approach to distribution is equivalent to in-
ference with all mentions and entities on a single
machine with a restricted proposer, but is faster
since it exploits independencies to propose multiple
jumps simultaneously. By restricting the jumps as
described above, the acceptance probability calcu-
lation is exact. Partitioning the entities and propos-
ing local jumps are restrictions to the single-machine
proposal distribution; redistribution stages ensure
the equivalent Markov chains are still irreducible.
See Singh et al (2010) for more details.
4 Hierarchical Coreference Model
The proposal function for MCMC-based MAP infer-
ence presents changes to the current entities. Since
we use MCMC to reach high-scoring regions of the
hypothesis space, we are interested in the changes
that improve the current configuration. But as the
number of mentions and entities increases, these
fruitful samples become extremely rare due to the
blowup in the possible space of configurations, re-
sulting in rejection of a large number of proposals.
By distributing as described in the previous section,
we propose samples in parallel, improving chances
of finding changes that result in better configura-
tions. However, due to random redistribution and a
naive proposal function within each machine, a large
fraction of proposals are still wasted. We address
these concerns by adding hierarchy to the model.
4.1 Sub-Entities
Consider the task of proposing moves of mentions
(within a machine). Given the large number of
mentions and entities, the probability that a ran-
domly picked mention that is moved to a random
entity results in a better configuration is extremely
small. If such a move is accepted, this gives us ev-
idence that the mention did not belong to the pre-
vious entity, and we should also move similar men-
tions from the previous entity simultaneously to the
same entity. Since the proposer moves only a sin-
gle mention at a time, a large number of samples
may be required to discover these fruitful moves.
To enable block proposals that move similar men-
tions simultaneously, we introduce latent sub-entity
variables that represent groups of similar mentions
within an entity, where the similarity is defined by
the model. For inference, we have stages of sam-
pling sub-entities (moving individual mentions) in-
terleaved with stages of entity sampling (moving all
mentions within a sub-entity). Even though our con-
figuration space has become larger due to these ex-
tra variables, the proposal distribution has also im-
proved since it proposes composite moves.
4.2 Super-Entities
Another issue faced during distributed inference is
that random redistribution is often wasteful. For ex-
ample, if dissimilar entities are assigned to a ma-
chine, none of the proposals may be accepted. For a
large number of entities and machines, the probabil-
ity that similar entities will be assigned to the same
machine is extremely small, leading to a larger num-
ber of wasted proposals. To alleviate this problem,
we introduce super-entities that represent groups of
similar entities. During redistribution, we ensure all
entities in the same super-entity are assigned to the
same machine. As for sub-entities above, inference
switches between regular sampling of entities and
sampling of super-entities (by moving entities). Al-
though these extra variables have made the config-
uration space larger, they also allow more efficient
distribution of entities, leading to useful proposals.
4.3 Combined Hierarchical Model
Each of the described levels of the hierarchy are sim-
ilar to the initial model (Section 2.1): mentions/sub-
entities have the same structure as the entities/super-
entities, and are modeled using similar factors. To
represent the ?context? of a sub-entity we take the
union of the bags-of-words of the constituent men-
tion contexts. Similarly, we take the union of sub-
797
Super-Entities
Entities
Mentions
Sub-Entities
Figure 4: Combined Hierarchical Model with factors instantiated for a hypothesis containing 2 super-
entities, 4 entities, and 8 sub-entities, shown as colored circles, over 16 mentions. Dotted lines represent
repulsion factors and solid lines represent affinity factors (the color denotes the type of variable that the
factor touches). The boxes on factors were excluded for clarity.
entity contexts to represent the context of an entity.
The factors are instantiated in the same manner as
Section 2.1 except that we change the bias factor
b for each level (increasing it for sub-entities, and
decreasing it for super-entities). The exact values
of these biases indirectly determines the number of
predicted sub-entities and super-entities.
Since these two levels of hierarchy operate at
separate granularities from each other, we combine
them into a single hierarchical model that contains
both sub- and super-entities. We illustrate this hi-
erarchical structure in Figure 4. Inference for this
model takes a round-robin approach by fixing two
of the levels of the hierarchy and sampling the third,
cycling through these three levels. Unless specified
otherwise, the initial configuration is the singleton
configuration, in which all sub-entities, entities, and
super-entities are of size 1.
5 Experiments
We evaluate our models and algorithms on a number
of datasets. First, we compare performance on the
small, publicly-available ?John Smith? dataset. Sec-
ond, we run the automated Person-X evaluation to
obtain thousands of mentions that we use to demon-
strate accuracy and scalability improvements. Most
importantly, we create a large labeled corpus using
links to Wikipedia to explore the performance in the
large-scale setting.
5.1 John Smith Corpus
To compare with related work, we run an evalua-
tion on the ?John Smith? corpus (Bagga and Bald-
win, 1998), containing 197 mentions of the name
?John Smith? from New York Times articles (la-
beled to obtain 35 true entities). The bias b for
our approach is set to result in the correct number
of entities. Our model achieves B3 F1 accuracy of
66.4% on this dataset. In comparison, Rao et al
(2010) obtains 61.8% using the model most similar
to ours, while their best model (which uses sophis-
ticated topic-model features that do not scale easily)
achieves 69.7%. It is encouraging to note that our
approach, using only a subset of the features, per-
forms competitively with related work. However,
due to the small size of the dataset, we require fur-
ther evaluation before reaching any conclusions.
5.2 Person-X Evaluation
There is a severe lack of labeled corpora for cross-
document coreference due to the effort required
to evaluate the coreference decisions. Related
approaches have used automated Person-X evalu-
ation (Gooi and Allan, 2004), in which unique
person-name strings are treated as the true entity
labels for the mentions. Every mention string is
replaced with an ?X? for the coreference system.
We use this evaluation methodology on 25k person-
name mentions from the New York Times cor-
pus (Sandhaus, 2008) each with one of 50 unique
strings. As before, we set the bias b to achieve the
same number of entities. We use 1 million samples
in each round of inference, followed by random re-
distribution in the flat model, and super-entities in
the hierarchical model. Results are averaged over
five runs.
798
Figure 5: Person-X Evaluation of Pairwise model:
Performance as number of machines is varied, aver-
aged over 5 runs.
Number of Entities 43,928
Number of Mentions 1,567,028
Size of Largest Entity 6,096
Average Mentions per Entity 35.7
Variance of Mentions per Entity 5191.7
Table 1: Wikipedia Link Corpus Statistics. Size
of an entity is the number of mentions of that entity.
Figure 5 shows accuracy compared to relative
wallclock running time for distributed inference on
the flat, pairwise model. Speed and accuracy im-
prove as additional machines are added, but larger
number of machines lead to diminishing returns for
this small dataset. Distributed inference on our hi-
erarchical model is evaluated in Figure 6 against in-
ference on the pairwise model from Figure 5. We
see that the individual hierarchical models perform
much better than the pairwise model; they achieve
the same accuracy as the pairwise model in approx-
imately 10% of the time. Moreover, distributed in-
ference on the combined hierarchical model is both
faster and more accurate than the individual hierar-
chical models.
5.3 Wikipedia Link Corpus
To explore the application of the proposed approach
to a larger, realistic dataset, we construct a corpus
based on the insight that links to Wikipedia that ap-
pear on webpages can be treated as mentions, and
since the links were added manually by the page au-
thor, we use the destination Wikipedia page as the
Figure 6: Person-X Evaluation of Hierarchical
Models: Performance of inference on hierarchical
models compared to the pairwise model. Experi-
ments were run using 50 machines.
entity the link refers to.
The dataset is created as follows: First, we crawl
the web and select hyperlinks on webpages that link
to an English Wikipedia page.2 The anchors of
these links form our set of mentions, with the sur-
rounding block of clean text (obtained after remov-
ing markup, etc.) around each link being its con-
text. We assign the title of the linked Wikipedia
page as the entity label of that link. Since this set
of mentions and labels can be noisy, we use the
following filtering steps. All links that have less
than 36 words in their block, or whose anchor text
has a large string edit distance from the title of the
Wikipedia page, are discarded. While this results in
cases in which ?President? is discarded when linked
to the ?Barack Obama? Wikipedia page, it was nec-
essary to reduce noise. Further, we also discard
links to Wikipedia pages that are concepts (such as
?public_domain?) rather than entities. All enti-
ties with less than 6 links to them are also discarded.
Table 1 shows some statistics about our automat-
ically generated data set. We randomly sampled 5%
of the entities to create a development set, treating
the remaining entities as the test set. Unlike the
John Smith and Person-X evaluation, this data set
also contains non-person entities such as organiza-
tions and locations.
For our models, we augment the factor potentials
with mention-string similarity:
2e.g. http://en.wikipedia.org/Hillary_Clinton
799
?a/r(m,n) = ? (?mn ? b+ wSTREQ(m,n))
where STREQ is 1 if mentions m and n are string
identical (0 otherwise), and w is the weight to this
feature.3 In our experiments we found that setting
w = 0.8 and b = 1e? 4 gave the best results on the
development set.
Due to the large size of the corpus, existing cross-
document coreference approaches could not be ap-
plied to this dataset. However, since a majority
of related work consists of using clustering after
defining a similarity function (Section 6), we pro-
vide a baseline evaluation of clustering with Sub-
Square (Bshouty and Long, 2010), a scalable, dis-
tributed clustering method. Subsquare takes as in-
put a weighted graph with mentions as nodes and
similarity between mentions used as edge weights.
Subsquare works by stochastically assigning a ver-
tex to the cluster of one its neighbors if they have
significant neighborhood overlap. This algorithm
is an efficient form of approximate spectral cluster-
ing (Bshouty and Long, 2010), and since it is given
the same distances between mentions as our models,
we expect it to get similar accuracy. We also gen-
erate another baseline clustering by assigning men-
tions with identical strings to the same entity. This
mention-string clustering is also used as the initial
configuration of our inference.
Figure 7: Wikipedia Link Evaluation: Perfor-
mance of inference for different number of machines
(N = 100, 500). Mention-string match clustering is
used as the initial configuration.
3Note that we do not use mention-string similarity for John
Smith or Person-X as the mention strings are all identical.
Method
Pairwise B3 Score
P/ R F1 P/ R F1
String-Match 30.0 / 66.7 41.5 82.7 / 43.8 57.3
Subsquare 38.2 / 49.1 43.0 87.6 / 51.4 64.8
Our Model 44.2 / 61.4 51.4 89.4 / 62.5 73.7
Table 2: F1 Scores on the Wikipedia Link Data.
The results are significant at the 0.0001 level over
Subsquare according to the difference of proportions
significance test.
Inference is run for 20 rounds of 10 million sam-
ples each, distributed over N machines. We use
N = 100, 500 and the B3 F1 score results obtained
set for each case are shown in Figure 7. It can
be seen that N = 500 converges to a better solu-
tion faster, showing effective use of parallelism. Ta-
ble 2 compares the results of our approach (at con-
vergence for N = 500), the baseline mention-string
match and the Subsquare algorithm. Our approach
significantly outperforms the competitors.
6 Related Work
Although the cross-document coreference problem
is challenging and lacks large labeled datasets, its
ubiquitous role as a key component of many knowl-
edge discovery tasks has inspired several efforts.
A number of previous techniques use scoring
functions between pairs of contexts, which are then
used for clustering. One of the first approaches
to cross-document coreference (Bagga and Bald-
win, 1998) uses an idf-based cosine-distance scor-
ing function for pairs of contexts, similar to the one
we use. Ravin and Kazi (1999) extend this work to
be somewhat scalable by comparing pairs of con-
texts only if the mentions are deemed ?ambiguous?
using a heuristic. Others have explored multiple
methods of context similarity, and concluded that
agglomerative clustering provides effective means
of inference (Gooi and Allan, 2004). Pedersen et
al. (2006) and Purandare and Pedersen (2004) inte-
grate second-order co-occurrence of words into the
similarity function. Mann and Yarowsky (2003) use
biographical facts from the Web as features for clus-
tering. Niu et al (2004) incorporate information ex-
traction into the context similarity model, and anno-
tate a small dataset to learn the parameters. A num-
ber of other approaches include various forms of
800
hand-tuned weights, dictionaries, and heuristics to
define similarity for name disambiguation (Blume,
2005; Baron and Freedman, 2008; Popescu et al,
2008). These approaches are greedy and differ in the
choice of the distance function and the clustering al-
gorithm used. Daume? III and Marcu (2005) propose
a generative approach to supervised clustering, and
Haghighi and Klein (2010) use entity profiles to as-
sist within-document coreference.
Since many related methods use clustering, there
are a number of distributed clustering algorithms
that may help scale these approaches. Datta et
al. (2006) propose an algorithm for distributed k-
means. Chen et al (2010) describe a parallel spectral
clustering algorithm. We use the Subsquare algo-
rithm (Bshouty and Long, 2010) as baseline because
it works well in practice. Mocian (2009) presents a
survey of distributed clustering algorithms.
Rao et al (2010) have proposed an online deter-
ministic method that uses a stream of input mentions
and assigns them greedily to entities. Although it
can resolve mentions from non-trivial sized datasets,
the method is restricted to a single machine, which
is not scalable to the very large number of mentions
that are encountered in practice.
Our representation of the problem as an undi-
rected graphical model, and performing distributed
inference on it, provides a combination of advan-
tages not available in any of these approaches. First,
most of the methods will not scale to the hundreds
of millions of mentions that are present in real-world
applications. By utilizing parallelism across ma-
chines, our method can run on very large datasets
simply by increasing the number of machines used.
Second, approaches that use clustering are limited
to using pairwise distance functions for which ad-
ditional supervision and features are difficult to in-
corporate. In addition to representing features from
all of the related work, graphical models can also
use more complex entity-wide features (Culotta et
al., 2007; Wick et al, 2009a), and parameters can
be learned using supervised (Collins, 2002) or semi-
supervised techniques (Mann and McCallum, 2008).
Finally, the inference for most of the related ap-
proaches is greedy, and earlier decisions are not re-
visited. Our technique is based on MCMC inference
and simulated annealing, which are able to escape
local maxima.
7 Conclusions
Motivated by the problem of solving the corefer-
ence problem on billions of mentions from all of the
newswire documents from the past few decades, we
make the following contributions. First, we intro-
duce distributed version of MCMC-based inference
technique that can utilize parallelism to enable scal-
ability. Second, we augment the model with hierar-
chical variables that facilitate fruitful proposal distri-
butions. As an additional contribution, we use links
to Wikipedia pages to obtain a high-quality cross-
document corpus. Scalability and accuracy gains of
our method are evaluated on multiple datasets.
There are a number of avenues for future work.
Although we demonstrate scalability to more than a
million mentions, we plan to explore performance
on datasets in the billions. We also plan to examine
inference on complex coreference models (such as
with entity-wide factors). Another possible avenue
for future work is that of learning the factors. Since
our approach supports parameter estimation, we ex-
pect significant accuracy gains with additional fea-
tures and supervised data. Our work enables cross-
document coreference on very large corpora, and we
would like to explore the downstream applications
that can benefit from it.
Acknowledgments
This work was done when the first author was an
intern at Google Research. The authors would
like to thank Mark Dredze, Sebastian Riedel, and
anonymous reviewers for their valuable feedback.
This work was supported in part by the Center
for Intelligent Information Retrieval, the Univer-
sity of Massachusetts gratefully acknowledges the
support of Defense Advanced Research Projects
Agency (DARPA) Machine Reading Program under
Air Force Research Laboratory (AFRL) prime con-
tract no. FA8750-09-C-0181., in part by an award
from Google, in part by The Central Intelligence
Agency, the National Security Agency and National
Science Foundation under NSF grant #IIS-0326249,
in part by NSF grant #CNS-0958392, and in part
by UPenn NSF medium IIS-0803847. Any opin-
ions, findings and conclusions or recommendations
expressed in this material are those of the authors
and do not necessarily reflect those of the sponsor.
801
References
Amit Bagga and Breck Baldwin. 1998. Entity-based
cross-document coreferencing using the vector space
model. In International Conference on Computational
Linguistics, pages 79?85.
A. Baron and M. Freedman. 2008. Who is who and what
is what: experiments in cross-document co-reference.
In Empirical Methods in Natural Language Process-
ing (EMNLP), pages 274?283.
Eric Bengston and Dan Roth. 2008. Understanding
the value of features for coreference resolution. In
Empirical Methods in Natural Language Processing
(EMNLP).
Matthias Blume. 2005. Automatic entity disambigua-
tion: Benefits to NER, relation extraction, link anal-
ysis, and inference. In International Conference on
Intelligence Analysis (ICIA).
Nader H. Bshouty and Philip M. Long. 2010. Find-
ing planted partitions in nearly linear time using ar-
rested spectral clustering. In Johannes Fu?rnkranz
and Thorsten Joachims, editors, Proceedings of the
27th International Conference on Machine Learning
(ICML-10), pages 135?142, Haifa, Israel, June. Omni-
press.
Yuan Changhe, Lu Tsai-Ching, and Druzdzel Marek.
2004. Annealed MAP. In Uncertainty in Artificial In-
telligence (UAI), pages 628?635, Arlington , Virginia.
AUAI Press.
Wen-Yen Chen, Yangqiu Song, Hongjie Bai, Chih-Jen
Lin, and Edward Y. Chang. 2010. Parallel spectral
clustering in distributed systems. IEEE Transactions
on Pattern Analysis and Machine Intelligence.
Michael Collins. 2002. Discriminative training methods
for hidden markov models: Theory and experiments
with perceptron algorithm. In Annual Meeting of the
Association for Computational Linguistics (ACL).
Aron Culotta, Michael Wick, and Andrew McCallum.
2007. First-order probabilistic models for coreference
resolution. In North American Chapter of the Associa-
tion for Computational Linguistics - Human Language
Technologies (NAACL HLT).
S. Datta, C. Giannella, and H. Kargupta. 2006. K-Means
Clustering over a Large, Dynamic Network. In SIAM
Data Mining Conference (SDM).
Hal Daume? III and Daniel Marcu. 2005. A Bayesian
model for supervised clustering with the Dirichlet pro-
cess prior. Journal of Machine Learning Research
(JMLR), 6:1551?1577.
Jeffrey Dean and Sanjay Ghemawat. 2004. Mapreduce:
Simplified data processing on large clusters. Sympo-
sium on Operating Systems Design & Implementation
(OSDI).
Sharon Goldwater and Tom Griffiths. 2007. A fully
bayesian approach to unsupervised part-of-speech tag-
ging. In Annual Meeting of the Association for Com-
putational Linguistics (ACL), pages 744?751.
Chung Heong Gooi and James Allan. 2004. Cross-
document coreference on a large scale corpus. In
North American Chapter of the Association for Com-
putational Linguistics - Human Language Technolo-
gies (NAACL HLT), pages 9?16.
Aria Haghighi and Dan Klein. 2007. Unsupervised
coreference resolution in a nonparametric bayesian
model. In Annual Meeting of the Association for Com-
putational Linguistics (ACL), pages 848?855.
Aria Haghighi and Dan Klein. 2009. Simple coreference
resolution with rich syntactic and semantic features. In
Empirical Methods in Natural Language Processing
(EMNLP), pages 1152?1161.
Aria Haghighi and Dan Klein. 2010. Coreference reso-
lution in a modular, entity-centered model. In North
American Chapter of the Association for Computa-
tional Linguistics - Human Language Technologies
(NAACL HLT), pages 385?393.
Gideon S. Mann and Andrew McCallum. 2008. General-
ized expectation criteria for semi-supervised learning
of conditional random fields. In Annual Meeting of
the Association for Computational Linguistics (ACL),
pages 870?878.
Gideon S. Mann and David Yarowsky. 2003. Unsuper-
vised personal name disambiguation. In North Amer-
ican Chapter of the Association for Computational
Linguistics - Human Language Technologies (NAACL
HLT), pages 33?40.
Andre Martins, Noah Smith, Eric Xing, Pedro Aguiar,
and Mario Figueiredo. 2010. Turbo parsers: Depen-
dency parsing by approximate variational inference.
In Empirical Methods in Natural Language Process-
ing (EMNLP), pages 34?44, Cambridge, MA, October.
Association for Computational Linguistics.
J. Mayfield, D. Alexander, B. Dorr, J. Eisner, T. Elsayed,
T. Finin, C. Fink, M. Freedman, N. Garera, P. Mc-
Namee, et al 2009. Cross-document coreference res-
olution: A key technology for learning by reading. In
AAAI Spring Symposium on Learning by Reading and
Learning to Read.
Andrew McCallum and Ben Wellner. 2004. Conditional
models of identity uncertainty with application to noun
coreference. In Neural Information Processing Sys-
tems (NIPS).
Andrew McCallum, Karl Schultz, and Sameer Singh.
2009. FACTORIE: Probabilistic programming via im-
peratively defined factor graphs. In Neural Informa-
tion Processing Systems (NIPS).
Horatiu Mocian. 2009. Survey of Distributed Clustering
Techniques. Ph.D. thesis, Imperial College of London.
802
Vincent Ng. 2005. Machine learning for coreference res-
olution: From local classification to global ranking. In
Annual Meeting of the Association for Computational
Linguistics (ACL).
Cheng Niu, Wei Li, and Rohini K. Srihari. 2004. Weakly
supervised learning for cross-document person name
disambiguation supported by information extraction.
In Annual Meeting of the Association for Computa-
tional Linguistics (ACL), page 597.
Ted Pedersen, Anagha Kulkarni, Roxana Angheluta, Zor-
nitsa Kozareva, and Thamar Solorio. 2006. An
unsupervised language independent method of name
discrimination using second order co-occurrence fea-
tures. In International Conference on Intelligent Text
Processing and Computational Linguistics (CICLing),
pages 208?222.
Hoifung Poon, Pedro Domingos, and Marc Sumner.
2008. A general method for reducing the complexity
of relational inference and its application to MCMC.
In AAAI Conference on Artificial Intelligence.
Octavian Popescu, Christian Girardi, Emanuele Pianta,
and Bernardo Magnini. 2008. Improving cross-
document coreference. Journe?es Internationales
d?Analyse statistique des Donne?es Textuelles, 9:961?
969.
A. Purandare and T. Pedersen. 2004. Word sense dis-
crimination by clustering contexts in vector and simi-
larity spaces. In Conference on Computational Natu-
ral Language Learning (CoNLL), pages 41?48.
Delip Rao, Paul McNamee, and Mark Dredze. 2010.
Streaming cross document entity coreference reso-
lution. In International Conference on Computa-
tional Linguistics (COLING), pages 1050?1058, Bei-
jing, China, August. Coling 2010 Organizing Commit-
tee.
Yael Ravin and Zunaid Kazi. 1999. Is Hillary Rodham
Clinton the president? disambiguating names across
documents. In Annual Meeting of the Association for
Computational Linguistics (ACL), pages 9?16.
Alexander M Rush, David Sontag, Michael Collins, and
Tommi Jaakkola. 2010. On dual decomposition and
linear programming relaxations for natural language
processing. In Empirical Methods in Natural Lan-
guage Processing (EMNLP), pages 1?11, Cambridge,
MA, October. Association for Computational Linguis-
tics.
Evan Sandhaus. 2008. The New York Times annotated
corpus. Linguistic Data Consortium.
Sameer Singh, Amarnag Subramanya, Fernando Pereira,
and Andrew McCallum. 2010. Distributed map in-
ference for undirected graphical models. In Neural
Information Processing Systems (NIPS), Workshop on
Learning on Cores, Clusters and Clouds.
Ben Wellner, Andrew McCallum, Fuchun Peng, and
Michael Hay. 2004. An integrated, conditional model
of information extraction and coreference with appli-
cation to citation matching. In Uncertainty in Artificial
Intelligence (UAI), pages 593?601.
Michael Wick, Aron Culotta, Khashayar Rohanimanesh,
and Andrew McCallum. 2009a. An entity-based
model for coreference resolution. In SIAM Interna-
tional Conference on Data Mining (SDM).
Michael Wick, Khashayar Rohanimanesh, Aron Culotta,
and Andrew McCallum. 2009b. Samplerank: Learn-
ing preferences from atomic gradients. In Neural In-
formation Processing Systems (NIPS), Workshop on
Advances in Ranking.
803
