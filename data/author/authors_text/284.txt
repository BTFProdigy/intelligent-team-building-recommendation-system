Proceedings of the COLING/ACL 2006 Main Conference Poster Sessions, pages 675?682,
Sydney, July 2006. c?2006 Association for Computational Linguistics
Argumentative Feedback: A Linguistically-motivated Term
Expansion for Information Retrieval
Patrick Ruch, Imad Tbahriti, Julien Gobeill
Medical Informatics Service
University of Geneva
24 Micheli du Crest
1201 Geneva
Switzerland
{patrick.ruch,julien.gobeill,imad.tbahriti}@hcuge.ch
Alan R. Aronson
Lister Hill Center
National Library of Medicine
8600 Rockville Pike
Bethesda, MD 20894
USA
alan@nlm.nih.gov
Abstract
We report on the development of a new au-
tomatic feedback model to improve informa-
tion retrieval in digital libraries. Our hy-
pothesis is that some particular sentences,
selected based on argumentative criteria,
can be more useful than others to perform
well-known feedback information retrieval
tasks. The argumentative model we ex-
plore is based on four disjunct classes, which
has been very regularly observed in scien-
tific reports: PURPOSE, METHODS, RE-
SULTS, CONCLUSION. To test this hy-
pothesis, we use the Rocchio algorithm as
baseline. While Rocchio selects the fea-
tures to be added to the original query
based on statistical evidence, we propose
to base our feature selection also on argu-
mentative criteria. Thus, we restrict the ex-
pansion on features appearing only in sen-
tences classified into one of our argumen-
tative categories. Our results, obtained on
the OHSUMED collection, show a signifi-
cant improvement when expansion is based
on PURPOSE (mean average precision =
+23%) and CONCLUSION (mean average
precision = +41%) contents rather than on
other argumentative contents. These results
suggest that argumentation is an important
linguistic dimension that could benefit in-
formation retrieval.
1 Introduction
Information retrieval (IR) is a challenging en-
deavor due to problems caused by the underly-
ing expressiveness of all natural languages. One
of these problems, synonymy, is that authors
and users frequently employ different words or
expressions to refer to the same meaning (acci-
dent may be expressed as event, incident, prob-
lem, difficulty, unfortunate situation, the subject
of your last letter, what happened last week, etc.)
(Furnas et al, 1987). Another problem is ambi-
guity, where a specific term may have several
(and sometimes contradictory) meanings and
interpretations (e.g., the word horse as in Tro-
jan horse, light horse, to work like a horse, horse
about). In order to obtain better meaning-based
matches between queries and documents, vari-
ous propositions have been suggested, usually
without giving any consideration to the under-
lying domain.
During our participation in different interna-
tional evaluation campaigns such as the TREC
Genomics track (Hersh, 2005), the BioCreative
initiative (Hirschman et al, 2005), as well as
in our attempts to deliver advanced search
tools for biologists (Ruch, 2006) and health-
care providers (Ruch, 2002) (Ruch, 2004), we
were more concerned with domain-specific in-
formation retrieval in which systems must re-
turn a ranked list of MEDLINE records in re-
sponse to an expert?s information request. This
involved a set of available queries describing
typical search interests, in which gene, pro-
tein names, and diseases were often essential
for an effective retrieval. Biomedical publica-
tions however tend to generate new informa-
tion very rapidly and also use a wide varia-
tion in terminology, thus leading to the cur-
rent situation whereby a large number of names,
symbols and synonyms are used to denote the
same concepts. Current solutions to these issues
can be classified into domain-specific strate-
gies, such as thesaurus-based expansion, and
domain-independent strategies, such as blind-
feedback. By proposing to explore a third type
of approach, which attempts to take advan-
tage of argumentative specificities of scientific
reports, our study initiates a new research di-
rection for natural language processing applied
to information retrieval.
The rest of this paper is organized as follows.
Section 2 presents some related work in infor-
mation retrieval and in argumentative parsing,
while Section 3 depicts the main characteristics
of our test collection and the metrics used in
our experiments. Section 4 details the strategy
675
used to develop our improved feedback method.
Section 5 reports on results obtained by varying
our model and Section 6 contains conclusions on
our experiments.
2 Related works
Our basic experimental hypothesis is that some
particular sentences, selected based on argu-
mentative categories, can be more useful than
others to support well-known feedback informa-
tion retrieval tasks. It means that selecting sen-
tences based on argumentative categories can
help focusing on content-bearing sections of sci-
entific articles.
2.1 Argumentation
Originally inspired by corpus linguistics studies
(Orasan, 2001), which suggests that scientific
reports (in chemistry, linguistics, computer sci-
ences, medicine...) exhibit a very regular logi-
cal distribution -confirmed by studies conducted
on biomedical corpora (Swales, 1990) and by
ANSI/ISO professional standards - the argu-
mentative model we experiment is based on four
disjunct classes: PURPOSE, METHODS, RE-
SULTS, CONCLUSION.
Argumentation belongs to discourse analy-
sis1, with fairly complex computational mod-
els such as the implementation of the rhetori-
cal structure theory proposed by (Marcu, 1997),
which proposes dozens of rhetorical classes.
More recent advances were applied to docu-
ment summarization. Of particular interest for
our approach, Teufel and Moens (Teufel and
Moens, 1999) propose using a list of manually
crafted triggers (using both words and expres-
sions such as we argued, in this article, the
paper is an attempt to, we aim at, etc.) to
automatically structure scientific articles into
a lighter model, with only seven categories:
BACKGROUND, TOPIC, RELATED WORK,
PURPOSE, METHOD, RESULT, and CON-
CLUSION.
More recently and for knowledge discovery in
molecular biology, more elaborated models were
proposed by (Mizuta and Collier, 2004) (Mizuta
et al, 2005) and by (Lisacek et al, 2005) for
novelty-detection. (McKnight and Srinivasan,
2003) propose a model very similar to our four-
class model but is inspired by clinical trials.
Preliminary applications were proposed for bib-
1After Aristotle, discourses structured following an
appropriate argumentative distribution belong to logics,
while ill-defined ones belong to rhetorics.
liometrics and related-article search (Tbahriti
et al, 2004) (Tbahriti et al, 2005), informa-
tion extraction and passage retrieval (Ruch et
al., 2005b). In these studies, sentences were se-
lected as the basic classification unit in order
to avoid as far as possible co-reference issues
(Hirst, 1981), which hinder readibity of auto-
matically generated and extracted sentences.
2.2 Query expansion
Various query expansion techniques have been
suggested to provide a better match between
user information needs and documents, and to
increase retrieval effectiveness. The general
principle is to expand the query using words
or phrases having a similar or related meaning
to those appearing in the original request. Vari-
ous empirical studies based on different IR mod-
els or collections have shown that this type of
search strategy should usually be effective in en-
hancing retrieval performance. Scheme propo-
sitions such as this should consider the various
relationships between words as well as term se-
lection mechanisms and term weighting schemes
(Robertson, 1990). The specific answers found
to these questions may vary; thus a variety
of query expansion approaches were suggested
(Efthimiadis, 1996).
In a first attempt to find related search terms,
we might ask the user to select additional terms
to be included in a new query, e.g. (Velez et
al., 1997). This could be handled interactively
through displaying a ranked list of retrieved
items returned by the first query. Voorhees
(Voorhees, 1994) proposed basing a scheme
based on the WordNet thesaurus. The au-
thor demonstrated that terms having a lexical-
semantic relation with the original query words
(extracted from a synonym relationship) pro-
vided very little improvement (around 1% when
compared to the original unexpanded query).
As a second strategy for expanding the orig-
inal query, Rocchio (Rocchio, 1971) proposed
accounting for the relevance or irrelevance of
top-ranked documents, according to the user?s
manual input. In this case, a new query was
automatically built in the form of a linear com-
bination of the term included in the previous
query and terms automatically extracted from
both the relevant documents (with a positive
weight) and non-relevant items (with a nega-
tive weight). Empirical studies (e.g., (Salton
and Buckley, 1990)) demonstrated that such an
approach is usually quite effective, and could
676
be used more than once per query (Aalbers-
berg, 1992). Buckley et al (Singhal et al,
1996b) suggested that we could assume, with-
out even looking at them or asking the user, that
the top k ranked documents are relevant. De-
noted the pseudo-relevance feedback or blind-
query expansion approach, this approach is usu-
ally effective, at least when handling relatively
large text collections.
As a third source, we might use large text
corpora to derive various term-term relation-
ships, using statistically or information-based
measures (Jones, 1971), (Manning and Schu?tze,
2000). For example, (Qiu and Frei, 1993)
suggested that terms to be added to a new
query could be extracted from a similarity the-
saurus automatically built through calculating
co-occurrence frequencies in the search collec-
tion. The underlying effect was to add idiosyn-
cratic terms to the underlying document col-
lection, related to the query terms by language
use. When using such query expansion ap-
proaches, we can assume that the new terms are
more appropriate for the retrieval of pertinent
items than are lexically or semantically related
terms provided by a general thesaurus or dic-
tionary. To complement this global document
analysis, (Croft, 1998) suggested that text pas-
sages (with a text window size of between 100
to 300 words) be taken into account. This local
document analysis seemed to be more effective
than a global term relationship generation.
As a forth source of additional terms, we
might account for specific user information
needs and/or the underlying domain. In this
vein, (Liu and Chu, 2005) suggested that terms
related to the user?s intention or scenario might
be included. In the medical domain, it was ob-
served that users looking for information usu-
ally have an underlying scenario in mind (or
a typical medical task). Knowing that the
number of scenarios for a user is rather lim-
ited (e.g., diagnosis, treatment, etiology), the
authors suggested automatically building a se-
mantic network based on a domain-specific the-
saurus (using the Unified Medical Language
System (UMLS) in this case). The effective-
ness of this strategy would of course depend
on the quality and completeness of domain-
specific knowledge sources. Using the well-
known term frequency (tf)/inverse document
frequency (idf) retrieval model, the domain-
specific query-expansion scheme suggested by
Liu and Chu (2005) produces better retrieval
performance than a scheme based on statis-
tics (MAP: 0.408 without query expansion,
0.433 using statistical methods and 0.452 with
domain-specific approaches).
In these different query expansion ap-
proaches, various underlying parameters must
be specified, and generally there is no sin-
gle theory able to help us find the most ap-
propriate values. Recent empirical studies
conducted in the context of the TREC Ge-
nomics track, using the OHSUGEN collection
(Hersh, 2005), show that neither blind expan-
sion (Rocchio), nor domain-specific query ex-
pansion (thesaurus-based Gene and Protein ex-
pansion) seem appropriate to improve retrieval
effectiveness (Aronson et al, 2006) (Abdou et
al., 2006).
3 Data and metrics
To test our hypothesis, we used the OHSUMED
collection (Hersh et al, 1994), originally devel-
oped for the TREC topic detection track, which
is the most popular information retrieval collec-
tion for evaluating information search in library
corpora. Alternative collections (cf. (Savoy,
2005)), such as the French Amaryllis collection,
are usually smaller and/or not appropriate to
evaluate our argumentative classifier, which can
only process English documents. Other MED-
LINE collections, which can be regarded as sim-
ilar in size or larger, such as the TREC Ge-
nomics 2004 and 2005 collections are unfortu-
nately more domain-specific since information
requests in these collection are usually target-
ing a particular gene or gene product.
Among the 348,566 MEDLINE citations of
the OHSUMED collection, we use the 233,455
records provided with an abstract. An exam-
ple of a MEDLINE citation is given in Table 1:
only Title, Abstract, MeSH and Chemical (RN)
fields of MEDLINE records were used for index-
ing. Out of the 105 queries of the OHSUMED
collection, only 101 queries have at least one
positive relevance judgement, therefore we used
only this subset for our experiments. The sub-
set has been randomly split into a training set
(75 queries), which is used to select the different
parameters of our retrieval model, and a test set
(26 queries), used for our final evaluation.
As usual in information retrieval evaluations,
the mean average precision, which computes the
precision of the engine at different levels (0%,
10%, 20%... 100%) of recall, will be used in our
experiments. The precision of the top returned
677
Title: Computerized extraction of coded find-
ings from free-text radiologic reports. Work in
progress.
Abstract: A computerized data acquisition
tool, the special purpose radiology understand-
ing system (SPRUS), has been implemented as
a module in the Health Evaluation through Log-
ical Processing Hospital Information System.
This tool uses semantic information from a di-
agnostic expert system to parse free-text radi-
ology reports and to extract and encode both
the findings and the radiologists? interpreta-
tions. These coded findings and interpretations
are then stored in a clinical data base. The sys-
tem recognizes both radiologic findings and di-
agnostic interpretations. Initial tests showed a
true-positive rate of 87% for radiographic find-
ings and a bad data rate of 5%. Diagnostic in-
terpretations are recognized at a rate of 95%
with a bad data rate of 6%. Testing suggests
that these rates can be improved through en-
hancements to the system?s thesaurus and the
computerized medical knowledge that drives it.
This system holds promise as a tool to obtain
coded radiologic data for research, medical au-
dit, and patient care.
MeSH Terms: Artificial Intelligence*; Deci-
sion Support Techniques; Diagnosis, Computer-
Assisted; Documentation; Expert Systems; Hos-
pital Information Systems*; Human; Natural
Language Processing*; Online Systems; Radi-
ology Information Systems*.
Table 1: MEDLINE records with, title, abstract
and keyword fields as provided by MEDLINE
librarians: major concepts are marked with *;
Subheadings and checktags are removed.
document, which is obviously of major impor-
tance is also provided together with the total
number of relevant retrieved documents for each
evaluated run.
4 Methods
To test our experimental hypothesis, we use the
Rocchio algorithm as baseline. In addition, we
also provide the score obtained by the engine
before the feedback step. This measure is nec-
essary to verify that feedback is useful for query-
ing the OHSUMED collection and to establish a
strong baseline. While Rocchio selects the fea-
tures to be added to the original queries based
on pure statistical analysis, we propose to base
our feature expansion also on argumentative cri-
teria. That is, we overweight features appear-
ing in sentences classified in a particular argu-
mentative category by the argumentative cate-
gorizer.
4.1 Retrieval engine and indexing units
The easyIR system is a standard vector-space
engine (Ruch, 2004), which computes state-
of-the-art tf.idf and probabilistic weighting
schema. All experiments were conducted with
pivoted normalization (Singhal et al, 1996a),
which has recently shown some effectiveness
on MEDLINE corpora (Aronson et al, 2006).
Query and document weighings are provided in
Equation (1): the dtu formula is applied to the
documents, while the dtn formula is applied to
the query; t the number of indexing terms, dfj
the number of documents in which the term tj ;
pivot and slope are constants (fixed at pivot =
0.14, slope = 146).
dtu: wij = (Ln(Ln(tfij)+1)+1)?idfj(1?slope)?pivot+slope?nti
dtn: wij = idfj ? (Ln(Ln(tfif ) + 1) + 1)
(1)
As already observed in several linguistically-
motivated studies (Hull, 1996), we observe that
common stemming methods do not perform well
on MEDLINE collections (Abdou et al, 2006),
therefore indexing units are stored in the in-
verted file using a simple S-stemmer (Harman,
1991), which basically handles most frequent
plural forms and exceptions of the English lan-
guage such as -ies, -es and -s and exclude end-
ings such as -aies, -eies, -ss, etc. This simple
normalization procedure performs better than
others and better than no stemming. We also
use a slightly modified standard stopword list of
544 items, where strings such as a, which stands
for alpha in chemistry and is relevant in biomed-
ical expressions such as vitamin a.
4.2 Argumentative categorizer
The argumentative classifier ranks and catego-
rizes abstract sentences as to their argumenta-
tive classes. To implement our argumentative
categorizer, we rely on four binary Bayesian
classifiers, which use lexical features, and a
Markov model, which models the logical distri-
bution of the argumentative classes in MED-
LINE abstracts. A comprehensive description
of the classifier with feature selection and com-
parative evaluation can be found in (Ruch et
al., 2005a)
To train the classifier, we obtained 19,555 ex-
plicitly structured abstracts from MEDLINE. A
678
Abstract: PURPOSE: The overall prognosis
for patients with congestive heart failure is poor.
Defining specific populations that might demon-
strate improved survival has been difficult [...]
PATIENTS AND METHODS: We identified 11
patients with severe congestive heart failure (av-
erage ejection fraction 21.9 +/- 4.23% (+/- SD)
who developed spontaneous, marked improve-
ment over a period of follow-up lasting 4.25 +/-
1.49 years [...] RESULTS: During the follow-up
period, the average ejection fraction improved
in 11 patients from 21.9 +/- 4.23% to 56.64
+/- 10.22%. Late follow-up indicates an aver-
age ejection fraction of 52.6 +/- 8.55% for the
group [...] CONCLUSIONS: We conclude that
selected patients with severe congestive heart
failure can markedly improve their left ventric-
ular function in association with complete reso-
lution of heart failure [...]
Table 2: MEDLINE records with explicit ar-
gumentative markers: PURPOSE, (PATIENTS
and) METHODS, RESULTS and CONCLU-
SION.
Bayesian classifier
PURP. METH. RESU. CONC.
PURP. 80.65 % 0 % 3.23 % 16 %
METH. 8 % 78 % 8 % 6 %
RESU. 18.58 % 5.31 % 52.21 % 23.89 %
CONC. 18.18 % 0 % 2.27 % 79.55 %
Bayesian classifier with Markov model
PURP. METH. RESU. CONC.
PURP. 93.35 % 0 % 3.23 % 3 %
METH. 3 % 78 % 8 % 6 %
RESU. 12.73 % 2.07 % 57.15 % 10.01 %
CONC. 2.27 % 0 % 2.27 % 95.45 %
Table 3: Confusion matrix for argumentative
classification. The harmonic means between re-
call and precision score (or F-score) is in the
range of 85% for the combined system.
conjunctive query was used to combine the fol-
lowing four strings: PURPOSE:, METHODS:,
RESULTS:, CONCLUSION:. From the original
set, we retained 12,000 abstracts used for train-
ing our categorizer, and 1,200 were used for fine-
tuning and evaluating the categorizer, following
removal of explicit argumentative markers. An
example of an abstract, structured with explicit
argumentative labels, is given in Table 2. The
per-class performance of the categorizer is given
by a contingency matrix in Table 3.
4.3 Rocchio feedback
Various general query expansion approaches
have been suggested, and in this paper we com-
pared ours with that of Rocchio. In this latter
case, the system was allowed to add m terms ex-
tracted from the k best-ranked abstracts from
the original query. Each new query was derived
by applying the following formula (Equation 2):
Q? = ? ? Q + (?/k) ?? kj = 1wij (2), in which
Q? denotes the new query built from the previ-
ous query Q, and wij denotes the indexing term
weight attached to the term tj in the document
Di. By direct use of the training data, we de-
termine the optimal values of our model: m =
10, k = 15. In our experiments, we fixed ? =
2.0, ? = 0.75. Without feedback the mean av-
erage precision of the evaluation run is 0.3066,
the Rocchio feedback (mean average precision =
0.353) represents an improvement of about 15%
(cf. Table 5), which is statistically2 significant
(p < 0.05).
4.4 Argumentative selection for
feedback
To apply our argumentation-driven feedback
strategy, we first have to classify the top-ranked
abstracts into our four argumentative moves:
PURPOSE, METHODS, RESULTS, and CON-
CLUSION. For the argumentative feedback, dif-
ferent m and k values are recomputed on the
training queries, depending on the argumenta-
tive category we want to over-weight. The ba-
sic segment is the sentence; therefore the ab-
stract is split into a set of sentences before being
processed by the argumentative classifier. The
sentence splitter simply applies as set of regu-
lar expressions to locate sentence boundaries.
The precision of this simple sentence splitter
equals 97% on MEDLINE abstracts. In this
setting only one argumentative category is at-
tributed to each sentence, which makes the de-
cision model binary.
Table 4 shows the output of the argumenta-
tive classifier when applied to an abstract. To
determine the respective value of each argumen-
tative contents for feedback, the argumenta-
tive categorizer parses each top-ranked abstract.
These abstracts are then used to generate four
groups of sentences. Each group corresponds to
a unique argumentative class. Each argumenta-
tive index contains sentences classified in one of
four argumentative classes. Because argumen-
2Tests are computed using a non-parametric signed
test, cf. (Zobel, 1998) for more details.
679
CONCLUSION (00160116) The highly favorable pathologic stage
(RI-RII, 58%) and the fact that the majority of patients were
alive and disease-free suggested a more favorable prognosis
for this type of renal cell carcinoma.
METHODS (00160119) Tumors were classified according to
well-established histologic criteria to determine stage of
disease; the system proposed by Robson was used.
METHODS (00162303) Of 250 renal cell carcinomas analyzed,
36 were classified as chromophobe renal cell carcinoma,
representing 14% of the group studied.
PURPOSE (00156456) In this study, we analyzed 250 renal cell
carcinomas to a) determine frequency of CCRC at our Hospital
and b) analyze clinical and pathologic features of CCRCs.
PURPOSE (00167817) Chromophobe renal cell carcinoma (CCRC)
comprises 5% of neoplasms of renal tubular epithelium. CCRC
may have a slightly better prognosis than clear cell carcinoma,
but outcome data are limited.
RESULTS (00155338) Robson staging was possible in all cases,
and 10 patients were stage 1) 11 stage II; 10 stage III, and
five stage IV.
Table 4: Output of the argumentative catego-
rizer when applied to an argumentatively struc-
tured abstract after removal of explicit mark-
ers. For each row, the attributed class is fol-
lowed by the score for the class, followed by the
extracted text segment. The reader can com-
pare this categorization with argumentative la-
bels as provided in the original abstract (PMID
12404725).
tative classes are equally distributed in MED-
LINE abstracts, each index contains approxi-
mately a quarter of the top-ranked abstracts
collection.
5 Results and Discussion
All results are computed using the treceval pro-
gram, using the top 1000 retrieved documents
for each evaluation query. We mainly evaluate
the impact of varying the feedback category on
the retrieval effectiveness, so we separately ex-
pand our queries based a single category. Query
expansion based on RESULTS or METHODS
sentences does not result in any improvement.
On the contrary, expansion based on PURPOSE
sentences improve the Rocchio baseline by +
23%, which is again significant (p < 0.05). But
the main improvement is observed when CON-
CLUSION sentences are used to generate the
expansion, with a remarkable gain of 41% when
compared to Rocchio. We also observe in Table
5 that other measures (top precision) and num-
ber of relevant retrieved articles do confirm this
trend.
For the PURPOSE category, the optimal k
parameter, computed on the test queries was
11. For the CONCLUSION category, the opti-
mal k parameter, computed on the test queries
was 10. The difference between the m values be-
tween Rocchio feedback and the argumentative
feedback, respectively 15 vs. 11 and 10 for Roc-
chio, PURPOSE, CONCLUSION sentences can
No feeback
Relevant Top Mean average
retrieved precision precision
1020 0.3871 0.3066
Rocchio feedback
Relevant Top Mean average
retrieved precision precision
1112 0.4020 0.353
Argumentative feedback: PURPOSE
Relevant Top Mean average
retrieved precision precision
1136 0.485 0.4353
Argumentative feedback: CONCLUSION
Relevant Top Mean average
retrieved precision precision
1143 0.550 0.4999
Table 5: Results without feedback, with Roc-
chio and with argumentative feedback applied
on PURPOSE and CONCLUSION sentences.
The number of relevant document for all queries
is 1178.
be explained by the fact that less textual mate-
rial is available when a particular class of sen-
tences is selected; therefore the number of words
that should be added to the original query is
more targeted.
From a more general perspective, the impor-
tance of CONCLUSION and PURPOSE sen-
tences is consistent with other studies, which
aimed at selecting highly content bearing sen-
tences for information extraction (Ruch et al,
2005b). This result is also consistent with
the state-of-the-art in automatic summariza-
tion, which tends to prefer sentences appearing
at the beginning or at the end of documents to
generate summaries.
6 Conclusion
We have reported on the evaluation of a
new linguistically-motivated feedback strategy,
which selects highly-content bearing features for
expansion based on argumentative criteria. Our
simple model is based on four classes, which
have been reported very stable in scientific re-
ports of all kinds. Our results suggest that
argumentation-driven expansion can improve
retrieval effectiveness of search engines by more
than 40%. The proposed methods open new
research directions and are generally promis-
ing for natural language processing applied to
information retrieval, whose positive impact is
still to be confirmed (Strzalkowski et al, 1998).
Finally, the proposed methods are important
from a theoretical perspective, if we consider
680
that it initiates a genre-specific paradigm as
opposed to the usual information retrieval ty-
pology, which distinguishes between domain-
specific and domain-independent approaches.
Acknowledgements
The first author was supported by a visiting
faculty grant (ORAU) at the Lister Hill Cen-
ter of the National Library of Medicine in 2005.
We would like to thank Dina Demner-Fushman,
Susanne M. Humphrey, Jimmy Lin, Hongfang
Liu, Miguel E. Ruiz, Lawrence H. Smith, Lor-
raine K. Tanabe, W. John Wilbur for the fruit-
ful discussions we had during our weekly TREC
meetings at the NLM. The study has also been
partially supported by the Swiss National Foun-
dation (Grant 3200-065228).
References
I Aalbersberg. 1992. Incremental Relevance
Feedback. In SIGIR, pages 11?22.
S Abdou, P Ruch, and J Savoy. 2006. Gen-
eral vs. Specific Blind Query Expansion for
Biomedical Searches. In TREC 2005.
A Aronson, D Demner-Fushman, S Humphrey,
J Lin, H Liu, P Ruch, M Ruiz, L Smith,
L Tanabe, and J Wilbur. 2006. Fusion
of Knowledge-intensive and Statistical Ap-
proaches for Retrieving and Annotating Tex-
tual Genomics Documents. In TREC 2005.
J Xu B Croft. 1998. Corpus-based stem-
ming using cooccurrence of word variants.
ACM-Transactions on Information Systems,
16(1):61?81.
E Efthimiadis. 1996. Query expansion. Annual
Review of Information Science and Technol-
ogy, 31.
G Furnas, T Landauer, L Gomez, and S Du-
mais. 1987. The vocabulary problem in
human-system communication. Communica-
tions of the ACM, 30(11).
D Harman. 1991. How effective is suffixing ?
JASIS, 42 (1):7?15.
W Hersh, C Buckley, T Leone, and D Hickam.
1994. OHSUMED: An interactive retrieval
evaluation and new large test collection for
research. In SIGIR, pages 192?201.
W Hersh. 2005. Report on the trec 2004 ge-
nomics track. pages 21?24.
Lynette Hirschman, Alexander Yeh, Chris-
tian Blaschke, and Alfonso Valencia. 2005.
Overview of BioCreAtIvE: critical assessment
of information extraction for biology. BMC
Bioinformatics, 6 (suppl. 1).
G Hirst. 1981. Anaphora in Natural Language
Understanding: A Survey. Lecture Notes in
Computer Science 119 - Springer.
D Hull. 1996. Stemming algorithms: A case
study for detailed evaluation. Journal of
the American Society of Information Science,
47(1):70?84.
K Sparck Jones. 1971. Automatic Keyword
Classification for Information Retrieval. But-
terworths.
F Lisacek, C Chichester, A Kaplan, and San-
dor. 2005. Discovering Paradigm Shift Pat-
terns in Biomedical Abstracts: Application
to Neurodegenerative Diseases. In Proceed-
ings of the First International Symposium on
Semantic Mining in Biomedicine (SMBM),
pages 212?217. Morgan Kaufmann.
Z Liu and W Chu. 2005. Knowledge-based
query expansion to support scenario-specific
retrieval of medical free text. ACM-SAC In-
formation Access and Retrieval Track, pages
1076?1083.
C Manning and H Schu?tze. 2000. Foundations
of Statistical Natural Language Processing.
MIT Press.
D Marcu. 1997. The Rhetorical Parsing of Nat-
ural Language Texts. pages 96?103.
L McKnight and P Srinivasan. 2003. Cate-
gorization of sentence types in medical ab-
stracts. AMIA Annu Symp Proc., pages 440?
444.
Y Mizuta and N Collier. 2004. Zone iden-
tification in biology articles as a basis for
information extraction. Proceedings of the
joint NLPBA/BioNLP Workshop on Natural
Language for Biomedical Applications, pages
119?125.
Y Mizuta, A Korhonen, T Mullen, and N Col-
lier. 2005. Zone Analysis in Biology Articles
as a Basis for Information Extraction. Inter-
national Journal of Medical Informatics, to
appear.
C Orasan. 2001. Patterns in Scientific Ab-
stracts. In Proceedings of Corpus Linguistics,
pages 433?445.
Y Qiu and H Frei. 1993. Concept based query
expansion. ACM-SIGIR, pages 160?69.
S Robertson. 1990. On term selection for
query expansion. Journal of Documentation,
46(4):359?364.
J Rocchio. 1971. Relevance feedback in infor-
mation retrieval in The SMART Retrieval
System - Experiments in Automatic Docu-
ment Processing. Prentice-Hall.
681
P Ruch, R Baud, C Chichester, A Geissbu?hler,
F Lisacek, J Marty, D Rebholz-Schuhmann,
I Tbahriti, and AL Veuthey. 2005a. Extract-
ing Key Sentences with Latent Argumenta-
tive Structuring. In Medical Informatica Eu-
rope (MIE), pages 835?40.
P Ruch, L Perret, and J Savoy. 2005b. Features
Combination for Extracting Gene Functions
from MEDLINE. In European Colloquium
on Information Retrieval (ECIR), pages 112?
126.
P Ruch. 2002. Using contextual spelling correc-
tion to improve retrieval effectiveness in de-
graded text collections. COLING 2002.
P Ruch. 2004. Query translation by text cate-
gorization. COLING 2004.
P Ruch. 2006. Automatic Assignment of
Biomedical Categories: Toward a Generic
Approach. Bioinformatics, 6.
G Salton and C Buckley. 1990. Improving re-
trieval performance by relevance feedback.
Journal of the American Society for Informa-
tion Science, 41(4).
J Savoy. 2005. Bibliographic database access
using free-text and controlled vocabulary: An
evaluation. Information Processing and Man-
agement, 41(4):873?890.
A Singhal, C Buckley, and M Mitra. 1996a.
Pivoted document length normalization.
ACM-SIGIR, pages 21?29.
C Buckley A Singhal, M Mitra, and G Salton.
1996b. New retrieval approaches using smart.
In Proceedings of TREC-4.
T Strzalkowski, G Stein, G Bowden Wise,
J Perez Carballo, P Tapanainen, T Jarvinen,
A Voutilainen, and J Karlgren. 1998. Natu-
ral language information retrieval: TREC-7
report. In Text REtrieval Conference, pages
164?173.
J Swales. 1990. Genre Analysis: English in
Academic and Research Settings. Cambridge
University Press.
I Tbahriti, C Chichester, F Lisacek, and
P Ruch. 2004. Using Argumention to
Retrieve Articles with Similar Citations
from MEDLINE. Proceedings of the joint
NLPBA/BioNLP Workshop on Natural Lan-
guage for Biomedical Applications.
I Tbahriti, C Chichester, F Lisacek, and
P Ruch. 2005. Using Argumentation to Re-
trieve Articles with Similar Citations: an In-
quiry into Improving Related Articles Search
in the MEDLINE Digital Library. Interna-
tional Journal of Medical Informatics, to ap-
pear.
S Teufel and M Moens. 1999. Argumenta-
tive Classification of Extracted Sentences as
a First Step Towards Flexible Abstracting.
Advances in Automatic Text Summarization,
MIT Press, pages 155?171.
B Velez, R Weiss, M Sheldon, and D Gifford.
1997. Fast and effective query refinement. In
ACM SIGIR, pages 6?15.
E Voorhees. 1994. Query expansion using
lexical-semantic relations. In ACM SIGIR,
pages 61?69.
J Zobel. 1998. How reliable are large-scale
information retrieval experiments? ACM-
SIGIR, pages 307?314.
682
Using Natural Language Processing, Locus Link, and the  
Gene Ontology to Compare OMIM to MEDLINE 
 
 
Bisharah 
Libbus 
Halil 
Kilicoglu 
Thomas C. 
Rindflesch 
James G. 
Mork 
Alan R. 
Aronson 
Lister Hill National Center for Biomedical Communications 
National Library of Medicine 
Bethesda, Maryland, 20894 
{libbus|halil|tcr|mork|alan}@nlm.nih.gov 
 
 
Abstract 
Researchers in the biomedical and molecular 
biology fields are faced with a wide variety of 
information sources. These are presented in 
the form of images, free text, and structured 
data files that include medical records, gene 
and protein sequence data, and whole genome 
microarray data, all gathered from a variety of 
experimental organisms and clinical subjects. 
The need to organize and relate this informa-
tion, particularly concerning genes,  has moti-
vated the development of resources, such as 
the Unified Medical Language System, Gene 
Ontology, LocusLink, and the Online Inheri-
tance In Man (OMIM) database. We describe 
a natural language processing application to 
extract information on genes from unstruc-
tured text and discuss ways to integrate this 
information with some of the available online 
resources.  
1 Introduction 
The current knowledge explosion in genetics and ge-
nomics poses a challenge to both researchers and medi-
cal practitioners. Traditionally, scientific reviews, which 
summarize and evaluate the literature, have been indis-
pensable in addressing this challenge. OMIM (Online 
Mendelian Inheritance in Man) (OMIM 2000), for ex-
ample, is a clinical and biomedical information resource 
on human genes and genetic disorders. It has close to 
15,000 entries detailing clinical phenotypes and disor-
ders as well as information on nearly 9,000 genes. The 
database can be searched by gene symbol, chromosomal 
location, or disorder. 
More recently, automated techniques for information 
and knowledge extraction from the literature are being 
developed to complement scientific reviews. These 
methods address the need to condense and efficiently 
present large amounts of data to the user.  The feasibil-
ity of applying natural language processing techniques 
to the biomedical literature (Friedman and Hripcsak 
1999; de Bruijn and Martin 2002) and to the wealth of 
genomics data now available (Jenssen et al 2001; Yan-
dell and Majoros 2002) is increasingly being recog-
nized. Efforts to develop systems that work toward this 
goal focus on the identification of such items as gene 
and protein names (Tanabe and Wilbur 2002) or groups 
of genes with similar function (Jenssen et al 2001; 
Masys et al 2001). Other groups are interested in identi-
fying protein-protein (Blaschke et al 1999; Temkin and 
Gilder 2003) or gene-gene interactions (Stephens et al 
2001; Tao et al 2002), inhibit relations (Pustejovsky et 
al. 2002), protein structure (Gaizauskas et al 2003), and 
pathways (Ng and Wong 1999; Friedman et al 2001). 
We discuss the modification of an existing natural 
language processing system, SemGen (Rindflesch et al 
2003), that has broad applicability to biomedical text 
and that takes advantage of online resources such as 
LocusLink and the Gene Ontology. We are pursuing 
research that identifies gene-gene interactions in text on 
genetic diseases. For example the system extracts (2) 
from (1).  
1) Here, we report that TSLC1 directly associates 
with MPP3, one of the human homologues of a 
Drosophila tumor suppressor gene, Discs large 
(Dlg). 
2) TSLC1|INTERACT_WITH|MPP3 
                                            Association for Computational Linguistics.
                   Linking Biological Literature, Ontologies and Databases, pp. 69-76.
                                                HLT-NAACL 2004 Workshop: Biolink 2004,
Due to the complexity of the language involved, the 
extraction of such predications is currently not accurate 
enough to support practical application. However, we 
suggest its potential in the context of an application that 
combines traditional, human-curated resources such as 
OMIM and emerging information extraction applica-
tions. 
2 
3 SemGen 
Molecular Biology Resources 
To support and supplement the information extracted by 
SemGen from biomedical text, we draw on two re-
sources, LocusLink and the Gene Ontology. LocusLink 
(Wheeler et al 2004) provides a single query interface 
to curated genomic sequences and genetic loci. It pre-
sents information on official nomenclature, aliases, se-
quence accessions, phenotypes, OMIM numbers, 
homology, map locations, and related Web sites, among 
others. Of particular interest is the Reference Sequence 
(RefSeq) collection, which provides a comprehensive, 
curated, integrated, non-redundant set of sequences, 
including genomic DNA, transcript (RNA), and protein 
products for major research organisms. Currently, 
SemGen uses LocusLink to obtain normalized gene 
names and Gene Ontology annotations.  
The Gene Ontology (GO) (The Gene Ontology Con-
sortium 2000, 2001, 2004) aims to provide a dynamic 
controlled vocabulary that can be applied to all organ-
isms, even while knowledge of gene and protein func-
tion is incomplete or unfolding. The GO consists of 
three separate ontologies: molecular function, biological 
process, and cellular component. These three branches 
are used to characterize gene function and products and 
provide a comprehensive structure that permits the an-
notation of molecular attributes of genes in various or-
ganisms. We use GO annotations to examine whether 
there are identifiable patterns, or concordance, in the 
function of gene pairs identified by SemGen.  
SemGen identifies gene interaction predications based  
on semantic interpretation adapted from SemRep (Srini-
vasan and Rindflesch 2002; Rindflesch and Fiszman 
2003), a general natural language processing system 
being developed for the biomedical domain. After the 
application of  a statistically-based labeled categorizer 
(Humphrey 1999) that limits input text to the molecular 
biology domain, SemGen processing proceeds in  three 
major phases: categorial analysis, identification of con-
cepts, and identification of relations.  
The initial phase relies on a parser that draws on the 
SPECIALIST Lexicon (McCray et al 1994) and the 
Xerox Part-of-Speech Tagger (Cutting et al 1992) to 
produce an underspecified categorial analysis.  
In the phase for identifying concepts, disorders as 
well as genes and proteins are isolated by mapping sim-
ple noun phrases from the previous phase to concepts in 
the Unified Medical Language System? (UMLS)? 
Metathesaurus? (Humphreys et al 1998), using 
MetaMap (Aronson 2001). ABGene, a program that 
identifies genes and proteins using several statistical and 
empirical methods (Tanabe and Wilbur 2002) is also 
consulted during this phase. In addition, a small list of 
signal words (such as gene, codon, and exon) helps 
identify genetic phenomena. For example, the genetic 
phenomena in (4) are identified from the sentence in (3). 
Concepts isolated in this phase serve as potential argu-
ments in the next phase.  
3) WIF1 was down-regulated in 64% of primary 
prostate cancers, while SFRP4 was up-regulated 
in 81% of the patients. 
4) genphenom|WIF1  
genphenom|SFRP4 
 
During the final phase, in which relations are identi-
fied, the predicates of semantic propositions are based 
on indicator rules. These stipulate verbs, nominaliza-
tions, and prepositions that ?indicate? semantic predi-
cates. During this phase, argument identification is 
constrained by an underspecified dependency grammar, 
which also attempts to accommodate coordinated argu-
ments as well as predicates.  
SemGen originally had twenty rules indicating one 
of three etiology relations between genetic phenomena 
and diseases, namely CAUSE, PREDISPOSE, and 
ASSOCIATED_WITH. In this project, we extended Sem-
Gen to cover gene-gene interaction relations: INHIBIT, 
STIMULATE, AND INTERACT_WITH. About 20 indicator 
rules were taken from MedMiner (Tanabe et al 1999). 
We supplemented this list by taking advantage of the 
verbs identified in syntactic predications by GeneScene 
(Leroy et al 2003). SemGen has 46 gene-gene interac-
tion indicator rules (mostly verbs), including 16 for 
INHIBIT (such as block, deplete, down-regulate); 12 for 
INTERACT_WITH (bind, implicate, influence, mediate); 
and 18 for STIMULATE (amplify, activate, induce, up-
regulate).  
An overview of the SemGen system is given in Fig-
ure 1, and an example is provided below. SemGen proc-
essing on input text (5) produces the underspecified 
syntactic structure (represented schematically) in (6). 
(7) illustrates genetic phenomena identified, and (8) 
shows the final semantic interpretation. 
 
 
 
Figure 1. SemGen system  
 
5) We show here that EGR1 binds to the AR in 
prostate carcinoma cells, and an EGR1-AR 
complex can be detected by chromatin im-
munoprecipitation at the enhancer of an en-
dogenous AR target gene. 
6) [We] [show] [here] [that] [EGR1] [binds] [to 
the AR] [in prostate carcinoma cells,] [and] [an 
EGR1-AR complex] [can] [be] [detected] [by 
chromatin immunoprecipitation] [at the enhan-
cer] [of an endogenous AR target gene] 
7) genphenom|egr1 
genphenom|ar 
genphenom|enhancer endogenous ar target gene 
8) egr1|INTERACT_WITH|ar 
During processing, SemGen normalizes gene sym-
bols using the preferred symbol from LocusLink. The 
final interpretation with LocusLink gene symbol is 
shown in (9).  
9) EGR1|INTERACT_WITH|AR 
As we retrieve the LocusLink symbol for a gene, we 
also get the GO terms associated with that gene. We are 
interested in extending the application of our textual 
analysis and knowledge extraction methodology and 
relating it to other biomedical and genomic resources. 
Gene Ontology is one such important resource, and be-
low we discuss the possibility that GO might shed addi-
tional light on the biological relationship between genes 
that are paired functionally based on textual analysis. 
The GO terms for the genes in (9) are given in (10) and 
(11). 
 
10) EGR1|[transcription factor activity; regulation 
of transcription, DNA-dependent; nucleus] 
11) AR|[androgen receptor activity; steroid binding; 
receptor activity; transcription factor activity; 
transport; sex differentiation; regulation of tran-
scription, DNA-dependent; signal transduction; 
cell-cell signaling; nucleus] 
4 SemGen Evaluation and Error Analysis 
Before suggesting an application using SemGen output, 
we discuss the results of error analysis performed on 
344 sentences from MEDLINE citations related to six 
genetic diseases: Alzheimer's disease, Crohn?s disease, 
lung cancer, ovarian cancer, prostate cancer and sickle 
cell anemia. Out of 442 predications identified by Sem-
Gen, 181 were correct, for 41% precision. This is not 
yet accurate enough to support a production system; 
however, the majority of the errors are focused in two 
syntactic areas, and we believe that with further devel-
opment it is possible to provide output effective for 
supporting practical applications. 
The majority of the errors fall into one of two major 
syntactic classes, relativization and coordination. A fur-
ther source of error is the fact that we have not yet ad-
dressed interaction relations that involve a process in 
addition to a gene.  
Reduced relative clauses, such as mediated by Tip60 
in (12), are a rich source of argument identification er-
rors.  
12) LRPICD dramatically inhibits APP-derived in-
tracellular domain/Fe65 transactivation medi-
ated by Tip60.  
SemGen wrongly interpreted this sentence as asserting 
that LRPICD inhibits Tip60. The rules of the under-
specified dependency grammar that identify arguments 
essentially look to the left and right of a verb for a noun 
phrase that has been marked as referring to a genetic 
phenomenon. Arguments are not allowed to be used in 
more than one predication (unless licensed by coordina-
tion or as the head of a relative clause).  
A number of phenomena conspire in (12) to wrongly 
allow TIP60  to be analyzed as the object of inhibits. 
The actual object, transactivation, was not recognized 
because we have not yet addressed processes as argu-
ments of gene interaction predications. Further, the 
predication on transactivation, with argument TIP60, 
was not interpreted, and hence TIP60 was available (in-
correctly) for the object of inhibits.  If we had recog-
nized the relative clause in (12), TIP60 would not have 
been reused as an argument of inhibits, since only heads 
of relative clauses can be reused. 
The underspecified analysis on which SemGen is 
based is not always effective in identifying verb phrase 
coordination, as in (13), leading to the incorrect inter-
pretation that WIF1 interacts with SFRP4. 
13) WIF1 was down-regulated in 64% of primary 
prostate cancers, while SFRP4 was up-regulated 
in 81% of the patients.  
A further source of error in this sentence is that 
down-regulated was analyzed by the tagger as a past 
tense rather than past participle, thus causing the argu-
ment identification phase to look for an object to the 
right of this verb form. A further issue here is that we 
have not yet addressed truncated passives.  
5 Using SemGen to Compare OMIM and 
MEDLINE  
SemGen errors notwithstanding, we are investigating 
possibilities for exploiting automatically extracted gene 
interaction predications. We discuss an application 
which compares MEDLINE text to OMIM documents, 
for specified diseases. LocusLink preferred gene sym-
bols and GO terms are an integral part of this process-
ing. We feel it is instructive to investigate the 
consequences of this comparison, anticipating results 
that are effective enough for practical application. 
We selected five diseases with a genetic component 
(Alzheimer?s disease, Crohn?s disease, lung cancer, 
prostate cancer, and sickle cell anemia), and retrieved 
the corresponding OMIM report for each disease, auto-
matically discarding sections such as references, head-
ings, and edit history. We also queried PubMed for each 
disease and retrieved all MEDLINE citations that were 
more recent than the corresponding OMIM report. Both 
OMIM and MEDLINE files were then submitted to 
SemGen.  
For each disease, the MEDLINE file was larger than 
the corresponding OMIM file, and the categorizer 
eliminated some parts of each file as not being in the 
molecular biology domain. Table 1 shows the number 
of sentences in the original input files and the number 
processed after the categorizer eliminated sentences not 
in the molecular biology domain.  
 
 
 OMIM 
Orig. 
OMIM 
Proc. 
MEDLINE 
Orig. 
MEDLINE 
Proc. 
Alz 408 264 1639 862 
Crohn 188 124 4871 1236 
LungCa 55 34 9058 2966 
ProstCa 121 69 6989 2964 
SCA 184 79 4383 1057 
 
    Table 1. Input sentences processed by SemGen 
A paragraph in the OMIM file for Alzheimer?s dis-
ease beginning with the sentence Alzheimer disease is 
by far the most common cause of dementia, for example, 
was eliminated, while a MEDLINE citation with the 
title Semantic decision making in early probable AD: A 
PET activation study was removed.   
An overview of predication types retrieved by Sem-
Gen is given in Table 2 for the files on Alzheimer?s 
disease. Of the gene-disease predications, the majority 
had predicate ASSOCIATED_WITH (15 from OMIM and 
25 from MEDLINE). For gene-gene relations, 
INTERACT_WITH predominated (3 from OMIM and 12 
from MEDLINE). 
 
Alzheimer disease OMIM MEDLINE 
Gene-Disease 16 31 
Gene-Gene  3 22 
Total 19 53 
 
Table 2. Gene interaction predication types 
 
We developed a program that compares semantic 
predications found in MEDLINE abstracts to those 
found in an OMIM report associated with a particular 
disease and classifies the comparison between two 
predications as either an exact match, partial match, or 
no match. The category of a comparison is determined 
by examining the argument and predicate fields of the 
predications. If all three fields match, the comparison is 
an exact match; if any two fields match it is a partial 
match. All other cases are considered as no match.  
Although fewer than half of the predications ex-
tracted by SemGen are likely to be correct, we provide 
some examples from the files on Alzheimer?s disease.  
(The system retains the document ID?s, which are sup-
pressed here for clarity.) Examples of partial matches 
between gene-disease predications extracted from 
OMIM and MEDLINE are shown in (14) and (15).  
14) OM: APP | ASSOCIATED_WITH | Alz-
heimer?s Disease 
ML:  CD14 | ASSOCIATED_WITH | Alz-
heimer?s Disease 
15) OM: amyloid beta peptide | 
ASSOCIATED_WITH | Alzheimer?s Disease 
ML: amyloid beta peptide | 
ASSOCIATED_WITH | Senile Plaques 
Some of the gene-disease predications that only oc-
curred in OMIM are given in (16), and a few of those 
occurring exclusively in MEDLINE are given in (17). 
 
16) TGFB1 | ASSOCIATED_WITH | Amyloid 
deposition  
        PRNP | ASSOCIATED_WITH | Amyloid 
deposition 
        Mutation 4 gene | CAUSE | Alzheimer?s Dis-
ease 
 
17) MOG | ASSOCIATED_WITH | Nervous Sys-
tem Diseases 
Acetylcholinesterase | PREDISPOSE | Alz-
heimer?s Disease  
 
In (18) are listed some of the gene-gene interaction 
predications found in MEDLINE but not in OMIM. 
18) LAMR1 | STIMULATE | HTATIP 
MAPT|INTERACT_WITH | HSPA8  
CD14 | STIMULATE | amyloid peptide 
 
6 Using the GO Terms  
As noted above, for each gene argument in the predica-
tions identified by SemGen, we retrieved from Locus-
Link the GO terms associated with that gene. We have 
begun to investigate ways in which these terms might be 
used to compare genes by looking at the gene-gene in-
teraction predications extracted from MEDLINE that 
did not occur in OMIM.  
To support this work, we developed a program that 
sorts gene-gene interaction predications by the GO 
terms of their arguments. For each gene function, the 
predications in which both arguments share the same 
function are listed first. These are followed by the 
predications in which only the first argument has that 
gene function, and then the predications in which only 
the second argument has the relevant gene function. A 
typical output file of this process is shown in (19): 
 
19) RECEPTOR ACTIVITY 
        ----------------- 
        Both Arguments: 
        DTR|STIMULATE|EGFR 
        First Argument: 
        AR|STIMULATE|TRXR3 
        EPHB2|STIMULATE|ENO2 
        Second Argument: 
        EGR1|INTERACT_WITH|AR 
        PSMC6|STIMULATE|AR 
 
The three branches of the Gene Ontology provide a 
uniform system for relating genes by function. The 
terms in the molecular function and biological process 
branches are perhaps most useful for this purpose; how-
ever, we have begun by considering all three branches 
(including the cellular component branch). The most 
effective method of exploiting GO annotations remains 
a matter of research.  
It is important to recognize that GO mapping is not 
precise; different annotators may make different GO 
assignments for the same gene. Nevertheless, GO anno-
tations provide considerable potential for relating the 
molecular functions and biological processes of genes. 
We consider one of the predications extracted from the 
MEDLINE file for prostate cancer that did not occur in 
OMIM: 
19) EGR1|INTERACT_WITH|AR 
Both genes EGR1 and AR in LocusLink elicit the 
same human gene set (367 Hs AR; 1026 Hs CDKN1A; 
1958 Hs EGR1; 3949 Hs LDLR; 4664 Hs NAB1; 4665 
Hs NAB2; 5734 Hs PTGER4; 114034 Hs TOE1). This 
suggests a high degree of sequence homology and func-
tional similarity. In addition, LocusLink provides the 
following GO terms for the two genes: 
20) EGR1: early growth response 1; LocusID: 1958 
Gene Ontology: transcription factor activity; 
regulation of transcription, DNA-dependent; 
nucleus  
21)  AR: androgen receptor (dihydrotestosterone re-
ceptor; testicular feminization; spinal and bulb 
ar muscular atrophy; Kennedy disease) ; Lo-
cusID: 367 Gene Ontology:  androgen receptor 
activity; steroid binding; receptor activity; tran-
scription factor activity; transport; sex differen-
tiation; regulation of transcription, DNA-
dependent; signal transduction; cell-cell signal-
ing; nucleus 
(The GO provides additional, hierarchical information 
for terms, which we have not yet exploited.) 
Thirty percent of the predications examined had 
some degree of overlap in their GO terms. For example, 
the terms for EGR1 (transcription factor activity; regu-
lation of transcription, DNA- dependent; and nucleus) 
are identical to three of the GO terms for the AR gene. 
This concordance may not be typical of the majority of 
paired genes in our sample. However, in the case of 
genes that do not exhibit such complete overlap, con-
cordance might be obtained at higher nodes in the clas-
sification scheme. 
An alternate approach for assessing distance be-
tween GO annotations has been suggested by Lord et al 
(2003a, 2003b). They propose a ?semantic similarity 
measure? using ontologies to explore the relationships 
between genes that may have associated interaction or 
function. The authors consider the information content 
of each GO term, defined as the number of times each 
term, or any child term, occurs. 
The fact that any one gene has a number of GO an-
notations indicates that a particular gene may perform 
more than one function or its function may be classified 
under a number of molecular activities. Some of these 
activities may be part of, i.e. extending to a variable 
degree down, the same GO structure. For example, for 
gene AR, ?receptor activity? (GO 4872) partially over-
laps with ?androgen receptor activity? (GO 4882), as 
does ?steroid binding? (GO 5496) with ?transcription 
factor activity? (GO 3700), and ?signal transduction 
(GO 7165) and ?cell-cell signaling (GO 7267). This 
indicates that in assessing similarity one needs to exam-
ine the ontology structure and not rely solely on the GO 
terms.  
While we have no experimental evidence, we would 
like to speculate about the functional or biological sig-
nificance indicated by similarity in GO annotation. 
There are three orthogonal aspects to GO: molecular 
function, biological process, and cellular component. If 
two genes map more closely in one of the taxonomies, 
then their function is necessarily more closely related. 
The majority of GO terms are in the molecular function 
taxonomy. It is conceivable that genes that map more 
closely could be involved in the same cascade or par-
ticipate in the same genetic regulatory network. There is 
increasing interest in genetic networks (e.g. 
www.genome.ad.jp/kegg/ kegg2.html; http://ecocyc.org; 
http://us.expasy.org/tools/pathways; www.biocarta.com) 
and combining the ability to search and extract informa-
tion from the literature with GO mapping could prove 
effective in elucidating the functional interactions of 
genes. 
 
7 
8 Conclusion 
Potential Knowledge Discovery 
To determine whether our automatic comparison of 
MEDLINE to OMIM based on SemGen predications 
might throw new light on  gene-gene interactions, we 
examined predications found in the MEDLINE file that 
had no match in the OMIM file. We searched the 
OMIM reports for information on the genes found in 
such predications to confirm that they were absent from 
the OMIM reports. For example, while the OMIM re-
port on colon cancer did not mention BARD1, the 
SemGen output for MEDLINE had   
22) BARD1|INTERACT_WITH|hmsh2 
The abstract containing this predication (PMID 
11498787) asserts that the BARD1 gene (LocusID 580) 
interacts with the breast cancer gene BRCA1 as well as 
with hMSH2, a mismatch repair gene associated with 
colon cancer.  BARD1 shares homology with the two 
conserved regions of BRCA1 and also interacts with the 
N-terminal region of BRCA1. Interaction of BARD1 
with BRCA1 could be essential for the function of 
BRCA1 in tumor suppression.  
Conversely, disruption of this interaction may possi-
bly contribute to the process of oncogenesis. It has been 
reported that the BRCA1/BARD1 complex is responsi-
ble for many of the tumor suppression activities of 
BRCA1 (Baer and Ludwig 2002). The gene hMSH2 
(LocusID 4436) is one of a number of genes that, when 
mutated, predisposes to colon cancer type 1. It is the 
human homolog of the bacterial mismatch repair gene 
mutS. We hypothesize that the interaction of BARD1 
with hMSH2, in a similar fashion to BRCA1, may be 
necessary for tumor suppression. Disruption of this in-
teraction may increase the likelihood of developing co-
lon cancer. Furthermore, this observation serves to point 
toward a possible link between BRCA1 and colon can-
cer. 
 
We have extended earlier work with SemGen (Rind-
flesch et al 2003) and are now able to extract from text, 
in addition to names of gene and disorders, gene-
disorder and gene-gene relations. Although SemGen is 
not at a stage where it can be used indiscriminately and 
without selective review and evaluation, it may never-
theless prove useful for reviewers by providing an effi-
cient means of scanning a large number of references 
and extracting relations involving genes and diseases. 
The process of curation and review is time consum-
ing. Given the rate at which new publications are added 
to the scientific literature, the availability of tools for 
accelerating the review process would meet a real need. 
As demonstrated by our pilot study on six disorders, 
SemGen could prove useful, even at this prototype 
stage, in extracting relevant information from the litera-
ture concerning genes and diseases. Additionally, the 
ability to scan and extract information from diverse sci-
entific domains could play an important role in identify-
ing new relationships between genes and diseases that 
would promote hypothesis-generation and advance sci-
entific research. Even with the present limitations, 
SemGen could assist in making the scientific literature 
more accessible and reduce the time it takes for re-
searchers to update their knowledge and expertise. 
References 
Aronson, A.R. (2001). ?Effective mapping of biomedi-
cal text to the UMLS Metathesaurus: the MetaMap 
program.? In Proceedings of the AMIA Annual Sym-
posium, 17-21. 
Baer, R., and Ludwig, T. (2002). ?The BRCA1/BARD1 
heterodimer: a tumor suppressor complex with ubiq-
uitin E3 ligase activity.? Current Opinion in Genetics 
& Development, 12, 86-91 
Blaschke, C.; Andrade, M.A.; Ouzounis, C.; and Valen-
cia, A. (1999). ?Automatic extraction of biological 
information from scientific text: protein-protein in-
teractions.? In Proceedings of the Seventh Interna-
tional Conference on Intelligent Systems for 
Molecular Biology, 60-70. 
de Bruijn, B., and Martin, J. (2002). ?Getting to the 
(c)ore of knowledge: mining biomedical literature.? 
International Journal of Medical Informatics, 67, 7-
18. 
Cutting, D.; Kupiec, J.; Pedersen, J.; and Sibun, P. 
(1992). ?A practical part-of-speech tagger.? In Pro-
ceedings of the Third Conference on Applied Natural 
Language Processing. 
Friedman, C., and Hripcsak, G. (1999). ?Natural lan-
guage processing and its future in medicine.? Aca-
demic Medicine, 74 (8),890-5. 
Friedman, C.; Kra, P.; Yu, H.; Krauthammer, M.; and 
Rzhetsky, A. (2001). ?GENIES: a natural-language 
processing system for the extraction of molecular 
pathways from journal articles.? Bioinformatics, 17 
Suppl 1, S74-82. 
Gaizauskas, R; Demetriou, G.; Artymiuk, P.J.; and  
Willett, P. (2003). ?Protein Structures and Informa-
tion Extraction from Biological Texts: The PASTA 
System.? Bioinformatics, 19, 135-43. 
The Gene Ontology Consortium. (2000). ?Gene ontol-
ogy: tool for the unificaiton of biology.? Nature, 25, 
25-29. 
The Gene Ontology Consortium. (2001). ?Creating the 
Gene Ontology Resource: Design and implementa-
tion.? Genome Research, 11,1425-1433. 
The Gene Ontology Consortium. (2004). ?The Gene 
Ontology (GO) database and informatics resource.? 
Nucleic Acids Research, 32,D258-D261. 
Humphrey, S. (1999). ?Automatic indexing of docu-
ments from journal descriptors: A preliminary inves-
tigation.? Journal of the American Society for 
Information Science, 50(8), 661-74. 
Humphreys, B.L.; Lindberg, D.A.; Schoolman, H.M.; 
and Barnett, G.O. (1998). ?The Unified Medical lan-
guage System: An informatics research collabora-
tion.? Journal of American Medical Informatics 
Association, 5(1), 1-13. 
Jenssen, T.K.; Laegreid, A.; Komoroswski, J.; and 
Hovig, E. (2001). ?A literature network of human 
genes for high-throughput analysis of gene expres-
sion.? Nature Genetics, 28,21-28.  
Leroy, G.; Chen, H.; Martinez, J.D. (2003) ?A shallow 
parser based on closed-class words to capture rela-
tions in biomedical text.? Journal of Biomedical In-
formatics, 36, 145-58 . 
Lord, P.W.; Stevens, R.D.; Brass, A.; and Goble, C.A. 
(2003a). ?Investigating semantic similarity measures 
across the Gene Ontology: the relationship between 
sequence and annotation.? Bioinformatics 19:1275-
1283. 
Lord, P.W.; Stevens, R.D.; Brass, A.; and Goble, C.A. 
(2003b). ?Semantic similarity measures as tools for 
exploring the Gene Ontology.? Pacific Symposium 
on. Biocomputing,  601-612. 
Masys, D.R.; Welsh, J.B.; Fink, J.L.; Gribskov, M.; 
Klacansky, I.; and Vorbeil, J. (2001). ?Use of key-
word hierarchies to interpret gene expression pat-
terns.? Bioinformatics, 17(4), 319-26. 
McCray, A.T.; Srinivasan, S.; and Browne, A.C. (1994). 
?Lexical methods for managing variation in biomedi-
cal terminologies.? In Proceedings of the Annual 
Symposium on Computer Applications in Medical 
Care, 235-9. 
Ng, S.K., and Wong, M. (1999). ?Toward routine auto-
matic pathway discovery from on-line scientific text 
abstracts.? Genome Informatics, 10,104-112. 
Online Mendelian Inheritance in Man, OMIM (2000). 
McKusick-Nathans Institute for Genetic Medicine, 
Johns Hopkins University (Baltimore, MD) and Na-
tional Center for Biotechnology Information, Na-
tional Library of Medicine (Bethesda, MD). WWW 
URL: http://www.ncbi.nlm.nih.gov/omim/.  
Pustejovsky, J.; Castano, J.; Zhang, J.; Kotecki, M.; and 
Cochran, B. (2002). ?Robust relational parsing over 
biomedical literature: extracting inhibit relations.? 
Pacific Symposium on Biocomputing, 362-73. 
Rindflesch, T. C., and Fiszman, M. (2003). ?The inter-
action of domain knowledge and linguistic structure 
in natural language processing: Interpreting hy-
pernymic propositions in biomedical text.? Journal of 
Biomedical Informatics, 36(6):462-77. 
Rindflesch, T. C.; Libbus, B.; Hristovski, D.; Aronson, 
A.R.; and Kilicoglu, H. (2003). ?Semantic relations 
asserting the etiology of genetic diseases.? In Pro-
ceedings of the AMIA Annual Symposium, 554-8. 
Srinivasan, P., Rindflesch, T.C. (2002). ?Exploring Text 
Mining from MEDLINE.? In Proceedings of the 
AMIA Annual Symposium, 722-6. 
Stephens, M.; Palakal, M.; Mukhopadhyay, S.; and 
Raje, R. (2001). ?Detecting gene relations form Med-
line abstracts.? In Proceedings of the Sixth Pacific 
Symposium on Biocomputing, 6, 483-96. 
Tanabe, L.; Scherf, U.; Smith, L.H.; Lee, J.K.; Hunter, 
L.; Weinstein, J.N. (1999). ?MedMiner: An Internet 
text- mining tool for biomedical information, with 
application to gene expression profiling.? BioTech-
niques, 27(6),1210-17. 
Tanabe, L., Wilbur, W.J. (2002). ?Tagging gene and 
protein names in biomedical text.? Bioinformatics, 
18(8), 1124-32. 
Tao, Y-C., and Leibel, R.L. (2002). ?Identifying rela-
tionships among human genes by systematic analysis 
of biological literature.? BMC Bioinformatics, 3,16-
25. 
Temkin, J. M., and Gilder, M. R. (2003). ?Extraction of 
protein interaction information from unstructured text 
using a context-free grammar.? Bioinformatics, 
19(16), 2046-53. 
Wheeler, D.L.; Church, D.M.; Edgar, R.; Federhen, S.; 
Helmberg, W.; Madden, T.L.; Pontius, J.U.; Schuler, 
G.D.; Schriml, L.M.; Sequeira, E.; Suzek, T.O.; 
Tatusova, T.A.; Wagner, L. (2004). ?Database re-
sources of the National Center for Biotechnology In-
formation: update.? Nucleic Acids Research, 32(1), 
D35-40.  
Yandell, M.D., and Majoros, W.H. (2002) ?Genomics 
and natural language processing.? Nature Reviews 
Genetics, 3, 601-610. 
 
BioNLP 2007: Biological, translational, and clinical language processing, pages 105?112,
Prague, June 2007. c?2007 Association for Computational Linguistics
From Indexing the Biomedical Literature to Coding Clinical Text: 
Experience with MTI and Machine Learning Approaches 
Alan R. Aronson1, Olivier Bodenreider1, Dina Demner-Fushman1, Kin Wah Fung1,  
Vivian K. Lee1,2, James G. Mork1, Aur?lie N?v?ol1, Lee Peters1, Willie J. Rogers1
1Lister Hill Center 
National Library of Medicine 
Bethesda, MD 20894 
{alan, olivier, demnerd, 
kwfung, mork, neveola,  
peters, wrogers} 
@nlm.nih.gov 
 
2Vanderbilt University 
Nashville, TN 37235 
vivian.lee@vanderbilt.edu 
 
 
Abstract 
This paper describes the application of an 
ensemble of indexing and classification 
systems, which have been shown to be suc-
cessful in information retrieval and classi-
fication of medical literature, to a new task 
of assigning ICD-9-CM codes to the clini-
cal history and impression sections of radi-
ology reports. The basic methods used are: 
a modification of the NLM Medical Text 
Indexer system, SVM, k-NN and a simple 
pattern-matching method. The basic meth-
ods are combined using a variant of stack-
ing. Evaluated in the context of a Medical 
NLP Challenge, fusion produced an F-
score of 0.85 on the Challenge test set, 
which is considerably above the mean 
Challenge F-score of 0.77 for 44 participat-
ing groups. 
1 Introduction 
Researchers at the National Library of Medicine 
(NLM) have developed the Medical Text Indexer 
(MTI) for the automatic indexing of the biomedical 
literature (Aronson et al, 2004). The unsupervised 
methods within MTI were later successfully com-
bined with machine learning techniques and ap-
plied to the classification tasks in the Genomics 
Track evaluations at the Text Retrieval Conference 
(TREC) (Aronson et al, 2005 and Demner-
Fushman et al, 2006). This fusion approach con-
sists of using several basic classification methods 
with complementary strengths, combining the re-
sults using a modified ensemble method based on 
stacking (Ting and Witten, 1997). 
While these methods have shown reasonable 
performance on indexing and retrieval tasks of 
biomedical articles, it remains to be determined 
how they would perform on a different biomedical 
corpus (e.g., clinical text) and on a different task 
(e.g., coding to a different controlled vocabulary). 
However, except for competitive evaluations such 
as TREC or BioCreAtIvE, corpora and gold stan-
dards for such tasks are generally not available, 
which is a limiting factor for such studies. For a 
survey of currently available corpora and devel-
opments in biomedical language processing, see 
Hunter and Cohen, 2006. 
The Medical NLP Challenge 1  sponsored by a 
number of groups including the Computational 
Medicine Center (CMC) at the Cincinnati Chil-
dren?s Hospital Medical Center gave us the oppor-
tunity to apply our fusion approach to a clinical 
corpus. The Challenge was to assign ICD-9-CM 
codes (International Classification of Diseases, 9th 
Revision, Clinical Modification) 2  to clinical text 
consisting of anonymized clinical history and im-
pression sections of radiology reports. 
The Medical NLP Challenge organizers distrib-
uted a training corpus of almost 1,000 of the ano-
nymized, abbreviated radiology reports along with 
                                                 
1 See www.computationalmedicine.org/challenge/.
2 See www.cdc.gov/nchs/icd9.htm.
105
gold standard ICD-9-CM assignments for each 
report obtained via a consensus of three independ-
ent sets of assignments. The primary measure for 
the Challenge was defined as the balanced F-score, 
with a secondary measure being cost-sensitive ac-
curacy. These measures were computed for sub-
missions to the Challenge based on a test corpus 
similar in size to the training corpus but distributed 
without gold standard code assignments. 
The main objective of this study is to determine 
what adaptation of the original methods is required 
to code clinical text with ICD-9-CM, in contrast to 
indexing and retrieving MEDLINE?. Note that an 
earlier study (Gay et al, 2005) showed that only 
minor adaptations were required in extending the 
original model to full-text biomedical articles. A 
secondary objective is to evaluate the performance 
of our methods in this new setting. 
 
2 Methods 
In early experimentation with the training corpus 
provided by the Challenge organizers, we discov-
ered that several of the training cases involved ne-
gated assertions in the text and that deleting these 
improved the performance of all basic methods 
being tested. For example, ?no pneumonia? occurs 
many times in the impression section of a report, 
sometimes with additional context. Section 2.1 
describes the process we used to remove these ne-
gated expressions; section 2.2 consists of descrip-
tions of the four basic methods used in this study; 
and section 2.3 defines the fusion of the basic 
methods to form a final result. 
2.1 Document Preparation 
The NegEx program (Chapman et al, 2001a and 
2001b, and Goldin and Chapman, 2003), which 
discovers negated expressions in text, was used to 
find negated expressions in the training and test 
corpora using a dictionary generated from concepts 
from the 2006AD version of the UMLS? Metathe-
saurus? (excluding the AMA vocabularies). A ta-
ble containing the concept unique identifier (CUI) 
and English string (STR with LAT=?ENG?) was 
extracted from the main concept table, MRCON, 
and was used as input to NegEx to generate a dic-
tionary that was later used as the universe of ex-
pressions which NegEx could find to be negated in 
the target corpora. (See the Appendix for examples 
of the input and output to this process.) 
The XML text of the training and test corpora 
was converted to a tree representation and then 
traversed, operating on one radiology report at a 
time. The clinical history and impression sections 
of each report were tokenized to allow whitespace 
to be separated from the punctuation, numbers and 
alphabetic text. The concepts from the UMLS were 
tokenized in the same way, to allow the concepts 
found by NegEx to be aligned with the text. The 
negation phrases discovered by NegEx were also 
tokenized to find the appropriate negation phrase 
preceding or trailing the target concept. Using the 
location information obtained by matching the set 
of one or more target concepts and the associated 
negation phrase, the overlapping concept spans 
were merged and the span for the negation phrase 
and the outermost negated concept was removed. 
Any intervening concepts associated with the same 
negation phrase were removed, too. The abbrevi-
ated tree representation was then re-serialized back 
into XML. 
As an example of our use of NegEx, consider 
the report with clinical history ?13-year 2-month - 
old female evaluate for cough.? and impression 
?No focal pneumonia.? After removal of negated 
text, the clinical history becomes ?13-year 2-month 
- old female?, and the discussion is empty. 
2.2 Basic Methods 
The four basic methods used for the Medical NLP 
Challenge are MTI (a modification of NLM?s 
Medical Text Indexer system), SVM (Support 
Vector Machines), k-NN (k Nearest Neighbors) 
and Pattern Matching (a simple, pattern-based clas-
sifier). Each of these methods is described here. 
Note that the MTI method uses a ?Restrict to ICD-
9-CM? algorithm that is described in the next sec-
tion. 
 
MTI. The original Medical Text Indexer (MTI) 
system, shown in Figure 1, consists of an infra-
structure for applying alternative methods of dis-
covering MeSH? headings for citation titles and 
abstracts and then combining them into an ordered 
list of recommended indexing terms. The top por-
tion of the diagram consists of two paths, or meth-
ods, for creating a list of recommended indexing 
terms: MetaMap Indexing and PubMed? Related 
Citations. The MetaMap Indexing path actually 
106
computes UMLS Metathesaurus concepts, which 
are passed to the Restrict to MeSH process 
(Bodenreider et al, 1998). The results from each 
path are weighted and combined using Post-
Processing, which also refines the results to con-
form to NLM indexing policy. The system is 
highly parameterized not only by path weights but 
also by several parameters specific to the Restrict 
to MeSH and Post-Processing processes. 
 
 
 
Figure 1: Medical Text Indexer (MTI) System 
 
For use in the Challenge, the Medical Text In-
dexer (MTI) program itself required few adapta-
tions.  Most of the changes involved the environ-
ment from which MTI obtains the data it uses 
without changing the normal parameter settings. 
We also added a further post-processing compo-
nent to filter our results. 
For the environment, we replaced MTI?s normal 
?Restrict to MeSH? algorithm with a ?Restrict to 
ICD-9-CM? algorithm, described below, in order 
to map UMLS concepts to ICD-9-CM codes in-
stead of MeSH headings. We also trained the Pub-
Med Related Citations component, TexTool (Ta-
nabe and Wilbur, 2002), on the Medical NLP Chal-
lenge training data instead of the entire MED-
LINE/PubMed database as is the case for normal 
MTI use at NLM.  For both of these methods, we 
used the actual ICD-9-CM codes to mimic UMLS 
CUIs used internally by MTI. 
To create the new training data for the TexTool 
(Related Citations), we reformatted the Medical 
NLP Challenge training data into a pseudo-
MEDLINE format using the ?doc id? component 
as the PMID, the ?CLINICAL_HISTORY? text 
component for the Title, the ?IMPRESSION? text 
component for the Abstract, and all of the 
?CMC_MAJORITY? codes as MeSH Headings 
(see Figure 2).  This provided us with direct ICD-
9-CM codes to work with instead of MeSH Head-
ings. 
 
<doc id="97663756" type="RADIOLOGY_REPORT"> 
  <codes> 
    <code origin="CMC_MAJORITY" type="ICD-9-
CM">780.6</code> 
    <code origin="CMC_MAJORITY" type="ICD-9-
CM">786.2</code> 
    <code origin="COMPANY3" type="ICD-9-
CM">786.2</code> 
    <code origin="COMPANY1" type="ICD-9-
CM">780.6</code> 
    <code origin="COMPANY1" type="ICD-9-
CM">786.2</code> 
    <code origin="COMPANY2" type="ICD-9-
CM">780.6</code> 
    <code origin="COMPANY2" type="ICD-9-
CM">786.2</code> 
  </codes> 
  <texts> 
    <text origin="CCHMC_RADIOLOGY" 
type="CLINICAL_HISTORY">Cough and fever.</text> 
    <text origin="CCHMC_RADIOLOGY" 
type="IMPRESSION">Normal radiographic appear-
ance of the chest, no pneumonia.</text> 
  </texts> 
</doc> 
PMID- 97663756 
TI  - Cough and fever. 
AB  - Normal radiographic appearance of the 
chest, no pneumonia. 
MH  - Fever (780.6) 
MH  - Cough (786.2) 
 
Figure 2: XML Medical NLP Training Data modi-
fied to pseudo-ASCII MEDLINE format 
 
Within MTI we also utilized an experimental 
option for MetaMap (Composite Phrases), which 
provides a longer UMLS concept match than usual. 
We did not use the following: (1) UMLS concept-
specific checking and exclusion sections; and (2) 
the MeSH Subheading generation, checking, and 
removal elements, since they were not needed for 
this Challenge. We then had MTI use the new Re-
107
strict to ICD-9-CM file and the new TexTool to 
generate its results. 
 
Restrict to ICD-9-CM. The mapping of every 
UMLS concept to ICD-9-CM developed for the 
Medical NLP Challenge is an adaptation of the 
original mapping to MeSH, later generalized to any 
target vocabulary (Fung and Bodenreider, 2005). 
Based on the UMLS Metathesaurus, the mapping 
utilizes four increasingly aggressive techniques: 
synonymy, built-in mappings, hierarchical map-
pings and associative mappings. In order to comply 
with coding rules in ICD-9-CM, mappings to non-
leaf codes are later resolved into leaf codes. 
Mappings to ICD-9-CM are identified through 
synonymy when names from ICD-9-CM are in-
cluded in the UMLS concept identified by 
MetaMap. For example, the ICD-9-CM code 592.0 
Calculus of kidney is associated with the UMLS 
concept C0392525 Nephrolithiasis through synon-
ymy. 
Built-in mappings are mapping relations be-
tween UMLS concepts implied from mappings 
provided by source vocabularies in the UMLS. For 
example, the UMLS concept C0239937 Micro-
scopic hematuria is mapped to the concept 
C0018965 (which contains the ICD-9-CM code 
599.7 Hematuria) through a mapping provided by 
SNOMED CT. 
In the absence of a mapping through synonymy 
or built-in mapping, a hierarchical mapping is 
attempted. Starting from the concept identified by 
MetaMap, a graph of ancestors is built by first us-
ing its parent concepts and broader concepts, then 
adding the parent concepts and broader concepts of 
each concept, recursively. Semantic constraints 
(based on semantic types) are applied in order to 
prevent semantic drift. Ancestor concepts closest 
to the MetaMap source concept are selected from 
the graph. Only concepts that can be resolved into 
ICD-9-CM codes (through synonymy or built-in 
mapping) are selected. For example, starting from 
C0239574 Low grade pyrexia, a mapping is found 
to ICD-9-CM code 780.6 Fever, which is con-
tained in the concept C0015967, one of the ances-
tors of C0239574. 
The last attempt to find a mapping involves not 
only hierarchical, but also associative relations. 
Instead of starting from the concept identified by 
MetaMap, associative mappings explore the con-
cepts in associative relation to this concept. For 
example, the concept C1458136 Renal stone sub-
stance is mapped to ICD-9-CM code 592.0 Calcu-
lus of kidney. 
Finally, when the identified ICD-9-CM code 
was not a leaf code (e.g., 786.5 Chest pain), we 
remapped it to one of the corresponding leaf codes 
in the training set where possible (e.g., 786.50 Un-
specified chest pain). 
Of the 2,331 UMLS concepts identified by 
MetaMap in the test set after freezing the method, 
620 (27%) were mapped to ICD-9-CM. More spe-
cifically, 101 concepts were mapped to one of the 
45 target ICD-9-CM codes present in the training 
set. Of the 101 concepts, 40 were mapped through 
synonymy, 11 through built-in mappings, 40 
through hierarchical mapping and 10 through asso-
ciative mapping. 
 
After the main MTI processing was completed, 
we applied a post-processing filter, restricting our 
results to the list of 94 valid combinations of ICD-
9-CM codes provided in the training set (hence-
forth referred to as allowed combinations) and 
slightly emphasizing MetaMap results. Examples 
of the post-processing rules are: 
? If MTI recommended 079.99 (Unspecified 
viral infection in conditions?) via either 
MetaMap or Related Citations, use 079.99, 
493.90 (Asthma, unspecified type?), and 
780.6 (Fever) for indexing. This is the only 
valid combination for this code based on the 
training corpus. 
? Similarly, if MTI recommended ?Enlarge-
ment of lymph nodes? (785.6) via the 
MetaMap path with a score greater then 
zero, use 785.6 and 786.2 (Cough) for in-
dexing. 
The best F-score (F = 0.83) for the MTI method 
was obtained on the training set using the negation-
removed text.  This was a slight improvement over 
using the original text (F = 0.82). 
 
SVM. We utilized Yet Another Learning Envi-
ronment3 (YALE), an open source application de-
veloped for machine learning and data mining, to 
determine the data classification performance of 
support vector machine (SVM) learning on the 
                                                 
3 See http://rapid-i.com. 
108
training data. To prepare the Challenge data for 
analysis, we removed all stop words and created 
feature vectors for the free text extracted from the 
?CLINICAL_HISTORY? and ?IMPRESSION? 
fields of the records.  Since both the training and 
test Challenge data had a known finite number of 
individual ICD-9-CM labels (45) and distinct com-
binations of ICD-9-CM labels (94), the data was 
prepared both as feature vectors for 45 individual 
labels as well as a model with 94 combination la-
bels.  In addition, the feature vectors were created 
using both simple term frequency as well as in-
verse document frequency (IDF) weighting, where 
the weight is (1+log(term frequency))*(total 
documents/document frequency).  There were thus 
a total of four feature vector datasets: 1) 45 indi-
vidual ICD-9-CM labels and simple term fre-
quency, 2) 45 ICD-9-CM labels and IDF weight-
ing, 3) 94 ICD-9-CM combinations and simple 
term frequency, and 4) 94 ICD-9-CM combina-
tions and IDF weighting. 
The YALE tool encompasses a number of SVM 
learners and kernel types.  For the classification 
problem at hand, we chose the C-SVM learner and 
the radial basis function (rbf) kernel.  The C-SVM 
learner attempts to minimize the error function 
?
=
+
N
i
i
T Cww
1
,
2
1 ?  
Niandbxw iii
T
i ,,1,01))(( K=???+ ????
 
where w is the vector of coefficients, b is a con-
stant, ?  is the kernel function, x are the independ-
ent variables, and ?i are parameters for handling 
the inputs.  C > 0 is the penalty parameter of the 
error function.  The rbf kernel is defined as K(x, 
x?) = exp(?? |x ? x?|2), ? > 0 where ? is a kernel 
parameter that determines the rbf width. We ran 
cross-validation experiments using YALE on all 
training datasets and varying C (10, 100, 1000, 
10000) and ? (0.01, 0.001, 0.0001, 0.00001) to de-
termine the optimal C and ? combination.  The 
cross-validation experiments generated classifica-
tion models that were then applied to the complete 
training datasets to analyze the performance of the 
learner. The 94 ICD-9-CM combination and sim-
ple term frequency dataset with C = 10000 and ? = 
0.01 had the best F-score at 0.86.  The best F-score 
for the 94 ICD-9-CM combination and IDF weight 
dataset was 0.79, where C = 0.001 and ? = 10000.   
Further preprocessing the training dataset by 
removing negated expressions was found to im-
prove the best F-score from 0.86 to 0.87.  The C = 
10000 and ? = 0.01 combination was then applied 
to the test dataset, which was preprocessed to re-
move negation and stop words and transformed to 
a feature vector using 94 ICD-9-CM combinations 
and simple term weighting.  The predicted ICD-9-
CM classifications and confidence of the predic-
tions for each clinical free text report were output 
and later combined with other methods to optimize 
the accuracy and precision of our ICD-9-CM clas-
sifications. 
 
k-NN. The Challenge training set was used to 
build a k-NN classifier. The k-NN classification 
method works by identifying, within a labelled set, 
documents similar to the document being classi-
fied, and inferring a classification for it from the 
labels of the retrieved neighbors. 
The free text in the training data set was proc-
essed to obtain a vector-space representation of the 
patient reports.  
Several methods of obtaining this representation 
were tested: after stop words were removed, simple 
term frequency and inverse document frequency 
(IDF) weighting were applied alternatively. A 
higher weight was also given to words appearing in 
the history portion of the text (vs. impression). 
Eventually, the most efficient representation was 
obtained by using controlled vocabulary terms ex-
tracted from the free text with MetaMap.4 Further 
processing on this representation of the training 
data showed that removing negated portions of the 
free text improved the results, raising the F-score 
from 0.76 to 0.79.   
Other parameters were also assessed on the 
training data, such as the number of neighbors to 
use (2 was found to be the best vs. 5, 10 or 15) and 
the restriction of the ICD-9-CM predictions to the 
set of 94 allowed combinations. When the predic-
tion for a given document was not within the set of 
allowed 94 combinations, an allowed subset of the 
ICD-9-CM codes predicted was selected based on 
the individual scores obtained for each ICD-9-CM 
code.  
The best F-score (F = 0.79) obtained on the 
training set used the MetaMap-based representa-
                                                 
4 Note that this use of MetaMap is independent of its 
inclusion as a component of MTI. 
109
tion with simple frequency counts on the text with 
negated expressions removed. ICD-9-CM predic-
tions were obtained from the nearest neighbors and 
restricted to one of the 94 allowed combinations.   
 
Pattern Matching. We developed a pattern-
matching classifier as a baseline for our more so-
phisticated classification methods. A list of all 
UMLS string representations for each of 45 codes 
(including synonyms from source vocabularies 
other than ICD-9-CM) was created as described in 
the MTI section above. The strings were then con-
verted to lower case, punctuation was removed, 
and strings containing terms unlikely to be found 
in a clinical report were pruned. For example, Ab-
domen NOS pain and Abdominal pain (finding) 
were reduced to abdominal pain. For the same rea-
sons, some of the strings were relaxed into pat-
terns. For example, it is unlikely to see PAIN 
CHEST in a chart, but very likely to find pain in 
chest. The string, therefore, was relaxed to the fol-
lowing pattern: pain.*chest. The text of the clinical 
history and the impression fields of the radiology 
reports with negated expressions removed (see 
Section 2.2) was broken up into sentences. Each 
sentence was then searched for all available pat-
terns. A corresponding code was assigned to the 
document for each matched pattern. This pattern 
matching achieved F-score = 0.79 on the training 
set. To reduce the number of codes assigned to a 
document, a check for allowed combinations was 
added as a post-processing step. The combination 
of assigned codes was looked up in the table of 
allowed codes. If not present, the codes were re-
duced to the combination of assigned codes most 
frequently occurring in the training set. This 
brought the F-score up to 0.84 on the training data. 
As the performance of this classifier was compara-
ble to other methods, we decided to include these 
results when combining the predictions of the other 
classifiers.  
2.3 Fusion of  Basic Methods: Stacking 
Experience with ad hoc retrieval tasks in the TREC 
Genomics Track has shown that combining predic-
tions of several classifiers either significantly im-
proves classification results, or at least provides 
more consistent and stable results when the train-
ing data set is small (Aronson et al, 2005). We 
therefore experimented with stacking (Ting and 
Witten, 1997), using a simple majority vote and a 
union of all assigned codes as baselines. The pre-
dictions of base classifiers described in the previ-
ous section were combined using our re-
implementation of the stacked generalization pro-
posed by Ting and Witten.  
3 Results 
Table 1 shows the results obtained for the training 
set. The best stacking results were obtained using 
predictions of all four base classifiers on the text 
with deleted negated expressions and with check-
ing for allowed combinations. We retained all final 
predictions with probability of being a valid code 
greater than 0.3. Checking for the allowed combi-
nations for the ensemble classifiers degraded the F-
score significantly. 
 
Classifier F-score 
MTI 0.83 
SVM 0.87 (x-validation) 
k-NN 0.79 (x-validation) 
Pattern Matching 0.84 
Majority 0.82 
Stacking 0.89 
 
Table 1: Training results for each classifier, the ma-
jority and stacking 
 
Since stacking produced the best F-score on the 
training corpus and is known to be more robust 
than the individual classifiers, the corresponding 
results for the test corpus were submitted to the 
Challenge submission website. The stacking results 
for the test corpus achieved an F-score of 0.85 and 
a secondary, cost-sensitive accuracy score of 0.83. 
For comparison purposes, 44 Challenge submis-
sions had a mean F-score of 0.77 with a maximum 
of 0.89. Our F-score of 0.85 falls between the 70th 
and 75th percentiles. 
4 Discussion 
It is significant that it was fairly straightforward to 
port various methods developed for ad hoc MED-
LINE citation retrieval, indexing and classification 
to the assignment of codes to clinical text. The 
modifications to MTI consisted of replacing Re-
strict to MeSH with Restrict to ICD-9-CM, training 
the Related Citations method on clinical text and 
replacing MTI?s normal post-processing with a 
much simpler version. Preprocessing the text using 
110
NegEx to remove negated expressions was a fur-
ther modification of the overall approach. 
It is noteworthy that a simple pattern-matching 
method performed as well as much more sophisti-
cated methods in the effort to fuse results from 
several methods into a final outcome. This unex-
pected success might be explained by the follow-
ing limitations of the Challenge. 
Possible limitations on the extensibility of the 
current research arise from two observations: (1) 
the Challenge cases were limited to two relatively 
narrow topics, cough/fever/pneumonia and uri-
nary/kidney problems; and (2) the clinical text was 
almost error-free, a situation that would not be ex-
pected in the majority of clinical text. It is possible 
that these conditions contributed to the success of 
the pattern-matching method but also caused 
anomalous behavior, such as the fact that simple 
frequency counts provided a better representation 
than IDF for the SVM and k-NN methods. 
Finally, as a result of low confidence in the 
ICD-9-CM code assignment, no codes were as-
signed to 29 records in the test set. It is worthwhile 
to explore the causes for such null assignments. 
One of the reasons for low confidence could be the 
aggressive pruning of the text by the negation algo-
rithm. For example, after removal of negated text 
in the sample report given in section 2.1, the only 
remaining text is ?13-year 2-month - old female? 
from the clinical history field; this provided no 
evidence for code assignment. Secondly, in some 
cases the original text was not sufficient for confi-
dent code assignment. For example, for the docu-
ment with clinical history ?Bilateral grade 3.? and 
impression ?Interval growth of normal appearing 
Kidneys?, no code was assigned by the SVM, k-
NN, or pattern-matching classifiers. Code 593.70 
corresponding to the UMLS concept Vesicouret-
eral reflux with reflux nephropathy, unspecified or 
without reflux nephropathy was assigned by MTI 
with a very low confidence, which was not suffi-
cient for the final assignment of the code. The third 
reason for assigning no code to a document was 
the wide range of assignments provided by the 
base classifiers. For example, for the following 
document: ?CLINICAL_HISTORY: 3-year - old 
male with history of left ureteropelvic and uret-
erovesical obstruction. Status post left pyeloplasty 
and left ureteral reimplantation. IMPRESSION: 1. 
Stable appearance and degree of hydronephrosis 
involving the left kidney. Stable urothelial thicken-
ing. 2. Interval growth of kidneys, left greater than 
right. 3. Normal appearance of the right kidney 
with interval resolution of right urothelial thicken-
ing.? MTI assigned codes 593.89 Other specified 
disorders of kidney and ureter and 591 Hy-
dronephrosis. Codes 593.70 Vesicoureteral reflux 
with reflux nephropathy, unspecified or without 
reflux nephropathy and 753.3 Double kidney with 
double pelvis were assigned by the k-NN classifier. 
Pattern matching resulted in assignment of code 
591 with fairly low confidence. No code was as-
signed to this document by the SVM classifier. 
Despite failing to assign codes to these 29 records, 
the conservative approach (using threshold) re-
sulted in better performance, achieving F-score 
0.85 compared to F-score 0.80 when all 1,634 
codes assigned by the base classifiers were used. 
5 Conclusion 
We are left with two conclusions. First, this re-
search confirms that combining several comple-
mentary methods for accomplishing tasks, ranging 
from ad hoc retrieval to categorization, produces 
results that are better and more stable than the re-
sults for the contributing methods. Furthermore, 
we have shown that the basic methods employing 
domain knowledge and advanced statistical algo-
rithms are applicable to clinical text without sig-
nificant modification. Second, although there are 
some limitations of the current Challenge test col-
lection of clinical text, we appreciate the efforts of 
the Challenge organizers in the creation of a test 
collection of clinical text. This collection provides 
a unique opportunity to apply existing methods to a 
new and important domain. 
Acknowledgements 
This work was supported in part by the Intramural 
Research Program of the NIH, National Library of 
Medicine and by appointments of Aur?lie N?v?ol 
and Vivian Lee to the NLM Research Participation 
Program sponsored by the National Library of 
Medicine and administered by the Oak Ridge Insti-
tute for Science and Education. 
The authors gratefully acknowledge the many 
essential contributions to MTI, especially W. John 
Wilbur for the PubMed Related Citations indexing 
method, and Natalie Xie for adapting TexTool (an 
interface to Related Citations) for this paper. 
111
References 
Aronson AR, Demner-Fushman D, Humphrey SM, Lin 
J, Liu H, Ruch P, Ruiz ME, Smith LH, Tanabe LK, 
Wilbur WJ. Fusion of knowledge-intensive and sta-
tistical approaches for retrieving and annotating tex-
tual genomics documents. Proc TREC 2005, 36-45. 
Aronson AR, Mork JG, Gay CW, Humphrey SM and 
Rogers WJ. The NLM Indexing Initiative's Medical 
Text Indexer. Medinfo. 2004: 268-72. 
Bodenreider O, Nelson SJ, Hole WT and Chang HF. 
Beyond synonymy: exploiting the UMLS semantics 
in mapping vocabularies. Proc AMIA Symp 1998: 
815-9. 
Chapman WW, Bridewell W, Hanbury P, Cooper GF, 
Buchanan B. Evaluation of negation phrases in narra-
tive clinical reports. Proc AMIA Symp. 2001a:105-9.  
Chapman WW, Bridewell W, Hanbury P, Cooper GF 
and Buchanan BG. A simple algorithm for identify-
ing negated findings and diseases in discharge sum-
maries. J Biomed Inform. 2001b;34:301-10.  
Demner-Fushman D, Humphrey SM, Ide NC, Loane RF, 
Ruch P, Ruiz ME, Smith LH, Tanabe LK, Wilbur WJ 
and Aronson AR. Finding relevant passages in scien-
tific articles: fusion of automatic approaches vs. an 
interactive team effort. Proc TREC 2006, 569-76. 
Fung KW and Bodenreider O. Utilizing the UMLS for 
semantic mapping between terminologies. AMIA 
Annu Symp Proc 2005: 266-70. 
Gay CW, Kayaalp M and Aronson AR. Semi-automatic 
indexing of full text biomedical articles. AMIA Annu 
Symp Proc. 2005:271-5. 
Goldin I and Chapman WW. Learning to detect nega-
tion with ?not? in medical texts. Proc Workshop on 
Text Analysis and Search for Bioinformatics, ACM 
SIGIR, 2003. 
Hunter L and Cohen KB. Biomedical language process-
ing: what?s beyond PubMed? Mol Cell. 2006 Mar 
3;21(5):589-94. 
Tanabe L and Wilbur WJ. (2002) Tagging gene and 
protein names in biomedical text. Bioinformatics, 
Aug 2002; 18: 1124 ?32. 
Ting WK and Witten I. 1997. Stacking bagged and dag-
ged models. 367-375. Proc. of ICML'97. Morgan 
Kaufmann, San Francisco, CA.
Appendix  
A sample of the input to NegEx for dictionary generation:  
 
C0002390 pneumonitis, allergic interstitial 
C0002390 allergic interstitial pneumonitis, nos 
C0002390 extrinsic allergic bronchiolo alveolitis 
C0002390 extrinsic allergic bronchiolo alveolitis, nos 
C0002390 hypersensitivity pneumonia 
C0002390 hypersensitivity pneumonia, nos 
C0002390 eaa  extrinsic allergic alveolitis 
C0002390 allergic extrinsic alveolitis nos (disorder) 
C0002390 extrinsic allergic alveolitis (disorder) 
C0002390 hypersensitivity pneumonitis nos (disorder) 
 
A sample of the dictionary generated by NegEx for later use in detecting negated expressions:  
 
C0002098 hypersensitivity granuloma (morphologic abnormality 
C0151726 hypersensitivity injection site 
C0020517 hypersensitivity nos 
C0429891 hypersensitivity observations 
C0002390 hypersensitivity pneumonia 
C0002390 hypersensitivity pneumonia, nos 
C0002390 hypersensitivity pneumonitides 
C0005592 hypersensitivity pneumonitides, avian 
C0002390 hypersensitivity pneumonitis 
C0182792 hypersensitivity pneumonitis antibody determination re-
agents 
112
BioNLP 2007: Biological, translational, and clinical language processing, pages 183?190,
Prague, June 2007. c?2007 Association for Computational Linguistics
Automatic Indexing of Specialized Documents: 
Using Generic vs. Domain-Specific Document Representations
Aur?lie N?v?ol James G. Mork
{neveola,mork,alan}@nlm.nih.gov
National Library of Medicine
8600 Rockville Pike 
Bethesda, MD 20894
USA
Alan R. Aronson
Abstract
The shift from paper to electronic docu-
ments has caused the curation of informa-
tion sources in large electronic databases
to become more generalized. In the bio-
medical domain, continuing efforts aim at 
refining indexing tools to assist with the 
update and maintenance of databases such 
as MEDLINE?. In this paper, we evaluate 
two statistical methods of producing 
MeSH? indexing recommendations for 
the genetics literature, including recom-
mendations involving subheadings, which 
is a novel application for the methods. We 
show that a generic representation of the 
documents yields both better precision 
and recall. We also find that a domain-
specific representation of the documents 
can contribute to enhancing recall. 
1 Introduction
There are two major approaches for the automatic 
indexing of text documents: statistical approaches 
that rely on various word counting techniques [su-
ch as vector space models (Salton, 1989), Latent 
Semantic Indexing (Deerwester et al, 1990) or
probabilistic models (Sparck-Jones et al, 2000)] 
and linguistic approaches that involve syntactical 
and lexical analysis [see for example term extrac-
tion and term variation recognition in systems such 
as MetaMap (Aronson, 2001), FASTR (Jacquemin 
and Tzoukermann, 1999) or IndDoc (Nazarenko 
and Ait El Mekki, 2005)]. In many cases, the com-
bination of these approaches has been shown to 
improve the performance of a single approach both 
for controlled indexing (Aronson et al, 2004) and
free text indexing (Byrne and Klein, 2003).
Recently, N?v?ol et al (2007) presented lin-
guistic approaches for the indexing of documents 
in the field of genetics. In this paper, we explore a 
statistical approach of indexing for text documents 
also in the field of genetics. This approach was 
previously used successfully to produce Medical 
Subject Headings (MeSH) main heading recom-
mendations. Our goal in this experiment is two-
fold: first, extending an existing method to the pro-
duction of recommendations involving subhead-
ings and second, assessing the possible benefit of 
using a domain-specific variant of the method. 
2 A k-Nearest-Neighbors approach for 
indexing 
2.1 Principle
The k-Nearest-Neighbors (k-NN) approach views 
indexing as a multi-class classification problem 
where a document may be assigned several
?classes? in the form of indexing terms. It requires 
a large set of labeled data composed of previously 
indexed documents. k-NN relies on the assumption 
that similar documents should be classified in a 
similar way. The algorithm consists of two steps: 
1/documents that are most ?similar? to the query 
document must be retrieved from the set of labeled 
documents. They are considered as ?neighbors? for 
the query document; 2/an indexing set must be 
produced from these and assigned to the query 
document.
Finding similar documents
All documents are represented using a vector of 
distinctive features within the representation space. 
Based on this representation, labeled documents 
183
may be ranked according to their similarity to the 
query document using usual similarity measures 
such as cosine or Dice. The challenge in this step is 
to define an appropriate representation space for 
the documents and to select optimal features for 
each document. Another issue is the number (k) of 
neighbors that should be selected to use in the next 
step.
Producing an indexing set
When applied to a single-class classification prob-
lem, the class that is the most frequent among the k 
neighbors is usually assigned to the query docu-
ment. Indexing is a multi-class problem for which 
the number of classes a document should be as-
signed is not known, as it may vary from one 
document to another. Therefore, indexing terms 
from the neighbor documents are all taken into 
account and ranked according to the number of 
neighbors that were labeled with them. The more 
neighbors labeled with a given indexing term, the 
higher the confidence that it will be a relevant in-
dexing term for the query document. This resulting 
indexing set may then be filtered to select only the 
terms that were obtained from a defined minimum 
number of neighbors.
2.2 Document representation
Generic representation 
A generic representation of documents is obtained 
from the text formed by the title and abstract. This 
text is processed so that punctuation is removed, 
stop-words from a pre-defined list (of 310 words) 
are removed, remaining words are switched to 
lower case and a minimal amount of stemming is 
applied. As described by Salton (1989) words 
should be weighted according to the number of 
times they occur in the query document and the 
number of times they occur in the whole collection
(here, MEDLINE). Moreover, words from the title 
are given an additional weight compared to words 
from the abstract. Further adjustments relative to 
document length and local weighting according to 
the Poisson distribution are detailed in (Aronson et 
al, 2000; Kim et al, 2001) where the PubMed Re-
lated Citations (PRC) algorithm is discussed. Fur-
ther experiments showed that the best results were 
obtained by using the ten nearest neighbors.
Domain-specific representation 
In specialized domains, documents from the litera-
ture may be represented with concepts or objects 
commonly used or studied in the field. For exam-
ple, (Rhodes et al, 2007) meet specific chemistry 
oriented search needs by representing US patents 
and patent applications with molecular information 
in the form of chemical terms and structures. A
similar representation is used for PubChem
(http://pubchem.ncbi.nlm.nih.gov/) records. In the 
genetics domain, genes are among the most com-
monly discussed or manipulated concepts. There-
fore, genes should provide a relevant domain-
specific description of documents from the genet-
ics literature.
The second indexing algorithm that we describe 
in this paper, know as the Gene Reference Into 
Function (GeneRIF) Related Citations (GRC) algo-
rithm, uses ?GeneRIF? links (defined in the para-
graph below) to retrieve neighbors for a query 
document.
To form a specific representation of the docu-
ment, gene names are retrieved by ABGene1 (Ta-
nabe and Wilbur, 2002) and mapped to Entrez 
Gene2 unique identifiers. The mapping was per-
formed with a version of SemRep (Rindflesch and
Fiszman, 2003) restricted to human genes. It con-
sists in normalizing the gene name (switch to lower 
case, remove spaces and hyphens) and matching 
the resulting string to one of the gene names or 
aliases listed in Entrez Gene.
For each gene, the GeneRIF links supply a sub-
set of MEDLINE citations manually selected by 
NLM indexers for describing the functions associ-
ated with the gene. These sets were used in two 
ways: 
To complete the document representation. If a 
citation was included in the GeneRIF of a 
given gene, the gene was given an additional 
weight in the document representation. 
To limit the set of possible neighbors. In the 
generic representation, all MEDLINE cita-
tions contain the representation features, 
words. Therefore, they all have to be con-
sidered as potential neighbors. However, 
                                                          
1 Software downloaded January 17, 2007, from
http://www.ncbi.nlm.nih.gov/staff/lsmith/MedPost.html 
2 Retrieved January 17, 2007, from: 
http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?db=gene
184
only a subset of citations actually contains
genes. Therefore, only those citations need 
to be considered as potential neighbors. This 
observation enables us to limit the specific 
processing to relevant citations. Possible 
neighbors for a query document consist of 
the union of the GeneRIF citations corre-
sponding to each gene in the document rep-
resentation. 
Table 1: Gene description of a sample MEDLINE 
document and its two nearest neighbors
PubMed IDs ABGene Entrez Gene IDs
15645653 abcc6 
mrp6
ldl-r
pxe
fh
368
368; 6283
3949
368; 5823
2271
10835643 mrp6
pxe
368; 6283
368; 5823
16392638 abcc6
mrp6
pxe
368
368; 6283
368; 5823
For each query document, the set of possible 
neighbors was processed and ranked according to 
gene similarity using a cosine measure. Table 1 
shows the description of a sample MEDLINE cita-
tion and its two nearest neighbors. 
Based on experiments with the PubMed Related 
Citations algorithm, ten neighbors were retained to 
form a candidate set of indexing terms.
3 Experiment
3.1 Application to MeSH indexing  
In the MEDLINE database, publications of the bio-
medical domain are indexed with Medical Subject 
Headings, or MeSH descriptors. MeSH contains 
about 24,000 main headings denoting medical con-
cepts such as foot, bone neoplasm or appendec-
tomy. MeSH also contains 83 subheadings such as 
genetics, metabolism or surgery that can be associ-
ated with the main headings in order to refer to a 
specific aspect of the concept. Moreover, each de-
scriptor (a main heading alone or associated with 
one or more subheadings) is assigned a ?minor? or 
?major? weight depending on how substantially the 
concept it denotes is discussed in the article. ?Ma-
jor? descriptors are marked with a star. 
In order to form a candidate indexing set to be 
assigned to a query document, the descriptors as-
signed to each of the neighbors were broken down 
into a set of main headings and pairs (i.e. a main 
heading associated with a single subheading). For 
this experiment, indications of major terms were 
ignored. 
For example, the MeSH descriptor 
*Myocardium/cytology/metabolism would gener-
ate the main heading Myocardium and the two 
pairs Myocardium/cytology and Myocar-
dium/metabolism. 
3.2 Test Corpus
Both methods were tested on a corpus composed of 
a selection of the 49,863 citations entered into 
MEDLINE in January 2005. The 2006 version of 
MeSH was used for the indexing in these citations. 
About one fifth of the citations (10,161) are con-
sidered to be genetics-related, as determined by 
Journal Descriptor Indexing (Humphrey, 1999). 
Our test corpus was composed of genetics-related 
citations from which Entrez Gene IDs could be 
extracted ? about 40% of the cases. The final test 
corpus size was 3,962. Appendix A shows a sam-
ple citation from the corpus.
3.3 Protocol
Figure 1 shows the setting of our experiment. 
Documents from the test corpus described above 
were processed to obtain both a generic and spe-
cific representation as described in section 2.2. The 
corresponding ten nearest neighbors were retrieved 
using the PRC and GRC algorithms. All the 
neighbors? MeSH descriptors were pooled to form 
candidate indexing sets of descriptors that were
evaluated using precision and recall measures. Pre-
cision was the number of candidate descriptors that 
were selected as indexing terms by NLM indexers 
(according to reference MEDLINE indexing) over 
the total number of candidate descriptors. Recall 
was the number of candidate descriptors that were 
selected as indexing terms by NLM indexers over 
the total number of indexing terms expected (ac-
cording to reference MEDLINE indexing). For 
better comparison between the methods, we also 
computed F-measure giving equal weight to preci-
185
sion and recall - F1=2*PR/(P+R) and giving a 
higher weight to recall - F3=10*PR/(9P+R).
Four different categories of descriptors were 
considered in the evaluation: 
MH: MeSH main headings (regardless of 
whether subheadings were attached in the 
reference indexing)
SH: stand-alone subheadings (regardless of the 
main heading(s) they were attached to in the 
reference indexing)
MH/SH: main heading/subheading pairs
DESC: MeSH descriptors, i.e. main headings 
and main heading/subheading pairs
Similarly, four different candidate indexing sets 
were considered: the indexing set resulting from 
PRC, the indexing set resulting from GRC, the in-
dexing set resulting from the pooling of PRC and 
GRC sets and finally the indexing set resulting 
from the intersection of PRC and GRC indexing 
sets (common index terms). 
Figure 1: Producing candidate indexing sets with 
generic and domain-specific representations.
4 Results
Appendix B shows the indexing sets obtained from 
the GRC and PRC algorithms for a sample citation 
from the test corpus. Table 2 presents the results of 
our experiments. For each category of descriptors, 
the best performance was bolded. It can be ob-
served that in general, the best precision and F1
scores are obtained with the common indexing set, 
the best recall is obtained with the pooling of in-
dexing sets and the best F3 score is obtained with 
PRC algorithm, the pooling of indexing sets being 
a close second. 
5 Discussion
5.1 Performance of the methods
As can be seen from the bolded figures in table 2, 
the best performance is obtained either from the 
PRC algorithm, or from a combination of PRC and 
GRC. When indexing methods are combined, it is 
usually expected that statistical methods will pro-
vide the best recall whereas linguistic methods will 
provide the best precision. Combining complemen-
tary methods is then expected to provide the best 
overall performance. In this context, it seems that 
the option of pooling the indexing sets should be 
retained for further experiments. The most signifi-
cant result of this study is that the pooling of meth-
ods achieves a recall of 92% for stand-alone 
subheading retrieval. While the precision is only 
19%, the selection of stand-alone subheadings of-
fered by our methods is nearly exhaustive and it 
reduces by 70% the size of the list of allowable 
subheadings that could potentially be used. NLM 
indexers have declared this could prove very useful 
to enhance their indexing practice.
In order to qualify the added value of the spe-
cific description, we looked at the descriptors that 
were correctly recommended by GRC and not rec-
ommended by PRC. Check Tags (descriptors used 
to denote the species, age and gender of the sub-
jects discussed in an article) seemed prominent, but 
only Human was significantly recommended cor-
rectly more often than it was recommended incor-
rectly (~2.2 times more correct than incorrect 
recommendations ? 2,712 correct vs. 1,250 incor-
rect). No other descriptor could be identified as 
being consistently recommended either correctly or 
incorrectly.
Generic 
representation 
Text Words
Specific
 representation
Genes
1- Find similar 
documents
MEDLINE document
PubMed
Related Citations
(PRC)
GeneRIFs
Related Citations
(GRC)
2- Use index terms
in similar
 documents as 
indexing candidates
PRC 
indexing set
GRC
indexing set
186
For both methods, filtering the indexing sets 
according to the number of neighbors that lead to 
include the indexing terms results in an increase of 
precision and a loss of recall. The best trade-off
(measured by F1) is obtained when indexing terms 
come from at least three neighbors (data not 
shown).
5.2 A scale of indexing performance
The problem with evaluating indexing is that, 
although inter-indexer variability is reduced when 
a controlled vocabulary is used, indexing is an 
open cognitive task for which there is no unique 
?right? solution.
Table 2: performance of the indexing methods on the four categories of descriptors
SH MH SH/MH DESC
P      R       F1     F3 P      R      F1      F3 P      R      F1      F3 P      R      F1      F3
GRC 21     72     32     58 8      49     14      32 3      23     6        14 6      38     10      25
PRC 27     88     41     72 13    61     22      45 8      56     15      36 11    59     18      41
Pool 19     92     32     67 9      82     16      44 5      62     9        29 7      74     13     38
Common 36     68     47     62 22    27     24      27 18    17     17      17 21    23     22      23
In practice, this means that there is no ideal 
unique set of descriptors to use for the indexing 
of a particular document. Therefore, when com-
paring an indexing set obtained automatically 
(e.g. here with the PRC or GRC methods) to a 
?gold standard? indexing set produced by a 
trained indexer (e.g. here, NLM indexers) the 
difference observed can be due to erroneous de-
scriptors produced by the automatic methods. 
But it is also likely that the automatic methods 
will produce terms that are semantically close to 
what the human indexer selected or even rele-
vant terms that the human indexer considered or 
forgot to select. While evaluation methods to 
assess the semantic similarity between indexing 
sets are investigated (N?v?ol et al 2006), a con-
sistency study by Funk et al (1983) can shade 
some light on inter-indexer consistency in 
MEDLINE and what range of performance may 
be expected from automatic systems. In this 
study, Hooper?s consistency (the average pro-
portion of terms in agreement between two in-
dexers) for stand-alone subheadings (SH) was 
48.7%. It was 33.8% for pairs (MH/SH) and 
48.2% for main headings (MH). In light of these 
figures, although no direct comparison with the 
results of our experiment is possible, the preci-
sion obtained from the common recommenda-
tions (especially for stand-alone subheadings, 
36%) seems reasonably useful. Further more, 
when informally presenting the indexers sample 
recommendations obtained with these methods, 
they expressed their interest in the high recall as 
reviewing a larger selection of potentially useful 
terms might help them track important descrip-
tors they may not have thought of using other-
wise.
In comparison with other research, the results 
are also encouraging: the recall resulting from 
either PRC or pooling the indexing sets is sig-
nificantly better than that obtained by N?v?ol et 
al. (2007) on a larger set of MEDLINE 2005 
citations ? 20% at best for main head-
ing/subheading pairs with a dictionary-based 
method which consisted in extracting main head-
ing and subheading separately from the citations 
(using MTI and string matching dictionary en-
tries) before forming all the allowable pairs as 
recommendations.
5.3 Limitations of the experiment
In the specific description, the mapping between 
gene names and Entrez Gene IDs only takes hu-
man genes into account, which potentially limits 
the scope of the method, since many more or-
ganisms and their genes may be discussed in the 
literature. In some cases, this limitation can lead 
to confusion with other organisms. For example, 
the gene EPO ?erythropoietin? is listed in Entrez 
Gene for 11 organisms including Homo Sapiens. 
With our current algorithm, this gene will be 
assumed to be a human gene. In the case of 
PMID 15213094 in our test corpus, the organism 
discussed in the paper was in fact Mus Musculus
(common mouse). In this particular case, the 
check tag Humans, which was erroneous, could 
be found in the candidate indexing set. However, 
187
correct indexing terms could still be retrieved 
due to the fact that both the human and mouse 
gene share common functions. 
Another limitation is the size of the test cor-
pus, which was limited to less than 4,000 docu-
ments.
5.4 Mining the biomedical literature for 
gene-concept links
Other approaches to gene-keyword mapping ex-
ploit the links between genes and diseases or 
proteins as they are described either in the re-
cords of databases such as OMIM or more for-
mally expressed as in the GeneRIF. Substantial 
work has addressed linking DNA microarray 
data to keywords in controlled vocabulary such 
as MeSH (Masys et al 2001) or characterizing 
gene clusters with text words from the literature 
(Liu et al 2004). However, no normalized ?se-
mantic fingerprinting? has been yet produced 
between controlled sets such as Entrez Gene and 
MeSH terms.
6 Conclusion and future work
In this paper, we applied a statistical method for 
indexing documents from the genetics literature. 
We presented two different document represen-
tations, one generic and one specific to the ge-
netics domain. The results bear out our 
expectations that such statistical methods can 
also be used successfully to produce recommen-
dations involving subheadings. Furthermore, 
they yield higher recall than other more linguis-
tic-based methods. In terms of recall, the best 
results are obtained when the indexing sets from 
both the specific and generic representations are 
pooled.
In future work, we plan to refine the algorithm 
based on the specific method by expending its
scope to other organisms than Homo Sapiens
and to take the gene frequency in the title and 
abstract of documents into account for the repre-
sentation. Then, we shall conduct further evalua-
tions in order to observe the impact of these 
changes, and to verify that similar results can be 
obtained on a larger corpus.
Acknowledgments
This research was supported in part by an ap-
pointment of A. N?v?ol to the Lister Hill Center 
Fellows Program sponsored by the National Li-
brary of Medicine and administered by the Oak
Ridge Institute for Science and Education. The 
authors would like to thank Halil Kilicoglu for 
his help with obtaining Entrez Gene IDs from 
the ABgene output. We also thank Susanne 
Humphrey and Sonya Shooshan for their in-
sightful comments on the preparation and edit-
ing of this manuscript.
References
Alan R. Aronson, Olivier Bodenreider, H. Florence 
Chang, Susanne M. Humphrey, James G. Mork, 
Stuart J. Nelson, Thomas C. Rindflesch and W. 
John Wilbur. 2000. The NLM Indexing Initiative. 
Proceedings of the Annual American Medical In-
formatics Association Symposium. (AMIA 2000):
17-21.
Alan R. Aronson. 2001. Effective mapping of bio-
medical text to the UMLS Metathesaurus: the 
MetaMap program. Proceedings of the Annual 
AMIA Symposium. (AMIA 2001):17-21.
Alan R. Aronson, James G. Mork, Cliff W. Gay, Su-
sanne M. Humphrey and William J. Rogers. 2004. 
The NLM Indexing Initiative's Medical Text In-
dexer. Proceedings of Medinfo 2004: 268-72.
Kate Byrne and Ewan Klein. 2003. Image Retrieval 
using Natural Language and Content-Based tech-
niques. In Arjen P. de Vries, ed. Proceedings of 
the 4th Dutch-Belgian Information Retrieval 
Workshop (DIR 2003):57-62.
Scott Deerwester, Susan Dumais, Georges Furnas, 
Thomas Landauer and Richard Harshman. 1990. 
Indexing by latent semantic analysis. Journal of 
American Society for Information Science, 6 
(41):391-407.
Mark E. Funk, Carolyn A. Reid and Leon S. 
McGoogan. 1983. Indexing consistency in 
MEDLINE. Bull. Med. Libr. Assoc. 71(2):176-
183.
Susanne M. Humphrey. 1999. Automatic indexing of 
documents from journal descriptors: a preliminary 
investigation. J Am Soc Inf Sci Technol. 50(8):661-
674
Christian Jacquemin and Evelyne Tzoukermann. 
1999. NLP for term variant extraction: Synergy of 
morphology, lexicon, and syntax. In T. 
Strzalkowski (Ed.), Natural language information 
retrieval (p. 25-74). Boston, MA: Kluwer.
188
Won Kim, Alan R. Aronson and W. John Wilbur. 
2001. Automatic MeSH term assignment and qual-
ity assessment. Proceedings of the Annual AMIA
Symposium: 319-23.
Ying Liu, Martin Brandon, Shamkant Navathe, Ray 
Dingledine and Brian J. Ciliax. 2004. Text mining 
functional keywords associated with genes. Pro-
ceedings of MEDINFO 2004: 292-296
Daniel R. Masys, John B. Welsh, J. Lynn Fink, Mi-
chael Gribskov, Igor Klacansky and Jacques Cor-
beil. 2001. Use of keyword hierarchies to interpret 
gene expression patterns. In: Bioinformatics
17(4):319-326  
Adeline Nazarenko and Touria Ait El Mekki 2005. 
Building back-of-the-book indexes. In: Terminol-
ogy 11(1):199?224
Aur?lie N?v?ol, Kelly Zeng, Olivier Bodenreider. 
2006. Besides precision & recall: Exploring alter-
native approaches to evaluating an automatic in-
dexing tool for MEDLINE. Proceedings of the 
Annual AMIA Symposium: 589-93.
Aur?lie N?v?ol, Sonya E. Shooshan, Susanne M. 
Humphrey, Thomas C. Rindflesch and Alan R 
Aronson. 2007. Multiple approaches to fine-
grained indexing of the biomedical literature. Pro-
ceedings of the 12th Pacific Symposium on Bio-
computing. 12:292-303
James Rhodes, Stephen Boyer, Jeffrey Kreulen, Ying 
Chen, Patricia Ordonez. 2007. Mining Patents Us-
ing Molecular Similarity Search. Proceedings of 
the 12th Pacific Symposium on Biocomputing. 
12:304-315
Thomas C. Rindflesch and Marcelo Fiszman. 2003. 
The interaction of domain knowledge and linguis-
tic structure in natural language processing: inter-
preting hypernymic propositions in biomedical 
text. J Biomed Inform. 36(6), 462-77
Gerald Salton. 1989. Automatic text processing : The 
transformation, analysis, and retrieval of informa-
tion by computer. Reading, MA : Addison-Wesley.
Karen Sparck-Jones, Steve Walker and Stephen E. 
Robertson. 2000. A probalistic model of informa-
tion retrieval: development and comparative ex-
periments (part 1). Information Processing and 
Management, 36(3):779-808.
Lorraine Tanabe and W. John Wilbur. 2002. Tagging 
gene and protein names in biomedical text. Bioin-
formatics. 2002 Aug;18(8):1124-32.
Appendix A: Title, abstract and reference indexing set for a sample citation
PubMed ID 15645653
Title Identification of two novel missense mutations (p.R1221C and p.R1357W) in the ABCC6 (MRP6) 
gene in a Japanese patient with pseudoxanthoma elasticum (PXE).
Abstract Pseudoxanthoma elasticum (PXE) is a rare, inherited, systemic disease of elastic tissue that in par-
ticular affects the skin, eyes, and cardiovascular system. Recently, the ABCC6 (MRP6) gene was 
found to cause PXE. A defective type of ABCC6 gene (16pl3.1) was determined in two Japanese 
patients with PXE. In order to determine whether these patients have a defect in ABCC6 gene, we 
examined each of 31 exons and flanking intron sequences by PCR methods (SSCP screening and 
direct sequencing). We found two novel missense variants in exon 26 and 29 in a compound het-
erozygous state in the first patient. One is a missense mutation (c.3661C>T; p.R1221C) in exon 26 
and the other is a missense mutation (c.4069C>T; p.R1357W) in exon 29. These mutations have 
not been detected in our control panel of 200 alleles. To our knowledge, this is the first report of 
mutation identification in the ABCC6 gene in Japanese PXE patients. The second patient was ho-
mozygous for 2542_2543delG in ABCC6 gene and heterozygous for 6 kb deletion of LDL-R gene. 
This case is the first report of a genetically confirmed case of double mutations both in PXE and 
FH loci.
MeSH 
reference 
indexing set
Adult
Aged
Female
Humans
Japan
Multidrug Resistance-Associated Proteins/*genetics
*Mutation, Missense
Pedigree
Pseudoxanthoma Elasticum/*genetics
189
Appendix B: Sample indexing sets obtained from the GRC and PRC algorithms for 
a sample citation
PubMed ID 15645653
GRC indexing 
set* (top 15 terms)
Humans (10)
Multidrug Resistance-Associated Proteins (9)
Mutation (8)
Male (7)
Female (7)
Multidrug Resistance-Associated Proteins/genetics (7)
Pseudoxanthoma Elasticum (6)
Pseudoxanthoma Elasticum/genetics (6)
Pedigree (5)
Exons (4)
DNA Mutational Analysis (4)
Mutation/genetics (4)
Adult (4)
Introns (3)
Aged (3)
PRC indexing 
set* (top 15 terms)
Multidrug Resistance-Associated Proteins (10)
Multidrug Resistance-Associated Proteins /genetics (10)
Pseudoxanthoma Elasticum (10)
Pseudoxanthoma Elasticum/genetics (10)
Mutation (7)
DNA Mutational Analysis (6)
Pedigree (5)
Genotype (4)
Polymorphism, Genetic (4)
Alleles (4)
Mutation/genetics (3)
Haplotypes (3)
Models, Genetic (3)
Gene Deletion (3)
Exons (3)
                                                          
* Terms appearing in the reference set are underlined; the number of neighbors ? out of the 10 nearest neighbors ?
labeled with each term is shown between brackets after the term.
190
Proceedings of the 2011 Workshop on Biomedical Natural Language Processing, ACL-HLT 2011, pages 182?183,
Portland, Oregon, USA, June 23-24, 2011. c?2011 Association for Computational Linguistics
Self-training and co-training in biomedical word sense disambiguation
Antonio Jimeno-Yepes
National Library of Medicine
8600 Rockville Pike
Bethesda, 20894, MD, USA
antonio.jimeno@gmail.com
Alan R. Aronson
National Library of Medicine
8600 Rockville Pike
Bethesda, 20894, MD, USA
alan@nlm.nih.gov
Abstract
Word sense disambiguation (WSD) is an inter-
mediate task within information retrieval and
information extraction, attempting to select
the proper sense of ambiguous words. Due to
the scarcity of training data, semi-supervised
learning, which profits from seed annotated
examples and a large set of unlabeled data,
are worth researching. We present preliminary
results of two semi-supervised learning algo-
rithms on biomedical word sense disambigua-
tion. Both methods add relevant unlabeled ex-
amples to the training set, and optimal param-
eters are similar for each ambiguous word.
1 Introduction
Word sense disambiguation (WSD) is an interme-
diate task within information retrieval and informa-
tion extraction, attempting to select the proper sense
of ambiguous words. Supervised learning achieves
better performance compared to other WSD ap-
proaches (Jimeno-Yepes et al, 2011). Manual anno-
tation requires a large level of human effort whereas
there is a large quantity of unlabeled data. Our
work follows (Mihalcea, 2004) but is applied to the
biomedical domain; it relies on two semi-supervised
learning algorithms.
We have performed experiments of semi-
supervised learning for word sense disambiguation
in the biomedical domain. In the following section,
we present the evaluated algorithms. Then, we
present preliminary results for self-training and
co-training, which show a modest improvement
with a common set-up of the algorithms for the
evaluated ambiguous words.
2 Methods
For self-training we use the definition by (Clark et
al., 2003): ?a tagger that is retrained on its own
labeled cache on each round?. The classifier is
trained on the available training data which is then
used to label the unlabeled examples from which
the ones with enough prediction confidence are se-
lected and added to the training set. The process
is repeated for a number of predefined iterations.
Co-training (Blum and Mitchell, 1998) uses several
classifiers trained on independent views of the same
instances. These classifiers are then used to label the
unlabeled set, and from this newly annotated data
set the annotations with higher prediction probabil-
ity are selected. These newly labeled examples are
added to the training set and the process is repeated
for a number of iterations. Both bootstrapping algo-
rithms produce an enlarged training data set.
Co-training requires two independent views on
the same data set. As first view, we use the context
around the ambiguous word. As second view, we
use the MEDLINE MeSH indexing available from
PubMed which is obtained by human assignment of
MeSH heading based on their full-text articles.
Methods are evaluated with the accuracy mea-
sure on the MSH WSD set built automatically using
MeSH indexing from MEDLINE (Jimeno-Yepes et
al., 2011) 1 in which senses are denoted by UMLS
concept identifiers. To avoid any bias derived from
1Available from: http://wsd.nlm.nih.gov/collaboration.shtml
182
the indexing of the UMLS concept related to the am-
biguous word, the concept has been removed from
the MeSH indexing of the recovered citations.
10-fold cross validation using Na??ve Bayes (NB)
has been used to compare both views which achieve
similar accuracy (0.9386 context text, 0.9317 MeSH
indexing) while the combined view achieves even
better accuracy (0.9491).
In both algorithms a set of parameters is used: the
number of iterations (1-10), the size of the pool of
unlabeled examples (100, 500, 1000) and the growth
rate or number of unlabeled examples which are se-
lected to be added to the training set (1, 10, 20, 50,
100).
3 Results and discussion
Results shown in Table 1 have been obtained from
21 ambiguous words which achieved lower perfor-
mance in a preliminary cross-validation study. Each
ambiguous word has around 2 candidate senses with
100 examples for each sense. We have split the ex-
amples for each ambiguous word into 2/3 for train-
ing and 1/3 for test.
The baseline is NB trained and tested using this
split. Semi-supervised algorithms use this split, but
the training data is enlarged with selected unlabeled
examples. Self-training and the baseline use the
combined views while co-training relies on two NB
classifiers, each trained on one view of the train-
ing data. Even though we are willing to evalu-
ate other classifiers, NB was selected for this ex-
ploratory work since it is fast and space efficient.
Unlabeled examples are MEDLINE citations which
contain the ambiguous word and MeSH heading
terms. Any mention of MeSH heading related to the
ambiguous word has been removed. Optimal param-
eters were selected, and average accuracy is shown
in Table 1.
Method Accuracy
Baseline 0.8594
Self-training 0.8763 (1.93%)
Co-training 0.8759 (1.88%)
Table 1: Accuracy for the baseline, self-training and co-
training
Both semi-supervised algorithms show a modest
improvement on the baseline which is a bit higher
for self-training. Best results are achieved with a
small number of iterations (< 5), a small growth
rate (1-10) and a pool of unlabeled data over 100 in-
stances. Noise affects the performance with a larger
number of iterations, which after an initial increase,
shows a steep decrease in accuracy. Small growth
rate ensures a smoothed increase in accuracy. A
larger growth rate adds more noise after each iter-
ation. A larger pool of unlabeled data offers a larger
set of candidate unlabeled examples to choose from
at a higher computational cost.
4 Conclusions and Future work
Preliminary results show a modest improvement on
the baseline classifier. This means that the semi-
supervised algorithms have identified relevant dis-
ambiguated instances to be added to the training set.
We plan to evaluate the performance of these al-
gorithms on all the ambiguous words available in the
MSH WSD set. In addition, since the results have
shown that performance decreases rapidly after few
iterations, we would like to further explore smooth-
ing techniques applied to bootstrapping algorithms
and the effect on classifiers other than NB.
Acknowledgments
This work was supported by the Intramural Research
Program of the NIH, National Library of Medicine,
administered by ORISE.
References
A. Blum and T. Mitchell. 1998. Combining labeled and
unlabeled data with co-training. In Proceedings of the
eleventh annual conference on Computational learn-
ing theory, pages 92?100. ACM.
S. Clark, J.R. Curran, and M. Osborne. 2003. Bootstrap-
ping POS taggers using unlabelled data. In Proceed-
ings of the seventh conference on Natural language
learning at HLT-NAACL 2003-Volume 4, pages 49?55.
Association for Computational Linguistics.
A. Jimeno-Yepes, B.T. McInnes, and A.R. Aronson.
2011. Exploiting MeSH indexing in MEDLINE
to generate a data set for word sense disambigua-
tion(accepted). BMC bioinformatics.
R. Mihalcea. 2004. Co-training and self-training
for word sense disambiguation. In Proceedings of
the Conference on Computational Natural Language
Learning (CoNLL-2004).
183
Proceedings of the 2013 Workshop on Biomedical Natural Language Processing (BioNLP 2013), pages 102?110,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
Using the argumentative structure of scientific literature to improve
information access
Antonio Jimeno Yepes
National ICT Australia
Victoria Research Laboratory
Melbourne, Australia
antonio.jimeno@gmail.com
James G. Mork
National Library of Medicine
8600 Rockville Pike
Bethesda, 20894, MD, USA
mork@nlm.nih.gov
Alan R. Aronson
National Library of Medicine
8600 Rockville Pike
Bethesda, 20894, MD, USA
alan@nlm.nih.gov
Abstract
MEDLINE/PubMed contains structured
abstracts that can provide argumentative
labels. Selection of abstract sentences
based on the argumentative label has
shown to improve the performance of in-
formation retrieval tasks. These abstracts
make up less than one quarter of all the
abstracts in MEDLINE/PubMed, so it is
worthwhile to learn how to automatically
label the non-structured ones.
We have compared several machine learn-
ing algorithms trained on structured ab-
stracts to identify argumentative labels.
We have performed an intrinsic evalua-
tion on predicting argumentative labels for
non-structured abstracts and an extrinsic
evaluation to predict argumentative labels
on abstracts relevant to Gene Reference
Into Function (GeneRIF) indexing.
Intrinsic evaluation shows that argumen-
tative labels can be assigned effectively
to structured abstracts. Algorithms that
model the argumentative structure seem
to perform better than other algorithms.
Extrinsic results show that assigning ar-
gumentative labels to non-structured ab-
stracts improves the performance on
GeneRIF indexing. On the other hand, the
algorithms that model the argumentative
structure of the abstracts obtain lower per-
formance in the extrinsic evaluation.
1 Introduction
MEDLINE R?/PubMed R? is the largest repository
of biomedical abstracts. The large quantity of
unstructured information available from MED-
LINE/PubMed prevents finding information effi-
ciently. Reducing the information that users need
to process could improve information access and
support database curation. It has been suggested
that identifying the argumentative label of the ab-
stract sentences could provide better information
through information retrieval (Ruch et al, 2003;
Jonnalagadda et al, 2012) and/or information ex-
traction (Mizuta et al, 2006).
Some journals indexed in MEDLINE/PubMed
already provide the abstracts in a structured for-
mat (Ripple et al, 2012). A structured abstract1 is
an abstract with distinct labeled sections (e.g., In-
troduction, Background, or Results). In the MED-
LINE/PubMed data, these labels usually appear in
all uppercase letters and are followed by a colon
(e.g., MATERIALS AND METHODS:). Structured
abstracts are becoming an increasingly larger seg-
ment of the MEDLINE/PubMed database with al-
most a quarter of all abstracts added to the MED-
LINE/PubMed database each year being struc-
tured abstracts. A recent PubMed query (April 22,
2013) shows 1,050,748 citations from 2012, and
249,196 (23.72%)2 of these are considered struc-
tured abstracts.
On August 16, 2010, PubMed began display-
ing structured abstracts formatted to highlight the
various sections within the structured abstracts to
help readers identify areas of interest3. The XML
formatted abstract from MEDLINE/PubMed sep-
arates each label in the structured abstract and in-
cludes a mapping to one of five U.S. National Li-
brary of Medicine (NLM) assigned categories as
shown in the example below:
<AbstractText Label=?MATERIALS AND
METHODS? NlmCategory=?METHODS?>
The five NLM categories that all labels
are mapped to are OBJECTIVE, CONCLU-
SIONS, RESULTS, METHODS, and BACK-
GROUND (Ripple et al, 2011). If a label is new
1http://www.nlm.nih.gov/bsd/policy/structured abstracts.html
2hasstructuredabstract AND 2012[pdat]
3http://www.nlm.nih.gov/pubs/techbull/ja10/ja10 structured abstracts.html
102
or not in the list of reviewed structured abstract la-
bels, it will receive a category of UNASSIGNED.
There are multiple criteria for deciding what ab-
stracts are considered structured abstracts or not.
One simple definition would be that an abstract
contains one or more author defined labels. A
more rigid criterion which is followed by NLM4
is that an abstract must contain three or more
unique valid labels (previously identified and cat-
egorized), and one of the labels must be an ending
type label (e.g., CONCLUSIONS). The five NLM
categories are normally manually reviewed and as-
signed once a year to as many new labels as pos-
sible. Currently, NLM has identified 1,949 (Au-
gust 31, 2012) unique labels and categorized them
into one of the five categories. These 1,949 labels
make up approximately 98% of all labels and la-
bel variations found in the structured abstracts in
MEDLINE/PubMed3. An example of structured
abstract is presented in Table 1.
Several studies have shown that the labels of the
structured abstracts can be reassigned effectively
based on a Conditional Random Field (CRF) mod-
els (Hirohata et al, 2008). On the other hand, it
is unclear if these models are as effective on non-
structured abstracts (Agarwal and Yu, 2009).
In this paper, we compare several learning al-
gorithms trained on structured abstract data to as-
sign argumentative labels to non-structured ab-
stracts. We performed comparison tests of the
trained models both intrinsically on a held out set
of the structured abstracts and extrinsically on a
set of non-structured abstracts.
The intrinsic evaluation is performed on a data
set of held out structured abstracts that have had
their label identification removed to model non-
structured abstracts. Argumentative labels are as-
signed to the sentences based on the trained mod-
els and used to identify label categorization.
The extrinsic evaluation is performed on a data
set of non-structured abstracts on the task of iden-
tifying GeneRIF (Gene Into Function) sentences.
Argumentative labels are assigned to the sentences
based on the trained models and used to perform
the selection of relevant GeneRIF sentences.
Intrinsic evaluation shows that argumentative
labels can be assigned effectively to structured ab-
stracts. Algorithms that model the argumentative
structure, like Conditional Random Field (CRF),
seem to perform better than other algorithms. Re-
4http://structuredabstracts.nlm.nih.gov/Implementation.shtml
sults show that using the argumentative labels as-
signed by the learning algorithms improves the
performance in GeneRIF sentence selection. On
the other hand, models like CRF, which better
model the argumentative structure of the struc-
tured abstracts, tend to perform below other learn-
ing algorithms on the extrinsic evaluation. This
shows that non-structured abstracts do not have the
same layout compared to structured ones.
2 Related work
As presented in the introduction, one of the ob-
jectives of our work is to assign structured ab-
stract labels to abstracts without these labels. The
idea is to help in the curation process of exist-
ing databases and to improve the efficiency of
information access. Previous work on MED-
LINE/PubMed abstracts has focused on learning
to identify these labels mainly in the Randomized
Control Trials (RCT) domain. (McKnight and
Srinivasan, 2003) used a Support Vector Machine
(SVM) and a linear classifier and tried to pre-
dict the labels of MEDLINE structured abstracts.
Their work finds that it is possible to learn a model
to label the abstract with modest results. Further
studies have been conducted by (Ruch et al, 2003;
Tbahriti et al, 2005; Ruch et al, 2007) to use
the argumentative model of the abstracts. They
have used this to improve retrieval and indexing of
MEDLINE citations, respectively. In their work,
they have used a multi-class Na??ve Bayes classi-
fier.
(Hirohata et al, 2008) have shown that the la-
bels in structured abstracts follow a certain argu-
mentative structure. Using the current set of labels
used at the NLM, a typical argumentative struc-
ture consists of OBJECTIVE, METHODS, RE-
SULTS and CONCLUSION. This notion is some-
what already explored by (McKnight and Srini-
vasan, 2003) by using the position of the sentence.
More advanced approaches have been used that
train a model that considers the sequence of labels
in the structured abstracts. (Lin et al, 2006) used a
generative model, comparing them to discrimina-
tive ones. More recent work has been dealing with
Conditional Random Fields (Hirohata et al, 2008)
with good performance.
(Agarwal and Yu, 2009) used similar ap-
proaches and evaluated the labeling of full text
articles with the trained model on structured ab-
stracts. Their evaluation included as well a set of
103
<Abstract><AbstractText Label=?PURPOSE? NlmCategory=?OBJECTIVE?>To explore the effects of cervical loop
electrosurgical excision procedure (LEEP) or cold knife conization (CKC) on pregnancy outcomes.</AbstractText>
<AbstractText Label=?MATERIALS AND METHODS? NlmCategory=?METHODS?>Patients with cervical intraep-
ithelial neoplasia (CIN) who wanted to become pregnant and received LEEP or CKC were considered as the treat-
ment groups. Women who wanted to become pregnant and only underwent colposcopic biopsy without any treat-
ments were considered as the control group. The pregnancy outcomes were observed and compared in the three
groups.</AbstractText>
<AbstractText Label=?RESULTS? NlmCategory=?RESULTS?>Premature delivery rate was higher (p = 0.048) in the
CKC group (14/36, 38.88%) than in control group (14/68, 20.5%) with a odds ratio (OR) of 2.455 (1.007 - 5.985);
and premature delivery was related to cone depth, OR was significantly increased when the cone depth was more than
15 mm. There was no significant difference in premature delivery between LEEP (10 / 48, 20.83%) and the control
groups. The average gestational weeks were shorter (p = 0.049) in the CKC group (36.9 +/- 2.4) than in the control
group (37.8 +/- 2.6), but similar in LEEP (38.1 +/- 2.4) and control groups. There were no significant differences
in cesarean sections between the three groups. The ratio of neonatal birth weight less than 2,500 g was significantly
higher (p = 0.005) in the CKC group (15/36) than in the control group (10/68), but similar in the LEEP and control
groups.</AbstractText>
<AbstractText Label=?CONCLUSION? NlmCategory=?CONCLUSIONS?>Compared with CKC, LEEP is relatively
safe. LEEP should be a priority in the treatment of patients with CIN who want to become pregnant.</AbstractText>
</Abstract>
Table 1: XML example for PMID 23590007
abstracts manually annotated. They found that the
performance on full-text was below what was ex-
pected. A similar result was found in the manu-
ally annotated set. They found, as well, that the
abstract sentences are noisy and sometimes the
sentences from structured abstracts did not belong
with the label they were assigned to.
A large number of abstracts in MEDLINE are
not structured; thus intrinsic evaluation of the al-
gorithms trained to predict the argumentative la-
bels on structured abstracts is not completely real-
istic. Extrinsic evaluation has been previously per-
formed by (Ruch et al, 2003; Tbahriti et al, 2005;
Ruch et al, 2007) in information retrieval results
evaluating a Na??ve Bayes classifier. We have ex-
tended this work by evaluating a larger set of al-
gorithms and heuristics on a data set developed
to tune and evaluate a system for GeneRIF index-
ing on a data set containing mostly non-structured
abstracts. The idea is that GeneRIF relevant sen-
tences will be assigned distinctive argumentative
labels.
A Gene Reference Into Function (GeneRIF) de-
scribes novel functionality of genes. The cre-
ation of GeneRIF entries involves the identifica-
tion of the genes mentioned in MEDLINE cita-
tions and the citation sentences describing a novel
function. GeneRIFs are available from the NCBI
(National Center for Biotechnology Information)
Gene database5. An example sentence is shown
below linked to the BRCA1 gene with gene id
672 from the citation with PubMed R? identifier
(PMID) 22093627:
5http://www.ncbi.nlm.nih.gov/sites/entrez?db=gene
FISH-positive EGFR expression is associated
with gender and smoking status, but not
correlated with the expression of ERCC1 and
BRCA1 proteins in non-small cell lung cancer.
There is limited previous work related to
GeneRIF span extraction. Most of the available
publications are related to the TREC Genomics
Track in 2003 (Hersh and Bhupatiraju, 2003).
There were two main tasks in this track, the first
one consisted of identifying relevant citations to
be considered for GeneRIF annotation.
In the second task, the participants had to pro-
vide spans of text that would correspond to rel-
evant GeneRIF annotations for a set of citations.
Considering this second task, the participants were
not provided with a training data set. The Dice
coefficient was used to measure the similarity be-
tween the submitted span of text from the title and
abstract of the citation and the official GeneRIF
text in the test set.
Surprisingly, one of the main conclusions was
that a very competitive system could be obtained
by simply delivering the title of the citation as the
best GeneRIF span of text. Few teams (EMC (Je-
lier et al, 2003) and Berkley (Bhalotia et al, 2003)
being exceptions), achieved results better than that
simple strategy. Another conclusion of the Ge-
nomics Track was that the sentence position in the
citation is a good indicator for GeneRIF sentence
identification: either the title or sentences close to
the end of the citation were found to be the best
candidates.
Subsequent to the 2003 Genomics Track, there
has been some further work related to GeneRIF
104
sentence selection. (Lu et al, 2006; Lu et al,
2007) sought to reproduce the results already
available from Entrez Gene (former name for the
NCBI Gene database). In their approach, a set
of features is identified from the sentences and
used in the algorithm: Gene Ontology (GO) to-
ken matches, cue words and sentence position in
the abstract. (Gobeill et al, 2008) combined argu-
mentative features using discourse-analysis mod-
els (LASt) and an automatic text categorizer to
estimate the density of Gene Ontology categories
(GOEx). The combination of these two feature
sets produced results comparable to the best 2003
Genomics Track system.
3 Methods
As in previous work, we approach the problem
of learning to label sentences in abstracts us-
ing machine learning methods on structured ab-
stracts. We have compared a large range of ma-
chine learning algorithms, including Conditional
Random Field. The evaluation is performed in-
trinsically on a held out set of structured abstracts
and then evaluated extrinsically on a dataset devel-
oped for the evaluation of algorithms for GeneRIF
indexing.
3.1 Structured abstracts data set
This data set is used to train the machine learning
algorithms and to peform the intrinsic evaluation
of structured abstracts. The abstracts have been
collected from PubMed using the query hasstruc-
turedabstract, selecting the top 100k citations sat-
ifying the query.
The abstract defined within the Abstract at-
tribute is split into several AbstractText tags. Each
AbstractText tag has the label Label that shows
the original label as provided by the journal while
the NlmCategory represents the category as added
by the NLM.
From this set, 2/3 of the citations (66,666) are
considered for training the machine learning algo-
rithms while 1/3 of the citations (33,334) are re-
served for testing. The abstract paragraphs have
been split into sentences and the structured ab-
stract label has been transferred to them. For in-
stance, all the sentences in the INTRODUCTION
section are labeled as INTRODUCTION.
An analysis of the abstracts has shown that there
are cases in which the article keywords were in-
cluded as part of the abstract in a BACKGROUND
section. These were easily recognized by the orig-
inal label KEYWORD. We have removed these
paragraphs since they are not typical sentences
in MEDLINE but a list of keywords. We find
that there are sections like OBJECTIVE where the
number of sentences is very low, with less than 2
sentences on average, while RESULTS is the sec-
tion with the largest number of sentences on aver-
age with over 4.5 sentences.
There are five candidate labels identified from
the structured abstracts, presented in Table 2. The
distribution of labels shows that some labels like
CONCLUSIONS, METHODS and RESULTS are
very frequent. CONCLUSIONS and METHODS
are assigned to more than one paragraph since the
number is bigger compared to the number of cita-
tions in each set. This seems to happen when more
than one journal label in the same citation map
to METHODS or CONCLUSION, e.g. PMID:
23538919.
Label Paragraphs Sentences
BACKGROUND 53,348 132,890
CONCLUSIONS 101,830 205,394
METHODS 107,227 304,487
OBJECTIVE 60,846 95,547
RESULTS 95,824 436,653
Table 2: Structured abstracts data set statistics
We have compared the performance of sev-
eral learning algorithms. Among other classi-
fiers, we use Na??ve Bayes and Linear Regression,
which might be seen as a generative learner ver-
sus discriminative (Jordan, 2002) learner. We have
used the implementation available from the Mallet
package (McCallum, 2002).
In addition to these two classifiers, we have
used AdaBoostM1 and SVM. SVM has been
trained using stochastic gradient descent (Zhang,
2004), which is very efficient for linear ker-
nels. Table 2 shows a large imbalance between
the labels, so we have used the modified Huber
Loss (Zhang, 2004), which has already been used
in the context of MeSH indexing (Yeganova et al,
2011). Both algorithms were trained based on the
one-versus-all approach. We have turned the algo-
rithms into multi-class classifiers by selecting the
prediction with the highest confidence by the clas-
sifiers (Tsoumakas and Katakis, 2007). We have
used the implementation of these algorithms avail-
105
able from the MTI ML package6, previously used
in the task of MeSH indexing (Jimeno-Yepes et al,
2012).
The learning algorithms have been trained on
the text of the paragraph or sentences from the
data set presented above. The text is lowercased
and tokenized. In addition to the textual features,
the position of the sentence or paragraph from the
beginning of the abstract is used as well.
As we have seen, argumentative structure of the
abstract labels has been previously modeled using
a linear chain CRF (Lafferty et al, 2001). CRF
is trained using the text features from sentences or
paragraphs in conjunction of the abstract labels to
perform the label assignment. In our experiments,
we have used the implementation available from
the Mallet package, using only an order 1 model.
3.2 GeneRIF data set
We have developed a data set to compare and
evaluate GeneRIF indexing approaches (Jimeno-
Yepes et al, 2013) as part of the Gene Indexing
Assistant project at the NLM7. The current scope
of our work is limited to the human species. The
development is performed in two steps described
below. The first step consists of selecting cita-
tions from journals typically associated with hu-
man species. During the second step, we apply
Index Section rules for citation filtering plus ad-
ditional rules to further focus the set of selected
citations. Since there was no GeneRIF indexing
before 2002, only articles from 2002 through 2011
from the 2011 MEDLINE Baseline 8 (11/19/2010)
were used to build the data set.
A subset of the filtered citations was collected
for annotation. The annotations were performed
by two annotators. Guidelines were prepared and
tested on a small set by the two annotators and re-
fined before annotating the entire set.
The data set has been annotated with GeneRIF
categories of the sentences. The categories are:
Expression, Function, Isolation, Non-GeneRIF,
Other, Reference, and Structure. We assigned the
GeneRIF category to all the categories that did
not belong to Non-GeneRIF. The indexing task is
then to categorize the sentences into GeneRIF sen-
tences and Non-GeneRIF ones. Based on their an-
notation work on the data set, the F-measure for
6http://ii.nlm.nih.gov/MTI ML/index.shtml
7http://www.lhncbc.nlm.nih.gov/project/automated-
indexing-research
8http://mbr.nlm.nih.gov
the annotators is 0.81. We have used this annota-
tion for the extrinsic evaluation of GeneRIF index-
ing.
This data set has been further split into training
and testing subsets. Table 3 shows the distribution
between GeneRIF and Non-GeneRIF sentences.
Set Total GeneRIF Non-GeneRIF
Training 1987 829 (42%) 1158 (58%)
Testing 999 433 (43%) 566 (57%)
Table 3: GeneRIF sentence distribution
In previous work, the indexing of GeneRIF sen-
tences, on our data set, was performed based on
a trained classifier on a set of features that per-
formed well on the GeneRIF testing set (Jimeno-
Yepes et al, 2013). Na??ve Bayes was the learning
algorithm that performed the best compared to the
other methods and has been selected in this work
as the method to be used to combine the features
of the argumentative labeling algorithms.
The set of features in the baseline experiments
include the position of the sentence from the be-
ginning of the abstract, the position of the sentence
counting from the end of the abstract, the sen-
tence text, the annotation of disease terms, based
on MetaMap (Aronson and Lang, 2010), and gene
terms, based on a dictionary approach, and the
Gene Ontology term density (Gobeill et al, 2008).
4 Results
As mentioned before, we have performed the eval-
uation of the algorithms intrinsically, given a set
of structured abstracts, and extrinsically based on
their performance on GeneRIF sentence indexing.
4.1 Intrinsic evaluation (structured
abstracts)
Tables 4 and 5 show the results of the intrinsic
evaluation for paragraph and sentence experiments
respectively. The algorithms are trained to label
the paragraphs or sentences from the structured
abstracts. The precision (P), recall (R) and F1
(F) values are presented for each argumentative la-
bel. The methods evaluated include Na??ve Bayes
(NB), Logistic Regression (LR), SVM based on
modified Huber Loss (Huber) and AdaBoostM1
(ADA). These methods have been trained on the
text of either the sentence or the paragraph, and
might include their position feature, indicated with
the letter P (e.g. NB P for Na??ve Bayes trained
106
Label NB NB P LR LR P ADA ADA P Huber HuberP CRF
BACKGROUND P 0.6047 0.6853 0.6374 0.7369 0.6098 0.7308 0.5862 0.7166 0.7357
R 0.5672 0.7190 0.5868 0.7207 0.3676 0.7337 0.4984 0.6694 0.7093
F 0.5854 0.7017 0.6110 0.7287 0.4587 0.7323 0.5387 0.6922 0.7223
CONCLUSIONS P 0.7532 0.8626 0.8365 0.9413 0.6975 0.8862 0.7578 0.9051 0.9769
R 0.8606 0.9366 0.8675 0.9552 0.8246 0.9404 0.7987 0.9340 0.9784
F 0.8033 0.8981 0.8517 0.9482 0.7557 0.9125 0.7777 0.9193 0.9776
METHODS P 0.9002 0.9278 0.9113 0.9396 0.8256 0.9041 0.8668 0.9116 0.9684
R 0.9040 0.9126 0.9294 0.9493 0.8955 0.9250 0.9012 0.9237 0.9675
F 0.9021 0.9201 0.9203 0.9444 0.8591 0.9144 0.8837 0.9176 0.9680
OBJECTIVE P 0.7294 0.7650 0.7167 0.7531 0.6763 0.7565 0.6788 0.7160 0.7608
R 0.6453 0.7190 0.7255 0.7549 0.6937 0.7228 0.6733 0.7365 0.7759
F 0.6848 0.7413 0.7210 0.7540 0.6849 0.7393 0.6761 0.7261 0.7683
RESULTS P 0.8841 0.9106 0.9086 0.9372 0.8554 0.9157 0.8560 0.9122 0.9692
R 0.8414 0.8542 0.8857 0.9216 0.7842 0.8564 0.8447 0.8846 0.9758
F 0.8622 0.8815 0.8970 0.9294 0.8182 0.8851 0.8503 0.8981 0.9725
Average P 0.7743 0.8303 0.8021 0.8616 0.7329 0.8387 0.7491 0.8323 0.8822
R 0.7637 0.8283 0.7990 0.8604 0.7131 0.8357 0.7433 0.8296 0.8814
F 0.7690 0.8293 0.8005 0.8610 0.7229 0.8372 0.7462 0.8310 0.8818
Table 4: Intrinsic evaluation of paragraph based labeling
Label NB NB P LR LR P ADA ADA P Huber HuberP CRF
BACKGROUND P 0.4983 0.6313 0.5558 0.6862 0.4779 0.6417 0.5153 0.6495 0.6738
R 0.4980 0.6921 0.5084 0.7139 0.3207 0.6993 0.3372 0.6554 0.7104
F 0.4981 0.6603 0.5311 0.6998 0.3838 0.6693 0.4076 0.6524 0.6916
CONCLUSIONS P 0.5876 0.7270 0.6794 0.8431 0.5672 0.7651 0.6153 0.7767 0.8977
R 0.7103 0.8388 0.6788 0.8187 0.4998 0.6816 0.5163 0.7213 0.8671
F 0.6431 0.7789 0.6791 0.8307 0.5314 0.7209 0.5615 0.7480 0.8821
METHODS P 0.7857 0.8206 0.8193 0.8549 0.7224 0.7793 0.7343 0.7894 0.8931
R 0.8084 0.8366 0.8427 0.8696 0.7789 0.8152 0.7828 0.8250 0.8988
F 0.7969 0.8285 0.8308 0.8622 0.7496 0.7968 0.7578 0.8068 0.8960
OBJECTIVE P 0.5522 0.6237 0.6032 0.6696 0.5497 0.6671 0.5525 0.6259 0.6258
R 0.4894 0.5530 0.4995 0.5534 0.4082 0.4518 0.4479 0.5036 0.5779
F 0.5189 0.5862 0.5465 0.6060 0.4685 0.5388 0.4947 0.5581 0.6009
RESULTS P 0.8294 0.8517 0.8071 0.8449 0.6903 0.7665 0.6957 0.7877 0.8892
R 0.7517 0.7743 0.8429 0.8679 0.7998 0.8143 0.6957 0.8208 0.8995
F 0.7886 0.8112 0.8246 0.8563 0.7410 0.7897 0.6957 0.8039 0.8943
Average P 0.6506 0.7309 0.6930 0.7797 0.6015 0.7239 0.6226 0.7258 0.7959
R 0.6516 0.7390 0.6745 0.7647 0.5615 0.6924 0.5560 0.7052 0.7907
F 0.6511 0.7349 0.6836 0.7721 0.5808 0.7078 0.5874 0.7154 0.7933
Table 5: Intrinsic evaluation of sentence based labeling
with the features from text and the position). The
results include those based on CRF trained on the
text of either the sentence or the paragraph taking
into account the labeling sequence.
CRF has the best performance in both tables,
with the differences being more dramatic on the
paragraph results. These results are comparable
to (Hirohata et al, 2008), even though we are
working with a different set of labels. Compar-
ing the remaining learning algorithms, LR per-
forms better than the other classifiers. Both Ad-
aBoostM1 and SVM perform not as well as NB
and LR; this could be due to the noise referred
to by (Agarwal and Yu, 2009) that appears in the
structured abstract sentences. Considering either
the paragraph or the sentence text, the position in-
formation helps improve their performance.
CONCLUSIONS, METHODS and RESULTS
labels have the best performance, which matches
the most frequent labels in the dataset (see Ta-
ble 2). BACKGROUND and OBJECTIVE have
worse performance compared to the other labels.
These two labels have the largest imbalance com-
pared to the other labels, which seems to nega-
tively impact the classifiers performance.
The results based on the paragraphs outperform
the ones based on the sentences. Argumentative
structure of the paragraphs seems to be easier,
probably due to the fact that individual sentences
have been shown to be noisy (Agarwal and Yu,
2009), and this could explain this behaviour.
107
4.2 Extrinsic evaluation (GeneRIFs)
Extrinsic evaluation is performed on the GeneRIF
data set presented in the Methods section. The
idea of the evaluation is to assign one of the ar-
gumentative labels to the sentences, based on the
models trained on structured abstracts, and eval-
uate the impact of this assignment in the selec-
tion of GeneRIF sentences. From the set of ma-
chine learning algorithms intrinsically evaluated,
we have selected the LR models trained with and
without position information (Pos) and the CRF
model. The LR and CRF models are used to la-
bel the GeneRIF training and testing data with the
argumentative labels.
Table 6 shows the results of the extrinsic evalu-
ation. Results obtained with the argumentative la-
bel feature and with or without the set of features
used in the baseline are compared to the baseline
model, i.e. NB and the set of features presented
in the Methods section. In all the cases, precision
(P), recall (R) and F1 using the argumentative fea-
tures improve over the baseline.
The intrinsic evaluation was performed either
on sentences or paragraphs. The sentence mod-
els perform better than the paragraph based mod-
els. We find as well that LR with sentence position
performs slightly better than when combined with
the baseline features, with higher recall but lower
precision. Contrary to the intrinsic results, LR per-
forms better than CRF, even though both outper-
form the baseline. This means that non-structured
sentences do not necessarily follow the same argu-
mentative structure as the structured abstracts.
Label P R F
Baseline 0.6210 0.6605 0.6405
LR Par 0.7235 0.6767 0.6993
LR Par + Base 0.7184 0.8014 0.7576
LR Par Pos 0.5978 0.8891 0.7149
LR Par Pos + Base 0.6883 0.8060 0.7426
LR Sen 0.7039 0.7852 0.7424
LR Sen + Base 0.7325 0.7968 0.7633
LR Sen Pos 0.7014 0.9007 0.7887
LR Sen Pos + Base 0.7222 0.8406 0.7769
CRF Par 0.6682 0.6744 0.6713
CRF Par + Base 0.7036 0.8060 0.7513
CRF Sen 0.6536 0.8499 0.7390
CRF Sen + Base 0.7134 0.7875 0.7486
Table 6: GeneRIF extrinsic evaluation
5 Discussion
Results show that it is possible to automatically
predict the argumentative label of the structured
abstracts and to improve the performance for
GeneRIF annotation. Intrinsic evaluation shows
that paragraph labeling is easier compared to sen-
tence labeling, which might be partly due to the
noise in the sentences as identified by (Agarwal
and Yu, 2009). The excellent performance for
paragraph labeling was already shown by previous
work (Hirohata et al, 2008) while sentence label-
ing issues for structured abstracts was previously
introduced by (Agarwal and Yu, 2009). In both in-
trinsic tasks, adding the position of the paragraph
or sentence improves the performance of the learn-
ing algorithms.
Extrinsic evaluation shows that, compared to
the baseline features for GeneRIF annotation,
adding argumentative labeling using the trained
models improves its performance, which is close
to the human performance reported in the Meth-
ods section. On the other hand, we find that the
CRF models show lower performance compared
to the LR models. From the LR models, the po-
sition of the sentence or paragraph seems to have
better performance.
In addition, the LR model trained on the sen-
tences performs better compared to the model
trained on the paragraphs. This might be partly
due to the fact that sentence based models seem
to be better suited than the paragraph based ones
as might have been expected. The fact that the
CRF models performance is below the LR mod-
els denotes that the structured abstracts seem to
follow a pattern that is different in the case of
non-structured abstracts. Looking closer at the
assigned labels, the LR models tend to assign
more CONCLUSIONS and RESULTS labels to
the GeneRIF sentences compared to the CRF ones.
6 Conclusions and Future Work
We have presented an evaluation of several learn-
ing algorithms to label abstract text in MED-
LINE/PubMed with argumentative labels, based
on MEDLINE/PubMed structured abstracts. The
results show that this task can be achieved with
high performance in the case of labeling the para-
graphs but this is not the same in the case of sen-
tences. This intrinsic evaluation was performed on
structured abstracts, and in this set the CRF mod-
els seem to perform much better compared to the
108
other models that do not use the labeling sequence.
On the other hand, when applying the trained
models to MEDLINE/PubMed non-structured ab-
stracts, we find that the extrinsic evaluation of
these labeling on the GeneRIF task shows lower
performance for the CRF models. This indicates
that the structured abstracts follow a pattern that
non-structured ones do not follow. The extrin-
sic evaluation shows that labeling the sentences
with argumentative labels improves the indexing
of GeneRIF sentences. The argumentative labels
help identifying target sentences for the GeneRIF
indexing, but more refined labels learned from
non-structured abstracts could provide better per-
formance. An idea to extend this research would
be evaluating the latent discovery of section labels
and to apply this labeling to the proposed GeneRIF
task and to other tasks, e.g. MeSH indexing. La-
tent labels might accommodate better the argu-
mentative structure of non-structured abstracts.
As shown in this work, the argumentative lay-
out of non-structured abstracts and structured ab-
stracts is not the same. There is still the open ques-
tion if there is any layout regularity in the non-
structured abstracts that could be exploited to im-
prove information access.
7 Acknowledgements
NICTA is funded by the Australian Government
as represented by the Department of Broadband,
Communications and the Digital Economy and
the Australian Research Council through the ICT
Centre of Excellence program.
This work was also supported in part by the In-
tramural Research Program of the NIH, National
Library of Medicine.
References
S Agarwal and H Yu. 2009. Automatically classifying
sentences in full-text biomedical articles into Intro-
duction, Methods, Results and Discussion. Bioin-
formatics, 25(23):3174?3180.
A R Aronson and F M Lang. 2010. An overview
of MetaMap: historical perspective and recent ad-
vances. Journal of the American Medical Informat-
ics Association, 17(3):229?236.
G. Bhalotia, PI Nakov, A S Schwartz, and M A Hearst.
2003. BioText team report for the TREC 2003 ge-
nomics track. In Proceedings of TREC. Citeseer.
J Gobeill, I Tbahriti, F Ehrler, A Mottaz, A Veuthey,
and P Ruch. 2008. Gene Ontology density estima-
tion and discourse analysis for automatic GeneRiF
extraction. BMC Bioinformatics, 9(Suppl 3):S9.
W Hersh and R T Bhupatiraju. 2003. TREC genomics
track overview. In TREC 2003, pages 14?23.
K Hirohata, Naoaki Okazaki, Sophia Ananiadou, Mit-
suru Ishizuka, and Manchester Interdisciplinary
Biocentre. 2008. Identifying sections in scientific
abstracts using conditional random fields. In Proc.
of 3rd International Joint Conference on Natural
Language Processing, pages 381?388.
R Jelier, M Schuemie, C Eijk, M Weeber, E Mulligen,
B Schijvenaars, B Mons, and J Kors. 2003. Search-
ing for GeneRIFs: concept-based query expansion
and Bayes classification. In Proceedings of TREC,
pages 167?174.
A Jimeno-Yepes, J G Mork, D Demner-Fushman, and
A R Aronson. 2012. A One-Size-Fits-All Indexing
Method Does Not Exist: Automatic Selection Based
on Meta-Learning. Journal of Computing Science
and Engineering, 6(2):151?160.
A Jimeno-Yepes, J C Sticco, J G Mork, and A R Aron-
son. 2013. GeneRIF indexing: sentence selection
based on machine learning. BMC Bioinformatics,
14(1):147.
S Jonnalagadda, G D Fiol, R Medlin, C Weir, M Fisz-
man, J Mostafa, and H Liu. 2012. Automatically
extracting sentences from medline citations to sup-
port clinicians? information needs. In Healthcare
Informatics, Imaging and Systems Biology (HISB),
2012 IEEE Second International Conference on,
pages 72?72. IEEE.
A Jordan. 2002. On discriminative vs. generative
classifiers: A comparison of logistic regression and
naive bayes. Advances in neural information pro-
cessing systems, 14:841.
J D Lafferty, A McCallum, and F Pereira. 2001. Con-
ditional random fields: Probabilistic models for seg-
menting and labeling sequence data. In Proceed-
ings of the Eighteenth International Conference on
Machine Learning, ICML ?01, pages 282?289, San
Francisco, CA, USA. Morgan Kaufmann Publishers
Inc.
J Lin, D Karakos, D Demner-Fushman, and S Khudan-
pur. 2006. Generative content models for structural
analysis of medical abstracts. In Proceedings of the
HLT-NAACL BioNLP Workshop on Linking Natural
Language and Biology, pages 65?72. Association
for Computational Linguistics.
Z Lu, K B Cohen, and L Hunter. 2006. Finding
GeneRIFs via gene ontology annotations. In Pacific
Symposium on Biocomputing. Pacific Symposium on
Biocomputing, page 52. NIH Public Access.
Z Lu, K B Cohen, and L Hunter. 2007. GeneRIF qual-
ity assurance as summary revision. In Pacific Sym-
posium on Biocomputing, page 269. NIH Public Ac-
cess.
109
A McCallum. 2002. Mallet: A machine learning for
language toolkit. URL http://mallet.cs.umass.edu.
L McKnight and P Srinivasan. 2003. Categorization
of sentence types in medical abstracts. In AMIA An-
nual Symposium Proceedings, volume 2003, page
440. American Medical Informatics Association.
Y Mizuta, A Korhonen, T Mullen, and N Collier. 2006.
Zone analysis in biology articles as a basis for infor-
mation extraction. International journal of medical
informatics, 75(6):468?487.
A M Ripple, J G Mork, L S Knecht, and B L
Humphreys. 2011. A retrospective cohort study
of structured abstracts in MEDLINE, 1992?2006.
Journal of the Medical Library Association: JMLA,
99(2):160.
A M Ripple, J G Mork, J M Rozier, and L S Knecht.
2012. Structured Abstracts in MEDLINE: Twenty-
Five Years Later.
P Ruch, C Chichester, G Cohen, G Coray, F Ehrler,
H Ghorbel, and V Mu?ller, Hand Pallotta. 2003. Re-
port on the TREC 2003 experiment: Genomic track.
TREC-03.
P Ruch, A Geissbuhler, J Gobeill, F Lisacek, I Tbahriti,
A Veuthey, and A R Aronson. 2007. Using dis-
course analysis to improve text categorization in
MEDLINE. Studies in health technology and infor-
matics, 129(1):710.
I Tbahriti, C Chichester, F Lisacek, and P Ruch. 2005.
Using argumentation to retrieve articles with similar
citations: An inquiry into improving related articles
search in the MEDLINE digital library. In Interna-
tional Journal of Medical Informatics. Citeseer.
G Tsoumakas and I Katakis. 2007. Multi-label clas-
sification: An overview. International Journal of
Data Warehousing and Mining (IJDWM), 3(3):1?13.
L Yeganova, Donald C Comeau, W Kim, and J Wilbur.
2011. Text mining techniques for leveraging posi-
tively labeled data. In Proceedings of BioNLP 2011
Workshop, pages 155?163. Association for Compu-
tational Linguistics.
T Zhang. 2004. Solving large scale linear predic-
tion problems using stochastic gradient descent al-
gorithms. In Proceedings of the twenty-first inter-
national conference on Machine learning, page 116.
ACM.
110
Proceedings of the 2014 Workshop on Biomedical Natural Language Processing (BioNLP 2014), pages 93?97,
Baltimore, Maryland USA, June 26-27 2014.
c?2014 Association for Computational Linguistics
A repository of semantic types in the MIMIC II database clinical notes
Richard M. Osborne
Computational Bioscience
University of Colorado
School of Medicine
richard.osborne@ucdenver.edu
Alan R. Aronson
National Library of Medicine
Bethesda, MD
alan@nlm.nih.gov
K. Bretonnel Cohen
Computational Bioscience
University of Colorado
School of Medicine
kevin.cohen@gmail.com
Abstract
The MIMIC II database contains
1,237,686 clinical documents of vari-
ous kinds. A common task for researchers
working with this database is to run
MetaMap, which uses the UMLS
Metathesaurus, on those documents
to identify specific semantic types of
entities mentioned in them. However,
this task is computationally expensive
and time-consuming. Research in many
groups could be accelerated if there were
a community-accessible set of outputs
from running MetaMap on this document
collection, cached and available on the
MIMIC-II website. This paper describes a
repository of all MetaMap output from the
MIMIC II database, publicly available,
assuming compliance with usage agree-
ments required by UMLS and MIMIC-II.
Additionally, software for manipulating
MetaMap output, available on Source-
Forge with a liberal Open Source license,
is described.
1 Introduction
1.1 The MIMIC II database and its textual
contents
The Multiparameter Intelligent Monitoring in In-
tensive Care II (MIMIC-II) database is a public-
access intensive care unit database that contains
a broad array of information for over 33,000 pa-
tients. The data were collected over a 7 year pe-
riod, beginning in 2001 from Boston?s Beth Is-
rael Deaconess Medical Center (Saeed et al, 2011;
Goldberger et al., 2000).
Of particular interest are the 1,237,686 clini-
cal documents, which are broadly classified into
the following four groups: MD notes, discharge
summaries, radiology reports and nursing/other.
Each free-text note contains information describ-
ing such things as a given patient?s health, ill-
nesses, treatments and medications, among others.
1.2 Motivation for the resource: MetaMap
runtimes
Part of the motivation for making this resource
publicly available is that considerable resources
must be expended to process it; if multiple groups
can share the output of one processing run, the sav-
ings across the community as a whole could be
quite large. To illustrate why this would be valu-
able from a resources perspective, we provide here
some statistics on the performance of MetaMap.
Random samples of each category (10% each)
were chosen and Monte Carlo simulation was per-
formed (1,000 iterations per note) to obtain the
running times presented below. The clinical notes
ranged from a minimum of 0 words to a maximum
of 6,684 (some of the notes were 0 bytes because
the note for a particular patient and day contained
no text). The mean, median and mode per doc-
ument processed by MetaMap were 17, 5 and 2
seconds, respectively, with a minimum of 1 and a
maximum of 216 seconds.
Figure 1 below plots the number of words
against processing times in seconds for each of the
of notes, sampled as mentioned above.
The majority of the processing was done on a
Sun Fire X4600M2 server with 16 (4 x Quad-
Core AMD Opteron(tm) Processor 8356 cores,
2.3GHz), 128GB memory and 12 TB of disk stor-
age, currently running Fedora Core 17 Linux.
(An Apple MacBook Pro and a Windows desktop
server were also used to speed processing. The
analysis of the random sample of notes was per-
formed in its entirety on the Sun machine, thereby
providing consistent results for the data in Figure
1.)
93
Figure 1: MetaMap Runtimes
1.3 Motivation for the resource:
reproducibility
Any large-scale run of MetaMap over a huge doc-
ument collection will have occasional failures, etc.
The odds of any two runs having the same out-
put are therefore slim. Moreover, there is po-
tential variability in how documents are prepro-
cessed for use with MetaMap. Using this reposi-
tory of MetaMap outputs will ensure reproducibil-
ity of experiments and also preclude the neces-
sity of performing the same preparatory work and
MetaMap processing on the same data.
1.4 Motivation for the resource: semantic
types
The creation of the MIMIC-II repository is an in-
termediate step in our research. We are extracting
the semantic types found in each clinical note in
an attempt to determine if there exists evidence of
subdomains across the categories used by MIMIC-
II to group the notes.
2 Materials and Methods
2.1 Materials
MetaMap is a program developed at the National
Library of Medicine (NLM) that maps biomedical
concepts to the UMLS Metathesaurus and reports
on the corresponding semantic types.
1
The pro-
gram is used extensively by researchers in the field
of biomedical text mining. See Aronson, 2001;
Aronson and Lang, 2010.
1
Users of MetaMap must comply with the UMLS
Metathesaurus license agreement (https://uts.nlm.
nih.gov/license.html).
Although our focus is on the clinical notes con-
tained in a single table, noteevents, MIMIC-II is
both a relational database (PostgreSQL 9.1.9) con-
taining 39 tables of clinical data and bedside mon-
itor waveforms and the associated derived param-
eters and events stored in flat binary files (with
ASCII header descriptors). For each Intensive
Care Unit (ICU) patient, Saeed et al. (2011) col-
lected a wide range of data including inter alia
laboratory data, therapeutic intervention profiles,
MD and nursing progress notes, discharge sum-
maries, radiology reports, International Classifica-
tion of Diseases, 9th Revision codes, and, for a
subset of patients, high-resolution vital sign trends
and waveforms. All data were scrubbed for per-
sonal information to ensure compliance with the
Health Insurance Portability and Accountability
Act (HIPAA). These data were then uploaded to a
relational database thereby allowing for easy ac-
cess to extensive information for each patient?s
stay in the ICU (Saeed et al.). A more detailed
description of the use of the MIMIC-II database
may be found in(Clifford et al. , 2012).
The abbreviated schema in Table 1 below shows
that each ID uniquely identifies a note along with a
SubjectID, a Category and Text.
2
We added the ID
attribute to noteevents as a primary key because
SubjectID, Category and Text are not keys. Thus,
a particular patient might have many notes and
many categories but each note is uniquely iden-
tified.
Attribute Type Cardinality Sample Values
ID integer unique 1, 2, 3, 4...
SubjectID integer many to one ID 95, 100, 100, 99,
Category character varying(26) many to one ID radiology
Text text many to one ID interval placement of ICD
Table 1: Schema MIMIC-II noteevents table.
As mentioned above, the notes in the MIMIC-II
database are categorized as MD reports, radiology
reports, discharge summaries and nursing/other
reports. The contents of these notes varied greatly.
The MD and nursing notes tended to be short and
unstructured with a number of abbreviations and
misspellings, whereas the radiology reports were
longer, more structured and showed fewer errors.
The MIMIC-II version used for this research is
2.6 (April 2011; 32,536 subjects).
The distribution of reports with summary statis-
tics is below in Table 2.
2
The full noteevents table has ten other attributes such as
admission date, various timestamps and patient information
but these were not relevant to our research.
94
MD Discharge Radiology Nursing Totals
min words 0 0 0 0 0
max words 632 6684 2760 632 6684
median words 108 963.5 174 108 135
average words 131.6 1009.2 265.5 131.6 194.7
total notes 23,270 31,877 383,701 798,838 1,237,686
Table 2: MIMIC-II Clinical Note Summary Stats.
2.2 Methods
We used MetaMap to process the clinical notes in
order to find semantic concepts, the latter of which
are being used in our current research. For the
work in this paper, we used MetaMap 2013 with
the 2013AB database.
Before processing the notes with MetaMap,
a number of preparatory steps were taken. As
mentioned above, a primary key was added to
the noteevents table to provide a unique id for
each note. A Python script then queried the
database extracting each note and storing it in a
file named according to the following convention:
uniqueID subjectID category.txt where uniqueID
is the primary key value from the noteevents ta-
ble, subjectID is the unique number assigned to
each patient and category is one of the four cate-
gories mentioned above.
Each of the notes was then processed by a Bash
shell script to remove blank lines and control char-
acters. (This important step was added after a sig-
nificant amount of processing had already taken
place. If this is not done, a number of prob-
lems arise when running MetaMap). Finally, all
files with 0 bytes were removed. These files were
present because many tuples in the noteevents ta-
ble contained clinical note entries with no data.
The number of options available when running
MetaMap is considerable so we chose those that
would provide a full and robust result set which
would be useful to a wide range of researchers. In
our first run, we limited the threshold for the Can-
didate Score to 1,000. However, for the repository,
no threshold was set so that a full range of output
is provided.
3
The output is in XML in order to structure the
data systematically and provide an easier and con-
sistent way to parse the data. Although we chose
XML initially, we intend to provide the same data
in plain text and Prolog formats, again to provide
utility to a broad range of researchers.
In order to process all files, a Bash shell script
3
The exact MetaMap command we used was
metamap13 ?XMLf ?silent ?blanklines 3 filename.txt
was created that called MetaMap on each note and
created a corresponding XML file, named accord-
ing to the same convention as that for notes but
with the txt extension replaced by xml.
3 Results
3.1 The repository of MetaMap output
The repository for the MetaMap output contains
an XML file for each note that originally contained
text in the MIMIC-II database. Each XML file
contains a wealth of information about each note
and a discussion of this is beyond the scope of
this paper (see http://metamap.nlm.nih.
gov/Docs/MM12_XML_Info.shtml).
For our research, we are interested in the se-
mantic types associated with phrases identified by
MetaMap. Below is a section from output file
768591 19458 discharge.xml. This is a discharge
summary for subject 19458 with a unique note id
of 768591. The note contained the phrase ?Admis-
sion Date? which MetaMap matched with a candi-
date score of 1000 and indicated that it is a tempo-
ral concept (tmco).
Ultimately, the MetaMap output files will be up-
loaded to the PhysioNet website and made avail-
able to the public.
4
The files will be organized in
a fashion similar to the original data files on the
site. Namely, data are grouped by subject ids and
compressed in archives with approximately 1000
files each.
<Candidate>
<CandidateScore>-1000</CandidateScore>
<CandidateCUI>C1302393</CandidateCUI>
<CandidateMatched>Admission date
</CandidateMatched>
<CandidatePreferred>Date of admission
</CandidatePreferred>
<MatchedWords Count="2">
<MatchedWord>admission</MatchedWord>
<MatchedWord>date</MatchedWord>
</MatchedWords>
<SemTypes Count="1">
<SemType>tmco</SemType>
</SemTypes>
The original note contained 975 lines, whereas
the MetaMap xml file contained 248,198. Thus
it is obvious that there is a very large amount of
MetaMap output that we don?t consider but which
may be of interest to other researchers.
4
Subject again to the data usage agreement.
95
3.2 A Python module for manipulating
MetaMap output
In order to make information in the XML files ac-
cessible to others, we developed a Python module
(parseMM xml.py) containing a number of meth-
ods or functions that allow one to parse the XML
tree and extract relevant information.
Although we will add more functionality as
needed and requested, at this point the following
methods are implemented:
? parseXMLtree(filename) ? parses the con-
tents of filename and returns a node repre-
senting the top of the document tree.
? getXMLsummary(XMLtree) ? summarizes
the data contained in the parsed XML tree.
The summary contains top-level elements
and their corresponding text. The output is
much like that contained in typical MetaMap
text output.
? getCUIs(XMLtree) ? returns the MetaMap
CUIs found in the XML tree along with the
matching concepts.
? getNegatedConcepts(XMLtree) ? returns
negated concepts and their corresponding
CUIs.
? getSemanticTypes(XMLtree) ? returns
matched concepts, their CUIs, the candidate
scores and the semantic types associated
with the concept.
? findAttribute(attribute) ? searches the docu-
ment tree for an attribute of the user?s choos-
ing. Returns the attributes with their corre-
sponding text values.
We chose Python to create our module be-
cause of its ease of use and its multi-platform
capabilities. Once Python is installed and the
parseMM xml.py is placed in a directory along
with the MetaMap xml file which is to be ana-
lyzed, retrieving relevant information is relatively
straightforward.
5
5
Under most circumstances, Python is already installed
on the Mac OS X and Linux operating systems.
A stylized version of our code is presented be-
low.
# Parse XML tree and return semantic
types.
import parseMM_xml
xml_tree = \
parseXMLtree("noteid_subid_category.xml")
semTypes = getSemanticTypes(xml_tree)
print(semTypes)
A truncated listing of the output:
CandidateCUI ? C0011008
CandidateMatched ? Date
1 ? SemType ? Temporal Concept
CandidateCUI ? C2348077
CandidateMatched ? Date
2 ? SemType ? Food
In order to fully test the robustness of our mod-
ule, we will do further unit and regression testing,
in addition to providing more exception handling.
Ultimately, the code will be available on Source-
Forge, an Open Source web source code repository
available at www.sourceforge.net.
Acknowledgments
We would like to thank George Moody of MIT for
his help with questions concerning the MIMIC-II
database.
References
Alan R. Aronson 2001. Effective mapping of biomed-
ical text to the UMLS Metathesaurus: the MetaMap
program, Proc AMIA Symp. 2001; 17:21.
Alan R. Aronson, Franois-Michel Lang 2010. An
overview of MetaMap: historical perspective and
recent advances, J Am Med Inform Assoc 2010;
17:3 229-236
Gari D. Clifford, Daniel J. Scott, Mauricio Vil-
larroel 2012. User Guide and Documen-
tation for the MIMIC II Database, http:
//mimic.physionet.org/UserGuide/
UserGuide.pdf
Ary L. Goldberger; Luis A. N. Amaral; Leon Glass;
Jeffrey M. Hausdorff; Plamen Ch. Ivanov; Roger G.
Mark; Joseph E. Mietus; George B. Moody; Chung-
Kang Peng; H. Eugene Stanley, 2000 PhysioBank,
PhysioToolkit, and PhysioNet: Components of a
New Research Resource for Complex Physiologic
Signals, Circulation 101(23): e215-e220.
96
Mohammed Saeed, Mauricio Villarroel, Andrew T.
Reisner, Gari Clifford, Li-Wei Lehman, George
Moody, Thomas Heldt, Tin H. Kyaw, Benjamin
Moody, Roger G. Mark. 2011 The Multiparameter
intelligent monitoring in intensive care II (MIMIC-
II): A public-access ICU database, Critical Care
Medicine; 39(5):952-960
MetaMap Release Notes Website 2013. MetaMap
2013 Release Notes http://metamap.nlm.
nih.gov/Docs/MM12_XML_Info.shtml
MetaMap 2012 Output Website 2014.
MetaMap 2012 XML Output Explained
http://metamap.nlm.nih.gov/Docs/
MM12_XML_Info.shtml
PhysioNet Website 2014. PhysioNet MIMIC-II Web-
site http://physionet.org/mimic2/
97
