Proceedings of the Human Language Technology Conference of the North American Chapter of the ACL, pages 231?234,
New York, June 2006. c?2006 Association for Computational Linguistics
Detecting Emotion in Speech: Experiments in Three Domains
Jackson Liscombe
Columbia University
jaxin@cs.columbia.edu
Abstract
The goal of my proposed dissertation work
is to help answer two fundamental questions:
(1) How is emotion communicated in speech?
and (2) Does emotion modeling improve spo-
ken dialogue applications? In this paper I de-
scribe feature extraction and emotion classifi-
cation experiments I have conducted and plan
to conduct on three different domains: EPSaT,
HMIHY, and ITSpoke. In addition, I plan to
implement emotion modeling capabilities into
ITSpoke and evaluate the effectiveness of do-
ing so.
1 Introduction
The focus of my work is the expression of emotion in
human speech. As normally-functioning people, we are
each capable of vocally expressing and aurally recogniz-
ing the emotions of others. How often have you been
put off by the ?tone in someone?s voice? or tickled others
with the humorous telling of a good story? Though we
as everyday people are intimately familiar with emotion,
we as scientists do not actually know precisely how it is
that emotion is conveyed in human speech. This is of spe-
cial concern to us as engineers of natural language tech-
nology; in particular, spoken dialogue systems. Spoken
dialogue systems enable users to interact with computer
systems via natural dialogue, as they would with human
agents. In my view, a current deficiency of state-of-the-
art spoken dialogue systems is that the emotional state of
the user is not modeled. This results in non-human-like
and even inappropriate behavior on the part of the spoken
dialogue system.
There are two central questions I would like to at lest
partially answer with my dissertation research: (1) How
is emotion communicated in speech? and (2) Does emo-
tion modeling improve spoken dialogue applications? In
an attempt to answer the first question, I have adopted
the research paradigm of extracting features that charac-
terize emotional speech and applying machine learning
algorithms to determine the prediction accuracy of each
feature. With regard to the second research question, I
plan to implement an emotion modeler ? one that detects
and responds to uncertainty and frustration ? into an In-
telligent Tutoring System.
2 Completed Work
This section describes my current research on emotion
classification in three domains and forms the foundation
of my dissertation. For each domain, I have adopted an
experimental design wherein each utterance in a corpus
is annotated with one or more emotion labels, features
are extracted from these utterances, and machine learn-
ing experiments are run to determine emotion prediction
accuracy.
2.1 EPSaT
The publicly-available Emotional Prosody Speech and
Transcription corpus1 (EPSaT) comprises recordings of
professional actors reading short (four syllables each)
dates and numbers (e.g., ?two-thousand-four?) with dif-
ferent emotional states. I chose a subset of 44 utterances
from 4 speakers (2 male, 2 female) from this corpus and
conducted a web-based survey to subjectively label each
utterance for each of 10 emotions, divided evenly for va-
lence. These emotions included the positive emotion cat-
egories: condent, encouraging, friendly, happy, inter-
ested; and the negative emotion categories: angry, anx-
ious, bored, frustrated, sad.
Several features were extracted from each utterance in
this corpus, each one designed to capture emotional con-
tent. Global acoustic-prosodic information ? e.g., speak-
ing rate and minimum, maximum, and mean pitch and in-
tensity ? has been well known since the 1960s and 1970s
1LDC Catalog No.: LDC2002S28.
231
to convey emotion to some extent (e.g., (Davitz, 1964;
Scherer et al, 1972)). In addition to these features, I also
included linguistically meaningful prosodic information
in the form of ToBI labels (Beckman et al, 2005), as well
as the spectral tilt of the vowel in each utterance bearing
the nuclear pitch accent.
In order to evaluate the predictive power of each fea-
ture extracted from the EPSaT utterances, I ran machine
learning experiments using RIPPER, a rule-learning al-
gorithm. The EPSaT corpus was divided into training
(90%) and testing (10%) sets. A binary classification
scheme was adopted based on the observed ranking dis-
tributions from the perception survey: ?not at all? was
considered to be the absence of emotion x; all other ranks
was recorded as the presence of emotion x. Performance
accuracy varied with respect to emotion, but on average I
observed 75% prediction accuracy for any given emotion,
representing an average 22% improvement over chance
performance. The most predictive included the global
acoustic-prosodic features, but interesting novel findings
emerged as well; most notably, significant correlation
was observed between negative emotions and pitch con-
tours ending in a plateau boundary tone, whereas positive
emotions correlated with the standard declarative phrasal
ending (in ToBI, these would be labeled as /H-L%/ and
/L-L%/, respectively). Further discussion of such find-
ings can be found in (Liscombe et al, 2003).
2.2 HMIHY
?How May I Help YouSM? (HMIHY) is a natural lan-
guage human-computer spoken dialogue system devel-
oped at AT&T Research Labs. The system enables AT&T
customers to interact verbally with an automated agent
over the phone. Callers can ask for their account bal-
ance, help with AT&T rates and calling plans, explana-
tions of certain bill charges, or identification of num-
bers. Speech data collected from the deployed system
has been assembled into a corpus of human-computer
dialogues. The HMIHY corpus contains 5,690 com-
plete human-computer dialogues that collectively con-
tain 20,013 caller turns. Each caller turn in the corpus
was annotated with one of seven emotional labels: posi-
tive/neutral, somewhat frustrated, very frustrated, some-
what angry, very angry, somewhat other negative2, very
other negative. However, the distribution of the labels
was so skewed (73.1% were labeled as positive/neutral)
that the emotions were collapsed to negative and non-
negative.
In addition to the set of automatic acoustic-prosodic
features found to be useful for emotional classification of
the EPSaT corpus, the features I examined in the HMIHY
corpus were designed to exploit the discourse information
2?Other negative? refers to any emotion that is perceived
negatively but is not anger nor frustration.
available in the domain of spontaneous human-machine
conversation. Transcriptive features ? lexical items, filled
pauses, and non-speech human noises ? we recorded as
features, as too were the dialogue acts of each caller turn.
In addition, I included contextual features that were de-
signed to track the history of the previously mentioned
features over the course of the dialogue. Specifically,
contextual information included the rate of change of the
acoustic-prosodic features of the previous two turns plus
the transcriptive and pragmatic features of the previous
two turns as well.
The corpus was divided into training (75%) and testing
(25%) sets. The machine learning algorithm employed
was BOOSTEXTER, an algorithm that forms a hypothesis
by combining the results of several iterations of weak-
learner decisions. Classification accuracy using the auto-
matic acoustic-prosodic features was recorded to be ap-
proximately 75%. The majority class baseline (always
guessing non-negative) was 73%. By adding the other
feature-sets one by one, prediction accuracy was itera-
tively improved, as described more fully in (Liscombe et
al., 2005b). Using all the features combined ? acoustic-
prosodic, lexical, pragmatic, and contextual ? the result-
ing classification accuracy was 79%, a healthy 8% im-
provement over baseline performance and a 5% improve-
ment over the automatic acoustic-prosodic features alone.
2.3 ITSpoke
This section describes more recent research I have been
conducting with the University of Pittsburgh?s Intelli-
gent Tutoring Spoken Dialogue System (ITSpoke) (Lit-
man and Silliman, 2004). The goal of this research is to
wed spoken language technology with instructional tech-
nology in order to promote learning gains by enhanc-
ing communication richness. ITSpoke is built upon the
Why2-Atlas tutoring back-end (VanLehn et al, 2002), a
text-based Intelligent Tutoring System designed to tutor
students in the domain of qualitative physics using natural
language interaction. Several corpora have been recorded
for development of ITSpoke, though most of the work
presented here involves tutorial data between a student
and human tutor. To date, we have labeled the human-
human corpus for anger, frustration, and uncertainty.
As this work is an extension of previous work, I chose
to extract most of the same features I had extracted from
the EPSaT and HMIHY corpora. Specifically, I extracted
the same set of automatic acoustic-prosodic features, as
well as contextual features measuring the rate of change
of acoustic-prosodic features of past student turns. A
new feature set was introduced as well, which I refer
to as the breath-group feature set, and which is an auto-
matic method for segmenting utterances into intonation-
ally meaningful units by identifying pauses using back-
ground noise estimation. The breath group feature set
232
comprises the number of breath-groups in each turn, the
pause time, and global acoustic-prosodic features calcu-
lated for the first, last, and longest breath-group in each
student turn.
I used the WEKA machine learning software package
to classify whether a student answer was perceived to be
uncertain, certain, or neutral3 in the ITSpoke human-
human corpus. As a predictor, C4.5, a decision-tree
learner, was boosted with AdaBoost, a learning strategy
similar to the one presented in Section 2.2. The data
were randomly split into a training set (90%) and a test-
ing set (10%). The automatic acoustic-prosodic features
performed at 75% accuracy, a relative improvement of
13% over the baseline performance of always guessing
neutral. By adding additional feature-sets ? contextual
and breath-group information ? I observed an improved
prediction accuracy of 77%. Thus indicating that breath-
group features are useful. I refer the reader to (Liscombe
et al, 2005a) for in-depth implications and further analy-
sis of these results. In the immediate future, I will extract
features previously mentioned in Section 2.2 as well as
the exploratory features I will discuss in the following
section.
3 Work-in-progress
In this section I describe research I have begun to con-
duct and plan to complete in the coming year, as agreed-
upon in February, 2006 by my dissertation committee. I
will explore features that are not well studied in emotion
classification research, primarily pitch contour and voice
quality approximation. Furthermore, I will outline how I
plan to implement and evaluate an emotion detection and
response module into ITSpoke.
3.1 Pitch Contour Clustering
The global acoustic-prosodic features used in most emo-
tion prediction studies capture meaningful prosodic vari-
ation, but are not capable of describing the linguisti-
cally meaningful intonational behavior of an utterance.
Though phonological labeling methods exist, such as
ToBI, annotation of this sort is time-consuming and must
be done manually. Instead, I propose an automatic al-
gorithm that directly compares pitch contours and then
groups them into classes based on abstract form. Specif-
ically, I intend to use partition clustering to define a
disjoint set of similar prosodic contour types over our
data. I hypothesize that the resultant clusters will be the-
oretically meaningful and useful for emotion modeling.
The similarity metric used to compare two contours will
be edit distance, calculated using dynamic time warping
techniques. Essentially, the algorithm finds the best fit
between two contours by stretching and shrinking each
3With respect to certainness.
contour as necessary. The score of a comparison is calcu-
lated as the sum of the normalized real-valued distances
between mapped points in the contours.
3.2 Voice Quality
Voice quality is a term used to describe a perceptual col-
oring of the acoustic speech signal and is generally be-
lieved to play an important role in the vocal communica-
tion of emotion. However, it has rarely been used in au-
tomatic classification experiments because the exact pa-
rameters defining each quality of voice (e.g., creaky and
breathy) are still largely unknown. Yet, some researchers
believe much of what constitutes voice quality can be
described using information about glottis excitation pro-
duced by the vocal folds, most commonly referred to
as the glottal pulse waveform. While there are ways of
directly measuring the glottal pulse waveform, such as
with an electroglottograph, these techniques are too inva-
sive for practical purposes. Therefore, the glottal pulse
waveform is usually approximated by inverse filtering of
the speech signal. I will derive glottal pulse waveforms
from the data using an algorithm that automatically iden-
tifies voiced regions of speech, obtains an estimate of the
glottal flow derivative, and then represents this using the
Liljencrants-Fant parametric model. The final result is a
glottal pulse waveform, from which features can be ex-
tracted that describe the shape of this waveform, such as
the Open and Skewing Quotients.
3.3 Implementation
The motivating force behind much of the research I have
presented herein is the common assumption in the re-
search community that emotion modeling will improve
spoken dialogue systems. However, there is little to no
empirical proof testing this claim (See (Pon-Barry et al,
In publication) for a notable exception.). For this rea-
son, I will implement functionality for detecting and re-
sponding to student emotion in ITSpoke (the Intelligent
Tutoring System described in Section 2.3) and analyze
the effect it has on student behavior, hopefully showing
(quantitatively) that doing so improves the system?s ef-
fectiveness.
Research has shown that frustrated students learn less
than non-frustrated students (Lewis and Williams, 1989)
and that human tutors respond differently in the face of
student uncertainty than they do when presented with cer-
tainty (Forbes-Riley and Litman, 2005). These findings
indicate that emotion plays an important role in Intelli-
gent Tutoring Systems. Though I do not have the ability
to alter the discourse-flow of ITSpoke, I will insert active
listening prompts on the part of ITSpoke when the sys-
tem has detected either frustration or uncertainty. Active
listening is a technique that has been shown to diffuse
negative emotion in general (Klein et al, 2002). I hy-
233
pothesize that diffusing user frustration and uncertainty
will improve ITSpoke.
After collecting data from an emotion-enabled IT-
Spoke I will compare evaluation metrics with those of
a control study conducted with the original ITSpoke sys-
tem. One such metric will be learning gain, the differ-
ence between student pre- and post-test scores and the
standard metric for quantifying the effectiveness of edu-
cational devices. Since learning gain is a crude measure
of academic achievement and may overlook behavioral
and cognitive improvements, I will explore other metrics
as well, such as: the amount of time taken for the stu-
dent to produce a correct answer, the amount of negative
emotional states expressed, the quality and correctness of
answers, the willingness to continue, and subjective post-
tutoring assessments.
4 Contributions
I see the contributions of my dissertation to be the extent
to which I have helped to answer the questions I posed at
the outset of this paper.
4.1 How is emotion communicated in speech?
The experimental design of extracting features from spo-
ken utterances and conducting machine learning experi-
ments to predict emotion classes identifies features im-
portant for the vocal communication of emotion. Most of
the features I have described here are well established in
the research community; statistic measurements of fun-
damental frequency and energy, for example. However, I
have also described more experimental features as a way
of improving upon the state-of-the-art in emotion mod-
eling. These exploratory features include breath-group
segmentation, contextual information, pitch contour clus-
tering, and voice quality estimation. In addition, explor-
ing three domains will allow me to comparatively ana-
lyze the results, with the ultimate goal of identifying uni-
versal qualities of spoken emotions as well as those that
may particular to specific domains. The findings of such
a comparative analysis will be of practical benefit to fu-
ture system builders and to those attempting to define a
universal model of human emotion alike.
4.2 Does emotion modeling help?
By collecting data of students interacting with an
emotion-enabled ITSpoke, I will be able to report quan-
titatively the results of emotion modeling in a spoken di-
alogue system. Though this is the central motivation for
most researchers in this field, there is currently no defini-
tive evidence either supporting or refuting this claim.
References
M. E. Beckman, J. Hirschberg, and S. Shattuck-Hufnagel,
2005. Prosodic Typology ? The Phonology of Intona-
tion and Phrasing, chapter 2 The original ToBI sys-
tem and the evolution of the ToBI framework. Oxford,
OUP.
J. R. Davitz, 1964. The Communication of Emotional
Meaning, chapter 8 Auditory Correlates of Vocal Ex-
pression of Emotional Feeling, pages 101?112. New
York: McGraw-Hill.
Kate Forbes-Riley and Diane J. Litman. 2005. Using
bigrams to identify relationships between student cer-
tainness states and tutor responses in a spoken dialogue
corpus. In Proceedings of 6th SIGdial Workshop on
Discourse and Dialogue,, Lisbon, Portugal.
J. Klein, Y. Moon, and R. W. Picard. 2002. This com-
puter responds to user frustration: Theory, design, and
results. Interacting with Computers, 14(2):119?140,
February.
V. E. Lewis and R. N. Williams. 1989. Mood-congruent
vs. mood-state-dependent learning: Implications for a
view of emotion. D. Kuiken (Ed.), Mood and Mem-
ory: Theory, Research, and Applications, Special Is-
sue of the Journal of Social Behavior and Personality,
4(2):157?171.
Jackson Liscombe, Jennifer Venditti, and Julia
Hirschberg. 2003. Classifying subject ratings
of emotional speech using acoustic features. In
Proceedings of Eurospeech, Geneva, Switzerland.
Jackson Liscombe, Julia Hirschberg, and Jennifer Ven-
ditti. 2005a. Detecting certainness in spoken tutorial
dialogues. In Proceedings of Interspeech, Lisbon, Por-
tugal.
Jackson Liscombe, Guiseppe Riccardi, and Dilek
Hakkani-Tu?r. 2005b. Using context to improve emo-
tion detection in spoken dialogue systems. In Proceed-
ings of Interspeech, Lisbon, Portugal.
Diane Litman and Scott Silliman. 2004. Itspoke: An in-
telligent tutoring spoken dialogue system. In Proceed-
ings of the 4th Meeting of HLT/NAACL (Companion
Proceedings), Boston, MA, May.
Heather Pon-Barry, Karl Schultz, Elizabeth Owen Bratt,
Brady Clark, and Stanley Peters. In publication. Re-
sponding to student uncertainty in spoken tutorial dia-
logue systems. International Journal of Articial In-
telligence in Education (IJAIED).
K. R. Scherer, J. Koivumaki, and R. Rosenthal. 1972.
Minimal cues in the vocal communication of affect:
Judging emotions from content-masked speech. Jour-
nal of Psycholinguistic Research, 1:269?285.
K. VanLehn, P. Jordan, and C. P. Rose. 2002. The archi-
tecture of why2-atlas: A coach for qualitative physics
essay writing. In Proceedings of the Intelligent Tutor-
ing Systems Conference, Biarritz, France.
234
Proceedings of SIGDIAL 2009: the 10th Annual Meeting of the Special Interest Group in Discourse and Dialogue, pages 128?131,
Queen Mary University of London, September 2009. c?2009 Association for Computational Linguistics
On NoMatchs, NoInputs and BargeIns:
Do Non-Acoustic Features Support Anger Detection?
Alexander Schmitt, Tobias Heinroth
Dialogue Systems Research Group
Institute for Information Technology
Ulm University, Germany
alexander.schmitt@uni-ulm.de
tobias.heinroth@uni-ulm.de
Jackson Liscombe
SpeechCycle, Inc.
Broadway 26
New York City, USA
jackson@speechcycle.com
Abstract
Most studies on speech-based emotion
recognition are based on prosodic and
acoustic features, only employing artifi-
cial acted corpora where the results cannot
be generalized to telephone-based speech
applications. In contrast, we present an
approach based on utterances from 1,911
calls from a deployed telephone-based
speech application, taking advantage of
additional dialogue features, NLU features
and ASR features that are incorporated
into the emotion recognition process. De-
pending on the task, non-acoustic features
add 2.3% in classification accuracy com-
pared to using only acoustic features.
1 Introduction
Certainly, the most relevant employment of
speech-based emotion recognition is that of a
telephone-based Interactive Voice Response Sys-
tem (IVR).
Emotion recognition for IVR differs insofar
to ?traditional? emotion recognition, that it can
be reduced to a binary classification problem,
namely the distinction between angry and non-
angry whereas studies on speech-based emotion
recognition analyze complete and relatively long
sentences covering the full bandwidth of human
emotions. In a way, emotion recognition in the
telephone domain is less challenging since a dis-
tinction between two different emotion classes,
angry and non-angry, is sufficient. We don?t have
to expect callers talking to IVRs in a sad, anxious,
happy, disgusted or bored manner. I.e., even if a
caller is happy, the effect on the dialogue will be
the same as if he is neutral. However, there still
remain challenges for the system developer such
as varying speech quality caused by, e.g., vary-
ing distance to the receiver during the call lead-
ing to loudness variations (which emotion recog-
nizers might mistakenly interpret as anger). But
also bandwidth limitation introduced by the tele-
phone channel and a strongly unbalanced distribu-
tion of non-angry and angry utterances with more
than 80% non-angry utterances make a reliable
distinction of the caller emotion difficult. While
hot anger with studio quality conditions can be de-
termined with over 90% (Pittermann et al, 2009)
studies on IVR anger recognition report lower ac-
curacies due to these limitations. However, there
is one advantage of anger recognition in IVR sys-
tems that can be exploited: additional information
is available from the dialogue context, the speech
recognizer and the natural language parser.
This contribution is organized as follows: first,
we introduce related work and describe our cor-
pus. In Section 4 we outline our employed features
with emphasis on the non-acoustic ones. Experi-
ments are shown in Section 5 where we analyze
the impact of the newly developed features before
we summarize our work in Section 6.
2 Related Work
Speech-based emotion research regarding tele-
phone applications has been increasingly dis-
cussed in the speech community. While in early
studies acted corpora were used, such as in (Ya-
coub et al, 2003), training and testing data in later
studies has been more and more based on real-
life data, see (Burkhardt et al, 2008),(Burkhardt
et al, 2009). Most studies are limited to acous-
tic/prosodic features that have been extracted out
of the audio data. Linguistic information was ad-
ditionaly exploited in (Lee et al, 2002) resulting in
128
a 45.7% accuracy improvement compared to using
only acoustic features. In (Liscombe et al, 2005)
the lexical and prosodic features were additionaly
enriched with dialogue act features leading to an
increase in accuracy of 2.3%.
3 Corpus Description
For our studies we employed a corpus of 1,911
calls from an automated agent helping to resolve
internet-related problems comprising 22,724 utter-
ances. Three labelers divided the corpus into an-
gry, annoyed and non-angry utterances (Cohen?s
? = 0.70 on whole corpus; L1 vs. L2 ? = 0.8,
L1 vs. L3 ? = 0.71, L2 vs. L3 ? = 0.59). The
reason for choosing three emotion classes instead
of a binary classification lies in the hope to find
clearer patterns for strong anger. A distinction be-
tween non-angry and somewhat annoyed callers
is rather difficult even for humans. The final la-
bel was defined based on majority voting resulting
in 90.2% non-angry, 5.1% garbage, 3.4% annoyed
and 0.7% angry utterances. 0.6% of the samples in
the corpus were sorted out since all three raters had
different opinions. The raters were asked to label
?garbage? when the utterance is incomprehensible
or consists of non-speech events. While the num-
ber of angry and annoyed utterances seems very
low, 429 calls (i.e. 22.4%) contained annoyed or
angry utterances.
4 Features
We created two different feature sets: one based
on typical acoustic/prosodic features and another
one to which we will refer as ?non-acoustic? fea-
tures consisting of features from the Automatich
Speech Recognition (ASR), Natural Language
Understanding (NLU), Dialogue Manager (DM)
and Context features.
4.1 Acoustic Features
The acoustic/prosodic features were extracted
with the aid of Praat (Boersma, 2001) and con-
sist of power, mean, rms, mean harmonicity, pitch
(mean, deviation, voiced frames, time step, mean
slope, minimum, maximum, range), voiced pitch
(mean, minimum mean, maximum mean, range),
intensity (mean, maximum, minimum, deviation,
range), jitter points, formants 1-5, MFCC 1-12.
The extraction was performed on the complete
short utterance.
4.2 Non-Acoustic Features
The second, i.e. non-acoustic, feature set is based
on features logged with the aid of the speech plat-
form hosting the IVR application and is presented
here in more detail. They include:
ASR features: raw ASR transcription of
caller?s utterance (Utterance) (unigram bag-of-
words); ASR confidence of returned utterance
transcription, as floating point number between 0
(least confident) and 1 (most confident) (Confi-
dence); names of all grammars active (Grammar-
Name); name of the grammar that returned the
parse (TriggeredGrammarName); did the caller
begin speaking before the prompt completed?
(?yes?, ?no?) (BargedIn); did the caller communi-
cate with speech (?voice?) or keypad (?dtmf?) (In-
putModeName); was the speech recognizer suc-
cessful (?Complete?) or not and if it was not suc-
cessful, an error message is recorded such as
?NoInput? or ?NoMatch? (RecognitionStatus)
NLU-Features: the semantic parse of the caller
utterance as returned by the activated grammar in
the current dialog module (Interpretation); given
caller speech input, we need to try and recognize
the semantic meaning. The first time we try to do
this, this is indicated with a value of ?Initial?. If
we were not returned a parse then we have to re-
prompt (?Retry1? or ?Timeout1?). Similar for if the
caller asks for help or a repetition of the prompt.
Etc. (LoopName)
DM-Features: the text of what the auto-
mated agent said prior to recording the user input
(PromptName); the number of tries to elicit a de-
sired response. Integer values range from 0 (first
try) to 7 (6th try) (RoleIndex); an activity may re-
quest substantive user input (?Collection?) or con-
firm previous substantive input (?Confirmation?)
(RoleName); within a call each event is sequen-
tially organized by these numbers (SequenceID);
the name of the activity (aka dialog module) that
is active (ActivityName); type of activity. Possible
values are: Question, PlatformValue, Announce-
ment, Wait, Escalate (ActivityType)
Context-Features: We further developed addi-
tional cumulative features based on the previous
ones in order to keep track of the NoMatch, NoIn-
puts and similar parameters serving as an indicator
for the call quality: number of non-empty NLU
parses (CumUserTurns); number of statements
and questions by the system (CumSysTurns); num-
ber of questions (CumSysQuestions); number of
129
help requests by the user (CumHelpReq); num-
ber of operator requests (CumOperatorReq); num-
ber of NoInput events (CumNoInputs); number
of NoMatch events (CumNoMatchs) number of
BargeIns (CumBargeIns).
5 Experiments
In order to prevent an adaption of the anger model
to specific callers we seperated the corpus ran-
domly into 75% training and 25% testing material
and ensured that no speaker contained in training
was used for testing. To exclude that we receive a
good classification result by chance, we performed
50 iterations in each test and calculated the per-
formance?s mean and standard deviation over all
iterations.
Note, that our aim in this study is less finding
an optimum classifer, than finding additional fea-
tures that support the distinction between angry
and non-angry callers. Support Vector Machines
and Artificial Neural Networks are thus not con-
sidered, although the best performances are re-
ported with those learning algorithms. A simi-
lar performance, i.e. only slightly poorer, can be
reached with Rule Learners. They enable a thor-
ough study of the features, leading to the decision
for one or the other class, since they produce a
human readable set of if-then-else rules. Our hy-
potheses on a perfect feature set can thus easily be
confirmed or rejected.
We performed experiments with two differ-
ent classes: ?angry? vs. ?non-angry? and ?an-
gry+annoyed? vs. ?non-angry?. Merging angry
and annoyed utterances aims on finding all callers,
where the customer satisfaction is endangered. In
both tasks, we employ a) only acoustic features
b) only ASR/NLU/DM/Context features and c) a
combination of both feature sets. The number of
utterances used for training and testing is shown in
Table 1.
As result we expect acoustic features to per-
form better than non-acoustic features. Among
the relevant non-acoustic features we assume as
an indicator for angry utterances low ASR confi-
dences and high barge-in rates, which we consider
as signal for the caller?s impatience. All tests have
been performed with the machine learning frame-
work RapidMiner (Mierswa et al, 2006) featuring
all common supervised and unsupervised learning
schemes.
Results are listed in Table 2, including preci-
Test A Test B
angry+
annoyed non-a. angry non-a.
Training ? 320 ? 320 ? 80 ? 80
Testing ? 140 ? 140 ? 40 ? 40
Table 1: Number of utterances employed for both
tests per iteration. Since the samples are selected
randomly and the corpus was separated by speak-
ers before training and testing, the numbers may
vary in each iteration.
sion and recall values. As expected, Test B (an-
gry vs. non-angry) has the highest accuracy with
87.23% since the patterns are more clearly sep-
arable compared to Test A (annoyed vs. non-
angry, 72.57%). Obviously, adding non-acoustic
features increases classification accuracy signifi-
cantly, but only where the acoustic features are
not expressive enough. While the additional in-
formation increases the accuracy of the combined
angry+annoyed task by 2.3 % (Test A), it does
not advance the distinction between only angry vs.
non-angry (Test B).
5.1 Emotional History
One could expect, that the probability of
an angry/annoyed turn following another an-
gry/annoyed turn is rather high and that this in-
formation could be exploited. Thus, we further
included two features PrevEmotion and PrevPre-
vEmotion, taking into account the two previous
hand-labeled emotions in the dialogue discourse.
If they would contribute to the recognition pro-
cess, we would replace them by automatically la-
belled ones. All test results, however, did not im-
prove.
5.2 Ruleset Analysis
For a determination of the relevant features in the
non-acoustic feature set, we analyzed the ruleset
generated by the RuleLearner in Test A. Interest-
ingly, a dominant feature in the resulting ruleset is
?AudioDuration?. While shorter utterances were
assigned to non-angry (about <2s), longer utter-
ances tended to be assigned to angry/annoyed. A
following analysis of the utterance length confirms
this rule: utterances labeled as angry averaged
2.07 (+/-0.73) seconds, annoyed utterances lasted
1.82 (+/-0.57) s and non-angry samples were 1.57
(+/- 0.66) s in average. The number of NoMatch
130
Test A: Angry/Annoyed vs. Non-angry only Acoustic only Non-Acoustic both
Accuracy 70.29 (+-2.94) % 61.43 (+-2.75) % 72.57 (+-2.37) %
Precision/Recall Class ?Ang./Ann.? 71.51% / 61.57% 68.35% / 42.57% 73.67% / 70.14%
Precision/Recall Class ?Non-angry? 69.19% / 73.00% 58.30% / 80.29% 71.57% / 75.00%
Test B: Angry vs. Non-angry only Acoustic only Non-Acoustic both
Accuracy 87.06 (+-3.76) % 64.29 (+-1.32) % 87.23 (+-3.72) %
Precision/Recall Class ?Angry? 87.13% / 86.55% 66.0% / 58.9% 86.88% / 87.11%
Precision/Recall Class ?Non-angry? 86.97% / 87.53% 62.9% 69.9% 87.55% / 87.33%
Table 2: Classification results for angry+annoyed vs. non-angry and angry vs. non-angry utterances.
events (CumNoMatch) up to the angry turn played
a less dominant role than expected: only 8 samples
were assigned to angry/annoyed due to reoccur-
ring NoMatch events (>5 NoMatchs). Utterances
that contained ?Operator?, ?Agent? or ?Help? were,
as expected, assigned to angry/annoyed, however,
in combination with high AudioDuration values
(>2s). Non-angry utterances were typically better
recognized: average ASR confidence values are
0.82 (+/-0.288) (non-angry), 0.71 (+/- 0.36) (an-
noyed) and 0.56 (+/- 0.41) (angry).
6 Conclusion and Discussion
In IVR systems, we can take advantage of non-
acoustic information, that comes from the dia-
logue context. As demonstrated in this work,
ASR, NLU, DM and contextual features sup-
port the distinction between angry and non-angry
callers. However, where the samples can be sepa-
rated into clear patterns, such as in Test B, no ben-
efit from the additional feature set can be expected.
In what sense a late fusion of linguistic, dialogue
and context features would improve the classifier,
i.e. by building various subsystems whose opin-
ions are subject to a voting mechanism, will be
evaluated in future work. We will also analyze
why the linguistic features did not have any vis-
ible impact on the classifier. Presumably a combi-
nation of n-grams, bag-of-words and bag of emo-
tional salience will improve classification.
7 Acknowledgements
We would like to take the opportunity to thank the
following colleagues for contributing to the devel-
opment of our emotion recognizer: Ulrich Tschaf-
fon, Shu Ding and Alexey Indiryakov.
References
Paul Boersma. 2001. Praat, a System for Do-
ing Phonetics by Computer. Glot International,
5(9/10):341?345.
Felix Burkhardt, Richard Huber, and Joachim
Stegmann. 2008. Advances in anger detection with
real life data.
Felix Burkhardt, Tim Polzehl, Joachim Stegmann, Flo-
rian Metze, and Richard Huber. 2009. Detecting
real life anger. In Proc. of ICASSP, April.
Chul Min Lee, Shrikanth Narayanan, and Roberto Pier-
accini. 2002. Combining Acoustic and Language
Information for Emotion Recognition. In Interna-
tional Conference on Speech and Language Process-
ing (ICSLP), Denver, USA, October.
Jackson Liscombe, Guiseppe Riccardi, and Dilek
Hakkani-Tu?r. 2005. Using Context to Improve
Emotion Detection in Spoken Dialog Systems. In
International Conference on Speech and Language
Processing (ICSLP), Lisbon, Portugal, September.
Ingo Mierswa, Michael Wurst, Ralf Klinkenberg, Mar-
tin Scholz, and Timm Euler. 2006. Yale: Rapid
prototyping for complex data mining tasks. In KDD
?06: Proceedings of the 12th ACM SIGKDD interna-
tional conference on Knowledge discovery and data
mining, New York, NY, USA, August.
Johannes Pittermann, A. Pittermann, and Wolfgang
Minker. 2009. Handling Emotions in Human-
Computer Dialogues. Text, Speech and Language
Technology. Springer, Dordrecht (The Netherlands).
Sherif Yacoub, Steven Simske, Xiaofan Lin, and John
Burns. 2003. Recognition of emotions in interac-
tive voice response systems. In Proc. Eurospeech,
Geneva, pages 1?4.
131
Proceedings of SIGDIAL 2009: the 10th Annual Meeting of the Special Interest Group in Discourse and Dialogue, pages 349?356,
Queen Mary University of London, September 2009. c?2009 Association for Computational Linguistics
A Handsome Set of Metrics to Measure Utterance Classification
Performance in Spoken Dialog Systems
David Suendermann, Jackson Liscombe, Krishna Dayanidhi, Roberto Pieraccini?
SpeechCycle Labs, New York, USA
{david, jackson, krishna, roberto}@speechcycle.com
Abstract
We present a set of metrics describing
classification performance for individual
contexts of a spoken dialog system as well
as for the entire system. We show how
these metrics can be used to train and tune
system components and how they are re-
lated to Caller Experience, a subjective
measure describing how well a caller was
treated by the dialog system.
1 Introduction
Most of the speech recognition contexts in com-
mercial spoken dialog systems aim at mapping the
caller input to one out of a set of context-specific
semantic classes (Knight et al, 2001). This is done
by providing a grammar to the speech recognizer
at a given recognition context. A grammar serves
two purposes:
? It constraints the lexical content the recog-
nizer is able to recognize in this context (the
language model) and
? It assigns one out of a set of possible classes
to the recognition hypothesis (the classifier).
This basic concept is independent of the nature of
a grammar: it can be a rule-based one, manually or
automatically generated; it can comprise a statisti-
cal language model and a classifier; it can consist
of sets of grammars, language models, or classi-
fiers; or it can be a holistic grammar, i.e., a sta-
tistical model combining a language model and a
classification model in one large search tree.
Most commercial dialog systems utilize gram-
mars that return a semantic parse in one of these
contexts:
? directed dialogs (e.g., yes/no contexts, menus
with several choices, collection of informa-
tion out of a restricted set [Which type of
modem do you have?]?usually, less than 50
classes)
? open-ended prompts (e.g. for call routing,
problem capture; likewise to collect infor-
mation out of a restricted set [Tell me what
?Patent pending.
you are calling about today]?possibly sev-
eral hundred classes (Gorin et al, 1997; Boye
and Wiren, 2007))
? information collection out of a huge (or infi-
nite) set of classes (e.g., collection of phone
numbers, dates, names, etc.)
When the performance of spoken dialog sys-
tems is to be measured, there is a multitude of
objective metrics to do so, many of which feature
major disadvantages. Examples include
? Completion rate is calculated as the number
of completed calls divided by the total num-
ber of calls. The main disadvantage of this
metric is that it is influenced by many fac-
tors out of the system?s control, such as caller
hang-ups, opt-outs, or call reasons that fall
out of the system?s scope. Furthermore, there
are several system characteristics that impact
this metric, such as recognition performance,
dialog design, technical stability, availability
of back-end integration, etc. As experience
shows, all of these factors can have unpre-
dictable influence on the completion rate. On
the one hand, a simple wording change in the
introduction prompt of a system can make
this rate improve significantly, whereas, on
the other hand, major improvement of the
open-ended speech recognition grammar fol-
lowing this very prompt may not have any
impact.
? Average holding time is a common term for
the average call duration. This metric is often
considered to be quite controversial since it is
unclear whether longer calls are preferred or
dispreferred. Consider the following two in-
congruous behaviors resulting in longer call
duration:
? The system fails to appropriately treat
callers, asking too many questions, per-
forming redundant operations, acting
unintelligently because of missing back-
end integration, or letting the caller wait
in never-ending wait music loops.
? The system is so well-designed that it
engages callers to interact with the sys-
tem longer.
349
? Hang-up and opt-out rates. These metrics
try to encapsulate how many callers choose
not to use the dialog system, either because
they hang up or because they request to speak
with a human operator. However, it is unclear
how such events are related to dialog system
performance. Certainly, many callers may
have a prejudice against speaking with auto-
mated systems and may hang up or request
a human regardless of how well-performing
the dialog system is with cooperative users.
Furthermore, callers who hang up may do so
because they are unable to get their problem
solved or they may hang up precisely because
their problem was solved (instead of waiting
for the more felicitous post-problem-solving
dialog modules).
? Retry rate is calculated as the average num-
ber of times that the system has to re-prompt
for caller input because the caller?s previ-
ous utterance was determined to be Out-of-
Grammar. The intuition behind this metric
is that the lower the retry rate, the better
the system. However, this metric is prob-
lematic because it is tied to grammar per-
formance itself. Consider a well-performing
grammar that correctly accepts In-Grammar
utterances and rejects Out-of-Grammar utter-
ances. This grammar will cause the system to
produce retries for all Out-of-Grammar utter-
ances. Consider a poorly designed grammar
that accepts everything (incorrectly), even
background noise. This grammar would de-
crease the retry rate but would not be indica-
tive of a well-performing dialog system.
As opposed to these objective measures, there is
a subjective measure directly related to the system
performance as perceived by the user:
? Caller Experience. This metric is used to
describe how well the caller is treated by the
system according to its design. Caller Expe-
rience is measured on a scale between 1 (bad)
and 5 (excellent). This is the only subjective
measure in this list and is usually estimated
based on averaging scores given by multi-
ple voice user interface experts which listen
to multiple full calls. Although this metric
directly represents the ultimate design goal
for spoken dialog systems?i.e., to achieve
highest possible user experience?it is very
expensive to be repeatedly produced and not
suitable to be generated on-the-fly.
Our former research has suggested, however,
that it may be possible to automatically esti-
mate Caller Experience based on several ob-
jective measures (Evanini et al, 2008). These
measures include the overall number of no-
matches and substitutions in a call, opera-
tor requests, hang-ups, non-heard speech, the
fact whether the call reason could be suc-
cessfully captured and whether the call rea-
son was finally satisfied. Initial experiments
showed a near-human accuracy of the auto-
matic predictor trained on several hundred
calls with available manual Caller Experi-
ence scores. The most powerful objective
metric turned out to be the overall number
of no-matches and substitutions, indicating a
high correlation between the latter and Caller
Experience.
No-matches and substitutions are objective met-
rics defined in the scope of semantic classification
of caller utterances. They are part of a larger set of
semantic classification metrics which we system-
atically demonstrate in Section 2. The remainder
of the paper examines three case studies exploring
the usefulness and interplay of different evaluation
metrics, including:
? the correlation between True Total (one of the
introduced metrics) and Caller Experience in
Section 3,
? the estimation of speech recognition and clas-
sification parameters based on True Total and
True Confirm Total (another metric) in Sec-
tion 4, and
? the tuning of large-scale spoken dialog sys-
tems to maximize True Total and its effect on
Caller Experience in Section 5.
2 Metrics for Utterance Classification
Acoustic events processed by spoken dialog sys-
tems are usually split into two main categories:
In-Grammar and Out-of-Grammar. In-Grammar
utterances are all those that belong to one of the
semantic classes processable by the system logic
in the given context. Out-of-Grammar utterances
comprise all remaining events, such as utterances
whose meanings are not handled by the grammar
or when the input is non-speech noise.
Spoken dialog systems usually respond to
acoustic events after being processed by the gram-
mar in one of three ways:
? The event gets rejected. This is when the sys-
tem either assumes that the event was Out-
of-Grammar, or it is so uncertain about its
(In-Grammar) finding that it rejects the utter-
ance. Most often, the callers get re-prompted
for their input.
350
Table 1: Event Acronyms
I In-Grammar
O Out-of-Grammar
A Accept
R Reject
C Correct
W Wrong
Y Confirm
N Not-Confirm
TA True Accept
FA False Accept
TR True Reject
FR False Reject
TAC True Accept Correct
TAW True Accept Wrong
FRC False Reject Correct
FRW False Reject Wrong
FAC False Accept Confirm
FAA False Accept Accept
TACC True Accept Correct Confirm
TACA True Accept Correct Accept
TAWC True Accept Wrong Confirm
TAWA True Accept Wrong Accept
TT True Total
TCT True Confirm Total
? The event gets accepted. This is when the
system is certain to have correctly detected
an In-Grammar semantic class.
? The event gets confirmed. This is when the
system assumes to have correctly detected an
In-Grammar class but still is not absolutely
certain about it. Consequently, the caller is
asked to verify the class. Historically, confir-
mations are not used in many contexts where
they would sound confusing or distracting,
for instance in yes/no contexts (?I am sorry.
Did you say NO????No!???This was NO,
yes????No!!!?).
Based on these categories, an acoustic event and
how the system responds to it can be described by
four binary questions:
1. Is the event In-Grammar?
2. Is the event accepted?
3. Is the event correctly classified?
4. Is the event confirmed?
Now, we can draw a diagram containing the first
two questions as in Table 2. See Table 1 for all
Table 2: In-Grammar? Accepted?
A R
I TA FR
O FA TR
Table 3: In-Grammar? Accepted? Correct?
A R
C W C W
I TAC TAW FRC FRW
O FA TR
acoustic event classification types used in the re-
mainder of this paper.
Extending the diagram to include the third ques-
tion is only applicable to In-Grammar events since
Out-of-Grammar is a single class and, therefore,
can only be either falsely accepted or correctly re-
jected as shown in Table 3.
Further extending the diagram to accomodate
the fourth question on whether a recognized class
was confirmed is similarly only applicable if an
event was accepted, as rejections are never con-
firmed; see Table 4. Table 5 gives one example for
each of the above introduced events for a yes/no
grammar.
When the performance of a given recognition
context is to be measured, one can collect a cer-
tain number of utterances recorded in this context,
look at the recognition and application logs to see
whether these utterances where accepted or con-
firmed and which class they were assigned to, tran-
scribe and annotate the utterances for their seman-
tic class and finally count the events and divide
them by the total number of utterances. If X is an
event from the list in Table 1, we want to refer to
x as this average score, e.g., tac is the fraction of
total events correctly accepted. One characteristic
of these scores is that they sum up to 1 for each of
the Diagrams 2 to 4 as for example
a + r = 1, (1)
i + o = 1, (2)
ta + fr + fa + tr = 1. (3)
In order to enable system tuning and to report
system performance at-a-glance, the multitude of
metrics must be consolidated into a single power-
ful metric. In the industry, one often uses weights
to combine metrics since they are assumed to have
different importance. For instance, a False Ac-
cept is considered worse than a False Reject since
the latter allows for correction in the first retry
whereas the former may lead the caller down the
351
Table 5: Examples for utterance classification metrics. This table shows the transcription of an utterance,
the semantic class it maps to (if In-Grammar), a binary flag for whether the utterance is In-Grammar, the
recognized class (i.e. the grammar output), a flag for whether the recognized class was accepted, a flag
for whether the recognized class was correct (i.e. matched the transcription?s semantic class), a flag
for whether the recognized class was confirmed, and the acronym of the type of event the respective
combination results in.
utterance class In-Grammar? rec. class accepted? correct? confirmed? event
yeah YES 1 I
what 0 O
NO 1 A
NO 0 R
no no no NO 1 NO 1 C
yes ma?am YES 1 NO 0 W
1 Y
0 N
i said no NO 1 YES 1 TA
oh my god 0 NO 1 FA
i can?t tell 0 NO 0 TR
yes always YES 1 YES 0 FR
yes i guess so YES 1 YES 1 1 TAC
no i don?t think so NO 1 YES 1 0 TAW
definitely yes YES 1 YES 0 1 FRC
no man NO 1 YES 0 0 FRW
sunshine 0 YES 1 1 FAC
choices 0 NO 1 0 FAA
right YES 1 YES 1 1 1 TACC
yup YES 1 YES 1 1 0 TACA
this is true YES 1 NO 1 0 1 TAWC
no nothing NO 1 YES 1 0 0 TAWA
Table 4: In-Grammar? Accepted? Correct? Con-
firmed?
A R
C W C W
Y TACC TAWC
I N TACA TAWA FRC FRW
Y FACO N FAA TR
wrong path. However, these weights are heavily
negotiable and depend on customer, application,
and even the recognition context, making it im-
possible to produce a comprehensive and widely
applicable consolidated metric. This is why we
propose to split the set of metrics into two groups:
good and bad. The sought-for consolidated met-
ric is the sum of all good metrics (hence, an over-
all accuracy) or, alternatively, the sum of all bad
events (overall error rate). In Tables 3 and 4, good
metrics are highlighted. Accordingly, we define
two consolidated metrics True Total and True Con-
firm Total as follows:
tt = tac + tr, (4)
tct = taca + tawc + fac + tr. (5)
In the aforementioned special case that a recog-
nition context never confirms, Equation 5 equals
Equation 4 since the confirmation terms tawc and
fac disappear.
The following sections report on three case
studies on the applicability of True Total and True
Confirm Total to the tuning of spoken dialog sys-
tems and how they relate to Caller Experience.
3 On the Correlation between True Total
and Caller Experience
As motivated in Section 1, initial experiments on
predicting Caller Experience based on objective
metrics indicated that there is a considerable cor-
relation between Caller Experience and semantic
352
Table 6: Pearson correlation coefficient for sev-
eral utterance classification metrics on the source
data.
A R
C W
I 0.394 -0.160 ......-0.230......
O -0.242 -0.155
r(TT) = 0.378
classification metrics such as those introduced in
Section 2. In the first of our case studies, this effect
is to be deeper analyzed and quantified. For this
purpose, we selected 446 calls from four different
spoken dialog systems of the customer service hot-
lines of three major cable service providers. The
spoken dialog systems comprised
? a call routing application?cf. (Suendermann
et al, 2008),
? a cable TV troubleshooting application,
? a broadband Internet troubleshooting appli-
cation, and
? a Voice-over-IP troubleshooting
application?see for instance (Acomb et
al., 2007).
The calls were evaluated by voice user interface
experts and Caller Experience was rated according
to the scale introduced in Section 1. Furthermore,
all speech recognition utterances (4480) were tran-
scribed and annotated with their semantic classes.
Thereafter, all utterance classification metrics in-
troduced in Section 2 were computed for every call
individually by averaging across all utterances of
a call. Finally, we applied the Pearson correlation
coefficient (Rodgers and Nicewander, 1988) to the
source data points to correlate the Caller Experi-
ence score of a single call to the metrics of the
same call. This was done in Table 6.
Looking at these numbers, whose magnitude is
rather low, one may be suspect of the findings.
E.g., |r(FR)| > |r(TAW)| suggesting that False
Reject has a more negative impact on Caller Expe-
rience than True Accept Wrong (aka Substitution)
which is against common experience. Reasons for
the messiness of the results are that
? Caller Experience is subjective and affected
by inter- and intra-expert inconsistency. E.g.,
in a consistency cross-validation test, we ob-
served identical calls rated by one subject as
1 and by another as 5.
Figure 1: Dependency between Caller Experience
and True Total.
? Caller Experience scores are discrete, and,
hence, can vary by ?1, even in case of strong
consistency.
? Although utterance classification metrics are
(almost) objective metrics measuring the per-
centage of how often certain events happen
in average, this average generated for indi-
vidual calls may not be very meaningful. For
instance, a very brief call with a single yes/no
utterance correctly classified results in the
same True Total score like a series of 50 cor-
rect recognitions in a 20-minutes conversa-
tion. While the latter is virtually impossible,
the former happens rather often and domi-
nates the picture.
? The sample size of the experiment conducted
in the present case study (446 calls) is per-
haps too small for deep analyses on events
rarely happening in the investigated calls.
Trying to overcome these problems, we com-
puted all utterance classification metrics intro-
duced in Section 2, grouping and averaging them
for the five distinct values of Caller Experience.
As an example, we show the almost linear graph
expressing the relationship between True Total and
Caller Experience in Figure 1. Applying the Pear-
son correlation coefficient to this five-point curve
yields r = 0.972 confirming that what we see is
pretty much a straight line. Comparing this value
to the coefficients produced by the individual met-
rics TAC, TAW, FR, FA, and TR as done in Ta-
ble 7, shows that no other line is as straight as the
one produced by True Total supposing its maxi-
mization to produce spoken dialog systems with
highest level of user experience.
353
Table 7: Pearson correlation coefficient for sev-
eral utterance classification metrics after group-
ing and averaging.
A R
C W
I 0.969 -0.917 ......-0.539......
O -0.953 -0.939
r(TT) = 0.972
4 Estimating Speech Parameters by
Maximizing True Total or True
Confirm Total
The previous section tried to shed some light on
the relationship between some of the utterance
classification metrics and Caller Experience. We
saw that, on average, increasing Caller Experience
comes with increasing True Total as the almost lin-
ear curve of Figure 1 supposes. As a consequence,
much of our effort was dedicated to maximizing
True Total in diverse scenarios. Speech recogni-
tion as well as semantic classification with all their
components (such as acoustic, language, and clas-
sification models) and parameters (such as acous-
tic and semantic rejection and confirmation confi-
dence thresholds, time-outs, etc.) was set up and
tuned to produce highest possible scores. This sec-
tion gives two examples of how parameter settings
influence True Total.
4.1 Acoustic Confirmation Threshold
When a speech recognizer produces a hypothesis
of what has been said, it also returns an acoustic
confidence score which the application can utilize
to decide whether to reject the utterance, confirm
it, or accept it right away. The setting of these
thresholds has obviously a large impact on Caller
Experience since the application is to reject as few
valid utterances as possible, not confirm every sin-
gle input, but, at the same time, not falsely accept
wrong hypotheses. It is also known that these set-
tings can strongly vary from context to context.
E.g., in announcements, where no caller input is
expected, but, nonetheless utterances like ?agent?
or ?help? are supposed to be recognized, rejection
must be used much more aggressively than in col-
lection contexts. True Total or True Confirm To-
tal are suitable measures to detect the optimum
tradeoff. Figure 2 shows the True Confirm Total
graph for a collection context with 30 distinguish-
able classes. At a confidence value of 0.12, there
is a local and global maximum indicating the opti-
mum setting for the confirmation threshold for this
grammar context.
Figure 2: Tuning the acoustic confirmation thresh-
old.
4.2 Maximum Speech Time-Out
This parameter influences the maximum time the
speech recognizer keeps recognizing once speech
has started until it gives up and discards the recog-
nition hypothesis. Maximum speech time-out is
primarily used to limit processor load on speech
recognition servers and avoid situations in which
line noise and other long-lasting events keep the
recognizer busy for an unnecessarily long time. As
it anecdotally happened to callers that they were
interrupted by the dialog system, on the one hand,
some voice user interface designers tend to chose
rather large values for this time-out setting, e.g.,
15 or 20 seconds. On the other hand, very long
speech input tends to produce more likely a clas-
sification error than shorter ones. Might there be a
setting which is optimum from the utterance clas-
sification point of view?
To investigate this behavior, we took 115,885
transcribed and annotated utterances collected in
the main collection context of a call routing ap-
plication and aligned them to their utterance dura-
Figure 3: Dependency between utterance duration
and True Total.
354
Figure 4: Dependency between maximum speech
time-out and True Total.
tions. Then, we ordered the utterances in descend-
ing order of their duration, grouped always 1000
successive utterances together, and averaged over
duration and True Total. This generated 116 data
points showing the relationship between the dura-
tion of an utterance and its expected True Total,
see Figure 3.
The figure shows a clear maximum somewhere
around 2.5 seconds and then descends with in-
creasing duration towards zero. Utterances with
a duration of 9 seconds exhibited a very low True
Total score (20%). Furthermore, it would appear
that one should never allow utterances to exceed
four second in this context. However, upon fur-
ther evaluation of the situation, we also have to
consider that long utterances occur much less fre-
quently than short ones. To integrate the frequency
distribution into this analysis, we produced an-
other graph that shows the average True Total ac-
cumulated over all utterances shorter than a cer-
tain duration. This simulates the effect of using
a different maximum speech time-out setting and
is displayed in Figure 4. We also show a graph
on how many of the utterances would have been
interrupted in Figure 5.
The curve shows an interesting down-up-down
trajection which can be explained as follows:
? Acoustic events shorter than 1.0 seconds are
mostly noise events which are correctly iden-
tified since the speech recognizer could not
even build a search tree and returns an empty
hypothesis which the classifier, in turn, cor-
rectly rejects.
? Utterances with a duration around 1.5s are
dominated by single words which cannot
properly evaluated by the (trigram) language
model. So, the acoustic model takes over the
main work and, because of its imperfectness,
lowers the True Total.
Figure 5: Percentage of utterances interrupted by
maximum speech time-out.
? Utterances with a moderate number of words
are best covered by the language model, so
we achieve highest accuracy for them (?3s).
? The longer the utterances continues after 4
seconds, the less likely the language model
and classfier are to have seen such utterances,
and True Total declines.
Evaluating the case from the pure classifier per-
formance perspective, the maximum speech time-
out would have to be set to a very low value
(around 3 seconds). However, at this point, about
20% of the callers would be interrupted. The deci-
sion whether this optimimum should be accepcted
depends on how elegantly the interruption can be
designed:
?I?m so sorry to interrupt, but I?m hav-
ing a little trouble getting that. So, let?s
try this a different way.?
5 Continuous Tuning of a Spoken Dialog
System to Maximize True Total and Its
Effect on Caller Experience
In the last two sections, we investigated the corre-
lation between True Total and Caller Experience
and gave examples on how system parameters can
be tuned by maximizing True Total. The present
section gives a practical example of how rigorous
improvement of utterance classification leads to
real improvement of Caller Experience.
The application in question is a combination of
the four systems listed in Section 3 which work
in an interconnected fashion. When callers access
the service hotline, they are first asked to briefly
describe their call reason. After up to two follow-
up questions to further disambiguate their reason,
they are either connected to a human operator or
one of the three automated troubleshooting sys-
tems. Escalation from one of them can connect
355
Figure 6: Increase of the True Total of a large-
vocabulary grammar with more than 250 classes
over release time.
the caller to an agent, transfer the caller back to
the call router or to one of the other troubleshoot-
ing systems.
When the application was launched in June
2008, its True Total averaged 78%. During the fol-
lowing three months, almost 2.2 million utterances
were collected, transcribed, and annotated for their
semantic classes to train statistical update gram-
mars in a continuously running process (Suender-
mann et al, 2009). Whenever a grammar sig-
nificantly outperformed the most recent baseline,
it was released and put into production leading
to an incremental improvement of performance
throughout the application. As an example, Fig-
ure 6 shows the True Total increase of the top-level
large-vocabulary grammar that distinguishes more
than 250 classes. The overall performance of the
application went up to more than 90% True Total
within three months of its launch.
Having witnessed a significant gain of a spoken
dialog system?s True Total, we would now like to
know to what extent this improvement manifests
itself in an increase of Caller Experience. Fig-
ure 7 shows that, indeed, Caller Experience was
strongly positively affected. Over the same three
month period, we achieved an iterative increase
from an initial Caller Experience of 3.4 to 4.6.
6 Conclusion
Several of our investigations have suggested a con-
siderable correlation between True Total, an objec-
tive utterance classification metric, and Caller Ex-
perience, a subjective score of overall system per-
formance usually rated by expert listeners. This
observation leads to our main conclusions:
? True Total and several of the other utterance
classification metrics introduced in this paper
can be used as input to a Caller Experience
predictor?as tentative results in (Evanini et
al., 2008) confirm.
Figure 7: Increase of Caller Experience over re-
lease time.
? Efforts towards improvement of speech
recognition in spoken dialog applications
should be focused on increasing True Total
since this will directly influence Caller Expe-
rience.
References
K. Acomb, J. Bloom, K. Dayanidhi, P. Hunter,
P. Krogh, E. Levin, and R. Pieraccini. 2007. Techni-
cal Support Dialog Systems: Issues, Problems, and
Solutions. In Proc. of the HLT-NAACL, Rochester,
USA.
J. Boye and M. Wiren. 2007. Multi-Slot Semantics for
Natural-Language Call Routing Systems. In Proc.
of the HLT-NAACL, Rochester, USA.
K. Evanini, P. Hunter, J. Liscombe, D. Suendermann,
K. Dayanidhi, and R. Pieraccini:. 2008. Caller Ex-
perience: A Method for Evaluating Dialog Systems
and Its Automatic Prediction. In Proc. of the SLT,
Goa, India.
A. Gorin, G. Riccardi, and J. Wright. 1997. How May
I Help You? Speech Communication, 23(1/2).
S. Knight, G. Gorrell, M. Rayner, D. Milward, R. Koel-
ing, and I. Lewin. 2001. Comparing Grammar-
Based and Robust Approaches to Speech Under-
standing: A Case Study. In Proc. of the Eurospeech,
Aalborg, Denmark.
J. Rodgers and W. Nicewander. 1988. Thirteen Ways
to Look at the Correlation Coefficient. The Ameri-
can Statistician, 42(1).
D. Suendermann, P. Hunter, and R. Pieraccini. 2008.
Call Classification with Hundreds of Classes and
Hundred Thousands of Training Utterances ... and
No Target Domain Data. In Proc. of the PIT, Kloster
Irsee, Germany.
D. Suendermann, J. Liscombe, K. Evanini,
K. Dayanidhi, and R. Pieraccini. 2009. From
Rule-Based to Statistical Grammars: Continu-
ous Improvement of Large-Scale Spoken Dialog
Systems. In Proc. of the ICASSP, Taipei, Taiwan.
356
Proceedings of SIGDIAL 2010: the 11th Annual Meeting of the Special Interest Group on Discourse and Dialogue, pages 257?260,
The University of Tokyo, September 24-25, 2010. c?2010 Association for Computational Linguistics
How to Drink from a Fire Hose:
One Person Can Annoscribe 693 Thousand Utterances in One Month
David Suendermann, Jackson Liscombe, Roberto Pieraccini
SpeechCycle Labs
New York, USA
{david, jackson, roberto}@speechcycle.com
Abstract
.
Transcription and semantic annotation
(annoscription) of utterances is crucial
part of speech performance analysis and
tuning of spoken dialog systems and other
natural language processing disciplines.
However, the fact that these are manual
tasks makes them expensive and slow. In
this paper, we will discuss how anno-
scription can be partially automated. We
will show that annoscription can reach a
throughput of 693 thousand utterances per
person month under certain assumptions.
1 Introduction
Ever since spoken dialog systems entered the com-
mercial market in the mid 1990s, the caller?s
speech input is subject to collection, transcription,
and often also semantic annotation. Utterance
transcriptions and annotations (annoscriptions) are
used to measure speech recognition and spoken
language understanding performance of the appli-
cation. Furthermore, they are used to improve
speech recognition and application functionality
by tuning grammars, introducing new transitions
in the call flow to cover more of the callers? de-
mands, or changing prompt wording or applica-
tion logic to influence the speech input. Anno-
scriptions are also crucial for training statistical
language models and utterance classifiers for call
routing or other unconstrained speech input con-
texts (Gorin et al, 1997). Since very recently, sta-
tistical methods are used to replace conventional
rule-based grammars in every recognition context
of commercial spoken dialog systems (Suender-
mann et al, 2009b). This replacement is only
possible by collecting massive amounts of anno-
scribed data from all contexts of an application.
To give the reader an idea of what massive means
in this case, in (Suendermann et al, 2009b), we
used 2,184,203 utterances to build a complex call
routing system. In (Suendermann et al, 2009a),
4,293,898 utterances were used to localize an En-
glish Internet troubleshooting application to Span-
ish.
Considering that professional service providers
may charge as much as 50 US cents for annoscrib-
ing a single utterance, the usage of these amounts
of data seems prohibitive since costs for such a
project could potentially add up to several million
US dollars. Furthermore, one has to consider the
average speed of annoscription which rarely ex-
ceeds 1000 utterances per hour and person. This
means that the turn-around of a project as men-
tioned above would be several years unless teams
of many people work simultaneously. However,
the integration of the work of a large team be-
comes the more tricky the more people are in-
volved. This is especially true for the annotation
portion since it requires a thorough understand-
ing of the spoken dialog system?s domain and de-
sign and very often can only be conducted under
close supervision by the interaction designer in
charge of the project. Furthermore, there are cru-
cial issues related to intra- and inter-labeler incon-
sistency becoming more critical the more people
work on the same or similar recognition contexts
of a given project.
This paper is to show how it is possible to au-
tomate large portions of both transcription and an-
notation while meeting human performance1 stan-
dards. As an example case, we show how the pro-
posed automation techniques can increase anno-
scription speed to nearly 693 thousand utterances
per person and month.
2 Automatic Transcription
2.1 Two Fundamentals
Automatic transcription of spoken utterances may
not sound as something new to the reader. In
fact, the entire field of automatic speech recogni-
tion is about machine transcription. So, why is it
worth dedicating a full section to something well-
covered in research and industry for half a cen-
tury? The reason is the demand for achieving hu-
man performance as formulated in the introduc-
tion which, as is also well-known, cannot be satis-
fied by any of the large-vocabulary speech recog-
nizers ever developed. In order to demonstrate that
there is indeed a way to achieve human transcrip-
tion performance using automatic speech recogni-
tion, we would like to refer to two fundamental
observations on the performance of speech recog-
1In this paper, performance stands for quality or accuracy
of transcription or annotation. It does not refer to speed or
throughput.
257
nition:
(1) Speech recognition performance can be very
high for contexts of constrained vocabulary. An
example is the recognition of isolated letters in
the scope of a name spelling task as discussed
in (Waibel and Lee, 1990) that achieved a word
error rate of only 1.1%. In contrast, the word error
rate of large-vocabulary continuous speech recog-
nition can be as high as 40 to 65% on telephone
speech (Yuk and Flanagan, 1999).
(2) The positive dependence between speech
recognition performance and amount of data used
to train acoustic and language models, so far, did
not reach a saturation point even considering bil-
lions of training tokens (Och, 2006).
Both of these fundamentals can be applied to the
transcription task for utterances collected on spo-
ken dialog production systems as follows:
(1) The vocabulary of spoken dialog systems can
be rather complex. E.g., the caller utterances used
for the localization project mentioned in Section 1
distinguish more than 13,000 types. However,
the nature of commercial spoken dialog applica-
tions being mostly system-driven strongly con-
strains the vocabulary in many recognition con-
texts. E.g., when the prompt reads
You can say: recording problems, new
installation, frozen screen, or won?t turn
on
callers mostly respond things matching the pro-
posed phrases, occasionally altering the wording,
and only seldomly using completely unexpected
utterances.
(2) The continuous data feed available on high-
traffic spoken dialog systems in production pro-
cessing millions of calls per month can provide
large numbers of utterances for every possible
recognition context. Even if the context appears to
be of a simple nature, as for a yes/no question, the
continuous collection of more data will still have
an impact on the performance of a language model
built using this data.
2.2 How to Achieve Human Performance
Even though we have suggested that the recog-
nition performance in many contexts of spoken
dialog systems may be very high, we have still
not shown how our observations can be utilized to
achieve human performance as demanded in Sec-
tion 1. How would a context-dependent speech
recognizer respond when the caller says some-
thing completely unexpected such as let?s wreck a
nice beach when asked for the cell phone number?
While a human transcriber may still be able to cor-
rectly transcribe this sentence, automatic speech
recognition will certainly fail even with the largest
possible training set. The answer to this question
is that the speech recognizer should not respond at
all in this case but admit that it had trouble rec-
ognizing this utterance. Rejection of hypotheses
based on confidence scores is common practice in
many speech and language processing tasks and
is heavily used in spoken dialog systems to avoid
mis-interpretation of user inputs.
So, we now know that we can limit automatic
transcriptions to hypotheses of a minimum relia-
bility. However, how do we prove that this limited
set resembles human performance? What is actu-
ally human performance? Does the human make
errors transcribing? And, if so, how do we mea-
sure human error? What do we compare it against?
To err is human. Accordingly, there is an error
associated with manual transcription which can
only be estimated by comparing somebody?s tran-
scription with somebody else?s due to a lack of
ground truth. Preferably, one should have a good
number of people transcribe the same speech ut-
terances and than compute the average word error
rate comparing every transcription batch with ev-
ery other producing a reliable estimate of the man-
ual error inherent to the transcription task of spo-
ken dialog system utterances. In order to do so,
we compared transcriptions of 258,843 utterances
collected from a variety of applications and recog-
nition contexts partially shared by up to six tran-
scribers and found that they averaged at an inter-
transcriber word error rate of WER0 = 1.3%.
Now, for every recognition context a language
model had been trained, we performed automatic
speech recognition on held-out test sets of N =
1000 utterances producing N hypotheses and their
associated confidence scores P = {p1, . . . pN}.
Now, we determined that minimum confidence
threshold p0 for which the word error rate between
the set of hypotheses and manual reference tran-
scriptions was not statistically significantly greater
than WER0:
p0 = arg min
p?P
WER(V (p)) ?6> WER0; (1)
V (p) = {?1, . . . , ?K} : ?k ? {1, . . . , N}, p?k ? p.
Statistical significance was achieved when the
delta resulted in a p value greater than 0.05 using
the ?2 calculus. For the number of test utterances,
1000, this point is reached when the word error
on the test set falls below WER1 = 2.2%. This
means that Equation 2.2?s ?not statistically signifi-
cantly greater than? sign can be replaced by a reg-
ular smaller-than sign as
WER ?6> WER0 ? WER < WER1. (2)
This essentially means that there is a chance that
the error produced by automatic transcription is
greater than that of manual transcription, however,
on the test set it could not be found to be of signifi-
cance. Requesting to lower the p value or even de-
manding that the test set performance falls below
the reported manual error can drastically lower the
automation rate and, in the latter case, is not even
reasonable?how can a machine possibly commit
258
tr
ai
n
in
g
u
tte
ra
n
ce
s
. tran
scriptio
n
auto
m
atio
n
rate
. training date
Figure 1: Dependency between amount of training
data and transcription automation rate
less errors than a human being as it is trained on
human transcriptions?
As a proof of concept, we ran automatic tran-
scription against the same set of utterances used
to determine the manual transcription error, and
we found that the average word error rate between
manual and automatic annotation was as low as
1.1% for all utterances whose confidence score ex-
ceeded the context-dependent threshold trained as
described above. In this initial experiment, a total
of 60,608 utterances, i.e., 23.4%, had been auto-
mated.
2.3 On Automation Rate
Formally, transcription automation rate is the ra-
tio of utterances whose confidence exeeded p0 in
Equation 2.2:
transcription automation rate = |V (p0)|
N
(3)
where |V | refers to the cardinality of the set V ,
i.e., the number of V ?s members.
The above example?s transcription automation
rate of 23.4% does not yet sound tremendously
high, so we should look at what can be done to
increase the automation rate as much as possible.
It is predictable that the two fundamentals formu-
lated in Section 2.1 have a large impact on recog-
nition performance and, hence, the transcription
automation rate:
(1) In large-scale experiments, we were able to
show a significant (negative) correlation between
the annotation automation rate and task complex-
ity. Since this study does not fit the present paper?s
scope, we will refrain from reporting on details at
this point.
(2) As an example which influence the amount of
training data can have on the transcription automa-
tion rate, Figure 1 shows statistics drawn from
twenty runs of language model training carried out
over the course of seven months while collecting
more and more data.
3 Automatic Annotation
Semantic annotation of utterances into one of a fi-
nal set of classes is a task which may require pro-
found understanding of the application and recog-
nition context the specific utterances were col-
lected in. Examples include simple contexts such
as yes/no questions which may be easily manage-
able also by annotators unfamiliar with the ap-
plication, high-resolution open prompt contexts
with hundreds of technical and highly application-
specific classes, or number collection contexts al-
lowing for billions of classes. All these contexts
can benefit from two rules which help to signifi-
cantly reduce an annotator?s workload:
(A) Never do anything twice. This simple state-
ment means that there should be functionality built
into the annotation software or the underlying
database that
? lets the annotator process multiple utterances
with identical transcription in a single step and
? makes sure that whenever a new utterance shows
up with a transcription identical to a formerly an-
notated one, the new utterance gets assigned the
same class automatically.
Figure 2 demonstrates the impact of Rule (A) with
two typical examples. The first is a yes/no context
allowing for the additional global commands help,
hold, agent, repeat, and i don?t know. The other is
an open prompt context distinguishing 79 classes.
When using the token/type distinction, the im-
pact of Rule (A) is that annotation effort becomes
linear with the number of types to work on. While
the ratio between types and tokens in a given cor-
pus can be very small (i.e., the automation rate is
very high, e.g., 95% in the above yes/no example),
this ratio reaches saturation at some point. In the
yes/no example, there is only a gradual difference
between the automation rates for 10 thousand and
1 million utterances. Hence, at a certain point, the
effort becomes virtually linear with the number of
tokens to be processed.
(B) Predict as much as possible. Most of the
recognition contexts for which utterances are tran-
scribed and annotated use grammars to implement
speech recognition functionality. Many of these
an
n
o
ta
tio
n
au
to
m
at
io
n
ra
te
. training utterances
Figure 2: Dependency between number of col-
lected utterances and annotation automation rate
based on Rule (A) for two different contexts
259
Table 1: Annotation automation rates for three dif-
ferent recognition contexts based on Rule (B)
.
grammar #symptoms ann. auto. rate
modem type 43 70.3%
blue/black/snow 10 77.0%
yes/no 10 88.6%
grammars will be rule-based grammars. Even if
the grammars are statistical, most often, earlier
in time, rule-based grammars had been used in
the same recognition context. Hence, we can as-
sume that we are given rule-based grammars for
many recognition contexts of the dialog system
in question. Per definition, rule-based grammars
shall contain canonical rules expressing the rela-
tionship between expected utterances in a given
context and the semantic classes these utterances
are to be associated with. Consequently, when-
ever for an utterance recorded in the context un-
der consideration there is a rule in the grammar,
it provides the correct class for this utterance, and
it can be excluded from annotation. These rules
can be strongly extended to allow for complex pre-
fix and suffix rules, repetitions, sub-grammars &c.
making sure that the majority of utterances will
be covered by the rule-based grammars thereby
minimizing the annotation effort. Table 1 shows
three example grammars of different complex-
ity: One that collects the type of the caller?s mo-
dem, one for the identification of a TV set?s pic-
ture color (blue/black/snow), and a yes/no con-
text with global commands. Annotation automa-
tion rates for these grammars that were not specif-
ically tuned for maximizing automation but di-
rectly taken from the production dialog systems
varied between 70.3% and 88.6%.
To never ever touch a formerly annotated utter-
ance type again and to blindly rely on (mayby out-
dated or erroneous) rule-based grammars to pro-
vide baseline annotations may result in annota-
tion mistakes, possibly major ones when frequent
utterances are concerned. So, how do we make
sure that high annotation performance standards
are met?
To answer this question, the authors have de-
veloped a set of techniques called C7 taking care
of completeness, consistency, congruence, corre-
lation, confusion, coverage, and corpus size of an
annotation set (Suendermann et al, 2008). The
mentioned techniques are also useful in the fre-
quent event of changes to the number or scope of
annotation classes. This can happen e.g. due to
functional changes to the application, changes to
prompts, user behavior, or to contexts preceeding
the current annotation context. Another frequent
reason is the introduction of additional classes to
enlarge the scope of the current context2.
2In a specific context, callers may be asked whether they
want A, B, or C, but they may respond D. The introduc-
tion of a new class D which the application is able to handle
4 693 Thousand Utterances
Finally, we want to return to the initial statement
of this paper claiming that one person is able to
annoscribe 693 thousand utterances within one
month. An approximated automation rate of 80%
for transcription and 90% for annotation is possi-
ble when there is already a massive database of
annoscriptions available to be exploited for au-
tomation. These rates result in about 139 thou-
sand transcriptions and 69 thousand annotations
outstanding. At a pace of 1000 transcribed or 2000
annotated utterances per hour, the required time
would be 139 hours transcription and 35 hours an-
notation which averages at 40 hours per week3.
5 Conclusion
This paper has demonstrated how automated
annoscription of utterances collected in the
production scope of spoken dialog systems can
effectively accelerate this conventionally entirely
manual effort. When allowing for some overtime,
we have shown that a single person is able to
produce 693 thousand annoscriptions within one
month.
References
A. Gorin, G. Riccardi, and J. Wright. 1997. How May
I Help You? Speech Communication, 23(1/2).
F. Och. 2006. Challenges in Machine Translation. In
Proc. of the TC-Star Workshop, Barcelona, Spain.
D. Suendermann, J. Liscombe, K. Evanini,
K. Dayanidhi, and R. Pieraccini. 2008. C5.
In Proc. of the SLT, Goa, India.
D. Suendermann, J. Liscombe, K. Dayanidhi, and
R. Pieraccini. 2009a. Localization of Speech
Recognition in Spoken Dialog Systems: How Ma-
chine Translation Can Make Our Lives Easier. In
Proc. of the Interspeech, Brighton, UK.
D. Suendermann, J. Liscombe, K. Evanini,
K. Dayanidhi, and R. Pieraccini. 2009b. From
Rule-Based to Statistical Grammars: Continu-
ous Improvement of Large-Scale Spoken Dialog
Systems. In Proc. of the ICASSP, Taipei, Taiwan.
A. Waibel and K.-F. Lee. 1990. Readings in Speech
Recognition. Morgan Kaufmann, San Francisco,
USA.
D. Yuk and J. Flanagan. 1999. Telephone Speech
Recognition Using Neural Networks and Hidden
Markov Models. In Proc. of the ICASSP, Phoenix,
USA.
requires the re-annotation of all utterances falling into D?s
scope.
3The original title of this paper claimed that one person
could annoscribe even one million utterances in a month.
However, after receiving multiple complaints about the un-
lawfulness of a 58-hour workweek, we had to change the title
accordingly to avoid disputes with the Department of Labor.
Furthermore, as discussed earlier, at the starting point of an
annoscription project, automation rates are much lower than
later.
260
